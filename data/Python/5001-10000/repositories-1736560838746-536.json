{
  "metadata": {
    "timestamp": 1736560838746,
    "page": 536,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/CogVLM",
      "stars": 6260,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".deepspeed_env",
          "type": "blob",
          "size": 0.0400390625,
          "content": "SAT_HOME=~/.sat_models\nLOCAL_WORLD_SIZE=8"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1455078125,
          "content": ".hypothesis/\n__pycache__\noutput.png\nfewshot-data/\ncheckpoints/\nrecords.db\nserver.py\nexamples/*grounding.png\narchive*\nhostfile\nruns/\n*.idea/\n.DS_Store"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0849609375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2024 CogVLM team @ Zhipu AI\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 5.0703125,
          "content": "The CogVLM License\n\n1. Definitions\n\nâ€œLicensorâ€ means the CogVLM Model Team that distributes its Software.\n\nâ€œSoftwareâ€ means the CogVLM model parameters made available under this license.\n\n2. License Grant\n\nUnder the terms and conditions of this license, the Licensor hereby grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license.\nThis license permits you to use all open-source models in this repository for academic research free. Users who wish to use the models for commercial purposes must register [here](https://open.bigmodel.cn/mla/form).\nRegistered users may use the models for commercial activities free of charge, but must comply with all terms and conditions of this license.\nThe license notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of Peopleâ€™s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us at license@zhipuai.cn.\n\n7. Llama2 and EVA-CLIP2 License\n\nFor CogVLM-17B version, Llama2 license conditions (https://ai.meta.com/llama/license/) and EVA license conditions (MIT, https://github.com/baaivision/EVA/blob/master/LICENSE) Also applies to model weights.\n\n\n1. å®šä¹‰\n\nâ€œè®¸å¯æ–¹â€æ˜¯æŒ‡åˆ†å‘å…¶è½¯ä»¶çš„ CogVLM æ¨¡å‹å›¢é˜Ÿã€‚\n\nâ€œè½¯ä»¶â€æ˜¯æŒ‡æ ¹æ®æœ¬è®¸å¯æä¾›çš„ CogVLM æ¨¡å‹å‚æ•°ã€‚\n\n2. è®¸å¯æˆäºˆ\n\næ ¹æ®æœ¬è®¸å¯çš„æ¡æ¬¾å’Œæ¡ä»¶ï¼Œè®¸å¯æ–¹ç‰¹æ­¤æˆäºˆæ‚¨éæ’ä»–æ€§ã€å…¨çƒæ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯å†è®¸å¯ã€å¯æ’¤é”€ã€å…ç‰ˆç¨çš„ç‰ˆæƒè®¸å¯ã€‚\næœ¬è®¸å¯å…è®¸æ‚¨å…è´¹ä½¿ç”¨æœ¬ä»“åº“ä¸­çš„æ‰€æœ‰å¼€æºæ¨¡å‹è¿›è¡Œå­¦æœ¯ç ”ç©¶ï¼Œå¯¹äºå¸Œæœ›å°†æ¨¡å‹ç”¨äºå•†ä¸šç›®çš„çš„ç”¨æˆ·ï¼Œéœ€åœ¨[è¿™é‡Œ](https://open.bigmodel.cn/mla/form)å®Œæˆç™»è®°ã€‚\nç»è¿‡ç™»è®°çš„ç”¨æˆ·å¯ä»¥å…è´¹ä½¿ç”¨æœ¬æ¨¡å‹è¿›è¡Œå•†ä¸šæ´»åŠ¨ï¼Œä½†å¿…é¡»éµå®ˆæœ¬è®¸å¯çš„æ‰€æœ‰æ¡æ¬¾å’Œæ¡ä»¶ã€‚\nä¸Šè¿°ç‰ˆæƒå£°æ˜å’Œæœ¬è®¸å¯å£°æ˜åº”åŒ…å«åœ¨æœ¬è½¯ä»¶çš„æ‰€æœ‰å‰¯æœ¬æˆ–é‡è¦éƒ¨åˆ†ä¸­ã€‚\n\n3.é™åˆ¶\n\næ‚¨ä¸å¾—å‡ºäºä»»ä½•å†›äº‹æˆ–éæ³•ç›®çš„ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹ã€åˆå¹¶ã€å‘å¸ƒã€åˆ†å‘ã€å¤åˆ¶æˆ–åˆ›å»ºæœ¬è½¯ä»¶çš„å…¨éƒ¨æˆ–éƒ¨åˆ†è¡ç”Ÿä½œå“ã€‚\n\næ‚¨ä¸å¾—åˆ©ç”¨æœ¬è½¯ä»¶ä»äº‹ä»»ä½•å±å®³å›½å®¶å®‰å…¨å’Œå›½å®¶ç»Ÿä¸€ã€å±å®³ç¤¾ä¼šå…¬å…±åˆ©ç›Šã€ä¾µçŠ¯äººèº«æƒç›Šçš„è¡Œä¸ºã€‚\n\n4.å…è´£å£°æ˜\n\næœ¬è½¯ä»¶â€œæŒ‰åŸæ ·â€æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¯¹é€‚é”€æ€§ã€ç‰¹å®šç”¨é€”çš„é€‚ç”¨æ€§å’Œéä¾µæƒæ€§çš„ä¿è¯ã€‚ åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œä½œè€…æˆ–ç‰ˆæƒæŒæœ‰äººå‡ä¸å¯¹ä»»ä½•ç´¢èµ”ã€æŸå®³æˆ–å…¶ä»–è´£ä»»è´Ÿè´£ï¼Œæ— è®ºæ˜¯åœ¨åˆåŒè¯‰è®¼ã€ä¾µæƒè¡Œä¸ºè¿˜æ˜¯å…¶ä»–æ–¹é¢ï¼Œç”±è½¯ä»¶æˆ–è½¯ä»¶çš„ä½¿ç”¨æˆ–å…¶ä»–äº¤æ˜“å¼•èµ·ã€ç”±è½¯ä»¶å¼•èµ·æˆ–ä¸ä¹‹ç›¸å…³ è½¯ä»¶ã€‚\n\n5. è´£ä»»é™åˆ¶\n\né™¤é€‚ç”¨æ³•å¾‹ç¦æ­¢çš„èŒƒå›´å¤–ï¼Œåœ¨ä»»ä½•æƒ…å†µä¸‹ä¸”æ ¹æ®ä»»ä½•æ³•å¾‹ç†è®ºï¼Œæ— è®ºæ˜¯åŸºäºä¾µæƒè¡Œä¸ºã€ç–å¿½ã€åˆåŒã€è´£ä»»æˆ–å…¶ä»–åŸå› ï¼Œä»»ä½•è®¸å¯æ–¹å‡ä¸å¯¹æ‚¨æ‰¿æ‹…ä»»ä½•ç›´æ¥ã€é—´æ¥ã€ç‰¹æ®Šã€å¶ç„¶ã€ç¤ºèŒƒæ€§ã€ æˆ–é—´æ¥æŸå®³ï¼Œæˆ–ä»»ä½•å…¶ä»–å•†ä¸šæŸå¤±ï¼Œå³ä½¿è®¸å¯äººå·²è¢«å‘ŠçŸ¥æ­¤ç±»æŸå®³çš„å¯èƒ½æ€§ã€‚\n\n6.äº‰è®®è§£å†³\n\næœ¬è®¸å¯å—ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹ç®¡è¾–å¹¶æŒ‰å…¶è§£é‡Šã€‚ å› æœ¬è®¸å¯å¼•èµ·çš„æˆ–ä¸æœ¬è®¸å¯æœ‰å…³çš„ä»»ä½•äº‰è®®åº”æäº¤åŒ—äº¬å¸‚æµ·æ·€åŒºäººæ°‘æ³•é™¢ã€‚\n\nè¯·æ³¨æ„ï¼Œè®¸å¯è¯å¯èƒ½ä¼šæ›´æ–°åˆ°æ›´å…¨é¢çš„ç‰ˆæœ¬ã€‚ æœ‰å…³è®¸å¯å’Œç‰ˆæƒçš„ä»»ä½•é—®é¢˜ï¼Œè¯·é€šè¿‡ license@zhipuai.cn ä¸æˆ‘ä»¬è”ç³»ã€‚\n\n7. Llama2 å’Œ EVA-CLIP2 è®¸å¯\n\né’ˆå¯¹ CogVLM-17B ç‰ˆæœ¬ï¼Œ Llama2 è®¸å¯æ¡ä»¶ (https://ai.meta.com/llama/license/) å’Œ EVA è®¸å¯æ¡ä»¶ (MIT, https://github.com/baaivision/EVA/blob/master/LICENSE) åŒæ—¶é€‚ç”¨äºæ¨¡å‹æƒé‡ã€‚"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 27.92578125,
          "content": "# CogVLM & CogAgent\n\nğŸ“— [ä¸­æ–‡ç‰ˆREADME](./README_zh.md)\n\nğŸŒŸ **Jump to detailed introduction: [Introduction to CogVLM](#introduction-to-cogvlm)ï¼Œ\nğŸ†• [Introduction to CogAgent](#introduction-to-cogagent)**\n\nğŸ“” For more detailed usage information, please refer to: [CogVLM & CogAgent's technical documentation (in Chinese)](https://zhipu-ai.feishu.cn/wiki/LXQIwqo1OiIVTykMh9Lc3w1Fn7g) \n\n<table>\n  <tr>\n    <td>\n      <h2> CogVLM </h2>\n      <p> ğŸ“–  Paper: <a href=\"https://arxiv.org/abs/2311.03079\">CogVLM: Visual Expert for Pretrained Language Models</a></p>\n      <p><b>CogVLM</b> is a powerful open-source visual language model (VLM). CogVLM-17B has 10 billion visual parameters and 7 billion language parameters, <b>supporting image understanding and multi-turn dialogue with a resolution of 490*490</b>.</p>\n      <p><b>CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks</b>, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC.</p>\n    </td>\n    <td>\n      <h2> CogAgent </h2>\n      <p> ğŸ“–  Paper: <a href=\"https://arxiv.org/abs/2312.08914\">CogAgent: A Visual Language Model for GUI Agents </a></p>\n      <p><b>CogAgent</b> is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters and 7 billion language parameters, <b>supporting image understanding at a resolution of 1120*1120</b>. <b>On top of the capabilities of CogVLM, it further possesses GUI image Agent capabilities</b>.</p>\n      <p> <b>CogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks</b>, including VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. <b>It significantly surpasses existing models on GUI operation datasets</b> including AITW and Mind2Web.</p>\n    </td>\n  </tr>\n  <tr>\n    <td colspan=\"2\" align=\"center\">\n      <p>ğŸŒ Web Demo for both CogVLM2: <a href=\"http://36.103.203.44:7861\">this link</a></p>\n    </td>\n  </tr>\n</table>\n\n\n**Table of Contents**\n\n- [CogVLM \\& CogAgent](#cogvlm--cogagent)\n    - [Release](#release)\n    - [Get Started](#get-started)\n        - [Option 1: Inference Using Web Demo.](#option-1-inference-using-web-demo)\n        - [Option 2ï¼šDeploy CogVLM / CogAgent by yourself](#option-2deploy-cogvlm--cogagent-by-yourself)\n            - [Situation 2.1 CLI (SAT version)](#situation-21-cli-sat-version)\n            - [Situation 2.2 CLI (Huggingface version)](#situation-22-cli-huggingface-version)\n            - [Situation 2.3 Web Demo](#situation-23-web-demo)\n        - [Option 3ï¼šFinetuning CogAgent / CogVLM](#option-3finetuning-cogagent--cogvlm)\n        - [Option 4: OpenAI Vision format](#option-4-openai-vision-format)\n        - [Hardware requirement](#hardware-requirement)\n        - [Model checkpoints](#model-checkpoints)\n    - [Introduction to CogVLM](#introduction-to-cogvlm)\n        - [Examples](#examples)\n    - [Introduction to CogAgent](#introduction-to-cogagent)\n        - [GUI Agent Examples](#gui-agent-examples)\n    - [Cookbook](#cookbook)\n        - [Task Prompts](#task-prompts)\n        - [Which --version to use](#which---version-to-use)\n        - [FAQ](#faq)\n    - [License](#license)\n    - [Citation \\& Acknowledgements](#citation--acknowledgements)\n\n## Release\n- ğŸ”¥ğŸ”¥ğŸ”¥  **News**: ```2024/5/20```: We released the **next generation of model, [CogVLM2](https://github.com/THUDM/CogVLM2)**, which is based on llama3-8b and on the par of (or better than) GPT-4V in most cases! DOWNLOAD and TRY!\n- ğŸ”¥ğŸ”¥  **News**: ```2024/4/5```: [CogAgent](https://arxiv.org/abs/2312.08914) was selected as a CVPR 2024 Highlights!\n- ğŸ”¥  **News**: ```2023/12/26```: We have released the [CogVLM-SFT-311K](dataset.md) dataset, \n  which contains over 150,000 pieces of data that we used for **CogVLM v1.0 only** training. Welcome to follow and use.\n- **News**: ```2023/12/18```: **New Web UI Launched!** We have launched a new web UI based on Streamlit,\n  users can painlessly talk to CogVLM, CogAgent in our UI. Have a better user experience.\n- **News**: ```2023/12/15```: **CogAgent Officially Launched!** CogAgent is an image understanding model developed\n  based on CogVLM. It features **visual-based GUI Agent capabilities** and has further enhancements in image\n  understanding. It supports image input with a resolution of 1120*1120, and possesses multiple abilities including\n  multi-turn dialogue with images, GUI Agent, Grounding, and more.\n\n- **News**: ```2023/12/8``` We have updated the checkpoint of cogvlm-grounding-generalist to\n  cogvlm-grounding-generalist-v1.1, with image augmentation during training, therefore more robust.\n  See [details](#introduction-to-cogvlm).\n\n- **News**: ```2023/12/7``` CogVLM supports **4-bit quantization** now! You can inference with just **11GB** GPU memory!\n\n- **News**: ```2023/11/20``` We have updated the checkpoint of cogvlm-chat to cogvlm-chat-v1.1, unified the versions of\n  chat and VQA, and refreshed the SOTA on various datasets. See [details](#introduction-to-cogvlm)\n\n- **News**: ```2023/11/20``` We release **[cogvlm-chat](https://huggingface.co/THUDM/cogvlm-chat-hf)**, **[cogvlm-grounding-generalist](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)/[base](https://huggingface.co/THUDM/cogvlm-grounding-base-hf)**, **[cogvlm-base-490](https://huggingface.co/THUDM/cogvlm-base-490-hf)/[224](https://huggingface.co/THUDM/cogvlm-base-224-hf)** on ğŸ¤—Huggingface. you can infer with transformers in [a few lines of code](#situation-22-cli-huggingface-version)now!\n\n- ```2023/10/27``` CogVLM bilingual version is available [online](https://chatglm.cn/)! Welcome to try it out!\n\n- ```2023/10/5``` CogVLM-17B releasedã€‚\n\n## Get Started\n\n### Option 1: Inference Using Web Demo.\n\n* Click here to enter [CogVLM2 Demo](http://36.103.203.44:7861/)ã€‚\n\nIf you need to use Agent and Grounding functions, please refer to [Cookbook - Task Prompts](#task-prompts)\n\n### Option 2ï¼šDeploy CogVLM / CogAgent by yourself\n\nWe support two GUIs for model inference, **CLI** and **web demo** . If you want to use it in your python code, it is\neasy to modify the CLI scripts for your case.\n\nFirst, we need to install the dependencies.\n\n```bash\n# CUDA >= 11.8\npip install -r requirements.txt\npython -m spacy download en_core_web_sm\n```\n\n**All code for inference is located under the ``basic_demo/`` directory. Please switch to this directory first before\nproceeding with further operations.**\n\n#### Situation 2.1 CLI (SAT version)\n\nRun CLI demo via:\n\n```bash\n# CogAgent\npython cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogagent-vqa --version chat_old --bf16  --stream_chat\n\n# CogVLM\npython cli_demo_sat.py --from_pretrained cogvlm-chat --version chat_old --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogvlm-grounding-generalist --version base --bf16  --stream_chat\n```\n\nThe program will automatically download the sat model and interact in the command line. You can generate replies by\nentering instructions and pressing enter.\nEnter `clear` to clear the conversation history and `stop` to stop the program.\n\nWe also support model parallel inference, which splits model to multiple (2/4/8) GPUs. `--nproc-per-node=[n]` in the\nfollowing command controls the number of used GPUs.\n\n```\ntorchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16\n```\n\n- If you want to manually download the weights, you can replace the path after ``--from_pretrained`` with the model\n  path.\n\n- Our model supports SAT's **4-bit quantization** and **8-bit quantization**.\n  You can change ``--bf16`` to ``--fp16``, or ``--fp16 --quant 4``, or ``--fp16 --quant 8``.\n\n  For example\n\n    ```bash\n    python cli_demo_sat.py --from_pretrained cogagent-chat --fp16 --quant 8 --stream_chat\n    python cli_demo_sat.py --from_pretrained cogvlm-chat-v1.1 --fp16 --quant 4 --stream_chat\n    # In SAT versionï¼Œ--quant should be used with --fp16\n    ```\n\n- The program provides the following hyperparameters to control the generation process:\n    ```\n    usage: cli_demo_sat.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE]\n\n    optional arguments:\n    -h, --help            show this help message and exit\n    --max_length MAX_LENGTH\n                            max length of the total sequence\n    --top_p TOP_P         top p for nucleus sampling\n    --top_k TOP_K         top k for top k sampling\n    --temperature TEMPERATURE\n                            temperature for sampling\n    ```\n\n- Click [here](#which---version-to-use) to view the correspondence between different models and the ``--version``\n  parameter.\n\n#### Situation 2.2 CLI (Huggingface version)\n\nRun CLI demo via:\n\n```bash\n# CogAgent\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-vqa-hf --bf16\n\n# CogVLM\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-grounding-generalist-hf --bf16\n```\n\n- If you want to manually download the weights, you can replace the path after ``--from_pretrained`` with the model\n  path.\n\n- You can change ``--bf16`` to ``--fp16``, or ``--quant 4``. For example, our model supports Huggingface's **4-bit\n  quantization**:\n\n    ```bash\n    python cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --quant 4\n    ```\n\n#### Situation 2.3 Web Demo\n\nWe also offer a local web demo based on Gradio. First, install Gradio by running: `pip install gradio`. Then download\nand enter this repository and run `web_demo.py`. See the next section for detailed usage:\n\n```bash\npython web_demo.py --from_pretrained cogagent-chat --version chat --bf16\npython web_demo.py --from_pretrained cogagent-vqa --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-chat-v1.1 --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --bf16\n```\n\nThe GUI of the web demo looks like:\n\n<div align=\"center\">\n    <img src=assets/web_demo-min.png width=70% />\n</div>\n\n### Option 3ï¼šFinetuning CogAgent / CogVLM\n\nYou may want to use CogVLM in your own task, which needs a **different output style or domain knowledge**. **All code\nfor finetuning is located under the ``finetune_demo/`` directory.**\n\nWe here provide a finetuning example for **Captcha Recognition** using lora.\n\n1. Start by downloading the [Captcha Images dataset](https://www.kaggle.com/datasets/aadhavvignesh/captcha-images). Once\n   downloaded, extract the contents of the ZIP file.\n\n2. To create a train/validation/test split in the ratio of 80/5/15, execute the following:\n    ```bash\n    python utils/split_dataset.py\n    ```\n\n3. Start the fine-tuning process with this command:\n\n    ```bash\n    bash finetune_demo/finetune_(cogagent/cogvlm)_lora.sh\n    ```\n\n4. Merge the model to `model_parallel_size=1`: (replace the 4 below with your training `MP_SIZE`)\n\n    ```bash\n    torchrun --standalone --nnodes=1 --nproc-per-node=4 utils/merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(cogagent/cogvlm490/cogvlm224)\n    ```\n\n5. Evaluate the performance of your model.\n    ```bash\n    bash finetune_demo/evaluate_(cogagent/cogvlm).sh\n    ```\n\n### Option 4: OpenAI Vision format\n\nWe provide the same API examples as `GPT-4V`, which you can view in `openai_demo`.\n\n1. First, start the node\n\n```\npython openai_demo/openai_api.py\n```\n\n2. Next, run the request example node, which is an example of a continuous dialogue\n\n```\npython openai_demo/openai_api_request.py\n```\n\n3. You will get output similar to the following\n\n```\nThis image showcases a tranquil natural scene with a wooden pathway leading through a field of lush green grass. In the distance, there are trees and some scattered structures, possibly houses or small buildings. The sky is clear with a few scattered clouds, suggesting a bright and sunny day.\n```\n\n### Hardware requirement\n\n* Model Inference:\n\n  For INT4 quantization: 1 * RTX 3090(24G)   (CogAgent takes ~ 12.6GB, CogVLM takes ~ 11GB)\n\n  For FP16: 1 * A100(80G) or 2 * RTX 3090(24G)\n\n* Finetuning:\n\n  For FP16: 4 * A100(80G) *[Recommend]* or 8* RTX 3090(24G).\n\n### Model checkpoints\n\nIf you run the `basic_demo/cli_demo*.py` from the code repository, it will automatically download SAT or Hugging Face\nweights. Alternatively, you can choose to manually download the necessary weights.\n\n- CogAgent\n\n  |   Model name    | Input resolution |                             Introduction                             | Huggingface model | SAT model |\n  | :-----------: | :----: | :----------------------------------------------------------: | :------: | :-------: |\n  | cogagent-chat |  1120  | Chat version of CogAgent. Supports GUI Agent, multiple-round  chat and visual grounding. |  [HF link](https://huggingface.co/THUDM/cogagent-chat-hf) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogagent-chat-hf)    |   [HF link](https://huggingface.co/THUDM/CogAgent/tree/main)<br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogAgent)           |\n  | cogagent-vqa |  1120  | VQA version of CogAgent. Has stronger capabilities in single-turn visual dialogue. Recommended for VQA benchmarks. |  [HF link](https://huggingface.co/THUDM/cogagent-vqa-hf)<br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogagent-vqa-hf)        |    [HF link](https://huggingface.co/THUDM/CogAgent/tree/main) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogAgent)      |\nc\n- CogVLM\n\n  |          Model name           | Input resolution |                           Introduction                            | Huggingface model | SAT model |\n  | :-------------------------: | :----: | :-------------------------------------------------------: | :------: | :-------: |\n  |         cogvlm-chat-v1.1         |  490   |  Supports multiple rounds of chat and vqa simultaneously, with different prompts.   |  [HF link](https://huggingface.co/THUDM/cogvlm-chat-hf) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogvlm-chat-hf)        |    [HF link](https://huggingface.co/THUDM/CogVLM/tree/main)  <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogVLM)       |\n  |       cogvlm-base-224       |  224   |               The original checkpoint after text-image pretraining.               |   [HF link](https://huggingface.co/THUDM/cogvlm-base-224-hf) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogvlm-base-224-hf)       |     [HF link](https://huggingface.co/THUDM/CogVLM/tree/main) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogVLM)       |\n  |       cogvlm-base-490       |  490   |      Amplify the resolution to 490 through position encoding interpolation from `cogvlm-base-224`.      |   [HF link](https://huggingface.co/THUDM/cogvlm-base-490-hf) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogvlm-base-490-hf)       |     [HF link](https://huggingface.co/THUDM/CogVLM/tree/main) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogVLM)       |\n  | cogvlm-grounding-generalist |  490   | This checkpoint supports different visual grounding tasks, e.g. REC, Grounding Captioning, etc.  |    [HF link](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)  <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogvlm-grounding-generalist-hf)       |     [HF link](https://huggingface.co/THUDM/CogVLM/tree/main)   <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogVLM)     |\n\n## Introduction to CogVLM\n\n- CogVLM is a powerful **open-source visual language model** (**VLM**). CogVLM-17B has 10 billion vision parameters and\n  7 billion language parameters.\n\n- CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k\n  captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2,\n  OKVQA, TextVQA, COCO captioning, etc., **surpassing or matching PaLI-X 55B**. CogVLM can\n  also [chat with you](http://36.103.203.44:7861) about images.\n\n<div align=\"center\">\n    <img src=assets/metrics-min.png width=50% />\n</div>\n\n<details>\n<summary>Click to view results on MM-VET, POPE, TouchStone. </summary>\n\n<table>\n    <tr>\n        <td>Method</td>\n        <td>LLM</td>\n        <td>MM-VET</td>\n        <td>POPE(adversarial)</td>\n        <td>TouchStone</td>\n    </tr>\n    <tr>\n        <td>BLIP-2</td>\n        <td>Vicuna-13B</td>\n        <td>22.4</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Otter</td>\n        <td>MPT-7B</td>\n        <td>24.7</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>MiniGPT4</td>\n        <td>Vicuna-13B</td>\n        <td>24.4</td>\n        <td>70.4</td>\n        <td>531.7</td>\n    </tr>\n    <tr>\n        <td>InstructBLIP</td>\n        <td>Vicuna-13B</td>\n        <td>25.6</td>\n        <td>77.3</td>\n        <td>552.4</td>\n    </tr>\n    <tr>\n        <td>LLaMA-Adapter v2</td>\n        <td>LLaMA-7B</td>\n        <td>31.4</td>\n        <td>-</td>\n        <td>590.1</td>\n    </tr>\n    <tr>\n        <td>LLaVA</td>\n        <td>LLaMA2-7B</td>\n        <td>28.1</td>\n        <td>66.3</td>\n        <td>602.7</td>\n    </tr>\n    <tr>\n        <td>mPLUG-Owl</td>\n        <td>LLaMA-7B</td>\n        <td>-</td>\n        <td>66.8</td>\n        <td>605.4</td>\n    </tr>\n    <tr>\n        <td>LLaVA-1.5</td>\n        <td>Vicuna-13B</td>\n        <td>36.3</td>\n        <td>84.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Emu</td>\n        <td>LLaMA-13B</td>\n        <td>36.3</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Qwen-VL-Chat</td>\n        <td>-</td>\n        <td>-</td>\n        <td>-</td>\n        <td>645.2</td>\n    </tr>\n    <tr>\n        <td>DreamLLM</td>\n        <td>Vicuna-7B</td>\n        <td>35.9</td>\n        <td>76.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>CogVLM</td>\n        <td>Vicuna-7B</td>\n        <td> <b>52.8</b> </td>\n        <td><b>87.6</b></td>\n        <td><b>742.0</b></td>\n    </tr>\n</table>\n\n</details>\n\n<details>\n<summary>Click to view results of cogvlm-grounding-generalist-v1.1. </summary>\n\n<table>\n    <tr>\n        <td></td>\n        <td>RefCOCO</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCO+</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCOg</td>\n        <td></td>\n        <td>Visual7W</td>\n    </tr>\n    <tr>\n        <td></td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>test</td>\n        <td>test</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist</td>\n        <td>92.51</td>\n        <td>93.95</td>\n        <td>88.73</td>\n        <td>87.52</td>\n        <td>91.81</td>\n        <td>81.43</td>\n        <td>89.46</td>\n        <td>90.09</td>\n        <td>90.96</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist-v1.1</td>\n        <td>**92.76**</td>\n        <td>**94.75**</td>\n        <td>**88.99**</td>\n        <td>**88.68**</td>\n        <td>**92.91**</td>\n        <td>**83.39**</td>\n        <td>**89.75**</td>\n        <td>**90.79**</td>\n        <td>**91.05**</td>\n    </tr>\n</table>\n</details>\n\n### Examples\n\n<!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description & Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. -->\n\n* CogVLM can accurately describe images in details with **very few hallucinations**.\n    <details>\n    <summary>Click for comparison with LLAVA-1.5 and MiniGPT-4.</summary>\n\n    <img src=assets/llava-comparison-min.png width=50% />\n\n    </details>\n    <br>\n\n* CogVLM can understand and answer various types of questions, and has a **visual grounding** version.\n\n<div align=\"center\">\n    <img src=assets/pear_grounding.png width=50% />\n</div>\n\n<br>\n\n* CogVLM sometimes captures more detailed content than GPT-4V(ision).\n\n<div align=\"center\">\n    <img src=assets/compare-min.png width=50% />\n</div>\n\n<!-- ![compare](assets/compare.png) -->\n<br> \n\n<details>\n<summary>Click to expand more examples.</summary>\n\n![Chat Examples](assets/chat.png)\n\n</details>\n\n## Introduction to CogAgent\n\nCogAgent is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters\nand 7 billion language parameters\n\nCogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks, including VQAv2,\nOK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. It significantly surpasses existing models on GUI\noperation datasets such as AITW and Mind2Web.\n\nIn addition to all the features already present in CogVLM (visual multi-round dialogue, visual grounding), CogAgent:\n\n1. Supports higher resolution visual input and dialogue question-answering. **It supports ultra-high-resolution image\n   inputs of 1120x1120.**\n\n2. **Possesses the capabilities of a visual Agent**, being able to return a plan, next action, and specific operations\n   with coordinates for any given task on any GUI screenshot.\n\n3. **Enhanced GUI-related question-answering capabilities**, allowing it to handle questions about any GUI screenshot,\n   such as web pages, PC apps, mobile applications, etc.\n\n4. Enhanced capabilities in OCR-related tasks through improved pre-training and fine-tuning.\n\n<div align=\"center\">\n    <img src=assets/cogagent_function.jpg width=60% />\n</div>\n\n### GUI Agent Examples\n\n<div align=\"center\">\n    <img src=assets/cogagent_main_demo.jpg width=90% />\n</div>\n\n## Cookbook\n\n### Task Prompts\n\n1. **General Multi-Round Dialogue**: Say whatever you want.\n\n2. **GUI Agent Task**: Use the [Agent template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761)\n   and replace \\<TASK\\> with the task instruction enclosed in double quotes. This query can make CogAgent infer Plan and\n   Next Action. If adding ``(with grounding)`` at the end of the query, the model will return a formalized action\n   representation with coordinates.\n\nFor example, to ask the model how to complete the task \"Search for CogVLM\" on a current GUI screenshot, follow these\nsteps:\n\n1. Randomly select a template from\n   the [Agent template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761). Here, we\n   choose ``What steps do I need to take to <TASK>?``.\n\n2. Replace <TASK> with the task instruction enclosed in double quotes, for\n   example, ``What steps do I need to take to \"Search for CogVLM\"?`` . Inputting this to the model yields:\n\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n\n3. If adding ``(with grounding)`` at the end, i.e. changing the input\n   to ``What steps do I need to take to \"Search for CogVLM\"?(with grounding)``, the output of CogAgent would be:\n\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n> Grounded Operation:[combobox] Search -> TYPE: CogVLM at the box [[212,498,787,564]]\n\nTip: For GUI Agent tasks, it is recommended to conduct only single-round dialogues for each image for better results.\n\n3. **Visual Grounding**. Three modes of grounding are supported:\n\n    - Image description with grounding coordinates (bounding box). Use any template\n      from [caption_with_box template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L537) as model\n      input. For example:\n\n   > Can you provide a description of the image and include the coordinates [[x0,y0,x1,y1]] for each mentioned object?\n\n    - Returning grounding coordinates (bounding box) based on the description of objects. Use any template\n      from [caption2box template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L345),\n      replacing ``<expr>`` with the object's description. For example:\n\n   > Can you point out *children in blue T-shirts* in the image and provide the bounding boxes of their location?\n\n    - Providing a description based on bounding box coordinates. Use a template\n      from [box2caption template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L400),\n      replacing ``<objs>`` with the position coordinates. For example:\n\n   > Tell me what you see within the designated area *[[086,540,400,760]]* in the picture.\n\n**Format of coordination:** The bounding box coordinates in the model's input and output use the\nformat ``[[x1, y1, x2, y2]]``, with the origin at the top left corner, the x-axis to the right, and the y-axis\ndownward. (x1, y1) and (x2, y2) are the top-left and bottom-right corners, respectively, with values as relative\ncoordinates multiplied by 1000 (prefixed with zeros to three digits).\n\n### Which --version to use\n\nDue to differences in model functionalities, different model versions may have distinct ``--version`` specifications for\nthe text processor, meaning the format of the prompts used varies.\n\n|         model name          | --version |\n|:---------------------------:|:---------:|\n|        cogagent-chat        |   chat    |\n|        cogagent-vqa         | chat_old  |\n|         cogvlm-chat         | chat_old  |\n|      cogvlm-chat-v1.1       | chat_old  |\n| cogvlm-grounding-generalist |   base    |\n|       cogvlm-base-224       |   base    |\n|       cogvlm-base-490       |   base    |\n\n### FAQ\n\n* If you have trouble in accessing huggingface.co, you can add `--local_tokenizer /path/to/vicuna-7b-v1.5` to load the\n  tokenizer.\n* If you have trouble in automatically downloading model with ğŸ”¨[SAT](https://github.com/THUDM/SwissArmyTransformer), try\n  downloading from ğŸ¤–[modelscope](https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary) or\n  ğŸ¤—[huggingface](https://huggingface.co/THUDM/CogVLM) or ğŸ’¡[wisemodel](https://www.wisemodel.cn/models/ZhipuAI/CogVLM)\n  manually.\n* Download model using ğŸ”¨[SAT](https://github.com/THUDM/SwissArmyTransformer), the model will be saved to the default\n  location `~/.sat_models`. Change the default location by setting the environment variable `SAT_HOME`. For example, if\n  you want to save the model to `/path/to/my/models`, you can run `export SAT_HOME=/path/to/my/models` before running\n  the python command.\n\n## License\n\nThe code in this repository is open source under the [Apache-2.0 license](./LICENSE), while the use of the CogVLM model\nweights must comply with the [Model License](./MODEL_LICENSE).\n\n## Citation & Acknowledgements\n\nIf you find our work helpful, please consider citing the following papers\n\n```\n@misc{wang2023cogvlm,\n      title={CogVLM: Visual Expert for Pretrained Language Models}, \n      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2311.03079},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n@misc{hong2023cogagent,\n      title={CogAgent: A Visual Language Model for GUI Agents}, \n      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2312.08914},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n```\n\nIn the instruction fine-tuning phase of the CogVLM, there are some English image-text data from\nthe [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLAVA](https://github.com/haotian-liu/LLaVA), [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction), [LLaVAR](https://github.com/SALT-NLP/LLaVAR)\nand [Shikra](https://github.com/shikras/shikra) projects, as well as many classic cross-modal work datasets. We\nsincerely thank them for their contributions.\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 25.0830078125,
          "content": "# CogVLM & CogAgent\n\nğŸ“— [README in English](./README.md)\n\nğŸŒŸ **è·³è½¬åˆ°è¯¦ç»†ä»‹ç»: [CogVLMä»‹ç»](#introduction-to-cogvlm)ï¼Œ\nğŸ†• [CogAgentçš„ä»‹ç»](#introduction-to-cogagent)**\n\nğŸ“” å¦‚éœ€è·å–æ›´è¯¦ç»†çš„ä½¿ç”¨ä¿¡æ¯ï¼Œè¯·å‚é˜…: [CogVLM&CogAgentæŠ€æœ¯æ–‡æ¡£](https://zhipu-ai.feishu.cn/wiki/LXQIwqo1OiIVTykMh9Lc3w1Fn7g)\n\n<table>\n  <tr>\n    <td>\n      <h2> CogVLM </h2>\n      <p> ğŸ“–  Paper: <a href=\"https://arxiv.org/abs/2311.03079\">CogVLM: Visual Expert for Pretrained Language Models</a></p>\n      <p><b>CogVLM</b> æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚CogVLM-17Bæ‹¥æœ‰100äº¿çš„è§†è§‰å‚æ•°å’Œ70äº¿çš„è¯­è¨€å‚æ•°ï¼Œæ”¯æŒ490*490åˆ†è¾¨ç‡çš„å›¾åƒç†è§£å’Œå¤šè½®å¯¹è¯ã€‚</p>\n      <p><b>CogVLM-17B 17Båœ¨10ä¸ªç»å…¸çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½</b>åŒ…æ‹¬NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA å’Œ TDIUC åŸºå‡†æµ‹è¯•ã€‚</p>\n    </td>\n    <td>\n      <h2> CogAgent </h2>\n      <p> ğŸ“–  Paper: <a href=\"https://arxiv.org/abs/2312.08914\">CogAgent: A Visual Language Model for GUI Agents </a></p>\n      <p><b>CogAgent</b> æ˜¯ä¸€ä¸ªåŸºäºCogVLMæ”¹è¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ã€‚CogAgent-18Bæ‹¥æœ‰110äº¿çš„è§†è§‰å‚æ•°å’Œ70äº¿çš„è¯­è¨€å‚æ•°, <b>æ”¯æŒ1120*1120åˆ†è¾¨ç‡çš„å›¾åƒç†è§£ã€‚åœ¨CogVLMçš„èƒ½åŠ›ä¹‹ä¸Šï¼Œå®ƒè¿›ä¸€æ­¥æ‹¥æœ‰äº†GUIå›¾åƒAgentçš„èƒ½åŠ›ã€‚</b></p>\n      <p> <b>CogAgent-18B åœ¨9ä¸ªç»å…¸çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é€šç”¨æ€§èƒ½ï¼Œ</b>åŒ…æ‹¬ VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, å’Œ POPE æµ‹è¯•åŸºå‡†ã€‚å®ƒåœ¨åŒ…æ‹¬AITWå’ŒMind2Webåœ¨å†…çš„GUIæ“ä½œæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹ã€‚</p>\n    </td>\n  </tr>\n  <tr>\n    <td colspan=\"2\" align=\"center\">\n      <p>ğŸŒ CogVLM2 åœ¨çº¿ä½“éªŒ: <a href=\"http://36.103.203.44:7861\">this link</a></p>\n    </td>\n  </tr>\n</table>\n\n\n**ç›®å½•**\n\n- [CogVLM \\& CogAgent](#cogvlm--cogagent)\n    - [Release](#å‘å¸ƒ)\n    - [å¼€å§‹ä½¿ç”¨](#å¼€å§‹ä½¿ç”¨)\n        - [é€‰é¡¹1ï¼šä½¿ç”¨ç½‘é¡µæ¼”ç¤ºè¿›è¡Œæ¨ç†](#é€‰é¡¹1ä½¿ç”¨ç½‘é¡µæ¼”ç¤ºè¿›è¡Œæ¨ç†)\n        - [é€‰é¡¹2ï¼šè‡ªè¡Œéƒ¨ç½²CogVLM / CogAgent](#é€‰é¡¹2è‡ªè¡Œéƒ¨ç½²cogvlm--cogagent)\n            - [Situation 2.1 CLI (SAT version)](#situation-21-cli-sat-version)\n            - [Situation 2.2 CLI (Huggingface version)](#situation-22-cli-huggingface-version)\n            - [Situation 2.3 Web Demo](#situation-23-web-demo)\n        - [é€‰é¡¹3ï¼šå¾®è°ƒ CogAgent / CogVLM](#é€‰é¡¹3å¾®è°ƒ-cogagent--cogvlm)\n        - [é€‰é¡¹4ï¼šOpenAIæ ¼å¼](#é€‰é¡¹4OpenAIæ ¼å¼)\n        - [ç¡¬ä»¶éœ€æ±‚](#ç¡¬ä»¶éœ€æ±‚)\n        - [Model checkpoints](#model-checkpoints)\n    - [Introduction to CogVLM](#introduction-to-cogvlm)\n        - [ç¤ºä¾‹](#ç¤ºä¾‹)\n    - [Introduction to CogAgent](#introduction-to-cogagent)\n        - [GUI Agent Examples](#gui-agent-examples)\n    - [Cookbook](#cookbook)\n        - [Task Prompts](#task-prompts)\n        - [é€‰æ‹©é€‚åˆçš„æ¨¡å‹](#é€‰æ‹©é€‚åˆçš„æ¨¡å‹)\n    - [License](#license)\n    - [Citation \\& Acknowledgements](#citation--acknowledgements)\n\n## å‘å¸ƒ\n- ğŸ”¥ğŸ”¥ğŸ”¥  **News**: ```2024/4/5```: [CogAgent](https://arxiv.org/abs/2312.08914) æˆåŠŸè¢«è¯„é€‰ä¸ºCVPR 2024 Highlights!\n- ğŸ”¥ğŸ”¥ **News**: ```2023/12/26```:æˆ‘ä»¬å…¬å¼€äº† [CogVLM-SFT-311K](dataset_zh.md) æ•°æ®é›†ï¼Œå®ƒåŒ…å«äº†è¶…è¿‡15ä¸‡æ¡æˆ‘ä»¬ç”¨äºè®­ç»ƒ **CogVLM v1.0(ä»…è¯¥æ¨¡å‹)** çš„æ•°æ®ã€‚æ¬¢è¿å…³æ³¨å’Œä½¿ç”¨ã€‚\n- ğŸ”¥ **News**: ```2023/12/18```: **æ–°çš„Streamlitç”¨æˆ·ç•Œé¢**å·²ç»ä¸Šçº¿ï¼æˆ‘ä»¬å·²ç»åŸºäºStreamlitæ¨å‡ºäº†æ–°çš„ç½‘é¡µç”¨æˆ·ç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥åœ¨æˆ‘ä»¬çš„ç•Œé¢ä¸Šè½»æ¾ä¸CogVLMï¼ŒCogAgentäº¤è°ˆã€‚å¸¦æ¥æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚\n- ğŸ”¥ **News**: ```2023/12/15```: **CogAgent æ­£å¼å‘å¸ƒï¼** CogAgentæ˜¯åŸºäºCogVLMå¼€å‘çš„å›¾åƒç†è§£æ¨¡å‹ã€‚å®ƒå…·æœ‰åŸºäºè§†è§‰çš„GUI\n  AgentåŠŸèƒ½ï¼Œå¹¶åœ¨å›¾åƒç†è§£æ–¹é¢è¿›è¡Œäº†è¿›ä¸€æ­¥çš„å¢å¼ºã€‚å®ƒæ”¯æŒåˆ†è¾¨ç‡ä¸º1120*1120çš„å›¾åƒè¾“å…¥ï¼Œå¹¶å…·æœ‰åŒ…æ‹¬ä¸å›¾åƒè¿›è¡Œå¤šè½®å¯¹è¯ã€GUI\n  Agentã€Groundingç­‰å¤šç§èƒ½åŠ›ã€‚\n\n- **News**: ```2023/12/8```:\n  æˆ‘ä»¬å·²å°†cogvlm-grounding-generalistçš„æ£€æŸ¥ç‚¹æ›´æ–°ä¸ºcogvlm-grounding-generalist-v1.1ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­å¢åŠ äº†å›¾åƒå¢å¼ºï¼Œå› æ­¤æ›´åŠ ç¨³å¥ã€‚æŸ¥çœ‹[è¯¦æƒ…](#introduction-to-cogvlm)ã€‚\n\n- **News**: ```2023/12/7``` CogVLMç°åœ¨æ”¯æŒ**4-bit**é‡åŒ–ï¼æ‚¨åªéœ€è¦11GBçš„GPUå†…å­˜å°±å¯ä»¥è¿›è¡Œæ¨ç†ï¼\n\n- **News**: ```2023/11/20```æˆ‘ä»¬å·²å°†cogvlm-chatçš„æ£€æŸ¥ç‚¹æ›´æ–°ä¸ºcogvlm-chat-v1.1ï¼Œç»Ÿä¸€äº†èŠå¤©å’ŒVQAçš„ç‰ˆæœ¬ï¼Œå¹¶åˆ·æ–°äº†å„ç§æ•°æ®é›†ä¸Šçš„SOTAï¼ŒæŸ¥çœ‹[è¯¦æƒ…](#introduction-to-cogvlm)ã€‚\n\n- **News**: ```2023/11/20``` æˆ‘ä»¬åœ¨ğŸ¤—Huggingfaceä¸Šå‘å¸ƒäº† **[cogvlm-chat](https://huggingface.co/THUDM/cogvlm-chat-hf)**, **[cogvlm-grounding-generalist](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)/[base](https://huggingface.co/THUDM/cogvlm-grounding-base-hf)**, **[cogvlm-base-490](https://huggingface.co/THUDM/cogvlm-base-490-hf)/[224](https://huggingface.co/THUDM/cogvlm-base-224-hf)**ï¼Œä½¿ç”¨transformers å¿«é€Ÿ [æ¨ç†](#situation-22-cli-huggingface-version)ã€‚\n\n- ```2023/10/27``` CogVLMåŒè¯­ç‰ˆæœ¬å·²ç»åœ¨çº¿ä¸Šå¯ç”¨ï¼æ¬¢è¿[è¯•ç”¨](https://chatglm.cn/)ã€‚\n\n- ```2023/10/5``` CogVLM-17B v1.0 å‘å¸ƒã€‚\n\n## å¼€å§‹ä½¿ç”¨\n\n### é€‰é¡¹1ï¼šä½¿ç”¨ç½‘é¡µæ¼”ç¤ºè¿›è¡Œæ¨ç†\n\n* ç‚¹å‡»æ­¤å¤„è¿›å…¥ [CogVLM2 Web Demo](http://36.103.203.44:7861/)ã€‚\n\nå¦‚æœæ‚¨éœ€è¦ä½¿ç”¨ä»£ç†å’Œæ¥åœ°åŠŸèƒ½ï¼Œè¯·å‚è€ƒ[Cookbook - Task Prompts](#task-prompts)ã€‚\n\n### é€‰é¡¹2ï¼šè‡ªè¡Œéƒ¨ç½²CogVLM / CogAgent\n\næˆ‘ä»¬æ”¯æŒä¸¤ç§æ¨¡å‹æ¨ç†çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼Œå‘½ä»¤è¡Œç•Œé¢å’Œç½‘ç»œæ¼”ç¤ºã€‚å¦‚æœä½ æƒ³åœ¨ä½ çš„Pythonä»£ç ä¸­ä½¿ç”¨å®ƒï¼Œä¿®æ”¹å‘½ä»¤è¡Œè„šæœ¬ä»¥é€‚åº”ä½ çš„æƒ…å†µã€‚\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ä¾èµ–é¡¹ã€‚\n\n```bash\n# CUDA >= 11.8\npip install -r requirements.txt\npython -m spacy download en_core_web_sm\n```\n\n**æ‰€æœ‰çš„æ¨ç†ä»£ç éƒ½ä½äº `basic_demo/` ç›®å½•ä¸‹ã€‚è¯·åœ¨è¿›è¡Œè¿›ä¸€æ­¥æ“ä½œä¹‹å‰ï¼Œå…ˆåˆ‡æ¢åˆ°è¿™ä¸ªç›®å½•ã€‚**\n\n#### Situation 2.1 CLI (SAT version)\n\né€šè¿‡ä»¥ä¸‹æ–¹å¼è¿è¡ŒCLIæ¼”ç¤ºï¼š\n\n```bash\n# CogAgent\npython cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogagent-vqa --version chat_old --bf16  --stream_chat\n\n# CogVLM\npython cli_demo_sat.py --from_pretrained cogvlm-chat --version chat_old --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogvlm-grounding-generalist --version base --bf16  --stream_chat\n```\n\nè¯¥ç¨‹åºå°†è‡ªåŠ¨ä¸‹è½½å«æ˜Ÿæ¨¡å‹å¹¶åœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’ã€‚æ‚¨å¯ä»¥é€šè¿‡è¾“å…¥æŒ‡ä»¤å¹¶æŒ‰å›è½¦æ¥ç”Ÿæˆå›å¤ã€‚è¾“å…¥`clear` ä»¥æ¸…é™¤å¯¹è¯å†å²ï¼Œè¾“å…¥`stop` ä»¥åœæ­¢ç¨‹åºã€‚\n\næˆ‘ä»¬ä¹Ÿæ”¯æŒæ¨¡å‹å¹¶è¡Œæ¨ç†ï¼Œè¯¥æ¨ç†å°†æ¨¡å‹åˆ†å‰²åˆ°å¤šä¸ªï¼ˆ2/4/8ï¼‰GPUä¸Šã€‚ä½¿ç”¨ `--nproc-per-node=[n]` æ§åˆ¶ä½¿ç”¨çš„GPUæ•°é‡ã€‚\n\n```\ntorchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16\n```\n\n- å¦‚æœä½ æƒ³æ‰‹åŠ¨ä¸‹è½½æƒé‡ï¼Œä½ å¯ä»¥ç”¨æ¨¡å‹è·¯å¾„æ›¿æ¢ ``--from_pretrained`` åçš„è·¯å¾„ã€‚\n\n- æˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒSATçš„4ä½é‡åŒ–å’Œ8ä½é‡åŒ–ã€‚ä½ å¯ä»¥å°† ``--bf16`` æ›´æ”¹ä¸º ``--fp16``, æˆ– ``--fp16 --quant 4``, æˆ– ``--fp16 --quant 8``.\n\n  ä¾‹å¦‚\n\n    ```bash\n    python cli_demo_sat.py --from_pretrained cogagent-chat --fp16 --quant 8 --stream_chat\n    python cli_demo_sat.py --from_pretrained cogvlm-chat-v1.1 --fp16 --quant 4 --stream_chat\n    # In SAT versionï¼Œ--quant should be used with --fp16\n    ```\n\n- è¯¥ç¨‹åºæä¾›ä»¥ä¸‹è¶…å‚æ•°æ¥æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼š\n    ```\n    usage: cli_demo_sat.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE]\n\n    optional arguments:\n    -h, --help            show this help message and exit\n    --max_length MAX_LENGTH\n                            max length of the total sequence\n    --top_p TOP_P         top p for nucleus sampling\n    --top_k TOP_K         top k for top k sampling\n    --temperature TEMPERATURE\n                            temperature for sampling\n    ```\n\n- ç‚¹å‡» [è¿™é‡Œ](#which---version-to-use) æŸ¥çœ‹ä¸åŒæ¨¡å‹ä¸ ``--version``  å‚æ•°ä¹‹é—´çš„å¯¹åº”å…³ç³»çš„å¯¹åº”å…³ç³»ã€‚\n\n#### Situation 2.2 CLI (Huggingface version)\n\né€šè¿‡ä»¥ä¸‹æ–¹å¼è¿è¡ŒCLIæ¼”ç¤ºï¼š\n\n```bash\n# CogAgent\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-vqa-hf --bf16\n\n# CogVLM\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-grounding-generalist --bf16\n```\n\n- å¦‚æœä½ æƒ³æ‰‹åŠ¨ä¸‹è½½æƒé‡ï¼Œä½ å¯ä»¥å°† ``--from_pretrained`` åçš„è·¯å¾„æ›¿æ¢ä¸ºæ¨¡å‹è·¯å¾„ã€‚\n\n- ä½ å¯ä»¥å°† ``--bf16`` æ›´æ”¹ä¸º ``--fp16``, æˆ–è€… ``--quant 4``ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒHuggingfaceçš„**4-bit quantization**:\n    ```bash\n    python cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --quant 4\n    ```\n\n#### Situation 2.3 Web Demo\n\næˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªåŸºäºGradioçš„æœ¬åœ°ç½‘ç»œæ¼”ç¤ºã€‚é¦–å…ˆï¼Œé€šè¿‡è¿è¡Œ `pip install gradio` æ¥å®‰è£…Gradioã€‚ç„¶åä¸‹è½½å¹¶è¿›å…¥è¿™ä¸ªä»“åº“ï¼Œè¿è¡Œ `web_demo.py`ã€‚\nè¯¦ç»†çš„ä½¿ç”¨æ–¹æ³•è¯·å‚è§ä¸‹ä¸€èŠ‚ï¼š\n\n```bash\npython web_demo.py --from_pretrained cogagent-chat --version chat --bf16\npython web_demo.py --from_pretrained cogagent-vqa --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-chat-v1.1 --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --bf16\n```\n\nç½‘é¡µæ¼”ç¤ºçš„å›¾å½¢ç”¨æˆ·ç•Œé¢å¦‚ä¸‹ï¼š\n\n<div align=\"center\">\n    <img src=assets/web_demo-min.png width=70% />\n</div>\n\n### é€‰é¡¹3ï¼šå¾®è°ƒ CogAgent / CogVLM\n\nä½ å¯èƒ½æƒ³åœ¨ä½ è‡ªå·±çš„ä»»åŠ¡ä¸­ä½¿ç”¨CogVLMï¼Œè¿™éœ€è¦ **ä¸åŒçš„è¾“å‡ºé£æ ¼æˆ–é¢†åŸŸçŸ¥è¯†**. **æ‰€æœ‰ç”¨äºå¾®è°ƒçš„ä»£ç éƒ½ä½äº  ``finetune_demo/`` ç›®å½•ä¸­ã€‚**\n\næˆ‘ä»¬åœ¨è¿™é‡Œæä¾›äº†ä¸€ä¸ªä½¿ç”¨loraè¿›è¡Œ **éªŒè¯ç è¯†åˆ«** çš„å¾®è°ƒç¤ºä¾‹ã€‚\n\n1. é¦–å…ˆä¸‹è½½ [Captcha Images](https://www.kaggle.com/datasets/aadhavvignesh/captcha-images)æ•°æ®é›†ã€‚ä¸‹è½½å®Œæˆåï¼Œè§£å‹ZIPæ–‡ä»¶çš„å†…å®¹ã€‚\n\n2. è¦åˆ›å»ºä¸€ä¸ªä»¥80/5/15çš„æ¯”ä¾‹è¿›è¡Œè®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n    ```bash\n    python utils/split_dataset.py\n    ```\n\n3. ä½¿ç”¨æ­¤å‘½ä»¤å¼€å§‹å¾®è°ƒï¼š\n\n    ```bash\n    bash finetune_demo/finetune_(cogagent/cogvlm)_lora.sh\n    ```\n\n4. å°†æ¨¡å‹åˆå¹¶åˆ°  `model_parallel_size=1`: (ç”¨ä½ çš„è®­ç»ƒ `MP_SIZE` æ›¿æ¢ä¸‹é¢çš„4)\n\n    ```bash\n    torchrun --standalone --nnodes=1 --nproc-per-node=4 utils/merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(cogagent/cogvlm490/cogvlm224)\n    ```\n\n5. ä¼°ä½ çš„æ¨¡å‹çš„æ€§èƒ½ã€‚\n    ```bash\n    bash finetune_demo/evaluate_(cogagent/cogvlm).sh\n    ```\n\n### é€‰é¡¹4ï¼šOpenAIæ ¼å¼\n\nWe provide the same API examples as `GPT-4V`, which you can view in `openai_demo`.\n\n1. é¦–å…ˆï¼Œå¯åŠ¨èŠ‚ç‚¹\n\n```\npython openai_demo/openai_api.py\n```\n\n2. æ¥ä¸‹æ¥ï¼Œè¿è¡Œè¯·æ±‚ç¤ºä¾‹èŠ‚ç‚¹ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿ç»­å¯¹è¯çš„ä¾‹å­\n\n```\npython openai_demo/openai_api_request.py\n```\n\n3. ä½ å°†å¾—åˆ°ç±»ä¼¼äºä»¥ä¸‹çš„è¾“å‡º\n\n```\nThis image showcases a tranquil natural scene with a wooden pathway leading through a field of lush green grass. In the distance, there are trees and some scattered structures, possibly houses or small buildings. The sky is clear with a few scattered clouds, suggesting a bright and sunny day.\n```\n\n### ç¡¬ä»¶éœ€æ±‚\n\n* æ¨¡å‹æ¨ç†:\n\n  For INT4 quantization: 1 * RTX 3090(24G)   (CogAgent takes ~ 12.6GB, CogVLM takes ~ 11GB)\n\n  For FP16: 1 * A100(80G) or 2 * RTX 3090(24G)\n\n* å¾®è°ƒ:\n\n  For FP16: 4 * A100(80G) *[Recommend]* or 8* RTX 3090(24G).\n\n### Model checkpoints\n\nå¦‚æœä½ ä»ä»£ç ä»“åº“è¿è¡Œ `basic_demo/cli_demo*.py`ï¼Œå®ƒå°†è‡ªåŠ¨ä¸‹è½½SATæˆ–Hugging Faceçš„æƒé‡ã€‚æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©æ‰‹åŠ¨ä¸‹è½½å¿…è¦çš„æƒé‡ã€‚\n\n- CogAgent\n\n  |   æ¨¡å‹åç§°    | è¾“å…¥åˆ†è¾¨ç‡ |                             ä»‹ç»                             | Huggingface model | SAT model |\n  | :-----------: | :----: | :----------------------------------------------------------: | :------: | :-------: |\n  | cogagent-chat |  1120  | CogAgentçš„èŠå¤©ç‰ˆæœ¬ã€‚æ”¯æŒGUIä»£ç†ï¼Œå¤šè½®èŠå¤©å’Œè§†è§‰å®šä½ã€‚ |  [link](https://huggingface.co/THUDM/cogagent-chat-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |\n  | cogagent-vqa |  1120  | CogAgentçš„VQAç‰ˆæœ¬ã€‚åœ¨å•è½®è§†è§‰å¯¹è¯ä¸­å…·æœ‰æ›´å¼ºçš„èƒ½åŠ›ã€‚æ¨èç”¨äºVQAåŸºå‡†æµ‹è¯•ã€‚ |  [link](https://huggingface.co/THUDM/cogagent-vqa-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |\n\n- CogVLM\n\n  |          æ¨¡å‹åç§°            | è¾“å…¥åˆ†è¾¨ç‡ |                                               ä»‹ç»                                                | Huggingface model | SAT model |\n  | :-------------------------: | :----: |:-----------------------------------------------------------------------------------------------:| :------: | :-------: |\n  |         cogvlm-chat-v1.1         |  490   |                    æ”¯æŒåŒæ—¶è¿›è¡Œå¤šè½®èŠå¤©å’Œè§†è§‰é—®ç­”ï¼Œæ”¯æŒè‡ªç”±çš„æç¤ºè¯ã€‚                                                    |  [link](https://huggingface.co/THUDM/cogvlm-chat-hf)        |    [link](https://huggingface.co/THUDM/CogVLM/tree/main)        |\n  |       cogvlm-base-224       |  224   |      æ–‡æœ¬-å›¾åƒé¢„è®­ç»ƒåçš„åŸå§‹æ£€æŸ¥ç‚¹ã€‚             |   [link](https://huggingface.co/THUDM/cogvlm-base-224-hf)      |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n  |       cogvlm-base-490       |  490   |  é€šè¿‡ä» cogvlm-base-224 è¿›è¡Œä½ç½®ç¼–ç æ’å€¼ï¼Œå°†åˆ†è¾¨ç‡æå‡åˆ°490ã€‚  |   [link](https://huggingface.co/THUDM/cogvlm-base-490-hf)      |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n  | cogvlm-grounding-generalist |  490   | æ­¤æ£€æŸ¥ç‚¹æ”¯æŒä¸åŒçš„è§†è§‰å®šä½ä»»åŠ¡ï¼Œä¾‹å¦‚RECï¼Œå®šä½å­—å¹•ç­‰ã€‚ |    [link](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)     |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n\n## Introduction to CogVLM\n\n- CogVLMæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚CogVLM-17Bæ‹¥æœ‰100äº¿çš„è§†è§‰å‚æ•°å’Œ70äº¿çš„è¯­è¨€å‚æ•°ã€‚\n- CogVLM-17Båœ¨10ä¸ªç»å…¸çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬ NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, å¹¶åœ¨ VQAv2, OKVQA, TextVQA, COCO å­—å¹•ç­‰æ–¹é¢æ’åç¬¬äºŒ., **è¶…è¶Šæˆ–åŒ¹æ•Œ PaLI-X 55B**. CogVLMè¿˜å¯ä»¥å’Œä½ èŠå…³äºå›¾ç‰‡çš„è¯é¢˜ã€‚ \n\n<div align=\"center\">\n    <img src=assets/metrics-min.png width=50% />\n</div>\n\n<details>\n<summary>ç‚¹å‡»æŸ¥çœ‹MM-VETï¼ŒPOPEï¼ŒTouchStoneçš„ç»“æœã€‚ </summary>\n\n<table>\n    <tr>\n        <td>Method</td>\n        <td>LLM</td>\n        <td>MM-VET</td>\n        <td>POPE(adversarial)</td>\n        <td>TouchStone</td>\n    </tr>\n    <tr>\n        <td>BLIP-2</td>\n        <td>Vicuna-13B</td>\n        <td>22.4</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Otter</td>\n        <td>MPT-7B</td>\n        <td>24.7</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>MiniGPT4</td>\n        <td>Vicuna-13B</td>\n        <td>24.4</td>\n        <td>70.4</td>\n        <td>531.7</td>\n    </tr>\n    <tr>\n        <td>InstructBLIP</td>\n        <td>Vicuna-13B</td>\n        <td>25.6</td>\n        <td>77.3</td>\n        <td>552.4</td>\n    </tr>\n    <tr>\n        <td>LLaMA-Adapter v2</td>\n        <td>LLaMA-7B</td>\n        <td>31.4</td>\n        <td>-</td>\n        <td>590.1</td>\n    </tr>\n    <tr>\n        <td>LLaVA</td>\n        <td>LLaMA2-7B</td>\n        <td>28.1</td>\n        <td>66.3</td>\n        <td>602.7</td>\n    </tr>\n    <tr>\n        <td>mPLUG-Owl</td>\n        <td>LLaMA-7B</td>\n        <td>-</td>\n        <td>66.8</td>\n        <td>605.4</td>\n    </tr>\n    <tr>\n        <td>LLaVA-1.5</td>\n        <td>Vicuna-13B</td>\n        <td>36.3</td>\n        <td>84.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Emu</td>\n        <td>LLaMA-13B</td>\n        <td>36.3</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Qwen-VL-Chat</td>\n        <td>-</td>\n        <td>-</td>\n        <td>-</td>\n        <td>645.2</td>\n    </tr>\n    <tr>\n        <td>DreamLLM</td>\n        <td>Vicuna-7B</td>\n        <td>35.9</td>\n        <td>76.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>CogVLM</td>\n        <td>Vicuna-7B</td>\n        <td> <b>52.8</b> </td>\n        <td><b>87.6</b></td>\n        <td><b>742.0</b></td>\n    </tr>\n</table>\n\n</details>\n\n<details>\n<summary>ç‚¹å‡»æŸ¥çœ‹cogvlm-grounding-generalist-v1.1çš„ç»“æœã€‚</summary>\n\n<table>\n    <tr>\n        <td></td>\n        <td>RefCOCO</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCO+</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCOg</td>\n        <td></td>\n        <td>Visual7W</td>\n    </tr>\n    <tr>\n        <td></td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>test</td>\n        <td>test</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist</td>\n        <td>92.51</td>\n        <td>93.95</td>\n        <td>88.73</td>\n        <td>87.52</td>\n        <td>91.81</td>\n        <td>81.43</td>\n        <td>89.46</td>\n        <td>90.09</td>\n        <td>90.96</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist-v1.1</td>\n        <td>**92.76**</td>\n        <td>**94.75**</td>\n        <td>**88.99**</td>\n        <td>**88.68**</td>\n        <td>**92.91**</td>\n        <td>**83.39**</td>\n        <td>**89.75**</td>\n        <td>**90.79**</td>\n        <td>**91.05**</td>\n    </tr>\n</table>\n</details>\n\n### ç¤ºä¾‹\n\n<!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description & Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. -->\n\n* CogVLMèƒ½å¤Ÿå‡†ç¡®åœ°è¯¦ç»†æè¿°å›¾åƒï¼Œå‡ ä¹ä¸ä¼šäº§ç”Ÿå¹»è§‰ã€‚\n    <details>\n    <summary>ç‚¹å‡»ä»¥ä¸LLAVA-1.5å’ŒMiniGPT-4è¿›è¡Œæ¯”è¾ƒã€‚.</summary>\n\n    <img src=assets/llava-comparison-min.png width=50% />\n\n    </details>\n    <br>\n\n* CogVLMèƒ½ç†è§£å¹¶å›ç­”å„ç§ç±»å‹çš„é—®é¢˜ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªè§†è§‰åŸºç¡€ç‰ˆæœ¬ã€‚\n\n<div align=\"center\">\n    <img src=assets/pear_grounding.png width=50% />\n</div>\n\n<br>\n\n* CogVLMæœ‰æ—¶æ¯”GPT-4V(ision)æ•è·æ›´è¯¦ç»†çš„å†…å®¹ã€‚\n\n<div align=\"center\">\n    <img src=assets/compare-min.png width=50% />\n</div>\n\n<!-- ![compare](assets/compare.png) -->\n<br> \n\n<details>\n<summary>ç‚¹å‡»ä»¥å±•å¼€æ›´å¤šç¤ºä¾‹ã€‚</summary>\n\n![Chat Examples](assets/chat.png)\n\n</details>\n\n## Introduction to CogAgent\n\nCogAgentæ˜¯ä¸€ä¸ªåŸºäºCogVLMæ”¹è¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ã€‚CogAgent-18Bæ‹¥æœ‰110äº¿çš„è§†è§‰å‚æ•°å’Œ70äº¿çš„è¯­è¨€å‚æ•°ã€‚\n\nCogAgent-18Båœ¨9ä¸ªç»å…¸çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å…¨èƒ½æ€§èƒ½ï¼ŒåŒ…æ‹¬VQAv2ã€OK-VQã€TextVQAã€ST-VQAã€ChartQAã€infoVQAã€DocVQAã€MM-Vetå’ŒPOPEã€‚å®ƒåœ¨å¦‚AITWå’ŒMind2Webç­‰GUIæ“ä½œæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹ã€‚\n\né™¤äº†CogVLMå·²æœ‰çš„æ‰€æœ‰åŠŸèƒ½ï¼ˆè§†è§‰å¤šè½®å¯¹è¯ï¼Œè§†è§‰å®šä½ï¼‰ä¹‹å¤–ï¼ŒCogAgentï¼š\n\n1. æ”¯æŒ**æ›´é«˜åˆ†è¾¨ç‡**çš„è§†è§‰è¾“å…¥å’Œå¯¹è¯å¼é—®ç­”ã€‚å®ƒæ”¯æŒè¶…é«˜åˆ†è¾¨ç‡çš„å›¾åƒè¾“å…¥ï¼Œè¾¾åˆ°**1120x1120**ã€‚\n\n2. **æ‹¥æœ‰è§†è§‰Agentçš„èƒ½åŠ›**ï¼Œèƒ½å¤Ÿåœ¨ä»»ä½•å›¾å½¢ç”¨æˆ·ç•Œé¢æˆªå›¾ä¸Šï¼Œä¸ºä»»ä½•ç»™å®šä»»åŠ¡è¿”å›ä¸€ä¸ªè®¡åˆ’ï¼Œä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œä»¥åŠå¸¦æœ‰åæ ‡çš„ç‰¹å®šæ“ä½œã€‚\n\n3. **å¢å¼ºäº†ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ç›¸å…³çš„é—®ç­”èƒ½åŠ›**ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å…³äºä»»ä½•å›¾å½¢ç”¨æˆ·ç•Œé¢æˆªå›¾çš„é—®é¢˜ï¼Œä¾‹å¦‚ç½‘é¡µã€PCåº”ç”¨ã€ç§»åŠ¨åº”ç”¨ç­‰ã€‚\n\n4. é€šè¿‡æ”¹è¿›é¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œæé«˜äº†OCRç›¸å…³ä»»åŠ¡çš„èƒ½åŠ›ã€‚\n\n<div align=\"center\">\n    <img src=assets/cogagent_function.jpg width=60% />\n</div>\n\n### GUI Agent Examples\n\n<div align=\"center\">\n    <img src=assets/cogagent_main_demo.jpg width=90% />\n</div>\n\n## Cookbook\n\n### Task Prompts\n\n1. **é€šç”¨å¤šè½®å¯¹è¯**: éšä¾¿ä½ è¯´ä»€ä¹ˆ.\n\n2. **GUIä»£ç†ä»»åŠ¡**: ä½¿ç”¨ [ä»£ç†æ¨¡æ¿](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761)å¹¶ç”¨åŒå¼•å·æ‹¬èµ·æ¥çš„ä»»åŠ¡æŒ‡ä»¤æ›¿æ¢ `\\<TASK\\>`ã€‚è¿™ä¸ªæŸ¥è¯¢å¯ä»¥è®©CogAgentæ¨æ–­å‡ºè®¡åˆ’å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚å¦‚æœåœ¨æŸ¥è¯¢çš„æœ«å°¾æ·»åŠ `(with grounding)` æ¨¡å‹å°†è¿”å›ä¸€ä¸ªå¸¦æœ‰åæ ‡çš„æ­£å¼åŒ–åŠ¨ä½œè¡¨ç¤ºã€‚\n\nä¾‹å¦‚ï¼Œè¦è¯¢é—®æ¨¡å‹å¦‚ä½•å®Œæˆ\"åœ¨å½“å‰GUIæˆªå›¾ä¸Šæœç´¢CogVLM\"çš„ä»»åŠ¡ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n\n1. ä»ä»£ç†æ¨¡æ¿ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡æ¿ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†``What steps do I need to take to <TASK>?``.\n\n2. è¯·ç”¨åŒå¼•å·ä¸­çš„ä»»åŠ¡æŒ‡ä»¤æ›¿æ¢ï¼Œä¾‹å¦‚ï¼Œ ``What steps do I need to take to \"Search for CogVLM\"?``ã€‚å°†æ­¤è¾“å…¥åˆ°æ¨¡å‹ä¼šäº§ç”Ÿï¼š\n\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n\n3. å¦‚æœåœ¨æœ«å°¾æ·»åŠ  ``(with grounding)`` å³å°†è¾“å…¥æ”¹ä¸º``What steps do I need to take to \"Search for CogVLM\"?(with grounding)``,é‚£ä¹ˆCogAgentçš„è¾“å‡ºå°†ä¼šæ˜¯:\n\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n> Grounded Operation:[combobox] Search -> TYPE: CogVLM at the box [[212,498,787,564]]\n\næç¤ºï¼šå¯¹äºGUIä»£ç†ä»»åŠ¡ï¼Œå»ºè®®æ¯ä¸ªå›¾åƒåªè¿›è¡Œä¸€è½®å¯¹è¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚\n\n3. **è§†è§‰å®šä½**. Tæ”¯æŒä¸‰ç§å®šä½æ¨¡å¼ï¼š\n\n    - å¸¦æœ‰å®šä½åæ ‡ï¼ˆè¾¹ç•Œæ¡†ï¼‰çš„å›¾åƒæè¿°ã€‚ä½¿ç”¨caption_with_boxæ¨¡æ¿ä¸­çš„ä»»ä½•æ¨¡æ¿ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚ä¾‹å¦‚:\n\n   > Can you provide a description of the image and include the coordinates [[x0,y0,x1,y1]] for each mentioned object?\n\n    - æ ¹æ®ç‰©ä½“çš„æè¿°è¿”å›æ¥åœ°åæ ‡ï¼ˆè¾¹ç•Œæ¡†ï¼‰ã€‚ä½¿ç”¨caption2boxæ¨¡æ¿ä¸­çš„ä»»ä½•æ¨¡æ¿ï¼Œå°† <expr> æ›¿æ¢ä¸ºç‰©ä½“çš„æè¿°ã€‚ä¾‹å¦‚:\n\n   > Can you point out *children in blue T-shirts* in the image and provide the bounding boxes of their location?\n\n    - æ ¹æ®è¾¹ç•Œæ¡†åæ ‡æä¾›æè¿°ã€‚ä½¿ç”¨box2captionæ¨¡æ¿ä¸­çš„æ¨¡æ¿ï¼Œå°† <objs> æ›¿æ¢ä¸ºä½ç½®åæ ‡ã€‚ä¾‹å¦‚ï¼š\n\n   > Tell me what you see within the designated area *[[086,540,400,760]]* in the picture.\n\n**åæ ‡æ ¼å¼:** æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºä¸­çš„è¾¹ç•Œæ¡†åæ ‡ä½¿ç”¨ `[[x1, y1, x2, y2]]` æ ¼å¼ï¼ŒåŸç‚¹ä½äºå·¦ä¸Šè§’ï¼Œxè½´å‘å³ï¼Œyè½´å‘ä¸‹ã€‚ (x1, y1) å’Œ (x2, y2) åˆ†åˆ«æ˜¯å·¦ä¸Šè§’å’Œå³ä¸‹è§’ï¼Œå…¶å€¼ä¸ºç›¸å¯¹åæ ‡ä¹˜ä»¥1000ï¼ˆå‰ç¼€ä¸ºé›¶ï¼Œä¸‰ä½æ•°ï¼‰ã€‚\n\n### é€‰æ‹©é€‚åˆçš„æ¨¡å‹\n\nç”±äºæ¨¡å‹åŠŸèƒ½çš„å·®å¼‚ï¼Œä¸åŒçš„æ¨¡å‹ç‰ˆæœ¬å¯èƒ½ä¼šæœ‰ä¸åŒçš„æ–‡æœ¬å¤„ç†å™¨ `--version`ï¼Œè¿™æ„å‘³ç€ä½¿ç”¨çš„æç¤ºæ ¼å¼ä¼šæœ‰æ‰€ä¸åŒã€‚\n\n|         model name          | --version |\n|:---------------------------:|:---------:|\n|        cogagent-chat        |   chat    |\n|        cogagent-vqa         | chat_old  |\n|         cogvlm-chat         | chat_old  |\n|      cogvlm-chat-v1.1       | chat_old  |\n| cogvlm-grounding-generalist |   base    |\n|       cogvlm-base-224       |   base    |\n|       cogvlm-base-490       |   base    |\n\n### å¸¸è§é—®é¢˜\n\n* å¦‚æœä½ åœ¨è®¿é—®huggingface.coæ—¶é‡åˆ°é—®é¢˜ï¼Œä½ å¯ä»¥æ·»åŠ  `--local_tokenizer /path/to/vicuna-7b-v1.5` æ¥åŠ è½½åˆ†è¯å™¨ã€‚\n* å¦‚æœä½ åœ¨ä½¿ç”¨ğŸ”¨ [SAT](https://github.com/THUDM/SwissArmyTransformer)è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ—¶é‡åˆ°é—®é¢˜ , å°è¯•ä» ğŸ¤–[modelscope](https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary) æˆ–\n  ğŸ¤—[huggingface](https://huggingface.co/THUDM/CogVLM) or ğŸ’¡[wisemodel](https://www.wisemodel.cn/models/ZhipuAI/CogVLM) æ‰‹åŠ¨ä¸‹è½½ã€‚\n* ä½¿ç”¨ğŸ”¨ SATä¸‹è½½æ¨¡å‹ï¼Œæ¨¡å‹å°†è¢«ä¿å­˜åˆ°é»˜è®¤ä½ç½® `~/.sat_models` ã€‚é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ `SAT_HOME` æ¥æ›´æ”¹é»˜è®¤ä½ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³å°†æ¨¡å‹ä¿å­˜åˆ° `/path/to/my/models` ï¼Œä½ å¯ä»¥åœ¨è¿è¡Œpythonå‘½ä»¤ä¹‹å‰è¿è¡Œ `export SAT_HOME=/path/to/my/models`ã€‚\n\n## License\n\næ­¤ä»“åº“ä¸­çš„ä»£ç æ˜¯åœ¨[Apache-2.0 license](./LICENSE)çš„å¼€æºä»£ç ï¼Œè€Œä½¿ç”¨CogVLMæ¨¡å‹æƒé‡å¿…é¡»éµå®ˆ[æ¨¡å‹è®¸å¯](./MODEL_LICENSE).\n\n## Citation & Acknowledgements\n\nå¦‚æœä½ å‘ç°æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡\n```\n@misc{wang2023cogvlm,\n      title={CogVLM: Visual Expert for Pretrained Language Models}, \n      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2311.03079},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n@misc{hong2023cogagent,\n      title={CogAgent: A Visual Language Model for GUI Agents}, \n      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2312.08914},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n```\n\nåœ¨CogVLMçš„æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ª [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLAVA](https://github.com/haotian-liu/LLaVA), [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction), [LLaVAR](https://github.com/SALT-NLP/LLaVAR) å’Œ [Shikra](https://github.com/shikras/shikra)é¡¹ç›®çš„ä¸€äº›è‹±æ–‡å›¾åƒ-æ–‡æœ¬æ•°æ®ï¼Œä»¥åŠè®¸å¤šç»å…¸çš„è·¨æ¨¡æ€å·¥ä½œæ•°æ®é›†ã€‚æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢ä»–ä»¬çš„è´¡çŒ®ã€‚"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "basic_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "composite_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset.md",
          "type": "blob",
          "size": 4.0517578125,
          "content": "# CogVLM-SFT-311K: Bilingual Visual Instruction Data in CogVLM SFT\n\nCogVLM-SFT-311K is the primary aligned corpus used in the initial training of CogVLM v1.0. The process of constructing this dataset is as follows:\n1. Approximately 3500 high-quality data samples were selected from the open source [MiniGPT-4](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align), known as minigpt4-3500.\n2. Minigpt4-3500 was integrated with [Llava-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) and translated into Chinese through a language model.\n3. We discovered significant noise in the detailed description part of minigpt4-3500 and Llava-instruct. Thus, we corrected these Chinese corpora and retranslated them into English.\n\n## License\n\n+ Due to non-commercial agreements, we did not use these data in the bilingual version of CogVLM or any other models involving commercialization.\n+ The dataset license adheres to: <br> Attribution-NonCommercial 4.0 International. It should abide by the policy of OpenAI: https://openai.com/policies/terms-of-use\nThis will not allow you to use these data for any **commercial activitiesI**.\n\n## Dataset Address\n\n+ [CogVLM-SFT-311K](https://huggingface.co/datasets/THUDM/CogVLM-SFT-311K)\n\n## Dataset Information\n\nThe dataset contains three folders corresponding to the mixed part of minigpt4-3500 and llava, the llava solo conversation, and the multi-turn conversation datasets. Their layout is as follows:\n```\n.CogVLM-SFT-311K\nâ”œâ”€â”€ llava_details-minigpt4_3500_formate\nâ”œâ”€â”€ llava_instruction_multi_conversations_formate\nâ””â”€â”€ llava_instruction_single_conversation_formate\n```\nIn our open-source data, the datasets are distributed as follows:\n```\n.llava_details-minigpt4_3500_formate\nâ”œâ”€â”€ images\nâ”‚   â””â”€â”€ 00000001.jpg\nâ””â”€â”€ labels\n    â””â”€â”€ 00000001.json\n```\nImages are stored in the images folder, while labels containing the corresponding image description or dialogue are stored in the labels folder.\n\n## Dataset Quantity\n\n+ llava_details-minigpt4_3500_formate: 22,464 images and descriptions\n+ llava_instruction_multi_conversations_formate: 56,673 images and multi-turn conversations\n+ llava_instruction_single_conversation_formate: 76,634 images and single-turn conversations\n\nDataset Format\n\nCaption format for image description\n```\n{\n  \"captions\": [\n    {\n      \"role\": \"caption\",\n      \"content\": \"The photograph features a beach scene with a group of people surfing in the ocean. There are ten individuals spaced out in the water, sitting or lying on various sizes of body boards. The surfers line up in a row, close to the sandy shoreline. Some of the body boards are visible floating on the surface of the water, with the surfers riding on them or nearby.\"\n    }\n  ]\n}\n```\nConversation format for image dialogue\n```\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What can be inferred about the zebras' behavior and surroundings?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Based on the image, we can infer that the two zebras are likely seeking relief from the sun's heat, as they are standing side by side under the branches of a thorny tree. This shade-providing tree offers some respite from the sun, possibly during the hottest part of the day. The zebras are in a green field with grass, providing them with an ideal environment to graze and eat while staying near their source of shelter. This shows that the zebras' behavior is influenced by the conditions and available resources in their surroundings. It also highlights that these animals adopt strategies to adapt to the fluctuating conditions of their environment, such as cooperation and seeking shelter, to survive and thrive in their natural habitat.\"\n    }\n  ]\n}\n```\n\n## References\nThis project utilizes data and concepts based on the following research papers:\n- Zhu, D., Chen, J., Shen, X., Li, X., & Elhoseiny, M. (2023). MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. arXiv preprint arXiv:2304.10592.\n- Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual Instruction Tuning. arXiv:2304.08485. "
        },
        {
          "name": "dataset_zh.md",
          "type": "blob",
          "size": 3.587890625,
          "content": "# CogVLM-SFT-311Kï¼šCogVLM SFT ä¸­çš„åŒè¯­è§†è§‰æŒ‡ä»¤æ•°æ®é›†\n\nCogVLM-SFT-311K æ˜¯æˆ‘ä»¬åœ¨è®­ç»ƒ **CogVLM v1.0** æœ€åˆç‰ˆæœ¬æ—¶ä½¿ç”¨çš„ä¸»è¦å¯¹é½è¯­æ–™åº“ã€‚æ­¤æ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹å¦‚ä¸‹ï¼š\n1. ä»å¼€æºçš„ [MiniGPT-4](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align) ä¸­é€‰å–äº†å¤§çº¦3500ä¸ªé«˜è´¨é‡æ•°æ®æ ·æœ¬ï¼Œç§°ä¸º minigpt4-3500ã€‚\n2. å°† minigpt4-3500 ä¸ [Llava-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) æ•´åˆï¼Œå¹¶é€šè¿‡è¯­è¨€æ¨¡å‹ç¿»è¯‘è·å¾—ä¸­æ–‡éƒ¨åˆ†ã€‚\n3. æˆ‘ä»¬å‘ç°åœ¨ minigpt4-3500 å’Œ Llava-instruct çš„è¯¦ç»†æè¿°éƒ¨åˆ†å­˜åœ¨è®¸å¤šå™ªå£°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çº æ­£äº†è¿™ä¸¤éƒ¨åˆ†çš„ä¸­æ–‡è¯­æ–™ï¼Œå¹¶å°†çº æ­£åçš„è¯­æ–™é‡æ–°ç¿»è¯‘æˆè‹±è¯­ã€‚\n\n## è®¸å¯è¯\n+ ç”±äºéå•†ä¸šåè®®é™åˆ¶ï¼Œæˆ‘ä»¬æ²¡æœ‰åœ¨ CogVLMçš„åŒè¯­ç‰ˆæœ¬ å’Œå…¶ä»–ä»»ä½• æ¶‰åŠå•†ä¸šåŒ–çš„æ¨¡å‹ ä¸­ä½¿ç”¨è¿™äº›æ•°æ®ã€‚ \n+ æ•°æ®é›†è®¸å¯è¯éµå®ˆï¼š<br> Attribution-NonCommercial 4.0 International It should abide by the policy of OpenAI: https://openai.com/policies/terms-of-use\nè¿™å°†ä¸å…è®¸ä½ ä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œä»»ä½• **å•†ä¸šåŒ–è¡Œä¸º**ã€‚\n\n## æ•°æ®é›†åœ°å€\n\n+ [CogVLM-SFT-311K](https://huggingface.co/datasets/THUDM/CogVLM-SFT-311K)\n\n## æ•°æ®é›†ä¿¡æ¯\næ•°æ®é›†å…±æœ‰ä¸‰ä¸ªæ–‡ä»¶å¤¹ï¼Œåˆ†åˆ«å¯¹åº”æ··åˆ minigpt4-3500 ä¸llavaæ··åˆçš„ä¸€éƒ¨åˆ†æ•°æ®é›†ï¼Œllava å•è®ºå¯¹è¯å’Œå¤šè½®å¯¹è¯æ•°æ®é›†ã€‚å…¶å¸ƒå±€å¦‚ä¸‹ï¼š\n```\n.CogVLM-SFT-311K\nâ”œâ”€â”€ llava_details-minigpt4_3500_formate\nâ”œâ”€â”€ llava_instruction_multi_conversations_formate\nâ””â”€â”€ llava_instruction_single_conversation_formate\n```\nåœ¨æˆ‘ä»¬å¼€æºçš„æ•°æ®ä¸­ï¼Œæ•°æ®é›†æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†å¸ƒ\n```\n.llava_details-minigpt4_3500_formate\nâ”œâ”€â”€ images\nâ”‚   â””â”€â”€ 00000001.jpg\nâ””â”€â”€ labels\n    â””â”€â”€  00000001.json\n```\nå…¶ä¸­ï¼Œimageså­˜å‚¨å›¾åƒæ•°æ®ï¼Œè€Œlabelså­˜å‚¨è¿™å¼ å›¾åƒå¯¹åº”çš„æè¿°æˆ–å¯¹è¯ã€‚\n\n## æ•°æ®é›†æ•°é‡\n+ llava_details-minigpt4_3500_formate 22464 å¼ å›¾ç‰‡å’Œæè¿°\n+ llava_instruction_muti_conversations_formate 56673 å¼ å›¾ç‰‡å’Œå¤šè½®è¿ç»­å¯¹è¯\n+ llava_instruction_single_conversation_formate 76634 å¼ å›¾ç‰‡å’Œå•è½®å¯¹è¯\n## æ•°æ®é›†æ ¼å¼\n\nå›¾åƒæè¿° Caption æ ¼å¼\n```\n{\n  \"captions\": [\n    {\n      \"role\": \"caption\",\n      \"content\": \"è¿™å¼ ç…§ç‰‡å±•ç¤ºäº†ä¸€ç”·ä¸€å¥³ï¼Œä»–ä»¬éƒ½ç©¿ç€çº¢å˜¿è‰²è¡£æœï¼Œååœ¨æ¡Œå­æ—ï¼Œåœ¨å‚åŠ æ´»åŠ¨æ—¶é¢å¸¦å¾®ç¬‘ã€‚è¿™å¯¹æƒ…ä¾£æ­£åœ¨ä¸€èµ·æ‘†å§¿åŠ¿ï¼ŒèƒŒæ™¯ä¸­è¿˜æœ‰å…¶ä»–äººã€‚åœ¨æ•´ä¸ªåœºæ™¯ä¸­å¯ä»¥çœ‹åˆ°å¦å¤–å‡ ä¸ªäººï¼Œä¸€äº›äººååœ¨é™„è¿‘çš„æ¡Œå­æ—ï¼Œå¦ä¸€äº›äººåå¾—ç¦»è¿™å¯¹å¤«å¦‡æ›´è¿‘ã€‚å„ç§é¤æ¡Œè®¾ç½®ï¼Œå¦‚é…’æ¯å’Œæ¯å­ï¼Œè¢«æ”¾ç½®åœ¨æ¡Œå­ä¸Šã€‚æ’åœ¨æœ€å‰é¢çš„ç”·å­æ‰“ç€é¢†å¸¦ã€‚\"\n    }\n  ]\n}\n```\nå›¾åƒå¯¹è¯ Json æ ¼å¼\n``` \n{\n    \"conversations\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"åœ¨å°è¯•ä¿æŒè¿™ä¸ªæµ´å®¤æœ‰åºæ—¶å¯èƒ½ä¼šé‡åˆ°å“ªäº›æŒ‘æˆ˜ï¼Ÿ\",\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": \"åœ¨å°è¯•ä¿æŒè¿™ç©ºé—´å’Œå„ç§ç‰©å“çš„å­˜åœ¨ï¼Œä¾‹å¦‚æ¯å­å’Œæ‰‹æœºå ç”¨äº†å¯ç”¨çš„ç©ºé—´ã€‚åœ¨å›¾åƒä¸­ï¼Œæµ´å®¤æŸœå°ä¸Šåªæœ‰ä¸€ä¸ªæ°´æ§½å¯ç”¨ï¼Œè¿™å¯¼è‡´å¤šä¸ªç”¨æˆ·å¯èƒ½å¿…é¡»å…±äº«ç©ºé—´æ”¾ç½®è‡ªå·±çš„ç‰©å“ã€‚è¿™å¯èƒ½å¯¼è‡´æ‚ä¹±å’Œæ··ä¹±å¤–çš„å‚¨ç‰©è§£å†³æ–¹æ¡ˆï¼Œå¦‚æ¶å­ã€æ©±æŸœæˆ–å¢™å£æŒ‚æ¶ï¼Œä»¥ä¿æŒæµ´å®¤æœ‰åºå¹¶æœ€å°åŒ–æŸœå°ä¸Šçš„æ‚ä¹±ã€‚\"\n      },\n    ]\n}\n```\n\n## References\nThis project utilizes data and concepts based on the following research papers:\n- Zhu, D., Chen, J., Shen, X., Li, X., & Elhoseiny, M. (2023). MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. arXiv preprint arXiv:2304.10592.\n- Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual Instruction Tuning. arXiv:2304.08485."
        },
        {
          "name": "finetune_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3515625,
          "content": "SwissArmyTransformer>=0.4.9\ntransformers>=4.36.2\nxformers>=0.0.22\ntorch>=2.1.0\ntorchvision>=0.16.2\nspacy>=3.6.0\npillow>=10.2.0\ndeepspeed>=0.13.1\nseaborn>=0.13.2\nloguru~=0.7.2\nstreamlit>=1.31.0\ntimm>=0.9.12\naccelerate>=0.26.1\npydantic>=2.6.0\n\n# for openai demo\nopenai>=1.16.0\nsse-starlette>=1.8.2\nfastapi>=0.110.1\nhttpx>=0.27.0\nuvicorn>=0.29.0\njsonlines>=4.0.0\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}