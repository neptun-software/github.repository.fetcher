{
  "metadata": {
    "timestamp": 1736560870673,
    "page": 583,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "yangjianxin1/Firefly",
      "stars": 6009,
      "defaultBranch": "master",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 32.0380859375,
          "content": "# Firefly: ä¸€ç«™å¼å¤§æ¨¡å‹è®­ç»ƒå·¥å…·\n\n<div align=\"left\">\n\n![GitHub Repo stars](https://img.shields.io/github/stars/yangjianxin1/Firefly?style=social)\n[![Generic badge](https://img.shields.io/badge/å¾®ä¿¡äº¤æµç¾¤-Firefly-brightgreen?logo=wechat)](./pics/wechat-group.jpeg)\n[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/YeungNLP)\n\n[//]: # ([![Generic badge]&#40;https://img.shields.io/badge/å¾®ä¿¡-Firefly-brightgreen?logo=wechat&#41;]&#40;./pics/wechat.jpeg&#41;)\n</div>\n\n<img src=\"pics/firefly_logo.png\" width=\"250\">\n\næ¬¢è¿åŠ å…¥Fireflyå¤§æ¨¡å‹æŠ€æœ¯äº¤æµç¾¤ï¼Œå…³æ³¨æˆ‘ä»¬çš„å…¬ä¼—å·ï¼Œç‚¹å‡»åŠ ç¾¤æŒ‰é’®å³å¯ã€‚\n\n<img src=\"pics/gongzhonghao.png\" width=\"300\">\n\næ¬¢è¿å…³æ³¨æˆ‘ä»¬çš„çŸ¥ä¹è¿›è¡Œäº¤æµè®¨è®ºï¼š**[çº¢é›¨ç“¢æ³¼](https://www.zhihu.com/people/jian-xin-15-96)**\n\n## é¡¹ç›®ç®€ä»‹\n**Firefly** æ˜¯ä¸€ä¸ªå¼€æºçš„å¤§æ¨¡å‹è®­ç»ƒé¡¹ç›®ï¼Œæ”¯æŒå¯¹ä¸»æµçš„å¤§æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’ŒDPOï¼ŒåŒ…æ‹¬ä½†ä¸é™äºQwen2ã€Yi-1.5ã€Llama3ã€Gemmaã€Qwen1.5ã€MiniCPMã€MiniCPM3ã€Llamaã€InternLMã€Baichuanã€ChatGLMã€Yiã€Deepseekã€Qwenã€Orionã€Ziyaã€Xverseã€Mistralã€Mixtral-8x7Bã€Zephyrã€Vicunaã€Bloomç­‰ã€‚\næœ¬é¡¹ç›®æ”¯æŒ**å…¨é‡å‚æ•°è®­ç»ƒã€LoRAã€QLoRAé«˜æ•ˆè®­ç»ƒ**ï¼Œæ”¯æŒ**é¢„è®­ç»ƒã€SFTã€DPO**ã€‚ å¦‚æœä½ çš„è®­ç»ƒèµ„æºæœ‰é™ï¼Œæˆ‘ä»¬æåŠ›æ¨èä½¿ç”¨QLoRAè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå› ä¸ºæˆ‘ä»¬åœ¨Open LLM Leaderboardä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”å–å¾—äº†éå¸¸ä¸é”™çš„æˆç»©ã€‚\n\nğŸ”” æœ¬é¡¹ç›®ä¸»è¦å†…å®¹å¦‚ä¸‹ï¼š\n- ğŸ“— æ”¯æŒé¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒã€DPOï¼Œæ”¯æŒå…¨é‡å‚æ•°è®­ç»ƒã€LoRAã€QLoRAé«˜æ•ˆè®­ç»ƒã€‚é€šè¿‡é…ç½®æ–‡ä»¶çš„æ–¹å¼è®­ç»ƒä¸åŒçš„æ¨¡å‹ï¼Œå°ç™½äº¦å¯å¿«é€Ÿä¸Šæ‰‹è®­ç»ƒæ¨¡å‹ã€‚\n- ğŸ“— æ”¯æŒä½¿ç”¨[Unsloth](https://github.com/yangjianxin1/unsloth)åŠ é€Ÿè®­ç»ƒï¼Œå¹¶ä¸”èŠ‚çœæ˜¾å­˜ã€‚\n- ğŸ“— æ”¯æŒç»å¤§éƒ¨åˆ†ä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œå¦‚Llama3ã€Gemmaã€MiniCPMã€Llamaã€InternLMã€Baichuanã€ChatGLMã€Yiã€Deepseekã€Qwenã€Orionã€Ziyaã€Xverseã€Mistralã€Mixtral-8x7Bã€Zephyrã€Vicunaã€Bloomï¼Œè®­ç»ƒæ—¶ä¸å„ä¸ªå®˜æ–¹çš„chatæ¨¡å‹çš„templateå¯¹é½ã€‚\n- ğŸ“— æ•´ç†å¹¶å¼€æºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼šfirefly-train-1.1M ã€moss-003-sft-dataã€ultrachatã€ WizardLM_evol_instruct_V2_143kã€school_math_0.25Mã€‚\n- ğŸ“— å¼€æº[Fireflyç³»åˆ—æŒ‡ä»¤å¾®è°ƒæ¨¡å‹æƒé‡](https://huggingface.co/YeungNLP) ã€‚\n- ğŸ“— åœ¨Open LLM Leaderboardä¸ŠéªŒè¯äº†QLoRAè®­ç»ƒæµç¨‹çš„æœ‰æ•ˆæ€§ã€‚\n\nå½“å‰ç‰ˆæœ¬é’ˆå¯¹ä¸åŒçš„chatæ¨¡å‹çš„templateè¿›è¡Œäº†é€‚é…ï¼Œä»£ç å­˜åœ¨è¾ƒå¤§çš„æ›´æ–°ã€‚è‹¥ä½ æ›´å–œæ¬¢æ­¤å‰çš„ç‰ˆæœ¬ï¼Œå¯ä¸‹è½½ä»£ç [v0.0.1-alpha](https://github.com/yangjianxin1/Firefly/releases/tag/v0.0.1-alpha)\n\n## News\n- ğŸ”¥ Unsloth x Qwen2æ¨¡å‹ç»“æ„çš„PRå·²mergeåˆ°[Unsloth](https://github.com/unslothai/unsloth)çš„masteråˆ†æ”¯ã€‚\n- ğŸ”¥ æ‰©å±•Unslothï¼Œæ”¯æŒQwen2æ¨¡å‹ç»“æ„ï¼ŒåŒ…æ‹¬Qwen1.5ç³»åˆ—çš„Denseæ¨¡å‹ï¼Œä»£ç åº“ï¼š[Unsloth](https://github.com/yangjianxin1/unsloth)ã€‚ [æŠ€æœ¯æ–‡ç« ](https://mp.weixin.qq.com/s/x2N3p1qgJy_RyRsO2PHS_A)\n- ğŸ”¥ æ”¯æŒ[Unsloth](https://github.com/unslothai/unsloth)ï¼Œè®­ç»ƒLlama3-8Bä»…éœ€7.75GBæ˜¾å­˜ï¼Œå¯å‡å°‘42.58%æ˜¾å­˜å ç”¨ï¼Œå‡å°‘30.72%è®­ç»ƒæ—¶é—´ã€‚ [è®­ç»ƒå¢ç›Šè¯„æµ‹](https://mp.weixin.qq.com/s/Zlp7GM37_bkvvQZedzNp0g)ã€‚\n- ğŸ”¥ ä¼˜åŒ–è®­ç»ƒæµç¨‹ï¼Œæ”¯æŒå…¨é‡è®­ç»ƒã€LoRAã€QLoRAé«˜æ•ˆè®­ç»ƒï¼Œæ”¯æŒé¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’ŒDPOã€‚æŒ‡ä»¤å¾®è°ƒä¸DPOçš„templateä¸åŸæœ‰çš„chatæ¨¡å‹å¯¹é½ï¼Œæ”¯æŒç»å¤§å¤šæ•°å¼€æºæ¨¡å‹ï¼ŒåŒ…æ‹¬Gemmaã€MiniCPMã€Llamaã€InternLMã€Baichuanã€ChatGLMã€Yiã€Deepseekã€Qwenã€Orionã€Ziyaã€Xverseã€Mistralã€Mixtral-8x7Bã€Zephyrã€Vicunaã€Bloomç­‰ã€‚\n- ğŸ”¥ å¼€æºæ¨¡å‹æƒé‡[firefly-mixtral-8x7b](https://huggingface.co/YeungNLP/firefly-mixtral-8x7b) ï¼Œåœ¨[ğŸ¤—Open LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)åˆ†æ•°ä¸º70.34ï¼Œè¶…è¶ŠYi-34Bã€Llama2-65B-Chatã€Qwen-14Bã€Vicuna-33B-v1.3ç­‰æ¨¡å‹ã€‚\n- ğŸ”¥ å¼€æº[LongQLoRA](https://github.com/yangjianxin1/LongQLoRA)ï¼Œ ã€[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2311.04879)ã€‘ã€‚å¯é«˜æ•ˆæ‰©å±•LLamaä¸Šä¸‹æ–‡é•¿åº¦ï¼Œåœ¨å•å¼ 32GB V100ä¸Šå°†Llama2é•¿åº¦æ‰©å±•è‡³8kï¼ˆäº¦å¯æ‰©å±•è‡³12kï¼‰ï¼Œä»…å¾®è°ƒ1000 stepï¼Œåœ¨PG19å’ŒProof-pileæ•°æ®é›†ä¸Šçš„perplexityä¼˜äºLongLoRAï¼Œåœ¨PG19ä¸Šç•¥èƒœMPT-7B-8Kã€‚\n- ğŸ”¥ å¼€æº[Firefly-LLaMA2-Chineseé¡¹ç›®](https://github.com/yangjianxin1/Firefly-LLaMA2-Chinese)ï¼Œ**åœ¨4*V100ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒ**ï¼Œç»è¿‡ä¸­æ–‡è¯è¡¨æ‰©å……ã€å¢é‡é¢„è®­ç»ƒã€å¤šè½®æŒ‡ä»¤å¾®è°ƒï¼Œåœ¨CMMLUä¸Šè¶…è¶ŠLinlyã€Yayiã€FlagAlphaç­‰ï¼Œä¸Ziyaã€Chinese-Alpacaè¡¨ç°åŸºæœ¬æŒå¹³ã€‚\n- ğŸ”¥ å¼€æº[firefly-baichuan2-13b](https://huggingface.co/YeungNLP/firefly-baichuan2-13b)ï¼Œåœ¨OpenCompassçš„CMMLUæ¦œå•ä¸Šä»¥56.83çš„åˆ†æ•°ï¼Œä½åˆ—ç¬¬8ï¼Œæ¯”ç™¾å·å®˜æ–¹Chatæ¨¡å‹ç•¥ä½1.57åˆ†ã€‚\n- ğŸ”¥ å¼€æº[firefly-llama-30b](https://huggingface.co/YeungNLP/firefly-llama-30b)ï¼Œåœ¨[ğŸ¤—Open LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ä¸Šä»¥64.83åˆ†ï¼ŒåŒé‡çº§æ¨¡å‹**æ’åç¬¬10**ã€‚\n- ğŸ”¥ å¼€æº[firefly-llama2-13b](https://huggingface.co/YeungNLP/firefly-llama2-13b)ï¼Œåœ¨[ğŸ¤—Open LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ä¸Šä»¥62åˆ†ï¼ŒåŒé‡çº§æ¨¡å‹**æ’åç¬¬3**ï¼Œæ¯”æ¦œé¦–ç•¥ä½0.5åˆ†ã€‚\n- ğŸ”¥ å¼€æº[firefly-llama-13b](https://huggingface.co/YeungNLP/firefly-llama-13b)ï¼Œåœ¨[Hugging Faceçš„Open LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ä¸Šå¤åˆ»Vicuna-13Bï¼Œæ¯”Vicuna-13b-1.1ç•¥é«˜0.2åˆ†ï¼Œæ¯”llams-2-13b-chatç•¥ä½0.5åˆ†ã€‚\n- [LLMPrunerï¼šå¤§è¯­è¨€æ¨¡å‹è£å‰ªå·¥å…·](https://github.com/yangjianxin1/LLMPruner) ï¼Œå¼€æº[è£å‰ªåçš„Bloomæ¨¡å‹æƒé‡](https://huggingface.co/YeungNLP) ã€‚\n\n## ç›¸å…³é¡¹ç›®\n- [Firefly-LLaMA2-Chinese](https://github.com/yangjianxin1/Firefly-LLaMA2-Chinese)ï¼šä¸­æ–‡Llama2æ¨¡å‹ï¼Œå¯¹Llama2è¿›è¡Œä¸­æ–‡è¯è¡¨æ‰©å……ã€å¢é‡é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒã€‚\n- [LongQLoRA](https://github.com/yangjianxin1/LongQLoRA)ï¼šå¤§æ¨¡å‹é•¿åº¦æ‰©å±•é¡¹ç›®ï¼Œå¯åœ¨å•å¡V100ä¸Šå°†LLaMA-13Bçš„é•¿åº¦æ‰©å±•è‡³8192ï¼Œä¸”æ€§èƒ½é€¼è¿‘MPT-8Kã€‚\n- [LLMPruner](https://github.com/yangjianxin1/LLMPruner)ï¼šå¯¹Bloomè¿›è¡Œè¯è¡¨è£å‰ªï¼Œå‡å°‘æ¨¡å‹å‚æ•°é‡ã€‚\n\n## æŠ€æœ¯åšå®¢\n<details><summary><b>æŠ€æœ¯åšå®¢</b></summary>\n\n- [Unsloth x Qwen2ï¼Œæé€Ÿ47.32%ï¼ŒèŠ‚çœ39.13%æ˜¾å­˜ï¼Œæœ€å°‘ä»…éœ€8.43GBæ˜¾å­˜](https://mp.weixin.qq.com/s/x2N3p1qgJy_RyRsO2PHS_A)\n- [Unslothå¾®è°ƒLlama3-8Bï¼Œæé€Ÿ44.35%ï¼ŒèŠ‚çœ42.58%æ˜¾å­˜ï¼Œæœ€å°‘ä»…éœ€7.75GBæ˜¾å­˜](https://mp.weixin.qq.com/s/Zlp7GM37_bkvvQZedzNp0g)\n- [å¼±æ™ºå§ç¥›é­…ï¼Œä¸å¼ºBaselineçš„å¯¹æ¯”å®éªŒï¼Œå·®è·æ˜æ˜¾](https://mp.weixin.qq.com/s/LwGgMbPdC_UTCefqWSkXEQ)\n- [å…³äºå¼±æ™ºå§æ•°æ®å°ç¥çš„è‹¥å¹²ç–‘é—®å’ŒçŒœæƒ³ï¼Œä»¥åŠæ•°æ®éªŒè¯å®éªŒ](https://mp.weixin.qq.com/s/PnJVA66QLp4-gZTss46PqQ)\n- [å›¾è§£å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–ä¹‹KV Cache](https://mp.weixin.qq.com/s/7Fm8LbUN9jQ2HqxPbUU7UQ)\n- [Mixtral-8x7B MoEå¤§æ¨¡å‹å¾®è°ƒå®è·µï¼Œè¶…è¶ŠLlama2-65B](https://mp.weixin.qq.com/s/f24e-Tp-1WyXTbVOzePvhg)\n- [LongQLoRAï¼šå•å¡é«˜æ•ˆæ‰©å±•LLaMA2-13Bçš„ä¸Šä¸‹æ–‡é•¿åº¦](https://mp.weixin.qq.com/s/lptWXi9sZXd2MTTXZsDiPw)\n- [è¯¦è§£åŸºäºè°ƒæ•´RoPEæ—‹è½¬è§’åº¦çš„å¤§æ¨¡å‹é•¿åº¦å¤–æ¨æ–¹æ³•](https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ)\n- [å›¾è§£RoPEæ—‹è½¬ä½ç½®ç¼–ç åŠå…¶ç‰¹æ€§](https://mp.weixin.qq.com/s/-1xVXjoM0imXMC7DKqo-Gw)\n- [QLoRAè½»é‡çº§å¢é‡é¢„è®­ç»ƒæ–¹æ¡ˆï¼ŒåŠæ±‰åŒ–Llama2çš„å®è·µ](https://mp.weixin.qq.com/s/26-Qxma9M2wGoTQgOlKRmQ)\n- [Fireflyå¤šè½®å¯¹è¯å¾®è°ƒä¹¦ç”ŸÂ·æµ¦è¯­InternLM-7Bå®è·µ](https://mp.weixin.qq.com/s/98OLdkHjoGDHNDbYL7RerA)\n- [ğŸ¤—Fireflyå¾®è°ƒLLaMA-30Bï¼ŒOpen LLMæ¦œå•åŒé‡çº§ç¬¬10å](https://mp.weixin.qq.com/s/fFT0Pxfecma4n_fXQYb2Mw)\n- [é€šä¹‰åƒé—®Qwen-7Bæ•ˆæœå¦‚ä½•ï¼ŸFireflyå¾®è°ƒå®è·µï¼Œæ•ˆæœå‡ºè‰²](https://mp.weixin.qq.com/s/5OAx83j6Op299XAfa496ww)\n- [æºç è§£æChatGLM2å¤šè½®å¯¹è¯è®­ç»ƒæ–¹æ³•çš„ä¸è¶³ï¼Œä»¥åŠæ”¹è¿›æ–¹æ³•](https://mp.weixin.qq.com/s/nhogoWnzl3nrs_77r38_UA)\n- [Fireflyå¢å¼ºBaichuan-13Bçš„å¤šè½®å¯¹è¯èƒ½åŠ›](https://mp.weixin.qq.com/s/djO8Tg3emmy6wzw_rTUlcw)\n- [ğŸ¤—Open LLMæ’è¡Œæ¦œï¼Œfirefly-llama2-13båœ¨æ‰€æœ‰13Bæ¨¡å‹ä¸­æ’åç¬¬ä¸‰ï¼Œæ¯”æ¦œé¦–ç•¥ä½0.5åˆ†](https://mp.weixin.qq.com/s/w1V3QGvsRTQsQqAKp2z6Kg)\n- [ç™¾ä¸‡æ•°æ®å¢å¼ºBaichuan-13Bçš„å¤šè½®å¯¹è¯èƒ½åŠ›](https://mp.weixin.qq.com/s/djO8Tg3emmy6wzw_rTUlcw)\n- [Fireflyå•å¡å¤åˆ»Vicuna-13Bï¼ŒOpen LLMæ¦œå•ğŸ¤—ç•¥é«˜0.2åˆ†](https://mp.weixin.qq.com/s/QG2YMo_QxaxS_Rr2yJrIeA)\n- [å¾®è°ƒç™¾å·Baichuan-13Bä¿å§†å¼æ•™ç¨‹ï¼Œæ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒç™¾äº¿å¤§æ¨¡å‹](https://mp.weixin.qq.com/s/ZBY6kbogHjbCQvZBzNEqag)\n- [Firefly-Ziya-13Bå¼€æºï¼ŒQLoRA+ç™¾ä¸‡æ•°æ®ï¼Œå•å¡å¯è®­ç™¾äº¿å¤§æ¨¡å‹](https://mp.weixin.qq.com/s/vgNK6D-_0j4Chk2H1Ev-Ig)\n- [Fireflyï½œç™¾å·baichuan-7Bå®æµ‹ï¼ŒQLoRA+ç™¾ä¸‡æŒ‡ä»¤æ•°æ®å¾®è°ƒ](https://mp.weixin.qq.com/s/_eTkDGG5DmxyWeiQ6DIxBw)\n- [Firefly | QLoRA+ç™¾ä¸‡æ•°æ®ï¼Œå¤šå¡é«˜æ•ˆå¾®è°ƒbloom-7b1æ¨¡å‹](https://mp.weixin.qq.com/s/lA4YUJ9XGpKlUUUjz0Le-g)\n- [QLoRAæ–‡ç« è§£è¯» & å•å¡é«˜æ•ˆå¾®è°ƒbloom-7b1](https://mp.weixin.qq.com/s/DED7yeiE0DibsVzTmMeDOw)\n- [Firefly(æµè¤): ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹](https://mp.weixin.qq.com/s/TX7wj8IzD_EaMTvk0bjRtA)\n- [LLMPrunerï¼šå¤§è¯­è¨€æ¨¡å‹è£å‰ªå·¥å…·](https://mp.weixin.qq.com/s/leVtrwZc1zLput51Nr99lw)\n\n</details>\n\n\n## æ¨¡å‹è¯„æµ‹\n\n### Open LLM Leaderboardè¯„æµ‹\nè¯„æµ‹ç»“æœæ¥æºäºHugging Faceçš„[Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ã€‚æˆ‘ä»¬çš„æ¨¡å‹å‡é‡‡ç”¨QLoRAè„šæœ¬è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒä»…ä½¿ç”¨1~2å¼ V100ã€‚\n\n\n| æ¨¡å‹                          | Average | ARC   | HellaSwag | MMLU  | TruthfulQA |\n|-----------------------------|---------|-------|-----------|-------|------------|\n| **firefly-mixtral-8x7b**    | 70.16   | 68.09 | 85.76     | 71.49 | 55.31      |\n| Yi-34B-Chat                 | 69.97   | 65.44 | 84.16     | 74.9  | 55.37      |\n| **firefly-llama-30b**       | 64.83   | 64.25 | 83.64     | 58.23 | 53.2       |\n| falcon-40b-instruct         | 63.47   | 61.6  | 84.31     | 55.45 | 52.52      |\n| guanaco-33b                 | 62.98   | 62.46 | 84.48     | 53.78 | 51.22      |\n| **firefly-llama2-13b-v1.2** | 62.17   | 60.67 | 80.46     | 56.51 | 51.03      |\n| **firefly-llama2-13b**      | 62.04   | 59.13 | 81.99     | 55.49 | 51.57      |\n| vicuna-13b-v1.5             | 61.63   | 56.57 | 81.24     | 56.67 | 51.51      |\n| mpt-30b-chat                | 61.21   | 58.7  | 82.54     | 51.16 | 52.42      |\n| wizardlm-13b-v1.2           | 60.79   | 59.04 | 82.21     | 54.64 | 47.27      |\n| vicuna-13b-v1.3             | 60.01   | 54.61 | 80.41     | 52.88 | 52.14      |\n| llama-2-13b-chat            | 59.93   | 59.04 | 81.94     | 54.64 | 44.12      |\n| vicuna-13b-v1.1             | 59.21   | 52.73 | 80.14     | 51.9  | 52.08      |\n| guanaco-13b                 | 59.18   | 57.85 | 83.84     | 48.28 | 46.73      |\n\n\n## æ¨¡å‹åˆ—è¡¨\n\nğŸ”” ä½¿ç”¨æœ¬é¡¹ç›®çš„è®­ç»ƒä»£ç ï¼Œä»¥åŠä¸Šè¿°è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬è®­ç»ƒå¹¶å¼€æºäº†ä»¥ä¸‹æ¨¡å‹æƒé‡ã€‚\n\nä¸­æ–‡æ¨¡å‹ï¼š\n\n| æ¨¡å‹                                                                             | åŸºåº§æ¨¡å‹                                | è®­ç»ƒé•¿åº¦ |\n|--------------------------------------------------------------------------------|-------------------------------------|------|\n| [firefly-baichuan2-13b](https://huggingface.co/YeungNLP/firefly-baichuan2-13b) | baichuan-inc/Baichuan2-13B-Base     | 1024 |  \n| [firefly-baichuan-13b](https://huggingface.co/YeungNLP/firefly-baichuan-13b)   | baichuan-inc/Baichuan-13B-Base      | 1024 |  \n| [firefly-qwen-7b](https://huggingface.co/YeungNLP/firefly-qwen-7b)             | Qwen/Qwen-7B                        | 1024 |  \n| [firefly-chatglm2-6b](https://huggingface.co/YeungNLP/firefly-chatglm2-6b)     | THUDM/chatglm2-6b                   | 1024 |  \n| [firefly-internlm-7b](https://huggingface.co/YeungNLP/firefly-internlm-7b)     | internlm/internlm-7b                | 1024 |  \n| [firefly-baichuan-7b](https://huggingface.co/YeungNLP/firefly-baichuan-7b)     | baichuan-inc/baichuan-7B            | 1024 |           \n| [firefly-ziya-13b](https://huggingface.co/YeungNLP/firefly-ziya-13b)           | YeungNLP/Ziya-LLaMA-13B-Pretrain-v1 | 1024 |           \n| [firefly-bloom-7b1](https://huggingface.co/YeungNLP/firefly-bloom-7b1)         | bigscience/bloom-7b1                | 1024 |\n| [firefly-bloom-2b6-v2](https://huggingface.co/YeungNLP/firefly-bloom-2b6-v2)   | YeungNLP/bloom-2b6-zh               | 512  |\n| [firefly-bloom-2b6](https://huggingface.co/YeungNLP/firefly-bloom-2b6)         | YeungNLP/bloom-2b6-zh               | 512  |\n| [firefly-bloom-1b4](https://huggingface.co/YeungNLP/firefly-bloom-1b4)         | YeungNLP/bloom-1b4-zh               | 512  |\n\n\nè‹±æ–‡æ¨¡å‹ï¼š\n\n| æ¨¡å‹                                                                     | åŸºåº§æ¨¡å‹              | è®­ç»ƒé•¿åº¦ |\n|------------------------------------------------------------------------|-------------------|------|\n| [firefly-mixtral-8x7b](https://huggingface.co/YeungNLP/firefly-mixtral-8x7b)    | mistralai/Mixtral-8x7B-v0.1                  | 1024 |\n| [firefly-llama-30b](https://huggingface.co/YeungNLP/firefly-llama-30b) | huggyllama/llama-30b | 1024 |  \n| [firefly-llama-13-v1.2](https://huggingface.co/YeungNLP/firefly-llama2-13b-v1.2) | NousResearch/Llama-2-13b-hf | 1024 |  \n| [firefly-llama2-13b](https://huggingface.co/YeungNLP/firefly-llama2-13b) | NousResearch/Llama-2-13b-hf | 1024 |           \n| [firefly-llama-13b-v1.2](https://huggingface.co/YeungNLP/firefly-llama-13b-v1.2) | huggyllama/llama-13b | 1024 |           \n| [firefly-llama-13b](https://huggingface.co/YeungNLP/firefly-llama-13b) | huggyllama/llama-13b | 1024 |\n\n\n\n## è®­ç»ƒæ•°æ®\n### æŒ‡ä»¤å¾®è°ƒæ•°æ®\nğŸ”” ç›®å‰æœ¬é¡¹ç›®ä¸»è¦æ•´ç†äº†å¦‚ä¸‹æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶å°†å…¶æ•´ç†æˆç»Ÿä¸€çš„æ•°æ®æ ¼å¼ï¼š\n\n| æ•°æ®é›†                                                                                                          | ä»‹ç»                                                                                                      |\n|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n| [firefly-train-1.1M](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)                            | æˆ‘ä»¬æ”¶é›†äº†23ç§å¸¸è§çš„ä¸­æ–‡NLPä»»åŠ¡çš„æ•°æ®ï¼Œå¹¶ä¸”æ„é€ äº†è®¸å¤šä¸ä¸­åæ–‡åŒ–ç›¸å…³çš„æ•°æ®ï¼Œå¦‚å¯¹è”ã€ä½œè¯—ã€æ–‡è¨€æ–‡ç¿»è¯‘ã€æ•£æ–‡ã€é‡‘åº¸å°è¯´ç­‰ã€‚å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œç”±äººå·¥ä¹¦å†™è‹¥å¹²ç§æŒ‡ä»¤æ¨¡æ¿ï¼Œä¿è¯æ•°æ®çš„é«˜è´¨é‡ä¸ä¸°å¯Œåº¦ï¼Œæ•°æ®é‡ä¸º115ä¸‡ |\n| [moss-003-sft-data](https://huggingface.co/datasets/YeungNLP/moss-003-sft-data)                              | ç”±å¤æ—¦å¤§å­¦MOSSå›¢é˜Ÿå¼€æºçš„ä¸­è‹±æ–‡å¤šè½®å¯¹è¯æ•°æ®ï¼ŒåŒ…å«100ä¸‡+æ•°æ®                                                                       |\n| [ultrachat](https://huggingface.co/datasets/YeungNLP/ultrachat)                                              | ç”±æ¸…åå¤§å­¦å¼€æºçš„è‹±æ–‡å¤šè½®å¯¹è¯æ•°æ®ï¼ŒåŒ…å«140ä¸‡+æ•°æ®                                                                              |\n| [WizardLM_evol_instruct_V2_143k](https://huggingface.co/datasets/YeungNLP/WizardLM_evol_instruct_V2_143k) | ç”±WizardLMé¡¹ç›®å¼€æºçš„è‹±æ–‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œé€šè¿‡Evol-Instructæ–¹æ³•è®©æŒ‡ä»¤è¿›åŒ–ï¼ŒåŠ å¼ºæŒ‡ä»¤çš„å¤æ‚åº¦ï¼Œä»¥æå‡æ¨¡å‹å¯¹å¤æ‚æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›ã€‚åŒ…å«143kæ¡æ•°æ®ã€‚                      |\n| [school_math_0.25M](https://huggingface.co/datasets/YeungNLP/school_math_0.25M)     | ç”±BELLEé¡¹ç›®ç»„å¼€æºçš„æ•°å­¦è¿ç®—æŒ‡ä»¤æ•°æ®ï¼ŒåŒ…å«25ä¸‡æ¡æ•°æ®ã€‚                                                                          |\n| [shareAI/CodeChat](https://huggingface.co/datasets/shareAI/CodeChat)      | ä¸»è¦åŒ…å«é€»è¾‘æ¨ç†ã€ä»£ç é—®ç­”ã€ä»£ç ç”Ÿæˆç›¸å…³è¯­æ–™æ ·æœ¬ã€‚                                                                               |\n| [shareAI/ShareGPT-Chinese-English-90k](https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k)     | ä¸­è‹±æ–‡å¹³è¡ŒåŒè¯­ä¼˜è´¨äººæœºé—®ç­”æ•°æ®é›†ï¼Œè¦†ç›–çœŸå®å¤æ‚åœºæ™¯ä¸‹çš„ç”¨æˆ·æé—®ã€‚                                                                        |\n| [ultrachat_200k](https://huggingface.co/datasets/YeungNLP/ultrachat_200k)      | ç”±Zephyré¡¹ç›®å¼€æºçš„è‹±æ–‡æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œåœ¨ultrachatæ•°æ®åŸºç¡€ä¸Šè¿›è¡Œæ¸…æ´—                                                                |\n| [ultrafeedback_binarized](https://huggingface.co/datasets/YeungNLP/ultrafeedback_binarized)      | è‹±æ–‡åå¥½æ•°æ®é›†ï¼Œå¯ç”¨äºDPOè®­ç»ƒ                                                                                        |\n\n\nå¯æŒ‰éœ€é€‰æ‹©ã€é‡‡æ ·æˆ–ç»„åˆä¸Šè¿°æ•°æ®ã€‚è‹¥æ˜¯è®­ç»ƒä¸­æ–‡æ¨¡å‹ï¼Œå»ºè®®å°è¯•ä½¿ç”¨mossæ•°æ®ï¼Œè‹±æ–‡å¯å°è¯•ultrachatå’ŒWizardLMæ•°æ®ã€‚\n\nè®­ç»ƒæ•°æ®ä¸ºjsonlæ ¼å¼ï¼Œæ¯ä¸€è¡Œçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼Œå…¶ä¸­conversationå­—æ®µæ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚æ·»åŠ æˆ–åˆ é™¤å…¶ä»–å­—æ®µã€‚\nå¯ä»¥å‚è€ƒé¡¹ç›®ä¸­çš„data/dummy_data.jsonlæ–‡ä»¶ã€‚\n```json\n{\n    \"conversation_id\":44,\n    \"category\":\"Brainstorming\",\n    \"conversation\":[\n        {\n            \"human\":\"æ€ä¹ˆèƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡ä¸ªäººèƒ½åŠ›ï¼Ÿ\",\n            \"assistant\":\"æå‡ä¸ªäººèƒ½åŠ›å¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å®ç°ï¼š\\n\\n1. å­¦ä¹ æ–°çŸ¥è¯†å’ŒæŠ€èƒ½ï¼šé˜…è¯»ç›¸å…³ä¹¦ç±ã€å‚åŠ è¯¾ç¨‹å’Œå·¥ä½œåŠã€è§‚çœ‹æ•™è‚²è§†é¢‘ç­‰å­¦ä¹ æ–¹å¼ï¼Œå¯ä»¥å¸®åŠ©ä½ æŒæ¡æ–°çš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œä»è€Œæå‡ä¸ªäººèƒ½åŠ›ã€‚\\n\\n2. æŒç»­ç»ƒä¹ å’Œå®è·µï¼šä¸æ–­åœ°ç»ƒä¹ å’Œå®è·µæ‰€å­¦çš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œå¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°æŒæ¡å’Œåº”ç”¨è¿™äº›æŠ€èƒ½ï¼Œæé«˜ä½ çš„è¡¨ç°å’Œæ•ˆç‡ã€‚\\n\\n3. å¯»æ±‚åé¦ˆå’Œæ”¹è¿›ï¼šå¯»æ±‚ä»–äººçš„åé¦ˆå’Œå»ºè®®å¯ä»¥å¸®åŠ©ä½ å‘ç°è‡ªå·±çš„ä¸è¶³ä¹‹å¤„ï¼Œè¿›ä¸€æ­¥å®Œå–„å’Œæ”¹è¿›è‡ªå·±çš„èƒ½åŠ›ã€‚\\n\\n4. å»ºç«‹è‰¯å¥½çš„å·¥ä½œå’Œç”Ÿæ´»ä¹ æƒ¯ï¼šå»ºç«‹è‰¯å¥½çš„å·¥ä½œå’Œç”Ÿæ´»ä¹ æƒ¯ï¼Œå¦‚åˆç†å®‰æ’æ—¶é—´ã€æ³¨æ„å¥åº·ã€ä¿æŒç§¯ææ€åº¦ç­‰ï¼Œå¯ä»¥æé«˜ä½ çš„å·¥ä½œæ•ˆç‡å’Œç”Ÿæ´»å“è´¨ï¼Œä»è€Œæå‡ä¸ªäººèƒ½åŠ›ã€‚\\n\\n5. ç¤¾äº¤å’Œäº¤æµï¼šä¸ä»–äººäº¤æµå’Œåˆä½œï¼Œå¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£ä»–äººçš„è§‚ç‚¹å’Œç»éªŒï¼Œä»è€Œæå‡è‡ªå·±çš„èƒ½åŠ›å’Œè§è¯†ã€‚\"\n        },\n        {\n            \"human\":\"å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œå¦‚ä½•å¿«é€ŸæŒæ¡ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ\",\n            \"assistant\":\"ä»¥ä¸‹æ˜¯å‡ ä¸ªå»ºè®®ï¼Œå¯ä»¥å¸®åŠ©åˆå­¦è€…å¿«é€ŸæŒæ¡ç¼–ç¨‹æŠ€èƒ½ï¼š\\n\\n1. å­¦ä¹ åŸºç¡€çŸ¥è¯†ï¼šå»ºè®®å…ˆå­¦ä¹ ç¼–ç¨‹è¯­è¨€çš„åŸºç¡€çŸ¥è¯†å’Œè¯­æ³•ï¼Œä¾‹å¦‚å˜é‡ã€æ•°æ®ç±»å‹ã€æ§åˆ¶ç»“æ„ã€å‡½æ•°ç­‰ã€‚å¯ä»¥é€šè¿‡åœ¨çº¿è¯¾ç¨‹ã€ç¼–ç¨‹ä¹¦ç±ã€è§†é¢‘æ•™ç¨‹ç­‰æ–¹å¼å­¦ä¹ ã€‚\\n\\n2. å®è·µç¼–ç¨‹ï¼šè¿›è¡Œå®é™…çš„ç¼–ç¨‹ç»ƒä¹ æ˜¯éå¸¸é‡è¦çš„ï¼Œå¯ä»¥é€šè¿‡å‚åŠ ç¼–ç¨‹æŒ‘æˆ˜ã€ç¼–å†™å°é¡¹ç›®ç­‰æ–¹å¼æ¥è¿›è¡Œå®è·µã€‚\\n\\n3. å¯»æ±‚åé¦ˆï¼šå¯ä»¥å¯»æ±‚ä»–äººçš„åé¦ˆå’Œå»ºè®®ï¼Œä¾‹å¦‚åœ¨çº¿ç¼–ç¨‹ç¤¾åŒºã€ç¼–ç¨‹å¯¼å¸ˆç­‰ï¼Œä»–ä»¬å¯ä»¥å¸®åŠ©ä½ å‘ç°é—®é¢˜å¹¶åŠæ—¶çº æ­£ã€‚\\n\\n4. é˜…è¯»ä¼˜ç§€ä»£ç ï¼šé˜…è¯»å’Œåˆ†æä¼˜ç§€çš„ä»£ç å¯ä»¥å¸®åŠ©ä½ å­¦ä¹ å…¶ä»–ç¨‹åºå‘˜çš„ç¼–ç¨‹æ€è·¯å’ŒæŠ€å·§ï¼Œä»è€Œæé«˜è‡ªå·±çš„ç¼–ç¨‹èƒ½åŠ›ã€‚\\n\\n5. ä¸æ–­å­¦ä¹ å’Œæ›´æ–°ï¼šç¼–ç¨‹æŠ€æœ¯å’Œå·¥å…·åœ¨ä¸æ–­æ›´æ–°å’Œå‘å±•ï¼Œå› æ­¤éœ€è¦ä¸æ–­å­¦ä¹ æ–°çš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œä¾‹å¦‚å­¦ä¹ æ–°çš„ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ã€å·¥å…·ç­‰ï¼Œä»¥è·Ÿä¸ŠæŠ€æœ¯çš„å‘å±•å’Œå˜åŒ–ã€‚\\n\\næ€»ä¹‹ï¼Œå­¦ä¹ ç¼–ç¨‹éœ€è¦è€å¿ƒå’ŒåšæŒï¼Œå¹¶ä¸”éœ€è¦ä¸æ–­åœ°å®è·µå’Œå­¦ä¹ ã€‚é€šè¿‡ä»¥ä¸Šå»ºè®®ï¼Œå¯ä»¥å¸®åŠ©åˆå­¦è€…å¿«é€ŸæŒæ¡ç¼–ç¨‹æŠ€èƒ½ã€‚\"\n        },\n        {\n            \"human\":\"é™¤äº†å‚åŠ ç¼–ç¨‹æŒ‘æˆ˜å’Œé˜…è¯»ä»£ç ï¼Œè¿˜æœ‰å“ªäº›æ–¹å¼å¯ä»¥æå‡è‡ªå·±çš„ç¼–ç¨‹èƒ½åŠ›å‘¢ï¼Ÿ\",\n            \"assistant\":\"é™¤äº†å‚åŠ ç¼–ç¨‹æŒ‘æˆ˜å’Œé˜…è¯»ä»£ç ï¼Œè¿˜æœ‰ä»¥ä¸‹å‡ ç§æ–¹å¼å¯ä»¥æå‡è‡ªå·±çš„ç¼–ç¨‹èƒ½åŠ›ï¼š\\n\\n1. å‚ä¸å¼€æºé¡¹ç›®ï¼šåŠ å…¥ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå¯ä»¥è®©ä½ äº†è§£åˆ°å®é™…çš„è½¯ä»¶å¼€å‘æµç¨‹å’Œå›¢é˜Ÿåˆä½œæ–¹å¼ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–å¼€å‘äººå‘˜åˆä½œï¼Œå­¦ä¹ å’Œåˆ†äº«ç¼–ç¨‹æŠ€å·§å’Œç»éªŒã€‚\\n\\n2. è¿›è¡Œä»£ç é‡æ„ï¼šå¯¹å·²ç»ç¼–å†™çš„ä»£ç è¿›è¡Œé‡æ„ï¼Œå¯ä»¥å¸®åŠ©ä½ äº†è§£æ›´å¥½çš„ç¼–ç¨‹å®è·µå’Œè®¾è®¡æ¨¡å¼ï¼Œä»è€Œæé«˜ä»£ç çš„å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚\\n\\n3. ç»ƒä¹ ç®—æ³•å’Œæ•°æ®ç»“æ„ï¼šç®—æ³•å’Œæ•°æ®ç»“æ„æ˜¯ç¼–ç¨‹ä¸­çš„é‡è¦åŸºç¡€ï¼Œç»ƒä¹ ç®—æ³•å’Œæ•°æ®ç»“æ„å¯ä»¥å¸®åŠ©ä½ äº†è§£ä¸åŒçš„æ•°æ®ç»“æ„å’Œç®—æ³•ï¼Œä»è€Œæ›´å¥½åœ°è§£å†³å®é™…é—®é¢˜ã€‚\\n\\n4. å­¦ä¹ è½¯ä»¶å·¥ç¨‹å’Œæ¶æ„ï¼šå­¦ä¹ è½¯ä»¶å·¥ç¨‹å’Œæ¶æ„çš„çŸ¥è¯†ï¼Œå¯ä»¥å¸®åŠ©ä½ äº†è§£å¦‚ä½•è®¾è®¡å’Œæ„å»ºå¯é ã€å¯æ‰©å±•å’Œå¯ç»´æŠ¤çš„è½¯ä»¶ç³»ç»Ÿã€‚\\n\\n5. å‚åŠ ç¼–ç¨‹ç¤¾åŒºå’Œè®ºå›ï¼šå‚åŠ ç¼–ç¨‹ç¤¾åŒºå’Œè®ºå›å¯ä»¥è®©ä½ ä¸å…¶ä»–å¼€å‘äººå‘˜åˆ†äº«å’Œè®¨è®ºç¼–ç¨‹ç»éªŒå’ŒæŠ€å·§ï¼Œäº†è§£æœ€æ–°çš„æŠ€æœ¯è¶‹åŠ¿å’Œè§£å†³æ–¹æ¡ˆã€‚\\n\\næ€»ä¹‹ï¼Œæå‡ç¼–ç¨‹èƒ½åŠ›éœ€è¦ä¸æ–­åœ°å­¦ä¹ å’Œå®è·µï¼Œå¹¶ä¸å…¶ä»–å¼€å‘äººå‘˜å’Œç¼–ç¨‹ä¸“å®¶è¿›è¡Œäº¤æµå’Œåˆä½œï¼Œä»¥è·å–æ›´å¤šçš„ç»éªŒå’ŒçŸ¥è¯†ã€‚\"\n        },\n        {\n            \"human\":\"ä½ æœ‰æ²¡æœ‰æ¨èçš„ç¼–ç¨‹æŒ‘æˆ˜ç½‘ç«™ï¼Ÿ\",\n            \"assistant\":\"å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å€¼å¾—æ¨èçš„ç¼–ç¨‹æŒ‘æˆ˜ç½‘ç«™ï¼š\\n\\n1. HackerRankï¼šHackerRank æ˜¯ä¸€ä¸ªå…¨çƒé¢†å…ˆçš„æŠ€æœ¯æ‹›è˜å¹³å°ï¼Œæä¾›æ•°ç™¾ç§ç¼–ç¨‹æŒ‘æˆ˜å’Œæ¯”èµ›ï¼ŒåŒ…æ‹¬ç®—æ³•ã€æ•°æ®ç»“æ„ã€æ•°å­¦ã€SQLã€AI ç­‰å¤šä¸ªæŠ€æœ¯é¢†åŸŸã€‚\\n\\n2. LeetCodeï¼šLeetCode æ˜¯ä¸€ä¸ªåœ¨çº¿ç¼–ç¨‹å­¦ä¹ å’Œå®è·µå¹³å°ï¼Œæä¾›æ•°åƒé“ç¼–ç¨‹é¢˜ç›®ï¼Œä¸»è¦æ¶µç›–ç®—æ³•å’Œæ•°æ®ç»“æ„ã€‚\\n\\n3. Codeforcesï¼šCodeforces æ˜¯ä¸€ä¸ªå…¨çƒçŸ¥åçš„ç¼–ç¨‹ç«èµ›ç½‘ç«™ï¼Œæä¾›æ•°ç™¾ä¸ªæ¯”èµ›å’Œç»ƒä¹ é¢˜ç›®ï¼Œä¸»è¦æ¶µç›–ç®—æ³•å’Œæ•°æ®ç»“æ„ã€‚\\n\\n4. Codewarsï¼šCodewars æ˜¯ä¸€ä¸ªåœ¨çº¿ç¼–ç¨‹å­¦ä¹ å’Œå®è·µå¹³å°ï¼Œæä¾›æ•°åƒé“ç¼–ç¨‹é¢˜ç›®ï¼Œä¸»è¦æ¶µç›–ç®—æ³•ã€æ•°æ®ç»“æ„ã€é¢å‘å¯¹è±¡ç¼–ç¨‹ç­‰å¤šä¸ªæŠ€æœ¯é¢†åŸŸã€‚\\n\\n5. Project Eulerï¼šProject Euler æ˜¯ä¸€ä¸ªéè¥åˆ©æ€§çš„åœ¨çº¿ç¼–ç¨‹æŒ‘æˆ˜å¹³å°ï¼Œæä¾›ä¸€ç³»åˆ—æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦é—®é¢˜ï¼Œéœ€è¦ç”¨ç¼–ç¨‹è¯­è¨€è¿›è¡Œè§£ç­”ã€‚\\n\\nä»¥ä¸Šè¿™äº›ç¼–ç¨‹æŒ‘æˆ˜ç½‘ç«™å¯ä»¥å¸®åŠ©ä½ æé«˜ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–ç¼–ç¨‹çˆ±å¥½è€…è¿›è¡Œäº¤æµå’Œç«äº‰ã€‚\"\n        },\n        {\n            \"human\":\"åœ¨å­¦ä¹ ç¼–ç¨‹çš„è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•é¿å…é™·å…¥å›°å¢ƒå’Œæ”¾å¼ƒå‘¢ï¼Ÿ\",\n            \"assistant\":\"å­¦ä¹ ç¼–ç¨‹æ˜¯ä¸€ä¸ªéœ€è¦é•¿æœŸåšæŒå’Œä¸æ–­å­¦ä¹ çš„è¿‡ç¨‹ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›é¿å…é™·å…¥å›°å¢ƒå’Œæ”¾å¼ƒçš„æ–¹æ³•ï¼š\\n\\n1. åˆ¶å®šåˆç†çš„å­¦ä¹ è®¡åˆ’ï¼šåˆ¶å®šåˆç†çš„å­¦ä¹ è®¡åˆ’ï¼ŒåŒ…æ‹¬å­¦ä¹ æ—¶é—´ã€å­¦ä¹ å†…å®¹ã€ç›®æ ‡ç­‰ï¼Œå¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°ç®¡ç†æ—¶é—´å’Œç²¾åŠ›ï¼Œé¿å…å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¾æ‡ˆå’Œåˆ†å¿ƒã€‚\\n\\n2. å¯»æ‰¾åˆé€‚çš„å­¦ä¹ èµ„æºï¼šé€‰æ‹©é€‚åˆè‡ªå·±çš„å­¦ä¹ èµ„æºï¼Œä¾‹å¦‚åœ¨çº¿è¯¾ç¨‹ã€ç¼–ç¨‹ä¹¦ç±ã€è§†é¢‘æ•™ç¨‹ç­‰ï¼Œå¯ä»¥è®©ä½ æ›´å¥½åœ°äº†è§£å’ŒæŒæ¡ç¼–ç¨‹çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚\\n\\n3. å¯»æ±‚å¸®åŠ©å’Œæ”¯æŒï¼šåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œé‡åˆ°é—®é¢˜å’Œå›°éš¾æ˜¯å¾ˆæ­£å¸¸çš„ï¼Œå¯ä»¥å¯»æ±‚ä»–äººçš„å¸®åŠ©å’Œæ”¯æŒï¼Œä¾‹å¦‚å‚åŠ ç¼–ç¨‹ç¤¾åŒºã€æ‰¾åˆ°ç¼–ç¨‹å¯¼å¸ˆç­‰ã€‚\\n\\n4. è¿›è¡Œå®è·µå’Œé¡¹ç›®ï¼šå®è·µå’Œé¡¹ç›®æ˜¯å­¦ä¹ ç¼–ç¨‹çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£å’ŒæŒæ¡ç¼–ç¨‹æŠ€èƒ½ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æé«˜å­¦ä¹ çš„å…´è¶£å’ŒåŠ¨åŠ›ã€‚\\n\\n5. åšæŒå¹¶ä¿æŒå…´è¶£ï¼šåšæŒå­¦ä¹ å’Œä¿æŒå…´è¶£æ˜¯å­¦ä¹ ç¼–ç¨‹çš„å…³é”®ã€‚å¯ä»¥é€šè¿‡å‚åŠ ç¼–ç¨‹ç¤¾åŒºã€å‚åŠ ç¼–ç¨‹ç«èµ›ã€ä¸å…¶ä»–ç¼–ç¨‹çˆ±å¥½è€…äº¤æµç­‰æ–¹å¼æ¥ä¿æŒå…´è¶£å’ŒåŠ¨åŠ›ã€‚\\n\\næ€»ä¹‹ï¼Œå­¦ä¹ ç¼–ç¨‹éœ€è¦è€å¿ƒå’ŒåšæŒï¼Œå¹¶éœ€è¦ä¸æ–­å­¦ä¹ å’Œå®è·µã€‚é€šè¿‡ä»¥ä¸Šæ–¹æ³•å¯ä»¥å¸®åŠ©ä½ é¿å…é™·å…¥å›°å¢ƒå’Œæ”¾å¼ƒã€‚\"\n        }\n    ],\n}\n```\n\nå…¶ä¸­firefly-train-1.1Mçš„æ•°æ®åˆ†å¸ƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n<img src=\"pics/task_distribution.png\" width=\"380\"> \n\n### é¢„è®­ç»ƒæ•°æ®\næ•°æ®æ ¼å¼å¯ä»¥å‚è€ƒé¡¹ç›®ä¸­çš„data/pretrain/dummy_pretrain.jsonlæ–‡ä»¶ã€‚\n\n### DPOæ•°æ®\næ•°æ®æ ¼å¼å¯ä»¥å‚è€ƒé¡¹ç›®ä¸­çš„data/dummy_dpo.jsonlæ–‡ä»¶ã€‚\n\n## æ¨¡å‹è®­ç»ƒ\nè‹¥è®­ç»ƒä¸­æŠ¥é”™ï¼Œå¯å…ˆæŸ¥çœ‹[FAQ]()ã€‚\n\næˆ‘ä»¬å°†è®­ç»ƒä¸­ä½¿ç”¨çš„å„ç§ç»„ä»¶æŠ½å–å‡ºæ¥ï¼Œä»¥ä¾¿åç»­çš„æ‰©å±•å’Œä¼˜åŒ–ï¼Œè¯¦è§componentç›®å½•ä¸‹çš„å®ç°ã€‚è®­ç»ƒæ—¶çš„å‚æ•°é…ç½®å­˜å‚¨åœ¨train_argsç›®å½•ä¸‹ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç®¡ç†å’Œæ›´æ”¹ã€‚å¤§å®¶å¯ä»¥åœ¨train_argsç›®å½•ä¸‹æŸ¥çœ‹ä¸åŒæ¨¡å‹çš„è®­ç»ƒé…ç½®ï¼ŒæŒ‰éœ€ä¿®æ”¹æˆ–æ·»åŠ ã€‚\n\n### å®‰è£…ç¯å¢ƒ\nåœ¨requirements.txtä¸‹å›ºå®šäº†å‡ ä¸ªä¸»è¦çš„pythonåŒ…çš„ç‰ˆæœ¬ï¼Œæ‰§è¡Œå¦‚ä¸‹è„šæœ¬å³å¯ã€‚æ³¨æ„ï¼š\n- å¯¹äºç»å¤§éƒ¨åˆ†æ¨¡å‹ï¼Œæˆ‘ä»¬å‡åœ¨torch==1.13ï¼Œtransformers==4.36ç¯å¢ƒä¸Šè¿›è¡Œè°ƒè¯•å’Œè®­ç»ƒã€‚ä½†éƒ¨åˆ†è¾ƒæ–°çš„æ¨¡å‹ï¼Œéœ€è¦æ›´æ–°transformersç‰ˆæœ¬ã€‚\n  - Qwen1.5éœ€è¦å°†transformersæ›´æ–°åª4.37ã€‚\n  - Gemmaéœ€è¦å°†transformersæ›´æ–°åª4.38.1ï¼Œtorch==2.0.0ã€‚\n- ä½¿ç”¨QLoRAè®­ç»ƒBaichuan2æ—¶ï¼Œéœ€è¦å®‰è£…torch==2.0ï¼Œå¹¶ä¸”å¸è½½xformerså’Œapexã€‚\n- ä½¿ç”¨QLoRAè®­ç»ƒQwenæ—¶ï¼Œéœ€å°†flash-attnå¸è½½ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚\n```bash\npip install requirements.txt\n```\n\nå¦‚æœéœ€è¦å¼€å¯Unslothï¼Œå»ºè®®å®‰è£…æˆ–è€…æ›´æ–°ä»¥ä¸‹PythonåŒ…ï¼š\n```bash\npip install git+https://github.com/unslothai/unsloth.git\npip install bitsandbytes==0.43.1\npip install peft==0.10.0\npip install torch==2.2.2\npip install xformers==0.0.25.post1\n```\n\nå¦‚æœéœ€è¦ä½¿ç”¨Unslothå¯¹Qwen1.5è¿›è¡Œè®­ç»ƒï¼Œå®‰è£…å¦‚ä¸‹åŒ…ï¼š\n```bash\npip install git+https://github.com/yangjianxin1/unsloth.git\n```\n\n### æŸå¤±å‡½æ•°\né¢„è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»å…¸çš„è‡ªå›å½’æŸå¤±ï¼Œå³æ¯ä¸ªä½ç½®çš„tokenéƒ½ä¼šå‚ä¸lossè®¡ç®—ã€‚\n\næŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬ä»…è®¡ç®—assistantå›å¤éƒ¨åˆ†çš„lossã€‚\n\n### å‚æ•°è¯´æ˜\nğŸ“ train_argsç›®å½•ä¸‹å­˜å‚¨äº†ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒè®­ç»ƒæ–¹å¼çš„é…ç½®æ–‡ä»¶ï¼Œä¸»è¦å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š\n- output_dirï¼šè®­ç»ƒè¾“å‡ºç›®å½•ï¼Œå­˜å‚¨checkpointã€tokenizerã€tensorboardç­‰\n- model_name_or_pathï¼šé¢„è®­ç»ƒæ¨¡å‹çš„æœ¬åœ°ç›®å½•ï¼Œæˆ–è€…åœ¨huggingfaceä¸Šçš„æ¨¡å‹åç§°ã€‚\n- train_fileï¼šè®­ç»ƒæ•°æ®é›†è·¯å¾„ã€‚sftæ—¶ï¼Œéœ€è¦è®¾ç½®ä¸ºæ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨data/dummy_data.jsonlè¿›è¡Œdebugã€‚pretrainæ—¶ï¼Œéœ€è¦è®¾ç½®ä¸ºç›®å½•ã€‚è„šæœ¬ä¼šè‡ªåŠ¨æ‰«æç›®å½•ä¸‹çš„æ‰€æœ‰jsonlæ–‡ä»¶ã€‚\n- template_nameï¼šæŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œä½¿ç”¨çš„æ¨¡æ¿åç§°ã€‚å…·ä½“æœ‰å“ªäº›template_nameï¼Œå¯å‚è€ƒcomponent/template.pyæ–‡ä»¶\n- num_train_epochsï¼šè®­ç»ƒçš„è½®æ¬¡ã€‚å¦‚æœæ•°æ®é‡è¶³å¤Ÿå¤§ï¼Œä¸€èˆ¬å»ºè®®åªè®­ä¸€ä¸ªepochã€‚\n- tokenize_num_workersï¼šé¢„è®­ç»ƒæ—¶ï¼Œtokenizeçš„çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º10ã€‚\n- deepspeedï¼šdeepspeedçš„è®­ç»ƒé…ç½®æ–‡ä»¶ã€‚å…¨é‡å‚æ•°è®­ç»ƒæ—¶ï¼Œå°†é‡‡ç”¨deepspeedï¼Œå…³äºdeepspeedçš„å‚æ•°é…ç½®è¯´æ˜ï¼Œè¯·å‚è€ƒ[deepspeedæ–‡æ¡£](https://hf-mirror.com/docs/transformers/main/en/deepspeed#deepspeed)\n- train_modeï¼šè®­ç»ƒæ¨¡å¼ï¼Œfullã€loraæˆ–qloraï¼Œé»˜è®¤ä¸ºqloraã€‚\n- task_typeï¼šä»»åŠ¡ç±»å‹ï¼Œpretrainã€sftæˆ–dpoï¼Œé»˜è®¤ä¸ºsftã€‚\n- per_device_train_batch_sizeï¼šæ¯å¼ æ˜¾å¡çš„batch sizeã€‚\n- gradient_accumulation_stepsï¼šæ¢¯åº¦ç´¯è®¡æ­¥æ•°ã€‚global batch=num_gpus * per_device_train_batch_size * gradient_accumulation_stepsã€‚\n- gradient_checkpointingï¼šå¦‚æœæ˜¾å­˜æ‰è¥Ÿè§è‚˜ï¼Œå¯ä»¥å¼€å¯ã€‚ä»¥æ—¶é—´æ¢ç©ºé—´ï¼Œæ¨¡å‹ä¸ç¼“å­˜æ¿€æ´»çŠ¶æ€ï¼Œä¼šè¿›è¡Œä¸¤æ¬¡forwardè®¡ç®—ï¼Œä»¥èŠ‚çœæ˜¾å­˜ã€‚\n- learning_rateï¼šå­¦ä¹ ç‡ã€‚å…¨é‡å‚æ•°å¾®è°ƒçš„æ—¶å€™ï¼Œå»ºè®®å°ä¸€äº›ï¼Œ1e-5æˆ–5e-6ã€‚\n- max_seq_lengthï¼šè®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦ã€‚æŒ‰ç…§è‡ªå·±çš„è®¾å¤‡è¿›è¡Œè®¾ç½®ï¼Œè¶Šé•¿éœ€è¦å ç”¨è¶Šå¤šæ˜¾å­˜ã€‚\n- max_prompt_lengthï¼šè¿›è¡Œdpoæ—¶ï¼Œpromptçš„æœ€å¤§é•¿åº¦ã€‚\n- logging_stepsï¼šæ¯éš”å¤šå°‘æ­¥ç»Ÿè®¡ä¸€æ¬¡train lossã€‚\n- save_stepsï¼šæ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€ä¸ªæ¨¡å‹ã€‚\n- save_total_limitï¼šoutput_dirç›®å½•ä¸­æœ€å¤šä¿å­˜å¤šå°‘ä¸ªcheckpointï¼Œè¶…å‡ºåˆ™ä¼šå°†æœ€æ—§çš„åˆ é™¤ã€‚\n- lr_scheduler_typeï¼šå­¦ä¹ ç‡å˜åŒ–ç­–ç•¥ã€‚\n- warmup_stepsï¼šwarm upæ­¥æ•°ã€‚å­¦ä¹ ç‡ç»è¿‡å¤šå°‘æ­¥ï¼Œå¢é•¿åˆ°æŒ‡å®šçš„æ•°å€¼ã€‚\n- optimï¼šä¼˜åŒ–å™¨ã€‚å¦‚æœæ˜¯å…¨é‡å‚æ•°å¾®è°ƒï¼Œå»ºè®®ä½¿ç”¨adamw_hfã€‚\n- seedï¼šéšæœºç§å­ï¼Œç”¨äºå¤ç°å®éªŒç»“æœã€‚\n- fp16ï¼šä½¿ç”¨ä½¿ç”¨fp16æ··åˆç²¾åº¦ã€‚V100å»ºè®®å¼€å¯ã€‚\n- bf16ï¼šä½¿ç”¨ä½¿ç”¨bf16æ··åˆç²¾åº¦ã€‚A100å»ºè®®å¼€å¯ã€‚\n- use_unslothï¼šæ˜¯å¦ä½¿ç”¨unslothï¼Œç›®å‰unslothä»…æ”¯æŒéƒ¨åˆ†æ¨¡å‹ï¼Œä¾‹å¦‚Llama3ã€Mistralã€Gemmaã€TinyLlamaç­‰ï¼Œè¯¦æƒ…è§[Unsloth](https://github.com/unslothai/unsloth)ã€‚\n\nä»¥ä¸‹å‡ ä¸ªå‚æ•°ï¼Œå½“ä½¿ç”¨QLoRAè®­ç»ƒçš„æ—¶å€™ï¼Œéœ€è¦è®¾ç½®ï¼š\n- lora_rankï¼šqloraçŸ©é˜µçš„ç§©ã€‚ä¸€èˆ¬è®¾ç½®ä¸º8ã€16ã€32ã€64ç­‰ï¼Œåœ¨qloraè®ºæ–‡ä¸­ä½œè€…è®¾ä¸º64ã€‚è¶Šå¤§åˆ™å‚ä¸è®­ç»ƒçš„å‚æ•°é‡è¶Šå¤§ï¼Œä¸€èˆ¬æ¥è¯´æ•ˆæœä¼šæ›´å¥½ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜ï¼Œã€‚\n- lora_alpha: qloraä¸­çš„ç¼©æ”¾å‚æ•°ã€‚ä¸€èˆ¬è®¾ä¸º16ã€32å³å¯ã€‚\n- lora_dropout: loraæƒé‡çš„dropout rateã€‚\n\nå…³äºdeepspeedçš„å‚æ•°é…ç½®ï¼Œå¯æŒ‰éœ€è‡ªè¡Œä¿®æ”¹ã€‚\n\n\n### å¼€å§‹è®­ç»ƒ\n\nğŸ’» å…¨é‡å‚æ•°é¢„è®­ç»ƒï¼Œå°†{num_gpus}æ›¿æ¢ä¸ºæ˜¾å¡æ•°é‡ï¼š\n```bash\ndeepspeed --num_gpus={num_gpus} train.py --train_args_file train_args/pretrain/full/bloom-1b1-pretrain-full.json\n```\n\nğŸ’» å…¨é‡å‚æ•°æŒ‡ä»¤å¾®è°ƒï¼Œå°†{num_gpus}æ›¿æ¢ä¸ºæ˜¾å¡æ•°é‡ï¼š\n```bash\ndeepspeed --num_gpus={num_gpus} train.py --train_args_file train_args/sft/full/bloom-1b1-sft-full.json\n```\n\nğŸ’» å•å¡QLoRAé¢„è®­ç»ƒï¼š\n```bash\npython train.py --train_args_file train_args/pretrain/qlora/yi-6b-pretrain-qlora.json\n```\n\nğŸ’» å•å¡QLoRAæŒ‡ä»¤å¾®è°ƒï¼š\n```bash\npython train.py --train_args_file train_args/sft/qlora/yi-6b-sft-qlora.json\n```\n\nğŸ’» å¤šå¡QLoRAé¢„è®­ç»ƒï¼š\n```bash\ntorchrun --nproc_per_node={num_gpus} train.py --train_args_file train_args/pretrain/qlora/yi-6b-pretrain-qlora.json\n```\n\nğŸ’» å¤šå¡QLoRAæŒ‡ä»¤å¾®è°ƒï¼š\n```bash\ntorchrun --nproc_per_node={num_gpus} train.py --train_args_file train_args/sft/qlora/yi-6b-sft-qlora.json\n```\n\nğŸ’» å•å¡QLoRAè¿›è¡ŒDPOè®­ç»ƒï¼š\n```bash\npython train.py --train_args_file train_args/sft/qlora/minicpm-2b-dpo-qlora.json\n```\n\n## æ¨¡å‹ä½¿ç”¨\n\n### æƒé‡åˆå¹¶\nå¦‚æœä½¿ç”¨LoRAæˆ–è€…QLoRAè¿›è¡Œè®­ç»ƒï¼Œæœ¬é¡¹ç›®ä»…ä¿å­˜adapterçš„æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œéœ€è¦å°†adapteræƒé‡ä¸base modelè¿›è¡Œåˆå¹¶ã€‚è„šæœ¬è§script/merge_lora.py\n\n### æ¨¡å‹æ¨ç†\næˆ‘ä»¬æä¾›äº†å¤šè½®å¯¹è¯çš„äº¤äº’è„šæœ¬ï¼Œè¯¦è§script/chatç›®å½•ï¼Œè¯¥è„šæœ¬å¯åŒæ—¶å…¼å®¹æœ¬é¡¹ç›®è®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚è„šæœ¬ä¸­è®¾ç½®çš„template_nameï¼Œéœ€è¦ä¸æ¨¡å‹è®­ç»ƒæ—¶çš„template_nameä¸€è‡´ã€‚\n```bash\ncd script/chat\npython chat.py\n```\n\nç”Ÿæˆè„šæœ¬ä¸­çš„top_pã€temperatureã€repetition_penaltyã€do_sampleç­‰å‚æ•°å¯¹æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœå½±å“è¾ƒå¤§ï¼Œå¯æŒ‰ç…§è‡ªå·±çš„ä½¿ç”¨åœºæ™¯è¿›è¡Œè°ƒè¯•ä¿®æ”¹ã€‚\n\næ¨ç†è„šæœ¬ä¸­æ”¯æŒä½¿ç”¨base modelå’Œadapterè¿›è¡Œæ¨ç†ï¼Œç¼ºç‚¹æ˜¯æ¯æ¬¡å¯åŠ¨è„šæœ¬éƒ½éœ€è¦åˆå¹¶ä¸€æ¬¡æƒé‡ï¼Œç­‰å¾…æ—¶é—´è¾ƒä¹…ã€‚\n\næ”¯æŒä½¿ç”¨4bitè¿›è¡Œæ¨ç†ï¼Œæ˜¾å­˜è¦æ±‚ä½ï¼Œæ•ˆæœä¼šç•¥æœ‰ä¸‹é™ã€‚\n\n\n## FAQ\n#### é—®é¢˜1ï¼šOOMå¦‚ä½•è§£å†³ï¼Ÿ\nå¦‚æœå‘ç”ŸOOMï¼Œå¯ä»¥ç¼©å°per_device_train_batch_sizeã€max_seq_lengthç­‰å‚æ•°æ¥ç¼“è§£ã€‚ä¹Ÿå¯ä»¥è®¾gradient_checkpointing=trueï¼Œå¯ä»¥å¤§å¹…é™ä½æ˜¾å­˜å ç”¨ï¼Œä½†è®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢ä¸€äº›ã€‚\n\n#### é—®é¢˜2ï¼šå®‰è£…åŒ…é”™è¯¯\nrequirements.txtä¸­æœ‰å„pythonåŒ…çš„ç‰ˆæœ¬\n```bash\npip install -r requirements.txt\n```\n\n#### é—®é¢˜3ï¼šå¦‚ä½•æŒ‡å®šä½¿ç”¨æŸäº›å¡è®­ç»ƒï¼Ÿ\né€šè¿‡å¦‚ä¸‹æ–¹å¼ï¼Œå³å¯æŒ‡å®šä½¿ç”¨0å’Œ1å·å¡è¿›è¡Œè®­ç»ƒ:\n```bash\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node={num_gpus} train_qlora.py --train_args_file train_args/qlora/baichuan-7b-sft-qlora.json\n```\n\n#### é—®é¢˜4ï¼šè®­ç»ƒBaichuan2å¤±è´¥\nè®­ç»ƒBaichuan2éœ€è¦å®‰è£…torch==2.0ï¼Œå¹¶ä¸”å¸è½½xformerså’Œapexï¼Œå¦åˆ™ä¼šæŠ¥é”™\n```\nRuntimeError: No such operator xformers::efficient_attention_forward_generic - did you forget to build xformers with `python setup.py develop`?\n```\n\n#### é—®é¢˜5ï¼šè®­ç»ƒQwenå¤±è´¥\nQwenè¿›è¡ŒQLoRAè®­ç»ƒéœ€è¦å¸è½½flash-attnï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼š\n```\nassert all((i.dtype in [torch.float16, torch.bfloat16] for i in (q, k, v))) \n```\n\n#### é—®é¢˜6ï¼šQwen-Baseå’ŒYi-Baseç»è¿‡SFTä¹‹åï¼Œæ²¡æ³•ç”Ÿæˆ<|im_end|>ï¼Œæ— æ³•æ­£å¸¸åœæ­¢\nç»æŸ¥è¯¢ï¼Œè¯¥é—®é¢˜å¹¿æ³›å­˜åœ¨äºQwenå®˜æ–¹ä»£ç åº“çš„issueä¸­ï¼Œå¦‚æœè®­ç»ƒQwen-Baseå’ŒYi-Baseï¼Œå»ºè®®è®¾template_name=\"default\"ï¼Œå¯ä»¥é¿å…è¯¥é—®é¢˜ã€‚\nå¦‚æœå¯¹Qwen-Chatå’ŒYi-Chatæ¨¡å‹è¿›è¡ŒSFTï¼Œåˆ™ä¸ä¼šäº§ç”Ÿè¯¥é—®é¢˜ï¼Œå¯å°†template_nameåˆ†åˆ«è®¾ä¸º\"qwen\"å’Œ\"yi\"ã€‚\n\næ³¨æ„ï¼šè¯¥é—®é¢˜åœ¨Qwen1.5ä¸­ä¸å­˜åœ¨\n\n\n## å±€é™æ€§å’Œä½¿ç”¨é™åˆ¶\nç”±äºæ¨¡å‹å‚æ•°é‡é™åˆ¶ã€è®­ç»ƒæ•°æ®çš„æ¸…æ´—ç¨‹åº¦ç­‰å› ç´ ï¼Œæœ¬é¡¹ç›®å¼€æºçš„æ¨¡å‹å¯èƒ½å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š\n- å¯¹äºäº‹å®æ€§çŸ¥è¯†ï¼Œå®¹æ˜“äº§ç”Ÿé”™è¯¯çš„å›å¤ã€‚\n- ç”±äºæœªç»è¿‡æ— å®³åŒ–å¾®è°ƒï¼Œå¯èƒ½ä¼šäº§ç”Ÿæ­§è§†ã€å±å®³ã€è¿èƒŒä¼¦ç†é“å¾·çš„è¨€è®ºã€‚\n- åœ¨ä»£ç ã€æ¨ç†ä¸Šçš„èƒ½åŠ›ä»æœ‰æ¬ ç¼ºã€‚\n\nåŸºäºä»¥ä¸Šæ¨¡å‹çš„å±€é™æ€§ï¼Œæˆ‘ä»¬è¦æ±‚æœ¬é¡¹ç›®çš„ä»£ç ã€æ•°æ®ã€æ¨¡å‹ä¸å¾—ç”¨äºå¯¹ç¤¾ä¼šé€ æˆå±å®³çš„ç”¨é€”ï¼Œä¸”åº”å½“éµå¾ªåŸºåº§æ¨¡å‹çš„å•†ä¸šè®¸å¯ã€‚\n\n## å¼•ç”¨\nè‹¥ä½¿ç”¨æœ¬é¡¹ç›®çš„æ•°æ®ã€ä»£ç æˆ–æ¨¡å‹ï¼Œè¯·å¼•ç”¨æœ¬é¡¹ç›®ã€‚\n```text\n@misc{Firefly,\n  author = {Jianxin Yang},\n  title = {Firefly(æµè¤): ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/yangjianxin1/Firefly}},\n}\n```\n\n[//]: # (## å…³æ³¨æˆ‘ä»¬)\n\n[//]: # ()\n[//]: # (<img src=\"pics/gongzhonghao.jpeg\" width=\"250\"> )\n\n## Star History\n![Star History Chart](https://api.star-history.com/svg?repos=yangjianxin1/Firefly&type=Date)\n\n\n\n\n"
        },
        {
          "name": "component",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.33203125,
          "content": "accelerate==0.21.0\ntransformers==4.34\npeft==0.4.0\nbitsandbytes==0.39.0\nloguru==0.7.0\nnumpy==1.21.4\npandas==1.2.5\ntqdm==4.62.3\ndeepspeed==0.9.5\ntensorboard\nsentencepiece\ntransformers_stream_generator\ntiktoken\neinops\nhttpx\nscipy\ntorch==1.13.1\nmmengine\nxformers\nastunparse==1.6.2\nflash_attn\ndatasets\ntrl==0.7.11\ntyping_extensions==4.9.0\nmpi4py"
        },
        {
          "name": "script",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 17.5546875,
          "content": "import argparse\nfrom loguru import logger\nimport os\nfrom os.path import join\nimport torch\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport bitsandbytes as bnb\nfrom component.collator import PretrainCollator, SFTDataCollator\nfrom component.argument import CustomizedArguments\nfrom component.template import template_dict\nfrom component.dataset import (\n    UnifiedSFTDataset,\n    ChatGLM2SFTDataset,\n    ChatGLM3SFTDataset,\n    UnifiedDPODataset\n)\nfrom transformers import (\n    set_seed,\n    HfArgumentParser,\n    TrainingArguments,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    AutoConfig,\n    BitsAndBytesConfig,\n    Trainer,\n    AddedToken\n)\nimport importlib\nif importlib.util.find_spec('unsloth') is not None:\n    from unsloth import FastLanguageModel\nfrom datasets import load_dataset, concatenate_datasets\nimport datasets\nfrom itertools import chain\nfrom tqdm import tqdm\nimport json\nfrom trl import DPOTrainer, get_kbit_device_map\nimport torch.nn as nn\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n\ndef setup_everything():\n    parser = argparse.ArgumentParser()\n    # parser.add_argument(\"--train_args_file\", type=str, default='train_args/pretrain/full/bloom-1b1-pretrain-full.json', help=\"\")\n    parser.add_argument(\"--train_args_file\", type=str, default='train_args/sft/qlora/qwen-7b-sft-qlora.json', help=\"\")\n    parser.add_argument(\"--local_rank\", type=int, help=\"\")\n    args = parser.parse_args()\n    train_args_file = args.train_args_file\n    # è¯»å–è®­ç»ƒçš„å‚æ•°é…ç½®\n    parser = HfArgumentParser((CustomizedArguments, TrainingArguments))\n    # è§£æå¾—åˆ°è‡ªå®šä¹‰å‚æ•°ï¼Œä»¥åŠè‡ªå¸¦å‚æ•°\n    args, training_args = parser.parse_json_file(json_file=train_args_file)\n    # åˆ›å»ºè¾“å‡ºç›®å½•\n    if not os.path.exists(training_args.output_dir):\n        os.makedirs(training_args.output_dir)\n    logger.add(join(training_args.output_dir, 'train.log'))\n    logger.info(\"train_args:{}\".format(training_args))\n    # åŠ è½½è®­ç»ƒé…ç½®æ–‡ä»¶\n    with open(train_args_file, \"r\") as f:\n        train_args = json.load(f)\n    # ä¿å­˜è®­ç»ƒå‚æ•°åˆ°è¾“å‡ºç›®å½•\n    with open(join(training_args.output_dir, 'train_args.json'), \"w\") as f:\n        json.dump(train_args, f, indent=4)\n    # è®¾ç½®éšæœºç§å­\n    set_seed(training_args.seed)\n\n    # check some setting\n    assert args.task_type in ['pretrain', 'sft', 'dpo'], \"task_type should be in ['pretrain', 'sft', 'dpo']\"\n    assert args.train_mode in ['full', 'lora', 'qlora'], \"task_type should be in ['full', 'lora', 'qlora']\"\n    assert sum([training_args.fp16, training_args.bf16]) == 1, \"only one of fp16 and bf16 can be True\"\n    # assert not (args.task_type == 'dpo' and args.use_unsloth), 'We have not tested Unsloth during DPO yet. Please set use_unsloth=False when task_type=dpo'\n\n    return args, training_args\n\n\ndef find_all_linear_names(model, train_mode):\n    \"\"\"\n    æ‰¾å‡ºæ‰€æœ‰å…¨è¿æ¥å±‚ï¼Œä¸ºæ‰€æœ‰å…¨è¿æ¥æ·»åŠ adapter\n    \"\"\"\n    assert train_mode in ['lora', 'qlora']\n    cls = bnb.nn.Linear4bit if train_mode == 'qlora' else nn.Linear\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    lora_module_names = list(lora_module_names)\n    logger.info(f'LoRA target module names: {lora_module_names}')\n    return lora_module_names\n\n\ndef load_pretrain_dataset(training_args, args, tokenizer):\n    \"\"\"\n    å¤šçº¿ç¨‹é¢„å¤„ç†é¢„è®­ç»ƒæ•°æ®\n    \"\"\"\n    def tokenize_function(examples):\n        output = tokenizer(examples[\"text\"])\n        output = {'input_ids': output.input_ids}\n        return output\n\n    def group_texts(examples):\n        # Concatenate all texts.\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n        if total_length >= max_seq_length:\n            total_length = (total_length // max_seq_length) * max_seq_length\n        # Split by chunks of max_len.\n        result = {\n            k: [t[i: i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n            for k, t in concatenated_examples.items()\n        }\n        return result\n\n    data_path = args.train_file\n    max_seq_length = args.max_seq_length\n    # åˆ›å»ºç¼“å­˜è·¯å¾„\n    cache_dir = join(data_path, 'cache')\n    os.makedirs(cache_dir, exist_ok=True)\n    logger.info('Pretraining data path: {}'.format(data_path))\n\n    # æ‰«ææ‰€æœ‰jsonlæ–‡ä»¶\n    logger.info('Scanning all the training file...')\n    files = []\n    for root, dir_names, file_names in os.walk(data_path):\n        for file_name in file_names:\n            file = join(root, file_name)\n            if file_name.endswith('.jsonl'):\n                files.append(file)\n    logger.info(f'Total num of training file: {len(files)}')\n\n    # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬ï¼Œå°†å…¶idåŒ–ï¼Œå¹¶ä¸”è¿›è¡Œpackingæ“ä½œ\n    with training_args.main_process_first(desc=\"dataset map tokenization and grouping\"):\n        pretrain_dataset = []  # æ±‡æ€»æ‰€æœ‰dataset\n        for idx, file in enumerate(tqdm(files)):\n            logger.info(f'Loading file: {file}')\n            file_name = os.path.basename(file)\n            file_name = file_name.replace('.jsonl', '')\n            cache_path = os.path.join(cache_dir, file_name)\n            os.makedirs(cache_path, exist_ok=True)\n\n            try:\n                processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=False)\n                logger.info(f'Finished loading datasets-{file_name} from cache')\n            except Exception:\n                tmp_cache_path = join(cache_path, 'tmp')    # ä¸´æ—¶ç¼“å­˜ç›®å½•ï¼Œä¼šè¢«è‡ªåŠ¨åˆ é™¤\n                logger.info(f'There is no cache of file {file_name}, start preprocessing...')\n                raw_dataset = load_dataset(\"json\", data_files=file, cache_dir=tmp_cache_path, keep_in_memory=False)\n                tokenized_dataset = raw_dataset.map(\n                    tokenize_function,\n                    batched=True,\n                    num_proc=args.tokenize_num_workers,\n                    remove_columns=\"text\",\n                    load_from_cache_file=True,\n                    keep_in_memory=False,\n                    cache_file_names={k: os.path.join(tmp_cache_path, 'tokenized.arrow') for k in raw_dataset},\n                    desc=\"Running tokenizer on dataset\",\n                )\n                grouped_datasets = tokenized_dataset.map(\n                    group_texts,\n                    batched=True,\n                    num_proc=args.tokenize_num_workers,\n                    load_from_cache_file=True,\n                    keep_in_memory=False,\n                    cache_file_names={k: os.path.join(tmp_cache_path, 'grouped.arrow') for k in tokenized_dataset},\n                    desc=f\"Grouping texts in chunks of {max_seq_length}\",\n                )\n                processed_dataset = grouped_datasets\n                processed_dataset.save_to_disk(cache_path)\n                # åˆ é™¤ä¸´æ—¶ç›®å½•\n                # shutil.rmtree(tmp_cache_path)\n\n            logger.info(f\"Training number of {file_name}: {len(processed_dataset['train'])}\")\n            if idx == 0:\n                pretrain_dataset = processed_dataset['train']\n            else:\n                assert pretrain_dataset.features.type == processed_dataset[\"train\"].features.type\n                pretrain_dataset = concatenate_datasets([pretrain_dataset, processed_dataset[\"train\"]])\n    logger.info(f\"Total training number: {len(pretrain_dataset)}\")\n    return pretrain_dataset\n\n\ndef load_tokenizer(args):\n    config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=True)\n    # åŠ è½½tokenzier\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=True,\n        # llamaä¸æ”¯æŒfast\n        use_fast=False if config.model_type == 'llama' or config.model_type == 'internlm2' else True\n    )\n\n    # éƒ¨åˆ†æ¨¡å‹çš„baseä¸chatç‰ˆæœ¬çš„tokenizerå­˜åœ¨å·®å¼‚\n    if 'internlm2' in args.model_name_or_path.lower():\n        tokenizer._added_tokens_encoder.update({'<|im_start|>': 92543})\n        tokenizer._added_tokens_encoder.update({'<|im_end|>': 92542})\n        tokenizer._added_tokens_decoder.update({92543: AddedToken('<|im_start|>')})\n        tokenizer._added_tokens_decoder.update({92542: AddedToken('<|im_end|>')})\n        tokenizer.add_special_tokens({'additional_special_tokens': ['<|im_start|>', '<|im_end|>']})\n    elif 'orion' in args.model_name_or_path.lower():\n        tokenizer.add_special_tokens({'bos_token': '<s>', 'eos_token': '</s>'})\n    elif 'gemma' in args.model_name_or_path.lower():\n        tokenizer.add_special_tokens({'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']})\n\n    if tokenizer.__class__.__name__ == 'QWenTokenizer':\n        tokenizer.pad_token_id = tokenizer.eod_id\n        tokenizer.bos_token_id = tokenizer.eod_id\n        tokenizer.eos_token_id = tokenizer.eod_id\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    assert tokenizer.pad_token_id is not None, \"pad_token_id should not be None\"\n    assert tokenizer.eos_token_id is not None, \"eos_token_id should not be None\"\n    logger.info(f'vocab_size of tokenizer: {tokenizer.vocab_size}')\n    return tokenizer\n\n\ndef load_unsloth_model(args, training_args):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=args.model_name_or_path,\n        max_seq_length=args.max_seq_length,\n        dtype=None,\n        trust_remote_code=True,\n        load_in_4bit=True if args.train_mode == 'qlora' else False,\n    )\n    if args.train_mode in ['lora', 'qlora']:\n        logger.info('Initializing PEFT Model...')\n        target_modules = find_all_linear_names(model, args.train_mode)\n        model = FastLanguageModel.get_peft_model(\n            model,\n            r=args.lora_rank,\n            target_modules=target_modules,\n            lora_alpha=args.lora_alpha,\n            lora_dropout=args.lora_dropout,\n            bias=\"none\",\n            use_gradient_checkpointing=True,\n            random_state=training_args.seed,\n            max_seq_length=args.max_seq_length,\n        )\n        logger.info(f'target_modules: {target_modules}')\n    return {\n        'model': model,\n        'ref_model': None,\n        'peft_config': None\n    }\n\n\ndef load_model(args, training_args):\n    \"\"\"\n    åŠ è½½æ¨¡å‹\n    \"\"\"\n    assert training_args.bf16 or training_args.fp16, 'bf16 or fp16 should be True'\n    logger.info(f'Loading model from base model: {args.model_name_or_path}')\n    logger.info(f'Train model with {args.train_mode}')\n\n    # init model kwargs\n    # todo add flash attention\n    # attn_implementation = None\n    torch_dtype = torch.float16 if training_args.fp16 else torch.bfloat16\n    if args.train_mode == 'qlora':\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16 if training_args.fp16 else torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n        )\n    else:\n        quantization_config = None\n    model_kwargs = dict(\n        trust_remote_code=True,\n        # attn_implementation=attn_implementation,\n        torch_dtype=torch_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,\n        device_map=get_kbit_device_map() if quantization_config is not None else None,\n        quantization_config=quantization_config,\n    )\n    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs)\n\n    # moeæ¨¡å‹ï¼Œéœ€è¦è€ƒè™‘è´Ÿè½½å‡è¡¡çš„loss\n    if 'output_router_logits' in model.config.to_dict():\n        logger.info('set output_router_logits as True')\n        model.config.output_router_logits = True\n    # QLoRA: casts all the non int8 modules to full precision (fp32) for stability\n    if args.train_mode == 'qlora' and args.task_type in ['pretrain', 'sft']:\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n    # LoRA: Enables the gradients for the input embeddings\n    if args.train_mode == 'lora' and args.task_type in ['pretrain', 'sft']:\n        # For backward compatibility\n        if hasattr(model, \"enable_input_require_grads\"):\n            model.enable_input_require_grads()\n        else:\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n    # init peft_config\n    if args.train_mode == 'full':\n        peft_config = None\n    else:\n        # æ‰¾åˆ°æ‰€æœ‰éœ€è¦æ’å…¥adapterçš„å…¨è¿æ¥å±‚\n        target_modules = find_all_linear_names(model, args.train_mode)\n        peft_config = LoraConfig(\n            r=args.lora_rank,\n            lora_alpha=args.lora_alpha,\n            target_modules=target_modules,\n            lora_dropout=args.lora_dropout,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n    # init peft model\n    if args.train_mode in ['lora', 'qlora'] and args.task_type in ['pretrain', 'sft']:\n        model = get_peft_model(model, peft_config)\n        logger.info(f'memory footprint of model: {model.get_memory_footprint() / (1024 * 1024 * 1024)} GB')\n        model.print_trainable_parameters()\n\n    # init ref_model\n    if args.task_type == 'dpo':\n        ref_model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs) if args.train_mode == 'full' else None\n    # pretrainå’Œsftï¼Œä¸éœ€è¦ref_model\n    else:\n        ref_model = None\n\n    # è®¡ç®—æ¨¡å‹å‚æ•°é‡\n    total = sum(p.numel() for p in model.parameters())\n    logger.info(\"Total model params: %.2fM\" % (total / 1e6))\n\n    return {\n        'model': model,\n        'ref_model': ref_model,\n        'peft_config': peft_config\n    }\n\n\ndef load_sft_dataset(args, tokenizer):\n    if args.template_name not in template_dict.keys():\n        raise Exception(f\"template_name doesn't exist, all template_name: {template_dict.keys()}\")\n    template = template_dict[args.template_name]\n    if 'chatglm2' in args.model_name_or_path.lower():\n        logger.info('Loading data with ChatGLM2SFTDataset')\n        train_dataset = ChatGLM2SFTDataset(args.train_file, tokenizer, args.max_seq_length, template)\n    elif 'chatglm3' in args.model_name_or_path.lower():\n        logger.info('Loading data with ChatGLM3SFTDataset')\n        train_dataset = ChatGLM3SFTDataset(args.train_file, tokenizer, args.max_seq_length, template)\n    else:\n        logger.info('Loading data with UnifiedSFTDataset')\n        train_dataset = UnifiedSFTDataset(args.train_file, tokenizer, args.max_seq_length, template)\n    return train_dataset\n\n\ndef load_dpo_dataset(args, tokenizer):\n    if args.template_name not in template_dict.keys():\n        raise Exception(f\"template_name doesn't exist, all template_name: {template_dict.keys()}\")\n    template = template_dict[args.template_name]\n    train_dataset = UnifiedDPODataset(args.train_file, tokenizer, args.max_seq_length, args.max_prompt_length, template)\n    return train_dataset\n\n\ndef init_components(args, training_args):\n    \"\"\"\n    åˆå§‹åŒ–å„ä¸ªç»„ä»¶\n    \"\"\"\n    training_args.ddp_find_unused_parameters = False\n    logger.info('Initializing components...')\n\n    # åŠ è½½tokenizer\n    tokenizer = load_tokenizer(args)\n    # åŠ è½½model\n    if args.use_unsloth:\n        components = load_unsloth_model(args, training_args)\n    else:\n        components = load_model(args, training_args)\n    model = components['model']\n    ref_model = components['ref_model']\n    peft_config = components['peft_config']\n\n    # åˆå§‹åŒ–datasetå’Œcollator\n    if args.task_type == 'pretrain':\n        logger.info('Train model with pretrain task')\n        train_dataset = load_pretrain_dataset(training_args, args, tokenizer)\n        data_collator = PretrainCollator(tokenizer, args.max_seq_length)\n    elif args.task_type == 'sft':\n        logger.info('Train model with sft task')\n        train_dataset = load_sft_dataset(args, tokenizer)\n        data_collator = SFTDataCollator(tokenizer, args.max_seq_length)\n    else:\n        logger.info('Train model with dpo task')\n        train_dataset = load_dpo_dataset(args, tokenizer)\n        data_collator = None\n\n    # dpo\n    if args.task_type == 'dpo':\n        trainer = DPOTrainer(\n            model,\n            ref_model,\n            args=training_args,\n            beta=args.beta,\n            train_dataset=train_dataset,\n            data_collator=data_collator,\n            tokenizer=tokenizer,\n            peft_config=peft_config\n        )\n    # pretrain or sft\n    else:\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n        )\n    return trainer\n\n\ndef main():\n    # è¿›è¡Œä¸€äº›é…ç½®å’Œæ£€æŸ¥\n    args, training_args = setup_everything()\n    # åŠ è½½å„ç§ç»„ä»¶\n    trainer = init_components(args, training_args)\n    # å¼€å§‹è®­ç»ƒ\n    logger.info(\"*** starting training ***\")\n    train_result = trainer.train()\n    # ä¿å­˜æœ€å¥½çš„checkpoint\n    final_save_path = join(training_args.output_dir)\n    trainer.save_model(final_save_path)  # Saves the tokenizer too\n    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡\n    metrics = train_result.metrics\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "train_args",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}