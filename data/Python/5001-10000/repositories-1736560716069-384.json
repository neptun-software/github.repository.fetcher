{
  "metadata": {
    "timestamp": 1736560716069,
    "page": 384,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jaywalnut310/vits",
      "stars": 7020,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1044921875,
          "content": "DUMMY1\nDUMMY2\nDUMMY3\nlogs\n__pycache__\n.ipynb_checkpoints\n.*.swp\n\nbuild\n*.c\nmonotonic_align/monotonic_align\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2021 Jaehyeon Kim\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.578125,
          "content": "# VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\n### Jaehyeon Kim, Jungil Kong, and Juhee Son\n\nIn our recent [paper](https://arxiv.org/abs/2106.06103), we propose VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\n\nSeveral recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.\n\nVisit our [demo](https://jaywalnut310.github.io/vits-demo/index.html) for audio samples.\n\nWe also provide the [pretrained models](https://drive.google.com/drive/folders/1ksarh-cJf3F5eKJjLVWY0X1j1qsQqiS2?usp=sharing).\n\n** Update note: Thanks to [Rishikesh (ऋषिकेश)](https://github.com/jaywalnut310/vits/issues/1), our interactive TTS demo is now available on [Colab Notebook](https://colab.research.google.com/drive/1CO61pZizDj7en71NQG_aqqKdGaA_SaBf?usp=sharing).\n\n<table style=\"width:100%\">\n  <tr>\n    <th>VITS at training</th>\n    <th>VITS at inference</th>\n  </tr>\n  <tr>\n    <td><img src=\"resources/fig_1a.png\" alt=\"VITS at training\" height=\"400\"></td>\n    <td><img src=\"resources/fig_1b.png\" alt=\"VITS at inference\" height=\"400\"></td>\n  </tr>\n</table>\n\n\n## Pre-requisites\n0. Python >= 3.6\n0. Clone this repository\n0. Install python requirements. Please refer [requirements.txt](requirements.txt)\n    1. You may need to install espeak first: `apt-get install espeak`\n0. Download datasets\n    1. Download and extract the LJ Speech dataset, then rename or create a link to the dataset folder: `ln -s /path/to/LJSpeech-1.1/wavs DUMMY1`\n    1. For mult-speaker setting, download and extract the VCTK dataset, and downsample wav files to 22050 Hz. Then rename or create a link to the dataset folder: `ln -s /path/to/VCTK-Corpus/downsampled_wavs DUMMY2`\n0. Build Monotonic Alignment Search and run preprocessing if you use your own datasets.\n```sh\n# Cython-version Monotonoic Alignment Search\ncd monotonic_align\npython setup.py build_ext --inplace\n\n# Preprocessing (g2p) for your own datasets. Preprocessed phonemes for LJ Speech and VCTK have been already provided.\n# python preprocess.py --text_index 1 --filelists filelists/ljs_audio_text_train_filelist.txt filelists/ljs_audio_text_val_filelist.txt filelists/ljs_audio_text_test_filelist.txt \n# python preprocess.py --text_index 2 --filelists filelists/vctk_audio_sid_text_train_filelist.txt filelists/vctk_audio_sid_text_val_filelist.txt filelists/vctk_audio_sid_text_test_filelist.txt\n```\n\n\n## Training Exmaple\n```sh\n# LJ Speech\npython train.py -c configs/ljs_base.json -m ljs_base\n\n# VCTK\npython train_ms.py -c configs/vctk_base.json -m vctk_base\n```\n\n\n## Inference Example\nSee [inference.ipynb](inference.ipynb)\n"
        },
        {
          "name": "attentions.py",
          "type": "blob",
          "size": 11.50390625,
          "content": "import copy\nimport math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport commons\nimport modules\nfrom modules import LayerNorm\n   \n\nclass Encoder(nn.Module):\n  def __init__(self, hidden_channels, filter_channels, n_heads, n_layers, kernel_size=1, p_dropout=0., window_size=4, **kwargs):\n    super().__init__()\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.window_size = window_size\n\n    self.drop = nn.Dropout(p_dropout)\n    self.attn_layers = nn.ModuleList()\n    self.norm_layers_1 = nn.ModuleList()\n    self.ffn_layers = nn.ModuleList()\n    self.norm_layers_2 = nn.ModuleList()\n    for i in range(self.n_layers):\n      self.attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout, window_size=window_size))\n      self.norm_layers_1.append(LayerNorm(hidden_channels))\n      self.ffn_layers.append(FFN(hidden_channels, hidden_channels, filter_channels, kernel_size, p_dropout=p_dropout))\n      self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n  def forward(self, x, x_mask):\n    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n    x = x * x_mask\n    for i in range(self.n_layers):\n      y = self.attn_layers[i](x, x, attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_1[i](x + y)\n\n      y = self.ffn_layers[i](x, x_mask)\n      y = self.drop(y)\n      x = self.norm_layers_2[i](x + y)\n    x = x * x_mask\n    return x\n\n\nclass Decoder(nn.Module):\n  def __init__(self, hidden_channels, filter_channels, n_heads, n_layers, kernel_size=1, p_dropout=0., proximal_bias=False, proximal_init=True, **kwargs):\n    super().__init__()\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.proximal_bias = proximal_bias\n    self.proximal_init = proximal_init\n\n    self.drop = nn.Dropout(p_dropout)\n    self.self_attn_layers = nn.ModuleList()\n    self.norm_layers_0 = nn.ModuleList()\n    self.encdec_attn_layers = nn.ModuleList()\n    self.norm_layers_1 = nn.ModuleList()\n    self.ffn_layers = nn.ModuleList()\n    self.norm_layers_2 = nn.ModuleList()\n    for i in range(self.n_layers):\n      self.self_attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout, proximal_bias=proximal_bias, proximal_init=proximal_init))\n      self.norm_layers_0.append(LayerNorm(hidden_channels))\n      self.encdec_attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout))\n      self.norm_layers_1.append(LayerNorm(hidden_channels))\n      self.ffn_layers.append(FFN(hidden_channels, hidden_channels, filter_channels, kernel_size, p_dropout=p_dropout, causal=True))\n      self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n  def forward(self, x, x_mask, h, h_mask):\n    \"\"\"\n    x: decoder input\n    h: encoder output\n    \"\"\"\n    self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(device=x.device, dtype=x.dtype)\n    encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n    x = x * x_mask\n    for i in range(self.n_layers):\n      y = self.self_attn_layers[i](x, x, self_attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_0[i](x + y)\n\n      y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_1[i](x + y)\n      \n      y = self.ffn_layers[i](x, x_mask)\n      y = self.drop(y)\n      x = self.norm_layers_2[i](x + y)\n    x = x * x_mask\n    return x\n\n\nclass MultiHeadAttention(nn.Module):\n  def __init__(self, channels, out_channels, n_heads, p_dropout=0., window_size=None, heads_share=True, block_length=None, proximal_bias=False, proximal_init=False):\n    super().__init__()\n    assert channels % n_heads == 0\n\n    self.channels = channels\n    self.out_channels = out_channels\n    self.n_heads = n_heads\n    self.p_dropout = p_dropout\n    self.window_size = window_size\n    self.heads_share = heads_share\n    self.block_length = block_length\n    self.proximal_bias = proximal_bias\n    self.proximal_init = proximal_init\n    self.attn = None\n\n    self.k_channels = channels // n_heads\n    self.conv_q = nn.Conv1d(channels, channels, 1)\n    self.conv_k = nn.Conv1d(channels, channels, 1)\n    self.conv_v = nn.Conv1d(channels, channels, 1)\n    self.conv_o = nn.Conv1d(channels, out_channels, 1)\n    self.drop = nn.Dropout(p_dropout)\n\n    if window_size is not None:\n      n_heads_rel = 1 if heads_share else n_heads\n      rel_stddev = self.k_channels**-0.5\n      self.emb_rel_k = nn.Parameter(torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels) * rel_stddev)\n      self.emb_rel_v = nn.Parameter(torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels) * rel_stddev)\n\n    nn.init.xavier_uniform_(self.conv_q.weight)\n    nn.init.xavier_uniform_(self.conv_k.weight)\n    nn.init.xavier_uniform_(self.conv_v.weight)\n    if proximal_init:\n      with torch.no_grad():\n        self.conv_k.weight.copy_(self.conv_q.weight)\n        self.conv_k.bias.copy_(self.conv_q.bias)\n      \n  def forward(self, x, c, attn_mask=None):\n    q = self.conv_q(x)\n    k = self.conv_k(c)\n    v = self.conv_v(c)\n    \n    x, self.attn = self.attention(q, k, v, mask=attn_mask)\n\n    x = self.conv_o(x)\n    return x\n\n  def attention(self, query, key, value, mask=None):\n    # reshape [b, d, t] -> [b, n_h, t, d_k]\n    b, d, t_s, t_t = (*key.size(), query.size(2))\n    query = query.view(b, self.n_heads, self.k_channels, t_t).transpose(2, 3)\n    key = key.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n    value = value.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n\n    scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1))\n    if self.window_size is not None:\n      assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n      key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n      rel_logits = self._matmul_with_relative_keys(query /math.sqrt(self.k_channels), key_relative_embeddings)\n      scores_local = self._relative_position_to_absolute_position(rel_logits)\n      scores = scores + scores_local\n    if self.proximal_bias:\n      assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n      scores = scores + self._attention_bias_proximal(t_s).to(device=scores.device, dtype=scores.dtype)\n    if mask is not None:\n      scores = scores.masked_fill(mask == 0, -1e4)\n      if self.block_length is not None:\n        assert t_s == t_t, \"Local attention is only available for self-attention.\"\n        block_mask = torch.ones_like(scores).triu(-self.block_length).tril(self.block_length)\n        scores = scores.masked_fill(block_mask == 0, -1e4)\n    p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\n    p_attn = self.drop(p_attn)\n    output = torch.matmul(p_attn, value)\n    if self.window_size is not None:\n      relative_weights = self._absolute_position_to_relative_position(p_attn)\n      value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n      output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings)\n    output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\n    return output, p_attn\n\n  def _matmul_with_relative_values(self, x, y):\n    \"\"\"\n    x: [b, h, l, m]\n    y: [h or 1, m, d]\n    ret: [b, h, l, d]\n    \"\"\"\n    ret = torch.matmul(x, y.unsqueeze(0))\n    return ret\n\n  def _matmul_with_relative_keys(self, x, y):\n    \"\"\"\n    x: [b, h, l, d]\n    y: [h or 1, m, d]\n    ret: [b, h, l, m]\n    \"\"\"\n    ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n    return ret\n\n  def _get_relative_embeddings(self, relative_embeddings, length):\n    max_relative_position = 2 * self.window_size + 1\n    # Pad first before slice to avoid using cond ops.\n    pad_length = max(length - (self.window_size + 1), 0)\n    slice_start_position = max((self.window_size + 1) - length, 0)\n    slice_end_position = slice_start_position + 2 * length - 1\n    if pad_length > 0:\n      padded_relative_embeddings = F.pad(\n          relative_embeddings,\n          commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]))\n    else:\n      padded_relative_embeddings = relative_embeddings\n    used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\n    return used_relative_embeddings\n\n  def _relative_position_to_absolute_position(self, x):\n    \"\"\"\n    x: [b, h, l, 2*l-1]\n    ret: [b, h, l, l]\n    \"\"\"\n    batch, heads, length, _ = x.size()\n    # Concat columns of pad to shift from relative to absolute indexing.\n    x = F.pad(x, commons.convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\n\n    # Concat extra elements so to add up to shape (len+1, 2*len-1).\n    x_flat = x.view([batch, heads, length * 2 * length])\n    x_flat = F.pad(x_flat, commons.convert_pad_shape([[0,0],[0,0],[0,length-1]]))\n\n    # Reshape and slice out the padded elements.\n    x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\n    return x_final\n\n  def _absolute_position_to_relative_position(self, x):\n    \"\"\"\n    x: [b, h, l, l]\n    ret: [b, h, l, 2*l-1]\n    \"\"\"\n    batch, heads, length, _ = x.size()\n    # padd along column\n    x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\n    x_flat = x.view([batch, heads, length**2 + length*(length -1)])\n    # add 0's in the beginning that will skew the elements after reshape\n    x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n    x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\n    return x_final\n\n  def _attention_bias_proximal(self, length):\n    \"\"\"Bias for self-attention to encourage attention to close positions.\n    Args:\n      length: an integer scalar.\n    Returns:\n      a Tensor with shape [1, 1, length, length]\n    \"\"\"\n    r = torch.arange(length, dtype=torch.float32)\n    diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n    return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)\n\n\nclass FFN(nn.Module):\n  def __init__(self, in_channels, out_channels, filter_channels, kernel_size, p_dropout=0., activation=None, causal=False):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.activation = activation\n    self.causal = causal\n\n    if causal:\n      self.padding = self._causal_padding\n    else:\n      self.padding = self._same_padding\n\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size)\n    self.conv_2 = nn.Conv1d(filter_channels, out_channels, kernel_size)\n    self.drop = nn.Dropout(p_dropout)\n\n  def forward(self, x, x_mask):\n    x = self.conv_1(self.padding(x * x_mask))\n    if self.activation == \"gelu\":\n      x = x * torch.sigmoid(1.702 * x)\n    else:\n      x = torch.relu(x)\n    x = self.drop(x)\n    x = self.conv_2(self.padding(x * x_mask))\n    return x * x_mask\n  \n  def _causal_padding(self, x):\n    if self.kernel_size == 1:\n      return x\n    pad_l = self.kernel_size - 1\n    pad_r = 0\n    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n    x = F.pad(x, commons.convert_pad_shape(padding))\n    return x\n\n  def _same_padding(self, x):\n    if self.kernel_size == 1:\n      return x\n    pad_l = (self.kernel_size - 1) // 2\n    pad_r = self.kernel_size // 2\n    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n    x = F.pad(x, commons.convert_pad_shape(padding))\n    return x\n"
        },
        {
          "name": "commons.py",
          "type": "blob",
          "size": 4.666015625,
          "content": "import math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ndef init_weights(m, mean=0.0, std=0.01):\n  classname = m.__class__.__name__\n  if classname.find(\"Conv\") != -1:\n    m.weight.data.normal_(mean, std)\n\n\ndef get_padding(kernel_size, dilation=1):\n  return int((kernel_size*dilation - dilation)/2)\n\n\ndef convert_pad_shape(pad_shape):\n  l = pad_shape[::-1]\n  pad_shape = [item for sublist in l for item in sublist]\n  return pad_shape\n\n\ndef intersperse(lst, item):\n  result = [item] * (len(lst) * 2 + 1)\n  result[1::2] = lst\n  return result\n\n\ndef kl_divergence(m_p, logs_p, m_q, logs_q):\n  \"\"\"KL(P||Q)\"\"\"\n  kl = (logs_q - logs_p) - 0.5\n  kl += 0.5 * (torch.exp(2. * logs_p) + ((m_p - m_q)**2)) * torch.exp(-2. * logs_q)\n  return kl\n\n\ndef rand_gumbel(shape):\n  \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n  uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n  return -torch.log(-torch.log(uniform_samples))\n\n\ndef rand_gumbel_like(x):\n  g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n  return g\n\n\ndef slice_segments(x, ids_str, segment_size=4):\n  ret = torch.zeros_like(x[:, :, :segment_size])\n  for i in range(x.size(0)):\n    idx_str = ids_str[i]\n    idx_end = idx_str + segment_size\n    ret[i] = x[i, :, idx_str:idx_end]\n  return ret\n\n\ndef rand_slice_segments(x, x_lengths=None, segment_size=4):\n  b, d, t = x.size()\n  if x_lengths is None:\n    x_lengths = t\n  ids_str_max = x_lengths - segment_size + 1\n  ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n  ret = slice_segments(x, ids_str, segment_size)\n  return ret, ids_str\n\n\ndef get_timing_signal_1d(\n    length, channels, min_timescale=1.0, max_timescale=1.0e4):\n  position = torch.arange(length, dtype=torch.float)\n  num_timescales = channels // 2\n  log_timescale_increment = (\n      math.log(float(max_timescale) / float(min_timescale)) /\n      (num_timescales - 1))\n  inv_timescales = min_timescale * torch.exp(\n      torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment)\n  scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\n  signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\n  signal = F.pad(signal, [0, 0, 0, channels % 2])\n  signal = signal.view(1, channels, length)\n  return signal\n\n\ndef add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n  b, channels, length = x.size()\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n  return x + signal.to(dtype=x.dtype, device=x.device)\n\n\ndef cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n  b, channels, length = x.size()\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n  return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\n\n\ndef subsequent_mask(length):\n  mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n  return mask\n\n\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n  n_channels_int = n_channels[0]\n  in_act = input_a + input_b\n  t_act = torch.tanh(in_act[:, :n_channels_int, :])\n  s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n  acts = t_act * s_act\n  return acts\n\n\ndef convert_pad_shape(pad_shape):\n  l = pad_shape[::-1]\n  pad_shape = [item for sublist in l for item in sublist]\n  return pad_shape\n\n\ndef shift_1d(x):\n  x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n  return x\n\n\ndef sequence_mask(length, max_length=None):\n  if max_length is None:\n    max_length = length.max()\n  x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n  return x.unsqueeze(0) < length.unsqueeze(1)\n\n\ndef generate_path(duration, mask):\n  \"\"\"\n  duration: [b, 1, t_x]\n  mask: [b, 1, t_y, t_x]\n  \"\"\"\n  device = duration.device\n  \n  b, _, t_y, t_x = mask.shape\n  cum_duration = torch.cumsum(duration, -1)\n  \n  cum_duration_flat = cum_duration.view(b * t_x)\n  path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n  path = path.view(b, t_x, t_y)\n  path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n  path = path.unsqueeze(1).transpose(2,3) * mask\n  return path\n\n\ndef clip_grad_value_(parameters, clip_value, norm_type=2):\n  if isinstance(parameters, torch.Tensor):\n    parameters = [parameters]\n  parameters = list(filter(lambda p: p.grad is not None, parameters))\n  norm_type = float(norm_type)\n  if clip_value is not None:\n    clip_value = float(clip_value)\n\n  total_norm = 0\n  for p in parameters:\n    param_norm = p.grad.data.norm(norm_type)\n    total_norm += param_norm.item() ** norm_type\n    if clip_value is not None:\n      p.grad.data.clamp_(min=-clip_value, max=clip_value)\n  total_norm = total_norm ** (1. / norm_type)\n  return total_norm\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data_utils.py",
          "type": "blob",
          "size": 14.6748046875,
          "content": "import time\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.utils.data\n\nimport commons \nfrom mel_processing import spectrogram_torch\nfrom utils import load_wav_to_torch, load_filepaths_and_text\nfrom text import text_to_sequence, cleaned_text_to_sequence\n\n\nclass TextAudioLoader(torch.utils.data.Dataset):\n    \"\"\"\n        1) loads audio, text pairs\n        2) normalizes text and converts them to sequences of integers\n        3) computes spectrograms from audio files.\n    \"\"\"\n    def __init__(self, audiopaths_and_text, hparams):\n        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n        self.text_cleaners  = hparams.text_cleaners\n        self.max_wav_value  = hparams.max_wav_value\n        self.sampling_rate  = hparams.sampling_rate\n        self.filter_length  = hparams.filter_length \n        self.hop_length     = hparams.hop_length \n        self.win_length     = hparams.win_length\n        self.sampling_rate  = hparams.sampling_rate \n\n        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n\n        self.add_blank = hparams.add_blank\n        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n\n        random.seed(1234)\n        random.shuffle(self.audiopaths_and_text)\n        self._filter()\n\n\n    def _filter(self):\n        \"\"\"\n        Filter text & store spec lengths\n        \"\"\"\n        # Store spectrogram lengths for Bucketing\n        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n        # spec_length = wav_length // hop_length\n\n        audiopaths_and_text_new = []\n        lengths = []\n        for audiopath, text in self.audiopaths_and_text:\n            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n                audiopaths_and_text_new.append([audiopath, text])\n                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n        self.audiopaths_and_text = audiopaths_and_text_new\n        self.lengths = lengths\n\n    def get_audio_text_pair(self, audiopath_and_text):\n        # separate filename and text\n        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n        text = self.get_text(text)\n        spec, wav = self.get_audio(audiopath)\n        return (text, spec, wav)\n\n    def get_audio(self, filename):\n        audio, sampling_rate = load_wav_to_torch(filename)\n        if sampling_rate != self.sampling_rate:\n            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n                sampling_rate, self.sampling_rate))\n        audio_norm = audio / self.max_wav_value\n        audio_norm = audio_norm.unsqueeze(0)\n        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n        if os.path.exists(spec_filename):\n            spec = torch.load(spec_filename)\n        else:\n            spec = spectrogram_torch(audio_norm, self.filter_length,\n                self.sampling_rate, self.hop_length, self.win_length,\n                center=False)\n            spec = torch.squeeze(spec, 0)\n            torch.save(spec, spec_filename)\n        return spec, audio_norm\n\n    def get_text(self, text):\n        if self.cleaned_text:\n            text_norm = cleaned_text_to_sequence(text)\n        else:\n            text_norm = text_to_sequence(text, self.text_cleaners)\n        if self.add_blank:\n            text_norm = commons.intersperse(text_norm, 0)\n        text_norm = torch.LongTensor(text_norm)\n        return text_norm\n\n    def __getitem__(self, index):\n        return self.get_audio_text_pair(self.audiopaths_and_text[index])\n\n    def __len__(self):\n        return len(self.audiopaths_and_text)\n\n\nclass TextAudioCollate():\n    \"\"\" Zero-pads model inputs and targets\n    \"\"\"\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text and aduio\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized]\n        \"\"\"\n        # Right zero-pad all one-hot text sequences to max input length\n        _, ids_sorted_decreasing = torch.sort(\n            torch.LongTensor([x[1].size(1) for x in batch]),\n            dim=0, descending=True)\n\n        max_text_len = max([len(x[0]) for x in batch])\n        max_spec_len = max([x[1].size(1) for x in batch])\n        max_wav_len = max([x[2].size(1) for x in batch])\n\n        text_lengths = torch.LongTensor(len(batch))\n        spec_lengths = torch.LongTensor(len(batch))\n        wav_lengths = torch.LongTensor(len(batch))\n\n        text_padded = torch.LongTensor(len(batch), max_text_len)\n        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n        text_padded.zero_()\n        spec_padded.zero_()\n        wav_padded.zero_()\n        for i in range(len(ids_sorted_decreasing)):\n            row = batch[ids_sorted_decreasing[i]]\n\n            text = row[0]\n            text_padded[i, :text.size(0)] = text\n            text_lengths[i] = text.size(0)\n\n            spec = row[1]\n            spec_padded[i, :, :spec.size(1)] = spec\n            spec_lengths[i] = spec.size(1)\n\n            wav = row[2]\n            wav_padded[i, :, :wav.size(1)] = wav\n            wav_lengths[i] = wav.size(1)\n\n        if self.return_ids:\n            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, ids_sorted_decreasing\n        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths\n\n\n\"\"\"Multi speaker version\"\"\"\nclass TextAudioSpeakerLoader(torch.utils.data.Dataset):\n    \"\"\"\n        1) loads audio, speaker_id, text pairs\n        2) normalizes text and converts them to sequences of integers\n        3) computes spectrograms from audio files.\n    \"\"\"\n    def __init__(self, audiopaths_sid_text, hparams):\n        self.audiopaths_sid_text = load_filepaths_and_text(audiopaths_sid_text)\n        self.text_cleaners = hparams.text_cleaners\n        self.max_wav_value = hparams.max_wav_value\n        self.sampling_rate = hparams.sampling_rate\n        self.filter_length  = hparams.filter_length\n        self.hop_length     = hparams.hop_length\n        self.win_length     = hparams.win_length\n        self.sampling_rate  = hparams.sampling_rate\n\n        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n\n        self.add_blank = hparams.add_blank\n        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n\n        random.seed(1234)\n        random.shuffle(self.audiopaths_sid_text)\n        self._filter()\n\n    def _filter(self):\n        \"\"\"\n        Filter text & store spec lengths\n        \"\"\"\n        # Store spectrogram lengths for Bucketing\n        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n        # spec_length = wav_length // hop_length\n\n        audiopaths_sid_text_new = []\n        lengths = []\n        for audiopath, sid, text in self.audiopaths_sid_text:\n            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n                audiopaths_sid_text_new.append([audiopath, sid, text])\n                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n        self.audiopaths_sid_text = audiopaths_sid_text_new\n        self.lengths = lengths\n\n    def get_audio_text_speaker_pair(self, audiopath_sid_text):\n        # separate filename, speaker_id and text\n        audiopath, sid, text = audiopath_sid_text[0], audiopath_sid_text[1], audiopath_sid_text[2]\n        text = self.get_text(text)\n        spec, wav = self.get_audio(audiopath)\n        sid = self.get_sid(sid)\n        return (text, spec, wav, sid)\n\n    def get_audio(self, filename):\n        audio, sampling_rate = load_wav_to_torch(filename)\n        if sampling_rate != self.sampling_rate:\n            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n                sampling_rate, self.sampling_rate))\n        audio_norm = audio / self.max_wav_value\n        audio_norm = audio_norm.unsqueeze(0)\n        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n        if os.path.exists(spec_filename):\n            spec = torch.load(spec_filename)\n        else:\n            spec = spectrogram_torch(audio_norm, self.filter_length,\n                self.sampling_rate, self.hop_length, self.win_length,\n                center=False)\n            spec = torch.squeeze(spec, 0)\n            torch.save(spec, spec_filename)\n        return spec, audio_norm\n\n    def get_text(self, text):\n        if self.cleaned_text:\n            text_norm = cleaned_text_to_sequence(text)\n        else:\n            text_norm = text_to_sequence(text, self.text_cleaners)\n        if self.add_blank:\n            text_norm = commons.intersperse(text_norm, 0)\n        text_norm = torch.LongTensor(text_norm)\n        return text_norm\n\n    def get_sid(self, sid):\n        sid = torch.LongTensor([int(sid)])\n        return sid\n\n    def __getitem__(self, index):\n        return self.get_audio_text_speaker_pair(self.audiopaths_sid_text[index])\n\n    def __len__(self):\n        return len(self.audiopaths_sid_text)\n\n\nclass TextAudioSpeakerCollate():\n    \"\"\" Zero-pads model inputs and targets\n    \"\"\"\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text, audio and speaker identities\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized, sid]\n        \"\"\"\n        # Right zero-pad all one-hot text sequences to max input length\n        _, ids_sorted_decreasing = torch.sort(\n            torch.LongTensor([x[1].size(1) for x in batch]),\n            dim=0, descending=True)\n\n        max_text_len = max([len(x[0]) for x in batch])\n        max_spec_len = max([x[1].size(1) for x in batch])\n        max_wav_len = max([x[2].size(1) for x in batch])\n\n        text_lengths = torch.LongTensor(len(batch))\n        spec_lengths = torch.LongTensor(len(batch))\n        wav_lengths = torch.LongTensor(len(batch))\n        sid = torch.LongTensor(len(batch))\n\n        text_padded = torch.LongTensor(len(batch), max_text_len)\n        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n        text_padded.zero_()\n        spec_padded.zero_()\n        wav_padded.zero_()\n        for i in range(len(ids_sorted_decreasing)):\n            row = batch[ids_sorted_decreasing[i]]\n\n            text = row[0]\n            text_padded[i, :text.size(0)] = text\n            text_lengths[i] = text.size(0)\n\n            spec = row[1]\n            spec_padded[i, :, :spec.size(1)] = spec\n            spec_lengths[i] = spec.size(1)\n\n            wav = row[2]\n            wav_padded[i, :, :wav.size(1)] = wav\n            wav_lengths[i] = wav.size(1)\n\n            sid[i] = row[3]\n\n        if self.return_ids:\n            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid, ids_sorted_decreasing\n        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid\n\n\nclass DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):\n    \"\"\"\n    Maintain similar input lengths in a batch.\n    Length groups are specified by boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n  \n    It removes samples which are not included in the boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n    \"\"\"\n    def __init__(self, dataset, batch_size, boundaries, num_replicas=None, rank=None, shuffle=True):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n        self.lengths = dataset.lengths\n        self.batch_size = batch_size\n        self.boundaries = boundaries\n  \n        self.buckets, self.num_samples_per_bucket = self._create_buckets()\n        self.total_size = sum(self.num_samples_per_bucket)\n        self.num_samples = self.total_size // self.num_replicas\n  \n    def _create_buckets(self):\n        buckets = [[] for _ in range(len(self.boundaries) - 1)]\n        for i in range(len(self.lengths)):\n            length = self.lengths[i]\n            idx_bucket = self._bisect(length)\n            if idx_bucket != -1:\n                buckets[idx_bucket].append(i)\n  \n        for i in range(len(buckets) - 1, 0, -1):\n            if len(buckets[i]) == 0:\n                buckets.pop(i)\n                self.boundaries.pop(i+1)\n  \n        num_samples_per_bucket = []\n        for i in range(len(buckets)):\n            len_bucket = len(buckets[i])\n            total_batch_size = self.num_replicas * self.batch_size\n            rem = (total_batch_size - (len_bucket % total_batch_size)) % total_batch_size\n            num_samples_per_bucket.append(len_bucket + rem)\n        return buckets, num_samples_per_bucket\n  \n    def __iter__(self):\n      # deterministically shuffle based on epoch\n      g = torch.Generator()\n      g.manual_seed(self.epoch)\n  \n      indices = []\n      if self.shuffle:\n          for bucket in self.buckets:\n              indices.append(torch.randperm(len(bucket), generator=g).tolist())\n      else:\n          for bucket in self.buckets:\n              indices.append(list(range(len(bucket))))\n  \n      batches = []\n      for i in range(len(self.buckets)):\n          bucket = self.buckets[i]\n          len_bucket = len(bucket)\n          ids_bucket = indices[i]\n          num_samples_bucket = self.num_samples_per_bucket[i]\n  \n          # add extra samples to make it evenly divisible\n          rem = num_samples_bucket - len_bucket\n          ids_bucket = ids_bucket + ids_bucket * (rem // len_bucket) + ids_bucket[:(rem % len_bucket)]\n  \n          # subsample\n          ids_bucket = ids_bucket[self.rank::self.num_replicas]\n  \n          # batching\n          for j in range(len(ids_bucket) // self.batch_size):\n              batch = [bucket[idx] for idx in ids_bucket[j*self.batch_size:(j+1)*self.batch_size]]\n              batches.append(batch)\n  \n      if self.shuffle:\n          batch_ids = torch.randperm(len(batches), generator=g).tolist()\n          batches = [batches[i] for i in batch_ids]\n      self.batches = batches\n  \n      assert len(self.batches) * self.batch_size == self.num_samples\n      return iter(self.batches)\n  \n    def _bisect(self, x, lo=0, hi=None):\n      if hi is None:\n          hi = len(self.boundaries) - 1\n  \n      if hi > lo:\n          mid = (hi + lo) // 2\n          if self.boundaries[mid] < x and x <= self.boundaries[mid+1]:\n              return mid\n          elif x <= self.boundaries[mid]:\n              return self._bisect(x, lo, mid)\n          else:\n              return self._bisect(x, mid + 1, hi)\n      else:\n          return -1\n\n    def __len__(self):\n        return self.num_samples // self.batch_size\n"
        },
        {
          "name": "filelists",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference.ipynb",
          "type": "blob",
          "size": 5.9599609375,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%matplotlib inline\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import IPython.display as ipd\\n\",\n    \"\\n\",\n    \"import os\\n\",\n    \"import json\\n\",\n    \"import math\\n\",\n    \"import torch\\n\",\n    \"from torch import nn\\n\",\n    \"from torch.nn import functional as F\\n\",\n    \"from torch.utils.data import DataLoader\\n\",\n    \"\\n\",\n    \"import commons\\n\",\n    \"import utils\\n\",\n    \"from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\\n\",\n    \"from models import SynthesizerTrn\\n\",\n    \"from text.symbols import symbols\\n\",\n    \"from text import text_to_sequence\\n\",\n    \"\\n\",\n    \"from scipy.io.wavfile import write\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"def get_text(text, hps):\\n\",\n    \"    text_norm = text_to_sequence(text, hps.data.text_cleaners)\\n\",\n    \"    if hps.data.add_blank:\\n\",\n    \"        text_norm = commons.intersperse(text_norm, 0)\\n\",\n    \"    text_norm = torch.LongTensor(text_norm)\\n\",\n    \"    return text_norm\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## LJ Speech\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"hps = utils.get_hparams_from_file(\\\"./configs/ljs_base.json\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"net_g = SynthesizerTrn(\\n\",\n    \"    len(symbols),\\n\",\n    \"    hps.data.filter_length // 2 + 1,\\n\",\n    \"    hps.train.segment_size // hps.data.hop_length,\\n\",\n    \"    **hps.model).cuda()\\n\",\n    \"_ = net_g.eval()\\n\",\n    \"\\n\",\n    \"_ = utils.load_checkpoint(\\\"/path/to/pretrained_ljs.pth\\\", net_g, None)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"stn_tst = get_text(\\\"VITS is Awesome!\\\", hps)\\n\",\n    \"with torch.no_grad():\\n\",\n    \"    x_tst = stn_tst.cuda().unsqueeze(0)\\n\",\n    \"    x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).cuda()\\n\",\n    \"    audio = net_g.infer(x_tst, x_tst_lengths, noise_scale=.667, noise_scale_w=0.8, length_scale=1)[0][0,0].data.cpu().float().numpy()\\n\",\n    \"ipd.display(ipd.Audio(audio, rate=hps.data.sampling_rate, normalize=False))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## VCTK\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"hps = utils.get_hparams_from_file(\\\"./configs/vctk_base.json\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"net_g = SynthesizerTrn(\\n\",\n    \"    len(symbols),\\n\",\n    \"    hps.data.filter_length // 2 + 1,\\n\",\n    \"    hps.train.segment_size // hps.data.hop_length,\\n\",\n    \"    n_speakers=hps.data.n_speakers,\\n\",\n    \"    **hps.model).cuda()\\n\",\n    \"_ = net_g.eval()\\n\",\n    \"\\n\",\n    \"_ = utils.load_checkpoint(\\\"/path/to/pretrained_vctk.pth\\\", net_g, None)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"stn_tst = get_text(\\\"VITS is Awesome!\\\", hps)\\n\",\n    \"with torch.no_grad():\\n\",\n    \"    x_tst = stn_tst.cuda().unsqueeze(0)\\n\",\n    \"    x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).cuda()\\n\",\n    \"    sid = torch.LongTensor([4]).cuda()\\n\",\n    \"    audio = net_g.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8, length_scale=1)[0][0,0].data.cpu().float().numpy()\\n\",\n    \"ipd.display(ipd.Audio(audio, rate=hps.data.sampling_rate, normalize=False))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Voice Conversion\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"dataset = TextAudioSpeakerLoader(hps.data.validation_files, hps.data)\\n\",\n    \"collate_fn = TextAudioSpeakerCollate()\\n\",\n    \"loader = DataLoader(dataset, num_workers=8, shuffle=False,\\n\",\n    \"    batch_size=1, pin_memory=True,\\n\",\n    \"    drop_last=True, collate_fn=collate_fn)\\n\",\n    \"data_list = list(loader)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"with torch.no_grad():\\n\",\n    \"    x, x_lengths, spec, spec_lengths, y, y_lengths, sid_src = [x.cuda() for x in data_list[0]]\\n\",\n    \"    sid_tgt1 = torch.LongTensor([1]).cuda()\\n\",\n    \"    sid_tgt2 = torch.LongTensor([2]).cuda()\\n\",\n    \"    sid_tgt3 = torch.LongTensor([4]).cuda()\\n\",\n    \"    audio1 = net_g.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt1)[0][0,0].data.cpu().float().numpy()\\n\",\n    \"    audio2 = net_g.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt2)[0][0,0].data.cpu().float().numpy()\\n\",\n    \"    audio3 = net_g.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt3)[0][0,0].data.cpu().float().numpy()\\n\",\n    \"print(\\\"Original SID: %d\\\" % sid_src.item())\\n\",\n    \"ipd.display(ipd.Audio(y[0].cpu().numpy(), rate=hps.data.sampling_rate, normalize=False))\\n\",\n    \"print(\\\"Converted SID: %d\\\" % sid_tgt1.item())\\n\",\n    \"ipd.display(ipd.Audio(audio1, rate=hps.data.sampling_rate, normalize=False))\\n\",\n    \"print(\\\"Converted SID: %d\\\" % sid_tgt2.item())\\n\",\n    \"ipd.display(ipd.Audio(audio2, rate=hps.data.sampling_rate, normalize=False))\\n\",\n    \"print(\\\"Converted SID: %d\\\" % sid_tgt3.item())\\n\",\n    \"ipd.display(ipd.Audio(audio3, rate=hps.data.sampling_rate, normalize=False))\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.7\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "losses.py",
          "type": "blob",
          "size": 1.283203125,
          "content": "import torch \nfrom torch.nn import functional as F\n\nimport commons\n\n\ndef feature_loss(fmap_r, fmap_g):\n  loss = 0\n  for dr, dg in zip(fmap_r, fmap_g):\n    for rl, gl in zip(dr, dg):\n      rl = rl.float().detach()\n      gl = gl.float()\n      loss += torch.mean(torch.abs(rl - gl))\n\n  return loss * 2 \n\n\ndef discriminator_loss(disc_real_outputs, disc_generated_outputs):\n  loss = 0\n  r_losses = []\n  g_losses = []\n  for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n    dr = dr.float()\n    dg = dg.float()\n    r_loss = torch.mean((1-dr)**2)\n    g_loss = torch.mean(dg**2)\n    loss += (r_loss + g_loss)\n    r_losses.append(r_loss.item())\n    g_losses.append(g_loss.item())\n\n  return loss, r_losses, g_losses\n\n\ndef generator_loss(disc_outputs):\n  loss = 0\n  gen_losses = []\n  for dg in disc_outputs:\n    dg = dg.float()\n    l = torch.mean((1-dg)**2)\n    gen_losses.append(l)\n    loss += l\n\n  return loss, gen_losses\n\n\ndef kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n  \"\"\"\n  z_p, logs_q: [b, h, t_t]\n  m_p, logs_p: [b, h, t_t]\n  \"\"\"\n  z_p = z_p.float()\n  logs_q = logs_q.float()\n  m_p = m_p.float()\n  logs_p = logs_p.float()\n  z_mask = z_mask.float()\n\n  kl = logs_p - logs_q - 0.5\n  kl += 0.5 * ((z_p - m_p)**2) * torch.exp(-2. * logs_p)\n  kl = torch.sum(kl * z_mask)\n  l = kl / torch.sum(z_mask)\n  return l\n"
        },
        {
          "name": "mel_processing.py",
          "type": "blob",
          "size": 3.7353515625,
          "content": "import math\nimport os\nimport random\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.utils.data\nimport numpy as np\nimport librosa\nimport librosa.util as librosa_util\nfrom librosa.util import normalize, pad_center, tiny\nfrom scipy.signal import get_window\nfrom scipy.io.wavfile import read\nfrom librosa.filters import mel as librosa_mel_fn\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C\n\n\ndef spectral_normalize_torch(magnitudes):\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\n\n\ndef spectral_de_normalize_torch(magnitudes):\n    output = dynamic_range_decompression_torch(magnitudes)\n    return output\n\n\nmel_basis = {}\nhann_window = {}\n\n\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec\n\n\ndef spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    global mel_basis\n    dtype_device = str(spec.dtype) + '_' + str(spec.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n    return spec\n\n\ndef mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global mel_basis, hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n\n    return spec\n"
        },
        {
          "name": "models.py",
          "type": "blob",
          "size": 18.9208984375,
          "content": "import copy\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport commons\nimport modules\nimport attentions\nimport monotonic_align\n\nfrom torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\nfrom torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\nfrom commons import init_weights, get_padding\n\n\nclass StochasticDurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n    super().__init__()\n    filter_channels = in_channels # it needs to be removed from future version.\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.log_flow = modules.Log()\n    self.flows = nn.ModuleList()\n    self.flows.append(modules.ElementwiseAffine(2))\n    for i in range(n_flows):\n      self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.flows.append(modules.Flip())\n\n    self.post_pre = nn.Conv1d(1, filter_channels, 1)\n    self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    self.post_flows = nn.ModuleList()\n    self.post_flows.append(modules.ElementwiseAffine(2))\n    for i in range(4):\n      self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.post_flows.append(modules.Flip())\n\n    self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n    self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n  def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n    x = torch.detach(x)\n    x = self.pre(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.convs(x, x_mask)\n    x = self.proj(x) * x_mask\n\n    if not reverse:\n      flows = self.flows\n      assert w is not None\n\n      logdet_tot_q = 0 \n      h_w = self.post_pre(w)\n      h_w = self.post_convs(h_w, x_mask)\n      h_w = self.post_proj(h_w) * x_mask\n      e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n      z_q = e_q\n      for flow in self.post_flows:\n        z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n        logdet_tot_q += logdet_q\n      z_u, z1 = torch.split(z_q, [1, 1], 1) \n      u = torch.sigmoid(z_u) * x_mask\n      z0 = (w - u) * x_mask\n      logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n      logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n\n      logdet_tot = 0\n      z0, logdet = self.log_flow(z0, x_mask)\n      logdet_tot += logdet\n      z = torch.cat([z0, z1], 1)\n      for flow in flows:\n        z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n        logdet_tot = logdet_tot + logdet\n      nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n      return nll + logq # [b]\n    else:\n      flows = list(reversed(self.flows))\n      flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n      z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n      for flow in flows:\n        z = flow(z, x_mask, g=x, reverse=reverse)\n      z0, z1 = torch.split(z, [1, 1], 1)\n      logw = z0\n      return logw\n\n\nclass DurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, gin_channels=0):\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.gin_channels = gin_channels\n\n    self.drop = nn.Dropout(p_dropout)\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_1 = modules.LayerNorm(filter_channels)\n    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_2 = modules.LayerNorm(filter_channels)\n    self.proj = nn.Conv1d(filter_channels, 1, 1)\n\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, in_channels, 1)\n\n  def forward(self, x, x_mask, g=None):\n    x = torch.detach(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.conv_1(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_1(x)\n    x = self.drop(x)\n    x = self.conv_2(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_2(x)\n    x = self.drop(x)\n    x = self.proj(x * x_mask)\n    return x * x_mask\n\n\nclass TextEncoder(nn.Module):\n  def __init__(self,\n      n_vocab,\n      out_channels,\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout):\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n\n    self.emb = nn.Embedding(n_vocab, hidden_channels)\n    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n    self.encoder = attentions.Encoder(\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout)\n    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths):\n    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n    x = torch.transpose(x, 1, -1) # [b, h, t]\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n\n    x = self.encoder(x * x_mask, x_mask)\n    stats = self.proj(x) * x_mask\n\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    return x, m, logs, x_mask\n\n\nclass ResidualCouplingBlock(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      n_flows=4,\n      gin_channels=0):\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.flows = nn.ModuleList()\n    for i in range(n_flows):\n      self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n      self.flows.append(modules.Flip())\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    if not reverse:\n      for flow in self.flows:\n        x, _ = flow(x, x_mask, g=g, reverse=reverse)\n    else:\n      for flow in reversed(self.flows):\n        x = flow(x, x_mask, g=g, reverse=reverse)\n    return x\n\n\nclass PosteriorEncoder(nn.Module):\n  def __init__(self,\n      in_channels,\n      out_channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      gin_channels=0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n\n    self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n    self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n    self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths, g=None):\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n    x = self.pre(x) * x_mask\n    x = self.enc(x, x_mask, g=g)\n    stats = self.proj(x) * x_mask\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n    return z, m, logs, x_mask\n\n\nclass Generator(torch.nn.Module):\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n          x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n\n\nclass DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        self.use_spectral_norm = use_spectral_norm\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(get_padding(kernel_size, 1), 0))),\n        ])\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        fmap = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0: # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n            norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n            norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n            norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n        ])\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        fmap = []\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminator, self).__init__()\n        periods = [2,3,5,7,11]\n\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods]\n        self.discriminators = nn.ModuleList(discs)\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            y_d_gs.append(y_d_g)\n            fmap_rs.append(fmap_r)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n\n\n\nclass SynthesizerTrn(nn.Module):\n  \"\"\"\n  Synthesizer for Training\n  \"\"\"\n\n  def __init__(self, \n    n_vocab,\n    spec_channels,\n    segment_size,\n    inter_channels,\n    hidden_channels,\n    filter_channels,\n    n_heads,\n    n_layers,\n    kernel_size,\n    p_dropout,\n    resblock, \n    resblock_kernel_sizes, \n    resblock_dilation_sizes, \n    upsample_rates, \n    upsample_initial_channel, \n    upsample_kernel_sizes,\n    n_speakers=0,\n    gin_channels=0,\n    use_sdp=True,\n    **kwargs):\n\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.spec_channels = spec_channels\n    self.inter_channels = inter_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.resblock = resblock\n    self.resblock_kernel_sizes = resblock_kernel_sizes\n    self.resblock_dilation_sizes = resblock_dilation_sizes\n    self.upsample_rates = upsample_rates\n    self.upsample_initial_channel = upsample_initial_channel\n    self.upsample_kernel_sizes = upsample_kernel_sizes\n    self.segment_size = segment_size\n    self.n_speakers = n_speakers\n    self.gin_channels = gin_channels\n\n    self.use_sdp = use_sdp\n\n    self.enc_p = TextEncoder(n_vocab,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout)\n    self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n    self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n    self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n\n    if use_sdp:\n      self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n    else:\n      self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n\n    if n_speakers > 1:\n      self.emb_g = nn.Embedding(n_speakers, gin_channels)\n\n  def forward(self, x, x_lengths, y, y_lengths, sid=None):\n\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n    if self.n_speakers > 0:\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n    else:\n      g = None\n\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n    z_p = self.flow(z, y_mask, g=g)\n\n    with torch.no_grad():\n      # negative cross-entropy\n      s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n      neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n      neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n      neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n      neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n      neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n\n      attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n      attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n\n    w = attn.sum(2)\n    if self.use_sdp:\n      l_length = self.dp(x, x_mask, w, g=g)\n      l_length = l_length / torch.sum(x_mask)\n    else:\n      logw_ = torch.log(w + 1e-6) * x_mask\n      logw = self.dp(x, x_mask, g=g)\n      l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging \n\n    # expand prior\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n\n    z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\n    o = self.dec(z_slice, g=g)\n    return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\n  def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n    if self.n_speakers > 0:\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n    else:\n      g = None\n\n    if self.use_sdp:\n      logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n    else:\n      logw = self.dp(x, x_mask, g=g)\n    w = torch.exp(logw) * x_mask * length_scale\n    w_ceil = torch.ceil(w)\n    y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n    y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n    attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n    attn = commons.generate_path(w_ceil, attn_mask)\n\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n\n    z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n    z = self.flow(z_p, y_mask, g=g, reverse=True)\n    o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n    return o, attn, y_mask, (z, z_p, m_p, logs_p)\n\n  def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n    g_src = self.emb_g(sid_src).unsqueeze(-1)\n    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n    z_p = self.flow(z, y_mask, g=g_src)\n    z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n    o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n    return o_hat, y_mask, (z, z_p, z_hat)\n\n"
        },
        {
          "name": "modules.py",
          "type": "blob",
          "size": 12.857421875,
          "content": "import copy\nimport math\nimport numpy as np\nimport scipy\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\nfrom torch.nn.utils import weight_norm, remove_weight_norm\n\nimport commons\nfrom commons import init_weights, get_padding\nfrom transforms import piecewise_rational_quadratic_transform\n\n\nLRELU_SLOPE = 0.1\n\n\nclass LayerNorm(nn.Module):\n  def __init__(self, channels, eps=1e-5):\n    super().__init__()\n    self.channels = channels\n    self.eps = eps\n\n    self.gamma = nn.Parameter(torch.ones(channels))\n    self.beta = nn.Parameter(torch.zeros(channels))\n\n  def forward(self, x):\n    x = x.transpose(1, -1)\n    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n    return x.transpose(1, -1)\n\n \nclass ConvReluNorm(nn.Module):\n  def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, n_layers, p_dropout):\n    super().__init__()\n    self.in_channels = in_channels\n    self.hidden_channels = hidden_channels\n    self.out_channels = out_channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.p_dropout = p_dropout\n    assert n_layers > 1, \"Number of layers should be larger than 0.\"\n\n    self.conv_layers = nn.ModuleList()\n    self.norm_layers = nn.ModuleList()\n    self.conv_layers.append(nn.Conv1d(in_channels, hidden_channels, kernel_size, padding=kernel_size//2))\n    self.norm_layers.append(LayerNorm(hidden_channels))\n    self.relu_drop = nn.Sequential(\n        nn.ReLU(),\n        nn.Dropout(p_dropout))\n    for _ in range(n_layers-1):\n      self.conv_layers.append(nn.Conv1d(hidden_channels, hidden_channels, kernel_size, padding=kernel_size//2))\n      self.norm_layers.append(LayerNorm(hidden_channels))\n    self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n    self.proj.weight.data.zero_()\n    self.proj.bias.data.zero_()\n\n  def forward(self, x, x_mask):\n    x_org = x\n    for i in range(self.n_layers):\n      x = self.conv_layers[i](x * x_mask)\n      x = self.norm_layers[i](x)\n      x = self.relu_drop(x)\n    x = x_org + self.proj(x)\n    return x * x_mask\n\n\nclass DDSConv(nn.Module):\n  \"\"\"\n  Dialted and Depth-Separable Convolution\n  \"\"\"\n  def __init__(self, channels, kernel_size, n_layers, p_dropout=0.):\n    super().__init__()\n    self.channels = channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.p_dropout = p_dropout\n\n    self.drop = nn.Dropout(p_dropout)\n    self.convs_sep = nn.ModuleList()\n    self.convs_1x1 = nn.ModuleList()\n    self.norms_1 = nn.ModuleList()\n    self.norms_2 = nn.ModuleList()\n    for i in range(n_layers):\n      dilation = kernel_size ** i\n      padding = (kernel_size * dilation - dilation) // 2\n      self.convs_sep.append(nn.Conv1d(channels, channels, kernel_size, \n          groups=channels, dilation=dilation, padding=padding\n      ))\n      self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n      self.norms_1.append(LayerNorm(channels))\n      self.norms_2.append(LayerNorm(channels))\n\n  def forward(self, x, x_mask, g=None):\n    if g is not None:\n      x = x + g\n    for i in range(self.n_layers):\n      y = self.convs_sep[i](x * x_mask)\n      y = self.norms_1[i](y)\n      y = F.gelu(y)\n      y = self.convs_1x1[i](y)\n      y = self.norms_2[i](y)\n      y = F.gelu(y)\n      y = self.drop(y)\n      x = x + y\n    return x * x_mask\n\n\nclass WN(torch.nn.Module):\n  def __init__(self, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0):\n    super(WN, self).__init__()\n    assert(kernel_size % 2 == 1)\n    self.hidden_channels =hidden_channels\n    self.kernel_size = kernel_size,\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n    self.p_dropout = p_dropout\n\n    self.in_layers = torch.nn.ModuleList()\n    self.res_skip_layers = torch.nn.ModuleList()\n    self.drop = nn.Dropout(p_dropout)\n\n    if gin_channels != 0:\n      cond_layer = torch.nn.Conv1d(gin_channels, 2*hidden_channels*n_layers, 1)\n      self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name='weight')\n\n    for i in range(n_layers):\n      dilation = dilation_rate ** i\n      padding = int((kernel_size * dilation - dilation) / 2)\n      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n                                 dilation=dilation, padding=padding)\n      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n      self.in_layers.append(in_layer)\n\n      # last one is not necessary\n      if i < n_layers - 1:\n        res_skip_channels = 2 * hidden_channels\n      else:\n        res_skip_channels = hidden_channels\n\n      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n      self.res_skip_layers.append(res_skip_layer)\n\n  def forward(self, x, x_mask, g=None, **kwargs):\n    output = torch.zeros_like(x)\n    n_channels_tensor = torch.IntTensor([self.hidden_channels])\n\n    if g is not None:\n      g = self.cond_layer(g)\n\n    for i in range(self.n_layers):\n      x_in = self.in_layers[i](x)\n      if g is not None:\n        cond_offset = i * 2 * self.hidden_channels\n        g_l = g[:,cond_offset:cond_offset+2*self.hidden_channels,:]\n      else:\n        g_l = torch.zeros_like(x_in)\n\n      acts = commons.fused_add_tanh_sigmoid_multiply(\n          x_in,\n          g_l,\n          n_channels_tensor)\n      acts = self.drop(acts)\n\n      res_skip_acts = self.res_skip_layers[i](acts)\n      if i < self.n_layers - 1:\n        res_acts = res_skip_acts[:,:self.hidden_channels,:]\n        x = (x + res_acts) * x_mask\n        output = output + res_skip_acts[:,self.hidden_channels:,:]\n      else:\n        output = output + res_skip_acts\n    return output * x_mask\n\n  def remove_weight_norm(self):\n    if self.gin_channels != 0:\n      torch.nn.utils.remove_weight_norm(self.cond_layer)\n    for l in self.in_layers:\n      torch.nn.utils.remove_weight_norm(l)\n    for l in self.res_skip_layers:\n     torch.nn.utils.remove_weight_norm(l)\n\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.convs1 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n                               padding=get_padding(kernel_size, dilation[2])))\n        ])\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1)))\n        ])\n        self.convs2.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c2(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)\n\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.convs = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1])))\n        ])\n        self.convs.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)\n\n\nclass Log(nn.Module):\n  def forward(self, x, x_mask, reverse=False, **kwargs):\n    if not reverse:\n      y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n      logdet = torch.sum(-y, [1, 2])\n      return y, logdet\n    else:\n      x = torch.exp(x) * x_mask\n      return x\n    \n\nclass Flip(nn.Module):\n  def forward(self, x, *args, reverse=False, **kwargs):\n    x = torch.flip(x, [1])\n    if not reverse:\n      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n      return x, logdet\n    else:\n      return x\n\n\nclass ElementwiseAffine(nn.Module):\n  def __init__(self, channels):\n    super().__init__()\n    self.channels = channels\n    self.m = nn.Parameter(torch.zeros(channels,1))\n    self.logs = nn.Parameter(torch.zeros(channels,1))\n\n  def forward(self, x, x_mask, reverse=False, **kwargs):\n    if not reverse:\n      y = self.m + torch.exp(self.logs) * x\n      y = y * x_mask\n      logdet = torch.sum(self.logs * x_mask, [1,2])\n      return y, logdet\n    else:\n      x = (x - self.m) * torch.exp(-self.logs) * x_mask\n      return x\n\n\nclass ResidualCouplingLayer(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      p_dropout=0,\n      gin_channels=0,\n      mean_only=False):\n    assert channels % 2 == 0, \"channels should be divisible by 2\"\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.half_channels = channels // 2\n    self.mean_only = mean_only\n\n    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n    self.enc = WN(hidden_channels, kernel_size, dilation_rate, n_layers, p_dropout=p_dropout, gin_channels=gin_channels)\n    self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n    self.post.weight.data.zero_()\n    self.post.bias.data.zero_()\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n    h = self.pre(x0) * x_mask\n    h = self.enc(h, x_mask, g=g)\n    stats = self.post(h) * x_mask\n    if not self.mean_only:\n      m, logs = torch.split(stats, [self.half_channels]*2, 1)\n    else:\n      m = stats\n      logs = torch.zeros_like(m)\n\n    if not reverse:\n      x1 = m + x1 * torch.exp(logs) * x_mask\n      x = torch.cat([x0, x1], 1)\n      logdet = torch.sum(logs, [1,2])\n      return x, logdet\n    else:\n      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n      x = torch.cat([x0, x1], 1)\n      return x\n\n\nclass ConvFlow(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, n_layers, num_bins=10, tail_bound=5.0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.num_bins = num_bins\n    self.tail_bound = tail_bound\n    self.half_channels = in_channels // 2\n\n    self.pre = nn.Conv1d(self.half_channels, filter_channels, 1)\n    self.convs = DDSConv(filter_channels, kernel_size, n_layers, p_dropout=0.)\n    self.proj = nn.Conv1d(filter_channels, self.half_channels * (num_bins * 3 - 1), 1)\n    self.proj.weight.data.zero_()\n    self.proj.bias.data.zero_()\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n    h = self.pre(x0)\n    h = self.convs(h, x_mask, g=g)\n    h = self.proj(h) * x_mask\n\n    b, c, t = x0.shape\n    h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2) # [b, cx?, t] -> [b, c, t, ?]\n\n    unnormalized_widths = h[..., :self.num_bins] / math.sqrt(self.filter_channels)\n    unnormalized_heights = h[..., self.num_bins:2*self.num_bins] / math.sqrt(self.filter_channels)\n    unnormalized_derivatives = h[..., 2 * self.num_bins:]\n\n    x1, logabsdet = piecewise_rational_quadratic_transform(x1,\n        unnormalized_widths,\n        unnormalized_heights,\n        unnormalized_derivatives,\n        inverse=reverse,\n        tails='linear',\n        tail_bound=self.tail_bound\n    )\n\n    x = torch.cat([x0, x1], 1) * x_mask\n    logdet = torch.sum(logabsdet * x_mask, [1,2])\n    if not reverse:\n        return x, logdet\n    else:\n        return x\n"
        },
        {
          "name": "monotonic_align",
          "type": "tree",
          "content": null
        },
        {
          "name": "preprocess.py",
          "type": "blob",
          "size": 1.041015625,
          "content": "import argparse\nimport text\nfrom utils import load_filepaths_and_text\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\"--out_extension\", default=\"cleaned\")\n  parser.add_argument(\"--text_index\", default=1, type=int)\n  parser.add_argument(\"--filelists\", nargs=\"+\", default=[\"filelists/ljs_audio_text_val_filelist.txt\", \"filelists/ljs_audio_text_test_filelist.txt\"])\n  parser.add_argument(\"--text_cleaners\", nargs=\"+\", default=[\"english_cleaners2\"])\n\n  args = parser.parse_args()\n    \n\n  for filelist in args.filelists:\n    print(\"START:\", filelist)\n    filepaths_and_text = load_filepaths_and_text(filelist)\n    for i in range(len(filepaths_and_text)):\n      original_text = filepaths_and_text[i][args.text_index]\n      cleaned_text = text._clean_text(original_text, args.text_cleaners)\n      filepaths_and_text[i][args.text_index] = cleaned_text\n\n    new_filelist = filelist + \".\" + args.out_extension\n    with open(new_filelist, \"w\", encoding=\"utf-8\") as f:\n      f.writelines([\"|\".join(x) + \"\\n\" for x in filepaths_and_text])\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.158203125,
          "content": "Cython==0.29.21\nlibrosa==0.8.0\nmatplotlib==3.3.1\nnumpy==1.18.5\nphonemizer==2.2.1\nscipy==1.5.2\ntensorboard==2.3.0\ntorch==1.6.0\ntorchvision==0.7.0\nUnidecode==1.1.1\n"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "text",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 10.4609375,
          "content": "import os\nimport json\nimport argparse\nimport itertools\nimport math\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport commons\nimport utils\nfrom data_utils import (\n  TextAudioLoader,\n  TextAudioCollate,\n  DistributedBucketSampler\n)\nfrom models import (\n  SynthesizerTrn,\n  MultiPeriodDiscriminator,\n)\nfrom losses import (\n  generator_loss,\n  discriminator_loss,\n  feature_loss,\n  kl_loss\n)\nfrom mel_processing import mel_spectrogram_torch, spec_to_mel_torch\nfrom text.symbols import symbols\n\n\ntorch.backends.cudnn.benchmark = True\nglobal_step = 0\n\n\ndef main():\n  \"\"\"Assume Single Node Multi GPUs Training Only\"\"\"\n  assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n\n  n_gpus = torch.cuda.device_count()\n  os.environ['MASTER_ADDR'] = 'localhost'\n  os.environ['MASTER_PORT'] = '80000'\n\n  hps = utils.get_hparams()\n  mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hps,))\n\n\ndef run(rank, n_gpus, hps):\n  global global_step\n  if rank == 0:\n    logger = utils.get_logger(hps.model_dir)\n    logger.info(hps)\n    utils.check_git_hash(hps.model_dir)\n    writer = SummaryWriter(log_dir=hps.model_dir)\n    writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n\n  dist.init_process_group(backend='nccl', init_method='env://', world_size=n_gpus, rank=rank)\n  torch.manual_seed(hps.train.seed)\n  torch.cuda.set_device(rank)\n\n  train_dataset = TextAudioLoader(hps.data.training_files, hps.data)\n  train_sampler = DistributedBucketSampler(\n      train_dataset,\n      hps.train.batch_size,\n      [32,300,400,500,600,700,800,900,1000],\n      num_replicas=n_gpus,\n      rank=rank,\n      shuffle=True)\n  collate_fn = TextAudioCollate()\n  train_loader = DataLoader(train_dataset, num_workers=8, shuffle=False, pin_memory=True,\n      collate_fn=collate_fn, batch_sampler=train_sampler)\n  if rank == 0:\n    eval_dataset = TextAudioLoader(hps.data.validation_files, hps.data)\n    eval_loader = DataLoader(eval_dataset, num_workers=8, shuffle=False,\n        batch_size=hps.train.batch_size, pin_memory=True,\n        drop_last=False, collate_fn=collate_fn)\n\n  net_g = SynthesizerTrn(\n      len(symbols),\n      hps.data.filter_length // 2 + 1,\n      hps.train.segment_size // hps.data.hop_length,\n      **hps.model).cuda(rank)\n  net_d = MultiPeriodDiscriminator(hps.model.use_spectral_norm).cuda(rank)\n  optim_g = torch.optim.AdamW(\n      net_g.parameters(), \n      hps.train.learning_rate, \n      betas=hps.train.betas, \n      eps=hps.train.eps)\n  optim_d = torch.optim.AdamW(\n      net_d.parameters(),\n      hps.train.learning_rate, \n      betas=hps.train.betas, \n      eps=hps.train.eps)\n  net_g = DDP(net_g, device_ids=[rank])\n  net_d = DDP(net_d, device_ids=[rank])\n\n  try:\n    _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\"), net_g, optim_g)\n    _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"D_*.pth\"), net_d, optim_d)\n    global_step = (epoch_str - 1) * len(train_loader)\n  except:\n    epoch_str = 1\n    global_step = 0\n\n  scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=hps.train.lr_decay, last_epoch=epoch_str-2)\n  scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=hps.train.lr_decay, last_epoch=epoch_str-2)\n\n  scaler = GradScaler(enabled=hps.train.fp16_run)\n\n  for epoch in range(epoch_str, hps.train.epochs + 1):\n    if rank==0:\n      train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, eval_loader], logger, [writer, writer_eval])\n    else:\n      train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, None], None, None)\n    scheduler_g.step()\n    scheduler_d.step()\n\n\ndef train_and_evaluate(rank, epoch, hps, nets, optims, schedulers, scaler, loaders, logger, writers):\n  net_g, net_d = nets\n  optim_g, optim_d = optims\n  scheduler_g, scheduler_d = schedulers\n  train_loader, eval_loader = loaders\n  if writers is not None:\n    writer, writer_eval = writers\n\n  train_loader.batch_sampler.set_epoch(epoch)\n  global global_step\n\n  net_g.train()\n  net_d.train()\n  for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(train_loader):\n    x, x_lengths = x.cuda(rank, non_blocking=True), x_lengths.cuda(rank, non_blocking=True)\n    spec, spec_lengths = spec.cuda(rank, non_blocking=True), spec_lengths.cuda(rank, non_blocking=True)\n    y, y_lengths = y.cuda(rank, non_blocking=True), y_lengths.cuda(rank, non_blocking=True)\n\n    with autocast(enabled=hps.train.fp16_run):\n      y_hat, l_length, attn, ids_slice, x_mask, z_mask,\\\n      (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths)\n\n      mel = spec_to_mel_torch(\n          spec, \n          hps.data.filter_length, \n          hps.data.n_mel_channels, \n          hps.data.sampling_rate,\n          hps.data.mel_fmin, \n          hps.data.mel_fmax)\n      y_mel = commons.slice_segments(mel, ids_slice, hps.train.segment_size // hps.data.hop_length)\n      y_hat_mel = mel_spectrogram_torch(\n          y_hat.squeeze(1), \n          hps.data.filter_length, \n          hps.data.n_mel_channels, \n          hps.data.sampling_rate, \n          hps.data.hop_length, \n          hps.data.win_length, \n          hps.data.mel_fmin, \n          hps.data.mel_fmax\n      )\n\n      y = commons.slice_segments(y, ids_slice * hps.data.hop_length, hps.train.segment_size) # slice \n\n      # Discriminator\n      y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n      with autocast(enabled=False):\n        loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g)\n        loss_disc_all = loss_disc\n    optim_d.zero_grad()\n    scaler.scale(loss_disc_all).backward()\n    scaler.unscale_(optim_d)\n    grad_norm_d = commons.clip_grad_value_(net_d.parameters(), None)\n    scaler.step(optim_d)\n\n    with autocast(enabled=hps.train.fp16_run):\n      # Generator\n      y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n      with autocast(enabled=False):\n        loss_dur = torch.sum(l_length.float())\n        loss_mel = F.l1_loss(y_mel, y_hat_mel) * hps.train.c_mel\n        loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * hps.train.c_kl\n\n        loss_fm = feature_loss(fmap_r, fmap_g)\n        loss_gen, losses_gen = generator_loss(y_d_hat_g)\n        loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n    optim_g.zero_grad()\n    scaler.scale(loss_gen_all).backward()\n    scaler.unscale_(optim_g)\n    grad_norm_g = commons.clip_grad_value_(net_g.parameters(), None)\n    scaler.step(optim_g)\n    scaler.update()\n\n    if rank==0:\n      if global_step % hps.train.log_interval == 0:\n        lr = optim_g.param_groups[0]['lr']\n        losses = [loss_disc, loss_gen, loss_fm, loss_mel, loss_dur, loss_kl]\n        logger.info('Train Epoch: {} [{:.0f}%]'.format(\n          epoch,\n          100. * batch_idx / len(train_loader)))\n        logger.info([x.item() for x in losses] + [global_step, lr])\n        \n        scalar_dict = {\"loss/g/total\": loss_gen_all, \"loss/d/total\": loss_disc_all, \"learning_rate\": lr, \"grad_norm_d\": grad_norm_d, \"grad_norm_g\": grad_norm_g}\n        scalar_dict.update({\"loss/g/fm\": loss_fm, \"loss/g/mel\": loss_mel, \"loss/g/dur\": loss_dur, \"loss/g/kl\": loss_kl})\n\n        scalar_dict.update({\"loss/g/{}\".format(i): v for i, v in enumerate(losses_gen)})\n        scalar_dict.update({\"loss/d_r/{}\".format(i): v for i, v in enumerate(losses_disc_r)})\n        scalar_dict.update({\"loss/d_g/{}\".format(i): v for i, v in enumerate(losses_disc_g)})\n        image_dict = { \n            \"slice/mel_org\": utils.plot_spectrogram_to_numpy(y_mel[0].data.cpu().numpy()),\n            \"slice/mel_gen\": utils.plot_spectrogram_to_numpy(y_hat_mel[0].data.cpu().numpy()), \n            \"all/mel\": utils.plot_spectrogram_to_numpy(mel[0].data.cpu().numpy()),\n            \"all/attn\": utils.plot_alignment_to_numpy(attn[0,0].data.cpu().numpy())\n        }\n        utils.summarize(\n          writer=writer,\n          global_step=global_step, \n          images=image_dict,\n          scalars=scalar_dict)\n\n      if global_step % hps.train.eval_interval == 0:\n        evaluate(hps, net_g, eval_loader, writer_eval)\n        utils.save_checkpoint(net_g, optim_g, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"G_{}.pth\".format(global_step)))\n        utils.save_checkpoint(net_d, optim_d, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"D_{}.pth\".format(global_step)))\n    global_step += 1\n  \n  if rank == 0:\n    logger.info('====> Epoch: {}'.format(epoch))\n\n \ndef evaluate(hps, generator, eval_loader, writer_eval):\n    generator.eval()\n    with torch.no_grad():\n      for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(eval_loader):\n        x, x_lengths = x.cuda(0), x_lengths.cuda(0)\n        spec, spec_lengths = spec.cuda(0), spec_lengths.cuda(0)\n        y, y_lengths = y.cuda(0), y_lengths.cuda(0)\n\n        # remove else\n        x = x[:1]\n        x_lengths = x_lengths[:1]\n        spec = spec[:1]\n        spec_lengths = spec_lengths[:1]\n        y = y[:1]\n        y_lengths = y_lengths[:1]\n        break\n      y_hat, attn, mask, *_ = generator.module.infer(x, x_lengths, max_len=1000)\n      y_hat_lengths = mask.sum([1,2]).long() * hps.data.hop_length\n\n      mel = spec_to_mel_torch(\n        spec, \n        hps.data.filter_length, \n        hps.data.n_mel_channels, \n        hps.data.sampling_rate,\n        hps.data.mel_fmin, \n        hps.data.mel_fmax)\n      y_hat_mel = mel_spectrogram_torch(\n        y_hat.squeeze(1).float(),\n        hps.data.filter_length,\n        hps.data.n_mel_channels,\n        hps.data.sampling_rate,\n        hps.data.hop_length,\n        hps.data.win_length,\n        hps.data.mel_fmin,\n        hps.data.mel_fmax\n      )\n    image_dict = {\n      \"gen/mel\": utils.plot_spectrogram_to_numpy(y_hat_mel[0].cpu().numpy())\n    }\n    audio_dict = {\n      \"gen/audio\": y_hat[0,:,:y_hat_lengths[0]]\n    }\n    if global_step == 0:\n      image_dict.update({\"gt/mel\": utils.plot_spectrogram_to_numpy(mel[0].cpu().numpy())})\n      audio_dict.update({\"gt/audio\": y[0,:,:y_lengths[0]]})\n\n    utils.summarize(\n      writer=writer_eval,\n      global_step=global_step, \n      images=image_dict,\n      audios=audio_dict,\n      audio_sampling_rate=hps.data.sampling_rate\n    )\n    generator.train()\n\n                           \nif __name__ == \"__main__\":\n  main()\n"
        },
        {
          "name": "train_ms.py",
          "type": "blob",
          "size": 10.6904296875,
          "content": "import os\nimport json\nimport argparse\nimport itertools\nimport math\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport commons\nimport utils\nfrom data_utils import (\n  TextAudioSpeakerLoader,\n  TextAudioSpeakerCollate,\n  DistributedBucketSampler\n)\nfrom models import (\n  SynthesizerTrn,\n  MultiPeriodDiscriminator,\n)\nfrom losses import (\n  generator_loss,\n  discriminator_loss,\n  feature_loss,\n  kl_loss\n)\nfrom mel_processing import mel_spectrogram_torch, spec_to_mel_torch\nfrom text.symbols import symbols\n\n\ntorch.backends.cudnn.benchmark = True\nglobal_step = 0\n\n\ndef main():\n  \"\"\"Assume Single Node Multi GPUs Training Only\"\"\"\n  assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n\n  n_gpus = torch.cuda.device_count()\n  os.environ['MASTER_ADDR'] = 'localhost'\n  os.environ['MASTER_PORT'] = '80000'\n\n  hps = utils.get_hparams()\n  mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hps,))\n\n\ndef run(rank, n_gpus, hps):\n  global global_step\n  if rank == 0:\n    logger = utils.get_logger(hps.model_dir)\n    logger.info(hps)\n    utils.check_git_hash(hps.model_dir)\n    writer = SummaryWriter(log_dir=hps.model_dir)\n    writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n\n  dist.init_process_group(backend='nccl', init_method='env://', world_size=n_gpus, rank=rank)\n  torch.manual_seed(hps.train.seed)\n  torch.cuda.set_device(rank)\n\n  train_dataset = TextAudioSpeakerLoader(hps.data.training_files, hps.data)\n  train_sampler = DistributedBucketSampler(\n      train_dataset,\n      hps.train.batch_size,\n      [32,300,400,500,600,700,800,900,1000],\n      num_replicas=n_gpus,\n      rank=rank,\n      shuffle=True)\n  collate_fn = TextAudioSpeakerCollate()\n  train_loader = DataLoader(train_dataset, num_workers=8, shuffle=False, pin_memory=True,\n      collate_fn=collate_fn, batch_sampler=train_sampler)\n  if rank == 0:\n    eval_dataset = TextAudioSpeakerLoader(hps.data.validation_files, hps.data)\n    eval_loader = DataLoader(eval_dataset, num_workers=8, shuffle=False,\n        batch_size=hps.train.batch_size, pin_memory=True,\n        drop_last=False, collate_fn=collate_fn)\n\n  net_g = SynthesizerTrn(\n      len(symbols),\n      hps.data.filter_length // 2 + 1,\n      hps.train.segment_size // hps.data.hop_length,\n      n_speakers=hps.data.n_speakers,\n      **hps.model).cuda(rank)\n  net_d = MultiPeriodDiscriminator(hps.model.use_spectral_norm).cuda(rank)\n  optim_g = torch.optim.AdamW(\n      net_g.parameters(), \n      hps.train.learning_rate, \n      betas=hps.train.betas, \n      eps=hps.train.eps)\n  optim_d = torch.optim.AdamW(\n      net_d.parameters(),\n      hps.train.learning_rate, \n      betas=hps.train.betas, \n      eps=hps.train.eps)\n  net_g = DDP(net_g, device_ids=[rank])\n  net_d = DDP(net_d, device_ids=[rank])\n\n  try:\n    _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\"), net_g, optim_g)\n    _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"D_*.pth\"), net_d, optim_d)\n    global_step = (epoch_str - 1) * len(train_loader)\n  except:\n    epoch_str = 1\n    global_step = 0\n\n  scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=hps.train.lr_decay, last_epoch=epoch_str-2)\n  scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=hps.train.lr_decay, last_epoch=epoch_str-2)\n\n  scaler = GradScaler(enabled=hps.train.fp16_run)\n\n  for epoch in range(epoch_str, hps.train.epochs + 1):\n    if rank==0:\n      train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, eval_loader], logger, [writer, writer_eval])\n    else:\n      train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, None], None, None)\n    scheduler_g.step()\n    scheduler_d.step()\n\n\ndef train_and_evaluate(rank, epoch, hps, nets, optims, schedulers, scaler, loaders, logger, writers):\n  net_g, net_d = nets\n  optim_g, optim_d = optims\n  scheduler_g, scheduler_d = schedulers\n  train_loader, eval_loader = loaders\n  if writers is not None:\n    writer, writer_eval = writers\n\n  train_loader.batch_sampler.set_epoch(epoch)\n  global global_step\n\n  net_g.train()\n  net_d.train()\n  for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths, speakers) in enumerate(train_loader):\n    x, x_lengths = x.cuda(rank, non_blocking=True), x_lengths.cuda(rank, non_blocking=True)\n    spec, spec_lengths = spec.cuda(rank, non_blocking=True), spec_lengths.cuda(rank, non_blocking=True)\n    y, y_lengths = y.cuda(rank, non_blocking=True), y_lengths.cuda(rank, non_blocking=True)\n    speakers = speakers.cuda(rank, non_blocking=True)\n\n    with autocast(enabled=hps.train.fp16_run):\n      y_hat, l_length, attn, ids_slice, x_mask, z_mask,\\\n      (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths, speakers)\n\n      mel = spec_to_mel_torch(\n          spec, \n          hps.data.filter_length, \n          hps.data.n_mel_channels, \n          hps.data.sampling_rate,\n          hps.data.mel_fmin, \n          hps.data.mel_fmax)\n      y_mel = commons.slice_segments(mel, ids_slice, hps.train.segment_size // hps.data.hop_length)\n      y_hat_mel = mel_spectrogram_torch(\n          y_hat.squeeze(1), \n          hps.data.filter_length, \n          hps.data.n_mel_channels, \n          hps.data.sampling_rate, \n          hps.data.hop_length, \n          hps.data.win_length, \n          hps.data.mel_fmin, \n          hps.data.mel_fmax\n      )\n\n      y = commons.slice_segments(y, ids_slice * hps.data.hop_length, hps.train.segment_size) # slice \n\n      # Discriminator\n      y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n      with autocast(enabled=False):\n        loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g)\n        loss_disc_all = loss_disc\n    optim_d.zero_grad()\n    scaler.scale(loss_disc_all).backward()\n    scaler.unscale_(optim_d)\n    grad_norm_d = commons.clip_grad_value_(net_d.parameters(), None)\n    scaler.step(optim_d)\n\n    with autocast(enabled=hps.train.fp16_run):\n      # Generator\n      y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n      with autocast(enabled=False):\n        loss_dur = torch.sum(l_length.float())\n        loss_mel = F.l1_loss(y_mel, y_hat_mel) * hps.train.c_mel\n        loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * hps.train.c_kl\n\n        loss_fm = feature_loss(fmap_r, fmap_g)\n        loss_gen, losses_gen = generator_loss(y_d_hat_g)\n        loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n    optim_g.zero_grad()\n    scaler.scale(loss_gen_all).backward()\n    scaler.unscale_(optim_g)\n    grad_norm_g = commons.clip_grad_value_(net_g.parameters(), None)\n    scaler.step(optim_g)\n    scaler.update()\n\n    if rank==0:\n      if global_step % hps.train.log_interval == 0:\n        lr = optim_g.param_groups[0]['lr']\n        losses = [loss_disc, loss_gen, loss_fm, loss_mel, loss_dur, loss_kl]\n        logger.info('Train Epoch: {} [{:.0f}%]'.format(\n          epoch,\n          100. * batch_idx / len(train_loader)))\n        logger.info([x.item() for x in losses] + [global_step, lr])\n        \n        scalar_dict = {\"loss/g/total\": loss_gen_all, \"loss/d/total\": loss_disc_all, \"learning_rate\": lr, \"grad_norm_d\": grad_norm_d, \"grad_norm_g\": grad_norm_g}\n        scalar_dict.update({\"loss/g/fm\": loss_fm, \"loss/g/mel\": loss_mel, \"loss/g/dur\": loss_dur, \"loss/g/kl\": loss_kl})\n\n        scalar_dict.update({\"loss/g/{}\".format(i): v for i, v in enumerate(losses_gen)})\n        scalar_dict.update({\"loss/d_r/{}\".format(i): v for i, v in enumerate(losses_disc_r)})\n        scalar_dict.update({\"loss/d_g/{}\".format(i): v for i, v in enumerate(losses_disc_g)})\n        image_dict = { \n            \"slice/mel_org\": utils.plot_spectrogram_to_numpy(y_mel[0].data.cpu().numpy()),\n            \"slice/mel_gen\": utils.plot_spectrogram_to_numpy(y_hat_mel[0].data.cpu().numpy()), \n            \"all/mel\": utils.plot_spectrogram_to_numpy(mel[0].data.cpu().numpy()),\n            \"all/attn\": utils.plot_alignment_to_numpy(attn[0,0].data.cpu().numpy())\n        }\n        utils.summarize(\n          writer=writer,\n          global_step=global_step, \n          images=image_dict,\n          scalars=scalar_dict)\n\n      if global_step % hps.train.eval_interval == 0:\n        evaluate(hps, net_g, eval_loader, writer_eval)\n        utils.save_checkpoint(net_g, optim_g, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"G_{}.pth\".format(global_step)))\n        utils.save_checkpoint(net_d, optim_d, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"D_{}.pth\".format(global_step)))\n    global_step += 1\n  \n  if rank == 0:\n    logger.info('====> Epoch: {}'.format(epoch))\n\n \ndef evaluate(hps, generator, eval_loader, writer_eval):\n    generator.eval()\n    with torch.no_grad():\n      for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths, speakers) in enumerate(eval_loader):\n        x, x_lengths = x.cuda(0), x_lengths.cuda(0)\n        spec, spec_lengths = spec.cuda(0), spec_lengths.cuda(0)\n        y, y_lengths = y.cuda(0), y_lengths.cuda(0)\n        speakers = speakers.cuda(0)\n\n        # remove else\n        x = x[:1]\n        x_lengths = x_lengths[:1]\n        spec = spec[:1]\n        spec_lengths = spec_lengths[:1]\n        y = y[:1]\n        y_lengths = y_lengths[:1]\n        speakers = speakers[:1]\n        break\n      y_hat, attn, mask, *_ = generator.module.infer(x, x_lengths, speakers, max_len=1000)\n      y_hat_lengths = mask.sum([1,2]).long() * hps.data.hop_length\n\n      mel = spec_to_mel_torch(\n        spec, \n        hps.data.filter_length, \n        hps.data.n_mel_channels, \n        hps.data.sampling_rate,\n        hps.data.mel_fmin, \n        hps.data.mel_fmax)\n      y_hat_mel = mel_spectrogram_torch(\n        y_hat.squeeze(1).float(),\n        hps.data.filter_length,\n        hps.data.n_mel_channels,\n        hps.data.sampling_rate,\n        hps.data.hop_length,\n        hps.data.win_length,\n        hps.data.mel_fmin,\n        hps.data.mel_fmax\n      )\n    image_dict = {\n      \"gen/mel\": utils.plot_spectrogram_to_numpy(y_hat_mel[0].cpu().numpy())\n    }\n    audio_dict = {\n      \"gen/audio\": y_hat[0,:,:y_hat_lengths[0]]\n    }\n    if global_step == 0:\n      image_dict.update({\"gt/mel\": utils.plot_spectrogram_to_numpy(mel[0].cpu().numpy())})\n      audio_dict.update({\"gt/audio\": y[0,:,:y_lengths[0]]})\n\n    utils.summarize(\n      writer=writer_eval,\n      global_step=global_step, \n      images=image_dict,\n      audios=audio_dict,\n      audio_sampling_rate=hps.data.sampling_rate\n    )\n    generator.train()\n\n                           \nif __name__ == \"__main__\":\n  main()\n"
        },
        {
          "name": "transforms.py",
          "type": "blob",
          "size": 8.291015625,
          "content": "import torch\nfrom torch.nn import functional as F\n\nimport numpy as np\n\n\nDEFAULT_MIN_BIN_WIDTH = 1e-3\nDEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\n\n\ndef piecewise_rational_quadratic_transform(inputs, \n                                           unnormalized_widths,\n                                           unnormalized_heights,\n                                           unnormalized_derivatives,\n                                           inverse=False,\n                                           tails=None, \n                                           tail_bound=1.,\n                                           min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                                           min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                                           min_derivative=DEFAULT_MIN_DERIVATIVE):\n\n    if tails is None:\n        spline_fn = rational_quadratic_spline\n        spline_kwargs = {}\n    else:\n        spline_fn = unconstrained_rational_quadratic_spline\n        spline_kwargs = {\n            'tails': tails,\n            'tail_bound': tail_bound\n        }\n\n    outputs, logabsdet = spline_fn(\n            inputs=inputs,\n            unnormalized_widths=unnormalized_widths,\n            unnormalized_heights=unnormalized_heights,\n            unnormalized_derivatives=unnormalized_derivatives,\n            inverse=inverse,\n            min_bin_width=min_bin_width,\n            min_bin_height=min_bin_height,\n            min_derivative=min_derivative,\n            **spline_kwargs\n    )\n    return outputs, logabsdet\n\n\ndef searchsorted(bin_locations, inputs, eps=1e-6):\n    bin_locations[..., -1] += eps\n    return torch.sum(\n        inputs[..., None] >= bin_locations,\n        dim=-1\n    ) - 1\n\n\ndef unconstrained_rational_quadratic_spline(inputs,\n                                            unnormalized_widths,\n                                            unnormalized_heights,\n                                            unnormalized_derivatives,\n                                            inverse=False,\n                                            tails='linear',\n                                            tail_bound=1.,\n                                            min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                                            min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                                            min_derivative=DEFAULT_MIN_DERIVATIVE):\n    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n    outside_interval_mask = ~inside_interval_mask\n\n    outputs = torch.zeros_like(inputs)\n    logabsdet = torch.zeros_like(inputs)\n\n    if tails == 'linear':\n        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\n        constant = np.log(np.exp(1 - min_derivative) - 1)\n        unnormalized_derivatives[..., 0] = constant\n        unnormalized_derivatives[..., -1] = constant\n\n        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n        logabsdet[outside_interval_mask] = 0\n    else:\n        raise RuntimeError('{} tails are not implemented.'.format(tails))\n\n    outputs[inside_interval_mask], logabsdet[inside_interval_mask] = rational_quadratic_spline(\n        inputs=inputs[inside_interval_mask],\n        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\n        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\n        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\n        inverse=inverse,\n        left=-tail_bound, right=tail_bound, bottom=-tail_bound, top=tail_bound,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative\n    )\n\n    return outputs, logabsdet\n\ndef rational_quadratic_spline(inputs,\n                              unnormalized_widths,\n                              unnormalized_heights,\n                              unnormalized_derivatives,\n                              inverse=False,\n                              left=0., right=1., bottom=0., top=1.,\n                              min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                              min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                              min_derivative=DEFAULT_MIN_DERIVATIVE):\n    if torch.min(inputs) < left or torch.max(inputs) > right:\n        raise ValueError('Input to a transform is not within its domain')\n\n    num_bins = unnormalized_widths.shape[-1]\n\n    if min_bin_width * num_bins > 1.0:\n        raise ValueError('Minimal bin width too large for the number of bins')\n    if min_bin_height * num_bins > 1.0:\n        raise ValueError('Minimal bin height too large for the number of bins')\n\n    widths = F.softmax(unnormalized_widths, dim=-1)\n    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n    cumwidths = torch.cumsum(widths, dim=-1)\n    cumwidths = F.pad(cumwidths, pad=(1, 0), mode='constant', value=0.0)\n    cumwidths = (right - left) * cumwidths + left\n    cumwidths[..., 0] = left\n    cumwidths[..., -1] = right\n    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n\n    derivatives = min_derivative + F.softplus(unnormalized_derivatives)\n\n    heights = F.softmax(unnormalized_heights, dim=-1)\n    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n    cumheights = torch.cumsum(heights, dim=-1)\n    cumheights = F.pad(cumheights, pad=(1, 0), mode='constant', value=0.0)\n    cumheights = (top - bottom) * cumheights + bottom\n    cumheights[..., 0] = bottom\n    cumheights[..., -1] = top\n    heights = cumheights[..., 1:] - cumheights[..., :-1]\n\n    if inverse:\n        bin_idx = searchsorted(cumheights, inputs)[..., None]\n    else:\n        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n\n    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n\n    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n    delta = heights / widths\n    input_delta = delta.gather(-1, bin_idx)[..., 0]\n\n    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)[..., 0]\n\n    input_heights = heights.gather(-1, bin_idx)[..., 0]\n\n    if inverse:\n        a = (((inputs - input_cumheights) * (input_derivatives\n                                             + input_derivatives_plus_one\n                                             - 2 * input_delta)\n              + input_heights * (input_delta - input_derivatives)))\n        b = (input_heights * input_derivatives\n             - (inputs - input_cumheights) * (input_derivatives\n                                              + input_derivatives_plus_one\n                                              - 2 * input_delta))\n        c = - input_delta * (inputs - input_cumheights)\n\n        discriminant = b.pow(2) - 4 * a * c\n        assert (discriminant >= 0).all()\n\n        root = (2 * c) / (-b - torch.sqrt(discriminant))\n        outputs = root * input_bin_widths + input_cumwidths\n\n        theta_one_minus_theta = root * (1 - root)\n        denominator = input_delta + ((input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n                                     * theta_one_minus_theta)\n        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * root.pow(2)\n                                                     + 2 * input_delta * theta_one_minus_theta\n                                                     + input_derivatives * (1 - root).pow(2))\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, -logabsdet\n    else:\n        theta = (inputs - input_cumwidths) / input_bin_widths\n        theta_one_minus_theta = theta * (1 - theta)\n\n        numerator = input_heights * (input_delta * theta.pow(2)\n                                     + input_derivatives * theta_one_minus_theta)\n        denominator = input_delta + ((input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n                                     * theta_one_minus_theta)\n        outputs = input_cumheights + numerator / denominator\n\n        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * theta.pow(2)\n                                                     + 2 * input_delta * theta_one_minus_theta\n                                                     + input_derivatives * (1 - theta).pow(2))\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, logabsdet\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 7.22265625,
          "content": "import os\nimport glob\nimport sys\nimport argparse\nimport logging\nimport json\nimport subprocess\nimport numpy as np\nfrom scipy.io.wavfile import read\nimport torch\n\nMATPLOTLIB_FLAG = False\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogger = logging\n\n\ndef load_checkpoint(checkpoint_path, model, optimizer=None):\n  assert os.path.isfile(checkpoint_path)\n  checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n  iteration = checkpoint_dict['iteration']\n  learning_rate = checkpoint_dict['learning_rate']\n  if optimizer is not None:\n    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n  saved_state_dict = checkpoint_dict['model']\n  if hasattr(model, 'module'):\n    state_dict = model.module.state_dict()\n  else:\n    state_dict = model.state_dict()\n  new_state_dict= {}\n  for k, v in state_dict.items():\n    try:\n      new_state_dict[k] = saved_state_dict[k]\n    except:\n      logger.info(\"%s is not in the checkpoint\" % k)\n      new_state_dict[k] = v\n  if hasattr(model, 'module'):\n    model.module.load_state_dict(new_state_dict)\n  else:\n    model.load_state_dict(new_state_dict)\n  logger.info(\"Loaded checkpoint '{}' (iteration {})\" .format(\n    checkpoint_path, iteration))\n  return model, optimizer, learning_rate, iteration\n\n\ndef save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):\n  logger.info(\"Saving model and optimizer state at iteration {} to {}\".format(\n    iteration, checkpoint_path))\n  if hasattr(model, 'module'):\n    state_dict = model.module.state_dict()\n  else:\n    state_dict = model.state_dict()\n  torch.save({'model': state_dict,\n              'iteration': iteration,\n              'optimizer': optimizer.state_dict(),\n              'learning_rate': learning_rate}, checkpoint_path)\n\n\ndef summarize(writer, global_step, scalars={}, histograms={}, images={}, audios={}, audio_sampling_rate=22050):\n  for k, v in scalars.items():\n    writer.add_scalar(k, v, global_step)\n  for k, v in histograms.items():\n    writer.add_histogram(k, v, global_step)\n  for k, v in images.items():\n    writer.add_image(k, v, global_step, dataformats='HWC')\n  for k, v in audios.items():\n    writer.add_audio(k, v, global_step, audio_sampling_rate)\n\n\ndef latest_checkpoint_path(dir_path, regex=\"G_*.pth\"):\n  f_list = glob.glob(os.path.join(dir_path, regex))\n  f_list.sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))\n  x = f_list[-1]\n  print(x)\n  return x\n\n\ndef plot_spectrogram_to_numpy(spectrogram):\n  global MATPLOTLIB_FLAG\n  if not MATPLOTLIB_FLAG:\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    MATPLOTLIB_FLAG = True\n    mpl_logger = logging.getLogger('matplotlib')\n    mpl_logger.setLevel(logging.WARNING)\n  import matplotlib.pylab as plt\n  import numpy as np\n  \n  fig, ax = plt.subplots(figsize=(10,2))\n  im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n                  interpolation='none')\n  plt.colorbar(im, ax=ax)\n  plt.xlabel(\"Frames\")\n  plt.ylabel(\"Channels\")\n  plt.tight_layout()\n\n  fig.canvas.draw()\n  data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n  plt.close()\n  return data\n\n\ndef plot_alignment_to_numpy(alignment, info=None):\n  global MATPLOTLIB_FLAG\n  if not MATPLOTLIB_FLAG:\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    MATPLOTLIB_FLAG = True\n    mpl_logger = logging.getLogger('matplotlib')\n    mpl_logger.setLevel(logging.WARNING)\n  import matplotlib.pylab as plt\n  import numpy as np\n\n  fig, ax = plt.subplots(figsize=(6, 4))\n  im = ax.imshow(alignment.transpose(), aspect='auto', origin='lower',\n                  interpolation='none')\n  fig.colorbar(im, ax=ax)\n  xlabel = 'Decoder timestep'\n  if info is not None:\n      xlabel += '\\n\\n' + info\n  plt.xlabel(xlabel)\n  plt.ylabel('Encoder timestep')\n  plt.tight_layout()\n\n  fig.canvas.draw()\n  data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n  plt.close()\n  return data\n\n\ndef load_wav_to_torch(full_path):\n  sampling_rate, data = read(full_path)\n  return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n\n\ndef load_filepaths_and_text(filename, split=\"|\"):\n  with open(filename, encoding='utf-8') as f:\n    filepaths_and_text = [line.strip().split(split) for line in f]\n  return filepaths_and_text\n\n\ndef get_hparams(init=True):\n  parser = argparse.ArgumentParser()\n  parser.add_argument('-c', '--config', type=str, default=\"./configs/base.json\",\n                      help='JSON file for configuration')\n  parser.add_argument('-m', '--model', type=str, required=True,\n                      help='Model name')\n  \n  args = parser.parse_args()\n  model_dir = os.path.join(\"./logs\", args.model)\n\n  if not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\n  config_path = args.config\n  config_save_path = os.path.join(model_dir, \"config.json\")\n  if init:\n    with open(config_path, \"r\") as f:\n      data = f.read()\n    with open(config_save_path, \"w\") as f:\n      f.write(data)\n  else:\n    with open(config_save_path, \"r\") as f:\n      data = f.read()\n  config = json.loads(data)\n  \n  hparams = HParams(**config)\n  hparams.model_dir = model_dir\n  return hparams\n\n\ndef get_hparams_from_dir(model_dir):\n  config_save_path = os.path.join(model_dir, \"config.json\")\n  with open(config_save_path, \"r\") as f:\n    data = f.read()\n  config = json.loads(data)\n\n  hparams =HParams(**config)\n  hparams.model_dir = model_dir\n  return hparams\n\n\ndef get_hparams_from_file(config_path):\n  with open(config_path, \"r\") as f:\n    data = f.read()\n  config = json.loads(data)\n\n  hparams =HParams(**config)\n  return hparams\n\n\ndef check_git_hash(model_dir):\n  source_dir = os.path.dirname(os.path.realpath(__file__))\n  if not os.path.exists(os.path.join(source_dir, \".git\")):\n    logger.warn(\"{} is not a git repository, therefore hash value comparison will be ignored.\".format(\n      source_dir\n    ))\n    return\n\n  cur_hash = subprocess.getoutput(\"git rev-parse HEAD\")\n\n  path = os.path.join(model_dir, \"githash\")\n  if os.path.exists(path):\n    saved_hash = open(path).read()\n    if saved_hash != cur_hash:\n      logger.warn(\"git hash values are different. {}(saved) != {}(current)\".format(\n        saved_hash[:8], cur_hash[:8]))\n  else:\n    open(path, \"w\").write(cur_hash)\n\n\ndef get_logger(model_dir, filename=\"train.log\"):\n  global logger\n  logger = logging.getLogger(os.path.basename(model_dir))\n  logger.setLevel(logging.DEBUG)\n  \n  formatter = logging.Formatter(\"%(asctime)s\\t%(name)s\\t%(levelname)s\\t%(message)s\")\n  if not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n  h = logging.FileHandler(os.path.join(model_dir, filename))\n  h.setLevel(logging.DEBUG)\n  h.setFormatter(formatter)\n  logger.addHandler(h)\n  return logger\n\n\nclass HParams():\n  def __init__(self, **kwargs):\n    for k, v in kwargs.items():\n      if type(v) == dict:\n        v = HParams(**v)\n      self[k] = v\n    \n  def keys(self):\n    return self.__dict__.keys()\n\n  def items(self):\n    return self.__dict__.items()\n\n  def values(self):\n    return self.__dict__.values()\n\n  def __len__(self):\n    return len(self.__dict__)\n\n  def __getitem__(self, key):\n    return getattr(self, key)\n\n  def __setitem__(self, key, value):\n    return setattr(self, key, value)\n\n  def __contains__(self, key):\n    return key in self.__dict__\n\n  def __repr__(self):\n    return self.__dict__.__repr__()\n"
        }
      ]
    }
  ]
}