{
  "metadata": {
    "timestamp": 1736560672385,
    "page": 319,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "zilliztech/GPTCache",
      "stars": 7335,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.92578125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n*.DS_Store\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n*.db\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n.idea\n**/data_map**.txt\n**/faiss**.index\n**/sqlite**.db\n**/gpt_cache**.db\n**/example.py\n**/example.db\n**/.chroma\ndocs/references/*\n!docs/references/index.rst"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "The MIT License (MIT)\n\nCopyright (c) Zilliz\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0244140625,
          "content": "include requirements.txt\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.5791015625,
          "content": "install:\n\t@pip install -r requirements.txt\n\t@python setup.py install\n\npip_upgrade:\n\t@python -m pip install --upgrade pip\n\npackage:\n\t@python setup.py sdist bdist_wheel\n\nupload:\n\t@python -m twine upload dist/*\n\nupload_test:\n\t@python -m twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n\nremove_example_cache:\n\t@bash ./scripts/remove_example_cache.sh\n\ncreate_conda_env:\n\t@bash ./scripts/manage_conda_env.sh create\n\nremove_conda_env:\n\t@bash ./scripts/manage_conda_env.sh remove\n\npylint_check:\n\tpylint --rcfile=pylint.conf --output-format=colorized gptcache\n\npytest:\n\tpytest tests/"
        },
        {
          "name": "OWNERS",
          "type": "blob",
          "size": 0.1396484375,
          "content": "filters:\n  \".*\":\n    reviewers:\n      - SimFG\n      - xiaofan-luan\n      - cxie\n    approvers:\n      - SimFG\n      - xiaofan-luan\n      - cxie\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 23.28515625,
          "content": "# GPTCache : A Library for Creating Semantic Cache for LLM Queries\nSlash Your LLM API Costs by 10x 💰, Boost Speed by 100x ⚡ \n\n[![Release](https://img.shields.io/pypi/v/gptcache?label=Release&color&logo=Python)](https://pypi.org/project/gptcache/)\n[![pip download](https://img.shields.io/pypi/dm/gptcache.svg?color=bright-green&logo=Pypi)](https://pypi.org/project/gptcache/)\n[![Codecov](https://img.shields.io/codecov/c/github/zilliztech/GPTCache/dev?label=Codecov&logo=codecov&token=E30WxqBeJJ)](https://codecov.io/gh/zilliztech/GPTCache)\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/license/mit/)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/zilliz_universe.svg?style=social&label=Follow%20%40Zilliz)](https://twitter.com/zilliz_universe)\n[![Discord](https://img.shields.io/discord/1092648432495251507?label=Discord&logo=discord)](https://discord.gg/Q8C6WEjSWV)\n\n🎉 GPTCache has been fully integrated with 🦜️🔗[LangChain](https://github.com/hwchase17/langchain) ! Here are detailed [usage instructions](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/llm_caching#gptcache).\n\n🐳 [The GPTCache server docker image](https://github.com/zilliztech/GPTCache/blob/main/docs/usage.md#Use-GPTCache-server) has been released, which means that **any language** will be able to use GPTCache!\n\n📔 This project is undergoing swift development, and as such, the API may be subject to change at any time. For the most up-to-date information, please refer to the latest [documentation]( https://gptcache.readthedocs.io/en/latest/) and [release note](https://github.com/zilliztech/GPTCache/blob/main/docs/release_note.md).\n\n**NOTE:** As the number of large models is growing explosively and their API shape is constantly evolving, we no longer add support for new API or models. We encourage the usage of using the get and set API in gptcache, here is the demo code: https://github.com/zilliztech/GPTCache/blob/main/examples/adapter/api.py\n\n## Quick Install\n\n`pip install gptcache`\n\n## 🚀 What is GPTCache?\n\nChatGPT and various large language models (LLMs) boast incredible versatility, enabling the development of a wide range of applications. However, as your application grows in popularity and encounters higher traffic levels, the expenses related to LLM API calls can become substantial. Additionally, LLM services might exhibit slow response times, especially when dealing with a significant number of requests.\n\nTo tackle this challenge, we have created GPTCache, a project dedicated to building a semantic cache for storing LLM responses. \n\n## 😊 Quick Start\n\n**Note**:\n\n- You can quickly try GPTCache and put it into a production environment without heavy development. However, please note that the repository is still under heavy development.\n- By default, only a limited number of libraries are installed to support the basic cache functionalities. When you need to use additional features, the related libraries will be **automatically installed**.\n- Make sure that the Python version is **3.8.1 or higher**, check: `python --version`\n- If you encounter issues installing a library due to a low pip version, run: `python -m pip install --upgrade pip`.\n\n### dev install\n\n```bash\n# clone GPTCache repo\ngit clone -b dev https://github.com/zilliztech/GPTCache.git\ncd GPTCache\n\n# install the repo\npip install -r requirements.txt\npython setup.py install\n```\n\n### example usage\n\nThese examples will help you understand how to use exact and similar matching with caching. You can also run the example on [Colab](https://colab.research.google.com/drive/1m1s-iTDfLDk-UwUAQ_L8j1C-gzkcr2Sk?usp=share_link). And more examples you can refer to the [Bootcamp](https://gptcache.readthedocs.io/en/latest/bootcamp/openai/chat.html)\n\nBefore running the example, **make sure** the OPENAI_API_KEY environment variable is set by executing `echo $OPENAI_API_KEY`. \n\nIf it is not already set, it can be set by using `export OPENAI_API_KEY=YOUR_API_KEY` on Unix/Linux/MacOS systems or `set OPENAI_API_KEY=YOUR_API_KEY` on Windows systems. \n\n> It is important to note that this method is only effective temporarily, so if you want a permanent effect, you'll need to modify the environment variable configuration file. For instance, on a Mac, you can modify the file located at `/etc/profile`.\n\n<details>\n\n<summary> Click to <strong>SHOW</strong> example code </summary>\n\n#### OpenAI API original usage\n\n```python\nimport os\nimport time\n\nimport openai\n\n\ndef response_text(openai_resp):\n    return openai_resp['choices'][0]['message']['content']\n\n\nquestion = 'what‘s chatgpt'\n\n# OpenAI API original usage\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nstart_time = time.time()\nresponse = openai.ChatCompletion.create(\n  model='gpt-3.5-turbo',\n  messages=[\n    {\n        'role': 'user',\n        'content': question\n    }\n  ],\n)\nprint(f'Question: {question}')\nprint(\"Time consuming: {:.2f}s\".format(time.time() - start_time))\nprint(f'Answer: {response_text(response)}\\n')\n\n```\n\n#### OpenAI API + GPTCache, exact match cache\n\n> If you ask ChatGPT the exact same two questions, the answer to the second question will be obtained from the cache without requesting ChatGPT again.\n\n```python\nimport time\n\n\ndef response_text(openai_resp):\n    return openai_resp['choices'][0]['message']['content']\n\nprint(\"Cache loading.....\")\n\n# To use GPTCache, that's all you need\n# -------------------------------------------------\nfrom gptcache import cache\nfrom gptcache.adapter import openai\n\ncache.init()\ncache.set_openai_key()\n# -------------------------------------------------\n\nquestion = \"what's github\"\nfor _ in range(2):\n    start_time = time.time()\n    response = openai.ChatCompletion.create(\n      model='gpt-3.5-turbo',\n      messages=[\n        {\n            'role': 'user',\n            'content': question\n        }\n      ],\n    )\n    print(f'Question: {question}')\n    print(\"Time consuming: {:.2f}s\".format(time.time() - start_time))\n    print(f'Answer: {response_text(response)}\\n')\n```\n\n#### OpenAI API + GPTCache, similar search cache\n\n> After obtaining an answer from ChatGPT in response to several similar questions, the answers to subsequent questions can be retrieved from the cache without the need to request ChatGPT again.\n\n```python\nimport time\n\n\ndef response_text(openai_resp):\n    return openai_resp['choices'][0]['message']['content']\n\nfrom gptcache import cache\nfrom gptcache.adapter import openai\nfrom gptcache.embedding import Onnx\nfrom gptcache.manager import CacheBase, VectorBase, get_data_manager\nfrom gptcache.similarity_evaluation.distance import SearchDistanceEvaluation\n\nprint(\"Cache loading.....\")\n\nonnx = Onnx()\ndata_manager = get_data_manager(CacheBase(\"sqlite\"), VectorBase(\"faiss\", dimension=onnx.dimension))\ncache.init(\n    embedding_func=onnx.to_embeddings,\n    data_manager=data_manager,\n    similarity_evaluation=SearchDistanceEvaluation(),\n    )\ncache.set_openai_key()\n\nquestions = [\n    \"what's github\",\n    \"can you explain what GitHub is\",\n    \"can you tell me more about GitHub\",\n    \"what is the purpose of GitHub\"\n]\n\nfor question in questions:\n    start_time = time.time()\n    response = openai.ChatCompletion.create(\n        model='gpt-3.5-turbo',\n        messages=[\n            {\n                'role': 'user',\n                'content': question\n            }\n        ],\n    )\n    print(f'Question: {question}')\n    print(\"Time consuming: {:.2f}s\".format(time.time() - start_time))\n    print(f'Answer: {response_text(response)}\\n')\n```\n\n#### OpenAI API + GPTCache, use temperature\n\n> You can always pass a parameter of temperature while requesting the API service or model.\n> \n> The range of `temperature` is [0, 2], default value is 0.0.\n> \n> A higher temperature means a higher possibility of skipping cache search and requesting large model directly.\n> When temperature is 2, it will skip cache and send request to large model directly for sure. When temperature is 0, it will search cache before requesting large model service.\n> \n> The default `post_process_messages_func` is `temperature_softmax`. In this case, refer to [API reference](https://gptcache.readthedocs.io/en/latest/references/processor.html#module-gptcache.processor.post) to learn about how `temperature` affects output.\n\n```python\nimport time\n\nfrom gptcache import cache, Config\nfrom gptcache.manager import manager_factory\nfrom gptcache.embedding import Onnx\nfrom gptcache.processor.post import temperature_softmax\nfrom gptcache.similarity_evaluation.distance import SearchDistanceEvaluation\nfrom gptcache.adapter import openai\n\ncache.set_openai_key()\n\nonnx = Onnx()\ndata_manager = manager_factory(\"sqlite,faiss\", vector_params={\"dimension\": onnx.dimension})\n\ncache.init(\n    embedding_func=onnx.to_embeddings,\n    data_manager=data_manager,\n    similarity_evaluation=SearchDistanceEvaluation(),\n    post_process_messages_func=temperature_softmax\n    )\n# cache.config = Config(similarity_threshold=0.2)\n\nquestion = \"what's github\"\n\nfor _ in range(3):\n    start = time.time()\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        temperature = 1.0,  # Change temperature here\n        messages=[{\n            \"role\": \"user\",\n            \"content\": question\n        }],\n    )\n    print(\"Time elapsed:\", round(time.time() - start, 3))\n    print(\"Answer:\", response[\"choices\"][0][\"message\"][\"content\"])\n```\n\n</details>\n\nTo use GPTCache exclusively, only the following lines of code are required, and there is no need to modify any existing code.\n\n```python\nfrom gptcache import cache\nfrom gptcache.adapter import openai\n\ncache.init()\ncache.set_openai_key()\n```\n\nMore Docs：\n\n- [Usage, how to use GPTCache better](docs/usage.md)\n- [Features, all features currently supported by the cache](docs/feature.md)\n- [Examples, learn better custom caching](examples/README.md)\n- [Distributed Caching and Horizontal Scaling ](docs/horizontal-scaling-usage.md)\n\n## 🎓 Bootcamp\n\n- GPTCache with **LangChain**\n  - [QA Generation](https://gptcache.readthedocs.io/en/latest/bootcamp/langchain/qa_generation.html)\n  - [Question Answering](https://gptcache.readthedocs.io/en/latest/bootcamp/langchain/question_answering.html)\n  - [SQL Chain](https://gptcache.readthedocs.io/en/latest/bootcamp/langchain/sqlite.html)\n  - [BabyAGI User Guide](https://gptcache.readthedocs.io/en/latest/bootcamp/langchain/baby_agi.html)\n- GPTCache with **Llama_index**\n  - [WebPage QA](https://gptcache.readthedocs.io/en/latest/bootcamp/llama_index/webpage_qa.html)\n- GPTCache with **OpenAI**\n  - [Chat completion](https://gptcache.readthedocs.io/en/latest/bootcamp/openai/chat.html)\n  - [Language Translation](https://gptcache.readthedocs.io/en/latest/bootcamp/openai/language_translate.html)\n  - [SQL Translate](https://gptcache.readthedocs.io/en/latest/bootcamp/openai/sql_translate.html)\n  - [Twitter Classifier](https://gptcache.readthedocs.io/en/latest/bootcamp/openai/tweet_classifier.html)\n  - [Multimodal: Image Generation](https://gptcache.readthedocs.io/en/latest/bootcamp/openai/image_generation.html)\n  - [Multimodal: Speech to Text](https://gptcache.readthedocs.io/en/latest/bootcamp/openai/speech_to_text.html)\n- GPTCache with **Replicate**\n  - [Visual Question Answering](https://gptcache.readthedocs.io/en/latest/bootcamp/replicate/visual_question_answering.html)\n- GPTCache with **Temperature Param**\n  - [OpenAI Chat](https://gptcache.readthedocs.io/en/latest/bootcamp/temperature/chat.html)\n  - [OpenAI Image Creation](https://gptcache.readthedocs.io/en/latest/bootcamp/temperature/create_image.html)\n\n## 😎 What can this help with?\nGPTCache offers the following primary benefits:\n\n- **Decreased expenses**: Most LLM services charge fees based on a combination of number of requests and [token count](https://openai.com/pricing). GPTCache effectively minimizes your expenses by caching query results, which in turn reduces the number of requests and tokens sent to the LLM service. As a result, you can enjoy a more cost-efficient experience when using the service.\n- **Enhanced performance**: LLMs employ generative AI algorithms to generate responses in real-time, a process that can sometimes be time-consuming. However, when a similar query is cached, the response time significantly improves, as the result is fetched directly from the cache, eliminating the need to interact with the LLM service. In most situations, GPTCache can also provide superior query throughput compared to standard LLM services.\n- **Adaptable development and testing environment**: As a developer working on LLM applications, you're aware that connecting to LLM APIs is generally necessary, and comprehensive testing of your application is crucial before moving it to a production environment. GPTCache provides an interface that mirrors LLM APIs and accommodates storage of both LLM-generated and mocked data. This feature enables you to effortlessly develop and test your application, eliminating the need to connect to the LLM service.\n- **Improved scalability and availability**: LLM services frequently enforce [rate limits](https://platform.openai.com/docs/guides/rate-limits), which are constraints that APIs place on the number of times a user or client can access the server within a given timeframe. Hitting a rate limit means that additional requests will be blocked until a certain period has elapsed, leading to a service outage. With GPTCache, you can easily scale to accommodate an increasing volume of of queries, ensuring consistent performance as your application's user base expands.\n\n## 🤔 How does it work?\n\nOnline services often exhibit data locality, with users frequently accessing popular or trending content. Cache systems take advantage of this behavior by storing commonly accessed data, which in turn reduces data retrieval time, improves response times, and eases the burden on backend servers. Traditional cache systems typically utilize an exact match between a new query and a cached query to determine if the requested content is available in the cache before fetching the data.\n\nHowever, using an exact match approach for LLM caches is less effective due to the complexity and variability of LLM queries, resulting in a low cache hit rate. To address this issue, GPTCache adopt alternative strategies like semantic caching. Semantic caching identifies and stores similar or related queries, thereby increasing cache hit probability and enhancing overall caching efficiency. \n\nGPTCache employs embedding algorithms to convert queries into embeddings and uses a vector store for similarity search on these embeddings. This process allows GPTCache to identify and retrieve similar or related queries from the cache storage, as illustrated in the [Modules section](https://github.com/zilliztech/GPTCache#-modules). \n\nFeaturing a modular design, GPTCache makes it easy for users to customize their own semantic cache. The system offers various implementations for each module, and users can even develop their own implementations to suit their specific needs.\n\nIn a semantic cache, you may encounter false positives during cache hits and false negatives during cache misses. GPTCache offers three metrics to gauge its performance, which are helpful for developers to optimize their caching systems:\n\n- **Hit Ratio**: This metric quantifies the cache's ability to fulfill content requests successfully, compared to the total number of requests it receives. A higher hit ratio indicates a more effective cache.\n- **Latency**: This metric measures the time it takes for a query to be processed and the corresponding data to be retrieved from the cache. Lower latency signifies a more efficient and responsive caching system.\n- **Recall**: This metric represents the proportion of queries served by the cache out of the total number of queries that should have been served by the cache. Higher recall percentages indicate that the cache is effectively serving the appropriate content.\n\nA [sample benchmark](https://github.com/zilliztech/gpt-cache/blob/main/examples/benchmark/benchmark_sqlite_faiss_onnx.py) is included for users to start with assessing the performance of their semantic cache.\n\n## 🤗 Modules\n\n![GPTCache Struct](docs/GPTCacheStructure.png)\n\n- **LLM Adapter**: \nThe LLM Adapter is designed to integrate different LLM models by unifying their APIs and request protocols. GPTCache offers a standardized interface for this purpose, with current support for ChatGPT integration.\n  - [x] Support OpenAI ChatGPT API.\n  - [x] Support [langchain](https://github.com/hwchase17/langchain).\n  - [x] Support [minigpt4](https://github.com/Vision-CAIR/MiniGPT-4.git).\n  - [x] Support [Llamacpp](https://github.com/ggerganov/llama.cpp.git).\n  - [x] Support [dolly](https://github.com/databrickslabs/dolly.git).\n  - [ ] Support other LLMs, such as Hugging Face Hub, Bard, Anthropic.\n- **Multimodal Adapter (experimental)**: \nThe Multimodal Adapter is designed to integrate different large multimodal models by unifying their APIs and request protocols. GPTCache offers a standardized interface for this purpose, with current support for integrations of image generation, audio transcription.\n  - [x] Support OpenAI Image Create API.\n  - [x] Support OpenAI Audio Transcribe API.\n  - [x] Support Replicate BLIP API.\n  - [x] Support Stability Inference API.\n  - [x] Support Hugging Face Stable Diffusion Pipeline (local inference).\n  - [ ] Support other multimodal services or self-hosted large multimodal models.\n- **Embedding Generator**: \nThis module is created to extract embeddings from requests for similarity search. GPTCache offers a generic interface that supports multiple embedding APIs, and presents a range of solutions to choose from. \n  - [x] Disable embedding. This will turn GPTCache into a keyword-matching cache.\n  - [x] Support OpenAI embedding API.\n  - [x] Support [ONNX](https://onnx.ai/) with the GPTCache/paraphrase-albert-onnx model.\n  - [x] Support [Hugging Face](https://huggingface.co/) embedding with transformers, ViTModel, Data2VecAudio.\n  - [x] Support [Cohere](https://docs.cohere.ai/reference/embed) embedding API.\n  - [x] Support [fastText](https://fasttext.cc) embedding.\n  - [x] Support [SentenceTransformers](https://www.sbert.net) embedding.\n  - [x] Support [Timm](https://timm.fast.ai/) models for image embedding.\n  - [ ] Support other embedding APIs.\n- **Cache Storage**:\n**Cache Storage** is where the response from LLMs, such as ChatGPT, is stored. Cached responses are retrieved to assist in evaluating similarity and are returned to the requester if there is a good semantic match. At present, GPTCache supports SQLite and offers a universally accessible interface for extension of this module.\n  - [x] Support [SQLite](https://sqlite.org/docs.html).\n  - [x] Support [DuckDB](https://duckdb.org/).\n  - [x] Support [PostgreSQL](https://www.postgresql.org/).\n  - [x] Support [MySQL](https://www.mysql.com/).\n  - [x] Support [MariaDB](https://mariadb.org/).\n  - [x] Support [SQL Server](https://www.microsoft.com/en-us/sql-server/).\n  - [x] Support [Oracle](https://www.oracle.com/).\n  - [x] Support [DynamoDB](https://aws.amazon.com/dynamodb/).\n  - [ ] Support [MongoDB](https://www.mongodb.com/).\n  - [ ] Support [Redis](https://redis.io/).\n  - [ ] Support [Minio](https://min.io/).\n  - [ ] Support [HBase](https://hbase.apache.org/).\n  - [ ] Support [ElasticSearch](https://www.elastic.co/).\n  - [ ] Support other storages.\n- **Vector Store**:\nThe **Vector Store** module helps find the K most similar requests from the input request's extracted embedding. The results can help assess similarity. GPTCache provides a user-friendly interface that supports various vector stores, including Milvus, Zilliz Cloud, and FAISS. More options will be available in the future.\n  - [x] Support [Milvus](https://milvus.io/), an open-source vector database for production-ready AI/LLM applicaionts. \n  - [x] Support [Zilliz Cloud](https://cloud.zilliz.com/), a fully-managed cloud vector database based on Milvus.\n  - [x] Support [Milvus Lite](https://github.com/milvus-io/milvus-lite), a lightweight version of Milvus that can be embedded into your Python application.\n  - [x] Support [FAISS](https://faiss.ai/), a library for efficient similarity search and clustering of dense vectors.\n  - [x] Support [Hnswlib](https://github.com/nmslib/hnswlib), header-only C++/python library for fast approximate nearest neighbors.\n  - [x] Support [PGVector](https://github.com/pgvector/pgvector), open-source vector similarity search for Postgres.\n  - [x] Support [Chroma](https://github.com/chroma-core/chroma), the AI-native open-source embedding database.\n  - [x] Support [DocArray](https://github.com/docarray/docarray), DocArray is a library for representing, sending and storing multi-modal data, perfect for Machine Learning applications.\n  - [x] Support qdrant\n  - [x] Support weaviate\n  - [ ] Support other vector databases.\n- **Cache Manager**:\nThe **Cache Manager** is responsible for controlling the operation of both the **Cache Storage** and **Vector Store**.\n  - **Eviction Policy**:\n  Cache eviction can be managed in memory using python's `cachetools` or in a distributed fashion using Redis as a key-value store. \n  - **In-Memory Caching**\n  \n  Currently, GPTCache makes decisions about evictions based solely on the number of lines. This approach can result in inaccurate resource evaluation and may cause out-of-memory (OOM) errors. We are actively investigating and developing a more sophisticated strategy.\n    - [x] Support LRU eviction policy.\n    - [x] Support FIFO eviction policy.\n    - [x] Support LFU eviction policy.\n    - [x] Support RR eviction policy.\n    - [ ] Support more complicated eviction policies.\n  - **Distributed Caching**\n  \n  If you were to scale your GPTCache deployment horizontally using in-memory caching, it won't be possible. Since the cached information would be limited to the single pod.\n  \n  With Distributed Caching, cache information consistent across all replicas we can use Distributed Cache stores like Redis. \n    - [x] Support Redis distributed cache\n    - [x] Support memcached distributed cache\n  \n- **Similarity Evaluator**: \nThis module collects data from both the **Cache Storage** and **Vector Store**, and uses various strategies to determine the similarity between the input request and the requests from the **Vector Store**. Based on this similarity, it determines whether a request matches the cache. GPTCache provides a standardized interface for integrating various strategies, along with a collection of implementations to use. The following similarity definitions are currently supported or will be supported in the future:\n  - [x] The distance we obtain from the **Vector Store**.\n  - [x] A model-based similarity determined using the GPTCache/albert-duplicate-onnx model from [ONNX](https://onnx.ai/).\n  - [x] Exact matches between the input request and the requests obtained from the **Vector Store**.\n  - [x] Distance represented by applying linalg.norm from numpy to the embeddings.\n  - [ ] BM25 and other similarity measurements.\n  - [ ] Support other model serving framework such as PyTorch.\n \n  \n  **Note**:Not all combinations of different modules may be compatible with each other. For instance, if we disable the **Embedding Extractor**, the **Vector Store** may not function as intended. We are currently working on implementing a combination sanity check for **GPTCache**.\n\n## 😇 Roadmap\nComing soon! [Stay tuned!](https://twitter.com/zilliz_universe)\n\n## 😍 Contributing\nWe are extremely open to contributions, be it through new features, enhanced infrastructure, or improved documentation.\n\nFor comprehensive instructions on how to contribute, please refer to our [contribution guide](docs/contributing.md).\n"
        },
        {
          "name": "cache_config_template.yml",
          "type": "blob",
          "size": 0.5625,
          "content": "# For `model_src`, `evaluation`, `post_function`, `pre_function`,\n# `storage_config` options, Check README for more.\n\nembedding:\n    onnx\nembedding_config:\n    # Set model kws here including `model`, `api_key` if needed\nstorage_config:\n    data_dir:\n        gptcache_data\n    manager:\n        sqlite,faiss\n    vector_params:\n        # Set vector storage related params here\nevaluation: \n    distance\nevaluation_config:\n    # Set evaluation metric kws here\npre_function:\n    get_prompt\npost_function:\n    first\nconfig:\n    similarity_threshold: 0.8\n    # Set other config here\n"
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.71875,
          "content": "# Configuration File for CodeCov\ncodecov:\n  require_ci_to_pass: no\n  notify:\n    require_ci_to_pass: no\n    wait_for_ci: false\n\ncoverage:\n  precision: 2\n  round: down\n  range: \"70...100\"\n\n  status:\n    project:\n      default:\n        target: 90%\n        threshold: 0% #Allow the coverage to drop by threshold%, and posting a success status.\n    patch:\n      default:\n        target: 90%   #target of patch diff\n        threshold: 0%\n        if_ci_failed: error #success, failure, error, ignore\n\ncomment:\n  layout: \"reach, diff, flags, files\"\n  behavior: default\n  require_changes: false\n  branches: # branch names that can post comment\n    - main\n    - dev\n\nignore:\n  - \"LICENSES\"\n  - \".git\"\n  - \"*.yml\"\n  - \"*.md\"\n  - \"**/minigpt4.py\"\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "gptcache",
          "type": "tree",
          "content": null
        },
        {
          "name": "gptcache_server",
          "type": "tree",
          "content": null
        },
        {
          "name": "pylint.conf",
          "type": "blob",
          "size": 14.3173828125,
          "content": "# This Pylint rcfile contains a best-effort configuration to uphold the\n# best-practices and style described in the Google Python style guide:\n#   https://google.github.io/styleguide/pyguide.html\n#\n# Its canonical open-source location is:\n#   https://google.github.io/styleguide/pylintrc\n\n[MASTER]\n\n# Files or directories to be skipped. They should be base names, not paths.\nignore=third_party\n\n# Files or directories matching the regex patterns are skipped. The regex\n# matches against base names, not paths.\nignore-patterns=\n\n# Pickle collected data for later comparisons.\npersistent=no\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Use multiple processes to speed up Pylint.\njobs=4\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED\nconfidence=\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).You can also use \"--disable=all\" to\n# disable everything first and then reenable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use\"--disable=all --enable=classes\n# --disable=W\"\ndisable=abstract-method,\n        apply-builtin,\n        arguments-differ,\n        attribute-defined-outside-init,\n        backtick,\n        bad-option-value,\n        basestring-builtin,\n        buffer-builtin,\n        c-extension-no-member,\n        consider-using-enumerate,\n        cmp-builtin,\n        cmp-method,\n        coerce-builtin,\n        coerce-method,\n        delslice-method,\n        div-method,\n        duplicate-code,\n        eq-without-hash,\n        execfile-builtin,\n        file-builtin,\n        filter-builtin-not-iterating,\n        fixme,\n        getslice-method,\n        global-statement,\n        hex-method,\n        idiv-method,\n        implicit-str-concat-in-sequence,\n        import-error,\n        import-self,\n        import-star-module-level,\n        inconsistent-return-statements,\n        input-builtin,\n        intern-builtin,\n        invalid-str-codec,\n        locally-disabled,\n        long-builtin,\n        long-suffix,\n        map-builtin-not-iterating,\n        misplaced-comparison-constant,\n        missing-function-docstring,\n        metaclass-assignment,\n        next-method-called,\n        next-method-defined,\n        no-absolute-import,\n        no-else-break,\n        no-else-continue,\n        no-else-raise,\n        no-else-return,\n        no-init,  # added\n        no-member,\n        no-name-in-module,\n        no-self-use,\n        nonzero-method,\n        oct-method,\n        old-division,\n        old-ne-operator,\n        old-octal-literal,\n        old-raise-syntax,\n        parameter-unpacking,\n        print-statement,\n        raising-string,\n        range-builtin-not-iterating,\n        raw_input-builtin,\n        rdiv-method,\n        reduce-builtin,\n        relative-import,\n        reload-builtin,\n        round-builtin,\n        setslice-method,\n        signature-differs,\n        standarderror-builtin,\n        suppressed-message,\n        sys-max-int,\n        too-few-public-methods,\n        too-many-ancestors,\n        too-many-arguments,\n        too-many-boolean-expressions,\n        too-many-branches,\n        too-many-instance-attributes,\n        too-many-locals,\n        too-many-nested-blocks,\n        too-many-public-methods,\n        too-many-return-statements,\n        too-many-statements,\n        trailing-newlines,\n        unichr-builtin,\n        unicode-builtin,\n        unnecessary-pass,\n        unpacking-in-except,\n        useless-else-on-loop,\n        useless-object-inheritance,\n        useless-suppression,\n        using-cmp-argument,\n        wrong-import-order,\n        xrange-builtin,\n        zip-builtin-not-iterating,\n        missing-module-docstring,\n        super-init-not-called,\n        wrong-import-position\n\n\n[REPORTS]\n\n# Set the output format. Available formats are text, parseable, colorized, msvs\n# (visual studio) and html. You can also give a reporter class, eg\n# mypackage.mymodule.MyReporterClass.\noutput-format=text\n\n# Put messages in a separate file for each module / package specified on the\n# command line instead of printing them on stdout. Reports (if any) will be\n# written in a file name \"pylint_global.[txt|html]\". This option is deprecated\n# and it will be removed in Pylint 2.0.\nfiles-output=no\n\n# Tells whether to display a full report or only the messages\nreports=no\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\n#msg-template=\n\n\n[BASIC]\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=main,_\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Include a hint for the correct naming format with invalid-name\ninclude-naming-hint=no\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\nproperty-classes=abc.abstractproperty,cached_property.cached_property,cached_property.threaded_cached_property,cached_property.cached_property_with_ttl,cached_property.threaded_cached_property_with_ttl\n\n# Regular expression matching correct function names\nfunction-rgx=^(?:(?P<exempt>setUp|tearDown|setUpModule|tearDownModule)|(?P<camel_case>_?[A-Z][a-zA-Z0-9]*)|(?P<snake_case>_?[a-z][a-z0-9_]*))$\n\n# Regular expression matching correct variable names\nvariable-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct constant names\nconst-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct attribute names\nattr-rgx=^_{0,2}[a-z][a-z0-9_]*$\n\n# Regular expression matching correct argument names\nargument-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class attribute names\nclass-attribute-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct inline iteration names\ninlinevar-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class names\nclass-rgx=^_?[A-Z][a-zA-Z0-9]*$\n\n# Regular expression matching correct module names\nmodule-rgx=^(_?[a-z][a-z0-9_]*|__init__|__main__)$\n\n# Regular expression matching correct method names\nmethod-rgx=(?x)^(?:(?P<exempt>_[a-z0-9_]+__|runTest|setUp|tearDown|setUpTestCase|tearDownTestCase|setupSelf|tearDownClass|setUpClass|(test|assert)_*[A-Z0-9][a-zA-Z0-9_]*|next)|(?P<camel_case>_{0,2}[A-Z][a-zA-Z0-9_]*)|(?P<snake_case>_{0,2}[a-z][a-z0-9_]*))$\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=(__.*__|main|test.*|.*test|.*Test)$\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=10\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager,contextlib2.contextmanager\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis. It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n\n[FORMAT]\n\n# Maximum number of characters on a single line.\nmax-line-length=150\n\n# TODO(https://github.com/PyCQA/pylint/issues/3352): Direct pylint to exempt\n# lines made too long by directives to pytype.\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=(?x)(\n  ^\\s*(\\#\\ )?<?https?://\\S+>?$|\n  ^\\s*(from\\s+\\S+\\s+)?import\\s+.+$)\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=yes\n\n# List of optional constructs for which whitespace checking is disabled. `dict-\n# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\\n222: 2}.\n# `trailing-comma` allows a space between comma and closing bracket: (a, ).\n# `empty-line` allows space-only lines.\nno-space-check=\n\n# Maximum number of lines in a module\nmax-module-lines=99999\n\n# String used as indentation unit.  The internal Google style guide mandates 2\n# spaces.  Google's externaly-published style guide says 4, consistent with\n# PEP 8.  Here, we use 2 spaces, for conformity with many open-sourced Google\n# projects (like TensorFlow).\nindent-string='    '\n\n# Number of spaces of indent required inside a hanging  or continued line.\nindent-after-paren=4\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=TODO\n\n\n[STRING]\n\n# This flag controls whether inconsistent-quotes generates a warning when the\n# character used as a quote delimiter is used inconsistently within a module.\ncheck-quote-consistency=yes\n\n\n[VARIABLES]\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# A regular expression matching the name of dummy variables (i.e. expectedly\n# not used).\ndummy-variables-rgx=^\\*{0,2}(_$|unused_|dummy_)\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,_cb\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six,six.moves,past.builtins,future.builtins,functools\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format\nlogging-modules=logging,absl.logging,tensorflow.io.logging\n\n\n[SIMILARITIES]\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n\n[SPELLING]\n\n# Spelling dictionary name. Available dictionaries: none. To make it working\n# install python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[IMPORTS]\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=regsub,\n                   TERMIOS,\n                   Bastion,\n                   rexec,\n                   sets\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant, absl\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,\n                  _fields,\n                  _replace,\n                  _source,\n                  _make\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls,\n                            class_\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=StandardError,\n                       Exception,\n                       BaseException\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0244140625,
          "content": "numpy\ncachetools\nrequests"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.6923828125,
          "content": "import codecs\nimport os\nimport re\nfrom typing import List\n\nimport setuptools\nfrom setuptools import find_packages\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\n\ndef parse_requirements(file_name: str) -> List[str]:\n    with open(file_name) as f:\n        return [\n            require.strip() for require in f\n            if require.strip() and not require.startswith('#')\n        ]\n\n\ndef read(*parts):\n    with codecs.open(os.path.join(here, *parts), \"r\") as fp:\n        return fp.read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(\"Unable to find version string.\")\n\n\nsetuptools.setup(\n    name=\"gptcache\",\n    packages=find_packages(),\n    version=find_version(\"gptcache\", \"__init__.py\"),\n    author=\"SimFG\",\n    author_email=\"bang.fu@zilliz.com\",\n    description=\"GPTCache, a powerful caching library that can be used to speed up and lower the cost of chat \"\n                \"applications that rely on the LLM service. GPTCache works as a memcache for AIGC applications, \"\n                \"similar to how Redis works for traditional applications.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    install_requires=parse_requirements('requirements.txt'),\n    url=\"https://github.com/zilliztech/GPTCache\",\n    license='https://opensource.org/license/mit/',\n    python_requires='>=3.8.1',\n    entry_points={\n        'console_scripts': [\n            'gptcache_server=gptcache_server.server:main',\n        ],\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}