{
  "metadata": {
    "timestamp": 1736561069242,
    "page": 859,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pytorch/captum",
      "stars": 5025,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".conda",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0458984375,
          "content": "tutorials/* linguist-documentation\n*.pt binary\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.6083984375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# Downloaded data\ndata/\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\ncaptum/insights/frontend/node_modules/\ncaptum/insights/frontend/.pnp/\ncaptum/insights/frontend/build/\ncaptum/insights/widget/static/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n\n*.egg-info/\n.installed.cfg\n*.egg\n\n# watchman\n# Watchman helper\n.watchmanconfig\n\n# OSX\n*.DS_Store\n\n# Atom plugin files and ctags\n.ftpconfig\n.ftpconfig.cson\n.ftpignore\n*.tags\n*.tags1\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\ncaptum/insights/frontend/npm-debug.log*\ncaptum/insights/frontend/yarn-debug.log*\ncaptum/insights/frontend/yarn-error.log*\n\n# Unit test / coverage reports\nhtmlcov/\ncaptum/insights/frontend/coverage\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# mypy\n.mypy_cache/\n\n# vim\n*.swp\n\n# Sphinx documentation\nsphinx/build/\n\n# Docusaurus\nwebsite/build/\nwebsite/i18n/\nwebsite/node_modules/\n\n## Generated for tutorials\nwebsite/_tutorials/\nwebsite/static/files/\nwebsite/pages/tutorials/*\n!website/pages/tutorials/index.js\n\n## Generated for Sphinx\nwebsite/pages/api/\nwebsite/static/_sphinx/\n\n# Insight\ncaptum/insights/attr_vis/frontend/node_modules/\ncaptum/insights/attr_vis/widget/static\n"
        },
        {
          "name": "AWESOME_LIST.md",
          "type": "blob",
          "size": 1.8935546875,
          "content": "# Awesome List\n\nThere is a lot of awesome research and development happening out in the interpretability community that we would like to share.  Here we will maintain a curated list of research, implementations and resources.  We would love to learn about more!  Please feel free to make a pull request to contribute to the list.\n\n\n#### TorchRay: Visualization methods for deep CNNs\nTorchRay focuses on attribution, namely the problem of determining which part of the input, usually an image, is responsible for the value computed by a neural network.\n  - [https://github.com/facebookresearch/TorchRay](https://github.com/facebookresearch/TorchRay)\n\n\n#### Score Cam: A gradient-free CAM extension\nScore-CAM is a gradient-free visualization method extended from Grad-CAM and Grad-CAM++.  It provides score-weighted visual explanations for CNNs.\n  - [Paper](https://arxiv.org/abs/1910.01279)\n  - [https://github.com/haofanwang/Score-CAM](https://github.com/haofanwang/Score-CAM)\n\n\n#### White Noise Analysis\nWhite noise stimuli is fed to a classifier and the ones that are categorized into a particular class are averaged. It gives an estimate of the templates a classifier uses for classification, and is based on two popular and related methods in psychophysics and neurophysiology namely classification images and spike triggered analysis.\n- [Paper](https://arxiv.org/abs/1912.12106)\n- [https://github.com/aliborji/WhiteNoiseAnalysis.git](https://github.com/aliborji/WhiteNoiseAnalysis.git)\n\n\n#### FastCAM: Multiscale Saliency Map with SMOE scale\nAn attribution method that uses information at the end of each network scale which is then combined into a single saliency map. \n- [Paper](https://arxiv.org/abs/1911.11293)\n- [https://github.com/LLNL/fastcam](https://github.com/LLNL/fastcam)\n- [pull request](https://github.com/pytorch/captum/pull/442)\n- [jupyter notebook demo](https://github.com/LLNL/fastcam/blob/captum/demo-captum.ipynb)\n"
        },
        {
          "name": "CITATION",
          "type": "blob",
          "size": 0.4306640625,
          "content": "@misc{kokhlikyan2020captum,\n    title={Captum: A unified and generic model interpretability library for PyTorch},\n    author={Narine Kokhlikyan and Vivek Miglani and Miguel Martin and Edward Wang and Bilal Alsallakh and Jonathan Reynolds and Alexander Melnikov and Natalia Kliushkina and Carlos Araya and Siqi Yan and Orion Reblitz-Richardson},\n    year={2020},\n    eprint={2009.07896},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.2646484375,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <conduct@pytorch.org>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5.630859375,
          "content": "# Contributing to Captum\n\nThank you for your interest in contributing to Captum! We want to make contributing to Captum as easy and transparent as possible.\nBefore you begin writing code, it is important that you share your intention to contribute with the team, based on the type of contribution:\n\n\n1. You want to propose and implement a new algorithm, add a new feature or fix a bug. This can be both code and documentation proposals.\n    1. For all non-outstanding features, bug-fixes and algorithms in the Captum issue list (https://github.com/pytorch/captum/issues) please create an issue first.\n    2. If the implementation requires API or any other major code changes (new files, packages or algorithms), we will likely request a design document to review and discuss the design and implementation before making changes. An example design document for LIME can be found here (https://github.com/pytorch/captum/issues/467).\n    3. Once we agree that the plan looks good or confirmed that the change is small enough to not require a detailed design discussion, go ahead and implement it!\n\n2. You want to implement a feature or bug-fix for an outstanding issue.\n\n    1. Search for your issue in the Captum issue list (https://github.com/pytorch/captum/issues).\n    2. Pick an issue and comment that you'd like to work on the feature or bug-fix.\n    3. If you need more context on a particular issue, please ask and we’ll be happy to help.\n\nOnce you implement and test your feature or bug-fix, please submit a Pull Request to https://github.com/pytorch/captum (https://github.com/pytorch/pytorch).\n\nThis document covers some of the techical aspects of contributing to Captum. More details on what we are looking for in the contributions can be found in the [Contributing Guidelines](https://captum.ai/docs/contribution_guidelines).\n\n\n## Development installation\n\nTo get the development installation with all the necessary dependencies for\nlinting, testing, and building the documentation, run the following:\n```bash\ngit clone https://github.com/pytorch/captum.git\ncd captum\npip install -e .[dev]\n```\n\n\n## Our Development Process\n\n#### Code Style\n\nCaptum uses [ufmt](https://pypi.org/project/ufmt/) and  [flake8](https://github.com/PyCQA/flake8) to\nenforce a common code style across the code base. ufmt and flake8 are installed easily via\npip using `pip install ufmt flake8`, and run locally by calling\n```bash\nufmt format .\nflake8 .\n```\nfrom the repository root.\n\nWe feel strongly that having a consistent code style is extremely important, so\nGithub Actions will fail on your PR if it does not adhere to the ufmt or flake8 formatting style.\n\n\n#### Type Hints\n\nCaptum is fully typed using python 3.6+\n[type hints](https://www.python.org/dev/peps/pep-0484/).\nWe expect any contributions to also use proper type annotations, and we enforce\nconsistency of these in our continuous integration tests.\n\nTo type check your code locally, install [mypy](https://github.com/python/mypy),\nwhich can be done with pip using `pip install \"mypy>=0.760\"`\nThen run this script from the repository root:\n```bash\n./scripts/run_mypy.sh\n```\nNote that we expect mypy to have version 0.760 or higher, and when type checking, use PyTorch 1.4 or\nhigher due to fixes to PyTorch type hints available in 1.4. We also use the Literal feature which is\navailable only in Python 3.9 or above. If type-checking using a previous version of Python, you will\nneed to install the typing-extension package which can be done with pip using `pip install typing-extensions`.\n\n#### Unit Tests\n\nTo run the unit tests, you can either use `pytest` (if installed):\n```bash\npytest -ra\n```\nor python's `unittest`:\n```bash\npython -m unittest\n```\n\nTo get coverage reports we recommend using the `pytest-cov` plugin:\n```bash\npytest -ra --cov=. --cov-report term-missing\n```\n\n\n#### Documentation\n\nCaptum's website is also open source, and is part of this very repository (the\ncode can be found in the [website](/website/) folder).\nIt is built using [Docusaurus](https://docusaurus.io/), and consists of three\nmain elements:\n\n1. The documentation in Docusaurus itself (if you know Markdown, you can\n   already contribute!). This lives in the [docs](/docs/).\n2. The API reference, auto-generated from the docstrings using\n   [Sphinx](http://www.sphinx-doc.org), and embedded into the Docusaurus website.\n   The sphinx .rst source files for this live in [sphinx/source](/sphinx/source/).\n3. The Jupyter notebook tutorials, parsed by `nbconvert`, and embedded into the\n   Docusaurus website. These live in [tutorials](/tutorials/).\n\nTo build the documentation you will need [Node](https://nodejs.org/en/) >= 8.x\nand [Yarn](https://yarnpkg.com/en/) >= 1.5.\n\nThe following command will both build the docs and serve the site locally:\n```bash\n./scripts/build_docs.sh\n```\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. If you have added code that should be tested, add unit tests.\n   In other words, add unit tests.\n3. If you have changed APIs, update the documentation. Make sure the\n   documentation builds.\n4. Ensure the test suite passes.\n5. Make sure your code passes both `black` and `flake8` formatting checks.\n\n\n## Issues\n\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n\n## License\n\nBy contributing to Captum, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4765625,
          "content": "BSD 3-Clause License\n\nCopyright (c) 2019, PyTorch team\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 23.3681640625,
          "content": "![Captum Logo](./website/static/img/captum_logo.png)\n\n<hr/>\n\n<!--- BADGES: START --->\n[![GitHub - License](https://img.shields.io/github/license/pytorch/captum?logo=github&style=flat&color=green)][#github-license]\n[![Conda](https://img.shields.io/conda/vn/pytorch/captum?logo=anaconda&style=flat&color=orange)](https://anaconda.org/pytorch/captum)\n[![PyPI](https://img.shields.io/pypi/v/captum.svg)][#pypi-package]\n[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/captum?logo=anaconda&style=flat)][#conda-forge-package]\n[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/captum?logo=anaconda&style=flat&color=orange)][#conda-forge-package]\n[![Conda Recipe](https://img.shields.io/static/v1?logo=conda-forge&style=flat&color=green&label=recipe&message=captum)][#conda-forge-feedstock]\n[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=captum&style=flat&color=pink&label=docs&message=captum)][#docs-package]\n\n[#github-license]: https://github.com/pytorch/captum/blob/master/LICENSE\n[#pypi-package]: https://pypi.org/project/captum/\n[#conda-forge-package]: https://anaconda.org/conda-forge/captum\n[#conda-forge-feedstock]: https://github.com/conda-forge/captum-feedstock\n[#docs-package]: https://captum.ai/\n<!--- BADGES: END --->\n\n\nCaptum is a model interpretability and understanding library for PyTorch.\nCaptum means comprehension in Latin and contains general purpose implementations\nof integrated gradients, saliency maps, smoothgrad, vargrad and others for\nPyTorch models. It has quick integration for models built with domain-specific\nlibraries such as torchvision, torchtext, and others.\n\n*Captum is currently in beta and under active development!*\n\n\n#### About Captum\n\nWith the increase in model complexity and the resulting lack of transparency, model interpretability methods have become increasingly important. Model understanding is both an active area of research as well as an area of focus for practical applications across industries using machine learning. Captum provides state-of-the-art algorithms such as Integrated Gradients, Testing with Concept Activation Vectors (TCAV), TracIn influence functions, just to name a few, that provide researchers and developers with an easy way to understand which features, training examples or concepts contribute to a models' predictions and in general what and how the model learns. In addition to that, Captum also provides adversarial attacks and minimal input perturbation capabilities that can be used both for generating counterfactual explanations and adversarial perturbations.\n\n<!--For model developers, Captum can be used to improve and troubleshoot models by facilitating the identification of different features that contribute to a model’s output in order to design better models and troubleshoot unexpected model outputs. -->\n\nCaptum helps ML researchers more easily implement interpretability algorithms that can interact with PyTorch models. Captum also allows researchers to quickly benchmark their work against other existing algorithms available in the library.\n\n![Overview of Attribution Algorithms](./docs/Captum_Attribution_Algos.png)\n\n#### Target Audience\n\nThe primary audiences for Captum are model developers who are looking to improve their models and understand which concepts, features or training examples are important and interpretability researchers focused on identifying algorithms that can better interpret many types of models.\n\nCaptum can also be used by application engineers who are using trained models in production. Captum provides easier troubleshooting through improved model interpretability, and the potential for delivering better explanations to end users on why they’re seeing a specific piece of content, such as a movie recommendation.\n\n## Installation\n\n**Installation Requirements**\n- Python >= 3.9\n- PyTorch >= 1.10\n\n\n##### Installing the latest release\n\nThe latest release of Captum is easily installed either via\n[Anaconda](https://www.anaconda.com/distribution/#download-section) (recommended) or via `pip`.\n\n**with `conda`**\n\nYou can install captum from any of the following supported conda channels:\n\n- channel: `pytorch`\n\n  ```sh\n  conda install captum -c pytorch\n  ```\n\n- channel: `conda-forge`\n\n  ```sh\n  conda install captum -c conda-forge\n  ```\n\n**With `pip`**\n\n```bash\npip install captum\n```\n\n**Manual / Dev install**\n\nIf you'd like to try our bleeding edge features (and don't mind potentially\nrunning into the occasional bug here or there), you can install the latest\nmaster directly from GitHub. For a basic install, run:\n```bash\ngit clone https://github.com/pytorch/captum.git\ncd captum\npip install -e .\n```\n\nTo customize the installation, you can also run the following variants of the\nabove:\n* `pip install -e .[insights]`: Also installs all packages necessary for running Captum Insights.\n* `pip install -e .[dev]`: Also installs all tools necessary for development\n  (testing, linting, docs building; see [Contributing](#contributing) below).\n* `pip install -e .[tutorials]`: Also installs all packages necessary for running the tutorial notebooks.\n\nTo execute unit tests from a manual install, run:\n```bash\n# running a single unit test\npython -m unittest -v tests.attr.test_saliency\n# running all unit tests\npytest -ra\n```\n\n## Getting Started\nCaptum helps you interpret and understand predictions of PyTorch models by\nexploring features that contribute to a prediction the model makes.\nIt also helps understand which neurons and layers are important for\nmodel predictions.\n\nLet's apply some of those algorithms to a toy model we have created for\ndemonstration purposes.\nFor simplicity, we will use the following architecture, but users are welcome\nto use any PyTorch model of their choice.\n\n\n```python\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nfrom captum.attr import (\n    GradientShap,\n    DeepLift,\n    DeepLiftShap,\n    IntegratedGradients,\n    LayerConductance,\n    NeuronConductance,\n    NoiseTunnel,\n)\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin1 = nn.Linear(3, 3)\n        self.relu = nn.ReLU()\n        self.lin2 = nn.Linear(3, 2)\n\n        # initialize weights and biases\n        self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))\n        self.lin1.bias = nn.Parameter(torch.zeros(1,3))\n        self.lin2.weight = nn.Parameter(torch.arange(-3.0, 3.0).view(2, 3))\n        self.lin2.bias = nn.Parameter(torch.ones(1,2))\n\n    def forward(self, input):\n        return self.lin2(self.relu(self.lin1(input)))\n```\n\nLet's create an instance of our model and set it to eval mode.\n```python\nmodel = ToyModel()\nmodel.eval()\n```\n\nNext, we need to define simple input and baseline tensors.\nBaselines belong to the input space and often carry no predictive signal.\nZero tensor can serve as a baseline for many tasks.\nSome interpretability algorithms such as `IntegratedGradients`, `Deeplift` and `GradientShap` are designed to attribute the change\nbetween the input and baseline to a predictive class or a value that the neural\nnetwork outputs.\n\nWe will apply model interpretability algorithms on the network\nmentioned above in order to understand the importance of individual\nneurons/layers and the parts of the input that play an important role in the\nfinal prediction.\n\nTo make computations deterministic, let's fix random seeds.\n\n```python\ntorch.manual_seed(123)\nnp.random.seed(123)\n```\n\nLet's define our input and baseline tensors. Baselines are used in some\ninterpretability algorithms such as `IntegratedGradients, DeepLift,\nGradientShap, NeuronConductance, LayerConductance, InternalInfluence` and\n`NeuronIntegratedGradients`.\n\n```python\ninput = torch.rand(2, 3)\nbaseline = torch.zeros(2, 3)\n```\nNext we will use `IntegratedGradients` algorithms to assign attribution\nscores to each input feature with respect to the first target output.\n```python\nig = IntegratedGradients(model)\nattributions, delta = ig.attribute(input, baseline, target=0, return_convergence_delta=True)\nprint('IG Attributions:', attributions)\nprint('Convergence Delta:', delta)\n```\nOutput:\n```\nIG Attributions: tensor([[-0.5922, -1.5497, -1.0067],\n                         [ 0.0000, -0.2219, -5.1991]])\nConvergence Delta: tensor([2.3842e-07, -4.7684e-07])\n```\nThe algorithm outputs an attribution score for each input element and a\nconvergence delta. The lower the absolute value of the convergence delta the better\nis the approximation. If we choose not to return delta,\nwe can simply not provide the `return_convergence_delta` input\nargument. The absolute value of the returned deltas can be interpreted as an\napproximation error for each input sample.\nIt can also serve as a proxy of how accurate the integral approximation for given\ninputs and baselines is.\nIf the approximation error is large, we can try a larger number of integral\napproximation steps by setting `n_steps` to a larger value. Not all algorithms\nreturn approximation error. Those which do, though, compute it based on the\ncompleteness property of the algorithms.\n\nPositive attribution score means that the input in that particular position\npositively contributed to the final prediction and negative means the opposite.\nThe magnitude of the attribution score signifies the strength of the contribution.\nZero attribution score means no contribution from that particular feature.\n\nSimilarly, we can apply `GradientShap`, `DeepLift` and other attribution algorithms to the model.\n\n`GradientShap` first chooses a random baseline from baselines' distribution, then\n adds gaussian noise with std=0.09 to each input example `n_samples` times.\nAfterwards, it chooses a random point between each example-baseline pair and\ncomputes the gradients with respect to target class (in this case target=0). Resulting\nattribution is the mean of gradients * (inputs - baselines)\n```python\ngs = GradientShap(model)\n\n# We define a distribution of baselines and draw `n_samples` from that\n# distribution in order to estimate the expectations of gradients across all baselines\nbaseline_dist = torch.randn(10, 3) * 0.001\nattributions, delta = gs.attribute(input, stdevs=0.09, n_samples=4, baselines=baseline_dist,\n                                   target=0, return_convergence_delta=True)\nprint('GradientShap Attributions:', attributions)\nprint('Convergence Delta:', delta)\n```\nOutput\n```\nGradientShap Attributions: tensor([[-0.1542, -1.6229, -1.5835],\n                                   [-0.3916, -0.2836, -4.6851]])\nConvergence Delta: tensor([ 0.0000, -0.0005, -0.0029, -0.0084, -0.0087, -0.0405,  0.0000, -0.0084])\n\n```\nDeltas are computed for each `n_samples * input.shape[0]` example. The user can,\nfor instance, average them:\n```python\ndeltas_per_example = torch.mean(delta.reshape(input.shape[0], -1), dim=1)\n```\nin order to get per example average delta.\n\n\nBelow is an example of how we can apply `DeepLift` and `DeepLiftShap` on the\n`ToyModel` described above. The current implementation of DeepLift supports only the\n`Rescale` rule.\nFor more details on alternative implementations, please see the [DeepLift paper](https://arxiv.org/abs/1704.02685).\n\n```python\ndl = DeepLift(model)\nattributions, delta = dl.attribute(input, baseline, target=0, return_convergence_delta=True)\nprint('DeepLift Attributions:', attributions)\nprint('Convergence Delta:', delta)\n```\nOutput\n```\nDeepLift Attributions: tensor([[-0.5922, -1.5497, -1.0067],\n                               [ 0.0000, -0.2219, -5.1991])\nConvergence Delta: tensor([0., 0.])\n```\n`DeepLift` assigns similar attribution scores as `IntegratedGradients` to inputs,\nhowever it has lower execution time. Another important thing to remember about\nDeepLift is that it currently doesn't support all non-linear activation types.\nFor more details on limitations of the current implementation, please see the\n[DeepLift paper](https://arxiv.org/abs/1704.02685).\n\nSimilar to integrated gradients, DeepLift returns a convergence delta score\nper input example. The approximation error is then the absolute\nvalue of the convergence deltas and can serve as a proxy of how accurate the\nalgorithm's approximation is.\n\nNow let's look into `DeepLiftShap`. Similar to `GradientShap`, `DeepLiftShap` uses\nbaseline distribution. In the example below, we use the same baseline distribution\nas for `GradientShap`.\n\n```python\ndl = DeepLiftShap(model)\nattributions, delta = dl.attribute(input, baseline_dist, target=0, return_convergence_delta=True)\nprint('DeepLiftSHAP Attributions:', attributions)\nprint('Convergence Delta:', delta)\n```\nOutput\n```\nDeepLiftShap Attributions: tensor([[-5.9169e-01, -1.5491e+00, -1.0076e+00],\n                                   [-4.7101e-03, -2.2300e-01, -5.1926e+00]], grad_fn=<MeanBackward1>)\nConvergence Delta: tensor([-4.6120e-03, -1.6267e-03, -5.1045e-04, -1.4184e-03, -6.8886e-03,\n                           -2.2224e-02,  0.0000e+00, -2.8790e-02, -4.1285e-03, -2.7295e-02,\n                           -3.2349e-03, -1.6265e-03, -4.7684e-07, -1.4191e-03, -6.8889e-03,\n                           -2.2224e-02,  0.0000e+00, -2.4792e-02, -4.1289e-03, -2.7296e-02])\n```\n`DeepLiftShap` uses `DeepLift` to compute attribution score for each\ninput-baseline pair and averages it for each input across all baselines.\n\nIt computes deltas for each input example-baseline pair, thus resulting to\n`input.shape[0] * baseline.shape[0]` delta values.\n\nSimilar to GradientShap in order to compute example-based deltas we can average them per example:\n```python\ndeltas_per_example = torch.mean(delta.reshape(input.shape[0], -1), dim=1)\n```\nIn order to smooth and improve the quality of the attributions we can run\n`IntegratedGradients` and other attribution methods through a `NoiseTunnel`.\n`NoiseTunnel` allows us to use `SmoothGrad`, `SmoothGrad_Sq` and `VarGrad` techniques\nto smoothen the attributions by aggregating them for multiple noisy\nsamples that were generated by adding gaussian noise.\n\nHere is an example of how we can use `NoiseTunnel` with `IntegratedGradients`.\n\n```python\nig = IntegratedGradients(model)\nnt = NoiseTunnel(ig)\nattributions, delta = nt.attribute(input, nt_type='smoothgrad', stdevs=0.02, nt_samples=4,\n      baselines=baseline, target=0, return_convergence_delta=True)\nprint('IG + SmoothGrad Attributions:', attributions)\nprint('Convergence Delta:', delta)\n```\nOutput\n```\nIG + SmoothGrad Attributions: tensor([[-0.4574, -1.5493, -1.0893],\n                                      [ 0.0000, -0.2647, -5.1619]])\nConvergence Delta: tensor([ 0.0000e+00,  2.3842e-07,  0.0000e+00, -2.3842e-07,  0.0000e+00,\n        -4.7684e-07,  0.0000e+00, -4.7684e-07])\n\n```\nThe number of elements in the `delta` tensor is equal to: `nt_samples * input.shape[0]`\nIn order to get an example-wise delta, we can, for example, average them:\n```python\ndeltas_per_example = torch.mean(delta.reshape(input.shape[0], -1), dim=1)\n```\n\nLet's look into the internals of our network and understand which layers\nand neurons are important for the predictions.\n\nWe will start with the `NeuronConductance`. `NeuronConductance` helps us to identify\ninput features that are important for a particular neuron in a given\nlayer. It decomposes the computation of integrated gradients via the chain rule by\ndefining the importance of a neuron as path integral of the derivative of the output\nwith respect to the neuron times the derivatives of the neuron with respect to the\ninputs of the model.\n\nIn this case, we choose to analyze the first neuron in the linear layer.\n\n```python\nnc = NeuronConductance(model, model.lin1)\nattributions = nc.attribute(input, neuron_selector=1, target=0)\nprint('Neuron Attributions:', attributions)\n```\nOutput\n```\nNeuron Attributions: tensor([[ 0.0000,  0.0000,  0.0000],\n                             [ 1.3358,  0.0000, -1.6811]])\n```\n\nLayer conductance shows the importance of neurons for a layer and given input.\nIt is an extension of path integrated gradients for hidden layers and holds the\ncompleteness property as well.\n\nIt doesn't attribute the contribution scores to the input features\nbut shows the importance of each neuron in the selected layer.\n```python\nlc = LayerConductance(model, model.lin1)\nattributions, delta = lc.attribute(input, baselines=baseline, target=0, return_convergence_delta=True)\nprint('Layer Attributions:', attributions)\nprint('Convergence Delta:', delta)\n```\nOutputs\n```\nLayer Attributions: tensor([[ 0.0000,  0.0000, -3.0856],\n                            [ 0.0000, -0.3488, -4.9638]], grad_fn=<SumBackward1>)\nConvergence Delta: tensor([0.0630, 0.1084])\n```\n\nSimilar to other attribution algorithms that return convergence delta, `LayerConductance`\nreturns the deltas for each example. The approximation error is then the absolute\nvalue of the convergence deltas and can serve as a proxy of how accurate integral\napproximation for given inputs and baselines is.\n\nMore details on the list of supported algorithms and how to apply\nCaptum on different types of models can be found in our tutorials.\n\n\n## Captum Insights\n\nCaptum provides a web interface called Insights for easy visualization and\naccess to a number of our interpretability algorithms.\n\nTo analyze a sample model on CIFAR10 via Captum Insights run\n\n```\npython -m captum.insights.attr_vis.example\n```\n\nand navigate to the URL specified in the output.\n\n![Captum Insights Screenshot](./website/static/img/captum_insights_screenshot.png)\n\nTo build Insights you will need [Node](https://nodejs.org/en/) >= 8.x\nand [Yarn](https://yarnpkg.com/en/) >= 1.5.\n\nTo build and launch from a checkout in a conda environment run\n\n```\nconda install -c conda-forge yarn\nBUILD_INSIGHTS=1 python setup.py develop\npython captum/insights/example.py\n```\n\n### Captum Insights Jupyter Widget\nCaptum Insights also has a Jupyter widget providing the same user interface as the web app.\nTo install and enable the widget, run\n\n```\njupyter nbextension install --py --symlink --sys-prefix captum.insights.attr_vis.widget\njupyter nbextension enable captum.insights.attr_vis.widget --py --sys-prefix\n```\n\nTo build the widget from a checkout in a conda environment run\n\n```\nconda install -c conda-forge yarn\nBUILD_INSIGHTS=1 python setup.py develop\n```\n\n## FAQ\nIf you have questions about using Captum methods, please check this [FAQ](docs/faq.md), which addresses many common issues.\n\n## Contributing\nSee the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.\n\n## Talks and Papers\n**NeurIPS 2019:**\nThe slides of our presentation  can be found [here](docs/presentations/Captum_NeurIPS_2019_final.key)\n\n**KDD 2020:**\nThe slides of our presentation from KDD 2020 tutorial can be found [here](https://pytorch-tutorial-assets.s3.amazonaws.com/Captum_KDD_2020.pdf).\nYou can watch the recorded talk [here](https://www.youtube.com/watch?v=hY_XzglTkak)\n\n**GTC 2020:**\nOpening Up the Black Box: Model Understanding with Captum and PyTorch.\nYou can watch the recorded talk [here](https://www.youtube.com/watch?v=0QLrRyLndFI)\n\n**XAI Summit 2020:**\nUsing Captum and Fiddler to Improve Model Understanding with Explainable AI.\nYou can watch the recorded talk [here](https://www.youtube.com/watch?v=dvuVld5Hyc8)\n\n**PyTorch Developer Day 2020**\nModel Interpretability.\nYou can watch the recorded talk [here](https://www.youtube.com/watch?v=Lj5hHBGue58)\n\n**NAACL 2021**\nTutorial on Fine-grained Interpretation and Causation Analysis in Deep NLP Models.\nYou can watch the recorded talk [here](https://www.youtube.com/watch?v=ayhBHZYjeqs\n)\n\n**ICLR 2021 workshop on Responsible AI**:\n- [Paper](https://arxiv.org/abs/2009.07896) on the Captum Library\n- [Paper](https://arxiv.org/abs/2106.07475) on Investigating Sanity Checks for Saliency Maps\n\n\nSummer school on medical imaging at University of Lyon. A class on model explainability (link to the video)\nhttps://www.youtube.com/watch?v=vn-jLzY67V0\n\n## References of Algorithms\n\n* `IntegratedGradients`, `LayerIntegratedGradients`: [Axiomatic Attribution for Deep Networks, Mukund Sundararajan et al. 2017](https://arxiv.org/abs/1703.01365) and [Did the Model Understand the Question?, Pramod K. Mudrakarta, et al. 2018](https://arxiv.org/abs/1805.05492)\n* `InputXGradient`: [Not Just a Black Box: Learning Important Features Through Propagating Activation Differences, Avanti Shrikumar et al. 2016](https://arxiv.org/abs/1605.01713)\n* `SmoothGrad`: [SmoothGrad: removing noise by adding noise, Daniel Smilkov et al. 2017](https://arxiv.org/abs/1706.03825)\n* `NoiseTunnel`: [Sanity Checks for Saliency Maps, Julius Adebayo et al. 2018](https://arxiv.org/abs/1810.03292)\n* `NeuronConductance`: [How Important is a neuron?, Kedar Dhamdhere et al. 2018](https://arxiv.org/abs/1805.12233)\n* `LayerConductance`: [Computationally Efficient Measures of Internal Neuron Importance, Avanti Shrikumar et al. 2018](https://arxiv.org/abs/1807.09946)\n* `DeepLift`, `NeuronDeepLift`, `LayerDeepLift`: [Learning Important Features Through Propagating Activation Differences, Avanti Shrikumar et al. 2017](https://arxiv.org/abs/1704.02685) and [Towards better understanding of gradient-based attribution methods for deep neural networks, Marco Ancona et al. 2018](https://openreview.net/pdf?id=Sy21R9JAW)\n* `NeuronIntegratedGradients`: [Computationally Efficient Measures of Internal Neuron Importance, Avanti Shrikumar et al. 2018](https://arxiv.org/abs/1807.09946)\n* `GradientShap`, `NeuronGradientShap`, `LayerGradientShap`, `DeepLiftShap`, `NeuronDeepLiftShap`, `LayerDeepLiftShap`: [A Unified Approach to Interpreting Model Predictions, Scott M. Lundberg et al. 2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions)\n* `InternalInfluence`: [Influence-Directed Explanations for Deep Convolutional Networks, Klas Leino et al. 2018](https://arxiv.org/abs/1802.03788)\n* `Saliency`, `NeuronGradient`: [Deep Inside Convolutional Networks: Visualising\nImage Classification Models and Saliency Maps, K. Simonyan, et. al. 2014](https://arxiv.org/abs/1312.6034)\n* `GradCAM`, `Guided GradCAM`: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, Ramprasaath R. Selvaraju et al. 2017](https://arxiv.org/abs/1610.02391)\n* `Deconvolution`, `Neuron Deconvolution`: [Visualizing and Understanding Convolutional Networks, Matthew D Zeiler et al. 2014](https://arxiv.org/abs/1311.2901)\n* `Guided Backpropagation`, `Neuron Guided Backpropagation`: [Striving for Simplicity: The All Convolutional Net, Jost Tobias Springenberg et al. 2015](https://arxiv.org/abs/1412.6806)\n* `Feature Permutation`: [Permutation Feature Importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n* `Occlusion`: [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)\n* `Shapley Value`: [A value for n-person games. Contributions to the Theory of Games 2.28 (1953): 307-317](https://apps.dtic.mil/dtic/tr/fulltext/u2/604084.pdf)\n* `Shapley Value Sampling`: [Polynomial calculation of the Shapley value based on sampling](https://www.sciencedirect.com/science/article/pii/S0305054808000804)\n* `Infidelity and Sensitivity`: [On the (In)fidelity and Sensitivity for Explanations](https://arxiv.org/abs/1901.09392)\n* `TracInCP, TracInCPFast, TracInCPRandProj`: [Estimating Training Data Influence by Tracing Gradient Descent](https://arxiv.org/abs/2002.08484)\n* `SimilarityInfluence`: [Pairwise similarities between train and test examples based on predefined similarity metrics]\n* `BinaryConcreteStochasticGates`: [Stochastic Gates with Binary Concrete Distribution](https://arxiv.org/abs/1712.01312)\n* `GaussianStochasticGates`: [Stochastic Gates with Gaussian Distribution](https://arxiv.org/abs/1810.04247)\n\nMore details about the above mentioned [attribution algorithms](https://captum.ai/docs/attribution_algorithms) and their pros and cons can be found on our [web-site](https://captum.ai/docs/algorithms_comparison_matrix).\n\n## License\nCaptum is BSD licensed, as found in the [LICENSE](LICENSE) file.\n"
        },
        {
          "name": "captum",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.0791015625,
          "content": "name: captum\nchannels:\n  - pytorch\ndependencies:\n  - numpy<2.0\n  - pytorch>=1.10\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.0810546875,
          "content": "[tool.usort]\nfirst_party_detection = false\n\n[tool.black]\ntarget-version = ['py36']\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.40625,
          "content": "[flake8]\n# E203: black and flake8 disagree on whitespace before ':'\n# W503: black and flake8 disagree on how to place operators\n# E704: black and flake8 disagree on Multiple statements on one line (def)\nignore = E203, W503, E704\nmax-line-length = 88\nexclude =\n  build, dist, tutorials, website\n\n[coverage:report]\nomit =\n    test/*\n    setup.py\n\n[mypy]\nexclude = ^.*fb.*$\n\n[mypy-captum.log.fb.*]\nignore_errors = True\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.9814453125,
          "content": "#!/usr/bin/env python3\n\n# Welcome to the PyTorch Captum setup.py.\n#\n# Environment variables for feature toggles:\n#\n#   BUILD_INSIGHTS\n#     enables Captum Insights build via yarn\n#\n\nimport os\nimport re\nimport subprocess\nimport sys\n\nfrom setuptools import find_packages, setup\n\nREQUIRED_MAJOR = 3\nREQUIRED_MINOR = 9\n\n# Check for python version\nif sys.version_info < (REQUIRED_MAJOR, REQUIRED_MINOR):\n    error = (\n        \"Your version of python ({major}.{minor}) is too old. You need \"\n        \"python >= {required_major}.{required_minor}.\"\n    ).format(\n        major=sys.version_info.major,\n        minor=sys.version_info.minor,\n        required_minor=REQUIRED_MINOR,\n        required_major=REQUIRED_MAJOR,\n    )\n    sys.exit(error)\n\n\n# Allow for environment variable checks\ndef check_env_flag(name, default=\"\"):\n    return os.getenv(name, default).upper() in [\"ON\", \"1\", \"YES\", \"TRUE\", \"Y\"]\n\n\nBUILD_INSIGHTS = check_env_flag(\"BUILD_INSIGHTS\")\nVERBOSE_SCRIPT = True\nfor arg in sys.argv:\n    if arg == \"-q\" or arg == \"--quiet\":\n        VERBOSE_SCRIPT = False\n\n\ndef report(*args):\n    if VERBOSE_SCRIPT:\n        print(*args)\n    else:\n        pass\n\n\nINSIGHTS_REQUIRES = [\"flask\", \"ipython\", \"ipywidgets\", \"jupyter\", \"flask-compress\"]\n\nINSIGHTS_FILE_SUBDIRS = [\n    \"insights/attr_vis/frontend/build\",\n    \"insights/attr_vis/models\",\n    \"insights/attr_vis/widget/static\",\n]\n\nTUTORIALS_REQUIRES = INSIGHTS_REQUIRES + [\"torchtext\", \"torchvision\"]\n\nTEST_REQUIRES = [\"pytest\", \"pytest-cov\", \"parameterized\"]\n\nDEV_REQUIRES = (\n    INSIGHTS_REQUIRES\n    + TEST_REQUIRES\n    + [\n        \"black\",\n        \"flake8\",\n        \"sphinx\",\n        \"sphinx-autodoc-typehints\",\n        \"sphinxcontrib-katex\",\n        \"mypy>=0.760\",\n        \"usort==1.0.2\",\n        \"ufmt\",\n        \"scikit-learn\",\n        \"annoy\",\n    ]\n)\n\n# get version string from module\nwith open(os.path.join(os.path.dirname(__file__), \"captum/__init__.py\"), \"r\") as f:\n    version_match = re.search(r\"__version__ = ['\\\"]([^'\\\"]*)['\\\"]\", f.read(), re.M)\n    assert version_match is not None, \"Unable to find version string.\"\n    version = version_match.group(1)\n    report(\"-- Building version \" + version)\n\n# read in README.md as the long description\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\n\n# optionally build Captum Insights via yarn\ndef build_insights():\n    report(\"-- Building Captum Insights\")\n    command = \"./scripts/build_insights.sh\"\n    report(\"Running: \" + command)\n    subprocess.check_call(command)\n\n\n# explore paths under root and subdirs to gather package files\ndef get_package_files(root, subdirs):\n    paths = []\n    for subroot in subdirs:\n        paths.append(os.path.join(subroot, \"*\"))\n        for path, dirs, _ in os.walk(os.path.join(root, subroot)):\n            for d in dirs:\n                paths.append(os.path.join(path, d, \"*\")[len(root) + 1 :])\n    return paths\n\n\nif __name__ == \"__main__\":\n\n    if BUILD_INSIGHTS:\n        build_insights()\n\n    package_files = get_package_files(\"captum\", INSIGHTS_FILE_SUBDIRS)\n\n    setup(\n        name=\"captum\",\n        version=version,\n        description=\"Model interpretability for PyTorch\",\n        author=\"PyTorch Team\",\n        license=\"BSD-3\",\n        url=\"https://captum.ai\",\n        project_urls={\n            \"Documentation\": \"https://captum.ai\",\n            \"Source\": \"https://github.com/pytorch/captum\",\n            \"conda\": \"https://anaconda.org/pytorch/captum\",\n        },\n        keywords=[\n            \"Model Interpretability\",\n            \"Model Understanding\",\n            \"Feature Importance\",\n            \"Neuron Importance\",\n            \"PyTorch\",\n        ],\n        classifiers=[\n            \"Development Status :: 4 - Beta\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Education\",\n            \"Intended Audience :: Science/Research\",\n            \"License :: OSI Approved :: BSD License\",\n            \"Programming Language :: Python :: 3 :: Only\",\n            \"Topic :: Scientific/Engineering\",\n        ],\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        python_requires=\">=3.9\",\n        install_requires=[\n            \"matplotlib\",\n            \"numpy<2.0\",\n            \"packaging\",\n            \"torch>=1.10\",\n            \"tqdm\",\n        ],\n        packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n        extras_require={\n            \"dev\": DEV_REQUIRES,\n            \"insights\": INSIGHTS_REQUIRES,\n            \"test\": TEST_REQUIRES,\n            \"tutorials\": TUTORIALS_REQUIRES,\n        },\n        package_data={\"captum\": package_files},\n        data_files=[\n            (\n                \"share/jupyter/nbextensions/jupyter-captum-insights\",\n                [\n                    \"captum/insights/attr_vis/frontend/widget/src/extension.js\",\n                    \"captum/insights/attr_vis/frontend/widget/src/index.js\",\n                ],\n            ),\n            (\n                \"etc/jupyter/nbconfig/notebook.d\",\n                [\"captum/insights/attr_vis/widget/jupyter-captum-insights.json\"],\n            ),\n        ],\n    )\n"
        },
        {
          "name": "sphinx",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorials",
          "type": "tree",
          "content": null
        },
        {
          "name": "website",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}