{
  "metadata": {
    "timestamp": 1736560493490,
    "page": 86,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Cadene/pretrained-models.pytorch",
      "stars": 9054,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.166015625,
          "content": "# Added by ~ cadene ~\n.DS_Store\n._.DS_Store\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 3.69921875,
          "content": "language: python\npython:\n  - \"3.6\"\n # - \"2.7\"\n\n# cache:\n#   directories:\n#     - $HOME/.torch\n\nstages:\n#  - lint_check\n  - test\n#  - docs\n\ninstall:\n  - sudo apt-get update\n  - wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;\n  - bash miniconda.sh -b -p $HOME/miniconda\n  - export PATH=\"$HOME/miniconda/bin:$PATH\"\n  - hash -r\n  - conda config --set always_yes yes --set changeps1 no\n  - conda update -q conda\n  # Useful for debugging any issues with conda\n  - conda info -a\n  - conda create -q -n test-environment -c pytorch python=$TRAVIS_PYTHON_VERSION numpy mock pytorch-cpu\n  - if [[ $TRAVIS_PYTHON_VERSION == 2.7 ]]; then pip install enum34; fi\n  - source activate test-environment\n  - python setup.py install\n  - pip install --upgrade pytest #codecov pytest-cov\n  # Test contrib dependencies\n  # - pip install scikit-learn\n  # Examples dependencies\n  - pip install -r requirements.txt\n  #visdom torchvision tensorboardX\n  #- pip install gym\n  #- pip install tqdm\n\nscript:\n  - python -m pytest -s tests #--cov pretrainedmodels --cov-report term-missing\n\n  # Smoke tests for the examples\n  # Mnist\n  # 1) mnist.py\n  #- python examples/mnist/mnist.py --epochs=1\n  # 2) mnist_with_visdom.py\n  # - python -c \"from visdom.server import download_scripts; download_scripts()\" # download scripts : https://github.com/facebookresearch/visdom/blob/master/py/server.py#L929\n  # - python -m visdom.server &\n  # - sleep 10\n  # - python examples/mnist/mnist_with_visdom.py --epochs=1\n  # - kill %1\n  # # 3) mnist_with_tensorboardx.py\n  # - python examples/mnist/mnist_with_tensorboardx.py --epochs=1\n\n  # # dcgan.py\n  # - python examples/gan/dcgan.py --dataset fake --dataroot /tmp/fakedata --output-dir /tmp/outputs-dcgan --batch-size 2 --epochs 2  --workers 0\n\n  # # RL\n  # # 1) Actor-Critic\n  # - python examples/reinforcement_learning/actor_critic.py --max-episodes=2\n  # # 1) Reinforce\n  # - python examples/reinforcement_learning/reinforce.py --max-episodes=2\n\n  # #fast-neural-style\n  # #train\n  # - python examples/fast_neural_style/neural_style.py train --epochs 1 --cuda 0 --dataset test --dataroot . --image_size 32 --style_image examples/fast_neural_style/images/style_images/mosaic.jpg --style_size 32\n\nafter_success:\n  # Ignore codecov failures as the codecov server is not\n  # very reliable but we don't want travis to report a failure\n  # in the github UI just because the coverage report failed to\n  # be published.\n  - codecov || echo \"codecov upload failed\"\n\n# jobs:\n#   include:\n#     - stage: lint_check\n#       python: \"3.6\"\n#       install: pip install flake8\n#       script: flake8\n#       after_success: # Nothing to do\n\n\n#     # GitHub Pages Deployment: https://docs.travis-ci.com/user/deployment/pages/\n#     - stage: docs\n#       python: \"3.6\"\n#       install:\n#         # Minimal install : ignite and dependencies just to build the docs\n#         - pip install -r docs/requirements.txt\n#         - pip install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp35-cp35m-linux_x86_64.whl\n#         # Add contrib dependencies (otherwise doc is not built)\n#         - pip install scikit-learn scipy\n#         # `pip install .` vs `python setup.py install` : 1st works better to produce _module/ignite with source links\n#         - pip install .\n#       script:\n#         - cd docs && make html\n#         # Create .nojekyll file to serve correctly _static and friends\n#         - touch build/html/.nojekyll\n#       after_success: # Nothing to do\n#       deploy:\n#         provider: pages\n#         skip-cleanup: true\n#         github-token: $GITHUB_TOKEN  # Set in the settings page of your repository, as a secure variable\n#         keep-history: false\n#         local_dir: docs/build/html\n#         on:\n#           branch: master"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.4755859375,
          "content": "BSD 3-Clause License\n\nCopyright (c) 2017, Remi Cadene\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 28.6357421875,
          "content": "# Pretrained models for Pytorch (Work in progress)\n\nThe goal of this repo is:\n\n- to help to reproduce research papers results (transfer learning setups for instance),\n- to access pretrained ConvNets with a unique interface/API inspired by torchvision.\n\n<a href=\"https://travis-ci.org/Cadene/pretrained-models.pytorch\"><img src=\"https://api.travis-ci.org/Cadene/pretrained-models.pytorch.svg?branch=master\"/></a>\n\nNews:\n- 27/10/2018: Fix compatibility issues, Add tests, Add travis\n- 04/06/2018: [PolyNet](https://github.com/CUHK-MMLAB/polynet) and [PNASNet-5-Large](https://arxiv.org/abs/1712.00559) thanks to [Alex Parinov](https://github.com/creafz)\n- 16/04/2018: [SE-ResNet* and SE-ResNeXt*](https://github.com/hujie-frank/SENet) thanks to [Alex Parinov](https://github.com/creafz)\n- 09/04/2018: [SENet154](https://github.com/hujie-frank/SENet) thanks to [Alex Parinov](https://github.com/creafz)\n- 22/03/2018: CaffeResNet101 (good for localization with FasterRCNN)\n- 21/03/2018: NASNet Mobile thanks to [Veronika Yurchuk](https://github.com/veronikayurchuk) and [Anastasiia](https://github.com/DagnyT)\n- 25/01/2018: DualPathNetworks thanks to [Ross Wightman](https://github.com/rwightman/pytorch-dpn-pretrained), Xception thanks to [T Standley](https://github.com/tstandley/Xception-PyTorch), improved TransformImage API\n- 13/01/2018: `pip install pretrainedmodels`, `pretrainedmodels.model_names`, `pretrainedmodels.pretrained_settings`\n- 12/01/2018: `python setup.py install`\n- 08/12/2017: update data url (/!\\ `git pull` is needed)\n- 30/11/2017: improve API (`model.features(input)`, `model.logits(features)`, `model.forward(input)`, `model.last_linear`)\n- 16/11/2017: nasnet-a-large pretrained model ported by T. Durand and R. Cadene\n- 22/07/2017: torchvision pretrained models\n- 22/07/2017: momentum in inceptionv4 and inceptionresnetv2 to 0.1\n- 17/07/2017: model.input_range attribut\n- 17/07/2017: BNInception pretrained on Imagenet\n\n## Summary\n\n- [Installation](https://github.com/Cadene/pretrained-models.pytorch#installation)\n- [Quick examples](https://github.com/Cadene/pretrained-models.pytorch#quick-examples)\n- [Few use cases](https://github.com/Cadene/pretrained-models.pytorch#few-use-cases)\n    - [Compute imagenet logits](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-logits)\n    - [Compute imagenet validation metrics](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-validation-metrics)\n- [Evaluation on ImageNet](https://github.com/Cadene/pretrained-models.pytorch#evaluation-on-imagenet)\n    - [Accuracy on valset](https://github.com/Cadene/pretrained-models.pytorch#accuracy-on-validation-set)\n    - [Reproducing results](https://github.com/Cadene/pretrained-models.pytorch#reproducing-results)\n- [Documentation](https://github.com/Cadene/pretrained-models.pytorch#documentation)\n    - [Available models](https://github.com/Cadene/pretrained-models.pytorch#available-models)\n        - [AlexNet](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception)\n        - [CaffeResNet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet)\n        - [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [DualPathNet92](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [DualPathNet107](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [DualPathNet113](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet)\n        - [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception)\n        - [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception)\n        - [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception)\n        - [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet)\n        - [NASNet-A-Mobile](https://github.com/Cadene/pretrained-models.pytorch#nasnet)\n        - [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet)\n        - [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet)\n        - [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)\n        - [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)\n        - [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [ResNet152](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception)\n    - [Model API](https://github.com/Cadene/pretrained-models.pytorch#model-api)\n        - [model.input_size](https://github.com/Cadene/pretrained-models.pytorch#modelinput_size)\n        - [model.input_space](https://github.com/Cadene/pretrained-models.pytorch#modelinput_space)\n        - [model.input_range](https://github.com/Cadene/pretrained-models.pytorch#modelinput_range)\n        - [model.mean](https://github.com/Cadene/pretrained-models.pytorch#modelmean)\n        - [model.std](https://github.com/Cadene/pretrained-models.pytorch#modelstd)\n        - [model.features](https://github.com/Cadene/pretrained-models.pytorch#modelfeatures)\n        - [model.logits](https://github.com/Cadene/pretrained-models.pytorch#modellogits)\n        - [model.forward](https://github.com/Cadene/pretrained-models.pytorch#modelforward)\n- [Reproducing porting](https://github.com/Cadene/pretrained-models.pytorch#reproducing)\n    - [ResNet*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-resnet152)\n    - [ResNeXt*](https://github.com/Cadene/pretrained-models.pytorch#automatic-porting-of-resnext)\n    - [Inception*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-inceptionv4-and-inceptionresnetv2)\n\n## Installation\n\n1. [python3 with anaconda](https://www.continuum.io/downloads)\n2. [pytorch with/out CUDA](http://pytorch.org)\n\n### Install from pip\n\n3. `pip install pretrainedmodels`\n\n### Install from repo\n\n3. `git clone https://github.com/Cadene/pretrained-models.pytorch.git`\n4. `cd pretrained-models.pytorch`\n5. `python setup.py install`\n\n\n## Quick examples\n\n- To import `pretrainedmodels`:\n\n```python\nimport pretrainedmodels\n```\n\n- To print the available pretrained models:\n\n```python\nprint(pretrainedmodels.model_names)\n> ['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetalarge', 'nasnetamobile', 'cafferesnet101', 'senet154',  'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'polynet', 'pnasnet5large']\n```\n\n- To print the available pretrained settings for a chosen model:\n\n```python\nprint(pretrainedmodels.pretrained_settings['nasnetalarge'])\n> {'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth', 'input_space': 'RGB', 'input_size': [3, 331, 331], 'input_range': [0, 1], 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'num_classes': 1000}, 'imagenet+background': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth', 'input_space': 'RGB', 'input_size': [3, 331, 331], 'input_range': [0, 1], 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'num_classes': 1001}}\n```\n\n- To load a pretrained models from imagenet:\n\n```python\nmodel_name = 'nasnetalarge' # could be fbresnet152 or inceptionresnetv2\nmodel = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\nmodel.eval()\n```\n\n**Note**: By default, models will be downloaded to your `$HOME/.torch` folder. You can modify this behavior using the `$TORCH_HOME` variable as follow: `export TORCH_HOME=\"/local/pretrainedmodels\"`\n\n- To load an image and do a complete forward pass:\n\n```python\nimport torch\nimport pretrainedmodels.utils as utils\n\nload_img = utils.LoadImage()\n\n# transformations depending on the model\n# rescale, center crop, normalize, and others (ex: ToBGR, ToRange255)\ntf_img = utils.TransformImage(model) \n\npath_img = 'data/cat.jpg'\n\ninput_img = load_img(path_img)\ninput_tensor = tf_img(input_img)         # 3x400x225 -> 3x299x299 size may differ\ninput_tensor = input_tensor.unsqueeze(0) # 3x299x299 -> 1x3x299x299\ninput = torch.autograd.Variable(input_tensor,\n    requires_grad=False)\n\noutput_logits = model(input) # 1x1000\n```\n\n- To extract features (beware this API is not available for all networks):\n\n```python\noutput_features = model.features(input) # 1x14x14x2048 size may differ\noutput_logits = model.logits(output_features) # 1x1000\n```\n\n## Few use cases\n\n### Compute imagenet logits\n\n- See [examples/imagenet_logits.py](https://github.com/Cadene/pretrained-models.pytorch/blob/master/examples/imagenet_logits.py) to compute logits of classes appearance over a single image with a pretrained model on imagenet.\n\n```\n$ python examples/imagenet_logits.py -h\n> nasnetalarge, resnet152, inceptionresnetv2, inceptionv4, ...\n```\n\n```\n$ python examples/imagenet_logits.py -a nasnetalarge --path_img data/cat.jpg\n> 'nasnetalarge': data/cat.jpg' is a 'tiger cat' \n```\n\n### Compute imagenet evaluation metrics\n\n- See [examples/imagenet_eval.py](https://github.com/Cadene/pretrained-models.pytorch/blob/master/examples/imagenet_eval.py) to evaluate pretrained models on imagenet valset. \n\n```\n$ python examples/imagenet_eval.py /local/common-data/imagenet_2012/images -a nasnetalarge -b 20 -e\n> * Acc@1 82.693, Acc@5 96.13\n```\n\n\n## Evaluation on imagenet\n\n### Accuracy on validation set (single model)\n\nResults were obtained using (center cropped) images of the same size than during the training process.\n\nModel | Version | Acc@1 | Acc@5\n--- | --- | --- | ---\nPNASNet-5-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.858 | 96.182\n[PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet) | Our porting | 82.736 | 95.992\nNASNet-A-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.693 | 96.163\n[NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet) | Our porting | 82.566 | 96.086\nSENet154 | [Caffe](https://github.com/hujie-frank/SENet) | 81.32 | 95.53\n[SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 81.304 | 95.498\nPolyNet | [Caffe](https://github.com/CUHK-MMLAB/polynet) | 81.29 | 95.75\n[PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet) | Our porting | 81.002 | 95.624\nInceptionResNetV2 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.4 | 95.3\nInceptionV4 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.2 | 95.3\n[SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 80.236 | 95.028\nSE-ResNeXt101_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 80.19 | 95.04\n[InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.170 | 95.234\n[InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.062 | 94.926\n[DualPathNet107_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.746 | 94.684\nResNeXt101_64x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 79.6 | 94.7\n[DualPathNet131](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.432 | 94.574\n[DualPathNet92_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.400 | 94.620\n[DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.224 | 94.488\n[SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 79.076 | 94.434\nSE-ResNeXt50_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 79.03 | 94.46\n[Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | [Keras](https://github.com/keras-team/keras/blob/master/keras/applications/xception.py) | 79.000 | 94.500\n[ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.956 | 94.252\n[Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | Our porting | 78.888 | 94.292\nResNeXt101_32x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 78.8 | 94.4\nSE-ResNet152 | [Caffe](https://github.com/hujie-frank/SENet) | 78.66 | 94.46\n[SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.658 | 94.374\nResNet152 | [Pytorch](https://github.com/pytorch/vision#models) | 78.428 | 94.110\n[SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.396 | 94.258\nSE-ResNet101 | [Caffe](https://github.com/hujie-frank/SENet) | 78.25 | 94.28\n[ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.188 | 93.886\nFBResNet152 | [Torch7](https://github.com/facebook/fb.resnet.torch) | 77.84 | 93.84\nSE-ResNet50 | [Caffe](https://github.com/hujie-frank/SENet) | 77.63 | 93.64\n[SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 77.636 | 93.752\n[DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.560 | 93.798\n[ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.438 | 93.672\n[FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet) | Our porting | 77.386 | 93.594\n[InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception) | [Pytorch](https://github.com/pytorch/vision#models) | 77.294 | 93.454\n[DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.152 | 93.548\n[DualPathNet68b_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 77.034 | 93.590\n[CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | [Caffe](https://github.com/KaimingHe/deep-residual-networks) | 76.400 | 92.900\n[CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | Our porting | 76.200 | 92.766\n[DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.026 | 92.992\n[ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.002 | 92.980\n[DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 75.868 | 92.774\n[DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.646 | 92.136\n[VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.266 | 92.066\nNASNet-A-Mobile | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 74.0 | 91.6\n[NASNet-A-Mobile](https://github.com/veronikayurchuk/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet_mobile.py) | Our porting | 74.080 | 91.740\n[ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.554 | 91.456\n[BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception) | Our porting | 73.524 | 91.562\n[VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.518 | 91.608\n[VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 72.080 | 90.822\n[VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.636 | 90.354\n[VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.508 | 90.494\n[VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.452 | 89.818\n[ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.142 | 89.274\n[VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 69.662 | 89.264\n[VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 68.970 | 88.746\n[SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.250 | 80.800\n[SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.108 | 80.428\n[Alexnet](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 56.432 | 79.194\n\nNotes:\n- the Pytorch version of ResNet152 is not a porting of the Torch7 but has been retrained by facebook.\n- For the PolyNet evaluation each image was resized to 378x378 without preserving the aspect ratio and then the central 331×331 patch from the resulting image was used.\n\nBeware, the accuracy reported here is not always representative of the transferable capacity of the network on other tasks and datasets. You must try them all! :P\n    \n### Reproducing results\n\nPlease see [Compute imagenet validation metrics](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-validation-metrics)\n\n\n## Documentation\n\n### Available models\n\n#### NASNet*\n\nSource: [TensorFlow Slim repo](https://github.com/tensorflow/models/tree/master/research/slim)\n\n- `nasnetalarge(num_classes=1000, pretrained='imagenet')`\n- `nasnetalarge(num_classes=1001, pretrained='imagenet+background')`\n- `nasnetamobile(num_classes=1000, pretrained='imagenet')`\n\n#### FaceBook ResNet*\n\nSource: [Torch7 repo of FaceBook](https://github.com/facebook/fb.resnet.torch)\n\nThere are a bit different from the ResNet* of torchvision. ResNet152 is currently the only one available.\n\n- `fbresnet152(num_classes=1000, pretrained='imagenet')`\n\n#### Caffe ResNet*\n\nSource: [Caffe repo of KaimingHe](https://github.com/KaimingHe/deep-residual-networks)\n\n- `cafferesnet101(num_classes=1000, pretrained='imagenet')`\n\n\n#### Inception*\n\nSource: [TensorFlow Slim repo](https://github.com/tensorflow/models/tree/master/slim) and [Pytorch/Vision repo](https://github.com/pytorch/vision/tree/master/torchvision) for `inceptionv3`\n\n- `inceptionresnetv2(num_classes=1000, pretrained='imagenet')`\n- `inceptionresnetv2(num_classes=1001, pretrained='imagenet+background')`\n- `inceptionv4(num_classes=1000, pretrained='imagenet')`\n- `inceptionv4(num_classes=1001, pretrained='imagenet+background')`\n- `inceptionv3(num_classes=1000, pretrained='imagenet')`\n\n#### BNInception\n\nSource: [Trained with Caffe](https://github.com/Cadene/tensorflow-model-zoo.torch/pull/2) by [Xiong Yuanjun](http://yjxiong.me)\n\n- `bninception(num_classes=1000, pretrained='imagenet')`\n\n#### ResNeXt*\n\nSource: [ResNeXt repo of FaceBook](https://github.com/facebookresearch/ResNeXt)\n\n- `resnext101_32x4d(num_classes=1000, pretrained='imagenet')`\n- `resnext101_62x4d(num_classes=1000, pretrained='imagenet')`\n\n#### DualPathNetworks\n\nSource: [MXNET repo of Chen Yunpeng](https://github.com/cypw/DPNs)\n\nThe porting has been made possible by [Ross Wightman](http://rwightman.com) in his [PyTorch repo](https://github.com/rwightman/pytorch-dpn-pretrained).\n\nAs you can see [here](https://github.com/rwightman/pytorch-dpn-pretrained) DualPathNetworks allows you to try different scales. The default one in this repo is 0.875 meaning that the original input size is 256 before croping to 224.\n\n- `dpn68(num_classes=1000, pretrained='imagenet')`\n- `dpn98(num_classes=1000, pretrained='imagenet')`\n- `dpn131(num_classes=1000, pretrained='imagenet')`\n- `dpn68b(num_classes=1000, pretrained='imagenet+5k')`\n- `dpn92(num_classes=1000, pretrained='imagenet+5k')`\n- `dpn107(num_classes=1000, pretrained='imagenet+5k')`\n\n`'imagenet+5k'` means that the network has been pretrained on imagenet5k before being finetuned on imagenet1k.\n\n#### Xception\n\nSource: [Keras repo](https://github.com/keras-team/keras/blob/master/keras/applications/xception.py)\n\nThe porting has been made possible by [T Standley](https://github.com/tstandley/Xception-PyTorch).\n\n- `xception(num_classes=1000, pretrained='imagenet')`\n\n\n#### SENet*\n\nSource: [Caffe repo of Jie Hu](https://github.com/hujie-frank/SENet)\n\n- `senet154(num_classes=1000, pretrained='imagenet')`\n- `se_resnet50(num_classes=1000, pretrained='imagenet')`\n- `se_resnet101(num_classes=1000, pretrained='imagenet')`\n- `se_resnet152(num_classes=1000, pretrained='imagenet')`\n- `se_resnext50_32x4d(num_classes=1000, pretrained='imagenet')`\n- `se_resnext101_32x4d(num_classes=1000, pretrained='imagenet')`\n\n#### PNASNet*\n\nSource: [TensorFlow Slim repo](https://github.com/tensorflow/models/tree/master/research/slim)\n\n- `pnasnet5large(num_classes=1000, pretrained='imagenet')`\n- `pnasnet5large(num_classes=1001, pretrained='imagenet+background')`\n\n#### PolyNet\n\nSource: [Caffe repo of the CUHK Multimedia Lab](https://github.com/CUHK-MMLAB/polynet)\n\n- `polynet(num_classes=1000, pretrained='imagenet')`\n\n#### TorchVision\n\nSource: [Pytorch/Vision repo](https://github.com/pytorch/vision/tree/master/torchvision)\n\n(`inceptionv3` included in [Inception*](https://github.com/Cadene/pretrained-models.pytorch#inception))\n\n- `resnet18(num_classes=1000, pretrained='imagenet')`\n- `resnet34(num_classes=1000, pretrained='imagenet')`\n- `resnet50(num_classes=1000, pretrained='imagenet')`\n- `resnet101(num_classes=1000, pretrained='imagenet')`\n- `resnet152(num_classes=1000, pretrained='imagenet')`\n- `densenet121(num_classes=1000, pretrained='imagenet')`\n- `densenet161(num_classes=1000, pretrained='imagenet')`\n- `densenet169(num_classes=1000, pretrained='imagenet')`\n- `densenet201(num_classes=1000, pretrained='imagenet')`\n- `squeezenet1_0(num_classes=1000, pretrained='imagenet')`\n- `squeezenet1_1(num_classes=1000, pretrained='imagenet')`\n- `alexnet(num_classes=1000, pretrained='imagenet')`\n- `vgg11(num_classes=1000, pretrained='imagenet')`\n- `vgg13(num_classes=1000, pretrained='imagenet')`\n- `vgg16(num_classes=1000, pretrained='imagenet')`\n- `vgg19(num_classes=1000, pretrained='imagenet')`\n- `vgg11_bn(num_classes=1000, pretrained='imagenet')`\n- `vgg13_bn(num_classes=1000, pretrained='imagenet')`\n- `vgg16_bn(num_classes=1000, pretrained='imagenet')`\n- `vgg19_bn(num_classes=1000, pretrained='imagenet')`\n\n\n### Model API\n\nOnce a pretrained model has been loaded, you can use it that way.\n\n**Important note**: All image must be loaded using `PIL` which scales the pixel values between 0 and 1.\n\n#### `model.input_size`\n\nAttribut of type `list` composed of 3 numbers:\n\n- number of color channels,\n- height of the input image,\n- width of the input image.\n\nExample:\n\n- `[3, 299, 299]` for inception* networks,\n- `[3, 224, 224]` for resnet* networks.\n\n\n#### `model.input_space`\n\nAttribut of type `str` representating the color space of the image. Can be `RGB` or `BGR`.\n\n\n#### `model.input_range`\n\nAttribut of type `list` composed of 2 numbers:\n\n- min pixel value,\n- max pixel value.\n\nExample:\n\n- `[0, 1]` for resnet* and inception* networks,\n- `[0, 255]` for bninception network.\n\n\n#### `model.mean`\n\nAttribut of type `list` composed of 3 numbers which are used to normalize the input image (substract \"color-channel-wise\").\n\nExample:\n\n- `[0.5, 0.5, 0.5]` for inception* networks,\n- `[0.485, 0.456, 0.406]` for resnet* networks.\n\n\n#### `model.std`\n\nAttribut of type `list` composed of 3 numbers which are used to normalize the input image (divide \"color-channel-wise\").\n\nExample:\n\n- `[0.5, 0.5, 0.5]` for inception* networks,\n- `[0.229, 0.224, 0.225]` for resnet* networks.\n\n\n#### `model.features`\n\n/!\\ work in progress (may not be available)\n\nMethod which is used to extract the features from the image.\n\nExample when the model is loaded using `fbresnet152`:\n\n```python\nprint(input_224.size())            # (1,3,224,224)\noutput = model.features(input_224) \nprint(output.size())               # (1,2048,1,1)\n\n# print(input_448.size())          # (1,3,448,448)\noutput = model.features(input_448)\n# print(output.size())             # (1,2048,7,7)\n```\n\n#### `model.logits`\n\n/!\\ work in progress (may not be available)\n\nMethod which is used to classify the features from the image.\n\nExample when the model is loaded using `fbresnet152`:\n\n```python\noutput = model.features(input_224) \nprint(output.size())               # (1,2048, 1, 1)\noutput = model.logits(output)\nprint(output.size())               # (1,1000)\n```\n\n#### `model.forward`\n\nMethod used to call `model.features` and `model.logits`. It can be overwritten as desired.\n\n**Note**: A good practice is to use `model.__call__` as your function of choice to forward an input to your model. See the example bellow.\n\n```python\n# Without model.__call__\noutput = model.forward(input_224)\nprint(output.size())      # (1,1000)\n\n# With model.__call__\noutput = model(input_224)\nprint(output.size())      # (1,1000)\n```\n\n#### `model.last_linear`\n\nAttribut of type `nn.Linear`. This module is the last one to be called during the forward pass.\n\n- Can be replaced by an adapted `nn.Linear` for fine tuning.\n- Can be replaced by `pretrained.utils.Identity` for features extraction. \n\nExample when the model is loaded using `fbresnet152`:\n\n```python\nprint(input_224.size())            # (1,3,224,224)\noutput = model.features(input_224) \nprint(output.size())               # (1,2048,1,1)\noutput = model.logits(output)\nprint(output.size())               # (1,1000)\n\n# fine tuning\ndim_feats = model.last_linear.in_features # =2048\nnb_classes = 4\nmodel.last_linear = nn.Linear(dim_feats, nb_classes)\noutput = model(input_224)\nprint(output.size())               # (1,4)\n\n# features extraction\nmodel.last_linear = pretrained.utils.Identity()\noutput = model(input_224)\nprint(output.size())               # (1,2048)\n```\n\n## Reproducing\n\n### Hand porting of ResNet152\n\n```\nth pretrainedmodels/fbresnet/resnet152_dump.lua\npython pretrainedmodels/fbresnet/resnet152_load.py\n```\n\n### Automatic porting of ResNeXt\n\nhttps://github.com/clcarwin/convert_torch_to_pytorch\n\n### Hand porting of NASNet, InceptionV4 and InceptionResNetV2\n\nhttps://github.com/Cadene/tensorflow-model-zoo.torch\n\n\n## Acknowledgement\n\nThanks to the deep learning community and especially to the contributers of the pytorch ecosystem.\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretrainedmodels",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0341796875,
          "content": "torch\ntorchvision\nmunch\ntqdm\nscipy\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0380859375,
          "content": "[metadata]\ndescription-file = README.md"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 6.4873046875,
          "content": "from __future__ import print_function, division, absolute_import\n\"\"\"A setuptools based setup module.\n\nSee:\nhttps://packaging.python.org/en/latest/distributing.html\nhttps://github.com/pypa/sampleproject\n\"\"\"\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\n# Arguments marked as \"Required\" below must be included for upload to PyPI.\n# Fields marked as \"Optional\" may be commented out.\n\n# https://stackoverflow.com/questions/458550/standard-way-to-embed-version-into-python-package/16084844#16084844\nexec(open('pretrainedmodels/version.py').read())\nsetup(\n    # This is the name of your project. The first time you publish this\n    # package, this name will be registered for you. It will determine how\n    # users can install this project, e.g.:\n    #\n    # $ pip install sampleproject\n    #\n    # And where it will live on PyPI: https://pypi.org/project/sampleproject/\n    #\n    # There are some restrictions on what makes a valid project name\n    # specification here:\n    # https://packaging.python.org/specifications/core-metadata/#name\n    name='pretrainedmodels',  # Required\n\n    # Versions should comply with PEP 440:\n    # https://www.python.org/dev/peps/pep-0440/\n    #\n    # For a discussion on single-sourcing the version across setup.py and the\n    # project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=__version__,  # Required\n\n    # This is a one-line description or tagline of what your project does. This\n    # corresponds to the \"Summary\" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#summary\n    description='Pretrained models for Pytorch',  # Required\n\n    # This is an optional longer description of your project that represents\n    # the body of text which users will see when they visit PyPI.\n    #\n    # Often, this is the same as your README, so you can just read it in from\n    # that file directly (as we have already done above)\n    #\n    # This field corresponds to the \"Description\" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#description-optional\n    long_description=long_description,  # Optional\n\n    # This should be a valid link to your project's main homepage.\n    #\n    # This field corresponds to the \"Home-Page\" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#home-page-optional\n    url='https://github.com/cadene/pretrained-models.pytorch',  # Optional\n\n    # This should be your name or the name of the organization which owns the\n    # project.\n    author='Remi Cadene',  # Optional\n\n    # This should be a valid email address corresponding to the author listed\n    # above.\n    author_email='remi.cadene@icloud.com',  # Optional\n\n    # Classifiers help users find your project by categorizing it.\n    #\n    # For a list of valid classifiers, see\n    # https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[  # Optional\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        'Development Status :: 3 - Alpha',\n\n        # Indicate who your project is intended for\n        'Intended Audience :: Developers',\n        'Topic :: Software Development :: Build Tools',\n\n        # Pick your license as you wish\n        'License :: OSI Approved :: MIT License',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        'Programming Language :: Python :: 3.6',\n    ],\n\n    # This field adds keywords for your project which will appear on the\n    # project page. What does your project relate to?\n    #\n    # Note that this is a string of words separated by whitespace, not a list.\n    keywords='pytorch pretrained models deep learning',  # Optional\n\n    # You can just specify package directories manually here if your project is\n    # simple. Or you can use find_packages().\n    #\n    # Alternatively, if you just want to distribute a single Python file, use\n    # the `py_modules` argument instead as follows, which will expect a file\n    # called `my_module.py` to exist:\n    #\n    #   py_modules=[\"my_module\"],\n    #\n    packages=find_packages(exclude=['data', 'examples']),  # Required\n\n    # This field lists other packages that your project depends on to run.\n    # Any package you put here will be installed by pip when your project is\n    # installed, so they must be valid existing projects.\n    #\n    # For an analysis of \"install_requires\" vs pip's requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=['torch', 'torchvision', 'munch', 'tqdm'],  # Optional\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). Users will be able to install these using the \"extras\"\n    # syntax, for example:\n    #\n    #   $ pip install sampleproject[dev]\n    #\n    # Similar to `install_requires` above, these must be valid existing\n    # projects.\n    # extras_require={  # Optional\n    #     'dev': ['check-manifest'],\n    #     'test': ['coverage'],\n    # },\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.\n    #\n    # If using Python 2.6 or earlier, then these have to be included in\n    # MANIFEST.in as well.\n    # package_data={  # Optional\n    #     'sample': ['package_data.dat'],\n    # },\n\n    # Although 'package_data' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files\n    #\n    # In this case, 'data_file' will be installed into '<sys.prefix>/my_data'\n    #data_files=[('my_data', ['data/data_file'])],  # Optional\n\n    # To provide executable scripts, use entry points in preference to the\n    # \"scripts\" keyword. Entry points provide cross-platform support and allow\n    # `pip` to create the appropriate form of executable for the target\n    # platform.\n    #\n    # For example, the following would provide a command called `sample` which\n    # executes the function `main` from this package when invoked:\n    # entry_points={  # Optional\n    #     'console_scripts': [\n    #         'sample=sample:main',\n    #     ],\n    # },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}