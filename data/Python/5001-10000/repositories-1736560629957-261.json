{
  "metadata": {
    "timestamp": 1736560629957,
    "page": 261,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jwyang/faster-rcnn.pytorch",
      "stars": 7729,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.8212890625,
          "content": "data/*\n\n# READ THIS BEFORE YOU REFACTOR ME\n#\n# setup.py uses the list of patterns in this file to decide\n# what to delete, but it's not 100% sound.  So, for example,\n# if you delete aten/build/ because it's redundant with build/,\n# aten/build/ will stop being cleaned.  So be careful when\n# refactoring this file!\n\n## PyTorch\n\n.mypy_cache\n*.pyc\n*/*.pyc\n*/*.so*\n*/**/__pycache__\n*/**/*.dylib*\n*/**/*.pyc\n*/**/*.pyd\n*/**/*.so*\n*/**/**/*.pyc\n*/**/**/**/*.pyc\n*/**/**/**/**/*.pyc\naten/build/\naten/src/ATen/Config.h\naten/src/ATen/cuda/CUDAConfig.h\nbuild/\ndist/\ndocs/src/**/*\ntest/.coverage\ntest/cpp/api/mnist\ntest/data/gpu_tensors.pt\ntest/data/legacy_modules.t7\ntest/data/legacy_serialized.pt\ntest/data/linear.pt\ntest/htmlcov\nthird_party/build/\ntools/shared/_utils_internal.py\ntorch.egg-info/\ntorch/csrc/autograd/generated/*\ntorch/csrc/cudnn/cuDNN.cpp\ntorch/csrc/generated\ntorch/csrc/generic/TensorMethods.cpp\ntorch/csrc/jit/generated/*\ntorch/csrc/nn/THCUNN.cpp\ntorch/csrc/nn/THCUNN.cwrap\ntorch/csrc/nn/THNN_generic.cpp\ntorch/csrc/nn/THNN_generic.cwrap\ntorch/csrc/nn/THNN_generic.h\ntorch/csrc/nn/THNN.cpp\ntorch/csrc/nn/THNN.cwrap\ntorch/lib/*.a*\ntorch/lib/*.dll*\ntorch/lib/*.dylib*\ntorch/lib/*.h\ntorch/lib/*.lib\ntorch/lib/*.so*\ntorch/lib/build\ntorch/lib/cmake\ntorch/lib/include\ntorch/lib/pkgconfig\ntorch/lib/protoc\ntorch/lib/tmp_install\ntorch/lib/torch_shm_manager\ntorch/version.py\n\n# IPython notebook checkpoints\n.ipynb_checkpoints\n\n# Editor temporaries\n*.swn\n*.swo\n*.swp\n*.swm\n*~\n\n# macOS dir files\n.DS_Store\n\n# Symbolic files\ntools/shared/cwrap_common.py\n\n# Ninja files\n.ninja_deps\n.ninja_log\ncompile_commands.json\n*.egg-info/\ndocs/source/scripts/activation_images/\n\n## General\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.cuo\n*.obj\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Compiled protocol buffers\n*.pb.h\n*.pb.cc\n*_pb2.py\n\n# Compiled python\n*.pyc\n*.pyd\n\n# Compiled MATLAB\n*.mex*\n\n# IPython notebook checkpoints\n.ipynb_checkpoints\n\n# Editor temporaries\n*.swn\n*.swo\n*.swp\n*~\n\n# Sublime Text settings\n*.sublime-workspace\n*.sublime-project\n\n# Eclipse Project settings\n*.*project\n.settings\n\n# QtCreator files\n*.user\n\n# PyCharm files\n.idea\n\n# Visual Studio Code files\n.vscode\n.vs\n\n# OSX dir files\n.DS_Store\n\n## Caffe2\n\n# build, distribute, and bins (+ python proto bindings)\nbuild\nbuild_host_protoc\nbuild_android\nbuild_ios\n/build_*\n.build_debug/*\n.build_release/*\ndistribute/*\n*.testbin\n*.bin\ncmake_build\n.cmake_build\ngen\n.setuptools-cmake-build\n.pytest_cache\naten/build/*\n\n# Bram\nplsdontbreak\n\n# Generated documentation\ndocs/_site\ndocs/gathered\n_site\ndoxygen\ndocs/dev\n\n# LevelDB files\n*.sst\n*.ldb\nLOCK\nLOG*\nCURRENT\nMANIFEST-*\n\n# generated version file\ncaffe2/version.py\n\n# setup.py intermediates\n.eggs\ncaffe2.egg-info\n\n# Atom/Watchman required file\n.watchmanconfig\n\n# cython generated files\nlib/model/utils/bbox.c\nlib/pycocotools/_mask.c"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2017 Jianwei Yang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.6005859375,
          "content": "# A *Faster* Pytorch Implementation of Faster R-CNN\n\n## Write at the beginning\n\n[05/29/2020] This repo was initaited about two years ago, developed as the first open-sourced object detection code which supports multi-gpu training. It has been integrating tremendous efforts from many people. However, we have seen many high-quality repos emerged in the last years, such as:\n\n* [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)\n* [detectron2](https://github.com/facebookresearch/detectron2)\n* [mmdetection](https://github.com/open-mmlab/mmdetection)\n\n**At this point, I think this repo is out-of-data in terms of the pipeline and coding style, and will not maintain actively. Though you can still use this repo as a playground, I highly recommend you move to the above repos to delve into west world of object detection!**\n\n## Introduction\n\n### :boom: Good news! This repo supports pytorch-1.0 now!!! We borrowed some code and techniques from [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Just go to pytorch-1.0 branch!\n\nThis project is a *faster* pytorch implementation of faster R-CNN, aimed to accelerating the training of faster R-CNN object detection models. Recently, there are a number of good implementations:\n\n* [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn), developed based on Pycaffe + Numpy\n\n* [longcw/faster_rcnn_pytorch](https://github.com/longcw/faster_rcnn_pytorch), developed based on Pytorch + Numpy\n\n* [endernewton/tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn), developed based on TensorFlow + Numpy\n\n* [ruotianluo/pytorch-faster-rcnn](https://github.com/ruotianluo/pytorch-faster-rcnn), developed based on Pytorch + TensorFlow + Numpy\n\nDuring our implementing, we referred the above implementations, especailly [longcw/faster_rcnn_pytorch](https://github.com/longcw/faster_rcnn_pytorch). However, our implementation has several unique and new features compared with the above implementations:\n\n* **It is pure Pytorch code**. We convert all the numpy implementations to pytorch!\n\n* **It supports multi-image batch training**. We revise all the layers, including dataloader, rpn, roi-pooling, etc., to support multiple images in each minibatch.\n\n* **It supports multiple GPUs training**. We use a multiple GPU wrapper (nn.DataParallel here) to make it flexible to use one or more GPUs, as a merit of the above two features.\n\n* **It supports three pooling methods**. We integrate three pooling methods: roi pooing, roi align and roi crop. More importantly, we modify all of them to support multi-image batch training.\n\n* **It is memory efficient**. We limit the image aspect ratio, and group images with similar aspect ratios into a minibatch. As such, we can train resnet101 and VGG16 with batchsize = 4 (4 images) on a single Titan X (12 GB). When training with 8 GPU, the maximum batchsize for each GPU is 3 (Res101), totaling 24.\n\n* **It is faster**. Based on the above modifications, the training is much faster. We report the training speed on NVIDIA TITAN Xp in the tables below.\n\n### What we are doing and going to do\n\n- [x] Support both python2 and python3 (great thanks to [cclauss](https://github.com/cclauss)).\n- [x] Add deformable pooling layer (mainly supported by [Xander](https://github.com/xanderchf)).\n- [x] Support pytorch-0.4.0 (this branch).\n- [x] Support tensorboardX.\n- [x] Support pytorch-1.0 (go to pytorch-1.0 branch).\n\n## Other Implementations\n\n* [Feature Pyramid Network (FPN)](https://github.com/jwyang/fpn.pytorch)\n\n* [Mask R-CNN](https://github.com/roytseng-tw/mask-rcnn.pytorch) (~~ongoing~~ already implemented by [roytseng-tw](https://github.com/roytseng-tw))\n\n* [Graph R-CNN](https://github.com/jwyang/graph-rcnn.pytorch) (extension to scene graph generation)\n\n## Tutorial\n\n* [Blog](http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/) by [ankur6ue](https://github.com/ankur6ue)\n\n## Benchmarking\n\nWe benchmark our code thoroughly on three datasets: pascal voc, coco and visual genome, using two different network architectures: vgg16 and resnet101. Below are the results:\n\n1). PASCAL VOC 2007 (Train/Test: 07trainval/07test, scale=600, ROI Align)\n\nmodel    | #GPUs | batch size | lr        | lr_decay | max_epoch     |  time/epoch | mem/GPU | mAP\n---------|--------|-----|--------|-----|-----|-------|--------|-----\n[VGG-16](https://www.dropbox.com/s/6ief4w7qzka6083/faster_rcnn_1_6_10021.pth?dl=0)     | 1 | 1 | 1e-3 | 5   | 6   |  0.76 hr | 3265MB   | 70.1\n[VGG-16](https://www.dropbox.com/s/cpj2nu35am0f9hp/faster_rcnn_1_9_2504.pth?dl=0)     | 1 | 4 | 4e-3 | 8   | 9  |  0.50 hr | 9083MB   | 69.6\n[VGG-16](https://www.dropbox.com/s/1a31y7vicby0kvy/faster_rcnn_1_10_625.pth?dl=0)     | 8 | 16| 1e-2 | 8   | 10  |  0.19 hr | 5291MB   | 69.4\n[VGG-16](https://www.dropbox.com/s/hkj7i6mbhw9tq4k/faster_rcnn_1_11_416.pth?dl=0)     | 8 | 24| 1e-2 | 10  | 11  |  0.16 hr | 11303MB  | 69.2\n[Res-101](https://www.dropbox.com/s/4v3or0054kzl19q/faster_rcnn_1_7_10021.pth?dl=0)   | 1 | 1 | 1e-3 | 5   | 7   |  0.88 hr | 3200 MB  | 75.2\n[Res-101](https://www.dropbox.com/s/8bhldrds3mf0yuj/faster_rcnn_1_10_2504.pth?dl=0)    | 1 | 4 | 4e-3 | 8   | 10  |  0.60 hr | 9700 MB  | 74.9\n[Res-101](https://www.dropbox.com/s/5is50y01m1l9hbu/faster_rcnn_1_10_625.pth?dl=0)    | 8 | 16| 1e-2 | 8   | 10  |  0.23 hr | 8400 MB  | 75.2 \n[Res-101](https://www.dropbox.com/s/cn8gneumg4gjo9i/faster_rcnn_1_12_416.pth?dl=0)    | 8 | 24| 1e-2 | 10  | 12  |  0.17 hr | 10327MB  | 75.1  \n\n\n2). COCO (Train/Test: coco_train+coco_val-minival/minival, scale=800, max_size=1200, ROI Align)\n\nmodel     | #GPUs | batch size |lr        | lr_decay | max_epoch     |  time/epoch | mem/GPU | mAP\n---------|--------|-----|--------|-----|-----|-------|--------|-----\nVGG-16     | 8 | 16    |1e-2| 4   | 6  |  4.9 hr | 7192 MB  | 29.2\n[Res-101](https://www.dropbox.com/s/5if6l7mqsi4rfk9/faster_rcnn_1_6_14657.pth?dl=0)    | 8 | 16    |1e-2| 4   | 6  |  6.0 hr    |10956 MB  | 36.2\n[Res-101](https://www.dropbox.com/s/be0isevd22eikqb/faster_rcnn_1_10_14657.pth?dl=0)    | 8 | 16    |1e-2| 4   | 10  |  6.0 hr    |10956 MB  | 37.0\n\n**NOTE**. Since the above models use scale=800, you need add \"--ls\" at the end of test command.\n\n3). COCO (Train/Test: coco_train+coco_val-minival/minival, scale=600, max_size=1000, ROI Align)\n\nmodel     | #GPUs | batch size |lr        | lr_decay | max_epoch     |  time/epoch | mem/GPU | mAP\n---------|--------|-----|--------|-----|-----|-------|--------|-----\n[Res-101](https://www.dropbox.com/s/y171ze1sdw1o2ph/faster_rcnn_1_6_9771.pth?dl=0)    | 8 | 24    |1e-2| 4   | 6  |  5.4 hr    |10659 MB  | 33.9\n[Res-101](https://www.dropbox.com/s/dpq6qv0efspelr3/faster_rcnn_1_10_9771.pth?dl=0)    | 8 | 24    |1e-2| 4   | 10  |  5.4 hr    |10659 MB  | 34.5\n\n4). Visual Genome (Train/Test: vg_train/vg_test, scale=600, max_size=1000, ROI Align, category=2500)\n\nmodel     | #GPUs | batch size |lr        | lr_decay | max_epoch     |  time/epoch | mem/GPU | mAP\n---------|--------|-----|--------|-----|-----|-------|--------|-----\n[VGG-16](http://data.lip6.fr/cadene/faster-rcnn.pytorch/faster_rcnn_1_19_48611.pth)    | 1 P100 | 4    |1e-3| 5   | 20  |  3.7 hr    |12707 MB  | 4.4\n\nThanks to [Remi](https://github.com/Cadene) for providing the pretrained detection model on visual genome!\n\n* Click the links in the above tables to download our pre-trained faster r-cnn models.\n* If not mentioned, the GPU we used is NVIDIA Titan X Pascal (12GB).\n\n## Preparation\n\n\nFirst of all, clone the code\n```\ngit clone https://github.com/jwyang/faster-rcnn.pytorch.git\n```\n\nThen, create a folder:\n```\ncd faster-rcnn.pytorch && mkdir data\n```\n\n### prerequisites\n\n* Python 2.7 or 3.6\n* Pytorch 0.4.0 (**now it does not support 0.4.1 or higher**)\n* CUDA 8.0 or higher\n\n### Data Preparation\n\n* **PASCAL_VOC 07+12**: Please follow the instructions in [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to prepare VOC datasets. Actually, you can refer to any others. After downloading the data, create softlinks in the folder data/.\n\n* **COCO**: Please also follow the instructions in [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to prepare the data.\n\n* **Visual Genome**: Please follow the instructions in [bottom-up-attention](https://github.com/peteanderson80/bottom-up-attention) to prepare Visual Genome dataset. You need to download the images and object annotation files first, and then perform proprecessing to obtain the vocabulary and cleansed annotations based on the scripts provided in this repository.\n\n### Pretrained Model\n\nWe used two pretrained models in our experiments, VGG and ResNet101. You can download these two models from:\n\n* VGG16: [Dropbox](https://www.dropbox.com/s/s3brpk0bdq60nyb/vgg16_caffe.pth?dl=0), [VT Server](https://filebox.ece.vt.edu/~jw2yang/faster-rcnn/pretrained-base-models/vgg16_caffe.pth)\n\n* ResNet101: [Dropbox](https://www.dropbox.com/s/iev3tkbz5wyyuz9/resnet101_caffe.pth?dl=0), [VT Server](https://filebox.ece.vt.edu/~jw2yang/faster-rcnn/pretrained-base-models/resnet101_caffe.pth)\n\nDownload them and put them into the data/pretrained_model/.\n\n**NOTE**. We compare the pretrained models from Pytorch and Caffe, and surprisingly find Caffe pretrained models have slightly better performance than Pytorch pretrained. We would suggest to use Caffe pretrained models from the above link to reproduce our results.\n\n**If you want to use pytorch pre-trained models, please remember to transpose images from BGR to RGB, and also use the same data transformer (minus mean and normalize) as used in pretrained model.**\n\n### Compilation\n\nAs pointed out by [ruotianluo/pytorch-faster-rcnn](https://github.com/ruotianluo/pytorch-faster-rcnn), choose the right `-arch` in `make.sh` file, to compile the cuda code:\n\n  | GPU model  | Architecture |\n  | ------------- | ------------- |\n  | TitanX (Maxwell/Pascal) | sm_52 |\n  | GTX 960M | sm_50 |\n  | GTX 1080 (Ti) | sm_61 |\n  | Grid K520 (AWS g2.2xlarge) | sm_30 |\n  | Tesla K80 (AWS p2.xlarge) | sm_37 |\n\nMore details about setting the architecture can be found [here](https://developer.nvidia.com/cuda-gpus) or [here](http://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/)\n\nInstall all the python dependencies using pip:\n```\npip install -r requirements.txt\n```\n\nCompile the cuda dependencies using following simple commands:\n\n```\ncd lib\nsh make.sh\n```\n\nIt will compile all the modules you need, including NMS, ROI_Pooing, ROI_Align and ROI_Crop. The default version is compiled with Python 2.7, please compile by yourself if you are using a different python version.\n\n**As pointed out in this [issue](https://github.com/jwyang/faster-rcnn.pytorch/issues/16), if you encounter some error during the compilation, you might miss to export the CUDA paths to your environment.**\n\n## Train\n\nBefore training, set the right directory to save and load the trained models. Change the arguments \"save_dir\" and \"load_dir\" in trainval_net.py and test_net.py to adapt to your environment.\n\nTo train a faster R-CNN model with vgg16 on pascal_voc, simply run:\n```\nCUDA_VISIBLE_DEVICES=$GPU_ID python trainval_net.py \\\n                   --dataset pascal_voc --net vgg16 \\\n                   --bs $BATCH_SIZE --nw $WORKER_NUMBER \\\n                   --lr $LEARNING_RATE --lr_decay_step $DECAY_STEP \\\n                   --cuda\n```\nwhere 'bs' is the batch size with default 1. Alternatively, to train with resnet101 on pascal_voc, simple run:\n```\n CUDA_VISIBLE_DEVICES=$GPU_ID python trainval_net.py \\\n                    --dataset pascal_voc --net res101 \\\n                    --bs $BATCH_SIZE --nw $WORKER_NUMBER \\\n                    --lr $LEARNING_RATE --lr_decay_step $DECAY_STEP \\\n                    --cuda\n```\nAbove, BATCH_SIZE and WORKER_NUMBER can be set adaptively according to your GPU memory size. **On Titan Xp with 12G memory, it can be up to 4**.\n\nIf you have multiple (say 8) Titan Xp GPUs, then just use them all! Try:\n```\npython trainval_net.py --dataset pascal_voc --net vgg16 \\\n                       --bs 24 --nw 8 \\\n                       --lr $LEARNING_RATE --lr_decay_step $DECAY_STEP \\\n                       --cuda --mGPUs\n\n```\n\nChange dataset to \"coco\" or 'vg' if you want to train on COCO or Visual Genome.\n\n## Test\n\nIf you want to evaluate the detection performance of a pre-trained vgg16 model on pascal_voc test set, simply run\n```\npython test_net.py --dataset pascal_voc --net vgg16 \\\n                   --checksession $SESSION --checkepoch $EPOCH --checkpoint $CHECKPOINT \\\n                   --cuda\n```\nSpecify the specific model session, checkepoch and checkpoint, e.g., SESSION=1, EPOCH=6, CHECKPOINT=416.\n\n## Demo\n\nIf you want to run detection on your own images with a pre-trained model, download the pretrained model listed in above tables or train your own models at first, then add images to folder $ROOT/images, and then run\n```\npython demo.py --net vgg16 \\\n               --checksession $SESSION --checkepoch $EPOCH --checkpoint $CHECKPOINT \\\n               --cuda --load_dir path/to/model/directoy\n```\n\nThen you will find the detection results in folder $ROOT/images.\n\n**Note the default demo.py merely support pascal_voc categories. You need to change the [line](https://github.com/jwyang/faster-rcnn.pytorch/blob/530f3fdccaa60d05fa068bc2148695211586bd88/demo.py#L156) to adapt your own model.**\n\nBelow are some detection results:\n\n<div style=\"color:#0000FF\" align=\"center\">\n<img src=\"images/img3_det_res101.jpg\" width=\"430\"/> <img src=\"images/img4_det_res101.jpg\" width=\"430\"/>\n</div>\n\n## Webcam Demo\n\nYou can use a webcam in a real-time demo by running\n```\npython demo.py --net vgg16 \\\n               --checksession $SESSION --checkepoch $EPOCH --checkpoint $CHECKPOINT \\\n               --cuda --load_dir path/to/model/directoy \\\n               --webcam $WEBCAM_ID\n```\nThe demo is stopped by clicking the image window and then pressing the 'q' key.\n\n## Authorship\n\nThis project is equally contributed by [Jianwei Yang](https://github.com/jwyang) and [Jiasen Lu](https://github.com/jiasenlu), and many others (thanks to them!).\n\n## Citation\n\n    @article{jjfaster2rcnn,\n        Author = {Jianwei Yang and Jiasen Lu and Dhruv Batra and Devi Parikh},\n        Title = {A Faster Pytorch Implementation of Faster R-CNN},\n        Journal = {https://github.com/jwyang/faster-rcnn.pytorch},\n        Year = {2017}\n    }\n\n    @inproceedings{renNIPS15fasterrcnn,\n        Author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},\n        Title = {Faster {R-CNN}: Towards Real-Time Object Detection\n                 with Region Proposal Networks},\n        Booktitle = {Advances in Neural Information Processing Systems ({NIPS})},\n        Year = {2015}\n    }\n"
        },
        {
          "name": "_init_paths.py",
          "type": "blob",
          "size": 0.3046875,
          "content": "import os.path as osp\nimport sys\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nthis_dir = osp.dirname(__file__)\n\n# Add lib to PYTHONPATH\nlib_path = osp.join(this_dir, 'lib')\nadd_path(lib_path)\n\ncoco_path = osp.join(this_dir, 'data', 'coco', 'PythonAPI')\nadd_path(coco_path)\n"
        },
        {
          "name": "cfgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 13.37890625,
          "content": "# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport pprint\nimport pdb\nimport time\nimport cv2\nimport imutils\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dset\nfrom scipy.misc import imread\nfrom roi_data_layer.roidb import combined_roidb\nfrom roi_data_layer.roibatchLoader import roibatchLoader\nfrom model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\nfrom model.rpn.bbox_transform import clip_boxes\nfrom model.nms.nms_wrapper import nms\nfrom model.rpn.bbox_transform import bbox_transform_inv\nfrom model.utils.net_utils import save_net, load_net, vis_detections\nfrom model.utils.blob import im_list_to_blob\nfrom model.faster_rcnn.vgg16 import vgg16\nfrom model.faster_rcnn.resnet import resnet\nimport pdb\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\ndef parse_args():\n  \"\"\"\n  Parse input arguments\n  \"\"\"\n  parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n  parser.add_argument('--dataset', dest='dataset',\n                      help='training dataset',\n                      default='pascal_voc', type=str)\n  parser.add_argument('--cfg', dest='cfg_file',\n                      help='optional config file',\n                      default='cfgs/vgg16.yml', type=str)\n  parser.add_argument('--net', dest='net',\n                      help='vgg16, res50, res101, res152',\n                      default='res101', type=str)\n  parser.add_argument('--set', dest='set_cfgs',\n                      help='set config keys', default=None,\n                      nargs=argparse.REMAINDER)\n  parser.add_argument('--load_dir', dest='load_dir',\n                      help='directory to load models',\n                      default=\"/srv/share/jyang375/models\")\n  parser.add_argument('--image_dir', dest='image_dir',\n                      help='directory to load images for demo',\n                      default=\"images\")\n  parser.add_argument('--cuda', dest='cuda',\n                      help='whether use CUDA',\n                      action='store_true')\n  parser.add_argument('--mGPUs', dest='mGPUs',\n                      help='whether use multiple GPUs',\n                      action='store_true')\n  parser.add_argument('--cag', dest='class_agnostic',\n                      help='whether perform class_agnostic bbox regression',\n                      action='store_true')\n  parser.add_argument('--parallel_type', dest='parallel_type',\n                      help='which part of model to parallel, 0: all, 1: model before roi pooling',\n                      default=0, type=int)\n  parser.add_argument('--checksession', dest='checksession',\n                      help='checksession to load model',\n                      default=1, type=int)\n  parser.add_argument('--checkepoch', dest='checkepoch',\n                      help='checkepoch to load network',\n                      default=1, type=int)\n  parser.add_argument('--checkpoint', dest='checkpoint',\n                      help='checkpoint to load network',\n                      default=10021, type=int)\n  parser.add_argument('--bs', dest='batch_size',\n                      help='batch_size',\n                      default=1, type=int)\n  parser.add_argument('--vis', dest='vis',\n                      help='visualization mode',\n                      action='store_true')\n  parser.add_argument('--webcam_num', dest='webcam_num',\n                      help='webcam ID number',\n                      default=-1, type=int)\n\n  args = parser.parse_args()\n  return args\n\nlr = cfg.TRAIN.LEARNING_RATE\nmomentum = cfg.TRAIN.MOMENTUM\nweight_decay = cfg.TRAIN.WEIGHT_DECAY\n\ndef _get_image_blob(im):\n  \"\"\"Converts an image into a network input.\n  Arguments:\n    im (ndarray): a color image in BGR order\n  Returns:\n    blob (ndarray): a data blob holding an image pyramid\n    im_scale_factors (list): list of image scales (relative to im) used\n      in the image pyramid\n  \"\"\"\n  im_orig = im.astype(np.float32, copy=True)\n  im_orig -= cfg.PIXEL_MEANS\n\n  im_shape = im_orig.shape\n  im_size_min = np.min(im_shape[0:2])\n  im_size_max = np.max(im_shape[0:2])\n\n  processed_ims = []\n  im_scale_factors = []\n\n  for target_size in cfg.TEST.SCALES:\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n      im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n            interpolation=cv2.INTER_LINEAR)\n    im_scale_factors.append(im_scale)\n    processed_ims.append(im)\n\n  # Create a blob to hold the input images\n  blob = im_list_to_blob(processed_ims)\n\n  return blob, np.array(im_scale_factors)\n\nif __name__ == '__main__':\n\n  args = parse_args()\n\n  print('Called with args:')\n  print(args)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  cfg.USE_GPU_NMS = args.cuda\n\n  print('Using config:')\n  pprint.pprint(cfg)\n  np.random.seed(cfg.RNG_SEED)\n\n  # train set\n  # -- Note: Use validation set and disable the flipped to enable faster loading.\n\n  input_dir = args.load_dir + \"/\" + args.net + \"/\" + args.dataset\n  if not os.path.exists(input_dir):\n    raise Exception('There is no input directory for loading network from ' + input_dir)\n  load_name = os.path.join(input_dir,\n    'faster_rcnn_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))\n\n  pascal_classes = np.asarray(['__background__',\n                       'aeroplane', 'bicycle', 'bird', 'boat',\n                       'bottle', 'bus', 'car', 'cat', 'chair',\n                       'cow', 'diningtable', 'dog', 'horse',\n                       'motorbike', 'person', 'pottedplant',\n                       'sheep', 'sofa', 'train', 'tvmonitor'])\n\n  # initilize the network here.\n  if args.net == 'vgg16':\n    fasterRCNN = vgg16(pascal_classes, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res101':\n    fasterRCNN = resnet(pascal_classes, 101, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res50':\n    fasterRCNN = resnet(pascal_classes, 50, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res152':\n    fasterRCNN = resnet(pascal_classes, 152, pretrained=False, class_agnostic=args.class_agnostic)\n  else:\n    print(\"network is not defined\")\n    pdb.set_trace()\n\n  fasterRCNN.create_architecture()\n\n  print(\"load checkpoint %s\" % (load_name))\n  if args.cuda > 0:\n    checkpoint = torch.load(load_name)\n  else:\n    checkpoint = torch.load(load_name, map_location=(lambda storage, loc: storage))\n  fasterRCNN.load_state_dict(checkpoint['model'])\n  if 'pooling_mode' in checkpoint.keys():\n    cfg.POOLING_MODE = checkpoint['pooling_mode']\n\n\n  print('load model successfully!')\n\n  # pdb.set_trace()\n\n  print(\"load checkpoint %s\" % (load_name))\n\n  # initilize the tensor holder here.\n  im_data = torch.FloatTensor(1)\n  im_info = torch.FloatTensor(1)\n  num_boxes = torch.LongTensor(1)\n  gt_boxes = torch.FloatTensor(1)\n\n  # ship to cuda\n  if args.cuda > 0:\n    im_data = im_data.cuda()\n    im_info = im_info.cuda()\n    num_boxes = num_boxes.cuda()\n    gt_boxes = gt_boxes.cuda()\n\n  # make variable\n  with torch.no_grad():\n    im_data = Variable(im_data)\n    im_info = Variable(im_info)\n    num_boxes = Variable(num_boxes)\n    gt_boxes = Variable(gt_boxes)\n\n  if args.cuda > 0:\n    cfg.CUDA = True\n\n  if args.cuda > 0:\n    fasterRCNN.cuda()\n\n  fasterRCNN.eval()\n\n  start = time.time()\n  max_per_image = 100\n  thresh = 0.05\n  vis = True\n\n  webcam_num = args.webcam_num\n  # Set up webcam or get image directories\n  if webcam_num >= 0 :\n    cap = cv2.VideoCapture(webcam_num)\n    num_images = 0\n  else:\n    imglist = os.listdir(args.image_dir)\n    num_images = len(imglist)\n\n  print('Loaded Photo: {} images.'.format(num_images))\n\n\n  while (num_images >= 0):\n      total_tic = time.time()\n      if webcam_num == -1:\n        num_images -= 1\n\n      # Get image from the webcam\n      if webcam_num >= 0:\n        if not cap.isOpened():\n          raise RuntimeError(\"Webcam could not open. Please check connection.\")\n        ret, frame = cap.read()\n        im_in = np.array(frame)\n      # Load the demo image\n      else:\n        im_file = os.path.join(args.image_dir, imglist[num_images])\n        # im = cv2.imread(im_file)\n        im_in = np.array(imread(im_file))\n        if len(im_in.shape) == 2:\n          im_in = im_in[:,:,np.newaxis]\n          im_in = np.concatenate((im_in,im_in,im_in), axis=2)\n        # rgb -> bgr\n        im_in = im_in[:,:,::-1]\n      im = im_in\n\n      blobs, im_scales = _get_image_blob(im)\n      assert len(im_scales) == 1, \"Only single-image batch implemented\"\n      im_blob = blobs\n      im_info_np = np.array([[im_blob.shape[1], im_blob.shape[2], im_scales[0]]], dtype=np.float32)\n\n      im_data_pt = torch.from_numpy(im_blob)\n      im_data_pt = im_data_pt.permute(0, 3, 1, 2)\n      im_info_pt = torch.from_numpy(im_info_np)\n\n      im_data.data.resize_(im_data_pt.size()).copy_(im_data_pt)\n      im_info.data.resize_(im_info_pt.size()).copy_(im_info_pt)\n      gt_boxes.data.resize_(1, 1, 5).zero_()\n      num_boxes.data.resize_(1).zero_()\n\n      # pdb.set_trace()\n      det_tic = time.time()\n\n      rois, cls_prob, bbox_pred, \\\n      rpn_loss_cls, rpn_loss_box, \\\n      RCNN_loss_cls, RCNN_loss_bbox, \\\n      rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n\n      scores = cls_prob.data\n      boxes = rois.data[:, :, 1:5]\n\n      if cfg.TEST.BBOX_REG:\n          # Apply bounding-box regression deltas\n          box_deltas = bbox_pred.data\n          if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n          # Optionally normalize targets by a precomputed mean and stdev\n            if args.class_agnostic:\n                if args.cuda > 0:\n                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                else:\n                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS) \\\n                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS)\n\n                box_deltas = box_deltas.view(1, -1, 4)\n            else:\n                if args.cuda > 0:\n                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                else:\n                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS) \\\n                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS)\n                box_deltas = box_deltas.view(1, -1, 4 * len(pascal_classes))\n\n          pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n          pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n      else:\n          # Simply repeat the boxes, once for each class\n          _ = torch.from_numpy(np.tile(boxes, (1, scores.shape[1])))\n          pred_boxes = _.cuda() if args.cuda > 0 else _\n\n      pred_boxes /= im_scales[0]\n\n      scores = scores.squeeze()\n      pred_boxes = pred_boxes.squeeze()\n      det_toc = time.time()\n      detect_time = det_toc - det_tic\n      misc_tic = time.time()\n      if vis:\n          im2show = np.copy(im)\n      for j in xrange(1, len(pascal_classes)):\n          inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n          # if there is det\n          if inds.numel() > 0:\n            cls_scores = scores[:,j][inds]\n            _, order = torch.sort(cls_scores, 0, True)\n            if args.class_agnostic:\n              cls_boxes = pred_boxes[inds, :]\n            else:\n              cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n            \n            cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n            # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n            cls_dets = cls_dets[order]\n            keep = nms(cls_dets, cfg.TEST.NMS, force_cpu=not cfg.USE_GPU_NMS)\n            cls_dets = cls_dets[keep.view(-1).long()]\n            if vis:\n              im2show = vis_detections(im2show, pascal_classes[j], cls_dets.cpu().numpy(), 0.5)\n\n      misc_toc = time.time()\n      nms_time = misc_toc - misc_tic\n\n      if webcam_num == -1:\n          sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n                           .format(num_images + 1, len(imglist), detect_time, nms_time))\n          sys.stdout.flush()\n\n      if vis and webcam_num == -1:\n          # cv2.imshow('test', im2show)\n          # cv2.waitKey(0)\n          result_path = os.path.join(args.image_dir, imglist[num_images][:-4] + \"_det.jpg\")\n          cv2.imwrite(result_path, im2show)\n      else:\n          cv2.imshow(\"frame\", im2show)\n          total_toc = time.time()\n          total_time = total_toc - total_tic\n          frame_rate = 1 / total_time\n          print('Frame rate:', frame_rate)\n          if cv2.waitKey(1) & 0xFF == ord('q'):\n              break\n  if webcam_num >= 0:\n      cap.release()\n      cv2.destroyAllWindows()\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.078125,
          "content": "cython\ncffi\nopencv-python\nscipy\nmsgpack\neasydict\nmatplotlib\npyyaml\ntensorboardX\n"
        },
        {
          "name": "test_net.py",
          "type": "blob",
          "size": 11.94921875,
          "content": "# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport pprint\nimport pdb\nimport time\n\nimport cv2\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport pickle\nfrom roi_data_layer.roidb import combined_roidb\nfrom roi_data_layer.roibatchLoader import roibatchLoader\nfrom model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\nfrom model.rpn.bbox_transform import clip_boxes\nfrom model.nms.nms_wrapper import nms\nfrom model.rpn.bbox_transform import bbox_transform_inv\nfrom model.utils.net_utils import save_net, load_net, vis_detections\n\nfrom model.faster_rcnn.vgg16 import vgg16\nfrom model.faster_rcnn.resnet import resnet\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\ndef parse_args():\n  \"\"\"\n  Parse input arguments\n  \"\"\"\n  parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n  parser.add_argument('--dataset', dest='dataset',\n                      help='training dataset',\n                      default='pascal_voc', type=str)\n  parser.add_argument('--cfg', dest='cfg_file',\n                      help='optional config file',\n                      default='cfgs/vgg16.yml', type=str)\n  parser.add_argument('--net', dest='net',\n                      help='vgg16, res50, res101, res152',\n                      default='res101', type=str)\n  parser.add_argument('--set', dest='set_cfgs',\n                      help='set config keys', default=None,\n                      nargs=argparse.REMAINDER)\n  parser.add_argument('--load_dir', dest='load_dir',\n                      help='directory to load models', default=\"models\",\n                      type=str)\n  parser.add_argument('--cuda', dest='cuda',\n                      help='whether use CUDA',\n                      action='store_true')\n  parser.add_argument('--ls', dest='large_scale',\n                      help='whether use large imag scale',\n                      action='store_true')\n  parser.add_argument('--mGPUs', dest='mGPUs',\n                      help='whether use multiple GPUs',\n                      action='store_true')\n  parser.add_argument('--cag', dest='class_agnostic',\n                      help='whether perform class_agnostic bbox regression',\n                      action='store_true')\n  parser.add_argument('--parallel_type', dest='parallel_type',\n                      help='which part of model to parallel, 0: all, 1: model before roi pooling',\n                      default=0, type=int)\n  parser.add_argument('--checksession', dest='checksession',\n                      help='checksession to load model',\n                      default=1, type=int)\n  parser.add_argument('--checkepoch', dest='checkepoch',\n                      help='checkepoch to load network',\n                      default=1, type=int)\n  parser.add_argument('--checkpoint', dest='checkpoint',\n                      help='checkpoint to load network',\n                      default=10021, type=int)\n  parser.add_argument('--vis', dest='vis',\n                      help='visualization mode',\n                      action='store_true')\n  args = parser.parse_args()\n  return args\n\nlr = cfg.TRAIN.LEARNING_RATE\nmomentum = cfg.TRAIN.MOMENTUM\nweight_decay = cfg.TRAIN.WEIGHT_DECAY\n\nif __name__ == '__main__':\n\n  args = parse_args()\n\n  print('Called with args:')\n  print(args)\n\n  if torch.cuda.is_available() and not args.cuda:\n    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n\n  np.random.seed(cfg.RNG_SEED)\n  if args.dataset == \"pascal_voc\":\n      args.imdb_name = \"voc_2007_trainval\"\n      args.imdbval_name = \"voc_2007_test\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n  elif args.dataset == \"pascal_voc_0712\":\n      args.imdb_name = \"voc_2007_trainval+voc_2012_trainval\"\n      args.imdbval_name = \"voc_2007_test\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n  elif args.dataset == \"coco\":\n      args.imdb_name = \"coco_2014_train+coco_2014_valminusminival\"\n      args.imdbval_name = \"coco_2014_minival\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n  elif args.dataset == \"imagenet\":\n      args.imdb_name = \"imagenet_train\"\n      args.imdbval_name = \"imagenet_val\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n  elif args.dataset == \"vg\":\n      args.imdb_name = \"vg_150-50-50_minitrain\"\n      args.imdbval_name = \"vg_150-50-50_minival\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n\n  args.cfg_file = \"cfgs/{}_ls.yml\".format(args.net) if args.large_scale else \"cfgs/{}.yml\".format(args.net)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print('Using config:')\n  pprint.pprint(cfg)\n\n  cfg.TRAIN.USE_FLIPPED = False\n  imdb, roidb, ratio_list, ratio_index = combined_roidb(args.imdbval_name, False)\n  imdb.competition_mode(on=True)\n\n  print('{:d} roidb entries'.format(len(roidb)))\n\n  input_dir = args.load_dir + \"/\" + args.net + \"/\" + args.dataset\n  if not os.path.exists(input_dir):\n    raise Exception('There is no input directory for loading network from ' + input_dir)\n  load_name = os.path.join(input_dir,\n    'faster_rcnn_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))\n\n  # initilize the network here.\n  if args.net == 'vgg16':\n    fasterRCNN = vgg16(imdb.classes, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res101':\n    fasterRCNN = resnet(imdb.classes, 101, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res50':\n    fasterRCNN = resnet(imdb.classes, 50, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res152':\n    fasterRCNN = resnet(imdb.classes, 152, pretrained=False, class_agnostic=args.class_agnostic)\n  else:\n    print(\"network is not defined\")\n    pdb.set_trace()\n\n  fasterRCNN.create_architecture()\n\n  print(\"load checkpoint %s\" % (load_name))\n  checkpoint = torch.load(load_name)\n  fasterRCNN.load_state_dict(checkpoint['model'])\n  if 'pooling_mode' in checkpoint.keys():\n    cfg.POOLING_MODE = checkpoint['pooling_mode']\n\n\n  print('load model successfully!')\n  # initilize the tensor holder here.\n  im_data = torch.FloatTensor(1)\n  im_info = torch.FloatTensor(1)\n  num_boxes = torch.LongTensor(1)\n  gt_boxes = torch.FloatTensor(1)\n\n  # ship to cuda\n  if args.cuda:\n    im_data = im_data.cuda()\n    im_info = im_info.cuda()\n    num_boxes = num_boxes.cuda()\n    gt_boxes = gt_boxes.cuda()\n\n  # make variable\n  im_data = Variable(im_data)\n  im_info = Variable(im_info)\n  num_boxes = Variable(num_boxes)\n  gt_boxes = Variable(gt_boxes)\n\n  if args.cuda:\n    cfg.CUDA = True\n\n  if args.cuda:\n    fasterRCNN.cuda()\n\n  start = time.time()\n  max_per_image = 100\n\n  vis = args.vis\n\n  if vis:\n    thresh = 0.05\n  else:\n    thresh = 0.0\n\n  save_name = 'faster_rcnn_10'\n  num_images = len(imdb.image_index)\n  all_boxes = [[[] for _ in xrange(num_images)]\n               for _ in xrange(imdb.num_classes)]\n\n  output_dir = get_output_dir(imdb, save_name)\n  dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n                        imdb.num_classes, training=False, normalize = False)\n  dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n                            shuffle=False, num_workers=0,\n                            pin_memory=True)\n\n  data_iter = iter(dataloader)\n\n  _t = {'im_detect': time.time(), 'misc': time.time()}\n  det_file = os.path.join(output_dir, 'detections.pkl')\n\n  fasterRCNN.eval()\n  empty_array = np.transpose(np.array([[],[],[],[],[]]), (1,0))\n  for i in range(num_images):\n\n      data = next(data_iter)\n      im_data.data.resize_(data[0].size()).copy_(data[0])\n      im_info.data.resize_(data[1].size()).copy_(data[1])\n      gt_boxes.data.resize_(data[2].size()).copy_(data[2])\n      num_boxes.data.resize_(data[3].size()).copy_(data[3])\n\n      det_tic = time.time()\n      rois, cls_prob, bbox_pred, \\\n      rpn_loss_cls, rpn_loss_box, \\\n      RCNN_loss_cls, RCNN_loss_bbox, \\\n      rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n\n      scores = cls_prob.data\n      boxes = rois.data[:, :, 1:5]\n\n      if cfg.TEST.BBOX_REG:\n          # Apply bounding-box regression deltas\n          box_deltas = bbox_pred.data\n          if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n          # Optionally normalize targets by a precomputed mean and stdev\n            if args.class_agnostic:\n                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                box_deltas = box_deltas.view(1, -1, 4)\n            else:\n                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n\n          pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n          pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n      else:\n          # Simply repeat the boxes, once for each class\n          _ = torch.from_numpy(np.tile(boxes, (1, scores.shape[1])))\n          pred_boxes = _.cuda() if args.cuda > 0 else _\n\n      pred_boxes /= data[1][0][2].item()\n\n      scores = scores.squeeze()\n      pred_boxes = pred_boxes.squeeze()\n      det_toc = time.time()\n      detect_time = det_toc - det_tic\n      misc_tic = time.time()\n      if vis:\n          im = cv2.imread(imdb.image_path_at(i))\n          im2show = np.copy(im)\n      for j in xrange(1, imdb.num_classes):\n          inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n          # if there is det\n          if inds.numel() > 0:\n            cls_scores = scores[:,j][inds]\n            _, order = torch.sort(cls_scores, 0, True)\n            if args.class_agnostic:\n              cls_boxes = pred_boxes[inds, :]\n            else:\n              cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n            \n            cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n            # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n            cls_dets = cls_dets[order]\n            keep = nms(cls_dets, cfg.TEST.NMS)\n            cls_dets = cls_dets[keep.view(-1).long()]\n            if vis:\n              im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n            all_boxes[j][i] = cls_dets.cpu().numpy()\n          else:\n            all_boxes[j][i] = empty_array\n\n      # Limit to max_per_image detections *over all classes*\n      if max_per_image > 0:\n          image_scores = np.hstack([all_boxes[j][i][:, -1]\n                                    for j in xrange(1, imdb.num_classes)])\n          if len(image_scores) > max_per_image:\n              image_thresh = np.sort(image_scores)[-max_per_image]\n              for j in xrange(1, imdb.num_classes):\n                  keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                  all_boxes[j][i] = all_boxes[j][i][keep, :]\n\n      misc_toc = time.time()\n      nms_time = misc_toc - misc_tic\n\n      sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n          .format(i + 1, num_images, detect_time, nms_time))\n      sys.stdout.flush()\n\n      if vis:\n          cv2.imwrite('result.png', im2show)\n          pdb.set_trace()\n          #cv2.imshow('test', im2show)\n          #cv2.waitKey(0)\n\n  with open(det_file, 'wb') as f:\n      pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n  print('Evaluating detections')\n  imdb.evaluate_detections(all_boxes, output_dir)\n\n  end = time.time()\n  print(\"test time: %0.4fs\" % (end - start))\n"
        },
        {
          "name": "trainval_net.py",
          "type": "blob",
          "size": 14.2177734375,
          "content": "# --------------------------------------------------------\n# Pytorch multi-GPU Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport pprint\nimport pdb\nimport time\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import Sampler\n\nfrom roi_data_layer.roidb import combined_roidb\nfrom roi_data_layer.roibatchLoader import roibatchLoader\nfrom model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\nfrom model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n      adjust_learning_rate, save_checkpoint, clip_gradient\n\nfrom model.faster_rcnn.vgg16 import vgg16\nfrom model.faster_rcnn.resnet import resnet\n\ndef parse_args():\n  \"\"\"\n  Parse input arguments\n  \"\"\"\n  parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n  parser.add_argument('--dataset', dest='dataset',\n                      help='training dataset',\n                      default='pascal_voc', type=str)\n  parser.add_argument('--net', dest='net',\n                    help='vgg16, res101',\n                    default='vgg16', type=str)\n  parser.add_argument('--start_epoch', dest='start_epoch',\n                      help='starting epoch',\n                      default=1, type=int)\n  parser.add_argument('--epochs', dest='max_epochs',\n                      help='number of epochs to train',\n                      default=20, type=int)\n  parser.add_argument('--disp_interval', dest='disp_interval',\n                      help='number of iterations to display',\n                      default=100, type=int)\n  parser.add_argument('--checkpoint_interval', dest='checkpoint_interval',\n                      help='number of iterations to display',\n                      default=10000, type=int)\n\n  parser.add_argument('--save_dir', dest='save_dir',\n                      help='directory to save models', default=\"models\",\n                      type=str)\n  parser.add_argument('--nw', dest='num_workers',\n                      help='number of workers to load data',\n                      default=0, type=int)\n  parser.add_argument('--cuda', dest='cuda',\n                      help='whether use CUDA',\n                      action='store_true')\n  parser.add_argument('--ls', dest='large_scale',\n                      help='whether use large imag scale',\n                      action='store_true')                      \n  parser.add_argument('--mGPUs', dest='mGPUs',\n                      help='whether use multiple GPUs',\n                      action='store_true')\n  parser.add_argument('--bs', dest='batch_size',\n                      help='batch_size',\n                      default=1, type=int)\n  parser.add_argument('--cag', dest='class_agnostic',\n                      help='whether to perform class_agnostic bbox regression',\n                      action='store_true')\n\n# config optimization\n  parser.add_argument('--o', dest='optimizer',\n                      help='training optimizer',\n                      default=\"sgd\", type=str)\n  parser.add_argument('--lr', dest='lr',\n                      help='starting learning rate',\n                      default=0.001, type=float)\n  parser.add_argument('--lr_decay_step', dest='lr_decay_step',\n                      help='step to do learning rate decay, unit is epoch',\n                      default=5, type=int)\n  parser.add_argument('--lr_decay_gamma', dest='lr_decay_gamma',\n                      help='learning rate decay ratio',\n                      default=0.1, type=float)\n\n# set training session\n  parser.add_argument('--s', dest='session',\n                      help='training session',\n                      default=1, type=int)\n\n# resume trained model\n  parser.add_argument('--r', dest='resume',\n                      help='resume checkpoint or not',\n                      default=False, type=bool)\n  parser.add_argument('--checksession', dest='checksession',\n                      help='checksession to load model',\n                      default=1, type=int)\n  parser.add_argument('--checkepoch', dest='checkepoch',\n                      help='checkepoch to load model',\n                      default=1, type=int)\n  parser.add_argument('--checkpoint', dest='checkpoint',\n                      help='checkpoint to load model',\n                      default=0, type=int)\n# log and display\n  parser.add_argument('--use_tfb', dest='use_tfboard',\n                      help='whether use tensorboard',\n                      action='store_true')\n\n  args = parser.parse_args()\n  return args\n\n\nclass sampler(Sampler):\n  def __init__(self, train_size, batch_size):\n    self.num_data = train_size\n    self.num_per_batch = int(train_size / batch_size)\n    self.batch_size = batch_size\n    self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n    self.leftover_flag = False\n    if train_size % batch_size:\n      self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n      self.leftover_flag = True\n\n  def __iter__(self):\n    rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n    self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n\n    self.rand_num_view = self.rand_num.view(-1)\n\n    if self.leftover_flag:\n      self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n\n    return iter(self.rand_num_view)\n\n  def __len__(self):\n    return self.num_data\n\nif __name__ == '__main__':\n\n  args = parse_args()\n\n  print('Called with args:')\n  print(args)\n\n  if args.dataset == \"pascal_voc\":\n      args.imdb_name = \"voc_2007_trainval\"\n      args.imdbval_name = \"voc_2007_test\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '20']\n  elif args.dataset == \"pascal_voc_0712\":\n      args.imdb_name = \"voc_2007_trainval+voc_2012_trainval\"\n      args.imdbval_name = \"voc_2007_test\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '20']\n  elif args.dataset == \"coco\":\n      args.imdb_name = \"coco_2014_train+coco_2014_valminusminival\"\n      args.imdbval_name = \"coco_2014_minival\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '50']\n  elif args.dataset == \"imagenet\":\n      args.imdb_name = \"imagenet_train\"\n      args.imdbval_name = \"imagenet_val\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '30']\n  elif args.dataset == \"vg\":\n      # train sizes: train, smalltrain, minitrain\n      # train scale: ['150-50-20', '150-50-50', '500-150-80', '750-250-150', '1750-700-450', '1600-400-20']\n      args.imdb_name = \"vg_150-50-50_minitrain\"\n      args.imdbval_name = \"vg_150-50-50_minival\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '50']\n\n  args.cfg_file = \"cfgs/{}_ls.yml\".format(args.net) if args.large_scale else \"cfgs/{}.yml\".format(args.net)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print('Using config:')\n  pprint.pprint(cfg)\n  np.random.seed(cfg.RNG_SEED)\n\n  #torch.backends.cudnn.benchmark = True\n  if torch.cuda.is_available() and not args.cuda:\n    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n\n  # train set\n  # -- Note: Use validation set and disable the flipped to enable faster loading.\n  cfg.TRAIN.USE_FLIPPED = True\n  cfg.USE_GPU_NMS = args.cuda\n  imdb, roidb, ratio_list, ratio_index = combined_roidb(args.imdb_name)\n  train_size = len(roidb)\n\n  print('{:d} roidb entries'.format(len(roidb)))\n\n  output_dir = args.save_dir + \"/\" + args.net + \"/\" + args.dataset\n  if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n  sampler_batch = sampler(train_size, args.batch_size)\n\n  dataset = roibatchLoader(roidb, ratio_list, ratio_index, args.batch_size, \\\n                           imdb.num_classes, training=True)\n\n  dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size,\n                            sampler=sampler_batch, num_workers=args.num_workers)\n\n  # initilize the tensor holder here.\n  im_data = torch.FloatTensor(1)\n  im_info = torch.FloatTensor(1)\n  num_boxes = torch.LongTensor(1)\n  gt_boxes = torch.FloatTensor(1)\n\n  # ship to cuda\n  if args.cuda:\n    im_data = im_data.cuda()\n    im_info = im_info.cuda()\n    num_boxes = num_boxes.cuda()\n    gt_boxes = gt_boxes.cuda()\n\n  # make variable\n  im_data = Variable(im_data)\n  im_info = Variable(im_info)\n  num_boxes = Variable(num_boxes)\n  gt_boxes = Variable(gt_boxes)\n\n  if args.cuda:\n    cfg.CUDA = True\n\n  # initilize the network here.\n  if args.net == 'vgg16':\n    fasterRCNN = vgg16(imdb.classes, pretrained=True, class_agnostic=args.class_agnostic)\n  elif args.net == 'res101':\n    fasterRCNN = resnet(imdb.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)\n  elif args.net == 'res50':\n    fasterRCNN = resnet(imdb.classes, 50, pretrained=True, class_agnostic=args.class_agnostic)\n  elif args.net == 'res152':\n    fasterRCNN = resnet(imdb.classes, 152, pretrained=True, class_agnostic=args.class_agnostic)\n  else:\n    print(\"network is not defined\")\n    pdb.set_trace()\n\n  fasterRCNN.create_architecture()\n\n  lr = cfg.TRAIN.LEARNING_RATE\n  lr = args.lr\n  #tr_momentum = cfg.TRAIN.MOMENTUM\n  #tr_momentum = args.momentum\n\n  params = []\n  for key, value in dict(fasterRCNN.named_parameters()).items():\n    if value.requires_grad:\n      if 'bias' in key:\n        params += [{'params':[value],'lr':lr*(cfg.TRAIN.DOUBLE_BIAS + 1), \\\n                'weight_decay': cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY or 0}]\n      else:\n        params += [{'params':[value],'lr':lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]\n\n  if args.optimizer == \"adam\":\n    lr = lr * 0.1\n    optimizer = torch.optim.Adam(params)\n\n  elif args.optimizer == \"sgd\":\n    optimizer = torch.optim.SGD(params, momentum=cfg.TRAIN.MOMENTUM)\n\n  if args.cuda:\n    fasterRCNN.cuda()\n\n  if args.resume:\n    load_name = os.path.join(output_dir,\n      'faster_rcnn_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))\n    print(\"loading checkpoint %s\" % (load_name))\n    checkpoint = torch.load(load_name)\n    args.session = checkpoint['session']\n    args.start_epoch = checkpoint['epoch']\n    fasterRCNN.load_state_dict(checkpoint['model'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    lr = optimizer.param_groups[0]['lr']\n    if 'pooling_mode' in checkpoint.keys():\n      cfg.POOLING_MODE = checkpoint['pooling_mode']\n    print(\"loaded checkpoint %s\" % (load_name))\n\n  if args.mGPUs:\n    fasterRCNN = nn.DataParallel(fasterRCNN)\n\n  iters_per_epoch = int(train_size / args.batch_size)\n\n  if args.use_tfboard:\n    from tensorboardX import SummaryWriter\n    logger = SummaryWriter(\"logs\")\n\n  for epoch in range(args.start_epoch, args.max_epochs + 1):\n    # setting to train mode\n    fasterRCNN.train()\n    loss_temp = 0\n    start = time.time()\n\n    if epoch % (args.lr_decay_step + 1) == 0:\n        adjust_learning_rate(optimizer, args.lr_decay_gamma)\n        lr *= args.lr_decay_gamma\n\n    data_iter = iter(dataloader)\n    for step in range(iters_per_epoch):\n      data = next(data_iter)\n      im_data.data.resize_(data[0].size()).copy_(data[0])\n      im_info.data.resize_(data[1].size()).copy_(data[1])\n      gt_boxes.data.resize_(data[2].size()).copy_(data[2])\n      num_boxes.data.resize_(data[3].size()).copy_(data[3])\n\n      fasterRCNN.zero_grad()\n      rois, cls_prob, bbox_pred, \\\n      rpn_loss_cls, rpn_loss_box, \\\n      RCNN_loss_cls, RCNN_loss_bbox, \\\n      rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n\n      loss = rpn_loss_cls.mean() + rpn_loss_box.mean() \\\n           + RCNN_loss_cls.mean() + RCNN_loss_bbox.mean()\n      loss_temp += loss.item()\n\n      # backward\n      optimizer.zero_grad()\n      loss.backward()\n      if args.net == \"vgg16\":\n          clip_gradient(fasterRCNN, 10.)\n      optimizer.step()\n\n      if step % args.disp_interval == 0:\n        end = time.time()\n        if step > 0:\n          loss_temp /= (args.disp_interval + 1)\n\n        if args.mGPUs:\n          loss_rpn_cls = rpn_loss_cls.mean().item()\n          loss_rpn_box = rpn_loss_box.mean().item()\n          loss_rcnn_cls = RCNN_loss_cls.mean().item()\n          loss_rcnn_box = RCNN_loss_bbox.mean().item()\n          fg_cnt = torch.sum(rois_label.data.ne(0))\n          bg_cnt = rois_label.data.numel() - fg_cnt\n        else:\n          loss_rpn_cls = rpn_loss_cls.item()\n          loss_rpn_box = rpn_loss_box.item()\n          loss_rcnn_cls = RCNN_loss_cls.item()\n          loss_rcnn_box = RCNN_loss_bbox.item()\n          fg_cnt = torch.sum(rois_label.data.ne(0))\n          bg_cnt = rois_label.data.numel() - fg_cnt\n\n        print(\"[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e\" \\\n                                % (args.session, epoch, step, iters_per_epoch, loss_temp, lr))\n        print(\"\\t\\t\\tfg/bg=(%d/%d), time cost: %f\" % (fg_cnt, bg_cnt, end-start))\n        print(\"\\t\\t\\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\" \\\n                      % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_rcnn_box))\n        if args.use_tfboard:\n          info = {\n            'loss': loss_temp,\n            'loss_rpn_cls': loss_rpn_cls,\n            'loss_rpn_box': loss_rpn_box,\n            'loss_rcnn_cls': loss_rcnn_cls,\n            'loss_rcnn_box': loss_rcnn_box\n          }\n          logger.add_scalars(\"logs_s_{}/losses\".format(args.session), info, (epoch - 1) * iters_per_epoch + step)\n\n        loss_temp = 0\n        start = time.time()\n\n    \n    save_name = os.path.join(output_dir, 'faster_rcnn_{}_{}_{}.pth'.format(args.session, epoch, step))\n    save_checkpoint({\n      'session': args.session,\n      'epoch': epoch + 1,\n      'model': fasterRCNN.module.state_dict() if args.mGPUs else fasterRCNN.state_dict(),\n      'optimizer': optimizer.state_dict(),\n      'pooling_mode': cfg.POOLING_MODE,\n      'class_agnostic': args.class_agnostic,\n    }, save_name)\n    print('save model: {}'.format(save_name))\n\n  if args.use_tfboard:\n    logger.close()\n"
        }
      ]
    }
  ]
}