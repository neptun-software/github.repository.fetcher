{
  "metadata": {
    "timestamp": 1736560951141,
    "page": 698,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "meta-llama/llama-models",
      "stars": 5535,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 1.037109375,
          "content": "[flake8]\n# Suggested config from pytorch that we can adapt\nselect = B,C,E,F,N,P,T4,W,B9,TOR0,TOR1,TOR2\nmax-line-length = 120\n# C408 ignored because we like the dict keyword argument syntax\n# E501 is not flexible enough, we're using B950 instead\n# N812 ignored because import torch.nn.functional as F is PyTorch convention\n# N817 ignored because importing using acronyms is convention (DistributedDataParallel as DDP)\n# E731 allow usage of assigning lambda expressions\n# E701 let black auto-format statements on one line\n# E704 let black auto-format statements on one line\nignore =\n    E203,E305,E402,E501,E721,E741,F405,F821,F841,F999,W503,W504,C408,E302,W291,E303,N812,N817,E731,E701,E704\n    # shebang has extra meaning in fbcode lints, so I think it's not worth trying\n    # to line this up with executable bit\n    EXE001,\n    # these ignores are from flake8-bugbear; please fix!\n    B007,B008,B950\noptional-ascii-coding = True\nexclude =\n    ./.git,\n    ./docs\n    ./build\n    ./scripts,\n    ./venv,\n    *.pyi\n    .pre-commit-config.yaml\n    *.md\n    .flake8\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.04296875,
          "content": "__pycache__\ndist\n*.egg-info\nbuild\n.DS_Store\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.3134765625,
          "content": "exclude: 'build'\n\ndefault_language_version:\n    python: python3\n\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: 6306a48f7dae5861702d573c9c247e4e9498e867\n    hooks:\n    -   id: trailing-whitespace\n    -   id: check-ast\n    -   id: check-merge-conflict\n    -   id: check-added-large-files\n        args: ['--maxkb=1000']\n    -   id: end-of-file-fixer\n        exclude: '^(.*\\.svg)$'\n\n# Temporarily disabling this\n#    -   id: no-commit-to-branch\n#        args: ['--branch=main']\n\n-   repo: https://github.com/Lucas-C/pre-commit-hooks\n    rev: v1.5.4\n    hooks:\n    -   id: insert-license\n        files: \\.py$|\\.sh$\n        args:\n          - --license-filepath\n          - docs/license_header.txt\n\n-   repo: https://github.com/pycqa/flake8\n    rev: 34cbf8ef3950f43d09b85e2e45c15ae5717dc37b\n    hooks:\n    -   id: flake8\n        additional_dependencies:\n          - flake8-bugbear == 22.4.25\n          - pep8-naming == 0.12.1\n          - torchfix\n        args: ['--config=.flake8']\n\n-   repo: https://github.com/omnilib/ufmt\n    rev: v2.7.0\n    hooks:\n    -   id: ufmt\n        additional_dependencies:\n          - black == 24.4.2\n          - usort == 1.0.8\n\n# - repo: https://github.com/jsh9/pydoclint\n#   rev: d88180a8632bb1602a4d81344085cf320f288c5a\n#   hooks:\n#     - id: pydoclint\n#       args: [--config=pyproject.toml]\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.4541015625,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@meta.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.3076171875,
          "content": "# Contributing to Llama-Models\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Meta's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nMeta has a [bounty program](http://facebook.com/whitehat/info) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style  \n* 2 spaces for indentation rather than tabs\n* 80 character line length\n* ...\n\n## License\nBy contributing to Llama, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.076171875,
          "content": "https://github.com/meta-llama/llama-models/blob/main/README.md#llama-models-1\n"
        },
        {
          "name": "Llama_Repo.jpeg",
          "type": "blob",
          "size": 275.5693359375,
          "content": null
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.3154296875,
          "content": "include requirements.txt\ninclude llama_models/llama3/api/tokenizer.model\ninclude llama_models/scripts/resources/dog.jpg\ninclude llama_models/scripts/resources/pasta.jpeg\ninclude llama_models/llama3_1/prompt_format.md\ninclude llama_models/llama3_2/text_prompt_format.md\ninclude llama_models/llama3_2/vision_prompt_format.md\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.6767578125,
          "content": "<p align=\"center\">\n  <img src=\"/Llama_Repo.jpeg\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n        🤗 <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp\n<br>\n\n---\n\n# Llama Models\n\nLlama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n\nOur mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n\n## Llama Models\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)\n[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)\n\n|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/USE_POLICY.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/USE_POLICY.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n| Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |\n| Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |\n\n## Download\n\nTo download the model weights and tokenizer:\n\n1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n2. Read and accept the license.\n3. Once your request is approved you will receive a signed URL via email.\n4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-stack`. (**<-- Start Here if you have received an email already.**)\n5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:\nIf you want older versions of models, run `llama model list --show-all` to show all the available Llama models.\n\n6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`\n7. Pass the URL provided when prompted to start the download.\n\nRemember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n\n## Running the models\n\nYou need to install the following dependencies (in addition to the `requirements.txt` in the root directory of this repository) to run the models:\n```\npip install torch fairscale fire blobfile\n```\n\nAfter installing the dependencies, you can run the example scripts (within `llama_models/scripts/` sub-directory) as follows:\n```bash\n#!/bin/bash\n\nCHECKPOINT_DIR=~/.llama/checkpoints/Meta-Llama3.1-8B-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) torchrun llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n```\n\nThe above script should be used with an Instruct (Chat) model. For a Base model, use the script `llama_models/scripts/example_text_completion.py`. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.\n\nFor running larger models with tensor parallelism, you should modify as:\n```bash\n#!/bin/bash\n\nNGPUS=8\nPYTHONPATH=$(git rev-parse --show-toplevel) torchrun \\\n  --nproc_per_node=$NGPUS \\\n  llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR \\\n  --model_parallel_size $NGPUS\n```\n\nFor more flexibility in running inference (including running FP8 inference), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) repository.\n\n\n## Access to Hugging Face\n\nWe also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n\n- Visit one of the repos, for example [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\n- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n\n```bash\nhuggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n```\n\n**NOTE** The original native weights of meta-llama/Meta-Llama-3.1-405B would not be available through this HugginFace repo.\n\n\n- To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:\n\n  ```python\n  import transformers\n  import torch\n\n  model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n  pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device=\"cuda\",\n  )\n  ```\n\n## Installations\n\nYou can install this repository as a [package](https://pypi.org/project/llama-models/) by just doing `pip install llama-models`\n\n## Responsible Use\n\nLlama models are a new technology that carries potential risks with use. Testing conducted to date has not — and could not — cover all scenarios.\nTo help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n\n## Issues\n\nPlease report any software “bug” or other problems with the models through one of the following means:\n- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n\n## Questions\n\nFor common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.1328125,
          "content": "# Security Policy\n\n## Reporting a Vulnerability\n\nPlease report vulnerabilities to our bug bounty program at https://bugbounty.meta.com/\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "llama_models",
          "type": "blob",
          "size": 0.005859375,
          "content": "models"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.0849609375,
          "content": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.041015625,
          "content": "PyYAML\njinja2\ntiktoken\npydantic>=2\nPillow\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.595703125,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the terms described in the LICENSE file in\n# top-level folder for each specific model found within the models/ directory at\n# the top-level of this source tree.\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the terms described in the LICENSE file in\n# the root directory of this source tree.\n\nfrom setuptools import setup\n\n\ndef read_requirements():\n    with open(\"requirements.txt\") as fp:\n        content = fp.readlines()\n    return [line.strip() for line in content if not line.startswith(\"#\")]\n\n\nsetup(\n    name=\"llama_models\",\n    version=\"0.0.63\",\n    author=\"Meta Llama\",\n    author_email=\"llama-oss@meta.com\",\n    description=\"Llama models\",\n    entry_points={\n        \"console_scripts\": [\n            \"multimodal_example_chat_completion = llama_models.scripts.multimodal_example_chat_completion:main\",\n            \"multimodal_example_text_completion = llama_models.scripts.multimodal_example_text_completion:main\",\n            \"example_chat_completion = llama_models.scripts.example_chat_completion:main\",\n            \"example_text_completion = llama_models.scripts.example_text_completion:main\",\n        ]\n    },\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/meta-llama/llama-models\",\n    package_dir={\"llama_models\": \"llama_models\"},\n    classifiers=[],\n    python_requires=\">=3.10\",\n    install_requires=read_requirements(),\n    include_package_data=True,\n)\n"
        }
      ]
    }
  ]
}