{
  "metadata": {
    "timestamp": 1736560475050,
    "page": 60,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "karpathy/minbpe",
      "stars": 9315,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0634765625,
          "content": "__pycache__/\n.DS_Store\nmodels/**/*\n*.pytest_cache\n*.model\n*.vocab"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0380859375,
          "content": "MIT License\n\nCopyright (c) 2024 Andrej\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.0654296875,
          "content": "# minbpe\n\nMinimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is \"byte-level\" because it runs on UTF-8 encoded strings.\n\nThis algorithm was popularized for LLMs by the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and the associated GPT-2 [code release](https://github.com/openai/gpt-2) from OpenAI. [Sennrich et al. 2015](https://arxiv.org/abs/1508.07909) is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.\n\nThere are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:\n\n1. [minbpe/base.py](minbpe/base.py): Implements the `Tokenizer` class, which is the base class. It contains the `train`, `encode`, and `decode` stubs, save/load functionality, and there are also a few common utility functions. This class is not meant to be used directly, but rather to be inherited from.\n2. [minbpe/basic.py](minbpe/basic.py): Implements the `BasicTokenizer`, the simplest implementation of the BPE algorithm that runs directly on text.\n3. [minbpe/regex.py](minbpe/regex.py): Implements the `RegexTokenizer` that further splits the input text by a regex pattern, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. This was introduced in the GPT-2 paper and continues to be in use as of GPT-4. This class also handles special tokens, if any.\n4. [minbpe/gpt4.py](minbpe/gpt4.py): Implements the `GPT4Tokenizer`. This class is a light wrapper around the `RegexTokenizer` (2, above) that exactly reproduces the tokenization of GPT-4 in the [tiktoken](https://github.com/openai/tiktoken) library. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations.\n\nFinally, the script [train.py](train.py) trains the two major tokenizers on the input text [tests/taylorswift.txt](tests/taylorswift.txt) (this is the Wikipedia entry for her kek) and saves the vocab to disk for visualization. This script runs in about 25 seconds on my (M1) MacBook.\n\nAll of the files above are very short and thoroughly commented, and also contain a usage example on the bottom of the file.\n\n## quick start\n\nAs the simplest example, we can reproduce the [Wikipedia article on BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) as follows:\n\n```python\nfrom minbpe import BasicTokenizer\ntokenizer = BasicTokenizer()\ntext = \"aaabdaaabac\"\ntokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges\nprint(tokenizer.encode(text))\n# [258, 100, 258, 97, 99]\nprint(tokenizer.decode([258, 100, 258, 97, 99]))\n# aaabdaaabac\ntokenizer.save(\"toy\")\n# writes two files: toy.model (for loading) and toy.vocab (for viewing)\n```\n\nAccording to Wikipedia, running bpe on the input string: \"aaabdaaabac\" for 3 merges results in the string: \"XdXac\" where  X=ZY, Y=ab, and Z=aa. The tricky thing to note is that minbpe always allocates the 256 individual bytes as tokens, and then merges bytes as needed from there. So for us a=97, b=98, c=99, d=100 (their [ASCII](https://www.asciitable.com) values). Then when (a,a) is merged to Z, Z will become 256. Likewise Y will become 257 and X 258. So we start with the 256 bytes, and do 3 merges to get to the result above, with the expected output of [258, 100, 258, 97, 99].\n\n## inference: GPT-4 comparison\n\nWe can verify that the `RegexTokenizer` has feature parity with the GPT-4 tokenizer from [tiktoken](https://github.com/openai/tiktoken) as follows:\n\n```python\ntext = \"hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ðŸ˜‰\"\n\n# tiktoken\nimport tiktoken\nenc = tiktoken.get_encoding(\"cl100k_base\")\nprint(enc.encode(text))\n# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]\n\n# ours\nfrom minbpe import GPT4Tokenizer\ntokenizer = GPT4Tokenizer()\nprint(tokenizer.encode(text))\n# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]\n```\n\n(you'll have to `pip install tiktoken` to run). Under the hood, the `GPT4Tokenizer` is just a light wrapper around `RegexTokenizer`, passing in the merges and the special tokens of GPT-4. We can also ensure the special tokens are handled correctly:\n\n```python\ntext = \"<|endoftext|>hello world\"\n\n# tiktoken\nimport tiktoken\nenc = tiktoken.get_encoding(\"cl100k_base\")\nprint(enc.encode(text, allowed_special=\"all\"))\n# [100257, 15339, 1917]\n\n# ours\nfrom minbpe import GPT4Tokenizer\ntokenizer = GPT4Tokenizer()\nprint(tokenizer.encode(text, allowed_special=\"all\"))\n# [100257, 15339, 1917]\n```\n\nNote that just like tiktoken, we have to explicitly declare our intent to use and parse special tokens in the call to encode. Otherwise this can become a major footgun, unintentionally tokenizing attacker-controlled data (e.g. user prompts) with special tokens. The `allowed_special` parameter can be set to \"all\", \"none\", or a list of special tokens to allow.\n\n## training\n\nUnlike tiktoken, this code allows you to train your own tokenizer. In principle and to my knowledge, if you train the `RegexTokenizer` on a large dataset with a vocabulary size of 100K, you would reproduce the GPT-4 tokenizer.\n\nThere are two paths you can follow. First, you can decide that you don't want the complexity of splitting and preprocessing text with regex patterns, and you also don't care for special tokens. In that case, reach for the `BasicTokenizer`. You can train it, and then encode and decode for example as follows:\n\n```python\nfrom minbpe import BasicTokenizer\ntokenizer = BasicTokenizer()\ntokenizer.train(very_long_training_string, vocab_size=4096)\ntokenizer.encode(\"hello world\") # string -> tokens\ntokenizer.decode([1000, 2000, 3000]) # tokens -> string\ntokenizer.save(\"mymodel\") # writes mymodel.model and mymodel.vocab\ntokenizer.load(\"mymodel.model\") # loads the model back, the vocab is just for vis\n```\n\nIf you instead want to follow along with OpenAI did for their text tokenizer, it's a good idea to adopt their approach of using regex pattern to split the text by categories. The GPT-4 pattern is a default with the `RegexTokenizer`, so you'd simple do something like:\n\n```python\nfrom minbpe import RegexTokenizer\ntokenizer = RegexTokenizer()\ntokenizer.train(very_long_training_string, vocab_size=32768)\ntokenizer.encode(\"hello world\") # string -> tokens\ntokenizer.decode([1000, 2000, 3000]) # tokens -> string\ntokenizer.save(\"tok32k\") # writes tok32k.model and tok32k.vocab\ntokenizer.load(\"tok32k.model\") # loads the model back from disk\n```\n\nWhere, of course, you'd want to change around the vocabulary size depending on the size of your dataset.\n\n**Special tokens**. Finally, you might wish to add special tokens to your tokenizer. Register these using the `register_special_tokens` function. For example if you train with vocab_size of 32768, then the first 256 tokens are raw byte tokens, the next 32768-256 are merge tokens, and after those you can add the special tokens. The last \"real\" merge token will have id of 32767 (vocab_size - 1), so your first special token should come right after that, with an id of exactly 32768. So:\n\n```python\nfrom minbpe import RegexTokenizer\ntokenizer = RegexTokenizer()\ntokenizer.train(very_long_training_string, vocab_size=32768)\ntokenizer.register_special_tokens({\"<|endoftext|>\": 32768})\ntokenizer.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")\n```\n\nYou can of course add more tokens after that as well, as you like. Finally, I'd like to stress that I tried hard to keep the code itself clean, readable and hackable. You should not have feel scared to read the code and understand how it works. The tests are also a nice place to look for more usage examples. That reminds me:\n\n## tests\n\nWe use the pytest library for tests. All of them are located in the `tests/` directory. First `pip install pytest` if you haven't already, then:\n\n```bash\n$ pytest -v .\n```\n\nto run the tests. (-v is verbose, slightly prettier).\n\n## community extensions\n\n* [gnp/minbpe-rs](https://github.com/gnp/minbpe-rs): A Rust implementation of `minbpe` providing (near) one-to-one correspondence with the Python version\n\n## exercise\n\nFor those trying to study BPE, here is the advised progression exercise for how you can build your own minbpe step by step. See [exercise.md](exercise.md).\n\n## lecture\n\nI built the code in this repository in this [YouTube video](https://www.youtube.com/watch?v=zduSFxRajkE). You can also find this lecture in text form in [lecture.md](lecture.md).\n\n## todos\n\n- write a more optimized Python version that could run over large files and big vocabs\n- write an even more optimized C or Rust version (think through)\n- rename GPT4Tokenizer to GPTTokenizer and support GPT-2/GPT-3/GPT-3.5 as well?\n- write a LlamaTokenizer similar to GPT4Tokenizer (i.e. attempt sentencepiece equivalent)\n\n## License\n\nMIT\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "exercise.md",
          "type": "blob",
          "size": 3.5205078125,
          "content": "# exercise\n\nBuild your own GPT-4 Tokenizer!\n\n### Step 1\n\nWrite the `BasicTokenizer` class, with the following three core functions:\n\n- `def train(self, text, vocab_size, verbose=False)`\n- `def encode(self, text)`\n- `def decode(self, ids)`\n\nTrain your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file `tests/taylorswift.txt`.\n\n### Step 2\n\nConvert you `BasicTokenizer` into a `RegexTokenizer`, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n\n```\nGPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n```\n\n\n### Step 3\n\nYou're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both `encode` and `decode`, matching [tiktoken](https://github.com/openai/tiktoken).\n\n```\n# match this\nimport tiktoken\nenc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\nids = enc.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\ntext = enc.decode(ids) # get the same text back\n```\n\nUnfortunately, you will run into two issues:\n\n1. It is not trivial to recover the raw merges from the GPT-4 tokenizer. You can easily recover what we call `vocab` here, and what they call and store under `enc._mergeable_ranks`. Feel free to copy paste the `recover_merges` function in `minbpe/gpt4.py`, which takes these ranks and returns the raw merges. If you wish to know how this function works, read [this](https://github.com/openai/tiktoken/issues/60) and [this](https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306). Basically, under some conditions it is enough to only store the parent nodes (and their rank) and get rid of the precise details of which children merged up to any parent.\n2. Second, the GPT-4 tokenizer for some reason permutes its raw bytes. It stores this permutation in the first 256 elements of the mergeable ranks, so you can recover this byte shuffle relatively simply as `byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}`. In both your encode and decode, you'll have to shuffle bytes around accordingly. If you're stuck, reference the minbpe/gpt4.py` file for hints.\n\n### Step 4\n\n(Optional, irritating, not obviously useful) Add the ability to handle special tokens. You'll then be able to match the output of tiktoken even when special tokens are present, e.g.:\n\n```\nimport tiktoken\nenc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\nids = enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")\n```\n\nWithout `allowed_special` tiktoken will error.\n\n### Step 5\n\nIf you've made it this far, you're now a pro at LLM Tokenization! Sadly, you're not exactly done yet because a lot of LLMs outside of OpenAI (e.g. Llama, Mistral) use [sentencepiece](https://github.com/google/sentencepiece) instead. Primary difference being that sentencepiece runs BPE directly on Unicode code points instead of on UTF-8 encoded bytes. Feel free to explore sentencepiece on your own (good luck, it's not too pretty), and stretch goal if you really experience and suffer from the burden of time, re-write your BPE to be on Unicode code points and match the Llama 2 tokenizer.\n"
        },
        {
          "name": "lecture.md",
          "type": "blob",
          "size": 8.3076171875,
          "content": "# LLM Tokenization\n\nHi everyone, today we are going to look at Tokenization in Large Language Models (LLMs). Sadly, tokenization is a relatively complex and gnarly component of the state of the art LLMs, but it is necessary to understand in some detail because a lot of the shortcomings of LLMs that may be attributed to the neural network or otherwise appear mysterious actually trace back to tokenization.\n\n### Previously: character-level tokenization\n\nSo what is tokenization? Well it turns out that in our previous video, [Let's build GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY), we already covered tokenization but it was only a very simple, naive, character-level version of it. When you go to the [Google colab](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing) for that video, you'll see that we started with our training data ([Shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)), which is just a large string in Python:\n\n```\nFirst Citizen: Before we proceed any further, hear me speak.\n\nAll: Speak, speak.\n\nFirst Citizen: You are all resolved rather to die than to famish?\n\nAll: Resolved. resolved.\n\nFirst Citizen: First, you know Caius Marcius is chief enemy to the people.\n\nAll: We know't, we know't.\n```\n\nBut how do we feed strings into a language model? Well, we saw that we did this by first constructing a vocabulary of all the possible characters we found in the entire training set:\n\n```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n# 65\n```\n\nAnd then creating a lookup table for converting between individual characters and integers according to the vocabulary above. This lookup table was just a Python dictionary:\n\n```python\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\n# encoder: take a string, output a list of integers\nencode = lambda s: [stoi[c] for c in s]\n# decoder: take a list of integers, output a string\ndecode = lambda l: ''.join([itos[i] for i in l])\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n# [46, 47, 47, 1, 58, 46, 43, 56, 43]\n# hii there\n```\n\nOnce we've converted a string into a sequence of integers, we saw that each integer was used as an index into a 2-dimensional embedding of trainable parameters. Because we have a vocabulary size of `vocab_size=65`, this embedding table will also have 65 rows:\n\n```python\nclass BigramLanguageModel(nn.Module):\n\ndef __init__(self, vocab_size):\n\tsuper().__init__()\n\tself.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n\ndef forward(self, idx, targets=None):\n\ttok_emb = self.token_embedding_table(idx) # (B,T,C)\n```\n\nHere, the integer \"plucks out\" a row of this embedding table and this row is the vector that represents this token. This vector then feeds into the Transformer as the input at the corresponding time step.\n\n### \"Character chunks\" for tokenization using the BPE algorithm\n\nThis is all well and good for the naive setting of a character-level language model. But in practice, in state of the art language models, people use a lot more complicated schemes for constructing these token vocabularies. In particular, these schemes work not on a character level, but on character chunk level. And the way these chunk vocabularies are constructed is by using algorithms such as the **Byte Pair Encoding** (BPE) algorithm, which we are going to cover in detail below.\n\nTurning to the historical development of this approach for a moment, the paper that popularized the use of the byte-level BPE algorithm for language model tokenization is the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) from OpenAI in 2019, \"Language Models are Unsupervised Multitask Learners\". Scroll down to Section 2.2 on \"Input Representation\" where they describe and motivate this algorithm. At the end of this section you'll see them say:\n\n> *The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used.*\n\nRecall that in the attention layer of a Transformer, every token is attending to a finite list of tokens previously in the sequence. The paper here says that the GPT-2 model has a context length of 1024 tokens, up from 512 in GPT-1. In other words, tokens are the fundamental \"atoms\" at the input to the LLM. And tokenization is the process for taking raw strings in Python and converting them to a list of tokens, and vice versa. As another popular example to demonstrate the pervasiveness of this abstraction, if you go to the [Llama 2](https://arxiv.org/abs/2307.09288) paper as well and you search for \"token\", you're going to get 63 hits. So for example, the paper claims that they trained on 2 trillion tokens, etc.\n\n### Brief taste of the complexities of tokenization\n\nBefore we dive into details of the implementation, let's briefly motivate the need to understand the tokenization process in some detail. Tokenization is at the heart of a lot of weirdness in LLMs and I would advise that you do not brush it off. A lot of the issues that may look like issues with the neural network architecture actually trace back to tokenization. Here are just a few examples:\n\n- Why can't LLM spell words? **Tokenization**.\n- Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization**.\n- Why is LLM worse at non-English languages (e.g. Japanese)? **Tokenization**.\n- Why is LLM bad at simple arithmetic? **Tokenization**.\n- Why did GPT-2 have more than necessary trouble coding in Python? **Tokenization**.\n- Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? **Tokenization**.\n- What is this weird warning I get about a \"trailing whitespace\"? **Tokenization**.\n- Why did the LLM break if I ask it about \"SolidGoldMagikarp\"? **Tokenization**.\n- Why should I prefer to use YAML over JSON with LLMs? **Tokenization**.\n- Why is LLM not actually end-to-end language modeling? **Tokenization**.\n- What is the real root of suffering? **Tokenization**.\n\nWe will loop back around to these at the end of the video.\n\n### Visual preview of tokenization\n\nNext, let's load this [tokenization webapp](https://tiktokenizer.vercel.app). What is nice about this webapp is that tokenization is running live in your web browser, allowing you to easily input some text string at the input, and see the tokenization on the right. On the top, you can see that we are currently using the `gpt2` tokenizer, and we see that the string that we pasted in with this example is currently tokenizing into 300 tokens. Here they are shown explicitly in colors:\n\n![tiktokenizer](assets/tiktokenizer.png)\n\nSo for example, the string \"Tokenization\" encoded into the tokens 30642 followed by the token 1634. The token \" is\" (note that these is three characters, including the space in the front, this is important!) is index 318. Be careful with whitespace because it is absolutely present in the string and must be tokenized along with all the other characters, but is usually omitted in visualization for clarity. You can toggle on and off its visualization at the bottom of the app. In the same way, the token \" at\" is 379, \" the\" is 262, etc.\n\nNext, we have a simple example of some arithmetic. Here, we see that numbers may be inconsistently decomposed by the tokenizer. For example, the number 127 is a single token of three characters, but the number 677 because two tokens: the token \" 6\" (again, note the space in the front!) and the token \"77\". We rely on the large language model to make sense of this arbitrariness. It has to learn inside its parameters and during training that these two tokens (\" 6\" and \"77\" actually combine to create the number 677). In the same way, we see that if the LLM wanted to predict that the result of this sum is the number 804, it would have to output that in two time steps: first it has to emit the token \" 8\", and then the token \"04\". Note that all of these splits look completely arbitrary. In the example right below, we see that 1275 is \"12\" followed by \"75\", 6773 is actually two tokens \" 6\", \"773\", and 8041 is \" 8\", \"041\".\n\n(to be continued...)\n(TODO: may continue this unless we figure out how to generate it automatically from the video :))\n"
        },
        {
          "name": "minbpe",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.013671875,
          "content": "regex\ntiktoken"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 0.861328125,
          "content": "\"\"\"\nTrain our Tokenizers on some data, just to see them in action.\nThe whole thing runs in ~25 seconds on my laptop.\n\"\"\"\n\nimport os\nimport time\nfrom minbpe import BasicTokenizer, RegexTokenizer\n\n# open some text and train a vocab of 512 tokens\ntext = open(\"tests/taylorswift.txt\", \"r\", encoding=\"utf-8\").read()\n\n# create a directory for models, so we don't pollute the current directory\nos.makedirs(\"models\", exist_ok=True)\n\nt0 = time.time()\nfor TokenizerClass, name in zip([BasicTokenizer, RegexTokenizer], [\"basic\", \"regex\"]):\n\n    # construct the Tokenizer object and kick off verbose training\n    tokenizer = TokenizerClass()\n    tokenizer.train(text, 512, verbose=True)\n    # writes two files in the models directory: name.model, and name.vocab\n    prefix = os.path.join(\"models\", name)\n    tokenizer.save(prefix)\nt1 = time.time()\n\nprint(f\"Training took {t1 - t0:.2f} seconds\")"
        }
      ]
    }
  ]
}