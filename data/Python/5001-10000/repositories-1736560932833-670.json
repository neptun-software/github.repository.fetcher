{
  "metadata": {
    "timestamp": 1736560932833,
    "page": 670,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "naver/dust3r",
      "stars": 5655,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.7763671875,
          "content": "data/\ncheckpoints/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.0703125,
          "content": "[submodule \"croco\"]\n\tpath = croco\n\turl = https://github.com/naver/croco\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.3525390625,
          "content": "DUSt3R, Copyright (c) 2024-present Naver Corporation, is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license.\n\nA summary of the CC BY-NC-SA 4.0 license is located here:\n\thttps://creativecommons.org/licenses/by-nc-sa/4.0/\n\nThe CC BY-NC-SA 4.0 license is located here:\n\thttps://creativecommons.org/licenses/by-nc-sa/4.0/legalcode\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.3505859375,
          "content": "DUSt3R\nCopyright 2024-present NAVER Corp.\n\nThis project contains subcomponents with separate copyright notices and license terms. \nYour use of the source code for these subcomponents is subject to the terms and conditions of the following licenses.\n\n====\n\nnaver/croco\nhttps://github.com/naver/croco/\n\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 23.8330078125,
          "content": "![demo](assets/dust3r.jpg)\n\nOfficial implementation of `DUSt3R: Geometric 3D Vision Made Easy`  \n[[Project page](https://dust3r.europe.naverlabs.com/)], [[DUSt3R arxiv](https://arxiv.org/abs/2312.14132)]  \n\n> **Make sure to also check [MASt3R](https://github.com/naver/mast3r): Our new model with a local feature head, metric pointmaps, and a more scalable global alignment!**\n\n![Example of reconstruction from two images](assets/pipeline1.jpg)\n\n![High level overview of DUSt3R capabilities](assets/dust3r_archi.jpg)\n\n```bibtex\n@inproceedings{dust3r_cvpr24,\n      title={DUSt3R: Geometric 3D Vision Made Easy}, \n      author={Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud},\n      booktitle = {CVPR},\n      year = {2024}\n}\n\n@misc{dust3r_arxiv23,\n      title={DUSt3R: Geometric 3D Vision Made Easy}, \n      author={Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud},\n      year={2023},\n      eprint={2312.14132},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## Table of Contents\n\n- [Table of Contents](#table-of-contents)\n- [License](#license)\n- [Get Started](#get-started)\n  - [Installation](#installation)\n  - [Checkpoints](#checkpoints)\n  - [Interactive demo](#interactive-demo)\n  - [Interactive demo with docker](#interactive-demo-with-docker)\n- [Usage](#usage)\n- [Training](#training)\n  - [Datasets](#datasets)\n  - [Demo](#demo)\n  - [Our Hyperparameters](#our-hyperparameters)\n\n## License\n\nThe code is distributed under the CC BY-NC-SA 4.0 License.\nSee [LICENSE](LICENSE) for more information.\n\n```python\n# Copyright (C) 2024-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n```\n\n## Get Started\n\n### Installation\n\n1. Clone DUSt3R.\n```bash\ngit clone --recursive https://github.com/naver/dust3r\ncd dust3r\n# if you have already cloned dust3r:\n# git submodule update --init --recursive\n```\n\n2. Create the environment, here we show an example using conda.\n```bash\nconda create -n dust3r python=3.11 cmake=3.14.0\nconda activate dust3r \nconda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia  # use the correct version of cuda for your system\npip install -r requirements.txt\n# Optional: you can also install additional packages to:\n# - add support for HEIC images\n# - add pyrender, used to render depthmap in some datasets preprocessing\n# - add required packages for visloc.py\npip install -r requirements_optional.txt\n```\n\n3. Optional, compile the cuda kernels for RoPE (as in CroCo v2).\n```bash\n# DUST3R relies on RoPE positional embeddings for which you can compile some cuda kernels for faster runtime.\ncd croco/models/curope/\npython setup.py build_ext --inplace\ncd ../../../\n```\n\n### Checkpoints\n\nYou can obtain the checkpoints by two ways:\n\n1) You can use our huggingface_hub integration: the models will be downloaded automatically.\n\n2) Otherwise, We provide several pre-trained models:\n\n| Modelname   | Training resolutions | Head | Encoder | Decoder |\n|-------------|----------------------|------|---------|---------|\n| [`DUSt3R_ViTLarge_BaseDecoder_224_linear.pth`](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_224_linear.pth) | 224x224 | Linear | ViT-L | ViT-B |\n| [`DUSt3R_ViTLarge_BaseDecoder_512_linear.pth`](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_linear.pth)   | 512x384, 512x336, 512x288, 512x256, 512x160 | Linear | ViT-L | ViT-B |\n| [`DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth`](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth) | 512x384, 512x336, 512x288, 512x256, 512x160 | DPT | ViT-L | ViT-B |\n\nYou can check the hyperparameters we used to train these models in the [section: Our Hyperparameters](#our-hyperparameters)\n\nTo download a specific model, for example `DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth`:\n```bash\nmkdir -p checkpoints/\nwget https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth -P checkpoints/\n```\n\nFor the checkpoints, make sure to agree to the license of all the public training datasets and base checkpoints we used, in addition to CC-BY-NC-SA 4.0. Again, see [section: Our Hyperparameters](#our-hyperparameters) for details.\n\n### Interactive demo\n\nIn this demo, you should be able run DUSt3R on your machine to reconstruct a scene.\nFirst select images that depicts the same scene.\n\nYou can adjust the global alignment schedule and its number of iterations.\n\n> [!NOTE]\n> If you selected one or two images, the global alignment procedure will be skipped (mode=GlobalAlignerMode.PairViewer)\n\nHit \"Run\" and wait.\nWhen the global alignment ends, the reconstruction appears.\nUse the slider \"min_conf_thr\" to show or remove low confidence areas.\n\n```bash\npython3 demo.py --model_name DUSt3R_ViTLarge_BaseDecoder_512_dpt\n\n# Use --weights to load a checkpoint from a local file, eg --weights checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n# Use --image_size to select the correct resolution for the selected checkpoint. 512 (default) or 224\n# Use --local_network to make it accessible on the local network, or --server_name to specify the url manually\n# Use --server_port to change the port, by default it will search for an available port starting at 7860\n# Use --device to use a different device, by default it's \"cuda\"\n```\n\n### Interactive demo with docker\n\nTo run DUSt3R using Docker, including with NVIDIA CUDA support, follow these instructions:\n\n1. **Install Docker**: If not already installed, download and install `docker` and `docker compose` from the [Docker website](https://www.docker.com/get-started).\n\n2. **Install NVIDIA Docker Toolkit**: For GPU support, install the NVIDIA Docker toolkit from the [Nvidia website](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).\n\n3. **Build the Docker image and run it**: `cd` into the `./docker` directory and run the following commands: \n\n```bash\ncd docker\nbash run.sh --with-cuda --model_name=\"DUSt3R_ViTLarge_BaseDecoder_512_dpt\"\n```\n\nOr if you want to run the demo without CUDA support, run the following command:\n\n```bash \ncd docker\nbash run.sh --model_name=\"DUSt3R_ViTLarge_BaseDecoder_512_dpt\"\n```\n\nBy default, `demo.py` is lanched with the option `--local_network`.  \nVisit `http://localhost:7860/` to access the web UI (or replace `localhost` with the machine's name to access it from the network).  \n\n`run.sh` will launch docker-compose using either the [docker-compose-cuda.yml](docker/docker-compose-cuda.yml) or [docker-compose-cpu.ym](docker/docker-compose-cpu.yml) config file, then it starts the demo using [entrypoint.sh](docker/files/entrypoint.sh).\n\n\n![demo](assets/demo.jpg)\n\n## Usage\n\n```python\nfrom dust3r.inference import inference\nfrom dust3r.model import AsymmetricCroCo3DStereo\nfrom dust3r.utils.image import load_images\nfrom dust3r.image_pairs import make_pairs\nfrom dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n\nif __name__ == '__main__':\n    device = 'cuda'\n    batch_size = 1\n    schedule = 'cosine'\n    lr = 0.01\n    niter = 300\n\n    model_name = \"naver/DUSt3R_ViTLarge_BaseDecoder_512_dpt\"\n    # you can put the path to a local checkpoint in model_name if needed\n    model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)\n    # load_images can take a list of images or a directory\n    images = load_images(['croco/assets/Chateau1.png', 'croco/assets/Chateau2.png'], size=512)\n    pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=True)\n    output = inference(pairs, model, device, batch_size=batch_size)\n\n    # at this stage, you have the raw dust3r predictions\n    view1, pred1 = output['view1'], output['pred1']\n    view2, pred2 = output['view2'], output['pred2']\n    # here, view1, pred1, view2, pred2 are dicts of lists of len(2)\n    #  -> because we symmetrize we have (im1, im2) and (im2, im1) pairs\n    # in each view you have:\n    # an integer image identifier: view1['idx'] and view2['idx']\n    # the img: view1['img'] and view2['img']\n    # the image shape: view1['true_shape'] and view2['true_shape']\n    # an instance string output by the dataloader: view1['instance'] and view2['instance']\n    # pred1 and pred2 contains the confidence values: pred1['conf'] and pred2['conf']\n    # pred1 contains 3D points for view1['img'] in view1['img'] space: pred1['pts3d']\n    # pred2 contains 3D points for view2['img'] in view1['img'] space: pred2['pts3d_in_other_view']\n\n    # next we'll use the global_aligner to align the predictions\n    # depending on your task, you may be fine with the raw output and not need it\n    # with only two input images, you could use GlobalAlignerMode.PairViewer: it would just convert the output\n    # if using GlobalAlignerMode.PairViewer, no need to run compute_global_alignment\n    scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n    loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n\n    # retrieve useful values from scene:\n    imgs = scene.imgs\n    focals = scene.get_focals()\n    poses = scene.get_im_poses()\n    pts3d = scene.get_pts3d()\n    confidence_masks = scene.get_masks()\n\n    # visualize reconstruction\n    scene.show()\n\n    # find 2D-2D matches between the two images\n    from dust3r.utils.geometry import find_reciprocal_matches, xy_grid\n    pts2d_list, pts3d_list = [], []\n    for i in range(2):\n        conf_i = confidence_masks[i].cpu().numpy()\n        pts2d_list.append(xy_grid(*imgs[i].shape[:2][::-1])[conf_i])  # imgs[i].shape[:2] = (H, W)\n        pts3d_list.append(pts3d[i].detach().cpu().numpy()[conf_i])\n    reciprocal_in_P2, nn2_in_P1, num_matches = find_reciprocal_matches(*pts3d_list)\n    print(f'found {num_matches} matches')\n    matches_im1 = pts2d_list[1][reciprocal_in_P2]\n    matches_im0 = pts2d_list[0][nn2_in_P1][reciprocal_in_P2]\n\n    # visualize a few matches\n    import numpy as np\n    from matplotlib import pyplot as pl\n    n_viz = 10\n    match_idx_to_viz = np.round(np.linspace(0, num_matches-1, n_viz)).astype(int)\n    viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]\n\n    H0, W0, H1, W1 = *imgs[0].shape[:2], *imgs[1].shape[:2]\n    img0 = np.pad(imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)\n    img1 = np.pad(imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)\n    img = np.concatenate((img0, img1), axis=1)\n    pl.figure()\n    pl.imshow(img)\n    cmap = pl.get_cmap('jet')\n    for i in range(n_viz):\n        (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T\n        pl.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)\n    pl.show(block=True)\n\n```\n![matching example on croco pair](assets/matching.jpg)\n\n## Training\n\nIn this section, we present a short demonstration to get started with training DUSt3R.\n\n### Datasets\nAt this moment, we have added the following training datasets:\n  - [CO3Dv2](https://github.com/facebookresearch/co3d) - [Creative Commons Attribution-NonCommercial 4.0 International](https://github.com/facebookresearch/co3d/blob/main/LICENSE)\n  - [ARKitScenes](https://github.com/apple/ARKitScenes) - [Creative Commons Attribution-NonCommercial-ShareAlike 4.0](https://github.com/apple/ARKitScenes/tree/main?tab=readme-ov-file#license)\n  - [ScanNet++](https://kaldir.vc.in.tum.de/scannetpp/) - [non-commercial research and educational purposes](https://kaldir.vc.in.tum.de/scannetpp/static/scannetpp-terms-of-use.pdf)\n  - [BlendedMVS](https://github.com/YoYo000/BlendedMVS) - [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/)\n  - [WayMo Open dataset](https://github.com/waymo-research/waymo-open-dataset) - [Non-Commercial Use](https://waymo.com/open/terms/)\n  - [Habitat-Sim](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md)\n  - [MegaDepth](https://www.cs.cornell.edu/projects/megadepth/)\n  - [StaticThings3D](https://github.com/lmb-freiburg/robustmvd/blob/master/rmvd/data/README.md#staticthings3d)\n  - [WildRGB-D](https://github.com/wildrgbd/wildrgbd/)\n\nFor each dataset, we provide a preprocessing script in the `datasets_preprocess` directory and an archive containing the list of pairs when needed.\nYou have to download the datasets yourself from their official sources, agree to their license, download our list of pairs, and run the preprocessing script.\n\nLinks:  \n  \n[ARKitScenes pairs](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/arkitscenes_pairs.zip)  \n[ScanNet++ pairs](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/scannetpp_pairs.zip)  \n[BlendedMVS pairs](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/blendedmvs_pairs.npy)  \n[WayMo Open dataset pairs](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/waymo_pairs.npz)  \n[Habitat metadata](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/habitat_5views_v1_512x512_metadata.tar.gz)  \n[MegaDepth pairs](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/megadepth_pairs.npz)  \n[StaticThings3D pairs](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/staticthings_pairs.npy)  \n\n> [!NOTE]\n> They are not strictly equivalent to what was used to train DUSt3R, but they should be close enough.\n\n### Demo\nFor this training demo, we're going to download and prepare a subset of [CO3Dv2](https://github.com/facebookresearch/co3d) - [Creative Commons Attribution-NonCommercial 4.0 International](https://github.com/facebookresearch/co3d/blob/main/LICENSE) and launch the training code on it.\nThe demo model will be trained for a few epochs on a very small dataset.\nIt will not be very good.\n\n```bash\n# download and prepare the co3d subset\nmkdir -p data/co3d_subset\ncd data/co3d_subset\ngit clone https://github.com/facebookresearch/co3d\ncd co3d\npython3 ./co3d/download_dataset.py --download_folder ../ --single_sequence_subset\nrm ../*.zip\ncd ../../..\n\npython3 datasets_preprocess/preprocess_co3d.py --co3d_dir data/co3d_subset --output_dir data/co3d_subset_processed  --single_sequence_subset\n\n# download the pretrained croco v2 checkpoint\nmkdir -p checkpoints/\nwget https://download.europe.naverlabs.com/ComputerVision/CroCo/CroCo_V2_ViTLarge_BaseDecoder.pth -P checkpoints/\n\n# the training of dust3r is done in 3 steps.\n# for this example we'll do fewer epochs, for the actual hyperparameters we used in the paper, see the next section: \"Our Hyperparameters\"\n# step 1 - train dust3r for 224 resolution\ntorchrun --nproc_per_node=4 train.py \\\n    --train_dataset \"1000 @ Co3d(split='train', ROOT='data/co3d_subset_processed', aug_crop=16, mask_bg='rand', resolution=224, transform=ColorJitter)\" \\\n    --test_dataset \"100 @ Co3d(split='test', ROOT='data/co3d_subset_processed', resolution=224, seed=777)\" \\\n    --model \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='linear', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)\" \\\n    --train_criterion \"ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\" \\\n    --test_criterion \"Regr3D_ScaleShiftInv(L21, gt_scale=True)\" \\\n    --pretrained \"checkpoints/CroCo_V2_ViTLarge_BaseDecoder.pth\" \\\n    --lr 0.0001 --min_lr 1e-06 --warmup_epochs 1 --epochs 10 --batch_size 16 --accum_iter 1 \\\n    --save_freq 1 --keep_freq 5 --eval_freq 1 \\\n    --output_dir \"checkpoints/dust3r_demo_224\"\t  \n\n# step 2 - train dust3r for 512 resolution\ntorchrun --nproc_per_node=4 train.py \\\n    --train_dataset \"1000 @ Co3d(split='train', ROOT='data/co3d_subset_processed', aug_crop=16, mask_bg='rand', resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter)\" \\\n    --test_dataset \"100 @ Co3d(split='test', ROOT='data/co3d_subset_processed', resolution=(512,384), seed=777)\" \\\n    --model \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', patch_embed_cls='ManyAR_PatchEmbed', img_size=(512, 512), head_type='linear', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)\" \\\n    --train_criterion \"ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\" \\\n    --test_criterion \"Regr3D_ScaleShiftInv(L21, gt_scale=True)\" \\\n    --pretrained \"checkpoints/dust3r_demo_224/checkpoint-best.pth\" \\\n    --lr 0.0001 --min_lr 1e-06 --warmup_epochs 1 --epochs 10 --batch_size 4 --accum_iter 4 \\\n    --save_freq 1 --keep_freq 5 --eval_freq 1 \\\n    --output_dir \"checkpoints/dust3r_demo_512\"\n\n# step 3 - train dust3r for 512 resolution with dpt\ntorchrun --nproc_per_node=4 train.py \\\n    --train_dataset \"1000 @ Co3d(split='train', ROOT='data/co3d_subset_processed', aug_crop=16, mask_bg='rand', resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter)\" \\\n    --test_dataset \"100 @ Co3d(split='test', ROOT='data/co3d_subset_processed', resolution=(512,384), seed=777)\" \\\n    --model \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', patch_embed_cls='ManyAR_PatchEmbed', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)\" \\\n    --train_criterion \"ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\" \\\n    --test_criterion \"Regr3D_ScaleShiftInv(L21, gt_scale=True)\" \\\n    --pretrained \"checkpoints/dust3r_demo_512/checkpoint-best.pth\" \\\n    --lr 0.0001 --min_lr 1e-06 --warmup_epochs 1 --epochs 10 --batch_size 2 --accum_iter 8 \\\n    --save_freq 1 --keep_freq 5 --eval_freq 1 --disable_cudnn_benchmark \\\n    --output_dir \"checkpoints/dust3r_demo_512dpt\"\n\n```\n\n### Our Hyperparameters\n\nHere are the commands we used for training the models:\n\n```bash\n# NOTE: ROOT path omitted for datasets\n# 224 linear\ntorchrun --nproc_per_node 8 train.py \\\n    --train_dataset=\" + 100_000 @ Habitat(1_000_000, split='train', aug_crop=16, resolution=224, transform=ColorJitter) + 100_000 @ BlendedMVS(split='train', aug_crop=16, resolution=224, transform=ColorJitter) + 100_000 @ MegaDepth(split='train', aug_crop=16, resolution=224, transform=ColorJitter) + 100_000 @ ARKitScenes(aug_crop=256, resolution=224, transform=ColorJitter) + 100_000 @ Co3d(split='train', aug_crop=16, mask_bg='rand', resolution=224, transform=ColorJitter) + 100_000 @ StaticThings3D(aug_crop=256, mask_bg='rand', resolution=224, transform=ColorJitter) + 100_000 @ ScanNetpp(split='train', aug_crop=256, resolution=224, transform=ColorJitter) + 100_000 @ InternalUnreleasedDataset(aug_crop=128, resolution=224, transform=ColorJitter) \" \\\n    --test_dataset=\" Habitat(1_000, split='val', resolution=224, seed=777) + 1_000 @ BlendedMVS(split='val', resolution=224, seed=777) + 1_000 @ MegaDepth(split='val', resolution=224, seed=777) + 1_000 @ Co3d(split='test', mask_bg='rand', resolution=224, seed=777) \" \\\n    --train_criterion=\"ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\" \\\n    --test_criterion=\"Regr3D_ScaleShiftInv(L21, gt_scale=True)\" \\\n    --model=\"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='linear', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)\" \\\n    --pretrained=\"checkpoints/CroCo_V2_ViTLarge_BaseDecoder.pth\" \\\n    --lr=0.0001 --min_lr=1e-06 --warmup_epochs=10 --epochs=100 --batch_size=16 --accum_iter=1 \\\n    --save_freq=5 --keep_freq=10 --eval_freq=1 \\\n    --output_dir=\"checkpoints/dust3r_224\"\n\n# 512 linear\ntorchrun --nproc_per_node 8 train.py \\\n    --train_dataset=\" + 10_000 @ Habitat(1_000_000, split='train', aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ BlendedMVS(split='train', aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ MegaDepth(split='train', aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ ARKitScenes(aug_crop=256, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ Co3d(split='train', aug_crop=16, mask_bg='rand', resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ StaticThings3D(aug_crop=256, mask_bg='rand', resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ ScanNetpp(split='train', aug_crop=256, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ InternalUnreleasedDataset(aug_crop=128, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) \" \\\n    --test_dataset=\" Habitat(1_000, split='val', resolution=(512,384), seed=777) + 1_000 @ BlendedMVS(split='val', resolution=(512,384), seed=777) + 1_000 @ MegaDepth(split='val', resolution=(512,336), seed=777) + 1_000 @ Co3d(split='test', resolution=(512,384), seed=777) \" \\\n    --train_criterion=\"ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\" \\\n    --test_criterion=\"Regr3D_ScaleShiftInv(L21, gt_scale=True)\" \\\n    --model=\"AsymmetricCroCo3DStereo(pos_embed='RoPE100', patch_embed_cls='ManyAR_PatchEmbed', img_size=(512, 512), head_type='linear', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)\" \\\n    --pretrained=\"checkpoints/dust3r_224/checkpoint-best.pth\" \\\n    --lr=0.0001 --min_lr=1e-06 --warmup_epochs=20 --epochs=100 --batch_size=4 --accum_iter=2 \\\n    --save_freq=10 --keep_freq=10 --eval_freq=1 --print_freq=10 \\\n    --output_dir=\"checkpoints/dust3r_512\"\n\n# 512 dpt\ntorchrun --nproc_per_node 8 train.py \\\n    --train_dataset=\" + 10_000 @ Habitat(1_000_000, split='train', aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ BlendedMVS(split='train', aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ MegaDepth(split='train', aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ ARKitScenes(aug_crop=256, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ Co3d(split='train', aug_crop=16, mask_bg='rand', resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ StaticThings3D(aug_crop=256, mask_bg='rand', resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ ScanNetpp(split='train', aug_crop=256, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ InternalUnreleasedDataset(aug_crop=128, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) \" \\\n    --test_dataset=\" Habitat(1_000, split='val', resolution=(512,384), seed=777) + 1_000 @ BlendedMVS(split='val', resolution=(512,384), seed=777) + 1_000 @ MegaDepth(split='val', resolution=(512,336), seed=777) + 1_000 @ Co3d(split='test', resolution=(512,384), seed=777) \" \\\n    --train_criterion=\"ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\" \\\n    --test_criterion=\"Regr3D_ScaleShiftInv(L21, gt_scale=True)\" \\\n    --model=\"AsymmetricCroCo3DStereo(pos_embed='RoPE100', patch_embed_cls='ManyAR_PatchEmbed', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)\" \\\n    --pretrained=\"checkpoints/dust3r_512/checkpoint-best.pth\" \\\n    --lr=0.0001 --min_lr=1e-06 --warmup_epochs=15 --epochs=90 --batch_size=4 --accum_iter=2 \\\n    --save_freq=5 --keep_freq=10 --eval_freq=1 --print_freq=10 --disable_cudnn_benchmark \\\n    --output_dir=\"checkpoints/dust3r_512dpt\"\n\n```\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "croco",
          "type": "commit",
          "content": null
        },
        {
          "name": "datasets_preprocess",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 1.5341796875,
          "content": "#!/usr/bin/env python3\n# Copyright (C) 2024-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n#\n# --------------------------------------------------------\n# dust3r gradio demo executable\n# --------------------------------------------------------\nimport os\nimport torch\nimport tempfile\n\nfrom dust3r.model import AsymmetricCroCo3DStereo\nfrom dust3r.demo import get_args_parser, main_demo, set_print_with_timestamp\n\nimport matplotlib.pyplot as pl\npl.ion()\n\ntorch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\n\nif __name__ == '__main__':\n    parser = get_args_parser()\n    args = parser.parse_args()\n    set_print_with_timestamp()\n\n    if args.tmp_dir is not None:\n        tmp_path = args.tmp_dir\n        os.makedirs(tmp_path, exist_ok=True)\n        tempfile.tempdir = tmp_path\n\n    if args.server_name is not None:\n        server_name = args.server_name\n    else:\n        server_name = '0.0.0.0' if args.local_network else '127.0.0.1'\n\n    if args.weights is not None:\n        weights_path = args.weights\n    else:\n        weights_path = \"naver/\" + args.model_name\n    model = AsymmetricCroCo3DStereo.from_pretrained(weights_path).to(args.device)\n\n    # dust3r will write the 3D model inside tmpdirname\n    with tempfile.TemporaryDirectory(suffix='dust3r_gradio_demo') as tmpdirname:\n        if not args.silent:\n            print('Outputing stuff in', tmpdirname)\n        main_demo(tmpdirname, model, args.device, args.image_size, server_name, args.server_port, silent=args.silent)\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "dust3r",
          "type": "tree",
          "content": null
        },
        {
          "name": "dust3r_visloc",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.126953125,
          "content": "torch\ntorchvision\nroma\ngradio\nmatplotlib\ntqdm\nopencv-python\nscipy\neinops\ntrimesh\ntensorboard\npyglet<2\nhuggingface-hub[torch]>=0.22"
        },
        {
          "name": "requirements_optional.txt",
          "type": "blob",
          "size": 0.1962890625,
          "content": "pillow-heif  # add heif/heic image support\npyrender  # for rendering depths in scannetpp\nkapture  # for visloc data loading\nkapture-localization\nnumpy-quaternion\npycolmap  # for pnp\nposelib  # for pnp\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 0.447265625,
          "content": "#!/usr/bin/env python3\n# Copyright (C) 2024-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n#\n# --------------------------------------------------------\n# training executable for DUSt3R\n# --------------------------------------------------------\nfrom dust3r.training import get_args_parser, train\n\nif __name__ == '__main__':\n    args = get_args_parser()\n    args = args.parse_args()\n    train(args)\n"
        },
        {
          "name": "visloc.py",
          "type": "blob",
          "size": 8.9765625,
          "content": "#!/usr/bin/env python3\n# Copyright (C) 2024-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n#\n# --------------------------------------------------------\n# Simple visloc script\n# --------------------------------------------------------\nimport numpy as np\nimport random\nimport argparse\nfrom tqdm import tqdm\nimport math\n\nfrom dust3r.inference import inference\nfrom dust3r.model import AsymmetricCroCo3DStereo\nfrom dust3r.utils.geometry import find_reciprocal_matches, xy_grid, geotrf\n\nfrom dust3r_visloc.datasets import *\nfrom dust3r_visloc.localization import run_pnp\nfrom dust3r_visloc.evaluation import get_pose_error, aggregate_stats, export_results\n\n\ndef get_args_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset\", type=str, required=True, help=\"visloc dataset to eval\")\n    parser_weights = parser.add_mutually_exclusive_group(required=True)\n    parser_weights.add_argument(\"--weights\", type=str, help=\"path to the model weights\", default=None)\n    parser_weights.add_argument(\"--model_name\", type=str, help=\"name of the model weights\",\n                                choices=[\"DUSt3R_ViTLarge_BaseDecoder_512_dpt\",\n                                         \"DUSt3R_ViTLarge_BaseDecoder_512_linear\",\n                                         \"DUSt3R_ViTLarge_BaseDecoder_224_linear\"])\n    parser.add_argument(\"--confidence_threshold\", type=float, default=3.0,\n                        help=\"confidence values higher than threshold are invalid\")\n    parser.add_argument(\"--device\", type=str, default='cuda', help=\"pytorch device\")\n    parser.add_argument(\"--pnp_mode\", type=str, default=\"cv2\", choices=['cv2', 'poselib', 'pycolmap'],\n                        help=\"pnp lib to use\")\n    parser_reproj = parser.add_mutually_exclusive_group()\n    parser_reproj.add_argument(\"--reprojection_error\", type=float, default=5.0, help=\"pnp reprojection error\")\n    parser_reproj.add_argument(\"--reprojection_error_diag_ratio\", type=float, default=None,\n                               help=\"pnp reprojection error as a ratio of the diagonal of the image\")\n\n    parser.add_argument(\"--pnp_max_points\", type=int, default=100_000, help=\"pnp maximum number of points kept\")\n    parser.add_argument(\"--viz_matches\", type=int, default=0, help=\"debug matches\")\n\n    parser.add_argument(\"--output_dir\", type=str, default=None, help=\"output path\")\n    parser.add_argument(\"--output_label\", type=str, default='', help=\"prefix for results files\")\n    return parser\n\n\nif __name__ == '__main__':\n    parser = get_args_parser()\n    args = parser.parse_args()\n    conf_thr = args.confidence_threshold\n    device = args.device\n    pnp_mode = args.pnp_mode\n    reprojection_error = args.reprojection_error\n    reprojection_error_diag_ratio = args.reprojection_error_diag_ratio\n    pnp_max_points = args.pnp_max_points\n    viz_matches = args.viz_matches\n\n    if args.weights is not None:\n        weights_path = args.weights\n    else:\n        weights_path = \"naver/\" + args.model_name\n    model = AsymmetricCroCo3DStereo.from_pretrained(weights_path).to(args.device)\n\n    dataset = eval(args.dataset)\n    dataset.set_resolution(model)\n\n    query_names = []\n    poses_pred = []\n    pose_errors = []\n    angular_errors = []\n    for idx in tqdm(range(len(dataset))):\n        views = dataset[(idx)]  # 0 is the query\n        query_view = views[0]\n        map_views = views[1:]\n        query_names.append(query_view['image_name'])\n\n        query_pts2d = []\n        query_pts3d = []\n        for map_view in map_views:\n            # prepare batch\n            imgs = []\n            for idx, img in enumerate([query_view['rgb_rescaled'], map_view['rgb_rescaled']]):\n                imgs.append(dict(img=img.unsqueeze(0), true_shape=np.int32([img.shape[1:]]),\n                                 idx=idx, instance=str(idx)))\n            output = inference([tuple(imgs)], model, device, batch_size=1, verbose=False)\n            pred1, pred2 = output['pred1'], output['pred2']\n            confidence_masks = [pred1['conf'].squeeze(0) >= conf_thr,\n                                (pred2['conf'].squeeze(0) >= conf_thr) & map_view['valid_rescaled']]\n            pts3d = [pred1['pts3d'].squeeze(0), pred2['pts3d_in_other_view'].squeeze(0)]\n\n            # find 2D-2D matches between the two images\n            pts2d_list, pts3d_list = [], []\n            for i in range(2):\n                conf_i = confidence_masks[i].cpu().numpy()\n                true_shape_i = imgs[i]['true_shape'][0]\n                pts2d_list.append(xy_grid(true_shape_i[1], true_shape_i[0])[conf_i])\n                pts3d_list.append(pts3d[i].detach().cpu().numpy()[conf_i])\n\n            PQ, PM = pts3d_list[0], pts3d_list[1]\n            if len(PQ) == 0 or len(PM) == 0:\n                continue\n            reciprocal_in_PM, nnM_in_PQ, num_matches = find_reciprocal_matches(PQ, PM)\n            if viz_matches > 0:\n                print(f'found {num_matches} matches')\n            matches_im1 = pts2d_list[1][reciprocal_in_PM]\n            matches_im0 = pts2d_list[0][nnM_in_PQ][reciprocal_in_PM]\n            valid_pts3d = map_view['pts3d_rescaled'][matches_im1[:, 1], matches_im1[:, 0]]\n\n            # from cv2 to colmap\n            matches_im0 = matches_im0.astype(np.float64)\n            matches_im1 = matches_im1.astype(np.float64)\n            matches_im0[:, 0] += 0.5\n            matches_im0[:, 1] += 0.5\n            matches_im1[:, 0] += 0.5\n            matches_im1[:, 1] += 0.5\n            # rescale coordinates\n            matches_im0 = geotrf(query_view['to_orig'], matches_im0, norm=True)\n            matches_im1 = geotrf(query_view['to_orig'], matches_im1, norm=True)\n            # from colmap back to cv2\n            matches_im0[:, 0] -= 0.5\n            matches_im0[:, 1] -= 0.5\n            matches_im1[:, 0] -= 0.5\n            matches_im1[:, 1] -= 0.5\n\n            # visualize a few matches\n            if viz_matches > 0:\n                viz_imgs = [np.array(query_view['rgb']), np.array(map_view['rgb'])]\n                from matplotlib import pyplot as pl\n                n_viz = viz_matches\n                match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)\n                viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]\n\n                H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]\n                img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)\n                img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)\n                img = np.concatenate((img0, img1), axis=1)\n                pl.figure()\n                pl.imshow(img)\n                cmap = pl.get_cmap('jet')\n                for i in range(n_viz):\n                    (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T\n                    pl.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)\n                pl.show(block=True)\n\n            if len(valid_pts3d) == 0:\n                pass\n            else:\n                query_pts3d.append(valid_pts3d.cpu().numpy())\n                query_pts2d.append(matches_im0)\n\n        if len(query_pts2d) == 0:\n            success = False\n            pr_querycam_to_world = None\n        else:\n            query_pts2d = np.concatenate(query_pts2d, axis=0).astype(np.float32)\n            query_pts3d = np.concatenate(query_pts3d, axis=0)\n            if len(query_pts2d) > pnp_max_points:\n                idxs = random.sample(range(len(query_pts2d)), pnp_max_points)\n                query_pts3d = query_pts3d[idxs]\n                query_pts2d = query_pts2d[idxs]\n\n            W, H = query_view['rgb'].size\n            if reprojection_error_diag_ratio is not None:\n                reprojection_error_img = reprojection_error_diag_ratio * math.sqrt(W**2 + H**2)\n            else:\n                reprojection_error_img = reprojection_error\n            success, pr_querycam_to_world = run_pnp(query_pts2d, query_pts3d,\n                                                    query_view['intrinsics'], query_view['distortion'],\n                                                    pnp_mode, reprojection_error_img, img_size=[W, H])\n\n        if not success:\n            abs_transl_error = float('inf')\n            abs_angular_error = float('inf')\n        else:\n            abs_transl_error, abs_angular_error = get_pose_error(pr_querycam_to_world, query_view['cam_to_world'])\n\n        pose_errors.append(abs_transl_error)\n        angular_errors.append(abs_angular_error)\n        poses_pred.append(pr_querycam_to_world)\n\n    xp_label = f'tol_conf_{conf_thr}'\n    if args.output_label:\n        xp_label = args.output_label + '_' + xp_label\n    if reprojection_error_diag_ratio is not None:\n        xp_label = xp_label + f'_reproj_diag_{reprojection_error_diag_ratio}'\n    else:\n        xp_label = xp_label + f'_reproj_err_{reprojection_error}'\n    export_results(args.output_dir, xp_label, query_names, poses_pred)\n    out_string = aggregate_stats(f'{args.dataset}', pose_errors, angular_errors)\n    print(out_string)\n"
        }
      ]
    }
  ]
}