{
  "metadata": {
    "timestamp": 1736560864065,
    "page": 574,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "thunil/TecoGAN",
      "stars": 6036,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.2587890625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# others\n# TecoGAN model\nmodel\n# inputs\nLR\n# outputs\nresults\n# targets\nHR\n# runs\nex_*\ntx_*"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "LPIPSmodels",
          "type": "tree",
          "content": null
        },
        {
          "name": "LR",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.4755859375,
          "content": "# TecoGAN\nThis repository contains source code and materials for the TecoGAN project, i.e. code for a TEmporally COherent GAN for video super-resolution.\n_Authors: Mengyu Chu, You Xie, Laura Leal-Taixe, Nils Thuerey. Technical University of Munich._\n\nThis repository so far contains the code for the TecoGAN _inference_ and _training_, and downloading the training data.\nPre-trained models are also available below, you can find links for downloading and instructions below.\nThis work was published in the ACM Transactions on Graphics as \"Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation (TecoGAN)\", https://doi.org/10.1145/3386569.3392457. The video and pre-print can be found here:\n\nVideo: <https://www.youtube.com/watch?v=pZXFXtfd-Ak>\nPreprint: <https://arxiv.org/pdf/1811.09393.pdf>\nSupplemental results: <https://ge.in.tum.de/wp-content/uploads/2020/05/ClickMe.html>\n\n![TecoGAN teaser image](resources/teaser.jpg)\n\n### Additional Generated Outputs\n\nOur method generates fine details that \npersist over the course of long generated video sequences. E.g., the mesh structures of the armor,\nthe scale patterns of the lizard, and the dots on the back of the spider highlight the capabilities of our method.\nOur spatio-temporal discriminator plays a key role to guide the generator network towards producing coherent detail.\n\n<img src=\"resources/tecoGAN-lizard.gif\" alt=\"Lizard\" width=\"900\"/><br>\n\n<img src=\"resources/tecoGAN-armour.gif\" alt=\"Armor\" width=\"900\"/><br>\n\n<img src=\"resources/tecoGAN-spider.gif\" alt=\"Spider\" width=\"600\" hspace=\"150\"/><br>\n\n### Running the TecoGAN Model\n\nBelow you can find a quick start guide for running a trained TecoGAN model.\nFor further explanations of the parameters take a look at the runGan.py file.  \nNote: evaluation (test case 2) currently requires an Nvidia GPU with `CUDA`. \n`tkinter` is also required and may be installed via the `python3-tk` package.\n\n```bash\n# Install tensorflow1.8+,\npip3 install --ignore-installed --upgrade tensorflow-gpu # or tensorflow\n# Install PyTorch (only necessary for the metric evaluations) and other things...\npip3 install -r requirements.txt\n\n# Download our TecoGAN model, the _Vid4_ and _TOS_ scenes shown in our paper and video.\npython3 runGan.py 0\n\n# Run the inference mode on the calendar scene.\n# You can take a look of the parameter explanations in the runGan.py, feel free to try other scenes!\npython3 runGan.py 1 \n\n# Evaluate the results with 4 metrics, PSNR, LPIPS[1], and our temporal metrics tOF and tLP with pytorch.\n# Take a look at the paper for more details! \npython3 runGan.py 2\n\n```\n\n### Train the TecoGAN Model\n\n#### 1. Prepare the Training Data\n\nThe training and validation dataset can be downloaded with the following commands into a chosen directory `TrainingDataPath`.  Note: online video downloading requires youtube-dl.  \n\n```bash\n# Install youtube-dl for online video downloading\npip install --user --upgrade youtube-dl\n\n# take a look of the parameters first:\npython3 dataPrepare.py --help\n\n# To be on the safe side, if you just want to see what will happen, the following line won't download anything,\n# and will only save information into log file.\n# TrainingDataPath is still important, it the directory where logs are saved: TrainingDataPath/log/logfile_mmddHHMM.txt\npython3 dataPrepare.py --start_id 2000 --duration 120 --disk_path TrainingDataPath --TEST\n\n# This will create 308 subfolders under TrainingDataPath, each with 120 frames, from 28 online videos.\n# It takes a long time.\npython3 dataPrepare.py --start_id 2000 --duration 120 --REMOVE --disk_path TrainingDataPath\n\n\n```\n\nOnce ready, please update the parameter TrainingDataPath in runGAN.py (for case 3 and case 4), and then you can start training with the downloaded data! \n\nNote: most of the data (272 out of 308 sequences) are the same as the ones we used for the published models, but some (36 out of 308) are not online anymore. Hence the script downloads suitable replacements.\n\n\n#### 2. Train the Model  \nThis section gives command to train a new TecoGAN model. Detail and additional parameters can be found in the runGan.py file. Note: the tensorboard gif summary requires ffmpeg.\n\n```bash\n# Install ffmpeg for the  gif summary\nsudo apt-get install ffmpeg # or conda install ffmpeg\n\n# Train the TecoGAN model, based on our FRVSR model\n# Please check and update the following parameters: \n# - VGGPath, it uses ./model/ by default. The VGG model is ca. 500MB\n# - TrainingDataPath (see above)\n# - in main.py you can also adjust the output directory of the  testWhileTrain() function if you like (it will write into a train/ sub directory by default)\npython3 runGan.py 3\n\n# Train without Dst, (i.e. a FRVSR model)\npython3 runGan.py 4\n\n# View log via tensorboard\ntensorboard --logdir='ex_TecoGANmm-dd-hh/log' --port=8008\n\n```\n\n### Tensorboard GIF Summary Example\n<img src=\"resources/gif_summary_example.gif\" alt=\"gif_summary_example\" width=\"600\" hspace=\"150\"/><br>\n\n### Acknowledgements\nThis work was funded by the ERC Starting Grant realFlow (ERC StG-2015-637014).  \nPart of the code is based on LPIPS[1], Photo-Realistic SISR[2] and gif_summary[3].\n\n### Reference\n[1] [The Unreasonable Effectiveness of Deep Features as a Perceptual Metric (LPIPS)](https://github.com/richzhang/PerceptualSimilarity)  \n[2] [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://github.com/brade31919/SRGAN-tensorflow.git)  \n[3] [gif_summary](https://colab.research.google.com/drive/1vgD2HML7Cea_z5c3kPBcsHUIxaEVDiIc)\n\nTUM I15 <https://ge.in.tum.de/> , TUM <https://www.tum.de/>\n"
        },
        {
          "name": "dataPrepare.py",
          "type": "blob",
          "size": 6.9599609375,
          "content": "import os, sys, datetime\r\nimport cv2 as cv\r\nimport argparse\r\nimport youtube_dl\r\n\r\nfrom lib.data import video\r\n\r\n\r\n# ------------------------------------parameters------------------------------#\r\nparser = argparse.ArgumentParser(description='Process parameters.', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\nparser.add_argument('--start_id', default=2000, type=int, help='starting scene index')\r\nparser.add_argument('--duration', default=120, type=int, help='scene duration')\r\nparser.add_argument('--disk_path', default=\"/mnt/netdisk/data/video/\", help='the path to save the dataset')\r\nparser.add_argument('--summary_dir', default=\"\", help='the path to save the log')\r\nparser.add_argument('--REMOVE', action='store_true', help='whether to remove the original video file after data preparation')\r\nparser.add_argument('--TEST', action='store_true', help='verify video links, save information in log, no real video downloading!')\r\n\r\nFlags = parser.parse_args()\r\n\r\nif Flags.summary_dir == \"\":\r\n    Flags.summary_dir = os.path.join(Flags.disk_path, \"log/\")\r\nos.path.isdir(Flags.disk_path) or os.makedirs(Flags.disk_path)\r\nos.path.isdir(Flags.summary_dir) or os.makedirs(Flags.summary_dir)\r\n\r\nlink_path = \"https://vimeo.com/\"\r\nvideo_data_dict = { \r\n# Videos and frames are hard-coded. \r\n# We select frames to make sure that there is no scene switching in the data\r\n# We assume that the Flags.duration is 120\r\n    \"121649159\" : [0, 310,460,720,860], #1\r\n    \"40439273\"  : [90,520,700,1760,2920,3120,3450,4750,4950,5220,6500,6900,9420,9750], #2\r\n    \"87389090\"  : [100,300,500,800,1000,1200,1500,1900,2050,2450,2900], #3\r\n    \"335874600\" : [287, 308, 621, 1308, 1538, 1768, 2036, 2181, 2544, 2749, 2867, 3404, 3543, 3842, 4318, 4439,\r\n                    4711, 4900, 7784, 8811, 9450],  # new, old #[4,6,13,14,19] 404\r\n    \"114053015\" : [30,1150,2160,2340,3190,3555], #5 \r\n    \"160578133\" : [550,940,1229,1460,2220,2900, 3180, 4080, 4340, 4612, 4935, \r\n                    5142, 5350, 5533, 7068], # new, old #[20,21,27,29,30,35] 404\r\n    \"148058982\" : [80,730,970,1230,1470,1740], #7\r\n    \"150225201\" : [0,560,1220,1590,1780], #8\r\n    \"145096806\" : [0,300,550,800,980,1500], #9\r\n    \"125621327\" : [240,900,1040,1300,1970,2130,2530,3020,3300,3620,3830,4300,4700,4960], #10\r\n    \"162166758\" : [120,350,540,750,950,1130,1320,1530,1730,1930], #11\r\n    \"115829238\" : [140,450,670,910,1100,1380,1520,1720], #12\r\n    \"159455925\" : [40,340,490,650,850,1180,1500,1800,2000,2300,2500,2800,3200], #15\r\n    \"193873193\" : [0,280,1720], #16\r\n    \"133842385\" : [300,430,970,1470,1740,2110,2240,2760,3080,3210,3400,3600], #17\r\n    \"97692560\"  : [0,210,620,930,1100,1460,1710,2400,2690,3200,3400,3560,3780], #18\r\n    \"142480565\" : [835,1380,1520,1700,2370,4880], #22\r\n    \"174952003\" : [480,680,925,1050,1200,1380,1600,1800,2100,2350,2480,2680,3000,3200,3460,4500,4780,\r\n                    5040,5630,5830,6400,6680,7300,7500,7800], #23\r\n    \"165643973\" : [300,600,1000,1500,1700,1900,2280,2600,2950,3200,3500,3900,4300,4500], #24\r\n    \"163736142\" : [120,400,700,1000,1300,1500,1750,2150,2390,2550,3100,3400,3800,4100,4400,4800,5100,5500,5800,6300], #25\r\n    \"189872577\" : [0,170,340,4380,4640,5140,7300,7470,7620,7860,9190,9370], #26\r\n    \"181180995\" : [30,160,400,660,990,2560,2780,3320,3610,5860,6450,7260,7440,8830,9020,9220,9390,], #28\r\n    \"167892347\" : [220,1540,2120,2430,5570,6380,6740],  #31\r\n    \"146484162\" : [1770,2240,3000,4800,4980,5420,6800],  #32\r\n    \"204313990\" : [110],   #33\r\n    \"169958461\" : [140,700,1000,1430,1630,1900,2400,2600,2800,3000,3200,3600,3900,4200,4600,5000,5700,\r\n                    6000,6400,6800,7100,7600,7900,8200],   #34\r\n    \"198634890\" : [200,320,440,1200,1320,1560,1680,1800,1920,3445],   #36\r\n    \"89936769\"  : [1260,1380,1880], #37\r\n}\r\n\r\n\r\n# ------------------------------------log------------------------------#\r\ndef print_configuration_op(FLAGS):\r\n    print('[Configurations]:')\r\n    for name, value in FLAGS.__dict__.items():\r\n        print('\\t%s: %s'%(name, str(value)))\r\n    print('End of configuration')\r\n    \r\nclass MyLogger(object):\r\n    def __init__(self):\r\n        self.terminal = sys.stdout\r\n        now_str = datetime.datetime.now().strftime(\"%m%d%H%M\")\r\n        self.log = open(Flags.summary_dir + \"logfile_%s.txt\"%now_str, \"a\") \r\n\r\n    def write(self, message):\r\n        self.terminal.write(message)\r\n        self.log.write(message) \r\n\r\n    def flush(self):\r\n        self.log.flush()\r\n        \r\nsys.stdout = MyLogger()\r\nprint_configuration_op(Flags)\r\n\r\n\r\n# ------------------------------------tool------------------------------#\r\ndef gen_frames(infile, outdir, width, height, start, duration, savePNG=True):\r\n    print(\"folder %s: %dx[%d,%d]//2 at frame %d of %s\"\r\n        %(outdir, duration, width, height, start,infile,))\r\n    \r\n    if savePNG:\r\n        cam = video.create_capture(infile)\r\n        for i in range(duration):\r\n            colFull = video.getImg(cam, i+start) \r\n            filename = outdir+'col_high'+(\"_%04d.png\"%(i))\r\n            cv.imwrite( filename, colFull)\r\n\r\n\r\n# ------------------------------------main------------------------------#\r\ncur_id, valid_video, try_num = Flags.start_id, 0, 0\r\n\r\nfor keys in video_data_dict:\r\n    try_num += len(video_data_dict[keys])\r\nprint(\"Try loading %dx%d.\"%(try_num, Flags.duration))\r\n             \r\nydl = youtube_dl.YoutubeDL( \r\n    {'format': 'bestvideo/best',\r\n     'outtmpl': os.path.join(Flags.disk_path, '%(id)s.%(ext)s'),})\r\n     \r\nsaveframes = not Flags.TEST\r\nfor keys in video_data_dict:\r\n    tar_vid_input = link_path + keys\r\n    print(tar_vid_input)\r\n    info_dict = {\"width\":-1, \"height\": -1, \"ext\": \"xxx\", }\r\n    \r\n    # download video from vimeo\r\n    try:\r\n        info_dict = ydl.extract_info(tar_vid_input, download=saveframes)\r\n        # we only need info_dict[\"ext\"], info_dict[\"width\"], info_dict[\"height\"]\r\n    except KeyboardInterrupt:\r\n        print(\"KeyboardInterrupt!\")\r\n        exit()\r\n    except:\r\n        print(\"youtube_dl error:\" + tar_vid_input)\r\n        pass\r\n    \r\n    # check the downloaded video\r\n    tar_vid_output = os.path.join(Flags.disk_path, keys+'.'+info_dict[\"ext\"])\r\n    if saveframes and (not os.path.exists(tar_vid_output)):\r\n        print(\"Skipped invalid link or other error:\" + tar_vid_input)\r\n        continue\r\n    if info_dict[\"width\"] < 400 or info_dict[\"height\"] < 400:\r\n        print(\"Skipped videos of small size %dx%d\"%(info_dict[\"width\"] , info_dict[\"height\"] ))\r\n        continue\r\n    valid_video = valid_video + 1\r\n    \r\n    # get training frames\r\n    for start_fr in video_data_dict[keys]:\r\n        tar_dir = os.path.join(Flags.disk_path, \"scene_%04d/\"% cur_id)\r\n        if(saveframes):\r\n            os.path.isdir(tar_dir) or os.makedirs(tar_dir)\r\n        gen_frames(tar_vid_output, tar_dir, info_dict[\"width\"], info_dict[\"height\"], start_fr, Flags.duration, saveframes)\r\n        cur_id = cur_id+1\r\n        \r\n    if saveframes and Flags.REMOVE:\r\n        print(\"remove \", tar_vid_output)\r\n        os.remove(tar_vid_output)\r\n        \r\nprint(\"Done: get %d valid folders with %d frames from %d videos.\" % (cur_id - Flags.start_id, Flags.duration, valid_video))\r\n\r\n"
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 21.0654296875,
          "content": "import numpy as np\nimport os, math, time, collections, numpy as np\n''' TF_CPP_MIN_LOG_LEVEL\n0 = all messages are logged (default behavior)\n1 = INFO messages are not printed\n2 = INFO and WARNING messages are not printed\n3 = INFO, WARNING, and ERROR messages are not printed\nDisable Logs for now '''\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport tensorflow as tf\nfrom tensorflow.python.util import deprecation\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\nimport random as rn\n\n# fix all randomness, except for multi-treading or GPU process\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(42)\nrn.seed(12345)\ntf.set_random_seed(1234)\n\nimport tensorflow.contrib.slim as slim\nimport sys, shutil, subprocess\n\nfrom lib.ops import *\nfrom lib.dataloader import inference_data_loader, frvsr_gpu_data_loader\nfrom lib.frvsr import generator_F, fnet\nfrom lib.Teco import FRVSR, TecoGAN\n\n\nFlags = tf.app.flags\n\nFlags.DEFINE_integer('rand_seed', 1 , 'random seed' )\n\n# Directories\nFlags.DEFINE_string('input_dir_LR', None, 'The directory of the input resolution input data, for inference mode')\nFlags.DEFINE_integer('input_dir_len', -1, 'length of the input for inference mode, -1 means all')\nFlags.DEFINE_string('input_dir_HR', None, 'The directory of the input resolution input data, for inference mode')\nFlags.DEFINE_string('mode', 'inference', 'train, or inference')\nFlags.DEFINE_string('output_dir', None, 'The output directory of the checkpoint')\nFlags.DEFINE_string('output_pre', '', 'The name of the subfolder for the images')\nFlags.DEFINE_string('output_name', 'output', 'The pre name of the outputs')\nFlags.DEFINE_string('output_ext', 'jpg', 'The format of the output when evaluating')\nFlags.DEFINE_string('summary_dir', None, 'The dirctory to output the summary')\n\n# Models\nFlags.DEFINE_string('checkpoint', None, 'If provided, the weight will be restored from the provided checkpoint')\nFlags.DEFINE_integer('num_resblock', 16, 'How many residual blocks are there in the generator')\n# Models for training\nFlags.DEFINE_boolean('pre_trained_model', False, 'If True, the weight of generator will be loaded as an initial point'\n                                                     'If False, continue the training')\nFlags.DEFINE_string('vgg_ckpt', None, 'path to checkpoint file for the vgg19')\n\n# Machine resources\nFlags.DEFINE_string('cudaID', '0', 'CUDA devices')\nFlags.DEFINE_integer('queue_thread', 6, 'The threads of the queue (More threads can speedup the training process.')\nFlags.DEFINE_integer('name_video_queue_capacity', 512, 'The capacity of the filename queue (suggest large to ensure'\n                                                  'enough random shuffle.')\nFlags.DEFINE_integer('video_queue_capacity', 256, 'The capacity of the video queue (suggest large to ensure'\n                                                   'enough random shuffle')\nFlags.DEFINE_integer('video_queue_batch', 2, 'shuffle_batch queue capacity')\n                                                   \n# Training details\n# The data preparing operation\nFlags.DEFINE_integer('RNN_N', 10, 'The number of the rnn recurrent length')\nFlags.DEFINE_integer('batch_size', 4, 'Batch size of the input batch')\nFlags.DEFINE_boolean('flip', True, 'Whether random flip data augmentation is applied')\nFlags.DEFINE_boolean('random_crop', True, 'Whether perform the random crop')\nFlags.DEFINE_boolean('movingFirstFrame', True, 'Whether use constant moving first frame randomly.')\nFlags.DEFINE_integer('crop_size', 32, 'The crop size of the training image')\n# Training data settings\nFlags.DEFINE_string('input_video_dir', '', 'The directory of the video input data, for training')\nFlags.DEFINE_string('input_video_pre', 'scene', 'The pre of the directory of the video input data')\nFlags.DEFINE_integer('str_dir', 1000, 'The starting index of the video directory')\nFlags.DEFINE_integer('end_dir', 2000, 'The ending index of the video directory')\nFlags.DEFINE_integer('end_dir_val', 2050, 'The ending index for validation of the video directory')\nFlags.DEFINE_integer('max_frm', 119, 'The ending index of the video directory')\n# The loss parameters\nFlags.DEFINE_float('vgg_scaling', -0.002, 'The scaling factor for the VGG perceptual loss, disable with negative value')\nFlags.DEFINE_float('warp_scaling', 1.0, 'The scaling factor for the warp')\nFlags.DEFINE_boolean('pingpang', False, 'use bi-directional recurrent or not')\nFlags.DEFINE_float('pp_scaling', 1.0, 'factor of pingpang term, only works when pingpang is True')\n# Training parameters\nFlags.DEFINE_float('EPS', 1e-12, 'The eps added to prevent nan')\nFlags.DEFINE_float('learning_rate', 0.0001, 'The learning rate for the network')\nFlags.DEFINE_integer('decay_step', 500000, 'The steps needed to decay the learning rate')\nFlags.DEFINE_float('decay_rate', 0.5, 'The decay rate of each decay step')\nFlags.DEFINE_boolean('stair', False, 'Whether perform staircase decay. True => decay in discrete interval.')\nFlags.DEFINE_float('beta', 0.9, 'The beta1 parameter for the Adam optimizer')\nFlags.DEFINE_float('adameps', 1e-8, 'The eps parameter for the Adam optimizer')\nFlags.DEFINE_integer('max_epoch', None, 'The max epoch for the training')\nFlags.DEFINE_integer('max_iter', 1000000, 'The max iteration of the training')\nFlags.DEFINE_integer('display_freq', 20, 'The diplay frequency of the training process')\nFlags.DEFINE_integer('summary_freq', 100, 'The frequency of writing summary')\nFlags.DEFINE_integer('save_freq', 10000, 'The frequency of saving images')\n# Dst parameters\nFlags.DEFINE_float('ratio', 0.01, 'The ratio between content loss and adversarial loss')\nFlags.DEFINE_boolean('Dt_mergeDs', True, 'Whether only use a merged Discriminator.')\nFlags.DEFINE_float('Dt_ratio_0', 1.0, 'The starting ratio for the temporal adversarial loss')\nFlags.DEFINE_float('Dt_ratio_add', 0.0, 'The increasing ratio for the temporal adversarial loss')\nFlags.DEFINE_float('Dt_ratio_max', 1.0, 'The max ratio for the temporal adversarial loss')\nFlags.DEFINE_float('Dbalance', 0.4, 'An adaptive balancing for Discriminators')\nFlags.DEFINE_float('crop_dt', 0.75, 'factor of dt crop') # dt input size = crop_size*crop_dt\nFlags.DEFINE_boolean('D_LAYERLOSS', True, 'Whether use layer loss from D')\n\nFLAGS = Flags.FLAGS\n\n# Set CUDA devices correctly if you use multiple gpu system\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=FLAGS.cudaID \n# Fix randomness\nmy_seed = FLAGS.rand_seed\nrn.seed(my_seed)\nnp.random.seed(my_seed)\ntf.set_random_seed(my_seed)\n\n# Check the output_dir is given\nif FLAGS.output_dir is None:\n    raise ValueError('The output directory is needed')\n# Check the output directory to save the checkpoint\nif not os.path.exists(FLAGS.output_dir):\n    os.mkdir(FLAGS.output_dir)\n# Check the summary directory to save the event\nif not os.path.exists(FLAGS.summary_dir):\n    os.mkdir(FLAGS.summary_dir)\n\n# custom Logger to write Log to file\nclass Logger(object):\n    def __init__(self):\n        self.terminal = sys.stdout\n        self.log = open(FLAGS.summary_dir + \"logfile.txt\", \"a\") \n    def write(self, message):\n        self.terminal.write(message)\n        self.log.write(message) \n    def flush(self):\n        self.log.flush()\n        \nsys.stdout = Logger()\n\ndef printVariable(scope, key = tf.GraphKeys.MODEL_VARIABLES):\n    print(\"Scope %s:\" % scope)\n    variables_names = [ [v.name, v.get_shape().as_list()] for v in tf.get_collection(key, scope=scope)]\n    total_sz = 0\n    for k in variables_names:\n        print (\"Variable: \" + k[0])\n        print (\"Shape: \" + str(k[1]))\n        total_sz += np.prod(k[1])\n    print(\"total size: %d\" %total_sz)\n    \ndef preexec(): # Don't forward signals.\n    os.setpgrp()\n    \ndef testWhileTrain(FLAGS, testno = 0):\n    '''\n        this function is called during training, Hard-Coded!!\n        to try the \"inference\" mode when a new model is saved.\n        The code has to be updated from machine to machine...\n        depending on python, and your training settings\n    '''\n    desstr = os.path.join(FLAGS.output_dir, 'train/') # saving in the ./train/ directory\n    cmd1 = [\"python3\", \"main.py\", # never tested with python2...\n        \"--output_dir\", desstr, \n        \"--summary_dir\", desstr,\n        \"--mode\",\"inference\",\n        \"--num_resblock\", \"%d\"%FLAGS.num_resblock,\n        \"--checkpoint\", os.path.join(FLAGS.output_dir, 'model-%d'%testno),\n        \"--cudaID\", FLAGS.cudaID]\n    # a folder for short test \n    cmd1 += [\"--input_dir_LR\", \"./LR/calendar/\", # update the testing sequence\n             \"--output_pre\", \"\", # saving in train folder directly\n             \"--output_name\", \"%09d\"%testno, # name\n             \"--input_dir_len\", \"10\",]\n    print('[testWhileTrain] step %d:'%testno)\n    print(' '.join(cmd1))\n    # ignore signals\n    return subprocess.Popen(cmd1, preexec_fn = preexec)\n    \nif False: # If you want to take a look of the configuration, True\n    print_configuration_op(FLAGS)\n\n# the inference mode (just perform super resolution on the input image)\nif FLAGS.mode == 'inference':\n    if FLAGS.checkpoint is None:\n        raise ValueError('The checkpoint file is needed to performing the test.')\n\n    # Declare the test data reader\n    inference_data = inference_data_loader(FLAGS)\n    input_shape = [1,] + list(inference_data.inputs[0].shape)\n    output_shape = [1,input_shape[1]*4, input_shape[2]*4, 3]\n    oh = input_shape[1] - input_shape[1]//8 * 8\n    ow = input_shape[2] - input_shape[2]//8 * 8\n    paddings = tf.constant([[0,0], [0,oh], [0,ow], [0,0]])\n    print(\"input shape:\", input_shape)\n    print(\"output shape:\", output_shape)\n    \n    # build the graph\n    inputs_raw = tf.placeholder(tf.float32, shape=input_shape, name='inputs_raw')\n    \n    pre_inputs = tf.Variable(tf.zeros(input_shape), trainable=False, name='pre_inputs')\n    pre_gen = tf.Variable(tf.zeros(output_shape), trainable=False, name='pre_gen')\n    pre_warp = tf.Variable(tf.zeros(output_shape), trainable=False, name='pre_warp')\n    \n    transpose_pre = tf.space_to_depth(pre_warp, 4)\n    inputs_all = tf.concat( (inputs_raw, transpose_pre), axis = -1)\n    with tf.variable_scope('generator'):\n        gen_output = generator_F(inputs_all, 3, reuse=False, FLAGS=FLAGS)\n        # Deprocess the images outputed from the model, and assign things for next frame\n        with tf.control_dependencies([ tf.assign(pre_inputs, inputs_raw)]):\n            outputs = tf.assign(pre_gen, deprocess(gen_output))\n    \n    inputs_frames = tf.concat( (pre_inputs, inputs_raw), axis = -1)\n    with tf.variable_scope('fnet'):\n        gen_flow_lr = fnet( inputs_frames, reuse=False)\n        gen_flow_lr = tf.pad(gen_flow_lr, paddings, \"SYMMETRIC\") \n        gen_flow = upscale_four(gen_flow_lr*4.0)\n        gen_flow.set_shape( output_shape[:-1]+[2] )\n    pre_warp_hi = tf.contrib.image.dense_image_warp(pre_gen, gen_flow)\n    before_ops = tf.assign(pre_warp, pre_warp_hi)\n\n    print('Finish building the network')\n    \n    # In inference time, we only need to restore the weight of the generator\n    var_list = tf.get_collection(tf.GraphKeys.MODEL_VARIABLES, scope='generator')\n    var_list = var_list + tf.get_collection(tf.GraphKeys.MODEL_VARIABLES, scope='fnet')\n    \n    weight_initiallizer = tf.train.Saver(var_list)\n    \n    # Define the initialization operation\n    init_op = tf.global_variables_initializer()\n    local_init_op = tf.local_variables_initializer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    if (FLAGS.output_pre == \"\"):\n        image_dir = FLAGS.output_dir\n    else:\n        image_dir = os.path.join(FLAGS.output_dir, FLAGS.output_pre)\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\n        \n    with tf.Session(config=config) as sess:\n        # Load the pretrained model\n        sess.run(init_op)\n        sess.run(local_init_op)\n        \n        print('Loading weights from ckpt model')\n        weight_initiallizer.restore(sess, FLAGS.checkpoint)\n        if False: # If you want to take a look of the weights, True\n            printVariable('generator')\n            printVariable('fnet')\n        max_iter = len(inference_data.inputs)\n                \n        srtime = 0\n        print('Frame evaluation starts!!')\n        for i in range(max_iter):\n            input_im = np.array([inference_data.inputs[i]]).astype(np.float32)\n            feed_dict={inputs_raw: input_im}\n            t0 = time.time()\n            if(i != 0):\n                sess.run(before_ops, feed_dict=feed_dict)\n            output_frame = sess.run(outputs, feed_dict=feed_dict)\n            srtime += time.time()-t0\n            \n            if(i >= 5): \n                name, _ = os.path.splitext(os.path.basename(str(inference_data.paths_LR[i])))\n                filename = FLAGS.output_name+'_'+name\n                print('saving image %s' % filename)\n                out_path = os.path.join(image_dir, \"%s.%s\"%(filename,FLAGS.output_ext))\n                save_img(out_path, output_frame[0])\n            else:# First 5 is a hard-coded symmetric frame padding, ignored but time added!\n                print(\"Warming up %d\"%(5-i))\n    print( \"total time \" + str(srtime) + \", frame number \" + str(max_iter) )\n        \n# The training mode\nelif FLAGS.mode == 'train':\n    # hard coded save\n    filelist = ['main.py','lib/Teco.py','lib/frvsr.py','lib/dataloader.py','lib/ops.py']\n    for filename in filelist:\n        shutil.copyfile('./' + filename, FLAGS.summary_dir + filename.replace(\"/\",\"_\"))\n        \n    useValidat = tf.placeholder_with_default( tf.constant(False, dtype=tf.bool), shape=() )\n    rdata = frvsr_gpu_data_loader(FLAGS, useValidat)\n    # Data = collections.namedtuple('Data', 'paths_HR, s_inputs, s_targets, image_count, steps_per_epoch')\n    print('tData count = %d, steps per epoch %d' % (rdata.image_count, rdata.steps_per_epoch))\n    if (FLAGS.ratio>0):\n        Net = TecoGAN( rdata.s_inputs, rdata.s_targets, FLAGS )\n    else:\n        Net = FRVSR( rdata.s_inputs, rdata.s_targets, FLAGS )\n    # Network = collections.namedtuple('Network', 'gen_output, train, learning_rate, update_list, '\n    #                                     'update_list_name, update_list_avg, image_summary')\n    \n    # Add scalar summary\n    tf.summary.scalar('learning_rate', Net.learning_rate)\n    train_summary = []\n    for key, value in zip(Net.update_list_name, Net.update_list_avg):\n        # 'map_loss, scale_loss, FrameA_loss, FrameA_loss,...'\n        train_summary += [tf.summary.scalar(key, value)]\n    train_summary += Net.image_summary\n    merged = tf.summary.merge(train_summary)\n    \n    validat_summary = [] # val data statistics is not added to average\n    uplen = len(Net.update_list)\n    for key, value in zip(Net.update_list_name[:uplen], Net.update_list):\n        # 'map_loss, scale_loss, FrameA_loss, FrameA_loss,...'\n        validat_summary += [tf.summary.scalar(\"val_\" + key, value)]\n    val_merged = tf.summary.merge(validat_summary)\n\n    # Define the saver and weight initiallizer\n    saver = tf.train.Saver(max_to_keep=50)\n    # variable lists\n    all_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    tfflag = tf.GraphKeys.MODEL_VARIABLES #tf.GraphKeys.TRAINABLE_VARIABLES\n    \n    if (FLAGS.checkpoint is not None) and (FLAGS.pre_trained_model is True):\n        model_var_list = tf.get_collection(tfflag, scope='generator') + tf.get_collection(tfflag, scope='fnet')\n        assign_ops = get_existing_from_ckpt(FLAGS.checkpoint, model_var_list, rest_zero=True, print_level=1)\n        print('Prepare to load %d weights from the pre-trained model for generator and fnet'%len(assign_ops))\n        if FLAGS.ratio>0:\n            model_var_list = tf.get_collection(tfflag, scope='tdiscriminator')\n            dis_list = get_existing_from_ckpt(FLAGS.checkpoint, model_var_list, print_level=0)\n            print('Prepare to load %d weights from the pre-trained model for discriminator'%len(dis_list))\n            assign_ops += dis_list\n        \n    if FLAGS.vgg_scaling > 0.0: # VGG weights are not trainable\n        vgg_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='vgg_19')\n        vgg_restore = tf.train.Saver(vgg_var_list)\n    \n    print('Finish building the network.')\n    \n    # Start the session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    # init_op = tf.initialize_all_variables() # MonitoredTrainingSession will initialize automatically\n    with tf.train.MonitoredTrainingSession(config=config, save_summaries_secs=None, save_checkpoint_secs=None) as sess:\n        train_writer = tf.summary.FileWriter(FLAGS.summary_dir, sess.graph)\n        \n        printVariable('generator')\n        printVariable('fnet')\n        if FLAGS.ratio>0:\n            printVariable('tdiscriminator')\n                \n        if FLAGS.vgg_scaling > 0.0:\n            printVariable('vgg_19', tf.GraphKeys.GLOBAL_VARIABLES)\n            vgg_restore.restore(sess, FLAGS.vgg_ckpt)\n            print('VGG19 restored successfully!!')\n                \n        if (FLAGS.checkpoint is not None):\n            if (FLAGS.pre_trained_model is False):\n                print('Loading everything from the checkpoint to continue the training...')\n                saver.restore(sess, FLAGS.checkpoint)\n                # this will restore everything, including ADAM training parameters and global_step\n            else:\n                print('Loading weights from the pre-trained model to start a new training...')\n                sess.run(assign_ops) # only restore existing model weights\n            \n        print('The first run takes longer time for training data loading...')\n        # get the session for save\n        _sess = sess\n        while type(_sess).__name__ != 'Session':\n            # pylint: disable=W0212\n            _sess = _sess._sess\n        save_sess = _sess\n\n        if 1:\n            print('Save initial checkpoint, before any training')\n            init_run_no = sess.run(Net.global_step)\n            saver.save(save_sess, os.path.join(FLAGS.output_dir, 'model'), global_step=init_run_no)\n            testWhileTrain(FLAGS, init_run_no) # make sure that testWhileTrain works\n        \n        # Performing the training\n        frame_len = (FLAGS.RNN_N*2-1) if FLAGS.pingpang else FLAGS.RNN_N\n        max_iter, step, start = FLAGS.max_iter, 0, time.time()\n        if max_iter is None:\n            if FLAGS.max_epoch is None:\n                raise ValueError('one of max_epoch or max_iter should be provided')\n            else:\n                max_iter = FLAGS.max_epoch * rdata.steps_per_epoch\n        try:\n            for step in range(max_iter):\n                run_step = sess.run(Net.global_step) + 1\n                fetches = { \"train\": Net.train, \"learning_rate\": Net.learning_rate }\n            \n                if (run_step % FLAGS.display_freq) == 0:\n                    for key, value in zip(Net.update_list_name, Net.update_list_avg):\n                        fetches[str(key)] = value\n                if (run_step % FLAGS.summary_freq) == 0:\n                    fetches[\"summary\"] = merged\n                    \n                results = sess.run(fetches)\n                if(step == 0):\n                    print('Optimization starts!!!(Ctrl+C to stop, will try saving the last model...)')\n                \n                if (run_step % FLAGS.summary_freq) == 0:\n                    print('Run and Recording summary!!')\n                    train_writer.add_summary(results['summary'], run_step)\n                    val_fetches = {}\n                    for name, value in zip(Net.update_list_name[:uplen], Net.update_list):\n                        val_fetches['val_' + name] = value\n                    val_fetches['summary'] = val_merged\n                    val_results = sess.run(val_fetches, feed_dict={useValidat: True})\n                    train_writer.add_summary(val_results['summary'], run_step)\n                    print('-----------Validation data scalars-----------')\n                    for name in Net.update_list_name[:uplen]:\n                        print('val_' + name, val_results['val_' + name])\n                        \n                if (run_step % FLAGS.display_freq) == 0:\n                    train_epoch = math.ceil(run_step / rdata.steps_per_epoch)\n                    train_step = (run_step - 1) % rdata.steps_per_epoch + 1\n                    rate = (step + 1) * FLAGS.batch_size / (time.time() - start)\n                    remaining = (max_iter - step) * FLAGS.batch_size / rate\n                    print(\"progress  epoch %d  step %d  image/sec %0.1fx%02d  remaining %dh%dm\" % \n                        (train_epoch, train_step, rate, frame_len, \n                        remaining // 3600, (remaining%3600) // 60))\n                        \n                    print(\"global_step\", run_step)\n                    print(\"learning_rate\", results['learning_rate'])\n                    for name in Net.update_list_name:\n                        print(name, results[name])\n                        \n                if (run_step % FLAGS.save_freq) == 0:\n                    print('Save the checkpoint')\n                    saver.save(save_sess, os.path.join(FLAGS.output_dir, 'model'), global_step=int(run_step))\n                    testWhileTrain(FLAGS, run_step)\n                \n        except KeyboardInterrupt:\n            if step > 1:\n                print('main.py: KeyboardInterrupt->saving the checkpoint')\n                saver.save(save_sess, os.path.join(FLAGS.output_dir, 'model'), global_step=int(run_step))\n                testWhileTrain(FLAGS, run_step).communicate()\n            print('main.py: quit')\n            exit()\n        print('Optimization done!!!!!!!!!!!!')\n"
        },
        {
          "name": "metrics.py",
          "type": "blob",
          "size": 9.7275390625,
          "content": "import numpy as np\r\nimport cv2\r\nimport os, sys\r\nimport pandas as pd\r\nfrom LPIPSmodels import util\r\nimport LPIPSmodels.dist_model as dm\r\nfrom skimage.measure import compare_ssim\r\n\r\nfrom absl import flags\r\nflags.DEFINE_string('output', None, 'the path of output directory')\r\nflags.DEFINE_string('results', None, 'the list of paths of result directory')\r\nflags.DEFINE_string('targets', None, 'the list of paths of target directory')\r\n\r\nFLAGS = flags.FLAGS\r\nFLAGS(sys.argv)\r\n\r\nif(not os.path.exists(FLAGS.output)):\r\n    os.mkdir(FLAGS.output)\r\n    \r\n# The operation used to print out the configuration\r\ndef print_configuration_op(FLAGS):\r\n    print('[Configurations]:')\r\n    for name, value in FLAGS.flag_values_dict().items():\r\n        print('\\t%s: %s'%(name, str(value)))\r\n    print('End of configuration')\r\n# custom Logger to write Log to file\r\n\r\ndef listPNGinDir(dirpath):\r\n    filelist = os.listdir(dirpath)\r\n    filelist = [_ for _ in filelist if _.endswith(\".png\")] \r\n    filelist = [_ for _ in filelist if not _.startswith(\"IB\")] \r\n    filelist = sorted(filelist)\r\n    filelist.sort(key=lambda f: int(''.join(list(filter(str.isdigit, f))) or -1))\r\n    result = [os.path.join(dirpath,_) for _ in filelist if _.endswith(\".png\")]\r\n    return result\r\n\r\ndef _rgb2ycbcr(img, maxVal=255):\r\n##### color space transform, originally from https://github.com/yhjo09/VSR-DUF ##### \r\n    O = np.array([[16],\r\n                  [128],\r\n                  [128]])\r\n    T = np.array([[0.256788235294118, 0.504129411764706, 0.097905882352941],\r\n                  [-0.148223529411765, -0.290992156862745, 0.439215686274510],\r\n                  [0.439215686274510, -0.367788235294118, -0.071427450980392]])\r\n\r\n    if maxVal == 1:\r\n        O = O / 255.0\r\n\r\n    t = np.reshape(img, (img.shape[0]*img.shape[1], img.shape[2]))\r\n    t = np.dot(t, np.transpose(T))\r\n    t[:, 0] += O[0]\r\n    t[:, 1] += O[1]\r\n    t[:, 2] += O[2]\r\n    ycbcr = np.reshape(t, [img.shape[0], img.shape[1], img.shape[2]])\r\n\r\n    return ycbcr\r\n\r\ndef to_uint8(x, vmin, vmax):\r\n##### color space transform, originally from https://github.com/yhjo09/VSR-DUF ##### \r\n    x = x.astype('float32')\r\n    x = (x-vmin)/(vmax-vmin)*255 # 0~255\r\n    return np.clip(np.round(x), 0, 255)\r\n\r\ndef psnr(img_true, img_pred):\r\n##### PSNR with color space transform, originally from https://github.com/yhjo09/VSR-DUF ##### \r\n    Y_true = _rgb2ycbcr(to_uint8(img_true, 0, 255), 255)[:,:,0]\r\n    Y_pred = _rgb2ycbcr(to_uint8(img_pred, 0, 255), 255)[:,:,0]\r\n    diff =  Y_true - Y_pred\r\n    rmse = np.sqrt(np.mean(np.power(diff,2)))\r\n    return 20*np.log10(255./rmse)\r\n    \r\ndef ssim(img_true, img_pred): ##### SSIM ##### \r\n    Y_true = _rgb2ycbcr(to_uint8(img_true, 0, 255), 255)[:,:,0]\r\n    Y_pred = _rgb2ycbcr(to_uint8(img_pred, 0, 255), 255)[:,:,0]\r\n    return compare_ssim(Y_true, Y_pred, data_range=Y_pred.max() - Y_pred.min())\r\n\r\ndef crop_8x8( img ):\r\n    ori_h = img.shape[0]\r\n    ori_w = img.shape[1]\r\n    \r\n    h = (ori_h//32) * 32\r\n    w = (ori_w//32) * 32\r\n    \r\n    while(h > ori_h - 16):\r\n        h = h - 32\r\n    while(w > ori_w - 16):\r\n        w = w - 32\r\n    \r\n    y = (ori_h - h) // 2\r\n    x = (ori_w - w) // 2\r\n    crop_img = img[y:y+h, x:x+w]\r\n    return crop_img, y, x\r\n\r\nclass Logger(object):\r\n    def __init__(self):\r\n        self.terminal = sys.stdout\r\n        filename = \"metricsfile.txt\"\r\n        self.log = open(os.path.join(FLAGS.output, filename), \"a\") \r\n    def write(self, message):\r\n        self.terminal.write(message)\r\n        self.log.write(message) \r\n    def flush(self):\r\n        self.log.flush()\r\n        \r\nsys.stdout = Logger()\r\n\r\nprint_configuration_op(FLAGS)\r\n\r\nresult_list = FLAGS.results.split(',')\r\ntarget_list = FLAGS.targets.split(',')\r\nfolder_n = len(result_list)\r\n\r\n\r\nmodel = dm.DistModel()\r\nmodel.initialize(model='net-lin',net='alex',use_gpu=True)\r\n\r\ncutfr = 2\r\n# maxV = 0.4, for line 154-166\r\n\r\nkeys = [\"PSNR\", \"SSIM\", \"LPIPS\", \"tOF\", \"tLP100\"] # keys = [\"LPIPS\"]\r\nsum_dict    = dict.fromkeys([\"FrameAvg_\"+_ for _ in keys], 0)\r\nlen_dict    = dict.fromkeys(keys, 0)\r\navg_dict    = dict.fromkeys([\"Avg_\"+_ for _ in keys], 0)\r\nfolder_dict = dict.fromkeys([\"FolderAvg_\"+_ for _ in keys], 0)\r\n\r\nfor folder_i in range(folder_n):\r\n    result = listPNGinDir(result_list[folder_i])\r\n    target = listPNGinDir(target_list[folder_i])\r\n    image_no = len(target)\r\n    \r\n    list_dict = {}\r\n    for key_i in keys:\r\n        list_dict[key_i] = []\r\n    \r\n    for i in range(cutfr, image_no-cutfr):\r\n        output_img = cv2.imread(result[i])[:,:,::-1]\r\n        target_img = cv2.imread(target[i])[:,:,::-1]\r\n        msg = \"frame %d, tar %s, out %s, \"%(i, str(target_img.shape), str(output_img.shape))\r\n        if( target_img.shape[0] < output_img.shape[0]) or ( target_img.shape[1] < output_img.shape[1]): # target is not dividable by 4\r\n            output_img = output_img[:target_img.shape[0],:target_img.shape[1]]\r\n        print(result[i])\r\n        \r\n        if \"tOF\" in keys:# tOF\r\n            output_grey = cv2.cvtColor(output_img, cv2.COLOR_RGB2GRAY)\r\n            target_grey = cv2.cvtColor(target_img, cv2.COLOR_RGB2GRAY)\r\n            if (i > cutfr): # temporal metrics\r\n                target_OF=cv2.calcOpticalFlowFarneback(pre_tar_grey, target_grey, None, 0.5, 3, 15, 3, 5, 1.2, 0)\r\n                output_OF=cv2.calcOpticalFlowFarneback(pre_out_grey, output_grey, None, 0.5, 3, 15, 3, 5, 1.2, 0)\r\n                target_OF, ofy, ofx = crop_8x8(target_OF)\r\n                output_OF, ofy, ofx = crop_8x8(output_OF)\r\n                OF_diff = np.absolute(target_OF - output_OF)\r\n                if False: # for motion visualization\r\n                    tOFpath = os.path.join(FLAGS.output,\"%03d_tOF\"%folder_i)\r\n                    if(not os.path.exists(tOFpath)): os.mkdir(tOFpath)\r\n                    hsv = np.zeros_like(output_img)\r\n                    hsv[...,1] = 255\r\n                    out_path = os.path.join(tOFpath, \"flow_%04d.jpg\" %i)\r\n                    mag, ang = cv2.cartToPolar(OF_diff[...,0], OF_diff[...,1])\r\n                    # print(\"tar max %02.6f, min %02.6f, avg %02.6f\" % (mag.max(), mag.min(), mag.mean()))\r\n                    mag = np.clip(mag, 0.0, maxV)/maxV\r\n                    hsv[...,0] = ang*180/np.pi/2\r\n                    hsv[...,2] = mag * 255.0 #\r\n                    bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\r\n                    cv2.imwrite(out_path, bgr)\r\n                    \r\n                OF_diff = np.sqrt(np.sum(OF_diff * OF_diff, axis = -1)) # l1 vector norm\r\n                # OF_diff, ofy, ofx = crop_8x8(OF_diff)\r\n                list_dict[\"tOF\"].append( OF_diff.mean() )\r\n                msg += \"tOF %02.2f, \" %(list_dict[\"tOF\"][-1])\r\n            \r\n            pre_out_grey = output_grey\r\n            pre_tar_grey = target_grey\r\n\r\n        target_img, ofy, ofx = crop_8x8(target_img)\r\n        output_img, ofy, ofx = crop_8x8(output_img)\r\n            \r\n        if \"PSNR\" in keys:# psnr\r\n            list_dict[\"PSNR\"].append( psnr(target_img, output_img) )\r\n            msg +=\"psnr %02.2f\" %(list_dict[\"PSNR\"][-1])\r\n        \r\n        if \"SSIM\" in keys:# ssim\r\n            list_dict[\"SSIM\"].append( ssim(target_img, output_img) )\r\n            msg +=\", ssim %02.2f\" %(list_dict[\"SSIM\"][-1])\r\n            \r\n        if \"LPIPS\" in keys or \"tLP100\" in keys:\r\n            img0 = util.im2tensor(target_img) # RGB image from [-1,1]\r\n            img1 = util.im2tensor(output_img)\r\n        \r\n            if \"LPIPS\" in keys: # LPIPS\r\n                dist01 = model.forward(img0,img1)\r\n                list_dict[\"LPIPS\"].append( dist01[0] )\r\n                msg +=\", lpips %02.2f\" %(dist01[0])\r\n            \r\n            if \"tLP100\" in keys and (i > cutfr):# tLP, temporal metrics\r\n                dist0t = model.forward(pre_img0, img0)\r\n                dist1t = model.forward(pre_img1, img1)\r\n                # print (\"tardis %f, outdis %f\" %(dist0t, dist1t))\r\n                dist01t = np.absolute(dist0t - dist1t) * 100.0 ##########!!!!!\r\n                list_dict[\"tLP100\"].append( dist01t[0] )\r\n                msg += \", tLPx100 %02.2f\" %(dist01t[0])\r\n            pre_img0 = img0\r\n            pre_img1 = img1\r\n        \r\n        msg +=\", crop (%d, %d)\" %(ofy, ofx)\r\n        print(msg)\r\n    mode = 'w' if folder_i==0 else 'a'\r\n    \r\n    pd_dict = {}\r\n    for cur_num_data in keys:\r\n        num_data = cur_num_data+\"_%02d\" % folder_i\r\n        cur_list = np.float32(list_dict[cur_num_data])\r\n        pd_dict[num_data] = pd.Series(cur_list)\r\n        \r\n        num_data_sum = cur_list.sum()\r\n        num_data_len = cur_list.shape[0]\r\n        num_data_mean = num_data_sum / num_data_len\r\n        print(\"%s, max %02.4f, min %02.4f, avg %02.4f\" % \r\n            (num_data, cur_list.max(), cur_list.min(), num_data_mean))\r\n            \r\n        if folder_i == 0:\r\n            avg_dict[\"Avg_\"+cur_num_data] = [num_data_mean]\r\n        else:\r\n            avg_dict[\"Avg_\"+cur_num_data] += [num_data_mean]\r\n        \r\n        sum_dict[\"FrameAvg_\"+cur_num_data] += num_data_sum\r\n        len_dict[cur_num_data] += num_data_len\r\n        folder_dict[\"FolderAvg_\"+cur_num_data] += num_data_mean\r\n        \r\n    pd.DataFrame(pd_dict).to_csv(os.path.join(FLAGS.output,\"metrics.csv\"), mode=mode)\r\n    \r\nfor num_data in keys:\r\n    sum_dict[\"FrameAvg_\"+num_data] = pd.Series([sum_dict[\"FrameAvg_\"+num_data] / len_dict[num_data]])\r\n    folder_dict[\"FolderAvg_\"+num_data] = pd.Series([folder_dict[\"FolderAvg_\"+num_data] / folder_n])\r\n    avg_dict[\"Avg_\"+num_data] = pd.Series(np.float32(avg_dict[\"Avg_\"+num_data]))\r\n    print(\"%s, total frame %d, total avg %02.4f, folder avg %02.4f\" % \r\n        (num_data, len_dict[num_data], sum_dict[\"FrameAvg_\"+num_data][0], folder_dict[\"FolderAvg_\"+num_data][0]))\r\npd.DataFrame(avg_dict).to_csv(os.path.join(FLAGS.output,\"metrics.csv\"), mode='a')\r\npd.DataFrame(folder_dict).to_csv(os.path.join(FLAGS.output,\"metrics.csv\"), mode='a')\r\npd.DataFrame(sum_dict).to_csv(os.path.join(FLAGS.output,\"metrics.csv\"), mode='a')\r\nprint(\"Finished.\")"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1689453125,
          "content": "numpy>=1.14.3\r\nscipy>=1.0.1\r\nscikit-image>=0.13.0\r\nmatplotlib>=1.5.1\r\npandas>=0.23.1\r\nKeras>=2.1.2\r\ntorch>=0.4.0\r\ntorchvision>=0.2.1\r\nopencv-python>=2.4.11\r\nipython>=7.4.0\r\n"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "runGan.py",
          "type": "blob",
          "size": 13.2001953125,
          "content": "'''\nseveral running examples, run with\npython3 runGan.py 1 # the last number is the run case number\n\nruncase == 1    inference a trained model\nruncase == 2    calculate the metrics, and save the numbers in csv\nruncase == 3    training TecoGAN\nruncase == 4    training FRVSR\nruncase == ...  coming... data preparation and so on...\n'''\nimport os, subprocess, sys, datetime, signal, shutil\n\nruncase = int(sys.argv[1])\nprint (\"Testing test case %d\" % runcase)\n\ndef preexec(): # Don't forward signals.\n    os.setpgrp()\n    \ndef mycall(cmd, block=False):\n    if not block:\n        return subprocess.Popen(cmd)\n    else:\n        return subprocess.Popen(cmd, preexec_fn = preexec)\n    \ndef folder_check(path):\n    try_num = 1\n    oripath = path[:-1] if path.endswith('/') else path\n    while os.path.exists(path):\n        print(\"Delete existing folder \" + path + \"?(Y/N)\")\n        decision = input()\n        if decision == \"Y\":\n            shutil.rmtree(path, ignore_errors=True)\n            break\n        else:\n            path = oripath + \"_%d/\"%try_num\n            try_num += 1\n            print(path)\n    \n    return path\n\nif( runcase == 0 ): # download inference data, trained models\n    # download the trained model\n    if(not os.path.exists(\"./model/\")): os.mkdir(\"./model/\")\n    cmd1 = \"wget https://ge.in.tum.de/download/data/TecoGAN/model.zip -O model/model.zip;\"\n    cmd1 += \"unzip model/model.zip -d model; rm model/model.zip\"\n    subprocess.call(cmd1, shell=True)\n    \n    # download some test data\n    cmd2 = \"wget https://ge.in.tum.de/download/data/TecoGAN/vid3_LR.zip -O LR/vid3.zip;\"\n    cmd2 += \"unzip LR/vid3.zip -d LR; rm LR/vid3.zip\"\n    subprocess.call(cmd2, shell=True)\n    \n    cmd2 = \"wget https://ge.in.tum.de/download/data/TecoGAN/tos_LR.zip -O LR/tos.zip;\"\n    cmd2 += \"unzip LR/tos.zip -d LR; rm LR/tos.zip\"\n    subprocess.call(cmd2, shell=True)\n    \n    # download the ground-truth data\n    if(not os.path.exists(\"./HR/\")): os.mkdir(\"./HR/\")\n    cmd3 = \"wget https://ge.in.tum.de/download/data/TecoGAN/vid4_HR.zip -O HR/vid4.zip;\"\n    cmd3 += \"unzip HR/vid4.zip -d HR; rm HR/vid4.zip\"\n    subprocess.call(cmd3, shell=True)\n    \n    cmd3 = \"wget https://ge.in.tum.de/download/data/TecoGAN/tos_HR.zip -O HR/tos.zip;\"\n    cmd3 += \"unzip HR/tos.zip -d HR; rm HR/tos.zip\"\n    subprocess.call(cmd3, shell=True)\n    \nelif( runcase == 1 ): # inference a trained model\n    \n    dirstr = './results/' # the place to save the results\n    testpre = ['calendar'] # the test cases\n\n    if (not os.path.exists(dirstr)): os.mkdir(dirstr)\n    \n    # run these test cases one by one:\n    for nn in range(len(testpre)):\n        cmd1 = [\"python3\", \"main.py\",\n            \"--cudaID\", \"0\",            # set the cudaID here to use only one GPU\n            \"--output_dir\",  dirstr,    # Set the place to put the results.\n            \"--summary_dir\", os.path.join(dirstr, 'log/'), # Set the place to put the log. \n            \"--mode\",\"inference\", \n            \"--input_dir_LR\", os.path.join(\"./LR/\", testpre[nn]),   # the LR directory\n            #\"--input_dir_HR\", os.path.join(\"./HR/\", testpre[nn]),  # the HR directory\n            # one of (input_dir_HR,input_dir_LR) should be given\n            \"--output_pre\", testpre[nn], # the subfolder to save current scene, optional\n            \"--num_resblock\", \"16\",  # our model has 16 residual blocks, \n            # the pre-trained FRVSR and TecoGAN mini have 10 residual blocks\n            \"--checkpoint\", './model/TecoGAN',  # the path of the trained model,\n            \"--output_ext\", \"png\"               # png is more accurate, jpg is smaller\n        ]\n        mycall(cmd1).communicate()\n\nelif( runcase == 2 ): # calculate all metrics, and save the csv files, should use png\n\n    testpre = [\"calendar\"] # just put more scenes to evaluate all of them\n    dirstr = './results/'  # the outputs\n    tarstr = './HR/'       # the GT\n\n    tar_list = [(tarstr+_) for _ in testpre]\n    out_list = [(dirstr+_) for _ in testpre]\n    cmd1 = [\"python3\", \"metrics.py\",\n        \"--output\", dirstr+\"metric_log/\",\n        \"--results\", \",\".join(out_list),\n        \"--targets\", \",\".join(tar_list),\n    ]\n    mycall(cmd1).communicate()\n    \nelif( runcase == 3 ): # Train TecoGAN\n    '''\n    In order to use the VGG as a perceptual loss,\n    we download from TensorFlow-Slim image classification model library:\n    https://github.com/tensorflow/models/tree/master/research/slim    \n    '''\n    VGGPath = \"model/\" # the path for the VGG model, there should be a vgg_19.ckpt inside\n    VGGModelPath = os.path.join(VGGPath, \"vgg_19.ckpt\")\n    if(not os.path.exists(VGGPath)): os.mkdir(VGGPath)\n    if(not os.path.exists(VGGModelPath)):\n        # Download the VGG 19 model from \n        print(\"VGG model not found, downloading to %s\"%VGGPath)\n        cmd0 = \"wget http://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz -O \" + os.path.join(VGGPath, \"vgg19.tar.gz\")\n        cmd0 += \";tar -xvf \" + os.path.join(VGGPath,\"vgg19.tar.gz\") + \" -C \" + VGGPath + \"; rm \"+ os.path.join(VGGPath, \"vgg19.tar.gz\")\n        subprocess.call(cmd0, shell=True)\n        \n    '''\n    Use our pre-trained FRVSR model. If you want to train one, try runcase 4, and update this path by:\n    FRVSRModel = \"ex_FRVSRmm-dd-hh/model-500000\"\n    '''\n    FRVSRModel = \"model/ourFRVSR\" \n    if(not os.path.exists(FRVSRModel+\".data-00000-of-00001\")):\n        # Download our pre-trained FRVSR model\n        print(\"pre-trained FRVSR model not found, downloading\")\n        cmd0 = \"wget http://ge.in.tum.de/download/2019-TecoGAN/FRVSR_Ours.zip -O model/ofrvsr.zip;\"\n        cmd0 += \"unzip model/ofrvsr.zip -d model; rm model/ofrvsr.zip\"\n        subprocess.call(cmd0, shell=True)\n    \n    TrainingDataPath = \"/mnt/netdisk/video_data/\" \n    \n    '''Prepare Training Folder'''\n    # path appendix, manually define it, or use the current datetime, now_str = \"mm-dd-hh\"\n    now_str = datetime.datetime.now().strftime(\"%m-%d-%H\")\n    train_dir = folder_check(\"ex_TecoGAN%s/\"%now_str)\n    # train TecoGAN, loss = l2 + VGG54 loss + A spatio-temporal Discriminator\n    cmd1 = [\"python3\", \"main.py\",\n        \"--cudaID\", \"0\", # set the cudaID here to use only one GPU\n        \"--output_dir\", train_dir, # Set the place to save the models.\n        \"--summary_dir\", os.path.join(train_dir,\"log/\"), # Set the place to save the log. \n        \"--mode\",\"train\",\n        \"--batch_size\", \"4\" , # small, because GPU memory is not big\n        \"--RNN_N\", \"10\" , # train with a sequence of RNN_N frames, >6 is better, >10 is not necessary\n        \"--movingFirstFrame\", # a data augmentation\n        \"--random_crop\",\n        \"--crop_size\", \"32\",\n        \"--learning_rate\", \"0.00005\",\n        # -- learning_rate step decay, here it is not used --\n        \"--decay_step\", \"500000\", \n        \"--decay_rate\", \"1.0\", # 1.0 means no decay\n        \"--stair\",\n        \"--beta\", \"0.9\", # ADAM training parameter beta\n        \"--max_iter\", \"500000\", # 500k or more, the one we present is trained for 900k\n        \"--save_freq\", \"10000\", # the frequency we save models\n        # -- network architecture parameters --\n        \"--num_resblock\", \"16\", # FRVSR and TecoGANmini has num_resblock as 10. The TecoGAN has 16.\n        # -- VGG loss, disable with vgg_scaling < 0\n        \"--vgg_scaling\", \"0.2\",\n        \"--vgg_ckpt\", VGGModelPath, # necessary if vgg_scaling > 0\n    ]\n    '''Video Training data:\n    please udate the TrainingDataPath according to ReadMe.md\n    input_video_pre is hard coded as scene in dataPrepare.py at line 142\n    str_dir is the starting index for training data\n    end_dir is the ending index for training data\n    end_dir+1 is the starting index for validation data\n    end_dir_val is the ending index for validation data\n    max_frm should be duration (in dataPrepare.py) -1\n    queue_thread: how many cpu can be used for loading data when training\n    name_video_queue_capacity, video_queue_capacity: how much memory can be used\n    '''\n    cmd1 += [\n        \"--input_video_dir\", TrainingDataPath, \n        \"--input_video_pre\", \"scene\",\n        \"--str_dir\", \"2000\",\n        \"--end_dir\", \"2250\",\n        \"--end_dir_val\", \"2290\",\n        \"--max_frm\", \"119\",\n        # -- cpu memory for data loading --\n        \"--queue_thread\", \"12\",# Cpu threads for the data. >4 to speedup the training\n        \"--name_video_queue_capacity\", \"1024\",\n        \"--video_queue_capacity\", \"1024\",\n    ]\n    '''\n    loading the pre-trained model from FRVSR can make the training faster\n    --checkpoint, path of the model, here our pre-trained FRVSR is given\n    --pre_trained_model,  to continue an old (maybe accidentally stopeed) training, \n        pre_trained_model should be false, and checkpoint should be the last model such as \n        ex_TecoGANmm-dd-hh/model-xxxxxxx\n        To start a new and different training, pre_trained_model is True. \n        The difference here is \n        whether to load the whole graph icluding ADAM training averages/momentums/ and so on\n        or just load existing pre-trained weights.\n    '''\n    cmd1 += [ # based on a pre-trained FRVSR model. Here we want to train a new adversarial training\n        \"--pre_trained_model\", # True\n        \"--checkpoint\", FRVSRModel,\n    ]\n    \n    # the following can be used to train TecoGAN continuously\n    # old_model = \"model/ex_TecoGANmm-dd-hh/model-xxxxxxx\" \n    # cmd1 += [ # Here we want to train continuously\n    #     \"--nopre_trained_model\", # False\n    #     \"--checkpoint\", old_model,\n    # ]\n    \n    ''' parameters for GAN training '''\n    cmd1 += [\n        \"--ratio\", \"0.01\",  # the ratio for the adversarial loss from the Discriminator to the Generator\n        \"--Dt_mergeDs\",     # if Dt_mergeDs == False, only use temporal inputs, so we have a temporal Discriminator\n                            # else, use both temporal and spatial inputs, then we have a Dst, the spatial and temporal Discriminator\n    ]\n    ''' if the generator is pre-trained, to fade in the discriminator is usually more stable.\n    the weight of the adversarial loss will be weighed with a weight, started from Dt_ratio_0, \n    and increases until Dt_ratio_max, the increased value is Dt_ratio_add per training step\n    For example, fading Dst in smoothly in the first 4k steps is \n    \"--Dt_ratio_max\", \"1.0\", \"--Dt_ratio_0\", \"0.0\", \"--Dt_ratio_add\", \"0.00025\"\n    '''\n    cmd1 += [ # here, the fading in is disabled \n        \"--Dt_ratio_max\", \"1.0\",\n        \"--Dt_ratio_0\", \"1.0\", \n        \"--Dt_ratio_add\", \"0.0\", \n    ]\n    ''' Other Losses '''\n    cmd1 += [\n        \"--pingpang\",           # our Ping-Pang loss\n        \"--pp_scaling\", \"0.5\",  # the weight of the our bi-directional loss, 0.0~0.5\n        \"--D_LAYERLOSS\",        # use feature layer losses from the discriminator\n    ]\n    \n    pid = mycall(cmd1, block=True) \n    try: # catch interruption for training\n        pid.communicate()\n    except KeyboardInterrupt: # Ctrl + C to stop current training try to save the last model \n        print(\"runGAN.py: sending SIGINT signal to the sub process...\")\n        pid.send_signal(signal.SIGINT)\n        # try to save the last model \n        pid.communicate()\n        print(\"runGAN.py: finished...\")\n        \n        \nelif( runcase == 4 ): # Train FRVSR, loss = l2 warp + l2 content\n    now_str = datetime.datetime.now().strftime(\"%m-%d-%H\")\n    train_dir = folder_check(\"ex_FRVSR%s/\"%now_str)\n    cmd1 = [\"python3\", \"main.py\",\n        \"--cudaID\", \"0\", # set the cudaID here to use only one GPU\n        \"--output_dir\", train_dir, # Set the place to save the models.\n        \"--summary_dir\", os.path.join(train_dir,\"log/\"), # Set the place to save the log. \n        \"--mode\",\"train\",\n        \"--batch_size\", \"4\" , # small, because GPU memory is not big\n        \"--RNN_N\", \"10\" , # train with a sequence of RNN_N frames, >6 is better, >10 is not necessary\n        \"--movingFirstFrame\", # a data augmentation\n        \"--random_crop\",\n        \"--crop_size\", \"32\",\n        \"--learning_rate\", \"0.00005\",\n        # -- learning_rate step decay, here it is not used --\n        \"--decay_step\", \"500000\", \n        \"--decay_rate\", \"1.0\", # 1.0 means no decay\n        \"--stair\",\n        \"--beta\", \"0.9\", # ADAM training parameter beta\n        \"--max_iter\", \"500000\", # 500k is usually fine for FRVSR, GAN versions need more to be stable\n        \"--save_freq\", \"10000\", # the frequency we save models\n        # -- network architecture parameters --\n        \"--num_resblock\", \"10\", # a smaller model\n        \"--ratio\", \"-0.01\",  # the ratio for the adversarial loss, negative means disabled\n        \"--nopingpang\",\n    ]\n    '''Video Training data... Same as runcase 3...'''\n    TrainingDataPath = \"/mnt/netdisk/video_data/\"\n    cmd1 += [\r\n        \"--input_video_dir\", TrainingDataPath, \n        \"--input_video_pre\", \"scene\",\n        \"--str_dir\", \"2000\",\n        \"--end_dir\", \"2250\",\n        \"--end_dir_val\", \"2290\",\n        \"--max_frm\", \"119\",\n        # -- cpu memory for data loading --\n        \"--queue_thread\", \"12\",# Cpu threads for the data. >4 to speedup the training\n        \"--name_video_queue_capacity\", \"1024\",\n        \"--video_queue_capacity\", \"1024\",\n    ]\n    \n    pid = mycall(cmd1, block=True)\n    try: # catch interruption for training\n        pid.communicate()\n    except KeyboardInterrupt: # Ctrl + C to stop current training try to save the last model \n        print(\"runGAN.py: sending SIGINT signal to the sub process...\")\n        pid.send_signal(signal.SIGINT)\n        # try to save the last model \n        pid.communicate()\n        print(\"runGAN.py: finished...\")"
        }
      ]
    }
  ]
}