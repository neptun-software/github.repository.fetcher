{
  "metadata": {
    "timestamp": 1736560695274,
    "page": 352,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "bup/bup",
      "stars": 7171,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".cirrus.yml",
          "type": "blob",
          "size": 1.771484375,
          "content": "\ntask:\n  name: debian check/lint root\n  container:\n    image: debian:bullseye\n    cpu: 4\n    memory: 2\n  script: |\n    set -xe\n    dev/prep-for-debianish-build python3\n    dev/system-info\n    BUP_PYTHON_CONFIG=python3-config ./configure --with-pylint=yes\n    make -j6 check\n\ntask:\n  name: debian long-check\n  container:\n    image: debian:bullseye\n    cpu: 4\n    memory: 2\n  script: |\n    set -xe\n    dev/prep-for-debianish-build python3\n    DEBIAN_FRONTEND=noninteractive apt-get -y install bup\n    export BUP_TEST_OTHER_BUP=\"$(command -v bup)\"\n    \"$BUP_TEST_OTHER_BUP\" version\n    dev/system-info\n    adduser --disabled-password --gecos '' bup\n    chown -R bup:bup .\n    printf \"make -j6 -C %q BUP_PYTHON_CONFIG=python3-config long-check\" \\\n      \"$(pwd)\" | su -l -w BUP_TEST_OTHER_BUP bup\n\ntask:\n  name: debian check\n  container:\n    image: debian:buster\n    cpu: 4\n    memory: 2\n  script: |\n    set -xe\n    dev/prep-for-debianish-build python3\n    dev/system-info\n    adduser --disabled-password --gecos '' bup\n    chown -R bup:bup .\n    printf \"make -j6 -C %q BUP_PYTHON_CONFIG=python3-config check\" \\\n      \"$(pwd)\" | su -l bup\n\ntask:\n  name: freebsd check\n  freebsd_instance:\n    image: freebsd-13-2-release-amd64\n    cpu: 4\n    memory: 4\n  script: |\n    set -xe\n    dev/prep-for-freebsd-build python3\n    dev/system-info\n    gmake -j6 check\n\ntask:\n  name: macos check\n  macos_instance:\n    # https://cirrus-ci.org/guide/macOS/\n    image: ghcr.io/cirruslabs/macos-runner:sonoma\n  script: |\n    set -xe\n    dev/prep-for-macos-build python3\n    brew install bup\n    export BUP_TEST_OTHER_BUP=\"$(command -v bup)\"\n    \"$BUP_TEST_OTHER_BUP\" version\n    export PKG_CONFIG_PATH=/usr/local/opt/readline/lib/pkgconfig\n    dev/system-info\n    gmake -j6 BUP_PYTHON_CONFIG=python3-config LDFLAGS=-L/usr/local/lib check\n"
        },
        {
          "name": ".dir-locals.el",
          "type": "blob",
          "size": 0.2763671875,
          "content": "((nil . ())\n (python-mode . ((indent-tabs-mode . nil)\n                 (python-indent-offset . 4)))\n (sh-mode . ((indent-tabs-mode . nil)\n             (sh-basic-offset . 4)))\n (c-mode . ((indent-tabs-mode . nil)\n            (c-basic-offset . 4)\n            (c-file-style . \"BSD\"))))\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.8974609375,
          "content": "*.swp\n*~\n/config/config.h.tmp\n/config/finished\n/dev/bup-exec\n/dev/bup-exec.d\n/dev/bup-python\n/dev/bup-python.d\n/dev/python\n/dev/python-proposed\n/dev/python-proposed.d\n/issue/missing-objects-fig-bloom-get.svg\n/issue/missing-objects-fig-bloom-set.svg\n/issue/missing-objects-fig-bup-model-2.svg\n/issue/missing-objects-fig-bup-model.svg\n/issue/missing-objects-fig-gc-dangling.svg\n/issue/missing-objects-fig-get-bug-save.svg\n/issue/missing-objects-fig-git-model.svg\n/issue/missing-objects-fig-rm-after-gc.svg\n/issue/missing-objects-fig-rm-after.svg\n/issue/missing-objects-fig-rm-before.svg\n/issue/missing-objects.html\n/lib/bup/_helpers.d\n/lib/bup/_helpers.dll\n/lib/bup/_helpers.so\n/lib/bup/checkout_info.py\n/lib/cmd/bup\n/lib/cmd/bup.d\n/nbproject/\n/test/int/__init__.pyc\n/test/lib/__init__.pyc\n/test/lib/buptest/__init__.pyc\n/test/lib/buptest/vfs.pyc\n/test/lib/wvpytest.pyc\n/test/sampledata/var/\n/test/tmp/\n\\#*#\n__pycache__/\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 0.3330078125,
          "content": "# -*-conf-*-\n[GENERAL OPTIONS]\n\n[MESSAGES CONTROL]\ndisable=all\nenable=\n  syntax-error,\n  catching-non-exception,\n  consider-using-in,\n  inconsistent-return-statements,\n  return-in-init,\n  trailing-whitespace,\n  undefined-variable,\n  unidiomatic-typecheck,\n  unused-import,\n  unused-wildcard-import,\n  useless-return,\n  super-init-not-called\n"
        },
        {
          "name": "CODINGSTYLE",
          "type": "blob",
          "size": 0.9013671875,
          "content": ".. -*-rst-*-\n\nC\n=\n\nThe C implementations should follow the `kernel/git coding style\n<http://www.kernel.org/doc/Documentation/CodingStyle>`_.\n\n\nPython\n======\n\nPython code follows `PEP8 <http://www.python.org/dev/peps/pep-0008/>`_\nwith regard to coding style and `PEP257\n<http://www.python.org/dev/peps/pep-0257/>`_ with regard to docstring\nstyle. Multi-line docstrings should have one short summary line,\nfollowed by a blank line and a series of paragraphs. The last\nparagraph should be followed by a line that closes the docstring (no\nblank line in between). Here's an example from\n``lib/bup/helpers.py``::\n\n  def unlink(f):\n      \"\"\"Delete a file at path 'f' if it currently exists.\n\n      Unlike os.unlink(), does not throw an exception if the file didn't already\n      exist.\n      \"\"\"\n      ...\n\nModule-level docstrings follow exactly the same guidelines but without the\nblank line between the summary and the details.\n"
        },
        {
          "name": "DESIGN.md",
          "type": "blob",
          "size": 39.765625,
          "content": "\nThe Crazy Hacker's Crazy Guide to Bup Craziness\n===============================================\n\nDespite what you might have heard, bup is not that crazy, and neither are\nyou if you're trying to figure out how it works.  But it's also (as of this\nwriting) rather new and the source code doesn't have a lot of comments, so\nit can be a little confusing at first glance.  This document is designed to\nmake it easier for you to get started if you want to add a new feature, fix\na bug, or just understand how it all works.\n\n\nBup Source Code Layout\n----------------------\n\nAs you're reading this, you might want to look at different parts of the bup\nsource code to follow along and see what we're talking about.  bup's code is\nwritten primarily in python with a bit of C code in speed-sensitive places. \nHere are the most important things to know:\n\n - The main program is a fairly small C program that mostly just\n   initializes the correct Python interpreter and then runs\n   bup.main.main().  This arrangement was chosen in order to give us\n   more flexibility.  For example:\n\n     - It allows us to avoid\n       [crashing on some Unicode-unfriendly command line arguments](https://bugs.python.org/issue35883)\n       which is critical, given that paths can be arbitrary byte\n       sequences.\n\n     - It allows more flexibility in dealing with upstream changes\n       like the breakage of our ability to manipulate the\n       processes arguement list on platforms that support it during\n       the Python 3.9 series.\n\n     - It means that we'll no longer be affected by any changes to the\n       `#!/...` path, i.e. if `/usr/bin/python`, or\n       `/usr/bin/python3`, or whatever we'd previously selected during\n       `./configure` were to change from 2 to 3, or 3.5 to 3.20.\n\n   The version of python bup uses is determined by the `python-config`\n   program selected by `./configure`.  It tries to find a suitable\n   default unless `BUP_PYTHON_CONFIG` is set in the environment.\n\n - bup supports both internal and external subcommands.  The former\n   are the most common, and are all located in lib/bup/cmd/.  They\n   must be python modules named lib/bup/cmd/COMMAND.py, and must\n   contain a `main(argv)` function that will be passed the *binary*\n   command line arguments (bytes, not strings).  The filename must\n   have underscores for any dashes in the subcommand name.  The\n   external subcommands are in lib/cmd/.\n\n - The python code is all in lib/bup.\n\n - lib/bup/\\*.py contains the python code (modules) that bup depends on.\n   That directory name seems a little silly (and worse, redundant) but there\n   seemed to be no better way to let programs write \"from bup import\n   index\" and have it work.  Putting bup in the top level conflicted with\n   the 'bup' command; calling it anything other than 'bup' was fundamentally\n   wrong, and doesn't work when you install bup on your system in /usr/lib\n   somewhere.  So we get the annoyingly long paths.\n\n\nRepository Structure\n====================\n\nBefore you can talk about how bup works, we need to first address what it\ndoes.  The purpose of bup is essentially to let you \"replicate\" data between\ntwo main data structures:\n\n1. Your computer's filesystem;\n\n2. A bup repository. (Yes, we know, that part also resides in your\n   filesystem.  Stop trying to confuse yourself.  Don't worry, we'll be\n   plenty confusing enough as it is.)\n\nEssentially, copying data from the filesystem to your repository is called\n\"backing stuff up,\" which is what bup specializes in.  Normally you initiate\na backup using the 'bup save' command, but that's getting ahead of\nourselves.\n\nFor the inverse operation, ie. copying from the repository to your\nfilesystem, you have several choices; the main ones are 'bup restore', 'bup\nftp', 'bup fuse', and 'bup web'.\n\nNow, those are the basics of backups.  In other words, we just spent about\nhalf a page telling you that bup backs up and restores data.  Are we having\nfun yet?\n\nThe next thing you'll want to know is the format of the bup repository,\nbecause hacking on bup is rather impossible unless you understand that part. \nIn short, a bup repository is a git repository.  If you don't know about\ngit, you'll want to read about it now.  A really good article to read is\n\"Git for Computer Scientists\" - you can find it in Google.  Go read it now. \nWe'll wait.\n\nGot it?  Okay, so now you're an expert in blobs, trees, commits, and refs,\nthe four building blocks of a git repository.  bup uses these four things,\nand they're formatted in exactly the same way as git does it, so you can use\ngit to manipulate the bup repository if you want, and you probably won't\nbreak anything.  It's also a comfort to know you can squeeze data out using\ngit, just in case bup fails you, and as a developer, git offers some nice\ntools (like 'git rev-list' and 'git log' and 'git diff' and 'git show' and\nso on) that allow you to explore your repository and help debug when things\ngo wrong.\n\nNow, bup does use these tools a little bit differently than plain git.  We\nneed to do this in order to address two deficiencies in git when used for\nlarge backups, namely a) git bogs down and crashes if you give it really\nlarge files; b) git is too slow when you give it too many files; and c) git\ndoesn't store detailed filesystem metadata.\n\nLet's talk about each of those problems in turn.\n\n\nHandling large files (cmd/split, hashsplit.split_to_blob_or_tree)\n--------------------\n\nThe primary reason git can't handle huge files is that it runs them through\nxdelta, which generally means it tries to load the entire contents of a file\ninto memory at once.  If it didn't do this, it would have to store the\nentire contents of every single revision of every single file, even if you\nonly changed a few bytes of that file.  That would be a terribly inefficient\nuse of disk space, and git is well known for its amazingly efficient\nrepository format.\n\nUnfortunately, xdelta works great for small files and gets amazingly slow\nand memory-hungry for large files.  For git's main purpose, ie. managing\nyour source code, this isn't a problem.  But when backing up your\nfilesystem, you're going to have at least a few large files, and so it's a\nnon-starter.  bup has to do something totally different.\n\nWhat bup does instead of xdelta is what we call \"hashsplitting.\"  We wanted\na general-purpose way to efficiently back up *any* large file that might\nchange in small ways, without storing the entire file every time.  In fact,\nthe original versions of bup could only store a single file at a time;\nsurprisingly enough, this was enough to give us a large part of bup's\nfunctionality.  If you just take your entire filesystem and put it in a\ngiant tarball each day, then send that tarball to bup, bup will be able to\nefficiently store only the changes to that tarball from one day to the next. \nFor small files, bup's compression won't be as good as xdelta's, but for\nanything over a few megabytes in size, bup's compression will actually\n*work*, which is a big advantage over xdelta.\n\nHow does hashsplitting work?  It's deceptively simple.  We read through the\nfile one byte at a time, calculating a rolling checksum of the last 64\nbytes.  (Why 64?  No reason.  Literally.  We picked it out of the air.\nProbably some other number is better.  Feel free to join the mailing list\nand tell us which one and why.)  (The rolling checksum idea is actually\nstolen from rsync and xdelta, although we use it differently.  And they use\nsome kind of variable window size based on a formula we don't totally\nunderstand.)\n\nThe original rolling checksum algorithm we used was called \"stupidsum,\"\nbecause it was based on the only checksum Avery remembered how to calculate at\nthe time.  He also remembered that it was the introductory checksum\nalgorithm in a whole article about how to make good checksums that he read\nabout 15 years ago, and it was thoroughly discredited in that article for\nbeing very stupid.  But, as so often happens, Avery couldn't remember any\nbetter algorithms from the article.  So what we got is stupidsum.\n\nSince then, we have replaced the stupidsum algorithm with what we call\n\"rollsum,\" based on code in librsync.  It's essentially the same as what\nrsync does, except we use a fixed window size.\n\n(If you're a computer scientist and can demonstrate that some other rolling\nchecksum would be faster and/or better and/or have fewer screwy edge cases,\nwe need your help!  Avery's out of control!  Join our mailing list!  Please! \nSave us! ...  oh boy, I sure hope he doesn't read this)\n\nIn any case, rollsum seems to do pretty well at its job.  You can find\nit in bupsplit.c.  Basically, it converts the last 64 bytes read into\na 32-bit integer.  What we then do is take the lowest 13 bits of the\nrollsum, and if they're all 1's, we consider that to be the end of a\nchunk.  This happens on average once every 2^13 = 8192 bytes, so the\naverage chunk size is 8192 bytes.  We also cap the maximum chunk size\nat four times the average chunk size, i.e. 4 * 2^13, to avoid having\nany arbitrarily large chunks.\n\n(Why 13 bits?  Well, we picked the number at random and... eugh.  You're\ngetting the idea, right?  Join the mailing list and tell us why we're\nwrong.)\n\n(Incidentally, even though the average chunk size is 8192 bytes, the actual\nprobability distribution of block sizes ends up being non-uniform; if we\nremember our stats classes correctly, which we probably don't, it's probably\nan \"exponential distribution.\"  The idea is that for each byte in the block,\nthe probability that it's the last block is one in 8192.  Thus, the\nblock sizes end up being skewed toward the smaller end.  That's not\nnecessarily for the best, but maybe it is.  Computer science to the rescue? \nYou know the drill.)\n\nAnyway, so we're dividing up those files into chunks based on the rolling\nchecksum.  Then we store each chunk separately (indexed by its sha1sum) as a\ngit blob.  Why do we split this way?  Well, because the results are actually\nreally nice.  Let's imagine you have a big mysql database dump (produced by\nmysqldump) and it's basically 100 megs of SQL text.  Tomorrow's database\ndump adds 100 rows to the middle of the file somewhere, soo it's 100.01 megs\nof text.\n\nA naive block splitting algorithm - for example, just dividing the file into\n8192-byte blocks - would be a disaster.  After the first bit of text has\nchanged, every block after that would have a different boundary, so most of\nthe blocks in the new backup would be different from the previous ones, and\nyou'd have to store the same data all over again.  But with hashsplitting,\nno matter how much data you add, modify, or remove in the middle of the\nfile, all the chunks *before* and *after* the affected chunk are absolutely\nthe same.  All that matters to the hashsplitting algorithm is the\n\"separator\" sequence, and a single change can only affect, at most, one\nseparator sequence or the bytes between two separator sequences.  And\nbecause of rollsum, about one in 8192 possible 64-byte sequences is a\nseparator sequence.  Like magic, the hashsplit chunking algorithm will chunk\nyour file the same way every time, even without knowing how it had chunked\nit previously.\n\nThe next problem is less obvious: after you store your series of chunks as\ngit blobs, how do you store their sequence?  Each blob has a 20-byte sha1\nidentifier, which means the simple list of blobs is going to be 20/8192 =\n0.25% of the file length.  For a 200GB file, that's 488 megs of just\nsequence data.\n\nAs an overhead percentage, 0.25% basically doesn't matter.  488 megs sounds\nlike a lot, but compared to the 200GB you have to store anyway, it's\nirrelevant.  What *is* relevant is that 488 megs is a lot of memory you have\nto use in order to keep track of the list.  Worse, if you back up an\nalmost-identical file tomorrow, you'll have *another* 488 meg blob to keep\ntrack of, and it'll be almost but not quite the same as last time.\n\nHmm, big files, each one almost the same as the last... you know where this\nis going, right?\n\nActually no!  Ha!  We didn't split this list in the same way.  We could\nhave, in fact, but it wouldn't have been very \"git-like\", since we'd like to\nstore the list as a git 'tree' object in order to make sure git's\nrefcounting and reachability analysis doesn't get confused.  Never mind the\nfact that we want you to be able to 'git checkout' your data without any\nspecial tools.\n\nWhat we do instead is we extend the hashsplit algorithm a little further\nusing what we call \"fanout.\" Instead of checking just the last 13 bits of\nthe checksum, we use additional checksum bits to produce additional splits.\nNote that (most likely due to an implementation bug), the next higher bit\nafter the 13 bits (marked 'x'):\n\n  ...... '..x1'1111'1111'1111\n\nis actually ignored (so the fanout starts just to the left of the x).\nNow, let's say we use a 4-bit fanout. That means we'll break a series\nof chunks into its own tree object whenever the next 4 bits of the\nrolling checksum are 1, in addition to the 13 lowest ones.  Since the\n13 lowest bits already have to be 1, the boundary of a group of chunks\nis necessarily also always the boundary of a particular chunk.\n\nAnd so on.  Eventually you'll have too many chunk groups, but you can group\nthem into supergroups by using another 4 bits, and continue from\nthere.\n\nWhat you end up with is an actual tree of blobs - which git 'tree'\nobjects are ideal to represent.  For example, with a fanout of 4,\n\n  ...... '1011'1111'1111' 'x1'1111'1111'1111\n\nwould produce a tree two levels deep because there are 8 (two blocks\nof 4) contiguous 1 bits to the left of the ignored 14th bit.\n\nAnd if you think about it, just like the original list of chunks, the\ntree itself is pretty stable across file modifications.  Any one\nmodification will only affect the chunks actually containing the\nmodifications, thus only the groups containing those chunks, and so on\nup the tree.  Essentially, the number of changed git objects is O(log\nn) where n is the number of chunks.  Since log 200 GB, using a base of\n16 or so, is not a very big number, this is pretty awesome.  Remember,\nany git object we *don't* change in a new backup is one we can reuse\nfrom last time, so the deduplication effect is pretty awesome.\n\nBetter still, the hashsplit-tree format is good for a) random instead of\nsequential access to data (which you can see in action with 'bup fuse'); and\nb) quickly showing the differences between huge files (which we haven't\nreally implemented because we don't need it, but you can try 'git diff -M -C\n-C backup1 backup2 -- filename' for a good start).\n\nSo now we've split out 200 GB file into about 24 million pieces.  That\nbrings us to git limitation number 2.\n\n\nHandling huge numbers of files (git.PackWriter)\n------------------------------\n\ngit is designed for handling reasonably-sized repositories that change\nrelatively infrequently.  (You might think you change your source code\n\"frequently\" and that git handles much more frequent changes than, say, svn\ncan handle.  But that's not the same kind of \"frequently\" we're talking\nabout.  Imagine you're backing up all the files on your disk, and one of\nthose files is a 100 GB database file with hundreds of daily users.  Your\ndisk changes so frequently you can't even back up all the revisions even if\nyou were backing stuff up 24 hours a day.  That's \"frequently.\")\n\ngit's way of doing things works really nicely for the way software\ndevelopers write software, but it doesn't really work so well for everything\nelse.  The #1 killer is the way it adds new objects to the repository: it\ncreates one file per blob.  Then you later run 'git gc' and combine those\nfiles into a single file (using highly efficient xdelta compression, and\nignoring any files that are no longer relevant).\n\n'git gc' is slow, but for source code repositories, the resulting\nsuper-efficient storage (and associated really fast access to the stored\nfiles) is worth it.  For backups, it's not; you almost never access your\nbacked-up data, so storage time is paramount, and retrieval time is mostly\nunimportant.\n\nTo back up that 200 GB file with git and hashsplitting, you'd have to create\n24 million little 8k files, then copy them into a 200 GB packfile, then\ndelete the 24 million files again.  That would take about 400 GB of disk\nspace to run, require lots of random disk seeks, and require you to go\nthrough your data twice.\n\nSo bup doesn't do that.  It just writes packfiles directly.  Luckily, these\npackfiles are still git-formatted, so git can happily access them once\nthey're written.\n\nBut that leads us to our next problem.\n\n\nHuge numbers of huge packfiles (midx.py, bloom.py, cmd/midx, cmd/bloom)\n------------------------------\n\nGit isn't actually designed to handle super-huge repositories.  Most git\nrepositories are small enough that it's reasonable to merge them all into a\nsingle packfile, which 'git gc' usually does eventually.\n\nThe problematic part of large packfiles isn't the packfiles themselves - git\nis designed to expect the total size of all packs to be larger than\navailable memory, and once it can handle that, it can handle virtually any\namount of data about equally efficiently.  The problem is the packfile\nindexes (.idx) files.  In bup we call these idx (pronounced \"idix\") files\ninstead of using the word \"index,\" because the word index is already used\nfor something totally different in git (and thus bup) and we'll become\nhopelessly confused otherwise.\n\nAnyway, each packfile (*.pack) in git has an associated idx (*.idx) that's a\nsorted list of git object hashes and file offsets.  If you're looking for a\nparticular object based on its sha1, you open the idx, binary search it to\nfind the right hash, then take the associated file offset, seek to that\noffset in the packfile, and read the object contents.\n\nThe performance of the binary search is about O(log n) with the number of\nhashes in the pack, with an optimized first step (you can read about it\nelsewhere) that somewhat improves it to O(log(n)-7).\n\nUnfortunately, this breaks down a bit when you have *lots* of packs.  Say\nyou have 24 million objects (containing around 200 GB of data) spread across\n200 packfiles of 1GB each.  To look for an object requires you search\nthrough about 122000 objects per pack; ceil(log2(122000)-7) = 10, so you'll\nhave to search 10 times.  About 7 of those searches will be confined to a\nsingle 4k memory page, so you'll probably have to page in about 3-4 pages\nper file, times 200 files, which makes 600-800 4k pages (2.4-3.6 megs)...\nevery single time you want to look for an object.\n\nThis brings us to another difference between git's and bup's normal use\ncase.  With git, there's a simple optimization possible here: when looking\nfor an object, always search the packfiles in MRU (most recently used)\norder.  Related objects are usually clusted together in a single pack, so\nyou'll usually end up searching around 3 pages instead of 600, which is a\ntremendous improvement.  (And since you'll quickly end up swapping in all\nthe pages in a particular idx file this way, it isn't long before searching\nfor a nearby object doesn't involve any swapping at all.)\n\nbup isn't so lucky.  git users spend most of their time examining existing\nobjects (looking at logs, generating diffs, checking out branches), which\nlends itself to the above optimization.  bup, on the other hand, spends most\nof its time looking for *nonexistent* objects in the repository so that it\ncan back them up.  When you're looking for objects that aren't in the\nrepository, there's no good way to optimize; you have to exhaustively check\nall the packs, one by one, to ensure that none of them contain the data you\nwant.\n\nTo improve performance of this sort of operation, bup introduces midx\n(pronounced \"midix\" and short for \"multi-idx\") files.  As the name implies,\nthey index multiple packs at a time.\n\nImagine you had a midx file for your 200 packs.  midx files are a lot like\nidx files; they have a lookup table at the beginning that narrows down the\ninitial search, followed by a binary search.  Then unlike idx files (which\nhave a fixed-size 256-entry lookup table) midx tables have a variably-sized\ntable that makes sure the entire binary search can be contained to a single\npage of the midx file.  Basically, the lookup table tells you which page to\nload, and then you binary search inside that page.  A typical search thus\nonly requires the kernel to swap in two pages, which is better than results\nwith even a single large idx file.  And if you have lots of RAM, eventually\nthe midx lookup table (at least) will end up cached in memory, so only a\nsingle page should be needed for each lookup.\n\nYou generate midx files with 'bup midx'.  The downside of midx files is that\ngenerating one takes a while, and you have to regenerate it every time you\nadd a few packs.\n\nUPDATE: Brandon Low contributed an implementation of \"bloom filters\", which\nhave even better characteristics than midx for certain uses.  Look it up in\nWikipedia.  He also massively sped up both midx and bloom by rewriting the\nkey parts in C.  The nicest thing about bloom filters is we can update them\nincrementally every time we get a new idx, without regenerating from\nscratch.  That makes the update phase much faster, and means we can also get\naway with generating midxes less often.\n\nmidx files are a bup-specific optimization and git doesn't know what to do\nwith them.  However, since they're stored as separate files, they don't\ninterfere with git's ability to read the repository.\n\n\nHandling large directories (tree splitting)\n-------------------------------------------\n\nSimilarly to large files, git doesn't handle frequently changing large\ndirectories well either, since they're stored in a single tree object\nusing up 28 bytes plus the length of the filename for each entry\n(record). If there are a lot of files, these tree objects can become\nvery large, and every minor change to a directory such as creating a\nsingle new file in it requires storing an entirely new tree object.\nImagine an active Maildir containing tens or hundreds of thousands of\nfiles.\n\nIf tree splitting is permitted (via the `bup.split-trees` config\noption), then instead of writing a tree as one potentially large git\ntree object, bup may split it into a subtree whose \"leaves\" are git\ntree objects, each containing part of the original tree.  Note that\nfor `bup on HOST ...`  `split-trees` must be set on the `HOST`.\n\nThe trees are split in a manner similar to the file hashsplitting\ndescribed above, but with the restriction that splits may only occur\nbetween (not within) tree entries.\n\nWhen a tree is split, a large directory like\n\n\tdir/{.bupm,aa,bb,...,zz}\n\nmight become\n\n\tdir/.b/{.bupm,aa,...,ii}\n\tdir/.bupd.1.bupd\n\tdir/j/{jj,...,rr}\n\tdir/s/{ss,...,zz}\n\nwhere the \".bupd.1.bupd\" inside dir/ indicates that the tree for dir/\nwas split, and the number (here \"1\") describes the number of levels\nthat were created (just one in this case).  The names in an\nintermediate level (inside dir/, but not the leaves -- in this\nexample, \".b\", \"j\", \"s\", etc.) are derived from the first filename\ncontained within each subtree, abbreviated to the shortest valid\nunique prefix.  At any level, the names contained in a subtree will\nalways be greater than or equal to the name of the subtree itself and\nless than the name of the next subtree at that level.  This makes it\npossible to know which split subtree to read at every level when\nlooking for a given filename.\n\nWhen parsing the split tree depth info file name, i.e. `.bupd.1.bupd`\nin the example above, any extra bytes in the name after the\n`.bupd.DEPTH` prefix, and before the final `.bupd` suffix must be\nignored.  The `DEPTH` will always be a sequence of `[0-9]+`, and any\nextra bytes will begin with `.`, e.g.  `.bupd.3.SOMETHING.NEW.bupd`.\nThis allows future extensions.\n\nDetailed Metadata\n-----------------\n\nSo that's the basic structure of a bup repository, which is also a git\nrepository.  There's just one more thing we have to deal with:\nfilesystem metadata.  Git repositories are really only intended to\nstore file contents with a small bit of extra information, like\nsymlink targets and executable bits, so we have to store the rest\nsome other way.\n\nBup stores more complete metadata in the VFS in a file named .bupm in\neach tree.  This file contains one entry for each file in the tree\nobject, sorted in the same order as the tree.  The first .bupm entry\nis for the directory itself, i.e. \".\", and its name is the empty\nstring, \"\".\n\nEach .bupm entry contains a variable length sequence of records\ncontaining the metadata for the corresponding path.  Each record\nrecords one type of metadata.  Current types include a common record\ntype (containing the normal stat information), a symlink target type,\na hardlink target type, a POSIX1e ACL type, etc.  See metadata.py for\nthe complete list.\n\nThe .bupm file is optional, and when it's missing, bup will behave as\nit did before the addition of metadata, and restore files using the\ntree information.\n\nThe nice thing about this design is that you can walk through each\nfile in a tree just by opening the tree and the .bupm contents, and\niterating through both at the same time.\n\nSince the contents of any .bupm file should match the state of the\nfilesystem when it was *indexed*, bup must record the detailed\nmetadata in the index.  To do this, bup records four values in the\nindex, the atime, mtime, and ctime (as timespecs), and an integer\noffset into a secondary \"metadata store\" which has the same name as\nthe index, but with \".meta\" appended.  This secondary store contains\nthe encoded Metadata object corresponding to each path in the index.\n\nCurrently, in order to decrease the storage required for the metadata\nstore, bup only writes unique values there, reusing offsets when\nappropriate across the index.  The effectiveness of this approach\nrelies on the expectation that there will be many duplicate metadata\nrecords.  Storing the full timestamps in the index is intended to make\nthat more likely, because it makes it unnecessary to record those\nvalues in the secondary store.  So bup clears them before encoding the\nMetadata objects destined for the index, and timestamp differences\ndon't contribute to the uniqueness of the metadata.\n\nBup supports recording and restoring hardlinks, and it does so by\ntracking sets of paths that correspond to the same dev/inode pair when\nindexing.  This information is stored in an optional file with the\nsame name as the index, but ending with \".hlink\".\n\nIf there are multiple index runs, and the hardlinks change, bup will\nnotice this (within whatever subtree it is asked to reindex) and\nupdate the .hlink information accordingly.\n\nThe current hardlink implementation will refuse to link to any file\nthat resides outside the restore tree, and if the restore tree spans a\ndifferent set of filesystems than the save tree, complete sets of\nhardlinks may not be restored.\n\n\nFilesystem Interaction\n======================\n\nStoring data is just half of the problem of making a backup; figuring out\nwhat to store is the other half.\n\nAt the most basic level, piping the output of 'tar' into 'bup split' is an\neasy way to offload that decision; just let tar do all the hard stuff.  And\nif you like tar files, that's a perfectly acceptable way to do it.  But we\ncan do better.\n\nBacking up with tarballs would totally be the way to go, except for two\nserious problems:\n\n1. The result isn't easily \"seekable.\"  Tar files have no index, so if (as\n   commonly happens) you only want to restore one file in a 200 GB backup,\n   you'll have to read up to 200 GB before you can get to the beginning of\n   that file.  tar is short for \"tape archive\"; on a tape, there was no\n   better way to do it anyway, so they didn't try.  But on a disk, random\n   file access is much, much better when you can figure out how.\n   \n2. tar doesn't remember which files it backed up last time, so it has to\n   read through the entire file contents again in order to generate the\n   tarball, large parts of which will then be skipped by bup since they've\n   already been stored.  This is much slower than necessary.\n\n(The second point isn't entirely true for all versions of tar. For example,\nGNU tar has an \"incremental\" mode that can somewhat mitigate this problem,\nif you're smart enough to know how to use it without hurting yourself.  But\nyou still have to decide which backups are \"incremental\" and which ones will\nbe \"full\" and so on, so even when it works, it's more error-prone than bup.)\n\nbup divides the backup process into two major steps: a) indexing the\nfilesystem, and b) saving file contents into the repository.  Let's look at\nthose steps in detail.\n\n\nIndexing the filesystem (cmd/drecurse, cmd/index, index.py)\n-----------------------\n\nSplitting the filesystem indexing phase into its own program is\nnontraditional, but it gives us several advantages.\n\nThe first advantage is trivial, but might be the most important: you can\nindex files a lot faster than you can back them up.  That means we can\ngenerate the index (.bup/bupindex) first, then have a nice, reliable,\nnon-lying completion bar that tells you how much of your filesystem remains\nto be backed up.  The alternative would be annoying failures like counting\nthe number of *files* remaining (as rsync does), even though one of the\nfiles is a virtual machine image of 80 GB, and the 1000 other files are each\nunder 10k.  With bup, the percentage complete is the *real* percentage\ncomplete, which is very pleasant.\n\nSecondly, it makes it easier to debug and test; you can play with the index\nwithout actually backing up any files.\n\nThirdly, you can replace the 'bup index' command with something else and not\nhave to change anything about the 'bup save' command.  The current 'bup\nindex' implementation just blindly walks the whole filesystem looking for\nfiles that have changed since the last time it was indexed; this works fine,\nbut something using inotify instead would be orders of magnitude faster. \nWindows and MacOS both have inotify-like services too, but they're totally\ndifferent; if we want to support them, we can simply write new bup commands\nthat do the job, and they'll never interfere with each other.\n\nAnd fourthly, git does it that way, and git is awesome, so who are we to\nargue?\n\nSo let's look at how the index file works.\n\nFirst of all, note that the \".bup/bupindex\" file is not the same as git's\n\".git/index\" file.  The latter isn't used in bup; as far as git is\nconcerned, your bup repository is a \"bare\" git repository and doesn't have a\nworking tree, and thus it doesn't have an index either.\n\nHowever, the bupindex file actually serves exactly the same purpose as git's\nindex file, which is why we still call it \"the index.\" We just had to\nredesign it for the usual bup-vs-git reasons, mostly that git just isn't\ndesigned to handle millions of files in a single repository.  (The only way\nto find a file in git's index is to search it linearly; that's very fast in\ngit-sized repositories, but very slow in bup-sized ones.)\n\nLet's not worry about the exact format of the bupindex file; it's still not\noptimal, and will probably change again.  The most important things to know\nabout bupindex are:\n\n - You can iterate through it much faster than you can iterate through the\n   \"real\" filesystem (using something like the 'find' command).\n   \n - If you delete it, you can get it back just by reindexing your filesystem\n   (although that can be annoying to wait for); it's not critical to the\n   repository itself.\n   \n - You can iterate through only particular subtrees if you want.\n \n - There is no need to have more than one index for a particular filesystem,\n   since it doesn't store anything about backups; it just stores file\n   metadata.  It's really just a cache (or 'index') of your filesystem's\n   existing metadata.  You could share the bupindex between repositories, or\n   between multiple users on the same computer.  If you back up your\n   filesystem to multiple remote repositories to be extra safe, you can\n   still use the same bupindex file across all of them, because it's the\n   same filesystem every time.\n   \n - Filenames in the bupindex are absolute paths, because that's the best way\n   to ensure that you only need one bupindex file and that they're\n   interchangeable.\n   \n\nA note on file \"dirtiness\"\n--------------------------\n\nThe concept on which 'bup save' operates is simple enough; it reads through\nthe index and backs up any file that is \"dirty,\" that is, doesn't already\nexist in the repository.\n\nDetermination of dirtiness is a little more complicated than it sounds.  The\nmost dirtiness-relevant flag in the bupindex is IX_HASHVALID; if\nthis flag is reset, the file *definitely* is dirty and needs to be backed\nup.  But a file may be dirty even if IX_HASHVALID is set, and that's the\nconfusing part.\n\nThe index stores a listing of files, their attributes, and\ntheir git object ids (sha1 hashes), if known.  The \"if known\" is what\nIX_HASHVALID is about.  When 'bup save' backs up a file, it sets\nthe sha1 and sets IX_HASHVALID; when 'bup index' sees that a file has\nchanged, it leaves the sha1 alone and resets IX_HASHVALID.\n\nRemember that the index can be shared between users, repositories, and\nbackups.  So IX_HASHVALID doesn't mean your repository *has* that sha1 in\nit; it only means that if you *do* have it, that you don't need to back up\nthe file.  Thus, 'bup save' needs to check every file in the index to make\nsure its hash exists, not just that it's valid.\n\nThere's an optimization possible, however: if you know a particular tree's\nhash is valid and exists (say /usr), then you don't need to check the\nvalidity of all its children; because of the way git trees and blobs work,\nif your repository is valid and you have a tree object, then you have all\nthe blobs it points to.  You won't back up a tree object without backing up\nits blobs first, so you don't need to double check it next time.  (If you\nreally want to double check this, it belongs in a tool like 'bup fsck' or\n'git fsck'.) So in short, 'bup save' on a \"clean\" index (all files are\nmarked IX_HASHVALID) can be very fast; we just check our repository and see\nif the top level IX_HASHVALID sha1 exists.  If it does, then we're done.\n\nSimilarly, if not the entire index is valid, you can still avoid recursing\ninto subtrees if those particular subtrees are IX_HASHVALID and their sha1s\nare in the repository.  The net result is that, as long as you never lose\nyour index, 'bup save' can always run very fast.\n\nAnother interesting trick is that you can skip backing up files even if\nIX_HASHVALID *isn't* set, as long as you have that file's sha1 in the\nrepository.  What that means is you've chosen not to backup the latest\nversion of that file; instead, your new backup set just contains the\nmost-recently-known valid version of that file.  This is a good trick if you\nwant to do frequent backups of smallish files and infrequent backups of\nlarge ones.  Each of your backups will be \"complete,\" in that they contain\nall the small files and the large ones, but intermediate ones will just\ncontain out-of-date copies of the large files. Note that this isn't done\nright now, and 'bup save --smaller' doesn't store bigger files _at all_.\n\nA final game we can play with the bupindex involves restoring: when you\nrestore a directory from a previous backup, you can update the bupindex\nright away.  Then, if you want to restore a different backup on top, you can\ncompare the files in the index against the ones in the backup set, and\nupdate only the ones that have changed.  (Even more interesting things\nhappen if people are using the files on the restored system and you haven't\nupdated the index yet; the net result would be an automated merge of all\nnon-conflicting files.) This would be a poor man's distributed filesystem. \nThe only catch is that nobody has written this feature for 'bup restore'\nyet.  Someday!\n\n\nHow 'bup save' works (cmd/save)\n--------------------\n\nThis section is too boring and has been omitted.  Once you understand the\nindex, there's nothing special about bup save.\n\n\nRetrieving backups: the bup vfs layer (vfs.py, cmd/ls, cmd/ftp, cmd/fuse)\n=====================================\n\nOne of the neat things about bup's storage format, at least compared to most\nbackup tools, is it's easy to read a particular file, or even part of a\nfile.  That means a read-only virtual filesystem is easy to generate and\nit'll have good performance characteristics.  Because of git's commit\nstructure, you could even use branching and merging to make a transactional\nread-write filesystem... but that's probably getting a little out of bup's\nscope.  Who knows what the future might bring, though?\n\nRead-only filesystems are well within our reach today, however.  The 'bup\nls', 'bup ftp', and 'bup fuse' commands all use a \"VFS\" (virtual filesystem)\nlayer to let you access your repositories.  Feel free to explore the source\ncode for these tools and vfs.py - they're pretty straightforward.  Some\nthings to note:\n\n - None of these use the bupindex for anything.\n \n - For user-friendliness, they present your refs/commits/trees as a single\n   hierarchy (ie.  a filesystem), which isn't really how git repositories\n   are formatted.  So don't get confused!\n\n\nHandling Python 3's insistence on strings\n=========================================\n\nIn Python 2 strings were bytes, and bup used them for all kinds of\ndata.  Python 3 made a pervasive backward-incompatible change to treat\nall strings as Unicode, i.e. in Python 2 'foo' and b'foo' were the\nsame thing, while u'foo' was a Unicode string.  In Python 3 'foo'\nbecame synonymous with u'foo', completely changing the type and\npotential content, depending on the locale.\n\nIn addition, and particularly bad for bup, Python 3 also (initially)\ninsisted that all kinds of things were strings that just aren't (at\nleast not on many platforms), i.e. user names, groups, filesystem\npaths, etc.  There's no guarantee that any of those are always\nrepresentable in Unicode.\n\nOver the years, Python 3 has gradually backed away from that initial\nposition, adding alternate interfaces like os.environb or allowing\nbytes arguments to many functions like open(b'foo'...), so that in\nthose cases it's at least possible to accurately retrieve the system\ndata.\n\nAfter a while, they devised the concept of\n[bytesmuggling](https://www.python.org/dev/peps/pep-0383/) as a more\ncomprehensive solution.  In theory, this might be sufficient, but our\ninitial randomized testing discovered that some binary arguments would\ncrash Python during startup[1].  Eventually Johannes Berg tracked down\nthe [cause](https://sourceware.org/bugzilla/show_bug.cgi?id=26034),\nand we hope that the problem will be fixed eventually in glibc or\nworked around by Python, but in either case, it will be a long time\nbefore any fix is widely available.\n\nBefore we tracked down that bug we were pursuing an approach that\nwould let us side step the issue entirely by manipulating the\nLC_CTYPE, but that approach was somewhat complicated, and once we\nunderstood what was causing the crashes, we decided to just let Python\n3 operate \"normally\", and work around the issues.\n\nConsequently, we've had to wrap a number of things ourselves that\nincorrectly return Unicode strings (libacl, libreadline, hostname,\netc.)  and we've had to come up with a way to avoid the fatal crashes\ncaused by some command line arguments (sys.argv) described above.  To\nfix the latter, for the time being, we just use a trivial sh wrapper\nto redirect all of the command line arguments through the environment\nin BUP_ARGV_{0,1,2,...} variables, since the variables are unaffected,\nand we can access them directly in Python 3 via environb.\n\n[1] Our randomized argv testing found that the byte smuggling approach\n    was not working correctly for some values (initially discovered in\n    Python 3.7, and observed in other versions).  The interpreter\n    would just crash while starting up like this:\n\n    Fatal Python error: _PyMainInterpreterConfig_Read: memory allocation failed\n    ValueError: character U+134bd2 is not in range [U+0000; U+10ffff]\n\n    Current thread 0x00007f2f0e1d8740 (most recent call first):\n    Traceback (most recent call last):\n      File \"t/test-argv\", line 28, in <module>\n        out = check_output(cmd)\n      File \"/usr/lib/python3.7/subprocess.py\", line 395, in check_output\n        **kwargs).stdout\n      File \"/usr/lib/python3.7/subprocess.py\", line 487, in run\n        output=stdout, stderr=stderr)\n\nWe hope you'll enjoy bup.  Looking forward to your patches!\n\n-- apenwarr and the rest of the bup team\n\n<!--\nLocal Variables:\nmode: markdown\nEnd:\n-->\n"
        },
        {
          "name": "Documentation",
          "type": "tree",
          "content": null
        },
        {
          "name": "GNUmakefile",
          "type": "blob",
          "size": 11.533203125,
          "content": "\n# Technically we need 4.1.90 for .SHELLSTATUS, but everything relevant\n# seems to have at least 4.2 (mostly 4.4 and higher) anyway, so let's\n# just require 4.2+ to simplify, and if we ever raise the limit to\n# 4.4+, we'll have intcmp.\n\n# Assumes make versions always have at least two components.\nmake_maj := $(word 1,$(subst ., ,$(MAKE_VERSION)))\nifneq (,$(filter $(make_maj),0 1 2 3))\n  $(error $(MAKE) version $(MAKE_VERSION) is < 4.2)\nendif\nmake_min := $(word 2,$(subst ., ,$(MAKE_VERSION)))\nifeq (4,$(make_maj))\n  ifneq (,$(filter $(make_min),0 1))\n    $(error $(MAKE) version $(MAKE_VERSION) is < 4.2)\n  endif\nendif\n\n$(shell mkdir -p config/config.var && echo \"$(MAKE)\" > config/config.var/make)\nifneq (0, $(.SHELLSTATUS))\n  $(error Unable to record config/config.var/make)\nendif\n\nMAKEFLAGS += --warn-undefined-variables\n\nSHELL := bash\n.DEFAULT_GOAL := all\n\n# So where possible we can make tests more reproducible\nexport BUP_TEST_RANDOM_SEED ?= $(shell echo \"$$RANDOM\")\n\n# Guard against accidentally using/testing a local bup\nexport PATH := $(CURDIR)/dev/shadow-bin:$(PATH)\n\nclean_paths :=\ngenerated_dependencies :=\n\n# See config/config.vars.in (sets bup_python_config, among other things)\ninclude config/config.vars\n-include $(generated_dependencies)\n\npf := set -o pipefail\n\ndefine isok\n  && echo \" ok\" || echo \" no\"\nendef\n\n# If ok, strip trailing \" ok\" and return the output, otherwise, error\ndefine shout\n$(if $(subst ok,,$(lastword $(1))),$(error $(2)),$(shell x=\"$(1)\"; echo $${x%???}))\nendef\n\nsampledata_rev := $(shell dev/configure-sampledata --revision $(isok))\nsampledata_rev := \\\n  $(call shout,$(sampledata_rev),Could not parse sampledata revision)\n\ncurrent_sampledata := test/sampledata/var/rev/v$(sampledata_rev)\n\nos := $(shell ($(pf); uname | sed 's/[-_].*//') $(isok))\nos := $(call shout,$(os),Unable to determine OS)\n\n# CFLAGS CPPFLAGS LDFLAGS are handled vis config/config.vars.in\n\n# Satisfy --warn-undefined-variables\nDESTDIR ?=\nTARGET_ARCH ?=\n\nbup_shared_cflags := -O2 -Wall -Werror -Wformat=2 -MMD -MP\nbup_shared_cflags := -Wno-unknown-pragmas -Wsign-compare $(bup_shared_cflags)\nbup_shared_cflags := -D_FILE_OFFSET_BITS=64 $(bup_shared_cflags)\nbup_shared_cflags := $(bup_config_cflags) $(bup_shared_cflags)\n\nbup_shared_ldflags :=\n\nsoext := .so\nifeq ($(os),CYGWIN)\n  soext := .dll\nendif\n\nifdef TMPDIR\n  test_tmp := $(TMPDIR)\nelse\n  test_tmp := $(CURDIR)/test/tmp\nendif\n\ninitial_setup := $(shell dev/update-checkout-info lib/bup/checkout_info.py $(isok))\ninitial_setup := $(call shout,$(initial_setup),update-checkout-info failed))\nclean_paths += lib/bup/checkout_info.py\n\n# Dependency changes here should be mirrored in Makefile\nconfig/config.vars: configure config/configure config/configure.inc config/*.in\n\tMAKE=\"$(MAKE)\" ./configure\n\n# On some platforms, Python.h and readline.h fight over the\n# _XOPEN_SOURCE version, i.e. -Werror crashes on a mismatch, so for\n# now, we're just going to let Python's version win.\n\nhelpers_cflags = $(bup_python_cflags) $(bup_shared_cflags) -I$(CURDIR)/src\nhelpers_ldflags := $(bup_python_ldflags) $(bup_shared_ldflags)\n\nifneq ($(strip $(bup_readline_cflags)),)\n  readline_cflags += $(bup_readline_cflags)\n  readline_xopen := $(filter -D_XOPEN_SOURCE=%,$(readline_cflags))\n  readline_xopen := $(subst -D_XOPEN_SOURCE=,,$(readline_xopen))\n  readline_cflags := $(filter-out -D_XOPEN_SOURCE=%,$(readline_cflags))\n  readline_cflags += $(addprefix -DBUP_RL_EXPECTED_XOPEN_SOURCE=,$(readline_xopen))\n  helpers_cflags += $(readline_cflags)\nendif\n\nhelpers_ldflags += $(bup_readline_ldflags)\n\nifeq ($(bup_have_libacl),1)\n  helpers_cflags += $(bup_libacl_cflags)\n  helpers_ldflags += $(bup_libacl_ldflags)\nendif\n\nbup_ext_cmds := lib/cmd/bup-import-rdiff-backup lib/cmd/bup-import-rsnapshot\n\nbup_deps := lib/bup/_helpers$(soext) lib/cmd/bup\n\nincomplete_saves_svg := \\\n  issue/missing-objects-fig-bloom-get.svg \\\n  issue/missing-objects-fig-bloom-set.svg \\\n  issue/missing-objects-fig-bup-model-2.svg \\\n  issue/missing-objects-fig-bup-model.svg \\\n  issue/missing-objects-fig-gc-dangling.svg \\\n  issue/missing-objects-fig-get-bug-save.svg \\\n  issue/missing-objects-fig-git-model.svg \\\n  issue/missing-objects-fig-rm-after-gc.svg \\\n  issue/missing-objects-fig-rm-after.svg \\\n  issue/missing-objects-fig-rm-before.svg\nclean_paths += $(incomplete_saves_svg)\n\nissue/missing-objects.html: $(incomplete_saves_svg)\n\nissue/%.svg: issue/%.dot\n\t$(DOT) -Tsvg $< > $@\n\nissue/%.html: issue/%.md\n\t$(PANDOC) -s --embed-resources --resource-path issue \\\n\t  -r markdown -w html -o $@ $<\n\nissues :=\nman_md :=\n\nDOT ?= $(shell type -p dot)\nPANDOC ?= $(shell type -p pandoc)\n\nifeq (,$(PANDOC))\n  $(info Warning: pandoc not found; skipping generation of related documents)\nelse\n  man_md := $(wildcard Documentation/*.md)\n  ifeq (,$(findstring --embed-resources,$(shell $(PANDOC) --help)))\n    $(info Warning: no pandoc --embed-resources; skipping generation of related documents)\n  else\n    ifeq (,$(DOT))\n      $(info Warning: graphviz dot not found; skipping generation of related documents)\n    else\n      issues += issue/missing-objects.html\n    endif\n  endif\nendif\n\n\nall: dev/bup-exec dev/bup-python dev/python $(bup_deps) Documentation/all \\\n  $(issues) $(current_sampledata)\n\n$(current_sampledata):\n\tdev/configure-sampledata --setup\n\nman_roff := $(man_md:.md=)\nman_html := $(man_md:.md=.html)\n\nINSTALL=install\nPREFIX=/usr/local\nMANDIR=$(PREFIX)/share/man\nDOCDIR=$(PREFIX)/share/doc/bup\nBINDIR=$(PREFIX)/bin\nLIBDIR=$(PREFIX)/lib/bup\n\ndest_mandir := $(DESTDIR)$(MANDIR)\ndest_docdir := $(DESTDIR)$(DOCDIR)\ndest_bindir := $(DESTDIR)$(BINDIR)\ndest_libdir := $(DESTDIR)$(LIBDIR)\n\ninstall: all\n\t$(INSTALL) -d $(dest_bindir) $(dest_libdir)/bup/cmd $(dest_libdir)/cmd \\\n\t  $(dest_libdir)/web/static\n\tfor f in $(man_roff); do \\\n\t    sec=\"$${f##*.}\"; \\\n\t    $(INSTALL) -d $(dest_mandir)/man\"$$sec\"; \\\n\t    $(INSTALL) -m 0644 \"$$f\" $(dest_mandir)/man\"$$sec\"; \\\n\tdone\n\ttest -z \"$(man_html)\" || install -d $(dest_docdir)\n\ttest -z \"$(man_html)\" || $(INSTALL) -m 0644 $(man_html) $(dest_docdir)\n\t$(INSTALL) -pm 0755 lib/cmd/bup \"$(dest_libdir)/cmd/bup\"\n\t$(INSTALL) -pm 0755 $(bup_ext_cmds) \"$(dest_libdir)/cmd/\"\n\tcd \"$(dest_bindir)\" && \\\n\t  ln -sf \"$$($(CURDIR)/dev/python -c 'import os; print(os.path.relpath(\"$(abspath $(dest_libdir))/cmd/bup\"))')\" \\\n\t    .\n\tset -e; \\\n\t$(INSTALL) -pm 0644 lib/bup/*.py $(dest_libdir)/bup/\n\t$(INSTALL) -pm 0644 lib/bup/cmd/*.py $(dest_libdir)/bup/cmd/\n\t$(INSTALL) -pm 0755 \\\n\t\tlib/bup/*$(soext) \\\n\t\t$(dest_libdir)/bup\n\t$(INSTALL) -pm 0644 \\\n\t\tlib/web/static/* \\\n\t\t$(dest_libdir)/web/static/\n\t$(INSTALL) -pm 0644 \\\n\t\tlib/web/*.html \\\n\t\t$(dest_libdir)/web/\n\tif test -e lib/bup/checkout_info.py; then \\\n\t    $(INSTALL) -pm 0644 lib/bup/checkout_info.py \\\n\t        $(dest_libdir)/bup/source_info.py; \\\n\telse \\\n\t    ! grep -qF '$$Format' lib/bup/source_info.py; \\\n\t    $(INSTALL) -pm 0644 lib/bup/source_info.py $(dest_libdir)/bup/; \\\n\tfi\n\nembed_cflags = $(bup_python_cflags_embed) $(bup_shared_cflags) -I$(CURDIR)/src\nembed_ldflags := $(bup_python_ldflags_embed) $(bup_shared_ldflags)\n\nconfig/config.h: config/config.vars\nclean_paths += config/config.h.tmp\n\ncc_bin = $(CC) $(embed_cflags) -I src $(CPPFLAGS) $(CFLAGS) $^ \\\n  $(embed_ldflags) $(LDFLAGS) -fPIE -o $@\n\nclean_paths += dev/python-proposed\ngenerated_dependencies += dev/python-proposed.d\ndev/python-proposed: dev/python.c src/bup/compat.c src/bup/io.c\n\trm -f dev/python\n\t$(cc_bin)\n\nclean_paths += dev/python\ndev/python: dev/python-proposed\n\tdev/validate-python $@-proposed\n\tcp -R -p $@-proposed $@\n\nclean_paths += dev/bup-exec\ngenerated_dependencies += dev/bup-exec.d\ndev/bup-exec: bup_shared_cflags += -D BUP_DEV_BUP_EXEC=1\ndev/bup-exec: lib/cmd/bup.c src/bup/compat.c src/bup/io.c\n\t$(cc_bin)\n\nclean_paths += dev/bup-python\ngenerated_dependencies += dev/bup-python.d\ndev/bup-python: bup_shared_cflags += -D BUP_DEV_BUP_PYTHON=1\ndev/bup-python: lib/cmd/bup.c src/bup/compat.c src/bup/io.c\n\t$(cc_bin)\n\nclean_paths += lib/cmd/bup\ngenerated_dependencies += lib/cmd/bup.d\nlib/cmd/bup: lib/cmd/bup.c src/bup/compat.c src/bup/io.c\n\t$(cc_bin)\n\nclean_paths += lib/bup/_helpers$(soext)\ngenerated_dependencies += lib/bup/_helpers.d\nlib/bup/_helpers$(soext): lib/bup/_helpers.c src/bup/pyutil.c lib/bup/bupsplit.c lib/bup/_hashsplit.c\n\t$(CC) $(helpers_cflags) $(CPPFLAGS) $(CFLAGS) $^ \\\n\t  $(helpers_ldflags) $(LDFLAGS) -o $@\n\ntest/tmp:\n\tmkdir test/tmp\n\n# MAKEFLAGS must not be in an immediate := assignment\nparallel_opt = $(lastword $(filter -j%,$(MAKEFLAGS)))\nget_parallel_n = $(patsubst -j%,%,$(parallel_opt))\nmaybe_specific_n = $(if $(filter -j%,$(parallel_opt)),-n$(get_parallel_n))\nxdist_opt = $(if $(filter -j,$(parallel_opt)),-nauto,$(maybe_specific_n))\n\n.PHONY: lint-lib\nlint-lib: dev/bup-exec dev/bup-python\n\t./pylint lib\n\n# unused-wildcard-import: we always \"import * from wvpytest\"\n.PHONY: lint-test\nlint-test: dev/bup-exec dev/bup-python\n\t./pylint -d unused-wildcard-import test/lib test/int\n\n.PHONY: lint\nlint: lint-lib lint-test\n\ntest: all test/tmp dev/python lint\n\t! bup version  # Ensure we can't test the local bup (cf. dev/shadow-bin)\n\t./bup features\n\t./pytest $(xdist_opt)\n\nstupid:\n\tPATH=/bin:/usr/bin $(MAKE) test\n\ncheck: test\n\ndistcheck: all\n\tif test yes = $$(dev/python -c \"import xdist; print('yes')\" 2>/dev/null); then \\\n\t  (set -x; ./pytest $(xdist_opt) -m release;) \\\n\telse \\\n\t  (set -x; ./pytest -m release;) \\\n\tfi\n\nlong-test: export BUP_TEST_LEVEL=11\nlong-test: test\n\nlong-check: export BUP_TEST_LEVEL=11\nlong-check: check\n\n.PHONY: check-py3\ncheck-py3:\n\t$(MAKE) clean && BUP_PYTHON_CONFIG=python3-config $(MAKE) check\n\n.PHONY: Documentation/all\nDocumentation/all: $(man_roff) $(man_html)\n\nDocumentation/substvars: $(bup_deps)\n        # FIXME: real temp file\n\tset -e; bup_ver=$$(./bup version); \\\n\techo \"s,%BUP_VERSION%,$$bup_ver,g\" > $@.tmp;\n\tset -e; bup_date=$$(./bup version --date); \\\n\techo \"s,%BUP_DATE%,$$bup_date,g\" >> $@.tmp\n\tmv $@.tmp $@\n\ndefine render_page\n  $(pf); sed -f Documentation/substvars $< \\\n    | \"$(PANDOC)\" -s -r markdown -w $(1) -o $(2)\nendef\n\nDocumentation/%: Documentation/%.md Documentation/substvars\n\t$(call render_page,man,$@)\nDocumentation/%.html: Documentation/%.md Documentation/substvars\n\t$(call render_page,html,$@)\n\n.PHONY: Documentation/clean\nDocumentation/clean:\n\tcd Documentation && rm -f *~ .*~ *.[0-9] *.html substvars\n\n# Note: this adds commits containing the current manpages in roff and\n# html format to the man and html branches respectively.  The version\n# is determined by \"git describe --always\".\n.PHONY: update-doc-branches\nupdate-doc-branches: Documentation/all\n\tdev/update-doc-branches refs/heads/man refs/heads/html\n\n# push the pregenerated doc files to origin/man and origin/html\npush-docs: export-docs\n\tgit push origin man html\n\n# import pregenerated doc files from origin/man and origin/html, in case you\n# don't have pandoc but still want to be able to install the docs.\nimport-docs: Documentation/clean\n\t$(pf); git archive origin/html | (cd Documentation && tar -xvf -)\n\t$(pf); git archive origin/man | (cd Documentation && tar -xvf -)\n\nclean: Documentation/clean\n\tcd config && rm -rf finished bin config.var\n\n        # Clean up the mounts first, so that find, etc. won't crash later\n\tif test -e test/mnt; then dev/cleanup-mounts-under test/mnt; fi\n\tif test -e test/mnt; then rm -r test/mnt; fi\n\tif test -e test/tmp; then dev/cleanup-mounts-under test/tmp; fi\n        # FIXME: migrate these to test/mnt/\n\tif test -e test/int/testfs; \\\n\t  then umount test/int/testfs || true; fi\n\trm -rf test/int/testfs test/int/testfs.img testfs.img\n\n\tcd config && rm -f \\\n\t  ${CONFIGURE_DETRITUS} ${CONFIGURE_FILES} ${GENERATED_FILES}\n\trm -rf $(clean_paths) .pytest_cache\n\trm -f $(generated_dependencies)\n\tfind . -name __pycache__ -exec rm -rf {} +\n\tif test -e test/tmp; then dev/force-delete test/tmp; fi\n\tdev/configure-sampledata --clean\n"
        },
        {
          "name": "HACKING",
          "type": "blob",
          "size": 0.009765625,
          "content": "HACKING.md"
        },
        {
          "name": "HACKING.md",
          "type": "blob",
          "size": 5.107421875,
          "content": "\nConventions?  Are you kidding?  OK fine.\n\nCode Branching Model\n====================\n\nThe main branch is the development branch, and stable releases are\ntagged either from there, or from `VERSION.x` branches, created as\nneeded, for example `0.33.x`.\n\nAny branch with a \"tmp/\" prefix might be rebased (often), so keep that\nin mind when using or depending on one.\n\nAny branch with a \"tmp/review/\" prefix corresponds to a patchset\nsubmitted to the mailing list.  We try to maintain these branches to\nmake the review process easier for those not as familiar with patches\nvia email.\n\n\nCurrent Trajectory\n==================\n\nNow that we've finished the 0.33 release, we're working on 0.34, and\nalthough we're not certain which new features will be included, we're\nconsidering:\n\n  - Migrating hashsplitting to C.\n\n  - Automatically splitting trees to avoid having to save large tree\n    objects for large directories even if only a few files have\n    changed or been added (e.g. maildirs).\n\n  - Moving all of the compoents of the index to sqlite.  Right now the\n    main index is an mmapped file, and the hard link and metadata\n    databases are pickled.  As a result the index isn't transactional\n    and suffers from bugs caused by \"skew\" across the components.\n\n  - Better VFS performance for large repositories (i.e. fuse, ls,\n    web...).\n\n  - Better VFS caching.\n\n  - Index improvements.\n\n  - Incremental indexing via inotify.\n\n  - Smarter (and quieter) handling of cross-filesystem metadata.\n\n  - Encryption.\n\n  - Support for alternate remote storage APIs.\n\nIf you have the time and inclination, please help review patches\nposted to the list, or post your own.  (See \"ways to help\" below.)\n\n\nMore specific ways to help\n==========================\n\nTesting -- yes please.\n\nWith respect to patches, bup development is handled via the mailing\nlist, and all patches should be sent to the list for review (see\n\"Submitting Patches\" below).\n\nIn most cases, we try to wait until we have at least one or two\n\"Reviewed-by:\" replies to a patch posted to the list before\nincorporating it into main, so reviews are an important way to help.\nWe also love a good \"Tested-by:\" -- the more the merrier.\n\n\nTesting\n=======\n\nIndividual tests can be run via\n\n    ./pytest TEST\n\nFor example:\n\n    ./pytest test/int/test_git.py\n    ./pytest test/ext/test-ftp\n\nIf you have the xdist module installed, then you can specify its `-n`\noption to run the tests in parallel (e.g. `./pytest -nauto ...`), or\nyou can specify `-j` to make, which will be translated to xdist with\n`-j` becoming `-nauto` and `-jN` becoming `-nN`.\n\nInternal tests that test bup's code directly are located in test/int,\nand external tests that test bup from the outside, typically by\nrunning the executable, are located in test/ext.\n\nCurrently, all pytests must be located in either test/ext or test/int.\nInternal test filenames must match test_*.py, and external tests must\nbe located in text/ext and their filenames must match test-* (see\ntest/ext/conftest.py for the handling of the latter).  Any paths\nmatching those criteria will be automatically collected by pytest.\n\nSome aspects of the environment are automatically restored after each\ntest via fixtures in conftest.py, including the state of the\nenvironment variables and the working directory; the latter is reset\nto the top of the source tree.\n\nSubmitting patches\n==================\n\nAs mentioned, all patches should be posted to the mailing list for\nreview, and must be \"signed off\" by the author before official\ninclusion (see ./SIGNED-OFF-BY).  You can create a \"signed off\" set of\npatches in ./patches, ready for submission to the list, like this:\n\n    git format-patch -s -o patches origin/main\n\nwhich will include all of the patches since origin/main on your\ncurrent branch.  Then you can send them to the list like this:\n\n    git send-email --to bup-list@googlegroups.com --compose patches/*\n\nThe use of --compose will cause git to ask you to edit a cover letter\nthat will be sent as the first message.\n\nIt's also possible to handle everything in one step:\n\n    git send-email -s --to bup-list@googlegroups.com --compose origin/main\n\nand you can add --annotate if you'd like to review or edit each patch\nbefore it's sent.\n\nFor single patches, this might be easier:\n\n    git send-email -s --to bup-list@googlegroups.com --annotate -n1 HEAD\n\nwhich will send the top patch on the current branch, and will stop to\nallow you to add comments.  You can add comments to the section with\nthe diffstat without affecting the commit message.\n\nOf course, unless your machine is set up to handle outgoing mail\nlocally, you may need to configure git to be able to send mail.  See\ngit-send-email(1) for further details.\n\nOh, and we do have a ./CODINGSTYLE, hobgoblins and all, though don't\nlet that scare you off.  We're not all that fierce.\n\n\nEven More Generally\n===================\n\nIt's not like we have a lot of hard and fast rules, but some of the\nideas here aren't altogether terrible:\n\n  http://www.kernel.org/doc/Documentation/SubmittingPatches\n\nIn particular, we've been paying at least some attention to the bits\nregarding Acked-by:, Reported-by:, Tested-by: and Reviewed-by:.\n\n<!--\nLocal Variables:\nmode: markdown\nEnd:\n-->\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 25.0615234375,
          "content": "\nUnless otherwise stated below, the files in this project may be\ndistributed under the terms of the following license. (The LGPL\nversion 2.)\n\nIn addition, bupsplit.c, bupsplit.h, and options.py may be\nredistributed according to the separate (BSD-style) license written\ninside those files.\n\n\n                  GNU LIBRARY GENERAL PUBLIC LICENSE\n                       Version 2, June 1991\n\n Copyright (C) 1991 Free Software Foundation, Inc.\n 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n[This is the first released version of the library GPL.  It is\n numbered 2 because it goes with version 2 of the ordinary GPL.]\n\n                            Preamble\n\n  The licenses for most software are designed to take away your\nfreedom to share and change it.  By contrast, the GNU General Public\nLicenses are intended to guarantee your freedom to share and change\nfree software--to make sure the software is free for all its users.\n\n  This license, the Library General Public License, applies to some\nspecially designated Free Software Foundation software, and to any\nother libraries whose authors decide to use it.  You can use it for\nyour libraries, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthis service if you wish), that you receive source code or can get it\nif you want it, that you can change the software or use pieces of it\nin new free programs; and that you know you can do these things.\n\n  To protect your rights, we need to make restrictions that forbid\nanyone to deny you these rights or to ask you to surrender the rights.\nThese restrictions translate to certain responsibilities for you if\nyou distribute copies of the library, or if you modify it.\n\n  For example, if you distribute copies of the library, whether gratis\nor for a fee, you must give the recipients all the rights that we gave\nyou.  You must make sure that they, too, receive or can get the source\ncode.  If you link a program with the library, you must provide\ncomplete object files to the recipients so that they can relink them\nwith the library, after making changes to the library and recompiling\nit.  And you must show them these terms so they know their rights.\n\n  Our method of protecting your rights has two steps: (1) copyright\nthe library, and (2) offer you this license which gives you legal\npermission to copy, distribute and/or modify the library.\n\n  Also, for each distributor's protection, we want to make certain\nthat everyone understands that there is no warranty for this free\nlibrary.  If the library is modified by someone else and passed on, we\nwant its recipients to know that what they have is not the original\nversion, so that any problems introduced by others will not reflect on\nthe original authors' reputations.\n\n  Finally, any free program is threatened constantly by software\npatents.  We wish to avoid the danger that companies distributing free\nsoftware will individually obtain patent licenses, thus in effect\ntransforming the program into proprietary software.  To prevent this,\nwe have made it clear that any patent must be licensed for everyone's\nfree use or not licensed at all.\n\n  Most GNU software, including some libraries, is covered by the ordinary\nGNU General Public License, which was designed for utility programs.  This\nlicense, the GNU Library General Public License, applies to certain\ndesignated libraries.  This license is quite different from the ordinary\none; be sure to read it in full, and don't assume that anything in it is\nthe same as in the ordinary license.\n\n  The reason we have a separate public license for some libraries is that\nthey blur the distinction we usually make between modifying or adding to a\nprogram and simply using it.  Linking a program with a library, without\nchanging the library, is in some sense simply using the library, and is\nanalogous to running a utility program or application program.  However, in\na textual and legal sense, the linked executable is a combined work, a\nderivative of the original library, and the ordinary General Public License\ntreats it as such.\n\n  Because of this blurred distinction, using the ordinary General\nPublic License for libraries did not effectively promote software\nsharing, because most developers did not use the libraries.  We\nconcluded that weaker conditions might promote sharing better.\n\n  However, unrestricted linking of non-free programs would deprive the\nusers of those programs of all benefit from the free status of the\nlibraries themselves.  This Library General Public License is intended to\npermit developers of non-free programs to use free libraries, while\npreserving your freedom as a user of such programs to change the free\nlibraries that are incorporated in them.  (We have not seen how to achieve\nthis as regards changes in header files, but we have achieved it as regards\nchanges in the actual functions of the Library.)  The hope is that this\nwill lead to faster development of free libraries.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.  Pay close attention to the difference between a\n\"work based on the library\" and a \"work that uses the library\".  The\nformer contains code derived from the library, while the latter only\nworks together with the library.\n\n  Note that it is possible for a library to be covered by the ordinary\nGeneral Public License rather than by this special one.\n\n                  GNU LIBRARY GENERAL PUBLIC LICENSE\n   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n\n  0. This License Agreement applies to any software library which\ncontains a notice placed by the copyright holder or other authorized\nparty saying it may be distributed under the terms of this Library\nGeneral Public License (also called \"this License\").  Each licensee is\naddressed as \"you\".\n\n  A \"library\" means a collection of software functions and/or data\nprepared so as to be conveniently linked with application programs\n(which use some of those functions and data) to form executables.\n\n  The \"Library\", below, refers to any such software library or work\nwhich has been distributed under these terms.  A \"work based on the\nLibrary\" means either the Library or any derivative work under\ncopyright law: that is to say, a work containing the Library or a\nportion of it, either verbatim or with modifications and/or translated\nstraightforwardly into another language.  (Hereinafter, translation is\nincluded without limitation in the term \"modification\".)\n\n  \"Source code\" for a work means the preferred form of the work for\nmaking modifications to it.  For a library, complete source code means\nall the source code for all modules it contains, plus any associated\ninterface definition files, plus the scripts used to control compilation\nand installation of the library.\n\n  Activities other than copying, distribution and modification are not\ncovered by this License; they are outside its scope.  The act of\nrunning a program using the Library is not restricted, and output from\nsuch a program is covered only if its contents constitute a work based\non the Library (independent of the use of the Library in a tool for\nwriting it).  Whether that is true depends on what the Library does\nand what the program that uses the Library does.\n\n  1. You may copy and distribute verbatim copies of the Library's\ncomplete source code as you receive it, in any medium, provided that\nyou conspicuously and appropriately publish on each copy an\nappropriate copyright notice and disclaimer of warranty; keep intact\nall the notices that refer to this License and to the absence of any\nwarranty; and distribute a copy of this License along with the\nLibrary.\n\n  You may charge a fee for the physical act of transferring a copy,\nand you may at your option offer warranty protection in exchange for a\nfee.\n\n  2. You may modify your copy or copies of the Library or any portion\nof it, thus forming a work based on the Library, and copy and\ndistribute such modifications or work under the terms of Section 1\nabove, provided that you also meet all of these conditions:\n\n    a) The modified work must itself be a software library.\n\n    b) You must cause the files modified to carry prominent notices\n    stating that you changed the files and the date of any change.\n\n    c) You must cause the whole of the work to be licensed at no\n    charge to all third parties under the terms of this License.\n\n    d) If a facility in the modified Library refers to a function or a\n    table of data to be supplied by an application program that uses\n    the facility, other than as an argument passed when the facility\n    is invoked, then you must make a good faith effort to ensure that,\n    in the event an application does not supply such function or\n    table, the facility still operates, and performs whatever part of\n    its purpose remains meaningful.\n\n    (For example, a function in a library to compute square roots has\n    a purpose that is entirely well-defined independent of the\n    application.  Therefore, Subsection 2d requires that any\n    application-supplied function or table used by this function must\n    be optional: if the application does not supply it, the square\n    root function must still compute square roots.)\n\nThese requirements apply to the modified work as a whole.  If\nidentifiable sections of that work are not derived from the Library,\nand can be reasonably considered independent and separate works in\nthemselves, then this License, and its terms, do not apply to those\nsections when you distribute them as separate works.  But when you\ndistribute the same sections as part of a whole which is a work based\non the Library, the distribution of the whole must be on the terms of\nthis License, whose permissions for other licensees extend to the\nentire whole, and thus to each and every part regardless of who wrote\nit.\n\nThus, it is not the intent of this section to claim rights or contest\nyour rights to work written entirely by you; rather, the intent is to\nexercise the right to control the distribution of derivative or\ncollective works based on the Library.\n\nIn addition, mere aggregation of another work not based on the Library\nwith the Library (or with a work based on the Library) on a volume of\na storage or distribution medium does not bring the other work under\nthe scope of this License.\n\n  3. You may opt to apply the terms of the ordinary GNU General Public\nLicense instead of this License to a given copy of the Library.  To do\nthis, you must alter all the notices that refer to this License, so\nthat they refer to the ordinary GNU General Public License, version 2,\ninstead of to this License.  (If a newer version than version 2 of the\nordinary GNU General Public License has appeared, then you can specify\nthat version instead if you wish.)  Do not make any other change in\nthese notices.\n\n  Once this change is made in a given copy, it is irreversible for\nthat copy, so the ordinary GNU General Public License applies to all\nsubsequent copies and derivative works made from that copy.\n\n  This option is useful when you wish to copy part of the code of\nthe Library into a program that is not a library.\n\n  4. You may copy and distribute the Library (or a portion or\nderivative of it, under Section 2) in object code or executable form\nunder the terms of Sections 1 and 2 above provided that you accompany\nit with the complete corresponding machine-readable source code, which\nmust be distributed under the terms of Sections 1 and 2 above on a\nmedium customarily used for software interchange.\n\n  If distribution of object code is made by offering access to copy\nfrom a designated place, then offering equivalent access to copy the\nsource code from the same place satisfies the requirement to\ndistribute the source code, even though third parties are not\ncompelled to copy the source along with the object code.\n\n  5. A program that contains no derivative of any portion of the\nLibrary, but is designed to work with the Library by being compiled or\nlinked with it, is called a \"work that uses the Library\".  Such a\nwork, in isolation, is not a derivative work of the Library, and\ntherefore falls outside the scope of this License.\n\n  However, linking a \"work that uses the Library\" with the Library\ncreates an executable that is a derivative of the Library (because it\ncontains portions of the Library), rather than a \"work that uses the\nlibrary\".  The executable is therefore covered by this License.\nSection 6 states terms for distribution of such executables.\n\n  When a \"work that uses the Library\" uses material from a header file\nthat is part of the Library, the object code for the work may be a\nderivative work of the Library even though the source code is not.\nWhether this is true is especially significant if the work can be\nlinked without the Library, or if the work is itself a library.  The\nthreshold for this to be true is not precisely defined by law.\n\n  If such an object file uses only numerical parameters, data\nstructure layouts and accessors, and small macros and small inline\nfunctions (ten lines or less in length), then the use of the object\nfile is unrestricted, regardless of whether it is legally a derivative\nwork.  (Executables containing this object code plus portions of the\nLibrary will still fall under Section 6.)\n\n  Otherwise, if the work is a derivative of the Library, you may\ndistribute the object code for the work under the terms of Section 6.\nAny executables containing that work also fall under Section 6,\nwhether or not they are linked directly with the Library itself.\n\n  6. As an exception to the Sections above, you may also compile or\nlink a \"work that uses the Library\" with the Library to produce a\nwork containing portions of the Library, and distribute that work\nunder terms of your choice, provided that the terms permit\nmodification of the work for the customer's own use and reverse\nengineering for debugging such modifications.\n\n  You must give prominent notice with each copy of the work that the\nLibrary is used in it and that the Library and its use are covered by\nthis License.  You must supply a copy of this License.  If the work\nduring execution displays copyright notices, you must include the\ncopyright notice for the Library among them, as well as a reference\ndirecting the user to the copy of this License.  Also, you must do one\nof these things:\n\n    a) Accompany the work with the complete corresponding\n    machine-readable source code for the Library including whatever\n    changes were used in the work (which must be distributed under\n    Sections 1 and 2 above); and, if the work is an executable linked\n    with the Library, with the complete machine-readable \"work that\n    uses the Library\", as object code and/or source code, so that the\n    user can modify the Library and then relink to produce a modified\n    executable containing the modified Library.  (It is understood\n    that the user who changes the contents of definitions files in the\n    Library will not necessarily be able to recompile the application\n    to use the modified definitions.)\n\n    b) Accompany the work with a written offer, valid for at\n    least three years, to give the same user the materials\n    specified in Subsection 6a, above, for a charge no more\n    than the cost of performing this distribution.\n\n    c) If distribution of the work is made by offering access to copy\n    from a designated place, offer equivalent access to copy the above\n    specified materials from the same place.\n\n    d) Verify that the user has already received a copy of these\n    materials or that you have already sent this user a copy.\n\n  For an executable, the required form of the \"work that uses the\nLibrary\" must include any data and utility programs needed for\nreproducing the executable from it.  However, as a special exception,\nthe source code distributed need not include anything that is normally\ndistributed (in either source or binary form) with the major\ncomponents (compiler, kernel, and so on) of the operating system on\nwhich the executable runs, unless that component itself accompanies\nthe executable.\n\n  It may happen that this requirement contradicts the license\nrestrictions of other proprietary libraries that do not normally\naccompany the operating system.  Such a contradiction means you cannot\nuse both them and the Library together in an executable that you\ndistribute.\n\n  7. You may place library facilities that are a work based on the\nLibrary side-by-side in a single library together with other library\nfacilities not covered by this License, and distribute such a combined\nlibrary, provided that the separate distribution of the work based on\nthe Library and of the other library facilities is otherwise\npermitted, and provided that you do these two things:\n\n    a) Accompany the combined library with a copy of the same work\n    based on the Library, uncombined with any other library\n    facilities.  This must be distributed under the terms of the\n    Sections above.\n\n    b) Give prominent notice with the combined library of the fact\n    that part of it is a work based on the Library, and explaining\n    where to find the accompanying uncombined form of the same work.\n\n  8. You may not copy, modify, sublicense, link with, or distribute\nthe Library except as expressly provided under this License.  Any\nattempt otherwise to copy, modify, sublicense, link with, or\ndistribute the Library is void, and will automatically terminate your\nrights under this License.  However, parties who have received copies,\nor rights, from you under this License will not have their licenses\nterminated so long as such parties remain in full compliance.\n\n  9. You are not required to accept this License, since you have not\nsigned it.  However, nothing else grants you permission to modify or\ndistribute the Library or its derivative works.  These actions are\nprohibited by law if you do not accept this License.  Therefore, by\nmodifying or distributing the Library (or any work based on the\nLibrary), you indicate your acceptance of this License to do so, and\nall its terms and conditions for copying, distributing or modifying\nthe Library or works based on it.\n\n  10. Each time you redistribute the Library (or any work based on the\nLibrary), the recipient automatically receives a license from the\noriginal licensor to copy, distribute, link with or modify the Library\nsubject to these terms and conditions.  You may not impose any further\nrestrictions on the recipients' exercise of the rights granted herein.\nYou are not responsible for enforcing compliance by third parties to\nthis License.\n\n  11. If, as a consequence of a court judgment or allegation of patent\ninfringement or for any other reason (not limited to patent issues),\nconditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot\ndistribute so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you\nmay not distribute the Library at all.  For example, if a patent\nlicense would not permit royalty-free redistribution of the Library by\nall those who receive copies directly or indirectly through you, then\nthe only way you could satisfy both it and this License would be to\nrefrain entirely from distribution of the Library.\n\nIf any portion of this section is held invalid or unenforceable under any\nparticular circumstance, the balance of the section is intended to apply,\nand the section as a whole is intended to apply in other circumstances.\n\nIt is not the purpose of this section to induce you to infringe any\npatents or other property right claims or to contest validity of any\nsuch claims; this section has the sole purpose of protecting the\nintegrity of the free software distribution system which is\nimplemented by public license practices.  Many people have made\ngenerous contributions to the wide range of software distributed\nthrough that system in reliance on consistent application of that\nsystem; it is up to the author/donor to decide if he or she is willing\nto distribute software through any other system and a licensee cannot\nimpose that choice.\n\nThis section is intended to make thoroughly clear what is believed to\nbe a consequence of the rest of this License.\n\n  12. If the distribution and/or use of the Library is restricted in\ncertain countries either by patents or by copyrighted interfaces, the\noriginal copyright holder who places the Library under this License may add\nan explicit geographical distribution limitation excluding those countries,\nso that distribution is permitted only in or among countries not thus\nexcluded.  In such case, this License incorporates the limitation as if\nwritten in the body of this License.\n\n  13. The Free Software Foundation may publish revised and/or new\nversions of the Library General Public License from time to time.\nSuch new versions will be similar in spirit to the present version,\nbut may differ in detail to address new problems or concerns.\n\nEach version is given a distinguishing version number.  If the Library\nspecifies a version number of this License which applies to it and\n\"any later version\", you have the option of following the terms and\nconditions either of that version or of any later version published by\nthe Free Software Foundation.  If the Library does not specify a\nlicense version number, you may choose any version ever published by\nthe Free Software Foundation.\n\n  14. If you wish to incorporate parts of the Library into other free\nprograms whose distribution conditions are incompatible with these,\nwrite to the author to ask for permission.  For software which is\ncopyrighted by the Free Software Foundation, write to the Free\nSoftware Foundation; we sometimes make exceptions for this.  Our\ndecision will be guided by the two goals of preserving the free status\nof all derivatives of our free software and of promoting the sharing\nand reuse of software generally.\n\n                            NO WARRANTY\n\n  15. BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO\nWARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW.\nEXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR\nOTHER PARTIES PROVIDE THE LIBRARY \"AS IS\" WITHOUT WARRANTY OF ANY\nKIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE\nLIBRARY IS WITH YOU.  SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME\nTHE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN\nWRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY\nAND/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU\nFOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR\nCONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE\nLIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING\nRENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A\nFAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF\nSUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGES.\n\n                     END OF TERMS AND CONDITIONS\n\n           How to Apply These Terms to Your New Libraries\n\n  If you develop a new library, and you want it to be of the greatest\npossible use to the public, we recommend making it free software that\neveryone can redistribute and change.  You can do so by permitting\nredistribution under these terms (or, alternatively, under the terms of the\nordinary General Public License).\n\n  To apply these terms, attach the following notices to the library.  It is\nsafest to attach them to the start of each source file to most effectively\nconvey the exclusion of warranty; and each file should have at least the\n\"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the library's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This library is free software; you can redistribute it and/or\n    modify it under the terms of the GNU Library General Public\n    License as published by the Free Software Foundation; either\n    version 2 of the License, or (at your option) any later version.\n\n    This library is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n    Library General Public License for more details.\n\n    You should have received a copy of the GNU Library General Public\n    License along with this library; if not, write to the Free Software\n    Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n\nAlso add information on how to contact you by electronic and paper mail.\n\nYou should also get your employer (if you work as a programmer) or your\nschool, if any, to sign a \"copyright disclaimer\" for the library, if\nnecessary.  Here is a sample; alter the names:\n\n  Yoyodyne, Inc., hereby disclaims all copyright interest in the\n  library `Frob' (a library for tweaking knobs) written by James Random Hacker.\n\n  <signature of Ty Coon>, 1 April 1990\n  Ty Coon, President of Vice\n\nThat's all there is to it!\n"
        },
        {
          "name": "README",
          "type": "blob",
          "size": 0.0087890625,
          "content": "README.md"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 23.7744140625,
          "content": "bup: It backs things up\n=======================\n\nbup is a program that backs things up.  It's short for \"backup.\" Can you\nbelieve that nobody else has named an open source program \"bup\" after all\nthis time?  Me neither.\n\nDespite its unassuming name, bup is pretty cool.  To give you an idea of\njust how cool it is, I wrote you this poem:\n\n                             Bup is teh awesome\n                          What rhymes with awesome?\n                            I guess maybe possum\n                           But that's irrelevant.\n\t\t\t\nHmm.  Did that help?  Maybe prose is more useful after all.\n\n\nReasons bup is awesome\n----------------------\n\nbup has a few advantages over other backup software:\n\n - It uses a rolling checksum algorithm (similar to rsync) to split large\n   files into chunks.  The most useful result of this is you can backup huge\n   virtual machine (VM) disk images, databases, and XML files incrementally,\n   even though they're typically all in one huge file, and not use tons of\n   disk space for multiple versions.\n   \n - It uses the packfile format from git (the open source version control\n   system), so you can access the stored data even if you don't like bup's\n   user interface.\n   \n - Unlike git, it writes packfiles *directly* (instead of having a separate\n   garbage collection / repacking stage) so it's fast even with gratuitously\n   huge amounts of data.  bup's improved index formats also allow you to\n   track far more filenames than git (millions) and keep track of far more\n   objects (hundreds or thousands of gigabytes).\n   \n - Data is \"automagically\" shared between incremental backups without having\n   to know which backup is based on which other one - even if the backups\n   are made from two different computers that don't even know about each\n   other.  You just tell bup to back stuff up, and it saves only the minimum\n   amount of data needed.\n   \n - You can back up directly to a remote bup server, without needing tons of\n   temporary disk space on the computer being backed up.  And if your backup\n   is interrupted halfway through, the next run will pick up where you left\n   off.  And it's easy to set up a bup server: just install bup on any\n   machine where you have ssh access.\n   \n - Bup can use \"par2\" redundancy to recover corrupted backups even if your\n   disk has undetected bad sectors.\n   \n - Even when a backup is incremental, you don't have to worry about\n   restoring the full backup, then each of the incrementals in turn; an\n   incremental backup *acts* as if it's a full backup, it just takes less\n   disk space.\n   \n - You can mount your bup repository as a FUSE filesystem and access the\n   content that way, and even export it over Samba.\n   \n - It's written in python (with some C parts to make it faster) so it's easy\n   for you to extend and maintain.\n\n\nReasons you might want to avoid bup\n-----------------------------------\n\n - It's not remotely as well tested as something like tar, so it's\n   more likely to eat your data.  It's also missing some\n   probably-critical features, though fewer than it used to be.\n   \n - It requires python 3.7 or newer, a C compiler, and an installed git\n   version >= 1.7.2.  It also requires par2 if you want fsck to be\n   able to generate the information needed to recover from some types\n   of corruption.\n \n - It currently only works on Linux, FreeBSD, NetBSD, OS X >= 10.4,\n   Solaris, or Windows (with Cygwin, and WSL).  Patches to support\n   other platforms are welcome.\n\n - Any items in \"Things that are stupid\" below.\n\n\nNotable changes introduced by a release\n=======================================\n\n - <a href=\"note/0.33.6-from-0.33.5.md\">Changes in 0.33.6 as compared to 0.33.5</a>\n - <a href=\"note/0.33.5-from-0.33.4.md\">Changes in 0.33.5 as compared to 0.33.4</a>\n - <a href=\"note/0.33.4-from-0.33.3.md\">Changes in 0.33.4 as compared to 0.33.3</a>\n - <a href=\"note/0.33.3-from-0.33.2.md\">Changes in 0.33.3 as compared to 0.33.2</a>\n - <a href=\"note/0.33.2-from-0.33.1.md\">Changes in 0.33.2 as compared to 0.33.1</a>\n - <a href=\"note/0.33.1-from-0.33.md\">Changes in 0.33.1 as compared to 0.33</a>\n - <a href=\"note/0.33-from-0.32.md\">Changes in 0.33 as compared to 0.32</a>\n - <a href=\"note/0.32.1-from-0.32.md\">Changes in 0.32.1 as compared to 0.32</a>\n - <a href=\"note/0.32-from-0.31.md\">Changes in 0.32 as compared to 0.31</a>\n - <a href=\"note/0.31-from-0.30.1.md\">Changes in 0.31 as compared to 0.30.1</a>\n - <a href=\"note/0.30.1-from-0.30.md\">Changes in 0.30.1 as compared to 0.30</a>\n - <a href=\"note/0.30-from-0.29.3.md\">Changes in 0.30 as compared to 0.29.3</a>\n - <a href=\"note/0.29.3-from-0.29.2.md\">Changes in 0.29.3 as compared to 0.29.2</a>\n - <a href=\"note/0.29.2-from-0.29.1.md\">Changes in 0.29.2 as compared to 0.29.1</a>\n - <a href=\"note/0.29.1-from-0.29.md\">Changes in 0.29.1 as compared to 0.29</a>\n - <a href=\"note/0.29-from-0.28.1.md\">Changes in 0.29 as compared to 0.28.1</a>\n - <a href=\"note/0.28.1-from-0.28.md\">Changes in 0.28.1 as compared to 0.28</a>\n - <a href=\"note/0.28-from-0.27.1.md\">Changes in 0.28 as compared to 0.27.1</a>\n - <a href=\"note/0.27.1-from-0.27.md\">Changes in 0.27.1 as compared to 0.27</a>\n\n\nTest status\n===========\n\n| main |\n|--------|\n| [![main branch test status](https://api.cirrus-ci.com/github/bup/bup.svg?branch=main)](https://cirrus-ci.com/github/bup/bup) |\n\nGetting started\n===============\n\nFrom source\n-----------\n\n - Check out the bup source code using git (for Cygwin, use a Cygwin\n   installed git so that symlinks will work by defaul):\n\n    ```sh\n    git clone https://github.com/bup/bup\n    ```\n\n - This will leave you on the main branch, which is perfect if you\n   would like to help with development, but if you'd just like to use\n   bup, please check out the latest stable release like this:\n\n    ```sh\n    git checkout 0.33.6\n    ```\n\n   You can see the latest stable release here:\n   https://github.com/bup/bup/tags\n\n - Ensure you have a Python 3.7+ development environment available and\n   install the required python libraries (including the development\n   libraries).\n\n   See the relevant [platform specific information](#platform-specific-information)\n   below, if available, and if not, refer to the [Debian notes](#notes-on-debian)\n   to get a general idea of what's required.  You might also want to\n   look at the `dev/prep-for-*` scripts.  Those include all the\n   commands used to prepare to build bup for testing on the platform.\n\n   For `bup fuse` you will need to install\n   [python-fuse](https://github.com/libfuse/python-fuse) rather than\n   [fusepy](https://github.com/fusepy/fusepy).  For example, in\n   Debian, install python3-fuse rather than python3-fusepy.\n\n   If you would like to use the optional bup web server on systems\n   without a tornado package, you may want to try this:\n\n    ```sh\n    pip install tornado\n    ```\n\n - Build (below, replace `make` with whatever GNU make is called on\n   the current system if it isn't `make`, e.g. commonly `gmake`):\n\n    ```sh\n    make\n    ```\n\n   At the moment the build treats compiler warnings as errors.  If the\n   build fails as a result, try this:\n\n   ```sh\n   CFLAGS=-Wno-error ./configure\n   make\n   ```\n\n - Run the tests:\n\n    ```sh\n    make long-check\n    ```\n\n    or if you're in a bit more of a hurry:\n\n    ```sh\n    make check\n    ```\n \t\n    If you have the Python xdist module installed, then you can\n    probably run the tests faster by adding the make -j option (see <a\n    href=\"HACKING\">./HACKING</a> for additional information):\n\n    ```sh\n    make -j check\n    ```\n\n    The tests should pass (with some skipped tests that weren't\n    applicable in your environment).  If they don't pass for you, stop\n    here and send an email to bup-list@googlegroups.com.  Though if\n    there are symbolic links along the current working directory path,\n    the tests may fail.  Running something like this before \"make\n    test\" should sidestep the problem:\n\n    ```sh\n    cd \"$(pwd -P)\"\n    ```\n\n - You can install bup via \"make install\", and override the default\n   destination with DESTDIR and PREFIX.\n\n   Files are normally installed to \"$DESTDIR/$PREFIX\" where DESTDIR is\n   empty by default, and PREFIX is set to /usr/local.  So if you wanted to\n   install bup to /opt/bup, you might do something like this:\n\n    ```sh\n    make install DESTDIR=/opt/bup PREFIX=''\n    ```\n\n - The Python version that bup will use is determined by the\n   `python-config` program chosen by `./configure`, which will search\n   for a reasonable version unless `BUP_PYTHON_CONFIG` is set in the\n   environment.  You can see which Python executable was chosen by\n   looking at the configure output, or examining\n   `config/config.var/bup-python-config`, and you can change the\n   selection by re-running `./configure`.\n\n- If you want to specify your own `CPPFLAGS`, `CFLAGS`, or `LDFLAGS`,\n  you can set them for individual `make` invocations, e.g. `make\n  CFLAGS=-O0 check`, or persistently via `./configure` with\n  `CFLAGS=-O0 ./configure`.  At the moment, `make clean` clears the\n  configuration, but we may change that at some point, perhaps by\n  adding and requiring a `make distclean` to clear the configuration.\n\nFrom binary packages\n--------------------\n\nBinary packages of bup are known to be built for the following OSes:\n\n - [Debian](https://packages.debian.org/bup)\n - [Ubuntu](https://packages.ubuntu.com/bup)\n - [pkgsrc.se (NetBSD, Dragonfly, and others)](https://pkgsrc.se/sysutils/bup)\n - [NetBSD](https://cvsweb.netbsd.org/bsdweb.cgi/pkgsrc/sysutils/bup/)\n - [Arch Linux](https://www.archlinux.org/packages/?sort=&q=bup)\n - [macOS (Homebrew)](https://formulae.brew.sh/formula/bup)\n\n\nUsing bup\n---------\n\n - Get help for any bup command:\n\n    ```sh\n    bup help\n    bup help init\n    bup help index\n    bup help save\n    bup help restore\n    ...\n    ```\n\n - Initialize the default bup repository (~/.bup -- you can choose\n   another by either specifying `bup -d DIR ...` or setting the\n   `BUP_DIR` environment variable for a command):\n\n    ```sh\n    bup init\n    ```\n\n - Make a local backup (-v or -vv will increase the verbosity):\n\n    ```sh\n    bup index /etc\n    bup save -n local-etc /etc\n    ```\n\n - Restore a local backup to ./dest:\n\n    ```sh\n    bup restore -C ./dest local-etc/latest/etc\n    ls -l dest/etc\n    ```\n\n - Look at how much disk space your backup took:\n\n    ```sh\n    du -s ~/.bup\n    ```\n\n - Make another backup (which should be mostly identical to the last one;\n   notice that you don't have to *specify* that this backup is incremental,\n   it just saves space automatically):\n\n    ```sh\n    bup index /etc\n    bup save -n local-etc /etc\n    ```\n\n - Look how little extra space your second backup used (on top of the first):\n\n    ```sh\n    du -s ~/.bup\n    ```\n\n - Get a list of your previous backups:\n\n    ```sh\n    bup ls local-etc\n    ```\n\n - Restore your first backup again:\n\n    ```sh\n    bup restore -C ./dest-2 local-etc/2013-11-23-11195/etc\n    ```\n\n - Make a backup to a remote server which must already have the 'bup' command\n   somewhere in its PATH (see /etc/profile, etc/environment, ~/.profile, or\n   ~/.bashrc), and be accessible via ssh.\n   Make sure to replace SERVERNAME with the actual hostname of your server:\n\n    ```sh\n    bup init -r SERVERNAME:path/to/remote-bup-dir\n    bup index /etc\n    bup save -r SERVERNAME:path/to/remote-bup-dir -n local-etc /etc\n    ```\n\n - Make a remote backup to ~/.bup on SERVER:\n\n    ```sh\n    bup index /etc\n    bup save -r SERVER: -n local-etc /etc\n    ```\n\n - See what saves are available in ~/.bup on SERVER:\n\n    ```sh\n    bup ls -r SERVER:\n    ```\n\n - Restore the remote backup to ./dest:\n\n    ```sh\n    bup restore -r SERVER: -C ./dest local-etc/latest/etc\n    ls -l dest/etc\n    ```\n\n - Defend your backups from death rays (OK fine, more likely from the\n   occasional bad disk block).  This writes parity information\n   (currently via par2) for all of the existing data so that bup may\n   be able to recover from some amount of repository corruption:\n\n    ```sh\n    bup fsck -g\n    ```\n\n - Use split/join instead of index/save/restore.  Try making a local\n   backup using tar:\n\n    ```sh\n    tar -cvf - /etc | bup split -n local-etc -vv\n    ```\n \t\n - Try restoring the tarball:\n\n    ```sh\n    bup join local-etc | tar -tf -\n    ```\n \t\n - Look at how much disk space your backup took:\n\n    ```sh\n    du -s ~/.bup\n    ```\n \t\n - Make another tar backup:\n\n    ```sh\n    tar -cvf - /etc | bup split -n local-etc -vv\n    ```\n \t\n - Look at how little extra space your second backup used on top of\n   the first:\n\n    ```sh\n    du -s ~/.bup\n    ```\n \t\n - Restore the first tar backup again (the ~1 is git notation for \"one\n   older than the most recent\"):\n\n    ```sh\n    bup join local-etc~1 | tar -tf -\n    ```\n \n - Get a list of your previous split-based backups:\n\n    ```sh\n    GIT_DIR=~/.bup git log local-etc\n    ```\n\t\n - Save a tar archive to a remote server (without tar -z to facilitate\n   deduplication):\n\n    ```sh\n    tar -cvf - /etc | bup split -r SERVERNAME: -n local-etc -vv\n    ```\n \n - Restore the archive:\n\n    ```sh\n    bup join -r SERVERNAME: local-etc | tar -tf -\n    ```\n \t\nThat's all there is to it!\n\n\nPlatform specific information\n=============================\n\nNotes on Debian (and likely, on derivatives like Ubuntu)\n--------------------------------------------------------\n\nIf your distribution is recent enough, or includes new enough `apt`\nsources, this may be sufficient (run as root):\n\n  ```sh\n  apt-get build-dep bup\n  ```\n\nOtherwise try this:\n\n  ```sh\n  apt-get install python3-dev python3-fuse\n  apt-get install python3-pyxattr python3-pytest\n  apt-get install python3-distutils\n  apt-get install pkg-config linux-libc-dev libacl1-dev\n  apt-get install gcc make acl attr rsync\n  apt-get isntall python3-pytest-xdist # optional (parallel tests)\n  apt-get install par2 # optional (error correction)\n  apt-get install libreadline-dev # optional (bup ftp)\n  apt-get install python3-tornado # optional (bup web)\n  ```\n\nNotes on FreeBSD\n----------------\n\n- In order to compile the code, run tests and install bup, you need to\n  install and run GNU Make from the `gmake` port.\n\n- Python's development headers are automatically installed with the 'python'\n  port so there's no need to install them separately.\n\n- To use the 'bup fuse' command, you need to install the fuse kernel module\n  from the 'fusefs-kmod' port in the 'sysutils' section and the libraries from\n  the port named 'py-fusefs' in the 'devel' section.\n\n- The 'par2' command can be found in the port named 'par2cmdline'.\n\n- In order to compile the documentation, you need pandoc which can be found in\n  the port named 'hs-pandoc' in the 'textproc' section.\n\n\nNotes on NetBSD/pkgsrc\n----------------------\n\n - See pkgsrc/sysutils/bup, which should be the most recent stable\n   release and includes man pages.  It also has a reasonable set of\n   dependencies (git, par2, py-fuse-bindings).\n\n - The \"fuse-python\" package referred to is hard to locate, and is a\n   separate tarball for the python language binding distributed by the\n   fuse project on sourceforge.  It is available as\n   pkgsrc/filesystems/py-fuse-bindings and on NetBSD 5, \"bup fuse\"\n   works with it.\n\n - \"bup fuse\" presents every directory/file as inode 0.  The directory\n   traversal code (\"fts\") in NetBSD's libc will interpret this as a\n   cycle and error out, so \"ls -R\" and \"find\" will not work.\n\n - There is no support for ACLs.  If/when some enterprising person\n   fixes this, adjust dev/compare-trees.\n\n\nNotes on Cygwin\n---------------\n\n- To prepare to build build bup, install the gcc-core, git,\n  python3-devel, make, and rsync packages.  Optionally, install\n  libreadline-devel (for `bup ftp`) and par2.  To generate the help\n  pages, install pandoc (outside of Cygwin).\n\n  Use a Cygwin installed git so that symlinks will work by default.\n  If you try some other git version, ensure that the symlinks (e.g.\n  `./bup`) are not converted to text files.\n\n - There is no support for ACLs.  If/when some enterprising person\n   fixes this, adjust dev/compare-trees.\n\n - In test/ext/test-misc, two tests have been disabled.  These tests\n   check to see that repeated saves produce identical trees and that\n   an intervening index doesn't change the SHA1.  Apparently Cygwin\n   has some unusual behaviors with respect to access times (that\n   probably warrant further investigation).  Possibly related:\n   http://cygwin.com/ml/cygwin/2007-06/msg00436.html\n\n\nNotes on OS X\n-------------\n\n - There is no support for ACLs.  If/when some enterprising person\n   fixes this, adjust dev/compare-trees.\n\n\nHow it works\n============\n\nBasic storage:\n--------------\n\nbup stores its data in a git-formatted repository.  Unfortunately, git\nitself doesn't actually behave very well for bup's use case (huge numbers of\nfiles, files with huge sizes, retaining file permissions/ownership are\nimportant), so we mostly don't use git's *code* except for a few helper\nprograms.  For example, bup has its own git packfile writer written in\npython.\n\nBasically, 'bup split' reads the data on stdin (or from files specified on\nthe command line), breaks it into chunks using a rolling checksum (similar to\nrsync), and saves those chunks into a new git packfile.  There is at least one\ngit packfile per backup.\n\nWhen deciding whether to write a particular chunk into the new packfile, bup\nfirst checks all the other packfiles that exist to see if they already have that\nchunk.  If they do, the chunk is skipped.\n\ngit packs come in two parts: the pack itself (*.pack) and the index (*.idx).\nThe index is pretty small, and contains a list of all the objects in the\npack.  Thus, when generating a remote backup, we don't have to have a copy\nof the packfiles from the remote server: the local end just downloads a copy\nof the server's *index* files, and compares objects against those when\ngenerating the new pack, which it sends directly to the server.\n\nThe \"-n\" option to 'bup split' and 'bup save' is the name of the backup you\nwant to create, but it's actually implemented as a git branch.  So you can\ndo cute things like checkout a particular branch using git, and receive a\nbunch of chunk files corresponding to the file you split.\n\nIf you use '-b' or '-t' or '-c' instead of '-n', bup split will output a\nlist of blobs, a tree containing that list of blobs, or a commit containing\nthat tree, respectively, to stdout.  You can use this to construct your own\nscripts that do something with those values.\n\nThe bup index:\n--------------\n\n'bup index' walks through your filesystem and updates a file (whose name is,\nby default, ~/.bup/bupindex) to contain the name, attributes, and an\noptional git SHA1 (blob id) of each file and directory.\n\n'bup save' basically just runs the equivalent of 'bup split' a whole bunch\nof times, once per file in the index, and assembles a git tree\nthat contains all the resulting objects.  Among other things, that makes\n'git diff' much more useful (compared to splitting a tarball, which is\nessentially a big binary blob).  However, since bup splits large files into\nsmaller chunks, the resulting tree structure doesn't *exactly* correspond to\nwhat git itself would have stored.  Also, the tree format used by 'bup save'\nwill probably change in the future to support storing file ownership, more\ncomplex file permissions, and so on.\n\nIf a file has previously been written by 'bup save', then its git blob/tree\nid is stored in the index.  This lets 'bup save' avoid reading that file to\nproduce future incremental backups, which means it can go *very* fast unless\na lot of files have changed.\n\n \nThings that are stupid for now but which we'll fix later\n========================================================\n\nHelp with any of these problems, or others, is very welcome.  Join the\nmailing list (see below) if you'd like to help.\n\n - 'bup save' and 'bup restore' have immature metadata support.\n \n    On the plus side, they actually do have support now, but it's new,\n    and not remotely as well tested as tar/rsync/whatever's.  However,\n    you have to start somewhere, and as of 0.25, we think it's ready\n    for more general use.  Please let us know if you have any trouble.\n\n    Also, if any strip or graft-style options are specified to 'bup\n    save', then no metadata will be written for the root directory.\n    That's obviously less than ideal.\n\n - bup is overly optimistic about mmap.  Right now bup just assumes\n   that it can mmap as large a block as it likes, and that mmap will\n   never fail.  Yeah, right... If nothing else, this has failed on\n   32-bit architectures (and 31-bit is even worse -- looking at you,\n   s390).\n\n   To fix this, we might just implement a FakeMmap[1] class that uses\n   normal file IO and handles all of the mmap methods[2] that bup\n   actually calls.  Then we'd swap in one of those whenever mmap\n   fails.\n\n   This would also require implementing some of the methods needed to\n   support \"[]\" array access, probably at a minimum __getitem__,\n   __setitem__, and __setslice__ [3].\n\n     [1] http://comments.gmane.org/gmane.comp.sysutils.backup.bup/613\n     [2] http://docs.python.org/3/library/mmap.html\n     [3] http://docs.python.org/3/reference/datamodel.html#emulating-container-types\n\n - 'bup index' is slower than it should be.\n \n    It's still rather fast: it can iterate through all the filenames on my\n    600,000 file filesystem in a few seconds.  But it still needs to rewrite\n    the entire index file just to add a single filename, which is pretty\n    nasty; it should just leave the new files in a second \"extra index\" file\n    or something.\n   \n - bup could use inotify for *really* efficient incremental backups.\n\n    You could even have your system doing \"continuous\" backups: whenever a\n    file changes, we immediately send an image of it to the server.  We could\n    give the continuous-backup process a really low CPU and I/O priority so\n    you wouldn't even know it was running.\n\n - bup has never been tested on anything but Linux, FreeBSD, NetBSD,\n   OS X, and Windows+Cygwin.\n \n    There's nothing that makes it *inherently* non-portable, though, so\n    that's mostly a matter of someone putting in some effort.  (For a\n    \"native\" Windows port, the most annoying thing is the absence of ssh in\n    a default Windows installation.)\n    \n - bup needs better documentation.\n \n    According to an article about bup in Linux Weekly News\n    (https://lwn.net/Articles/380983/), \"it's a bit short on examples and\n    a user guide would be nice.\"  Documentation is the sort of thing that\n    will never be great unless someone from outside contributes it (since\n    the developers can never remember which parts are hard to understand).\n    \n - bup is \"relatively speedy\" and has \"pretty good\" compression.\n \n    ...according to the same LWN article.  Clearly neither of those is good\n    enough.  We should have awe-inspiring speed and crazy-good compression. \n    Must work on that.  Writing more parts in C might help with the speed.\n   \n - bup has no GUI.\n \n   Actually, that's not stupid, but you might consider it a\n   limitation.  See the [\"Related Projects\"](https://bup.github.io/)\n   list for some possible options.\n    \nMore Documentation\n==================\n\nbup has an extensive set of man pages.  Try using 'bup help' to get\nstarted, or use 'bup help SUBCOMMAND' for any bup subcommand (like split,\njoin, index, save, etc.) to get details on that command.\n\nFor further technical details, please see ./DESIGN.\n\n\nHow you can help\n================\n\nbup is a work in progress and there are many ways it can still be improved.\nIf you'd like to contribute patches, ideas, or bug reports, please join the\n<a href=\"mailto:bup-list@googlegroups.com\">bup mailing list</a>:\n\nYou can find the mailing list archives here:\n\n\thttp://groups.google.com/group/bup-list\n\t\nand you can subscribe by sending a message to:\n\n\tbup-list+subscribe@googlegroups.com\n\nYou can also reach us via the\n\\#bup IRC channel at ircs://irc.libera.chat:6697/bup\non the [libera.chat](https://libera.chat/) network or via this\n[web interface](https://web.libera.chat/?channels=bup).\n\nPlease see <a href=\"HACKING\">./HACKING</a> for\nadditional information, i.e. how to submit patches (hint - no pull\nrequests), how we handle branches, etc.\n\n\nHave fun,\n\nAvery\n\n<!--\nLocal Variables:\nmode: markdown\nEnd:\n-->\n"
        },
        {
          "name": "SIGNED-OFF-BY",
          "type": "blob",
          "size": 0.291015625,
          "content": "\nPatches to bup should have a Signed-off-by: header.  Including this\nheader in your patches signifies that you are licensing your changes\nunder the terms described in the LICENSE file residing in the\ntop-level directory of the source tree, the directory that also\ncontains this SIGNED-OFF-BY file.\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "bup",
          "type": "blob",
          "size": 0.0107421875,
          "content": "lib/cmd/bup"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "configure",
          "type": "blob",
          "size": 0.0498046875,
          "content": "#!/bin/sh\n\nset -e\n\ncd config\nexec ./configure \"$@\"\n"
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 3.0234375,
          "content": "\nfrom os.path import basename, dirname, realpath, relpath\nfrom time import tzset\nfrom traceback import extract_stack\nimport errno\nimport os\nimport pytest\nimport re\nimport subprocess\nimport sys\nimport tempfile\n\nsys.path[:0] = ['lib']\n\nfrom bup import helpers\nfrom bup.compat import environ, fsencode\n\n\n_bup_src_top = realpath(dirname(fsencode(__file__)))\n\n# The \"pwd -P\" here may not be appropriate in the long run, but we\n# need it until we settle the relevant drecurse/exclusion questions:\n# https://groups.google.com/forum/#!topic/bup-list/9ke-Mbp10Q0\nos.chdir(realpath(os.getcwd()))\n\n# Make the test results available to fixtures\n@pytest.hookimpl(tryfirst=True, hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    other_hooks = yield\n    report = other_hooks.get_result()\n    bup = item.__dict__.setdefault('bup', {})\n    bup[report.when + '-report'] = report  # setup, call, teardown\n    item.bup = bup\n\ndef bup_test_sort_order(item):\n    # Pull some slower tests forward to speed parallel runs\n    if item.fspath.basename in ('test_get.py', 'test-index.sh'):\n        return (0, str(item.fspath))\n    return (1, str(item.fspath))\n\ndef pytest_collection_modifyitems(session, config, items):\n    items.sort(key=bup_test_sort_order)\n\n@pytest.fixture(autouse=True)\ndef no_lingering_errors():\n    def fail_if_errors():\n        if helpers.saved_errors:\n            bt = extract_stack()\n            src_file, src_line, src_func, src_txt = bt[-4]\n            msg = 'saved_errors ' + repr(helpers.saved_errors)\n            assert False, '%s:%-4d %s' % (basename(src_file),\n                                          src_line, msg)\n\n    fail_if_errors()\n    helpers.clear_errors()\n    yield None\n    fail_if_errors()\n    helpers.clear_errors()\n\n@pytest.fixture(autouse=True)\ndef ephemeral_env_changes():\n    orig_env = environ.copy()\n    yield None\n    for k, orig_v in orig_env.items():\n        v = environ.get(k)\n        if v is not orig_v:\n            environ[k] = orig_v\n            if k == b'TZ':\n                tzset()\n    for k in environ.keys():\n        if k not in orig_env:\n            del environ[k]\n            if k == b'TZ':\n                tzset()\n    os.chdir(_bup_src_top)\n\n# Assumes (of course) this file is at the top-level of the source tree\n_bup_test_dir = realpath(dirname(fsencode(__file__))) + b'/test'\n_bup_tmp = _bup_test_dir + b'/tmp'\ntry:\n    os.makedirs(_bup_tmp)\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\n\n_safe_path_rx = re.compile(br'[^a-zA-Z0-9_-]')\n\n@pytest.fixture()\ndef tmpdir(request):\n    rp = realpath(fsencode(request.fspath))\n    rp = relpath(rp, _bup_test_dir)\n    if request.function:\n        rp += b'-' + fsencode(request.function.__name__)\n    safe = _safe_path_rx.sub(b'-', rp)\n    tmpdir = tempfile.mkdtemp(dir=_bup_tmp, prefix=safe)\n    yield tmpdir\n    if request.node.bup['call-report'].failed:\n        print('\\nPreserving:', b'test/' + relpath(tmpdir, _bup_test_dir),\n              file=sys.stderr)\n    else:\n        subprocess.call(['chmod', '-R', 'u+rwX', tmpdir])\n        subprocess.call(['rm', '-rf', tmpdir])\n"
        },
        {
          "name": "dev",
          "type": "tree",
          "content": null
        },
        {
          "name": "issue",
          "type": "tree",
          "content": null
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "note",
          "type": "tree",
          "content": null
        },
        {
          "name": "pylint",
          "type": "blob",
          "size": 1.0986328125,
          "content": "#!/usr/bin/env bash\n\n# Changes here might also be appropriate for ./pytest\n\nset -eu\n\nwith_pylint=$(cat config/config.var/with-pylint)\n\ncase \"$with_pylint\" in\n    yes) ;;\n    no)\n        echo \"./pylint: doing nothing given ./configure --with-pylint=no\" 1>&2\n        exit 0\n        ;;\n    maybe)\n        rc=0\n        dev/have-pylint || rc=$?\n        case \"$rc\" in\n            0) ;;\n            1)\n                echo \"./pylint: doing nothing (pylint not found)\" 1>&2\n                exit 0\n                ;;\n            *) exit \"$rc\" ;;\n        esac\n        ;;\n    *)\n        printf \"./pylint: unexpected config/config.var/with-pylint value %q\\n\" \\\n               \"$with_pylint\" 1>&2\n        exit 2\n        ;;\nesac\n\nscript_home=\"$(cd \"$(dirname \"$0\")\" && pwd -P)\"\ntestlibdir=\"$script_home/test/lib\"\n\nexport PYTHONPATH=\"$testlibdir${PYTHONPATH:+:$PYTHONPATH}\"\n\nif test \"$#\" -eq 0; then\n    set -x\n    dev/bup-python -m pylint lib\n    # unused-wildcard-import: we always \"import * from wvpytest\"\n    dev/bup-python -m pylint -d unused-wildcard-import test/lib test/int\nelse\n    set -x\n    exec dev/bup-python -m pylint \"$@\"\nfi\n"
        },
        {
          "name": "pytest",
          "type": "blob",
          "size": 1.197265625,
          "content": "#!/bin/sh\n\"\"\"\": # -*-python-*-\nset -eu\n\n# Changes here might also be appropriate for ./pylint\nscript_home=\"$(cd \"$(dirname \"$0\")\" && pwd -P)\"\ntestlibdir=\"$script_home/test/lib\"\n\nexport BUP_DIR=/dev/null\nexport GIT_DIR=/dev/null\n\nexport PYTHONPATH=\"$testlibdir${PYTHONPATH:+:$PYTHONPATH}\"\n\nexec dev/bup-python \"$0\" ${1+\"$@\"}\n\"\"\"\n\nimport pytest, shlex, sys\n\nargv = ['-v', '-m', 'not release']\n\n## Drop all xdist related opts if xdist isn't available.  Otherwise\n## default to worksteal if the version's new enough since it claims,\n## and appears, to handle test sets with more widely varying run times\n## better.\n\ntry:\n    import xdist\n    xdist_ver = xdist.__version__.split('.')\n    if xdist_ver >= ['3', '2', '1']: # #884: Fixed hang in worksteal scheduler\n        argv.extend(('--dist', 'worksteal'))\n    argv.extend(sys.argv[1:])\nexcept ModuleNotFoundError: # delete all -n opts\n    i = 1\n    while i < len(sys.argv):\n        arg = sys.argv[i]\n        if arg == '-n':\n            i += 2\n        elif arg.startswith('-n'):\n            i += 1\n        else:\n            argv.append(sys.argv[i])\n            i += 1\n\nprint(' '.join([shlex.quote(x) for x in ['pytest'] + argv]), file=sys.stderr)\nsys.exit(pytest.main(args=argv))\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.171875,
          "content": "\n[pytest]\n\n# See ./pytest for default arguments (instead of addopts)\n\ntestpaths = test/int test/ext\n\nmarkers =\n    release: tests to check that the tree is ready for a release\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "wvtest-bash.sh",
          "type": "blob",
          "size": 0.576171875,
          "content": "\ndeclare -a _wvbtstack\n\n_wvpushcall()\n{\n    _wvbtstack[${#_wvbtstack[@]}]=\"$*\"\n}\n\n_wvpopcall()\n{\n    unset _wvbtstack[$((${#_wvbtstack[@]} - 1))]\n}\n\n_wvbacktrace()\n{\n    local i loc\n    local call=$((${#_wvbtstack[@]} - 1))\n    for ((i=0; i <= ${#FUNCNAME[@]}; i++)); do\n\tlocal name=\"${FUNCNAME[$i]}\"\n\tif test \"${name:0:2}\" == WV; then\n            loc=\"${BASH_SOURCE[$i+1]}:${BASH_LINENO[$i]}\"\n\t    echo \"called from $loc ${FUNCNAME[$i]} ${_wvbtstack[$call]}\" 1>&2\n\t    ((call--))\n\tfi\n    done\n}\n\n_wvfind_caller()\n{\n    WVCALLER_FILE=${BASH_SOURCE[2]}\n    WVCALLER_LINE=${BASH_LINENO[1]}\n}\n"
        },
        {
          "name": "wvtest-bup.sh",
          "type": "blob",
          "size": 0.447265625,
          "content": "# Include in your test script like this:\n#\n#   #!/usr/bin/env bash\n#   . ./wvtest-bup.sh\n\n. ./wvtest.sh\n\n_wvtop=\"$(pwd -P)\"\n\nwvmktempdir ()\n{\n    local script_name=\"$(basename $0)\"\n    mkdir -p \"$_wvtop/test/tmp\" || exit $?\n    mktemp -d \"$_wvtop/test/tmp/$script_name-XXXXXXX\" || exit $?\n}\n\nwvmkmountpt ()\n{\n    local script_name=\"$(basename $0)\"\n    mkdir -p \"$_wvtop/test/mnt\" || exit $?\n    mktemp -d \"$_wvtop/test/mnt/$script_name-XXXXXXX\" || exit $?\n}\n"
        },
        {
          "name": "wvtest.sh",
          "type": "blob",
          "size": 2.498046875,
          "content": "#\n# Include this file in your shell script by using:\n#         #!/bin/sh\n#         . ./wvtest.sh\n#\n\n# Here just because it's already sourced \"everywhere\".\nexport BUP_DIR=/dev/null\nexport GIT_DIR=/dev/null\n\n# we don't quote $TEXT in case it contains newlines; newlines\n# aren't allowed in test output.  However, we set -f so that\n# at least shell glob characters aren't processed.\n_wvtextclean()\n{\n\t( set -f; echo $* )\n}\n\n\nif [ -n \"$BASH_VERSION\" ]; then\n\t. ./wvtest-bash.sh  # This keeps sh from choking on the syntax.\nelse\n\t_wvbacktrace() { true; }\n\t_wvpushcall() { true; }\n\t_wvpopcall() { true; }\n\n\t_wvfind_caller()\n\t{\n\t\tWVCALLER_FILE=\"unknown\"\n\t\tWVCALLER_LINE=0\n\t}\nfi\n\n\n_wvcheck()\n{\n\tlocal CODE=\"$1\"\n\tlocal TEXT=$(_wvtextclean \"$2\")\n\tlocal OK=ok\n\tif [ \"$CODE\" -ne 0 ]; then\n\t\tOK=FAILED\n\tfi\n\techo \"! $WVCALLER_FILE:$WVCALLER_LINE  $TEXT  $OK\" >&2\n\tif [ \"$CODE\" -ne 0 ]; then\n\t\t_wvbacktrace\n\t\texit $CODE\n\telse\n\t\treturn 0\n\tfi\n}\n\n\nWVPASS()\n{\n\tlocal TEXT=\"$*\"\n\t_wvpushcall \"$@\"\n\n\t_wvfind_caller\n\tif \"$@\"; then\n\t\t_wvpopcall\n\t\t_wvcheck 0 \"$TEXT\"\n\t\treturn 0\n\telse\n\t\t_wvcheck 1 \"$TEXT\"\n\t\t# NOTREACHED\n\t\treturn 1\n\tfi\n}\n\n\nWVFAIL()\n{\n\tlocal TEXT=\"$*\"\n\t_wvpushcall \"$@\"\n\n\t_wvfind_caller\n\tif \"$@\"; then\n\t\t_wvcheck 1 \"NOT($TEXT)\"\n\t\t# NOTREACHED\n\t\treturn 1\n\telse\n\t\t_wvcheck 0 \"NOT($TEXT)\"\n\t\t_wvpopcall\n\t\treturn 0\n\tfi\n}\n\n\n_wvgetrv()\n{\n\t( \"$@\" >&2 )\n\techo -n $?\n}\n\n\nWVPASSEQ()\n{\n\t_wvpushcall \"$@\"\n\t_wvfind_caller\n\t_wvcheck $(_wvgetrv [ \"$#\" -eq 2 ]) \"exactly 2 arguments\"\n\techo \"Comparing:\" >&2\n\techo \"$1\" >&2\n\techo \"--\" >&2\n\techo \"$2\" >&2\n\t_wvcheck $(_wvgetrv [ \"$1\" = \"$2\" ]) \"'$1' = '$2'\"\n\t_wvpopcall\n}\n\n\nWVPASSNE()\n{\n\t_wvpushcall \"$@\"\n\t_wvfind_caller\n\t_wvcheck $(_wvgetrv [ \"$#\" -eq 2 ]) \"exactly 2 arguments\"\n\techo \"Comparing:\" >&2\n\techo \"$1\" >&2\n\techo \"--\" >&2\n\techo \"$2\" >&2\n\t_wvcheck $(_wvgetrv [ \"$1\" != \"$2\" ]) \"'$1' != '$2'\"\n\t_wvpopcall\n}\n\n\nWVPASSRC()\n{\n\tlocal RC=$?\n\t_wvpushcall \"$@\"\n\t_wvfind_caller\n\t_wvcheck $(_wvgetrv [ $RC -eq 0 ]) \"return code($RC) == 0\"\n\t_wvpopcall\n}\n\n\nWVFAILRC()\n{\n\tlocal RC=$?\n\t_wvpushcall \"$@\"\n\t_wvfind_caller\n\t_wvcheck $(_wvgetrv [ $RC -ne 0 ]) \"return code($RC) != 0\"\n\t_wvpopcall\n}\n\n\nWVSTART()\n{\n\techo >&2\n\t_wvfind_caller\n\techo \"Testing \\\"$*\\\" in $WVCALLER_FILE:\" >&2\n}\n\n\nWVSKIP()\n{\n\tlocal TEXT=$(_wvtextclean \"$@\")\n\t_wvpushcall \"$@\"\n\t_wvfind_caller\n\techo \"! $WVCALLER_FILE:$WVCALLER_LINE  $TEXT  skip ok\" 1>&2\n}\n\n\nWVDIE()\n{\n\tlocal TEXT=$(_wvtextclean \"$@\")\n\t_wvpushcall \"$@\"\n\t_wvfind_caller\n\techo \"! $WVCALLER_FILE:$WVCALLER_LINE  $TEXT  FAILED\" 1>&2\n\texit 1\n}\n\n\n# Local Variables:\n# indent-tabs-mode: t\n# sh-basic-offset: 8\n# End:\n"
        }
      ]
    }
  ]
}