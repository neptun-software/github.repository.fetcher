{
  "metadata": {
    "timestamp": 1736561056715,
    "page": 839,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "arcee-ai/mergekit",
      "stars": 5083,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.005859375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.5,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n      - id: check-added-large-files\n      - id: check-yaml\n        args: [\"--allow-multiple-documents\"]\n  - repo: https://github.com/PyCQA/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n  - repo: https://github.com/psf/black\n    rev: 23.11.0\n    hooks:\n      - id: black\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 7.47265625,
          "content": "                   GNU LESSER GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n\n  This version of the GNU Lesser General Public License incorporates\nthe terms and conditions of version 3 of the GNU General Public\nLicense, supplemented by the additional permissions listed below.\n\n  0. Additional Definitions.\n\n  As used herein, \"this License\" refers to version 3 of the GNU Lesser\nGeneral Public License, and the \"GNU GPL\" refers to version 3 of the GNU\nGeneral Public License.\n\n  \"The Library\" refers to a covered work governed by this License,\nother than an Application or a Combined Work as defined below.\n\n  An \"Application\" is any work that makes use of an interface provided\nby the Library, but which is not otherwise based on the Library.\nDefining a subclass of a class defined by the Library is deemed a mode\nof using an interface provided by the Library.\n\n  A \"Combined Work\" is a work produced by combining or linking an\nApplication with the Library.  The particular version of the Library\nwith which the Combined Work was made is also called the \"Linked\nVersion\".\n\n  The \"Minimal Corresponding Source\" for a Combined Work means the\nCorresponding Source for the Combined Work, excluding any source code\nfor portions of the Combined Work that, considered in isolation, are\nbased on the Application, and not on the Linked Version.\n\n  The \"Corresponding Application Code\" for a Combined Work means the\nobject code and/or source code for the Application, including any data\nand utility programs needed for reproducing the Combined Work from the\nApplication, but excluding the System Libraries of the Combined Work.\n\n  1. Exception to Section 3 of the GNU GPL.\n\n  You may convey a covered work under sections 3 and 4 of this License\nwithout being bound by section 3 of the GNU GPL.\n\n  2. Conveying Modified Versions.\n\n  If you modify a copy of the Library, and, in your modifications, a\nfacility refers to a function or data to be supplied by an Application\nthat uses the facility (other than as an argument passed when the\nfacility is invoked), then you may convey a copy of the modified\nversion:\n\n   a) under this License, provided that you make a good faith effort to\n   ensure that, in the event an Application does not supply the\n   function or data, the facility still operates, and performs\n   whatever part of its purpose remains meaningful, or\n\n   b) under the GNU GPL, with none of the additional permissions of\n   this License applicable to that copy.\n\n  3. Object Code Incorporating Material from Library Header Files.\n\n  The object code form of an Application may incorporate material from\na header file that is part of the Library.  You may convey such object\ncode under terms of your choice, provided that, if the incorporated\nmaterial is not limited to numerical parameters, data structure\nlayouts and accessors, or small macros, inline functions and templates\n(ten or fewer lines in length), you do both of the following:\n\n   a) Give prominent notice with each copy of the object code that the\n   Library is used in it and that the Library and its use are\n   covered by this License.\n\n   b) Accompany the object code with a copy of the GNU GPL and this license\n   document.\n\n  4. Combined Works.\n\n  You may convey a Combined Work under terms of your choice that,\ntaken together, effectively do not restrict modification of the\nportions of the Library contained in the Combined Work and reverse\nengineering for debugging such modifications, if you also do each of\nthe following:\n\n   a) Give prominent notice with each copy of the Combined Work that\n   the Library is used in it and that the Library and its use are\n   covered by this License.\n\n   b) Accompany the Combined Work with a copy of the GNU GPL and this license\n   document.\n\n   c) For a Combined Work that displays copyright notices during\n   execution, include the copyright notice for the Library among\n   these notices, as well as a reference directing the user to the\n   copies of the GNU GPL and this license document.\n\n   d) Do one of the following:\n\n       0) Convey the Minimal Corresponding Source under the terms of this\n       License, and the Corresponding Application Code in a form\n       suitable for, and under terms that permit, the user to\n       recombine or relink the Application with a modified version of\n       the Linked Version to produce a modified Combined Work, in the\n       manner specified by section 6 of the GNU GPL for conveying\n       Corresponding Source.\n\n       1) Use a suitable shared library mechanism for linking with the\n       Library.  A suitable mechanism is one that (a) uses at run time\n       a copy of the Library already present on the user's computer\n       system, and (b) will operate properly with a modified version\n       of the Library that is interface-compatible with the Linked\n       Version.\n\n   e) Provide Installation Information, but only if you would otherwise\n   be required to provide such information under section 6 of the\n   GNU GPL, and only to the extent that such information is\n   necessary to install and execute a modified version of the\n   Combined Work produced by recombining or relinking the\n   Application with a modified version of the Linked Version. (If\n   you use option 4d0, the Installation Information must accompany\n   the Minimal Corresponding Source and Corresponding Application\n   Code. If you use option 4d1, you must provide the Installation\n   Information in the manner specified by section 6 of the GNU GPL\n   for conveying Corresponding Source.)\n\n  5. Combined Libraries.\n\n  You may place library facilities that are a work based on the\nLibrary side by side in a single library together with other library\nfacilities that are not Applications and are not covered by this\nLicense, and convey such a combined library under terms of your\nchoice, if you do both of the following:\n\n   a) Accompany the combined library with a copy of the same work based\n   on the Library, uncombined with any other library facilities,\n   conveyed under the terms of this License.\n\n   b) Give prominent notice with the combined library that part of it\n   is a work based on the Library, and explaining where to find the\n   accompanying uncombined form of the same work.\n\n  6. Revised Versions of the GNU Lesser General Public License.\n\n  The Free Software Foundation may publish revised and/or new versions\nof the GNU Lesser General Public License from time to time. Such new\nversions will be similar in spirit to the present version, but may\ndiffer in detail to address new problems or concerns.\n\n  Each version is given a distinguishing version number. If the\nLibrary as you received it specifies that a certain numbered version\nof the GNU Lesser General Public License \"or any later version\"\napplies to it, you have the option of following the terms and\nconditions either of that published version or of any later version\npublished by the Free Software Foundation. If the Library as you\nreceived it does not specify a version number of the GNU Lesser\nGeneral Public License, you may choose any version of the GNU Lesser\nGeneral Public License ever published by the Free Software Foundation.\n\n  If the Library as you received it specifies that a proxy can decide\nwhether future versions of the GNU Lesser General Public License shall\napply, that proxy's public statement of acceptance of any version is\npermanent authorization for you to choose that version for the\nLibrary.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 19.3662109375,
          "content": "# mergekit\n\n`mergekit` is a toolkit for merging pre-trained language models. `mergekit` uses an out-of-core approach to perform unreasonably elaborate merges in resource-constrained situations. Merges can be run entirely on CPU or accelerated with as little as 8 GB of VRAM. Many merging algorithms are supported, with more coming as they catch my attention.\n\n## Contents\n\n- [Why Merge Models?](#why-merge-models)\n- [Features](#features)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Merge Configuration](#merge-configuration)\n  - [Parameter Specification](#parameter-specification)\n  - [Tokenizer Configuration](#tokenizer-configuration)\n  - [Chat Template Configuration](#chat-template-configuration)\n  - [Examples](#examples)\n- [Merge Methods](#merge-methods)\n- [LoRA extraction](#lora-extraction)\n- [Mixture of Experts merging](#mixture-of-experts-merging)\n- [Evolutionary merge methods](#evolutionary-merge-methods)\n- [Merge in the Cloud](#-merge-in-the-cloud-)\n- [Citation](#citation)\n\n## Why Merge Models?\n\nModel merging is a powerful technique that allows combining the strengths of different models without the computational overhead of ensembling or the need for additional training. By operating directly in the weight space of models, merging can:\n\n- Combine multiple specialized models into a single versatile model\n- Transfer capabilities between models without access to training data\n- Find optimal trade-offs between different model behaviors\n- Improve performance while maintaining inference costs\n- Create new capabilities through creative model combinations\n\nUnlike traditional ensembling which requires running multiple models, merged models maintain the same inference cost as a single model while often achieving comparable or superior performance.\n\n## Features\n\nKey features of `mergekit` include:\n\n- Supports Llama, Mistral, GPT-NeoX, StableLM, and more\n- Many [merge methods](#merge-methods)\n- GPU or CPU execution\n- Lazy loading of tensors for low memory use\n- Interpolated gradients for parameter values (inspired by Gryphe's [BlockMerge_Gradient](https://github.com/Gryphe/BlockMerge_Gradient) script)\n- Piecewise assembly of language models from layers (\"Frankenmerging\")\n- [Mixture of Experts merging](#mixture-of-experts-merging)\n- [LORA extraction](#lora-extraction)\n- [Evolutionary merge methods](#evolutionary-merge-methods)\n\n🌐 GUI Launch Alert 🤗 - We are excited to announce the launch of a mega-GPU backed graphical user interface for mergekit in Arcee! This GUI simplifies the merging process, making it more accessible to a broader audience. Check it out and contribute at the [Arcee App](https://app.arcee.ai). There is also a [Hugging Face Space](https://huggingface.co/mergekit-community) with limited amounts of GPUs.\n\n## Installation\n\n```sh\ngit clone https://github.com/arcee-ai/mergekit.git\ncd mergekit\n\npip install -e .  # install the package and make scripts available\n```\n\nIf the above fails with the error of:\n\n```\nERROR: File \"setup.py\" or \"setup.cfg\" not found. Directory cannot be installed in editable mode:\n(A \"pyproject.toml\" file was found, but editable mode currently requires a setuptools-based build.)\n```\n\nYou may need to upgrade pip to > 21.3 with the command `python3 -m pip install --upgrade pip`\n\n## Usage\n\nThe script `mergekit-yaml` is the main entry point for `mergekit`. It takes a YAML configuration file and an output path, like so:\n\n```sh\nmergekit-yaml path/to/your/config.yml ./output-model-directory [--cuda] [--lazy-unpickle] [--allow-crimes] [... other options]\n```\n\nThis will run the merge and write your merged model to `./output-model-directory`.\n\nFor more information on the arguments accepted by `mergekit-yaml` run the command `mergekit-yaml --help`.\n\n### Uploading to Huggingface\n\nWhen you have a merged model you're happy with, you may want to share it on the Hugging Face Hub. `mergekit` generates a `README.md` for your merge with some basic information for a model card. You can edit it to include more details about your merge, like giving it a good name or explaining what it's good at; rewrite it entirely; or use the generated `README.md` as-is. It is also possible to edit your `README.md` online once it has been uploaded to the Hub.\n\nOnce you're happy with your model card and merged model, you can upload it to the Hugging Face Hub using the [huggingface_hub](https://huggingface.co/docs/huggingface_hub/index) Python library.\n\n```sh\n# log in to huggingface with an access token (must have write permission)\nhuggingface-cli login\n# upload your model\nhuggingface-cli upload your_hf_username/my-cool-model ./output-model-directory .\n```\n\nThe [documentation](https://huggingface.co/docs/huggingface_hub/guides/cli#huggingface-cli-upload) for `huggingface_hub` goes into more detail about other options for uploading.\n\n## Merge Configuration\n\nMerge configurations are YAML documents specifying the operations to perform in order to produce your merged model.\nBelow are the primary elements of a configuration file:\n\n- `merge_method`: Specifies the method to use for merging models. See [Merge Methods](#merge-methods) for a list.\n- `slices`: Defines slices of layers from different models to be used. This field is mutually exclusive with `models`.\n- `models`: Defines entire models to be used for merging. This field is mutually exclusive with `slices`.\n- `base_model`: Specifies the base model used in some merging methods.\n- `parameters`: Holds various parameters such as weights and densities, which can also be specified at different levels of the configuration.\n- `dtype`: Specifies the data type used for the merging operation.\n- `tokenizer` or `tokenizer_source`: Determines how to construct a tokenizer for the merged model.\n- `chat_template`: Specifies a chat template for the merged model.\n\n### Parameter Specification\n\nParameters are flexible and can be set with varying precedence. They can be specified conditionally using tensor name filters, which allows finer control such as differentiating between attention heads and fully connected layers.\n\nParameters can be specified as:\n\n- **Scalars**: Single floating-point values.\n- **Gradients**: List of floating-point values, specifying an interpolated gradient.\n\nThe parameters can be set at different levels, with decreasing precedence as follows:\n\n1. `slices.*.sources.parameters` - applying to a specific input slice\n2. `slices.*.parameters` - applying to a specific output slice\n3. `models.*.parameters` or `input_model_parameters` - applying to any tensors coming from specific input models\n4. `parameters` - catchall\n\n### Tokenizer Configuration\n\nThe tokenizer behavior can be configured in two ways: using the new `tokenizer` field (recommended) or the legacy `tokenizer_source` field (maintained for backward compatibility). These fields are mutually exclusive - you should use one or the other, not both.\n\n#### Modern Configuration (tokenizer)\n\nThe `tokenizer` field provides fine-grained control over vocabulary and embeddings:\n\n```yaml\ntokenizer:\n  source: \"union\"  # or \"base\" or a specific model path\n  tokens:          # Optional: configure specific tokens\n    <token_name>:\n      source: ...  # Specify embedding source\n      force: false # Optional: force this embedding for all models\n  pad_to_multiple_of: null  # Optional: pad vocabulary size\n```\n\n##### Tokenizer Source\n\nThe `source` field determines the vocabulary of the output model:\n\n- `union`: Combine vocabularies from all input models (default)\n- `base`: Use vocabulary from the base model\n- `\"path/to/model\"`: Use vocabulary from a specific model\n\n##### Token Embedding Handling\n\nWhen merging models with different vocabularies, mergekit uses smart defaults to handle token embeddings:\n\n- If a token exists in the base model, its embedding is used as the default\n- If only one model has the token, that model's embedding is used\n- Otherwise, an average of all available embeddings is used\n\nYou can override these defaults for specific tokens:\n\n```yaml\ntokenizer:\n  source: union\n  tokens:\n    # Use embedding from a specific model\n    <|im_start|>:\n      source: \"path/to/chatml/model\"\n\n    # Force a specific embedding for all models\n    <|special|>:\n      source: \"path/to/model\"\n      force: true\n\n    # Map a token to another model's token embedding\n    <|renamed_token|>:\n      source:\n        kind: \"model_token\"\n        model: \"path/to/model\"\n        token: \"<|original_token|>\"  # or use token_id: 1234\n```\n\n##### Practical Example\n\nHere's how you might preserve both Llama 3 Instruct and ChatML prompt formats when merging models:\n\n```yaml\ntokenizer:\n  source: union\n  tokens:\n    # ChatML tokens\n    <|im_start|>:\n      source: \"chatml_model\"\n    <|im_end|>:\n      source: \"chatml_model\"\n\n    # Llama 3 tokens - force original embeddings\n    <|start_header_id|>:\n      source: \"llama3_model\"\n      force: true\n    <|end_header_id|>:\n      source: \"llama3_model\"\n      force: true\n    <|eot_id|>:\n      source: \"llama3_model\"\n      force: true\n```\n\n#### Legacy Configuration (tokenizer_source)\n\nFor backward compatibility, the `tokenizer_source` field is still supported:\n\n```yaml\ntokenizer_source: \"union\"  # or \"base\" or a model path\n```\n\nThis provides basic tokenizer selection but lacks the fine-grained control of the modern `tokenizer` field.\n\n### Chat Template Configuration\n\nThe optional `chat_template` field allows overriding the chat template used for the merged model.\n\n```yaml\nchat_template: \"auto\"  # or a template name or Jinja2 template\n```\n\nOptions include:\n\n- `\"auto\"`: Automatically select the most common template among input models\n- Built-in templates: `\"alpaca\"`, `\"chatml\"`, `\"llama3\"`, `\"mistral\"`, `\"exaone\"`\n- A Jinja2 template string for custom formatting\n\n### Examples\n\nSeveral examples of merge configurations are available in [`examples/`](examples/).\n\n## Merge Methods\n\nA quick overview of the currently supported merge methods:\n\n| Method                                                                                           | `merge_method` value | Multi-Model | Uses base model |\n| ------------------------------------------------------------------------------------------------ | -------------------- | ----------- | --------------- |\n| Linear ([Model Soups](https://arxiv.org/abs/2203.05482))                                         | `linear`             | ✅          | ❌              |\n| SLERP                                                                                            | `slerp`              | ❌          | ✅              |\n| [Task Arithmetic](https://arxiv.org/abs/2212.04089)                                              | `task_arithmetic`    | ✅          | ✅              |\n| [TIES](https://arxiv.org/abs/2306.01708)                                                         | `ties`               | ✅          | ✅              |\n| [DARE](https://arxiv.org/abs/2311.03099) [TIES](https://arxiv.org/abs/2306.01708)                | `dare_ties`          | ✅          | ✅              |\n| [DARE](https://arxiv.org/abs/2311.03099) [Task Arithmetic](https://arxiv.org/abs/2212.04089)     | `dare_linear`        | ✅          | ✅              |\n| Passthrough                                                                                      | `passthrough`        | ❌          | ❌              |\n| [Model Breadcrumbs](https://arxiv.org/abs/2312.06795)                                            | `breadcrumbs`        | ✅          | ✅              |\n| [Model Breadcrumbs](https://arxiv.org/abs/2312.06795) + [TIES](https://arxiv.org/abs/2306.01708) | `breadcrumbs_ties`   | ✅          | ✅              |\n| [Model Stock](https://arxiv.org/abs/2403.19522)                                                  | `model_stock`        | ✅          | ✅              |\n| NuSLERP                                                                                          | `nuslerp`            | ❌          | ✅              |\n| [DELLA](https://arxiv.org/abs/2406.11617)                                                        | `della`              | ✅          | ✅              |\n| [DELLA](https://arxiv.org/abs/2406.11617) [Task Arithmetic](https://arxiv.org/abs/2212.04089)    | `della_linear`       | ✅          | ✅              |\n\n### Linear\n\nThe classic merge method - a simple weighted average.\n\nParameters:\n\n- `weight` - relative (or absolute if `normalize=False`) weighting of a given tensor\n- `normalize` - if true, the weights of all models contributing to a tensor will be normalized. Default behavior.\n\n### SLERP\n\nSpherically interpolate the parameters of two models. One must be set as `base_model`.\n\nParameters:\n\n- `t` - interpolation factor. At `t=0` will return `base_model`, at `t=1` will return the other one.\n\n### [Task Arithmetic](https://arxiv.org/abs/2212.04089)\n\nComputes \"task vectors\" for each model by subtracting a base model. Merges the task vectors linearly and adds back the base. Works great for models that were fine tuned from a common ancestor. Also a super useful mental framework for several of the more involved merge methods.\n\nParameters: same as [Linear](#linear)\n\n### [TIES](https://arxiv.org/abs/2306.01708)\n\nBuilds on the task arithmetic framework. Resolves interference between models by sparsifying the task vectors and applying a sign consensus algorithm. Allows you to merge a larger number of models and retain more of their strengths.\n\nParameters: same as [Linear](#linear), plus:\n\n- `density` - fraction of weights in differences from the base model to retain\n\n### [DARE](https://arxiv.org/abs/2311.03099)\n\nIn the same vein as TIES, sparsifies task vectors to reduce interference. Differs in that DARE uses random pruning with a novel rescaling to better match performance of the original models. DARE can be used either with the sign consensus algorithm of TIES (`dare_ties`) or without (`dare_linear`).\n\nParameters: same as [TIES](#ties) for `dare_ties`, or [Linear](#linear) for `dare_linear`\n\n### Passthrough\n\n`passthrough` is a no-op that simply passes input tensors through unmodified. It is meant to be used for layer-stacking type merges where you have only one input model. Useful for frankenmerging.\n\n### [Model Breadcrumbs](https://arxiv.org/abs/2312.06795)\n\nAn extension of task arithmetic that discards both small and extremely large differences from the base model. As with DARE, the Model Breadcrumbs algorithm can be used with (`breadcrumbs_ties`) or without (`breadcrumbs`) the sign consensus algorithm of TIES.\n\nParameters: same as [Linear](#linear), plus:\n\n- `density` - fraction of weights in differences from the base model to retain\n- `gamma` - fraction of largest magnitude differences to remove\n\nNote that `gamma` corresponds with the parameter `β` described in the paper, while `density` is the final density of the sparsified tensors (related to `γ` and `β` by `density = 1 - γ - β`). For good default values, try `density: 0.9` and `gamma: 0.01`.\n\n### [Model Stock](https://arxiv.org/abs/2403.19522)\n\nUses some neat geometric properties of fine tuned models to compute good weights for linear interpolation. Requires at least three models, including a base model.\n\nParameters:\n\n- `filter_wise`: if true, weight calculation will be per-row rather than per-tensor. Not recommended.\n\n### NuSLERP\n\nSpherically interpolate between parameters, but with more options and more sensical configuration! Does not require a base model, but can use one to do spherical interpolation of task vectors. Only works with either two models or two plus a base model.\n\nParameters:\n\n- `weight`: relative weighting of a given tensor\n- `nuslerp_flatten`: set to false to do row-wise/column-wise interpolation instead of treating tensors as vectors\n- `nuslerp_row_wise`: SLERP row vectors instead of column vectors\n\nTo replicate the behavior of the original `slerp` method, set `weight` to `1-t` and `t` for your first and second model respectively.\n\n### [DELLA](https://arxiv.org/abs/2406.11617)\n\nBuilding upon DARE, DELLA uses adaptive pruning based on parameter magnitudes. DELLA first ranks parameters in each row of delta parameters and assigns drop probabilities inversely proportional to their magnitudes. This allows it to retain more important changes while reducing interference. After pruning, it rescales the remaining parameters similar to [DARE](#dare). DELLA can be used with (`della`) or without (`della_linear`) the sign elect step of TIES\n\nParameters: same as [Linear](#linear), plus:\n\n- `density` - fraction of weights in differences from the base model to retain\n- `epsilon` - maximum change in drop probability based on magnitude. Drop probabilities assigned will range from `density - epsilon` to `density + epsilon`. (When selecting values for `density` and `epsilon`, ensure that the range of probabilities falls within 0 to 1)\n- `lambda` - scaling factor for the final merged delta parameters before merging with the base parameters.\n\n## LoRA extraction\n\nMergekit allows extracting PEFT-compatible low-rank approximations of finetuned models.\n\n### Usage\n\n```sh\nmergekit-extract-lora finetuned_model_id_or_path base_model_id_or_path output_path [--no-lazy-unpickle] --rank=desired_rank\n```\n\n## Mixture of Experts merging\n\nThe `mergekit-moe` script supports merging multiple dense models into a mixture of experts, either for direct use or for further training. For more details see the [`mergekit-moe` documentation](docs/moe.md).\n\n## Evolutionary merge methods\n\nSee [`docs/evolve.md`](docs/evolve.md) for details.\n\n## ✨ Merge in the Cloud ✨\n\nWe host merging on Arcee's cloud GPUs - you can launch a cloud merge in the [Arcee App](https://app.arcee.ai). Or through python - grab an ARCEE_API_KEY:\n\n`export ARCEE_API_KEY=<your-api-key>`\n`pip install -q arcee-py`\n\n```python\nimport arcee\narcee.merge_yaml(\"bio-merge\",\"./examples/bio-merge.yml\")\n```\n\nCheck your merge status at the [Arcee App](https://app.arcee.ai)\n\nWhen complete, either deploy your merge:\n\n```python\narcee.start_deployment(\"bio-merge\", merging=\"bio-merge\")\n```\n\nOr download your merge:\n\n`!arcee merging download bio-merge`\n\n## Citation\n\nIf you find `mergekit` useful in your research, please consider citing the [paper](https://aclanthology.org/2024.emnlp-industry.36/):\n\n```bibtex\n@inproceedings{goddard-etal-2024-arcees,\n    title = \"Arcee{'}s {M}erge{K}it: A Toolkit for Merging Large Language Models\",\n    author = \"Goddard, Charles  and\n      Siriwardhana, Shamane  and\n      Ehghaghi, Malikeh  and\n      Meyers, Luke  and\n      Karpukhin, Vladimir  and\n      Benedict, Brian  and\n      McQuade, Mark  and\n      Solawetz, Jacob\",\n    editor = \"Dernoncourt, Franck  and\n      Preo{\\c{t}}iuc-Pietro, Daniel  and\n      Shimorina, Anastasia\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, US\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-industry.36\",\n    doi = \"10.18653/v1/2024.emnlp-industry.36\",\n    pages = \"477--485\",\n    abstract = \"The rapid growth of open-source language models provides the opportunity to merge model checkpoints, combining their parameters to improve performance and versatility. Advances in transfer learning have led to numerous task-specific models, which model merging can integrate into powerful multitask models without additional training. MergeKit is an open-source library designed to support this process with an efficient and extensible framework suitable for any hardware. It has facilitated the merging of thousands of models, contributing to some of the world{'}s most powerful open-source model checkpoints. The library is accessible at: https://github.com/arcee-ai/mergekit.\",\n}\n```\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "mergekit",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebook.ipynb",
          "type": "blob",
          "size": 2.5380859375,
          "content": "{\n  \"cells\": [\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"collapsed\": true,\n        \"id\": \"cmjOVVtJdiPZ\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"!git clone https://github.com/cg123/mergekit.git\\n\",\n        \"%cd mergekit\\n\",\n        \"%pip install -e .\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": 2,\n      \"metadata\": {\n        \"id\": \"84cRJT6_ecbw\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"OUTPUT_PATH = \\\"./merged\\\"  # folder to store the result in\\n\",\n        \"LORA_MERGE_CACHE = \\\"/tmp\\\"  # change if you want to keep these for some reason\\n\",\n        \"CONFIG_YML = \\\"./examples/gradient-slerp.yml\\\"  # merge configuration file\\n\",\n        \"COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\\n\",\n        \"LAZY_UNPICKLE = False  # experimental low-memory model loader\\n\",\n        \"LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"6nw26xQLkBax\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"# actually do merge\\n\",\n        \"import torch\\n\",\n        \"import yaml\\n\",\n        \"\\n\",\n        \"from mergekit.config import MergeConfiguration\\n\",\n        \"from mergekit.merge import MergeOptions, run_merge\\n\",\n        \"\\n\",\n        \"with open(CONFIG_YML, \\\"r\\\", encoding=\\\"utf-8\\\") as fp:\\n\",\n        \"    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\\n\",\n        \"\\n\",\n        \"run_merge(\\n\",\n        \"    merge_config,\\n\",\n        \"    out_path=OUTPUT_PATH,\\n\",\n        \"    options=MergeOptions(\\n\",\n        \"        lora_merge_cache=LORA_MERGE_CACHE,\\n\",\n        \"        cuda=torch.cuda.is_available(),\\n\",\n        \"        copy_tokenizer=COPY_TOKENIZER,\\n\",\n        \"        lazy_unpickle=LAZY_UNPICKLE,\\n\",\n        \"        low_cpu_memory=LOW_CPU_MEMORY,\\n\",\n        \"    ),\\n\",\n        \")\\n\",\n        \"print(\\\"Done!\\\")\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"accelerator\": \"GPU\",\n    \"colab\": {\n      \"gpuType\": \"T4\",\n      \"provenance\": []\n    },\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"codemirror_mode\": {\n        \"name\": \"ipython\",\n        \"version\": 3\n      },\n      \"file_extension\": \".py\",\n      \"mimetype\": \"text/x-python\",\n      \"name\": \"python\",\n      \"nbconvert_exporter\": \"python\",\n      \"pygments_lexer\": \"ipython3\",\n      \"version\": \"3.10.13\"\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0\n}\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.28125,
          "content": "[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"mergekit\"\ndescription = \"Tools for merging pre-trained large language models\"\nreadme = \"README.md\"\nlicense = { text = \"LGPL-3.0-or-later\" }\nversion = \"0.0.5.2\"\nauthors = [{ name = \"Charles Goddard\", email = \"chargoddard@gmail.com\" }]\ndependencies = [\n    \"torch>=2.0.0\",\n    \"tqdm==4.66.5\",\n    \"click==8.1.7\",\n    \"safetensors~=0.4.3\",\n    \"accelerate~=1.0.1\",\n    \"pydantic~=2.9.2\",\n    \"immutables==0.20\",\n    \"transformers>=4.45.2\",\n    \"tokenizers>=0.20.1\",\n    \"huggingface_hub\",\n    \"peft\",\n    \"typing-extensions\",\n    \"sentencepiece\",\n    \"protobuf\",\n    \"scipy\",\n    \"datasets\",\n]\n\n[project.optional-dependencies]\ndev = [\"black~=24.10.0\", \"isort~=5.13.2\", \"pre-commit~=4.0.1\"]\ntest = [\"pytest~=8.3.3\"]\nevolve = [\"ray\", \"cma\", \"lm_eval\", \"wandb\"]\nvllm = [\"vllm==0.3.2\", \"lm_eval[vllm]\"]\n\n[project.urls]\nrepository = \"https://github.com/cg123/mergekit\"\n\n\n[project.scripts]\nmergekit-yaml = \"mergekit.scripts.run_yaml:main\"\nmergekit-mega = \"mergekit.scripts.megamerge:main\"\nmergekit-legacy = \"mergekit.scripts.legacy:main\"\nmergekit-layershuffle = \"mergekit.scripts.layershuffle:main\"\nbakllama = \"mergekit.scripts.bakllama:main\"\nmergekit-moe = \"mergekit.scripts.moe:main\"\nmergekit-tokensurgeon = \"mergekit.scripts.tokensurgeon:main\"\nmergekit-extract-lora = \"mergekit.scripts.extract_lora:main\"\nmergekit-evolve = \"mergekit.scripts.evolve:main\"\n\n[tool.setuptools]\npackages = [\n    \"mergekit\",\n    \"mergekit.io\",\n    \"mergekit.merge_methods\",\n    \"mergekit.moe\",\n    \"mergekit.scripts\",\n    \"mergekit.evo\",\n    \"mergekit.tokenizer\",\n    \"mergekit._data\",\n    \"mergekit._data.architectures\",\n    \"mergekit._data.chat_templates\",\n]\ninclude-package-data = true\npackage-data = { \"mergekit._data.architectures\" = [\n    \"*.json\",\n], \"mergekit._data.chat_templates\" = [\n    \"*.jinja\",\n] }\n\n[tool.isort]\nprofile = \"black\"\n\n[tool.black]\nline-length = 88\ntarget-version = ['py37']\ninclude = '\\.pyi?$'\n\n[tool.pytest.ini_options]\nminversion = \"6.0\"\nfilterwarnings = [\n    \"ignore::pydantic.PydanticDeprecatedSince20:huggingface_hub.*:\",\n    \"ignore::FutureWarning:huggingface_hub.*:\",\n    \"ignore:(read_text|open_text|contents|is_resource) is deprecated:DeprecationWarning\", # yes i know, but files() doesn't exist in 3.8\n]\ntestpaths = [\"tests\"]\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}