{
  "metadata": {
    "timestamp": 1736560690550,
    "page": 346,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "LiheYoung/Depth-Anything",
      "stars": 7194,
      "defaultBranch": "main",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.4736328125,
          "content": "<div align=\"center\">\n<h2>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</h2>\n\n[**Lihe Yang**](https://liheyoung.github.io/)<sup>1</sup> · [**Bingyi Kang**](https://scholar.google.com/citations?user=NmHgX-wAAAAJ)<sup>2&dagger;</sup> · [**Zilong Huang**](http://speedinghzl.github.io/)<sup>2</sup> · [**Xiaogang Xu**](https://xiaogang00.github.io/)<sup>3,4</sup> · [**Jiashi Feng**](https://sites.google.com/site/jshfeng/)<sup>2</sup> · [**Hengshuang Zhao**](https://hszhao.github.io/)<sup>1*</sup>\n\n<sup>1</sup>HKU&emsp;&emsp;&emsp;&emsp;<sup>2</sup>TikTok&emsp;&emsp;&emsp;&emsp;<sup>3</sup>CUHK&emsp;&emsp;&emsp;&emsp;<sup>4</sup>ZJU\n\n&dagger;project lead&emsp;*corresponding author\n\n**CVPR 2024**\n\n<a href=\"https://arxiv.org/abs/2401.10891\"><img src='https://img.shields.io/badge/arXiv-Depth Anything-red' alt='Paper PDF'></a>\n<a href='https://depth-anything.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything-green' alt='Project Page'></a>\n<a href='https://huggingface.co/spaces/LiheYoung/Depth-Anything'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a>\n<a href='https://huggingface.co/papers/2401.10891'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Paper-yellow'></a>\n</div>\n\nThis work presents Depth Anything, a highly practical solution for robust monocular depth estimation by training on a combination of 1.5M labeled images and **62M+ unlabeled images**.\n\n![teaser](assets/teaser.png)\n\n<div align=\"center\">\n    <a href=\"https://github.com/DepthAnything/Depth-Anything-V2\"><b>Try our latest Depth Anything V2 models!</b></a><br>\n</div>\n\n## News\n\n* **2024-06-14:** [Depth Anything V2](https://github.com/DepthAnything/Depth-Anything-V2) is released.\n* **2024-02-27:** Depth Anything is accepted by CVPR 2024.\n* **2024-02-05:** [Depth Anything Gallery](./gallery.md) is released. Thank all the users!\n* **2024-02-02:** Depth Anything serves as the default depth processor for [InstantID](https://github.com/InstantID/InstantID) and [InvokeAI](https://github.com/invoke-ai/InvokeAI/releases/tag/v3.6.1).\n* **2024-01-25:** Support [video depth visualization](./run_video.py). An [online demo for video](https://huggingface.co/spaces/JohanDL/Depth-Anything-Video) is also available.\n* **2024-01-23:** The new ControlNet based on Depth Anything is integrated into [ControlNet WebUI](https://github.com/Mikubill/sd-webui-controlnet) and [ComfyUI's ControlNet](https://github.com/Fannovel16/comfyui_controlnet_aux).\n* **2024-01-23:** Depth Anything [ONNX](https://github.com/fabio-sim/Depth-Anything-ONNX) and [TensorRT](https://github.com/spacewalk01/depth-anything-tensorrt) versions are supported.\n* **2024-01-22:** Paper, project page, code, models, and demo ([HuggingFace](https://huggingface.co/spaces/LiheYoung/Depth-Anything), [OpenXLab](https://openxlab.org.cn/apps/detail/yyfan/depth_anything)) are released.\n\n\n## Features of Depth Anything\n\n***If you need other features, please first check [existing community supports](#community-support).***\n\n- **Relative depth estimation**:\n    \n    Our foundation models listed [here](https://huggingface.co/spaces/LiheYoung/Depth-Anything/tree/main/checkpoints) can provide relative depth estimation for any given image robustly. Please refer [here](#running) for details.\n\n- **Metric depth estimation**\n\n    We fine-tune our Depth Anything model with metric depth information from NYUv2 or KITTI. It offers strong capabilities of both in-domain and zero-shot metric depth estimation. Please refer [here](./metric_depth) for details.\n\n\n- **Better depth-conditioned ControlNet**\n\n    We re-train **a better depth-conditioned ControlNet** based on Depth Anything. It offers more precise synthesis than the previous MiDaS-based ControlNet. Please refer [here](./controlnet/) for details. You can also use our new ControlNet based on Depth Anything in [ControlNet WebUI](https://github.com/Mikubill/sd-webui-controlnet) or [ComfyUI's ControlNet](https://github.com/Fannovel16/comfyui_controlnet_aux).\n\n- **Downstream high-level scene understanding**\n\n    The Depth Anything encoder can be fine-tuned to downstream high-level perception tasks, *e.g.*, semantic segmentation, 86.2 mIoU on Cityscapes and 59.4 mIoU on ADE20K. Please refer [here](./semseg/) for details.\n\n\n## Performance\n\nHere we compare our Depth Anything with the previously best MiDaS v3.1 BEiT<sub>L-512</sub> model.\n\nPlease note that the latest MiDaS is also trained on KITTI and NYUv2, while we do not.\n\n| Method | Params | KITTI || NYUv2 || Sintel || DDAD || ETH3D || DIODE ||\n|-|-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n| | | AbsRel | $\\delta_1$ | AbsRel | $\\delta_1$ | AbsRel | $\\delta_1$ | AbsRel | $\\delta_1$ | AbsRel | $\\delta_1$ | AbsRel | $\\delta_1$ |\n| MiDaS | 345.0M | 0.127 | 0.850 | 0.048 | *0.980* | 0.587 | 0.699 | 0.251 | 0.766 | 0.139 | 0.867 | 0.075 | 0.942 | \n| **Ours-S** | 24.8M | 0.080 | 0.936 | 0.053 | 0.972 | 0.464 | 0.739 | 0.247 | 0.768 | 0.127 | **0.885** | 0.076 | 0.939 |\n| **Ours-B** | 97.5M | *0.080* | *0.939* | *0.046* | 0.979 | **0.432** | *0.756* | *0.232* | *0.786* | **0.126** | *0.884* | *0.069* | *0.946* |\n| **Ours-L** | 335.3M | **0.076** | **0.947** | **0.043** | **0.981** | *0.458* | **0.760** | **0.230** | **0.789** | *0.127* | 0.882 | **0.066** | **0.952** |\n\nWe highlight the **best** and *second best* results in **bold** and *italic* respectively (**better results**: AbsRel $\\downarrow$ , $\\delta_1 \\uparrow$).\n\n## Pre-trained models\n\nWe provide three models of varying scales for robust relative depth estimation:\n\n| Model | Params | Inference Time on V100 (ms) | A100 | RTX4090 ([TensorRT](https://github.com/spacewalk01/depth-anything-tensorrt)) |\n|:-|-:|:-:|:-:|:-:|\n| Depth-Anything-Small | 24.8M | 12 | 8 | 3 |\n| Depth-Anything-Base | 97.5M | 13 | 9 | 6 |\n| Depth-Anything-Large | 335.3M | 20 | 13 | 12 |\n\nNote that the V100 and A100 inference time (*without TensorRT*) is computed by excluding the pre-processing and post-processing stages, whereas the last column RTX4090 (*with TensorRT*) is computed by including these two stages (please refer to [Depth-Anything-TensorRT](https://github.com/spacewalk01/depth-anything-tensorrt)).\n\nYou can easily load our pre-trained models by:\n```python\nfrom depth_anything.dpt import DepthAnything\n\nencoder = 'vits' # can also be 'vitb' or 'vitl'\ndepth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{:}14'.format(encoder))\n```\n\nDepth Anything is also supported in [``transformers``](https://github.com/huggingface/transformers). You can use it for depth prediction within [3 lines of code](https://huggingface.co/docs/transformers/main/model_doc/depth_anything) (credit to [@niels](https://huggingface.co/nielsr)).\n\n### *No network connection, cannot load these models?*\n\n<details>\n<summary>Click here for solutions</summary>\n\n- First, manually download the three checkpoints: [depth-anything-large](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints/depth_anything_vitl14.pth), [depth-anything-base](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints/depth_anything_vitb14.pth), and [depth-anything-small](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints/depth_anything_vits14.pth).\n\n- Second, upload the folder containing the checkpoints to your remote server.\n\n- Lastly, load the model locally:\n```python\nfrom depth_anything.dpt import DepthAnything\n\nmodel_configs = {\n    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]}\n}\n\nencoder = 'vitl' # or 'vitb', 'vits'\ndepth_anything = DepthAnything(model_configs[encoder])\ndepth_anything.load_state_dict(torch.load(f'./checkpoints/depth_anything_{encoder}14.pth'))\n```\nNote that in this locally loading manner, you also do not have to install the ``huggingface_hub`` package. In this way, please feel free to delete this [line](https://github.com/LiheYoung/Depth-Anything/blob/e7ef4b4b7a0afd8a05ce9564f04c1e5b68268516/depth_anything/dpt.py#L5) and the ``PyTorchModelHubMixin`` in this [line](https://github.com/LiheYoung/Depth-Anything/blob/e7ef4b4b7a0afd8a05ce9564f04c1e5b68268516/depth_anything/dpt.py#L169).\n</details>\n\n\n## Usage \n\n### Installation\n\n```bash\ngit clone https://github.com/LiheYoung/Depth-Anything\ncd Depth-Anything\npip install -r requirements.txt\n```\n\n### Running\n\n```bash\npython run.py --encoder <vits | vitb | vitl> --img-path <img-directory | single-img | txt-file> --outdir <outdir> [--pred-only] [--grayscale]\n```\nArguments:\n- ``--img-path``: you can either 1) point it to an image directory storing all interested images, 2) point it to a single image, or 3) point it to a text file storing all image paths.\n- ``--pred-only`` is set to save the predicted depth map only. Without it, by default, we visualize both image and its depth map side by side.\n- ``--grayscale`` is set to save the grayscale depth map. Without it, by default, we apply a color palette to the depth map.\n\nFor example:\n```bash\npython run.py --encoder vitl --img-path assets/examples --outdir depth_vis\n```\n\n**If you want to use Depth Anything on videos:**\n```bash\npython run_video.py --encoder vitl --video-path assets/examples_video --outdir video_depth_vis\n```\n\n### Gradio demo <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a> \n\nTo use our gradio demo locally:\n\n```bash\npython app.py\n```\n\nYou can also try our [online demo](https://huggingface.co/spaces/LiheYoung/Depth-Anything).\n\n### Import Depth Anything to your project\n\nIf you want to use Depth Anything in your own project, you can simply follow [``run.py``](run.py) to load our models and define data pre-processing. \n\n<details>\n<summary>Code snippet (note the difference between our data pre-processing and that of MiDaS)</summary>\n\n```python\nfrom depth_anything.dpt import DepthAnything\nfrom depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n\nimport cv2\nimport torch\nfrom torchvision.transforms import Compose\n\nencoder = 'vits' # can also be 'vitb' or 'vitl'\ndepth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{:}14'.format(encoder)).eval()\n\ntransform = Compose([\n    Resize(\n        width=518,\n        height=518,\n        resize_target=False,\n        keep_aspect_ratio=True,\n        ensure_multiple_of=14,\n        resize_method='lower_bound',\n        image_interpolation_method=cv2.INTER_CUBIC,\n    ),\n    NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    PrepareForNet(),\n])\n\nimage = cv2.cvtColor(cv2.imread('your image path'), cv2.COLOR_BGR2RGB) / 255.0\nimage = transform({'image': image})['image']\nimage = torch.from_numpy(image).unsqueeze(0)\n\n# depth shape: 1xHxW\ndepth = depth_anything(image)\n```\n</details>\n\n### Do not want to define image pre-processing or download model definition files?\n\nEasily use Depth Anything through [``transformers``](https://github.com/huggingface/transformers) within 3 lines of code! Please refer to [these instructions](https://huggingface.co/docs/transformers/main/model_doc/depth_anything) (credit to [@niels](https://huggingface.co/nielsr)).\n\n**Note:** If you encounter ``KeyError: 'depth_anything'``, please install the latest [``transformers``](https://github.com/huggingface/transformers) from source:\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n<details>\n<summary>Click here for a brief demo:</summary>\n\n```python\nfrom transformers import pipeline\nfrom PIL import Image\n\nimage = Image.open('Your-image-path')\npipe = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-small-hf\")\ndepth = pipe(image)[\"depth\"]\n```\n</details>\n\n## Community Support\n\n**We sincerely appreciate all the extensions built on our Depth Anything from the community. Thank you a lot!**\n\nHere we list the extensions we have found:\n- Depth Anything TensorRT: \n    - https://github.com/spacewalk01/depth-anything-tensorrt\n    - https://github.com/thinvy/DepthAnythingTensorrtDeploy\n    - https://github.com/daniel89710/trt-depth-anything\n- Depth Anything ONNX: https://github.com/fabio-sim/Depth-Anything-ONNX\n- Depth Anything in Transformers.js (3D visualization): https://huggingface.co/spaces/Xenova/depth-anything-web\n- Depth Anything for video (online demo): https://huggingface.co/spaces/JohanDL/Depth-Anything-Video\n- Depth Anything in ControlNet WebUI: https://github.com/Mikubill/sd-webui-controlnet\n- Depth Anything in ComfyUI's ControlNet: https://github.com/Fannovel16/comfyui_controlnet_aux\n- Depth Anything in X-AnyLabeling: https://github.com/CVHub520/X-AnyLabeling\n- Depth Anything in OpenXLab: https://openxlab.org.cn/apps/detail/yyfan/depth_anything\n- Depth Anything in OpenVINO: https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/280-depth-anything\n- Depth Anything ROS:\n    - https://github.com/scepter914/DepthAnything-ROS\n    - https://github.com/polatztrk/depth_anything_ros\n- Depth Anything Android:\n    - https://github.com/FeiGeChuanShu/ncnn-android-depth_anything\n    - https://github.com/shubham0204/Depth-Anything-Android\n- Depth Anything in TouchDesigner: https://github.com/olegchomp/TDDepthAnything\n- LearnOpenCV research article on Depth Anything: https://learnopencv.com/depth-anything\n- Learn more about the DPT architecture we used: https://github.com/heyoeyo/muggled_dpt\n- Depth Anything in NVIDIA Jetson Orin: https://github.com/ZhuYaoHui1998/jetson-examples/blob/main/reComputer/scripts/depth-anything\n\n\nIf you have your amazing projects supporting or improving (*e.g.*, speed) Depth Anything, please feel free to drop an issue. We will add them here.\n\n\n## Acknowledgement\n\nWe would like to express our deepest gratitude to [AK(@_akhaliq)](https://twitter.com/_akhaliq) and the awesome HuggingFace team ([@niels](https://huggingface.co/nielsr), [@hysts](https://huggingface.co/hysts), and [@yuvraj](https://huggingface.co/ysharma)) for helping improve the online demo and build the HF models.\n\nBesides, we thank the [MagicEdit](https://magic-edit.github.io/) team for providing some video examples for video depth estimation, and [Tiancheng Shen](https://scholar.google.com/citations?user=iRY1YVoAAAAJ) for evaluating the depth maps with MagicEdit.\n\n## Citation\n\nIf you find this project useful, please consider citing:\n\n```bibtex\n@inproceedings{depthanything,\n      title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}, \n      author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},\n      booktitle={CVPR},\n      year={2024}\n}\n```\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 3.310546875,
          "content": "import gradio as gr\nimport cv2\nimport numpy as np\nimport os\nfrom PIL import Image\nimport torch\nimport torch.nn.functional as F\nfrom torchvision.transforms import Compose\nimport tempfile\nfrom gradio_imageslider import ImageSlider\n\nfrom depth_anything.dpt import DepthAnything\nfrom depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n\ncss = \"\"\"\n#img-display-container {\n    max-height: 100vh;\n    }\n#img-display-input {\n    max-height: 80vh;\n    }\n#img-display-output {\n    max-height: 80vh;\n    }\n\"\"\"\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = DepthAnything.from_pretrained('LiheYoung/depth_anything_vitl14').to(DEVICE).eval()\n\ntitle = \"# Depth Anything\"\ndescription = \"\"\"Official demo for **Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data**.\n\nPlease refer to our [paper](https://arxiv.org/abs/2401.10891), [project page](https://depth-anything.github.io), or [github](https://github.com/LiheYoung/Depth-Anything) for more details.\"\"\"\n\ntransform = Compose([\n        Resize(\n            width=518,\n            height=518,\n            resize_target=False,\n            keep_aspect_ratio=True,\n            ensure_multiple_of=14,\n            resize_method='lower_bound',\n            image_interpolation_method=cv2.INTER_CUBIC,\n        ),\n        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        PrepareForNet(),\n])\n\n@torch.no_grad()\ndef predict_depth(model, image):\n    return model(image)\n\nwith gr.Blocks(css=css) as demo:\n    gr.Markdown(title)\n    gr.Markdown(description)\n    gr.Markdown(\"### Depth Prediction demo\")\n    gr.Markdown(\"You can slide the output to compare the depth prediction with input image\")\n\n    with gr.Row():\n        input_image = gr.Image(label=\"Input Image\", type='numpy', elem_id='img-display-input')\n        depth_image_slider = ImageSlider(label=\"Depth Map with Slider View\", elem_id='img-display-output', position=0.5)\n    raw_file = gr.File(label=\"16-bit raw depth (can be considered as disparity)\")\n    submit = gr.Button(\"Submit\")\n\n    def on_submit(image):\n        original_image = image.copy()\n\n        h, w = image.shape[:2]\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n        image = transform({'image': image})['image']\n        image = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n\n        depth = predict_depth(model, image)\n        depth = F.interpolate(depth[None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n\n        raw_depth = Image.fromarray(depth.cpu().numpy().astype('uint16'))\n        tmp = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n        raw_depth.save(tmp.name)\n\n        depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n        depth = depth.cpu().numpy().astype(np.uint8)\n        colored_depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)[:, :, ::-1]\n\n        return [(original_image, colored_depth), tmp.name]\n\n    submit.click(on_submit, inputs=[input_image], outputs=[depth_image_slider, raw_file])\n\n    example_files = os.listdir('assets/examples')\n    example_files.sort()\n    example_files = [os.path.join('assets/examples', filename) for filename in example_files]\n    examples = gr.Examples(examples=example_files, inputs=[input_image], outputs=[depth_image_slider, raw_file], fn=on_submit, cache_examples=False)\n    \n\nif __name__ == '__main__':\n    demo.queue().launch()"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "controlnet",
          "type": "tree",
          "content": null
        },
        {
          "name": "depth_anything",
          "type": "tree",
          "content": null
        },
        {
          "name": "gallery.md",
          "type": "blob",
          "size": 5.2470703125,
          "content": "# $Depth$ $Anything$ ${\\color{crimson}G\\color{coral}a\\color{royalblue}l\\color{olive}l\\color{teal}e\\color{navy}r\\color{plum}y}$\n\n\n\nHere we exhibit awesome community showcases of Depth Anything. Thank all the users for sharing them on the Internet (mainly from Twitter).\n\nWe organize these cases into three groups: [**image**](#image), [**video**](#video), and [**3D**](#3d).\n\n\n## Image\n\nYou can click on the titles below to be directed to corresponding source pages.\n\n### [Monument Valley](https://twitter.com/weebney/status/1749541957108441309)\n\n<img src=\"assets/gallery/monument_valley.jpg\" width=\"60%\"/>\n\n### [Cyber rabbit monitoring screens](https://twitter.com/hayas1357/status/1749298607260316139)\n\n<img src=\"assets/gallery/cyber_rabbit.jpg\" width=\"60%\"/>\n\n### [Astronaut cat](https://twitter.com/nanase_ja/status/1749653152406884392)\n\n<img src=\"assets/gallery/astronaut_cat.jpg\" width=\"60%\"/>\n\n### [Animation images](https://twitter.com/PlayShingo/status/1750368475867128200)\n\n<img src=\"assets/gallery/animation_image.jpg\" width=\"90%\"/>\n\n### [DALL·E bear](https://twitter.com/letalvoj/status/1749341999646347741)\n\n<img src=\"assets/gallery/dalle_bear.jpg\" width=\"60%\"/>\n\n### [Cat](https://twitter.com/sajilobroker/status/1749364184419016846)\n\n<img src=\"assets/gallery/cat.jpg\" width=\"60%\"/>\n\n### [Surprised bald man](https://twitter.com/mayfer/status/1749712454408679780)\n\n<img src=\"assets/gallery/surprised_bald_man.jpg\" width=\"60%\"/>\n\n### [Minecraft](https://twitter.com/BarlowTwin/status/1749353070008693224)\n\n<img src=\"assets/gallery/minecraft.jpg\" width=\"90%\"/>\n\n### [Robotic knight amidst lightning](https://twitter.com/IterIntellectus/status/1749432836158021738)\n\n<img src=\"assets/gallery/robotic_knight.jpg\" width=\"45%\"/>\n\n### [Football game](https://twitter.com/AB9Mamun/status/1751202608545456235)\n\n<img src=\"assets/gallery/football_game.jpg\" width=\"60%\"/>\n\n### [Classical raft painting](https://twitter.com/acidbjazz/status/1749491155698331774)\n\n<img src=\"assets/gallery/raft_painting.jpg\" width=\"60%\"/>\n\n### [Diner scene](https://twitter.com/R0b0tSp1der/status/1749301061964435846)\n\n<img src=\"assets/gallery/diner_scene.jpg\" width=\"60%\"/>\n\n### [Elon Musk](https://twitter.com/ai_for_success/status/1749304903418482954)\n\n<img src=\"assets/gallery/elon_musk.jpg\" width=\"60%\"/>\n\n### [Painted tunnel](https://twitter.com/NodiMend/status/1750800040304492814)\n\n<img src=\"assets/gallery/painted_tunnel.jpg\" width=\"40%\"/>\n\n### [Iron man](https://twitter.com/ai_for_success/status/1749304906664808751)\n\n<img src=\"assets/gallery/iron_man.jpg\" width=\"60%\"/>\n\n### [Skull](https://twitter.com/ai_for_success/status/1749304909730906381)\n\n<img src=\"assets/gallery/skull.jpg\" width=\"60%\"/>\n\n### [Chibi cat-eared character](https://twitter.com/nanase_ja/status/1749484958522204605)\n\n<img src=\"assets/gallery/chibi_cateared_character.jpg\" width=\"60%\"/>\n\n### [Exuberant gamer celebration](https://twitter.com/hmaon/status/1749372352016625748)\n\n<img src=\"assets/gallery/gamer_celebration.jpg\" width=\"60%\"/>\n\n### [Ocean](https://twitter.com/jarrahorphin/status/1749878678111309870)\n\n<img src=\"assets/gallery/ocean.jpg\" width=\"60%\"/>\n\n### [Aerial images](https://twitter.com/lTlanual/status/1749641678124892384)\n\n<img src=\"assets/gallery/aerial_image.jpg\" width=\"60%\"/>\n\n### [Grilled chicken skewers](https://twitter.com/promptlord/status/1752323556409856157)\n\n<img src=\"assets/gallery/grilled_chicken_skewers.jpg\" width=\"60%\"/>\n\n### [Artistic images](https://twitter.com/ZainHasan6/status/1753553755998416933)\n\n<img src=\"assets/gallery/artistic_image.jpg\" width=\"90%\"/>\n\n### [Iconic distracted man](https://twitter.com/ZainHasan6/status/1749308193237303620)\n\n<img src=\"assets/gallery/distracted_man.jpg\" width=\"60%\"/>\n\n### [Eye-stalked](https://twitter.com/RJdoesVR/status/1749494967800590780)\n\n<img src=\"assets/gallery/eye-stalked.jpg\" width=\"60%\"/>\n\n### [Tearful green frog](https://twitter.com/qsdnl/status/1749298425064313080)\n\n<img src=\"assets/gallery/tearful_green_frog.jpg\" width=\"60%\"/>\n\n\n## Video\n\nFor more online showcases, please refer to https://twitter.com/WilliamLamkin/status/1755623301907460582.\n\nThe videos below may be slow to load. Please wait a moment.\n\n### [Racing game](https://twitter.com/i/status/1750683014152040853)\n\n<img src=\"assets/gallery/racing_car.gif\" width=\"80%\"/>\n\n### [Building](https://twitter.com/WayneINR/status/1750945037863551247)\n\n<img src=\"assets/gallery/building.gif\" width=\"80%\"/>\n\n### [nuScenes](https://github.com/scepter914/DepthAnything-ROS)\n\n<img src=\"assets/gallery/nuscenes.gif\" width=\"80%\"/>\n\n### [Indoor moving](https://twitter.com/PINTO03091/status/1750162506453041437)\n\n<img src=\"assets/gallery/indoor_moving.gif\" width=\"40%\"/>\n\n\n## 3D\n\nThe videos below may be slow to load. Please wait a moment.\n\n### [3D visualization](https://twitter.com/victormustar/status/1753008143469093212)\n\n<img src=\"assets/gallery/3d_vis1.gif\" width=\"50%\"/><br><br>\n<img src=\"assets/gallery/3d_vis2.gif\" width=\"50%\"/>\n\n### [2D videos to 3D videos](https://twitter.com/stspanho/status/1751709292913143895)\n\n<img src=\"assets/gallery/3d_video.gif\" width=\"60%\"/>\n\n### Reconstruction\n\n- [case1](https://twitter.com/Artoid_XYZ/status/1751542601772421378)\n\n<img src=\"assets/gallery/reconstruction2.jpeg\" width=\"60%\"/>\n\n- [case2](https://twitter.com/DennisLoevlie/status/1753846358463709489)\n\n<img src=\"assets/gallery/reconstruction.jpg\" width=\"60%\"/>\n\n"
        },
        {
          "name": "metric_depth",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0791015625,
          "content": "gradio_imageslider\ngradio==4.14.0\ntorch\ntorchvision\nopencv-python\nhuggingface_hub"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 4.2451171875,
          "content": "import argparse\nimport cv2\nimport numpy as np\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom torchvision.transforms import Compose\nfrom tqdm import tqdm\n\nfrom depth_anything.dpt import DepthAnything\nfrom depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--img-path', type=str)\n    parser.add_argument('--outdir', type=str, default='./vis_depth')\n    parser.add_argument('--encoder', type=str, default='vitl', choices=['vits', 'vitb', 'vitl'])\n    \n    parser.add_argument('--pred-only', dest='pred_only', action='store_true', help='only display the prediction')\n    parser.add_argument('--grayscale', dest='grayscale', action='store_true', help='do not apply colorful palette')\n    \n    args = parser.parse_args()\n    \n    margin_width = 50\n    caption_height = 60\n    \n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 1\n    font_thickness = 2\n    \n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{}14'.format(args.encoder)).to(DEVICE).eval()\n    \n    total_params = sum(param.numel() for param in depth_anything.parameters())\n    print('Total parameters: {:.2f}M'.format(total_params / 1e6))\n    \n    transform = Compose([\n        Resize(\n            width=518,\n            height=518,\n            resize_target=False,\n            keep_aspect_ratio=True,\n            ensure_multiple_of=14,\n            resize_method='lower_bound',\n            image_interpolation_method=cv2.INTER_CUBIC,\n        ),\n        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        PrepareForNet(),\n    ])\n    \n    if os.path.isfile(args.img_path):\n        if args.img_path.endswith('txt'):\n            with open(args.img_path, 'r') as f:\n                filenames = f.read().splitlines()\n        else:\n            filenames = [args.img_path]\n    else:\n        filenames = os.listdir(args.img_path)\n        filenames = [os.path.join(args.img_path, filename) for filename in filenames if not filename.startswith('.')]\n        filenames.sort()\n    \n    os.makedirs(args.outdir, exist_ok=True)\n    \n    for filename in tqdm(filenames):\n        raw_image = cv2.imread(filename)\n        image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB) / 255.0\n        \n        h, w = image.shape[:2]\n        \n        image = transform({'image': image})['image']\n        image = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n        \n        with torch.no_grad():\n            depth = depth_anything(image)\n        \n        depth = F.interpolate(depth[None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n        depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n        \n        depth = depth.cpu().numpy().astype(np.uint8)\n        \n        if args.grayscale:\n            depth = np.repeat(depth[..., np.newaxis], 3, axis=-1)\n        else:\n            depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n        \n        filename = os.path.basename(filename)\n        \n        if args.pred_only:\n            cv2.imwrite(os.path.join(args.outdir, filename[:filename.rfind('.')] + '_depth.png'), depth)\n        else:\n            split_region = np.ones((raw_image.shape[0], margin_width, 3), dtype=np.uint8) * 255\n            combined_results = cv2.hconcat([raw_image, split_region, depth])\n            \n            caption_space = np.ones((caption_height, combined_results.shape[1], 3), dtype=np.uint8) * 255\n            captions = ['Raw image', 'Depth Anything']\n            segment_width = w + margin_width\n            \n            for i, caption in enumerate(captions):\n                # Calculate text size\n                text_size = cv2.getTextSize(caption, font, font_scale, font_thickness)[0]\n\n                # Calculate x-coordinate to center the text\n                text_x = int((segment_width * i) + (w - text_size[0]) / 2)\n\n                # Add text caption\n                cv2.putText(caption_space, caption, (text_x, 40), font, font_scale, (0, 0, 0), font_thickness)\n            \n            final_result = cv2.vconcat([caption_space, combined_results])\n            \n            cv2.imwrite(os.path.join(args.outdir, filename[:filename.rfind('.')] + '_img_depth.png'), final_result)\n        "
        },
        {
          "name": "run_video.py",
          "type": "blob",
          "size": 3.5791015625,
          "content": "import argparse\nimport cv2\nimport numpy as np\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom torchvision.transforms import Compose\n\nfrom depth_anything.dpt import DepthAnything\nfrom depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--video-path', type=str)\n    parser.add_argument('--outdir', type=str, default='./vis_video_depth')\n    parser.add_argument('--encoder', type=str, default='vitl', choices=['vits', 'vitb', 'vitl'])\n    \n    args = parser.parse_args()\n    \n    margin_width = 50\n\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{}14'.format(args.encoder)).to(DEVICE).eval()\n    \n    total_params = sum(param.numel() for param in depth_anything.parameters())\n    print('Total parameters: {:.2f}M'.format(total_params / 1e6))\n    \n    transform = Compose([\n        Resize(\n            width=518,\n            height=518,\n            resize_target=False,\n            keep_aspect_ratio=True,\n            ensure_multiple_of=14,\n            resize_method='lower_bound',\n            image_interpolation_method=cv2.INTER_CUBIC,\n        ),\n        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        PrepareForNet(),\n    ])\n\n    if os.path.isfile(args.video_path):\n        if args.video_path.endswith('txt'):\n            with open(args.video_path, 'r') as f:\n                lines = f.read().splitlines()\n        else:\n            filenames = [args.video_path]\n    else:\n        filenames = os.listdir(args.video_path)\n        filenames = [os.path.join(args.video_path, filename) for filename in filenames if not filename.startswith('.')]\n        filenames.sort()\n    \n    os.makedirs(args.outdir, exist_ok=True)\n    \n    for k, filename in enumerate(filenames):\n        print('Progress {:}/{:},'.format(k+1, len(filenames)), 'Processing', filename)\n        \n        raw_video = cv2.VideoCapture(filename)\n        frame_width, frame_height = int(raw_video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(raw_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        frame_rate = int(raw_video.get(cv2.CAP_PROP_FPS))\n        output_width = frame_width * 2 + margin_width\n        \n        filename = os.path.basename(filename)\n        output_path = os.path.join(args.outdir, filename[:filename.rfind('.')] + '_video_depth.mp4')\n        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), frame_rate, (output_width, frame_height))\n        \n        while raw_video.isOpened():\n            ret, raw_frame = raw_video.read()\n            if not ret:\n                break\n            \n            frame = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2RGB) / 255.0\n            \n            frame = transform({'image': frame})['image']\n            frame = torch.from_numpy(frame).unsqueeze(0).to(DEVICE)\n            \n            with torch.no_grad():\n                depth = depth_anything(frame)\n\n            depth = F.interpolate(depth[None], (frame_height, frame_width), mode='bilinear', align_corners=False)[0, 0]\n            depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n            \n            depth = depth.cpu().numpy().astype(np.uint8)\n            depth_color = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n            \n            split_region = np.ones((frame_height, margin_width, 3), dtype=np.uint8) * 255\n            combined_frame = cv2.hconcat([raw_frame, split_region, depth_color])\n            \n            out.write(combined_frame)\n        \n        raw_video.release()\n        out.release()\n"
        },
        {
          "name": "semseg",
          "type": "tree",
          "content": null
        },
        {
          "name": "torchhub",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}