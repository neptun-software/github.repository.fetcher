{
  "metadata": {
    "timestamp": 1736560930896,
    "page": 666,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dpkp/kafka-python",
      "stars": 5663,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".covrc",
          "type": "blob",
          "size": 0.03125,
          "content": "[run]\nomit =\n    kafka/vendor/*\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1787109375,
          "content": "*.egg-info\n*.pyc\n.tox\nbuild\ndist\nMANIFEST\nenv\nservers/*/kafka-bin*\nservers/*/resources/ssl*\n.coverage*\n.noseids\ndocs/_build\n.cache*\n.idea/\nintegration-test/\ntests-env/\n.pytest_cache/\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.7685546875,
          "content": "language: python\n\ndist: xenial\n\npython:\n    - 2.7\n    - 3.4\n    - 3.7\n    - 3.8\n    - pypy2.7-6.0\n\nenv:\n    - KAFKA_VERSION=0.8.2.2\n    - KAFKA_VERSION=0.9.0.1\n    - KAFKA_VERSION=0.10.2.2\n    - KAFKA_VERSION=0.11.0.3\n    - KAFKA_VERSION=1.1.1\n    - KAFKA_VERSION=2.4.0\n    - KAFKA_VERSION=2.5.0\n    - KAFKA_VERSION=2.6.0\n\naddons:\n  apt:\n    packages:\n      - libsnappy-dev\n      - libzstd-dev\n      - openjdk-8-jdk\n\ncache:\n  directories:\n    - $HOME/.cache/pip\n    - servers/dist\n\nbefore_install:\n    - source travis_java_install.sh\n    - ./build_integration.sh\n\ninstall:\n    - pip install tox coveralls\n    - pip install .\n\nscript:\n  - tox -e `if [ \"$TRAVIS_PYTHON_VERSION\" == \"pypy2.7-6.0\" ]; then echo pypy; else echo py${TRAVIS_PYTHON_VERSION/./}; fi`\n\nafter_success:\n  - coveralls\n"
        },
        {
          "name": "AUTHORS.md",
          "type": "blob",
          "size": 2.5810546875,
          "content": "# Current Maintainer\n* Dana Powers, [@dpkp](https://github.com/dpkp)\n\n# Original Author and First Commit\n* David Arthur, [@mumrah](https://github.com/mumrah)\n\n# Contributors - 2015 (alpha by username)\n* Alex Couture-Beil, [@alexcb](https://github.com/alexcb)\n* Ali-Akber Saifee, [@alisaifee](https://github.com/alisaifee)\n* Christophe-Marie Duquesne, [@chmduquesne](https://github.com/chmduquesne)\n* Thomas Dimson, [@cosbynator](https://github.com/cosbynator)\n* Kasper Jacobsen, [@Dinoshauer](https://github.com/Dinoshauer)\n* Ross Duggan, [@duggan](https://github.com/duggan)\n* Enrico Canzonieri, [@ecanzonieri](https://github.com/ecanzonieri)\n* haosdent, [@haosdent](https://github.com/haosdent)\n* Arturo Filastò, [@hellais](https://github.com/hellais)\n* Job Evers‐Meltzer, [@jobevers](https://github.com/jobevers)\n* Martin Olveyra, [@kalessin](https://github.com/kalessin)\n* Kubilay Kocak, [@koobs](https://github.com/koobs)\n* Matthew L Daniel <mdaniel@gmail.com>\n* Eric Hewitt, [@meandthewallaby](https://github.com/meandthewallaby)\n* Oliver Jowett [@mutability](https://github.com/mutability)\n* Shaolei Zhou, [@reAsOn2010](https://github.com/reAsOn2010)\n* Oskari Saarenmaa, [@saaros](https://github.com/saaros)\n* John Anderson, [@sontek](https://github.com/sontek)\n* Eduard Iskandarov, [@toidi](https://github.com/toidi)\n* Todd Palino, [@toddpalino](https://github.com/toddpalino)\n* trbs, [@trbs](https://github.com/trbs)\n* Viktor Shlapakov, [@vshlapakov](https://github.com/vshlapakov)\n* Will Daly, [@wedaly](https://github.com/wedaly)\n* Warren Kiser, [@wkiser](https://github.com/wkiser)\n* William Ting, [@wting](https://github.com/wting)\n* Zack Dever, [@zackdever](https://github.com/zackdever)\n\n# More Contributors\n* Bruno Renié, [@brutasse](https://github.com/brutasse)\n* Thomas Dimson, [@cosbynator](https://github.com/cosbynator)\n* Jesse Myers, [@jessemyers](https://github.com/jessemyers)\n* Mahendra M, [@mahendra](https://github.com/mahendra)\n* Miguel Eduardo Gil Biraud, [@mgilbir](https://github.com/mgilbir)\n* Marc Labbé, [@mrtheb](https://github.com/mrtheb)\n* Patrick Lucas, [@patricklucas](https://github.com/patricklucas)\n* Omar Ghishan, [@rdiomar](https://github.com/rdiomar) - RIP, Omar. 2014\n* Ivan Pouzyrevsky, [@sandello](https://github.com/sandello)\n* Lou Marvin Caraig, [@se7entyse7en](https://github.com/se7entyse7en)\n* waliaashish85, [@waliaashish85](https://github.com/waliaashish85)\n* Mark Roberts, [@wizzat](https://github.com/wizzat)\n* Christophe Lecointe [@christophelec](https://github.com/christophelec)\n* Mohamed Helmi Hichri [@hellich](https://github.com/hellich)\n\nThanks to all who have contributed!\n"
        },
        {
          "name": "CHANGES.md",
          "type": "blob",
          "size": 55.78515625,
          "content": "# 2.0.2 (Sep 29, 2020)\n\nConsumer\n* KIP-54: Implement sticky partition assignment strategy (aynroot / PR #2057)\n* Fix consumer deadlock when heartbeat thread request timeout (huangcuiyang / PR #2064)\n\nCompatibility\n* Python 3.8 support (Photonios / PR #2088)\n\nCleanups\n* Bump dev requirements (jeffwidman / PR #2129)\n* Fix crc32c deprecation warning (crc32c==2.1) (jeffwidman / PR #2128)\n* Lint cleanup (jeffwidman / PR #2126)\n* Fix initialization order in KafkaClient (pecalleja / PR #2119)\n* Allow installing crc32c via extras (mishas / PR #2069)\n* Remove unused imports (jameslamb / PR #2046)\n\nAdmin Client\n* Merge _find_coordinator_id methods (jeffwidman / PR #2127)\n* Feature: delete consumergroups (swenzel / PR #2040)\n* Allow configurable timeouts in admin client check version (sunnyakaxd / PR #2107)\n* Enhancement for Kafka Admin Client's \"Describe Consumer Group\" (Apurva007 / PR #2035)\n\nProtocol\n* Add support for zstd compression (gabriel-tincu / PR #2021)\n* Add protocol support for brokers 1.1.0 - 2.5.0 (gabriel-tincu / PR #2038)\n* Add ProduceRequest/ProduceResponse v6/v7/v8 (gabriel-tincu / PR #2020)\n* Fix parsing NULL header values (kvfi / PR #2024)\n\nTests\n* Add 2.5.0 to automated CI tests (gabriel-tincu / PR #2038)\n* Add 2.1.1 to build_integration (gabriel-tincu / PR #2019)\n\nDocumentation / Logging / Errors\n* Disable logging during producer object gc (gioele / PR #2043)\n* Update example.py; use threading instead of multiprocessing (Mostafa-Elmenbawy / PR #2081)\n* Fix typo in exception message (haracejacob / PR #2096)\n* Add kafka.structs docstrings (Mostafa-Elmenbawy / PR #2080)\n* Fix broken compatibility page link (anuragrana / PR #2045)\n* Rename README to README.md (qhzxc0015 / PR #2055)\n* Fix docs by adding SASL mention (jeffwidman / #1990)\n\n# 2.0.1 (Feb 19, 2020)\n\nAdmin Client\n* KAFKA-8962: Use least_loaded_node() for AdminClient.describe_topics() (jeffwidman / PR #2000)\n* Fix AdminClient topic error parsing in MetadataResponse (jtribble / PR #1997)\n\n# 2.0.0 (Feb 10, 2020)\n\nThis release includes breaking changes for any application code that has not\nmigrated from older Simple-style classes to newer Kafka-style classes.\n\nDeprecation\n* Remove deprecated SimpleClient, Producer, Consumer, Unittest (jeffwidman / PR #1196)\n\nAdmin Client\n* Use the controller for topic metadata requests (TylerLubeck / PR #1995)\n* Implement list_topics, describe_topics, and describe_cluster (TylerLubeck / PR #1993)\n* Implement __eq__ and __hash__ for ACL objects (TylerLubeck / PR #1955)\n* Fixes KafkaAdminClient returning `IncompatibleBrokerVersion` when passing an `api_version` (ian28223 / PR #1953)\n* Admin protocol updates (TylerLubeck / PR #1948)\n* Fix describe config for multi-broker clusters (jlandersen  / PR #1869)\n\nMiscellaneous Bugfixes / Improvements\n* Enable SCRAM-SHA-256 and SCRAM-SHA-512 for sasl (swenzel / PR #1918)\n* Fix slots usage and use more slots (carsonip / PR #1987)\n* Optionally return OffsetAndMetadata from consumer.committed(tp) (dpkp / PR #1979)\n* Reset conn configs on exception in conn.check_version() (dpkp / PR #1977)\n* Do not block on sender thread join after timeout in producer.close() (dpkp / PR #1974)\n* Implement methods to convert a Struct object to a pythonic object (TylerLubeck / PR #1951)\n\nTest Infrastructure / Documentation / Maintenance\n* Update 2.4.0 resource files for sasl integration (dpkp)\n* Add kafka 2.4.0 to CI testing (vvuibert / PR #1972)\n* convert test_admin_integration to pytest (ulrikjohansson / PR #1923)\n* xfail test_describe_configs_topic_resource_returns_configs (dpkp / Issue #1929)\n* Add crc32c to README and docs (dpkp)\n* Improve docs for reconnect_backoff_max_ms (dpkp / PR #1976)\n* Fix simple typo: managementment -> management (timgates42 / PR #1966)\n* Fix typos (carsonip / PR #1938)\n* Fix doc import paths (jeffwidman / PR #1933)\n* Update docstring to match conn.py's (dabcoder / PR #1921)\n* Do not log topic-specific errors in full metadata fetch (dpkp / PR #1980)\n* Raise AssertionError if consumer closed in poll() (dpkp / PR #1978)\n* Log retriable coordinator NodeNotReady, TooManyInFlightRequests as debug not error (dpkp / PR #1975)\n* Remove unused import (jeffwidman)\n* Remove some dead code (jeffwidman)\n* Fix a benchmark to Use print() function in both Python 2 and Python 3 (cclauss / PR #1983)\n* Fix a test to use ==/!= to compare str, bytes, and int literals (cclauss / PR #1984)\n* Fix benchmarks to use pyperf (carsonip / PR #1986)\n* Remove unused/empty .gitsubmodules file (jeffwidman / PR #1928)\n* Remove deprecated `ConnectionError` (jeffwidman / PR #1816)\n\n\n# 1.4.7 (Sep 30, 2019)\n\nThis is a minor release focused on KafkaConsumer performance, Admin Client\nimprovements, and Client concurrency. The KafkaConsumer iterator implementation\nhas been greatly simplified so that it just wraps consumer.poll(). The prior\nimplementation will remain available for a few more releases using the optional\nKafkaConsumer config: `legacy_iterator=True` . This is expected to improve\nconsumer throughput substantially and help reduce heartbeat failures / group\nrebalancing.\n\nClient\n* Send socket data via non-blocking IO with send buffer (dpkp / PR #1912)\n* Rely on socket selector to detect completed connection attempts (dpkp / PR #1909)\n* Improve connection lock handling; always use context manager (melor,dpkp / PR #1895)\n* Reduce client poll timeout when there are no in-flight requests (dpkp / PR #1823)\n\nKafkaConsumer\n* Do not use wakeup when sending fetch requests from consumer (dpkp / PR #1911)\n* Wrap `consumer.poll()` for KafkaConsumer iteration (dpkp / PR #1902)\n* Allow the coordinator to auto-commit on old brokers (justecorruptio / PR #1832)\n* Reduce internal client poll timeout for (legacy) consumer iterator interface (dpkp / PR #1824)\n* Use dedicated connection for group coordinator (dpkp / PR #1822)\n* Change coordinator lock acquisition order (dpkp / PR #1821)\n* Make `partitions_for_topic` a read-through cache (Baisang / PR #1781,#1809)\n* Fix consumer hanging indefinitely on topic deletion while rebalancing (commanderdishwasher / PR #1782)\n\nMiscellaneous Bugfixes / Improvements\n* Fix crc32c avilability on non-intel architectures (ossdev07 / PR #1904)\n* Load system default SSL CAs if `ssl_cafile` is not provided (iAnomaly / PR #1883)\n* Catch py3 TimeoutError in BrokerConnection send/recv (dpkp / PR #1820)\n* Added a function to determine if bootstrap is successfully connected (Wayde2014 / PR #1876)\n\nAdmin Client\n* Add ACL api support to KafkaAdminClient (ulrikjohansson / PR #1833)\n* Add `sasl_kerberos_domain_name` config to KafkaAdminClient (jeffwidman / PR #1852)\n* Update `security_protocol` config documentation for KafkaAdminClient (cardy31 / PR #1849)\n* Break FindCoordinator into request/response methods in KafkaAdminClient (jeffwidman / PR #1871)\n* Break consumer operations into request / response methods in KafkaAdminClient (jeffwidman / PR #1845)\n* Parallelize calls to `_send_request_to_node()` in KafkaAdminClient (davidheitman / PR #1807)\n\nTest Infrastructure / Documentation / Maintenance\n* Add Kafka 2.3.0 to test matrix and compatibility docs (dpkp / PR #1915)\n* Convert remaining `KafkaConsumer` tests to `pytest` (jeffwidman / PR #1886)\n* Bump integration tests to 0.10.2.2 and 0.11.0.3 (jeffwidman / #1890)\n* Cleanup handling of `KAFKA_VERSION` env var in tests (jeffwidman / PR #1887)\n* Minor test cleanup (jeffwidman / PR #1885)\n* Use `socket.SOCK_STREAM` in test assertions (iv-m / PR #1879)\n* Sanity test for `consumer.topics()` and `consumer.partitions_for_topic()` (Baisang / PR #1829)\n* Cleanup seconds conversion in client poll timeout calculation (jeffwidman / PR #1825)\n* Remove unused imports (jeffwidman / PR #1808)\n* Cleanup python nits in RangePartitionAssignor (jeffwidman / PR #1805)\n* Update links to kafka consumer config docs (jeffwidman)\n* Fix minor documentation typos (carsonip / PR #1865)\n* Remove unused/weird comment line (jeffwidman / PR #1813)\n* Update docs for `api_version_auto_timeout_ms` (jeffwidman / PR #1812)\n\n\n# 1.4.6 (Apr 2, 2019)\n\nThis is a patch release primarily focused on bugs related to concurrency,\nSSL connections and testing, and SASL authentication:\n\nClient Concurrency Issues (Race Conditions / Deadlocks)\n* Fix race condition in `protocol.send_bytes` (isamaru / PR #1752)\n* Do not call `state_change_callback` with lock (dpkp / PR #1775)\n* Additional BrokerConnection locks to synchronize protocol/IFR state (dpkp / PR #1768)\n* Send pending requests before waiting for responses (dpkp / PR #1762)\n* Avoid race condition on `client._conns` in send() (dpkp / PR #1772)\n* Hold lock during `client.check_version` (dpkp / PR #1771)\n\nProducer Wakeup / TimeoutError\n* Dont wakeup during `maybe_refresh_metadata` -- it is only called by poll() (dpkp / PR #1769)\n* Dont do client wakeup when sending from sender thread (dpkp / PR #1761)\n\nSSL - Python3.7 Support / Bootstrap Hostname Verification / Testing\n* Wrap SSL sockets after connecting for python3.7 compatibility (dpkp / PR #1754)\n* Allow configuration of SSL Ciphers (dpkp / PR #1755)\n* Maintain shadow cluster metadata for bootstrapping (dpkp / PR #1753)\n* Generate SSL certificates for local testing (dpkp / PR #1756)\n* Rename ssl.keystore.location and ssl.truststore.location config files (dpkp)\n* Reset reconnect backoff on SSL connection (dpkp / PR #1777)\n\nSASL - OAuthBearer support / api version bugfix\n* Fix 0.8.2 protocol quick detection / fix SASL version check (dpkp / PR #1763)\n* Update sasl configuration docstrings to include supported mechanisms (dpkp)\n* Support SASL OAuthBearer Authentication (pt2pham / PR #1750)\n\nMiscellaneous Bugfixes\n* Dont force metadata refresh when closing unneeded bootstrap connections (dpkp / PR #1773)\n* Fix possible AttributeError during conn._close_socket (dpkp / PR #1776)\n* Return connection state explicitly after close in connect() (dpkp / PR #1778)\n* Fix flaky conn tests that use time.time (dpkp / PR #1758)\n* Add py to requirements-dev (dpkp)\n* Fixups to benchmark scripts for py3 / new KafkaFixture interface (dpkp)\n\n\n# 1.4.5 (Mar 14, 2019)\n\nThis release is primarily focused on addressing lock contention\nand other coordination issues between the KafkaConsumer and the\nbackground heartbeat thread that was introduced in the 1.4 release.\n\nConsumer\n* connections_max_idle_ms must be larger than request_timeout_ms (jeffwidman / PR #1688)\n* Avoid race condition during close() / join heartbeat thread (dpkp / PR #1735)\n* Use last offset from fetch v4 if available to avoid getting stuck in compacted topic (keithks / PR #1724)\n* Synchronize puts to KafkaConsumer protocol buffer during async sends (dpkp / PR #1733)\n* Improve KafkaConsumer join group / only enable Heartbeat Thread during stable group (dpkp / PR #1695)\n* Remove unused `skip_double_compressed_messages` (jeffwidman / PR #1677)\n* Fix commit_offsets_async() callback (Faqa / PR #1712)\n\nClient\n* Retry bootstrapping after backoff when necessary (dpkp / PR #1736)\n* Recheck connecting nodes sooner when refreshing metadata (dpkp / PR #1737)\n* Avoid probing broker versions twice on newer brokers (dpkp / PR #1738)\n* Move all network connections and writes to KafkaClient.poll() (dpkp / PR #1729)\n* Do not require client lock for read-only operations (dpkp / PR #1730)\n* Timeout all unconnected conns (incl SSL) after request_timeout_ms (dpkp / PR #1696)\n\nAdmin Client\n* Fix AttributeError in response topic error codes checking (jeffwidman)\n* Fix response error checking in KafkaAdminClient send_to_controller (jeffwidman)\n* Fix NotControllerError check (jeffwidman)\n\nCore/Protocol\n* Fix default protocol parser version / 0.8.2 version probe (dpkp / PR #1740)\n* Make NotEnoughReplicasError/NotEnoughReplicasAfterAppendError retriable (le-linh / PR #1722)\n\nBugfixes\n* Use copy() in metrics() to avoid thread safety issues (emeric254 / PR #1682)\n\nTest Infrastructure\n* Mock dns lookups in test_conn (dpkp / PR #1739)\n* Use test.fixtures.version not test.conftest.version to avoid warnings (dpkp / PR #1731)\n* Fix test_legacy_correct_metadata_response on x86 arch (stanislavlevin / PR #1718)\n* Travis CI: 'sudo' tag is now deprecated in Travis (cclauss / PR #1698)\n* Use Popen.communicate() instead of Popen.wait() (Baisang / PR #1689)\n\nCompatibility\n* Catch thrown OSError by python 3.7 when creating a connection (danjo133 / PR #1694)\n* Update travis test coverage: 2.7, 3.4, 3.7, pypy2.7 (jeffwidman, dpkp / PR #1614)\n* Drop dependency on sphinxcontrib-napoleon (stanislavlevin / PR #1715)\n* Remove unused import from kafka/producer/record_accumulator.py (jeffwidman / PR #1705)\n* Fix SSL connection testing in Python 3.7 (seanthegeek, silentben / PR #1669)\n\n\n# 1.4.4 (Nov 20, 2018)\n\nBugfixes\n* (Attempt to) Fix deadlock between consumer and heartbeat (zhgjun / dpkp #1628)\n* Fix Metrics dict memory leak (kishorenc #1569)\n\nClient\n* Support Kafka record headers (hnousiainen #1574)\n* Set socket timeout for the write-side of wake socketpair (Fleurer #1577)\n* Add kerberos domain name config for gssapi sasl mechanism handshake (the-sea #1542)\n* Support smaller topic metadata fetch during bootstrap (andyxning #1541)\n* Use TypeError for invalid timeout type (jeffwidman #1636)\n* Break poll if closed (dpkp)\n\nAdmin Client\n* Add KafkaAdminClient class (llamahunter #1540)\n* Fix list_consumer_groups() to query all brokers (jeffwidman #1635)\n* Stop using broker-errors for client-side problems (jeffwidman #1639)\n* Fix send to controller (jeffwidman #1640)\n* Add group coordinator lookup (jeffwidman #1641)\n* Fix describe_groups (jeffwidman #1642)\n* Add list_consumer_group_offsets() (jeffwidman #1643)\n* Remove support for api versions as strings from KafkaAdminClient (jeffwidman #1644)\n* Set a clear default value for `validate_only`/`include_synonyms` (jeffwidman #1645)\n* Bugfix: Always set this_groups_coordinator_id (jeffwidman #1650)\n\nConsumer\n* Fix linter warning on import of ConsumerRebalanceListener (ben-harack #1591)\n* Remove ConsumerTimeout (emord #1587)\n* Return future from commit_offsets_async() (ekimekim #1560)\n\nCore / Protocol\n* Add protocol structs for {Describe,Create,Delete} Acls (ulrikjohansson #1646/partial)\n* Pre-compile pack/unpack function calls (billyevans / jeffwidman #1619)\n* Don't use `kafka.common` internally (jeffwidman #1509)\n* Be explicit with tuples for %s formatting (jeffwidman #1634)\n\nDocumentation\n* Document connections_max_idle_ms (jeffwidman #1531)\n* Fix sphinx url (jeffwidman #1610)\n* Update remote urls: snappy, https, etc (jeffwidman #1603)\n* Minor cleanup of testing doc (jeffwidman #1613)\n* Various docstring / pep8 / code hygiene cleanups (jeffwidman #1647)\n\nTest Infrastructure\n* Stop pinning `pylint` (jeffwidman #1611)\n* (partial) Migrate from `Unittest` to `pytest` (jeffwidman #1620)\n* Minor aesthetic cleanup of partitioner tests (jeffwidman #1618)\n* Cleanup fixture imports (jeffwidman #1616)\n* Fix typo in test file name (jeffwidman)\n* Remove unused ivy_root variable (jeffwidman)\n* Add test fixtures for kafka versions 1.0.2 -> 2.0.1 (dpkp)\n* Bump travis test for 1.x brokers to 1.1.1 (dpkp)\n\nLogging / Error Messages\n* raising logging level on messages signalling data loss (sibiryakov #1553)\n* Stop using deprecated log.warn() (jeffwidman #1615)\n* Fix typo in logging message (jeffwidman)\n\nCompatibility\n* Vendor enum34 (jeffwidman #1604)\n* Bump vendored `six` to `1.11.0` (jeffwidman #1602)\n* Vendor `six` consistently (jeffwidman #1605)\n* Prevent `pylint` import errors on `six.moves` (jeffwidman #1609)\n\n\n# 1.4.3 (May 26, 2018)\n\nCompatibility\n* Fix for python 3.7 support: remove 'async' keyword from SimpleProducer (dpkp #1454)\n\nClient\n* Improve BrokerConnection initialization time (romulorosa #1475)\n* Ignore MetadataResponses with empty broker list (dpkp #1506)\n* Improve connection handling when bootstrap list is invalid (dpkp #1507)\n\nConsumer\n* Check for immediate failure when looking up coordinator in heartbeat thread (dpkp #1457)\n\nCore / Protocol\n* Always acquire client lock before coordinator lock to avoid deadlocks (dpkp #1464)\n* Added AlterConfigs and DescribeConfigs apis (StephenSorriaux #1472)\n* Fix CreatePartitionsRequest_v0 (StephenSorriaux #1469)\n* Add codec validators to record parser and builder for all formats (tvoinarovskyi #1447)\n* Fix MemoryRecord bugs re error handling and add test coverage (tvoinarovskyi #1448)\n* Force lz4 to disable Kafka-unsupported block linking when encoding (mnito #1476)\n* Stop shadowing `ConnectionError` (jeffwidman #1492)\n\nDocumentation\n* Document methods that return None (jeffwidman #1504)\n* Minor doc capitalization cleanup (jeffwidman)\n* Adds add_callback/add_errback example to docs (Berkodev #1441)\n* Fix KafkaConsumer docstring for request_timeout_ms default (dpkp #1459)\n\nTest Infrastructure\n* Skip flakey SimpleProducer test (dpkp)\n* Fix skipped integration tests if KAFKA_VERSION unset (dpkp #1453)\n\nLogging / Error Messages\n* Stop using deprecated log.warn() (jeffwidman)\n* Change levels for some heartbeat thread logging (dpkp #1456)\n* Log Heartbeat thread start / close for debugging (dpkp)\n\n\n# 1.4.2 (Mar 10, 2018)\n\nBugfixes\n* Close leaked selector in version check (dpkp #1425)\n* Fix `BrokerConnection.connection_delay()` to return milliseconds (dpkp #1414)\n* Use local copies in `Fetcher._fetchable_partitions` to avoid mutation errors (dpkp #1400)\n* Fix error var name in `_unpack` (j2gg0s #1403)\n* Fix KafkaConsumer compacted offset handling (dpkp #1397)\n* Fix byte size estimation with kafka producer (blakeembrey #1393)\n* Fix coordinator timeout in consumer poll interface (braedon #1384)\n\nClient\n* Add `BrokerConnection.connect_blocking()` to improve bootstrap to multi-address hostnames (dpkp #1411)\n* Short-circuit `BrokerConnection.close()` if already disconnected (dpkp #1424)\n* Only increase reconnect backoff if all addrinfos have been tried (dpkp #1423)\n* Make BrokerConnection .host / .port / .afi immutable to avoid incorrect 'metadata changed' checks (dpkp #1422)\n* Connect with sockaddrs to support non-zero ipv6 scope ids (dpkp #1433)\n* Check timeout type in KafkaClient constructor (asdaraujo #1293)\n* Update string representation of SimpleClient (asdaraujo #1293)\n* Do not validate `api_version` against known versions (dpkp #1434)\n\nConsumer\n* Avoid tight poll loop in consumer when brokers are down (dpkp #1415)\n* Validate `max_records` in KafkaConsumer.poll (dpkp #1398)\n* KAFKA-5512: Awake heartbeat thread when it is time to poll (dpkp #1439)\n\nProducer\n* Validate that serializers generate bytes-like (or None) data (dpkp #1420)\n\nCore / Protocol\n* Support alternative lz4 package: lz4framed (everpcpc #1395)\n* Use hardware accelerated CRC32C function if available (tvoinarovskyi #1389)\n* Add Admin CreatePartitions API call (alexef #1386)\n\nTest Infrastructure\n* Close KafkaConsumer instances during tests (dpkp #1410)\n* Introduce new fixtures to prepare for migration to pytest (asdaraujo #1293)\n* Removed pytest-catchlog dependency (asdaraujo #1380)\n* Fixes racing condition when message is sent to broker before topic logs are created (asdaraujo #1293)\n* Add kafka 1.0.1 release to test fixtures (dpkp #1437)\n\nLogging / Error Messages\n* Re-enable logging during broker version check (dpkp #1430)\n* Connection logging cleanups (dpkp #1432)\n* Remove old CommitFailed error message from coordinator (dpkp #1436)\n\n\n# 1.4.1 (Feb 9, 2018)\n\nBugfixes\n* Fix consumer poll stuck error when no available partition (ckyoog #1375)\n* Increase some integration test timeouts (dpkp #1374)\n* Use raw in case string overriden (jeffwidman #1373)\n* Fix pending completion IndexError bug caused by multiple threads (dpkp #1372)\n\n\n# 1.4.0 (Feb 6, 2018)\n\nThis is a substantial release. Although there are no known 'showstopper' bugs as of release,\nwe do recommend you test any planned upgrade to your application prior to running in production.\n\nSome of the major changes include:\n* We have officially dropped python 2.6 support\n* The KafkaConsumer now includes a background thread to handle coordinator heartbeats\n* API protocol handling has been separated from networking code into a new class, KafkaProtocol\n* Added support for kafka message format v2\n* Refactored DNS lookups during kafka broker connections\n* SASL authentication is working (we think)\n* Removed several circular references to improve gc on close()\n\nThanks to all contributors -- the state of the kafka-python community is strong!\n\nDetailed changelog are listed below:\n\nClient\n* Fixes for SASL support\n  * Refactor SASL/gssapi support (dpkp #1248 #1249 #1257 #1262 #1280)\n  * Add security layer negotiation to the GSSAPI authentication (asdaraujo #1283)\n  * Fix overriding sasl_kerberos_service_name in KafkaConsumer / KafkaProducer (natedogs911 #1264)\n  * Fix typo in _try_authenticate_plain (everpcpc #1333)\n  * Fix for Python 3 byte string handling in SASL auth (christophelec #1353)\n* Move callback processing from BrokerConnection to KafkaClient (dpkp #1258)\n* Use socket timeout of request_timeout_ms to prevent blocking forever on send (dpkp #1281)\n* Refactor dns lookup in BrokerConnection (dpkp #1312)\n* Read all available socket bytes (dpkp #1332)\n* Honor reconnect_backoff in conn.connect() (dpkp #1342)\n\nConsumer\n* KAFKA-3977: Defer fetch parsing for space efficiency, and to raise exceptions to user (dpkp #1245)\n* KAFKA-4034: Avoid unnecessary consumer coordinator lookup (dpkp #1254)\n* Handle lookup_coordinator send failures (dpkp #1279)\n* KAFKA-3888 Use background thread to process consumer heartbeats (dpkp #1266)\n* Improve KafkaConsumer cleanup (dpkp #1339)\n* Fix coordinator join_future race condition (dpkp #1338)\n* Avoid KeyError when filtering fetchable partitions (dpkp #1344)\n* Name heartbeat thread with group_id; use backoff when polling (dpkp #1345)\n* KAFKA-3949: Avoid race condition when subscription changes during rebalance (dpkp #1364)\n* Fix #1239 regression to avoid consuming duplicate compressed messages from mid-batch (dpkp #1367)\n\nProducer\n* Fix timestamp not passed to RecordMetadata (tvoinarovskyi #1273)\n* Raise non-API exceptions (jeffwidman #1316)\n* Fix reconnect_backoff_max_ms default config bug in KafkaProducer (YaoC #1352)\n\nCore / Protocol\n* Add kafka.protocol.parser.KafkaProtocol w/ receive and send (dpkp #1230)\n* Refactor MessageSet and Message into LegacyRecordBatch to later support v2 message format (tvoinarovskyi #1252)\n* Add DefaultRecordBatch implementation aka V2 message format parser/builder. (tvoinarovskyi #1185)\n* optimize util.crc32 (ofek #1304)\n* Raise better struct pack/unpack errors (jeffwidman #1320)\n* Add Request/Response structs for kafka broker 1.0.0 (dpkp #1368)\n\nBugfixes\n* use python standard max value (lukekingbru #1303)\n* changed for to use enumerate() (TheAtomicOption #1301)\n* Explicitly check for None rather than falsey (jeffwidman #1269)\n* Minor Exception cleanup (jeffwidman #1317)\n* Use non-deprecated exception handling (jeffwidman a699f6a)\n* Remove assertion with side effect in client.wakeup() (bgedik #1348)\n* use absolute imports everywhere (kevinkjt2000 #1362)\n\nTest Infrastructure\n* Use 0.11.0.2 kafka broker for integration testing (dpkp #1357 #1244)\n* Add a Makefile to help build the project, generate docs, and run tests (tvoinarovskyi #1247)\n* Add fixture support for 1.0.0 broker (dpkp #1275)\n* Add kafka 1.0.0 to travis integration tests (dpkp #1365)\n* Change fixture default host to localhost (asdaraujo #1305)\n* Minor test cleanups (dpkp #1343)\n* Use latest pytest 3.4.0, but drop pytest-sugar due to incompatibility (dpkp #1361)\n\nDocumentation\n* Expand metrics docs (jeffwidman #1243)\n* Fix docstring (jeffwidman #1261)\n* Added controlled thread shutdown to example.py (TheAtomicOption #1268)\n* Add license to wheel (jeffwidman #1286)\n* Use correct casing for MB (jeffwidman #1298)\n\nLogging / Error Messages\n* Fix two bugs in printing bytes instance (jeffwidman #1296)\n\n\n# 1.3.5 (Oct 7, 2017)\n\nBugfixes\n* Fix partition assignment race condition (jeffwidman #1240)\n* Fix consumer bug when seeking / resetting to the middle of a compressed messageset (dpkp #1239)\n* Fix traceback sent to stderr not logging (dbgasaway #1221)\n* Stop using mutable types for default arg values (jeffwidman #1213)\n* Remove a few unused imports (jameslamb #1188)\n\nClient\n* Refactor BrokerConnection to use asynchronous receive_bytes pipe (dpkp #1032)\n\nConsumer\n* Drop unused sleep kwarg to poll (dpkp #1177)\n* Enable KafkaConsumer beginning_offsets() and end_offsets() with older broker versions (buptljy #1200)\n* Validate consumer subscription topic strings (nikeee #1238)\n\nDocumentation\n* Small fixes to SASL documentation and logging; validate security_protocol (dpkp #1231)\n* Various typo and grammar fixes (jeffwidman)\n\n\n# 1.3.4 (Aug 13, 2017)\n\nBugfixes\n* Avoid multiple connection attempts when refreshing metadata (dpkp #1067)\n* Catch socket.errors when sending / recving bytes on wake socketpair (dpkp #1069)\n* Deal with brokers that reappear with different IP address (originsmike #1085)\n* Fix join-time-max and sync-time-max metrics to use Max() measure function (billyevans #1146)\n* Raise AssertionError when decompression unsupported (bts-webber #1159)\n* Catch ssl.EOFErrors on Python3.3 so we close the failing conn (Ormod #1162)\n* Select on sockets to avoid busy polling during bootstrap (dpkp #1175)\n* Initialize metadata_snapshot in group coordinator to avoid unnecessary rebalance (dpkp #1174)\n\nClient\n* Timeout idle connections via connections_max_idle_ms (dpkp #1068)\n* Warn, dont raise, on DNS lookup failures (dpkp #1091)\n* Support exponential backoff for broker reconnections -- KIP-144 (dpkp #1124)\n* Add gssapi support (Kerberos) for SASL (Harald-Berghoff #1152)\n* Add private map of api key -> min/max versions to BrokerConnection (dpkp #1169)\n\nConsumer\n* Backoff on unavailable group coordinator retry (dpkp #1125)\n* Only change_subscription on pattern subscription when topics change (Artimi #1132)\n* Add offsets_for_times, beginning_offsets and end_offsets APIs (tvoinarovskyi #1161)\n\nProducer\n* Raise KafkaTimeoutError when flush times out (infecto)\n* Set producer atexit timeout to 0 to match del (Ormod #1126)\n\nCore / Protocol\n* 0.11.0.0 protocol updates (only - no client support yet) (dpkp #1127)\n* Make UnknownTopicOrPartitionError retriable error (tvoinarovskyi)\n\nTest Infrastructure\n* pylint 1.7.0+ supports python 3.6 and merge py36 into common testenv (jianbin-wei #1095)\n* Add kafka 0.10.2.1 into integration testing version (jianbin-wei #1096)\n* Disable automated tests for python 2.6 and kafka 0.8.0 and 0.8.1.1 (jianbin-wei #1096)\n* Support manual py26 testing; dont advertise 3.3 support (dpkp)\n* Add 0.11.0.0 server resources, fix tests for 0.11 brokers (dpkp)\n* Use fixture hostname, dont assume localhost (dpkp)\n* Add 0.11.0.0 to travis test matrix, remove 0.10.1.1; use scala 2.11 artifacts (dpkp #1176)\n\nLogging / Error Messages\n* Improve error message when expiring batches in KafkaProducer (dpkp #1077)\n* Update producer.send docstring -- raises KafkaTimeoutError (infecto)\n* Use logging's built-in string interpolation (jeffwidman)\n* Fix produce timeout message (melor #1151)\n* Fix producer batch expiry messages to use seconds (dnwe)\n\nDocumentation\n* Fix typo in KafkaClient docstring (jeffwidman #1054)\n* Update README: Prefer python-lz4 over lz4tools (kiri11 #1057)\n* Fix poll() hyperlink in KafkaClient (jeffwidman)\n* Update RTD links with https / .io (jeffwidman #1074)\n* Describe consumer thread-safety (ecksun)\n* Fix typo in consumer integration test (jeffwidman)\n* Note max_in_flight_requests_per_connection > 1 may change order of messages (tvoinarovskyi #1149)\n\n\n# 1.3.3 (Mar 14, 2017)\n\nCore / Protocol\n* Derive all api classes from Request / Response base classes (dpkp 1030)\n* Prefer python-lz4 if available (dpkp 1024)\n* Fix kwarg handing in kafka.protocol.struct.Struct (dpkp 1025)\n* Fixed couple of \"leaks\" when gc is disabled (Mephius 979)\n* Added `max_bytes` option and FetchRequest_v3 usage. (Drizzt1991 962)\n* CreateTopicsRequest / Response v1 (dpkp 1012)\n* Add MetadataRequest_v2 and MetadataResponse_v2 structures for KIP-78 (Drizzt1991 974)\n* KIP-88 / KAFKA-3853: OffsetFetch v2 structs (jeffwidman 971)\n* DRY-up the MetadataRequest_v1 struct (jeffwidman 966)\n* Add JoinGroup v1 structs (jeffwidman 965)\n* DRY-up the OffsetCommitResponse Structs (jeffwidman 970)\n* DRY-up the OffsetFetch structs (jeffwidman 964)\n* time --> timestamp to match Java API (jeffwidman 969)\n* Add support for offsetRequestV1 messages (jlafaye 951)\n* Add FetchRequest/Response_v3 structs (jeffwidman 943)\n* Add CreateTopics / DeleteTopics Structs (jeffwidman 944)\n\nTest Infrastructure\n* Add python3.6 to travis test suite, drop python3.3 (exponea 992)\n* Update to 0.10.1.1 for integration testing (dpkp 953)\n* Update vendored berkerpeksag/selectors34 to ff61b82 (Mephius 979)\n* Remove dead code (jeffwidman 967)\n* Update pytest fixtures to new yield syntax (jeffwidman 919)\n\nConsumer\n* Avoid re-encoding message for crc check (dpkp 1027)\n* Optionally skip auto-commit during consumer.close (dpkp 1031)\n* Return copy of consumer subscription set (dpkp 1029)\n* Short-circuit group coordinator requests when NodeNotReady (dpkp 995)\n* Avoid unknown coordinator after client poll (dpkp 1023)\n* No longer configure a default consumer group (dpkp 1016)\n* Dont refresh metadata on failed group coordinator request unless needed (dpkp 1006)\n* Fail-fast on timeout constraint violations during KafkaConsumer creation (harelba 986)\n* Default max_poll_records to Java default of 500 (jeffwidman 947)\n* For 0.8.2, only attempt connection to coordinator if least_loaded_node succeeds (dpkp)\n\nProducer\n* change default timeout of KafkaProducer.close() to threading.TIMEOUT_MAX on py3 (mmyjona 991)\n\nClient\n* Add optional kwarg to ready/is_ready to disable metadata-priority logic (dpkp 1017)\n* When closing a broker connection without error, fail in-flight-requests with Cancelled (dpkp 1010)\n* Catch socket errors during ssl handshake (dpkp 1007)\n* Drop old brokers when rebuilding broker metadata (dpkp 1005)\n* Drop bad disconnect test -- just use the mocked-socket test (dpkp 982)\n* Add support for Python built without ssl (minagawa-sho 954)\n* Do not re-close a disconnected connection (dpkp)\n* Drop unused last_failure time from BrokerConnection (dpkp)\n* Use connection state functions where possible (dpkp)\n* Pass error to BrokerConnection.close() (dpkp)\n\nBugfixes\n* Free lz4 decompression context to avoid leak (dpkp 1024)\n* Fix sasl reconnect bug: auth future must be reset on close (dpkp 1003)\n* Fix raise exception from SubscriptionState.assign_from_subscribed (qntln 960)\n* Fix blackout calculation: mark last_attempt time during connection close (dpkp 1008)\n* Fix buffer pool reallocation after raising timeout (dpkp 999)\n\nLogging / Error Messages\n* Add client info logging re bootstrap; log connection attempts to balance with close (dpkp)\n* Minor additional logging for consumer coordinator (dpkp)\n* Add more debug-level connection logging (dpkp)\n* Do not need str(self) when formatting to %s (dpkp)\n* Add new broker response errors (dpkp)\n* Small style fixes in kafka.errors (dpkp)\n* Include the node id in BrokerConnection logging (dpkp 1009)\n* Replace %s with %r in producer debug log message (chekunkov 973)\n\nDocumentation\n* Sphinx documentation updates (jeffwidman 1019)\n* Add sphinx formatting to hyperlink methods (jeffwidman 898)\n* Fix BrokerConnection api_version docs default (jeffwidman 909)\n* PEP-8: Spacing & removed unused imports (jeffwidman 899)\n* Move BrokerConnection docstring to class (jeffwidman 968)\n* Move docstring so it shows up in Sphinx/RTD (jeffwidman 952)\n* Remove non-pip install instructions (jeffwidman 940)\n* Spelling and grammar changes (melissacrawford396 923)\n* Fix typo: coorelation --> correlation (jeffwidman 929)\n* Make SSL warning list the correct Python versions (jeffwidman 924)\n* Fixup comment reference to _maybe_connect (dpkp)\n* Add ClusterMetadata sphinx documentation (dpkp)\n\nLegacy Client\n* Add send_list_offset_request for searching offset by timestamp (charsyam 1001)\n* Use select to poll sockets for read to reduce CPU usage (jianbin-wei 958)\n* Use select.select without instance bounding (adamwen829 949)\n\n\n# 1.3.2 (Dec 28, 2016)\n\nCore\n* Add kafka.serializer interfaces (dpkp 912)\n* from kafka import ConsumerRebalanceListener, OffsetAndMetadata\n* Use 0.10.0.1 for integration tests (dpkp 803)\n\nConsumer\n* KAFKA-3007: KafkaConsumer max_poll_records (dpkp 831)\n* Raise exception if given a non-str topic (ssaamm 824)\n* Immediately update metadata for pattern subscription (laz2 915)\n\nProducer\n* Update Partitioners for use with KafkaProducer (barrotsteindev 827)\n* Sort partitions before calling partitioner (ms7s 905)\n* Added ssl_password config option to KafkaProducer class (kierkegaard13 830)\n\nClient\n* Always check for request timeouts (dpkp 887)\n* When hostname lookup is necessary, do every connect (benauthor 812)\n\nBugfixes\n* Fix errorcode check when socket.connect_ex raises an exception (guojh 907)\n* Fix fetcher bug when processing offset out of range (sibiryakov 860)\n* Fix possible request draining in ensure_active_group (dpkp 896)\n* Fix metadata refresh handling with 0.10+ brokers when topic list is empty (sibiryakov 867)\n* KafkaProducer should set timestamp in Message if provided (Drizzt1991 875)\n* Fix murmur2 bug handling python2 bytes that do not ascii encode (dpkp 815)\n* Monkeypatch max_in_flight_requests_per_connection when checking broker version (dpkp 834)\n* Fix message timestamp_type (qix 828)\n\nLogging / Error Messages\n* Always include an error for logging when the coordinator is marked dead (dpkp 890)\n* Only string-ify BrokerResponseError args if provided (dpkp 889)\n* Update warning re advertised.listeners / advertised.host.name (jeffwidman 878)\n* Fix unrecognized sasl_mechanism error message (sharego 883)\n\nDocumentation\n* Add docstring for max_records (jeffwidman 897)\n* Fixup doc references to max_in_flight_requests_per_connection\n* Fix typo: passowrd --> password (jeffwidman 901)\n* Fix documentation typo 'Defualt' -> 'Default'. (rolando 895)\n* Added doc for `max_poll_records` option (Drizzt1991 881)\n* Remove old design notes from Kafka 8 era (jeffwidman 876)\n* Fix documentation typos (jeffwidman 874)\n* Fix quota violation exception message (dpkp 809)\n* Add comment for round robin partitioner with different subscriptions\n* Improve KafkaProducer docstring for retries configuration\n\n\n# 1.3.1 (Aug 8, 2016)\n\nBugfixes\n* Fix AttributeError in BrokerConnectionMetrics after reconnecting\n\n\n# 1.3.0 (Aug 4, 2016)\n\nIncompatible Changes\n* Delete KafkaConnection class (dpkp 769)\n* Rename partition_assignment -> assignment in MemberMetadata for consistency\n* Move selectors34 and socketpair to kafka.vendor (dpkp 785)\n* Change api_version config to tuple; deprecate str with warning (dpkp 761)\n* Rename _DEFAULT_CONFIG -> DEFAULT_CONFIG in KafkaProducer (dpkp 788)\n\nImprovements\n* Vendor six 1.10.0 to eliminate runtime dependency (dpkp 785)\n* Add KafkaProducer and KafkaConsumer.metrics() with instrumentation similar to java client (dpkp 754 / 772 / 794)\n* Support Sasl PLAIN authentication (larsjsol PR 779)\n* Add checksum and size to RecordMetadata and ConsumerRecord (KAFKA-3196 / 770 / 594)\n* Use MetadataRequest v1 for 0.10+ api_version (dpkp 762)\n* Fix KafkaConsumer autocommit for 0.8 brokers (dpkp 756 / 706)\n* Improve error logging (dpkp 760 / 759)\n* Adapt benchmark scripts from https://github.com/mrafayaleem/kafka-jython (dpkp 754)\n* Add api_version config to KafkaClient (dpkp 761)\n* New Metadata method with_partitions() (dpkp 787)\n* Use socket_options configuration to setsockopts(). Default TCP_NODELAY (dpkp 783)\n* Expose selector type as config option (dpkp 764)\n* Drain pending requests to the coordinator before initiating group rejoin (dpkp 798)\n* Send combined size and payload bytes to socket to avoid potentially split packets with TCP_NODELAY (dpkp 797)\n\nBugfixes\n* Ignore socket.error when checking for protocol out of sync prior to socket close (dpkp 792)\n* Fix offset fetch when partitions are manually assigned (KAFKA-3960 / 786)\n* Change pickle_method to use python3 special attributes (jpaulodit 777)\n* Fix ProduceResponse v2 throttle_time_ms\n* Always encode size with MessageSet (#771)\n* Avoid buffer overread when compressing messageset in KafkaProducer\n* Explicit format string argument indices for python 2.6 compatibility\n* Simplify RecordMetadata; short circuit callbacks (#768)\n* Fix autocommit when partitions assigned manually (KAFKA-3486 / #767 / #626)\n* Handle metadata updates during consumer rebalance (KAFKA-3117 / #766 / #701)\n* Add a consumer config option to exclude internal topics (KAFKA-2832 / #765)\n* Protect writes to wakeup socket with threading lock (#763 / #709)\n* Fetcher spending unnecessary time during metrics recording (KAFKA-3785)\n* Always use absolute_import (dpkp)\n\nTest / Fixtures\n* Catch select errors while capturing test fixture logs\n* Fix consumer group test race condition (dpkp 795)\n* Retry fixture failures on a different port (dpkp 796)\n* Dump fixture logs on failure\n\nDocumentation\n* Fix misspelling of password (ssaamm 793)\n* Document the ssl_password config option (ssaamm 780)\n* Fix typo in KafkaConsumer documentation (ssaamm 775)\n* Expand consumer.fetcher inline comments\n* Update kafka configuration links -> 0.10.0.0 docs\n* Fixup metrics_sample_window_ms docstring in consumer\n\n\n# 1.2.5 (July 15, 2016)\n\nBugfixes\n* Fix bug causing KafkaProducer to double-compress message batches on retry\n* Check for double-compressed messages in KafkaConsumer, log warning and optionally skip\n* Drop recursion in _unpack_message_set; only decompress once\n\n\n# 1.2.4 (July 8, 2016)\n\nBugfixes\n* Update consumer_timeout_ms docstring - KafkaConsumer raises StopIteration, no longer ConsumerTimeout\n* Use explicit subscription state flag to handle seek() during message iteration\n* Fix consumer iteration on compacted topics (dpkp PR 752)\n* Support ssl_password config when loading cert chains (amckemie PR 750)\n\n\n# 1.2.3 (July 2, 2016)\n\nPatch Improvements\n* Fix gc error log: avoid AttributeError in _unregister_cleanup (dpkp PR 747)\n* Wakeup socket optimizations (dpkp PR 740)\n* Assert will be disabled by \"python -O\" (tyronecai PR 736)\n* Randomize order of topics/partitions processed by fetcher to improve balance (dpkp PR 732)\n* Allow client.check_version timeout to be set in Producer and Consumer constructors (eastlondoner PR 647)\n\n\n# 1.2.2 (June 21, 2016)\n\nBugfixes\n* Clarify timeout unit in KafkaProducer close and flush (ms7s PR 734)\n* Avoid busy poll during metadata refresh failure with retry_backoff_ms (dpkp PR 733)\n* Check_version should scan nodes until version found or timeout (dpkp PR 731)\n* Fix bug which could cause least_loaded_node to always return the same unavailable node (dpkp PR 730)\n* Fix producer garbage collection with weakref in atexit handler (dpkp PR 728)\n* Close client selector to fix fd leak (msmith PR 729)\n* Tweak spelling mistake in error const (steve8918 PR 719)\n* Rearrange connection tests to separate legacy KafkaConnection\n\n\n# 1.2.1 (June 1, 2016)\n\nBugfixes\n* Fix regression in MessageSet decoding wrt PartialMessages (#716)\n* Catch response decode errors and log details (#715)\n* Fix Legacy support url (#712 - JonasGroeger)\n* Update sphinx docs re 0.10 broker support\n\n\n# 1.2.0 (May 24, 2016)\n\nThis release officially adds support for Kafka 0.10\n* Add protocol support for ApiVersionRequest (dpkp PR 678)\n* KAFKA-3025: Message v1 -- add timetamp and relative offsets (dpkp PR 693)\n* Use Fetch/Produce API v2 for brokers >= 0.10 (uses message format v1) (dpkp PR 694)\n* Use standard LZ4 framing for v1 messages / kafka 0.10 (dpkp PR 695)\n\nConsumers\n* Update SimpleConsumer / legacy protocol to handle compressed messages (paulcavallaro PR 684)\n\nProducers\n* KAFKA-3388: Fix expiration of batches sitting in the accumulator (dpkp PR 699)\n* KAFKA-3197: when max.in.flight.request.per.connection = 1, attempt to guarantee ordering (dpkp PR 698)\n* Don't use soon-to-be-reserved keyword await as function name (FutureProduceResult) (dpkp PR 697)\n\nClients\n* Fix socket leaks in KafkaClient (dpkp PR 696)\n\nDocumentation\n<none>\n\nInternals\n* Support SSL CRL [requires python 2.7.9+ / 3.4+] (vincentbernat PR 683)\n* Use original hostname for SSL checks (vincentbernat PR 682)\n* Always pass encoded message bytes to MessageSet.encode()\n* Raise ValueError on protocol encode/decode errors\n* Supplement socket.gaierror exception in BrokerConnection.connect() (erikbeebe PR 687)\n* BrokerConnection check_version: expect 0.9 to fail with CorrelationIdError\n* Fix small bug in Sensor (zackdever PR 679)\n\n\n# 1.1.1 (Apr 26, 2016)\n\nquick bugfixes\n* fix throttle_time_ms sensor handling (zackdever pr 667)\n* improve handling of disconnected sockets (easypost pr 666 / dpkp)\n* disable standard metadata refresh triggers during bootstrap (dpkp)\n* more predictable future callback/errback exceptions (zackdever pr 670)\n* avoid some exceptions in coordinator.__del__ (dpkp pr 668)\n\n\n# 1.1.0 (Apr 25, 2016)\n\nConsumers\n* Avoid resending FetchRequests that are pending on internal queue\n* Log debug messages when skipping fetched messages due to offset checks\n* KAFKA-3013: Include topic-partition in exception for expired batches\n* KAFKA-3318: clean up consumer logging and error messages\n* Improve unknown coordinator error handling\n* Improve auto-commit error handling when group_id is None\n* Add paused() API (zackdever PR 602)\n* Add default_offset_commit_callback to KafkaConsumer DEFAULT_CONFIGS\n\nProducers\n<none>\n\nClients\n* Support SSL connections\n* Use selectors module for non-blocking IO\n* Refactor KafkaClient connection management\n* Fix AttributeError in __del__\n* SimpleClient: catch errors thrown by _get_leader_for_partition (zackdever PR 606)\n\nDocumentation\n* Fix serializer/deserializer examples in README\n* Update max.block.ms docstring\n* Remove errant next(consumer) from consumer documentation\n* Add producer.flush() to usage docs\n\nInternals\n* Add initial metrics implementation (zackdever PR 637)\n* KAFKA-2136: support Fetch and Produce v1 (throttle_time_ms)\n* Use version-indexed lists for request/response protocol structs (dpkp PR 630)\n* Split kafka.common into kafka.structs and kafka.errors\n* Handle partial socket send() (dpkp PR 611)\n* Fix windows support (dpkp PR 603)\n* IPv6 support (TimEvens PR 615; Roguelazer PR 642)\n\n\n# 1.0.2 (Mar 14, 2016)\n\nConsumers\n* Improve KafkaConsumer Heartbeat handling (dpkp PR 583)\n* Fix KafkaConsumer.position bug (stefanth PR 578)\n* Raise TypeError when partition is not a TopicPartition (dpkp PR 587)\n* KafkaConsumer.poll should sleep to prevent tight-loops (dpkp PR 597)\n\nProducers\n* Fix producer threading bug that can crash sender (dpkp PR 590)\n* Fix bug in producer buffer pool reallocation (dpkp PR 585)\n* Remove spurious warnings when closing sync SimpleProducer (twm PR 567)\n* Fix FutureProduceResult.await() on python2.6 (dpkp)\n* Add optional timeout parameter to KafkaProducer.flush() (dpkp)\n* KafkaProducer Optimizations (zackdever PR 598)\n\nClients\n* Improve error handling in SimpleClient.load_metadata_for_topics (dpkp)\n* Improve handling of KafkaClient.least_loaded_node failure (dpkp PR 588)\n\nDocumentation\n* Fix KafkaError import error in docs (shichao-an PR 564)\n* Fix serializer / deserializer examples (scribu PR 573)\n\nInternals\n* Update to Kafka 0.9.0.1 for integration testing\n* Fix ifr.future.failure in conn.py (mortenlj PR 566)\n* Improve Zookeeper / Kafka Fixture management (dpkp)\n\n\n# 1.0.1 (Feb 19, 2016)\n\nConsumers\n* Add RangePartitionAssignor (and use as default); add assignor tests (dpkp PR 550)\n* Make sure all consumers are in same generation before stopping group test\n* Verify node ready before sending offset fetch request from coordinator\n* Improve warning when offset fetch request returns unknown topic / partition\n\nProducers\n* Warn if pending batches failed during flush\n* Fix concurrency bug in RecordAccumulator.ready()\n* Fix bug in SimpleBufferPool memory condition waiting / timeout\n* Support batch_size = 0 in producer buffers (dpkp PR 558)\n* Catch duplicate batch.done() calls [e.g., maybe_expire then a response errback]\n\nClients\n\nDocumentation\n* Improve kafka.cluster docstrings\n* Migrate load_example.py to KafkaProducer / KafkaConsumer\n\nInternals\n* Don't override system rcvbuf or sndbuf unless configured explicitly (dpkp PR 557)\n* Some attributes may not exist in __del__ if we failed assertions\n* Break up some circular references and close client wake pipes on __del__ (aisch PR 554)\n\n\n# 1.0.0 (Feb 15, 2016)\n\nThis release includes significant code changes. Users of older kafka-python\nversions are encouraged to test upgrades before deploying to production as\nsome interfaces and configuration options have changed.\n\nUsers of SimpleConsumer / SimpleProducer / SimpleClient (formerly KafkaClient)\nfrom prior releases should migrate to KafkaConsumer / KafkaProducer. Low-level\nAPIs (Simple*) are no longer being actively maintained and will be removed in a\nfuture release.\n\nFor comprehensive API documentation, please see python help() / docstrings,\nkafka-python.readthedocs.org, or run `tox -e docs` from source to build\ndocumentation locally.\n\nConsumers\n* KafkaConsumer re-written to emulate the new 0.9 kafka consumer (java client)\n  and support coordinated consumer groups (feature requires >= 0.9.0.0 brokers)\n\n  * Methods no longer available:\n\n    * configure [initialize a new consumer instead]\n    * set_topic_partitions [use subscribe() or assign()]\n    * fetch_messages [use poll() or iterator interface]\n    * get_partition_offsets\n    * offsets [use committed(partition)]\n    * task_done [handled internally by auto-commit; or commit offsets manually]\n\n  * Configuration changes (consistent with updated java client):\n\n    * lots of new configuration parameters -- see docs for details\n    * auto_offset_reset: previously values were 'smallest' or 'largest', now\n      values are 'earliest' or 'latest'\n    * fetch_wait_max_ms is now fetch_max_wait_ms\n    * max_partition_fetch_bytes is now max_partition_fetch_bytes\n    * deserializer_class is now value_deserializer and key_deserializer\n    * auto_commit_enable is now enable_auto_commit\n    * auto_commit_interval_messages was removed\n    * socket_timeout_ms was removed\n    * refresh_leader_backoff_ms was removed\n\n* SimpleConsumer and MultiProcessConsumer are now deprecated and will be removed\n  in a future release. Users are encouraged to migrate to KafkaConsumer.\n\nProducers\n* new producer class: KafkaProducer. Exposes the same interface as official java client.\n  Async by default; returned future.get() can be called for synchronous blocking\n* SimpleProducer is now deprecated and will be removed in a future release. Users are\n  encouraged to migrate to KafkaProducer.\n\nClients\n* synchronous KafkaClient renamed to SimpleClient. For backwards compatibility, you\n  will get a SimpleClient via `from kafka import KafkaClient`. This will change in\n  a future release.\n* All client calls use non-blocking IO under the hood.\n* Add probe method check_version() to infer broker versions.\n\nDocumentation\n* Updated README and sphinx documentation to address new classes.\n* Docstring improvements to make python help() easier to use.\n\nInternals\n* Old protocol stack is deprecated. It has been moved to kafka.protocol.legacy\n  and may be removed in a future release.\n* Protocol layer re-written using Type classes, Schemas and Structs (modeled on\n  the java client).\n* Add support for LZ4 compression (including broken framing header checksum).\n\n\n# 0.9.5 (Dec 6, 2015)\n\nConsumers\n* Initial support for consumer coordinator: offsets only (toddpalino PR 420)\n* Allow blocking until some messages are received in SimpleConsumer (saaros PR 457)\n* Support subclass config changes in KafkaConsumer (zackdever PR 446)\n* Support retry semantics in MultiProcessConsumer (barricadeio PR 456)\n* Support partition_info in MultiProcessConsumer (scrapinghub PR 418)\n* Enable seek() to an absolute offset in SimpleConsumer (haosdent PR 412)\n* Add KafkaConsumer.close() (ucarion PR 426)\n\nProducers\n* Catch client.reinit() exceptions in async producer (dpkp)\n* Producer.stop() now blocks until async thread completes (dpkp PR 485)\n* Catch errors during load_metadata_for_topics in async producer (bschopman PR 467)\n* Add compression-level support for codecs that support it (trbs PR 454)\n* Fix translation of Java murmur2 code, fix byte encoding for Python 3 (chrischamberlin PR 439)\n* Only call stop() on not-stopped producer objects (docker-hub PR 435)\n* Allow null payload for deletion feature (scrapinghub PR 409)\n\nClients\n* Use non-blocking io for broker aware requests (ecanzonieri PR 473)\n* Use debug logging level for metadata request (ecanzonieri PR 415)\n* Catch KafkaUnavailableError in _send_broker_aware_request (mutability PR 436)\n* Lower logging level on replica not available and commit (ecanzonieri PR 415)\n\nDocumentation\n* Update docs and links wrt maintainer change (mumrah -> dpkp)\n\nInternals\n* Add py35 to tox testing\n* Update travis config to use container infrastructure\n* Add 0.8.2.2 and 0.9.0.0 resources for integration tests; update default official releases\n* new pylint disables for pylint 1.5.1 (zackdever PR 481)\n* Fix python3 / python2 comments re queue/Queue (dpkp)\n* Add Murmur2Partitioner to kafka __all__ imports (dpkp Issue 471)\n* Include LICENSE in PyPI sdist (koobs PR 441)\n\n# 0.9.4 (June 11, 2015)\n\nConsumers\n* Refactor SimpleConsumer internal fetch handling (dpkp PR 399)\n* Handle exceptions in SimpleConsumer commit() and reset_partition_offset() (dpkp PR 404)\n* Improve FailedPayloadsError handling in KafkaConsumer (dpkp PR 398)\n* KafkaConsumer: avoid raising KeyError in task_done (dpkp PR 389)\n* MultiProcessConsumer -- support configured partitions list (dpkp PR 380)\n* Fix SimpleConsumer leadership change handling (dpkp PR 393) \n* Fix SimpleConsumer connection error handling (reAsOn2010 PR 392)\n* Improve Consumer handling of 'falsy' partition values (wting PR 342)\n* Fix _offsets call error in KafkaConsumer (hellais PR 376)\n* Fix str/bytes bug in KafkaConsumer (dpkp PR 365)\n* Register atexit handlers for consumer and producer thread/multiprocess cleanup (dpkp PR 360)\n* Always fetch commit offsets in base consumer unless group is None (dpkp PR 356)\n* Stop consumer threads on delete (dpkp PR 357)\n* Deprecate metadata_broker_list in favor of bootstrap_servers in KafkaConsumer (dpkp PR 340)\n* Support pass-through parameters in multiprocess consumer (scrapinghub PR 336)\n* Enable offset commit on SimpleConsumer.seek (ecanzonieri PR 350)\n* Improve multiprocess consumer partition distribution (scrapinghub PR 335)\n* Ignore messages with offset less than requested (wkiser PR 328)\n* Handle OffsetOutOfRange in SimpleConsumer (ecanzonieri PR 296)\n\nProducers\n* Add Murmur2Partitioner (dpkp PR 378)\n* Log error types in SimpleProducer and SimpleConsumer (dpkp PR 405)\n* SimpleProducer support configuration of fail_on_error (dpkp PR 396)\n* Deprecate KeyedProducer.send() (dpkp PR 379)\n* Further improvements to async producer code (dpkp PR 388)\n* Add more configuration parameters for async producer (dpkp)\n* Deprecate SimpleProducer batch_send=True in favor of async (dpkp)\n* Improve async producer error handling and retry logic (vshlapakov PR 331)\n* Support message keys in async producer (vshlapakov PR 329)\n* Use threading instead of multiprocessing for Async Producer (vshlapakov PR 330)\n* Stop threads on __del__ (chmduquesne PR 324)\n* Fix leadership failover handling in KeyedProducer (dpkp PR 314)\n\nKafkaClient\n* Add .topics property for list of known topics (dpkp)\n* Fix request / response order guarantee bug in KafkaClient (dpkp PR 403)\n* Improve KafkaClient handling of connection failures in _get_conn (dpkp)\n* Client clears local metadata cache before updating from server (dpkp PR 367)\n* KafkaClient should return a response or error for each request - enable better retry handling (dpkp PR 366)\n* Improve str/bytes conversion in KafkaClient and KafkaConsumer (dpkp PR 332)\n* Always return sorted partition ids in client.get_partition_ids_for_topic() (dpkp PR 315)\n\nDocumentation\n* Cleanup Usage Documentation\n* Improve KafkaConsumer documentation (dpkp PR 341)\n* Update consumer documentation (sontek PR 317)\n* Add doc configuration for tox (sontek PR 316)\n* Switch to .rst doc format (sontek PR 321)\n* Fixup google groups link in README (sontek PR 320)\n* Automate documentation at kafka-python.readthedocs.org\n\nInternals\n* Switch integration testing from 0.8.2.0 to 0.8.2.1 (dpkp PR 402)\n* Fix most flaky tests, improve debug logging, improve fixture handling (dpkp)\n* General style cleanups (dpkp PR 394)\n* Raise error on duplicate topic-partition payloads in protocol grouping (dpkp)\n* Use module-level loggers instead of simply 'kafka' (dpkp)\n* Remove pkg_resources check for __version__ at runtime (dpkp PR 387)\n* Make external API consistently support python3 strings for topic (kecaps PR 361)\n* Fix correlation id overflow (dpkp PR 355)\n* Cleanup kafka/common structs (dpkp PR 338)\n* Use context managers in gzip_encode / gzip_decode (dpkp PR 337)\n* Save failed request as FailedPayloadsError attribute (jobevers PR 302)\n* Remove unused kafka.queue (mumrah)\n\n# 0.9.3 (Feb 3, 2015)\n\n* Add coveralls.io support (sontek PR 307)\n* Fix python2.6 threading.Event bug in ReentrantTimer (dpkp PR 312)\n* Add kafka 0.8.2.0 to travis integration tests (dpkp PR 310)\n* Auto-convert topics to utf-8 bytes in Producer (sontek PR 306)\n* Fix reference cycle between SimpleConsumer and ReentrantTimer (zhaopengzp PR 309)\n* Add Sphinx API docs (wedaly PR 282)\n* Handle additional error cases exposed by 0.8.2.0 kafka server (dpkp PR 295)\n* Refactor error class management (alexcb PR 289)\n* Expose KafkaConsumer in __all__ for easy imports (Dinoshauer PR 286)\n* SimpleProducer starts on random partition by default (alexcb PR 288)\n* Add keys to compressed messages (meandthewallaby PR 281)\n* Add new high-level KafkaConsumer class based on java client api (dpkp PR 234)\n* Add KeyedProducer.send_messages api (pubnub PR 277)\n* Fix consumer pending() method (jettify PR 276)\n* Update low-level demo in README (sunisdown PR 274)\n* Include key in KeyedProducer messages (se7entyse7en PR 268)\n* Fix SimpleConsumer timeout behavior in get_messages (dpkp PR 238)\n* Fix error in consumer.py test against max_buffer_size (rthille/wizzat PR 225/242)\n* Improve string concat performance on pypy / py3 (dpkp PR 233)\n* Reorg directory layout for consumer/producer/partitioners (dpkp/wizzat PR 232/243)\n* Add OffsetCommitContext (locationlabs PR 217)\n* Metadata Refactor (dpkp  PR 223)\n* Add Python 3 support (brutasse/wizzat - PR 227)\n* Minor cleanups - imports / README / PyPI classifiers (dpkp - PR 221)\n* Fix socket test (dpkp - PR 222)\n* Fix exception catching bug in test_failover_integration (zever - PR 216)\n\n# 0.9.2 (Aug 26, 2014)\n\n* Warn users that async producer does not reliably handle failures (dpkp - PR 213)\n* Fix spurious ConsumerFetchSizeTooSmall error in consumer (DataDog - PR 136)\n* Use PyLint for static error checking (dpkp - PR 208)\n* Strictly enforce str message type in producer.send_messages (dpkp - PR 211)\n* Add test timers via nose-timer plugin; list 10 slowest timings by default (dpkp)\n* Move fetching last known offset logic to a stand alone function (zever - PR 177)\n* Improve KafkaConnection and add more tests (dpkp - PR 196)\n* Raise TypeError if necessary when encoding strings (mdaniel - PR 204) \n* Use Travis-CI to publish tagged releases to pypi (tkuhlman / mumrah)\n* Use official binary tarballs for integration tests and parallelize travis tests (dpkp - PR 193)\n* Improve new-topic creation handling (wizzat - PR 174)\n\n# 0.9.1 (Aug 10, 2014)\n\n* Add codec parameter to Producers to enable compression (patricklucas - PR 166)\n* Support IPv6 hosts and network (snaury - PR 169)\n* Remove dependency on distribute (patricklucas - PR 163)\n* Fix connection error timeout and improve tests (wizzat - PR 158)\n* SimpleProducer randomization of initial round robin ordering (alexcb - PR 139)\n* Fix connection timeout in KafkaClient and KafkaConnection (maciejkula - PR 161)\n* Fix seek + commit behavior (wizzat - PR 148) \n\n\n# 0.9.0 (Mar 21, 2014)\n\n* Connection refactor and test fixes (wizzat - PR 134)\n* Fix when partition has no leader (mrtheb - PR 109)\n* Change Producer API to take topic as send argument, not as instance variable (rdiomar - PR 111)\n* Substantial refactor and Test Fixing (rdiomar - PR 88)\n* Fix Multiprocess Consumer on windows (mahendra - PR 62)\n* Improve fault tolerance; add integration tests (jimjh)\n* PEP8 / Flakes / Style cleanups (Vetoshkin Nikita; mrtheb - PR 59)\n* Setup Travis CI (jimjh - PR 53/54)\n* Fix import of BufferUnderflowError (jimjh - PR 49)\n* Fix code examples in README (StevenLeRoux - PR 47/48)\n\n# 0.8.0\n\n* Changing auto_commit to False in [SimpleConsumer](kafka/consumer.py), until 0.8.1 is release offset commits are unsupported\n* Adding fetch_size_bytes to SimpleConsumer constructor to allow for user-configurable fetch sizes\n* Allow SimpleConsumer to automatically increase the fetch size if a partial message is read and no other messages were read during that fetch request. The increase factor is 1.5\n* Exception classes moved to kafka.common\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0771484375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2015 David Arthur\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.099609375,
          "content": "recursive-include kafka *.py\ninclude README.rst\ninclude LICENSE\ninclude AUTHORS.md\ninclude CHANGES.md\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.9287109375,
          "content": "# Some simple testing tasks (sorry, UNIX only).\n\nFLAGS=\nKAFKA_VERSION=0.11.0.2\nSCALA_VERSION=2.12\n\nsetup:\n\tpip install -r requirements-dev.txt\n\tpip install -Ue .\n\nservers/$(KAFKA_VERSION)/kafka-bin:\n\tKAFKA_VERSION=$(KAFKA_VERSION) SCALA_VERSION=$(SCALA_VERSION) ./build_integration.sh\n\nbuild-integration: servers/$(KAFKA_VERSION)/kafka-bin\n\n# Test and produce coverage using tox. This is the same as is run on Travis\ntest37: build-integration\n\tKAFKA_VERSION=$(KAFKA_VERSION) SCALA_VERSION=$(SCALA_VERSION) tox -e py37 -- $(FLAGS)\n\ntest27: build-integration\n\tKAFKA_VERSION=$(KAFKA_VERSION) SCALA_VERSION=$(SCALA_VERSION) tox -e py27 -- $(FLAGS)\n\n# Test using pytest directly if you want to use local python. Useful for other\n# platforms that require manual installation for C libraries, ie. Windows.\ntest-local: build-integration\n\tKAFKA_VERSION=$(KAFKA_VERSION) SCALA_VERSION=$(SCALA_VERSION) pytest \\\n\t\t--pylint --pylint-rcfile=pylint.rc --pylint-error-types=EF $(FLAGS) kafka test\n\ncov-local: build-integration\n\tKAFKA_VERSION=$(KAFKA_VERSION) SCALA_VERSION=$(SCALA_VERSION) pytest \\\n\t\t--pylint --pylint-rcfile=pylint.rc --pylint-error-types=EF --cov=kafka \\\n\t\t--cov-config=.covrc --cov-report html $(FLAGS) kafka test\n\t@echo \"open file://`pwd`/htmlcov/index.html\"\n\n# Check the readme for syntax errors, which can lead to invalid formatting on\n# PyPi homepage (https://pypi.python.org/pypi/kafka-python)\ncheck-readme:\n\tpython setup.py check -rms\n\nclean:\n\trm -rf `find . -name __pycache__`\n\trm -f `find . -type f -name '*.py[co]' `\n\trm -f `find . -type f -name '*~' `\n\trm -f `find . -type f -name '.*~' `\n\trm -f `find . -type f -name '@*' `\n\trm -f `find . -type f -name '#*#' `\n\trm -f `find . -type f -name '*.orig' `\n\trm -f `find . -type f -name '*.rej' `\n\trm -f .coverage\n\trm -rf htmlcov\n\trm -rf docs/_build/\n\trm -rf cover\n\trm -rf dist\n\ndoc:\n\tmake -C docs html\n\t@echo \"open file://`pwd`/docs/_build/html/index.html\"\n\n.PHONY: all test37 test27 test-local cov-local clean doc\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 6.9765625,
          "content": "Kafka Python client\n------------------------\n\n.. image:: https://img.shields.io/badge/kafka-2.6%2C%202.5%2C%202.4%2C%202.3%2C%202.2%2C%202.1%2C%202.0%2C%201.1%2C%201.0%2C%200.11%2C%200.10%2C%200.9%2C%200.8-brightgreen.svg\n    :target: https://kafka-python.readthedocs.io/en/master/compatibility.html\n.. image:: https://img.shields.io/pypi/pyversions/kafka-python.svg\n    :target: https://pypi.python.org/pypi/kafka-python\n.. image:: https://coveralls.io/repos/dpkp/kafka-python/badge.svg?branch=master&service=github\n    :target: https://coveralls.io/github/dpkp/kafka-python?branch=master\n.. image:: https://img.shields.io/badge/license-Apache%202-blue.svg\n    :target: https://github.com/dpkp/kafka-python/blob/master/LICENSE\n.. image:: https://img.shields.io/pypi/dw/kafka-python.svg\n    :target: https://pypistats.org/packages/kafka-python\n.. image:: https://img.shields.io/pypi/v/kafka-python.svg\n    :target: https://pypi.org/project/kafka-python\n.. image:: https://img.shields.io/pypi/implementation/kafka-python\n    :target: https://github.com/dpkp/kafka-python/blob/master/setup.py\n\n\n**DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING**\n\nPython client for the Apache Kafka distributed stream processing system.\nkafka-python is designed to function much like the official java client, with a\nsprinkling of pythonic interfaces (e.g., consumer iterators).\n\nkafka-python is best used with newer brokers (0.9+), but is backwards-compatible with\nolder versions (to 0.8.0). Some features will only be enabled on newer brokers.\nFor example, fully coordinated consumer groups -- i.e., dynamic partition\nassignment to multiple consumers in the same group -- requires use of 0.9+ kafka\nbrokers. Supporting this feature for earlier broker releases would require\nwriting and maintaining custom leadership election and membership / health\ncheck code (perhaps using zookeeper or consul). For older brokers, you can\nachieve something similar by manually assigning different partitions to each\nconsumer instance with config management tools like chef, ansible, etc. This\napproach will work fine, though it does not support rebalancing on failures.\nSee <https://kafka-python.readthedocs.io/en/master/compatibility.html>\nfor more details.\n\nPlease note that the master branch may contain unreleased features. For release\ndocumentation, please see readthedocs and/or python's inline help.\n\n>>> pip install kafka-python\n\n\nKafkaConsumer\n*************\n\nKafkaConsumer is a high-level message consumer, intended to operate as similarly\nas possible to the official java client. Full support for coordinated\nconsumer groups requires use of kafka brokers that support the Group APIs: kafka v0.9+.\n\nSee <https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html>\nfor API and configuration details.\n\nThe consumer iterator returns ConsumerRecords, which are simple namedtuples\nthat expose basic message attributes: topic, partition, offset, key, and value:\n\n>>> from kafka import KafkaConsumer\n>>> consumer = KafkaConsumer('my_favorite_topic')\n>>> for msg in consumer:\n...     print (msg)\n\n>>> # join a consumer group for dynamic partition assignment and offset commits\n>>> from kafka import KafkaConsumer\n>>> consumer = KafkaConsumer('my_favorite_topic', group_id='my_favorite_group')\n>>> for msg in consumer:\n...     print (msg)\n\n>>> # manually assign the partition list for the consumer\n>>> from kafka import TopicPartition\n>>> consumer = KafkaConsumer(bootstrap_servers='localhost:1234')\n>>> consumer.assign([TopicPartition('foobar', 2)])\n>>> msg = next(consumer)\n\n>>> # Deserialize msgpack-encoded values\n>>> consumer = KafkaConsumer(value_deserializer=msgpack.loads)\n>>> consumer.subscribe(['msgpackfoo'])\n>>> for msg in consumer:\n...     assert isinstance(msg.value, dict)\n\n>>> # Access record headers. The returned value is a list of tuples\n>>> # with str, bytes for key and value\n>>> for msg in consumer:\n...     print (msg.headers)\n\n>>> # Get consumer metrics\n>>> metrics = consumer.metrics()\n\n\nKafkaProducer\n*************\n\nKafkaProducer is a high-level, asynchronous message producer. The class is\nintended to operate as similarly as possible to the official java client.\nSee <https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html>\nfor more details.\n\n>>> from kafka import KafkaProducer\n>>> producer = KafkaProducer(bootstrap_servers='localhost:1234')\n>>> for _ in range(100):\n...     producer.send('foobar', b'some_message_bytes')\n\n>>> # Block until a single message is sent (or timeout)\n>>> future = producer.send('foobar', b'another_message')\n>>> result = future.get(timeout=60)\n\n>>> # Block until all pending messages are at least put on the network\n>>> # NOTE: This does not guarantee delivery or success! It is really\n>>> # only useful if you configure internal batching using linger_ms\n>>> producer.flush()\n\n>>> # Use a key for hashed-partitioning\n>>> producer.send('foobar', key=b'foo', value=b'bar')\n\n>>> # Serialize json messages\n>>> import json\n>>> producer = KafkaProducer(value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n>>> producer.send('fizzbuzz', {'foo': 'bar'})\n\n>>> # Serialize string keys\n>>> producer = KafkaProducer(key_serializer=str.encode)\n>>> producer.send('flipflap', key='ping', value=b'1234')\n\n>>> # Compress messages\n>>> producer = KafkaProducer(compression_type='gzip')\n>>> for i in range(1000):\n...     producer.send('foobar', b'msg %d' % i)\n\n>>> # Include record headers. The format is list of tuples with string key\n>>> # and bytes value.\n>>> producer.send('foobar', value=b'c29tZSB2YWx1ZQ==', headers=[('content-encoding', b'base64')])\n\n>>> # Get producer performance metrics\n>>> metrics = producer.metrics()\n\n\nThread safety\n*************\n\nThe KafkaProducer can be used across threads without issue, unlike the\nKafkaConsumer which cannot.\n\nWhile it is possible to use the KafkaConsumer in a thread-local manner,\nmultiprocessing is recommended.\n\n\nCompression\n***********\n\nkafka-python supports the following compression formats:\n\n- gzip\n- LZ4\n- Snappy\n- Zstandard (zstd)\n\ngzip is supported natively, the others require installing additional libraries.\nSee <https://kafka-python.readthedocs.io/en/master/install.html> for more information.\n\n\nOptimized CRC32 Validation\n**************************\n\nKafka uses CRC32 checksums to validate messages. kafka-python includes a pure\npython implementation for compatibility. To improve performance for high-throughput\napplications, kafka-python will use `crc32c` for optimized native code if installed.\nSee <https://kafka-python.readthedocs.io/en/master/install.html> for installation instructions.\nSee https://pypi.org/project/crc32c/ for details on the underlying crc32c lib.\n\n\nProtocol\n********\n\nA secondary goal of kafka-python is to provide an easy-to-use protocol layer\nfor interacting with kafka brokers via the python repl. This is useful for\ntesting, probing, and general experimentation. The protocol support is\nleveraged to enable a KafkaClient.check_version() method that\nprobes a kafka broker and attempts to identify which version it is running\n(0.8.0 to 2.6+).\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "build_integration.sh",
          "type": "blob",
          "size": 2.630859375,
          "content": "#!/bin/bash\n\n: ${ALL_RELEASES:=\"0.8.2.2 0.9.0.1 0.10.1.1 0.10.2.2 0.11.0.3 1.0.2 1.1.1 2.0.1 2.1.1 2.2.1 2.3.0 2.4.0 2.5.0\"}\n: ${SCALA_VERSION:=2.11}\n: ${DIST_BASE_URL:=https://archive.apache.org/dist/kafka/}\n: ${KAFKA_SRC_GIT:=https://github.com/apache/kafka.git}\n\n# On travis CI, empty KAFKA_VERSION means skip integration tests\n# so we don't try to get binaries\n# Otherwise it means test all official releases, so we get all of them!\nif [ -z \"$KAFKA_VERSION\" -a -z \"$TRAVIS\" ]; then\n  KAFKA_VERSION=$ALL_RELEASES\nfi\n\npushd servers\n  mkdir -p dist\n  pushd dist\n    for kafka in $KAFKA_VERSION; do\n      if [ \"$kafka\" == \"trunk\" ]; then\n        if [ ! -d \"$kafka\" ]; then\n          git clone $KAFKA_SRC_GIT $kafka\n        fi\n        pushd $kafka\n          git pull\n          ./gradlew -PscalaVersion=$SCALA_VERSION -Pversion=$kafka releaseTarGz -x signArchives\n        popd\n        # Not sure how to construct the .tgz name accurately, so use a wildcard (ugh)\n        tar xzvf $kafka/core/build/distributions/kafka_*.tgz -C ../$kafka/\n        rm $kafka/core/build/distributions/kafka_*.tgz\n        rm -rf ../$kafka/kafka-bin\n        mv ../$kafka/kafka_* ../$kafka/kafka-bin\n      else\n        echo \"-------------------------------------\"\n        echo \"Checking kafka binaries for ${kafka}\"\n        echo\n        if [ \"$kafka\" == \"0.8.0\" ]; then\n          KAFKA_ARTIFACT=\"kafka_2.8.0-${kafka}.tar.gz\"\n        else if [ \"$kafka\" \\> \"2.4.0\" ]; then\n          KAFKA_ARTIFACT=\"kafka_2.12-${kafka}.tgz\"\n        else\n          KAFKA_ARTIFACT=\"kafka_${SCALA_VERSION}-${kafka}.tgz\"\n        fi\n        fi\n        if [ ! -f \"../$kafka/kafka-bin/bin/kafka-run-class.sh\" ]; then\n          if [ -f \"${KAFKA_ARTIFACT}\" ]; then\n            echo \"Using cached artifact: ${KAFKA_ARTIFACT}\"\n          else\n            echo \"Downloading kafka ${kafka} tarball\"\n            TARBALL=${DIST_BASE_URL}${kafka}/${KAFKA_ARTIFACT}\n            if command -v wget 2>/dev/null; then\n              wget -N $TARBALL\n            else\n              echo \"wget not found... using curl\"\n              curl -f $TARBALL -o ${KAFKA_ARTIFACT}\n            fi\n          fi\n          echo\n          echo \"Extracting kafka ${kafka} binaries\"\n          tar xzvf ${KAFKA_ARTIFACT} -C ../$kafka/\n          rm -rf ../$kafka/kafka-bin\n          mv ../$kafka/${KAFKA_ARTIFACT/%.t*/} ../$kafka/kafka-bin\n          if [ ! -f \"../$kafka/kafka-bin/bin/kafka-run-class.sh\" ]; then\n            echo \"Extraction Failed ($kafka/kafka-bin/bin/kafka-run-class.sh does not exist)!\"\n            exit 1\n          fi\n        else\n          echo \"$kafka is already installed in servers/$kafka/ -- skipping\"\n        fi\n      fi\n      echo\n    done\n  popd\npopd\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "example.py",
          "type": "blob",
          "size": 1.9365234375,
          "content": "#!/usr/bin/env python\nimport threading, time\n\nfrom kafka import KafkaAdminClient, KafkaConsumer, KafkaProducer\nfrom kafka.admin import NewTopic\n\n\nclass Producer(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n        self.stop_event = threading.Event()\n\n    def stop(self):\n        self.stop_event.set()\n\n    def run(self):\n        producer = KafkaProducer(bootstrap_servers='localhost:9092')\n\n        while not self.stop_event.is_set():\n            producer.send('my-topic', b\"test\")\n            producer.send('my-topic', b\"\\xc2Hola, mundo!\")\n            time.sleep(1)\n\n        producer.close()\n\n\nclass Consumer(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n        self.stop_event = threading.Event()\n\n    def stop(self):\n        self.stop_event.set()\n\n    def run(self):\n        consumer = KafkaConsumer(bootstrap_servers='localhost:9092',\n                                 auto_offset_reset='earliest',\n                                 consumer_timeout_ms=1000)\n        consumer.subscribe(['my-topic'])\n\n        while not self.stop_event.is_set():\n            for message in consumer:\n                print(message)\n                if self.stop_event.is_set():\n                    break\n\n        consumer.close()\n\n\ndef main():\n    # Create 'my-topic' Kafka topic\n    try:\n        admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n\n        topic = NewTopic(name='my-topic',\n                         num_partitions=1,\n                         replication_factor=1)\n        admin.create_topics([topic])\n    except Exception:\n        pass\n\n    tasks = [\n        Producer(),\n        Consumer()\n    ]\n\n    # Start threads of a publisher/producer and a subscriber/consumer to 'my-topic' Kafka topic\n    for t in tasks:\n        t.start()\n\n    time.sleep(10)\n\n    # Stop threads\n    for task in tasks:\n        task.stop()\n\n    for task in tasks:\n        task.join()\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "kafka",
          "type": "tree",
          "content": null
        },
        {
          "name": "pylint.rc",
          "type": "blob",
          "size": 0.146484375,
          "content": "[TYPECHECK]\nignored-classes=SyncManager,_socketobject\nignored-modules=kafka.vendor.six.moves\ngenerated-members=py.*\n\n[MESSAGES CONTROL]\ndisable=E1129\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 0.142578125,
          "content": "coveralls\ncrc32c\ndocker-py\nflake8\nlz4\nmock\npy\npylint\npytest\npytest-cov\npytest-mock\npytest-pylint\npython-snappy\nSphinx\nsphinx-rtd-theme\ntox\nxxhash\n"
        },
        {
          "name": "servers",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0595703125,
          "content": "[bdist_wheel]\nuniversal=1\n\n[metadata]\nlicense_file = LICENSE\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.1875,
          "content": "import os\nimport sys\n\nfrom setuptools import setup, Command, find_packages\n\n# Pull version from source without importing\n# since we can't import something we haven't built yet :)\nexec(open('kafka/version.py').read())\n\n\nclass Tox(Command):\n\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    @classmethod\n    def run(cls):\n        import tox\n        sys.exit(tox.cmdline([]))\n\n\ntest_require = ['tox', 'mock']\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\nwith open(os.path.join(here, 'README.rst')) as f:\n    README = f.read()\n\nsetup(\n    name=\"kafka-python\",\n    version=__version__,\n\n    tests_require=test_require,\n    extras_require={\n        \"crc32c\": [\"crc32c\"],\n        \"lz4\": [\"lz4\"],\n        \"snappy\": [\"python-snappy\"],\n        \"zstd\": [\"zstandard\"],\n    },\n    cmdclass={\"test\": Tox},\n    packages=find_packages(exclude=['test']),\n    author=\"Dana Powers\",\n    author_email=\"dana.powers@gmail.com\",\n    url=\"https://github.com/dpkp/kafka-python\",\n    license=\"Apache License 2.0\",\n    description=\"Pure Python client for Apache Kafka\",\n    long_description=README,\n    keywords=[\n        \"apache kafka\",\n        \"kafka\",\n    ],\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 2\",\n        \"Programming Language :: Python :: 2.7\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.4\",\n        \"Programming Language :: Python :: 3.5\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ]\n)\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.904296875,
          "content": "[tox]\nenvlist = py{38,39,310,311,312,py}, docs\n\n[pytest]\ntestpaths = kafka test\naddopts = --durations=10\nlog_format = %(created)f %(filename)-23s %(threadName)s %(message)s\n\n[gh-actions]\npython =\n    3.8: py38\n    3.9: py39\n    3.10: py310\n    3.11: py311\n    3.12: py312\n    pypy-3.9: pypy\n\n[testenv]\ndeps =\n    pytest\n    pytest-cov\n    pylint\n    pytest-pylint\n    pytest-mock\n    mock\n    python-snappy\n    zstandard\n    lz4\n    xxhash\n    crc32c\ncommands =\n    pytest {posargs:--pylint --pylint-rcfile=pylint.rc --pylint-error-types=EF --cov=kafka --cov-config=.covrc}\nsetenv =\n    CRC32C_SW_MODE = auto\n    PROJECT_ROOT = {toxinidir}\npassenv = KAFKA_VERSION\n\n\n[testenv:pypy]\n# pylint is super slow on pypy...\ncommands = pytest {posargs:--cov=kafka --cov-config=.covrc}\n\n[testenv:docs]\ndeps =\n    sphinx_rtd_theme\n    sphinx\n\ncommands =\n    sphinx-apidoc -o docs/apidoc/ kafka/\n    sphinx-build -b html docs/ docs/_build\n"
        },
        {
          "name": "travis_java_install.sh",
          "type": "blob",
          "size": 0.7783203125,
          "content": "#!/bin/bash\n\n# borrowed from: https://github.com/mansenfranzen/pywrangler/blob/master/tests/travis_java_install.sh\n\n# Kafka requires Java 8 in order to work properly. However, TravisCI's Ubuntu\n# 16.04 ships with Java 11 and Java can't be set with `jdk` when python is\n# selected as language. Ubuntu 14.04 does not work due to missing python 3.7\n# support on TravisCI which does have Java 8 as default.\n\n# show current JAVA_HOME and java version\necho \"Current JAVA_HOME: $JAVA_HOME\"\necho \"Current java -version:\"\nwhich java\njava -version\n\necho \"Updating JAVA_HOME\"\n# change JAVA_HOME to Java 8\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n\necho \"Updating PATH\"\nexport PATH=${PATH/\\/usr\\/local\\/lib\\/jvm\\/openjdk11\\/bin/$JAVA_HOME\\/bin}\n\necho \"New java -version\"\nwhich java\njava -version\n"
        }
      ]
    }
  ]
}