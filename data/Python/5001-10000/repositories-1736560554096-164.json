{
  "metadata": {
    "timestamp": 1736560554096,
    "page": 164,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "llmware-ai/llmware",
      "stars": 8401,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".devcontainer",
          "type": "blob",
          "size": 0.01171875,
          "content": "devcontainer"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1337890625,
          "content": "# Backup fiels\n~*\n\n# Python build / packaging\nbuild/\ndist/\n__pycache__/\nllmware.egg-info/\nvenv/\n\n# Jekyll\ndocs/_site/\n\n\n# Mac\n.DS_Store/\n"
        },
        {
          "name": ".python-version",
          "type": "blob",
          "size": 0.0068359375,
          "content": "3.11.0\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0927734375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023-2024 by AI Bloks for LLMWare \n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 52.6484375,
          "content": "\nCopyright 2023 and 2024 by llmware\n\nNOTICE - Overview\n\nLLMWare Models.  This software contains links to the LLMWare public model repository currently hosted on Huggingface (www.huggingface.co/llmware).  LLMWare models in this repository are generally licensed under a separate Apache License 2.0 license, e.g., https://www.apache.org/licenses/LICENSE-2.0.  Where there are any exceptions to Apache 2.0, it is noted on the Model Card, e.g., Llama 2 Community License or CC-BY-SA-4.0, and is following licensing conditions of the underlying base models.   All of the LLMWare models that are listed in the ModelCatalog in this repository are permissively licensed and may be used for commercial purposes in conjunction with llmware, subject to the license terms found with the model cards.\n\n3rd Party Models.  The llmware software package also provides the ability to use other 3rd party models in conjunction with the llmware software.  Use of any third party models is subject to the licensing terms of those models.\n\nSample Testing Files.  This software includes sample files that are made available for testing, including documents and audio files.   These files have either been derived from open public domain sources, or produced as original materials by the llmware team to be used as test samples and  examples.  These files are made available solely for purposes of testing and illustrating the use of key llmware functions, and should not be used, distributed or published in any other manner.   The audio files, in particular, may be subject to copyright protections that preclude any other form of publishing beyond limited 'fair use' as testing files.\n\nCompiled Enabling Components.  This software consists primarily of Python source code, but also includes back-end enabling components consisting of prebuilt compiled C/C++ shared libraries and supporting 3rd party dependencies:\n\n    * libpdf_llmware - this is a PDF parser written in C developed and copyrighted separately by llmware, which implements the ISO-32000 specification.  The compiled parser is provided as a shared library as part of the llmware software package, under an Apache 2.0 license, but the source code has not been provided in the llmware project currently.  The interfaces to the shared library are in the Python source code in the Parser module, which enable the user to access, configure and control the behavior of the PDF parser as part of building applications and other solutions with llmware.\n    \n    * liboffice_llmware - this is an Office XML parser written in C developed and copyrighted by llmware, which implements the ISO-29500 Office Open XML specification for documents that conform with Word, Powerpoint and Excel formats.   The compiled parser is provided as a shared library as part of the llmware software package, under an Apache 2.0 license, but the source code has not been provided in the llmware project currently.  The interfaces to the shared library are in the Python source code in the Parser module, which enable the user to access, configure and control the behavior of the Office parser as part of building applications and other solutions with llmware.\n    \n    * libgraph_llmware - this module provides a set of NLP-based utility functions, written in C developed and copyrighted by llmware, and exposed in the Python code in the graph module.   The compiled code is provided as a shared library as part of the llmware software package, under an Apache 2.0 license, but the source code has not been released in the llmware project currently.\n    \n    * llama.cpp / GGUF - this a prebuilt shared library implementation of Llama.CPP and associated GGML middleware components, licensed under a MIT license with the source code and all other information in the repository:  www.github.com/ggerganov/llama.cpp.git.  The llmware implementation generally follows the core Python interface for Llama.CPP as provided in the repository:  www.github.com/abetlen/llama-cpp-python.git, although there will be differences from time-to-time, and llmware may have additional features not found in other Python interfaces and vice versa.   The llmware shared library implementation is regularly updated and re-compiled, and tested for integration into llmware, but may depart from the current source code of llama.cpp.\n    \n    * whisper.cpp / GGML - this is a prebuilt shared library implementation of Whisper.CPP and associated GGML middleware components, licensed under a MIT license with the source code and all other information in the repository:  www.github.com/ggerganov/whisper.cpp.git.   The llmware Python interface for Whisper.CPP was developed originally, but took inspiration from www.github.com/carloscdias/whisper-cpp-python.git, which is provided under a MIT license.  The llmware shared library implementation is regularly updated and recompiled, and tested for integration into llmware, but may depart from the current source code of whisper.cpp.\n\n    Why shared libraries rather than source code?  The objective of including these prebuilt shared libraries in this project is to enable a complete LLM-based pipeline development framework that works \"out of the box\" on multiple platforms, and also enables a developer to rapidly and intuitively scale to large, complex solutions.   We view these shared libraries as enabling components with the core of the llmware project consisting of the higher-level classes and functions exposed in the Python source code.  In the future, we may revisit the decision to publish the source code for the llmware parsers (e.g., libpdf_llmware, liboffice_llmware, libgraph_llmware), and/or provide under a separate project repository focused on lower-level components and in C/C++.   Generally, these parsers are complex, standalone, low-level code that implements the arcane rules of various ISO standards, and for the foreseeable future, we do not have the bandwidth to manage a full open source documentation, testing, and lifecycle around that source code base - without it overwhelming the main objective of llmware.\n\nVector Databases.  This software provides integration to a wide variety of vector database resources, using the open source Python drivers and associated SDKs provided by the vector database vendor with the licensing details outlined below.  Users need to install any vector databases separately and independently, subject to the licensing terms from those vendors.  llmware does not provide any licenses to vector databases, and llmware does not require the use of a vector database for a wide range of use cases.  In several Fast Start examples, we demonstrate how to use \"no-install\" vector databases, such as FAISS (MIT license), ChromaDB (Apache 2.0 license - www.github.com/chroma-core/chroma.git) and LanceDB (Apache 2.0 license - www.github.com/lancedb/lancedb.git), but no license is provided to those separate resources.\n\nGeneral Purpose Databases.  This software provides integration to three core database resources - Postgres, MongoDB and SQLite.  llmware connects to these resources using open source Python and C drivers.  Users need to install these databases separately and indendently, subject to the licensing terms from those vendors.  llmware does not provide any licenses to databases, and llmware does not require the use of a database for a wide range of use cases.\n\nHuggingface Integration.  This software provides features and functions that enable access to models, tokenizers and datasets hosted in Huggingface repositories and accessible via the Huggingface transformers, tokenizers, datasets and huggingface-hub libraries.  In providing interfaces to these resources, llmware uses transformers, in particular, which is provided by Huggingface under an Apache 2.0 license (see www.github.com/huggingface/transformers.git).  In addition, related to these interfaces, llmware provides code that was influenced, derived, and in a few limited cases, copied, from transformers source code to facilitate the integration.   Some of this transformers code, especially the underlying model class code, is subject to additional copyright notices from HuggingFace, Google AI, EleutherAI, NVIDIA and potentially others for specific models.  For use of any individual third party model, accessed through a Huggingface repository, we recommend reviewing the individual model card to confirm the licensing terms and other associated copyrights.\n\n=================================================================================================\n\nOpen Source Dependencies and Optional Components\n\nPlease note that the list below includes 'first-level' dependencies but does not include potential 'second-level' dependencies included within the components outlined below.\n\n3rd Party Open Source libraries, drivers and tools in Python (pip install) and C/C++ dependencies used in conjunction for the llmware software package:\n   \n3-clause BSD License (https://opensource.org/license/bsd-3-clause/)\n   * Software: libzip (https://libzip.org/)  [C library]\n   * Software: numpy (https://github.com/numpy/numpy) [Python pip install]\n   * Software: torch (https://github.com/pytorch/pytorch) [Python pip install]\n   * Software: colorama (https://github.com/tartley/colorama) [Python pip install]\n   * Software - Optional - lxml (https://github.com/lxml/lxml) [Optional / Python pip install]\n\nApache License 2.0 (https://www.apache.org/licenses/LICENSE-2.0)\n   * Software: boto3 (https://github.com/boto/boto3) [Python pip install]\n   * Software: mongo-c-driver (https://github.com/mongodb/mongo-c-driver) [C library]\n   * Software: pymongo (http://github.com/mongodb/mongo-python-driver)  [Python pip install]\n   * Software: tokenizers (https://github.com/huggingface/tokenizers) [Python pip install]\n   * Software: transformers (https://github.com/huggingface/transformers) [Python pip install]\n   * Software: huggingface-hub (https://github.com/huggingface/huggingface-hub) [Python pip install]\n   * Software: requests (https://github.com/psf/requests) [Python pip install]\n   * Software - Optional - pymilvus (https://github.com/milvus-io/pymilvus)  [Optional / Python pip install]\n   * Software - Optional - pytesseract (https://github.com/madmaze/pytesseract)  [Optional / Python pip install]\n   * Software - Optional - datasets (https://github.com/huggingface/datasets) [Optional / Python pip install]\n   * Software - Optional - sentence-transformers (https://github.com/UKPLabs/sentence-transformers) [Optional / Python pip install]\n   * Software - Optional - yfinance (https://github.com/ranaroussi/yfinance) [Optional / Python pip install]\n   * Software - Optional - chromadb (https://github.com/chroma-core/chroma) [Optional / Python pip install]\n   * Optional - Optional - neo4j-python-driver (https://github.com/neo4j/neo4j-python-driver) [Optional / Python pip install]\n   * Optional - Optional - Tiny-Llama base model (https://github.com/jzhang38/TinyLlama) [Separate Download and install]\n   * Optional - Optional - tesseract (https://github.com/tesseract-ocr/tesseract) [Optional / not included / C library]\n   \nICS License (https://opensource.org/license/isc-license-txt)\n   * Software: librosa - source code and license at: https://github.com/librosa/librosa [Python pip install]\n\n\nDatabase Drivers used in llmware\n   * Mongo C driver - libmongoc / libbson - Apache 2.0 (https://www.github.com/mongodb/mongo-c-driver) [C library]\n   * PostgreSQL C driver - libpq - PostgresSQL Global Development Group (https://www.github.com/postgres/postgres/blob/master/copyright) [C library]\n   * PostgreSQL psycopg - unmodified and linked dynamically from pypi release - GNU Lesser General Public License (https://www.github.com/psyocopg/psycopg/LICENSE.txt) [Python pip install]\n   * SQLite C driver - libsqlite3 - (https://www.github.com/LuaDist/libsqlite3) [C library]\n   * psycopg2 & psycopg2-binary - GNU Lesser General Public License - no modifications - it is provided through pip install as standard python library\n\n\nPNG Reference Library License\n   * libpng: source code and license (https://github.com/pnggroup/libpng) [C library]\n   \nlibtiff License (https://spdx.org/licenses/libtiff.html)\n   * Software: libtiff (http://www.libtiff.org/) [C library]\n\nMIT License (https://opensource.org/license/mit/)\n   * Software: libxml2 (https://github/com/GNOME/libxml2) [C library]\n   * Software: llama.cpp (https://github.com/ggerganov/llama.cpp) [C/C++ library]\n   * Software: whisper.cpp (https://github.com/ggerganov/whisper.cpp) [C/C++ library]\n   * Software: openai (https://github.com/openai/openai-python) [Python pip install]\n   * Software: Wikipedia-API (https://github.com/martin-majlis/Wikipedia-API) [Python pip install]\n   * Software: einops (https://github.com/arogozhinov/einops) [Python pip install]\n   * Software - Optional - beautifulsoup4 (https://pypi.org/project/beautifulsoup4/) [Optional / Python pip install]\n   * Software - Optional - Whisper models - openai (https://github.com/openai/whisper) [Separate download and install]\n   * Software - Optional - anthropic (https://github.com/anthropics/anthropic-sdk-python) [Optional / Python pip install]\n   * Software - Optional - faise-cpu (https://github.com/kyamagu/faiss-wheels) [Optional / Python pip install]\n   * Software - Optional - cohere (https://github.com/cohere-ai/cohere-python) [Optional / Python pip install]\n   * Software - Optional - tabulate (https://github.com/astanin/python-tabulate) [Optional / Python pip install]\n   * Software - Optional - pdf2image (https://github.com/Belval/pdf2image)  [Optional / Python pip install]\n   * Software - Optional - word2number (https://github.com/akshaynagpal/w2n) [Optional / Python pip install]\n   \nzlib License (https://github.com/madler/zlib/blob/develop/LICENSE)\n   * Software: zlib (https://www.zlib.net/) [C library]\n\nDatabases - separate and optional components that may be used in conjunction with llmware and require user to license and install independently:\n\n   * Postgres: source code for Postgres database provided at: https://github.com/postgres.  Copyright is by PostgreSQL Global Development Group and Regents of the University of California - (https://github.com/postgres/postgres/blob/master/copyright) - with license terms outlined below in the Copyright Notices section.  [Separate download and install]\n   \n   * MongoDB: source code for Community edition provided at: https://github.com/mongodb/mongo - \"MongoDB is free and the source is available\" subject to the Server Side Public License (SSPL) v1 - (https://github.com/mongodb/mongo/blob/master/LICENSE-Community.txt).  [Separate download and install]\n   \n   * SQLite: source code provided at: https://github.com/sqlite/sqlite - not subject to copyright - \"May you share freely, never taking more than you give.\"   [Separate download and install]\n\nVector Databases - separate and optional open source components that may be used in conjunction with llmware and require user to license and install independently:\n\n   * Milvus:    Apache 2.0 license - source code and license at: https://github.com/milvus-io/milvus\n   * Qdrant:    Apache 2.0 license - source code and license at: https://github.com/qdrant/qdrant\n   * ChromaDB:  Apache 2.0 license - source code and license at: https://github.com/chroma-core/chroma\n   * PGVector:  Postgres Development Group license - source code and license at: https://github.com/pgvector/pgvector\n   * Neo4J:     GPL-3.0 license - source and license at: https://github.com/neo4j/neo4j\n   * FAISS:     MIT license - source code and license at: https://github.com/facebookresearch/faiss\n   * LanceDB:   Apache 2.0 license - source code and license at:  www.github.com/lancedb/lancedb.git\n\n=================================================================================================\n\nRequired Copyright Notices Per Open Source Licenses for Components Distributed Directly with a Full Clone of the Repository - does not include components installed through a separate pip install process, or additional components used in conjunction with LLMWare, but installed independently from this repository.\n\nName: llama.cpp\nLicense: MIT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nName: whisper.cpp\nLicense: MIT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nName: libxml2\nLicense: MIT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nName: einops\nLicense: MIT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nName: wikipedia-api\nLicense: MIT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nName: openai (python sdk)\nLicense: MIT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\nName: mongo-c-driver (libmongoc and libbson)\nLicense: Apache 2.0 Copyright Notice\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\nName: pymongo\nLicense: Apache 2.0 Copyright Notice\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\nName: boto3\nLicense: Apache 2.0 Copyright Notice\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\nName: requests\nLicense: Apache 2.0 Copyright Notice\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\nName: transformers (Huggingface)\nLicense: Apache 2.0 Copyright Notice\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\nName: tokenizers (Huggingface)\nLicense: Apache 2.0 Copyright Notice\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\nName: huggingface_hub (HuggingFace)\nLicense: Apache 2.0 Copyright Notice\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\nName: librosa\nLicense: ISC License\nCopyright (c) 2013--2023, librosa development team.\nPermission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies.\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\nName: psycopg  [no modifications - it is provided through pip install as standard python library]\nLicense: Lesser GPL\n\npsycopg2 is free software: you can redistribute it and/or modify it\nunder the terms of the GNU Lesser General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\npsycopg2 is distributed in the hope that it will be useful, but WITHOUT\nANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\nFITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public\nLicense for more details.\n\nIn addition, as a special exception, the copyright holders give\npermission to link this program with the OpenSSL library (or with\nmodified versions of OpenSSL that use the same license as OpenSSL),\nand distribute linked combinations including the two.\n\nYou must obey the GNU Lesser General Public License in all respects for\nall of the code used other than OpenSSL. If you modify file(s) with this\nexception, you may extend this exception to your version of the file(s),\nbut you are not obligated to do so. If you do not wish to do so, delete\nthis exception statement from your version. If you delete this exception\nstatement from all source files in the program, then also delete it here.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with psycopg2 (see the doc/ directory.)\nIf not, see <https://www.gnu.org/licenses/>.\n\n\n\nName: numpy\nLicense: BSD-3-Clause\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nName: torch\nLicense: BSD-3-Clause\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nName: colorama\nLicense: BSD-3-Clause\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nName: libzip\nLicense: BSD-3-Clause\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nName: pgvector\nLicense: Postgres License\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\n\nPortions Copyright (c) 1994, The Regents of the University of California\n\nPermission to use, copy, modify, and distribute this software and its\ndocumentation for any purpose, without fee, and without a written agreement\nis hereby granted, provided that the above copyright notice and this\nparagraph and the following two paragraphs appear in all copies.\n\nIN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR\nDIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING\nLOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS\nDOCUMENTATION, EVEN IF THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n\nTHE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY\nAND FITNESS FOR A PARTICULAR PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS\nON AN \"AS IS\" BASIS, AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATIONS TO\nPROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n\n\nName: libpq\nLicense: GNU LESSER GENERAL PUBLIC LICENSE & Postgres License\n\nPostgreSQL License\n\nPostgreSQL is released under the PostgreSQL License, a liberal Open Source license, similar to the BSD or MIT licenses.\n\nPostgreSQL Database Management System\n(formerly known as Postgres, then as Postgres95)\n\nPortions Copyright © 1996-2024, The PostgreSQL Global Development Group\n\nPortions Copyright © 1994, The Regents of the University of California\n\nPermission to use, copy, modify, and distribute this software and its documentation for any purpose, without fee, and without a written agreement is hereby granted, provided that the above copyright notice and this paragraph and the following two paragraphs appear in all copies.\n\nIN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nTHE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS, AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATIONS TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n\n\nGNU Lesser General Public License - Version 3, 29 June 2007\n\nCopyright © 2007 Free Software Foundation, Inc. <https://fsf.org/>\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nThis version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below.\n\n0. Additional Definitions.\nAs used herein, “this License” refers to version 3 of the GNU Lesser General Public License, and the “GNU GPL” refers to version 3 of the GNU General Public License.\n\n“The Library” refers to a covered work governed by this License, other than an Application or a Combined Work as defined below.\n\nAn “Application” is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library.\n\nA “Combined Work” is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the “Linked Version”.\n\nThe “Minimal Corresponding Source” for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version.\n\nThe “Corresponding Application Code” for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work.\n\n1. Exception to Section 3 of the GNU GPL.\nYou may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL.\n\n2. Conveying Modified Versions.\nIf you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version:\n\na) under this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or\nb) under the GNU GPL, with none of the additional permissions of this License applicable to that copy.\n3. Object Code Incorporating Material from Library Header Files.\nThe object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following:\n\na) Give prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License.\nb) Accompany the object code with a copy of the GNU GPL and this license document.\n4. Combined Works.\nYou may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following:\n\na) Give prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License.\nb) Accompany the Combined Work with a copy of the GNU GPL and this license document.\nc) For a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document.\nd) Do one of the following:\n0) Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.\n1) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user's computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version.\ne) Provide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.)\n5. Combined Libraries.\nYou may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following:\n\na) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License.\nb) Give prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work.\n6. Revised Versions of the GNU Lesser General Public License.\nThe Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\n\nEach version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation.\n\nIf the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy's public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.\n\n\nName: libpng\nLicense: PNG Reference Library Copyright Noticee and License version 2\n\n * Copyright (c) 1995-2024 The PNG Reference Library Authors.\n * Copyright (c) 2018-2024 Cosmin Truta.\n * Copyright (c) 2000-2002, 2004, 2006-2018 Glenn Randers-Pehrson.\n * Copyright (c) 1996-1997 Andreas Dilger.\n * Copyright (c) 1995-1996 Guy Eric Schalnat, Group 42, Inc.\n\nThe software is supplied \"as is\", without warranty of any kind,\nexpress or implied, including, without limitation, the warranties\nof merchantability, fitness for a particular purpose, title, and\nnon-infringement.  In no event shall the Copyright owners, or\nanyone distributing the software, be liable for any damages or\nother liability, whether in contract, tort or otherwise, arising\nfrom, out of, or in connection with the software, or the use or\nother dealings in the software, even if advised of the possibility\nof such damage.\n\nPermission is hereby granted to use, copy, modify, and distribute\nthis software, or portions hereof, for any purpose, without fee,\nsubject to the following restrictions:\n\n 1. The origin of this software must not be misrepresented; you\n    must not claim that you wrote the original software.  If you\n    use this software in a product, an acknowledgment in the product\n    documentation would be appreciated, but is not required.\n\n 2. Altered source versions must be plainly marked as such, and must\n    not be misrepresented as being the original software.\n\n 3. This Copyright notice may not be removed or altered from any\n    source or altered source distribution.\n    \nName: libtiff\nLicense: Custom (https://github.com/libsdl-org/libtiff)\n\nSilicon Graphics has seen fit to allow us to give this work away. It is free. There is no support or guarantee of any sort as to its operations, correctness, or whatever. If you do anything useful with all or parts of it you need to honor the copyright notices. I would also be interested in knowing about it and, hopefully, be acknowledged.\n\nThe legal way of saying that is:\n\nCopyright (c) 1988-1997 Sam Leffler Copyright (c) 1991-1997 Silicon Graphics, Inc.\n\nPermission to use, copy, modify, distribute, and sell this software and its documentation for any purpose is hereby granted without fee, provided that (i) the above copyright notices and this permission notice appear in all copies of the software and related documentation, and (ii) the names of Sam Leffler and Silicon Graphics may not be used in any advertising or publicity relating to the software without the specific, prior written permission of Sam Leffler and Silicon Graphics.\n\nTHE SOFTWARE IS PROVIDED \"AS-IS\" AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR OTHERWISE, INCLUDING WITHOUT LIMITATION, ANY WARRANTY OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.\n\nIN NO EVENT SHALL SAM LEFFLER OR SILICON GRAPHICS BE LIABLE FOR ANY SPECIAL, INCIDENTAL, INDIRECT OR CONSEQUENTIAL DAMAGES OF ANY KIND, OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER OR NOT ADVISED OF THE POSSIBILITY OF DAMAGE, AND ON ANY THEORY OF LIABILITY, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\nName: zlib\nLicense: Custom (https://github.com/madler/zlib?tab=License-1-ov-file)\n\nCopyright notice:\n\n (C) 1995-2024 Jean-loup Gailly and Mark Adler\n\n  This software is provided 'as-is', without any express or implied\n  warranty.  In no event will the authors be held liable for any damages\n  arising from the use of this software.\n\n  Permission is granted to anyone to use this software for any purpose,\n  including commercial applications, and to alter it and redistribute it\n  freely, subject to the following restrictions:\n\n  1. The origin of this software must not be misrepresented; you must not\n     claim that you wrote the original software. If you use this software\n     in a product, an acknowledgment in the product documentation would be\n     appreciated but is not required.\n  2. Altered source versions must be plainly marked as such, and must not be\n     misrepresented as being the original software.\n  3. This notice may not be removed or altered from any source distribution.\n\n  Jean-loup Gailly        Mark Adler\n  jloup@gzip.org          madler@alumni.caltech.edu\n  \n  \n=================================================================================================\n\nCitations for Open Source Software, Models and Research used in the development of llmware:\n\nTransformers - https://github.com/huggingface/transformers\n\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n\nSentence Transformers - https://github.com/UKPLab/sentence-transformers\n\n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n\nPytorch - https://github.com/pytorch/blob/main/CITATION.cff  (with full and updated contributor list)\n\n    title: PyTorch\n    authors:  PyTorch Team\n    url: https://pytorch.org\n    type: conference-paper\n    title: \"PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation\"\n    collection-title: \"29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)\"\n    collection-type: proceedings\n    month: 4\n    year: 2024\n  publisher:\n    name: ACM\n  url: \"https://pytorch.org/assets/pytorch2-2.pdf\"\n  \n  Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library [Conference paper]. Advances in Neural Information Processing Systems 32, 8024–8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\n\n\nWhisper - OpenAI - https://github.com/openai/whisper & www.huggingface.co/openai/whisper\n\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n\nBERT\nTurc, Iulia; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models, arXiv preprint arXiv:1908.08962v2 (2019).\n\nDevlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina.  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, arXiv preprint arXiv:1810.04805 (2018).\n\nGPT2\nRadford, Alec; Wu, Jeff; Child, Rewon; Luan, David; Amodei, Dario; Sutskever, Ilya.  Language Models are Unsupervised Multitask Learners. (2019)\n\nTiny Llama\n@misc{zhang2024tinyllama,\n      title={TinyLlama: An Open-Source Small Language Model},\n      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},\n      year={2024},\n      eprint={2401.02385},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\nStableLM-3b-4e1t Model (StabilityAI)\n@misc{StableLM-3B-4E1T,\n      url={[https://huggingface.co/stabilityai/stablelm-3b-4e1t](https://huggingface.co/stabilityai/stablelm-3b-4e1t)},\n      title={StableLM 3B 4E1T},\n      author={Tow, Jonathan and Bellagente, Marco and Mahan, Dakota and Riquelme, Carlos}\n}\n\nSheared Llama\n@article{xia2023sheared, title={Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning}, author={Xia, Mengzhou and Gao, Tianyu, and Zeng Zhiyuan, and Chen Danqi}, year={2023} }\n\n\nFAISS\n@article{douze2024faiss,\n      title={The Faiss library},\n      author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},\n      year={2024},\n      eprint={2401.08281},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 68.4189453125,
          "content": "# llmware\n![Static Badge](https://img.shields.io/badge/python-3.9_%7C_3.10%7C_3.11%7C_3.12-blue?color=blue)\n![PyPI - Version](https://img.shields.io/pypi/v/llmware?color=blue)\n[![discord](https://img.shields.io/badge/Chat%20on-Discord-blue?logo=discord&logoColor=white)](https://discord.gg/MhZn5Nc39h)   \n[![Documentation](https://github.com/llmware-ai/llmware/actions/workflows/pages.yml/badge.svg)](https://github.com/llmware-ai/llmware/actions/workflows/pages.yml)  \n\n🆕Check out [Model Depot](https://medium.com/@darrenoberst/model-depot-9e6625c5fc55)  \nAre you using a Windows/Linux x86 machine?  \n- Getting started with [OpenVino example](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_openvino_models.py)  \n- Getting started with [ONNX example](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_onnx_models.py)  \n\n## Table of Contents\n\n- [Building Enterprise RAG Pipelines with Small, Specialized Models](%EF%B8%8Fbuilding-enterprise-rag-pipelines-with-small-specialized-models)\n- [Key Features](#--key-features)\n- [What's New](#️-whats-new)\n- [Getting Started](#-getting-started)\n- [Working with the llmware Github repository](#%EF%B8%8F-working-with-the-llmware-github-repository)\n- [Data Store Options](#data-store-options)\n- [Meet our Models](#meet-our-models)\n- [Using LLMs and setting-up API keys & secrets](#using-llms-and-setting-up-api-keys--secrets)\n- [Release notes and Change Log](#--release-notes-and-change-log)\n\n## 🧰🛠️🔩Building Enterprise RAG Pipelines with Small, Specialized Models  \n\n`llmware` provides a unified framework for building LLM-based applications (e.g., RAG, Agents), using small, specialized models that can be deployed privately, integrated with enterprise knowledge sources safely and securely, and cost-effectively tuned and adapted for any business process.  \n\n `llmware` has two main components:  \n \n 1.  **RAG Pipeline** - integrated components for the full lifecycle of connecting knowledge sources to generative AI models; and \n\n 2.  **50+ small, specialized models** fine-tuned for key tasks in enterprise process automation, including fact-based question-answering, classification, summarization, and extraction.  \n\nBy bringing together both of these components, along with integrating leading open source models and underlying technologies, `llmware` offers a comprehensive set of tools to rapidly build knowledge-based enterprise LLM applications.  \n\nMost of our examples can be run without a GPU server - get started right away on your laptop.   \n\n[Join us on Discord](https://discord.gg/MhZn5Nc39h)   |  [Watch Youtube Tutorials](https://www.youtube.com/@llmware)  | [Explore our Model Families on Huggingface](https://www.huggingface.co/llmware)   \n\nNew to Agents?  [Check out the Agent Fast Start series](https://github.com/llmware-ai/llmware/tree/main/fast_start/agents)  \n\nNew to RAG?  [Check out the Fast Start video series](https://www.youtube.com/playlist?list=PL1-dn33KwsmD7SB9iSO6vx4ZLRAWea1DB)  \n\n🔥🔥🔥 [**Multi-Model Agents with SLIM Models**](examples/SLIM-Agents/) - [**Intro-Video**](https://www.youtube.com/watch?v=cQfdaTcmBpY) 🔥🔥🔥   \n\n[Intro to SLIM Function Call Models](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_function_calls.py)  \nCan't wait?  Get SLIMs right away:  \n\n```python \nfrom llmware.models import ModelCatalog\n\nModelCatalog().get_llm_toolkit()  # get all SLIM models, delivered as small, fast quantized tools \nModelCatalog().tool_test_run(\"slim-sentiment-tool\") # see the model in action with test script included  \n```\n\n## 🎯  Key features \nWriting code with`llmware` is based on a few main concepts:\n\n<details>\n<summary><b>Model Catalog</b>: Access all models the same way with easy lookup, regardless of underlying implementation. \n</summary>  \n\n\n```python\n#   150+ Models in Catalog with 50+ RAG-optimized BLING, DRAGON and Industry BERT models\n#   Full support for GGUF, HuggingFace, Sentence Transformers and major API-based models\n#   Easy to extend to add custom models - see examples\n\nfrom llmware.models import ModelCatalog\nfrom llmware.prompts import Prompt\n\n#   all models accessed through the ModelCatalog\nmodels = ModelCatalog().list_all_models()\n\n#   to use any model in the ModelCatalog - \"load_model\" method and pass the model_name parameter\nmy_model = ModelCatalog().load_model(\"llmware/bling-phi-3-gguf\")\noutput = my_model.inference(\"what is the future of AI?\", add_context=\"Here is the article to read\")\n\n#   to integrate model into a Prompt\nprompter = Prompt().load_model(\"llmware/bling-tiny-llama-v0\")\nresponse = prompter.prompt_main(\"what is the future of AI?\", context=\"Insert Sources of information\")\n```\n\n</details>  \n\n<details>  \n<summary><b>Library</b>:  ingest, organize and index a collection of knowledge at scale - Parse, Text Chunk and Embed. </summary>  \n\n```python\n\nfrom llmware.library import Library\n\n#   to parse and text chunk a set of documents (pdf, pptx, docx, xlsx, txt, csv, md, json/jsonl, wav, png, jpg, html)  \n\n#   step 1 - create a library, which is the 'knowledge-base container' construct\n#          - libraries have both text collection (DB) resources, and file resources (e.g., llmware_data/accounts/{library_name})\n#          - embeddings and queries are run against a library\n\nlib = Library().create_new_library(\"my_library\")\n\n#    step 2 - add_files is the universal ingestion function - point it at a local file folder with mixed file types\n#           - files will be routed by file extension to the correct parser, parsed, text chunked and indexed in text collection DB\n\nlib.add_files(\"/folder/path/to/my/files\")\n\n#   to install an embedding on a library - pick an embedding model and vector_db\nlib.install_new_embedding(embedding_model_name=\"mini-lm-sbert\", vector_db=\"milvus\", batch_size=500)\n\n#   to add a second embedding to the same library (mix-and-match models + vector db)  \nlib.install_new_embedding(embedding_model_name=\"industry-bert-sec\", vector_db=\"chromadb\", batch_size=100)\n\n#   easy to create multiple libraries for different projects and groups\n\nfinance_lib = Library().create_new_library(\"finance_q4_2023\")\nfinance_lib.add_files(\"/finance_folder/\")\n\nhr_lib = Library().create_new_library(\"hr_policies\")\nhr_lib.add_files(\"/hr_folder/\")\n\n#    pull library card with key metadata - documents, text chunks, images, tables, embedding record\nlib_card = Library().get_library_card(\"my_library\")\n\n#   see all libraries\nall_my_libs = Library().get_all_library_cards()\n\n```\n</details>  \n\n<details> \n<summary><b>Query</b>: query libraries with mix of text, semantic, hybrid, metadata, and custom filters. </summary>\n\n```python\n\nfrom llmware.retrieval import Query\nfrom llmware.library import Library\n\n#   step 1 - load the previously created library \nlib = Library().load_library(\"my_library\")\n\n#   step 2 - create a query object and pass the library\nq = Query(lib)\n\n#    step 3 - run lots of different queries  (many other options in the examples)\n\n#    basic text query\nresults1 = q.text_query(\"text query\", result_count=20, exact_mode=False)\n\n#    semantic query\nresults2 = q.semantic_query(\"semantic query\", result_count=10)\n\n#    combining a text query restricted to only certain documents in the library and \"exact\" match to the query\nresults3 = q.text_query_with_document_filter(\"new query\", {\"file_name\": \"selected file name\"}, exact_mode=True)\n\n#   to apply a specific embedding (if multiple on library), pass the names when creating the query object\nq2 = Query(lib, embedding_model_name=\"mini_lm_sbert\", vector_db=\"milvus\")\nresults4 = q2.semantic_query(\"new semantic query\")\n```\n\n</details>  \n\n<details>\n<summary><b>Prompt with Sources</b>: the easiest way to combine knowledge retrieval with a LLM inference. </summary>\n\n```python\n\nfrom llmware.prompts import Prompt\nfrom llmware.retrieval import Query\nfrom llmware.library import Library\n\n#   build a prompt\nprompter = Prompt().load_model(\"llmware/bling-tiny-llama-v0\")\n\n#   add a file -> file is parsed, text chunked, filtered by query, and then packaged as model-ready context,\n#   including in batches, if needed, to fit the model context window\n\nsource = prompter.add_source_document(\"/folder/to/one/doc/\", \"filename\", query=\"fast query\")\n\n#   attach query results (from a Query) into a Prompt\nmy_lib = Library().load_library(\"my_library\")\nresults = Query(my_lib).query(\"my query\")\nsource2 = prompter.add_source_query_results(results)\n\n#   run a new query against a library and load directly into a prompt\nsource3 = prompter.add_source_new_query(my_lib, query=\"my new query\", query_type=\"semantic\", result_count=15)\n\n#   to run inference with 'prompt with sources'\nresponses = prompter.prompt_with_source(\"my query\")\n\n#   to run fact-checks - post inference\nfact_check = prompter.evidence_check_sources(responses)\n\n#   to view source materials (batched 'model-ready' and attached to prompt)\nsource_materials = prompter.review_sources_summary()\n\n#   to see the full prompt history\nprompt_history = prompter.get_current_history()\n```\n\n</details>  \n\n<details> \n<summary><b>RAG-Optimized Models</b> -  1-7B parameter models designed for RAG workflow integration and running locally. </summary>  \n\n```\n\"\"\" This 'Hello World' example demonstrates how to get started using local BLING models with provided context, using both\nPytorch and GGUF versions. \"\"\"\n\nimport time\nfrom llmware.prompts import Prompt\n\n\ndef hello_world_questions():\n\n    test_list = [\n\n    {\"query\": \"What is the total amount of the invoice?\",\n     \"answer\": \"$22,500.00\",\n     \"context\": \"Services Vendor Inc. \\n100 Elm Street Pleasantville, NY \\nTO Alpha Inc. 5900 1st Street \"\n                \"Los Angeles, CA \\nDescription Front End Engineering Service $5000.00 \\n Back End Engineering\"\n                \" Service $7500.00 \\n Quality Assurance Manager $10,000.00 \\n Total Amount $22,500.00 \\n\"\n                \"Make all checks payable to Services Vendor Inc. Payment is due within 30 days.\"\n                \"If you have any questions concerning this invoice, contact Bia Hermes. \"\n                \"THANK YOU FOR YOUR BUSINESS!  INVOICE INVOICE # 0001 DATE 01/01/2022 FOR Alpha Project P.O. # 1000\"},\n\n    {\"query\": \"What was the amount of the trade surplus?\",\n     \"answer\": \"62.4 billion yen ($416.6 million)\",\n     \"context\": \"Japan’s September trade balance swings into surplus, surprising expectations\"\n                \"Japan recorded a trade surplus of 62.4 billion yen ($416.6 million) for September, \"\n                \"beating expectations from economists polled by Reuters for a trade deficit of 42.5 \"\n                \"billion yen. Data from Japan’s customs agency revealed that exports in September \"\n                \"increased 4.3% year on year, while imports slid 16.3% compared to the same period \"\n                \"last year. According to FactSet, exports to Asia fell for the ninth straight month, \"\n                \"which reflected ongoing China weakness. Exports were supported by shipments to \"\n                \"Western markets, FactSet added. — Lim Hui Jie\"},\n\n    {\"query\": \"When did the LISP machine market collapse?\",\n     \"answer\": \"1987.\",\n     \"context\": \"The attendees became the leaders of AI research in the 1960s.\"\n                \"  They and their students produced programs that the press described as 'astonishing': \"\n                \"computers were learning checkers strategies, solving word problems in algebra, \"\n                \"proving logical theorems and speaking English.  By the middle of the 1960s, research in \"\n                \"the U.S. was heavily funded by the Department of Defense and laboratories had been \"\n                \"established around the world. Herbert Simon predicted, 'machines will be capable, \"\n                \"within twenty years, of doing any work a man can do'.  Marvin Minsky agreed, writing, \"\n                \"'within a generation ... the problem of creating 'artificial intelligence' will \"\n                \"substantially be solved'. They had, however, underestimated the difficulty of the problem.  \"\n                \"Both the U.S. and British governments cut off exploratory research in response \"\n                \"to the criticism of Sir James Lighthill and ongoing pressure from the US Congress \"\n                \"to fund more productive projects. Minsky's and Papert's book Perceptrons was understood \"\n                \"as proving that artificial neural networks approach would never be useful for solving \"\n                \"real-world tasks, thus discrediting the approach altogether.  The 'AI winter', a period \"\n                \"when obtaining funding for AI projects was difficult, followed.  In the early 1980s, \"\n                \"AI research was revived by the commercial success of expert systems, a form of AI \"\n                \"program that simulated the knowledge and analytical skills of human experts. By 1985, \"\n                \"the market for AI had reached over a billion dollars. At the same time, Japan's fifth \"\n                \"generation computer project inspired the U.S. and British governments to restore funding \"\n                \"for academic research. However, beginning with the collapse of the Lisp Machine market \"\n                \"in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\"},\n\n    {\"query\": \"What is the current rate on 10-year treasuries?\",\n     \"answer\": \"4.58%\",\n     \"context\": \"Stocks rallied Friday even after the release of stronger-than-expected U.S. jobs data \"\n                \"and a major increase in Treasury yields.  The Dow Jones Industrial Average gained 195.12 points, \"\n                \"or 0.76%, to close at 31,419.58. The S&P 500 added 1.59% at 4,008.50. The tech-heavy \"\n                \"Nasdaq Composite rose 1.35%, closing at 12,299.68. The U.S. economy added 438,000 jobs in \"\n                \"August, the Labor Department said. Economists polled by Dow Jones expected 273,000 \"\n                \"jobs. However, wages rose less than expected last month.  Stocks posted a stunning \"\n                \"turnaround on Friday, after initially falling on the stronger-than-expected jobs report. \"\n                \"At its session low, the Dow had fallen as much as 198 points; it surged by more than \"\n                \"500 points at the height of the rally. The Nasdaq and the S&P 500 slid by 0.8% during \"\n                \"their lowest points in the day.  Traders were unclear of the reason for the intraday \"\n                \"reversal. Some noted it could be the softer wage number in the jobs report that made \"\n                \"investors rethink their earlier bearish stance. Others noted the pullback in yields from \"\n                \"the day’s highs. Part of the rally may just be to do a market that had gotten extremely \"\n                \"oversold with the S&P 500 at one point this week down more than 9% from its high earlier \"\n                \"this year.  Yields initially surged after the report, with the 10-year Treasury rate trading \"\n                \"near its highest level in 14 years. The benchmark rate later eased from those levels, but \"\n                \"was still up around 6 basis points at 4.58%.  'We’re seeing a little bit of a give back \"\n                \"in yields from where we were around 4.8%. [With] them pulling back a bit, I think that’s \"\n                \"helping the stock market,' said Margaret Jones, chief investment officer at Vibrant Industries \"\n                \"Capital Advisors. 'We’ve had a lot of weakness in the market in recent weeks, and potentially \"\n                \"some oversold conditions.'\"},\n\n    {\"query\": \"Is the expected gross margin greater than 70%?\",\n     \"answer\": \"Yes, between 71.5% and 72.%\",\n     \"context\": \"Outlook NVIDIA’s outlook for the third quarter of fiscal 2024 is as follows:\"\n                \"Revenue is expected to be $16.00 billion, plus or minus 2%. GAAP and non-GAAP \"\n                \"gross margins are expected to be 71.5% and 72.5%, respectively, plus or minus \"\n                \"50 basis points.  GAAP and non-GAAP operating expenses are expected to be \"\n                \"approximately $2.95 billion and $2.00 billion, respectively.  GAAP and non-GAAP \"\n                \"other income and expense are expected to be an income of approximately $100 \"\n                \"million, excluding gains and losses from non-affiliated investments. GAAP and \"\n                \"non-GAAP tax rates are expected to be 14.5%, plus or minus 1%, excluding any discrete items.\"\n                \"Highlights NVIDIA achieved progress since its previous earnings announcement \"\n                \"in these areas:  Data Center Second-quarter revenue was a record $10.32 billion, \"\n                \"up 141% from the previous quarter and up 171% from a year ago. Announced that the \"\n                \"NVIDIA® GH200 Grace™ Hopper™ Superchip for complex AI and HPC workloads is shipping \"\n                \"this quarter, with a second-generation version with HBM3e memory expected to ship \"\n                \"in Q2 of calendar 2024. \"},\n\n    {\"query\": \"What is Bank of America's rating on Target?\",\n     \"answer\": \"Buy\",\n     \"context\": \"Here are some of the tickers on my radar for Thursday, Oct. 12, taken directly from \"\n                \"my reporter’s notebook: It’s the one-year anniversary of the S&P 500′s bear market bottom \"\n                \"of 3,577. Since then, as of Wednesday’s close of 4,376, the broad market index \"\n                \"soared more than 22%.  Hotter than expected September consumer price index, consumer \"\n                \"inflation. The Social Security Administration issues announced a 3.2% cost-of-living \"\n                \"adjustment for 2024.  Chipotle Mexican Grill (CMG) plans price increases. Pricing power. \"\n                \"Cites consumer price index showing sticky retail inflation for the fourth time \"\n                \"in two years. Bank of America upgrades Target (TGT) to buy from neutral. Cites \"\n                \"risk/reward from depressed levels. Traffic could improve. Gross margin upside. \"\n                \"Merchandising better. Freight and transportation better. Target to report quarter \"\n                \"next month. In retail, the CNBC Investing Club portfolio owns TJX Companies (TJX), \"\n                \"the off-price juggernaut behind T.J. Maxx, Marshalls and HomeGoods. Goldman Sachs \"\n                \"tactical buy trades on Club names Wells Fargo (WFC), which reports quarter Friday, \"\n                \"Humana (HUM) and Nvidia (NVDA). BofA initiates Snowflake (SNOW) with a buy rating.\"\n                \"If you like this story, sign up for Jim Cramer’s Top 10 Morning Thoughts on the \"\n                \"Market email newsletter for free. Barclays cuts price targets on consumer products: \"\n                \"UTZ Brands (UTZ) to $16 per share from $17. Kraft Heinz (KHC) to $36 per share from \"\n                \"$38. Cyclical drag. J.M. Smucker (SJM) to $129 from $160. Secular headwinds. \"\n                \"Coca-Cola (KO) to $59 from $70. Barclays cut PTs on housing-related stocks: Toll Brothers\"\n                \"(TOL) to $74 per share from $82. Keeps underweight. Lowers Trex (TREX) and Azek\"\n                \"(AZEK), too. Goldman Sachs (GS) announces sale of fintech platform and warns on \"\n                \"third quarter of 19-cent per share drag on earnings. The buyer: investors led by \"\n                \"private equity firm Sixth Street. Exiting a mistake. Rise in consumer engagement for \"\n                \"Spotify (SPOT), says Morgan Stanley. The analysts hike price target to $190 per share \"\n                \"from $185. Keeps overweight (buy) rating. JPMorgan loves elf Beauty (ELF). Keeps \"\n                \"overweight (buy) rating but lowers price target to $139 per share from $150. \"\n                \"Sees “still challenging” environment into third-quarter print. The Club owns shares \"\n                \"in high-end beauty company Estee Lauder (EL). Barclays upgrades First Solar (FSLR) \"\n                \"to overweight from equal weight (buy from hold) but lowers price target to $224 per \"\n                \"share from $230. Risk reward upgrade. Best visibility of utility scale names.\"},\n\n    {\"query\": \"What was the rate of decline in 3rd quarter sales?\",\n     \"answer\": \"20% year-on-year.\",\n     \"context\": \"Nokia said it would cut up to 14,000 jobs as part of a cost cutting plan following \"\n                \"third quarter earnings that plunged. The Finnish telecommunications giant said that \"\n                \"it will reduce its cost base and increase operation efficiency to “address the \"\n                \"challenging market environment. The substantial layoffs come after Nokia reported \"\n                \"third-quarter net sales declined 20% year-on-year to 4.98 billion euros. Profit over \"\n                \"the period plunged by 69% year-on-year to 133 million euros.\"},\n\n    {\"query\": \"What is a list of the key points?\",\n     \"answer\": \"•Stocks rallied on Friday with stronger-than-expected U.S jobs data and increase in \"\n               \"Treasury yields;\\n•Dow Jones gained 195.12 points;\\n•S&P 500 added 1.59%;\\n•Nasdaq Composite rose \"\n               \"1.35%;\\n•U.S. economy added 438,000 jobs in August, better than the 273,000 expected;\\n\"\n               \"•10-year Treasury rate trading near the highest level in 14 years at 4.58%.\",\n     \"context\": \"Stocks rallied Friday even after the release of stronger-than-expected U.S. jobs data \"\n               \"and a major increase in Treasury yields.  The Dow Jones Industrial Average gained 195.12 points, \"\n               \"or 0.76%, to close at 31,419.58. The S&P 500 added 1.59% at 4,008.50. The tech-heavy \"\n               \"Nasdaq Composite rose 1.35%, closing at 12,299.68. The U.S. economy added 438,000 jobs in \"\n               \"August, the Labor Department said. Economists polled by Dow Jones expected 273,000 \"\n               \"jobs. However, wages rose less than expected last month.  Stocks posted a stunning \"\n               \"turnaround on Friday, after initially falling on the stronger-than-expected jobs report. \"\n               \"At its session low, the Dow had fallen as much as 198 points; it surged by more than \"\n               \"500 points at the height of the rally. The Nasdaq and the S&P 500 slid by 0.8% during \"\n               \"their lowest points in the day.  Traders were unclear of the reason for the intraday \"\n               \"reversal. Some noted it could be the softer wage number in the jobs report that made \"\n               \"investors rethink their earlier bearish stance. Others noted the pullback in yields from \"\n               \"the day’s highs. Part of the rally may just be to do a market that had gotten extremely \"\n               \"oversold with the S&P 500 at one point this week down more than 9% from its high earlier \"\n               \"this year.  Yields initially surged after the report, with the 10-year Treasury rate trading \"\n               \"near its highest level in 14 years. The benchmark rate later eased from those levels, but \"\n               \"was still up around 6 basis points at 4.58%.  'We’re seeing a little bit of a give back \"\n               \"in yields from where we were around 4.8%. [With] them pulling back a bit, I think that’s \"\n               \"helping the stock market,' said Margaret Jones, chief investment officer at Vibrant Industries \"\n               \"Capital Advisors. 'We’ve had a lot of weakness in the market in recent weeks, and potentially \"\n               \"some oversold conditions.'\"}\n\n    ]\n\n    return test_list\n\n\n# this is the main script to be run\n\ndef bling_meets_llmware_hello_world (model_name):\n\n    t0 = time.time()\n\n    # load the questions\n    test_list = hello_world_questions()\n\n    print(f\"\\n > Loading Model: {model_name}...\")\n\n    # load the model \n    prompter = Prompt().load_model(model_name)\n\n    t1 = time.time()\n    print(f\"\\n > Model {model_name} load time: {t1-t0} seconds\")\n \n    for i, entries in enumerate(test_list):\n\n        print(f\"\\n{i+1}. Query: {entries['query']}\")\n     \n        # run the prompt\n        output = prompter.prompt_main(entries[\"query\"],context=entries[\"context\"]\n                                      , prompt_name=\"default_with_context\",temperature=0.30)\n\n        # print out the results\n        llm_response = output[\"llm_response\"].strip(\"\\n\")\n        print(f\"LLM Response: {llm_response}\")\n        print(f\"Gold Answer: {entries['answer']}\")\n        print(f\"LLM Usage: {output['usage']}\")\n\n    t2 = time.time()\n\n    print(f\"\\nTotal processing time: {t2-t1} seconds\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n\n    # list of 'rag-instruct' laptop-ready small bling models on HuggingFace\n\n    pytorch_models = [\"llmware/bling-1b-0.1\",                    #  most popular\n                      \"llmware/bling-tiny-llama-v0\",             #  fastest \n                      \"llmware/bling-1.4b-0.1\",\n                      \"llmware/bling-falcon-1b-0.1\",\n                      \"llmware/bling-cerebras-1.3b-0.1\",\n                      \"llmware/bling-sheared-llama-1.3b-0.1\",    \n                      \"llmware/bling-sheared-llama-2.7b-0.1\",\n                      \"llmware/bling-red-pajamas-3b-0.1\",\n                      \"llmware/bling-stable-lm-3b-4e1t-v0\",\n                      \"llmware/bling-phi-3\"                      # most accurate (and newest)  \n                      ]\n\n    #  Quantized GGUF versions generally load faster and run nicely on a laptop with at least 16 GB of RAM\n    gguf_models = [\"bling-phi-3-gguf\", \"bling-stablelm-3b-tool\", \"dragon-llama-answer-tool\", \"dragon-yi-answer-tool\", \"dragon-mistral-answer-tool\"]\n\n    #   try model from either pytorch or gguf model list\n    #   the newest (and most accurate) is 'bling-phi-3-gguf'  \n\n    bling_meets_llmware_hello_world(gguf_models[0]  \n\n    #   check out the model card on Huggingface for RAG benchmark test performance results and other useful information\n```\n\n</details>\n\n<details>\n<summary><b>Simple-to-Scale Database Options </b> - integrated data stores from laptop to parallelized cluster. </summary>\n\n```python\n\nfrom llmware.configs import LLMWareConfig\n\n#   to set the collection database - mongo, sqlite, postgres  \nLLMWareConfig().set_active_db(\"mongo\")  \n\n#   to set the vector database (or declare when installing)  \n#   --options: milvus, pg_vector (postgres), redis, qdrant, faiss, pinecone, mongo atlas  \nLLMWareConfig().set_vector_db(\"milvus\")  \n\n#   for fast start - no installations required  \nLLMWareConfig().set_active_db(\"sqlite\")  \nLLMWareConfig().set_vector_db(\"chromadb\")   # try also faiss and lancedb  \n\n#   for single postgres deployment  \nLLMWareConfig().set_active_db(\"postgres\")  \nLLMWareConfig().set_vector_db(\"postgres\")  \n\n#   to install mongo, milvus, postgres - see the docker-compose scripts as well as examples\n\n```\n\n</details>\n\n<details>\n\n<summary> 🔥 <b> Agents with Function Calls and SLIM Models </b> 🔥 </summary>  \n\n```python\n\nfrom llmware.agents import LLMfx\n\ntext = (\"Tesla stock fell 8% in premarket trading after reporting fourth-quarter revenue and profit that \"\n        \"missed analysts’ estimates. The electric vehicle company also warned that vehicle volume growth in \"\n        \"2024 'may be notably lower' than last year’s growth rate. Automotive revenue, meanwhile, increased \"\n        \"just 1% from a year earlier, partly because the EVs were selling for less than they had in the past. \"\n        \"Tesla implemented steep price cuts in the second half of the year around the world. In a Wednesday \"\n        \"presentation, the company warned investors that it’s 'currently between two major growth waves.'\")\n\n#   create an agent using LLMfx class\nagent = LLMfx()\n\n#   load text to process\nagent.load_work(text)\n\n#   load 'models' as 'tools' to be used in analysis process\nagent.load_tool(\"sentiment\")\nagent.load_tool(\"extract\")\nagent.load_tool(\"topics\")\nagent.load_tool(\"boolean\")\n\n#   run function calls using different tools\nagent.sentiment()\nagent.topics()\nagent.extract(params=[\"company\"])\nagent.extract(params=[\"automotive revenue growth\"])\nagent.xsum()\nagent.boolean(params=[\"is 2024 growth expected to be strong? (explain)\"])\n\n#   at end of processing, show the report that was automatically aggregated by key\nreport = agent.show_report()\n\n#   displays a summary of the activity in the process\nactivity_summary = agent.activity_summary()\n\n#   list of the responses gathered\nfor i, entries in enumerate(agent.response_list):\n    print(\"update: response analysis: \", i, entries)\n\noutput = {\"report\": report, \"activity_summary\": activity_summary, \"journal\": agent.journal}  \n\n```\n\n</details>\n<details>\n\n<summary> 🚀 <b>Start coding - Quick Start for RAG </b> 🚀 </summary>\n\n```python\n# This example illustrates a simple contract analysis\n# using a RAG-optimized LLM running locally\n\nimport os\nimport re\nfrom llmware.prompts import Prompt, HumanInTheLoop\nfrom llmware.setup import Setup\nfrom llmware.configs import LLMWareConfig\n\ndef contract_analysis_on_laptop (model_name):\n\n    #  In this scenario, we will:\n    #  -- download a set of sample contract files\n    #  -- create a Prompt and load a BLING LLM model\n    #  -- parse each contract, extract the relevant passages, and pass questions to a local LLM\n\n    #  Main loop - Iterate thru each contract:\n    #\n    #      1.  parse the document in memory (convert from PDF file into text chunks with metadata)\n    #      2.  filter the parsed text chunks with a \"topic\" (e.g., \"governing law\") to extract relevant passages\n    #      3.  package and assemble the text chunks into a model-ready context\n    #      4.  ask three key questions for each contract to the LLM\n    #      5.  print to the screen\n    #      6.  save the results in both json and csv for furthe processing and review.\n\n    #  Load the llmware sample files\n\n    print (f\"\\n > Loading the llmware sample files...\")\n\n    sample_files_path = Setup().load_sample_files()\n    contracts_path = os.path.join(sample_files_path,\"Agreements\")\n \n    #  Query list - these are the 3 main topics and questions that we would like the LLM to analyze for each contract\n\n    query_list = {\"executive employment agreement\": \"What are the name of the two parties?\",\n                  \"base salary\": \"What is the executive's base salary?\",\n                  \"vacation\": \"How many vacation days will the executive receive?\"}\n\n    #  Load the selected model by name that was passed into the function\n\n    print (f\"\\n > Loading model {model_name}...\")\n\n    prompter = Prompt().load_model(model_name, temperature=0.0, sample=False)\n\n    #  Main loop\n\n    for i, contract in enumerate(os.listdir(contracts_path)):\n\n        #   excluding Mac file artifact (annoying, but fact of life in demos)\n        if contract != \".DS_Store\":\n\n            print(\"\\nAnalyzing contract: \", str(i+1), contract)\n\n            print(\"LLM Responses:\")\n\n            for key, value in query_list.items():\n\n                # step 1 + 2 + 3 above - contract is parsed, text-chunked, filtered by topic key,\n                # ... and then packaged into the prompt\n\n                source = prompter.add_source_document(contracts_path, contract, query=key)\n\n                # step 4 above - calling the LLM with 'source' information already packaged into the prompt\n\n                responses = prompter.prompt_with_source(value, prompt_name=\"default_with_context\")  \n\n                # step 5 above - print out to screen\n\n                for r, response in enumerate(responses):\n                    print(key, \":\", re.sub(\"[\\n]\",\" \", response[\"llm_response\"]).strip())\n\n                # We're done with this contract, clear the source from the prompt\n                prompter.clear_source_materials()\n\n    # step 6 above - saving the analysis to jsonl and csv\n\n    # Save jsonl report to jsonl to /prompt_history folder\n    print(\"\\nPrompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(),prompter.prompt_id))\n    prompter.save_state()\n\n    # Save csv report that includes the model, response, prompt, and evidence for human-in-the-loop review\n    csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n    print(\"csv output saved at:  \", csv_output)\n\n\nif __name__ == \"__main__\":\n\n    # use local cpu model - try the newest - RAG finetune of Phi-3 quantized and packaged in GGUF  \n    model = \"bling-phi-3-gguf\"\n\n    contract_analysis_on_laptop(model)\n\n```\n</details>\n\n## 🔥 Latest Enhancements and Features 🔥  \n\n### Model Capabilities & Benchmarks\n\n- **Benchmarking Small Model Capabilities**  \n  Explore the latest benchmark results for small language models focusing on accuracy and enterprise use cases.  \n  - [Read benchmark results](https://medium.com/@darrenoberst/best-small-language-models-for-accuracy-and-enterprise-use-cases-benchmark-results-cf71964759c8)\n  - [Example code for model ranking](fast_start/agents/agents-15-get_model_benchmarks.py)\n\n### New Models and Functionality\n\n- **Qwen2 Models for RAG, Function Calling, and Chat**  \n  Start using Qwen2 models quickly with resources for Retrieval-Augmented Generation (RAG), function calling, and chat functionalities.\n  - [Quickstart example](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-qwen2-models.py)\n\n- **Phi-3 Function Calling Models**  \n  Get started in minutes with Phi-3 models designed for function calling.\n  - [Quickstart example](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-phi-3-function-calls.py)\n\n### New Use Cases & Applications\n\n- **BizBot: RAG + SQL Local Chatbot**  \n  Implement a local chatbot for business intelligence using RAG and SQL.\n  - [Code example](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/biz_bot.py) | [Demo video](https://youtu.be/4nBYDEjxxTE?si=o6PDPbu0PVcT-tYd)\n\n- **Lecture Tool**  \n  Enables Q&A on voice recordings for education and lecture analysis.\n  - [Lecture tool code](https://github.com/llmware-ai/llmware/blob/main/examples/Use_Cases/lecture_tool/)\n\n- **Web Services for Financial Research**  \n  An end-to-end example demonstrating web services with agent calls for financial research.\n  - [Demo video](https://youtu.be/l0jzsg1_Ik0?si=hmLhpT1iv_rxpkHo) | [Code example](examples/Use_Cases/web_services_slim_fx.py)\n\n### Audio & Text Processing\n\n- **Voice Transcription with WhisperCPP**  \n  Start transcription projects with WhisperCPP, featuring tools for sample file usage and famous speeches.\n  - [Getting started guide](examples/Models/using-whisper-cpp-getting-started.py) | [Parsing great speeches](examples/Use_Cases/parsing_great_speeches.py) | [Demo video](https://youtu.be/5y0ez5ZBpPE?si=KVxsXXtX5TzvlEws)\n\n- **Natural Language Query to CSV**  \n  Convert natural language queries to CSV with Slim-SQL, supporting custom Postgres tables.\n  - [Demo video](https://youtu.be/z48z5XOXJJg?si=V-CX1w-7KRioI4Bi) | [End-to-end example](examples/SLIM-Agents/text2sql-end-to-end-2.py) | [Custom table usage](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/agent_with_custom_tables.py)\n\n### Multi-Model Agents\n\n- **Multi-Model Agents with SLIM**  \n  Use SLIM models on CPU for multi-step agents in complex workflows.\n  - [Demo video](https://www.youtube.com/watch?v=cQfdaTcmBpY) | [Example directory](examples/SLIM-Agents)\n\n### Document & OCR Processing\n\n- **OCR Embedded Document Images**  \n  Extract text systematically from images embedded in documents for enhanced document processing.\n  - [OCR example](examples/Parsing/ocr_embedded_doc_images.py)\n\n- **Enhanced Document Parsing for PDFs, Word, PowerPoint, and Excel**  \n  Improved text-chunking controls, table extraction, and content parsing.\n  - [Parsing example](examples/Parsing/pdf_parser_new_configs.py)\n\n### Deployment & Optimization\n\n- **Agent Inference Server**  \n  Set up an inference server for multi-model agents to optimize deployments.\n  - [Server setup example](https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/agent_api_endpoint.py)\n\n- **Optimizing Accuracy of RAG Prompts**  \n  Tutorials for tuning RAG prompt settings for increased accuracy.\n  - [Settings example](examples/Models/adjusting_sampling_settings.py) | Videos: [Part I](https://youtu.be/7oMTGhSKuNY?si=14mS2pftk7NoKQbC), [Part II](https://youtu.be/iXp1tj-pPjM?si=T4teUAISnSWgtThu)\n\n\n\n## 🌱 Getting Started\n\n**Step 1 - Install llmware** -  `pip3 install llmware` or `pip3 install 'llmware[full]'`  \n\n- note: starting with v0.3.0, we provide options for a [core install](https://github.com/llmware-ai/llmware/blob/main/llmware/requirements.txt) (minimal set of dependencies) or [full install](https://github.com/llmware-ai/llmware/blob/main/llmware/requirements_extras.txt) (adds to the core with wider set of related python libraries).  \n\n<details>\n<summary><b>Step 2- Go to Examples</b> - Get Started Fast with 100+ 'Cut-and-Paste' Recipes </summary>\n\n## 🔥 Top New Examples 🔥  \n\nEnd-to-End Scenario - [**Function Calls with SLIM Extract and Web Services for Financial Research**](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/web_services_slim_fx.py)  \nAnalyzing Voice Files - [**Great Speeches with LLM Query and Extract**](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/parsing_great_speeches.py)  \nNew to LLMWare - [**Fast Start tutorial series**](https://github.com/llmware-ai/llmware/tree/main/fast_start)  \nGetting Setup - [**Getting Started**](https://github.com/llmware-ai/llmware/tree/main/examples/Getting_Started)  \nSLIM Examples -  [**SLIM Models**](examples/SLIM-Agents/)  \n\n| Example     |  Detail      |\n|-------------|--------------|\n| 1.   BLING models fast start ([code](examples/Models/bling_fast_start.py) / [video](https://www.youtube.com/watch?v=JjgqOZ2v5oU)) | Get started with fast, accurate, CPU-based models - question-answering, key-value extraction, and basic summarization.  |\n| 2.   Parse and Embed 500 PDF Documents ([code](examples/Embedding/docs2vecs_with_milvus-un_resolutions.py))  | End-to-end example for Parsing, Embedding and Querying UN Resolution documents with Milvus  |\n| 3.  Hybrid Retrieval - Semantic + Text ([code](examples/Retrieval/dual_pass_with_custom_filter.py)) | Using 'dual pass' retrieval to combine best of semantic and text search |  \n| 4.   Multiple Embeddings with PG Vector ([code](examples/Embedding/using_multiple_embeddings.py) / [video](https://www.youtube.com/watch?v=Bncvggy6m5Q)) | Comparing Multiple Embedding Models using Postgres / PG Vector |\n| 5.   DRAGON GGUF Models ([code](examples/Models/dragon_gguf_fast_start.py) / [video](https://www.youtube.com/watch?v=BI1RlaIJcsc&t=130s)) | State-of-the-Art 7B RAG GGUF Models.  | \n| 6.   RAG with BLING ([code](examples/Use_Cases/contract_analysis_on_laptop_with_bling_models.py) / [video](https://www.youtube.com/watch?v=8aV5p3tErP0)) | Using contract analysis as an example, experiment with RAG for complex document analysis and text extraction using `llmware`'s BLING ~1B parameter GPT model running on your laptop. |  \n| 7.   Master Service Agreement Analysis with DRAGON ([code](examples/Use_Cases/msa_processing.py) / [video](https://www.youtube.com/watch?v=Cf-07GBZT68&t=2s)) | Analyzing MSAs using DRAGON YI 6B Model.   |                                                                                                                         \n| 8.   Streamlit Example ([code](examples/UI/simple_rag_ui_with_streamlit.py))  | Ask questions to Invoices with UI run inference.  |  \n| 9.   Integrating LM Studio ([code](examples/Models/using-open-chat-models.py) / [video](https://www.youtube.com/watch?v=h2FDjUyvsKE&t=101s)) | Integrating LM Studio Models with LLMWare  |                                                                                                                                       \n| 10.  Prompts With Sources ([code](examples/Prompts/prompt_with_sources.py))  | Attach wide range of knowledge sources directly into Prompts.   |   \n| 11.  Fact Checking ([code](examples/Prompts/fact_checking.py))  | Explore the full set of evidence methods in this example script that analyzes a set of contracts.   |\n| 12.  Using 7B GGUF Chat Models ([code](examples/Models/chat_models_gguf_fast_start.py)) | Using 4 state of the art 7B chat models in minutes running locally |  \n\n\nCheck out:  [llmware examples](https://github.com/llmware-ai/llmware/blob/main/examples/README.md)  \n\n</details>  \n\n<details>\n<summary><b>Step 3 - Tutorial Videos</b> - check out our Youtube channel for high-impact 5-10 minute tutorials on the latest examples.   </summary>\n\n🎬 Check out these videos to get started quickly:  \n- [Document Summarization](https://youtu.be/Ps3W-P9A1m8?si=Rxvst3RJv8ZaOk0L)  \n- [Bling-3-GGUF Local Chatbot](https://youtu.be/gzzEVK8p3VM?si=8cNn_do0oxSzCEnM)  \n- [Agent-based Complex Research Analysis](https://youtu.be/y4WvwHqRR60?si=jX3KCrKcYkM95boe)  \n- [Getting Started with SLIMs (with code)](https://youtu.be/aWZFrTDmMPc?si=lmo98_quo_2Hrq0C)  \n- [Are you prompting wrong for RAG - Stochastic Sampling-Part I](https://youtu.be/7oMTGhSKuNY?si=_KSjuBnqArvWzYbx)  \n- [Are you prompting wrong for RAG - Stochastic Sampling-Part II- Code Experiments](https://youtu.be/iXp1tj-pPjM?si=3ZeMgipY0vJDHIMY)  \n- [SLIM Models Intro](https://www.youtube.com/watch?v=cQfdaTcmBpY)  \n- [Text2SQL Intro](https://youtu.be/BKZ6kO2XxNo?si=tXGt63pvrp_rOlIP)  \n- [RAG with BLING on your laptop](https://www.youtube.com/watch?v=JjgqOZ2v5oU)    \n- [DRAGON-7B-Models](https://www.youtube.com/watch?v=d_u7VaKu6Qk&t=37s)  \n- [Install and Compare Multiple Embeddings with Postgres and PGVector](https://www.youtube.com/watch?v=Bncvggy6m5Q)  \n- [Background on GGUF Quantization & DRAGON Model Example](https://www.youtube.com/watch?v=ZJyQIZNJ45E)  \n- [Using LM Studio Models](https://www.youtube.com/watch?v=h2FDjUyvsKE)  \n- [Using Ollama Models](https://www.youtube.com/watch?v=qITahpVDuV0)  \n- [Use any GGUF Model](https://www.youtube.com/watch?v=9wXJgld7Yow)  \n- [Use small LLMs for RAG for Contract Analysis (feat. LLMWare)](https://www.youtube.com/watch?v=8aV5p3tErP0)\n- [Invoice Processing with LLMware](https://www.youtube.com/watch?v=VHZSaBBG-Bo&t=10s)\n- [Ingest PDFs at Scale](https://www.youtube.com/watch?v=O0adUfrrxi8&t=10s)\n- [Evaluate LLMs for RAG with LLMWare](https://www.youtube.com/watch?v=s0KWqYg5Buk&t=105s)\n- [Fast Start to RAG with LLMWare Open Source Library](https://www.youtube.com/watch?v=0naqpH93eEU)\n- [Use Retrieval Augmented Generation (RAG) without a Database](https://www.youtube.com/watch?v=tAGz6yR14lw)\n- [Pop up LLMWare Inference Server](https://www.youtube.com/watch?v=qiEmLnSRDUA&t=20s)\n\n\n</details>  \n\n## ✍️ Working with the llmware Github repository  \n\nThe llmware repo can be pulled locally to get access to all the examples, or to work directly with the latest version of the llmware code.  \n\n```bash\ngit clone git@github.com:llmware-ai/llmware.git\n```  \n\nWe have provided a **welcome_to_llmware** automation script in the root of the repository folder.  After cloning:  \n- On Windows command line:  `.\\welcome_to_llmware_windows.sh`  \n- On Mac / Linux command line:  `sh ./welcome_to_llmware.sh`  \n\nAlternatively, if you prefer to complete setup without the welcome automation script, then the next steps include:  \n\n1.  **install requirements.txt** - inside the /llmware path - e.g., ```pip3 install -r llmware/requirements.txt```  \n\n2.  **install requirements_extras.txt** - inside the /llmware path - e.g., ```pip3 install -r llmware/requirements_extras.txt```(Depending upon your use case, you may not need all or any of these installs, but some of these will be used in the examples.)\n\n3.  **run examples** - copy one or more of the example .py files into the root project path.   (We have seen several IDEs that will attempt to run interactively from the nested /example path, and then not have access to the /llmware module - the easy fix is to just copy the example you want to run into the root path).  \n\n4.  **install vector db** - no-install vector db options include milvus lite, chromadb, faiss and lancedb - which do not require a server install, but do require that you install the python sdk library for that vector db, e.g., `pip3 install pymilvus`, or `pip3 install chromadb`.  If you look in [examples/Embedding](https://github.com/llmware-ai/llmware/tree/main/examples/Embedding), you will see examples for getting started with various vector DB, and in the root of the repo, you will see easy-to-get-started docker compose scripts for installing milvus, postgres/pgvector, mongo, qdrant, neo4j, and redis.  \n\n5.  Pytorch 2.3 note:  We have recently seen issues with Pytorch==2.3 on some platforms - if you run into any issues, we have seen that uninstalling Pytorch and downleveling to Pytorch==2.1 usually solves the problem.  \n\n6.  Numpy 2.0 note: we have seen issues with numpy 2.0 with many libraries not yet supporting.  Our pip install setup will accept numpy 2.0 (to avoid pip conflicts), but if you pull from repo, we restrict numpy to versions <2.   If you run into issues with numpy, we have found that they can be fixed by downgrading numpy to <2, e.g., 1.26.4.  To use WhisperCPP, you should downlevel to numpy <2.  \n\n\n## Data Store Options\n\n<details>\n<summary><b>Fast Start</b>:  use SQLite3 and ChromaDB (File-based) out-of-the-box - no install required </summary>  \n\n```python\nfrom llmware.configs import LLMWareConfig \nLLMWareConfig().set_active_db(\"sqlite\")   \nLLMWareConfig().set_vector_db(\"chromadb\")  \n```\n</details>  \n\n<details>\n<summary><b>Speed + Scale</b>:  use MongoDB (text collection) and Milvus (vector db) - install with Docker Compose </summary> \n\n```bash\ncurl -o docker-compose.yaml https://raw.githubusercontent.com/llmware-ai/llmware/main/docker-compose.yaml\ndocker compose up -d\n```\n\n```python\nfrom llmware.configs import LLMWareConfig\nLLMWareConfig().set_active_db(\"mongo\")\nLLMWareConfig().set_vector_db(\"milvus\")\n```\n\n</details>  \n\n<details>\n<summary><b>Postgres</b>:  use Postgres for both text collection and vector DB - install with Docker Compose </summary> \n\n```bash\ncurl -o docker-compose.yaml https://raw.githubusercontent.com/llmware-ai/llmware/main/docker-compose-pgvector.yaml\ndocker compose up -d\n```\n\n```python\nfrom llmware.configs import LLMWareConfig\nLLMWareConfig().set_active_db(\"postgres\")\nLLMWareConfig().set_vector_db(\"postgres\")\n```\n\n</details>  \n\n<details>\n<summary><b>Mix-and-Match</b>: LLMWare supports 3 text collection databases (Mongo, Postgres, SQLite) and \n10 vector databases (Milvus, PGVector-Postgres, Neo4j, Redis, Mongo-Atlas, Qdrant, Faiss, LanceDB, ChromaDB and Pinecone)  </summary>\n\n```bash\n# scripts to deploy other options\ncurl -o docker-compose.yaml https://raw.githubusercontent.com/llmware-ai/llmware/main/docker-compose-redis-stack.yaml\n```\n\n</details>  \n\n## Meet our Models   \n\n- **SLIM model series:** small, specialized models fine-tuned for function calling and multi-step, multi-model Agent workflows.  \n- **DRAGON model series:**  Production-grade RAG-optimized 6-9B parameter models - \"Delivering RAG on ...\" the leading foundation base models.  \n- **BLING model series:**  Small CPU-based RAG-optimized, instruct-following 1B-5B parameter models.  \n- **Industry BERT models:**  out-of-the-box custom trained sentence transformer embedding models fine-tuned for the following industries:  Insurance, Contracts, Asset Management, SEC.  \n- **GGUF Quantization:** we provide 'gguf' and 'tool' versions of many SLIM, DRAGON and BLING models, optimized for CPU deployment.  \n\n## Using LLMs and setting-up API keys & secrets\n\nLLMWare is an open platform and supports a wide range of open source and proprietary models.  To use LLMWare, you do not need to use any proprietary LLM - we would encourage you to experiment with [SLIM](https://www.huggingface.co/llmware/), [BLING](https://huggingface.co/llmware), [DRAGON](https://huggingface.co/llmware), [Industry-BERT](https://huggingface.co/llmware), the GGUF examples, along with bringing in your favorite models from HuggingFace and Sentence Transformers. \n\nIf you would like to use a proprietary model, you will need to provide your own API Keys.   API keys and secrets for models, aws, and pinecone can be set-up for use in environment variables or passed directly to method calls.  \n\n<details>  \n    \n<summary> ✨  <b>Roadmap - Where are we going ... </b>  </summary>\n\n- 💡 Making it easy to deploy fine-tuned open source models to build state-of-the-art RAG workflows  \n- 💡 Private cloud - keeping documents, data pipelines, data stores, and models safe and secure  \n- 💡 Model quantization, especially GGUF, and democratizing the game-changing use of 1-9B CPU-based LLMs  \n- 💡 Developing small specialized RAG optimized LLMs between 1B-9B parameters  \n- 💡 Industry-specific LLMs, embedding models and processes to support core knowledge-based use cases  \n- 💡 Enterprise scalability - containerization, worker deployments and Kubernetes  \n- 💡 Integration of SQL and other scale enterprise data sources  \n- 💡 Multi-step, multi-model Agent-based workflows with small, specialized function-calling models  \n\nLike our models, we aspire for llmware to be \"small, but mighty\" - easy to use and get started, but packing a powerful punch!  \n\n</details>\n\nInterested in contributing to llmware? Information on ways to participate can be found in our [Contributors Guide](https://github.com/llmware-ai/llmware/blob/main/repo_docs/CONTRIBUTING.md#contributing-to-llmware).  As with all aspects of this project, contributing is governed by our [Code of Conduct](https://github.com/llmware-ai/llmware/blob/main/repo_docs/CODE_OF_CONDUCT.md).\n\nQuestions and discussions are welcome in our [github discussions](https://github.com/llmware-ai/llmware/discussions).  \n\n## 📣  Release notes and Change Log  \n\nSee also [additional deployment/install release notes in wheel_archives](https://github.com/llmware-ai/llmware/tree/main/wheel_archives)   \n\n**Friday, November 8 - v0.3.9**  \n - Enhanced Azure OpenAI configuration, including streaming generation  \n - Removed deprecated parser binaries for Linux aarch64 and Mac x86  \n - Added generator option for CustomTable insert rows to provide progress on larger table builds  \n   \n**Sunday, October 27 - v0.3.8**\n - Integrating Model Depot collection of 100+ OpenVino and ONNX Models into LLMWare default model catalog  \n - Supporting changes in model classes, model catalog and model configs  \n   \n**Sunday, October 6 - v0.3.7**  \n- Added new model class - OVGenerativeModel - to support the use of models packaged in OpenVino format  \n- Added new model class - ONNXGenerativeModel - to support use of models packaged in ONNX format  \n- Getting started with [OpenVino example](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_openvino_models.py)  \n- Getting started with [ONNX example](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_onnx_models.py)  \n  \n**Tuesday, October 1 - v0.3.6**  \n- Added new prompt and chat templates  \n- Improved and updated model configurations    \n- New utility functions for locating and highlighting text matches in search results  \n- Improved hashing check utility functions  \n  \n**Monday, August 26 - v0.3.5**  \n- Added 10 new BLING+SLIM models to Model Catalog - featuring Qwen2, Phi-3 and Phi-3.5  \n- Launched new DRAGON models on Qwen-7B, Yi-9B, Mistral-v0.3, and Llama-3.1  \n- New Qwen2 Models (and RAG + function-calling fine-tunes) - [using-qwen2-models](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using-qwen2-models.py)  \n- New Phi-3 function calling models - [using-phi-3-function-calls](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using-phi-3-function-calls.py)  \n- New use case example - [lecture_tool](https://github.com/llmware-ai/llmware/blob/main/examples/Use_Cases/lecture_tool/)   \n- Improved GGUF Configs to expand context window  \n- Added model benchmark performance data to model configs \n- Enhanced Utilities hashing functions  \n  \nFor complete history of release notes, please open the Change log tab.  \n\n**Supported Operating Systems**: MacOS (Metal - M1/M2/M3), Linux (x86), and Windows  \n- Linux - support Ubuntu 20+  (glibc 2.31+)   \n- If you need support for another Linux version, please raise an issue - we will prioritize testing and ensure support.  \n\n**Supported Vector Databases**: Milvus, Postgres (PGVector), Neo4j, Redis, LanceDB, ChromaDB, Qdrant, FAISS, Pinecone, Mongo Atlas Vector Search\n\n**Supported Text Index Databases**: MongoDB, Postgres, SQLite  \n\n\n<details>\n<summary><b>Optional</b></summary>\n\n- [Docker](https://docs.docker.com/get-docker/)\n  \n- To enable the OCR parsing capabilities, install [Tesseract v5.3.3](https://tesseract-ocr.github.io/tessdoc/Installation.html) and [Poppler v23.10.0](https://poppler.freedesktop.org/) native packages.\n\n</details>\n\n<details>\n  <summary><b>🚧 Change Log</b></summary>\n\n**Monday, July 29 - v03.4**  \n- Enhanced safety protections for text2sql db reads for LLMfx agents   \n- New examples - see [example](https://github.com/llmware-ai/llmware/blob/main/examples/UI/dueling_chatbot.py)    \n- More Notebook examples - see [notebook examples](https://github.com/llmware-ai/llmware/blob/main/examples/Notebooks)      \n  \n**Monday, July 8 - v03.3**  \n- Improvements in model configuration options, logging, and various small fixes  \n- Improved Azure OpenAI configs - see [example](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using-azure-openai.py)  \n  \n**Saturday, June 29 - v0.3.2**  \n- Update to PDF and Office parsers - improvements to configurations in logging and text chunking options  \n  \n**Saturday, June 22 - v0.3.1**  \n- Added module 3 to Fast Start example series [examples 7-9 on Agents & Function Calls](https://github.com/llmware-ai/llmware/tree/main/fast_start)  \n- Added reranker Jina model for in-memory semantic similarity RAG - see [example](https://github.com/llmware-ai/llmware/tree/main/examples/Embedding/using_semantic_reranker_with_rag.py)  \n- Enhanced model fetching parameterization in model loading process  \n- Added new 'tiny' versions of slim-extract and slim-summary in both Pytorch and GGUF versions - check out 'slim-extract-tiny-tool' and 'slim-summary-tiny-tool'  \n- [Biz Bot] use case - see [example](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/biz_bot.py) and [video](https://youtu.be/4nBYDEjxxTE?si=o6PDPbu0PVcT-tYd)  \n- Updated numpy reqs <2 and updated yfinance version minimum (>=0.2.38)     \n\n**Tuesday, June 4 - v0.3.0**  \n- Added support for new Milvus Lite embedded 'no-install' database - see [example](https://github.com/llmware-ai/llmware/tree/main/examples/Embedding/using_milvus_lite.py).   \n- Added two new SLIM models to catalog and agent processes - ['q-gen'](https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/using-slim-q-gen.py) and ['qa-gen'](https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/using-slim-qa-gen.py)    \n- Updated model class instantiation to provide more extensibility to add new classes in different modules  \n- New welcome_to_llmware.sh and welcome_to_llmware_windows.sh fast install scripts  \n- Enhanced Model class base with new configurable post_init and register methods  \n- Created InferenceHistory to track global state of all inferences completed  \n- Multiple improvements and updates to logging at module level  \n- Note: starting with v0.3.0, pip install provides two options - a base minimal install `pip3 install llmware` which will support most use cases, and a larger install `pip3 install 'llmware[full]'` with other commonly-used libraries.  \n  \n**Wednesday, May 22 - v0.2.15**  \n- Improvements in Model class handling of Pytorch and Transformers dependencies (just-in-time loading, if needed)  \n- Expanding API endpoint options and inference server functionality - see new [client access options](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/llmware_inference_api_client.py)  and [server_launch](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/llmware_inference_server.py)  \n\n**Saturday, May 18 - v0.2.14**  \n- New OCR image parsing methods with [example](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/slicing_and_dicing_office_docs.py)  \n- Adding first part of logging improvements (WIP) in Configs and Models.    \n- New embedding model added to catalog - industry-bert-loans.  \n- Updates to model import methods and configurations.  \n\n**Sunday, May 12 - v0.2.13**  \n- New GGUF streaming method with [basic example](https://github.com/llmware-ai/llmware/tree/main/examples/Models/gguf_streaming.py) and [phi3 local chatbot](https://github.com/llmware-ai/llmware/tree/main/examples/UI/gguf_streaming_chatbot.py)  \n- Significant cleanups in ancillary imports and dependencies to reduce install complexity - note: the updated requirements.txt and setup.py files.  \n- Defensive code to provide informative warning of any missing dependencies in specialized parts of the code, e.g., OCR, Web Parser.  \n- Updates of tests, notice and documentation.   \n- OpenAIConfigs created to support Azure OpenAI.   \n  \n**Sunday, May 5 - v0.2.12 Update**  \n- Launched [\"bling-phi-3\"](https://huggingface.co/llmware/bling-phi-3) and [\"bling-phi-3-gguf\"](https://huggingface.co/llmware/bling-phi-3-gguf) in ModelCatalog - newest and most accurate BLING/DRAGON model  \n- New long document summarization method using slim-summary-tool [example](https://github.com/llmware-ai/llmware/tree/main/examples/Prompts/document_summarizer.py)  \n- New Office (Powerpoint, Word, Excel) sample files [example](https://github.com/llmware-ai/llmware/tree/main/examples/Parsing/parsing_microsoft_ir_docs.py)  \n- Added support for Python 3.12  \n- Deprecated faiss and replaced with 'no-install' chromadb in Fast Start examples  \n- Refactored Datasets, Graph and Web Services classes  \n- Updated Voice parsing with WhisperCPP into Library  \n  \n**Monday, April 29 - v0.2.11 Update**  \n- Updates to gguf libs for Phi-3 and Llama-3  \n- Added Phi-3 [example](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-microsoft-phi-3.py)  and Llama-3 [example](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-llama-3.py) and Quantized Versions to Model Catalog  \n- Integrated WhisperCPP Model class and prebuilt shared libraries - [getting-started-example](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-whisper-cpp-getting-started.py)  \n- New voice sample files for testing - [example](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-whisper-cpp-sample-files.py)  \n- Improved CUDA detection on Windows and safety checks for older Mac OS versions  \n\n**Monday, April 22 - v0.2.10 Update**  \n- Updates to Agent class to support Natural Language queries of Custom Tables on Postgres [example](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/agent_with_custom_tables.py)  \n- New Agent API endpoint implemented with LLMWare Inference Server and new Agent capabilities [example](https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/agent_api_endpoint.py)  \n  \n**Tuesday, April 16 - v0.2.9 Update**  \n- New CustomTable class to rapidly create custom DB tables in conjunction with LLM-based workflows.  \n- Enhanced methods for converting CSV and JSON/JSONL files into DB tables.  \n- See new examples [Creating Custom Table example](https://github.com/llmware-ai/llmware/tree/main/examples/Structured_Tables/create_custom_table-1.py)\n    \n**Tuesday, April 9 - v0.2.8 Update**  \n- Office Parser (Word Docx, Powerpoint PPTX, and Excel XLSX) - multiple improvements - new libs + Python method.  \n- Includes: several fixes, improved text chunking controls, header text extraction and configuration options.  \n- Generally, new office parser options conform with the new PDF parser options.  \n- Please see [Office Parsing Configs example](https://github.com/llmware-ai/llmware/tree/main/examples/Parsing/office_parser_new_configs.py)  \n\n**Wednesday, April 3 - v0.2.7 Update**  \n- PDF Parser - multiple improvements - new libs + Python methods.  \n- Includes: UTF-8 encoding for European languages.  \n- Includes: Better text chunking controls, header text extraction and configuration options.  \n- Please see [PDF Parsing Configs example](https://github.com/llmware-ai/llmware/tree/main/examples/Parsing/pdf_parser_new_configs.py) for more details.  \n- Note:  deprecating support for aarch64-linux (will use 0.2.6 parsers).  Full support going forward for Linux Ubuntu20+ on x86_64 + with CUDA.  \n  \n**Friday, March 22 - v0.2.6 Update**  \n- New SLIM models:  summary, extract, xsum, boolean, tags-3b, and combo sentiment-ner.  \n- New logit and sampling analytics.  \n- New SLIM examples showing how to use the new models.  \n  \n**Thursday, March 14 - v0.2.5 Update**  \n- Improved support for GGUF on CUDA (Windows and Linux), with new prebuilt binaries and exception handling.  \n- Enhanced model configuration options (sampling, temperature, top logit capture).  \n- Added full back-level support for Ubuntu 20+ with parsers and GGUF engine.  \n- Support for new Anthropic Claude 3 models.  \n- New retrieval methods: document_lookup and aggregate_text.  \n- New model:  bling-stablelm-3b-tool - fast, accurate 3b quantized question-answering model - one of our new favorites.  \n\n**Wednesday, February 28 - v0.2.4 Update**  \n- Major upgrade of GGUF Generative Model class - support for Stable-LM-3B, CUDA build options, and better control over sampling strategies.\n- Note: new GGUF llama.cpp built libs packaged with build starting in v0.2.4.  \n- Improved GPU support for HF Embedding Models.   \n  \n**Friday, February 16 - v0.2.3 Update**  \n- Added 10+ embedding models to ModelCatalog - nomic, jina, bge, gte, ember and uae-large.   \n- Updated OpenAI support >=1.0 and new text-3 embedding models.    \n- SLIM model keys and output_values now accessible in ModelCatalog.  \n- Updating encodings to 'utf-8-sig' to better handle txt/csv files with bom.  \n\n**Latest Updates - 19 Jan 2024 - llmware v0.2.0**\n  - Added new database integration options - Postgres and SQlite\n  - Improved status update and parser event logging options for parallelized parsing\n  - Significant enhancements to interactions between Embedding + Text collection databases\n  - Improved error exception handling in loading dynamic modules\n\n**Latest Updates - 15 Jan 2024: llmware v0.1.15**\n  - Enhancements to dual pass retrieval queries\n  - Expanded configuration objects and options for endpoint resources\n    \n**Latest Updates - 30 Dec 2023: llmware v0.1.14**\n  - Added support for Open Chat inference servers (compatible with OpenAI API)\n  - Improved capabilities for multiple embedding models and vector DB configurations\n  - Added docker-compose install scripts for PGVector and Redis vector databases\n  - Added 'bling-tiny-llama' to model catalog\n         \n**Latest Updates - 22 Dec 2023: llmware v0.1.13**\n  - Added 3 new vector databases - Postgres (PG Vector), Redis, and Qdrant\n  - Improved support for integrating sentence transformers directly in the model catalog\n  - Improvements in the model catalog attributes\n  - Multiple new Examples in Models & Embeddings, including GGUF, Vector database, and model catalog\n\n- **17 Dec 2023: llmware v0.1.12**\n  - dragon-deci-7b added to catalog - RAG-finetuned model on high-performance new 7B model base from Deci\n  - New GGUFGenerativeModel class for easy integration of GGUF Models\n  - Adding prebuilt llama_cpp / ctransformer shared libraries for Mac M1, Mac x86, Linux x86 and Windows\n  - 3 DRAGON models packaged as Q4_K_M GGUF models for CPU laptop use (dragon-mistral-7b, dragon-llama-7b, dragon-yi-6b)\n  - 4 leading open source chat models added to default catalog with Q4_K_M\n  \n- **8 Dec 2023: llmware v0.1.11**\n  - New fast start examples for high volume Document Ingestion and Embeddings with Milvus.\n  - New LLMWare 'Pop up' Inference Server model class and example script.\n  - New Invoice Processing example for RAG.\n  - Improved Windows stack management to support parsing larger documents.\n  - Enhancing debugging log output mode options for PDF and Office parsers.\n\n- **30 Nov 2023: llmware v0.1.10**\n  - Windows added as a supported operating system.\n  - Further enhancements to native code for stack management. \n  - Minor defect fixes.\n\n- **24 Nov 2023: llmware v0.1.9**\n  - Markdown (.md) files are now parsed and treated as text files.\n  - PDF and Office parser stack optimizations which should avoid the need to set ulimit -s.\n  - New llmware_models_fast_start.py example that allows discovery and selection of all llmware HuggingFace models.\n  - Native dependencies (shared libraries and dependencies) now included in repo to faciliate local development.\n  - Updates to the Status class to support PDF and Office document parsing status updates.\n  - Minor defect fixes including image block handling in library exports.\n\n- **17 Nov 2023: llmware v0.1.8**\n  - Enhanced generation performance by allowing each model to specific the trailing space parameter.\n  - Improved handling for eos_token_id for llama2 and mistral.\n  - Improved support for Hugging Face dynamic loading\n  - New examples with the new llmware DRAGON models.\n    \n- **14 Nov 2023: llmware v0.1.7**\n  - Moved to Python Wheel package format for PyPi distribution to provide seamless installation of native dependencies on all supported platforms.  \n  - ModelCatalog enhancements:\n    - OpenAI update to include newly announced ‘turbo’ 4 and 3.5 models.\n    - Cohere embedding v3 update to include new Cohere embedding models.\n    - BLING models as out-of-the-box registered options in the catalog. They can be instantiated like any other model, even without the “hf=True” flag.\n    - Ability to register new model names, within existing model classes, with the register method in ModelCatalog.\n  - Prompt enhancements:\n    - “evidence_metadata” added to prompt_main output dictionaries allowing prompt_main responses to be plug into the evidence and fact-checking steps without modification.\n    - API key can now be passed directly in a prompt.load_model(model_name, api_key = “[my-api-key]”)\n  - LLMWareInference Server - Initial delivery:\n    - New Class for LLMWareModel which is a wrapper on a custom HF-style API-based model.    \n    - LLMWareInferenceServer is a new class that can be instantiated on a remote (GPU) server to create a testing API-server that can be integrated into any Prompt workflow.    \n \n- **03 Nov 2023: llmware v0.1.6**\n  - Updated packaging to require mongo-c-driver 1.24.4 to temporarily workaround segmentation fault with mongo-c-driver 1.25.\n  - Updates in python code needed in anticipation of future Windows support.  \n\n- **27 Oct 2023: llmware v0.1.5**\n  - Four new example scripts focused on RAG workflows with small, fine-tuned instruct models that run on a laptop (`llmware` [BLING](https://huggingface.co/llmware) models).\n  - Expanded options for setting temperature inside a prompt class.\n  - Improvement in post processing of Hugging Face model generation.\n  - Streamlined loading of Hugging Face generative models into prompts.\n  - Initial delivery of a central status class: read/write of embedding status with a consistent interface for callers.\n  - Enhanced in-memory dictionary search support for multi-key queries.\n  - Removed trailing space in human-bot wrapping to improve generation quality in some fine-tuned models.\n  - Minor defect fixes, updated test scripts, and version update for Werkzeug to address [dependency security alert](https://github.com/llmware-ai/llmware/security/dependabot/2).\n- **20 Oct 2023: llmware v0.1.4**\n  - GPU support for Hugging Face models.\n  - Defect fixes and additional test scripts.\n- **13 Oct 2023: llmware v0.1.3**\n  - MongoDB Atlas Vector Search support.\n  - Support for authentication using a MongoDB connection string.\n  - Document summarization methods.\n  - Improvements in capturing the model context window automatically and passing changes in the expected output length.  \n  - Dataset card and description with lookup by name.\n  - Processing time added to model inference usage dictionary.\n  - Additional test scripts, examples, and defect fixes.\n- **06 Oct 2023: llmware v0.1.1**\n  - Added test scripts to the github repository for regression testing.\n  - Minor defect fixes and version update of Pillow to address [dependency security alert](https://github.com/llmware-ai/llmware/security/dependabot/1).\n- **02 Oct 2023: llmware v0.1.0**  🔥 Initial release of llmware to open source!! 🔥\n\n\n</details>\n<p align=\"centre\">\n  <a href=\"#top\">⬆️ Back to Top</a>\n</p>\n\n## 🤓 Read our White Papers\n\n\n- **Revolutionizing AI Deployment: Unleashing AI Acceleration with Intel's AI PCs and Model HQ by LLMWare** [AI PC Model HQ.pdf](https://github.com/user-attachments/files/18024139/AI.PC.Model.HQ.pdf)\n- **Revultionizing AI Deployment (Intel Abstract Version)**  [LNL White paper (Abstract Version) final.pdf](https://github.com/user-attachments/files/18281644/LNL.White.paper.Abstract.Version.final.pdf)\n\n- **Accelerating AI Powered Productivity with AI PCs** [Laptop.Performance.WP.Final (10).pdf](https://github.com/user-attachments/files/18024294/Laptop.Performance.WP.Final.10.pdf)\n\n## Intel Joint Solutions\n\n- **Arrow Lake** \n[IPA.Optimization.Summary.LLMWare (1).pdf](https://github.com/user-attachments/files/18292873/IPA.Optimization.Summary.LLMWare.1.pdf)\n\n## About Model HQ\n  - **Privacy Policy** [AI BLOKS PRIVACY POLICY (1.2.25).docx](https://github.com/user-attachments/files/18291352/AI.BLOKS.PRIVACY.POLICY.1.2.25.docx)\n\n- **Terms of Service** [AI Bloks Terms of Service 1.3.25.docx](https://github.com/user-attachments/files/18291360/AI.Bloks.Terms.of.Service.1.3.25.docx)\n- **Acceptable Use Policy**[Acceptable Use Policy for Model HQ by AI BLOKS LLC.docx](https://github.com/user-attachments/files/18291481/Acceptable.Use.Policy.for.Model.HQ.by.AI.BLOKS.LLC.docx)\n\n"
        },
        {
          "name": "devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "fast_start",
          "type": "tree",
          "content": null
        },
        {
          "name": "hackathon",
          "type": "tree",
          "content": null
        },
        {
          "name": "llmware",
          "type": "tree",
          "content": null
        },
        {
          "name": "repo_docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.84375,
          "content": "#!/usr/bin/env python\n\nimport os\nimport platform\nimport re\nimport sys\nfrom setuptools import find_packages, setup, Extension\nfrom pathlib import Path\n\nVERSION_FILE = \"llmware/__init__.py\"\nwith open(VERSION_FILE, encoding='utf-8') as version_file:\n    match = re.search(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\",version_file.read(), re.MULTILINE)\n\nif match:\n    version = match.group(1)\nelse:\n    raise RuntimeError(f\"Unable to find version string in {VERSION_FILE}.\")\n\nwith open(\"README.md\", encoding='utf-8') as readme_file:\n    long_description = readme_file.read()\n\ndef glob_fix(package_name, glob):\n    # this assumes setup.py lives in the folder that contains the package\n    package_path = Path(f'./{package_name}').resolve()\n    return [str(path.relative_to(package_path)) \n            for path in package_path.glob(glob)]\n\nsetup(\n    name=\"llmware\",  # Required\n    version=version,  # Required\n    description=\"An enterprise-grade LLM-based development framework, tools, and fine-tuned models\",  # Optional\n    long_description=long_description,  # Optional\n    long_description_content_type=\"text/markdown\",  # Optional\n    url=\"https://github.com/llmware-ai\",\n    project_urls={\n        'Repository': 'https://github.com/llmware-ai/llmware',\n    },\n    author=\"llmware\",\n    author_email=\"support@aibloks.com\",  # Optional\n    classifiers=[  # Optional\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"Topic :: Software Development\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\"\n    ],\n    keywords=\"ai,llm,rag,data,development\",  # Optional\n    packages=['llmware'],\n    package_data={'llmware': ['*.c', '*.so', '*.dylib', '.dylibs/*', *glob_fix('llmware', 'lib/**/*')], 'llmware.libs': ['*']},\n    python_requires=\">=3.9\",\n    zip_safe=True,\n    install_requires=[\n        'boto3>=1.24.53',\n        'huggingface-hub>=0.19.4',\n        'numpy>=1.23.2',\n        'pymongo>=4.7.0',\n        'tokenizers>=0.15.0',\n        'psycopg-binary==3.1.17',\n        'psycopg==3.1.17',\n        'pgvector==0.2.4',\n        'colorama==0.4.6',\n        'soundfile>=0.12.0',\n        'soxr>=0.5.0'\n    ],\n\n    extras_require={\n        'milvus': ['pymilvus>=2.3.0'],\n        'chromadb': ['chromadb>=0.4.22'],\n        'pinecone': ['pinecone-client==3.0.0'],\n        'lancedb': ['lancedb==0.5.0'],\n        'qdrant': ['qdrant-client==1.7.0'],\n        'redis': ['redis==5.0.1'],\n        'neo4j': ['neo4j==5.16.0'],\n        'full': ['torch>=1.13.1', 'transformers>=4.36.0', 'einops>=0.7.0', 'Wikipedia-API>=0.6.0',\n                 'openai>=1.0', 'datasets>=2.15.0', 'yfinance>=0.2.38', 'pymilvus>=2.3.0',\n                 'chromadb>=0.4.22', 'streamlit', 'Flask']\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "welcome_to_llmware.sh",
          "type": "blob",
          "size": 2.693359375,
          "content": "#! /bin/bash\n\n# Welcome to LLMWare script - handles some basic setup for first-time cloning of the repo\n# Mac / Linux version\n\n# Install core dependencies\npip3 install -r ./llmware/requirements.txt\n\n# # Note: this step is optional but adds many commonly-used optional dependencies (including in several examples)\npip3 install -r ./llmware/requirements_extras.txt\n\n# Move selected examples into root path for easy execution from command line\nscp ./examples/Getting_Started/welcome_example.py .\nscp ./fast_start/rag/*.py .\nscp ./examples/UI/gguf_streaming_chatbot.py .\nscp ./examples/SLIM-Agents/agent-llmfx-getting-started.py .\nscp ./examples/SLIM-Agents/using_slim_extract_model.py .\nscp ./examples/SLIM-Agents/using_slim_summary.py .\nscp ./examples/Models/bling_fast_start.py .\nscp ./examples/Models/dragon_gguf_fast_start.py .\nscp ./examples/Prompts/document_summarizer.py .\nscp ./examples/Use_Cases/web_services_slim_fx.py .\nscp ./examples/Use_Cases/invoice_processing.py .\nscp ./examples/Models/using-whisper-cpp-sample-files.py .\nscp ./examples/Parsing/parsing_microsoft_ir_docs.py .\nscp ./examples/Models/chat_models_gguf_fast_start.py .\nscp ./examples/Models/gguf_streaming.py .\n\necho \"\\nWelcome Steps Completed\"\necho \"1.  Installed Core Dependencies\"\necho \"2.  Installed Several Optional Dependencies Useful for Running Examples\"\necho \"3.  Moved selected Getting Started examples into /root path\"\necho \"    -- welcome_example.py\"\necho \"    -- example-1-create_first_library.py\"\necho \"    -- example-2-build_embeddings.py\"\necho \"    -- example-3-prompts_and_models.py\"\necho \"    -- example-4-rag-text-query.py\"\necho \"    -- example-5-rag-semantic-query.py\"\necho \"    -- example-6-rag-multi-step-query.py\"\necho \"    -- gguf_streaming.py\"\necho \"    -- bling_fast_start.py\"\necho \"    -- using_slim_extract_model.py\"\necho \"    -- using_slim_summary.py\"\necho \"    -- dragon_gguf_fast_start.py\"\necho \"    -- invoice_processing.py\"\necho \"    -- gguf_streaming_chatbot.py\"\necho \"    -- agent-llmfx-getting-started.py\"\necho \"    -- whisper-cpp-sample-files.py\"\necho \"    -- web_services_slim_fx.py\"\necho \"    -- parsing_microsoft_ir_docs.py\"\necho \"    -- document_summarizer.py\"\necho \"    -- chat_models_gguf_fast_start.py\"\n\necho \"\"\necho \"To run an example from command-line:  python3 welcome_example.py\"\necho \"To run gguf_streaming_chatbot.py: streamlit run gguf_streaming_chatbot.py\"\necho \"Note: check the /examples folder for 100+ additional examples\"\necho \"Note: on first time use, models will be downloaded and cached locally\"\necho \"Note: open up the examples and edit for more configuration options\"\necho \"\"\n\n# run welcome_example.py which serves as a test that the installation is successful\necho \"Running welcome_example.py\"\npython3 welcome_example.py\n"
        },
        {
          "name": "welcome_to_llmware_windows.sh",
          "type": "blob",
          "size": 2.6552734375,
          "content": "#! /bin/bash\n\n# Welcome to LLMWare script - handles some basic setup for first-time cloning of the repo\n# Windows version\n\n# Install core dependencies\npip3 install -r ./llmware/requirements.txt\n\n# Note: this step is optional but adds many commonly-used optional dependencies (including in several examples)\npip3 install -r ./llmware/requirements_extras.txt\n\n# Move selected examples into root path for easy execution from command line\nscp ./examples/Getting_Started/welcome_example.py .\nscp ./fast_start/rag/*.py .\nscp ./examples/UI/gguf_streaming_chatbot.py .\nscp ./examples/SLIM-Agents/agent-llmfx-getting-started.py .\nscp ./examples/SLIM-Agents/using_slim_extract_model.py .\nscp ./examples/SLIM-Agents/using_slim_summary.py .\nscp ./examples/Models/bling_fast_start.py .\nscp ./examples/Models/dragon_gguf_fast_start.py .\nscp ./examples/Prompts/document_summarizer.py .\nscp ./examples/Use_Cases/web_services_slim_fx.py .\nscp ./examples/Use_Cases/invoice_processing.py .\nscp ./examples/Models/using-whisper-cpp-sample-files.py .\nscp ./examples/Parsing/parsing_microsoft_ir_docs.py .\nscp ./examples/Models/chat_models_gguf_fast_start.py .\nscp ./examples/Models/gguf_streaming.py .\n\necho \"Welcome Steps Completed\"\necho \"1.  Installed Core Dependencies\"\necho \"2.  Installed Several Optional Dependencies Useful for Running Examples\"\necho \"3.  Moved selected Getting Started examples into /root path\"\necho \"    -- welcome_example.py\"\necho \"    -- example-1-create_first_library.py\"\necho \"    -- example-2-build_embeddings.py\"\necho \"    -- example-3-prompts_and_models.py\"\necho \"    -- example-4-rag-text-query.py\"\necho \"    -- example-5-rag-semantic-query.py\"\necho \"    -- example-6-rag-multi-step-query.py\"\necho \"    -- gguf_streaming.py\"\necho \"    -- bling_fast_start.py\"\necho \"    -- using_slim_extract_model.py\"\necho \"    -- using_slim_summary.py\"\necho \"    -- dragon_gguf_fast_start.py\"\necho \"    -- invoice_processing.py\"\necho \"    -- gguf_streaming_chatbot.py\"\necho \"    -- agent-llmfx-getting-started.py\"\necho \"    -- whisper-cpp-sample-files.py\"\necho \"    -- web_services_slim_fx.py\"\necho \"    -- parsing_microsoft_ir_docs.py\"\necho \"    -- document_summarizer.py\"\necho \"    -- chat_models_gguf_fast_start.py .\"\n\necho \"\"\necho \"To run an example from command-line:  py welcome_example.py\"\necho \"To run gguf_streaming_chatbot.py:  streamlit run gguf_streaming_chatbot.py\"\necho \"Note: check the /examples folder for 100+ additional examples\"\necho \"Note: on first time use, models will be downloaded and cached locally\"\necho \"Note: open up the examples and edit for more configuration options\"\necho \"\"\n\necho \"Running welcome_example.py\"\npy welcome_example.py\n\n# keeps bash console open (may be in separate window)\nbash\n"
        },
        {
          "name": "wheel_archives",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}