{
  "metadata": {
    "timestamp": 1736561036654,
    "page": 809,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "karpathy/arxiv-sanity-preserver",
      "stars": 5166,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.091796875,
          "content": "tmp/\npdf/\ntxt/\nstatic/thumbs/\n*.p\n*.pyc\nas.db\nsecret_key.txt\nenv/\ndata\n*.DS_Store\ntwitter.txt\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.0546875,
          "content": "The MIT License (MIT)\nCopyright (c) 2015 Andrej Karpathy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.794921875,
          "content": "\n# arxiv sanity preserver\n\n**Update Nov 27, 2021**: you may wish to look at my from-scratch re-write of arxiv-sanity: [arxiv-sanity-lite](https://github.com/karpathy/arxiv-sanity-lite). It is a smaller version of arxiv-sanity that focuses on the core value proposition, is significantly less likely to ever go down, scales better, and has a few additional goodies such as multiple possible tags per account, regular emails of new papers of interest, etc. It is also running live on [arxiv-sanity-lite.com](https://arxiv-sanity-lite.com).\n\nThis project is a web interface that attempts to tame the overwhelming flood of papers on Arxiv. It allows researchers to keep track of recent papers, search for papers, sort papers by similarity to any paper, see recent popular papers, to add papers to a personal library, and to get personalized recommendations of (new or old) Arxiv papers. This code is currently running live at [www.arxiv-sanity.com/](http://www.arxiv-sanity.com/), where it's serving 25,000+ Arxiv papers from Machine Learning (cs.[CV|AI|CL|LG|NE]/stat.ML) over the last ~3 years. With this code base you could replicate the website to any of your favorite subsets of Arxiv by simply changing the categories in `fetch_papers.py`.\n\n![user interface](https://raw.github.com/karpathy/arxiv-sanity-preserver/master/ui.jpeg)\n\n### Code layout\n\nThere are two large parts of the code:\n\n**Indexing code**. Uses Arxiv API to download the most recent papers in any categories you like, and then downloads all papers, extracts all text, creates tfidf vectors based on the content of each paper. This code is therefore concerned with the backend scraping and computation: building up a database of arxiv papers, calculating content vectors, creating thumbnails, computing SVMs for people, etc.\n\n**User interface**. Then there is a web server (based on Flask/Tornado/sqlite) that allows searching through the database and filtering papers by similarity, etc.\n\n### Dependencies\n\nSeveral: You will need numpy, feedparser (to process xml files), scikit learn (for tfidf vectorizer, training of SVM), flask (for serving the results), flask_limiter, and tornado (if you want to run the flask server in production). Also dateutil, and scipy. And sqlite3 for database (accounts, library support, etc.). Most of these are easy to get through `pip`, e.g.:\n\n```bash\n$ virtualenv env                # optional: use virtualenv\n$ source env/bin/activate       # optional: use virtualenv\n$ pip install -r requirements.txt\n```\n\nYou will also need [ImageMagick](http://www.imagemagick.org/script/index.php) and [pdftotext](https://poppler.freedesktop.org/), which you can install on Ubuntu as `sudo apt-get install imagemagick poppler-utils`. Bleh, that's a lot of dependencies isn't it.\n\n### Processing pipeline\n\nThe processing pipeline requires you to run a series of scripts, and at this stage I really encourage you to manually inspect each script, as they may contain various inline settings you might want to change. In order, the processing pipeline is:\n\n1. Run `fetch_papers.py` to query arxiv API and create a file `db.p` that contains all information for each paper. This script is where you would modify the **query**, indicating which parts of arxiv you'd like to use. Note that if you're trying to pull too many papers arxiv will start to rate limit you. You may have to run the script multiple times, and I recommend using the arg `--start-index` to restart where you left off when you were last interrupted by arxiv.\n2. Run `download_pdfs.py`, which iterates over all papers in parsed pickle and downloads the papers into folder `pdf`\n3. Run `parse_pdf_to_text.py` to export all text from pdfs to files in `txt`\n4. Run `thumb_pdf.py` to export thumbnails of all pdfs to `thumb`\n5. Run `analyze.py` to compute tfidf vectors for all documents based on bigrams. Saves a `tfidf.p`, `tfidf_meta.p` and `sim_dict.p` pickle files.\n6. Run `buildsvm.py` to train SVMs for all users (if any), exports a pickle `user_sim.p`\n7. Run `make_cache.py` for various preprocessing so that server starts faster (and make sure to run `sqlite3 as.db < schema.sql` if this is the very first time ever you're starting arxiv-sanity, which initializes an empty database).\n8. Start the mongodb daemon in the background. Mongodb can be installed by following the instructions here - https://docs.mongodb.com/tutorials/install-mongodb-on-ubuntu/.\n  * Start the mongodb server with - `sudo service mongod start`.\n  * Verify if the server is running in the background : The last line of /var/log/mongodb/mongod.log file must be - \n`[initandlisten] waiting for connections on port <port> `\n9. Run the flask server with `serve.py`. Visit localhost:5000 and enjoy sane viewing of papers!\n\nOptionally you can also run the `twitter_daemon.py` in a screen session, which uses your Twitter API credentials (stored in `twitter.txt`) to query Twitter periodically looking for mentions of papers in the database, and writes the results to the pickle file `twitter.p`.\n\nI have a simple shell script that runs these commands one by one, and every day I run this script to fetch new papers, incorporate them into the database, and recompute all tfidf vectors/classifiers. More details on this process below.\n\n**protip: numpy/BLAS**: The script `analyze.py` does quite a lot of heavy lifting with numpy. I recommend that you carefully set up your numpy to use BLAS (e.g. OpenBLAS), otherwise the computations will take a long time. With ~25,000 papers and ~5000 users the script runs in several hours on my current machine with a BLAS-linked numpy.\n\n### Running online\n\nIf you'd like to run the flask server online (e.g. AWS) run it as `python serve.py --prod`.\n\nYou also want to create a `secret_key.txt` file and fill it with random text (see top of `serve.py`).\n\n### Current workflow\n\nRunning the site live is not currently set up for a fully automatic plug and play operation. Instead it's a bit of a manual process and I thought I should document how I'm keeping this code alive right now. I have a script that performs the following update early morning after arxiv papers come out (~midnight PST):\n\n```bash\npython fetch_papers.py\npython download_pdfs.py\npython parse_pdf_to_text.py\npython thumb_pdf.py\npython analyze.py\npython buildsvm.py\npython make_cache.py\n```\n\nI run the server in a screen session, so `screen -S serve` to create it (or `-r` to reattach to it) and run:\n\n```bash\npython serve.py --prod --port 80\n```\n\nThe server will load the new files and begin hosting the site. Note that on some systems you can't use port 80 without `sudo`. Your two options are to use `iptables` to reroute ports or you can use [setcap](http://stackoverflow.com/questions/413807/is-there-a-way-for-non-root-processes-to-bind-to-privileged-ports-1024-on-l) to elavate the permissions of your `python` interpreter that runs `serve.py`. In this case I'd recommend careful permissions and maybe virtualenv, etc.\n"
        },
        {
          "name": "analyze.py",
          "type": "blob",
          "size": 3.359375,
          "content": "\"\"\"\nReads txt files of all papers and computes tfidf vectors for all papers.\nDumps results to file tfidf.p\n\"\"\"\nimport os\nimport pickle\nfrom random import shuffle, seed\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom utils import Config, safe_pickle_dump\n\nseed(1337)\nmax_train = 5000 # max number of tfidf training documents (chosen randomly), for memory efficiency\nmax_features = 5000\n\n# read database\ndb = pickle.load(open(Config.db_path, 'rb'))\n\n# read all text files for all papers into memory\ntxt_paths, pids = [], []\nn = 0\nfor pid,j in db.items():\n  n += 1\n  idvv = '%sv%d' % (j['_rawid'], j['_version'])\n  txt_path = os.path.join('data', 'txt', idvv) + '.pdf.txt'\n  if os.path.isfile(txt_path): # some pdfs dont translate to txt\n    with open(txt_path, 'r') as f:\n      txt = f.read()\n    if len(txt) > 1000 and len(txt) < 500000: # 500K is VERY conservative upper bound\n      txt_paths.append(txt_path) # todo later: maybe filter or something some of them\n      pids.append(idvv)\n      print(\"read %d/%d (%s) with %d chars\" % (n, len(db), idvv, len(txt)))\n    else:\n      print(\"skipped %d/%d (%s) with %d chars: suspicious!\" % (n, len(db), idvv, len(txt)))\n  else:\n    print(\"could not find %s in txt folder.\" % (txt_path, ))\nprint(\"in total read in %d text files out of %d db entries.\" % (len(txt_paths), len(db)))\n\n# compute tfidf vectors with scikits\nv = TfidfVectorizer(input='content', \n        encoding='utf-8', decode_error='replace', strip_accents='unicode', \n        lowercase=True, analyzer='word', stop_words='english', \n        token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b',\n        ngram_range=(1, 2), max_features = max_features, \n        norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,\n        max_df=1.0, min_df=1)\n\n# create an iterator object to conserve memory\ndef make_corpus(paths):\n  for p in paths:\n    with open(p, 'r') as f:\n      txt = f.read()\n    yield txt\n\n# train\ntrain_txt_paths = list(txt_paths) # duplicate\nshuffle(train_txt_paths) # shuffle\ntrain_txt_paths = train_txt_paths[:min(len(train_txt_paths), max_train)] # crop\nprint(\"training on %d documents...\" % (len(train_txt_paths), ))\ntrain_corpus = make_corpus(train_txt_paths)\nv.fit(train_corpus)\n\n# transform\nprint(\"transforming %d documents...\" % (len(txt_paths), ))\ncorpus = make_corpus(txt_paths)\nX = v.transform(corpus)\nprint(v.vocabulary_)\nprint(X.shape)\n\n# write full matrix out\nout = {}\nout['X'] = X # this one is heavy!\nprint(\"writing\", Config.tfidf_path)\nsafe_pickle_dump(out, Config.tfidf_path)\n\n# writing lighter metadata information into a separate (smaller) file\nout = {}\nout['vocab'] = v.vocabulary_\nout['idf'] = v._tfidf.idf_\nout['pids'] = pids # a full idvv string (id and version number)\nout['ptoi'] = { x:i for i,x in enumerate(pids) } # pid to ix in X mapping\nprint(\"writing\", Config.meta_path)\nsafe_pickle_dump(out, Config.meta_path)\n\nprint(\"precomputing nearest neighbor queries in batches...\")\nX = X.todense() # originally it's a sparse matrix\nsim_dict = {}\nbatch_size = 200\nfor i in range(0,len(pids),batch_size):\n  i1 = min(len(pids), i+batch_size)\n  xquery = X[i:i1] # BxD\n  ds = -np.asarray(np.dot(X, xquery.T)) #NxD * DxB => NxB\n  IX = np.argsort(ds, axis=0) # NxB\n  for j in range(i1-i):\n    sim_dict[pids[i+j]] = [pids[q] for q in list(IX[:50,j])]\n  print('%d/%d...' % (i, len(pids)))\n\nprint(\"writing\", Config.sim_path)\nsafe_pickle_dump(sim_dict, Config.sim_path)\n"
        },
        {
          "name": "buildsvm.py",
          "type": "blob",
          "size": 2.158203125,
          "content": "# standard imports\nimport os\nimport sys\nimport pickle\n# non-standard imports\nimport numpy as np\nfrom sklearn import svm\nfrom sqlite3 import dbapi2 as sqlite3\n# local imports\nfrom utils import safe_pickle_dump, strip_version, Config\n\nnum_recommendations = 1000 # papers to recommend per user\n# -----------------------------------------------------------------------------\n\nif not os.path.isfile(Config.database_path):\n  print(\"the database file as.db should exist. You can create an empty database with sqlite3 as.db < schema.sql\")\n  sys.exit()\n\nsqldb = sqlite3.connect(Config.database_path)\nsqldb.row_factory = sqlite3.Row # to return dicts rather than tuples\n\ndef query_db(query, args=(), one=False):\n  \"\"\"Queries the database and returns a list of dictionaries.\"\"\"\n  cur = sqldb.execute(query, args)\n  rv = cur.fetchall()\n  return (rv[0] if rv else None) if one else rv\n\n# -----------------------------------------------------------------------------\n\n# fetch all users\nusers = query_db('''select * from user''')\nprint('number of users: ', len(users))\n\n# load the tfidf matrix and meta\nmeta = pickle.load(open(Config.meta_path, 'rb'))\nout = pickle.load(open(Config.tfidf_path, 'rb'))\nX = out['X']\nX = X.todense().astype(np.float32)\n\nxtoi = { strip_version(x):i for x,i in meta['ptoi'].items() }\n\nuser_sim = {}\nfor ii,u in enumerate(users):\n  print(\"%d/%d building an SVM for %s\" % (ii, len(users), u['username'].encode('utf-8')))\n  uid = u['user_id']\n  lib = query_db('''select * from library where user_id = ?''', [uid])\n  pids = [x['paper_id'] for x in lib] # raw pids without version\n  posix = [xtoi[p] for p in pids if p in xtoi]\n  \n  if not posix:\n    continue # empty library for this user maybe?\n\n  print(pids)\n  y = np.zeros(X.shape[0])\n  for ix in posix: y[ix] = 1\n\n  clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=10000, tol=1e-6, C=0.1)\n  clf.fit(X,y)\n  s = clf.decision_function(X)\n\n  sortix = np.argsort(-s)\n  sortix = sortix[:min(num_recommendations, len(sortix))] # crop paper recommendations to save space\n  user_sim[uid] = [strip_version(meta['pids'][ix]) for ix in list(sortix)]\n\nprint('writing', Config.user_sim_path)\nsafe_pickle_dump(user_sim, Config.user_sim_path)\n"
        },
        {
          "name": "download_pdfs.py",
          "type": "blob",
          "size": 1.212890625,
          "content": "import os\nimport time\nimport pickle\nimport shutil\nimport random\nfrom  urllib.request import urlopen\n\nfrom utils import Config\n\ntimeout_secs = 10 # after this many seconds we give up on a paper\nif not os.path.exists(Config.pdf_dir): os.makedirs(Config.pdf_dir)\nhave = set(os.listdir(Config.pdf_dir)) # get list of all pdfs we already have\n\nnumok = 0\nnumtot = 0\ndb = pickle.load(open(Config.db_path, 'rb'))\nfor pid,j in db.items():\n  \n  pdfs = [x['href'] for x in j['links'] if x['type'] == 'application/pdf']\n  assert len(pdfs) == 1\n  pdf_url = pdfs[0] + '.pdf'\n  basename = pdf_url.split('/')[-1]\n  fname = os.path.join(Config.pdf_dir, basename)\n\n  # try retrieve the pdf\n  numtot += 1\n  try:\n    if not basename in have:\n      print('fetching %s into %s' % (pdf_url, fname))\n      req = urlopen(pdf_url, None, timeout_secs)\n      with open(fname, 'wb') as fp:\n          shutil.copyfileobj(req, fp)\n      time.sleep(0.05 + random.uniform(0,0.1))\n    else:\n      print('%s exists, skipping' % (fname, ))\n    numok+=1\n  except Exception as e:\n    print('error downloading: ', pdf_url)\n    print(e)\n  \n  print('%d/%d of %d downloaded ok.' % (numok, numtot, len(db)))\n  \nprint('final number of papers downloaded okay: %d/%d' % (numok, len(db)))\n\n"
        },
        {
          "name": "fetch_papers.py",
          "type": "blob",
          "size": 4.484375,
          "content": "\"\"\"\nQueries arxiv API and downloads papers (the query is a parameter).\nThe script is intended to enrich an existing database pickle (by default db.p),\nso this file will be loaded first, and then new results will be added to it.\n\"\"\"\n\nimport os\nimport time\nimport pickle\nimport random\nimport argparse\nimport urllib.request\nimport feedparser\n\nfrom utils import Config, safe_pickle_dump\n\ndef encode_feedparser_dict(d):\n  \"\"\" \n  helper function to get rid of feedparser bs with a deep copy. \n  I hate when libs wrap simple things in their own classes.\n  \"\"\"\n  if isinstance(d, feedparser.FeedParserDict) or isinstance(d, dict):\n    j = {}\n    for k in d.keys():\n      j[k] = encode_feedparser_dict(d[k])\n    return j\n  elif isinstance(d, list):\n    l = []\n    for k in d:\n      l.append(encode_feedparser_dict(k))\n    return l\n  else:\n    return d\n\ndef parse_arxiv_url(url):\n  \"\"\" \n  examples is http://arxiv.org/abs/1512.08756v2\n  we want to extract the raw id and the version\n  \"\"\"\n  ix = url.rfind('/')\n  idversion = url[ix+1:] # extract just the id (and the version)\n  parts = idversion.split('v')\n  assert len(parts) == 2, 'error parsing url ' + url\n  return parts[0], int(parts[1])\n\nif __name__ == \"__main__\":\n\n  # parse input arguments\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--search-query', type=str,\n                      default='cat:cs.CV+OR+cat:cs.AI+OR+cat:cs.LG+OR+cat:cs.CL+OR+cat:cs.NE+OR+cat:stat.ML',\n                      help='query used for arxiv API. See http://arxiv.org/help/api/user-manual#detailed_examples')\n  parser.add_argument('--start-index', type=int, default=0, help='0 = most recent API result')\n  parser.add_argument('--max-index', type=int, default=10000, help='upper bound on paper index we will fetch')\n  parser.add_argument('--results-per-iteration', type=int, default=100, help='passed to arxiv API')\n  parser.add_argument('--wait-time', type=float, default=5.0, help='lets be gentle to arxiv API (in number of seconds)')\n  parser.add_argument('--break-on-no-added', type=int, default=1, help='break out early if all returned query papers are already in db? 1=yes, 0=no')\n  args = parser.parse_args()\n\n  # misc hardcoded variables\n  base_url = 'http://export.arxiv.org/api/query?' # base api query url\n  print('Searching arXiv for %s' % (args.search_query, ))\n\n  # lets load the existing database to memory\n  try:\n    db = pickle.load(open(Config.db_path, 'rb'))\n  except Exception as e:\n    print('error loading existing database:')\n    print(e)\n    print('starting from an empty database')\n    db = {}\n\n  # -----------------------------------------------------------------------------\n  # main loop where we fetch the new results\n  print('database has %d entries at start' % (len(db), ))\n  num_added_total = 0\n  for i in range(args.start_index, args.max_index, args.results_per_iteration):\n\n    print(\"Results %i - %i\" % (i,i+args.results_per_iteration))\n    query = 'search_query=%s&sortBy=lastUpdatedDate&start=%i&max_results=%i' % (args.search_query,\n                                                         i, args.results_per_iteration)\n    with urllib.request.urlopen(base_url+query) as url:\n      response = url.read()\n    parse = feedparser.parse(response)\n    num_added = 0\n    num_skipped = 0\n    for e in parse.entries:\n\n      j = encode_feedparser_dict(e)\n\n      # extract just the raw arxiv id and version for this paper\n      rawid, version = parse_arxiv_url(j['id'])\n      j['_rawid'] = rawid\n      j['_version'] = version\n\n      # add to our database if we didn't have it before, or if this is a new version\n      if not rawid in db or j['_version'] > db[rawid]['_version']:\n        db[rawid] = j\n        print('Updated %s added %s' % (j['updated'].encode('utf-8'), j['title'].encode('utf-8')))\n        num_added += 1\n        num_added_total += 1\n      else:\n        num_skipped += 1\n\n    # print some information\n    print('Added %d papers, already had %d.' % (num_added, num_skipped))\n\n    if len(parse.entries) == 0:\n      print('Received no results from arxiv. Rate limiting? Exiting. Restart later maybe.')\n      print(response)\n      break\n\n    if num_added == 0 and args.break_on_no_added == 1:\n      print('No new papers were added. Assuming no new papers exist. Exiting.')\n      break\n\n    print('Sleeping for %i seconds' % (args.wait_time , ))\n    time.sleep(args.wait_time + random.uniform(0, 3))\n\n  # save the database before we quit, if we found anything new\n  if num_added_total > 0:\n    print('Saving database with %d papers to %s' % (len(db), Config.db_path))\n    safe_pickle_dump(db, Config.db_path)\n\n"
        },
        {
          "name": "make_cache.py",
          "type": "blob",
          "size": 3.48828125,
          "content": "\"\"\"\ncomputes various cache things on top of db.py so that the server\n(running from serve.py) can start up and serve faster when restarted.\n\nthis script should be run whenever db.p is updated, and \ncreates db2.p, which can be read by the server.\n\"\"\"\n\nimport os\nimport json\nimport time\nimport pickle\nimport dateutil.parser\n\nfrom sqlite3 import dbapi2 as sqlite3\nfrom utils import safe_pickle_dump, Config\n\nsqldb = sqlite3.connect(Config.database_path)\nsqldb.row_factory = sqlite3.Row # to return dicts rather than tuples\n\nCACHE = {}\n\nprint('loading the paper database', Config.db_path)\ndb = pickle.load(open(Config.db_path, 'rb'))\n\nprint('loading tfidf_meta', Config.meta_path)\nmeta = pickle.load(open(Config.meta_path, \"rb\"))\nvocab = meta['vocab']\nidf = meta['idf']\n\nprint('decorating the database with additional information...')\nfor pid,p in db.items():\n  timestruct = dateutil.parser.parse(p['updated'])\n  p['time_updated'] = int(timestruct.strftime(\"%s\")) # store in struct for future convenience\n  timestruct = dateutil.parser.parse(p['published'])\n  p['time_published'] = int(timestruct.strftime(\"%s\")) # store in struct for future convenience\n\nprint('computing min/max time for all papers...')\ntts = [time.mktime(dateutil.parser.parse(p['updated']).timetuple()) for pid,p in db.items()]\nttmin = min(tts)*1.0\nttmax = max(tts)*1.0\nfor pid,p in db.items():\n  tt = time.mktime(dateutil.parser.parse(p['updated']).timetuple())\n  p['tscore'] = (tt-ttmin)/(ttmax-ttmin)\n\nprint('precomputing papers date sorted...')\nscores = [(p['time_updated'], pid) for pid,p in db.items()]\nscores.sort(reverse=True, key=lambda x: x[0])\nCACHE['date_sorted_pids'] = [sp[1] for sp in scores]\n\n# compute top papers in peoples' libraries\nprint('computing top papers...')\nlibs = sqldb.execute('''select * from library''').fetchall()\ncounts = {}\nfor lib in libs:\n  pid = lib['paper_id']\n  counts[pid] = counts.get(pid, 0) + 1\ntop_paper_counts = sorted([(v,k) for k,v in counts.items() if v > 0], reverse=True)\nCACHE['top_sorted_pids'] = [q[1] for q in top_paper_counts]\n\n# some utilities for creating a search index for faster search\npunc = \"'!\\\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'\" # removed hyphen from string.punctuation\ntrans_table = {ord(c): None for c in punc}\ndef makedict(s, forceidf=None, scale=1.0):\n  words = set(s.lower().translate(trans_table).strip().split())\n  idfd = {}\n  for w in words: # todo: if we're using bigrams in vocab then this won't search over them\n    if forceidf is None:\n      if w in vocab:\n        # we have idf for this\n        idfval = idf[vocab[w]]*scale\n      else:\n        idfval = 1.0*scale # assume idf 1.0 (low)\n    else:\n      idfval = forceidf\n    idfd[w] = idfval\n  return idfd\n\ndef merge_dicts(dlist):\n  m = {}\n  for d in dlist:\n    for k,v in d.items():\n      m[k] = m.get(k,0) + v\n  return m\n\nprint('building an index for faster search...')\nsearch_dict = {}\nfor pid,p in db.items():\n  dict_title = makedict(p['title'], forceidf=5, scale=3)\n  dict_authors = makedict(' '.join(x['name'] for x in p['authors']), forceidf=5)\n  dict_categories = {x['term'].lower():5 for x in p['tags']}\n  if 'and' in dict_authors: \n    # special case for \"and\" handling in authors list\n    del dict_authors['and']\n  dict_summary = makedict(p['summary'])\n  search_dict[pid] = merge_dicts([dict_title, dict_authors, dict_categories, dict_summary])\nCACHE['search_dict'] = search_dict\n\n# save the cache\nprint('writing', Config.serve_cache_path)\nsafe_pickle_dump(CACHE, Config.serve_cache_path)\nprint('writing', Config.db_serve_path)\nsafe_pickle_dump(db, Config.db_serve_path)\n"
        },
        {
          "name": "parse_pdf_to_text.py",
          "type": "blob",
          "size": 1.57421875,
          "content": "\"\"\"\nVery simple script that simply iterates over all files data/pdf/f.pdf\nand create a file data/txt/f.pdf.txt that contains the raw text, extracted\nusing the \"pdftotext\" command. If a pdf cannot be converted, this\nscript will not produce the output file.\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport shutil\nimport pickle\n\nfrom utils import Config\n\n# make sure pdftotext is installed\nif not shutil.which('pdftotext'): # needs Python 3.3+\n  print('ERROR: you don\\'t have pdftotext installed. Install it first before calling this script')\n  sys.exit()\n\nif not os.path.exists(Config.txt_dir):\n  print('creating ', Config.txt_dir)\n  os.makedirs(Config.txt_dir)\n\nhave = set(os.listdir(Config.txt_dir))\nfiles = os.listdir(Config.pdf_dir)\nfor i,f in enumerate(files): # there was a ,start=1 here that I removed, can't remember why it would be there. shouldn't be, i think.\n\n  txt_basename = f + '.txt'\n  if txt_basename in have:\n    print('%d/%d skipping %s, already exists.' % (i, len(files), txt_basename, ))\n    continue\n\n  pdf_path = os.path.join(Config.pdf_dir, f)\n  txt_path = os.path.join(Config.txt_dir, txt_basename)\n  cmd = \"pdftotext %s %s\" % (pdf_path, txt_path)\n  os.system(cmd)\n\n  print('%d/%d %s' % (i, len(files), cmd))\n\n  # check output was made\n  if not os.path.isfile(txt_path):\n    # there was an error with converting the pdf\n    print('there was a problem with parsing %s to text, creating an empty text file.' % (pdf_path, ))\n    os.system('touch ' + txt_path) # create empty file, but it's a record of having tried to convert\n\n  time.sleep(0.01) # silly way for allowing for ctrl+c termination\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2421875,
          "content": "# feedparser is only if you want to scrape arxiv\nfeedparser\nnumpy\nscipy\n\n# sciki-learn is only for sparse arrays\nscikit-learn\n\n# twitter\npython-twitter\npytz\n\n# the following are only for serve.py\npython-dateutil\nflask\nflask_limiter\ntornado\npymongo\n"
        },
        {
          "name": "schema.sql",
          "type": "blob",
          "size": 0.337890625,
          "content": "drop table if exists user;\ncreate table user (\n  user_id integer primary key autoincrement,\n  username text not null,\n  pw_hash text not null,\n  creation_time integer\n);\ndrop table if exists library;\ncreate table library (\n  lib_id integer primary key autoincrement,\n  paper_id text not null,\n  user_id integer not null,\n  update_time integer\n);\n"
        },
        {
          "name": "serve.py",
          "type": "blob",
          "size": 25.8642578125,
          "content": "import os\nimport json\nimport time\nimport pickle\nimport argparse\nimport dateutil.parser\nfrom random import shuffle, randrange, uniform\n\nimport numpy as np\nfrom sqlite3 import dbapi2 as sqlite3\nfrom hashlib import md5\nfrom flask import Flask, request, session, url_for, redirect, \\\n     render_template, abort, g, flash, _app_ctx_stack\nfrom flask_limiter import Limiter\nfrom werkzeug import check_password_hash, generate_password_hash\nimport pymongo\n\nfrom utils import safe_pickle_dump, strip_version, isvalidid, Config\n\n# various globals\n# -----------------------------------------------------------------------------\n\n# database configuration\nif os.path.isfile('secret_key.txt'):\n  SECRET_KEY = open('secret_key.txt', 'r').read()\nelse:\n  SECRET_KEY = 'devkey, should be in a file'\napp = Flask(__name__)\napp.config.from_object(__name__)\nlimiter = Limiter(app, global_limits=[\"100 per hour\", \"20 per minute\"])\n\n# -----------------------------------------------------------------------------\n# utilities for database interactions \n# -----------------------------------------------------------------------------\n# to initialize the database: sqlite3 as.db < schema.sql\ndef connect_db():\n  sqlite_db = sqlite3.connect(Config.database_path)\n  sqlite_db.row_factory = sqlite3.Row # to return dicts rather than tuples\n  return sqlite_db\n\ndef query_db(query, args=(), one=False):\n  \"\"\"Queries the database and returns a list of dictionaries.\"\"\"\n  cur = g.db.execute(query, args)\n  rv = cur.fetchall()\n  return (rv[0] if rv else None) if one else rv\n\ndef get_user_id(username):\n  \"\"\"Convenience method to look up the id for a username.\"\"\"\n  rv = query_db('select user_id from user where username = ?',\n                [username], one=True)\n  return rv[0] if rv else None\n\ndef get_username(user_id):\n  \"\"\"Convenience method to look up the username for a user.\"\"\"\n  rv = query_db('select username from user where user_id = ?',\n                [user_id], one=True)\n  return rv[0] if rv else None\n\n# -----------------------------------------------------------------------------\n# connection handlers\n# -----------------------------------------------------------------------------\n\n@app.before_request\ndef before_request():\n  # this will always request database connection, even if we dont end up using it ;\\\n  g.db = connect_db()\n  # retrieve user object from the database if user_id is set\n  g.user = None\n  if 'user_id' in session:\n    g.user = query_db('select * from user where user_id = ?',\n                      [session['user_id']], one=True)\n\n@app.teardown_request\ndef teardown_request(exception):\n  db = getattr(g, 'db', None)\n  if db is not None:\n    db.close()\n\n# -----------------------------------------------------------------------------\n# search/sort functionality\n# -----------------------------------------------------------------------------\n\ndef papers_search(qraw):\n  qparts = qraw.lower().strip().split() # split by spaces\n  # use reverse index and accumulate scores\n  scores = []\n  for pid,p in db.items():\n    score = sum(SEARCH_DICT[pid].get(q,0) for q in qparts)\n    if score == 0:\n      continue # no match whatsoever, dont include\n    # give a small boost to more recent papers\n    score += 0.0001*p['tscore']\n    scores.append((score, p))\n  scores.sort(reverse=True, key=lambda x: x[0]) # descending\n  out = [x[1] for x in scores if x[0] > 0]\n  return out\n\ndef papers_similar(pid):\n  rawpid = strip_version(pid)\n\n  # check if we have this paper at all, otherwise return empty list\n  if not rawpid in db: \n    return []\n\n  # check if we have distances to this specific version of paper id (includes version)\n  if pid in sim_dict:\n    # good, simplest case: lets return the papers\n    return [db[strip_version(k)] for k in sim_dict[pid]]\n  else:\n    # ok we don't have this specific version. could be a stale URL that points to, \n    # e.g. v1 of a paper, but due to an updated version of it we only have v2 on file\n    # now. We want to use v2 in that case.\n    # lets try to retrieve the most recent version of this paper we do have\n    kok = [k for k in sim_dict if rawpid in k]\n    if kok:\n      # ok we have at least one different version of this paper, lets use it instead\n      id_use_instead = kok[0]\n      return [db[strip_version(k)] for k in sim_dict[id_use_instead]]\n    else:\n      # return just the paper. we dont have similarities for it for some reason\n      return [db[rawpid]]\n\ndef papers_from_library():\n  out = []\n  if g.user:\n    # user is logged in, lets fetch their saved library data\n    uid = session['user_id']\n    user_library = query_db('''select * from library where user_id = ?''', [uid])\n    libids = [strip_version(x['paper_id']) for x in user_library]\n    out = [db[x] for x in libids]\n    out = sorted(out, key=lambda k: k['updated'], reverse=True)\n  return out\n\ndef papers_from_svm(recent_days=None):\n  out = []\n  if g.user:\n\n    uid = session['user_id']\n    if not uid in user_sim:\n      return []\n    \n    # we want to exclude papers that are already in user library from the result, so fetch them.\n    user_library = query_db('''select * from library where user_id = ?''', [uid])\n    libids = {strip_version(x['paper_id']) for x in user_library}\n\n    plist = user_sim[uid]\n    out = [db[x] for x in plist if not x in libids]\n\n    if recent_days is not None:\n      # filter as well to only most recent papers\n      curtime = int(time.time()) # in seconds\n      out = [x for x in out if curtime - x['time_published'] < recent_days*24*60*60]\n\n  return out\n\ndef papers_filter_version(papers, v):\n  if v != '1': \n    return papers # noop\n  intv = int(v)\n  filtered = [p for p in papers if p['_version'] == intv]\n  return filtered\n\ndef encode_json(ps, n=10, send_images=True, send_abstracts=True):\n\n  libids = set()\n  if g.user:\n    # user is logged in, lets fetch their saved library data\n    uid = session['user_id']\n    user_library = query_db('''select * from library where user_id = ?''', [uid])\n    libids = {strip_version(x['paper_id']) for x in user_library}\n\n  ret = []\n  for i in range(min(len(ps),n)):\n    p = ps[i]\n    idvv = '%sv%d' % (p['_rawid'], p['_version'])\n    struct = {}\n    struct['title'] = p['title']\n    struct['pid'] = idvv\n    struct['rawpid'] = p['_rawid']\n    struct['category'] = p['arxiv_primary_category']['term']\n    struct['authors'] = [a['name'] for a in p['authors']]\n    struct['link'] = p['link']\n    struct['in_library'] = 1 if p['_rawid'] in libids else 0\n    if send_abstracts:\n      struct['abstract'] = p['summary']\n    if send_images:\n      struct['img'] = '/static/thumbs/' + idvv + '.pdf.jpg'\n    struct['tags'] = [t['term'] for t in p['tags']]\n    \n    # render time information nicely\n    timestruct = dateutil.parser.parse(p['updated'])\n    struct['published_time'] = '%s/%s/%s' % (timestruct.month, timestruct.day, timestruct.year)\n    timestruct = dateutil.parser.parse(p['published'])\n    struct['originally_published_time'] = '%s/%s/%s' % (timestruct.month, timestruct.day, timestruct.year)\n\n    # fetch amount of discussion on this paper\n    struct['num_discussion'] = comments.count({ 'pid': p['_rawid'] })\n\n    # arxiv comments from the authors (when they submit the paper)\n    cc = p.get('arxiv_comment', '')\n    if len(cc) > 100:\n      cc = cc[:100] + '...' # crop very long comments\n    struct['comment'] = cc\n\n    ret.append(struct)\n  return ret\n\n# -----------------------------------------------------------------------------\n# flask request handling\n# -----------------------------------------------------------------------------\n\ndef default_context(papers, **kws):\n  top_papers = encode_json(papers, args.num_results)\n\n  # prompt logic\n  show_prompt = 'no'\n  try:\n    if Config.beg_for_hosting_money and g.user and uniform(0,1) < 0.05:\n      uid = session['user_id']\n      entry = goaway_collection.find_one({ 'uid':uid })\n      if not entry:\n        lib_count = query_db('''select count(*) from library where user_id = ?''', [uid], one=True)\n        lib_count = lib_count['count(*)']\n        if lib_count > 0: # user has some items in their library too\n          show_prompt = 'yes'\n  except Exception as e:\n    print(e)\n\n  ans = dict(papers=top_papers, numresults=len(papers), totpapers=len(db), tweets=[], msg='', show_prompt=show_prompt, pid_to_users={})\n  ans.update(kws)\n  return ans\n\n@app.route('/goaway', methods=['POST'])\ndef goaway():\n  if not g.user: return # weird\n  uid = session['user_id']\n  entry = goaway_collection.find_one({ 'uid':uid })\n  if not entry: # ok record this user wanting it to stop\n    username = get_username(session['user_id'])\n    print('adding', uid, username, 'to goaway.')\n    goaway_collection.insert_one({ 'uid':uid, 'time':int(time.time()) })\n  return 'OK'\n\n@app.route(\"/\")\ndef intmain():\n  vstr = request.args.get('vfilter', 'all')\n  papers = [db[pid] for pid in DATE_SORTED_PIDS] # precomputed\n  papers = papers_filter_version(papers, vstr)\n  ctx = default_context(papers, render_format='recent',\n                        msg='Showing most recent Arxiv papers:')\n  return render_template('main.html', **ctx)\n\n@app.route(\"/<request_pid>\")\ndef rank(request_pid=None):\n  if not isvalidid(request_pid):\n    return '' # these are requests for icons, things like robots.txt, etc\n  papers = papers_similar(request_pid)\n  ctx = default_context(papers, render_format='paper')\n  return render_template('main.html', **ctx)\n\n@app.route('/discuss', methods=['GET'])\ndef discuss():\n  \"\"\" return discussion related to a paper \"\"\"\n  pid = request.args.get('id', '') # paper id of paper we wish to discuss\n  papers = [db[pid]] if pid in db else []\n\n  # fetch the comments\n  comms_cursor = comments.find({ 'pid':pid }).sort([('time_posted', pymongo.DESCENDING)])\n  comms = list(comms_cursor)\n  for c in comms:\n    c['_id'] = str(c['_id']) # have to convert these to strs from ObjectId, and backwards later http://api.mongodb.com/python/current/tutorial.html\n\n  # fetch the counts for all tags\n  tag_counts = []\n  for c in comms:\n    cc = [tags_collection.count({ 'comment_id':c['_id'], 'tag_name':t }) for t in TAGS]\n    tag_counts.append(cc);\n\n  # and render\n  ctx = default_context(papers, render_format='default', comments=comms, gpid=pid, tags=TAGS, tag_counts=tag_counts)\n  return render_template('discuss.html', **ctx)\n\n@app.route('/comment', methods=['POST'])\ndef comment():\n  \"\"\" user wants to post a comment \"\"\"\n  anon = int(request.form['anon'])\n\n  if g.user and (not anon):\n    username = get_username(session['user_id'])\n  else:\n    # generate a unique username if user wants to be anon, or user not logged in.\n    username = 'anon-%s-%s' % (str(int(time.time())), str(randrange(1000)))\n\n  # process the raw pid and validate it, etc\n  try:\n    pid = request.form['pid']\n    if not pid in db: raise Exception(\"invalid pid\")\n    version = db[pid]['_version'] # most recent version of this paper\n  except Exception as e:\n    print(e)\n    return 'bad pid. This is most likely Andrej\\'s fault.'\n\n  # create the entry\n  entry = {\n    'user': username,\n    'pid': pid, # raw pid with no version, for search convenience\n    'version': version, # version as int, again as convenience\n    'conf': request.form['conf'],\n    'anon': anon,\n    'time_posted': time.time(),\n    'text': request.form['text'],\n  }\n\n  # enter into database\n  print(entry)\n  comments.insert_one(entry)\n  return 'OK'\n\n@app.route(\"/discussions\", methods=['GET'])\ndef discussions():\n  # return most recently discussed papers\n  comms_cursor = comments.find().sort([('time_posted', pymongo.DESCENDING)]).limit(100)\n\n  # get the (unique) set of papers.\n  papers = []\n  have = set()\n  for e in comms_cursor:\n    pid = e['pid']\n    if pid in db and not pid in have:\n      have.add(pid)\n      papers.append(db[pid])\n\n  ctx = default_context(papers, render_format=\"discussions\")\n  return render_template('main.html', **ctx)\n\n@app.route('/toggletag', methods=['POST'])\ndef toggletag():\n\n  if not g.user: \n    return 'You have to be logged in to tag. Sorry - otherwise things could get out of hand FAST.'\n\n  # get the tag and validate it as an allowed tag\n  tag_name = request.form['tag_name']\n  if not tag_name in TAGS:\n    print('tag name %s is not in allowed tags.' % (tag_name, ))\n    return \"Bad tag name. This is most likely Andrej's fault.\"\n\n  pid = request.form['pid']\n  comment_id = request.form['comment_id']\n  username = get_username(session['user_id'])\n  time_toggled = time.time()\n  entry = {\n    'username': username,\n    'pid': pid,\n    'comment_id': comment_id,\n    'tag_name': tag_name,\n    'time': time_toggled,\n  }\n\n  # remove any existing entries for this user/comment/tag\n  result = tags_collection.delete_one({ 'username':username, 'comment_id':comment_id, 'tag_name':tag_name })\n  if result.deleted_count > 0:\n    print('cleared an existing entry from database')\n  else:\n    print('no entry existed, so this is a toggle ON. inserting:')\n    print(entry)\n    tags_collection.insert_one(entry)\n\n  return 'OK'\n\n@app.route(\"/search\", methods=['GET'])\ndef search():\n  q = request.args.get('q', '') # get the search request\n  papers = papers_search(q) # perform the query and get sorted documents\n  ctx = default_context(papers, render_format=\"search\")\n  return render_template('main.html', **ctx)\n\n@app.route('/recommend', methods=['GET'])\ndef recommend():\n  \"\"\" return user's svm sorted list \"\"\"\n  ttstr = request.args.get('timefilter', 'week') # default is week\n  vstr = request.args.get('vfilter', 'all') # default is all (no filter)\n  legend = {'day':1, '3days':3, 'week':7, 'month':30, 'year':365}\n  tt = legend.get(ttstr, None)\n  papers = papers_from_svm(recent_days=tt)\n  papers = papers_filter_version(papers, vstr)\n  ctx = default_context(papers, render_format='recommend',\n                        msg='Recommended papers: (based on SVM trained on tfidf of papers in your library, refreshed every day or so)' if g.user else 'You must be logged in and have some papers saved in your library.')\n  return render_template('main.html', **ctx)\n\n@app.route('/top', methods=['GET'])\ndef top():\n  \"\"\" return top papers \"\"\"\n  ttstr = request.args.get('timefilter', 'week') # default is week\n  vstr = request.args.get('vfilter', 'all') # default is all (no filter)\n  legend = {'day':1, '3days':3, 'week':7, 'month':30, 'year':365, 'alltime':10000}\n  tt = legend.get(ttstr, 7)\n  curtime = int(time.time()) # in seconds\n  top_sorted_papers = [db[p] for p in TOP_SORTED_PIDS]\n  papers = [p for p in top_sorted_papers if curtime - p['time_published'] < tt*24*60*60]\n  papers = papers_filter_version(papers, vstr)\n  ctx = default_context(papers, render_format='top',\n                        msg='Top papers based on people\\'s libraries:')\n  return render_template('main.html', **ctx)\n\n@app.route('/toptwtr', methods=['GET'])\ndef toptwtr():\n  \"\"\" return top papers \"\"\"\n  ttstr = request.args.get('timefilter', 'day') # default is day\n  tweets_top = {'day':tweets_top1, 'week':tweets_top7, 'month':tweets_top30}[ttstr]\n  cursor = tweets_top.find().sort([('vote', pymongo.DESCENDING)]).limit(100)\n  papers, tweets = [], []\n  for rec in cursor:\n    if rec['pid'] in db:\n      papers.append(db[rec['pid']])\n      tweet = {k:v for k,v in rec.items() if k != '_id'}\n      tweets.append(tweet)\n  ctx = default_context(papers, render_format='toptwtr', tweets=tweets,\n                        msg='Top papers mentioned on Twitter over last ' + ttstr + ':')\n  return render_template('main.html', **ctx)\n\n@app.route('/library')\ndef library():\n  \"\"\" render user's library \"\"\"\n  papers = papers_from_library()\n  ret = encode_json(papers, 500) # cap at 500 papers in someone's library. that's a lot!\n  if g.user:\n    msg = '%d papers in your library:' % (len(ret), )\n  else:\n    msg = 'You must be logged in. Once you are, you can save papers to your library (with the save icon on the right of each paper) and they will show up here.'\n  ctx = default_context(papers, render_format='library', msg=msg)\n  return render_template('main.html', **ctx)\n\n@app.route('/libtoggle', methods=['POST'])\ndef review():\n  \"\"\" user wants to toggle a paper in his library \"\"\"\n  \n  # make sure user is logged in\n  if not g.user:\n    return 'NO' # fail... (not logged in). JS should prevent from us getting here.\n\n  idvv = request.form['pid'] # includes version\n  if not isvalidid(idvv):\n    return 'NO' # fail, malformed id. weird.\n  pid = strip_version(idvv)\n  if not pid in db:\n    return 'NO' # we don't know this paper. wat\n\n  uid = session['user_id'] # id of logged in user\n\n  # check this user already has this paper in library\n  record = query_db('''select * from library where\n          user_id = ? and paper_id = ?''', [uid, pid], one=True)\n  print(record)\n\n  ret = 'NO'\n  if record:\n    # record exists, erase it.\n    g.db.execute('''delete from library where user_id = ? and paper_id = ?''', [uid, pid])\n    g.db.commit()\n    #print('removed %s for %s' % (pid, uid))\n    ret = 'OFF'\n  else:\n    # record does not exist, add it.\n    rawpid = strip_version(pid)\n    g.db.execute('''insert into library (paper_id, user_id, update_time) values (?, ?, ?)''',\n        [rawpid, uid, int(time.time())])\n    g.db.commit()\n    #print('added %s for %s' % (pid, uid))\n    ret = 'ON'\n\n  return ret\n\n@app.route('/friends', methods=['GET'])\ndef friends():\n    \n    ttstr = request.args.get('timefilter', 'week') # default is week\n    legend = {'day':1, '3days':3, 'week':7, 'month':30, 'year':365}\n    tt = legend.get(ttstr, 7)\n\n    papers = []\n    pid_to_users = {}\n    if g.user:\n        # gather all the people we are following\n        username = get_username(session['user_id'])\n        edges = list(follow_collection.find({ 'who':username }))\n        # fetch all papers in all of their libraries, and count the top ones\n        counts = {}\n        for edict in edges:\n            whom = edict['whom']\n            uid = get_user_id(whom)\n            user_library = query_db('''select * from library where user_id = ?''', [uid])\n            libids = [strip_version(x['paper_id']) for x in user_library]\n            for lid in libids:\n                if not lid in counts:\n                    counts[lid] = []\n                counts[lid].append(whom)\n\n        keys = list(counts.keys())\n        keys.sort(key=lambda k: len(counts[k]), reverse=True) # descending by count\n        papers = [db[x] for x in keys]\n        # finally filter by date\n        curtime = int(time.time()) # in seconds\n        papers = [x for x in papers if curtime - x['time_published'] < tt*24*60*60]\n        # trim at like 100\n        if len(papers) > 100: papers = papers[:100]\n        # trim counts as well correspondingly\n        pid_to_users = { p['_rawid'] : counts.get(p['_rawid'], []) for p in papers }\n\n    if not g.user:\n        msg = \"You must be logged in and follow some people to enjoy this tab.\"\n    else:\n        if len(papers) == 0:\n            msg = \"No friend papers present. Try to extend the time range, or add friends by clicking on your account name (top, right)\"\n        else:\n            msg = \"Papers in your friend's libraries:\"\n\n    ctx = default_context(papers, render_format='friends', pid_to_users=pid_to_users, msg=msg)\n    return render_template('main.html', **ctx)\n\n@app.route('/account')\ndef account():\n    ctx = { 'totpapers':len(db) }\n\n    followers = []\n    following = []\n    # fetch all followers/following of the logged in user\n    if g.user:\n        username = get_username(session['user_id'])\n        \n        following_db = list(follow_collection.find({ 'who':username }))\n        for e in following_db:\n            following.append({ 'user':e['whom'], 'active':e['active'] })\n\n        followers_db = list(follow_collection.find({ 'whom':username }))\n        for e in followers_db:\n            followers.append({ 'user':e['who'], 'active':e['active'] })\n\n    ctx['followers'] = followers\n    ctx['following'] = following\n    return render_template('account.html', **ctx)\n\n@app.route('/requestfollow', methods=['POST'])\ndef requestfollow():\n    if request.form['newf'] and g.user:\n        # add an entry: this user is requesting to follow a second user\n        who = get_username(session['user_id'])\n        whom = request.form['newf']\n        # make sure whom exists in our database\n        whom_id = get_user_id(whom)\n        if whom_id is not None:\n            e = { 'who':who, 'whom':whom, 'active':0, 'time_request':int(time.time()) }\n            print('adding request follow:')\n            print(e)\n            follow_collection.insert_one(e)\n\n    return redirect(url_for('account'))\n\n@app.route('/removefollow', methods=['POST'])\ndef removefollow():\n    user = request.form['user']\n    lst = request.form['lst']\n    if user and lst:\n        username = get_username(session['user_id'])\n        if lst == 'followers':\n            # user clicked \"X\" in their followers list. Erase the follower of this user\n            who = user\n            whom = username\n        elif lst == 'following':\n            # user clicked \"X\" in their following list. Stop following this user.\n            who = username\n            whom = user\n        else:\n            return 'NOTOK'\n\n        delq = { 'who':who, 'whom':whom }\n        print('deleting from follow collection:', delq)\n        follow_collection.delete_one(delq)\n        return 'OK'\n    else:\n        return 'NOTOK'\n\n@app.route('/addfollow', methods=['POST'])\ndef addfollow():\n    user = request.form['user']\n    lst = request.form['lst']\n    if user and lst:\n        username = get_username(session['user_id'])\n        if lst == 'followers':\n            # user clicked \"OK\" in the followers list, wants to approve some follower. make active.\n            who = user\n            whom = username\n            delq = { 'who':who, 'whom':whom }\n            print('making active in follow collection:', delq)\n            follow_collection.update_one(delq, {'$set':{'active':1}})\n            return 'OK'\n        \n    return 'NOTOK'\n\n@app.route('/login', methods=['POST'])\ndef login():\n  \"\"\" logs in the user. if the username doesn't exist creates the account \"\"\"\n  \n  if not request.form['username']:\n    flash('You have to enter a username')\n  elif not request.form['password']:\n    flash('You have to enter a password')\n  elif get_user_id(request.form['username']) is not None:\n    # username already exists, fetch all of its attributes\n    user = query_db('''select * from user where\n          username = ?''', [request.form['username']], one=True)\n    if check_password_hash(user['pw_hash'], request.form['password']):\n      # password is correct, log in the user\n      session['user_id'] = get_user_id(request.form['username'])\n      flash('User ' + request.form['username'] + ' logged in.')\n    else:\n      # incorrect password\n      flash('User ' + request.form['username'] + ' already exists, wrong password.')\n  else:\n    # create account and log in\n    creation_time = int(time.time())\n    g.db.execute('''insert into user (username, pw_hash, creation_time) values (?, ?, ?)''',\n      [request.form['username'], \n      generate_password_hash(request.form['password']), \n      creation_time])\n    user_id = g.db.execute('select last_insert_rowid()').fetchall()[0][0]\n    g.db.commit()\n\n    session['user_id'] = user_id\n    flash('New account %s created' % (request.form['username'], ))\n  \n  return redirect(url_for('intmain'))\n\n@app.route('/logout')\ndef logout():\n  session.pop('user_id', None)\n  flash('You were logged out')\n  return redirect(url_for('intmain'))\n\n# -----------------------------------------------------------------------------\n# int main\n# -----------------------------------------------------------------------------\nif __name__ == \"__main__\":\n   \n  parser = argparse.ArgumentParser()\n  parser.add_argument('-p', '--prod', dest='prod', action='store_true', help='run in prod?')\n  parser.add_argument('-r', '--num_results', dest='num_results', type=int, default=200, help='number of results to return per query')\n  parser.add_argument('--port', dest='port', type=int, default=5000, help='port to serve on')\n  args = parser.parse_args()\n  print(args)\n\n  if not os.path.isfile(Config.database_path):\n    print('did not find as.db, trying to create an empty database from schema.sql...')\n    print('this needs sqlite3 to be installed!')\n    os.system('sqlite3 as.db < schema.sql')\n\n  print('loading the paper database', Config.db_serve_path)\n  db = pickle.load(open(Config.db_serve_path, 'rb'))\n  \n  print('loading tfidf_meta', Config.meta_path)\n  meta = pickle.load(open(Config.meta_path, \"rb\"))\n  vocab = meta['vocab']\n  idf = meta['idf']\n\n  print('loading paper similarities', Config.sim_path)\n  sim_dict = pickle.load(open(Config.sim_path, \"rb\"))\n\n  print('loading user recommendations', Config.user_sim_path)\n  user_sim = {}\n  if os.path.isfile(Config.user_sim_path):\n    user_sim = pickle.load(open(Config.user_sim_path, 'rb'))\n  \n  print('loading serve cache...', Config.serve_cache_path)\n  cache = pickle.load(open(Config.serve_cache_path, \"rb\"))\n  DATE_SORTED_PIDS = cache['date_sorted_pids']\n  TOP_SORTED_PIDS = cache['top_sorted_pids']\n  SEARCH_DICT = cache['search_dict']\n\n  print('connecting to mongodb...')\n  client = pymongo.MongoClient()\n  mdb = client.arxiv\n  tweets_top1 = mdb.tweets_top1\n  tweets_top7 = mdb.tweets_top7\n  tweets_top30 = mdb.tweets_top30\n  comments = mdb.comments\n  tags_collection = mdb.tags\n  goaway_collection = mdb.goaway\n  follow_collection = mdb.follow\n  print('mongodb tweets_top1 collection size:', tweets_top1.count())\n  print('mongodb tweets_top7 collection size:', tweets_top7.count())\n  print('mongodb tweets_top30 collection size:', tweets_top30.count())\n  print('mongodb comments collection size:', comments.count())\n  print('mongodb tags collection size:', tags_collection.count())\n  print('mongodb goaway collection size:', goaway_collection.count())\n  print('mongodb follow collection size:', follow_collection.count())\n  \n  TAGS = ['insightful!', 'thank you', 'agree', 'disagree', 'not constructive', 'troll', 'spam']\n\n  # start\n  if args.prod:\n    # run on Tornado instead, since running raw Flask in prod is not recommended\n    print('starting tornado!')\n    from tornado.wsgi import WSGIContainer\n    from tornado.httpserver import HTTPServer\n    from tornado.ioloop import IOLoop\n    from tornado.log import enable_pretty_logging\n    enable_pretty_logging()\n    http_server = HTTPServer(WSGIContainer(app))\n    http_server.listen(args.port)\n    IOLoop.instance().start()\n  else:\n    print('starting flask!')\n    app.debug = False\n    app.run(port=args.port, host='0.0.0.0')\n"
        },
        {
          "name": "static",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "thumb_pdf.py",
          "type": "blob",
          "size": 3.71875,
          "content": "\"\"\"\nUse imagemagick to convert all pfds to a sequence of thumbnail images\nrequires: sudo apt-get install imagemagick\n\"\"\"\n\nimport os\nimport time\nimport shutil\nfrom subprocess import Popen\n\nfrom utils import Config\n\n# make sure imagemagick is installed\nif not shutil.which('convert'): # shutil.which needs Python 3.3+\n  print(\"ERROR: you don\\'t have imagemagick installed. Install it first before calling this script\")\n  sys.exit()\n\n# create if necessary the directories we're using for processing and output\npdf_dir = os.path.join('data', 'pdf')\nif not os.path.exists(Config.thumbs_dir): os.makedirs(Config.thumbs_dir)\nif not os.path.exists(Config.tmp_dir): os.makedirs(Config.tmp_dir)\n\n# fetch all pdf filenames in the pdf directory\nfiles_in_pdf_dir = os.listdir(pdf_dir)\npdf_files = [x for x in files_in_pdf_dir if x.endswith('.pdf')] # filter to just pdfs, just in case\n\n# iterate over all pdf files and create the thumbnails\nfor i,p in enumerate(pdf_files):\n  pdf_path = os.path.join(pdf_dir, p)\n  thumb_path = os.path.join(Config.thumbs_dir, p + '.jpg')\n\n  if os.path.isfile(thumb_path): \n    print(\"skipping %s, thumbnail already exists.\" % (pdf_path, ))\n    continue\n\n  print(\"%d/%d processing %s\" % (i, len(pdf_files), p))\n\n  # take first 8 pages of the pdf ([0-7]), since 9th page are references\n  # tile them horizontally, use JPEG compression 80, trim the borders for each image\n  #cmd = \"montage %s[0-7] -mode Concatenate -tile x1 -quality 80 -resize x230 -trim %s\" % (pdf_path, \"thumbs/\" + f + \".jpg\")\n  #print \"EXEC: \" + cmd\n  \n  # nvm, below using a roundabout alternative that is worse and requires temporary files, yuck!\n  # but i found that it succeeds more often. I can't remember wha thappened anymore but I remember\n  # that the version above, while more elegant, had some problem with it on some pdfs. I think.\n\n  # erase previous intermediate files thumb-*.png in the tmp directory\n  if os.path.isfile(os.path.join(Config.tmp_dir, 'thumb-0.png')):\n    for i in range(8):\n      f = os.path.join(Config.tmp_dir, 'thumb-%d.png' % (i,))\n      f2= os.path.join(Config.tmp_dir, 'thumbbuf-%d.png' % (i,))\n      if os.path.isfile(f):\n        cmd = 'mv %s %s' % (f, f2)\n        os.system(cmd)\n        # okay originally I was going to issue an rm call, but I am too terrified of\n        # running scripted rm queries, so what we will do is instead issue a \"mv\" call\n        # to rename the files. That's a bit safer, right? We have to do this because if\n        # some papers are shorter than 8 pages, then results from previous paper will\n        # \"leek\" over to this result, through the intermediate files.\n\n  # spawn async. convert can unfortunately enter an infinite loop, have to handle this.\n  # this command will generate 8 independent images thumb-0.png ... thumb-7.png of the thumbnails\n  pp = Popen(['convert', '%s[0-7]' % (pdf_path, ), '-thumbnail', 'x156', os.path.join(Config.tmp_dir, 'thumb.png')])\n  t0 = time.time()\n  while time.time() - t0 < 20: # give it 15 seconds deadline\n    ret = pp.poll()\n    if not (ret is None):\n      # process terminated\n      break\n    time.sleep(0.1)\n  ret = pp.poll()\n  if ret is None:\n    print(\"convert command did not terminate in 20 seconds, terminating.\")\n    pp.terminate() # give up\n\n  if not os.path.isfile(os.path.join(Config.tmp_dir, 'thumb-0.png')):\n    # failed to render pdf, replace with missing image\n    missing_thumb_path = os.path.join('static', 'missing.jpg')\n    os.system('cp %s %s' % (missing_thumb_path, thumb_path))\n    print(\"could not render pdf, creating a missing image placeholder\")\n  else:\n    cmd = \"montage -mode concatenate -quality 80 -tile x1 %s %s\" % (os.path.join(Config.tmp_dir, 'thumb-*.png'), thumb_path)\n    print(cmd)\n    os.system(cmd)\n\n  time.sleep(0.01) # silly way for allowing for ctrl+c termination\n"
        },
        {
          "name": "twitter_daemon.py",
          "type": "blob",
          "size": 7.27734375,
          "content": "\"\"\"\nPeriodically checks Twitter for tweets about arxiv papers we recognize\nand logs the tweets into mongodb database \"arxiv\", under \"tweets\" collection.\n\"\"\"\n\nimport os\nimport re\nimport pytz\nimport time\nimport math\nimport pickle\nimport datetime\n\nfrom dateutil import parser\nimport twitter # pip install python-twitter\nimport pymongo\n\nfrom utils import Config\n\n# settings\n# -----------------------------------------------------------------------------\nsleep_time = 60*10 # in seconds, between twitter API calls. Default rate limit is 180 per 15 minutes\nmax_tweet_records = 15\n\n# convenience functions\n# -----------------------------------------------------------------------------\ndef get_keys():\n  lines = open('twitter.txt', 'r').read().splitlines()\n  return lines\n\ndef extract_arxiv_pids(r):\n  pids = []\n  for u in r.urls:\n    m = re.search('arxiv.org/abs/(.+)', u.expanded_url)\n    if m: \n      rawid = m.group(1)\n      pids.append(rawid)\n  return pids\n\ndef get_latest_or_loop(q):\n  results = None\n  while results is None:\n    try:\n      results = api.GetSearch(raw_query=\"q=%s&result_type=recent&count=100\" % (q, ))\n    except Exception as e:\n      print('there was some problem (waiting some time and trying again):')\n      print(e)\n      time.sleep(sleep_time)\n  return results\n\nepochd = datetime.datetime(1970,1,1,tzinfo=pytz.utc) # time of epoch\n\ndef tprepro(tweet_text):\n  # take tweet, return set of words\n  t = tweet_text.lower()\n  t = re.sub(r'[^\\w\\s]','',t) # remove punctuation\n  ws = set([w for w in t.split() if not w.startswith('#')])\n  return ws\n\n# -----------------------------------------------------------------------------\n\n# authenticate to twitter API\nkeys = get_keys()\napi = twitter.Api(consumer_key=keys[0],\n                  consumer_secret=keys[1],\n                  access_token_key=keys[2],\n                  access_token_secret=keys[3])\n\n# connect to mongodb instance\nclient = pymongo.MongoClient()\nmdb = client.arxiv\ntweets = mdb.tweets # the \"tweets\" collection in \"arxiv\" database\ntweets_top1 = mdb.tweets_top1\ntweets_top7 = mdb.tweets_top7\ntweets_top30 = mdb.tweets_top30\nprint('mongodb tweets collection size:', tweets.count())\nprint('mongodb tweets_top1 collection size:', tweets_top1.count())\nprint('mongodb tweets_top7 collection size:', tweets_top7.count())\nprint('mongodb tweets_top30 collection size:', tweets_top30.count())\n\n# load banned accounts\nbanned = {}\nif os.path.isfile(Config.banned_path):\n  with open(Config.banned_path, 'r') as f:\n    lines = f.read().split('\\n')\n  for l in lines:\n    if l: banned[l] = 1 # mark banned\n  print('banning users:', list(banned.keys()))\n\n# main loop\nlast_db_load = None\nwhile True:\n\n  dnow_utc = datetime.datetime.now(datetime.timezone.utc)\n\n  # fetch all database arxiv pids that we know about (and handle an upadte of the db file)\n  if last_db_load is None or os.stat(Config.db_path).st_mtime > last_db_load:\n    last_db_load = time.time()\n    print('(re-) loading the paper database', Config.db_path)\n    db = pickle.load(open(Config.db_path, 'rb'))\n\n  # fetch the latest mentioning arxiv.org\n  results = get_latest_or_loop('arxiv.org')\n  to_insert = []\n  for r in results:\n    arxiv_pids = extract_arxiv_pids(r)\n    arxiv_pids = [p for p in arxiv_pids if p in db] # filter to those that are in our paper db\n    if not arxiv_pids: continue # nothing we know about here, lets move on\n    if tweets.find_one({'id':r.id}): continue # we already have this item\n    if r.user.screen_name in banned: continue # banned user, very likely a bot\n\n    # create the tweet. intentionally making it flat here without user nesting\n    d = parser.parse(r.created_at) # datetime instance\n    tweet = {}\n    tweet['id'] = r.id\n    tweet['pids'] = arxiv_pids # arxiv paper ids mentioned in this tweet\n    tweet['inserted_at_date'] = dnow_utc\n    tweet['created_at_date'] = d\n    tweet['created_at_time'] = (d - epochd).total_seconds() # seconds since epoch\n    tweet['lang'] = r.lang\n    tweet['text'] = r.text\n    tweet['user_screen_name'] = r.user.screen_name\n    tweet['user_image_url'] = r.user.profile_image_url\n    tweet['user_followers_count'] = r.user.followers_count\n    tweet['user_following_count'] = r.user.friends_count\n    to_insert.append(tweet)\n\n  if to_insert: tweets.insert_many(to_insert)\n  print('processed %d/%d new tweets. Currently maintaining total %d' % (len(to_insert), len(results), tweets.count()))\n\n  # run over 1,7,30 days\n  pid_to_words_cache = {}\n  for days in [1,7,30]:\n    tweets_top = {1:tweets_top1, 7:tweets_top7, 30:tweets_top30}[days]\n\n    # precompute: compile together all votes over last 5 days\n    dminus = dnow_utc - datetime.timedelta(days=days)\n    relevant = tweets.find({'created_at_date': {'$gt': dminus}})\n    raw_votes, votes, records_dict = {}, {}, {}\n    for tweet in relevant:\n      # some tweets are really boring, like an RT\n      tweet_words = tprepro(tweet['text'])\n      isok = not(tweet['text'].startswith('RT') or tweet['lang'] != 'en' or len(tweet['text']) < 40)\n\n      # give people with more followers more vote, as it's seen by more people and contributes to more hype\n      float_vote = min(math.log10(tweet['user_followers_count'] + 1), 4.0)/2.0\n      for pid in tweet['pids']:\n        if not pid in records_dict: \n          records_dict[pid] = {'pid':pid, 'tweets':[], 'vote': 0.0, 'raw_vote': 0} # create a new entry for this pid\n        \n        # good tweets make a comment, not just a boring RT, or exactly the post title. Detect these.\n        if pid in pid_to_words_cache:\n          title_words = pid_to_words_cache[pid]\n        else:\n          title_words = tprepro(db[pid]['title'])\n          pid_to_words_cache[pid] = title_words\n        comment_words = tweet_words - title_words # how much does the tweet have other than just the actual title of the article?\n        isok2 = int(isok and len(comment_words) >= 3)\n\n        # add up the votes for papers\n        tweet_sort_bonus = 10000 if isok2 else 0 # lets bring meaningful comments up front.\n        records_dict[pid]['tweets'].append({'screen_name':tweet['user_screen_name'], 'image_url':tweet['user_image_url'], 'text':tweet['text'], 'weight':float_vote + tweet_sort_bonus, 'ok':isok2, 'id':str(tweet['id']) })\n        votes[pid] = votes.get(pid, 0.0) + float_vote\n        raw_votes[pid] = raw_votes.get(pid, 0) + 1\n\n    # record the total amount of vote/raw_vote for each pid\n    for pid in votes:\n      records_dict[pid]['vote'] = votes[pid] # record the total amount of vote across relevant tweets\n      records_dict[pid]['raw_vote'] = raw_votes[pid] \n\n    # crop the tweets to only some number of highest weight ones (for efficiency)\n    for pid, d in records_dict.items():\n      d['num_tweets'] = len(d['tweets']) # back this up before we crop\n      d['tweets'].sort(reverse=True, key=lambda x: x['weight'])\n      if len(d['tweets']) > max_tweet_records: d['tweets'] = d['tweets'][:max_tweet_records]\n\n    # some debugging information\n    votes = [(v,k) for k,v in votes.items()]\n    votes.sort(reverse=True, key=lambda x: x[0]) # sort descending by votes\n    print('top votes:', votes[:min(len(votes), 10)])\n\n    # write the results to mongodb\n    if records_dict:\n      tweets_top.delete_many({}) # clear the whole tweets_top collection\n      tweets_top.insert_many(list(records_dict.values())) # insert all precomputed records (minimal tweets) with their votes\n\n  # and sleep for a while\n  print('sleeping', sleep_time)\n  time.sleep(sleep_time)\n"
        },
        {
          "name": "ui.jpeg",
          "type": "blob",
          "size": 253.064453125,
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 2.7685546875,
          "content": "from contextlib import contextmanager\n\nimport os\nimport re\nimport pickle\nimport tempfile\n\n# global settings\n# -----------------------------------------------------------------------------\nclass Config(object):\n    # main paper information repo file\n    db_path = 'db.p'\n    # intermediate processing folders\n    pdf_dir = os.path.join('data', 'pdf')\n    txt_dir = os.path.join('data', 'txt')\n    thumbs_dir = os.path.join('static', 'thumbs')\n    # intermediate pickles\n    tfidf_path = 'tfidf.p'\n    meta_path = 'tfidf_meta.p'\n    sim_path = 'sim_dict.p'\n    user_sim_path = 'user_sim.p'\n    # sql database file\n    db_serve_path = 'db2.p' # an enriched db.p with various preprocessing info\n    database_path = 'as.db'\n    serve_cache_path = 'serve_cache.p'\n    \n    beg_for_hosting_money = 1 # do we beg the active users randomly for money? 0 = no.\n    banned_path = 'banned.txt' # for twitter users who are banned\n    tmp_dir = 'tmp'\n\n# Context managers for atomic writes courtesy of\n# http://stackoverflow.com/questions/2333872/atomic-writing-to-file-with-python\n@contextmanager\ndef _tempfile(*args, **kws):\n    \"\"\" Context for temporary file.\n\n    Will find a free temporary filename upon entering\n    and will try to delete the file on leaving\n\n    Parameters\n    ----------\n    suffix : string\n        optional file suffix\n    \"\"\"\n\n    fd, name = tempfile.mkstemp(*args, **kws)\n    os.close(fd)\n    try:\n        yield name\n    finally:\n        try:\n            os.remove(name)\n        except OSError as e:\n            if e.errno == 2:\n                pass\n            else:\n                raise e\n\n\n@contextmanager\ndef open_atomic(filepath, *args, **kwargs):\n    \"\"\" Open temporary file object that atomically moves to destination upon\n    exiting.\n\n    Allows reading and writing to and from the same filename.\n\n    Parameters\n    ----------\n    filepath : string\n        the file path to be opened\n    fsync : bool\n        whether to force write the file to disk\n    kwargs : mixed\n        Any valid keyword arguments for :code:`open`\n    \"\"\"\n    fsync = kwargs.pop('fsync', False)\n\n    with _tempfile(dir=os.path.dirname(filepath)) as tmppath:\n        with open(tmppath, *args, **kwargs) as f:\n            yield f\n            if fsync:\n                f.flush()\n                os.fsync(f.fileno())\n        os.rename(tmppath, filepath)\n\ndef safe_pickle_dump(obj, fname):\n    with open_atomic(fname, 'wb') as f:\n        pickle.dump(obj, f, -1)\n\n\n# arxiv utils\n# -----------------------------------------------------------------------------\n\ndef strip_version(idstr):\n    \"\"\" identity function if arxiv id has no version, otherwise strips it. \"\"\"\n    parts = idstr.split('v')\n    return parts[0]\n\n# \"1511.08198v1\" is an example of a valid arxiv id that we accept\ndef isvalidid(pid):\n  return re.match('^\\d+\\.\\d+(v\\d+)?$', pid)\n"
        }
      ]
    }
  ]
}