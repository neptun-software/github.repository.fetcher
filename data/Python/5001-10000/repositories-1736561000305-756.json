{
  "metadata": {
    "timestamp": 1736561000305,
    "page": 756,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Turing-Project/WriteGPT",
      "stars": 5334,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.7568359375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": ".vs",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2020 EssayKiller\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "LanguageNetwork",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.7236328125,
          "content": "\n# WriteGPT\n\n![image](https://img.shields.io/badge/License-Apache--2.0-green) ![image](https://img.shields.io/badge/License-MIT-orange)  ![image](https://img.shields.io/badge/License-Anti--996-red)  ![image](https://img.shields.io/badge/pypi-v0.0.1a4-yellowgreen) ![image](https://img.shields.io/badge/stars-%3C%205k-blue) ![image](https://img.shields.io/badge/issues-30%20open-brightgreen)  \n\n通用型议论文创作人工智能框架，仅限交流与科普。\n\n\n## 项目简介\nWriteGPT是基于OCR、NLP领域的最新模型所构建的生成式文本创作AI框架，目前第一版finetune模型针对高考作文（主要是议论文），可以有效生成符合人类认知的文章，多数文章经过测试可以达到正常高中生及格作文水平。\n\n| 项目作者        | 主页1           | 主页2  | \n| ------------- |:------:|:----:|\n| Y1ran       | [CSDN](https://y1ran.blog.csdn.net/) |[Github](https://github.com/Y1ran) |\n\n\n**致谢**\n\n感谢开源作者[@imcaspar](https://github.com/imcaspar)  在GPT-2中文预训练框架与数据中的支持。\n感谢[@白小鱼博士](https://www.zhihu.com/people/youngfish42) 、[@YJango博士](https://www.zhihu.com/people/YJango) 、[@画渣花小烙](https://space.bili.com/402576555)、[@万物拣史](https://space.bilibili.com/328531988/) 、[@柴知道](https://space.bilibili.com/26798384/)、[@风羽酱-sdk](https://space.bilibili.com/17466521/)、[@WhatOnEarth](https://space.bilibili.com/410527811/)、[@这知识好冷](https://space.bilibili.com/403943112/)、[@科技狐](https://space.bilibili.com/40433405/) 的参与和支持\n<br>\n\n## 框架说明\n- [x] 基于EAST、CRNN、Bert和GPT-2语言模型的高考作文生成AI\n- [x] 支持bert tokenizer，当前版本基于clue chinese vocab\n- [x] 17亿参数多模块异构深度神经网络，超2亿条预训练数据\n- [x] 线上点击即用的文本生成效果demo：[17亿参数作文杀手](https://colab.research.google.com/github/EssayKillerBrain/writeGPT/blob/master/colab_online.ipynb)\n- [x] 端到端生成，从试卷识别到答题卡输出一条龙服务\n\n\n\n### Colab线上作文生成功能\n国内没有足够显存的免费GPU平台，所以配合Google Drive将训练好的AI核心功能Language Network写作模块迁移到Colab。\n\n当前线上仅开放文本生成功能，输入对应句子，AI返回生成文章。同一个句子可以输入多次，每一次输出都不同。也可以选择同时生成多篇文章。具体见：[17亿参数作文杀手](https://colab.research.google.com/github/EssayKillerBrain/writeGPT/blob/master/colab_online.ipynb)\n\n* 第一步：安装环境\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-15-22-13.png)\n\n* 第二部：加载模型\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-15-27-38.png)\n\n* 第三步：文章生成\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-15-27-14.png)\n\n* 写作效果\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-15-23-27.png)\n\n\n## 本地环境\n* Ubuntu 18.04.2\n* Pandas 0.24.2\n* Regex 2019.4.14\n* h5py 2.9.0\n* Numpy 1.16.2\n* Tensorboard 1.15.2\n* Tensorflow-gpu 1.15.2\n* Requests 2.22.0\n* OpenCV 3.4.2\n* CUDA >= 10.0\n* CuDNN >= 7.6.0\n\n## 开发日志\n\n* 2020.06.23 本地Git项目建立\n* 2020.07.03 整体模型架构搭建，开始语料收集\n* 2020.07.13 基于OCR的视觉网络训练\n* 2020.08.01 GPT-2中文预训练模型微调\n* 2020.08.14 Bert文本摘要模型\n* 2020.08.23 通顺度判分网络测试\n* 2020.09.14 排版脚本与输出装置改装\n* 2021.02.15 修复网页版模型打分\n* 2021.06.10 训练集中增加了《毛泽东选集》、《陈独秀文集》、《鲁迅文集》等著作\n\n\n## 模型结构\n整个框架分为EAST、CRNN、Bert、GPT-2、DNN 5个模块，每个模块的网络单独训练，参数相互独立。infer过程使用pipeline串联，通过外接装置直接输出到答题卡。  \n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-15-35-00.png)\n\n\n### 1. 输入\n高考语文试卷作文题\n>![浙江卷](https://images.shobserver.com/img/2020/7/7/37b2224ee3de441a8a040cb4f5576c2d.jpg)\n\n\n### 2. 识别网络\n#### 2.1 EAST文本检测\nOpenCV 的EAST文本检测器是一个深度学习模型，它能够在 720p 的图像上以13帧/秒的速度实时检测任意方向的文本，并可以获得很好的文本检测精度。  \n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-15-45-54.png)\n\n<br>\n\n**模型亮点**\n1. 简单的管道实现在当时较高精度的文本检测。\n2. 图像通过FCN处理产生像素级文本缩放地图和几何图形的多个频道。\n3. 可旋转的文本框，可以检测文本也可以检测单词。\n\nEAST文本检测器需要 OpenCV3.4.2 或更高的版本，有需要的读者可以查看 [OpenCV 安装教程](https://www.pyimagesearch.com/opencv-tutorials-resources-guides/)。虽然EAST的模型在检测自然场景下的英文文本有着较好的性能，要实现中文场景下的中文文本检测，仍然需要重新训练模型。\n\n**数据集处理**\n\n中文文本识别的数据集要按照原作者的命名方式修改，即使使用ICDAR3013这类标准数据集，也需要修改对应的图片命名方式。原代码数据集的命名方式：图片1.jpg 图片1.txt。\n\n此外，代码是通过获取文件类型然后重新命名以原来的文件类型保存的，所以文本数据和图片数据需要分开处理。\n\n*训练命令：*\n```bash\npython multigpu_train.py --gpu_list=0 --input_size=512 --batch_size_per_gpu=14 --checkpoint_path=/tmp/east_icdar2015_resnet_v1_50_rbox/ \\ --text_scale=512 --training_data_path=/data/ocr/icdar2015/ --geometry=RBOX --learning_rate=0.0001 --num_readers=24 \\ --pretrained_model_path=/tmp/resnet_v1_50.ckpt \n```\n\n\n更多细节可以参考：https://zhuanlan.zhihu.com/p/64737915\n\n<br>\n\n*检测结果*\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-16-25-01.png)\n\n除了EAST，也可以把识别网络替换为传统的CTPN等模型，github上有已经成熟的项目：https://github.com/Walleclipse/ChineseAddress_OCR\n\n#### 2.2 CRNN文本识别\n\n参考 https://github.com/ooooverflow/chinese-ocr \n\n**数据准备**\n\n下载[训练集](https://pan.baidu.com/s/1E_1iFERWr9Ro-dmlSVY8pA)：共约364万张图片，按照99: 1划分成训练集和验证集\n\n数据利用中文语料库（新闻 + 文言文），通过字体、大小、灰度、模糊、透视、拉伸等变化随机生成。包含汉字、英文字母、数字和标点共5990个字符，每个样本固定10个字符，字符随机截取自语料库中的句子，图片分辨率统一为280x32。  \n\n*修改/train/config.py中train_data_root，validation_data_root以及image_path*\n\n**训练**\n```bash\ncd train  \npython train.py\n```\n\n**训练结果**\n```python\nEpoch 3/100\n25621/25621 [==============================] - 15856s 619ms/step - loss: 0.1035 - acc: 0.9816 - val_loss: 0.1060 - val_acc: 0.9823\nEpoch 4/100\n25621/25621 [==============================] - 15651s 611ms/step - loss: 0.0798 - acc: 0.9879 - val_loss: 0.0848 - val_acc: 0.9878\nEpoch 5/100\n25621/25621 [==============================] - 16510s 644ms/step - loss: 0.0732 - acc: 0.9889 - val_loss: 0.0815 - val_acc: 0.9881\nEpoch 6/100\n25621/25621 [==============================] - 15621s 610ms/step - loss: 0.0691 - acc: 0.9895 - val_loss: 0.0791 - val_acc: 0.9886\nEpoch 7/100\n25621/25621 [==============================] - 15782s 616ms/step - loss: 0.0666 - acc: 0.9899 - val_loss: 0.0787 - val_acc: 0.9887\nEpoch 8/100\n25621/25621 [==============================] - 15560s 607ms/step - loss: 0.0645 - acc: 0.9903 - val_loss: 0.0771 - val_acc: 0.9888\n```\n>![](https://github.com/ooooverflow/chinese-ocr/raw/master/demo/ocr.png)\n\n<br>\n\n### 2. 语言网络\n#### 2.1 BERT文本摘要\n\n\nBERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder。模型的主要创新点在pre-train方法上，用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。\n\n模型的构成元素Transformer可以参考Google的 [Attention is all you need](https://arxiv.org/abs/1706.03762) ，BERT模型的结构如下图最左：\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-16-44-54.png)\n\n对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向RNN和双向RNN的区别，直觉上来讲效果会好一些。\n<br>\n\n在原论文中，作者展示了新的语言训练模型，称为编码语言模型与下一句预测 \n\n\nOriginal Paper : 3.3.1 Task #1: Masked LM\n>Input Sequence  : The man went to [MASK] store with [MASK] dog\nTarget Sequence :                  the                his\n\n规则: 会有15%的随机输入被改变，这些改变基于以下规则\n\n* 80%的tokens会成为‘掩码’token\n* 10%的tokens会称为‘随机’token\n* 10%的tokens会保持不变但需要被预测\n\n下一句预测\n\n> Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]\nLabel : Is Next\nInput = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\nLabel = NotNext\n\n规则:\n* 50%的下一句会（随机）成为连续句子\n* 50%的下一句会（随机）成为不关联句子\n\n<br>\n\n**训练**\n\n* 哈工大的新浪微博短文本摘要[LCSTS](http://icrc.hitsz.edu.cn/Article/show/139.html)\n* 教育新闻自动摘要语料[chinese_abstractive_corpus](https://github.com/wonderfulsuccess/chinese_abstractive_corpus)\n\n```bash\npython run.py --model bert\n```\n<br>\n\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-16-40-19.png)\n\n测试时，需要用正则表达式过滤考试专用词，包括“阅读下面的材料，根据要求写作”，“要求：xxx”，“请完成/请结合/请综合xx”。\n\n比如\n>![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-17-17-30.png)\n\n\n    人们用眼睛看他人、看世界，却无法直接看到完整的自己。所以，在人生的旅程中，我们需要寻找各种“镜子”、不断绘制“自画像”来审视自我，尝试回答“我是怎样的人”“我想过怎样的生活”“我能做些什么”“如何生活得更有意义”等重要的问题。\n\n\n<br>\n\n#### 2.2 GPT-2文本生成\n![](https://github.com/prakhar21/TextAugmentation-GPT2/raw/master/gpt2-sizes.png)\n\n参考：https://github.com/imcaspar/gpt2-ml/\n\n预训练语料来自 [THUCNews](http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews) 以及 [nlp_chinese_corpus](https://github.com/brightmart/nlp_chinese_corpus)，清洗后总文本量约 15G。\n Finetune语料来自历年满分高考作文、优质散文集以及近现代散文作品，约1000篇。  \n\n**预训练**  \n参考 [GPT2-ML](https://github.com/imcaspar/gpt2-ml/) 预训练模型，使用 [Quadro RTX 8000](https://www.nvidia.com/en-us/design-visualization/quadro/rtx-8000/) 训练 28w 步\n\n>![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/2233.PNG)\n\n\n<br>\n\n**Finetune**\n\n```bash\n1、进入dataset目录\npython pre_data.py --filepath /data/home/share1/gpt2-ml-Finetune/data-mayun_xiugai --outfile /data/home/share1/gpt2-ml-Finetune/data/22.json\nfilepath为finetune数据目录\n\n2、生成tfrecord训练数据\npython prepare_data.py -input_fn /data/home/share1/gpt2-ml-Finetune/data\n\n3、finetune\nCUDA_VISIBLE_DEVICES=0  python train/train_wc.py --input_file=/data/EssayKiller/gpt2-ml-Finetune/data/train.tfrecord --output_dir=/data/EssayKiller/gpt2-ml-Finetune/finetune_model --init_checkpoint=/data/EssayKiller/gpt2-ml/models/mega/model.ckpt-220000\n\n```\n\n<br>\n\n### 3.判分网络\n\n#### 3.1 DNN判分模型\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-18-59-12.png)\n\n这部分直接调用百度API。有现成的模型就不重复造轮子了，具体实现方式百度没有开源，这里简单描述一下语言模型的概念：\n语言模型是通过计算给定词组成的句子的概率，从而判断所组成的句子是否符合客观语言表达习惯。通常用于机器翻译、拼写纠错、语音识别、问答系统、词性标注、句法分析和信息检索等。  \n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-18-59-57.png)\n\n这里使用通顺度打分作为判断依据。  \n\n#### 3.2 高考排版器\n\n*标题*  \n复用BERT_SUM生成Top3的NER粒度token作为标题\n\n*主体*  \n高考议论文的写作格式要求如下：\n1. 标题居中，一般少于20字\n2. 每段段首缩进两格\n3. 每个字符尽量保持在字体框内\n4. 字数不能过长或过短\n\n由于模型输出的文章不保证换行和分段，通过统计高考作文的常见段数、每段句数，编写脚本对输出进行划分。大多数情况下分段排版的结果都比较合理。  \n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-19-04-24.png)\n\n<br>\n\n## 输出\n**答题卡**  \n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-19-07-53.png)\n\n**外接装置**\n\n基于aedraw，一款开源的CNC(Computer Numerical Control数控机床)画图机器人，具有绘制图案、写字等功能，它也可以升级为激光雕刻等用途。\n详细教程见 http://aelab.net/ ，不仅能自己制作一台写字绘画机器人，而且能够掌握其工作原理拓展更多的应用。  \n\n![](https://github.com/EssayKillerBrain/writeGPT/blob/master/References/attachments/Clipboard_2020-09-29-19-12-07.png)\n\n原版的输出临摹装置存在速度慢和格式不准的问题，通过改装和修改源代码得以优化\n\n* 因为时间原因目前的手写装置还有些问题，偶尔会有漏写、越格的问题\n* 视频中的作文经过后期的人工处理，补上了漏字\n\n<br>\n\n## 预训练模型\n\n| 模型        | 参数量           | 下载链接  | 备注 |\n| ------------- |:-------------:|:----:|:---:|\n| EAST  | < 0.1 Billion  | [GoogleDrive](https://drive.google.com/file/d/1fF4IYaL7CWghYCDvRrACM57WVx83Yvny/view?usp=sharing) | 检测模型 |\n| CRNN | < 0.1 Billion   | [网盘链接](https://eyun.baidu.com/s/3dEUJJg9) 提取码：vKeD| 识别模型 |\n| BERT | 0.1 Billion   | [GoogleDrive](https://drive.google.com/file/d/15DbA07DZNT3gMXu2aLliA3CkuR5XHhlt/view?usp=sharing) | 摘要模型 |\n| GPT-2 | 1.5 Billion   | [GoogleDrive](https://drive.google.com/file/d/1ujWYTOvRLGJX0raH-f-lPZa3-RN58ZQx/view?usp=sharing)  | 生成模型 |\n\n整个AI的参数量分布不均匀，主要原因在于，这是一个语言类AI，99%的参数量集中在语言网络中，其中GPT-2（15亿）占88%，BERT（1.1亿）占7%，其他的识别网络和判分网络共占5%。\n\n### 当前问题\n* 输出的格式和高考作文还不能完美契合，之后的参数需要微调一下。为了国庆前完成，我还没来得及优化\n* 生成的100篇作文里有很大一部分其实算不上合格的作文，有些只能勉强及格，有些甚至能拿零分（占比不多），显然GPT-2的能力有限。为了视频效果我只选了相对好的几篇做展示\n* 英文版的说明还没来得及写，有空的同学可以翻译一下提个pr\n\n## Q&A\n* **我能否用EssayKiller来帮自己写作业？**  \n  不能。所以有下一个问题：  \n  \n* **为什么缺少一些关键文件？**  \n项目在一开始是完全开源的，经过慎重考虑我认为完全开源会被部分别有用心的人用以牟利，甚至用作不法用途。参考咸鱼和淘宝上一些魔改的开源框架应用。部分懂技术又不想动笔的小同志可能会让Essaykiller帮自己写作业，比如读后感、课后作文、思修小论文。我想说，这样不好。  \n\n* **为什么不直接加密？**  \n本来打算用混淆加密，但一些模块本就是开源的，所以我开源了整体的模型文件，只隐藏了关键的，包括pipeline、输入输出在内的文件，另外有些文件里也加了盐。  \n\n* **有哪些模组可用？**  \n目前完全开源，可以独立复用的部分包括：\n  - [x] 检测网络\n  - [x] 文本摘要网络\n  - [x] 文本生成网络\n  - [x] 判分网络与排版脚本  \n\n* **为什么不用GPT-3**  \n训练一个中文GPT-3的价格至少为1200万美元，折合人民币将近1亿。要是真有人训练出来一个中文GPT-3还开源模型文件了，我愿称之为最强。  \n\n* **训练EssayKiller需要多少钱？**  \n从头到尾训练完pipeline的话在1K～100K人民币不等，取决于你有无分布式集群可用  \n\n<br>\n\n## Citation\n```\n@misc{EssayKillerBrain,\n  author = {Turing's Cat},\n  title = {Autowritting Ai Framework},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/EssayKillerBrain/writeGPT}},\n}\n```\n\n<br>\n\n\n## 参考资料  \n[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  \n[2] ERNIE: Enhanced Representation through Knowledge Integration  \n[3] Fine-tune BERT for Extractive Summarization  \n[4] EAST: An Efficient and Accurate Scene Text Detector  \n[5] An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition  \n[6] Language Models are Unsupervised Multitask Learners  \n[7] https://github.com/Morizeyao/GPT2-Chinese  \n[8] https://github.com/argman/EAST  \n[9] https://github.com/bgshih/crnn  \n[10] https://github.com/zhiyou720/chinese_summarizer  \n[11] https://zhuanlan.zhihu.com/p/64737915  \n[12] https://github.com/ouyanghuiyu/chineseocr_lite  \n[13] https://github.com/google-research/bert  \n[14] https://github.com/rowanz/grover  \n[15] https://github.com/wind91725/gpt2-ml-finetune-  \n[16] https://github.com/guodongxiaren/README  \n[17] https://www.jianshu.com/p/55560d3e0e8a  \n[18] https://github.com/YCG09/chinese_ocr  \n[19] https://github.com/xiaomaxiao/keras_ocr  \n[20] https://github.com/nghuyong/ERNIE-Pytorch  \n[21] https://zhuanlan.zhihu.com/p/43534801\n[22] https://blog.csdn.net/xuxunjie147/article/details/87178774/  \n[23] https://github.com/JiangYanting/Pre-modern_Chinese_corpus_dataset  \n[24] https://github.com/brightmart/nlp_chinese_corpus  \n[25] https://github.com/SophonPlus/ChineseNlpCorpus  \n[26] https://github.com/THUNLP-AIPoet/Resources  \n[27] https://github.com/OYE93/Chinese-NLP-Corpus  \n[28] https://github.com/CLUEbenchmark/CLUECorpus2020  \n[29] https://github.com/zhiyou720/chinese_summarizer  \n\n\n## 免责声明\n该项目中的内容仅供技术研究与科普，不作为任何结论性依据，不提供任何商业化应用授权\n"
        },
        {
          "name": "RecognizaitonNetwork",
          "type": "tree",
          "content": null
        },
        {
          "name": "References",
          "type": "tree",
          "content": null
        },
        {
          "name": "ScoringNetwork",
          "type": "tree",
          "content": null
        },
        {
          "name": "colab_online.ipynb",
          "type": "blob",
          "size": 4.9541015625,
          "content": "﻿{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"[![GitHub stars](https://img.shields.io/github/stars/EssayKillerBrain/EssayKiller?style=social)](https://github.com/imcaspar/gpt2-ml)\\n\",\n        \"[![GitHub](https://img.shields.io/github/license/EssayKillerBrain/EssayKiller)](https://github.com/imcaspar/gpt2-ml)\\n\",\n        \"[![Twitter URL](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Fgithub.com%2Fimcaspar%2Fgpt2-ml)](https://twitter.com/intent/tweet?text=Wow:&url=https://github.com/imcaspar/gpt2-ml)\\n\",\n        \"### 高考作文生成指南:\\n\",\n        \"\\n\",\n        \"* 点击代码框左上角的▶️，选择RUN ANYWAY\\n\",\n        \"* 等待对应代码文件、模型文件下载\\n\",\n        \"* 输入参数，代表长度\\n\",\n        \"* 输入作文题目（摘要模块没有部署，请自己提炼作文题中的主旨句）\\n\",\n        \"\\n\",\n        \"之后就会生成对应文章，效果如下:\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"#@title #参数与开源框架预加载\\n\",\n        \"!pip install pandas==0.24.2 &> tmp.log\\n\",\n        \"!pip install regex==2019.4.14 &> tmp.log\\n\",\n        \"!pip install h5py==2.10.0 &> tmp.log\\n\",\n        \"!pip install numpy==1.18.4 &> tmp.log\\n\",\n        \"!pip install tensorboard==1.15.0 &> tmp.log\\n\",\n        \"!pip install tensorflow-gpu==1.15.2 &> tmp.log\\n\",\n        \"!pip install tensorflow-estimator==1.15.1 &> tmp.log\\n\",\n        \"!pip install tqdm==4.31.1 &> tmp.log\\n\",\n        \"!pip install requests==2.22.0 &> tmp.log\\n\",\n        \"!pip install ujson==2.0.3 &> tmp.log\\n\",\n        \"!echo '预加载模块结束，请继续点击下方的▶️'\\n\",\n        \"#@title #Prerequisites\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"#@title #模型框架加载模块\\n\",\n        \"%cd /home\\n\",\n        \"!git clone -b master https://github.com/EssayKillerBrain/EssayKiller_V2.git \\n\",\n        \"\\n\",\n        \"#%tensorflow_version 1.5.2-GPU/TPU\\n\",\n        \"!mkdir -p /home/EssayKiller_V2/LanguageNetwork/GPT2/finetune/trained_models\\n\",\n        \"\\n\",\n        \"%cd /home/EssayKiller_V2/LanguageNetwork/GPT2/finetune/\\n\",\n        \"!perl /home/EssayKiller_V2/LanguageNetwork/GPT2/scripts/gdown.pl https://drive.google.com/file/d/1ajR-yVWmZC_z7HgNjz4tivNz-vUCgKBC trained_models/model.ckpt-280000.data-00000-of-00001\\n\",\n        \"!wget -q --show-progress https://github.com/EssayKillerBrain/EssayKiller/releases/download/v1.0/model.ckpt-280000.index -P /home/EssayKiller_V2/LanguageNetwork/GPT2/finetune/trained_models\\n\",\n        \"!wget -q --show-progress https://github.com/EssayKillerBrain/EssayKiller/releases/download/v1.0/model.ckpt-280000.meta -P /home/EssayKiller_V2/LanguageNetwork/GPT2/finetune/trained_models\\n\",\n        \"\\n\",\n        \"!echo '模型下载完成，Git项目已构建，请继续点击下方的▶️'\\n\",\n        \"# If gdown.pl failed, please uncomment following code & exec\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### 模型参数设置:\\n\",\n        \"\\n\",\n        \"* 1.作文最小篇幅：\\n\",\n        \"生成对应字数的高考作文，可自己调节长度，最长为1024个汉字。\\n\",\n        \"一般来说越短的文章AI表现越好。\\n\",\n        \"\\n\",\n        \"* 2.生成作文篇数：\\n\",\n        \"默认会生成1篇议论文，生成时间取决于服务器状态\\n\",\n        \"一般不超过60秒。受限于线上GPU内存，篇数最多为100。\\n\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"#!cat /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tpu/tpu_feed.py\\n\",\n        \"#@title #文章生成模块\\n\",\n        \"作文最小篇幅 = 1024 #@param {type:\\\"number\\\", min:800, max:1024, step:1}\\n\",\n        \"生成作文篇数 = 1 #@param {type:\\\"number\\\", min:1, max:100, step:1}\\n\",\n        \"%mv /home/EssayKiller_V2/LanguageNetwork/GPT2/finetune/models/mega/* /home/EssayKiller_V2/LanguageNetwork/GPT2/finetune/trained_models/\\n\",\n        \"%cd /home/EssayKiller_V2/LanguageNetwork/GPT2/\\n\",\n        \"!export TF_CPP_MIN_LOG_LEVEL=2\\n\",\n        \"!echo '模型加载中，请稍后......'\\n\",\n        \"!PYTHONPATH=$(pwd) python scripts/demo.py -ckpt_fn finetune/trained_models/model.ckpt-280000 -min_len $作文最小篇幅 -samples $生成作文篇数\\n\",\n        \"!PYTHONPATH=$(pwd) python scripts/formatter.py -org_text result.txt\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"colab\": {\n      \"name\": \"17亿参数-高考作文生成AI | 1.7B GPT2 Pretrained Essay Killer Brain\",\n      \"provenance\": [],\n      \"collapsed_sections\": []\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0\n}\n"
        },
        {
          "name": "pipeline.py",
          "type": "blob",
          "size": 0.943359375,
          "content": "# -*- coding: gbk -*-\n####################################################################\n# Original work Copyright 2018 The Google AI Language Team Authors.\n# Modified work Copyright 2019 Rowan Zellers\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# 本项目仅限于技术交流用途，包括所有开源部分，禁止用于任何商业用途\n#\n#\tIn order to prevent the EssayKiller framework from being maliciously registered, used or \n#\tcopied, we are using legal aid and strict copyright protection.\n#\n#\tIf you have academic needs, please bring an individual or institution's academic research \n#\tstatement and send an email to deanyuton@gmail.com. We are glad to help~\n#\t\n#\tThanks for understanding.\n#####################################################################\n\n\n"
        },
        {
          "name": "requirements-gpu.txt",
          "type": "blob",
          "size": 0.126953125,
          "content": "pandas==0.24.2\nregex==2019.4.14\nh5py==2.9.0\nnumpy==1.16.2\ntensorboard==1.13.1\ntensorflow-gpu==1.13.1\ntqdm==4.31.1\nrequests==2.22.0"
        },
        {
          "name": "requirements-tpu.txt",
          "type": "blob",
          "size": 0.1513671875,
          "content": "pandas==0.24.2\nregex==2019.4.14\nh5py==2.9.0\nnumpy==1.16.2\ntensorboard==1.13.1\ntensorflow==1.13.1\ntensorflow-estimator==1.13.0\ntqdm==4.31.1\nrequests==2.22.0"
        },
        {
          "name": "setup.sh",
          "type": "blob",
          "size": 1.234375,
          "content": "FROM tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\n\nRUN apt update && apt install -y --no-install-recommends git\n\n!pip install pandas==0.24.2 &> tmp.log\n!pip install regex==2019.4.14 &> tmp.log\n!pip install h5py==2.10.0 &> tmp.log\n!pip install numpy==1.18.4 &> tmp.log\n!pip install tensorboard==1.15.0 &> tmp.log\n!pip install tensorflow-gpu==1.15.2 &> tmp.log\n!pip install tensorflow-estimator==1.15.1 &> tmp.log\n!pip install tqdm==4.31.1 &> tmp.log\n!pip install requests==2.22.0 &> tmp.log\n!pip install ujson==2.0.3 &> tmp.log\n\n!mkdir -p /home/EssayKiller/AutoWritter/finetune/trained_models\n\n%cd /home/EssayKiller/AutoWritter/finetune/\n!perl /home/EssayKiller/AutoWritter/scripts/gdown.pl https://drive.google.com/open?id=1ujWYTOvRLGJX0raH-f-lPZa3-RN58ZQx trained_models/model.ckpt-280000.data-00000-of-00001\n!wget -q --show-progress https://github.com/EssayKillerBrain/EssayKiller/releases/download/v1.0/model.ckpt-280000.index -P /home/EssayKiller/AutoWritter/finetune/models/mega\n!wget -q --show-progress https://github.com/EssayKillerBrain/EssayKiller/releases/download/v1.0/model.ckpt-280000.meta -P /home/EssayKiller/AutoWritter/finetune/models/mega\n\n\nCMD [\"bash\", \"-c\", \"jupyter notebook --ip 0.0.0.0 --no-browser --allow-root pretrained_model_demo.ipynb\"]"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 2.3330078125,
          "content": "import jieba\nimport random\n\n\ndef ner(ques):\n    if \"叫\" in str(ques) and str(ques)[str(ques).index(\"叫\") + 1] != \"我\":\n        return ques[ques.index(\"叫\") + 1:]\n    elif \"是\" in str(ques):\n        return ques[ques.index(\"是\") + 1:]\n    elif \"姓名\" in str(ques):\n        return ques[ques.index(\"姓名\") + 2:]\n    elif \"名字\" in str(ques):\n        return ques[ques.index(\"名字\") + 2:]\n    elif \"叫我\" in str(ques):\n        return ques[ques.index(\"叫我\") + 2:]\n    elif \"喊我\" in str(ques):\n        return ques[ques.index(\"叫我\") + 2:]\n    elif \"称呼我\" in str(ques):\n        return ques[ques.index(\"称呼我\") + 3:]\n\n\ndef Levenshtein_Distance(str1, str2):\n    \"\"\"\n    计算字符串 str1 和 str2 的编辑距离\n    :param str1\n    :param str2\n    :return:\n    \"\"\"\n    matrix = [[i + j for j in range(len(str2) + 1)] for i in range(len(str1) + 1)]\n\n    for i in range(1, len(str1) + 1):\n        for j in range(1, len(str2) + 1):\n            if (str1[i - 1] == str2[j - 1]):\n                d = 0\n            else:\n                d = 1\n\n            matrix[i][j] = min(matrix[i - 1][j] + 1, matrix[i][j - 1] + 1, matrix[i - 1][j - 1] + d)\n\n    return matrix[len(str1)][len(str2)]\n\n\ndef question_review(ques, sensitive):\n    \"\"\"\n    审核用户对话，过滤敏感内容\n    :param ques: 用户说的话\n    :param sensitive: 敏感词库列表\n    :return:\n    \"\"\"\n    results = jieba.cut(ques)\n    _words = list(results)\n    flag = False\n    count = 0\n    for word in _words:\n        if word in sensitive:\n            if word in [\"台湾\", \"香港\", \"澳门\", \"西藏\", \"新疆\", \"共产党\"]:\n                flag = True\n            else:\n                count += 1\n    if flag and count != 0:\n        return random.choice([\"我们还是聊点别的吧\", \"听不大懂耶\", \"我们聊点别的吧\", \"听不大懂哎\"])\n    elif (flag == False) and count != 0:\n        return random.choice([\"我们还是聊点别的吧\", \"听不大懂耶\", \"我们聊点别的吧\", \"听不大懂哎\"])\n    elif flag and count == 0:\n        return \"approved\"\n    else:\n        return \"approved\"\n\n\ndef load_sensitive():\n    \"\"\"\n    加载敏感词库\n    :return:\n    \"\"\"\n    with open(\"sensitive/keywords\", 'r', encoding=\"utf-8\") as file:\n        sensitive_words = file.readlines()\n        sensitive_words = [word.strip() for word in sensitive_words]\n    return sensitive_words\n"
        },
        {
          "name": "~",
          "type": "blob",
          "size": 0.294921875,
          "content": "Merge branch 'master' of https://github.com/EssayKillerBrain/EssayKiller_V2\n# Please enter a commit message to explain why this merge is necessary,\n# especially if it merges an updated upstream into a topic branch.\n#\n# Lines starting with '#' will be ignored, and an empty message aborts\n# the commit.\n"
        }
      ]
    }
  ]
}