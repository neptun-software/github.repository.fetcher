{
  "metadata": {
    "timestamp": 1736560954868,
    "page": 701,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "kohya-ss/sd-scripts",
      "stars": 5534,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0693359375,
          "content": "logs\n__pycache__\nwd14_tagger_model\nvenv\n*.egg-info\nbuild\n.vscode\nwandb\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 11.076171875,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [2022] [kohya-ss]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README-ja.md",
          "type": "blob",
          "size": 7.6982421875,
          "content": "## リポジトリについて\nStable Diffusionの学習、画像生成、その他のスクリプトを入れたリポジトリです。\n\n[README in English](./README.md) ←更新情報はこちらにあります\n\n開発中のバージョンはdevブランチにあります。最新の変更点はdevブランチをご確認ください。\n\nFLUX.1およびSD3/SD3.5対応はsd3ブランチで行っています。それらの学習を行う場合はsd3ブランチをご利用ください。\n\nGUIやPowerShellスクリプトなど、より使いやすくする機能が[bmaltais氏のリポジトリ](https://github.com/bmaltais/kohya_ss)で提供されています（英語です）のであわせてご覧ください。bmaltais氏に感謝します。\n\n以下のスクリプトがあります。\n\n* DreamBooth、U-NetおよびText Encoderの学習をサポート\n* fine-tuning、同上\n* LoRAの学習をサポート\n* 画像生成\n* モデル変換（Stable Diffision ckpt/safetensorsとDiffusersの相互変換）\n\n## 使用法について\n\n* [学習について、共通編](./docs/train_README-ja.md) : データ整備やオプションなど\n    * [データセット設定](./docs/config_README-ja.md)\n* [SDXL学習](./docs/train_SDXL-en.md) （英語版）\n* [DreamBoothの学習について](./docs/train_db_README-ja.md)\n* [fine-tuningのガイド](./docs/fine_tune_README_ja.md):\n* [LoRAの学習について](./docs/train_network_README-ja.md)\n* [Textual Inversionの学習について](./docs/train_ti_README-ja.md)\n* [画像生成スクリプト](./docs/gen_img_README-ja.md)\n* note.com [モデル変換スクリプト](https://note.com/kohya_ss/n/n374f316fe4ad)\n\n## Windowsでの動作に必要なプログラム\n\nPython 3.10.6およびGitが必要です。\n\n- Python 3.10.6: https://www.python.org/ftp/python/3.10.6/python-3.10.6-amd64.exe\n- git: https://git-scm.com/download/win\n\nPowerShellを使う場合、venvを使えるようにするためには以下の手順でセキュリティ設定を変更してください。\n（venvに限らずスクリプトの実行が可能になりますので注意してください。）\n\n- PowerShellを管理者として開きます。\n- 「Set-ExecutionPolicy Unrestricted」と入力し、Yと答えます。\n- 管理者のPowerShellを閉じます。\n\n## Windows環境でのインストール\n\nスクリプトはPyTorch 2.1.2でテストしています。PyTorch 2.0.1、1.12.1でも動作すると思われます。\n\n（なお、python -m venv～の行で「python」とだけ表示された場合、py -m venv～のようにpythonをpyに変更してください。）\n\nPowerShellを使う場合、通常の（管理者ではない）PowerShellを開き以下を順に実行します。\n\n```powershell\ngit clone https://github.com/kohya-ss/sd-scripts.git\ncd sd-scripts\n\npython -m venv venv\n.\\venv\\Scripts\\activate\n\npip install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu118\npip install --upgrade -r requirements.txt\npip install xformers==0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118\n\naccelerate config\n```\n\nコマンドプロンプトでも同一です。\n\n注：`bitsandbytes==0.43.0`、`prodigyopt==1.0`、`lion-pytorch==0.0.6` は `requirements.txt` に含まれるようになりました。他のバージョンを使う場合は適宜インストールしてください。\n\nこの例では PyTorch および xfomers は2.1.2／CUDA 11.8版をインストールします。CUDA 12.1版やPyTorch 1.12.1を使う場合は適宜書き換えください。たとえば CUDA 12.1版の場合は `pip install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu121` および `pip install xformers==0.0.23.post1 --index-url https://download.pytorch.org/whl/cu121` としてください。\n\naccelerate configの質問には以下のように答えてください。（bf16で学習する場合、最後の質問にはbf16と答えてください。）\n\n```txt\n- This machine\n- No distributed training\n- NO\n- NO\n- NO\n- all\n- fp16\n```\n\n※場合によって ``ValueError: fp16 mixed precision requires a GPU`` というエラーが出ることがあるようです。この場合、6番目の質問（\n``What GPU(s) (by id) should be used for training on this machine as a comma-separated list? [all]:``）に「0」と答えてください。（id `0`のGPUが使われます。）\n\n## アップグレード\n\n新しいリリースがあった場合、以下のコマンドで更新できます。\n\n```powershell\ncd sd-scripts\ngit pull\n.\\venv\\Scripts\\activate\npip install --use-pep517 --upgrade -r requirements.txt\n```\n\nコマンドが成功すれば新しいバージョンが使用できます。\n\n## 謝意\n\nLoRAの実装は[cloneofsimo氏のリポジトリ](https://github.com/cloneofsimo/lora)を基にしたものです。感謝申し上げます。\n\nConv2d 3x3への拡大は [cloneofsimo氏](https://github.com/cloneofsimo/lora) が最初にリリースし、KohakuBlueleaf氏が [LoCon](https://github.com/KohakuBlueleaf/LoCon) でその有効性を明らかにしたものです。KohakuBlueleaf氏に深く感謝します。\n\n## ライセンス\n\nスクリプトのライセンスはASL 2.0ですが（Diffusersおよびcloneofsimo氏のリポジトリ由来のものも同様）、一部他のライセンスのコードを含みます。\n\n[Memory Efficient Attention Pytorch](https://github.com/lucidrains/memory-efficient-attention-pytorch): MIT\n\n[bitsandbytes](https://github.com/TimDettmers/bitsandbytes): MIT\n\n[BLIP](https://github.com/salesforce/BLIP): BSD-3-Clause\n\n## その他の情報\n\n### LoRAの名称について\n\n`train_network.py` がサポートするLoRAについて、混乱を避けるため名前を付けました。ドキュメントは更新済みです。以下は当リポジトリ内の独自の名称です。\n\n1. __LoRA-LierLa__ : (LoRA for __Li__ n __e__ a __r__  __La__ yers、リエラと読みます)\n\n    Linear 層およびカーネルサイズ 1x1 の Conv2d 層に適用されるLoRA\n\n2. __LoRA-C3Lier__ : (LoRA for __C__ olutional layers with __3__ x3 Kernel and  __Li__ n __e__ a __r__ layers、セリアと読みます)\n\n    1.に加え、カーネルサイズ 3x3 の Conv2d 層に適用されるLoRA\n\nデフォルトではLoRA-LierLaが使われます。LoRA-C3Lierを使う場合は `--network_args` に `conv_dim` を指定してください。\n\n<!-- \nLoRA-LierLa は[Web UI向け拡張](https://github.com/kohya-ss/sd-webui-additional-networks)、またはAUTOMATIC1111氏のWeb UIのLoRA機能で使用することができます。\n\nLoRA-C3Lierを使いWeb UIで生成するには拡張を使用してください。\n-->\n\n### 学習中のサンプル画像生成\n\nプロンプトファイルは例えば以下のようになります。\n\n```\n# prompt 1\nmasterpiece, best quality, (1girl), in white shirts, upper body, looking at viewer, simple background --n low quality, worst quality, bad anatomy,bad composition, poor, low effort --w 768 --h 768 --d 1 --l 7.5 --s 28\n\n# prompt 2\nmasterpiece, best quality, 1boy, in business suit, standing at street, looking back --n (low quality, worst quality), bad anatomy,bad composition, poor, low effort --w 576 --h 832 --d 2 --l 5.5 --s 40\n```\n\n  `#` で始まる行はコメントになります。`--n` のように「ハイフン二個＋英小文字」の形でオプションを指定できます。以下が使用可能できます。\n\n  * `--n` Negative prompt up to the next option.\n  * `--w` Specifies the width of the generated image.\n  * `--h` Specifies the height of the generated image.\n  * `--d` Specifies the seed of the generated image.\n  * `--l` Specifies the CFG scale of the generated image.\n  * `--s` Specifies the number of steps in the generation.\n\n  `( )` や `[ ]` などの重みづけも動作します。\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.9228515625,
          "content": "This repository contains training, generation and utility scripts for Stable Diffusion.\n\n[__Change History__](#change-history) is moved to the bottom of the page. \n更新履歴は[ページ末尾](#change-history)に移しました。\n\n[日本語版READMEはこちら](./README-ja.md)\n\nThe development version is in the `dev` branch. Please check the dev branch for the latest changes.\n\nFLUX.1 and SD3/SD3.5 support is done in the `sd3` branch. If you want to train them, please use the sd3 branch.\n\n\nFor easier use (GUI and PowerShell scripts etc...), please visit [the repository maintained by bmaltais](https://github.com/bmaltais/kohya_ss). Thanks to @bmaltais!\n\nThis repository contains the scripts for:\n\n* DreamBooth training, including U-Net and Text Encoder\n* Fine-tuning (native training), including U-Net and Text Encoder\n* LoRA training\n* Textual Inversion training\n* Image generation\n* Model conversion (supports 1.x and 2.x, Stable Diffision ckpt/safetensors and Diffusers)\n\n## About requirements.txt\n\nThe file does not contain requirements for PyTorch. Because the version of PyTorch depends on the environment, it is not included in the file. Please install PyTorch first according to the environment. See installation instructions below.\n\nThe scripts are tested with Pytorch 2.1.2. 2.0.1 and 1.12.1 is not tested but should work.\n\n## Links to usage documentation\n\nMost of the documents are written in Japanese.\n\n[English translation by darkstorm2150 is here](https://github.com/darkstorm2150/sd-scripts#links-to-usage-documentation). Thanks to darkstorm2150!\n\n* [Training guide - common](./docs/train_README-ja.md) : data preparation, options etc... \n  * [Chinese version](./docs/train_README-zh.md)\n* [SDXL training](./docs/train_SDXL-en.md) (English version)\n* [Dataset config](./docs/config_README-ja.md) \n  * [English version](./docs/config_README-en.md)\n* [DreamBooth training guide](./docs/train_db_README-ja.md)\n* [Step by Step fine-tuning guide](./docs/fine_tune_README_ja.md):\n* [Training LoRA](./docs/train_network_README-ja.md)\n* [Training Textual Inversion](./docs/train_ti_README-ja.md)\n* [Image generation](./docs/gen_img_README-ja.md)\n* note.com [Model conversion](https://note.com/kohya_ss/n/n374f316fe4ad)\n\n## Windows Required Dependencies\n\nPython 3.10.6 and Git:\n\n- Python 3.10.6: https://www.python.org/ftp/python/3.10.6/python-3.10.6-amd64.exe\n- git: https://git-scm.com/download/win\n\nGive unrestricted script access to powershell so venv can work:\n\n- Open an administrator powershell window\n- Type `Set-ExecutionPolicy Unrestricted` and answer A\n- Close admin powershell window\n\n## Windows Installation\n\nOpen a regular Powershell terminal and type the following inside:\n\n```powershell\ngit clone https://github.com/kohya-ss/sd-scripts.git\ncd sd-scripts\n\npython -m venv venv\n.\\venv\\Scripts\\activate\n\npip install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu118\npip install --upgrade -r requirements.txt\npip install xformers==0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118\n\naccelerate config\n```\n\nIf `python -m venv` shows only `python`, change `python` to `py`.\n\n__Note:__ Now `bitsandbytes==0.43.0`, `prodigyopt==1.0` and `lion-pytorch==0.0.6` are included in the requirements.txt. If you'd like to use the another version, please install it manually.\n\nThis installation is for CUDA 11.8. If you use a different version of CUDA, please install the appropriate version of PyTorch and xformers. For example, if you use CUDA 12, please install `pip install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu121` and `pip install xformers==0.0.23.post1 --index-url https://download.pytorch.org/whl/cu121`.\n\n<!-- \ncp .\\bitsandbytes_windows\\*.dll .\\venv\\Lib\\site-packages\\bitsandbytes\\\ncp .\\bitsandbytes_windows\\cextension.py .\\venv\\Lib\\site-packages\\bitsandbytes\\cextension.py\ncp .\\bitsandbytes_windows\\main.py .\\venv\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py\n-->\nAnswers to accelerate config:\n\n```txt\n- This machine\n- No distributed training\n- NO\n- NO\n- NO\n- all\n- fp16\n```\n\nIf you'd like to use bf16, please answer `bf16` to the last question.\n\nNote: Some user reports ``ValueError: fp16 mixed precision requires a GPU`` is occurred in training. In this case, answer `0` for the 6th question: \n``What GPU(s) (by id) should be used for training on this machine as a comma-separated list? [all]:`` \n\n(Single GPU with id `0` will be used.)\n\n## Upgrade\n\nWhen a new release comes out you can upgrade your repo with the following command:\n\n```powershell\ncd sd-scripts\ngit pull\n.\\venv\\Scripts\\activate\npip install --use-pep517 --upgrade -r requirements.txt\n```\n\nOnce the commands have completed successfully you should be ready to use the new version.\n\n### Upgrade PyTorch\n\nIf you want to upgrade PyTorch, you can upgrade it with `pip install` command in [Windows Installation](#windows-installation) section. `xformers` is also required to be upgraded when PyTorch is upgraded.\n\n## Credits\n\nThe implementation for LoRA is based on [cloneofsimo's repo](https://github.com/cloneofsimo/lora). Thank you for great work!\n\nThe LoRA expansion to Conv2d 3x3 was initially released by cloneofsimo and its effectiveness was demonstrated at [LoCon](https://github.com/KohakuBlueleaf/LoCon) by KohakuBlueleaf. Thank you so much KohakuBlueleaf!\n\n## License\n\nThe majority of scripts is licensed under ASL 2.0 (including codes from Diffusers, cloneofsimo's and LoCon), however portions of the project are available under separate license terms:\n\n[Memory Efficient Attention Pytorch](https://github.com/lucidrains/memory-efficient-attention-pytorch): MIT\n\n[bitsandbytes](https://github.com/TimDettmers/bitsandbytes): MIT\n\n[BLIP](https://github.com/salesforce/BLIP): BSD-3-Clause\n\n\n## Change History\n\n### Oct 27, 2024 / 2024-10-27:\n\n- `svd_merge_lora.py` VRAM usage has been reduced. However, main memory usage will increase (32GB is sufficient).\n- This will be included in the next release.\n- `svd_merge_lora.py` のVRAM使用量を削減しました。ただし、メインメモリの使用量は増加します（32GBあれば十分です）。\n- これは次回リリースに含まれます。\n\n### Oct 26, 2024 / 2024-10-26: \n\n- Fixed a bug in `svd_merge_lora.py`, `sdxl_merge_lora.py`, and `resize_lora.py` where the hash value of LoRA metadata was not correctly calculated when the `save_precision` was different from the  `precision` used in the calculation. See issue [#1722](https://github.com/kohya-ss/sd-scripts/pull/1722) for details. Thanks to JujoHotaru for raising the issue.\n- It will be included in the next release.\n\n- `svd_merge_lora.py`、`sdxl_merge_lora.py`、`resize_lora.py`で、保存時の精度が計算時の精度と異なる場合、LoRAメタデータのハッシュ値が正しく計算されない不具合を修正しました。詳細は issue [#1722](https://github.com/kohya-ss/sd-scripts/pull/1722) をご覧ください。問題提起していただいた JujoHotaru 氏に感謝します。\n- 以上は次回リリースに含まれます。\n\n### Sep 13, 2024 / 2024-09-13: \n\n- `sdxl_merge_lora.py` now supports OFT. Thanks to Maru-mee for the PR [#1580](https://github.com/kohya-ss/sd-scripts/pull/1580). \n- `svd_merge_lora.py` now supports LBW. Thanks to terracottahaniwa. See PR [#1575](https://github.com/kohya-ss/sd-scripts/pull/1575) for details.\n- `sdxl_merge_lora.py` also supports LBW. \n- See [LoRA Block Weight](https://github.com/hako-mikan/sd-webui-lora-block-weight) by hako-mikan for details on LBW.\n- These will be included in the next release.\n\n- `sdxl_merge_lora.py` が OFT をサポートされました。PR [#1580](https://github.com/kohya-ss/sd-scripts/pull/1580) Maru-mee 氏に感謝します。\n- `svd_merge_lora.py` で LBW がサポートされました。PR [#1575](https://github.com/kohya-ss/sd-scripts/pull/1575) terracottahaniwa 氏に感謝します。\n- `sdxl_merge_lora.py` でも LBW がサポートされました。\n- LBW の詳細は hako-mikan 氏の [LoRA Block Weight](https://github.com/hako-mikan/sd-webui-lora-block-weight) をご覧ください。\n- 以上は次回リリースに含まれます。\n\n### Jun 23, 2024 / 2024-06-23: \n\n- Fixed `cache_latents.py` and `cache_text_encoder_outputs.py` not working. (Will be included in the next release.)\n\n- `cache_latents.py` および `cache_text_encoder_outputs.py` が動作しなくなっていたのを修正しました。（次回リリースに含まれます。）\n\n### Apr 7, 2024 / 2024-04-07: v0.8.7\n\n- The default value of `huber_schedule` in Scheduled Huber Loss is changed from `exponential` to `snr`, which is expected to give better results.\n\n- Scheduled Huber Loss の `huber_schedule` のデフォルト値を `exponential` から、より良い結果が期待できる `snr` に変更しました。\n\n### Apr 7, 2024 / 2024-04-07: v0.8.6\n\n#### Highlights\n\n- The dependent libraries are updated. Please see [Upgrade](#upgrade) and update the libraries.\n  - Especially `imagesize` is newly added, so if you cannot update the libraries immediately, please install with `pip install imagesize==1.4.1` separately.\n  - `bitsandbytes==0.43.0`, `prodigyopt==1.0`, `lion-pytorch==0.0.6` are included in the requirements.txt.\n    - `bitsandbytes` no longer requires complex procedures as it now officially supports Windows.  \n  - Also, the PyTorch version is updated to 2.1.2 (PyTorch does not need to be updated immediately). In the upgrade procedure, PyTorch is not updated, so please manually install or update torch, torchvision, xformers if necessary (see [Upgrade PyTorch](#upgrade-pytorch)).\n- When logging to wandb is enabled, the entire command line is exposed. Therefore, it is recommended to write wandb API key and HuggingFace token in the configuration file (`.toml`). Thanks to bghira for raising the issue.\n  - A warning is displayed at the start of training if such information is included in the command line.\n  - Also, if there is an absolute path, the path may be exposed, so it is recommended to specify a relative path or write it in the configuration file. In such cases, an INFO log is displayed.\n  - See [#1123](https://github.com/kohya-ss/sd-scripts/pull/1123) and PR [#1240](https://github.com/kohya-ss/sd-scripts/pull/1240) for details.\n- Colab seems to stop with log output. Try specifying `--console_log_simple` option in the training script to disable rich logging.\n- Other improvements include the addition of masked loss, scheduled Huber Loss, DeepSpeed support, dataset settings improvements, and image tagging improvements. See below for details.\n\n#### Training scripts\n\n- `train_network.py` and `sdxl_train_network.py` are modified to record some dataset settings in the metadata of the trained model (`caption_prefix`, `caption_suffix`, `keep_tokens_separator`, `secondary_separator`, `enable_wildcard`).\n- Fixed a bug that U-Net and Text Encoders are included in the state in `train_network.py` and `sdxl_train_network.py`. The saving and loading of the state are faster, the file size is smaller, and the memory usage when loading is reduced.\n- DeepSpeed is supported. PR [#1101](https://github.com/kohya-ss/sd-scripts/pull/1101)  and [#1139](https://github.com/kohya-ss/sd-scripts/pull/1139) Thanks to BootsofLagrangian! See PR [#1101](https://github.com/kohya-ss/sd-scripts/pull/1101) for details.\n- The masked loss is supported in each training script. PR [#1207](https://github.com/kohya-ss/sd-scripts/pull/1207) See [Masked loss](#about-masked-loss) for details.\n- Scheduled Huber Loss has been introduced to each training scripts. PR [#1228](https://github.com/kohya-ss/sd-scripts/pull/1228/) Thanks to kabachuha for the PR and cheald, drhead, and others for the discussion! See the PR and [Scheduled Huber Loss](#about-scheduled-huber-loss) for details.\n- The options `--noise_offset_random_strength` and `--ip_noise_gamma_random_strength` are added to each training script. These options can be used to vary the noise offset and ip noise gamma in the range of 0 to the specified value. PR [#1177](https://github.com/kohya-ss/sd-scripts/pull/1177) Thanks to KohakuBlueleaf!\n- The options `--save_state_on_train_end` are added to each training script. PR [#1168](https://github.com/kohya-ss/sd-scripts/pull/1168) Thanks to gesen2egee!\n- The options `--sample_every_n_epochs` and `--sample_every_n_steps` in each training script now display a warning and ignore them when a number less than or equal to `0` is specified. Thanks to S-Del for raising the issue.\n\n#### Dataset settings\n\n- The [English version of the dataset settings documentation](./docs/config_README-en.md) is added. PR [#1175](https://github.com/kohya-ss/sd-scripts/pull/1175) Thanks to darkstorm2150!\n- The `.toml` file for the dataset config is now read in UTF-8 encoding. PR [#1167](https://github.com/kohya-ss/sd-scripts/pull/1167) Thanks to Horizon1704!\n- Fixed a bug that the last subset settings are applied to all images when multiple subsets of regularization images are specified in the dataset settings. The settings for each subset are correctly applied to each image. PR [#1205](https://github.com/kohya-ss/sd-scripts/pull/1205) Thanks to feffy380!\n- Some features are added to the dataset subset settings.\n  - `secondary_separator` is added to specify the tag separator that is not the target of shuffling or dropping. \n    - Specify `secondary_separator=\";;;\"`. When you specify `secondary_separator`, the part is not shuffled or dropped. \n  - `enable_wildcard` is added. When set to `true`, the wildcard notation `{aaa|bbb|ccc}` can be used. The multi-line caption is also enabled.\n  - `keep_tokens_separator` is updated to be used twice in the caption. When you specify `keep_tokens_separator=\"|||\"`, the part divided by the second `|||` is not shuffled or dropped and remains at the end.\n  - The existing features `caption_prefix` and `caption_suffix` can be used together. `caption_prefix` and `caption_suffix` are processed first, and then `enable_wildcard`, `keep_tokens_separator`, shuffling and dropping, and `secondary_separator` are processed in order.\n  - See [Dataset config](./docs/config_README-en.md) for details.\n- The dataset with DreamBooth method supports caching image information (size, caption). PR [#1178](https://github.com/kohya-ss/sd-scripts/pull/1178) and [#1206](https://github.com/kohya-ss/sd-scripts/pull/1206) Thanks to KohakuBlueleaf! See [DreamBooth method specific options](./docs/config_README-en.md#dreambooth-specific-options) for details.\n\n#### Image tagging\n\n- The support for v3 repositories is added to `tag_image_by_wd14_tagger.py` (`--onnx` option only). PR [#1192](https://github.com/kohya-ss/sd-scripts/pull/1192) Thanks to sdbds!\n  - Onnx may need to be updated. Onnx is not installed by default, so please install or update it with `pip install onnx==1.15.0 onnxruntime-gpu==1.17.1` etc. Please also check the comments in `requirements.txt`.\n- The model is now saved in the subdirectory as `--repo_id` in `tag_image_by_wd14_tagger.py` . This caches multiple repo_id models. Please delete unnecessary files under `--model_dir`.\n- Some options are added to `tag_image_by_wd14_tagger.py`.\n  - Some are added in PR [#1216](https://github.com/kohya-ss/sd-scripts/pull/1216) Thanks to Disty0!\n  - Output rating tags `--use_rating_tags` and `--use_rating_tags_as_last_tag`\n  - Output character tags first `--character_tags_first`\n  - Expand character tags and series `--character_tag_expand`\n  - Specify tags to output first `--always_first_tags`\n  - Replace tags `--tag_replacement`\n  - See [Tagging documentation](./docs/wd14_tagger_README-en.md) for details.\n- Fixed an error when specifying `--beam_search` and a value of 2 or more for `--num_beams` in `make_captions.py`.\n\n#### About Masked loss\n\nThe masked loss is supported in each training script. To enable the masked loss, specify the `--masked_loss` option.\n\nThe feature is not fully tested, so there may be bugs. If you find any issues, please open an Issue.\n\nControlNet dataset is used to specify the mask. The mask images should be the RGB images. The pixel value 255 in R channel is treated as the mask (the loss is calculated only for the pixels with the mask), and 0 is treated as the non-mask. The pixel values 0-255 are converted to 0-1 (i.e., the pixel value 128 is treated as the half weight of the loss). See details for the dataset specification in the [LLLite documentation](./docs/train_lllite_README.md#preparing-the-dataset).\n\n#### About Scheduled Huber Loss\n\nScheduled Huber Loss has been introduced to each training scripts. This is a method to improve robustness against outliers or anomalies (data corruption) in the training data.\n\nWith the traditional MSE (L2) loss function, the impact of outliers could be significant, potentially leading to a degradation in the quality of generated images. On the other hand, while the Huber loss function can suppress the influence of outliers, it tends to compromise the reproduction of fine details in images.\n\nTo address this, the proposed method employs a clever application of the Huber loss function. By scheduling the use of Huber loss in the early stages of training (when noise is high) and MSE in the later stages, it strikes a balance between outlier robustness and fine detail reproduction.\n\nExperimental results have confirmed that this method achieves higher accuracy on data containing outliers compared to pure Huber loss or MSE. The increase in computational cost is minimal.\n\nThe newly added arguments loss_type, huber_schedule, and huber_c allow for the selection of the loss function type (Huber, smooth L1, MSE), scheduling method (exponential, constant, SNR), and Huber's parameter. This enables optimization based on the characteristics of the dataset.\n\nSee PR [#1228](https://github.com/kohya-ss/sd-scripts/pull/1228/) for details.\n\n- `loss_type`: Specify the loss function type. Choose `huber` for Huber loss, `smooth_l1` for smooth L1 loss, and `l2` for MSE loss. The default is `l2`, which is the same as before.\n- `huber_schedule`: Specify the scheduling method. Choose `exponential`, `constant`, or `snr`. The default is `snr`.\n- `huber_c`: Specify the Huber's parameter. The default is `0.1`.\n\nPlease read [Releases](https://github.com/kohya-ss/sd-scripts/releases) for recent updates.\n\n#### 主要な変更点\n\n- 依存ライブラリが更新されました。[アップグレード](./README-ja.md#アップグレード) を参照しライブラリを更新してください。\n  - 特に `imagesize` が新しく追加されていますので、すぐにライブラリの更新ができない場合は `pip install imagesize==1.4.1` で個別にインストールしてください。\n  - `bitsandbytes==0.43.0`、`prodigyopt==1.0`、`lion-pytorch==0.0.6` が requirements.txt に含まれるようになりました。\n    - `bitsandbytes` が公式に Windows をサポートしたため複雑な手順が不要になりました。\n  - また PyTorch のバージョンを 2.1.2 に更新しました。PyTorch はすぐに更新する必要はありません。更新時は、アップグレードの手順では PyTorch が更新されませんので、torch、torchvision、xformers を手動でインストールしてください。\n- wandb へのログ出力が有効の場合、コマンドライン全体が公開されます。そのため、コマンドラインに wandb の API キーや HuggingFace のトークンなどが含まれる場合、設定ファイル（`.toml`）への記載をお勧めします。問題提起していただいた bghira 氏に感謝します。\n  - このような場合には学習開始時に警告が表示されます。\n  - また絶対パスの指定がある場合、そのパスが公開される可能性がありますので、相対パスを指定するか設定ファイルに記載することをお勧めします。このような場合は INFO ログが表示されます。\n  - 詳細は [#1123](https://github.com/kohya-ss/sd-scripts/pull/1123) および PR [#1240](https://github.com/kohya-ss/sd-scripts/pull/1240) をご覧ください。\n- Colab での動作時、ログ出力で停止してしまうようです。学習スクリプトに `--console_log_simple` オプションを指定し、rich のロギングを無効してお試しください。\n- その他、マスクロス追加、Scheduled Huber Loss 追加、DeepSpeed 対応、データセット設定の改善、画像タグ付けの改善などがあります。詳細は以下をご覧ください。\n\n#### 学習スクリプト\n\n- `train_network.py` および `sdxl_train_network.py` で、学習したモデルのメタデータに一部のデータセット設定が記録されるよう修正しました（`caption_prefix`、`caption_suffix`、`keep_tokens_separator`、`secondary_separator`、`enable_wildcard`）。\n- `train_network.py` および `sdxl_train_network.py` で、state に U-Net および Text Encoder が含まれる不具合を修正しました。state の保存、読み込みが高速化され、ファイルサイズも小さくなり、また読み込み時のメモリ使用量も削減されます。\n- DeepSpeed がサポートされました。PR [#1101](https://github.com/kohya-ss/sd-scripts/pull/1101) 、[#1139](https://github.com/kohya-ss/sd-scripts/pull/1139) BootsofLagrangian 氏に感謝します。詳細は PR [#1101](https://github.com/kohya-ss/sd-scripts/pull/1101) をご覧ください。\n- 各学習スクリプトでマスクロスをサポートしました。PR [#1207](https://github.com/kohya-ss/sd-scripts/pull/1207) 詳細は [マスクロスについて](#マスクロスについて) をご覧ください。\n- 各学習スクリプトに Scheduled Huber Loss を追加しました。PR [#1228](https://github.com/kohya-ss/sd-scripts/pull/1228/) ご提案いただいた kabachuha 氏、および議論を深めてくださった cheald 氏、drhead 氏を始めとする諸氏に感謝します。詳細は当該 PR および [Scheduled Huber Loss について](#scheduled-huber-loss-について) をご覧ください。\n- 各学習スクリプトに、noise offset、ip noise gammaを、それぞれ 0~指定した値の範囲で変動させるオプション `--noise_offset_random_strength` および `--ip_noise_gamma_random_strength` が追加されました。 PR [#1177](https://github.com/kohya-ss/sd-scripts/pull/1177) KohakuBlueleaf 氏に感謝します。\n- 各学習スクリプトに、学習終了時に state を保存する `--save_state_on_train_end` オプションが追加されました。 PR [#1168](https://github.com/kohya-ss/sd-scripts/pull/1168) gesen2egee 氏に感謝します。\n- 各学習スクリプトで `--sample_every_n_epochs` および `--sample_every_n_steps` オプションに `0` 以下の数値を指定した時、警告を表示するとともにそれらを無視するよう変更しました。問題提起していただいた S-Del 氏に感謝します。\n\n#### データセット設定\n\n- データセット設定の `.toml` ファイルが UTF-8 encoding で読み込まれるようになりました。PR [#1167](https://github.com/kohya-ss/sd-scripts/pull/1167) Horizon1704 氏に感謝します。\n- データセット設定で、正則化画像のサブセットを複数指定した時、最後のサブセットの各種設定がすべてのサブセットの画像に適用される不具合が修正されました。それぞれのサブセットの設定が、それぞれの画像に正しく適用されます。PR [#1205](https://github.com/kohya-ss/sd-scripts/pull/1205) feffy380 氏に感謝します。\n- データセットのサブセット設定にいくつかの機能を追加しました。\n  - シャッフルの対象とならないタグ分割識別子の指定 `secondary_separator` を追加しました。`secondary_separator=\";;;\"` のように指定します。`secondary_separator` で区切ることで、その部分はシャッフル、drop 時にまとめて扱われます。\n  - `enable_wildcard` を追加しました。`true` にするとワイルドカード記法 `{aaa|bbb|ccc}` が使えます。また複数行キャプションも有効になります。\n  - `keep_tokens_separator` をキャプション内に 2 つ使えるようにしました。たとえば `keep_tokens_separator=\"|||\"` と指定したとき、`1girl, hatsune miku, vocaloid ||| stage, mic ||| best quality, rating: general` とキャプションを指定すると、二番目の `|||` で分割された部分はシャッフル、drop されず末尾に残ります。\n  - 既存の機能 `caption_prefix` と `caption_suffix` とあわせて使えます。`caption_prefix` と `caption_suffix` は一番最初に処理され、その後、ワイルドカード、`keep_tokens_separator`、シャッフルおよび drop、`secondary_separator` の順に処理されます。\n  - 詳細は [データセット設定](./docs/config_README-ja.md) をご覧ください。\n- DreamBooth 方式の DataSet で画像情報（サイズ、キャプション）をキャッシュする機能が追加されました。PR [#1178](https://github.com/kohya-ss/sd-scripts/pull/1178)、[#1206](https://github.com/kohya-ss/sd-scripts/pull/1206) KohakuBlueleaf 氏に感謝します。詳細は [データセット設定](./docs/config_README-ja.md#dreambooth-方式専用のオプション) をご覧ください。\n- データセット設定の[英語版ドキュメント](./docs/config_README-en.md) が追加されました。PR [#1175](https://github.com/kohya-ss/sd-scripts/pull/1175) darkstorm2150 氏に感謝します。\n\n#### 画像のタグ付け\n\n- `tag_image_by_wd14_tagger.py` で v3 のリポジトリがサポートされました（`--onnx` 指定時のみ有効）。 PR [#1192](https://github.com/kohya-ss/sd-scripts/pull/1192) sdbds 氏に感謝します。\n  - Onnx のバージョンアップが必要になるかもしれません。デフォルトでは Onnx はインストールされていませんので、`pip install onnx==1.15.0 onnxruntime-gpu==1.17.1` 等でインストール、アップデートしてください。`requirements.txt` のコメントもあわせてご確認ください。\n- `tag_image_by_wd14_tagger.py` で、モデルを`--repo_id` のサブディレクトリに保存するようにしました。これにより複数のモデルファイルがキャッシュされます。`--model_dir` 直下の不要なファイルは削除願います。\n- `tag_image_by_wd14_tagger.py` にいくつかのオプションを追加しました。\n  - 一部は PR [#1216](https://github.com/kohya-ss/sd-scripts/pull/1216) で追加されました。Disty0 氏に感謝します。\n  - レーティングタグを出力する `--use_rating_tags` および `--use_rating_tags_as_last_tag`\n  - キャラクタタグを最初に出力する `--character_tags_first`\n  - キャラクタタグとシリーズを展開する `--character_tag_expand`\n  - 常に最初に出力するタグを指定する `--always_first_tags`\n  - タグを置換する `--tag_replacement`\n  - 詳細は [タグ付けに関するドキュメント](./docs/wd14_tagger_README-ja.md) をご覧ください。\n- `make_captions.py` で `--beam_search` を指定し `--num_beams` に2以上の値を指定した時のエラーを修正しました。\n\n#### マスクロスについて\n\n各学習スクリプトでマスクロスをサポートしました。マスクロスを有効にするには `--masked_loss` オプションを指定してください。\n\n機能は完全にテストされていないため、不具合があるかもしれません。その場合は Issue を立てていただけると助かります。\n\nマスクの指定には ControlNet データセットを使用します。マスク画像は RGB 画像である必要があります。R チャンネルのピクセル値 255 がロス計算対象、0 がロス計算対象外になります。0-255 の値は、0-1 の範囲に変換されます（つまりピクセル値 128 の部分はロスの重みが半分になります）。データセットの詳細は [LLLite ドキュメント](./docs/train_lllite_README-ja.md#データセットの準備) をご覧ください。\n\n#### Scheduled Huber Loss について\n\n各学習スクリプトに、学習データ中の異常値や外れ値（data corruption）への耐性を高めるための手法、Scheduled Huber Lossが導入されました。\n\n従来のMSE（L2）損失関数では、異常値の影響を大きく受けてしまい、生成画像の品質低下を招く恐れがありました。一方、Huber損失関数は異常値の影響を抑えられますが、画像の細部再現性が損なわれがちでした。\n\nこの手法ではHuber損失関数の適用を工夫し、学習の初期段階（ノイズが大きい場合）ではHuber損失を、後期段階ではMSEを用いるようスケジューリングすることで、異常値耐性と細部再現性のバランスを取ります。\n\n実験の結果では、この手法が純粋なHuber損失やMSEと比べ、異常値を含むデータでより高い精度を達成することが確認されています。また計算コストの増加はわずかです。\n\n具体的には、新たに追加された引数loss_type、huber_schedule、huber_cで、損失関数の種類（Huber, smooth L1, MSE）とスケジューリング方法（exponential, constant, SNR）を選択できます。これによりデータセットに応じた最適化が可能になります。\n\n詳細は PR [#1228](https://github.com/kohya-ss/sd-scripts/pull/1228/) をご覧ください。\n\n- `loss_type` : 損失関数の種類を指定します。`huber` で Huber損失、`smooth_l1` で smooth L1 損失、`l2` で MSE 損失を選択します。デフォルトは `l2` で、従来と同様です。\n- `huber_schedule` : スケジューリング方法を指定します。`exponential` で指数関数的、`constant` で一定、`snr` で信号対雑音比に基づくスケジューリングを選択します。デフォルトは `snr` です。\n- `huber_c` : Huber損失のパラメータを指定します。デフォルトは `0.1` です。\n\nPR 内でいくつかの比較が共有されています。この機能を試す場合、最初は `--loss_type smooth_l1 --huber_schedule snr --huber_c 0.1` などで試してみるとよいかもしれません。\n\n最近の更新情報は [Release](https://github.com/kohya-ss/sd-scripts/releases) をご覧ください。\n\n## Additional Information\n\n### Naming of LoRA\n\nThe LoRA supported by `train_network.py` has been named to avoid confusion. The documentation has been updated. The following are the names of LoRA types in this repository.\n\n1. __LoRA-LierLa__ : (LoRA for __Li__ n __e__ a __r__  __La__ yers)\n\n    LoRA for Linear layers and Conv2d layers with 1x1 kernel\n\n2. __LoRA-C3Lier__ : (LoRA for __C__ olutional layers with __3__ x3 Kernel and  __Li__ n __e__ a __r__ layers)\n\n    In addition to 1., LoRA for Conv2d layers with 3x3 kernel \n    \nLoRA-LierLa is the default LoRA type for `train_network.py` (without `conv_dim` network arg). \n<!-- \nLoRA-LierLa can be used with [our extension](https://github.com/kohya-ss/sd-webui-additional-networks) for AUTOMATIC1111's Web UI, or with the built-in LoRA feature of the Web UI.\n\nTo use LoRA-C3Lier with Web UI, please use our extension. \n-->\n\n### Sample image generation during training\n  A prompt file might look like this, for example\n\n```\n# prompt 1\nmasterpiece, best quality, (1girl), in white shirts, upper body, looking at viewer, simple background --n low quality, worst quality, bad anatomy,bad composition, poor, low effort --w 768 --h 768 --d 1 --l 7.5 --s 28\n\n# prompt 2\nmasterpiece, best quality, 1boy, in business suit, standing at street, looking back --n (low quality, worst quality), bad anatomy,bad composition, poor, low effort --w 576 --h 832 --d 2 --l 5.5 --s 40\n```\n\n  Lines beginning with `#` are comments. You can specify options for the generated image with options like `--n` after the prompt. The following can be used.\n\n  * `--n` Negative prompt up to the next option.\n  * `--w` Specifies the width of the generated image.\n  * `--h` Specifies the height of the generated image.\n  * `--d` Specifies the seed of the generated image.\n  * `--l` Specifies the CFG scale of the generated image.\n  * `--s` Specifies the number of steps in the generation.\n\n  The prompt weighting such as `( )` and `[ ]` are working.\n"
        },
        {
          "name": "XTI_hijack.py",
          "type": "blob",
          "size": 8.01171875,
          "content": "import torch\nfrom library.device_utils import init_ipex\ninit_ipex()\n\nfrom typing import Union, List, Optional, Dict, Any, Tuple\nfrom diffusers.models.unet_2d_condition import UNet2DConditionOutput\n\nfrom library.original_unet import SampleOutput\n\n\ndef unet_forward_XTI(\n    self,\n    sample: torch.FloatTensor,\n    timestep: Union[torch.Tensor, float, int],\n    encoder_hidden_states: torch.Tensor,\n    class_labels: Optional[torch.Tensor] = None,\n    return_dict: bool = True,\n) -> Union[Dict, Tuple]:\n    r\"\"\"\n    Args:\n        sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n        timestep (`torch.FloatTensor` or `float` or `int`): (batch) timesteps\n        encoder_hidden_states (`torch.FloatTensor`): (batch, sequence_length, feature_dim) encoder hidden states\n        return_dict (`bool`, *optional*, defaults to `True`):\n            Whether or not to return a dict instead of a plain tuple.\n\n    Returns:\n        `SampleOutput` or `tuple`:\n        `SampleOutput` if `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n    \"\"\"\n    # By default samples have to be AT least a multiple of the overall upsampling factor.\n    # The overall upsampling factor is equal to 2 ** (# num of upsampling layears).\n    # However, the upsampling interpolation output size can be forced to fit any upsampling size\n    # on the fly if necessary.\n    # デフォルトではサンプルは「2^アップサンプルの数」、つまり64の倍数である必要がある\n    # ただそれ以外のサイズにも対応できるように、必要ならアップサンプルのサイズを変更する\n    # 多分画質が悪くなるので、64で割り切れるようにしておくのが良い\n    default_overall_up_factor = 2**self.num_upsamplers\n\n    # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`\n    # 64で割り切れないときはupsamplerにサイズを伝える\n    forward_upsample_size = False\n    upsample_size = None\n\n    if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n        # logger.info(\"Forward upsample size to force interpolation output size.\")\n        forward_upsample_size = True\n\n    # 1. time\n    timesteps = timestep\n    timesteps = self.handle_unusual_timesteps(sample, timesteps)  # 変な時だけ処理\n\n    t_emb = self.time_proj(timesteps)\n\n    # timesteps does not contain any weights and will always return f32 tensors\n    # but time_embedding might actually be running in fp16. so we need to cast here.\n    # there might be better ways to encapsulate this.\n    # timestepsは重みを含まないので常にfloat32のテンソルを返す\n    # しかしtime_embeddingはfp16で動いているかもしれないので、ここでキャストする必要がある\n    # time_projでキャストしておけばいいんじゃね？\n    t_emb = t_emb.to(dtype=self.dtype)\n    emb = self.time_embedding(t_emb)\n\n    # 2. pre-process\n    sample = self.conv_in(sample)\n\n    # 3. down\n    down_block_res_samples = (sample,)\n    down_i = 0\n    for downsample_block in self.down_blocks:\n        # downblockはforwardで必ずencoder_hidden_statesを受け取るようにしても良さそうだけど、\n        # まあこちらのほうがわかりやすいかもしれない\n        if downsample_block.has_cross_attention:\n            sample, res_samples = downsample_block(\n                hidden_states=sample,\n                temb=emb,\n                encoder_hidden_states=encoder_hidden_states[down_i : down_i + 2],\n            )\n            down_i += 2\n        else:\n            sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n\n        down_block_res_samples += res_samples\n\n    # 4. mid\n    sample = self.mid_block(sample, emb, encoder_hidden_states=encoder_hidden_states[6])\n\n    # 5. up\n    up_i = 7\n    for i, upsample_block in enumerate(self.up_blocks):\n        is_final_block = i == len(self.up_blocks) - 1\n\n        res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n        down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]  # skip connection\n\n        # if we have not reached the final block and need to forward the upsample size, we do it here\n        # 前述のように最後のブロック以外ではupsample_sizeを伝える\n        if not is_final_block and forward_upsample_size:\n            upsample_size = down_block_res_samples[-1].shape[2:]\n\n        if upsample_block.has_cross_attention:\n            sample = upsample_block(\n                hidden_states=sample,\n                temb=emb,\n                res_hidden_states_tuple=res_samples,\n                encoder_hidden_states=encoder_hidden_states[up_i : up_i + 3],\n                upsample_size=upsample_size,\n            )\n            up_i += 3\n        else:\n            sample = upsample_block(\n                hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size\n            )\n\n    # 6. post-process\n    sample = self.conv_norm_out(sample)\n    sample = self.conv_act(sample)\n    sample = self.conv_out(sample)\n\n    if not return_dict:\n        return (sample,)\n\n    return SampleOutput(sample=sample)\n\n\ndef downblock_forward_XTI(\n    self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n):\n    output_states = ()\n    i = 0\n\n    for resnet, attn in zip(self.resnets, self.attentions):\n        if self.training and self.gradient_checkpointing:\n\n            def create_custom_forward(module, return_dict=None):\n                def custom_forward(*inputs):\n                    if return_dict is not None:\n                        return module(*inputs, return_dict=return_dict)\n                    else:\n                        return module(*inputs)\n\n                return custom_forward\n\n            hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            hidden_states = torch.utils.checkpoint.checkpoint(\n                create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states[i]\n            )[0]\n        else:\n            hidden_states = resnet(hidden_states, temb)\n            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states[i]).sample\n\n        output_states += (hidden_states,)\n        i += 1\n\n    if self.downsamplers is not None:\n        for downsampler in self.downsamplers:\n            hidden_states = downsampler(hidden_states)\n\n        output_states += (hidden_states,)\n\n    return hidden_states, output_states\n\n\ndef upblock_forward_XTI(\n    self,\n    hidden_states,\n    res_hidden_states_tuple,\n    temb=None,\n    encoder_hidden_states=None,\n    upsample_size=None,\n):\n    i = 0\n    for resnet, attn in zip(self.resnets, self.attentions):\n        # pop res hidden states\n        res_hidden_states = res_hidden_states_tuple[-1]\n        res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n        if self.training and self.gradient_checkpointing:\n\n            def create_custom_forward(module, return_dict=None):\n                def custom_forward(*inputs):\n                    if return_dict is not None:\n                        return module(*inputs, return_dict=return_dict)\n                    else:\n                        return module(*inputs)\n\n                return custom_forward\n\n            hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            hidden_states = torch.utils.checkpoint.checkpoint(\n                create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states[i]\n            )[0]\n        else:\n            hidden_states = resnet(hidden_states, temb)\n            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states[i]).sample\n\n        i += 1\n\n    if self.upsamplers is not None:\n        for upsampler in self.upsamplers:\n            hidden_states = upsampler(hidden_states, upsample_size)\n\n    return hidden_states\n"
        },
        {
          "name": "_typos.toml",
          "type": "blob",
          "size": 0.458984375,
          "content": "# Files for typos\n# Instruction:  https://github.com/marketplace/actions/typos-action#getting-started\n\n[default.extend-identifiers]\n\n[default.extend-words]\nNIN=\"NIN\"\nparms=\"parms\"\nnin=\"nin\"\nextention=\"extention\" # Intentionally left\nnd=\"nd\"\nshs=\"shs\"\nsts=\"sts\"\nscs=\"scs\"\ncpc=\"cpc\"\ncoc=\"coc\"\ncic=\"cic\"\nmsm=\"msm\"\nusu=\"usu\"\nici=\"ici\"\nlvl=\"lvl\"\ndii=\"dii\"\nmuk=\"muk\"\nori=\"ori\"\nhru=\"hru\"\nrik=\"rik\"\nkoo=\"koo\"\nyos=\"yos\"\nwn=\"wn\"\n\n\n[files]\nextend-exclude = [\"_typos.toml\", \"venv\"]\n"
        },
        {
          "name": "bitsandbytes_windows",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "fine_tune.py",
          "type": "blob",
          "size": 23.0009765625,
          "content": "# training with captions\n# XXX dropped option: hypernetwork training\n\nimport argparse\nimport math\nimport os\nfrom multiprocessing import Value\nimport toml\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library import deepspeed_utils\nfrom library.device_utils import init_ipex, clean_memory_on_device\n\ninit_ipex()\n\nfrom accelerate.utils import set_seed\nfrom diffusers import DDPMScheduler\n\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nimport library.train_util as train_util\nimport library.config_util as config_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    apply_snr_weight,\n    get_weighted_text_embeddings,\n    prepare_scheduler_for_custom_training,\n    scale_v_prediction_loss_like_noise_prediction,\n    apply_debiased_estimation,\n)\n\n\ndef train(args):\n    train_util.verify_training_args(args)\n    train_util.prepare_dataset_args(args, True)\n    deepspeed_utils.prepare_deepspeed_args(args)\n    setup_logging(args, reset=True)\n\n    cache_latents = args.cache_latents\n\n    if args.seed is not None:\n        set_seed(args.seed)  # 乱数系列を初期化する\n\n    tokenizer = train_util.load_tokenizer(args)\n\n    # データセットを準備する\n    if args.dataset_class is None:\n        blueprint_generator = BlueprintGenerator(ConfigSanitizer(False, True, False, True))\n        if args.dataset_config is not None:\n            logger.info(f\"Load dataset config from {args.dataset_config}\")\n            user_config = config_util.load_user_config(args.dataset_config)\n            ignored = [\"train_data_dir\", \"in_json\"]\n            if any(getattr(args, attr) is not None for attr in ignored):\n                logger.warning(\n                    \"ignore following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                        \", \".join(ignored)\n                    )\n                )\n        else:\n            user_config = {\n                \"datasets\": [\n                    {\n                        \"subsets\": [\n                            {\n                                \"image_dir\": args.train_data_dir,\n                                \"metadata_file\": args.in_json,\n                            }\n                        ]\n                    }\n                ]\n            }\n\n        blueprint = blueprint_generator.generate(user_config, args, tokenizer=tokenizer)\n        train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n    else:\n        train_dataset_group = train_util.load_arbitrary_dataset(args, tokenizer)\n\n    current_epoch = Value(\"i\", 0)\n    current_step = Value(\"i\", 0)\n    ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n    collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n    if args.debug_dataset:\n        train_util.debug_dataset(train_dataset_group)\n        return\n    if len(train_dataset_group) == 0:\n        logger.error(\n            \"No data found. Please verify the metadata file and train_data_dir option. / 画像がありません。メタデータおよびtrain_data_dirオプションを確認してください。\"\n        )\n        return\n\n    if cache_latents:\n        assert (\n            train_dataset_group.is_latent_cacheable()\n        ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n\n    # acceleratorを準備する\n    logger.info(\"prepare accelerator\")\n    accelerator = train_util.prepare_accelerator(args)\n\n    # mixed precisionに対応した型を用意しておき適宜castする\n    weight_dtype, save_dtype = train_util.prepare_dtype(args)\n    vae_dtype = torch.float32 if args.no_half_vae else weight_dtype\n\n    # モデルを読み込む\n    text_encoder, vae, unet, load_stable_diffusion_format = train_util.load_target_model(args, weight_dtype, accelerator)\n\n    # verify load/save model formats\n    if load_stable_diffusion_format:\n        src_stable_diffusion_ckpt = args.pretrained_model_name_or_path\n        src_diffusers_model_path = None\n    else:\n        src_stable_diffusion_ckpt = None\n        src_diffusers_model_path = args.pretrained_model_name_or_path\n\n    if args.save_model_as is None:\n        save_stable_diffusion_format = load_stable_diffusion_format\n        use_safetensors = args.use_safetensors\n    else:\n        save_stable_diffusion_format = args.save_model_as.lower() == \"ckpt\" or args.save_model_as.lower() == \"safetensors\"\n        use_safetensors = args.use_safetensors or (\"safetensors\" in args.save_model_as.lower())\n\n    # Diffusers版のxformers使用フラグを設定する関数\n    def set_diffusers_xformers_flag(model, valid):\n        #   model.set_use_memory_efficient_attention_xformers(valid)            # 次のリリースでなくなりそう\n        # pipeが自動で再帰的にset_use_memory_efficient_attention_xformersを探すんだって(;´Д｀)\n        # U-Netだけ使う時にはどうすればいいのか……仕方ないからコピって使うか\n        # 0.10.2でなんか巻き戻って個別に指定するようになった(;^ω^)\n\n        # Recursively walk through all the children.\n        # Any children which exposes the set_use_memory_efficient_attention_xformers method\n        # gets the message\n        def fn_recursive_set_mem_eff(module: torch.nn.Module):\n            if hasattr(module, \"set_use_memory_efficient_attention_xformers\"):\n                module.set_use_memory_efficient_attention_xformers(valid)\n\n            for child in module.children():\n                fn_recursive_set_mem_eff(child)\n\n        fn_recursive_set_mem_eff(model)\n\n    # モデルに xformers とか memory efficient attention を組み込む\n    if args.diffusers_xformers:\n        accelerator.print(\"Use xformers by Diffusers\")\n        set_diffusers_xformers_flag(unet, True)\n    else:\n        # Windows版のxformersはfloatで学習できないのでxformersを使わない設定も可能にしておく必要がある\n        accelerator.print(\"Disable Diffusers' xformers\")\n        set_diffusers_xformers_flag(unet, False)\n        train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n\n    # 学習を準備する\n    if cache_latents:\n        vae.to(accelerator.device, dtype=vae_dtype)\n        vae.requires_grad_(False)\n        vae.eval()\n        with torch.no_grad():\n            train_dataset_group.cache_latents(vae, args.vae_batch_size, args.cache_latents_to_disk, accelerator.is_main_process)\n        vae.to(\"cpu\")\n        clean_memory_on_device(accelerator.device)\n\n        accelerator.wait_for_everyone()\n\n    # 学習を準備する：モデルを適切な状態にする\n    training_models = []\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n    training_models.append(unet)\n\n    if args.train_text_encoder:\n        accelerator.print(\"enable text encoder training\")\n        if args.gradient_checkpointing:\n            text_encoder.gradient_checkpointing_enable()\n        training_models.append(text_encoder)\n    else:\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\n        text_encoder.requires_grad_(False)  # text encoderは学習しない\n        if args.gradient_checkpointing:\n            text_encoder.gradient_checkpointing_enable()\n            text_encoder.train()  # required for gradient_checkpointing\n        else:\n            text_encoder.eval()\n\n    if not cache_latents:\n        vae.requires_grad_(False)\n        vae.eval()\n        vae.to(accelerator.device, dtype=vae_dtype)\n\n    for m in training_models:\n        m.requires_grad_(True)\n\n    trainable_params = []\n    if args.learning_rate_te is None or not args.train_text_encoder:\n        for m in training_models:\n            trainable_params.extend(m.parameters())\n    else:\n        trainable_params = [\n            {\"params\": list(unet.parameters()), \"lr\": args.learning_rate},\n            {\"params\": list(text_encoder.parameters()), \"lr\": args.learning_rate_te},\n        ]\n\n    # 学習に必要なクラスを準備する\n    accelerator.print(\"prepare optimizer, data loader etc.\")\n    _, _, optimizer = train_util.get_optimizer(args, trainable_params=trainable_params)\n\n    # dataloaderを準備する\n    # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n    n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset_group,\n        batch_size=1,\n        shuffle=True,\n        collate_fn=collator,\n        num_workers=n_workers,\n        persistent_workers=args.persistent_data_loader_workers,\n    )\n\n    # 学習ステップ数を計算する\n    if args.max_train_epochs is not None:\n        args.max_train_steps = args.max_train_epochs * math.ceil(\n            len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n        )\n        accelerator.print(\n            f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n        )\n\n    # データセット側にも学習ステップを送信\n    train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n    # lr schedulerを用意する\n    lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　モデル全体をfp16にする\n    if args.full_fp16:\n        assert (\n            args.mixed_precision == \"fp16\"\n        ), \"full_fp16 requires mixed precision='fp16' / full_fp16を使う場合はmixed_precision='fp16'を指定してください。\"\n        accelerator.print(\"enable full fp16 training.\")\n        unet.to(weight_dtype)\n        text_encoder.to(weight_dtype)\n\n    if args.deepspeed:\n        if args.train_text_encoder:\n            ds_model = deepspeed_utils.prepare_deepspeed_model(args, unet=unet, text_encoder=text_encoder)\n        else:\n            ds_model = deepspeed_utils.prepare_deepspeed_model(args, unet=unet)\n        ds_model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            ds_model, optimizer, train_dataloader, lr_scheduler\n        )\n        training_models = [ds_model]\n    else:\n        # acceleratorがなんかよろしくやってくれるらしい\n        if args.train_text_encoder:\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n                unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n            )\n        else:\n            unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader, lr_scheduler)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n    if args.full_fp16:\n        train_util.patch_accelerator_for_fp16_training(accelerator)\n\n    # resumeする\n    train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n    # epoch数を計算する\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n        args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n    # 学習する\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    accelerator.print(\"running training / 学習開始\")\n    accelerator.print(f\"  num examples / サンプル数: {train_dataset_group.num_train_images}\")\n    accelerator.print(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n    accelerator.print(f\"  num epochs / epoch数: {num_train_epochs}\")\n    accelerator.print(f\"  batch size per device / バッチサイズ: {args.train_batch_size}\")\n    accelerator.print(\n        f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\"\n    )\n    accelerator.print(f\"  gradient accumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n    accelerator.print(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n    progress_bar = tqdm(range(args.max_train_steps), smoothing=0, disable=not accelerator.is_local_main_process, desc=\"steps\")\n    global_step = 0\n\n    noise_scheduler = DDPMScheduler(\n        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, clip_sample=False\n    )\n    prepare_scheduler_for_custom_training(noise_scheduler, accelerator.device)\n    if args.zero_terminal_snr:\n        custom_train_functions.fix_noise_scheduler_betas_for_zero_terminal_snr(noise_scheduler)\n\n    if accelerator.is_main_process:\n        init_kwargs = {}\n        if args.wandb_run_name:\n            init_kwargs[\"wandb\"] = {\"name\": args.wandb_run_name}\n        if args.log_tracker_config is not None:\n            init_kwargs = toml.load(args.log_tracker_config)\n        accelerator.init_trackers(\"finetuning\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs)\n\n    # For --sample_at_first\n    train_util.sample_images(accelerator, args, 0, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n    loss_recorder = train_util.LossRecorder()\n    for epoch in range(num_train_epochs):\n        accelerator.print(f\"\\nepoch {epoch+1}/{num_train_epochs}\")\n        current_epoch.value = epoch + 1\n\n        for m in training_models:\n            m.train()\n\n        for step, batch in enumerate(train_dataloader):\n            current_step.value = global_step\n            with accelerator.accumulate(*training_models):\n                with torch.no_grad():\n                    if \"latents\" in batch and batch[\"latents\"] is not None:\n                        latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                    else:\n                        # latentに変換\n                        latents = vae.encode(batch[\"images\"].to(dtype=vae_dtype)).latent_dist.sample().to(weight_dtype)\n                    latents = latents * 0.18215\n                b_size = latents.shape[0]\n\n                with torch.set_grad_enabled(args.train_text_encoder):\n                    # Get the text embedding for conditioning\n                    if args.weighted_captions:\n                        encoder_hidden_states = get_weighted_text_embeddings(\n                            tokenizer,\n                            text_encoder,\n                            batch[\"captions\"],\n                            accelerator.device,\n                            args.max_token_length // 75 if args.max_token_length else 1,\n                            clip_skip=args.clip_skip,\n                        )\n                    else:\n                        input_ids = batch[\"input_ids\"].to(accelerator.device)\n                        encoder_hidden_states = train_util.get_hidden_states(\n                            args, input_ids, tokenizer, text_encoder, None if not args.full_fp16 else weight_dtype\n                        )\n\n                # Sample noise, sample a random timestep for each image, and add noise to the latents,\n                # with noise offset and/or multires noise if specified\n                noise, noisy_latents, timesteps, huber_c = train_util.get_noise_noisy_latents_and_timesteps(args, noise_scheduler, latents)\n\n                # Predict the noise residual\n                with accelerator.autocast():\n                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                if args.v_parameterization:\n                    # v-parameterization training\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    target = noise\n\n                if args.min_snr_gamma or args.scale_v_pred_loss_like_noise_pred or args.debiased_estimation_loss:\n                    # do not mean over batch dimension for snr weight or scale v-pred loss\n                    loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c)\n                    loss = loss.mean([1, 2, 3])\n\n                    if args.min_snr_gamma:\n                        loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma, args.v_parameterization)\n                    if args.scale_v_pred_loss_like_noise_pred:\n                        loss = scale_v_prediction_loss_like_noise_prediction(loss, timesteps, noise_scheduler)\n                    if args.debiased_estimation_loss:\n                        loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)\n\n                    loss = loss.mean()  # mean over batch dimension\n                else:\n                    loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"mean\", loss_type=args.loss_type, huber_c=huber_c)\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients and args.max_grad_norm != 0.0:\n                    params_to_clip = []\n                    for m in training_models:\n                        params_to_clip.extend(m.parameters())\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                train_util.sample_images(\n                    accelerator, args, None, global_step, accelerator.device, vae, tokenizer, text_encoder, unet\n                )\n\n                # 指定ステップごとにモデルを保存\n                if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                    accelerator.wait_for_everyone()\n                    if accelerator.is_main_process:\n                        src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n                        train_util.save_sd_model_on_epoch_end_or_stepwise(\n                            args,\n                            False,\n                            accelerator,\n                            src_path,\n                            save_stable_diffusion_format,\n                            use_safetensors,\n                            save_dtype,\n                            epoch,\n                            num_train_epochs,\n                            global_step,\n                            accelerator.unwrap_model(text_encoder),\n                            accelerator.unwrap_model(unet),\n                            vae,\n                        )\n\n            current_loss = loss.detach().item()  # 平均なのでbatch sizeは関係ないはず\n            if args.logging_dir is not None:\n                logs = {\"loss\": current_loss}\n                train_util.append_lr_to_logs(logs, lr_scheduler, args.optimizer_type, including_unet=True)\n                accelerator.log(logs, step=global_step)\n\n            loss_recorder.add(epoch=epoch, step=step, loss=current_loss)\n            avr_loss: float = loss_recorder.moving_average\n            logs = {\"avr_loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        if args.logging_dir is not None:\n            logs = {\"loss/epoch\": loss_recorder.moving_average}\n            accelerator.log(logs, step=epoch + 1)\n\n        accelerator.wait_for_everyone()\n\n        if args.save_every_n_epochs is not None:\n            if accelerator.is_main_process:\n                src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n                train_util.save_sd_model_on_epoch_end_or_stepwise(\n                    args,\n                    True,\n                    accelerator,\n                    src_path,\n                    save_stable_diffusion_format,\n                    use_safetensors,\n                    save_dtype,\n                    epoch,\n                    num_train_epochs,\n                    global_step,\n                    accelerator.unwrap_model(text_encoder),\n                    accelerator.unwrap_model(unet),\n                    vae,\n                )\n\n        train_util.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n    is_main_process = accelerator.is_main_process\n    if is_main_process:\n        unet = accelerator.unwrap_model(unet)\n        text_encoder = accelerator.unwrap_model(text_encoder)\n\n    accelerator.end_training()\n\n    if is_main_process and (args.save_state or args.save_state_on_train_end):        \n        train_util.save_state_on_train_end(args, accelerator)\n\n    del accelerator  # この後メモリを使うのでこれは消す\n\n    if is_main_process:\n        src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n        train_util.save_sd_model_on_train_end(\n            args, src_path, save_stable_diffusion_format, use_safetensors, save_dtype, epoch, global_step, text_encoder, unet, vae\n        )\n        logger.info(\"model saved.\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, False, True, True)\n    train_util.add_training_arguments(parser, False)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_sd_saving_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser)\n\n    parser.add_argument(\n        \"--diffusers_xformers\", action=\"store_true\", help=\"use xformers by diffusers / Diffusersでxformersを使用する\"\n    )\n    parser.add_argument(\"--train_text_encoder\", action=\"store_true\", help=\"train text encoder / text encoderも学習する\")\n    parser.add_argument(\n        \"--learning_rate_te\",\n        type=float,\n        default=None,\n        help=\"learning rate for text encoder, default is same as unet / Text Encoderの学習率、デフォルトはunetと同じ\",\n    )\n    parser.add_argument(\n        \"--no_half_vae\",\n        action=\"store_true\",\n        help=\"do not use fp16/bf16 VAE in mixed precision (use float VAE) / mixed precisionでも fp16/bf16 VAEを使わずfloat VAEを使う\",\n    )\n\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    train(args)\n"
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "gen_img.py",
          "type": "blob",
          "size": 141.0244140625,
          "content": "import itertools\nimport json\nfrom typing import Any, List, NamedTuple, Optional, Tuple, Union, Callable\nimport glob\nimport importlib\nimport importlib.util\nimport sys\nimport inspect\nimport time\nimport zipfile\nfrom diffusers.utils import deprecate\nfrom diffusers.configuration_utils import FrozenDict\nimport argparse\nimport math\nimport os\nimport random\nimport re\n\nimport diffusers\nimport numpy as np\nimport torch\n\nfrom library.device_utils import init_ipex, clean_memory, get_preferred_device\n\ninit_ipex()\n\nimport torchvision\nfrom diffusers import (\n    AutoencoderKL,\n    DDPMScheduler,\n    EulerAncestralDiscreteScheduler,\n    DPMSolverMultistepScheduler,\n    DPMSolverSinglestepScheduler,\n    LMSDiscreteScheduler,\n    PNDMScheduler,\n    DDIMScheduler,\n    EulerDiscreteScheduler,\n    HeunDiscreteScheduler,\n    KDPM2DiscreteScheduler,\n    KDPM2AncestralDiscreteScheduler,\n    # UNet2DConditionModel,\n    StableDiffusionPipeline,\n)\nfrom einops import rearrange\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPImageProcessor\nimport PIL\nfrom PIL import Image\nfrom PIL.PngImagePlugin import PngInfo\n\nimport library.model_util as model_util\nimport library.train_util as train_util\nimport library.sdxl_model_util as sdxl_model_util\nimport library.sdxl_train_util as sdxl_train_util\nfrom networks.lora import LoRANetwork\nimport tools.original_control_net as original_control_net\nfrom tools.original_control_net import ControlNetInfo\nfrom library.original_unet import UNet2DConditionModel, InferUNet2DConditionModel\nfrom library.sdxl_original_unet import InferSdxlUNet2DConditionModel\nfrom library.original_unet import FlashAttentionFunction\nfrom networks.control_net_lllite import ControlNetLLLite\nfrom library.utils import GradualLatent, EulerAncestralDiscreteSchedulerGL\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# scheduler:\nSCHEDULER_LINEAR_START = 0.00085\nSCHEDULER_LINEAR_END = 0.0120\nSCHEDULER_TIMESTEPS = 1000\nSCHEDLER_SCHEDULE = \"scaled_linear\"\n\n# その他の設定\nLATENT_CHANNELS = 4\nDOWNSAMPLING_FACTOR = 8\n\nCLIP_VISION_MODEL = \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"\n\n# region モジュール入れ替え部\n\"\"\"\n高速化のためのモジュール入れ替え\n\"\"\"\n\n\ndef replace_unet_modules(unet: diffusers.models.unet_2d_condition.UNet2DConditionModel, mem_eff_attn, xformers, sdpa):\n    if mem_eff_attn:\n        logger.info(\"Enable memory efficient attention for U-Net\")\n\n        # これはDiffusersのU-Netではなく自前のU-Netなので置き換えなくても良い\n        unet.set_use_memory_efficient_attention(False, True)\n    elif xformers:\n        logger.info(\"Enable xformers for U-Net\")\n        try:\n            import xformers.ops\n        except ImportError:\n            raise ImportError(\"No xformers / xformersがインストールされていないようです\")\n\n        unet.set_use_memory_efficient_attention(True, False)\n    elif sdpa:\n        logger.info(\"Enable SDPA for U-Net\")\n        unet.set_use_memory_efficient_attention(False, False)\n        unet.set_use_sdpa(True)\n\n\n# TODO common train_util.py\ndef replace_vae_modules(vae: diffusers.models.AutoencoderKL, mem_eff_attn, xformers, sdpa):\n    if mem_eff_attn:\n        replace_vae_attn_to_memory_efficient()\n    elif xformers:\n        # replace_vae_attn_to_xformers() # 解像度によってxformersがエラーを出す？\n        vae.set_use_memory_efficient_attention_xformers(True)  # とりあえずこっちを使う\n    elif sdpa:\n        replace_vae_attn_to_sdpa()\n\n\ndef replace_vae_attn_to_memory_efficient():\n    logger.info(\"VAE Attention.forward has been replaced to FlashAttention (not xformers)\")\n    flash_func = FlashAttentionFunction\n\n    def forward_flash_attn(self, hidden_states, **kwargs):\n        q_bucket_size = 512\n        k_bucket_size = 1024\n\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        out = flash_func.apply(query_proj, key_proj, value_proj, None, False, q_bucket_size, k_bucket_size)\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_flash_attn_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_flash_attn(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_flash_attn_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_flash_attn\n\n\ndef replace_vae_attn_to_xformers():\n    logger.info(\"VAE: Attention.forward has been replaced to xformers\")\n    import xformers.ops\n\n    def forward_xformers(self, hidden_states, **kwargs):\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        query_proj = query_proj.contiguous()\n        key_proj = key_proj.contiguous()\n        value_proj = value_proj.contiguous()\n        out = xformers.ops.memory_efficient_attention(query_proj, key_proj, value_proj, attn_bias=None)\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_xformers_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_xformers(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_xformers_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_xformers\n\n\ndef replace_vae_attn_to_sdpa():\n    logger.info(\"VAE: Attention.forward has been replaced to sdpa\")\n\n    def forward_sdpa(self, hidden_states, **kwargs):\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b n h d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        out = torch.nn.functional.scaled_dot_product_attention(\n            query_proj, key_proj, value_proj, attn_mask=None, dropout_p=0.0, is_causal=False\n        )\n\n        out = rearrange(out, \"b n h d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_sdpa_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_sdpa(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_sdpa_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_sdpa\n\n\n# endregion\n\n# region 画像生成の本体：lpw_stable_diffusion.py （ASL）からコピーして修正\n# https://github.com/huggingface/diffusers/blob/main/examples/community/lpw_stable_diffusion.py\n# Pipelineだけ独立して使えないのと機能追加するのとでコピーして修正\n\n\nclass PipelineLike:\n    def __init__(\n        self,\n        is_sdxl,\n        device,\n        vae: AutoencoderKL,\n        text_encoders: List[CLIPTextModel],\n        tokenizers: List[CLIPTokenizer],\n        unet: InferSdxlUNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        clip_skip: int,\n    ):\n        super().__init__()\n        self.is_sdxl = is_sdxl\n        self.device = device\n        self.clip_skip = clip_skip\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n            )\n            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"clip_sample\"] = False\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        self.vae = vae\n        self.text_encoders = text_encoders\n        self.tokenizers = tokenizers\n        self.unet: Union[InferUNet2DConditionModel, InferSdxlUNet2DConditionModel] = unet\n        self.scheduler = scheduler\n        self.safety_checker = None\n\n        self.clip_vision_model: CLIPVisionModelWithProjection = None\n        self.clip_vision_processor: CLIPImageProcessor = None\n        self.clip_vision_strength = 0.0\n\n        # Textual Inversion\n        self.token_replacements_list = []\n        for _ in range(len(self.text_encoders)):\n            self.token_replacements_list.append({})\n\n        # ControlNet\n        self.control_nets: List[ControlNetInfo] = []  # only for SD 1.5\n        self.control_net_lllites: List[ControlNetLLLite] = []\n        self.control_net_enabled = True  # control_netsが空ならTrueでもFalseでもControlNetは動作しない\n\n        self.gradual_latent: GradualLatent = None\n\n    # Textual Inversion\n    def add_token_replacement(self, text_encoder_index, target_token_id, rep_token_ids):\n        self.token_replacements_list[text_encoder_index][target_token_id] = rep_token_ids\n\n    def set_enable_control_net(self, en: bool):\n        self.control_net_enabled = en\n\n    def get_token_replacer(self, tokenizer):\n        tokenizer_index = self.tokenizers.index(tokenizer)\n        token_replacements = self.token_replacements_list[tokenizer_index]\n\n        def replace_tokens(tokens):\n            # print(\"replace_tokens\", tokens, \"=>\", token_replacements)\n            if isinstance(tokens, torch.Tensor):\n                tokens = tokens.tolist()\n\n            new_tokens = []\n            for token in tokens:\n                if token in token_replacements:\n                    replacement = token_replacements[token]\n                    new_tokens.extend(replacement)\n                else:\n                    new_tokens.append(token)\n            return new_tokens\n\n        return replace_tokens\n\n    def set_control_nets(self, ctrl_nets):\n        self.control_nets = ctrl_nets\n\n    def set_control_net_lllites(self, ctrl_net_lllites):\n        self.control_net_lllites = ctrl_net_lllites\n\n    def set_gradual_latent(self, gradual_latent):\n        if gradual_latent is None:\n            logger.info(\"gradual_latent is disabled\")\n            self.gradual_latent = None\n        else:\n            logger.info(f\"gradual_latent is enabled: {gradual_latent}\")\n            self.gradual_latent = gradual_latent  # (ds_ratio, start_timesteps, every_n_steps, ratio_step)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        init_image: Union[torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]] = None,\n        mask_image: Union[torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]] = None,\n        height: int = 1024,\n        width: int = 1024,\n        original_height: int = None,\n        original_width: int = None,\n        original_height_negative: int = None,\n        original_width_negative: int = None,\n        crop_top: int = 0,\n        crop_left: int = 0,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_scale: float = None,\n        strength: float = 0.8,\n        # num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        vae_batch_size: float = None,\n        return_latents: bool = False,\n        # return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        is_cancelled_callback: Optional[Callable[[], bool]] = None,\n        callback_steps: Optional[int] = 1,\n        img2img_noise=None,\n        clip_guide_images=None,\n        emb_normalize_mode: str = \"original\",\n        **kwargs,\n    ):\n        # TODO support secondary prompt\n        num_images_per_prompt = 1  # fixed because already prompt is repeated\n\n        if isinstance(prompt, str):\n            batch_size = 1\n            prompt = [prompt]\n        elif isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n        regional_network = \" AND \" in prompt[0]\n\n        vae_batch_size = (\n            batch_size\n            if vae_batch_size is None\n            else (int(vae_batch_size) if vae_batch_size >= 1 else max(1, int(batch_size * vae_batch_size)))\n        )\n\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\" f\" {type(callback_steps)}.\"\n            )\n\n        # get prompt text embeddings\n\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        if not do_classifier_free_guidance and negative_scale is not None:\n            logger.warning(f\"negative_scale is ignored if guidance scalle <= 1.0\")\n            negative_scale = None\n\n        # get unconditional embeddings for classifier free guidance\n        if negative_prompt is None:\n            negative_prompt = [\"\"] * batch_size\n        elif isinstance(negative_prompt, str):\n            negative_prompt = [negative_prompt] * batch_size\n        if batch_size != len(negative_prompt):\n            raise ValueError(\n                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                \" the batch size of `prompt`.\"\n            )\n\n        tes_text_embs = []\n        tes_uncond_embs = []\n        tes_real_uncond_embs = []\n\n        for tokenizer, text_encoder in zip(self.tokenizers, self.text_encoders):\n            token_replacer = self.get_token_replacer(tokenizer)\n\n            # use last text_pool, because it is from text encoder 2\n            text_embeddings, text_pool, uncond_embeddings, uncond_pool, _ = get_weighted_text_embeddings(\n                self.is_sdxl,\n                tokenizer,\n                text_encoder,\n                prompt=prompt,\n                uncond_prompt=negative_prompt if do_classifier_free_guidance else None,\n                max_embeddings_multiples=max_embeddings_multiples,\n                clip_skip=self.clip_skip,\n                token_replacer=token_replacer,\n                device=self.device,\n                emb_normalize_mode=emb_normalize_mode,\n                **kwargs,\n            )\n            tes_text_embs.append(text_embeddings)\n            tes_uncond_embs.append(uncond_embeddings)\n\n            if negative_scale is not None:\n                _, real_uncond_embeddings, _ = get_weighted_text_embeddings(\n                    self.is_sdxl,\n                    token_replacer,\n                    prompt=prompt,  # こちらのトークン長に合わせてuncondを作るので75トークン超で必須\n                    uncond_prompt=[\"\"] * batch_size,\n                    max_embeddings_multiples=max_embeddings_multiples,\n                    clip_skip=self.clip_skip,\n                    token_replacer=token_replacer,\n                    device=self.device,\n                    emb_normalize_mode=emb_normalize_mode,\n                    **kwargs,\n                )\n                tes_real_uncond_embs.append(real_uncond_embeddings)\n\n        # concat text encoder outputs\n        text_embeddings = tes_text_embs[0]\n        uncond_embeddings = tes_uncond_embs[0]\n        for i in range(1, len(tes_text_embs)):\n            text_embeddings = torch.cat([text_embeddings, tes_text_embs[i]], dim=2)  # n,77,2048\n            if do_classifier_free_guidance:\n                uncond_embeddings = torch.cat([uncond_embeddings, tes_uncond_embs[i]], dim=2)  # n,77,2048\n\n        if do_classifier_free_guidance:\n            if negative_scale is None:\n                text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n            else:\n                text_embeddings = torch.cat([uncond_embeddings, text_embeddings, real_uncond_embeddings])\n\n        if self.control_net_lllites:\n            # ControlNetのhintにguide imageを流用する。ControlNetの場合はControlNet側で行う\n            if isinstance(clip_guide_images, PIL.Image.Image):\n                clip_guide_images = [clip_guide_images]\n            if isinstance(clip_guide_images[0], PIL.Image.Image):\n                clip_guide_images = [preprocess_image(im) for im in clip_guide_images]\n                clip_guide_images = torch.cat(clip_guide_images)\n            if isinstance(clip_guide_images, list):\n                clip_guide_images = torch.stack(clip_guide_images)\n\n            clip_guide_images = clip_guide_images.to(self.device, dtype=text_embeddings.dtype)\n\n        # create size embs\n        if original_height is None:\n            original_height = height\n        if original_width is None:\n            original_width = width\n        if original_height_negative is None:\n            original_height_negative = original_height\n        if original_width_negative is None:\n            original_width_negative = original_width\n        if crop_top is None:\n            crop_top = 0\n        if crop_left is None:\n            crop_left = 0\n        if self.is_sdxl:\n            emb1 = sdxl_train_util.get_timestep_embedding(torch.FloatTensor([original_height, original_width]).unsqueeze(0), 256)\n            uc_emb1 = sdxl_train_util.get_timestep_embedding(\n                torch.FloatTensor([original_height_negative, original_width_negative]).unsqueeze(0), 256\n            )\n            emb2 = sdxl_train_util.get_timestep_embedding(torch.FloatTensor([crop_top, crop_left]).unsqueeze(0), 256)\n            emb3 = sdxl_train_util.get_timestep_embedding(torch.FloatTensor([height, width]).unsqueeze(0), 256)\n            c_vector = torch.cat([emb1, emb2, emb3], dim=1).to(self.device, dtype=text_embeddings.dtype).repeat(batch_size, 1)\n            uc_vector = torch.cat([uc_emb1, emb2, emb3], dim=1).to(self.device, dtype=text_embeddings.dtype).repeat(batch_size, 1)\n\n            if regional_network:\n                # use last pool for conditioning\n                num_sub_prompts = len(text_pool) // batch_size\n                text_pool = text_pool[num_sub_prompts - 1 :: num_sub_prompts]  # last subprompt\n\n            if init_image is not None and self.clip_vision_model is not None:\n                logger.info(f\"encode by clip_vision_model and apply clip_vision_strength={self.clip_vision_strength}\")\n                vision_input = self.clip_vision_processor(init_image, return_tensors=\"pt\", device=self.device)\n                pixel_values = vision_input[\"pixel_values\"].to(self.device, dtype=text_embeddings.dtype)\n\n                clip_vision_embeddings = self.clip_vision_model(\n                    pixel_values=pixel_values, output_hidden_states=True, return_dict=True\n                )\n                clip_vision_embeddings = clip_vision_embeddings.image_embeds\n\n                if len(clip_vision_embeddings) == 1 and batch_size > 1:\n                    clip_vision_embeddings = clip_vision_embeddings.repeat((batch_size, 1))\n\n                clip_vision_embeddings = clip_vision_embeddings * self.clip_vision_strength\n                assert clip_vision_embeddings.shape == text_pool.shape, f\"{clip_vision_embeddings.shape} != {text_pool.shape}\"\n                text_pool = clip_vision_embeddings  # replace: same as ComfyUI (?)\n\n            c_vector = torch.cat([text_pool, c_vector], dim=1)\n            if do_classifier_free_guidance:\n                uc_vector = torch.cat([uncond_pool, uc_vector], dim=1)\n                vector_embeddings = torch.cat([uc_vector, c_vector])\n            else:\n                vector_embeddings = c_vector\n\n        # set timesteps\n        self.scheduler.set_timesteps(num_inference_steps, self.device)\n\n        latents_dtype = text_embeddings.dtype\n        init_latents_orig = None\n        mask = None\n\n        if init_image is None:\n            # get the initial random noise unless the user supplied it\n\n            # Unlike in other pipelines, latents need to be generated in the target device\n            # for 1-to-1 results reproducibility with the CompVis implementation.\n            # However this currently doesn't work in `mps`.\n            latents_shape = (\n                batch_size * num_images_per_prompt,\n                self.unet.in_channels,\n                height // 8,\n                width // 8,\n            )\n\n            if latents is None:\n                if self.device.type == \"mps\":\n                    # randn does not exist on mps\n                    latents = torch.randn(\n                        latents_shape,\n                        generator=generator,\n                        device=\"cpu\",\n                        dtype=latents_dtype,\n                    ).to(self.device)\n                else:\n                    latents = torch.randn(\n                        latents_shape,\n                        generator=generator,\n                        device=self.device,\n                        dtype=latents_dtype,\n                    )\n            else:\n                if latents.shape != latents_shape:\n                    raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n                latents = latents.to(self.device)\n\n            timesteps = self.scheduler.timesteps.to(self.device)\n\n            # scale the initial noise by the standard deviation required by the scheduler\n            latents = latents * self.scheduler.init_noise_sigma\n        else:\n            # image to tensor\n            if isinstance(init_image, PIL.Image.Image):\n                init_image = [init_image]\n            if isinstance(init_image[0], PIL.Image.Image):\n                init_image = [preprocess_image(im) for im in init_image]\n                init_image = torch.cat(init_image)\n            if isinstance(init_image, list):\n                init_image = torch.stack(init_image)\n\n            # mask image to tensor\n            if mask_image is not None:\n                if isinstance(mask_image, PIL.Image.Image):\n                    mask_image = [mask_image]\n                if isinstance(mask_image[0], PIL.Image.Image):\n                    mask_image = torch.cat([preprocess_mask(im) for im in mask_image])  # H*W, 0 for repaint\n\n            # encode the init image into latents and scale the latents\n            init_image = init_image.to(device=self.device, dtype=latents_dtype)\n            if init_image.size()[-2:] == (height // 8, width // 8):\n                init_latents = init_image\n            else:\n                if vae_batch_size >= batch_size:\n                    init_latent_dist = self.vae.encode(init_image.to(self.vae.dtype)).latent_dist\n                    init_latents = init_latent_dist.sample(generator=generator)\n                else:\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                    init_latents = []\n                    for i in tqdm(range(0, min(batch_size, len(init_image)), vae_batch_size)):\n                        init_latent_dist = self.vae.encode(\n                            (init_image[i : i + vae_batch_size] if vae_batch_size > 1 else init_image[i].unsqueeze(0)).to(\n                                self.vae.dtype\n                            )\n                        ).latent_dist\n                        init_latents.append(init_latent_dist.sample(generator=generator))\n                    init_latents = torch.cat(init_latents)\n\n                init_latents = (sdxl_model_util.VAE_SCALE_FACTOR if self.is_sdxl else 0.18215) * init_latents\n\n            if len(init_latents) == 1:\n                init_latents = init_latents.repeat((batch_size, 1, 1, 1))\n            init_latents_orig = init_latents\n\n            # preprocess mask\n            if mask_image is not None:\n                mask = mask_image.to(device=self.device, dtype=latents_dtype)\n                if len(mask) == 1:\n                    mask = mask.repeat((batch_size, 1, 1, 1))\n\n                # check sizes\n                if not mask.shape == init_latents.shape:\n                    raise ValueError(\"The mask and init_image should be the same size!\")\n\n            # get the original timestep using init_timestep\n            offset = self.scheduler.config.get(\"steps_offset\", 0)\n            init_timestep = int(num_inference_steps * strength) + offset\n            init_timestep = min(init_timestep, num_inference_steps)\n\n            timesteps = self.scheduler.timesteps[-init_timestep]\n            timesteps = torch.tensor([timesteps] * batch_size * num_images_per_prompt, device=self.device)\n\n            # add noise to latents using the timesteps\n            latents = self.scheduler.add_noise(init_latents, img2img_noise, timesteps)\n\n            t_start = max(num_inference_steps - init_timestep + offset, 0)\n            timesteps = self.scheduler.timesteps[t_start:].to(self.device)\n\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        num_latent_input = (3 if negative_scale is not None else 2) if do_classifier_free_guidance else 1\n\n        if self.control_nets:\n            guided_hints = original_control_net.get_guided_hints(self.control_nets, num_latent_input, batch_size, clip_guide_images)\n            each_control_net_enabled = [self.control_net_enabled] * len(self.control_nets)\n\n        if self.control_net_lllites:\n            # guided_hints = original_control_net.get_guided_hints(self.control_nets, num_latent_input, batch_size, clip_guide_images)\n            if self.control_net_enabled:\n                for control_net, _ in self.control_net_lllites:\n                    with torch.no_grad():\n                        control_net.set_cond_image(clip_guide_images)\n            else:\n                for control_net, _ in self.control_net_lllites:\n                    control_net.set_cond_image(None)\n\n            each_control_net_enabled = [self.control_net_enabled] * len(self.control_net_lllites)\n\n        enable_gradual_latent = False\n        if self.gradual_latent:\n            if not hasattr(self.scheduler, \"set_gradual_latent_params\"):\n                logger.warning(\"gradual_latent is not supported for this scheduler. Ignoring.\")\n                logger.warning(f\"{self.scheduler.__class__.__name__}\")\n            else:\n                enable_gradual_latent = True\n                step_elapsed = 1000\n                current_ratio = self.gradual_latent.ratio\n\n                # first, we downscale the latents to the specified ratio / 最初に指定された比率にlatentsをダウンスケールする\n                height, width = latents.shape[-2:]\n                org_dtype = latents.dtype\n                if org_dtype == torch.bfloat16:\n                    latents = latents.float()\n                latents = torch.nn.functional.interpolate(\n                    latents, scale_factor=current_ratio, mode=\"bicubic\", align_corners=False\n                ).to(org_dtype)\n\n                # apply unsharp mask / アンシャープマスクを適用する\n                if self.gradual_latent.gaussian_blur_ksize:\n                    latents = self.gradual_latent.apply_unshark_mask(latents)\n\n        for i, t in enumerate(tqdm(timesteps)):\n            resized_size = None\n            if enable_gradual_latent:\n                # gradually upscale the latents / latentsを徐々にアップスケールする\n                if (\n                    t < self.gradual_latent.start_timesteps\n                    and current_ratio < 1.0\n                    and step_elapsed >= self.gradual_latent.every_n_steps\n                ):\n                    current_ratio = min(current_ratio + self.gradual_latent.ratio_step, 1.0)\n                    # make divisible by 8 because size of latents must be divisible at bottom of UNet\n                    h = int(height * current_ratio) // 8 * 8\n                    w = int(width * current_ratio) // 8 * 8\n                    resized_size = (h, w)\n                    self.scheduler.set_gradual_latent_params(resized_size, self.gradual_latent)\n                    step_elapsed = 0\n                else:\n                    self.scheduler.set_gradual_latent_params(None, None)\n                step_elapsed += 1\n\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = latents.repeat((num_latent_input, 1, 1, 1))\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n            # disable ControlNet-LLLite if ratio is set. ControlNet is disabled in ControlNetInfo\n            if self.control_net_lllites:\n                for j, ((control_net, ratio), enabled) in enumerate(zip(self.control_net_lllites, each_control_net_enabled)):\n                    if not enabled or ratio >= 1.0:\n                        continue\n                    if ratio < i / len(timesteps):\n                        logger.info(f\"ControlNetLLLite {j} is disabled (ratio={ratio} at {i} / {len(timesteps)})\")\n                        control_net.set_cond_image(None)\n                        each_control_net_enabled[j] = False\n\n            # predict the noise residual\n            if self.control_nets and self.control_net_enabled:\n                if regional_network:\n                    num_sub_and_neg_prompts = len(text_embeddings) // batch_size\n                    text_emb_last = text_embeddings[num_sub_and_neg_prompts - 2 :: num_sub_and_neg_prompts]  # last subprompt\n                else:\n                    text_emb_last = text_embeddings\n\n                noise_pred = original_control_net.call_unet_and_control_net(\n                    i,\n                    num_latent_input,\n                    self.unet,\n                    self.control_nets,\n                    guided_hints,\n                    i / len(timesteps),\n                    latent_model_input,\n                    t,\n                    text_embeddings,\n                    text_emb_last,\n                ).sample\n            elif self.is_sdxl:\n                noise_pred = self.unet(latent_model_input, t, text_embeddings, vector_embeddings)\n            else:\n                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                if negative_scale is None:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(num_latent_input)  # uncond by negative prompt\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n                else:\n                    noise_pred_negative, noise_pred_text, noise_pred_uncond = noise_pred.chunk(\n                        num_latent_input\n                    )  # uncond is real uncond\n                    noise_pred = (\n                        noise_pred_uncond\n                        + guidance_scale * (noise_pred_text - noise_pred_uncond)\n                        - negative_scale * (noise_pred_negative - noise_pred_uncond)\n                    )\n\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n            if mask is not None:\n                # masking\n                init_latents_proper = self.scheduler.add_noise(init_latents_orig, img2img_noise, torch.tensor([t]))\n                latents = (init_latents_proper * mask) + (latents * (1 - mask))\n\n            # call the callback, if provided\n            if i % callback_steps == 0:\n                if callback is not None:\n                    callback(i, t, latents)\n                if is_cancelled_callback is not None and is_cancelled_callback():\n                    return None\n\n        if return_latents:\n            return latents\n\n        latents = 1 / (sdxl_model_util.VAE_SCALE_FACTOR if self.is_sdxl else 0.18215) * latents\n        if vae_batch_size >= batch_size:\n            image = self.vae.decode(latents.to(self.vae.dtype)).sample\n        else:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            images = []\n            for i in tqdm(range(0, batch_size, vae_batch_size)):\n                images.append(\n                    self.vae.decode(\n                        (latents[i : i + vae_batch_size] if vae_batch_size > 1 else latents[i].unsqueeze(0)).to(self.vae.dtype)\n                    ).sample\n                )\n            image = torch.cat(images)\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        if output_type == \"pil\":\n            # image = self.numpy_to_pil(image)\n            image = (image * 255).round().astype(\"uint8\")\n            image = [Image.fromarray(im) for im in image]\n\n        return image\n\n        # return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n\n\nre_attention = re.compile(\n    r\"\"\"\n\\\\\\(|\n\\\\\\)|\n\\\\\\[|\n\\\\]|\n\\\\\\\\|\n\\\\|\n\\(|\n\\[|\n:([+-]?[.\\d]+)\\)|\n\\)|\n]|\n[^\\\\()\\[\\]:]+|\n:\n\"\"\",\n    re.X,\n)\n\n\ndef parse_prompt_attention(text):\n    \"\"\"\n    Parses a string with attention tokens and returns a list of pairs: text and its associated weight.\n    Accepted tokens are:\n      (abc) - increases attention to abc by a multiplier of 1.1\n      (abc:3.12) - increases attention to abc by a multiplier of 3.12\n      [abc] - decreases attention to abc by a multiplier of 1.1\n      \\( - literal character '('\n      \\[ - literal character '['\n      \\) - literal character ')'\n      \\] - literal character ']'\n      \\\\ - literal character '\\'\n      anything else - just text\n    >>> parse_prompt_attention('normal text')\n    [['normal text', 1.0]]\n    >>> parse_prompt_attention('an (important) word')\n    [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n    >>> parse_prompt_attention('(unbalanced')\n    [['unbalanced', 1.1]]\n    >>> parse_prompt_attention('\\(literal\\]')\n    [['(literal]', 1.0]]\n    >>> parse_prompt_attention('(unnecessary)(parens)')\n    [['unnecessaryparens', 1.1]]\n    >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n    [['a ', 1.0],\n     ['house', 1.5730000000000004],\n     [' ', 1.1],\n     ['on', 1.0],\n     [' a ', 1.1],\n     ['hill', 0.55],\n     [', sun, ', 1.1],\n     ['sky', 1.4641000000000006],\n     ['.', 1.1]]\n    \"\"\"\n\n    res = []\n    round_brackets = []\n    square_brackets = []\n\n    round_bracket_multiplier = 1.1\n    square_bracket_multiplier = 1 / 1.1\n\n    def multiply_range(start_position, multiplier):\n        for p in range(start_position, len(res)):\n            res[p][1] *= multiplier\n\n    # keep break as separate token\n    text = text.replace(\"BREAK\", \"\\\\BREAK\\\\\")\n\n    for m in re_attention.finditer(text):\n        text = m.group(0)\n        weight = m.group(1)\n\n        if text.startswith(\"\\\\\"):\n            res.append([text[1:], 1.0])\n        elif text == \"(\":\n            round_brackets.append(len(res))\n        elif text == \"[\":\n            square_brackets.append(len(res))\n        elif weight is not None and len(round_brackets) > 0:\n            multiply_range(round_brackets.pop(), float(weight))\n        elif text == \")\" and len(round_brackets) > 0:\n            multiply_range(round_brackets.pop(), round_bracket_multiplier)\n        elif text == \"]\" and len(square_brackets) > 0:\n            multiply_range(square_brackets.pop(), square_bracket_multiplier)\n        else:\n            res.append([text, 1.0])\n\n    for pos in round_brackets:\n        multiply_range(pos, round_bracket_multiplier)\n\n    for pos in square_brackets:\n        multiply_range(pos, square_bracket_multiplier)\n\n    if len(res) == 0:\n        res = [[\"\", 1.0]]\n\n    # merge runs of identical weights\n    i = 0\n    while i + 1 < len(res):\n        if res[i][1] == res[i + 1][1] and res[i][0].strip() != \"BREAK\" and res[i + 1][0].strip() != \"BREAK\":\n            res[i][0] += res[i + 1][0]\n            res.pop(i + 1)\n        else:\n            i += 1\n\n    return res\n\n\ndef get_prompts_with_weights(tokenizer: CLIPTokenizer, token_replacer, prompt: List[str], max_length: int):\n    r\"\"\"\n    Tokenize a list of prompts and return its tokens with weights of each token.\n    No padding, starting or ending token is included.\n    \"\"\"\n    tokens = []\n    weights = []\n    truncated = False\n\n    for text in prompt:\n        texts_and_weights = parse_prompt_attention(text)\n        text_token = []\n        text_weight = []\n        for word, weight in texts_and_weights:\n            if word.strip() == \"BREAK\":\n                # pad until next multiple of tokenizer's max token length\n                pad_len = tokenizer.model_max_length - (len(text_token) % tokenizer.model_max_length)\n                logger.info(f\"BREAK pad_len: {pad_len}\")\n                for i in range(pad_len):\n                    # v2のときEOSをつけるべきかどうかわからないぜ\n                    # if i == 0:\n                    #     text_token.append(tokenizer.eos_token_id)\n                    # else:\n                    text_token.append(tokenizer.pad_token_id)\n                    text_weight.append(1.0)\n                continue\n\n            # tokenize and discard the starting and the ending token\n            token = tokenizer(word).input_ids[1:-1]\n\n            token = token_replacer(token)  # for Textual Inversion\n\n            text_token += token\n            # copy the weight by length of token\n            text_weight += [weight] * len(token)\n            # stop if the text is too long (longer than truncation limit)\n            if len(text_token) > max_length:\n                truncated = True\n                break\n        # truncate\n        if len(text_token) > max_length:\n            truncated = True\n            text_token = text_token[:max_length]\n            text_weight = text_weight[:max_length]\n        tokens.append(text_token)\n        weights.append(text_weight)\n    if truncated:\n        logger.warning(\"warning: Prompt was truncated. Try to shorten the prompt or increase max_embeddings_multiples\")\n    return tokens, weights\n\n\ndef pad_tokens_and_weights(tokens, weights, max_length, bos, eos, pad, no_boseos_middle=True, chunk_length=77):\n    r\"\"\"\n    Pad the tokens (with starting and ending tokens) and weights (with 1.0) to max_length.\n    \"\"\"\n    max_embeddings_multiples = (max_length - 2) // (chunk_length - 2)\n    weights_length = max_length if no_boseos_middle else max_embeddings_multiples * chunk_length\n    for i in range(len(tokens)):\n        tokens[i] = [bos] + tokens[i] + [eos] + [pad] * (max_length - 2 - len(tokens[i]))\n        if no_boseos_middle:\n            weights[i] = [1.0] + weights[i] + [1.0] * (max_length - 1 - len(weights[i]))\n        else:\n            w = []\n            if len(weights[i]) == 0:\n                w = [1.0] * weights_length\n            else:\n                for j in range(max_embeddings_multiples):\n                    w.append(1.0)  # weight for starting token in this chunk\n                    w += weights[i][j * (chunk_length - 2) : min(len(weights[i]), (j + 1) * (chunk_length - 2))]\n                    w.append(1.0)  # weight for ending token in this chunk\n                w += [1.0] * (weights_length - len(w))\n            weights[i] = w[:]\n\n    return tokens, weights\n\n\ndef get_unweighted_text_embeddings(\n    is_sdxl: bool,\n    text_encoder: CLIPTextModel,\n    text_input: torch.Tensor,\n    chunk_length: int,\n    clip_skip: int,\n    eos: int,\n    pad: int,\n    no_boseos_middle: Optional[bool] = True,\n):\n    \"\"\"\n    When the length of tokens is a multiple of the capacity of the text encoder,\n    it should be split into chunks and sent to the text encoder individually.\n    \"\"\"\n    max_embeddings_multiples = (text_input.shape[1] - 2) // (chunk_length - 2)\n    if max_embeddings_multiples > 1:\n        text_embeddings = []\n        pool = None\n        for i in range(max_embeddings_multiples):\n            # extract the i-th chunk\n            text_input_chunk = text_input[:, i * (chunk_length - 2) : (i + 1) * (chunk_length - 2) + 2].clone()\n\n            # cover the head and the tail by the starting and the ending tokens\n            text_input_chunk[:, 0] = text_input[0, 0]\n            if pad == eos:  # v1\n                text_input_chunk[:, -1] = text_input[0, -1]\n            else:  # v2\n                for j in range(len(text_input_chunk)):\n                    if text_input_chunk[j, -1] != eos and text_input_chunk[j, -1] != pad:  # 最後に普通の文字がある\n                        text_input_chunk[j, -1] = eos\n                    if text_input_chunk[j, 1] == pad:  # BOSだけであとはPAD\n                        text_input_chunk[j, 1] = eos\n\n            # in sdxl, value of clip_skip is same for Text Encoder 1 and 2\n            enc_out = text_encoder(text_input_chunk, output_hidden_states=True, return_dict=True)\n            text_embedding = enc_out[\"hidden_states\"][-clip_skip]\n            if not is_sdxl:  # SD 1.5 requires final_layer_norm\n                text_embedding = text_encoder.text_model.final_layer_norm(text_embedding)\n            if pool is None:\n                pool = enc_out.get(\"text_embeds\", None)  # use 1st chunk, if provided\n                if pool is not None:\n                    pool = train_util.pool_workaround(text_encoder, enc_out[\"last_hidden_state\"], text_input_chunk, eos)\n\n            if no_boseos_middle:\n                if i == 0:\n                    # discard the ending token\n                    text_embedding = text_embedding[:, :-1]\n                elif i == max_embeddings_multiples - 1:\n                    # discard the starting token\n                    text_embedding = text_embedding[:, 1:]\n                else:\n                    # discard both starting and ending tokens\n                    text_embedding = text_embedding[:, 1:-1]\n\n            text_embeddings.append(text_embedding)\n        text_embeddings = torch.concat(text_embeddings, axis=1)\n    else:\n        enc_out = text_encoder(text_input, output_hidden_states=True, return_dict=True)\n        text_embeddings = enc_out[\"hidden_states\"][-clip_skip]\n        if not is_sdxl:  # SD 1.5 requires final_layer_norm\n            text_embeddings = text_encoder.text_model.final_layer_norm(text_embeddings)\n        pool = enc_out.get(\"text_embeds\", None)  # text encoder 1 doesn't return this\n        if pool is not None:\n            pool = train_util.pool_workaround(text_encoder, enc_out[\"last_hidden_state\"], text_input, eos)\n    return text_embeddings, pool\n\n\ndef get_weighted_text_embeddings(\n    is_sdxl: bool,\n    tokenizer: CLIPTokenizer,\n    text_encoder: CLIPTextModel,\n    prompt: Union[str, List[str]],\n    uncond_prompt: Optional[Union[str, List[str]]] = None,\n    max_embeddings_multiples: Optional[int] = 1,\n    no_boseos_middle: Optional[bool] = False,\n    skip_parsing: Optional[bool] = False,\n    skip_weighting: Optional[bool] = False,\n    clip_skip: int = 1,\n    token_replacer=None,\n    device=None,\n    emb_normalize_mode: Optional[str] = \"original\",  # \"original\", \"abs\", \"none\"\n    **kwargs,\n):\n    max_length = (tokenizer.model_max_length - 2) * max_embeddings_multiples + 2\n    if isinstance(prompt, str):\n        prompt = [prompt]\n\n    # split the prompts with \"AND\". each prompt must have the same number of splits\n    new_prompts = []\n    for p in prompt:\n        new_prompts.extend(p.split(\" AND \"))\n    prompt = new_prompts\n\n    if not skip_parsing:\n        prompt_tokens, prompt_weights = get_prompts_with_weights(tokenizer, token_replacer, prompt, max_length - 2)\n        if uncond_prompt is not None:\n            if isinstance(uncond_prompt, str):\n                uncond_prompt = [uncond_prompt]\n            uncond_tokens, uncond_weights = get_prompts_with_weights(tokenizer, token_replacer, uncond_prompt, max_length - 2)\n    else:\n        prompt_tokens = [token[1:-1] for token in tokenizer(prompt, max_length=max_length, truncation=True).input_ids]\n        prompt_weights = [[1.0] * len(token) for token in prompt_tokens]\n        if uncond_prompt is not None:\n            if isinstance(uncond_prompt, str):\n                uncond_prompt = [uncond_prompt]\n            uncond_tokens = [token[1:-1] for token in tokenizer(uncond_prompt, max_length=max_length, truncation=True).input_ids]\n            uncond_weights = [[1.0] * len(token) for token in uncond_tokens]\n\n    # round up the longest length of tokens to a multiple of (model_max_length - 2)\n    max_length = max([len(token) for token in prompt_tokens])\n    if uncond_prompt is not None:\n        max_length = max(max_length, max([len(token) for token in uncond_tokens]))\n\n    max_embeddings_multiples = min(\n        max_embeddings_multiples,\n        (max_length - 1) // (tokenizer.model_max_length - 2) + 1,\n    )\n    max_embeddings_multiples = max(1, max_embeddings_multiples)\n    max_length = (tokenizer.model_max_length - 2) * max_embeddings_multiples + 2\n\n    # pad the length of tokens and weights\n    bos = tokenizer.bos_token_id\n    eos = tokenizer.eos_token_id\n    pad = tokenizer.pad_token_id\n    prompt_tokens, prompt_weights = pad_tokens_and_weights(\n        prompt_tokens,\n        prompt_weights,\n        max_length,\n        bos,\n        eos,\n        pad,\n        no_boseos_middle=no_boseos_middle,\n        chunk_length=tokenizer.model_max_length,\n    )\n    prompt_tokens = torch.tensor(prompt_tokens, dtype=torch.long, device=device)\n    if uncond_prompt is not None:\n        uncond_tokens, uncond_weights = pad_tokens_and_weights(\n            uncond_tokens,\n            uncond_weights,\n            max_length,\n            bos,\n            eos,\n            pad,\n            no_boseos_middle=no_boseos_middle,\n            chunk_length=tokenizer.model_max_length,\n        )\n        uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n\n    # get the embeddings\n    text_embeddings, text_pool = get_unweighted_text_embeddings(\n        is_sdxl,\n        text_encoder,\n        prompt_tokens,\n        tokenizer.model_max_length,\n        clip_skip,\n        eos,\n        pad,\n        no_boseos_middle=no_boseos_middle,\n    )\n\n    prompt_weights = torch.tensor(prompt_weights, dtype=text_embeddings.dtype, device=device)\n    if uncond_prompt is not None:\n        uncond_embeddings, uncond_pool = get_unweighted_text_embeddings(\n            is_sdxl,\n            text_encoder,\n            uncond_tokens,\n            tokenizer.model_max_length,\n            clip_skip,\n            eos,\n            pad,\n            no_boseos_middle=no_boseos_middle,\n        )\n        uncond_weights = torch.tensor(uncond_weights, dtype=uncond_embeddings.dtype, device=device)\n\n    # assign weights to the prompts and normalize in the sense of mean\n    # TODO: should we normalize by chunk or in a whole (current implementation)?\n    # →全体でいいんじゃないかな\n\n    if (not skip_parsing) and (not skip_weighting):\n        if emb_normalize_mode == \"abs\":\n            previous_mean = text_embeddings.float().abs().mean(axis=[-2, -1]).to(text_embeddings.dtype)\n            text_embeddings *= prompt_weights.unsqueeze(-1)\n            current_mean = text_embeddings.float().abs().mean(axis=[-2, -1]).to(text_embeddings.dtype)\n            text_embeddings *= (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)\n            if uncond_prompt is not None:\n                previous_mean = uncond_embeddings.float().abs().mean(axis=[-2, -1]).to(uncond_embeddings.dtype)\n                uncond_embeddings *= uncond_weights.unsqueeze(-1)\n                current_mean = uncond_embeddings.float().abs().mean(axis=[-2, -1]).to(uncond_embeddings.dtype)\n                uncond_embeddings *= (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)\n\n        elif emb_normalize_mode == \"none\":\n            text_embeddings *= prompt_weights.unsqueeze(-1)\n            if uncond_prompt is not None:\n                uncond_embeddings *= uncond_weights.unsqueeze(-1)\n\n        else:  # \"original\"\n            previous_mean = text_embeddings.float().mean(axis=[-2, -1]).to(text_embeddings.dtype)\n            text_embeddings *= prompt_weights.unsqueeze(-1)\n            current_mean = text_embeddings.float().mean(axis=[-2, -1]).to(text_embeddings.dtype)\n            text_embeddings *= (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)\n            if uncond_prompt is not None:\n                previous_mean = uncond_embeddings.float().mean(axis=[-2, -1]).to(uncond_embeddings.dtype)\n                uncond_embeddings *= uncond_weights.unsqueeze(-1)\n                current_mean = uncond_embeddings.float().mean(axis=[-2, -1]).to(uncond_embeddings.dtype)\n                uncond_embeddings *= (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)\n\n    if uncond_prompt is not None:\n        return text_embeddings, text_pool, uncond_embeddings, uncond_pool, prompt_tokens\n    return text_embeddings, text_pool, None, None, prompt_tokens\n\n\ndef preprocess_image(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0\n\n\ndef preprocess_mask(mask):\n    mask = mask.convert(\"L\")\n    w, h = mask.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    mask = mask.resize((w // 8, h // 8), resample=PIL.Image.BILINEAR)  # LANCZOS)\n    mask = np.array(mask).astype(np.float32) / 255.0\n    mask = np.tile(mask, (4, 1, 1))\n    mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?\n    mask = 1 - mask  # repaint white, keep black\n    mask = torch.from_numpy(mask)\n    return mask\n\n\n# regular expression for dynamic prompt:\n# starts and ends with \"{\" and \"}\"\n# contains at least one variant divided by \"|\"\n# optional framgments divided by \"$$\" at start\n# if the first fragment is \"E\" or \"e\", enumerate all variants\n# if the second fragment is a number or two numbers, repeat the variants in the range\n# if the third fragment is a string, use it as a separator\n\nRE_DYNAMIC_PROMPT = re.compile(r\"\\{((e|E)\\$\\$)?(([\\d\\-]+)\\$\\$)?(([^\\|\\}]+?)\\$\\$)?(.+?((\\|).+?)*?)\\}\")\n\n\ndef handle_dynamic_prompt_variants(prompt, repeat_count):\n    founds = list(RE_DYNAMIC_PROMPT.finditer(prompt))\n    if not founds:\n        return [prompt]\n\n    # make each replacement for each variant\n    enumerating = False\n    replacers = []\n    for found in founds:\n        # if \"e$$\" is found, enumerate all variants\n        found_enumerating = found.group(2) is not None\n        enumerating = enumerating or found_enumerating\n\n        separator = \", \" if found.group(6) is None else found.group(6)\n        variants = found.group(7).split(\"|\")\n\n        # parse count range\n        count_range = found.group(4)\n        if count_range is None:\n            count_range = [1, 1]\n        else:\n            count_range = count_range.split(\"-\")\n            if len(count_range) == 1:\n                count_range = [int(count_range[0]), int(count_range[0])]\n            elif len(count_range) == 2:\n                count_range = [int(count_range[0]), int(count_range[1])]\n            else:\n                logger.warning(f\"invalid count range: {count_range}\")\n                count_range = [1, 1]\n            if count_range[0] > count_range[1]:\n                count_range = [count_range[1], count_range[0]]\n            if count_range[0] < 0:\n                count_range[0] = 0\n            if count_range[1] > len(variants):\n                count_range[1] = len(variants)\n\n        if found_enumerating:\n            # make function to enumerate all combinations\n            def make_replacer_enum(vari, cr, sep):\n                def replacer():\n                    values = []\n                    for count in range(cr[0], cr[1] + 1):\n                        for comb in itertools.combinations(vari, count):\n                            values.append(sep.join(comb))\n                    return values\n\n                return replacer\n\n            replacers.append(make_replacer_enum(variants, count_range, separator))\n        else:\n            # make function to choose random combinations\n            def make_replacer_single(vari, cr, sep):\n                def replacer():\n                    count = random.randint(cr[0], cr[1])\n                    comb = random.sample(vari, count)\n                    return [sep.join(comb)]\n\n                return replacer\n\n            replacers.append(make_replacer_single(variants, count_range, separator))\n\n    # make each prompt\n    if not enumerating:\n        # if not enumerating, repeat the prompt, replace each variant randomly\n        prompts = []\n        for _ in range(repeat_count):\n            current = prompt\n            for found, replacer in zip(founds, replacers):\n                current = current.replace(found.group(0), replacer()[0], 1)\n            prompts.append(current)\n    else:\n        # if enumerating, iterate all combinations for previous prompts\n        prompts = [prompt]\n\n        for found, replacer in zip(founds, replacers):\n            if found.group(2) is not None:\n                # make all combinations for existing prompts\n                new_prompts = []\n                for current in prompts:\n                    replecements = replacer()\n                    for replecement in replecements:\n                        new_prompts.append(current.replace(found.group(0), replecement, 1))\n                prompts = new_prompts\n\n        for found, replacer in zip(founds, replacers):\n            # make random selection for existing prompts\n            if found.group(2) is None:\n                for i in range(len(prompts)):\n                    prompts[i] = prompts[i].replace(found.group(0), replacer()[0], 1)\n\n    return prompts\n\n\n# endregion\n\n# def load_clip_l14_336(dtype):\n#   print(f\"loading CLIP: {CLIP_ID_L14_336}\")\n#   text_encoder = CLIPTextModel.from_pretrained(CLIP_ID_L14_336, torch_dtype=dtype)\n#   return text_encoder\n\n\nclass BatchDataBase(NamedTuple):\n    # バッチ分割が必要ないデータ\n    step: int\n    prompt: str\n    negative_prompt: str\n    seed: int\n    init_image: Any\n    mask_image: Any\n    clip_prompt: str\n    guide_image: Any\n    raw_prompt: str\n\n\nclass BatchDataExt(NamedTuple):\n    # バッチ分割が必要なデータ\n    width: int\n    height: int\n    original_width: int\n    original_height: int\n    original_width_negative: int\n    original_height_negative: int\n    crop_left: int\n    crop_top: int\n    steps: int\n    scale: float\n    negative_scale: float\n    strength: float\n    network_muls: Tuple[float]\n    num_sub_prompts: int\n\n\nclass BatchData(NamedTuple):\n    return_latents: bool\n    base: BatchDataBase\n    ext: BatchDataExt\n\n\nclass ListPrompter:\n    def __init__(self, prompts: List[str]):\n        self.prompts = prompts\n        self.index = 0\n\n    def shuffle(self):\n        random.shuffle(self.prompts)\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __call__(self, *args, **kwargs):\n        if self.index >= len(self.prompts):\n            self.index = 0  # reset\n            return None\n\n        prompt = self.prompts[self.index]\n        self.index += 1\n        return prompt\n\n\ndef main(args):\n    if args.fp16:\n        dtype = torch.float16\n    elif args.bf16:\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n\n    highres_fix = args.highres_fix_scale is not None\n    # assert not highres_fix or args.image_path is None, f\"highres_fix doesn't work with img2img / highres_fixはimg2imgと同時に使えません\"\n\n    if args.v_parameterization and not args.v2:\n        logger.warning(\"v_parameterization should be with v2 / v1でv_parameterizationを使用することは想定されていません\")\n    if args.v2 and args.clip_skip is not None:\n        logger.warning(\"v2 with clip_skip will be unexpected / v2でclip_skipを使用することは想定されていません\")\n\n    # モデルを読み込む\n    if not os.path.exists(args.ckpt):  # ファイルがないならパターンで探し、一つだけ該当すればそれを使う\n        files = glob.glob(args.ckpt)\n        if len(files) == 1:\n            args.ckpt = files[0]\n\n    name_or_path = os.readlink(args.ckpt) if os.path.islink(args.ckpt) else args.ckpt\n    use_stable_diffusion_format = os.path.isfile(name_or_path)  # determine SD or Diffusers\n\n    # SDXLかどうかを判定する\n    is_sdxl = args.sdxl\n    if not is_sdxl and not args.v1 and not args.v2:  # どれも指定されていない場合は自動で判定する\n        if use_stable_diffusion_format:\n            # if file size > 5.5GB, sdxl\n            is_sdxl = os.path.getsize(name_or_path) > 5.5 * 1024**3\n        else:\n            # if `text_encoder_2` subdirectory exists, sdxl\n            is_sdxl = os.path.isdir(os.path.join(name_or_path, \"text_encoder_2\"))\n    logger.info(f\"SDXL: {is_sdxl}\")\n\n    if is_sdxl:\n        if args.clip_skip is None:\n            args.clip_skip = 2\n\n        (_, text_encoder1, text_encoder2, vae, unet, _, _) = sdxl_train_util._load_target_model(\n            args.ckpt, args.vae, sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, dtype\n        )\n        unet: InferSdxlUNet2DConditionModel = InferSdxlUNet2DConditionModel(unet)\n        text_encoders = [text_encoder1, text_encoder2]\n    else:\n        if args.clip_skip is None:\n            args.clip_skip = 2 if args.v2 else 1\n\n        if use_stable_diffusion_format:\n            logger.info(\"load StableDiffusion checkpoint\")\n            text_encoder, vae, unet = model_util.load_models_from_stable_diffusion_checkpoint(args.v2, args.ckpt)\n        else:\n            logger.info(\"load Diffusers pretrained models\")\n            loading_pipe = StableDiffusionPipeline.from_pretrained(args.ckpt, safety_checker=None, torch_dtype=dtype)\n            text_encoder = loading_pipe.text_encoder\n            vae = loading_pipe.vae\n            unet = loading_pipe.unet\n            tokenizer = loading_pipe.tokenizer\n            del loading_pipe\n\n            # Diffusers U-Net to original U-Net\n            original_unet = UNet2DConditionModel(\n                unet.config.sample_size,\n                unet.config.attention_head_dim,\n                unet.config.cross_attention_dim,\n                unet.config.use_linear_projection,\n                unet.config.upcast_attention,\n            )\n            original_unet.load_state_dict(unet.state_dict())\n            unet = original_unet\n        unet: InferUNet2DConditionModel = InferUNet2DConditionModel(unet)\n        text_encoders = [text_encoder]\n\n        # VAEを読み込む\n        if args.vae is not None:\n            vae = model_util.load_vae(args.vae, dtype)\n            logger.info(\"additional VAE loaded\")\n\n    # xformers、Hypernetwork対応\n    if not args.diffusers_xformers:\n        mem_eff = not (args.xformers or args.sdpa)\n        replace_unet_modules(unet, mem_eff, args.xformers, args.sdpa)\n        replace_vae_modules(vae, mem_eff, args.xformers, args.sdpa)\n\n    # tokenizerを読み込む\n    logger.info(\"loading tokenizer\")\n    if is_sdxl:\n        tokenizer1, tokenizer2 = sdxl_train_util.load_tokenizers(args)\n        tokenizers = [tokenizer1, tokenizer2]\n    else:\n        if use_stable_diffusion_format:\n            tokenizer = train_util.load_tokenizer(args)\n        tokenizers = [tokenizer]\n\n    # schedulerを用意する\n    sched_init_args = {}\n    has_steps_offset = True\n    has_clip_sample = True\n    scheduler_num_noises_per_step = 1\n\n    if args.sampler == \"ddim\":\n        scheduler_cls = DDIMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_ddim\n    elif args.sampler == \"ddpm\":  # ddpmはおかしくなるのでoptionから外してある\n        scheduler_cls = DDPMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_ddpm\n    elif args.sampler == \"pndm\":\n        scheduler_cls = PNDMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_pndm\n        has_clip_sample = False\n    elif args.sampler == \"lms\" or args.sampler == \"k_lms\":\n        scheduler_cls = LMSDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_lms_discrete\n        has_clip_sample = False\n    elif args.sampler == \"euler\" or args.sampler == \"k_euler\":\n        scheduler_cls = EulerDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_euler_discrete\n        has_clip_sample = False\n    elif args.sampler == \"euler_a\" or args.sampler == \"k_euler_a\":\n        scheduler_cls = EulerAncestralDiscreteSchedulerGL\n        scheduler_module = diffusers.schedulers.scheduling_euler_ancestral_discrete\n        has_clip_sample = False\n    elif args.sampler == \"dpmsolver\" or args.sampler == \"dpmsolver++\":\n        scheduler_cls = DPMSolverMultistepScheduler\n        sched_init_args[\"algorithm_type\"] = args.sampler\n        scheduler_module = diffusers.schedulers.scheduling_dpmsolver_multistep\n        has_clip_sample = False\n    elif args.sampler == \"dpmsingle\":\n        scheduler_cls = DPMSolverSinglestepScheduler\n        scheduler_module = diffusers.schedulers.scheduling_dpmsolver_singlestep\n        has_clip_sample = False\n        has_steps_offset = False\n    elif args.sampler == \"heun\":\n        scheduler_cls = HeunDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_heun_discrete\n        has_clip_sample = False\n    elif args.sampler == \"dpm_2\" or args.sampler == \"k_dpm_2\":\n        scheduler_cls = KDPM2DiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_k_dpm_2_discrete\n        has_clip_sample = False\n    elif args.sampler == \"dpm_2_a\" or args.sampler == \"k_dpm_2_a\":\n        scheduler_cls = KDPM2AncestralDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete\n        scheduler_num_noises_per_step = 2\n        has_clip_sample = False\n\n    if args.v_parameterization:\n        sched_init_args[\"prediction_type\"] = \"v_prediction\"\n\n    # 警告を出さないようにする\n    if has_steps_offset:\n        sched_init_args[\"steps_offset\"] = 1\n    if has_clip_sample:\n        sched_init_args[\"clip_sample\"] = False\n\n    # samplerの乱数をあらかじめ指定するための処理\n\n    # replace randn\n    class NoiseManager:\n        def __init__(self):\n            self.sampler_noises = None\n            self.sampler_noise_index = 0\n\n        def reset_sampler_noises(self, noises):\n            self.sampler_noise_index = 0\n            self.sampler_noises = noises\n\n        def randn(self, shape, device=None, dtype=None, layout=None, generator=None):\n            # print(\"replacing\", shape, len(self.sampler_noises), self.sampler_noise_index)\n            if self.sampler_noises is not None and self.sampler_noise_index < len(self.sampler_noises):\n                noise = self.sampler_noises[self.sampler_noise_index]\n                if shape != noise.shape:\n                    noise = None\n            else:\n                noise = None\n\n            if noise == None:\n                logger.warning(f\"unexpected noise request: {self.sampler_noise_index}, {shape}\")\n                noise = torch.randn(shape, dtype=dtype, device=device, generator=generator)\n\n            self.sampler_noise_index += 1\n            return noise\n\n    class TorchRandReplacer:\n        def __init__(self, noise_manager):\n            self.noise_manager = noise_manager\n\n        def __getattr__(self, item):\n            if item == \"randn\":\n                return self.noise_manager.randn\n            if hasattr(torch, item):\n                return getattr(torch, item)\n            raise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, item))\n\n    noise_manager = NoiseManager()\n    if scheduler_module is not None:\n        scheduler_module.torch = TorchRandReplacer(noise_manager)\n\n    scheduler = scheduler_cls(\n        num_train_timesteps=SCHEDULER_TIMESTEPS,\n        beta_start=SCHEDULER_LINEAR_START,\n        beta_end=SCHEDULER_LINEAR_END,\n        beta_schedule=SCHEDLER_SCHEDULE,\n        **sched_init_args,\n    )\n\n    # ↓以下は結局PipeでFalseに設定されるので意味がなかった\n    # # clip_sample=Trueにする\n    # if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is False:\n    #     print(\"set clip_sample to True\")\n    #     scheduler.config.clip_sample = True\n\n    # deviceを決定する\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # \"mps\"を考量してない\n\n    # custom pipelineをコピったやつを生成する\n    if args.vae_slices:\n        from library.slicing_vae import SlicingAutoencoderKL\n\n        sli_vae = SlicingAutoencoderKL(\n            act_fn=\"silu\",\n            block_out_channels=(128, 256, 512, 512),\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            in_channels=3,\n            latent_channels=4,\n            layers_per_block=2,\n            norm_num_groups=32,\n            out_channels=3,\n            sample_size=512,\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            num_slices=args.vae_slices,\n        )\n        sli_vae.load_state_dict(vae.state_dict())  # vaeのパラメータをコピーする\n        vae = sli_vae\n        del sli_vae\n\n    vae_dtype = dtype\n    if args.no_half_vae:\n        logger.info(\"set vae_dtype to float32\")\n        vae_dtype = torch.float32\n    vae.to(vae_dtype).to(device)\n    vae.eval()\n\n    for text_encoder in text_encoders:\n        text_encoder.to(dtype).to(device)\n        text_encoder.eval()\n    unet.to(dtype).to(device)\n    unet.eval()\n\n    # networkを組み込む\n    if args.network_module:\n        networks = []\n        network_default_muls = []\n        network_pre_calc = args.network_pre_calc\n\n        # merge関連の引数を統合する\n        if args.network_merge:\n            network_merge = len(args.network_module)  # all networks are merged\n        elif args.network_merge_n_models:\n            network_merge = args.network_merge_n_models\n        else:\n            network_merge = 0\n        logger.info(f\"network_merge: {network_merge}\")\n\n        for i, network_module in enumerate(args.network_module):\n            logger.info(\"import network module: {network_module}\")\n            imported_module = importlib.import_module(network_module)\n\n            network_mul = 1.0 if args.network_mul is None or len(args.network_mul) <= i else args.network_mul[i]\n\n            net_kwargs = {}\n            if args.network_args and i < len(args.network_args):\n                network_args = args.network_args[i]\n                # TODO escape special chars\n                network_args = network_args.split(\";\")\n                for net_arg in network_args:\n                    key, value = net_arg.split(\"=\")\n                    net_kwargs[key] = value\n\n            if args.network_weights is None or len(args.network_weights) <= i:\n                raise ValueError(\"No weight. Weight is required.\")\n\n            network_weight = args.network_weights[i]\n            logger.info(f\"load network weights from: {network_weight}\")\n\n            if model_util.is_safetensors(network_weight) and args.network_show_meta:\n                from safetensors.torch import safe_open\n\n                with safe_open(network_weight, framework=\"pt\") as f:\n                    metadata = f.metadata()\n                if metadata is not None:\n                    logger.info(f\"metadata for: {network_weight}: {metadata}\")\n\n            network, weights_sd = imported_module.create_network_from_weights(\n                network_mul, network_weight, vae, text_encoders, unet, for_inference=True, **net_kwargs\n            )\n            if network is None:\n                return\n\n            mergeable = network.is_mergeable()\n            if network_merge and not mergeable:\n                logger.warning(\"network is not mergiable. ignore merge option.\")\n\n            if not mergeable or i >= network_merge:\n                # not merging\n                network.apply_to(text_encoders, unet)\n                info = network.load_state_dict(weights_sd, False)  # network.load_weightsを使うようにするとよい\n                logger.info(f\"weights are loaded: {info}\")\n\n                if args.opt_channels_last:\n                    network.to(memory_format=torch.channels_last)\n                network.to(dtype).to(device)\n\n                if network_pre_calc:\n                    logger.info(\"backup original weights\")\n                    network.backup_weights()\n\n                networks.append(network)\n                network_default_muls.append(network_mul)\n            else:\n                network.merge_to(text_encoders, unet, weights_sd, dtype, device)\n\n    else:\n        networks = []\n\n    # upscalerの指定があれば取得する\n    upscaler = None\n    if args.highres_fix_upscaler:\n        logger.info(\"import upscaler module: {args.highres_fix_upscaler}\")\n        imported_module = importlib.import_module(args.highres_fix_upscaler)\n\n        us_kwargs = {}\n        if args.highres_fix_upscaler_args:\n            for net_arg in args.highres_fix_upscaler_args.split(\";\"):\n                key, value = net_arg.split(\"=\")\n                us_kwargs[key] = value\n\n        logger.info(\"create upscaler\")\n        upscaler = imported_module.create_upscaler(**us_kwargs)\n        upscaler.to(dtype).to(device)\n\n    # ControlNetの処理\n    control_nets: List[ControlNetInfo] = []\n    if args.control_net_models:\n        for i, model in enumerate(args.control_net_models):\n            prep_type = None if not args.control_net_preps or len(args.control_net_preps) <= i else args.control_net_preps[i]\n            weight = 1.0 if not args.control_net_weights or len(args.control_net_weights) <= i else args.control_net_weights[i]\n            ratio = 1.0 if not args.control_net_ratios or len(args.control_net_ratios) <= i else args.control_net_ratios[i]\n\n            ctrl_unet, ctrl_net = original_control_net.load_control_net(args.v2, unet, model)\n            prep = original_control_net.load_preprocess(prep_type)\n            control_nets.append(ControlNetInfo(ctrl_unet, ctrl_net, prep, weight, ratio))\n\n    control_net_lllites: List[Tuple[ControlNetLLLite, float]] = []\n    if args.control_net_lllite_models:\n        for i, model_file in enumerate(args.control_net_lllite_models):\n            logger.info(f\"loading ControlNet-LLLite: {model_file}\")\n\n            from safetensors.torch import load_file\n\n            state_dict = load_file(model_file)\n            mlp_dim = None\n            cond_emb_dim = None\n            for key, value in state_dict.items():\n                if mlp_dim is None and \"down.0.weight\" in key:\n                    mlp_dim = value.shape[0]\n                elif cond_emb_dim is None and \"conditioning1.0\" in key:\n                    cond_emb_dim = value.shape[0] * 2\n                if mlp_dim is not None and cond_emb_dim is not None:\n                    break\n            assert mlp_dim is not None and cond_emb_dim is not None, f\"invalid control net: {model_file}\"\n\n            multiplier = (\n                1.0\n                if not args.control_net_multipliers or len(args.control_net_multipliers) <= i\n                else args.control_net_multipliers[i]\n            )\n            ratio = 1.0 if not args.control_net_ratios or len(args.control_net_ratios) <= i else args.control_net_ratios[i]\n\n            control_net_lllite = ControlNetLLLite(unet, cond_emb_dim, mlp_dim, multiplier=multiplier)\n            control_net_lllite.apply_to()\n            control_net_lllite.load_state_dict(state_dict)\n            control_net_lllite.to(dtype).to(device)\n            control_net_lllite.set_batch_cond_only(False, False)\n            control_net_lllites.append((control_net_lllite, ratio))\n    assert (\n        len(control_nets) == 0 or len(control_net_lllites) == 0\n    ), \"ControlNet and ControlNet-LLLite cannot be used at the same time\"\n\n    if args.opt_channels_last:\n        logger.info(f\"set optimizing: channels last\")\n        for text_encoder in text_encoders:\n            text_encoder.to(memory_format=torch.channels_last)\n        vae.to(memory_format=torch.channels_last)\n        unet.to(memory_format=torch.channels_last)\n        if networks:\n            for network in networks:\n                network.to(memory_format=torch.channels_last)\n\n        for cn in control_nets:\n            cn.to(memory_format=torch.channels_last)\n\n        for cn in control_net_lllites:\n            cn.to(memory_format=torch.channels_last)\n\n    pipe = PipelineLike(\n        is_sdxl,\n        device,\n        vae,\n        text_encoders,\n        tokenizers,\n        unet,\n        scheduler,\n        args.clip_skip,\n    )\n    pipe.set_control_nets(control_nets)\n    pipe.set_control_net_lllites(control_net_lllites)\n    logger.info(\"pipeline is ready.\")\n\n    if args.diffusers_xformers:\n        pipe.enable_xformers_memory_efficient_attention()\n\n    # Deep Shrink\n    if args.ds_depth_1 is not None:\n        unet.set_deep_shrink(args.ds_depth_1, args.ds_timesteps_1, args.ds_depth_2, args.ds_timesteps_2, args.ds_ratio)\n\n    # Gradual Latent\n    if args.gradual_latent_timesteps is not None:\n        if args.gradual_latent_unsharp_params:\n            us_params = args.gradual_latent_unsharp_params.split(\",\")\n            us_ksize, us_sigma, us_strength = [float(v) for v in us_params[:3]]\n            us_target_x = True if len(us_params) <= 3 else bool(int(us_params[3]))\n            us_ksize = int(us_ksize)\n        else:\n            us_ksize, us_sigma, us_strength, us_target_x = None, None, None, None\n\n        gradual_latent = GradualLatent(\n            args.gradual_latent_ratio,\n            args.gradual_latent_timesteps,\n            args.gradual_latent_every_n_steps,\n            args.gradual_latent_ratio_step,\n            args.gradual_latent_s_noise,\n            us_ksize,\n            us_sigma,\n            us_strength,\n            us_target_x,\n        )\n        pipe.set_gradual_latent(gradual_latent)\n\n    #  Textual Inversionを処理する\n    if args.textual_inversion_embeddings:\n        token_ids_embeds1 = []\n        token_ids_embeds2 = []\n        for embeds_file in args.textual_inversion_embeddings:\n            if model_util.is_safetensors(embeds_file):\n                from safetensors.torch import load_file\n\n                data = load_file(embeds_file)\n            else:\n                data = torch.load(embeds_file, map_location=\"cpu\")\n\n            if \"string_to_param\" in data:\n                data = data[\"string_to_param\"]\n            if is_sdxl:\n\n                embeds1 = data[\"clip_l\"]  # text encoder 1\n                embeds2 = data[\"clip_g\"]  # text encoder 2\n            else:\n                embeds1 = next(iter(data.values()))\n                embeds2 = None\n\n            num_vectors_per_token = embeds1.size()[0]\n            token_string = os.path.splitext(os.path.basename(embeds_file))[0]\n\n            token_strings = [token_string] + [f\"{token_string}{i+1}\" for i in range(num_vectors_per_token - 1)]\n\n            # add new word to tokenizer, count is num_vectors_per_token\n            num_added_tokens1 = tokenizers[0].add_tokens(token_strings)\n            num_added_tokens2 = tokenizers[1].add_tokens(token_strings) if is_sdxl else 0\n            assert num_added_tokens1 == num_vectors_per_token and (\n                num_added_tokens2 == 0 or num_added_tokens2 == num_vectors_per_token\n            ), (\n                f\"tokenizer has same word to token string (filename): {embeds_file}\"\n                + f\" / 指定した名前（ファイル名）のトークンが既に存在します: {embeds_file}\"\n            )\n\n            token_ids1 = tokenizers[0].convert_tokens_to_ids(token_strings)\n            token_ids2 = tokenizers[1].convert_tokens_to_ids(token_strings) if is_sdxl else None\n            logger.info(f\"Textual Inversion embeddings `{token_string}` loaded. Tokens are added: {token_ids1} and {token_ids2}\")\n            assert (\n                min(token_ids1) == token_ids1[0] and token_ids1[-1] == token_ids1[0] + len(token_ids1) - 1\n            ), f\"token ids1 is not ordered\"\n            assert not is_sdxl or (\n                min(token_ids2) == token_ids2[0] and token_ids2[-1] == token_ids2[0] + len(token_ids2) - 1\n            ), f\"token ids2 is not ordered\"\n            assert len(tokenizers[0]) - 1 == token_ids1[-1], f\"token ids 1 is not end of tokenize: {len(tokenizers[0])}\"\n            assert (\n                not is_sdxl or len(tokenizers[1]) - 1 == token_ids2[-1]\n            ), f\"token ids 2 is not end of tokenize: {len(tokenizers[1])}\"\n\n            if num_vectors_per_token > 1:\n                pipe.add_token_replacement(0, token_ids1[0], token_ids1)  # hoge -> hoge, hogea, hogeb, ...\n                if is_sdxl:\n                    pipe.add_token_replacement(1, token_ids2[0], token_ids2)\n\n            token_ids_embeds1.append((token_ids1, embeds1))\n            if is_sdxl:\n                token_ids_embeds2.append((token_ids2, embeds2))\n\n        text_encoders[0].resize_token_embeddings(len(tokenizers[0]))\n        token_embeds1 = text_encoders[0].get_input_embeddings().weight.data\n        for token_ids, embeds in token_ids_embeds1:\n            for token_id, embed in zip(token_ids, embeds):\n                token_embeds1[token_id] = embed\n\n        if is_sdxl:\n            text_encoders[1].resize_token_embeddings(len(tokenizers[1]))\n            token_embeds2 = text_encoders[1].get_input_embeddings().weight.data\n            for token_ids, embeds in token_ids_embeds2:\n                for token_id, embed in zip(token_ids, embeds):\n                    token_embeds2[token_id] = embed\n\n    # promptを取得する\n    prompt_list = None\n    if args.from_file is not None:\n        logger.info(f\"reading prompts from {args.from_file}\")\n        with open(args.from_file, \"r\", encoding=\"utf-8\") as f:\n            prompt_list = f.read().splitlines()\n            prompt_list = [d for d in prompt_list if len(d.strip()) > 0 and d[0] != \"#\"]\n        prompter = ListPrompter(prompt_list)\n\n    elif args.from_module is not None:\n\n        def load_module_from_path(module_name, file_path):\n            spec = importlib.util.spec_from_file_location(module_name, file_path)\n            if spec is None:\n                raise ImportError(f\"Module '{module_name}' cannot be loaded from '{file_path}'\")\n            module = importlib.util.module_from_spec(spec)\n            sys.modules[module_name] = module\n            spec.loader.exec_module(module)\n            return module\n\n        logger.info(f\"reading prompts from module: {args.from_module}\")\n        prompt_module = load_module_from_path(\"prompt_module\", args.from_module)\n\n        prompter = prompt_module.get_prompter(args, pipe, networks)\n\n    elif args.prompt is not None:\n        prompter = ListPrompter([args.prompt])\n\n    else:\n        prompter = None  # interactive mode\n\n    if args.interactive:\n        args.n_iter = 1\n\n    # img2imgの前処理、画像の読み込みなど\n    def load_images(path):\n        if os.path.isfile(path):\n            paths = [path]\n        else:\n            paths = (\n                glob.glob(os.path.join(path, \"*.png\"))\n                + glob.glob(os.path.join(path, \"*.jpg\"))\n                + glob.glob(os.path.join(path, \"*.jpeg\"))\n                + glob.glob(os.path.join(path, \"*.webp\"))\n            )\n            paths.sort()\n\n        images = []\n        for p in paths:\n            image = Image.open(p)\n            if image.mode != \"RGB\":\n                logger.info(f\"convert image to RGB from {image.mode}: {p}\")\n                image = image.convert(\"RGB\")\n            images.append(image)\n\n        return images\n\n    def resize_images(imgs, size):\n        resized = []\n        for img in imgs:\n            r_img = img.resize(size, Image.Resampling.LANCZOS)\n            if hasattr(img, \"filename\"):  # filename属性がない場合があるらしい\n                r_img.filename = img.filename\n            resized.append(r_img)\n        return resized\n\n    if args.image_path is not None:\n        logger.info(f\"load image for img2img: {args.image_path}\")\n        init_images = load_images(args.image_path)\n        assert len(init_images) > 0, f\"No image / 画像がありません: {args.image_path}\"\n        logger.info(f\"loaded {len(init_images)} images for img2img\")\n\n        # CLIP Vision\n        if args.clip_vision_strength is not None:\n            logger.info(f\"load CLIP Vision model: {CLIP_VISION_MODEL}\")\n            vision_model = CLIPVisionModelWithProjection.from_pretrained(CLIP_VISION_MODEL, projection_dim=1280)\n            vision_model.to(device, dtype)\n            processor = CLIPImageProcessor.from_pretrained(CLIP_VISION_MODEL)\n\n            pipe.clip_vision_model = vision_model\n            pipe.clip_vision_processor = processor\n            pipe.clip_vision_strength = args.clip_vision_strength\n            logger.info(f\"CLIP Vision model loaded.\")\n\n    else:\n        init_images = None\n\n    if args.mask_path is not None:\n        logger.info(f\"load mask for inpainting: {args.mask_path}\")\n        mask_images = load_images(args.mask_path)\n        assert len(mask_images) > 0, f\"No mask image / マスク画像がありません: {args.image_path}\"\n        logger.info(f\"loaded {len(mask_images)} mask images for inpainting\")\n    else:\n        mask_images = None\n\n    # promptがないとき、画像のPngInfoから取得する\n    if init_images is not None and prompter is None and not args.interactive:\n        logger.info(\"get prompts from images' metadata\")\n        prompt_list = []\n        for img in init_images:\n            if \"prompt\" in img.text:\n                prompt = img.text[\"prompt\"]\n                if \"negative-prompt\" in img.text:\n                    prompt += \" --n \" + img.text[\"negative-prompt\"]\n                prompt_list.append(prompt)\n        prompter = ListPrompter(prompt_list)\n\n        # プロンプトと画像を一致させるため指定回数だけ繰り返す（画像を増幅する）\n        l = []\n        for im in init_images:\n            l.extend([im] * args.images_per_prompt)\n        init_images = l\n\n        if mask_images is not None:\n            l = []\n            for im in mask_images:\n                l.extend([im] * args.images_per_prompt)\n            mask_images = l\n\n    # 画像サイズにオプション指定があるときはリサイズする\n    if args.W is not None and args.H is not None:\n        # highres fix を考慮に入れる\n        w, h = args.W, args.H\n        if highres_fix:\n            w = int(w * args.highres_fix_scale + 0.5)\n            h = int(h * args.highres_fix_scale + 0.5)\n\n        if init_images is not None:\n            logger.info(f\"resize img2img source images to {w}*{h}\")\n            init_images = resize_images(init_images, (w, h))\n        if mask_images is not None:\n            logger.info(f\"resize img2img mask images to {w}*{h}\")\n            mask_images = resize_images(mask_images, (w, h))\n\n    regional_network = False\n    if networks and mask_images:\n        # mask を領域情報として流用する、現在は一回のコマンド呼び出しで1枚だけ対応\n        regional_network = True\n        logger.info(\"use mask as region\")\n\n        size = None\n        for i, network in enumerate(networks):\n            if (i < 3 and args.network_regional_mask_max_color_codes is None) or i < args.network_regional_mask_max_color_codes:\n                np_mask = np.array(mask_images[0])\n\n                if args.network_regional_mask_max_color_codes:\n                    # カラーコードでマスクを指定する\n                    ch0 = (i + 1) & 1\n                    ch1 = ((i + 1) >> 1) & 1\n                    ch2 = ((i + 1) >> 2) & 1\n                    np_mask = np.all(np_mask == np.array([ch0, ch1, ch2]) * 255, axis=2)\n                    np_mask = np_mask.astype(np.uint8) * 255\n                else:\n                    np_mask = np_mask[:, :, i]\n                size = np_mask.shape\n            else:\n                np_mask = np.full(size, 255, dtype=np.uint8)\n            mask = torch.from_numpy(np_mask.astype(np.float32) / 255.0)\n            network.set_region(i, i == len(networks) - 1, mask)\n        mask_images = None\n\n    prev_image = None  # for VGG16 guided\n    if args.guide_image_path is not None:\n        logger.info(f\"load image for ControlNet guidance: {args.guide_image_path}\")\n        guide_images = []\n        for p in args.guide_image_path:\n            guide_images.extend(load_images(p))\n\n        logger.info(f\"loaded {len(guide_images)} guide images for guidance\")\n        if len(guide_images) == 0:\n            logger.warning(\n                f\"No guide image, use previous generated image. / ガイド画像がありません。直前に生成した画像を使います: {args.image_path}\"\n            )\n            guide_images = None\n    else:\n        guide_images = None\n\n    # 新しい乱数生成器を作成する\n    if args.seed is not None:\n        if prompt_list and len(prompt_list) == 1 and args.images_per_prompt == 1:\n            # 引数のseedをそのまま使う\n            def fixed_seed(*args, **kwargs):\n                return args.seed\n\n            seed_random = SimpleNamespace(randint=fixed_seed)\n        else:\n            seed_random = random.Random(args.seed)\n    else:\n        seed_random = random.Random()\n\n    # デフォルト画像サイズを設定する：img2imgではこれらの値は無視される（またはW*Hにリサイズ済み）\n    if args.W is None:\n        args.W = 1024 if is_sdxl else 512\n    if args.H is None:\n        args.H = 1024 if is_sdxl else 512\n\n    # 画像生成のループ\n    os.makedirs(args.outdir, exist_ok=True)\n    max_embeddings_multiples = 1 if args.max_embeddings_multiples is None else args.max_embeddings_multiples\n\n    for gen_iter in range(args.n_iter):\n        logger.info(f\"iteration {gen_iter+1}/{args.n_iter}\")\n        if args.iter_same_seed:\n            iter_seed = seed_random.randint(0, 2**32 - 1)\n        else:\n            iter_seed = None\n\n        # shuffle prompt list\n        if args.shuffle_prompts:\n            prompter.shuffle()\n\n        # バッチ処理の関数\n        def process_batch(batch: List[BatchData], highres_fix, highres_1st=False):\n            batch_size = len(batch)\n\n            # highres_fixの処理\n            if highres_fix and not highres_1st:\n                # 1st stageのバッチを作成して呼び出す：サイズを小さくして呼び出す\n                is_1st_latent = upscaler.support_latents() if upscaler else args.highres_fix_latents_upscaling\n\n                logger.info(\"process 1st stage\")\n                batch_1st = []\n                for _, base, ext in batch:\n\n                    def scale_and_round(x):\n                        if x is None:\n                            return None\n                        return int(x * args.highres_fix_scale + 0.5)\n\n                    width_1st = scale_and_round(ext.width)\n                    height_1st = scale_and_round(ext.height)\n                    width_1st = width_1st - width_1st % 32\n                    height_1st = height_1st - height_1st % 32\n\n                    original_width_1st = scale_and_round(ext.original_width)\n                    original_height_1st = scale_and_round(ext.original_height)\n                    original_width_negative_1st = scale_and_round(ext.original_width_negative)\n                    original_height_negative_1st = scale_and_round(ext.original_height_negative)\n                    crop_left_1st = scale_and_round(ext.crop_left)\n                    crop_top_1st = scale_and_round(ext.crop_top)\n\n                    strength_1st = ext.strength if args.highres_fix_strength is None else args.highres_fix_strength\n\n                    ext_1st = BatchDataExt(\n                        width_1st,\n                        height_1st,\n                        original_width_1st,\n                        original_height_1st,\n                        original_width_negative_1st,\n                        original_height_negative_1st,\n                        crop_left_1st,\n                        crop_top_1st,\n                        args.highres_fix_steps,\n                        ext.scale,\n                        ext.negative_scale,\n                        strength_1st,\n                        ext.network_muls,\n                        ext.num_sub_prompts,\n                    )\n                    batch_1st.append(BatchData(is_1st_latent, base, ext_1st))\n\n                pipe.set_enable_control_net(True)  # 1st stageではControlNetを有効にする\n                images_1st = process_batch(batch_1st, True, True)\n\n                # 2nd stageのバッチを作成して以下処理する\n                logger.info(\"process 2nd stage\")\n                width_2nd, height_2nd = batch[0].ext.width, batch[0].ext.height\n\n                if upscaler:\n                    # upscalerを使って画像を拡大する\n                    lowreso_imgs = None if is_1st_latent else images_1st\n                    lowreso_latents = None if not is_1st_latent else images_1st\n\n                    # 戻り値はPIL.Image.Imageかtorch.Tensorのlatents\n                    batch_size = len(images_1st)\n                    vae_batch_size = (\n                        batch_size\n                        if args.vae_batch_size is None\n                        else (max(1, int(batch_size * args.vae_batch_size)) if args.vae_batch_size < 1 else args.vae_batch_size)\n                    )\n                    vae_batch_size = int(vae_batch_size)\n                    images_1st = upscaler.upscale(\n                        vae, lowreso_imgs, lowreso_latents, dtype, width_2nd, height_2nd, batch_size, vae_batch_size\n                    )\n\n                elif args.highres_fix_latents_upscaling:\n                    # latentを拡大する\n                    org_dtype = images_1st.dtype\n                    if images_1st.dtype == torch.bfloat16:\n                        images_1st = images_1st.to(torch.float)  # interpolateがbf16をサポートしていない\n                    images_1st = torch.nn.functional.interpolate(\n                        images_1st, (batch[0].ext.height // 8, batch[0].ext.width // 8), mode=\"bilinear\"\n                    )  # , antialias=True)\n                    images_1st = images_1st.to(org_dtype)\n\n                else:\n                    # 画像をLANCZOSで拡大する\n                    images_1st = [image.resize((width_2nd, height_2nd), resample=PIL.Image.LANCZOS) for image in images_1st]\n\n                batch_2nd = []\n                for i, (bd, image) in enumerate(zip(batch, images_1st)):\n                    bd_2nd = BatchData(False, BatchDataBase(*bd.base[0:3], bd.base.seed + 1, image, None, *bd.base[6:]), bd.ext)\n                    batch_2nd.append(bd_2nd)\n                batch = batch_2nd\n\n                if args.highres_fix_disable_control_net:\n                    pipe.set_enable_control_net(False)  # オプション指定時、2nd stageではControlNetを無効にする\n\n            # このバッチの情報を取り出す\n            (\n                return_latents,\n                (step_first, _, _, _, init_image, mask_image, _, guide_image, _),\n                (\n                    width,\n                    height,\n                    original_width,\n                    original_height,\n                    original_width_negative,\n                    original_height_negative,\n                    crop_left,\n                    crop_top,\n                    steps,\n                    scale,\n                    negative_scale,\n                    strength,\n                    network_muls,\n                    num_sub_prompts,\n                ),\n            ) = batch[0]\n            noise_shape = (LATENT_CHANNELS, height // DOWNSAMPLING_FACTOR, width // DOWNSAMPLING_FACTOR)\n\n            prompts = []\n            negative_prompts = []\n            raw_prompts = []\n            start_code = torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n            noises = [\n                torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n                for _ in range(steps * scheduler_num_noises_per_step)\n            ]\n            seeds = []\n            clip_prompts = []\n\n            if init_image is not None:  # img2img?\n                i2i_noises = torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n                init_images = []\n\n                if mask_image is not None:\n                    mask_images = []\n                else:\n                    mask_images = None\n            else:\n                i2i_noises = None\n                init_images = None\n                mask_images = None\n\n            if guide_image is not None:  # CLIP image guided?\n                guide_images = []\n            else:\n                guide_images = None\n\n            # バッチ内の位置に関わらず同じ乱数を使うためにここで乱数を生成しておく。あわせてimage/maskがbatch内で同一かチェックする\n            all_images_are_same = True\n            all_masks_are_same = True\n            all_guide_images_are_same = True\n            for i, (\n                _,\n                (_, prompt, negative_prompt, seed, init_image, mask_image, clip_prompt, guide_image, raw_prompt),\n                _,\n            ) in enumerate(batch):\n                prompts.append(prompt)\n                negative_prompts.append(negative_prompt)\n                seeds.append(seed)\n                clip_prompts.append(clip_prompt)\n                raw_prompts.append(raw_prompt)\n\n                if init_image is not None:\n                    init_images.append(init_image)\n                    if i > 0 and all_images_are_same:\n                        all_images_are_same = init_images[-2] is init_image\n\n                if mask_image is not None:\n                    mask_images.append(mask_image)\n                    if i > 0 and all_masks_are_same:\n                        all_masks_are_same = mask_images[-2] is mask_image\n\n                if guide_image is not None:\n                    if type(guide_image) is list:\n                        guide_images.extend(guide_image)\n                        all_guide_images_are_same = False\n                    else:\n                        guide_images.append(guide_image)\n                        if i > 0 and all_guide_images_are_same:\n                            all_guide_images_are_same = guide_images[-2] is guide_image\n\n                # make start code\n                torch.manual_seed(seed)\n                start_code[i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n                # make each noises\n                for j in range(steps * scheduler_num_noises_per_step):\n                    noises[j][i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n                if i2i_noises is not None:  # img2img noise\n                    i2i_noises[i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n            noise_manager.reset_sampler_noises(noises)\n\n            # すべての画像が同じなら1枚だけpipeに渡すことでpipe側で処理を高速化する\n            if init_images is not None and all_images_are_same:\n                init_images = init_images[0]\n            if mask_images is not None and all_masks_are_same:\n                mask_images = mask_images[0]\n            if guide_images is not None and all_guide_images_are_same:\n                guide_images = guide_images[0]\n\n            # ControlNet使用時はguide imageをリサイズする\n            if control_nets or control_net_lllites:\n                # TODO resampleのメソッド\n                guide_images = guide_images if type(guide_images) == list else [guide_images]\n                guide_images = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in guide_images]\n                if len(guide_images) == 1:\n                    guide_images = guide_images[0]\n\n            # generate\n            if networks:\n                # 追加ネットワークの処理\n                shared = {}\n                for n, m in zip(networks, network_muls if network_muls else network_default_muls):\n                    n.set_multiplier(m)\n                    if regional_network:\n                        # TODO バッチから ds_ratio を取り出すべき\n                        n.set_current_generation(batch_size, num_sub_prompts, width, height, shared, unet.ds_ratio)\n\n                if not regional_network and network_pre_calc:\n                    for n in networks:\n                        n.restore_weights()\n                    for n in networks:\n                        n.pre_calculation()\n                    logger.info(\"pre-calculation... done\")\n\n            images = pipe(\n                prompts,\n                negative_prompts,\n                init_images,\n                mask_images,\n                height,\n                width,\n                original_height,\n                original_width,\n                original_height_negative,\n                original_width_negative,\n                crop_top,\n                crop_left,\n                steps,\n                scale,\n                negative_scale,\n                strength,\n                latents=start_code,\n                output_type=\"pil\",\n                max_embeddings_multiples=max_embeddings_multiples,\n                img2img_noise=i2i_noises,\n                vae_batch_size=args.vae_batch_size,\n                return_latents=return_latents,\n                clip_prompts=clip_prompts,\n                clip_guide_images=guide_images,\n                emb_normalize_mode=args.emb_normalize_mode,\n            )\n            if highres_1st and not args.highres_fix_save_1st:  # return images or latents\n                return images\n\n            # save image\n            highres_prefix = (\"0\" if highres_1st else \"1\") if highres_fix else \"\"\n            ts_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n            for i, (image, prompt, negative_prompts, seed, clip_prompt, raw_prompt) in enumerate(\n                zip(images, prompts, negative_prompts, seeds, clip_prompts, raw_prompts)\n            ):\n                if highres_fix:\n                    seed -= 1  # record original seed\n                metadata = PngInfo()\n                metadata.add_text(\"prompt\", prompt)\n                metadata.add_text(\"seed\", str(seed))\n                metadata.add_text(\"sampler\", args.sampler)\n                metadata.add_text(\"steps\", str(steps))\n                metadata.add_text(\"scale\", str(scale))\n                if negative_prompt is not None:\n                    metadata.add_text(\"negative-prompt\", negative_prompt)\n                if negative_scale is not None:\n                    metadata.add_text(\"negative-scale\", str(negative_scale))\n                if clip_prompt is not None:\n                    metadata.add_text(\"clip-prompt\", clip_prompt)\n                if raw_prompt is not None:\n                    metadata.add_text(\"raw-prompt\", raw_prompt)\n                if is_sdxl:\n                    metadata.add_text(\"original-height\", str(original_height))\n                    metadata.add_text(\"original-width\", str(original_width))\n                    metadata.add_text(\"original-height-negative\", str(original_height_negative))\n                    metadata.add_text(\"original-width-negative\", str(original_width_negative))\n                    metadata.add_text(\"crop-top\", str(crop_top))\n                    metadata.add_text(\"crop-left\", str(crop_left))\n\n                if args.use_original_file_name and init_images is not None:\n                    if type(init_images) is list:\n                        fln = os.path.splitext(os.path.basename(init_images[i % len(init_images)].filename))[0] + \".png\"\n                    else:\n                        fln = os.path.splitext(os.path.basename(init_images.filename))[0] + \".png\"\n                elif args.sequential_file_name:\n                    fln = f\"im_{highres_prefix}{step_first + i + 1:06d}.png\"\n                else:\n                    fln = f\"im_{ts_str}_{highres_prefix}{i:03d}_{seed}.png\"\n\n                image.save(os.path.join(args.outdir, fln), pnginfo=metadata)\n\n            if not args.no_preview and not highres_1st and args.interactive:\n                try:\n                    import cv2\n\n                    for prompt, image in zip(prompts, images):\n                        cv2.imshow(prompt[:128], np.array(image)[:, :, ::-1])  # プロンプトが長いと死ぬ\n                        cv2.waitKey()\n                        cv2.destroyAllWindows()\n                except ImportError:\n                    logger.warning(\n                        \"opencv-python is not installed, cannot preview / opencv-pythonがインストールされていないためプレビューできません\"\n                    )\n\n            return images\n\n        # 画像生成のプロンプトが一周するまでのループ\n        prompt_index = 0\n        global_step = 0\n        batch_data = []\n        while True:\n            if args.interactive:\n                # interactive\n                valid = False\n                while not valid:\n                    logger.info(\"\\nType prompt:\")\n                    try:\n                        raw_prompt = input()\n                    except EOFError:\n                        break\n\n                    valid = len(raw_prompt.strip().split(\" --\")[0].strip()) > 0\n                if not valid:  # EOF, end app\n                    break\n            else:\n                raw_prompt = prompter(args, pipe, seed_random, iter_seed, prompt_index, global_step)\n                if raw_prompt is None:\n                    break\n\n            # sd-dynamic-prompts like variants:\n            # count is 1 (not dynamic) or images_per_prompt (no enumeration) or arbitrary (enumeration)\n            raw_prompts = handle_dynamic_prompt_variants(raw_prompt, args.images_per_prompt)\n\n            # repeat prompt\n            for pi in range(args.images_per_prompt if len(raw_prompts) == 1 else len(raw_prompts)):\n                raw_prompt = raw_prompts[pi] if len(raw_prompts) > 1 else raw_prompts[0]\n\n                if pi == 0 or len(raw_prompts) > 1:\n                    # parse prompt: if prompt is not changed, skip parsing\n                    width = args.W\n                    height = args.H\n                    original_width = args.original_width\n                    original_height = args.original_height\n                    original_width_negative = args.original_width_negative\n                    original_height_negative = args.original_height_negative\n                    crop_top = args.crop_top\n                    crop_left = args.crop_left\n                    scale = args.scale\n                    negative_scale = args.negative_scale\n                    steps = args.steps\n                    seed = None\n                    seeds = None\n                    strength = 0.8 if args.strength is None else args.strength\n                    negative_prompt = \"\"\n                    clip_prompt = None\n                    network_muls = None\n\n                    # Deep Shrink\n                    ds_depth_1 = None  # means no override\n                    ds_timesteps_1 = args.ds_timesteps_1\n                    ds_depth_2 = args.ds_depth_2\n                    ds_timesteps_2 = args.ds_timesteps_2\n                    ds_ratio = args.ds_ratio\n\n                    # Gradual Latent\n                    gl_timesteps = None  # means no override\n                    gl_ratio = args.gradual_latent_ratio\n                    gl_every_n_steps = args.gradual_latent_every_n_steps\n                    gl_ratio_step = args.gradual_latent_ratio_step\n                    gl_s_noise = args.gradual_latent_s_noise\n                    gl_unsharp_params = args.gradual_latent_unsharp_params\n\n                    prompt_args = raw_prompt.strip().split(\" --\")\n                    prompt = prompt_args[0]\n                    length = len(prompter) if hasattr(prompter, \"__len__\") else 0\n                    logger.info(f\"prompt {prompt_index+1}/{length}: {prompt}\")\n\n                    for parg in prompt_args[1:]:\n                        try:\n                            m = re.match(r\"w (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                width = int(m.group(1))\n                                logger.info(f\"width: {width}\")\n                                continue\n\n                            m = re.match(r\"h (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                height = int(m.group(1))\n                                logger.info(f\"height: {height}\")\n                                continue\n\n                            m = re.match(r\"ow (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                original_width = int(m.group(1))\n                                logger.info(f\"original width: {original_width}\")\n                                continue\n\n                            m = re.match(r\"oh (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                original_height = int(m.group(1))\n                                logger.info(f\"original height: {original_height}\")\n                                continue\n\n                            m = re.match(r\"nw (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                original_width_negative = int(m.group(1))\n                                logger.info(f\"original width negative: {original_width_negative}\")\n                                continue\n\n                            m = re.match(r\"nh (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                original_height_negative = int(m.group(1))\n                                logger.info(f\"original height negative: {original_height_negative}\")\n                                continue\n\n                            m = re.match(r\"ct (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                crop_top = int(m.group(1))\n                                logger.info(f\"crop top: {crop_top}\")\n                                continue\n\n                            m = re.match(r\"cl (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                crop_left = int(m.group(1))\n                                logger.info(f\"crop left: {crop_left}\")\n                                continue\n\n                            m = re.match(r\"s (\\d+)\", parg, re.IGNORECASE)\n                            if m:  # steps\n                                steps = max(1, min(1000, int(m.group(1))))\n                                logger.info(f\"steps: {steps}\")\n                                continue\n\n                            m = re.match(r\"d ([\\d,]+)\", parg, re.IGNORECASE)\n                            if m:  # seed\n                                seeds = [int(d) for d in m.group(1).split(\",\")]\n                                logger.info(f\"seeds: {seeds}\")\n                                continue\n\n                            m = re.match(r\"l ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # scale\n                                scale = float(m.group(1))\n                                logger.info(f\"scale: {scale}\")\n                                continue\n\n                            m = re.match(r\"nl ([\\d\\.]+|none|None)\", parg, re.IGNORECASE)\n                            if m:  # negative scale\n                                if m.group(1).lower() == \"none\":\n                                    negative_scale = None\n                                else:\n                                    negative_scale = float(m.group(1))\n                                logger.info(f\"negative scale: {negative_scale}\")\n                                continue\n\n                            m = re.match(r\"t ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # strength\n                                strength = float(m.group(1))\n                                logger.info(f\"strength: {strength}\")\n                                continue\n\n                            m = re.match(r\"n (.+)\", parg, re.IGNORECASE)\n                            if m:  # negative prompt\n                                negative_prompt = m.group(1)\n                                logger.info(f\"negative prompt: {negative_prompt}\")\n                                continue\n\n                            m = re.match(r\"c (.+)\", parg, re.IGNORECASE)\n                            if m:  # clip prompt\n                                clip_prompt = m.group(1)\n                                logger.info(f\"clip prompt: {clip_prompt}\")\n                                continue\n\n                            m = re.match(r\"am ([\\d\\.\\-,]+)\", parg, re.IGNORECASE)\n                            if m:  # network multiplies\n                                network_muls = [float(v) for v in m.group(1).split(\",\")]\n                                while len(network_muls) < len(networks):\n                                    network_muls.append(network_muls[-1])\n                                logger.info(f\"network mul: {network_muls}\")\n                                continue\n\n                            # Deep Shrink\n                            m = re.match(r\"dsd1 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink depth 1\n                                ds_depth_1 = int(m.group(1))\n                                logger.info(f\"deep shrink depth 1: {ds_depth_1}\")\n                                continue\n\n                            m = re.match(r\"dst1 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink timesteps 1\n                                ds_timesteps_1 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink timesteps 1: {ds_timesteps_1}\")\n                                continue\n\n                            m = re.match(r\"dsd2 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink depth 2\n                                ds_depth_2 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink depth 2: {ds_depth_2}\")\n                                continue\n\n                            m = re.match(r\"dst2 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink timesteps 2\n                                ds_timesteps_2 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink timesteps 2: {ds_timesteps_2}\")\n                                continue\n\n                            m = re.match(r\"dsr ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink ratio\n                                ds_ratio = float(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink ratio: {ds_ratio}\")\n                                continue\n\n                            # Gradual Latent\n                            m = re.match(r\"glt ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent timesteps\n                                gl_timesteps = int(m.group(1))\n                                logger.info(f\"gradual latent timesteps: {gl_timesteps}\")\n                                continue\n\n                            m = re.match(r\"glr ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent ratio\n                                gl_ratio = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent ratio: {ds_ratio}\")\n                                continue\n\n                            m = re.match(r\"gle ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent every n steps\n                                gl_every_n_steps = int(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent every n steps: {gl_every_n_steps}\")\n                                continue\n\n                            m = re.match(r\"gls ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent ratio step\n                                gl_ratio_step = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent ratio step: {gl_ratio_step}\")\n                                continue\n\n                            m = re.match(r\"glsn ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent s noise\n                                gl_s_noise = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent s noise: {gl_s_noise}\")\n                                continue\n\n                            m = re.match(r\"glus ([\\d\\.\\-,]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent unsharp params\n                                gl_unsharp_params = m.group(1)\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent unsharp params: {gl_unsharp_params}\")\n                                continue\n\n                        except ValueError as ex:\n                            logger.error(f\"Exception in parsing / 解析エラー: {parg}\")\n                            logger.error(f\"{ex}\")\n\n                # override Deep Shrink\n                if ds_depth_1 is not None:\n                    if ds_depth_1 < 0:\n                        ds_depth_1 = args.ds_depth_1 or 3\n                    unet.set_deep_shrink(ds_depth_1, ds_timesteps_1, ds_depth_2, ds_timesteps_2, ds_ratio)\n\n                # override Gradual Latent\n                if gl_timesteps is not None:\n                    if gl_timesteps < 0:\n                        gl_timesteps = args.gradual_latent_timesteps or 650\n                    if gl_unsharp_params is not None:\n                        unsharp_params = gl_unsharp_params.split(\",\")\n                        us_ksize, us_sigma, us_strength = [float(v) for v in unsharp_params[:3]]\n                        us_target_x = True if len(unsharp_params) < 4 else bool(int(unsharp_params[3]))\n                        us_ksize = int(us_ksize)\n                    else:\n                        us_ksize, us_sigma, us_strength, us_target_x = None, None, None, None\n                    gradual_latent = GradualLatent(\n                        gl_ratio,\n                        gl_timesteps,\n                        gl_every_n_steps,\n                        gl_ratio_step,\n                        gl_s_noise,\n                        us_ksize,\n                        us_sigma,\n                        us_strength,\n                        us_target_x,\n                    )\n                    pipe.set_gradual_latent(gradual_latent)\n\n                # prepare seed\n                if seeds is not None:  # given in prompt\n                    # num_images_per_promptが多い場合は足りなくなるので、足りない分は前のを使う\n                    if len(seeds) > 0:\n                        seed = seeds.pop(0)\n                else:\n                    if args.iter_same_seed:\n                        seed = iter_seed\n                    else:\n                        seed = None  # 前のを消す\n\n                if seed is None:\n                    seed = seed_random.randint(0, 2**32 - 1)\n                if args.interactive:\n                    logger.info(f\"seed: {seed}\")\n\n                # prepare init image, guide image and mask\n                init_image = mask_image = guide_image = None\n\n                # 同一イメージを使うとき、本当はlatentに変換しておくと無駄がないが面倒なのでとりあえず毎回処理する\n                if init_images is not None:\n                    init_image = init_images[global_step % len(init_images)]\n\n                    # img2imgの場合は、基本的に元画像のサイズで生成する。highres fixの場合はargs.W, args.Hとscaleに従いリサイズ済みなので無視する\n                    # 32単位に丸めたやつにresizeされるので踏襲する\n                    if not highres_fix:\n                        width, height = init_image.size\n                        width = width - width % 32\n                        height = height - height % 32\n                        if width != init_image.size[0] or height != init_image.size[1]:\n                            logger.warning(\n                                f\"img2img image size is not divisible by 32 so aspect ratio is changed / img2imgの画像サイズが32で割り切れないためリサイズされます。画像が歪みます\"\n                            )\n\n                if mask_images is not None:\n                    mask_image = mask_images[global_step % len(mask_images)]\n\n                if guide_images is not None:\n                    if control_nets or control_net_lllites:  # 複数件の場合あり\n                        c = max(len(control_nets), len(control_net_lllites))\n                        p = global_step % (len(guide_images) // c)\n                        guide_image = guide_images[p * c : p * c + c]\n                    else:\n                        guide_image = guide_images[global_step % len(guide_images)]\n\n                if regional_network:\n                    num_sub_prompts = len(prompt.split(\" AND \"))\n                    assert (\n                        len(networks) <= num_sub_prompts\n                    ), \"Number of networks must be less than or equal to number of sub prompts.\"\n                else:\n                    num_sub_prompts = None\n\n                b1 = BatchData(\n                    False,\n                    BatchDataBase(\n                        global_step, prompt, negative_prompt, seed, init_image, mask_image, clip_prompt, guide_image, raw_prompt\n                    ),\n                    BatchDataExt(\n                        width,\n                        height,\n                        original_width,\n                        original_height,\n                        original_width_negative,\n                        original_height_negative,\n                        crop_left,\n                        crop_top,\n                        steps,\n                        scale,\n                        negative_scale,\n                        strength,\n                        tuple(network_muls) if network_muls else None,\n                        num_sub_prompts,\n                    ),\n                )\n                if len(batch_data) > 0 and batch_data[-1].ext != b1.ext:  # バッチ分割必要？\n                    process_batch(batch_data, highres_fix)\n                    batch_data.clear()\n\n                batch_data.append(b1)\n                if len(batch_data) == args.batch_size:\n                    prev_image = process_batch(batch_data, highres_fix)[0]\n                    batch_data.clear()\n\n                global_step += 1\n\n            prompt_index += 1\n\n        if len(batch_data) > 0:\n            process_batch(batch_data, highres_fix)\n            batch_data.clear()\n\n    logger.info(\"done!\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    \n    parser.add_argument(\n        \"--sdxl\", action=\"store_true\", help=\"load Stable Diffusion XL model / Stable Diffusion XLのモデルを読み込む\"\n    )\n    parser.add_argument(\n        \"--v1\", action=\"store_true\", help=\"load Stable Diffusion v1.x model / Stable Diffusion 1.xのモデルを読み込む\"\n    )\n    parser.add_argument(\n        \"--v2\", action=\"store_true\", help=\"load Stable Diffusion v2.0 model / Stable Diffusion 2.0のモデルを読み込む\"\n    )\n    parser.add_argument(\n        \"--v_parameterization\", action=\"store_true\", help=\"enable v-parameterization training / v-parameterization学習を有効にする\"\n    )\n\n    parser.add_argument(\"--prompt\", type=str, default=None, help=\"prompt / プロンプト\")\n    parser.add_argument(\n        \"--from_file\",\n        type=str,\n        default=None,\n        help=\"if specified, load prompts from this file / 指定時はプロンプトをファイルから読み込む\",\n    )\n    parser.add_argument(\n        \"--from_module\",\n        type=str,\n        default=None,\n        help=\"if specified, load prompts from this module / 指定時はプロンプトをモジュールから読み込む\",\n    )\n    parser.add_argument(\n        \"--prompter_module_args\", type=str, default=None, help=\"args for prompter module / prompterモジュールの引数\"\n    )\n    parser.add_argument(\n        \"--interactive\",\n        action=\"store_true\",\n        help=\"interactive mode (generates one image) / 対話モード（生成される画像は1枚になります）\",\n    )\n    parser.add_argument(\n        \"--no_preview\", action=\"store_true\", help=\"do not show generated image in interactive mode / 対話モードで画像を表示しない\"\n    )\n    parser.add_argument(\n        \"--image_path\", type=str, default=None, help=\"image to inpaint or to generate from / img2imgまたはinpaintを行う元画像\"\n    )\n    parser.add_argument(\"--mask_path\", type=str, default=None, help=\"mask in inpainting / inpaint時のマスク\")\n    parser.add_argument(\"--strength\", type=float, default=None, help=\"img2img strength / img2img時のstrength\")\n    parser.add_argument(\"--images_per_prompt\", type=int, default=1, help=\"number of images per prompt / プロンプトあたりの出力枚数\")\n    parser.add_argument(\"--outdir\", type=str, default=\"outputs\", help=\"dir to write results to / 生成画像の出力先\")\n    parser.add_argument(\n        \"--sequential_file_name\", action=\"store_true\", help=\"sequential output file name / 生成画像のファイル名を連番にする\"\n    )\n    parser.add_argument(\n        \"--use_original_file_name\",\n        action=\"store_true\",\n        help=\"prepend original file name in img2img / img2imgで元画像のファイル名を生成画像のファイル名の先頭に付ける\",\n    )\n    # parser.add_argument(\"--ddim_eta\", type=float, default=0.0, help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\", )\n    parser.add_argument(\"--n_iter\", type=int, default=1, help=\"sample this often / 繰り返し回数\")\n    parser.add_argument(\"--H\", type=int, default=None, help=\"image height, in pixel space / 生成画像高さ\")\n    parser.add_argument(\"--W\", type=int, default=None, help=\"image width, in pixel space / 生成画像幅\")\n    parser.add_argument(\n        \"--original_height\",\n        type=int,\n        default=None,\n        help=\"original height for SDXL conditioning / SDXLの条件付けに用いるoriginal heightの値\",\n    )\n    parser.add_argument(\n        \"--original_width\",\n        type=int,\n        default=None,\n        help=\"original width for SDXL conditioning / SDXLの条件付けに用いるoriginal widthの値\",\n    )\n    parser.add_argument(\n        \"--original_height_negative\",\n        type=int,\n        default=None,\n        help=\"original height for SDXL unconditioning / SDXLのネガティブ条件付けに用いるoriginal heightの値\",\n    )\n    parser.add_argument(\n        \"--original_width_negative\",\n        type=int,\n        default=None,\n        help=\"original width for SDXL unconditioning / SDXLのネガティブ条件付けに用いるoriginal widthの値\",\n    )\n    parser.add_argument(\n        \"--crop_top\", type=int, default=None, help=\"crop top for SDXL conditioning / SDXLの条件付けに用いるcrop topの値\"\n    )\n    parser.add_argument(\n        \"--crop_left\", type=int, default=None, help=\"crop left for SDXL conditioning / SDXLの条件付けに用いるcrop leftの値\"\n    )\n    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"batch size / バッチサイズ\")\n    parser.add_argument(\n        \"--vae_batch_size\",\n        type=float,\n        default=None,\n        help=\"batch size for VAE, < 1.0 for ratio / VAE処理時のバッチサイズ、1未満の値の場合は通常バッチサイズの比率\",\n    )\n    parser.add_argument(\n        \"--vae_slices\",\n        type=int,\n        default=None,\n        help=\"number of slices to split image into for VAE to reduce VRAM usage, None for no splitting (default), slower if specified. 16 or 32 recommended / VAE処理時にVRAM使用量削減のため画像を分割するスライス数、Noneの場合は分割しない（デフォルト）、指定すると遅くなる。16か32程度を推奨\",\n    )\n    parser.add_argument(\n        \"--no_half_vae\", action=\"store_true\", help=\"do not use fp16/bf16 precision for VAE / VAE処理時にfp16/bf16を使わない\"\n    )\n    parser.add_argument(\"--steps\", type=int, default=50, help=\"number of ddim sampling steps / サンプリングステップ数\")\n    parser.add_argument(\n        \"--sampler\",\n        type=str,\n        default=\"ddim\",\n        choices=[\n            \"ddim\",\n            \"pndm\",\n            \"lms\",\n            \"euler\",\n            \"euler_a\",\n            \"heun\",\n            \"dpm_2\",\n            \"dpm_2_a\",\n            \"dpmsolver\",\n            \"dpmsolver++\",\n            \"dpmsingle\",\n            \"k_lms\",\n            \"k_euler\",\n            \"k_euler_a\",\n            \"k_dpm_2\",\n            \"k_dpm_2_a\",\n        ],\n        help=f\"sampler (scheduler) type / サンプラー（スケジューラ）の種類\",\n    )\n    parser.add_argument(\n        \"--scale\",\n        type=float,\n        default=7.5,\n        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty)) / guidance scale\",\n    )\n    parser.add_argument(\n        \"--ckpt\", type=str, default=None, help=\"path to checkpoint of model / モデルのcheckpointファイルまたはディレクトリ\"\n    )\n    parser.add_argument(\n        \"--vae\",\n        type=str,\n        default=None,\n        help=\"path to checkpoint of vae to replace / VAEを入れ替える場合、VAEのcheckpointファイルまたはディレクトリ\",\n    )\n    parser.add_argument(\n        \"--tokenizer_cache_dir\",\n        type=str,\n        default=None,\n        help=\"directory for caching Tokenizer (for offline training) / Tokenizerをキャッシュするディレクトリ（ネット接続なしでの学習のため）\",\n    )\n    # parser.add_argument(\"--replace_clip_l14_336\", action='store_true',\n    #                     help=\"Replace CLIP (Text Encoder) to l/14@336 / CLIP(Text Encoder)をl/14@336に入れ替える\")\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=None,\n        help=\"seed, or seed of seeds in multiple generation / 1枚生成時のseed、または複数枚生成時の乱数seedを決めるためのseed\",\n    )\n    parser.add_argument(\n        \"--iter_same_seed\",\n        action=\"store_true\",\n        help=\"use same seed for all prompts in iteration if no seed specified / 乱数seedの指定がないとき繰り返し内はすべて同じseedを使う（プロンプト間の差異の比較用）\",\n    )\n    parser.add_argument(\n        \"--shuffle_prompts\",\n        action=\"store_true\",\n        help=\"shuffle prompts in iteration / 繰り返し内のプロンプトをシャッフルする\",\n    )\n    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"use fp16 / fp16を指定し省メモリ化する\")\n    parser.add_argument(\"--bf16\", action=\"store_true\", help=\"use bfloat16 / bfloat16を指定し省メモリ化する\")\n    parser.add_argument(\"--xformers\", action=\"store_true\", help=\"use xformers / xformersを使用し高速化する\")\n    parser.add_argument(\"--sdpa\", action=\"store_true\", help=\"use sdpa in PyTorch 2 / sdpa\")\n    parser.add_argument(\n        \"--diffusers_xformers\",\n        action=\"store_true\",\n        help=\"use xformers by diffusers (Hypernetworks doesn't work) / Diffusersでxformersを使用する（Hypernetwork利用不可）\",\n    )\n    parser.add_argument(\n        \"--opt_channels_last\",\n        action=\"store_true\",\n        help=\"set channels last option to model / モデルにchannels lastを指定し最適化する\",\n    )\n    parser.add_argument(\n        \"--network_module\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"additional network module to use / 追加ネットワークを使う時そのモジュール名\",\n    )\n    parser.add_argument(\n        \"--network_weights\", type=str, default=None, nargs=\"*\", help=\"additional network weights to load / 追加ネットワークの重み\"\n    )\n    parser.add_argument(\n        \"--network_mul\", type=float, default=None, nargs=\"*\", help=\"additional network multiplier / 追加ネットワークの効果の倍率\"\n    )\n    parser.add_argument(\n        \"--network_args\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"additional arguments for network (key=value) / ネットワークへの追加の引数\",\n    )\n    parser.add_argument(\n        \"--network_show_meta\", action=\"store_true\", help=\"show metadata of network model / ネットワークモデルのメタデータを表示する\"\n    )\n    parser.add_argument(\n        \"--network_merge_n_models\",\n        type=int,\n        default=None,\n        help=\"merge this number of networks / この数だけネットワークをマージする\",\n    )\n    parser.add_argument(\n        \"--network_merge\", action=\"store_true\", help=\"merge network weights to original model / ネットワークの重みをマージする\"\n    )\n    parser.add_argument(\n        \"--network_pre_calc\",\n        action=\"store_true\",\n        help=\"pre-calculate network for generation / ネットワークのあらかじめ計算して生成する\",\n    )\n    parser.add_argument(\n        \"--network_regional_mask_max_color_codes\",\n        type=int,\n        default=None,\n        help=\"max color codes for regional mask (default is None, mask by channel) / regional maskの最大色数（デフォルトはNoneでチャンネルごとのマスク）\",\n    )\n    parser.add_argument(\n        \"--textual_inversion_embeddings\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"Embeddings files of Textual Inversion / Textual Inversionのembeddings\",\n    )\n    parser.add_argument(\n        \"--clip_skip\",\n        type=int,\n        default=None,\n        help=\"layer number from bottom to use in CLIP, default is 1 for SD1/2, 2 for SDXL \"\n        + \"/ CLIPの後ろからn層目の出力を使う（デフォルトはSD1/2の場合1、SDXLの場合2）\",\n    )\n    parser.add_argument(\n        \"--max_embeddings_multiples\",\n        type=int,\n        default=None,\n        help=\"max embedding multiples, max token length is 75 * multiples / トークン長をデフォルトの何倍とするか 75*この値 がトークン長となる\",\n    )\n    parser.add_argument(\n        \"--emb_normalize_mode\",\n        type=str,\n        default=\"original\",\n        choices=[\"original\", \"none\", \"abs\"],\n        help=\"embedding normalization mode / embeddingの正規化モード\",\n    )\n    parser.add_argument(\n        \"--guide_image_path\", type=str, default=None, nargs=\"*\", help=\"image to ControlNet / ControlNetでガイドに使う画像\"\n    )\n    parser.add_argument(\n        \"--highres_fix_scale\",\n        type=float,\n        default=None,\n        help=\"enable highres fix, reso scale for 1st stage / highres fixを有効にして最初の解像度をこのscaleにする\",\n    )\n    parser.add_argument(\n        \"--highres_fix_steps\",\n        type=int,\n        default=28,\n        help=\"1st stage steps for highres fix / highres fixの最初のステージのステップ数\",\n    )\n    parser.add_argument(\n        \"--highres_fix_strength\",\n        type=float,\n        default=None,\n        help=\"1st stage img2img strength for highres fix / highres fixの最初のステージのimg2img時のstrength、省略時はstrengthと同じ\",\n    )\n    parser.add_argument(\n        \"--highres_fix_save_1st\",\n        action=\"store_true\",\n        help=\"save 1st stage images for highres fix / highres fixの最初のステージの画像を保存する\",\n    )\n    parser.add_argument(\n        \"--highres_fix_latents_upscaling\",\n        action=\"store_true\",\n        help=\"use latents upscaling for highres fix / highres fixでlatentで拡大する\",\n    )\n    parser.add_argument(\n        \"--highres_fix_upscaler\",\n        type=str,\n        default=None,\n        help=\"upscaler module for highres fix / highres fixで使うupscalerのモジュール名\",\n    )\n    parser.add_argument(\n        \"--highres_fix_upscaler_args\",\n        type=str,\n        default=None,\n        help=\"additional arguments for upscaler (key=value) / upscalerへの追加の引数\",\n    )\n    parser.add_argument(\n        \"--highres_fix_disable_control_net\",\n        action=\"store_true\",\n        help=\"disable ControlNet for highres fix / highres fixでControlNetを使わない\",\n    )\n\n    parser.add_argument(\n        \"--negative_scale\",\n        type=float,\n        default=None,\n        help=\"set another guidance scale for negative prompt / ネガティブプロンプトのscaleを指定する\",\n    )\n\n    parser.add_argument(\n        \"--control_net_lllite_models\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"ControlNet models to use / 使用するControlNetのモデル名\",\n    )\n    parser.add_argument(\n        \"--control_net_models\", type=str, default=None, nargs=\"*\", help=\"ControlNet models to use / 使用するControlNetのモデル名\"\n    )\n    parser.add_argument(\n        \"--control_net_preps\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"ControlNet preprocess to use / 使用するControlNetのプリプロセス名\",\n    )\n    parser.add_argument(\n        \"--control_net_multipliers\", type=float, default=None, nargs=\"*\", help=\"ControlNet multiplier / ControlNetの適用率\"\n    )\n    parser.add_argument(\n        \"--control_net_ratios\",\n        type=float,\n        default=None,\n        nargs=\"*\",\n        help=\"ControlNet guidance ratio for steps / ControlNetでガイドするステップ比率\",\n    )\n    parser.add_argument(\n        \"--clip_vision_strength\",\n        type=float,\n        default=None,\n        help=\"enable CLIP Vision Conditioning for img2img with this strength / img2imgでCLIP Vision Conditioningを有効にしてこのstrengthで処理する\",\n    )\n\n    # Deep Shrink\n    parser.add_argument(\n        \"--ds_depth_1\",\n        type=int,\n        default=None,\n        help=\"Enable Deep Shrink with this depth 1, valid values are 0 to 8 / Deep Shrinkをこのdepthで有効にする\",\n    )\n    parser.add_argument(\n        \"--ds_timesteps_1\",\n        type=int,\n        default=650,\n        help=\"Apply Deep Shrink depth 1 until this timesteps / Deep Shrink depth 1を適用するtimesteps\",\n    )\n    parser.add_argument(\"--ds_depth_2\", type=int, default=None, help=\"Deep Shrink depth 2 / Deep Shrinkのdepth 2\")\n    parser.add_argument(\n        \"--ds_timesteps_2\",\n        type=int,\n        default=650,\n        help=\"Apply Deep Shrink depth 2 until this timesteps / Deep Shrink depth 2を適用するtimesteps\",\n    )\n    parser.add_argument(\n        \"--ds_ratio\", type=float, default=0.5, help=\"Deep Shrink ratio for downsampling / Deep Shrinkのdownsampling比率\"\n    )\n\n    # gradual latent\n    parser.add_argument(\n        \"--gradual_latent_timesteps\",\n        type=int,\n        default=None,\n        help=\"enable Gradual Latent hires fix and apply upscaling from this timesteps / Gradual Latent hires fixをこのtimestepsで有効にし、このtimestepsからアップスケーリングを適用する\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_ratio\",\n        type=float,\n        default=0.5,\n        help=\" this size ratio, 0.5 means 1/2 / Gradual Latent hires fixをこのサイズ比率で有効にする、0.5は1/2を意味する\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_ratio_step\",\n        type=float,\n        default=0.125,\n        help=\"step to increase ratio for Gradual Latent / Gradual Latentのratioをどのくらいずつ上げるか\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_every_n_steps\",\n        type=int,\n        default=3,\n        help=\"steps to increase size of latents every this steps for Gradual Latent / Gradual Latentでlatentsのサイズをこのステップごとに上げる\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_s_noise\",\n        type=float,\n        default=1.0,\n        help=\"s_noise for Gradual Latent / Gradual Latentのs_noise\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_unsharp_params\",\n        type=str,\n        default=None,\n        help=\"unsharp mask parameters for Gradual Latent: ksize, sigma, strength, target-x (1 means True). `3,0.5,0.5,1` or `3,1.0,1.0,0` is recommended /\"\n        + \" Gradual Latentのunsharp maskのパラメータ: ksize, sigma, strength, target-x. `3,0.5,0.5,1` または `3,1.0,1.0,0` が推奨\",\n    )\n\n    # # parser.add_argument(\n    #     \"--control_net_image_path\", type=str, default=None, nargs=\"*\", help=\"image for ControlNet guidance / ControlNetでガイドに使う画像\"\n    # )\n\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    main(args)\n"
        },
        {
          "name": "gen_img_diffusers.py",
          "type": "blob",
          "size": 171.6005859375,
          "content": "\"\"\"\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\"\"\"\n\nimport itertools\nimport json\nfrom typing import Any, List, NamedTuple, Optional, Tuple, Union, Callable\nimport glob\nimport importlib\nimport inspect\nimport time\nimport zipfile\nfrom diffusers.utils import deprecate\nfrom diffusers.configuration_utils import FrozenDict\nimport argparse\nimport math\nimport os\nimport random\nimport re\n\nimport diffusers\nimport numpy as np\n\nimport torch\nfrom library.device_utils import init_ipex, clean_memory, get_preferred_device\ninit_ipex()\n\nimport torchvision\nfrom diffusers import (\n    AutoencoderKL,\n    DDPMScheduler,\n    EulerAncestralDiscreteScheduler,\n    DPMSolverMultistepScheduler,\n    DPMSolverSinglestepScheduler,\n    LMSDiscreteScheduler,\n    PNDMScheduler,\n    DDIMScheduler,\n    EulerDiscreteScheduler,\n    HeunDiscreteScheduler,\n    KDPM2DiscreteScheduler,\n    KDPM2AncestralDiscreteScheduler,\n    # UNet2DConditionModel,\n    StableDiffusionPipeline,\n)\nfrom einops import rearrange\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPTextConfig\nimport PIL\nfrom PIL import Image\nfrom PIL.PngImagePlugin import PngInfo\n\nimport library.model_util as model_util\nimport library.train_util as train_util\nfrom networks.lora import LoRANetwork\nimport tools.original_control_net as original_control_net\nfrom tools.original_control_net import ControlNetInfo\nfrom library.original_unet import UNet2DConditionModel, InferUNet2DConditionModel\nfrom library.original_unet import FlashAttentionFunction\nfrom library.utils import GradualLatent, EulerAncestralDiscreteSchedulerGL\n\nfrom XTI_hijack import unet_forward_XTI, downblock_forward_XTI, upblock_forward_XTI\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# scheduler:\nSCHEDULER_LINEAR_START = 0.00085\nSCHEDULER_LINEAR_END = 0.0120\nSCHEDULER_TIMESTEPS = 1000\nSCHEDLER_SCHEDULE = \"scaled_linear\"\n\n# その他の設定\nLATENT_CHANNELS = 4\nDOWNSAMPLING_FACTOR = 8\n\n# CLIP_ID_L14_336 = \"openai/clip-vit-large-patch14-336\"\n\n# CLIP guided SD関連\nCLIP_MODEL_PATH = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\nFEATURE_EXTRACTOR_SIZE = (224, 224)\nFEATURE_EXTRACTOR_IMAGE_MEAN = [0.48145466, 0.4578275, 0.40821073]\nFEATURE_EXTRACTOR_IMAGE_STD = [0.26862954, 0.26130258, 0.27577711]\n\nVGG16_IMAGE_MEAN = [0.485, 0.456, 0.406]\nVGG16_IMAGE_STD = [0.229, 0.224, 0.225]\nVGG16_INPUT_RESIZE_DIV = 4\n\n# CLIP特徴量の取得時にcutoutを使うか：使う場合にはソースを書き換えてください\nNUM_CUTOUTS = 4\nUSE_CUTOUTS = False\n\n# region モジュール入れ替え部\n\"\"\"\n高速化のためのモジュール入れ替え\n\"\"\"\n\n\ndef replace_unet_modules(unet: diffusers.models.unet_2d_condition.UNet2DConditionModel, mem_eff_attn, xformers, sdpa):\n    if mem_eff_attn:\n        logger.info(\"Enable memory efficient attention for U-Net\")\n\n        # これはDiffusersのU-Netではなく自前のU-Netなので置き換えなくても良い\n        unet.set_use_memory_efficient_attention(False, True)\n    elif xformers:\n        logger.info(\"Enable xformers for U-Net\")\n        try:\n            import xformers.ops\n        except ImportError:\n            raise ImportError(\"No xformers / xformersがインストールされていないようです\")\n\n        unet.set_use_memory_efficient_attention(True, False)\n    elif sdpa:\n        logger.info(\"Enable SDPA for U-Net\")\n        unet.set_use_memory_efficient_attention(False, False)\n        unet.set_use_sdpa(True)\n\n\n# TODO common train_util.py\ndef replace_vae_modules(vae: diffusers.models.AutoencoderKL, mem_eff_attn, xformers, sdpa):\n    if mem_eff_attn:\n        replace_vae_attn_to_memory_efficient()\n    elif xformers:\n        replace_vae_attn_to_xformers()\n    elif sdpa:\n        replace_vae_attn_to_sdpa()\n\n\ndef replace_vae_attn_to_memory_efficient():\n    logger.info(\"VAE Attention.forward has been replaced to FlashAttention (not xformers)\")\n    flash_func = FlashAttentionFunction\n\n    def forward_flash_attn(self, hidden_states, **kwargs):\n        q_bucket_size = 512\n        k_bucket_size = 1024\n\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        out = flash_func.apply(query_proj, key_proj, value_proj, None, False, q_bucket_size, k_bucket_size)\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_flash_attn_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_flash_attn(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_flash_attn_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_flash_attn\n\n\ndef replace_vae_attn_to_xformers():\n    logger.info(\"VAE: Attention.forward has been replaced to xformers\")\n    import xformers.ops\n\n    def forward_xformers(self, hidden_states, **kwargs):\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        query_proj = query_proj.contiguous()\n        key_proj = key_proj.contiguous()\n        value_proj = value_proj.contiguous()\n        out = xformers.ops.memory_efficient_attention(query_proj, key_proj, value_proj, attn_bias=None)\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_xformers_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_xformers(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_xformers_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_xformers\n\n\ndef replace_vae_attn_to_sdpa():\n    logger.info(\"VAE: Attention.forward has been replaced to sdpa\")\n\n    def forward_sdpa(self, hidden_states, **kwargs):\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b n h d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        out = torch.nn.functional.scaled_dot_product_attention(\n            query_proj, key_proj, value_proj, attn_mask=None, dropout_p=0.0, is_causal=False\n        )\n\n        out = rearrange(out, \"b n h d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_sdpa_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_sdpa(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_sdpa_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_sdpa\n\n\n# endregion\n\n# region 画像生成の本体：lpw_stable_diffusion.py （ASL）からコピーして修正\n# https://github.com/huggingface/diffusers/blob/main/examples/community/lpw_stable_diffusion.py\n# Pipelineだけ独立して使えないのと機能追加するのとでコピーして修正\n\n\nclass PipelineLike:\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion without tokens length limit, and support parsing\n    weighting in prompt.\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4) for details.\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        device,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: InferUNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        clip_skip: int,\n        clip_model: CLIPModel,\n        clip_guidance_scale: float,\n        clip_image_guidance_scale: float,\n        vgg16_model: torchvision.models.VGG,\n        vgg16_guidance_scale: float,\n        vgg16_layer_no: int,\n        # safety_checker: StableDiffusionSafetyChecker,\n        # feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n        self.device = device\n        self.clip_skip = clip_skip\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n            )\n            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"clip_sample\"] = False\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        self.vae = vae\n        self.text_encoder = text_encoder\n        self.tokenizer = tokenizer\n        self.unet = unet\n        self.scheduler = scheduler\n        self.safety_checker = None\n\n        # Textual Inversion\n        self.token_replacements = {}\n\n        # XTI\n        self.token_replacements_XTI = {}\n\n        # CLIP guidance\n        self.clip_guidance_scale = clip_guidance_scale\n        self.clip_image_guidance_scale = clip_image_guidance_scale\n        self.clip_model = clip_model\n        self.normalize = transforms.Normalize(mean=FEATURE_EXTRACTOR_IMAGE_MEAN, std=FEATURE_EXTRACTOR_IMAGE_STD)\n        self.make_cutouts = MakeCutouts(FEATURE_EXTRACTOR_SIZE)\n\n        # VGG16 guidance\n        self.vgg16_guidance_scale = vgg16_guidance_scale\n        if self.vgg16_guidance_scale > 0.0:\n            return_layers = {f\"{vgg16_layer_no}\": \"feat\"}\n            self.vgg16_feat_model = torchvision.models._utils.IntermediateLayerGetter(\n                vgg16_model.features, return_layers=return_layers\n            )\n            self.vgg16_normalize = transforms.Normalize(mean=VGG16_IMAGE_MEAN, std=VGG16_IMAGE_STD)\n\n        # ControlNet\n        self.control_nets: List[ControlNetInfo] = []\n        self.control_net_enabled = True  # control_netsが空ならTrueでもFalseでもControlNetは動作しない\n\n        self.gradual_latent: GradualLatent = None\n\n    # Textual Inversion\n    def add_token_replacement(self, target_token_id, rep_token_ids):\n        self.token_replacements[target_token_id] = rep_token_ids\n\n    def set_enable_control_net(self, en: bool):\n        self.control_net_enabled = en\n\n    def replace_token(self, tokens, layer=None):\n        new_tokens = []\n        for token in tokens:\n            if token in self.token_replacements:\n                replacer_ = self.token_replacements[token]\n                if layer:\n                    replacer = []\n                for r in replacer_:\n                    if r in self.token_replacements_XTI:\n                        replacer.append(self.token_replacements_XTI[r][layer])\n                    else:\n                        replacer = replacer_\n                new_tokens.extend(replacer)\n            else:\n                new_tokens.append(token)\n        return new_tokens\n\n    def add_token_replacement_XTI(self, target_token_id, rep_token_ids):\n        self.token_replacements_XTI[target_token_id] = rep_token_ids\n\n    def set_control_nets(self, ctrl_nets):\n        self.control_nets = ctrl_nets\n\n    def set_gradual_latent(self, gradual_latent):\n        if gradual_latent is None:\n            logger.info(\"gradual_latent is disabled\")\n            self.gradual_latent = None\n        else:\n            logger.info(f\"gradual_latent is enabled: {gradual_latent}\")\n            self.gradual_latent = gradual_latent  # (ds_ratio, start_timesteps, every_n_steps, ratio_step)\n\n    # region xformersとか使う部分：独自に書き換えるので関係なし\n\n    def enable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Enable memory efficient attention as implemented in xformers.\n        When this option is enabled, you should observe lower GPU memory usage and a potential speed up at inference\n        time. Speed up at training time is not guaranteed.\n        Warning: When Memory Efficient Attention and Sliced attention are both enabled, the Memory Efficient Attention\n        is used.\n        \"\"\"\n        self.unet.set_use_memory_efficient_attention_xformers(True)\n\n    def disable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Disable memory efficient attention as implemented in xformers.\n        \"\"\"\n        self.unet.set_use_memory_efficient_attention_xformers(False)\n\n    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)\n\n    def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)\n\n    def enable_sequential_cpu_offload(self):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        # accelerateが必要になるのでとりあえず省略\n        raise NotImplementedError(\"cpu_offload is omitted.\")\n        # if is_accelerate_available():\n        #   from accelerate import cpu_offload\n        # else:\n        #   raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        # device = self.device\n\n        # for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\n        #   if cpu_offloaded_model is not None:\n        #     cpu_offload(cpu_offloaded_model, device)\n\n    # endregion\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        init_image: Union[torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]] = None,\n        mask_image: Union[torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]] = None,\n        height: int = 512,\n        width: int = 512,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_scale: float = None,\n        strength: float = 0.8,\n        # num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        vae_batch_size: float = None,\n        return_latents: bool = False,\n        # return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        is_cancelled_callback: Optional[Callable[[], bool]] = None,\n        callback_steps: Optional[int] = 1,\n        img2img_noise=None,\n        clip_prompts=None,\n        clip_guide_images=None,\n        networks: Optional[List[LoRANetwork]] = None,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            init_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process.\n            mask_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `init_image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n                contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n            height (`int`, *optional*, defaults to 512):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to 512):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to transform the reference `init_image`. Must be between 0 and 1.\n                `init_image` will be used as a starting point, adding more noise to it the larger the `strength`. The\n                number of denoising steps depends on the amount of noise initially added. When `strength` is 1, added\n                noise will be maximum and the denoising process will run for the full number of iterations specified in\n                `num_inference_steps`. A value of 1, therefore, essentially ignores `init_image`.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            max_embeddings_multiples (`int`, *optional*, defaults to `3`):\n                The max multiple length of prompt embeddings compared to the max output length of text encoder.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            is_cancelled_callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. If the function returns\n                `True`, the inference will be cancelled.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n        Returns:\n            `None` if cancelled by `is_cancelled_callback`,\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        num_images_per_prompt = 1  # fixed\n\n        if isinstance(prompt, str):\n            batch_size = 1\n            prompt = [prompt]\n        elif isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n        reginonal_network = \" AND \" in prompt[0]\n\n        vae_batch_size = (\n            batch_size\n            if vae_batch_size is None\n            else (int(vae_batch_size) if vae_batch_size >= 1 else max(1, int(batch_size * vae_batch_size)))\n        )\n\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\" f\" {type(callback_steps)}.\"\n            )\n\n        # get prompt text embeddings\n\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        if not do_classifier_free_guidance and negative_scale is not None:\n            logger.warning(f\"negative_scale is ignored if guidance scalle <= 1.0\")\n            negative_scale = None\n\n        # get unconditional embeddings for classifier free guidance\n        if negative_prompt is None:\n            negative_prompt = [\"\"] * batch_size\n        elif isinstance(negative_prompt, str):\n            negative_prompt = [negative_prompt] * batch_size\n        if batch_size != len(negative_prompt):\n            raise ValueError(\n                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                \" the batch size of `prompt`.\"\n            )\n\n        if not self.token_replacements_XTI:\n            text_embeddings, uncond_embeddings, prompt_tokens = get_weighted_text_embeddings(\n                pipe=self,\n                prompt=prompt,\n                uncond_prompt=negative_prompt if do_classifier_free_guidance else None,\n                max_embeddings_multiples=max_embeddings_multiples,\n                clip_skip=self.clip_skip,\n                **kwargs,\n            )\n\n        if negative_scale is not None:\n            _, real_uncond_embeddings, _ = get_weighted_text_embeddings(\n                pipe=self,\n                prompt=prompt,  # こちらのトークン長に合わせてuncondを作るので75トークン超で必須\n                uncond_prompt=[\"\"] * batch_size,\n                max_embeddings_multiples=max_embeddings_multiples,\n                clip_skip=self.clip_skip,\n                **kwargs,\n            )\n\n        if self.token_replacements_XTI:\n            text_embeddings_concat = []\n            for layer in [\n                \"IN01\",\n                \"IN02\",\n                \"IN04\",\n                \"IN05\",\n                \"IN07\",\n                \"IN08\",\n                \"MID\",\n                \"OUT03\",\n                \"OUT04\",\n                \"OUT05\",\n                \"OUT06\",\n                \"OUT07\",\n                \"OUT08\",\n                \"OUT09\",\n                \"OUT10\",\n                \"OUT11\",\n            ]:\n                text_embeddings, uncond_embeddings, prompt_tokens = get_weighted_text_embeddings(\n                    pipe=self,\n                    prompt=prompt,\n                    uncond_prompt=negative_prompt if do_classifier_free_guidance else None,\n                    max_embeddings_multiples=max_embeddings_multiples,\n                    clip_skip=self.clip_skip,\n                    layer=layer,\n                    **kwargs,\n                )\n                if do_classifier_free_guidance:\n                    if negative_scale is None:\n                        text_embeddings_concat.append(torch.cat([uncond_embeddings, text_embeddings]))\n                    else:\n                        text_embeddings_concat.append(torch.cat([uncond_embeddings, text_embeddings, real_uncond_embeddings]))\n                text_embeddings = torch.stack(text_embeddings_concat)\n        else:\n            if do_classifier_free_guidance:\n                if negative_scale is None:\n                    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n                else:\n                    text_embeddings = torch.cat([uncond_embeddings, text_embeddings, real_uncond_embeddings])\n\n        # CLIP guidanceで使用するembeddingsを取得する\n        if self.clip_guidance_scale > 0:\n            clip_text_input = prompt_tokens\n            if clip_text_input.shape[1] > self.tokenizer.model_max_length:\n                # TODO 75文字を超えたら警告を出す？\n                logger.info(f\"trim text input {clip_text_input.shape}\")\n                clip_text_input = torch.cat(\n                    [clip_text_input[:, : self.tokenizer.model_max_length - 1], clip_text_input[:, -1].unsqueeze(1)], dim=1\n                )\n                logger.info(f\"trimmed {clip_text_input.shape}\")\n\n            for i, clip_prompt in enumerate(clip_prompts):\n                if clip_prompt is not None:  # clip_promptがあれば上書きする\n                    clip_text_input[i] = self.tokenizer(\n                        clip_prompt,\n                        padding=\"max_length\",\n                        max_length=self.tokenizer.model_max_length,\n                        truncation=True,\n                        return_tensors=\"pt\",\n                    ).input_ids.to(self.device)\n\n            text_embeddings_clip = self.clip_model.get_text_features(clip_text_input)\n            text_embeddings_clip = text_embeddings_clip / text_embeddings_clip.norm(p=2, dim=-1, keepdim=True)  # prompt複数件でもOK\n\n        if (\n            self.clip_image_guidance_scale > 0\n            or self.vgg16_guidance_scale > 0\n            and clip_guide_images is not None\n            or self.control_nets\n        ):\n            if isinstance(clip_guide_images, PIL.Image.Image):\n                clip_guide_images = [clip_guide_images]\n\n            if self.clip_image_guidance_scale > 0:\n                clip_guide_images = [preprocess_guide_image(im) for im in clip_guide_images]\n                clip_guide_images = torch.cat(clip_guide_images, dim=0)\n\n                clip_guide_images = self.normalize(clip_guide_images).to(self.device).to(text_embeddings.dtype)\n                image_embeddings_clip = self.clip_model.get_image_features(clip_guide_images)\n                image_embeddings_clip = image_embeddings_clip / image_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n                if len(image_embeddings_clip) == 1:\n                    image_embeddings_clip = image_embeddings_clip.repeat((batch_size, 1, 1, 1))\n            elif self.vgg16_guidance_scale > 0:\n                size = (width // VGG16_INPUT_RESIZE_DIV, height // VGG16_INPUT_RESIZE_DIV)  # とりあえず1/4に（小さいか?）\n                clip_guide_images = [preprocess_vgg16_guide_image(im, size) for im in clip_guide_images]\n                clip_guide_images = torch.cat(clip_guide_images, dim=0)\n\n                clip_guide_images = self.vgg16_normalize(clip_guide_images).to(self.device).to(text_embeddings.dtype)\n                image_embeddings_vgg16 = self.vgg16_feat_model(clip_guide_images)[\"feat\"]\n                if len(image_embeddings_vgg16) == 1:\n                    image_embeddings_vgg16 = image_embeddings_vgg16.repeat((batch_size, 1, 1, 1))\n            else:\n                # ControlNetのhintにguide imageを流用する\n                # 前処理はControlNet側で行う\n                pass\n\n        # set timesteps\n        self.scheduler.set_timesteps(num_inference_steps, self.device)\n\n        latents_dtype = text_embeddings.dtype\n        init_latents_orig = None\n        mask = None\n\n        if init_image is None:\n            # get the initial random noise unless the user supplied it\n\n            # Unlike in other pipelines, latents need to be generated in the target device\n            # for 1-to-1 results reproducibility with the CompVis implementation.\n            # However this currently doesn't work in `mps`.\n            latents_shape = (\n                batch_size * num_images_per_prompt,\n                self.unet.in_channels,\n                height // 8,\n                width // 8,\n            )\n\n            if latents is None:\n                if self.device.type == \"mps\":\n                    # randn does not exist on mps\n                    latents = torch.randn(\n                        latents_shape,\n                        generator=generator,\n                        device=\"cpu\",\n                        dtype=latents_dtype,\n                    ).to(self.device)\n                else:\n                    latents = torch.randn(\n                        latents_shape,\n                        generator=generator,\n                        device=self.device,\n                        dtype=latents_dtype,\n                    )\n            else:\n                if latents.shape != latents_shape:\n                    raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n                latents = latents.to(self.device)\n\n            timesteps = self.scheduler.timesteps.to(self.device)\n\n            # scale the initial noise by the standard deviation required by the scheduler\n            latents = latents * self.scheduler.init_noise_sigma\n        else:\n            # image to tensor\n            if isinstance(init_image, PIL.Image.Image):\n                init_image = [init_image]\n            if isinstance(init_image[0], PIL.Image.Image):\n                init_image = [preprocess_image(im) for im in init_image]\n                init_image = torch.cat(init_image)\n            if isinstance(init_image, list):\n                init_image = torch.stack(init_image)\n\n            # mask image to tensor\n            if mask_image is not None:\n                if isinstance(mask_image, PIL.Image.Image):\n                    mask_image = [mask_image]\n                if isinstance(mask_image[0], PIL.Image.Image):\n                    mask_image = torch.cat([preprocess_mask(im) for im in mask_image])  # H*W, 0 for repaint\n\n            # encode the init image into latents and scale the latents\n            init_image = init_image.to(device=self.device, dtype=latents_dtype)\n            if init_image.size()[-2:] == (height // 8, width // 8):\n                init_latents = init_image\n            else:\n                if vae_batch_size >= batch_size:\n                    init_latent_dist = self.vae.encode(init_image).latent_dist\n                    init_latents = init_latent_dist.sample(generator=generator)\n                else:\n                    clean_memory()\n                    init_latents = []\n                    for i in tqdm(range(0, min(batch_size, len(init_image)), vae_batch_size)):\n                        init_latent_dist = self.vae.encode(\n                            init_image[i : i + vae_batch_size] if vae_batch_size > 1 else init_image[i].unsqueeze(0)\n                        ).latent_dist\n                        init_latents.append(init_latent_dist.sample(generator=generator))\n                    init_latents = torch.cat(init_latents)\n\n                init_latents = 0.18215 * init_latents\n\n            if len(init_latents) == 1:\n                init_latents = init_latents.repeat((batch_size, 1, 1, 1))\n            init_latents_orig = init_latents\n\n            # preprocess mask\n            if mask_image is not None:\n                mask = mask_image.to(device=self.device, dtype=latents_dtype)\n                if len(mask) == 1:\n                    mask = mask.repeat((batch_size, 1, 1, 1))\n\n                # check sizes\n                if not mask.shape == init_latents.shape:\n                    raise ValueError(\"The mask and init_image should be the same size!\")\n\n            # get the original timestep using init_timestep\n            offset = self.scheduler.config.get(\"steps_offset\", 0)\n            init_timestep = int(num_inference_steps * strength) + offset\n            init_timestep = min(init_timestep, num_inference_steps)\n\n            timesteps = self.scheduler.timesteps[-init_timestep]\n            timesteps = torch.tensor([timesteps] * batch_size * num_images_per_prompt, device=self.device)\n\n            # add noise to latents using the timesteps\n            latents = self.scheduler.add_noise(init_latents, img2img_noise, timesteps)\n\n            t_start = max(num_inference_steps - init_timestep + offset, 0)\n            timesteps = self.scheduler.timesteps[t_start:].to(self.device)\n\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        num_latent_input = (3 if negative_scale is not None else 2) if do_classifier_free_guidance else 1\n\n        if self.control_nets:\n            guided_hints = original_control_net.get_guided_hints(self.control_nets, num_latent_input, batch_size, clip_guide_images)\n\n        if reginonal_network:\n            num_sub_and_neg_prompts = len(text_embeddings) // batch_size\n            # last subprompt and negative prompt\n            text_emb_last = []\n            for j in range(batch_size):\n                text_emb_last.append(text_embeddings[(j + 1) * num_sub_and_neg_prompts - 2])\n                text_emb_last.append(text_embeddings[(j + 1) * num_sub_and_neg_prompts - 1])\n            text_emb_last = torch.stack(text_emb_last)\n        else:\n            text_emb_last = text_embeddings\n\n        enable_gradual_latent = False\n        if self.gradual_latent:\n            if not hasattr(self.scheduler, \"set_gradual_latent_params\"):\n                logger.info(\"gradual_latent is not supported for this scheduler. Ignoring.\")\n                logger.info(f'{self.scheduler.__class__.__name__}')\n            else:\n                enable_gradual_latent = True\n                step_elapsed = 1000\n                current_ratio = self.gradual_latent.ratio\n\n                # first, we downscale the latents to the specified ratio / 最初に指定された比率にlatentsをダウンスケールする\n                height, width = latents.shape[-2:]\n                org_dtype = latents.dtype\n                if org_dtype == torch.bfloat16:\n                    latents = latents.float()\n                latents = torch.nn.functional.interpolate(\n                    latents, scale_factor=current_ratio, mode=\"bicubic\", align_corners=False\n                ).to(org_dtype)\n\n                # apply unsharp mask / アンシャープマスクを適用する\n                if self.gradual_latent.gaussian_blur_ksize:\n                    latents = self.gradual_latent.apply_unshark_mask(latents)\n\n        for i, t in enumerate(tqdm(timesteps)):\n            resized_size = None\n            if enable_gradual_latent:\n                # gradually upscale the latents / latentsを徐々にアップスケールする\n                if (\n                    t < self.gradual_latent.start_timesteps\n                    and current_ratio < 1.0\n                    and step_elapsed >= self.gradual_latent.every_n_steps\n                ):\n                    current_ratio = min(current_ratio + self.gradual_latent.ratio_step, 1.0)\n                    # make divisible by 8 because size of latents must be divisible at bottom of UNet\n                    h = int(height * current_ratio) // 8 * 8\n                    w = int(width * current_ratio) // 8 * 8\n                    resized_size = (h, w)\n                    self.scheduler.set_gradual_latent_params(resized_size, self.gradual_latent)\n                    step_elapsed = 0\n                else:\n                    self.scheduler.set_gradual_latent_params(None, None)\n                step_elapsed += 1\n\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = latents.repeat((num_latent_input, 1, 1, 1))\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n            # predict the noise residual\n            if self.control_nets and self.control_net_enabled:\n                noise_pred = original_control_net.call_unet_and_control_net(\n                    i,\n                    num_latent_input,\n                    self.unet,\n                    self.control_nets,\n                    guided_hints,\n                    i / len(timesteps),\n                    latent_model_input,\n                    t,\n                    text_embeddings,\n                    text_emb_last,\n                ).sample\n            else:\n                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                if negative_scale is None:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(num_latent_input)  # uncond by negative prompt\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n                else:\n                    noise_pred_negative, noise_pred_text, noise_pred_uncond = noise_pred.chunk(\n                        num_latent_input\n                    )  # uncond is real uncond\n                    noise_pred = (\n                        noise_pred_uncond\n                        + guidance_scale * (noise_pred_text - noise_pred_uncond)\n                        - negative_scale * (noise_pred_negative - noise_pred_uncond)\n                    )\n\n            # perform clip guidance\n            if self.clip_guidance_scale > 0 or self.clip_image_guidance_scale > 0 or self.vgg16_guidance_scale > 0:\n                text_embeddings_for_guidance = (\n                    text_embeddings.chunk(num_latent_input)[1] if do_classifier_free_guidance else text_embeddings\n                )\n\n                if self.clip_guidance_scale > 0:\n                    noise_pred, latents = self.cond_fn(\n                        latents,\n                        t,\n                        i,\n                        text_embeddings_for_guidance,\n                        noise_pred,\n                        text_embeddings_clip,\n                        self.clip_guidance_scale,\n                        NUM_CUTOUTS,\n                        USE_CUTOUTS,\n                    )\n                if self.clip_image_guidance_scale > 0 and clip_guide_images is not None:\n                    noise_pred, latents = self.cond_fn(\n                        latents,\n                        t,\n                        i,\n                        text_embeddings_for_guidance,\n                        noise_pred,\n                        image_embeddings_clip,\n                        self.clip_image_guidance_scale,\n                        NUM_CUTOUTS,\n                        USE_CUTOUTS,\n                    )\n                if self.vgg16_guidance_scale > 0 and clip_guide_images is not None:\n                    noise_pred, latents = self.cond_fn_vgg16(\n                        latents, t, i, text_embeddings_for_guidance, noise_pred, image_embeddings_vgg16, self.vgg16_guidance_scale\n                    )\n\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n            if mask is not None:\n                # masking\n                init_latents_proper = self.scheduler.add_noise(init_latents_orig, img2img_noise, torch.tensor([t]))\n                latents = (init_latents_proper * mask) + (latents * (1 - mask))\n\n            # call the callback, if provided\n            if i % callback_steps == 0:\n                if callback is not None:\n                    callback(i, t, latents)\n                if is_cancelled_callback is not None and is_cancelled_callback():\n                    return None\n\n        if return_latents:\n            return (latents, False)\n\n        latents = 1 / 0.18215 * latents\n        if vae_batch_size >= batch_size:\n            image = self.vae.decode(latents).sample\n        else:\n            clean_memory()\n            images = []\n            for i in tqdm(range(0, batch_size, vae_batch_size)):\n                images.append(\n                    self.vae.decode(latents[i : i + vae_batch_size] if vae_batch_size > 1 else latents[i].unsqueeze(0)).sample\n                )\n            image = torch.cat(images)\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image,\n                clip_input=safety_checker_input.pixel_values.to(text_embeddings.dtype),\n            )\n        else:\n            has_nsfw_concept = None\n\n        if output_type == \"pil\":\n            # image = self.numpy_to_pil(image)\n            image = (image * 255).round().astype(\"uint8\")\n            image = [Image.fromarray(im) for im in image]\n\n        # if not return_dict:\n        return (image, has_nsfw_concept)\n\n        # return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n\n    def text2img(\n        self,\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        height: int = 512,\n        width: int = 512,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function for text-to-image generation.\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            height (`int`, *optional*, defaults to 512):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to 512):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            max_embeddings_multiples (`int`, *optional*, defaults to `3`):\n                The max multiple length of prompt embeddings compared to the max output length of text encoder.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        return self.__call__(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            generator=generator,\n            latents=latents,\n            max_embeddings_multiples=max_embeddings_multiples,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            **kwargs,\n        )\n\n    def img2img(\n        self,\n        init_image: Union[torch.FloatTensor, PIL.Image.Image],\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[torch.Generator] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function for image-to-image generation.\n        Args:\n            init_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to transform the reference `init_image`. Must be between 0 and 1.\n                `init_image` will be used as a starting point, adding more noise to it the larger the `strength`. The\n                number of denoising steps depends on the amount of noise initially added. When `strength` is 1, added\n                noise will be maximum and the denoising process will run for the full number of iterations specified in\n                `num_inference_steps`. A value of 1, therefore, essentially ignores `init_image`.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference. This parameter will be modulated by `strength`.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            max_embeddings_multiples (`int`, *optional*, defaults to `3`):\n                The max multiple length of prompt embeddings compared to the max output length of text encoder.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        return self.__call__(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            init_image=init_image,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            strength=strength,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            generator=generator,\n            max_embeddings_multiples=max_embeddings_multiples,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            **kwargs,\n        )\n\n    def inpaint(\n        self,\n        init_image: Union[torch.FloatTensor, PIL.Image.Image],\n        mask_image: Union[torch.FloatTensor, PIL.Image.Image],\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[torch.Generator] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function for inpaint.\n        Args:\n            init_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process. This is the image whose masked region will be inpainted.\n            mask_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `init_image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n                contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength`\n                is 1, the denoising process will be run on the masked area for the full number of iterations specified\n                in `num_inference_steps`. `init_image` will be used as a reference for the masked area, adding more\n                noise to that region the larger the `strength`. If `strength` is 0, no inpainting will occur.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The reference number of denoising steps. More denoising steps usually lead to a higher quality image at\n                the expense of slower inference. This parameter will be modulated by `strength`, as explained above.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            max_embeddings_multiples (`int`, *optional*, defaults to `3`):\n                The max multiple length of prompt embeddings compared to the max output length of text encoder.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        return self.__call__(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            init_image=init_image,\n            mask_image=mask_image,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            strength=strength,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            generator=generator,\n            max_embeddings_multiples=max_embeddings_multiples,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            **kwargs,\n        )\n\n    # CLIP guidance StableDiffusion\n    # copy from https://github.com/huggingface/diffusers/blob/main/examples/community/clip_guided_stable_diffusion.py\n\n    # バッチを分解して1件ずつ処理する\n    def cond_fn(\n        self,\n        latents,\n        timestep,\n        index,\n        text_embeddings,\n        noise_pred_original,\n        guide_embeddings_clip,\n        clip_guidance_scale,\n        num_cutouts,\n        use_cutouts=True,\n    ):\n        if len(latents) == 1:\n            return self.cond_fn1(\n                latents,\n                timestep,\n                index,\n                text_embeddings,\n                noise_pred_original,\n                guide_embeddings_clip,\n                clip_guidance_scale,\n                num_cutouts,\n                use_cutouts,\n            )\n\n        noise_pred = []\n        cond_latents = []\n        for i in range(len(latents)):\n            lat1 = latents[i].unsqueeze(0)\n            tem1 = text_embeddings[i].unsqueeze(0)\n            npo1 = noise_pred_original[i].unsqueeze(0)\n            gem1 = guide_embeddings_clip[i].unsqueeze(0)\n            npr1, cla1 = self.cond_fn1(lat1, timestep, index, tem1, npo1, gem1, clip_guidance_scale, num_cutouts, use_cutouts)\n            noise_pred.append(npr1)\n            cond_latents.append(cla1)\n\n        noise_pred = torch.cat(noise_pred)\n        cond_latents = torch.cat(cond_latents)\n        return noise_pred, cond_latents\n\n    @torch.enable_grad()\n    def cond_fn1(\n        self,\n        latents,\n        timestep,\n        index,\n        text_embeddings,\n        noise_pred_original,\n        guide_embeddings_clip,\n        clip_guidance_scale,\n        num_cutouts,\n        use_cutouts=True,\n    ):\n        latents = latents.detach().requires_grad_()\n\n        if isinstance(self.scheduler, LMSDiscreteScheduler):\n            sigma = self.scheduler.sigmas[index]\n            # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n            latent_model_input = latents / ((sigma**2 + 1) ** 0.5)\n        else:\n            latent_model_input = latents\n\n        # predict the noise residual\n        noise_pred = self.unet(latent_model_input, timestep, encoder_hidden_states=text_embeddings).sample\n\n        if isinstance(self.scheduler, (PNDMScheduler, DDIMScheduler)):\n            alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n            beta_prod_t = 1 - alpha_prod_t\n            # compute predicted original sample from predicted noise also called\n            # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n            pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n\n            fac = torch.sqrt(beta_prod_t)\n            sample = pred_original_sample * (fac) + latents * (1 - fac)\n        elif isinstance(self.scheduler, LMSDiscreteScheduler):\n            sigma = self.scheduler.sigmas[index]\n            sample = latents - sigma * noise_pred\n        else:\n            raise ValueError(f\"scheduler type {type(self.scheduler)} not supported\")\n\n        sample = 1 / 0.18215 * sample\n        image = self.vae.decode(sample).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n\n        if use_cutouts:\n            image = self.make_cutouts(image, num_cutouts)\n        else:\n            image = transforms.Resize(FEATURE_EXTRACTOR_SIZE)(image)\n        image = self.normalize(image).to(latents.dtype)\n\n        image_embeddings_clip = self.clip_model.get_image_features(image)\n        image_embeddings_clip = image_embeddings_clip / image_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n\n        if use_cutouts:\n            dists = spherical_dist_loss(image_embeddings_clip, guide_embeddings_clip)\n            dists = dists.view([num_cutouts, sample.shape[0], -1])\n            loss = dists.sum(2).mean(0).sum() * clip_guidance_scale\n        else:\n            # バッチサイズが複数だと正しく動くかわからない\n            loss = spherical_dist_loss(image_embeddings_clip, guide_embeddings_clip).mean() * clip_guidance_scale\n\n        grads = -torch.autograd.grad(loss, latents)[0]\n\n        if isinstance(self.scheduler, LMSDiscreteScheduler):\n            latents = latents.detach() + grads * (sigma**2)\n            noise_pred = noise_pred_original\n        else:\n            noise_pred = noise_pred_original - torch.sqrt(beta_prod_t) * grads\n        return noise_pred, latents\n\n    # バッチを分解して一件ずつ処理する\n    def cond_fn_vgg16(self, latents, timestep, index, text_embeddings, noise_pred_original, guide_embeddings, guidance_scale):\n        if len(latents) == 1:\n            return self.cond_fn_vgg16_b1(\n                latents, timestep, index, text_embeddings, noise_pred_original, guide_embeddings, guidance_scale\n            )\n\n        noise_pred = []\n        cond_latents = []\n        for i in range(len(latents)):\n            lat1 = latents[i].unsqueeze(0)\n            tem1 = text_embeddings[i].unsqueeze(0)\n            npo1 = noise_pred_original[i].unsqueeze(0)\n            gem1 = guide_embeddings[i].unsqueeze(0)\n            npr1, cla1 = self.cond_fn_vgg16_b1(lat1, timestep, index, tem1, npo1, gem1, guidance_scale)\n            noise_pred.append(npr1)\n            cond_latents.append(cla1)\n\n        noise_pred = torch.cat(noise_pred)\n        cond_latents = torch.cat(cond_latents)\n        return noise_pred, cond_latents\n\n    # 1件だけ処理する\n    @torch.enable_grad()\n    def cond_fn_vgg16_b1(self, latents, timestep, index, text_embeddings, noise_pred_original, guide_embeddings, guidance_scale):\n        latents = latents.detach().requires_grad_()\n\n        if isinstance(self.scheduler, LMSDiscreteScheduler):\n            sigma = self.scheduler.sigmas[index]\n            # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n            latent_model_input = latents / ((sigma**2 + 1) ** 0.5)\n        else:\n            latent_model_input = latents\n\n        # predict the noise residual\n        noise_pred = self.unet(latent_model_input, timestep, encoder_hidden_states=text_embeddings).sample\n\n        if isinstance(self.scheduler, (PNDMScheduler, DDIMScheduler)):\n            alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n            beta_prod_t = 1 - alpha_prod_t\n            # compute predicted original sample from predicted noise also called\n            # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n            pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n\n            fac = torch.sqrt(beta_prod_t)\n            sample = pred_original_sample * (fac) + latents * (1 - fac)\n        elif isinstance(self.scheduler, LMSDiscreteScheduler):\n            sigma = self.scheduler.sigmas[index]\n            sample = latents - sigma * noise_pred\n        else:\n            raise ValueError(f\"scheduler type {type(self.scheduler)} not supported\")\n\n        sample = 1 / 0.18215 * sample\n        image = self.vae.decode(sample).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        image = transforms.Resize((image.shape[-2] // VGG16_INPUT_RESIZE_DIV, image.shape[-1] // VGG16_INPUT_RESIZE_DIV))(image)\n        image = self.vgg16_normalize(image).to(latents.dtype)\n\n        image_embeddings = self.vgg16_feat_model(image)[\"feat\"]\n\n        # バッチサイズが複数だと正しく動くかわからない\n        loss = (\n            (image_embeddings - guide_embeddings) ** 2\n        ).mean() * guidance_scale  # MSE style transferでコンテンツの損失はMSEなので\n\n        grads = -torch.autograd.grad(loss, latents)[0]\n        if isinstance(self.scheduler, LMSDiscreteScheduler):\n            latents = latents.detach() + grads * (sigma**2)\n            noise_pred = noise_pred_original\n        else:\n            noise_pred = noise_pred_original - torch.sqrt(beta_prod_t) * grads\n        return noise_pred, latents\n\n\nclass MakeCutouts(torch.nn.Module):\n    def __init__(self, cut_size, cut_power=1.0):\n        super().__init__()\n\n        self.cut_size = cut_size\n        self.cut_power = cut_power\n\n    def forward(self, pixel_values, num_cutouts):\n        sideY, sideX = pixel_values.shape[2:4]\n        max_size = min(sideX, sideY)\n        min_size = min(sideX, sideY, self.cut_size)\n        cutouts = []\n        for _ in range(num_cutouts):\n            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)\n            offsetx = torch.randint(0, sideX - size + 1, ())\n            offsety = torch.randint(0, sideY - size + 1, ())\n            cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]\n            cutouts.append(torch.nn.functional.adaptive_avg_pool2d(cutout, self.cut_size))\n        return torch.cat(cutouts)\n\n\ndef spherical_dist_loss(x, y):\n    x = torch.nn.functional.normalize(x, dim=-1)\n    y = torch.nn.functional.normalize(y, dim=-1)\n    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n\n\nre_attention = re.compile(\n    r\"\"\"\n\\\\\\(|\n\\\\\\)|\n\\\\\\[|\n\\\\]|\n\\\\\\\\|\n\\\\|\n\\(|\n\\[|\n:([+-]?[.\\d]+)\\)|\n\\)|\n]|\n[^\\\\()\\[\\]:]+|\n:\n\"\"\",\n    re.X,\n)\n\n\ndef parse_prompt_attention(text):\n    \"\"\"\n    Parses a string with attention tokens and returns a list of pairs: text and its associated weight.\n    Accepted tokens are:\n      (abc) - increases attention to abc by a multiplier of 1.1\n      (abc:3.12) - increases attention to abc by a multiplier of 3.12\n      [abc] - decreases attention to abc by a multiplier of 1.1\n      \\( - literal character '('\n      \\[ - literal character '['\n      \\) - literal character ')'\n      \\] - literal character ']'\n      \\\\ - literal character '\\'\n      anything else - just text\n    >>> parse_prompt_attention('normal text')\n    [['normal text', 1.0]]\n    >>> parse_prompt_attention('an (important) word')\n    [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n    >>> parse_prompt_attention('(unbalanced')\n    [['unbalanced', 1.1]]\n    >>> parse_prompt_attention('\\(literal\\]')\n    [['(literal]', 1.0]]\n    >>> parse_prompt_attention('(unnecessary)(parens)')\n    [['unnecessaryparens', 1.1]]\n    >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n    [['a ', 1.0],\n     ['house', 1.5730000000000004],\n     [' ', 1.1],\n     ['on', 1.0],\n     [' a ', 1.1],\n     ['hill', 0.55],\n     [', sun, ', 1.1],\n     ['sky', 1.4641000000000006],\n     ['.', 1.1]]\n    \"\"\"\n\n    res = []\n    round_brackets = []\n    square_brackets = []\n\n    round_bracket_multiplier = 1.1\n    square_bracket_multiplier = 1 / 1.1\n\n    def multiply_range(start_position, multiplier):\n        for p in range(start_position, len(res)):\n            res[p][1] *= multiplier\n\n    # keep break as separate token\n    text = text.replace(\"BREAK\", \"\\\\BREAK\\\\\")\n\n    for m in re_attention.finditer(text):\n        text = m.group(0)\n        weight = m.group(1)\n\n        if text.startswith(\"\\\\\"):\n            res.append([text[1:], 1.0])\n        elif text == \"(\":\n            round_brackets.append(len(res))\n        elif text == \"[\":\n            square_brackets.append(len(res))\n        elif weight is not None and len(round_brackets) > 0:\n            multiply_range(round_brackets.pop(), float(weight))\n        elif text == \")\" and len(round_brackets) > 0:\n            multiply_range(round_brackets.pop(), round_bracket_multiplier)\n        elif text == \"]\" and len(square_brackets) > 0:\n            multiply_range(square_brackets.pop(), square_bracket_multiplier)\n        else:\n            res.append([text, 1.0])\n\n    for pos in round_brackets:\n        multiply_range(pos, round_bracket_multiplier)\n\n    for pos in square_brackets:\n        multiply_range(pos, square_bracket_multiplier)\n\n    if len(res) == 0:\n        res = [[\"\", 1.0]]\n\n    # merge runs of identical weights\n    i = 0\n    while i + 1 < len(res):\n        if res[i][1] == res[i + 1][1] and res[i][0].strip() != \"BREAK\" and res[i + 1][0].strip() != \"BREAK\":\n            res[i][0] += res[i + 1][0]\n            res.pop(i + 1)\n        else:\n            i += 1\n\n    return res\n\n\ndef get_prompts_with_weights(pipe: PipelineLike, prompt: List[str], max_length: int, layer=None):\n    r\"\"\"\n    Tokenize a list of prompts and return its tokens with weights of each token.\n    No padding, starting or ending token is included.\n    \"\"\"\n    tokens = []\n    weights = []\n    truncated = False\n\n    for text in prompt:\n        texts_and_weights = parse_prompt_attention(text)\n        text_token = []\n        text_weight = []\n        for word, weight in texts_and_weights:\n            if word.strip() == \"BREAK\":\n                # pad until next multiple of tokenizer's max token length\n                pad_len = pipe.tokenizer.model_max_length - (len(text_token) % pipe.tokenizer.model_max_length)\n                logger.info(f\"BREAK pad_len: {pad_len}\")\n                for i in range(pad_len):\n                    # v2のときEOSをつけるべきかどうかわからないぜ\n                    # if i == 0:\n                    #     text_token.append(pipe.tokenizer.eos_token_id)\n                    # else:\n                    text_token.append(pipe.tokenizer.pad_token_id)\n                    text_weight.append(1.0)\n                continue\n\n            # tokenize and discard the starting and the ending token\n            token = pipe.tokenizer(word).input_ids[1:-1]\n\n            token = pipe.replace_token(token, layer=layer)\n\n            text_token += token\n            # copy the weight by length of token\n            text_weight += [weight] * len(token)\n            # stop if the text is too long (longer than truncation limit)\n            if len(text_token) > max_length:\n                truncated = True\n                break\n        # truncate\n        if len(text_token) > max_length:\n            truncated = True\n            text_token = text_token[:max_length]\n            text_weight = text_weight[:max_length]\n        tokens.append(text_token)\n        weights.append(text_weight)\n    if truncated:\n        logger.warning(\"Prompt was truncated. Try to shorten the prompt or increase max_embeddings_multiples\")\n    return tokens, weights\n\n\ndef pad_tokens_and_weights(tokens, weights, max_length, bos, eos, pad, no_boseos_middle=True, chunk_length=77):\n    r\"\"\"\n    Pad the tokens (with starting and ending tokens) and weights (with 1.0) to max_length.\n    \"\"\"\n    max_embeddings_multiples = (max_length - 2) // (chunk_length - 2)\n    weights_length = max_length if no_boseos_middle else max_embeddings_multiples * chunk_length\n    for i in range(len(tokens)):\n        tokens[i] = [bos] + tokens[i] + [eos] + [pad] * (max_length - 2 - len(tokens[i]))\n        if no_boseos_middle:\n            weights[i] = [1.0] + weights[i] + [1.0] * (max_length - 1 - len(weights[i]))\n        else:\n            w = []\n            if len(weights[i]) == 0:\n                w = [1.0] * weights_length\n            else:\n                for j in range(max_embeddings_multiples):\n                    w.append(1.0)  # weight for starting token in this chunk\n                    w += weights[i][j * (chunk_length - 2) : min(len(weights[i]), (j + 1) * (chunk_length - 2))]\n                    w.append(1.0)  # weight for ending token in this chunk\n                w += [1.0] * (weights_length - len(w))\n            weights[i] = w[:]\n\n    return tokens, weights\n\n\ndef get_unweighted_text_embeddings(\n    pipe: PipelineLike,\n    text_input: torch.Tensor,\n    chunk_length: int,\n    clip_skip: int,\n    eos: int,\n    pad: int,\n    no_boseos_middle: Optional[bool] = True,\n):\n    \"\"\"\n    When the length of tokens is a multiple of the capacity of the text encoder,\n    it should be split into chunks and sent to the text encoder individually.\n    \"\"\"\n    max_embeddings_multiples = (text_input.shape[1] - 2) // (chunk_length - 2)\n    if max_embeddings_multiples > 1:\n        text_embeddings = []\n        for i in range(max_embeddings_multiples):\n            # extract the i-th chunk\n            text_input_chunk = text_input[:, i * (chunk_length - 2) : (i + 1) * (chunk_length - 2) + 2].clone()\n\n            # cover the head and the tail by the starting and the ending tokens\n            text_input_chunk[:, 0] = text_input[0, 0]\n            if pad == eos:  # v1\n                text_input_chunk[:, -1] = text_input[0, -1]\n            else:  # v2\n                for j in range(len(text_input_chunk)):\n                    if text_input_chunk[j, -1] != eos and text_input_chunk[j, -1] != pad:  # 最後に普通の文字がある\n                        text_input_chunk[j, -1] = eos\n                    if text_input_chunk[j, 1] == pad:  # BOSだけであとはPAD\n                        text_input_chunk[j, 1] = eos\n\n            if clip_skip is None or clip_skip == 1:\n                text_embedding = pipe.text_encoder(text_input_chunk)[0]\n            else:\n                enc_out = pipe.text_encoder(text_input_chunk, output_hidden_states=True, return_dict=True)\n                text_embedding = enc_out[\"hidden_states\"][-clip_skip]\n                text_embedding = pipe.text_encoder.text_model.final_layer_norm(text_embedding)\n\n            if no_boseos_middle:\n                if i == 0:\n                    # discard the ending token\n                    text_embedding = text_embedding[:, :-1]\n                elif i == max_embeddings_multiples - 1:\n                    # discard the starting token\n                    text_embedding = text_embedding[:, 1:]\n                else:\n                    # discard both starting and ending tokens\n                    text_embedding = text_embedding[:, 1:-1]\n\n            text_embeddings.append(text_embedding)\n        text_embeddings = torch.concat(text_embeddings, axis=1)\n    else:\n        if clip_skip is None or clip_skip == 1:\n            text_embeddings = pipe.text_encoder(text_input)[0]\n        else:\n            enc_out = pipe.text_encoder(text_input, output_hidden_states=True, return_dict=True)\n            text_embeddings = enc_out[\"hidden_states\"][-clip_skip]\n            text_embeddings = pipe.text_encoder.text_model.final_layer_norm(text_embeddings)\n    return text_embeddings\n\n\ndef get_weighted_text_embeddings(\n    pipe: PipelineLike,\n    prompt: Union[str, List[str]],\n    uncond_prompt: Optional[Union[str, List[str]]] = None,\n    max_embeddings_multiples: Optional[int] = 1,\n    no_boseos_middle: Optional[bool] = False,\n    skip_parsing: Optional[bool] = False,\n    skip_weighting: Optional[bool] = False,\n    clip_skip=None,\n    layer=None,\n    **kwargs,\n):\n    r\"\"\"\n    Prompts can be assigned with local weights using brackets. For example,\n    prompt 'A (very beautiful) masterpiece' highlights the words 'very beautiful',\n    and the embedding tokens corresponding to the words get multiplied by a constant, 1.1.\n    Also, to regularize of the embedding, the weighted embedding would be scaled to preserve the original mean.\n    Args:\n        pipe (`DiffusionPipeline`):\n            Pipe to provide access to the tokenizer and the text encoder.\n        prompt (`str` or `List[str]`):\n            The prompt or prompts to guide the image generation.\n        uncond_prompt (`str` or `List[str]`):\n            The unconditional prompt or prompts for guide the image generation. If unconditional prompt\n            is provided, the embeddings of prompt and uncond_prompt are concatenated.\n        max_embeddings_multiples (`int`, *optional*, defaults to `1`):\n            The max multiple length of prompt embeddings compared to the max output length of text encoder.\n        no_boseos_middle (`bool`, *optional*, defaults to `False`):\n            If the length of text token is multiples of the capacity of text encoder, whether reserve the starting and\n            ending token in each of the chunk in the middle.\n        skip_parsing (`bool`, *optional*, defaults to `False`):\n            Skip the parsing of brackets.\n        skip_weighting (`bool`, *optional*, defaults to `False`):\n            Skip the weighting. When the parsing is skipped, it is forced True.\n    \"\"\"\n    max_length = (pipe.tokenizer.model_max_length - 2) * max_embeddings_multiples + 2\n    if isinstance(prompt, str):\n        prompt = [prompt]\n\n    # split the prompts with \"AND\". each prompt must have the same number of splits\n    new_prompts = []\n    for p in prompt:\n        new_prompts.extend(p.split(\" AND \"))\n    prompt = new_prompts\n\n    if not skip_parsing:\n        prompt_tokens, prompt_weights = get_prompts_with_weights(pipe, prompt, max_length - 2, layer=layer)\n        if uncond_prompt is not None:\n            if isinstance(uncond_prompt, str):\n                uncond_prompt = [uncond_prompt]\n            uncond_tokens, uncond_weights = get_prompts_with_weights(pipe, uncond_prompt, max_length - 2, layer=layer)\n    else:\n        prompt_tokens = [token[1:-1] for token in pipe.tokenizer(prompt, max_length=max_length, truncation=True).input_ids]\n        prompt_weights = [[1.0] * len(token) for token in prompt_tokens]\n        if uncond_prompt is not None:\n            if isinstance(uncond_prompt, str):\n                uncond_prompt = [uncond_prompt]\n            uncond_tokens = [\n                token[1:-1] for token in pipe.tokenizer(uncond_prompt, max_length=max_length, truncation=True).input_ids\n            ]\n            uncond_weights = [[1.0] * len(token) for token in uncond_tokens]\n\n    # round up the longest length of tokens to a multiple of (model_max_length - 2)\n    max_length = max([len(token) for token in prompt_tokens])\n    if uncond_prompt is not None:\n        max_length = max(max_length, max([len(token) for token in uncond_tokens]))\n\n    max_embeddings_multiples = min(\n        max_embeddings_multiples,\n        (max_length - 1) // (pipe.tokenizer.model_max_length - 2) + 1,\n    )\n    max_embeddings_multiples = max(1, max_embeddings_multiples)\n    max_length = (pipe.tokenizer.model_max_length - 2) * max_embeddings_multiples + 2\n\n    # pad the length of tokens and weights\n    bos = pipe.tokenizer.bos_token_id\n    eos = pipe.tokenizer.eos_token_id\n    pad = pipe.tokenizer.pad_token_id\n    prompt_tokens, prompt_weights = pad_tokens_and_weights(\n        prompt_tokens,\n        prompt_weights,\n        max_length,\n        bos,\n        eos,\n        pad,\n        no_boseos_middle=no_boseos_middle,\n        chunk_length=pipe.tokenizer.model_max_length,\n    )\n    prompt_tokens = torch.tensor(prompt_tokens, dtype=torch.long, device=pipe.device)\n    if uncond_prompt is not None:\n        uncond_tokens, uncond_weights = pad_tokens_and_weights(\n            uncond_tokens,\n            uncond_weights,\n            max_length,\n            bos,\n            eos,\n            pad,\n            no_boseos_middle=no_boseos_middle,\n            chunk_length=pipe.tokenizer.model_max_length,\n        )\n        uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=pipe.device)\n\n    # get the embeddings\n    text_embeddings = get_unweighted_text_embeddings(\n        pipe,\n        prompt_tokens,\n        pipe.tokenizer.model_max_length,\n        clip_skip,\n        eos,\n        pad,\n        no_boseos_middle=no_boseos_middle,\n    )\n    prompt_weights = torch.tensor(prompt_weights, dtype=text_embeddings.dtype, device=pipe.device)\n    if uncond_prompt is not None:\n        uncond_embeddings = get_unweighted_text_embeddings(\n            pipe,\n            uncond_tokens,\n            pipe.tokenizer.model_max_length,\n            clip_skip,\n            eos,\n            pad,\n            no_boseos_middle=no_boseos_middle,\n        )\n        uncond_weights = torch.tensor(uncond_weights, dtype=uncond_embeddings.dtype, device=pipe.device)\n\n    # assign weights to the prompts and normalize in the sense of mean\n    # TODO: should we normalize by chunk or in a whole (current implementation)?\n    # →全体でいいんじゃないかな\n    if (not skip_parsing) and (not skip_weighting):\n        previous_mean = text_embeddings.float().mean(axis=[-2, -1]).to(text_embeddings.dtype)\n        text_embeddings *= prompt_weights.unsqueeze(-1)\n        current_mean = text_embeddings.float().mean(axis=[-2, -1]).to(text_embeddings.dtype)\n        text_embeddings *= (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)\n        if uncond_prompt is not None:\n            previous_mean = uncond_embeddings.float().mean(axis=[-2, -1]).to(uncond_embeddings.dtype)\n            uncond_embeddings *= uncond_weights.unsqueeze(-1)\n            current_mean = uncond_embeddings.float().mean(axis=[-2, -1]).to(uncond_embeddings.dtype)\n            uncond_embeddings *= (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)\n\n    if uncond_prompt is not None:\n        return text_embeddings, uncond_embeddings, prompt_tokens\n    return text_embeddings, None, prompt_tokens\n\n\ndef preprocess_guide_image(image):\n    image = image.resize(FEATURE_EXTRACTOR_SIZE, resample=Image.NEAREST)  # cond_fnと合わせる\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)  # nchw\n    image = torch.from_numpy(image)\n    return image  # 0 to 1\n\n\n# VGG16の入力は任意サイズでよいので入力画像を適宜リサイズする\ndef preprocess_vgg16_guide_image(image, size):\n    image = image.resize(size, resample=Image.NEAREST)  # cond_fnと合わせる\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)  # nchw\n    image = torch.from_numpy(image)\n    return image  # 0 to 1\n\n\ndef preprocess_image(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0\n\n\ndef preprocess_mask(mask):\n    mask = mask.convert(\"L\")\n    w, h = mask.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    mask = mask.resize((w // 8, h // 8), resample=PIL.Image.BILINEAR)  # LANCZOS)\n    mask = np.array(mask).astype(np.float32) / 255.0\n    mask = np.tile(mask, (4, 1, 1))\n    mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?\n    mask = 1 - mask  # repaint white, keep black\n    mask = torch.from_numpy(mask)\n    return mask\n\n\n# regular expression for dynamic prompt:\n# starts and ends with \"{\" and \"}\"\n# contains at least one variant divided by \"|\"\n# optional framgments divided by \"$$\" at start\n# if the first fragment is \"E\" or \"e\", enumerate all variants\n# if the second fragment is a number or two numbers, repeat the variants in the range\n# if the third fragment is a string, use it as a separator\n\nRE_DYNAMIC_PROMPT = re.compile(r\"\\{((e|E)\\$\\$)?(([\\d\\-]+)\\$\\$)?(([^\\|\\}]+?)\\$\\$)?(.+?((\\|).+?)*?)\\}\")\n\n\ndef handle_dynamic_prompt_variants(prompt, repeat_count):\n    founds = list(RE_DYNAMIC_PROMPT.finditer(prompt))\n    if not founds:\n        return [prompt]\n\n    # make each replacement for each variant\n    enumerating = False\n    replacers = []\n    for found in founds:\n        # if \"e$$\" is found, enumerate all variants\n        found_enumerating = found.group(2) is not None\n        enumerating = enumerating or found_enumerating\n\n        separator = \", \" if found.group(6) is None else found.group(6)\n        variants = found.group(7).split(\"|\")\n\n        # parse count range\n        count_range = found.group(4)\n        if count_range is None:\n            count_range = [1, 1]\n        else:\n            count_range = count_range.split(\"-\")\n            if len(count_range) == 1:\n                count_range = [int(count_range[0]), int(count_range[0])]\n            elif len(count_range) == 2:\n                count_range = [int(count_range[0]), int(count_range[1])]\n            else:\n                logger.warning(f\"invalid count range: {count_range}\")\n                count_range = [1, 1]\n            if count_range[0] > count_range[1]:\n                count_range = [count_range[1], count_range[0]]\n            if count_range[0] < 0:\n                count_range[0] = 0\n            if count_range[1] > len(variants):\n                count_range[1] = len(variants)\n\n        if found_enumerating:\n            # make function to enumerate all combinations\n            def make_replacer_enum(vari, cr, sep):\n                def replacer():\n                    values = []\n                    for count in range(cr[0], cr[1] + 1):\n                        for comb in itertools.combinations(vari, count):\n                            values.append(sep.join(comb))\n                    return values\n\n                return replacer\n\n            replacers.append(make_replacer_enum(variants, count_range, separator))\n        else:\n            # make function to choose random combinations\n            def make_replacer_single(vari, cr, sep):\n                def replacer():\n                    count = random.randint(cr[0], cr[1])\n                    comb = random.sample(vari, count)\n                    return [sep.join(comb)]\n\n                return replacer\n\n            replacers.append(make_replacer_single(variants, count_range, separator))\n\n    # make each prompt\n    if not enumerating:\n        # if not enumerating, repeat the prompt, replace each variant randomly\n        prompts = []\n        for _ in range(repeat_count):\n            current = prompt\n            for found, replacer in zip(founds, replacers):\n                current = current.replace(found.group(0), replacer()[0], 1)\n            prompts.append(current)\n    else:\n        # if enumerating, iterate all combinations for previous prompts\n        prompts = [prompt]\n\n        for found, replacer in zip(founds, replacers):\n            if found.group(2) is not None:\n                # make all combinations for existing prompts\n                new_prompts = []\n                for current in prompts:\n                    replecements = replacer()\n                    for replecement in replecements:\n                        new_prompts.append(current.replace(found.group(0), replecement, 1))\n                prompts = new_prompts\n\n        for found, replacer in zip(founds, replacers):\n            # make random selection for existing prompts\n            if found.group(2) is None:\n                for i in range(len(prompts)):\n                    prompts[i] = prompts[i].replace(found.group(0), replacer()[0], 1)\n\n    return prompts\n\n\n# endregion\n\n\n# def load_clip_l14_336(dtype):\n#   logger.info(f\"loading CLIP: {CLIP_ID_L14_336}\")\n#   text_encoder = CLIPTextModel.from_pretrained(CLIP_ID_L14_336, torch_dtype=dtype)\n#   return text_encoder\n\n\nclass BatchDataBase(NamedTuple):\n    # バッチ分割が必要ないデータ\n    step: int\n    prompt: str\n    negative_prompt: str\n    seed: int\n    init_image: Any\n    mask_image: Any\n    clip_prompt: str\n    guide_image: Any\n    raw_prompt: str\n\n\nclass BatchDataExt(NamedTuple):\n    # バッチ分割が必要なデータ\n    width: int\n    height: int\n    steps: int\n    scale: float\n    negative_scale: float\n    strength: float\n    network_muls: Tuple[float]\n    num_sub_prompts: int\n\n\nclass BatchData(NamedTuple):\n    return_latents: bool\n    base: BatchDataBase\n    ext: BatchDataExt\n\n\ndef main(args):\n    if args.fp16:\n        dtype = torch.float16\n    elif args.bf16:\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n\n    highres_fix = args.highres_fix_scale is not None\n    # assert not highres_fix or args.image_path is None, f\"highres_fix doesn't work with img2img / highres_fixはimg2imgと同時に使えません\"\n\n    if args.v_parameterization and not args.v2:\n        logger.warning(\"v_parameterization should be with v2 / v1でv_parameterizationを使用することは想定されていません\")\n    if args.v2 and args.clip_skip is not None:\n        logger.warning(\"v2 with clip_skip will be unexpected / v2でclip_skipを使用することは想定されていません\")\n\n    # モデルを読み込む\n    if not os.path.isfile(args.ckpt):  # ファイルがないならパターンで探し、一つだけ該当すればそれを使う\n        files = glob.glob(args.ckpt)\n        if len(files) == 1:\n            args.ckpt = files[0]\n\n    use_stable_diffusion_format = os.path.isfile(args.ckpt)\n    if use_stable_diffusion_format:\n        logger.info(\"load StableDiffusion checkpoint\")\n        text_encoder, vae, unet = model_util.load_models_from_stable_diffusion_checkpoint(args.v2, args.ckpt)\n    else:\n        logger.info(\"load Diffusers pretrained models\")\n        loading_pipe = StableDiffusionPipeline.from_pretrained(args.ckpt, safety_checker=None, torch_dtype=dtype)\n        text_encoder = loading_pipe.text_encoder\n        vae = loading_pipe.vae\n        unet = loading_pipe.unet\n        tokenizer = loading_pipe.tokenizer\n        del loading_pipe\n\n        # Diffusers U-Net to original U-Net\n        original_unet = UNet2DConditionModel(\n            unet.config.sample_size,\n            unet.config.attention_head_dim,\n            unet.config.cross_attention_dim,\n            unet.config.use_linear_projection,\n            unet.config.upcast_attention,\n        )\n        original_unet.load_state_dict(unet.state_dict())\n        unet = original_unet\n    unet: InferUNet2DConditionModel = InferUNet2DConditionModel(unet)\n\n    # VAEを読み込む\n    if args.vae is not None:\n        vae = model_util.load_vae(args.vae, dtype)\n        logger.info(\"additional VAE loaded\")\n\n    # # 置換するCLIPを読み込む\n    # if args.replace_clip_l14_336:\n    #   text_encoder = load_clip_l14_336(dtype)\n    #   logger.info(f\"large clip {CLIP_ID_L14_336} is loaded\")\n\n    if args.clip_guidance_scale > 0.0 or args.clip_image_guidance_scale:\n        logger.info(\"prepare clip model\")\n        clip_model = CLIPModel.from_pretrained(CLIP_MODEL_PATH, torch_dtype=dtype)\n    else:\n        clip_model = None\n\n    if args.vgg16_guidance_scale > 0.0:\n        logger.info(\"prepare resnet model\")\n        vgg16_model = torchvision.models.vgg16(torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n    else:\n        vgg16_model = None\n\n    # xformers、Hypernetwork対応\n    if not args.diffusers_xformers:\n        mem_eff = not (args.xformers or args.sdpa)\n        replace_unet_modules(unet, mem_eff, args.xformers, args.sdpa)\n        replace_vae_modules(vae, mem_eff, args.xformers, args.sdpa)\n\n    # tokenizerを読み込む\n    logger.info(\"loading tokenizer\")\n    if use_stable_diffusion_format:\n        tokenizer = train_util.load_tokenizer(args)\n\n    # schedulerを用意する\n    sched_init_args = {}\n    scheduler_num_noises_per_step = 1\n    if args.sampler == \"ddim\":\n        scheduler_cls = DDIMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_ddim\n    elif args.sampler == \"ddpm\":  # ddpmはおかしくなるのでoptionから外してある\n        scheduler_cls = DDPMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_ddpm\n    elif args.sampler == \"pndm\":\n        scheduler_cls = PNDMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_pndm\n    elif args.sampler == \"lms\" or args.sampler == \"k_lms\":\n        scheduler_cls = LMSDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_lms_discrete\n    elif args.sampler == \"euler\" or args.sampler == \"k_euler\":\n        scheduler_cls = EulerDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_euler_discrete\n    elif args.sampler == \"euler_a\" or args.sampler == \"k_euler_a\":\n        scheduler_cls = EulerAncestralDiscreteSchedulerGL\n        scheduler_module = diffusers.schedulers.scheduling_euler_ancestral_discrete\n    elif args.sampler == \"dpmsolver\" or args.sampler == \"dpmsolver++\":\n        scheduler_cls = DPMSolverMultistepScheduler\n        sched_init_args[\"algorithm_type\"] = args.sampler\n        scheduler_module = diffusers.schedulers.scheduling_dpmsolver_multistep\n    elif args.sampler == \"dpmsingle\":\n        scheduler_cls = DPMSolverSinglestepScheduler\n        scheduler_module = diffusers.schedulers.scheduling_dpmsolver_singlestep\n    elif args.sampler == \"heun\":\n        scheduler_cls = HeunDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_heun_discrete\n    elif args.sampler == \"dpm_2\" or args.sampler == \"k_dpm_2\":\n        scheduler_cls = KDPM2DiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_k_dpm_2_discrete\n    elif args.sampler == \"dpm_2_a\" or args.sampler == \"k_dpm_2_a\":\n        scheduler_cls = KDPM2AncestralDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete\n        scheduler_num_noises_per_step = 2\n\n    if args.v_parameterization:\n        sched_init_args[\"prediction_type\"] = \"v_prediction\"\n\n    # samplerの乱数をあらかじめ指定するための処理\n\n    # replace randn\n    class NoiseManager:\n        def __init__(self):\n            self.sampler_noises = None\n            self.sampler_noise_index = 0\n\n        def reset_sampler_noises(self, noises):\n            self.sampler_noise_index = 0\n            self.sampler_noises = noises\n\n        def randn(self, shape, device=None, dtype=None, layout=None, generator=None):\n            # logger.info(f\"replacing {shape} {len(self.sampler_noises)} {self.sampler_noise_index}\")\n            if self.sampler_noises is not None and self.sampler_noise_index < len(self.sampler_noises):\n                noise = self.sampler_noises[self.sampler_noise_index]\n                if shape != noise.shape:\n                    noise = None\n            else:\n                noise = None\n\n            if noise == None:\n                logger.warning(f\"unexpected noise request: {self.sampler_noise_index}, {shape}\")\n                noise = torch.randn(shape, dtype=dtype, device=device, generator=generator)\n\n            self.sampler_noise_index += 1\n            return noise\n\n    class TorchRandReplacer:\n        def __init__(self, noise_manager):\n            self.noise_manager = noise_manager\n\n        def __getattr__(self, item):\n            if item == \"randn\":\n                return self.noise_manager.randn\n            if hasattr(torch, item):\n                return getattr(torch, item)\n            raise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, item))\n\n    noise_manager = NoiseManager()\n    if scheduler_module is not None:\n        scheduler_module.torch = TorchRandReplacer(noise_manager)\n\n    scheduler = scheduler_cls(\n        num_train_timesteps=SCHEDULER_TIMESTEPS,\n        beta_start=SCHEDULER_LINEAR_START,\n        beta_end=SCHEDULER_LINEAR_END,\n        beta_schedule=SCHEDLER_SCHEDULE,\n        **sched_init_args,\n    )\n\n    # clip_sample=Trueにする\n    if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is False:\n        logger.info(\"set clip_sample to True\")\n        scheduler.config.clip_sample = True\n\n    # deviceを決定する\n    device = get_preferred_device()\n\n    # custom pipelineをコピったやつを生成する\n    if args.vae_slices:\n        from library.slicing_vae import SlicingAutoencoderKL\n\n        sli_vae = SlicingAutoencoderKL(\n            act_fn=\"silu\",\n            block_out_channels=(128, 256, 512, 512),\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            in_channels=3,\n            latent_channels=4,\n            layers_per_block=2,\n            norm_num_groups=32,\n            out_channels=3,\n            sample_size=512,\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            num_slices=args.vae_slices,\n        )\n        sli_vae.load_state_dict(vae.state_dict())  # vaeのパラメータをコピーする\n        vae = sli_vae\n        del sli_vae\n    vae.to(dtype).to(device)\n    vae.eval()\n\n    text_encoder.to(dtype).to(device)\n    unet.to(dtype).to(device)\n\n    text_encoder.eval()\n    unet.eval()\n\n    if clip_model is not None:\n        clip_model.to(dtype).to(device)\n        clip_model.eval()\n    if vgg16_model is not None:\n        vgg16_model.to(dtype).to(device)\n        vgg16_model.eval()\n\n    # networkを組み込む\n    if args.network_module:\n        networks = []\n        network_default_muls = []\n        network_pre_calc = args.network_pre_calc\n\n        # merge関連の引数を統合する\n        if args.network_merge:\n            network_merge = len(args.network_module)  # all networks are merged\n        elif args.network_merge_n_models:\n            network_merge = args.network_merge_n_models\n        else:\n            network_merge = 0\n\n        for i, network_module in enumerate(args.network_module):\n            logger.info(f\"import network module: {network_module}\")\n            imported_module = importlib.import_module(network_module)\n\n            network_mul = 1.0 if args.network_mul is None or len(args.network_mul) <= i else args.network_mul[i]\n\n            net_kwargs = {}\n            if args.network_args and i < len(args.network_args):\n                network_args = args.network_args[i]\n                # TODO escape special chars\n                network_args = network_args.split(\";\")\n                for net_arg in network_args:\n                    key, value = net_arg.split(\"=\")\n                    net_kwargs[key] = value\n\n            if args.network_weights is None or len(args.network_weights) <= i:\n                raise ValueError(\"No weight. Weight is required.\")\n\n            network_weight = args.network_weights[i]\n            logger.info(f\"load network weights from: {network_weight}\")\n\n            if model_util.is_safetensors(network_weight) and args.network_show_meta:\n                from safetensors.torch import safe_open\n\n                with safe_open(network_weight, framework=\"pt\") as f:\n                    metadata = f.metadata()\n                if metadata is not None:\n                    logger.info(f\"metadata for: {network_weight}: {metadata}\")\n\n            network, weights_sd = imported_module.create_network_from_weights(\n                network_mul, network_weight, vae, text_encoder, unet, for_inference=True, **net_kwargs\n            )\n            if network is None:\n                return\n\n            mergeable = network.is_mergeable()\n            if network_merge and not mergeable:\n                logger.warning(\"network is not mergiable. ignore merge option.\")\n\n            if not mergeable or i >= network_merge:\n                # not merging\n                network.apply_to(text_encoder, unet)\n                info = network.load_state_dict(weights_sd, False)  # network.load_weightsを使うようにするとよい\n                logger.info(f\"weights are loaded: {info}\")\n\n                if args.opt_channels_last:\n                    network.to(memory_format=torch.channels_last)\n                network.to(dtype).to(device)\n\n                if network_pre_calc:\n                    logger.info(\"backup original weights\")\n                    network.backup_weights()\n\n                networks.append(network)\n                network_default_muls.append(network_mul)\n            else:\n                network.merge_to(text_encoder, unet, weights_sd, dtype, device)\n\n    else:\n        networks = []\n\n    # upscalerの指定があれば取得する\n    upscaler = None\n    if args.highres_fix_upscaler:\n        logger.info(f\"import upscaler module {args.highres_fix_upscaler}\")\n        imported_module = importlib.import_module(args.highres_fix_upscaler)\n\n        us_kwargs = {}\n        if args.highres_fix_upscaler_args:\n            for net_arg in args.highres_fix_upscaler_args.split(\";\"):\n                key, value = net_arg.split(\"=\")\n                us_kwargs[key] = value\n\n        logger.info(\"create upscaler\")\n        upscaler = imported_module.create_upscaler(**us_kwargs)\n        upscaler.to(dtype).to(device)\n\n    # ControlNetの処理\n    control_nets: List[ControlNetInfo] = []\n    if args.control_net_models:\n        for i, model in enumerate(args.control_net_models):\n            prep_type = None if not args.control_net_preps or len(args.control_net_preps) <= i else args.control_net_preps[i]\n            weight = 1.0 if not args.control_net_weights or len(args.control_net_weights) <= i else args.control_net_weights[i]\n            ratio = 1.0 if not args.control_net_ratios or len(args.control_net_ratios) <= i else args.control_net_ratios[i]\n\n            ctrl_unet, ctrl_net = original_control_net.load_control_net(args.v2, unet, model)\n            prep = original_control_net.load_preprocess(prep_type)\n            control_nets.append(ControlNetInfo(ctrl_unet, ctrl_net, prep, weight, ratio))\n\n    if args.opt_channels_last:\n        logger.info(f\"set optimizing: channels last\")\n        text_encoder.to(memory_format=torch.channels_last)\n        vae.to(memory_format=torch.channels_last)\n        unet.to(memory_format=torch.channels_last)\n        if clip_model is not None:\n            clip_model.to(memory_format=torch.channels_last)\n        if networks:\n            for network in networks:\n                network.to(memory_format=torch.channels_last)\n        if vgg16_model is not None:\n            vgg16_model.to(memory_format=torch.channels_last)\n\n        for cn in control_nets:\n            cn.unet.to(memory_format=torch.channels_last)\n            cn.net.to(memory_format=torch.channels_last)\n\n    pipe = PipelineLike(\n        device,\n        vae,\n        text_encoder,\n        tokenizer,\n        unet,\n        scheduler,\n        args.clip_skip,\n        clip_model,\n        args.clip_guidance_scale,\n        args.clip_image_guidance_scale,\n        vgg16_model,\n        args.vgg16_guidance_scale,\n        args.vgg16_guidance_layer,\n    )\n    pipe.set_control_nets(control_nets)\n    logger.info(\"pipeline is ready.\")\n\n    if args.diffusers_xformers:\n        pipe.enable_xformers_memory_efficient_attention()\n\n    # Deep Shrink\n    if args.ds_depth_1 is not None:\n        unet.set_deep_shrink(args.ds_depth_1, args.ds_timesteps_1, args.ds_depth_2, args.ds_timesteps_2, args.ds_ratio)\n\n    # Gradual Latent\n    if args.gradual_latent_timesteps is not None:\n        if args.gradual_latent_unsharp_params:\n            us_params = args.gradual_latent_unsharp_params.split(\",\")\n            us_ksize, us_sigma, us_strength = [float(v) for v in us_params[:3]]\n            us_target_x = True if len(us_params) <= 3 else bool(int(us_params[3]))\n            us_ksize = int(us_ksize)\n        else:\n            us_ksize, us_sigma, us_strength, us_target_x = None, None, None, None\n\n        gradual_latent = GradualLatent(\n            args.gradual_latent_ratio,\n            args.gradual_latent_timesteps,\n            args.gradual_latent_every_n_steps,\n            args.gradual_latent_ratio_step,\n            args.gradual_latent_s_noise,\n            us_ksize,\n            us_sigma,\n            us_strength,\n            us_target_x,\n        )\n        pipe.set_gradual_latent(gradual_latent)\n\n    # Extended Textual Inversion および Textual Inversionを処理する\n    if args.XTI_embeddings:\n        diffusers.models.UNet2DConditionModel.forward = unet_forward_XTI\n        diffusers.models.unet_2d_blocks.CrossAttnDownBlock2D.forward = downblock_forward_XTI\n        diffusers.models.unet_2d_blocks.CrossAttnUpBlock2D.forward = upblock_forward_XTI\n\n    if args.textual_inversion_embeddings:\n        token_ids_embeds = []\n        for embeds_file in args.textual_inversion_embeddings:\n            if model_util.is_safetensors(embeds_file):\n                from safetensors.torch import load_file\n\n                data = load_file(embeds_file)\n            else:\n                data = torch.load(embeds_file, map_location=\"cpu\")\n\n            if \"string_to_param\" in data:\n                data = data[\"string_to_param\"]\n            embeds = next(iter(data.values()))\n\n            if type(embeds) != torch.Tensor:\n                raise ValueError(\n                    f\"weight file does not contains Tensor / 重みファイルのデータがTensorではありません: {embeds_file}\"\n                )\n\n            num_vectors_per_token = embeds.size()[0]\n            token_string = os.path.splitext(os.path.basename(embeds_file))[0]\n            token_strings = [token_string] + [f\"{token_string}{i+1}\" for i in range(num_vectors_per_token - 1)]\n\n            # add new word to tokenizer, count is num_vectors_per_token\n            num_added_tokens = tokenizer.add_tokens(token_strings)\n            assert (\n                num_added_tokens == num_vectors_per_token\n            ), f\"tokenizer has same word to token string (filename). please rename the file / 指定した名前（ファイル名）のトークンが既に存在します。ファイルをリネームしてください: {embeds_file}\"\n\n            token_ids = tokenizer.convert_tokens_to_ids(token_strings)\n            logger.info(f\"Textual Inversion embeddings `{token_string}` loaded. Tokens are added: {token_ids}\")\n            assert (\n                min(token_ids) == token_ids[0] and token_ids[-1] == token_ids[0] + len(token_ids) - 1\n            ), f\"token ids is not ordered\"\n            assert len(tokenizer) - 1 == token_ids[-1], f\"token ids is not end of tokenize: {len(tokenizer)}\"\n\n            if num_vectors_per_token > 1:\n                pipe.add_token_replacement(token_ids[0], token_ids)\n\n            token_ids_embeds.append((token_ids, embeds))\n\n        text_encoder.resize_token_embeddings(len(tokenizer))\n        token_embeds = text_encoder.get_input_embeddings().weight.data\n        for token_ids, embeds in token_ids_embeds:\n            for token_id, embed in zip(token_ids, embeds):\n                token_embeds[token_id] = embed\n\n    if args.XTI_embeddings:\n        XTI_layers = [\n            \"IN01\",\n            \"IN02\",\n            \"IN04\",\n            \"IN05\",\n            \"IN07\",\n            \"IN08\",\n            \"MID\",\n            \"OUT03\",\n            \"OUT04\",\n            \"OUT05\",\n            \"OUT06\",\n            \"OUT07\",\n            \"OUT08\",\n            \"OUT09\",\n            \"OUT10\",\n            \"OUT11\",\n        ]\n        token_ids_embeds_XTI = []\n        for embeds_file in args.XTI_embeddings:\n            if model_util.is_safetensors(embeds_file):\n                from safetensors.torch import load_file\n\n                data = load_file(embeds_file)\n            else:\n                data = torch.load(embeds_file, map_location=\"cpu\")\n            if set(data.keys()) != set(XTI_layers):\n                raise ValueError(\"NOT XTI\")\n            embeds = torch.concat(list(data.values()))\n            num_vectors_per_token = data[\"MID\"].size()[0]\n\n            token_string = os.path.splitext(os.path.basename(embeds_file))[0]\n            token_strings = [token_string] + [f\"{token_string}{i+1}\" for i in range(num_vectors_per_token - 1)]\n\n            # add new word to tokenizer, count is num_vectors_per_token\n            num_added_tokens = tokenizer.add_tokens(token_strings)\n            assert (\n                num_added_tokens == num_vectors_per_token\n            ), f\"tokenizer has same word to token string (filename). please rename the file / 指定した名前（ファイル名）のトークンが既に存在します。ファイルをリネームしてください: {embeds_file}\"\n\n            token_ids = tokenizer.convert_tokens_to_ids(token_strings)\n            logger.info(f\"XTI embeddings `{token_string}` loaded. Tokens are added: {token_ids}\")\n\n            # if num_vectors_per_token > 1:\n            pipe.add_token_replacement(token_ids[0], token_ids)\n\n            token_strings_XTI = []\n            for layer_name in XTI_layers:\n                token_strings_XTI += [f\"{t}_{layer_name}\" for t in token_strings]\n            tokenizer.add_tokens(token_strings_XTI)\n            token_ids_XTI = tokenizer.convert_tokens_to_ids(token_strings_XTI)\n            token_ids_embeds_XTI.append((token_ids_XTI, embeds))\n            for t in token_ids:\n                t_XTI_dic = {}\n                for i, layer_name in enumerate(XTI_layers):\n                    t_XTI_dic[layer_name] = t + (i + 1) * num_added_tokens\n                pipe.add_token_replacement_XTI(t, t_XTI_dic)\n\n            text_encoder.resize_token_embeddings(len(tokenizer))\n            token_embeds = text_encoder.get_input_embeddings().weight.data\n            for token_ids, embeds in token_ids_embeds_XTI:\n                for token_id, embed in zip(token_ids, embeds):\n                    token_embeds[token_id] = embed\n\n    # promptを取得する\n    if args.from_file is not None:\n        logger.info(f\"reading prompts from {args.from_file}\")\n        with open(args.from_file, \"r\", encoding=\"utf-8\") as f:\n            prompt_list = f.read().splitlines()\n            prompt_list = [d for d in prompt_list if len(d.strip()) > 0 and d[0] != \"#\"]\n    elif args.prompt is not None:\n        prompt_list = [args.prompt]\n    else:\n        prompt_list = []\n\n    if args.interactive:\n        args.n_iter = 1\n\n    # img2imgの前処理、画像の読み込みなど\n    def load_images(path):\n        if os.path.isfile(path):\n            paths = [path]\n        else:\n            paths = (\n                glob.glob(os.path.join(path, \"*.png\"))\n                + glob.glob(os.path.join(path, \"*.jpg\"))\n                + glob.glob(os.path.join(path, \"*.jpeg\"))\n                + glob.glob(os.path.join(path, \"*.webp\"))\n            )\n            paths.sort()\n\n        images = []\n        for p in paths:\n            image = Image.open(p)\n            if image.mode != \"RGB\":\n                logger.info(f\"convert image to RGB from {image.mode}: {p}\")\n                image = image.convert(\"RGB\")\n            images.append(image)\n\n        return images\n\n    def resize_images(imgs, size):\n        resized = []\n        for img in imgs:\n            r_img = img.resize(size, Image.Resampling.LANCZOS)\n            if hasattr(img, \"filename\"):  # filename属性がない場合があるらしい\n                r_img.filename = img.filename\n            resized.append(r_img)\n        return resized\n\n    if args.image_path is not None:\n        logger.info(f\"load image for img2img: {args.image_path}\")\n        init_images = load_images(args.image_path)\n        assert len(init_images) > 0, f\"No image / 画像がありません: {args.image_path}\"\n        logger.info(f\"loaded {len(init_images)} images for img2img\")\n    else:\n        init_images = None\n\n    if args.mask_path is not None:\n        logger.info(f\"load mask for inpainting: {args.mask_path}\")\n        mask_images = load_images(args.mask_path)\n        assert len(mask_images) > 0, f\"No mask image / マスク画像がありません: {args.image_path}\"\n        logger.info(f\"loaded {len(mask_images)} mask images for inpainting\")\n    else:\n        mask_images = None\n\n    # promptがないとき、画像のPngInfoから取得する\n    if init_images is not None and len(prompt_list) == 0 and not args.interactive:\n        logger.info(\"get prompts from images' meta data\")\n        for img in init_images:\n            if \"prompt\" in img.text:\n                prompt = img.text[\"prompt\"]\n                if \"negative-prompt\" in img.text:\n                    prompt += \" --n \" + img.text[\"negative-prompt\"]\n                prompt_list.append(prompt)\n\n        # プロンプトと画像を一致させるため指定回数だけ繰り返す（画像を増幅する）\n        l = []\n        for im in init_images:\n            l.extend([im] * args.images_per_prompt)\n        init_images = l\n\n        if mask_images is not None:\n            l = []\n            for im in mask_images:\n                l.extend([im] * args.images_per_prompt)\n            mask_images = l\n\n    # 画像サイズにオプション指定があるときはリサイズする\n    if args.W is not None and args.H is not None:\n        # highres fix を考慮に入れる\n        w, h = args.W, args.H\n        if highres_fix:\n            w = int(w * args.highres_fix_scale + 0.5)\n            h = int(h * args.highres_fix_scale + 0.5)\n\n        if init_images is not None:\n            logger.info(f\"resize img2img source images to {w}*{h}\")\n            init_images = resize_images(init_images, (w, h))\n        if mask_images is not None:\n            logger.info(f\"resize img2img mask images to {w}*{h}\")\n            mask_images = resize_images(mask_images, (w, h))\n\n    regional_network = False\n    if networks and mask_images:\n        # mask を領域情報として流用する、現在は一回のコマンド呼び出しで1枚だけ対応\n        regional_network = True\n        logger.info(\"use mask as region\")\n\n        size = None\n        for i, network in enumerate(networks):\n            if (i < 3 and args.network_regional_mask_max_color_codes is None) or i < args.network_regional_mask_max_color_codes:\n                np_mask = np.array(mask_images[0])\n\n                if args.network_regional_mask_max_color_codes:\n                    # カラーコードでマスクを指定する\n                    ch0 = (i + 1) & 1\n                    ch1 = ((i + 1) >> 1) & 1\n                    ch2 = ((i + 1) >> 2) & 1\n                    np_mask = np.all(np_mask == np.array([ch0, ch1, ch2]) * 255, axis=2)\n                    np_mask = np_mask.astype(np.uint8) * 255\n                else:\n                    np_mask = np_mask[:, :, i]\n                size = np_mask.shape\n            else:\n                np_mask = np.full(size, 255, dtype=np.uint8)\n            mask = torch.from_numpy(np_mask.astype(np.float32) / 255.0)\n            network.set_region(i, i == len(networks) - 1, mask)\n        mask_images = None\n\n    prev_image = None  # for VGG16 guided\n    if args.guide_image_path is not None:\n        logger.info(f\"load image for CLIP/VGG16/ControlNet guidance: {args.guide_image_path}\")\n        guide_images = []\n        for p in args.guide_image_path:\n            guide_images.extend(load_images(p))\n\n        logger.info(f\"loaded {len(guide_images)} guide images for guidance\")\n        if len(guide_images) == 0:\n            logger.info(\n                f\"No guide image, use previous generated image. / ガイド画像がありません。直前に生成した画像を使います: {args.image_path}\"\n            )\n            guide_images = None\n    else:\n        guide_images = None\n\n    # seed指定時はseedを決めておく\n    if args.seed is not None:\n        # dynamic promptを使うと足りなくなる→images_per_promptを適当に大きくしておいてもらう\n        random.seed(args.seed)\n        predefined_seeds = [random.randint(0, 0x7FFFFFFF) for _ in range(args.n_iter * len(prompt_list) * args.images_per_prompt)]\n        if len(predefined_seeds) == 1:\n            predefined_seeds[0] = args.seed\n    else:\n        predefined_seeds = None\n\n    # デフォルト画像サイズを設定する：img2imgではこれらの値は無視される（またはW*Hにリサイズ済み）\n    if args.W is None:\n        args.W = 512\n    if args.H is None:\n        args.H = 512\n\n    # 画像生成のループ\n    os.makedirs(args.outdir, exist_ok=True)\n    max_embeddings_multiples = 1 if args.max_embeddings_multiples is None else args.max_embeddings_multiples\n\n    for gen_iter in range(args.n_iter):\n        logger.info(f\"iteration {gen_iter+1}/{args.n_iter}\")\n        iter_seed = random.randint(0, 0x7FFFFFFF)\n\n        # shuffle prompt list\n        if args.shuffle_prompts:\n            random.shuffle(prompt_list)\n\n        # バッチ処理の関数\n        def process_batch(batch: List[BatchData], highres_fix, highres_1st=False):\n            batch_size = len(batch)\n\n            # highres_fixの処理\n            if highres_fix and not highres_1st:\n                # 1st stageのバッチを作成して呼び出す：サイズを小さくして呼び出す\n                is_1st_latent = upscaler.support_latents() if upscaler else args.highres_fix_latents_upscaling\n\n                logger.info(\"process 1st stage\")\n                batch_1st = []\n                for _, base, ext in batch:\n                    width_1st = int(ext.width * args.highres_fix_scale + 0.5)\n                    height_1st = int(ext.height * args.highres_fix_scale + 0.5)\n                    width_1st = width_1st - width_1st % 32\n                    height_1st = height_1st - height_1st % 32\n\n                    strength_1st = ext.strength if args.highres_fix_strength is None else args.highres_fix_strength\n\n                    ext_1st = BatchDataExt(\n                        width_1st,\n                        height_1st,\n                        args.highres_fix_steps,\n                        ext.scale,\n                        ext.negative_scale,\n                        strength_1st,\n                        ext.network_muls,\n                        ext.num_sub_prompts,\n                    )\n                    batch_1st.append(BatchData(is_1st_latent, base, ext_1st))\n\n                pipe.set_enable_control_net(True)  # 1st stageではControlNetを有効にする\n                images_1st = process_batch(batch_1st, True, True)\n\n                # 2nd stageのバッチを作成して以下処理する\n                logger.info(\"process 2nd stage\")\n                width_2nd, height_2nd = batch[0].ext.width, batch[0].ext.height\n\n                if upscaler:\n                    # upscalerを使って画像を拡大する\n                    lowreso_imgs = None if is_1st_latent else images_1st\n                    lowreso_latents = None if not is_1st_latent else images_1st\n\n                    # 戻り値はPIL.Image.Imageかtorch.Tensorのlatents\n                    batch_size = len(images_1st)\n                    vae_batch_size = (\n                        batch_size\n                        if args.vae_batch_size is None\n                        else (max(1, int(batch_size * args.vae_batch_size)) if args.vae_batch_size < 1 else args.vae_batch_size)\n                    )\n                    vae_batch_size = int(vae_batch_size)\n                    images_1st = upscaler.upscale(\n                        vae, lowreso_imgs, lowreso_latents, dtype, width_2nd, height_2nd, batch_size, vae_batch_size\n                    )\n\n                elif args.highres_fix_latents_upscaling:\n                    # latentを拡大する\n                    org_dtype = images_1st.dtype\n                    if images_1st.dtype == torch.bfloat16:\n                        images_1st = images_1st.to(torch.float)  # interpolateがbf16をサポートしていない\n                    images_1st = torch.nn.functional.interpolate(\n                        images_1st, (batch[0].ext.height // 8, batch[0].ext.width // 8), mode=\"bilinear\"\n                    )  # , antialias=True)\n                    images_1st = images_1st.to(org_dtype)\n\n                else:\n                    # 画像をLANCZOSで拡大する\n                    images_1st = [image.resize((width_2nd, height_2nd), resample=PIL.Image.LANCZOS) for image in images_1st]\n\n                batch_2nd = []\n                for i, (bd, image) in enumerate(zip(batch, images_1st)):\n                    bd_2nd = BatchData(False, BatchDataBase(*bd.base[0:3], bd.base.seed + 1, image, None, *bd.base[6:]), bd.ext)\n                    batch_2nd.append(bd_2nd)\n                batch = batch_2nd\n\n                if args.highres_fix_disable_control_net:\n                    pipe.set_enable_control_net(False)  # オプション指定時、2nd stageではControlNetを無効にする\n\n            # このバッチの情報を取り出す\n            (\n                return_latents,\n                (step_first, _, _, _, init_image, mask_image, _, guide_image, _),\n                (width, height, steps, scale, negative_scale, strength, network_muls, num_sub_prompts),\n            ) = batch[0]\n            noise_shape = (LATENT_CHANNELS, height // DOWNSAMPLING_FACTOR, width // DOWNSAMPLING_FACTOR)\n\n            prompts = []\n            negative_prompts = []\n            raw_prompts = []\n            start_code = torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n            noises = [\n                torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n                for _ in range(steps * scheduler_num_noises_per_step)\n            ]\n            seeds = []\n            clip_prompts = []\n\n            if init_image is not None:  # img2img?\n                i2i_noises = torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n                init_images = []\n\n                if mask_image is not None:\n                    mask_images = []\n                else:\n                    mask_images = None\n            else:\n                i2i_noises = None\n                init_images = None\n                mask_images = None\n\n            if guide_image is not None:  # CLIP image guided?\n                guide_images = []\n            else:\n                guide_images = None\n\n            # バッチ内の位置に関わらず同じ乱数を使うためにここで乱数を生成しておく。あわせてimage/maskがbatch内で同一かチェックする\n            all_images_are_same = True\n            all_masks_are_same = True\n            all_guide_images_are_same = True\n            for i, (\n                _,\n                (_, prompt, negative_prompt, seed, init_image, mask_image, clip_prompt, guide_image, raw_prompt),\n                _,\n            ) in enumerate(batch):\n                prompts.append(prompt)\n                negative_prompts.append(negative_prompt)\n                seeds.append(seed)\n                clip_prompts.append(clip_prompt)\n                raw_prompts.append(raw_prompt)\n\n                if init_image is not None:\n                    init_images.append(init_image)\n                    if i > 0 and all_images_are_same:\n                        all_images_are_same = init_images[-2] is init_image\n\n                if mask_image is not None:\n                    mask_images.append(mask_image)\n                    if i > 0 and all_masks_are_same:\n                        all_masks_are_same = mask_images[-2] is mask_image\n\n                if guide_image is not None:\n                    if type(guide_image) is list:\n                        guide_images.extend(guide_image)\n                        all_guide_images_are_same = False\n                    else:\n                        guide_images.append(guide_image)\n                        if i > 0 and all_guide_images_are_same:\n                            all_guide_images_are_same = guide_images[-2] is guide_image\n\n                # make start code\n                torch.manual_seed(seed)\n                start_code[i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n                # make each noises\n                for j in range(steps * scheduler_num_noises_per_step):\n                    noises[j][i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n                if i2i_noises is not None:  # img2img noise\n                    i2i_noises[i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n            noise_manager.reset_sampler_noises(noises)\n\n            # すべての画像が同じなら1枚だけpipeに渡すことでpipe側で処理を高速化する\n            if init_images is not None and all_images_are_same:\n                init_images = init_images[0]\n            if mask_images is not None and all_masks_are_same:\n                mask_images = mask_images[0]\n            if guide_images is not None and all_guide_images_are_same:\n                guide_images = guide_images[0]\n\n            # ControlNet使用時はguide imageをリサイズする\n            if control_nets:\n                # TODO resampleのメソッド\n                guide_images = guide_images if type(guide_images) == list else [guide_images]\n                guide_images = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in guide_images]\n                if len(guide_images) == 1:\n                    guide_images = guide_images[0]\n\n            # generate\n            if networks:\n                # 追加ネットワークの処理\n                shared = {}\n                for n, m in zip(networks, network_muls if network_muls else network_default_muls):\n                    n.set_multiplier(m)\n                    if regional_network:\n                        n.set_current_generation(batch_size, num_sub_prompts, width, height, shared)\n\n                if not regional_network and network_pre_calc:\n                    for n in networks:\n                        n.restore_weights()\n                    for n in networks:\n                        n.pre_calculation()\n                    logger.info(\"pre-calculation... done\")\n\n            images = pipe(\n                prompts,\n                negative_prompts,\n                init_images,\n                mask_images,\n                height,\n                width,\n                steps,\n                scale,\n                negative_scale,\n                strength,\n                latents=start_code,\n                output_type=\"pil\",\n                max_embeddings_multiples=max_embeddings_multiples,\n                img2img_noise=i2i_noises,\n                vae_batch_size=args.vae_batch_size,\n                return_latents=return_latents,\n                clip_prompts=clip_prompts,\n                clip_guide_images=guide_images,\n            )[0]\n            if highres_1st and not args.highres_fix_save_1st:  # return images or latents\n                return images\n\n            # save image\n            highres_prefix = (\"0\" if highres_1st else \"1\") if highres_fix else \"\"\n            ts_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n            for i, (image, prompt, negative_prompts, seed, clip_prompt, raw_prompt) in enumerate(\n                zip(images, prompts, negative_prompts, seeds, clip_prompts, raw_prompts)\n            ):\n                if highres_fix:\n                    seed -= 1  # record original seed\n                metadata = PngInfo()\n                metadata.add_text(\"prompt\", prompt)\n                metadata.add_text(\"seed\", str(seed))\n                metadata.add_text(\"sampler\", args.sampler)\n                metadata.add_text(\"steps\", str(steps))\n                metadata.add_text(\"scale\", str(scale))\n                if negative_prompt is not None:\n                    metadata.add_text(\"negative-prompt\", negative_prompt)\n                if negative_scale is not None:\n                    metadata.add_text(\"negative-scale\", str(negative_scale))\n                if clip_prompt is not None:\n                    metadata.add_text(\"clip-prompt\", clip_prompt)\n                if raw_prompt is not None:\n                    metadata.add_text(\"raw-prompt\", raw_prompt)\n\n                if args.use_original_file_name and init_images is not None:\n                    if type(init_images) is list:\n                        fln = os.path.splitext(os.path.basename(init_images[i % len(init_images)].filename))[0] + \".png\"\n                    else:\n                        fln = os.path.splitext(os.path.basename(init_images.filename))[0] + \".png\"\n                elif args.sequential_file_name:\n                    fln = f\"im_{highres_prefix}{step_first + i + 1:06d}.png\"\n                else:\n                    fln = f\"im_{ts_str}_{highres_prefix}{i:03d}_{seed}.png\"\n\n                image.save(os.path.join(args.outdir, fln), pnginfo=metadata)\n\n            if not args.no_preview and not highres_1st and args.interactive:\n                try:\n                    import cv2\n\n                    for prompt, image in zip(prompts, images):\n                        cv2.imshow(prompt[:128], np.array(image)[:, :, ::-1])  # プロンプトが長いと死ぬ\n                        cv2.waitKey()\n                        cv2.destroyAllWindows()\n                except ImportError:\n                    logger.info(\n                        \"opencv-python is not installed, cannot preview / opencv-pythonがインストールされていないためプレビューできません\"\n                    )\n\n            return images\n\n        # 画像生成のプロンプトが一周するまでのループ\n        prompt_index = 0\n        global_step = 0\n        batch_data = []\n        while args.interactive or prompt_index < len(prompt_list):\n            if len(prompt_list) == 0:\n                # interactive\n                valid = False\n                while not valid:\n                    logger.info(\"\")\n                    logger.info(\"Type prompt:\")\n                    try:\n                        raw_prompt = input()\n                    except EOFError:\n                        break\n\n                    valid = len(raw_prompt.strip().split(\" --\")[0].strip()) > 0\n                if not valid:  # EOF, end app\n                    break\n            else:\n                raw_prompt = prompt_list[prompt_index]\n\n            # sd-dynamic-prompts like variants:\n            # count is 1 (not dynamic) or images_per_prompt (no enumeration) or arbitrary (enumeration)\n            raw_prompts = handle_dynamic_prompt_variants(raw_prompt, args.images_per_prompt)\n\n            # repeat prompt\n            for pi in range(args.images_per_prompt if len(raw_prompts) == 1 else len(raw_prompts)):\n                raw_prompt = raw_prompts[pi] if len(raw_prompts) > 1 else raw_prompts[0]\n\n                if pi == 0 or len(raw_prompts) > 1:\n                    # parse prompt: if prompt is not changed, skip parsing\n                    width = args.W\n                    height = args.H\n                    scale = args.scale\n                    negative_scale = args.negative_scale\n                    steps = args.steps\n                    seed = None\n                    seeds = None\n                    strength = 0.8 if args.strength is None else args.strength\n                    negative_prompt = \"\"\n                    clip_prompt = None\n                    network_muls = None\n\n                    # Deep Shrink\n                    ds_depth_1 = None  # means no override\n                    ds_timesteps_1 = args.ds_timesteps_1\n                    ds_depth_2 = args.ds_depth_2\n                    ds_timesteps_2 = args.ds_timesteps_2\n                    ds_ratio = args.ds_ratio\n\n                    # Gradual Latent\n                    gl_timesteps = None  # means no override\n                    gl_ratio = args.gradual_latent_ratio\n                    gl_every_n_steps = args.gradual_latent_every_n_steps\n                    gl_ratio_step = args.gradual_latent_ratio_step\n                    gl_s_noise = args.gradual_latent_s_noise\n                    gl_unsharp_params = args.gradual_latent_unsharp_params\n\n                    prompt_args = raw_prompt.strip().split(\" --\")\n                    prompt = prompt_args[0]\n                    logger.info(f\"prompt {prompt_index+1}/{len(prompt_list)}: {prompt}\")\n\n                    for parg in prompt_args[1:]:\n                        try:\n                            m = re.match(r\"w (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                width = int(m.group(1))\n                                logger.info(f\"width: {width}\")\n                                continue\n\n                            m = re.match(r\"h (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                height = int(m.group(1))\n                                logger.info(f\"height: {height}\")\n                                continue\n\n                            m = re.match(r\"s (\\d+)\", parg, re.IGNORECASE)\n                            if m:  # steps\n                                steps = max(1, min(1000, int(m.group(1))))\n                                logger.info(f\"steps: {steps}\")\n                                continue\n\n                            m = re.match(r\"d ([\\d,]+)\", parg, re.IGNORECASE)\n                            if m:  # seed\n                                seeds = [int(d) for d in m.group(1).split(\",\")]\n                                logger.info(f\"seeds: {seeds}\")\n                                continue\n\n                            m = re.match(r\"l ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # scale\n                                scale = float(m.group(1))\n                                logger.info(f\"scale: {scale}\")\n                                continue\n\n                            m = re.match(r\"nl ([\\d\\.]+|none|None)\", parg, re.IGNORECASE)\n                            if m:  # negative scale\n                                if m.group(1).lower() == \"none\":\n                                    negative_scale = None\n                                else:\n                                    negative_scale = float(m.group(1))\n                                logger.info(f\"negative scale: {negative_scale}\")\n                                continue\n\n                            m = re.match(r\"t ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # strength\n                                strength = float(m.group(1))\n                                logger.info(f\"strength: {strength}\")\n                                continue\n\n                            m = re.match(r\"n (.+)\", parg, re.IGNORECASE)\n                            if m:  # negative prompt\n                                negative_prompt = m.group(1)\n                                logger.info(f\"negative prompt: {negative_prompt}\")\n                                continue\n\n                            m = re.match(r\"c (.+)\", parg, re.IGNORECASE)\n                            if m:  # clip prompt\n                                clip_prompt = m.group(1)\n                                logger.info(f\"clip prompt: {clip_prompt}\")\n                                continue\n\n                            m = re.match(r\"am ([\\d\\.\\-,]+)\", parg, re.IGNORECASE)\n                            if m:  # network multiplies\n                                network_muls = [float(v) for v in m.group(1).split(\",\")]\n                                while len(network_muls) < len(networks):\n                                    network_muls.append(network_muls[-1])\n                                logger.info(f\"network mul: {network_muls}\")\n                                continue\n\n                            # Deep Shrink\n                            m = re.match(r\"dsd1 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink depth 1\n                                ds_depth_1 = int(m.group(1))\n                                logger.info(f\"deep shrink depth 1: {ds_depth_1}\")\n                                continue\n\n                            m = re.match(r\"dst1 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink timesteps 1\n                                ds_timesteps_1 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink timesteps 1: {ds_timesteps_1}\")\n                                continue\n\n                            m = re.match(r\"dsd2 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink depth 2\n                                ds_depth_2 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink depth 2: {ds_depth_2}\")\n                                continue\n\n                            m = re.match(r\"dst2 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink timesteps 2\n                                ds_timesteps_2 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink timesteps 2: {ds_timesteps_2}\")\n                                continue\n\n                            m = re.match(r\"dsr ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink ratio\n                                ds_ratio = float(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink ratio: {ds_ratio}\")\n                                continue\n\n                            # Gradual Latent\n                            m = re.match(r\"glt ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent timesteps\n                                gl_timesteps = int(m.group(1))\n                                logger.info(f\"gradual latent timesteps: {gl_timesteps}\")\n                                continue\n\n                            m = re.match(r\"glr ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent ratio\n                                gl_ratio = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent ratio: {ds_ratio}\")\n                                continue\n\n                            m = re.match(r\"gle ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent every n steps\n                                gl_every_n_steps = int(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent every n steps: {gl_every_n_steps}\")\n                                continue\n\n                            m = re.match(r\"gls ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent ratio step\n                                gl_ratio_step = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent ratio step: {gl_ratio_step}\")\n                                continue\n\n                            m = re.match(r\"glsn ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent s noise\n                                gl_s_noise = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent s noise: {gl_s_noise}\")\n                                continue\n\n                            m = re.match(r\"glus ([\\d\\.\\-,]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent unsharp params\n                                gl_unsharp_params = m.group(1)\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent unsharp params: {gl_unsharp_params}\")\n                                continue\n\n                        except ValueError as ex:\n                            logger.info(f\"Exception in parsing / 解析エラー: {parg}\")\n                            logger.info(ex)\n\n                # override Deep Shrink\n                if ds_depth_1 is not None:\n                    if ds_depth_1 < 0:\n                        ds_depth_1 = args.ds_depth_1 or 3\n                    unet.set_deep_shrink(ds_depth_1, ds_timesteps_1, ds_depth_2, ds_timesteps_2, ds_ratio)\n\n                # override Gradual Latent\n                if gl_timesteps is not None:\n                    if gl_timesteps < 0:\n                        gl_timesteps = args.gradual_latent_timesteps or 650\n                    if gl_unsharp_params is not None:\n                        unsharp_params = gl_unsharp_params.split(\",\")\n                        us_ksize, us_sigma, us_strength = [float(v) for v in unsharp_params[:3]]\n                        logger.info(f'{unsharp_params}')\n                        us_target_x = True if len(unsharp_params) < 4 else bool(int(unsharp_params[3]))\n                        us_ksize = int(us_ksize)\n                    else:\n                        us_ksize, us_sigma, us_strength, us_target_x = None, None, None, None\n                    gradual_latent = GradualLatent(\n                        gl_ratio,\n                        gl_timesteps,\n                        gl_every_n_steps,\n                        gl_ratio_step,\n                        gl_s_noise,\n                        us_ksize,\n                        us_sigma,\n                        us_strength,\n                        us_target_x,\n                    )\n                    pipe.set_gradual_latent(gradual_latent)\n\n                # prepare seed\n                if seeds is not None:  # given in prompt\n                    # 数が足りないなら前のをそのまま使う\n                    if len(seeds) > 0:\n                        seed = seeds.pop(0)\n                else:\n                    if predefined_seeds is not None:\n                        if len(predefined_seeds) > 0:\n                            seed = predefined_seeds.pop(0)\n                        else:\n                            logger.info(\"predefined seeds are exhausted\")\n                            seed = None\n                    elif args.iter_same_seed:\n                        seed = iter_seed\n                    else:\n                        seed = None  # 前のを消す\n\n                if seed is None:\n                    seed = random.randint(0, 0x7FFFFFFF)\n                if args.interactive:\n                    logger.info(f\"seed: {seed}\")\n\n                # prepare init image, guide image and mask\n                init_image = mask_image = guide_image = None\n\n                # 同一イメージを使うとき、本当はlatentに変換しておくと無駄がないが面倒なのでとりあえず毎回処理する\n                if init_images is not None:\n                    init_image = init_images[global_step % len(init_images)]\n\n                    # img2imgの場合は、基本的に元画像のサイズで生成する。highres fixの場合はargs.W, args.Hとscaleに従いリサイズ済みなので無視する\n                    # 32単位に丸めたやつにresizeされるので踏襲する\n                    if not highres_fix:\n                        width, height = init_image.size\n                        width = width - width % 32\n                        height = height - height % 32\n                        if width != init_image.size[0] or height != init_image.size[1]:\n                            logger.info(\n                                f\"img2img image size is not divisible by 32 so aspect ratio is changed / img2imgの画像サイズが32で割り切れないためリサイズされます。画像が歪みます\"\n                            )\n\n                if mask_images is not None:\n                    mask_image = mask_images[global_step % len(mask_images)]\n\n                if guide_images is not None:\n                    if control_nets:  # 複数件の場合あり\n                        c = len(control_nets)\n                        p = global_step % (len(guide_images) // c)\n                        guide_image = guide_images[p * c : p * c + c]\n                    else:\n                        guide_image = guide_images[global_step % len(guide_images)]\n                elif args.clip_image_guidance_scale > 0 or args.vgg16_guidance_scale > 0:\n                    if prev_image is None:\n                        logger.info(\"Generate 1st image without guide image.\")\n                    else:\n                        logger.info(\"Use previous image as guide image.\")\n                        guide_image = prev_image\n\n                if regional_network:\n                    num_sub_prompts = len(prompt.split(\" AND \"))\n                    assert (\n                        len(networks) <= num_sub_prompts\n                    ), \"Number of networks must be less than or equal to number of sub prompts.\"\n                else:\n                    num_sub_prompts = None\n\n                b1 = BatchData(\n                    False,\n                    BatchDataBase(\n                        global_step, prompt, negative_prompt, seed, init_image, mask_image, clip_prompt, guide_image, raw_prompt\n                    ),\n                    BatchDataExt(\n                        width,\n                        height,\n                        steps,\n                        scale,\n                        negative_scale,\n                        strength,\n                        tuple(network_muls) if network_muls else None,\n                        num_sub_prompts,\n                    ),\n                )\n                if len(batch_data) > 0 and batch_data[-1].ext != b1.ext:  # バッチ分割必要？\n                    process_batch(batch_data, highres_fix)\n                    batch_data.clear()\n\n                batch_data.append(b1)\n                if len(batch_data) == args.batch_size:\n                    prev_image = process_batch(batch_data, highres_fix)[0]\n                    batch_data.clear()\n\n                global_step += 1\n\n            prompt_index += 1\n\n        if len(batch_data) > 0:\n            process_batch(batch_data, highres_fix)\n            batch_data.clear()\n\n    logger.info(\"done!\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n\n    parser.add_argument(\n        \"--v2\", action=\"store_true\", help=\"load Stable Diffusion v2.0 model / Stable Diffusion 2.0のモデルを読み込む\"\n    )\n    parser.add_argument(\n        \"--v_parameterization\", action=\"store_true\", help=\"enable v-parameterization training / v-parameterization学習を有効にする\"\n    )\n    parser.add_argument(\"--prompt\", type=str, default=None, help=\"prompt / プロンプト\")\n    parser.add_argument(\n        \"--from_file\",\n        type=str,\n        default=None,\n        help=\"if specified, load prompts from this file / 指定時はプロンプトをファイルから読み込む\",\n    )\n    parser.add_argument(\n        \"--interactive\",\n        action=\"store_true\",\n        help=\"interactive mode (generates one image) / 対話モード（生成される画像は1枚になります）\",\n    )\n    parser.add_argument(\n        \"--no_preview\", action=\"store_true\", help=\"do not show generated image in interactive mode / 対話モードで画像を表示しない\"\n    )\n    parser.add_argument(\n        \"--image_path\", type=str, default=None, help=\"image to inpaint or to generate from / img2imgまたはinpaintを行う元画像\"\n    )\n    parser.add_argument(\"--mask_path\", type=str, default=None, help=\"mask in inpainting / inpaint時のマスク\")\n    parser.add_argument(\"--strength\", type=float, default=None, help=\"img2img strength / img2img時のstrength\")\n    parser.add_argument(\"--images_per_prompt\", type=int, default=1, help=\"number of images per prompt / プロンプトあたりの出力枚数\")\n    parser.add_argument(\"--outdir\", type=str, default=\"outputs\", help=\"dir to write results to / 生成画像の出力先\")\n    parser.add_argument(\n        \"--sequential_file_name\", action=\"store_true\", help=\"sequential output file name / 生成画像のファイル名を連番にする\"\n    )\n    parser.add_argument(\n        \"--use_original_file_name\",\n        action=\"store_true\",\n        help=\"prepend original file name in img2img / img2imgで元画像のファイル名を生成画像のファイル名の先頭に付ける\",\n    )\n    # parser.add_argument(\"--ddim_eta\", type=float, default=0.0, help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\", )\n    parser.add_argument(\"--n_iter\", type=int, default=1, help=\"sample this often / 繰り返し回数\")\n    parser.add_argument(\"--H\", type=int, default=None, help=\"image height, in pixel space / 生成画像高さ\")\n    parser.add_argument(\"--W\", type=int, default=None, help=\"image width, in pixel space / 生成画像幅\")\n    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"batch size / バッチサイズ\")\n    parser.add_argument(\n        \"--vae_batch_size\",\n        type=float,\n        default=None,\n        help=\"batch size for VAE, < 1.0 for ratio / VAE処理時のバッチサイズ、1未満の値の場合は通常バッチサイズの比率\",\n    )\n    parser.add_argument(\n        \"--vae_slices\",\n        type=int,\n        default=None,\n        help=\"number of slices to split image into for VAE to reduce VRAM usage, None for no splitting (default), slower if specified. 16 or 32 recommended / VAE処理時にVRAM使用量削減のため画像を分割するスライス数、Noneの場合は分割しない（デフォルト）、指定すると遅くなる。16か32程度を推奨\",\n    )\n    parser.add_argument(\"--steps\", type=int, default=50, help=\"number of ddim sampling steps / サンプリングステップ数\")\n    parser.add_argument(\n        \"--sampler\",\n        type=str,\n        default=\"ddim\",\n        choices=[\n            \"ddim\",\n            \"pndm\",\n            \"lms\",\n            \"euler\",\n            \"euler_a\",\n            \"heun\",\n            \"dpm_2\",\n            \"dpm_2_a\",\n            \"dpmsolver\",\n            \"dpmsolver++\",\n            \"dpmsingle\",\n            \"k_lms\",\n            \"k_euler\",\n            \"k_euler_a\",\n            \"k_dpm_2\",\n            \"k_dpm_2_a\",\n        ],\n        help=f\"sampler (scheduler) type / サンプラー（スケジューラ）の種類\",\n    )\n    parser.add_argument(\n        \"--scale\",\n        type=float,\n        default=7.5,\n        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty)) / guidance scale\",\n    )\n    parser.add_argument(\n        \"--ckpt\", type=str, default=None, help=\"path to checkpoint of model / モデルのcheckpointファイルまたはディレクトリ\"\n    )\n    parser.add_argument(\n        \"--vae\",\n        type=str,\n        default=None,\n        help=\"path to checkpoint of vae to replace / VAEを入れ替える場合、VAEのcheckpointファイルまたはディレクトリ\",\n    )\n    parser.add_argument(\n        \"--tokenizer_cache_dir\",\n        type=str,\n        default=None,\n        help=\"directory for caching Tokenizer (for offline training) / Tokenizerをキャッシュするディレクトリ（ネット接続なしでの学習のため）\",\n    )\n    # parser.add_argument(\"--replace_clip_l14_336\", action='store_true',\n    #                     help=\"Replace CLIP (Text Encoder) to l/14@336 / CLIP(Text Encoder)をl/14@336に入れ替える\")\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=None,\n        help=\"seed, or seed of seeds in multiple generation / 1枚生成時のseed、または複数枚生成時の乱数seedを決めるためのseed\",\n    )\n    parser.add_argument(\n        \"--iter_same_seed\",\n        action=\"store_true\",\n        help=\"use same seed for all prompts in iteration if no seed specified / 乱数seedの指定がないとき繰り返し内はすべて同じseedを使う（プロンプト間の差異の比較用）\",\n    )\n    parser.add_argument(\n        \"--shuffle_prompts\",\n        action=\"store_true\",\n        help=\"shuffle prompts in iteration / 繰り返し内のプロンプトをシャッフルする\",\n    )\n    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"use fp16 / fp16を指定し省メモリ化する\")\n    parser.add_argument(\"--bf16\", action=\"store_true\", help=\"use bfloat16 / bfloat16を指定し省メモリ化する\")\n    parser.add_argument(\"--xformers\", action=\"store_true\", help=\"use xformers / xformersを使用し高速化する\")\n    parser.add_argument(\"--sdpa\", action=\"store_true\", help=\"use sdpa in PyTorch 2 / sdpa\")\n    parser.add_argument(\n        \"--diffusers_xformers\",\n        action=\"store_true\",\n        help=\"use xformers by diffusers (Hypernetworks doesn't work) / Diffusersでxformersを使用する（Hypernetwork利用不可）\",\n    )\n    parser.add_argument(\n        \"--opt_channels_last\",\n        action=\"store_true\",\n        help=\"set channels last option to model / モデルにchannels lastを指定し最適化する\",\n    )\n    parser.add_argument(\n        \"--network_module\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"additional network module to use / 追加ネットワークを使う時そのモジュール名\",\n    )\n    parser.add_argument(\n        \"--network_weights\", type=str, default=None, nargs=\"*\", help=\"additional network weights to load / 追加ネットワークの重み\"\n    )\n    parser.add_argument(\n        \"--network_mul\", type=float, default=None, nargs=\"*\", help=\"additional network multiplier / 追加ネットワークの効果の倍率\"\n    )\n    parser.add_argument(\n        \"--network_args\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"additional arguments for network (key=value) / ネットワークへの追加の引数\",\n    )\n    parser.add_argument(\n        \"--network_show_meta\", action=\"store_true\", help=\"show metadata of network model / ネットワークモデルのメタデータを表示する\"\n    )\n    parser.add_argument(\n        \"--network_merge_n_models\",\n        type=int,\n        default=None,\n        help=\"merge this number of networks / この数だけネットワークをマージする\",\n    )\n    parser.add_argument(\n        \"--network_merge\", action=\"store_true\", help=\"merge network weights to original model / ネットワークの重みをマージする\"\n    )\n    parser.add_argument(\n        \"--network_pre_calc\",\n        action=\"store_true\",\n        help=\"pre-calculate network for generation / ネットワークのあらかじめ計算して生成する\",\n    )\n    parser.add_argument(\n        \"--network_regional_mask_max_color_codes\",\n        type=int,\n        default=None,\n        help=\"max color codes for regional mask (default is None, mask by channel) / regional maskの最大色数（デフォルトはNoneでチャンネルごとのマスク）\",\n    )\n    parser.add_argument(\n        \"--textual_inversion_embeddings\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"Embeddings files of Textual Inversion / Textual Inversionのembeddings\",\n    )\n    parser.add_argument(\n        \"--XTI_embeddings\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"Embeddings files of Extended Textual Inversion / Extended Textual Inversionのembeddings\",\n    )\n    parser.add_argument(\n        \"--clip_skip\", type=int, default=None, help=\"layer number from bottom to use in CLIP / CLIPの後ろからn層目の出力を使う\"\n    )\n    parser.add_argument(\n        \"--max_embeddings_multiples\",\n        type=int,\n        default=None,\n        help=\"max embedding multiples, max token length is 75 * multiples / トークン長をデフォルトの何倍とするか 75*この値 がトークン長となる\",\n    )\n    parser.add_argument(\n        \"--clip_guidance_scale\",\n        type=float,\n        default=0.0,\n        help=\"enable CLIP guided SD, scale for guidance (DDIM, PNDM, LMS samplers only) / CLIP guided SDを有効にしてこのscaleを適用する（サンプラーはDDIM、PNDM、LMSのみ）\",\n    )\n    parser.add_argument(\n        \"--clip_image_guidance_scale\",\n        type=float,\n        default=0.0,\n        help=\"enable CLIP guided SD by image, scale for guidance / 画像によるCLIP guided SDを有効にしてこのscaleを適用する\",\n    )\n    parser.add_argument(\n        \"--vgg16_guidance_scale\",\n        type=float,\n        default=0.0,\n        help=\"enable VGG16 guided SD by image, scale for guidance / 画像によるVGG16 guided SDを有効にしてこのscaleを適用する\",\n    )\n    parser.add_argument(\n        \"--vgg16_guidance_layer\",\n        type=int,\n        default=20,\n        help=\"layer of VGG16 to calculate contents guide (1~30, 20 for conv4_2) / VGG16のcontents guideに使うレイヤー番号 (1~30、20はconv4_2)\",\n    )\n    parser.add_argument(\n        \"--guide_image_path\", type=str, default=None, nargs=\"*\", help=\"image to CLIP guidance / CLIP guided SDでガイドに使う画像\"\n    )\n    parser.add_argument(\n        \"--highres_fix_scale\",\n        type=float,\n        default=None,\n        help=\"enable highres fix, reso scale for 1st stage / highres fixを有効にして最初の解像度をこのscaleにする\",\n    )\n    parser.add_argument(\n        \"--highres_fix_steps\",\n        type=int,\n        default=28,\n        help=\"1st stage steps for highres fix / highres fixの最初のステージのステップ数\",\n    )\n    parser.add_argument(\n        \"--highres_fix_strength\",\n        type=float,\n        default=None,\n        help=\"1st stage img2img strength for highres fix / highres fixの最初のステージのimg2img時のstrength、省略時はstrengthと同じ\",\n    )\n    parser.add_argument(\n        \"--highres_fix_save_1st\",\n        action=\"store_true\",\n        help=\"save 1st stage images for highres fix / highres fixの最初のステージの画像を保存する\",\n    )\n    parser.add_argument(\n        \"--highres_fix_latents_upscaling\",\n        action=\"store_true\",\n        help=\"use latents upscaling for highres fix / highres fixでlatentで拡大する\",\n    )\n    parser.add_argument(\n        \"--highres_fix_upscaler\",\n        type=str,\n        default=None,\n        help=\"upscaler module for highres fix / highres fixで使うupscalerのモジュール名\",\n    )\n    parser.add_argument(\n        \"--highres_fix_upscaler_args\",\n        type=str,\n        default=None,\n        help=\"additional arguments for upscaler (key=value) / upscalerへの追加の引数\",\n    )\n    parser.add_argument(\n        \"--highres_fix_disable_control_net\",\n        action=\"store_true\",\n        help=\"disable ControlNet for highres fix / highres fixでControlNetを使わない\",\n    )\n\n    parser.add_argument(\n        \"--negative_scale\",\n        type=float,\n        default=None,\n        help=\"set another guidance scale for negative prompt / ネガティブプロンプトのscaleを指定する\",\n    )\n\n    parser.add_argument(\n        \"--control_net_models\", type=str, default=None, nargs=\"*\", help=\"ControlNet models to use / 使用するControlNetのモデル名\"\n    )\n    parser.add_argument(\n        \"--control_net_preps\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"ControlNet preprocess to use / 使用するControlNetのプリプロセス名\",\n    )\n    parser.add_argument(\"--control_net_weights\", type=float, default=None, nargs=\"*\", help=\"ControlNet weights / ControlNetの重み\")\n    parser.add_argument(\n        \"--control_net_ratios\",\n        type=float,\n        default=None,\n        nargs=\"*\",\n        help=\"ControlNet guidance ratio for steps / ControlNetでガイドするステップ比率\",\n    )\n    # parser.add_argument(\n    #     \"--control_net_image_path\", type=str, default=None, nargs=\"*\", help=\"image for ControlNet guidance / ControlNetでガイドに使う画像\"\n    # )\n\n    # Deep Shrink\n    parser.add_argument(\n        \"--ds_depth_1\",\n        type=int,\n        default=None,\n        help=\"Enable Deep Shrink with this depth 1, valid values are 0 to 3 / Deep Shrinkをこのdepthで有効にする\",\n    )\n    parser.add_argument(\n        \"--ds_timesteps_1\",\n        type=int,\n        default=650,\n        help=\"Apply Deep Shrink depth 1 until this timesteps / Deep Shrink depth 1を適用するtimesteps\",\n    )\n    parser.add_argument(\"--ds_depth_2\", type=int, default=None, help=\"Deep Shrink depth 2 / Deep Shrinkのdepth 2\")\n    parser.add_argument(\n        \"--ds_timesteps_2\",\n        type=int,\n        default=650,\n        help=\"Apply Deep Shrink depth 2 until this timesteps / Deep Shrink depth 2を適用するtimesteps\",\n    )\n    parser.add_argument(\n        \"--ds_ratio\", type=float, default=0.5, help=\"Deep Shrink ratio for downsampling / Deep Shrinkのdownsampling比率\"\n    )\n\n    # gradual latent\n    parser.add_argument(\n        \"--gradual_latent_timesteps\",\n        type=int,\n        default=None,\n        help=\"enable Gradual Latent hires fix and apply upscaling from this timesteps / Gradual Latent hires fixをこのtimestepsで有効にし、このtimestepsからアップスケーリングを適用する\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_ratio\",\n        type=float,\n        default=0.5,\n        help=\" this size ratio, 0.5 means 1/2 / Gradual Latent hires fixをこのサイズ比率で有効にする、0.5は1/2を意味する\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_ratio_step\",\n        type=float,\n        default=0.125,\n        help=\"step to increase ratio for Gradual Latent / Gradual Latentのratioをどのくらいずつ上げるか\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_every_n_steps\",\n        type=int,\n        default=3,\n        help=\"steps to increase size of latents every this steps for Gradual Latent / Gradual Latentでlatentsのサイズをこのステップごとに上げる\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_s_noise\",\n        type=float,\n        default=1.0,\n        help=\"s_noise for Gradual Latent / Gradual Latentのs_noise\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_unsharp_params\",\n        type=str,\n        default=None,\n        help=\"unsharp mask parameters for Gradual Latent: ksize, sigma, strength, target-x (1 means True). `3,0.5,0.5,1` or `3,1.0,1.0,0` is recommended /\"\n        + \" Gradual Latentのunsharp maskのパラメータ: ksize, sigma, strength, target-x. `3,0.5,0.5,1` または `3,1.0,1.0,0` が推奨\",\n    )\n\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    setup_logging(args, reset=True)\n    main(args)\n"
        },
        {
          "name": "library",
          "type": "tree",
          "content": null
        },
        {
          "name": "networks",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.8984375,
          "content": "accelerate==0.25.0\ntransformers==4.36.2\ndiffusers[torch]==0.25.0\nftfy==6.1.1\n# albumentations==1.3.0\nopencv-python==4.8.1.78\neinops==0.7.0\npytorch-lightning==1.9.0\nbitsandbytes==0.43.0\nprodigyopt==1.0\nlion-pytorch==0.0.6\ntensorboard\nsafetensors==0.4.2\n# gradio==3.16.2\naltair==4.2.2\neasygui==0.98.3\ntoml==0.10.2\nvoluptuous==0.13.1\nhuggingface-hub==0.20.1\n# for Image utils\nimagesize==1.4.1\n# for BLIP captioning\n# requests==2.28.2\n# timm==0.6.12\n# fairscale==0.4.13\n# for WD14 captioning (tensorflow)\n# tensorflow==2.10.1\n# for WD14 captioning (onnx)\n# onnx==1.15.0\n# onnxruntime-gpu==1.17.1\n# onnxruntime==1.17.1\n# for cuda 12.1(default 11.8)\n# onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n\n# this is for onnx: \n# protobuf==3.20.3\n# open clip for SDXL\n# open-clip-torch==2.20.0\n# For logging\nrich==13.7.0\n# for kohya_ss library\n-e .\n"
        },
        {
          "name": "sdxl_gen_img.py",
          "type": "blob",
          "size": 137.0693359375,
          "content": "import itertools\nimport json\nfrom typing import Any, List, NamedTuple, Optional, Tuple, Union, Callable\nimport glob\nimport importlib\nimport inspect\nimport time\nimport zipfile\nfrom diffusers.utils import deprecate\nfrom diffusers.configuration_utils import FrozenDict\nimport argparse\nimport math\nimport os\nimport random\nimport re\n\nimport diffusers\nimport numpy as np\n\nimport torch\nfrom library.device_utils import init_ipex, clean_memory, get_preferred_device\ninit_ipex()\n\nimport torchvision\nfrom diffusers import (\n    AutoencoderKL,\n    DDPMScheduler,\n    EulerAncestralDiscreteScheduler,\n    DPMSolverMultistepScheduler,\n    DPMSolverSinglestepScheduler,\n    LMSDiscreteScheduler,\n    PNDMScheduler,\n    DDIMScheduler,\n    EulerDiscreteScheduler,\n    HeunDiscreteScheduler,\n    KDPM2DiscreteScheduler,\n    KDPM2AncestralDiscreteScheduler,\n    # UNet2DConditionModel,\n    StableDiffusionPipeline,\n)\nfrom einops import rearrange\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPImageProcessor\nimport PIL\nfrom PIL import Image\nfrom PIL.PngImagePlugin import PngInfo\n\nimport library.model_util as model_util\nimport library.train_util as train_util\nimport library.sdxl_model_util as sdxl_model_util\nimport library.sdxl_train_util as sdxl_train_util\nfrom networks.lora import LoRANetwork\nfrom library.sdxl_original_unet import InferSdxlUNet2DConditionModel\nfrom library.original_unet import FlashAttentionFunction\nfrom networks.control_net_lllite import ControlNetLLLite\nfrom library.utils import GradualLatent, EulerAncestralDiscreteSchedulerGL\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# scheduler:\nSCHEDULER_LINEAR_START = 0.00085\nSCHEDULER_LINEAR_END = 0.0120\nSCHEDULER_TIMESTEPS = 1000\nSCHEDLER_SCHEDULE = \"scaled_linear\"\n\n# その他の設定\nLATENT_CHANNELS = 4\nDOWNSAMPLING_FACTOR = 8\n\nCLIP_VISION_MODEL = \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"\n\n# region モジュール入れ替え部\n\"\"\"\n高速化のためのモジュール入れ替え\n\"\"\"\n\n\ndef replace_unet_modules(unet: diffusers.models.unet_2d_condition.UNet2DConditionModel, mem_eff_attn, xformers, sdpa):\n    if mem_eff_attn:\n        logger.info(\"Enable memory efficient attention for U-Net\")\n\n        # これはDiffusersのU-Netではなく自前のU-Netなので置き換えなくても良い\n        unet.set_use_memory_efficient_attention(False, True)\n    elif xformers:\n        logger.info(\"Enable xformers for U-Net\")\n        try:\n            import xformers.ops\n        except ImportError:\n            raise ImportError(\"No xformers / xformersがインストールされていないようです\")\n\n        unet.set_use_memory_efficient_attention(True, False)\n    elif sdpa:\n        logger.info(\"Enable SDPA for U-Net\")\n        unet.set_use_memory_efficient_attention(False, False)\n        unet.set_use_sdpa(True)\n\n\n# TODO common train_util.py\ndef replace_vae_modules(vae: diffusers.models.AutoencoderKL, mem_eff_attn, xformers, sdpa):\n    if mem_eff_attn:\n        replace_vae_attn_to_memory_efficient()\n    elif xformers:\n        # replace_vae_attn_to_xformers() # 解像度によってxformersがエラーを出す？\n        vae.set_use_memory_efficient_attention_xformers(True)  # とりあえずこっちを使う\n    elif sdpa:\n        replace_vae_attn_to_sdpa()\n\n\ndef replace_vae_attn_to_memory_efficient():\n    logger.info(\"VAE Attention.forward has been replaced to FlashAttention (not xformers)\")\n    flash_func = FlashAttentionFunction\n\n    def forward_flash_attn(self, hidden_states, **kwargs):\n        q_bucket_size = 512\n        k_bucket_size = 1024\n\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        out = flash_func.apply(query_proj, key_proj, value_proj, None, False, q_bucket_size, k_bucket_size)\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_flash_attn_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_flash_attn(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_flash_attn_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_flash_attn\n\n\ndef replace_vae_attn_to_xformers():\n    logger.info(\"VAE: Attention.forward has been replaced to xformers\")\n    import xformers.ops\n\n    def forward_xformers(self, hidden_states, **kwargs):\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        query_proj = query_proj.contiguous()\n        key_proj = key_proj.contiguous()\n        value_proj = value_proj.contiguous()\n        out = xformers.ops.memory_efficient_attention(query_proj, key_proj, value_proj, attn_bias=None)\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_xformers_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_xformers(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_xformers_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_xformers\n\n\ndef replace_vae_attn_to_sdpa():\n    logger.info(\"VAE: Attention.forward has been replaced to sdpa\")\n\n    def forward_sdpa(self, hidden_states, **kwargs):\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.to_q(hidden_states)\n        key_proj = self.to_k(hidden_states)\n        value_proj = self.to_v(hidden_states)\n\n        query_proj, key_proj, value_proj = map(\n            lambda t: rearrange(t, \"b n (h d) -> b n h d\", h=self.heads), (query_proj, key_proj, value_proj)\n        )\n\n        out = torch.nn.functional.scaled_dot_product_attention(\n            query_proj, key_proj, value_proj, attn_mask=None, dropout_p=0.0, is_causal=False\n        )\n\n        out = rearrange(out, \"b n h d -> b n (h d)\")\n\n        # compute next hidden_states\n        # linear proj\n        hidden_states = self.to_out[0](hidden_states)\n        # dropout\n        hidden_states = self.to_out[1](hidden_states)\n\n        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n\n        # res connect and rescale\n        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n        return hidden_states\n\n    def forward_sdpa_0_14(self, hidden_states, **kwargs):\n        if not hasattr(self, \"to_q\"):\n            self.to_q = self.query\n            self.to_k = self.key\n            self.to_v = self.value\n            self.to_out = [self.proj_attn, torch.nn.Identity()]\n            self.heads = self.num_heads\n        return forward_sdpa(self, hidden_states, **kwargs)\n\n    if diffusers.__version__ < \"0.15.0\":\n        diffusers.models.attention.AttentionBlock.forward = forward_sdpa_0_14\n    else:\n        diffusers.models.attention_processor.Attention.forward = forward_sdpa\n\n\n# endregion\n\n# region 画像生成の本体：lpw_stable_diffusion.py （ASL）からコピーして修正\n# https://github.com/huggingface/diffusers/blob/main/examples/community/lpw_stable_diffusion.py\n# Pipelineだけ独立して使えないのと機能追加するのとでコピーして修正\n\n\nclass PipelineLike:\n    def __init__(\n        self,\n        device,\n        vae: AutoencoderKL,\n        text_encoders: List[CLIPTextModel],\n        tokenizers: List[CLIPTokenizer],\n        unet: InferSdxlUNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        clip_skip: int,\n    ):\n        super().__init__()\n        self.device = device\n        self.clip_skip = clip_skip\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n            )\n            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"clip_sample\"] = False\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        self.vae = vae\n        self.text_encoders = text_encoders\n        self.tokenizers = tokenizers\n        self.unet: InferSdxlUNet2DConditionModel = unet\n        self.scheduler = scheduler\n        self.safety_checker = None\n\n        self.clip_vision_model: CLIPVisionModelWithProjection = None\n        self.clip_vision_processor: CLIPImageProcessor = None\n        self.clip_vision_strength = 0.0\n\n        # Textual Inversion\n        self.token_replacements_list = []\n        for _ in range(len(self.text_encoders)):\n            self.token_replacements_list.append({})\n\n        # ControlNet # not supported yet\n        self.control_nets: List[ControlNetLLLite] = []\n        self.control_net_enabled = True  # control_netsが空ならTrueでもFalseでもControlNetは動作しない\n\n        self.gradual_latent: GradualLatent = None\n\n    # Textual Inversion\n    def add_token_replacement(self, text_encoder_index, target_token_id, rep_token_ids):\n        self.token_replacements_list[text_encoder_index][target_token_id] = rep_token_ids\n\n    def set_enable_control_net(self, en: bool):\n        self.control_net_enabled = en\n\n    def get_token_replacer(self, tokenizer):\n        tokenizer_index = self.tokenizers.index(tokenizer)\n        token_replacements = self.token_replacements_list[tokenizer_index]\n\n        def replace_tokens(tokens):\n            # logger.info(\"replace_tokens\", tokens, \"=>\", token_replacements)\n            if isinstance(tokens, torch.Tensor):\n                tokens = tokens.tolist()\n\n            new_tokens = []\n            for token in tokens:\n                if token in token_replacements:\n                    replacement = token_replacements[token]\n                    new_tokens.extend(replacement)\n                else:\n                    new_tokens.append(token)\n            return new_tokens\n\n        return replace_tokens\n\n    def set_control_nets(self, ctrl_nets):\n        self.control_nets = ctrl_nets\n\n    def set_gradual_latent(self, gradual_latent):\n        if gradual_latent is None:\n            logger.info(\"gradual_latent is disabled\")\n            self.gradual_latent = None\n        else:\n            logger.info(f\"gradual_latent is enabled: {gradual_latent}\")\n            self.gradual_latent = gradual_latent  # (ds_ratio, start_timesteps, every_n_steps, ratio_step)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        init_image: Union[torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]] = None,\n        mask_image: Union[torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]] = None,\n        height: int = 1024,\n        width: int = 1024,\n        original_height: int = None,\n        original_width: int = None,\n        original_height_negative: int = None,\n        original_width_negative: int = None,\n        crop_top: int = 0,\n        crop_left: int = 0,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_scale: float = None,\n        strength: float = 0.8,\n        # num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        vae_batch_size: float = None,\n        return_latents: bool = False,\n        # return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        is_cancelled_callback: Optional[Callable[[], bool]] = None,\n        callback_steps: Optional[int] = 1,\n        img2img_noise=None,\n        clip_guide_images=None,\n        **kwargs,\n    ):\n        # TODO support secondary prompt\n        num_images_per_prompt = 1  # fixed because already prompt is repeated\n\n        if isinstance(prompt, str):\n            batch_size = 1\n            prompt = [prompt]\n        elif isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n        reginonal_network = \" AND \" in prompt[0]\n\n        vae_batch_size = (\n            batch_size\n            if vae_batch_size is None\n            else (int(vae_batch_size) if vae_batch_size >= 1 else max(1, int(batch_size * vae_batch_size)))\n        )\n\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\" f\" {type(callback_steps)}.\"\n            )\n\n        # get prompt text embeddings\n\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        if not do_classifier_free_guidance and negative_scale is not None:\n            logger.info(f\"negative_scale is ignored if guidance scalle <= 1.0\")\n            negative_scale = None\n\n        # get unconditional embeddings for classifier free guidance\n        if negative_prompt is None:\n            negative_prompt = [\"\"] * batch_size\n        elif isinstance(negative_prompt, str):\n            negative_prompt = [negative_prompt] * batch_size\n        if batch_size != len(negative_prompt):\n            raise ValueError(\n                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                \" the batch size of `prompt`.\"\n            )\n\n        tes_text_embs = []\n        tes_uncond_embs = []\n        tes_real_uncond_embs = []\n\n        for tokenizer, text_encoder in zip(self.tokenizers, self.text_encoders):\n            token_replacer = self.get_token_replacer(tokenizer)\n\n            # use last text_pool, because it is from text encoder 2\n            text_embeddings, text_pool, uncond_embeddings, uncond_pool, _ = get_weighted_text_embeddings(\n                tokenizer,\n                text_encoder,\n                prompt=prompt,\n                uncond_prompt=negative_prompt if do_classifier_free_guidance else None,\n                max_embeddings_multiples=max_embeddings_multiples,\n                clip_skip=self.clip_skip,\n                token_replacer=token_replacer,\n                device=self.device,\n                **kwargs,\n            )\n            tes_text_embs.append(text_embeddings)\n            tes_uncond_embs.append(uncond_embeddings)\n\n            if negative_scale is not None:\n                _, real_uncond_embeddings, _ = get_weighted_text_embeddings(\n                    token_replacer,\n                    prompt=prompt,  # こちらのトークン長に合わせてuncondを作るので75トークン超で必須\n                    uncond_prompt=[\"\"] * batch_size,\n                    max_embeddings_multiples=max_embeddings_multiples,\n                    clip_skip=self.clip_skip,\n                    token_replacer=token_replacer,\n                    device=self.device,\n                    **kwargs,\n                )\n                tes_real_uncond_embs.append(real_uncond_embeddings)\n\n        # concat text encoder outputs\n        text_embeddings = tes_text_embs[0]\n        uncond_embeddings = tes_uncond_embs[0]\n        for i in range(1, len(tes_text_embs)):\n            text_embeddings = torch.cat([text_embeddings, tes_text_embs[i]], dim=2)  # n,77,2048\n            if do_classifier_free_guidance:\n                uncond_embeddings = torch.cat([uncond_embeddings, tes_uncond_embs[i]], dim=2)  # n,77,2048\n\n        if do_classifier_free_guidance:\n            if negative_scale is None:\n                text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n            else:\n                text_embeddings = torch.cat([uncond_embeddings, text_embeddings, real_uncond_embeddings])\n\n        if self.control_nets:\n            # ControlNetのhintにguide imageを流用する\n            if isinstance(clip_guide_images, PIL.Image.Image):\n                clip_guide_images = [clip_guide_images]\n            if isinstance(clip_guide_images[0], PIL.Image.Image):\n                clip_guide_images = [preprocess_image(im) for im in clip_guide_images]\n                clip_guide_images = torch.cat(clip_guide_images)\n            if isinstance(clip_guide_images, list):\n                clip_guide_images = torch.stack(clip_guide_images)\n\n            clip_guide_images = clip_guide_images.to(self.device, dtype=text_embeddings.dtype)\n\n        # create size embs\n        if original_height is None:\n            original_height = height\n        if original_width is None:\n            original_width = width\n        if original_height_negative is None:\n            original_height_negative = original_height\n        if original_width_negative is None:\n            original_width_negative = original_width\n        if crop_top is None:\n            crop_top = 0\n        if crop_left is None:\n            crop_left = 0\n        emb1 = sdxl_train_util.get_timestep_embedding(torch.FloatTensor([original_height, original_width]).unsqueeze(0), 256)\n        uc_emb1 = sdxl_train_util.get_timestep_embedding(\n            torch.FloatTensor([original_height_negative, original_width_negative]).unsqueeze(0), 256\n        )\n        emb2 = sdxl_train_util.get_timestep_embedding(torch.FloatTensor([crop_top, crop_left]).unsqueeze(0), 256)\n        emb3 = sdxl_train_util.get_timestep_embedding(torch.FloatTensor([height, width]).unsqueeze(0), 256)\n        c_vector = torch.cat([emb1, emb2, emb3], dim=1).to(self.device, dtype=text_embeddings.dtype).repeat(batch_size, 1)\n        uc_vector = torch.cat([uc_emb1, emb2, emb3], dim=1).to(self.device, dtype=text_embeddings.dtype).repeat(batch_size, 1)\n\n        if reginonal_network:\n            # use last pool for conditioning\n            num_sub_prompts = len(text_pool) // batch_size\n            text_pool = text_pool[num_sub_prompts - 1 :: num_sub_prompts]  # last subprompt\n\n        if init_image is not None and self.clip_vision_model is not None:\n            logger.info(f\"encode by clip_vision_model and apply clip_vision_strength={self.clip_vision_strength}\")\n            vision_input = self.clip_vision_processor(init_image, return_tensors=\"pt\", device=self.device)\n            pixel_values = vision_input[\"pixel_values\"].to(self.device, dtype=text_embeddings.dtype)\n\n            clip_vision_embeddings = self.clip_vision_model(pixel_values=pixel_values, output_hidden_states=True, return_dict=True)\n            clip_vision_embeddings = clip_vision_embeddings.image_embeds\n\n            if len(clip_vision_embeddings) == 1 and batch_size > 1:\n                clip_vision_embeddings = clip_vision_embeddings.repeat((batch_size, 1))\n\n            clip_vision_embeddings = clip_vision_embeddings * self.clip_vision_strength\n            assert clip_vision_embeddings.shape == text_pool.shape, f\"{clip_vision_embeddings.shape} != {text_pool.shape}\"\n            text_pool = clip_vision_embeddings  # replace: same as ComfyUI (?)\n\n        c_vector = torch.cat([text_pool, c_vector], dim=1)\n        if do_classifier_free_guidance:\n            uc_vector = torch.cat([uncond_pool, uc_vector], dim=1)\n            vector_embeddings = torch.cat([uc_vector, c_vector])\n        else:\n            vector_embeddings = c_vector\n\n        # set timesteps\n        self.scheduler.set_timesteps(num_inference_steps, self.device)\n\n        latents_dtype = text_embeddings.dtype\n        init_latents_orig = None\n        mask = None\n\n        if init_image is None:\n            # get the initial random noise unless the user supplied it\n\n            # Unlike in other pipelines, latents need to be generated in the target device\n            # for 1-to-1 results reproducibility with the CompVis implementation.\n            # However this currently doesn't work in `mps`.\n            latents_shape = (\n                batch_size * num_images_per_prompt,\n                self.unet.in_channels,\n                height // 8,\n                width // 8,\n            )\n\n            if latents is None:\n                if self.device.type == \"mps\":\n                    # randn does not exist on mps\n                    latents = torch.randn(\n                        latents_shape,\n                        generator=generator,\n                        device=\"cpu\",\n                        dtype=latents_dtype,\n                    ).to(self.device)\n                else:\n                    latents = torch.randn(\n                        latents_shape,\n                        generator=generator,\n                        device=self.device,\n                        dtype=latents_dtype,\n                    )\n            else:\n                if latents.shape != latents_shape:\n                    raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n                latents = latents.to(self.device)\n\n            timesteps = self.scheduler.timesteps.to(self.device)\n\n            # scale the initial noise by the standard deviation required by the scheduler\n            latents = latents * self.scheduler.init_noise_sigma\n        else:\n            # image to tensor\n            if isinstance(init_image, PIL.Image.Image):\n                init_image = [init_image]\n            if isinstance(init_image[0], PIL.Image.Image):\n                init_image = [preprocess_image(im) for im in init_image]\n                init_image = torch.cat(init_image)\n            if isinstance(init_image, list):\n                init_image = torch.stack(init_image)\n\n            # mask image to tensor\n            if mask_image is not None:\n                if isinstance(mask_image, PIL.Image.Image):\n                    mask_image = [mask_image]\n                if isinstance(mask_image[0], PIL.Image.Image):\n                    mask_image = torch.cat([preprocess_mask(im) for im in mask_image])  # H*W, 0 for repaint\n\n            # encode the init image into latents and scale the latents\n            init_image = init_image.to(device=self.device, dtype=latents_dtype)\n            if init_image.size()[-2:] == (height // 8, width // 8):\n                init_latents = init_image\n            else:\n                if vae_batch_size >= batch_size:\n                    init_latent_dist = self.vae.encode(init_image.to(self.vae.dtype)).latent_dist\n                    init_latents = init_latent_dist.sample(generator=generator)\n                else:\n                    clean_memory()\n                    init_latents = []\n                    for i in tqdm(range(0, min(batch_size, len(init_image)), vae_batch_size)):\n                        init_latent_dist = self.vae.encode(\n                            (init_image[i : i + vae_batch_size] if vae_batch_size > 1 else init_image[i].unsqueeze(0)).to(\n                                self.vae.dtype\n                            )\n                        ).latent_dist\n                        init_latents.append(init_latent_dist.sample(generator=generator))\n                    init_latents = torch.cat(init_latents)\n\n                init_latents = sdxl_model_util.VAE_SCALE_FACTOR * init_latents\n\n            if len(init_latents) == 1:\n                init_latents = init_latents.repeat((batch_size, 1, 1, 1))\n            init_latents_orig = init_latents\n\n            # preprocess mask\n            if mask_image is not None:\n                mask = mask_image.to(device=self.device, dtype=latents_dtype)\n                if len(mask) == 1:\n                    mask = mask.repeat((batch_size, 1, 1, 1))\n\n                # check sizes\n                if not mask.shape == init_latents.shape:\n                    raise ValueError(\"The mask and init_image should be the same size!\")\n\n            # get the original timestep using init_timestep\n            offset = self.scheduler.config.get(\"steps_offset\", 0)\n            init_timestep = int(num_inference_steps * strength) + offset\n            init_timestep = min(init_timestep, num_inference_steps)\n\n            timesteps = self.scheduler.timesteps[-init_timestep]\n            timesteps = torch.tensor([timesteps] * batch_size * num_images_per_prompt, device=self.device)\n\n            # add noise to latents using the timesteps\n            latents = self.scheduler.add_noise(init_latents, img2img_noise, timesteps)\n\n            t_start = max(num_inference_steps - init_timestep + offset, 0)\n            timesteps = self.scheduler.timesteps[t_start:].to(self.device)\n\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        num_latent_input = (3 if negative_scale is not None else 2) if do_classifier_free_guidance else 1\n\n        if self.control_nets:\n            # guided_hints = original_control_net.get_guided_hints(self.control_nets, num_latent_input, batch_size, clip_guide_images)\n            if self.control_net_enabled:\n                for control_net, _ in self.control_nets:\n                    with torch.no_grad():\n                        control_net.set_cond_image(clip_guide_images)\n            else:\n                for control_net, _ in self.control_nets:\n                    control_net.set_cond_image(None)\n\n        each_control_net_enabled = [self.control_net_enabled] * len(self.control_nets)\n\n        # # first, we downscale the latents to the half of the size\n        # # 最初に1/2に縮小する\n        # height, width = latents.shape[-2:]\n        # # latents = torch.nn.functional.interpolate(latents.float(), scale_factor=0.5, mode=\"bicubic\", align_corners=False).to(\n        # #     latents.dtype\n        # # )\n        # latents = latents[:, :, ::2, ::2]\n        # current_scale = 0.5\n\n        # # how much to increase the scale at each step: .125 seems to work well (because it's 1/8?)\n        # # 各ステップに拡大率をどのくらい増やすか：.125がよさそう（たぶん1/8なので）\n        # scale_step = 0.125\n\n        # # timesteps at which to start increasing the scale: 1000 seems to be enough\n        # # 拡大を開始するtimesteps: 1000で十分そうである\n        # start_timesteps = 1000\n\n        # # how many steps to wait before increasing the scale again\n        # # small values leads to blurry images (because the latents are blurry after the upscale, so some denoising might be needed)\n        # # large values leads to flat images\n\n        # # 何ステップごとに拡大するか\n        # # 小さいとボケる（拡大後のlatentsはボケた感じになるので、そこから数stepのdenoiseが必要と思われる）\n        # # 大きすぎると細部が書き込まれずのっぺりした感じになる\n        # every_n_steps = 5\n\n        # scale_step = input(\"scale step:\")\n        # scale_step = float(scale_step)\n        # start_timesteps = input(\"start timesteps:\")\n        # start_timesteps = int(start_timesteps)\n        # every_n_steps = input(\"every n steps:\")\n        # every_n_steps = int(every_n_steps)\n\n        # # for i, t in enumerate(tqdm(timesteps)):\n        # i = 0\n        # last_step = 0\n        # while i < len(timesteps):\n        #     t = timesteps[i]\n        #     print(f\"[{i}] t={t}\")\n\n        #     print(i, t, current_scale, latents.shape)\n        #     if t < start_timesteps and current_scale < 1.0 and i % every_n_steps == 0:\n        #         if i == last_step:\n        #             pass\n        #         else:\n        #             print(\"upscale\")\n        #             current_scale = min(current_scale + scale_step, 1.0)\n\n        #             h = int(height * current_scale) // 8 * 8\n        #             w = int(width * current_scale) // 8 * 8\n\n        #             latents = torch.nn.functional.interpolate(latents.float(), size=(h, w), mode=\"bicubic\", align_corners=False).to(\n        #                 latents.dtype\n        #             )\n        #             last_step = i\n        #             i = max(0, i - every_n_steps + 1)\n\n        #             diff = timesteps[i] - timesteps[last_step]\n        #             # resized_init_noise = torch.nn.functional.interpolate(\n        #             #     init_noise.float(), size=(h, w), mode=\"bicubic\", align_corners=False\n        #             # ).to(latents.dtype)\n        #             # latents = self.scheduler.add_noise(latents, resized_init_noise, diff)\n        #             latents = self.scheduler.add_noise(latents, torch.randn_like(latents), diff * 4)\n        #             # latents += torch.randn_like(latents) / 100 * diff\n        #             continue\n\n        enable_gradual_latent = False\n        if self.gradual_latent:\n            if not hasattr(self.scheduler, \"set_gradual_latent_params\"):\n                logger.info(\"gradual_latent is not supported for this scheduler. Ignoring.\")\n                logger.info(f'{self.scheduler.__class__.__name__}')\n            else:\n                enable_gradual_latent = True\n                step_elapsed = 1000\n                current_ratio = self.gradual_latent.ratio\n\n                # first, we downscale the latents to the specified ratio / 最初に指定された比率にlatentsをダウンスケールする\n                height, width = latents.shape[-2:]\n                org_dtype = latents.dtype\n                if org_dtype == torch.bfloat16:\n                    latents = latents.float()\n                latents = torch.nn.functional.interpolate(\n                    latents, scale_factor=current_ratio, mode=\"bicubic\", align_corners=False\n                ).to(org_dtype)\n\n                # apply unsharp mask / アンシャープマスクを適用する\n                if self.gradual_latent.gaussian_blur_ksize:\n                    latents = self.gradual_latent.apply_unshark_mask(latents)\n\n        for i, t in enumerate(tqdm(timesteps)):\n            resized_size = None\n            if enable_gradual_latent:\n                # gradually upscale the latents / latentsを徐々にアップスケールする\n                if (\n                    t < self.gradual_latent.start_timesteps\n                    and current_ratio < 1.0\n                    and step_elapsed >= self.gradual_latent.every_n_steps\n                ):\n                    current_ratio = min(current_ratio + self.gradual_latent.ratio_step, 1.0)\n                    # make divisible by 8 because size of latents must be divisible at bottom of UNet\n                    h = int(height * current_ratio) // 8 * 8\n                    w = int(width * current_ratio) // 8 * 8\n                    resized_size = (h, w)\n                    self.scheduler.set_gradual_latent_params(resized_size, self.gradual_latent)\n                    step_elapsed = 0\n                else:\n                    self.scheduler.set_gradual_latent_params(None, None)\n                step_elapsed += 1\n\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = latents.repeat((num_latent_input, 1, 1, 1))\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n            # disable control net if ratio is set\n            if self.control_nets and self.control_net_enabled:\n                for j, ((control_net, ratio), enabled) in enumerate(zip(self.control_nets, each_control_net_enabled)):\n                    if not enabled or ratio >= 1.0:\n                        continue\n                    if ratio < i / len(timesteps):\n                        logger.info(f\"ControlNet {j} is disabled (ratio={ratio} at {i} / {len(timesteps)})\")\n                        control_net.set_cond_image(None)\n                        each_control_net_enabled[j] = False\n\n            # predict the noise residual\n            # TODO Diffusers' ControlNet\n            # if self.control_nets and self.control_net_enabled:\n            #     if reginonal_network:\n            #         num_sub_and_neg_prompts = len(text_embeddings) // batch_size\n            #         text_emb_last = text_embeddings[num_sub_and_neg_prompts - 2 :: num_sub_and_neg_prompts]  # last subprompt\n            #     else:\n            #         text_emb_last = text_embeddings\n\n            #     # not working yet\n            #     noise_pred = original_control_net.call_unet_and_control_net(\n            #         i,\n            #         num_latent_input,\n            #         self.unet,\n            #         self.control_nets,\n            #         guided_hints,\n            #         i / len(timesteps),\n            #         latent_model_input,\n            #         t,\n            #         text_emb_last,\n            #     ).sample\n            # else:\n            noise_pred = self.unet(latent_model_input, t, text_embeddings, vector_embeddings)\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                if negative_scale is None:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(num_latent_input)  # uncond by negative prompt\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n                else:\n                    noise_pred_negative, noise_pred_text, noise_pred_uncond = noise_pred.chunk(\n                        num_latent_input\n                    )  # uncond is real uncond\n                    noise_pred = (\n                        noise_pred_uncond\n                        + guidance_scale * (noise_pred_text - noise_pred_uncond)\n                        - negative_scale * (noise_pred_negative - noise_pred_uncond)\n                    )\n\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n            if mask is not None:\n                # masking\n                init_latents_proper = self.scheduler.add_noise(init_latents_orig, img2img_noise, torch.tensor([t]))\n                latents = (init_latents_proper * mask) + (latents * (1 - mask))\n\n            # call the callback, if provided\n            if i % callback_steps == 0:\n                if callback is not None:\n                    callback(i, t, latents)\n                if is_cancelled_callback is not None and is_cancelled_callback():\n                    return None\n\n            i += 1\n\n        if return_latents:\n            return latents\n\n        latents = 1 / sdxl_model_util.VAE_SCALE_FACTOR * latents\n        if vae_batch_size >= batch_size:\n            image = self.vae.decode(latents.to(self.vae.dtype)).sample\n        else:\n            clean_memory()\n            images = []\n            for i in tqdm(range(0, batch_size, vae_batch_size)):\n                images.append(\n                    self.vae.decode(\n                        (latents[i : i + vae_batch_size] if vae_batch_size > 1 else latents[i].unsqueeze(0)).to(self.vae.dtype)\n                    ).sample\n                )\n            image = torch.cat(images)\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n\n        clean_memory()\n\n        if output_type == \"pil\":\n            # image = self.numpy_to_pil(image)\n            image = (image * 255).round().astype(\"uint8\")\n            image = [Image.fromarray(im) for im in image]\n\n        return image\n\n        # return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n\n\nre_attention = re.compile(\n    r\"\"\"\n\\\\\\(|\n\\\\\\)|\n\\\\\\[|\n\\\\]|\n\\\\\\\\|\n\\\\|\n\\(|\n\\[|\n:([+-]?[.\\d]+)\\)|\n\\)|\n]|\n[^\\\\()\\[\\]:]+|\n:\n\"\"\",\n    re.X,\n)\n\n\ndef parse_prompt_attention(text):\n    \"\"\"\n    Parses a string with attention tokens and returns a list of pairs: text and its associated weight.\n    Accepted tokens are:\n      (abc) - increases attention to abc by a multiplier of 1.1\n      (abc:3.12) - increases attention to abc by a multiplier of 3.12\n      [abc] - decreases attention to abc by a multiplier of 1.1\n      \\( - literal character '('\n      \\[ - literal character '['\n      \\) - literal character ')'\n      \\] - literal character ']'\n      \\\\ - literal character '\\'\n      anything else - just text\n    >>> parse_prompt_attention('normal text')\n    [['normal text', 1.0]]\n    >>> parse_prompt_attention('an (important) word')\n    [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n    >>> parse_prompt_attention('(unbalanced')\n    [['unbalanced', 1.1]]\n    >>> parse_prompt_attention('\\(literal\\]')\n    [['(literal]', 1.0]]\n    >>> parse_prompt_attention('(unnecessary)(parens)')\n    [['unnecessaryparens', 1.1]]\n    >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n    [['a ', 1.0],\n     ['house', 1.5730000000000004],\n     [' ', 1.1],\n     ['on', 1.0],\n     [' a ', 1.1],\n     ['hill', 0.55],\n     [', sun, ', 1.1],\n     ['sky', 1.4641000000000006],\n     ['.', 1.1]]\n    \"\"\"\n\n    res = []\n    round_brackets = []\n    square_brackets = []\n\n    round_bracket_multiplier = 1.1\n    square_bracket_multiplier = 1 / 1.1\n\n    def multiply_range(start_position, multiplier):\n        for p in range(start_position, len(res)):\n            res[p][1] *= multiplier\n\n    # keep break as separate token\n    text = text.replace(\"BREAK\", \"\\\\BREAK\\\\\")\n\n    for m in re_attention.finditer(text):\n        text = m.group(0)\n        weight = m.group(1)\n\n        if text.startswith(\"\\\\\"):\n            res.append([text[1:], 1.0])\n        elif text == \"(\":\n            round_brackets.append(len(res))\n        elif text == \"[\":\n            square_brackets.append(len(res))\n        elif weight is not None and len(round_brackets) > 0:\n            multiply_range(round_brackets.pop(), float(weight))\n        elif text == \")\" and len(round_brackets) > 0:\n            multiply_range(round_brackets.pop(), round_bracket_multiplier)\n        elif text == \"]\" and len(square_brackets) > 0:\n            multiply_range(square_brackets.pop(), square_bracket_multiplier)\n        else:\n            res.append([text, 1.0])\n\n    for pos in round_brackets:\n        multiply_range(pos, round_bracket_multiplier)\n\n    for pos in square_brackets:\n        multiply_range(pos, square_bracket_multiplier)\n\n    if len(res) == 0:\n        res = [[\"\", 1.0]]\n\n    # merge runs of identical weights\n    i = 0\n    while i + 1 < len(res):\n        if res[i][1] == res[i + 1][1] and res[i][0].strip() != \"BREAK\" and res[i + 1][0].strip() != \"BREAK\":\n            res[i][0] += res[i + 1][0]\n            res.pop(i + 1)\n        else:\n            i += 1\n\n    return res\n\n\ndef get_prompts_with_weights(tokenizer: CLIPTokenizer, token_replacer, prompt: List[str], max_length: int):\n    r\"\"\"\n    Tokenize a list of prompts and return its tokens with weights of each token.\n    No padding, starting or ending token is included.\n    \"\"\"\n    tokens = []\n    weights = []\n    truncated = False\n\n    for text in prompt:\n        texts_and_weights = parse_prompt_attention(text)\n        text_token = []\n        text_weight = []\n        for word, weight in texts_and_weights:\n            if word.strip() == \"BREAK\":\n                # pad until next multiple of tokenizer's max token length\n                pad_len = tokenizer.model_max_length - (len(text_token) % tokenizer.model_max_length)\n                logger.info(f\"BREAK pad_len: {pad_len}\")\n                for i in range(pad_len):\n                    # v2のときEOSをつけるべきかどうかわからないぜ\n                    # if i == 0:\n                    #     text_token.append(tokenizer.eos_token_id)\n                    # else:\n                    text_token.append(tokenizer.pad_token_id)\n                    text_weight.append(1.0)\n                continue\n\n            # tokenize and discard the starting and the ending token\n            token = tokenizer(word).input_ids[1:-1]\n\n            token = token_replacer(token)  # for Textual Inversion\n\n            text_token += token\n            # copy the weight by length of token\n            text_weight += [weight] * len(token)\n            # stop if the text is too long (longer than truncation limit)\n            if len(text_token) > max_length:\n                truncated = True\n                break\n        # truncate\n        if len(text_token) > max_length:\n            truncated = True\n            text_token = text_token[:max_length]\n            text_weight = text_weight[:max_length]\n        tokens.append(text_token)\n        weights.append(text_weight)\n    if truncated:\n        logger.warning(\"warning: Prompt was truncated. Try to shorten the prompt or increase max_embeddings_multiples\")\n    return tokens, weights\n\n\ndef pad_tokens_and_weights(tokens, weights, max_length, bos, eos, pad, no_boseos_middle=True, chunk_length=77):\n    r\"\"\"\n    Pad the tokens (with starting and ending tokens) and weights (with 1.0) to max_length.\n    \"\"\"\n    max_embeddings_multiples = (max_length - 2) // (chunk_length - 2)\n    weights_length = max_length if no_boseos_middle else max_embeddings_multiples * chunk_length\n    for i in range(len(tokens)):\n        tokens[i] = [bos] + tokens[i] + [eos] + [pad] * (max_length - 2 - len(tokens[i]))\n        if no_boseos_middle:\n            weights[i] = [1.0] + weights[i] + [1.0] * (max_length - 1 - len(weights[i]))\n        else:\n            w = []\n            if len(weights[i]) == 0:\n                w = [1.0] * weights_length\n            else:\n                for j in range(max_embeddings_multiples):\n                    w.append(1.0)  # weight for starting token in this chunk\n                    w += weights[i][j * (chunk_length - 2) : min(len(weights[i]), (j + 1) * (chunk_length - 2))]\n                    w.append(1.0)  # weight for ending token in this chunk\n                w += [1.0] * (weights_length - len(w))\n            weights[i] = w[:]\n\n    return tokens, weights\n\n\ndef get_unweighted_text_embeddings(\n    text_encoder: CLIPTextModel,\n    text_input: torch.Tensor,\n    chunk_length: int,\n    clip_skip: int,\n    eos: int,\n    pad: int,\n    no_boseos_middle: Optional[bool] = True,\n):\n    \"\"\"\n    When the length of tokens is a multiple of the capacity of the text encoder,\n    it should be split into chunks and sent to the text encoder individually.\n    \"\"\"\n    max_embeddings_multiples = (text_input.shape[1] - 2) // (chunk_length - 2)\n    if max_embeddings_multiples > 1:\n        text_embeddings = []\n        pool = None\n        for i in range(max_embeddings_multiples):\n            # extract the i-th chunk\n            text_input_chunk = text_input[:, i * (chunk_length - 2) : (i + 1) * (chunk_length - 2) + 2].clone()\n\n            # cover the head and the tail by the starting and the ending tokens\n            text_input_chunk[:, 0] = text_input[0, 0]\n            if pad == eos:  # v1\n                text_input_chunk[:, -1] = text_input[0, -1]\n            else:  # v2\n                for j in range(len(text_input_chunk)):\n                    if text_input_chunk[j, -1] != eos and text_input_chunk[j, -1] != pad:  # 最後に普通の文字がある\n                        text_input_chunk[j, -1] = eos\n                    if text_input_chunk[j, 1] == pad:  # BOSだけであとはPAD\n                        text_input_chunk[j, 1] = eos\n\n            # -2 is same for Text Encoder 1 and 2\n            enc_out = text_encoder(text_input_chunk, output_hidden_states=True, return_dict=True)\n            text_embedding = enc_out[\"hidden_states\"][-2]\n            if pool is None:\n                pool = enc_out.get(\"text_embeds\", None)  # use 1st chunk, if provided\n                if pool is not None:\n                    pool = train_util.pool_workaround(text_encoder, enc_out[\"last_hidden_state\"], text_input_chunk, eos)\n\n            if no_boseos_middle:\n                if i == 0:\n                    # discard the ending token\n                    text_embedding = text_embedding[:, :-1]\n                elif i == max_embeddings_multiples - 1:\n                    # discard the starting token\n                    text_embedding = text_embedding[:, 1:]\n                else:\n                    # discard both starting and ending tokens\n                    text_embedding = text_embedding[:, 1:-1]\n\n            text_embeddings.append(text_embedding)\n        text_embeddings = torch.concat(text_embeddings, axis=1)\n    else:\n        enc_out = text_encoder(text_input, output_hidden_states=True, return_dict=True)\n        text_embeddings = enc_out[\"hidden_states\"][-2]\n        pool = enc_out.get(\"text_embeds\", None)  # text encoder 1 doesn't return this\n        if pool is not None:\n            pool = train_util.pool_workaround(text_encoder, enc_out[\"last_hidden_state\"], text_input, eos)\n    return text_embeddings, pool\n\n\ndef get_weighted_text_embeddings(\n    tokenizer: CLIPTokenizer,\n    text_encoder: CLIPTextModel,\n    prompt: Union[str, List[str]],\n    uncond_prompt: Optional[Union[str, List[str]]] = None,\n    max_embeddings_multiples: Optional[int] = 1,\n    no_boseos_middle: Optional[bool] = False,\n    skip_parsing: Optional[bool] = False,\n    skip_weighting: Optional[bool] = False,\n    clip_skip=None,\n    token_replacer=None,\n    device=None,\n    **kwargs,\n):\n    max_length = (tokenizer.model_max_length - 2) * max_embeddings_multiples + 2\n    if isinstance(prompt, str):\n        prompt = [prompt]\n\n    # split the prompts with \"AND\". each prompt must have the same number of splits\n    new_prompts = []\n    for p in prompt:\n        new_prompts.extend(p.split(\" AND \"))\n    prompt = new_prompts\n\n    if not skip_parsing:\n        prompt_tokens, prompt_weights = get_prompts_with_weights(tokenizer, token_replacer, prompt, max_length - 2)\n        if uncond_prompt is not None:\n            if isinstance(uncond_prompt, str):\n                uncond_prompt = [uncond_prompt]\n            uncond_tokens, uncond_weights = get_prompts_with_weights(tokenizer, token_replacer, uncond_prompt, max_length - 2)\n    else:\n        prompt_tokens = [token[1:-1] for token in tokenizer(prompt, max_length=max_length, truncation=True).input_ids]\n        prompt_weights = [[1.0] * len(token) for token in prompt_tokens]\n        if uncond_prompt is not None:\n            if isinstance(uncond_prompt, str):\n                uncond_prompt = [uncond_prompt]\n            uncond_tokens = [token[1:-1] for token in tokenizer(uncond_prompt, max_length=max_length, truncation=True).input_ids]\n            uncond_weights = [[1.0] * len(token) for token in uncond_tokens]\n\n    # round up the longest length of tokens to a multiple of (model_max_length - 2)\n    max_length = max([len(token) for token in prompt_tokens])\n    if uncond_prompt is not None:\n        max_length = max(max_length, max([len(token) for token in uncond_tokens]))\n\n    max_embeddings_multiples = min(\n        max_embeddings_multiples,\n        (max_length - 1) // (tokenizer.model_max_length - 2) + 1,\n    )\n    max_embeddings_multiples = max(1, max_embeddings_multiples)\n    max_length = (tokenizer.model_max_length - 2) * max_embeddings_multiples + 2\n\n    # pad the length of tokens and weights\n    bos = tokenizer.bos_token_id\n    eos = tokenizer.eos_token_id\n    pad = tokenizer.pad_token_id\n    prompt_tokens, prompt_weights = pad_tokens_and_weights(\n        prompt_tokens,\n        prompt_weights,\n        max_length,\n        bos,\n        eos,\n        pad,\n        no_boseos_middle=no_boseos_middle,\n        chunk_length=tokenizer.model_max_length,\n    )\n    prompt_tokens = torch.tensor(prompt_tokens, dtype=torch.long, device=device)\n    if uncond_prompt is not None:\n        uncond_tokens, uncond_weights = pad_tokens_and_weights(\n            uncond_tokens,\n            uncond_weights,\n            max_length,\n            bos,\n            eos,\n            pad,\n            no_boseos_middle=no_boseos_middle,\n            chunk_length=tokenizer.model_max_length,\n        )\n        uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n\n    # get the embeddings\n    text_embeddings, text_pool = get_unweighted_text_embeddings(\n        text_encoder,\n        prompt_tokens,\n        tokenizer.model_max_length,\n        clip_skip,\n        eos,\n        pad,\n        no_boseos_middle=no_boseos_middle,\n    )\n    prompt_weights = torch.tensor(prompt_weights, dtype=text_embeddings.dtype, device=device)\n    if uncond_prompt is not None:\n        uncond_embeddings, uncond_pool = get_unweighted_text_embeddings(\n            text_encoder,\n            uncond_tokens,\n            tokenizer.model_max_length,\n            clip_skip,\n            eos,\n            pad,\n            no_boseos_middle=no_boseos_middle,\n        )\n        uncond_weights = torch.tensor(uncond_weights, dtype=uncond_embeddings.dtype, device=device)\n\n    # assign weights to the prompts and normalize in the sense of mean\n    # TODO: should we normalize by chunk or in a whole (current implementation)?\n    # →全体でいいんじゃないかな\n    if (not skip_parsing) and (not skip_weighting):\n        previous_mean = text_embeddings.float().mean(axis=[-2, -1]).to(text_embeddings.dtype)\n        text_embeddings *= prompt_weights.unsqueeze(-1)\n        current_mean = text_embeddings.float().mean(axis=[-2, -1]).to(text_embeddings.dtype)\n        text_embeddings *= (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)\n        if uncond_prompt is not None:\n            previous_mean = uncond_embeddings.float().mean(axis=[-2, -1]).to(uncond_embeddings.dtype)\n            uncond_embeddings *= uncond_weights.unsqueeze(-1)\n            current_mean = uncond_embeddings.float().mean(axis=[-2, -1]).to(uncond_embeddings.dtype)\n            uncond_embeddings *= (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)\n\n    if uncond_prompt is not None:\n        return text_embeddings, text_pool, uncond_embeddings, uncond_pool, prompt_tokens\n    return text_embeddings, text_pool, None, None, prompt_tokens\n\n\ndef preprocess_image(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0\n\n\ndef preprocess_mask(mask):\n    mask = mask.convert(\"L\")\n    w, h = mask.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    mask = mask.resize((w // 8, h // 8), resample=PIL.Image.BILINEAR)  # LANCZOS)\n    mask = np.array(mask).astype(np.float32) / 255.0\n    mask = np.tile(mask, (4, 1, 1))\n    mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?\n    mask = 1 - mask  # repaint white, keep black\n    mask = torch.from_numpy(mask)\n    return mask\n\n\n# regular expression for dynamic prompt:\n# starts and ends with \"{\" and \"}\"\n# contains at least one variant divided by \"|\"\n# optional framgments divided by \"$$\" at start\n# if the first fragment is \"E\" or \"e\", enumerate all variants\n# if the second fragment is a number or two numbers, repeat the variants in the range\n# if the third fragment is a string, use it as a separator\n\nRE_DYNAMIC_PROMPT = re.compile(r\"\\{((e|E)\\$\\$)?(([\\d\\-]+)\\$\\$)?(([^\\|\\}]+?)\\$\\$)?(.+?((\\|).+?)*?)\\}\")\n\n\ndef handle_dynamic_prompt_variants(prompt, repeat_count):\n    founds = list(RE_DYNAMIC_PROMPT.finditer(prompt))\n    if not founds:\n        return [prompt]\n\n    # make each replacement for each variant\n    enumerating = False\n    replacers = []\n    for found in founds:\n        # if \"e$$\" is found, enumerate all variants\n        found_enumerating = found.group(2) is not None\n        enumerating = enumerating or found_enumerating\n\n        separator = \", \" if found.group(6) is None else found.group(6)\n        variants = found.group(7).split(\"|\")\n\n        # parse count range\n        count_range = found.group(4)\n        if count_range is None:\n            count_range = [1, 1]\n        else:\n            count_range = count_range.split(\"-\")\n            if len(count_range) == 1:\n                count_range = [int(count_range[0]), int(count_range[0])]\n            elif len(count_range) == 2:\n                count_range = [int(count_range[0]), int(count_range[1])]\n            else:\n                logger.warning(f\"invalid count range: {count_range}\")\n                count_range = [1, 1]\n            if count_range[0] > count_range[1]:\n                count_range = [count_range[1], count_range[0]]\n            if count_range[0] < 0:\n                count_range[0] = 0\n            if count_range[1] > len(variants):\n                count_range[1] = len(variants)\n\n        if found_enumerating:\n            # make function to enumerate all combinations\n            def make_replacer_enum(vari, cr, sep):\n                def replacer():\n                    values = []\n                    for count in range(cr[0], cr[1] + 1):\n                        for comb in itertools.combinations(vari, count):\n                            values.append(sep.join(comb))\n                    return values\n\n                return replacer\n\n            replacers.append(make_replacer_enum(variants, count_range, separator))\n        else:\n            # make function to choose random combinations\n            def make_replacer_single(vari, cr, sep):\n                def replacer():\n                    count = random.randint(cr[0], cr[1])\n                    comb = random.sample(vari, count)\n                    return [sep.join(comb)]\n\n                return replacer\n\n            replacers.append(make_replacer_single(variants, count_range, separator))\n\n    # make each prompt\n    if not enumerating:\n        # if not enumerating, repeat the prompt, replace each variant randomly\n        prompts = []\n        for _ in range(repeat_count):\n            current = prompt\n            for found, replacer in zip(founds, replacers):\n                current = current.replace(found.group(0), replacer()[0], 1)\n            prompts.append(current)\n    else:\n        # if enumerating, iterate all combinations for previous prompts\n        prompts = [prompt]\n\n        for found, replacer in zip(founds, replacers):\n            if found.group(2) is not None:\n                # make all combinations for existing prompts\n                new_prompts = []\n                for current in prompts:\n                    replecements = replacer()\n                    for replecement in replecements:\n                        new_prompts.append(current.replace(found.group(0), replecement, 1))\n                prompts = new_prompts\n\n        for found, replacer in zip(founds, replacers):\n            # make random selection for existing prompts\n            if found.group(2) is None:\n                for i in range(len(prompts)):\n                    prompts[i] = prompts[i].replace(found.group(0), replacer()[0], 1)\n\n    return prompts\n\n\n# endregion\n\n# def load_clip_l14_336(dtype):\n#   logger.info(f\"loading CLIP: {CLIP_ID_L14_336}\")\n#   text_encoder = CLIPTextModel.from_pretrained(CLIP_ID_L14_336, torch_dtype=dtype)\n#   return text_encoder\n\n\nclass BatchDataBase(NamedTuple):\n    # バッチ分割が必要ないデータ\n    step: int\n    prompt: str\n    negative_prompt: str\n    seed: int\n    init_image: Any\n    mask_image: Any\n    clip_prompt: str\n    guide_image: Any\n    raw_prompt: str\n\n\nclass BatchDataExt(NamedTuple):\n    # バッチ分割が必要なデータ\n    width: int\n    height: int\n    original_width: int\n    original_height: int\n    original_width_negative: int\n    original_height_negative: int\n    crop_left: int\n    crop_top: int\n    steps: int\n    scale: float\n    negative_scale: float\n    strength: float\n    network_muls: Tuple[float]\n    num_sub_prompts: int\n\n\nclass BatchData(NamedTuple):\n    return_latents: bool\n    base: BatchDataBase\n    ext: BatchDataExt\n\n\ndef main(args):\n    if args.fp16:\n        dtype = torch.float16\n    elif args.bf16:\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n\n    highres_fix = args.highres_fix_scale is not None\n    # assert not highres_fix or args.image_path is None, f\"highres_fix doesn't work with img2img / highres_fixはimg2imgと同時に使えません\"\n\n    # モデルを読み込む\n    if not os.path.isfile(args.ckpt):  # ファイルがないならパターンで探し、一つだけ該当すればそれを使う\n        files = glob.glob(args.ckpt)\n        if len(files) == 1:\n            args.ckpt = files[0]\n\n    (_, text_encoder1, text_encoder2, vae, unet, _, _) = sdxl_train_util._load_target_model(\n        args.ckpt, args.vae, sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, dtype\n    )\n    unet: InferSdxlUNet2DConditionModel = InferSdxlUNet2DConditionModel(unet)\n\n    # xformers、Hypernetwork対応\n    if not args.diffusers_xformers:\n        mem_eff = not (args.xformers or args.sdpa)\n        replace_unet_modules(unet, mem_eff, args.xformers, args.sdpa)\n        replace_vae_modules(vae, mem_eff, args.xformers, args.sdpa)\n\n    # tokenizerを読み込む\n    logger.info(\"loading tokenizer\")\n    tokenizer1, tokenizer2 = sdxl_train_util.load_tokenizers(args)\n\n    # schedulerを用意する\n    sched_init_args = {}\n    has_steps_offset = True\n    has_clip_sample = True\n    scheduler_num_noises_per_step = 1\n\n    if args.sampler == \"ddim\":\n        scheduler_cls = DDIMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_ddim\n    elif args.sampler == \"ddpm\":  # ddpmはおかしくなるのでoptionから外してある\n        scheduler_cls = DDPMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_ddpm\n    elif args.sampler == \"pndm\":\n        scheduler_cls = PNDMScheduler\n        scheduler_module = diffusers.schedulers.scheduling_pndm\n        has_clip_sample = False\n    elif args.sampler == \"lms\" or args.sampler == \"k_lms\":\n        scheduler_cls = LMSDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_lms_discrete\n        has_clip_sample = False\n    elif args.sampler == \"euler\" or args.sampler == \"k_euler\":\n        scheduler_cls = EulerDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_euler_discrete\n        has_clip_sample = False\n    elif args.sampler == \"euler_a\" or args.sampler == \"k_euler_a\":\n        scheduler_cls = EulerAncestralDiscreteSchedulerGL\n        scheduler_module = diffusers.schedulers.scheduling_euler_ancestral_discrete\n        has_clip_sample = False\n    elif args.sampler == \"dpmsolver\" or args.sampler == \"dpmsolver++\":\n        scheduler_cls = DPMSolverMultistepScheduler\n        sched_init_args[\"algorithm_type\"] = args.sampler\n        scheduler_module = diffusers.schedulers.scheduling_dpmsolver_multistep\n        has_clip_sample = False\n    elif args.sampler == \"dpmsingle\":\n        scheduler_cls = DPMSolverSinglestepScheduler\n        scheduler_module = diffusers.schedulers.scheduling_dpmsolver_singlestep\n        has_clip_sample = False\n        has_steps_offset = False\n    elif args.sampler == \"heun\":\n        scheduler_cls = HeunDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_heun_discrete\n        has_clip_sample = False\n    elif args.sampler == \"dpm_2\" or args.sampler == \"k_dpm_2\":\n        scheduler_cls = KDPM2DiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_k_dpm_2_discrete\n        has_clip_sample = False\n    elif args.sampler == \"dpm_2_a\" or args.sampler == \"k_dpm_2_a\":\n        scheduler_cls = KDPM2AncestralDiscreteScheduler\n        scheduler_module = diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete\n        scheduler_num_noises_per_step = 2\n        has_clip_sample = False\n\n    # 警告を出さないようにする\n    if has_steps_offset:\n        sched_init_args[\"steps_offset\"] = 1\n    if has_clip_sample:\n        sched_init_args[\"clip_sample\"] = False\n\n    # samplerの乱数をあらかじめ指定するための処理\n\n    # replace randn\n    class NoiseManager:\n        def __init__(self):\n            self.sampler_noises = None\n            self.sampler_noise_index = 0\n\n        def reset_sampler_noises(self, noises):\n            self.sampler_noise_index = 0\n            self.sampler_noises = noises\n\n        def randn(self, shape, device=None, dtype=None, layout=None, generator=None):\n            # logger.info(\"replacing\", shape, len(self.sampler_noises), self.sampler_noise_index)\n            if self.sampler_noises is not None and self.sampler_noise_index < len(self.sampler_noises):\n                noise = self.sampler_noises[self.sampler_noise_index]\n                if shape != noise.shape:\n                    noise = None\n            else:\n                noise = None\n\n            if noise == None:\n                logger.warning(f\"unexpected noise request: {self.sampler_noise_index}, {shape}\")\n                noise = torch.randn(shape, dtype=dtype, device=device, generator=generator)\n\n            self.sampler_noise_index += 1\n            return noise\n\n    class TorchRandReplacer:\n        def __init__(self, noise_manager):\n            self.noise_manager = noise_manager\n\n        def __getattr__(self, item):\n            if item == \"randn\":\n                return self.noise_manager.randn\n            if hasattr(torch, item):\n                return getattr(torch, item)\n            raise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, item))\n\n    noise_manager = NoiseManager()\n    if scheduler_module is not None:\n        scheduler_module.torch = TorchRandReplacer(noise_manager)\n\n    scheduler = scheduler_cls(\n        num_train_timesteps=SCHEDULER_TIMESTEPS,\n        beta_start=SCHEDULER_LINEAR_START,\n        beta_end=SCHEDULER_LINEAR_END,\n        beta_schedule=SCHEDLER_SCHEDULE,\n        **sched_init_args,\n    )\n\n    # ↓以下は結局PipeでFalseに設定されるので意味がなかった\n    # # clip_sample=Trueにする\n    # if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is False:\n    #     logger.info(\"set clip_sample to True\")\n    #     scheduler.config.clip_sample = True\n\n    # deviceを決定する\n    device = get_preferred_device()\n\n    # custom pipelineをコピったやつを生成する\n    if args.vae_slices:\n        from library.slicing_vae import SlicingAutoencoderKL\n\n        sli_vae = SlicingAutoencoderKL(\n            act_fn=\"silu\",\n            block_out_channels=(128, 256, 512, 512),\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            in_channels=3,\n            latent_channels=4,\n            layers_per_block=2,\n            norm_num_groups=32,\n            out_channels=3,\n            sample_size=512,\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            num_slices=args.vae_slices,\n        )\n        sli_vae.load_state_dict(vae.state_dict())  # vaeのパラメータをコピーする\n        vae = sli_vae\n        del sli_vae\n\n    vae_dtype = dtype\n    if args.no_half_vae:\n        logger.info(\"set vae_dtype to float32\")\n        vae_dtype = torch.float32\n    vae.to(vae_dtype).to(device)\n    vae.eval()\n\n    text_encoder1.to(dtype).to(device)\n    text_encoder2.to(dtype).to(device)\n    unet.to(dtype).to(device)\n    text_encoder1.eval()\n    text_encoder2.eval()\n    unet.eval()\n\n    # networkを組み込む\n    if args.network_module:\n        networks = []\n        network_default_muls = []\n        network_pre_calc = args.network_pre_calc\n\n        # merge関連の引数を統合する\n        if args.network_merge:\n            network_merge = len(args.network_module)  # all networks are merged\n        elif args.network_merge_n_models:\n            network_merge = args.network_merge_n_models\n        else:\n            network_merge = 0\n        logger.info(f\"network_merge: {network_merge}\")\n\n        for i, network_module in enumerate(args.network_module):\n            logger.info(f\"import network module: {network_module}\")\n            imported_module = importlib.import_module(network_module)\n\n            network_mul = 1.0 if args.network_mul is None or len(args.network_mul) <= i else args.network_mul[i]\n\n            net_kwargs = {}\n            if args.network_args and i < len(args.network_args):\n                network_args = args.network_args[i]\n                # TODO escape special chars\n                network_args = network_args.split(\";\")\n                for net_arg in network_args:\n                    key, value = net_arg.split(\"=\")\n                    net_kwargs[key] = value\n\n            if args.network_weights is None or len(args.network_weights) <= i:\n                raise ValueError(\"No weight. Weight is required.\")\n\n            network_weight = args.network_weights[i]\n            logger.info(f\"load network weights from: {network_weight}\")\n\n            if model_util.is_safetensors(network_weight) and args.network_show_meta:\n                from safetensors.torch import safe_open\n\n                with safe_open(network_weight, framework=\"pt\") as f:\n                    metadata = f.metadata()\n                if metadata is not None:\n                    logger.info(f\"metadata for: {network_weight}: {metadata}\")\n\n            network, weights_sd = imported_module.create_network_from_weights(\n                network_mul, network_weight, vae, [text_encoder1, text_encoder2], unet, for_inference=True, **net_kwargs\n            )\n            if network is None:\n                return\n\n            mergeable = network.is_mergeable()\n            if network_merge and not mergeable:\n                logger.warning(\"network is not mergiable. ignore merge option.\")\n\n            if not mergeable or i >= network_merge:\n                # not merging\n                network.apply_to([text_encoder1, text_encoder2], unet)\n                info = network.load_state_dict(weights_sd, False)  # network.load_weightsを使うようにするとよい\n                logger.info(f\"weights are loaded: {info}\")\n\n                if args.opt_channels_last:\n                    network.to(memory_format=torch.channels_last)\n                network.to(dtype).to(device)\n\n                if network_pre_calc:\n                    logger.info(\"backup original weights\")\n                    network.backup_weights()\n\n                networks.append(network)\n                network_default_muls.append(network_mul)\n            else:\n                network.merge_to([text_encoder1, text_encoder2], unet, weights_sd, dtype, device)\n\n    else:\n        networks = []\n\n    # upscalerの指定があれば取得する\n    upscaler = None\n    if args.highres_fix_upscaler:\n        logger.info(f\"import upscaler module: {args.highres_fix_upscaler}\")\n        imported_module = importlib.import_module(args.highres_fix_upscaler)\n\n        us_kwargs = {}\n        if args.highres_fix_upscaler_args:\n            for net_arg in args.highres_fix_upscaler_args.split(\";\"):\n                key, value = net_arg.split(\"=\")\n                us_kwargs[key] = value\n\n        logger.info(\"create upscaler\")\n        upscaler = imported_module.create_upscaler(**us_kwargs)\n        upscaler.to(dtype).to(device)\n\n    # ControlNetの処理\n    control_nets: List[Tuple[ControlNetLLLite, float]] = []\n    # if args.control_net_models:\n    #     for i, model in enumerate(args.control_net_models):\n    #         prep_type = None if not args.control_net_preps or len(args.control_net_preps) <= i else args.control_net_preps[i]\n    #         weight = 1.0 if not args.control_net_weights or len(args.control_net_weights) <= i else args.control_net_weights[i]\n    #         ratio = 1.0 if not args.control_net_ratios or len(args.control_net_ratios) <= i else args.control_net_ratios[i]\n\n    #         ctrl_unet, ctrl_net = original_control_net.load_control_net(False, unet, model)\n    #         prep = original_control_net.load_preprocess(prep_type)\n    #         control_nets.append(ControlNetInfo(ctrl_unet, ctrl_net, prep, weight, ratio))\n    if args.control_net_lllite_models:\n        for i, model_file in enumerate(args.control_net_lllite_models):\n            logger.info(f\"loading ControlNet-LLLite: {model_file}\")\n\n            from safetensors.torch import load_file\n\n            state_dict = load_file(model_file)\n            mlp_dim = None\n            cond_emb_dim = None\n            for key, value in state_dict.items():\n                if mlp_dim is None and \"down.0.weight\" in key:\n                    mlp_dim = value.shape[0]\n                elif cond_emb_dim is None and \"conditioning1.0\" in key:\n                    cond_emb_dim = value.shape[0] * 2\n                if mlp_dim is not None and cond_emb_dim is not None:\n                    break\n            assert mlp_dim is not None and cond_emb_dim is not None, f\"invalid control net: {model_file}\"\n\n            multiplier = (\n                1.0\n                if not args.control_net_multipliers or len(args.control_net_multipliers) <= i\n                else args.control_net_multipliers[i]\n            )\n            ratio = 1.0 if not args.control_net_ratios or len(args.control_net_ratios) <= i else args.control_net_ratios[i]\n\n            control_net = ControlNetLLLite(unet, cond_emb_dim, mlp_dim, multiplier=multiplier)\n            control_net.apply_to()\n            control_net.load_state_dict(state_dict)\n            control_net.to(dtype).to(device)\n            control_net.set_batch_cond_only(False, False)\n            control_nets.append((control_net, ratio))\n\n    if args.opt_channels_last:\n        logger.info(f\"set optimizing: channels last\")\n        text_encoder1.to(memory_format=torch.channels_last)\n        text_encoder2.to(memory_format=torch.channels_last)\n        vae.to(memory_format=torch.channels_last)\n        unet.to(memory_format=torch.channels_last)\n        if networks:\n            for network in networks:\n                network.to(memory_format=torch.channels_last)\n\n        for cn in control_nets:\n            cn.to(memory_format=torch.channels_last)\n            # cn.unet.to(memory_format=torch.channels_last)\n            # cn.net.to(memory_format=torch.channels_last)\n\n    pipe = PipelineLike(\n        device,\n        vae,\n        [text_encoder1, text_encoder2],\n        [tokenizer1, tokenizer2],\n        unet,\n        scheduler,\n        args.clip_skip,\n    )\n    pipe.set_control_nets(control_nets)\n    logger.info(\"pipeline is ready.\")\n\n    if args.diffusers_xformers:\n        pipe.enable_xformers_memory_efficient_attention()\n\n    # Deep Shrink\n    if args.ds_depth_1 is not None:\n        unet.set_deep_shrink(args.ds_depth_1, args.ds_timesteps_1, args.ds_depth_2, args.ds_timesteps_2, args.ds_ratio)\n\n    # Gradual Latent\n    if args.gradual_latent_timesteps is not None:\n        if args.gradual_latent_unsharp_params:\n            us_params = args.gradual_latent_unsharp_params.split(\",\")\n            us_ksize, us_sigma, us_strength = [float(v) for v in us_params[:3]]\n            us_target_x = True if len(us_params) <= 3 else bool(int(us_params[3]))\n            us_ksize = int(us_ksize)\n        else:\n            us_ksize, us_sigma, us_strength, us_target_x = None, None, None, None\n\n        gradual_latent = GradualLatent(\n            args.gradual_latent_ratio,\n            args.gradual_latent_timesteps,\n            args.gradual_latent_every_n_steps,\n            args.gradual_latent_ratio_step,\n            args.gradual_latent_s_noise,\n            us_ksize,\n            us_sigma,\n            us_strength,\n            us_target_x,\n        )\n        pipe.set_gradual_latent(gradual_latent)\n\n    #  Textual Inversionを処理する\n    if args.textual_inversion_embeddings:\n        token_ids_embeds1 = []\n        token_ids_embeds2 = []\n        for embeds_file in args.textual_inversion_embeddings:\n            if model_util.is_safetensors(embeds_file):\n                from safetensors.torch import load_file\n\n                data = load_file(embeds_file)\n            else:\n                data = torch.load(embeds_file, map_location=\"cpu\")\n\n            if \"string_to_param\" in data:\n                data = data[\"string_to_param\"]\n\n            embeds1 = data[\"clip_l\"]  # text encoder 1\n            embeds2 = data[\"clip_g\"]  # text encoder 2\n\n            num_vectors_per_token = embeds1.size()[0]\n            token_string = os.path.splitext(os.path.basename(embeds_file))[0]\n\n            token_strings = [token_string] + [f\"{token_string}{i+1}\" for i in range(num_vectors_per_token - 1)]\n\n            # add new word to tokenizer, count is num_vectors_per_token\n            num_added_tokens1 = tokenizer1.add_tokens(token_strings)\n            num_added_tokens2 = tokenizer2.add_tokens(token_strings)\n            assert num_added_tokens1 == num_vectors_per_token and num_added_tokens2 == num_vectors_per_token, (\n                f\"tokenizer has same word to token string (filename): {embeds_file}\"\n                + f\" / 指定した名前（ファイル名）のトークンが既に存在します: {embeds_file}\"\n            )\n\n            token_ids1 = tokenizer1.convert_tokens_to_ids(token_strings)\n            token_ids2 = tokenizer2.convert_tokens_to_ids(token_strings)\n            logger.info(f\"Textual Inversion embeddings `{token_string}` loaded. Tokens are added: {token_ids1} and {token_ids2}\")\n            assert (\n                min(token_ids1) == token_ids1[0] and token_ids1[-1] == token_ids1[0] + len(token_ids1) - 1\n            ), f\"token ids1 is not ordered\"\n            assert (\n                min(token_ids2) == token_ids2[0] and token_ids2[-1] == token_ids2[0] + len(token_ids2) - 1\n            ), f\"token ids2 is not ordered\"\n            assert len(tokenizer1) - 1 == token_ids1[-1], f\"token ids 1 is not end of tokenize: {len(tokenizer1)}\"\n            assert len(tokenizer2) - 1 == token_ids2[-1], f\"token ids 2 is not end of tokenize: {len(tokenizer2)}\"\n\n            if num_vectors_per_token > 1:\n                pipe.add_token_replacement(0, token_ids1[0], token_ids1)  # hoge -> hoge, hogea, hogeb, ...\n                pipe.add_token_replacement(1, token_ids2[0], token_ids2)\n\n            token_ids_embeds1.append((token_ids1, embeds1))\n            token_ids_embeds2.append((token_ids2, embeds2))\n\n        text_encoder1.resize_token_embeddings(len(tokenizer1))\n        text_encoder2.resize_token_embeddings(len(tokenizer2))\n        token_embeds1 = text_encoder1.get_input_embeddings().weight.data\n        token_embeds2 = text_encoder2.get_input_embeddings().weight.data\n        for token_ids, embeds in token_ids_embeds1:\n            for token_id, embed in zip(token_ids, embeds):\n                token_embeds1[token_id] = embed\n        for token_ids, embeds in token_ids_embeds2:\n            for token_id, embed in zip(token_ids, embeds):\n                token_embeds2[token_id] = embed\n\n    # promptを取得する\n    if args.from_file is not None:\n        logger.info(f\"reading prompts from {args.from_file}\")\n        with open(args.from_file, \"r\", encoding=\"utf-8\") as f:\n            prompt_list = f.read().splitlines()\n            prompt_list = [d for d in prompt_list if len(d.strip()) > 0 and d[0] != \"#\"]\n    elif args.prompt is not None:\n        prompt_list = [args.prompt]\n    else:\n        prompt_list = []\n\n    if args.interactive:\n        args.n_iter = 1\n\n    # img2imgの前処理、画像の読み込みなど\n    def load_images(path):\n        if os.path.isfile(path):\n            paths = [path]\n        else:\n            paths = (\n                glob.glob(os.path.join(path, \"*.png\"))\n                + glob.glob(os.path.join(path, \"*.jpg\"))\n                + glob.glob(os.path.join(path, \"*.jpeg\"))\n                + glob.glob(os.path.join(path, \"*.webp\"))\n            )\n            paths.sort()\n\n        images = []\n        for p in paths:\n            image = Image.open(p)\n            if image.mode != \"RGB\":\n                logger.info(f\"convert image to RGB from {image.mode}: {p}\")\n                image = image.convert(\"RGB\")\n            images.append(image)\n\n        return images\n\n    def resize_images(imgs, size):\n        resized = []\n        for img in imgs:\n            r_img = img.resize(size, Image.Resampling.LANCZOS)\n            if hasattr(img, \"filename\"):  # filename属性がない場合があるらしい\n                r_img.filename = img.filename\n            resized.append(r_img)\n        return resized\n\n    if args.image_path is not None:\n        logger.info(f\"load image for img2img: {args.image_path}\")\n        init_images = load_images(args.image_path)\n        assert len(init_images) > 0, f\"No image / 画像がありません: {args.image_path}\"\n        logger.info(f\"loaded {len(init_images)} images for img2img\")\n\n        # CLIP Vision\n        if args.clip_vision_strength is not None:\n            logger.info(f\"load CLIP Vision model: {CLIP_VISION_MODEL}\")\n            vision_model = CLIPVisionModelWithProjection.from_pretrained(CLIP_VISION_MODEL, projection_dim=1280)\n            vision_model.to(device, dtype)\n            processor = CLIPImageProcessor.from_pretrained(CLIP_VISION_MODEL)\n\n            pipe.clip_vision_model = vision_model\n            pipe.clip_vision_processor = processor\n            pipe.clip_vision_strength = args.clip_vision_strength\n            logger.info(f\"CLIP Vision model loaded.\")\n\n    else:\n        init_images = None\n\n    if args.mask_path is not None:\n        logger.info(f\"load mask for inpainting: {args.mask_path}\")\n        mask_images = load_images(args.mask_path)\n        assert len(mask_images) > 0, f\"No mask image / マスク画像がありません: {args.image_path}\"\n        logger.info(f\"loaded {len(mask_images)} mask images for inpainting\")\n    else:\n        mask_images = None\n\n    # promptがないとき、画像のPngInfoから取得する\n    if init_images is not None and len(prompt_list) == 0 and not args.interactive:\n        logger.info(\"get prompts from images' metadata\")\n        for img in init_images:\n            if \"prompt\" in img.text:\n                prompt = img.text[\"prompt\"]\n                if \"negative-prompt\" in img.text:\n                    prompt += \" --n \" + img.text[\"negative-prompt\"]\n                prompt_list.append(prompt)\n\n        # プロンプトと画像を一致させるため指定回数だけ繰り返す（画像を増幅する）\n        l = []\n        for im in init_images:\n            l.extend([im] * args.images_per_prompt)\n        init_images = l\n\n        if mask_images is not None:\n            l = []\n            for im in mask_images:\n                l.extend([im] * args.images_per_prompt)\n            mask_images = l\n\n    # 画像サイズにオプション指定があるときはリサイズする\n    if args.W is not None and args.H is not None:\n        # highres fix を考慮に入れる\n        w, h = args.W, args.H\n        if highres_fix:\n            w = int(w * args.highres_fix_scale + 0.5)\n            h = int(h * args.highres_fix_scale + 0.5)\n\n        if init_images is not None:\n            logger.info(f\"resize img2img source images to {w}*{h}\")\n            init_images = resize_images(init_images, (w, h))\n        if mask_images is not None:\n            logger.info(f\"resize img2img mask images to {w}*{h}\")\n            mask_images = resize_images(mask_images, (w, h))\n\n    regional_network = False\n    if networks and mask_images:\n        # mask を領域情報として流用する、現在は一回のコマンド呼び出しで1枚だけ対応\n        regional_network = True\n        logger.info(\"use mask as region\")\n\n        size = None\n        for i, network in enumerate(networks):\n            if (i < 3 and args.network_regional_mask_max_color_codes is None) or i < args.network_regional_mask_max_color_codes:\n                np_mask = np.array(mask_images[0])\n\n                if args.network_regional_mask_max_color_codes:\n                    # カラーコードでマスクを指定する\n                    ch0 = (i + 1) & 1\n                    ch1 = ((i + 1) >> 1) & 1\n                    ch2 = ((i + 1) >> 2) & 1\n                    np_mask = np.all(np_mask == np.array([ch0, ch1, ch2]) * 255, axis=2)\n                    np_mask = np_mask.astype(np.uint8) * 255\n                else:\n                    np_mask = np_mask[:, :, i]\n                size = np_mask.shape\n            else:\n                np_mask = np.full(size, 255, dtype=np.uint8)\n            mask = torch.from_numpy(np_mask.astype(np.float32) / 255.0)\n            network.set_region(i, i == len(networks) - 1, mask)\n        mask_images = None\n\n    prev_image = None  # for VGG16 guided\n    if args.guide_image_path is not None:\n        logger.info(f\"load image for ControlNet guidance: {args.guide_image_path}\")\n        guide_images = []\n        for p in args.guide_image_path:\n            guide_images.extend(load_images(p))\n\n        logger.info(f\"loaded {len(guide_images)} guide images for guidance\")\n        if len(guide_images) == 0:\n            logger.warning(\n                f\"No guide image, use previous generated image. / ガイド画像がありません。直前に生成した画像を使います: {args.image_path}\"\n            )\n            guide_images = None\n    else:\n        guide_images = None\n\n    # seed指定時はseedを決めておく\n    if args.seed is not None:\n        # dynamic promptを使うと足りなくなる→images_per_promptを適当に大きくしておいてもらう\n        random.seed(args.seed)\n        predefined_seeds = [random.randint(0, 0x7FFFFFFF) for _ in range(args.n_iter * len(prompt_list) * args.images_per_prompt)]\n        if len(predefined_seeds) == 1:\n            predefined_seeds[0] = args.seed\n    else:\n        predefined_seeds = None\n\n    # デフォルト画像サイズを設定する：img2imgではこれらの値は無視される（またはW*Hにリサイズ済み）\n    if args.W is None:\n        args.W = 1024\n    if args.H is None:\n        args.H = 1024\n\n    # 画像生成のループ\n    os.makedirs(args.outdir, exist_ok=True)\n    max_embeddings_multiples = 1 if args.max_embeddings_multiples is None else args.max_embeddings_multiples\n\n    for gen_iter in range(args.n_iter):\n        logger.info(f\"iteration {gen_iter+1}/{args.n_iter}\")\n        iter_seed = random.randint(0, 0x7FFFFFFF)\n\n        # バッチ処理の関数\n        def process_batch(batch: List[BatchData], highres_fix, highres_1st=False):\n            batch_size = len(batch)\n\n            # highres_fixの処理\n            if highres_fix and not highres_1st:\n                # 1st stageのバッチを作成して呼び出す：サイズを小さくして呼び出す\n                is_1st_latent = upscaler.support_latents() if upscaler else args.highres_fix_latents_upscaling\n\n                logger.info(\"process 1st stage\")\n                batch_1st = []\n                for _, base, ext in batch:\n\n                    def scale_and_round(x):\n                        if x is None:\n                            return None\n                        return int(x * args.highres_fix_scale + 0.5)\n\n                    width_1st = scale_and_round(ext.width)\n                    height_1st = scale_and_round(ext.height)\n                    width_1st = width_1st - width_1st % 32\n                    height_1st = height_1st - height_1st % 32\n\n                    original_width_1st = scale_and_round(ext.original_width)\n                    original_height_1st = scale_and_round(ext.original_height)\n                    original_width_negative_1st = scale_and_round(ext.original_width_negative)\n                    original_height_negative_1st = scale_and_round(ext.original_height_negative)\n                    crop_left_1st = scale_and_round(ext.crop_left)\n                    crop_top_1st = scale_and_round(ext.crop_top)\n\n                    strength_1st = ext.strength if args.highres_fix_strength is None else args.highres_fix_strength\n\n                    ext_1st = BatchDataExt(\n                        width_1st,\n                        height_1st,\n                        original_width_1st,\n                        original_height_1st,\n                        original_width_negative_1st,\n                        original_height_negative_1st,\n                        crop_left_1st,\n                        crop_top_1st,\n                        args.highres_fix_steps,\n                        ext.scale,\n                        ext.negative_scale,\n                        strength_1st,\n                        ext.network_muls,\n                        ext.num_sub_prompts,\n                    )\n                    batch_1st.append(BatchData(is_1st_latent, base, ext_1st))\n\n                pipe.set_enable_control_net(True)  # 1st stageではControlNetを有効にする\n                images_1st = process_batch(batch_1st, True, True)\n\n                # 2nd stageのバッチを作成して以下処理する\n                logger.info(\"process 2nd stage\")\n                width_2nd, height_2nd = batch[0].ext.width, batch[0].ext.height\n\n                if upscaler:\n                    # upscalerを使って画像を拡大する\n                    lowreso_imgs = None if is_1st_latent else images_1st\n                    lowreso_latents = None if not is_1st_latent else images_1st\n\n                    # 戻り値はPIL.Image.Imageかtorch.Tensorのlatents\n                    batch_size = len(images_1st)\n                    vae_batch_size = (\n                        batch_size\n                        if args.vae_batch_size is None\n                        else (max(1, int(batch_size * args.vae_batch_size)) if args.vae_batch_size < 1 else args.vae_batch_size)\n                    )\n                    vae_batch_size = int(vae_batch_size)\n                    images_1st = upscaler.upscale(\n                        vae, lowreso_imgs, lowreso_latents, dtype, width_2nd, height_2nd, batch_size, vae_batch_size\n                    )\n\n                elif args.highres_fix_latents_upscaling:\n                    # latentを拡大する\n                    org_dtype = images_1st.dtype\n                    if images_1st.dtype == torch.bfloat16:\n                        images_1st = images_1st.to(torch.float)  # interpolateがbf16をサポートしていない\n                    images_1st = torch.nn.functional.interpolate(\n                        images_1st, (batch[0].ext.height // 8, batch[0].ext.width // 8), mode=\"bilinear\"\n                    )  # , antialias=True)\n                    images_1st = images_1st.to(org_dtype)\n\n                else:\n                    # 画像をLANCZOSで拡大する\n                    images_1st = [image.resize((width_2nd, height_2nd), resample=PIL.Image.LANCZOS) for image in images_1st]\n\n                batch_2nd = []\n                for i, (bd, image) in enumerate(zip(batch, images_1st)):\n                    bd_2nd = BatchData(False, BatchDataBase(*bd.base[0:3], bd.base.seed + 1, image, None, *bd.base[6:]), bd.ext)\n                    batch_2nd.append(bd_2nd)\n                batch = batch_2nd\n\n                if args.highres_fix_disable_control_net:\n                    pipe.set_enable_control_net(False)  # オプション指定時、2nd stageではControlNetを無効にする\n\n            # このバッチの情報を取り出す\n            (\n                return_latents,\n                (step_first, _, _, _, init_image, mask_image, _, guide_image, _),\n                (\n                    width,\n                    height,\n                    original_width,\n                    original_height,\n                    original_width_negative,\n                    original_height_negative,\n                    crop_left,\n                    crop_top,\n                    steps,\n                    scale,\n                    negative_scale,\n                    strength,\n                    network_muls,\n                    num_sub_prompts,\n                ),\n            ) = batch[0]\n            noise_shape = (LATENT_CHANNELS, height // DOWNSAMPLING_FACTOR, width // DOWNSAMPLING_FACTOR)\n\n            prompts = []\n            negative_prompts = []\n            raw_prompts = []\n            start_code = torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n            noises = [\n                torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n                for _ in range(steps * scheduler_num_noises_per_step)\n            ]\n            seeds = []\n            clip_prompts = []\n\n            if init_image is not None:  # img2img?\n                i2i_noises = torch.zeros((batch_size, *noise_shape), device=device, dtype=dtype)\n                init_images = []\n\n                if mask_image is not None:\n                    mask_images = []\n                else:\n                    mask_images = None\n            else:\n                i2i_noises = None\n                init_images = None\n                mask_images = None\n\n            if guide_image is not None:  # CLIP image guided?\n                guide_images = []\n            else:\n                guide_images = None\n\n            # バッチ内の位置に関わらず同じ乱数を使うためにここで乱数を生成しておく。あわせてimage/maskがbatch内で同一かチェックする\n            all_images_are_same = True\n            all_masks_are_same = True\n            all_guide_images_are_same = True\n            for i, (\n                _,\n                (_, prompt, negative_prompt, seed, init_image, mask_image, clip_prompt, guide_image, raw_prompt),\n                _,\n            ) in enumerate(batch):\n                prompts.append(prompt)\n                negative_prompts.append(negative_prompt)\n                seeds.append(seed)\n                clip_prompts.append(clip_prompt)\n                raw_prompts.append(raw_prompt)\n\n                if init_image is not None:\n                    init_images.append(init_image)\n                    if i > 0 and all_images_are_same:\n                        all_images_are_same = init_images[-2] is init_image\n\n                if mask_image is not None:\n                    mask_images.append(mask_image)\n                    if i > 0 and all_masks_are_same:\n                        all_masks_are_same = mask_images[-2] is mask_image\n\n                if guide_image is not None:\n                    if type(guide_image) is list:\n                        guide_images.extend(guide_image)\n                        all_guide_images_are_same = False\n                    else:\n                        guide_images.append(guide_image)\n                        if i > 0 and all_guide_images_are_same:\n                            all_guide_images_are_same = guide_images[-2] is guide_image\n\n                # make start code\n                torch.manual_seed(seed)\n                start_code[i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n                # make each noises\n                for j in range(steps * scheduler_num_noises_per_step):\n                    noises[j][i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n                if i2i_noises is not None:  # img2img noise\n                    i2i_noises[i] = torch.randn(noise_shape, device=device, dtype=dtype)\n\n            noise_manager.reset_sampler_noises(noises)\n\n            # すべての画像が同じなら1枚だけpipeに渡すことでpipe側で処理を高速化する\n            if init_images is not None and all_images_are_same:\n                init_images = init_images[0]\n            if mask_images is not None and all_masks_are_same:\n                mask_images = mask_images[0]\n            if guide_images is not None and all_guide_images_are_same:\n                guide_images = guide_images[0]\n\n            # ControlNet使用時はguide imageをリサイズする\n            if control_nets:\n                # TODO resampleのメソッド\n                guide_images = guide_images if type(guide_images) == list else [guide_images]\n                guide_images = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in guide_images]\n                if len(guide_images) == 1:\n                    guide_images = guide_images[0]\n\n            # generate\n            if networks:\n                # 追加ネットワークの処理\n                shared = {}\n                for n, m in zip(networks, network_muls if network_muls else network_default_muls):\n                    n.set_multiplier(m)\n                    if regional_network:\n                        n.set_current_generation(batch_size, num_sub_prompts, width, height, shared)\n\n                if not regional_network and network_pre_calc:\n                    for n in networks:\n                        n.restore_weights()\n                    for n in networks:\n                        n.pre_calculation()\n                    logger.info(\"pre-calculation... done\")\n\n            images = pipe(\n                prompts,\n                negative_prompts,\n                init_images,\n                mask_images,\n                height,\n                width,\n                original_height,\n                original_width,\n                original_height_negative,\n                original_width_negative,\n                crop_top,\n                crop_left,\n                steps,\n                scale,\n                negative_scale,\n                strength,\n                latents=start_code,\n                output_type=\"pil\",\n                max_embeddings_multiples=max_embeddings_multiples,\n                img2img_noise=i2i_noises,\n                vae_batch_size=args.vae_batch_size,\n                return_latents=return_latents,\n                clip_prompts=clip_prompts,\n                clip_guide_images=guide_images,\n            )\n            if highres_1st and not args.highres_fix_save_1st:  # return images or latents\n                return images\n\n            # save image\n            highres_prefix = (\"0\" if highres_1st else \"1\") if highres_fix else \"\"\n            ts_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n            for i, (image, prompt, negative_prompts, seed, clip_prompt, raw_prompt) in enumerate(\n                zip(images, prompts, negative_prompts, seeds, clip_prompts, raw_prompts)\n            ):\n                if highres_fix:\n                    seed -= 1  # record original seed\n                metadata = PngInfo()\n                metadata.add_text(\"prompt\", prompt)\n                metadata.add_text(\"seed\", str(seed))\n                metadata.add_text(\"sampler\", args.sampler)\n                metadata.add_text(\"steps\", str(steps))\n                metadata.add_text(\"scale\", str(scale))\n                if negative_prompt is not None:\n                    metadata.add_text(\"negative-prompt\", negative_prompt)\n                if negative_scale is not None:\n                    metadata.add_text(\"negative-scale\", str(negative_scale))\n                if clip_prompt is not None:\n                    metadata.add_text(\"clip-prompt\", clip_prompt)\n                if raw_prompt is not None:\n                    metadata.add_text(\"raw-prompt\", raw_prompt)\n                metadata.add_text(\"original-height\", str(original_height))\n                metadata.add_text(\"original-width\", str(original_width))\n                metadata.add_text(\"original-height-negative\", str(original_height_negative))\n                metadata.add_text(\"original-width-negative\", str(original_width_negative))\n                metadata.add_text(\"crop-top\", str(crop_top))\n                metadata.add_text(\"crop-left\", str(crop_left))\n\n                if args.use_original_file_name and init_images is not None:\n                    if type(init_images) is list:\n                        fln = os.path.splitext(os.path.basename(init_images[i % len(init_images)].filename))[0] + \".png\"\n                    else:\n                        fln = os.path.splitext(os.path.basename(init_images.filename))[0] + \".png\"\n                elif args.sequential_file_name:\n                    fln = f\"im_{highres_prefix}{step_first + i + 1:06d}.png\"\n                else:\n                    fln = f\"im_{ts_str}_{highres_prefix}{i:03d}_{seed}.png\"\n\n                image.save(os.path.join(args.outdir, fln), pnginfo=metadata)\n\n            if not args.no_preview and not highres_1st and args.interactive:\n                try:\n                    import cv2\n\n                    for prompt, image in zip(prompts, images):\n                        cv2.imshow(prompt[:128], np.array(image)[:, :, ::-1])  # プロンプトが長いと死ぬ\n                        cv2.waitKey()\n                        cv2.destroyAllWindows()\n                except ImportError:\n                    logger.error(\n                        \"opencv-python is not installed, cannot preview / opencv-pythonがインストールされていないためプレビューできません\"\n                    )\n\n            return images\n\n        # 画像生成のプロンプトが一周するまでのループ\n        prompt_index = 0\n        global_step = 0\n        batch_data = []\n        while args.interactive or prompt_index < len(prompt_list):\n            if len(prompt_list) == 0:\n                # interactive\n                valid = False\n                while not valid:\n                    logger.info(\"\")\n                    logger.info(\"Type prompt:\")\n                    try:\n                        raw_prompt = input()\n                    except EOFError:\n                        break\n\n                    valid = len(raw_prompt.strip().split(\" --\")[0].strip()) > 0\n                if not valid:  # EOF, end app\n                    break\n            else:\n                raw_prompt = prompt_list[prompt_index]\n\n            # sd-dynamic-prompts like variants:\n            # count is 1 (not dynamic) or images_per_prompt (no enumeration) or arbitrary (enumeration)\n            raw_prompts = handle_dynamic_prompt_variants(raw_prompt, args.images_per_prompt)\n\n            # repeat prompt\n            for pi in range(args.images_per_prompt if len(raw_prompts) == 1 else len(raw_prompts)):\n                raw_prompt = raw_prompts[pi] if len(raw_prompts) > 1 else raw_prompts[0]\n\n                if pi == 0 or len(raw_prompts) > 1:\n                    # parse prompt: if prompt is not changed, skip parsing\n                    width = args.W\n                    height = args.H\n                    original_width = args.original_width\n                    original_height = args.original_height\n                    original_width_negative = args.original_width_negative\n                    original_height_negative = args.original_height_negative\n                    crop_top = args.crop_top\n                    crop_left = args.crop_left\n                    scale = args.scale\n                    negative_scale = args.negative_scale\n                    steps = args.steps\n                    seed = None\n                    seeds = None\n                    strength = 0.8 if args.strength is None else args.strength\n                    negative_prompt = \"\"\n                    clip_prompt = None\n                    network_muls = None\n\n                    # Deep Shrink\n                    ds_depth_1 = None  # means no override\n                    ds_timesteps_1 = args.ds_timesteps_1\n                    ds_depth_2 = args.ds_depth_2\n                    ds_timesteps_2 = args.ds_timesteps_2\n                    ds_ratio = args.ds_ratio\n\n                    # Gradual Latent\n                    gl_timesteps = None  # means no override\n                    gl_ratio = args.gradual_latent_ratio\n                    gl_every_n_steps = args.gradual_latent_every_n_steps\n                    gl_ratio_step = args.gradual_latent_ratio_step\n                    gl_s_noise = args.gradual_latent_s_noise\n                    gl_unsharp_params = args.gradual_latent_unsharp_params\n\n                    prompt_args = raw_prompt.strip().split(\" --\")\n                    prompt = prompt_args[0]\n                    logger.info(f\"prompt {prompt_index+1}/{len(prompt_list)}: {prompt}\")\n\n                    for parg in prompt_args[1:]:\n                        try:\n                            m = re.match(r\"w (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                width = int(m.group(1))\n                                logger.info(f\"width: {width}\")\n                                continue\n\n                            m = re.match(r\"h (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                height = int(m.group(1))\n                                logger.info(f\"height: {height}\")\n                                continue\n\n                            m = re.match(r\"ow (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                original_width = int(m.group(1))\n                                logger.info(f\"original width: {original_width}\")\n                                continue\n\n                            m = re.match(r\"oh (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                original_height = int(m.group(1))\n                                logger.info(f\"original height: {original_height}\")\n                                continue\n\n                            m = re.match(r\"nw (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                original_width_negative = int(m.group(1))\n                                logger.info(f\"original width negative: {original_width_negative}\")\n                                continue\n\n                            m = re.match(r\"nh (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                original_height_negative = int(m.group(1))\n                                logger.info(f\"original height negative: {original_height_negative}\")\n                                continue\n\n                            m = re.match(r\"ct (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                crop_top = int(m.group(1))\n                                logger.info(f\"crop top: {crop_top}\")\n                                continue\n\n                            m = re.match(r\"cl (\\d+)\", parg, re.IGNORECASE)\n                            if m:\n                                crop_left = int(m.group(1))\n                                logger.info(f\"crop left: {crop_left}\")\n                                continue\n\n                            m = re.match(r\"s (\\d+)\", parg, re.IGNORECASE)\n                            if m:  # steps\n                                steps = max(1, min(1000, int(m.group(1))))\n                                logger.info(f\"steps: {steps}\")\n                                continue\n\n                            m = re.match(r\"d ([\\d,]+)\", parg, re.IGNORECASE)\n                            if m:  # seed\n                                seeds = [int(d) for d in m.group(1).split(\",\")]\n                                logger.info(f\"seeds: {seeds}\")\n                                continue\n\n                            m = re.match(r\"l ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # scale\n                                scale = float(m.group(1))\n                                logger.info(f\"scale: {scale}\")\n                                continue\n\n                            m = re.match(r\"nl ([\\d\\.]+|none|None)\", parg, re.IGNORECASE)\n                            if m:  # negative scale\n                                if m.group(1).lower() == \"none\":\n                                    negative_scale = None\n                                else:\n                                    negative_scale = float(m.group(1))\n                                logger.info(f\"negative scale: {negative_scale}\")\n                                continue\n\n                            m = re.match(r\"t ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # strength\n                                strength = float(m.group(1))\n                                logger.info(f\"strength: {strength}\")\n                                continue\n\n                            m = re.match(r\"n (.+)\", parg, re.IGNORECASE)\n                            if m:  # negative prompt\n                                negative_prompt = m.group(1)\n                                logger.info(f\"negative prompt: {negative_prompt}\")\n                                continue\n\n                            m = re.match(r\"c (.+)\", parg, re.IGNORECASE)\n                            if m:  # clip prompt\n                                clip_prompt = m.group(1)\n                                logger.info(f\"clip prompt: {clip_prompt}\")\n                                continue\n\n                            m = re.match(r\"am ([\\d\\.\\-,]+)\", parg, re.IGNORECASE)\n                            if m:  # network multiplies\n                                network_muls = [float(v) for v in m.group(1).split(\",\")]\n                                while len(network_muls) < len(networks):\n                                    network_muls.append(network_muls[-1])\n                                logger.info(f\"network mul: {network_muls}\")\n                                continue\n\n                            # Deep Shrink\n                            m = re.match(r\"dsd1 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink depth 1\n                                ds_depth_1 = int(m.group(1))\n                                logger.info(f\"deep shrink depth 1: {ds_depth_1}\")\n                                continue\n\n                            m = re.match(r\"dst1 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink timesteps 1\n                                ds_timesteps_1 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink timesteps 1: {ds_timesteps_1}\")\n                                continue\n\n                            m = re.match(r\"dsd2 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink depth 2\n                                ds_depth_2 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink depth 2: {ds_depth_2}\")\n                                continue\n\n                            m = re.match(r\"dst2 ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink timesteps 2\n                                ds_timesteps_2 = int(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink timesteps 2: {ds_timesteps_2}\")\n                                continue\n\n                            m = re.match(r\"dsr ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # deep shrink ratio\n                                ds_ratio = float(m.group(1))\n                                ds_depth_1 = ds_depth_1 if ds_depth_1 is not None else -1  # -1 means override\n                                logger.info(f\"deep shrink ratio: {ds_ratio}\")\n                                continue\n\n                            # Gradual Latent\n                            m = re.match(r\"glt ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent timesteps\n                                gl_timesteps = int(m.group(1))\n                                logger.info(f\"gradual latent timesteps: {gl_timesteps}\")\n                                continue\n\n                            m = re.match(r\"glr ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent ratio\n                                gl_ratio = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent ratio: {ds_ratio}\")\n                                continue\n\n                            m = re.match(r\"gle ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent every n steps\n                                gl_every_n_steps = int(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent every n steps: {gl_every_n_steps}\")\n                                continue\n\n                            m = re.match(r\"gls ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent ratio step\n                                gl_ratio_step = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent ratio step: {gl_ratio_step}\")\n                                continue\n\n                            m = re.match(r\"glsn ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent s noise\n                                gl_s_noise = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent s noise: {gl_s_noise}\")\n                                continue\n\n                            m = re.match(r\"glus ([\\d\\.\\-,]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent unsharp params\n                                gl_unsharp_params = m.group(1)\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent unsharp params: {gl_unsharp_params}\")\n                                continue\n\n                            # Gradual Latent\n                            m = re.match(r\"glt ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent timesteps\n                                gl_timesteps = int(m.group(1))\n                                logger.info(f\"gradual latent timesteps: {gl_timesteps}\")\n                                continue\n\n                            m = re.match(r\"glr ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent ratio\n                                gl_ratio = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent ratio: {ds_ratio}\")\n                                continue\n\n                            m = re.match(r\"gle ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent every n steps\n                                gl_every_n_steps = int(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent every n steps: {gl_every_n_steps}\")\n                                continue\n\n                            m = re.match(r\"gls ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent ratio step\n                                gl_ratio_step = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent ratio step: {gl_ratio_step}\")\n                                continue\n\n                            m = re.match(r\"glsn ([\\d\\.]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent s noise\n                                gl_s_noise = float(m.group(1))\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent s noise: {gl_s_noise}\")\n                                continue\n\n                            m = re.match(r\"glus ([\\d\\.\\-,]+)\", parg, re.IGNORECASE)\n                            if m:  # gradual latent unsharp params\n                                gl_unsharp_params = m.group(1)\n                                gl_timesteps = gl_timesteps if gl_timesteps is not None else -1  # -1 means override\n                                logger.info(f\"gradual latent unsharp params: {gl_unsharp_params}\")\n                                continue\n\n                        except ValueError as ex:\n                            logger.error(f\"Exception in parsing / 解析エラー: {parg}\")\n                            logger.error(f\"{ex}\")\n\n                # override Deep Shrink\n                if ds_depth_1 is not None:\n                    if ds_depth_1 < 0:\n                        ds_depth_1 = args.ds_depth_1 or 3\n                    unet.set_deep_shrink(ds_depth_1, ds_timesteps_1, ds_depth_2, ds_timesteps_2, ds_ratio)\n\n                # override Gradual Latent\n                if gl_timesteps is not None:\n                    if gl_timesteps < 0:\n                        gl_timesteps = args.gradual_latent_timesteps or 650\n                    if gl_unsharp_params is not None:\n                        unsharp_params = gl_unsharp_params.split(\",\")\n                        us_ksize, us_sigma, us_strength = [float(v) for v in unsharp_params[:3]]\n                        us_target_x = True if len(unsharp_params) < 4 else bool(int(unsharp_params[3]))\n                        us_ksize = int(us_ksize)\n                    else:\n                        us_ksize, us_sigma, us_strength, us_target_x = None, None, None, None\n                    gradual_latent = GradualLatent(\n                        gl_ratio,\n                        gl_timesteps,\n                        gl_every_n_steps,\n                        gl_ratio_step,\n                        gl_s_noise,\n                        us_ksize,\n                        us_sigma,\n                        us_strength,\n                        us_target_x,\n                    )\n                    pipe.set_gradual_latent(gradual_latent)\n\n                # prepare seed\n                if seeds is not None:  # given in prompt\n                    # 数が足りないなら前のをそのまま使う\n                    if len(seeds) > 0:\n                        seed = seeds.pop(0)\n                else:\n                    if predefined_seeds is not None:\n                        if len(predefined_seeds) > 0:\n                            seed = predefined_seeds.pop(0)\n                        else:\n                            logger.error(\"predefined seeds are exhausted\")\n                            seed = None\n                    elif args.iter_same_seed:\n                        seeds = iter_seed\n                    else:\n                        seed = None  # 前のを消す\n\n                if seed is None:\n                    seed = random.randint(0, 0x7FFFFFFF)\n                if args.interactive:\n                    logger.info(f\"seed: {seed}\")\n\n                # prepare init image, guide image and mask\n                init_image = mask_image = guide_image = None\n\n                # 同一イメージを使うとき、本当はlatentに変換しておくと無駄がないが面倒なのでとりあえず毎回処理する\n                if init_images is not None:\n                    init_image = init_images[global_step % len(init_images)]\n\n                    # img2imgの場合は、基本的に元画像のサイズで生成する。highres fixの場合はargs.W, args.Hとscaleに従いリサイズ済みなので無視する\n                    # 32単位に丸めたやつにresizeされるので踏襲する\n                    if not highres_fix:\n                        width, height = init_image.size\n                        width = width - width % 32\n                        height = height - height % 32\n                        if width != init_image.size[0] or height != init_image.size[1]:\n                            logger.warning(\n                                f\"img2img image size is not divisible by 32 so aspect ratio is changed / img2imgの画像サイズが32で割り切れないためリサイズされます。画像が歪みます\"\n                            )\n\n                if mask_images is not None:\n                    mask_image = mask_images[global_step % len(mask_images)]\n\n                if guide_images is not None:\n                    if control_nets:  # 複数件の場合あり\n                        c = len(control_nets)\n                        p = global_step % (len(guide_images) // c)\n                        guide_image = guide_images[p * c : p * c + c]\n                    else:\n                        guide_image = guide_images[global_step % len(guide_images)]\n\n                if regional_network:\n                    num_sub_prompts = len(prompt.split(\" AND \"))\n                    assert (\n                        len(networks) <= num_sub_prompts\n                    ), \"Number of networks must be less than or equal to number of sub prompts.\"\n                else:\n                    num_sub_prompts = None\n\n                b1 = BatchData(\n                    False,\n                    BatchDataBase(\n                        global_step, prompt, negative_prompt, seed, init_image, mask_image, clip_prompt, guide_image, raw_prompt\n                    ),\n                    BatchDataExt(\n                        width,\n                        height,\n                        original_width,\n                        original_height,\n                        original_width_negative,\n                        original_height_negative,\n                        crop_left,\n                        crop_top,\n                        steps,\n                        scale,\n                        negative_scale,\n                        strength,\n                        tuple(network_muls) if network_muls else None,\n                        num_sub_prompts,\n                    ),\n                )\n                if len(batch_data) > 0 and batch_data[-1].ext != b1.ext:  # バッチ分割必要？\n                    process_batch(batch_data, highres_fix)\n                    batch_data.clear()\n\n                batch_data.append(b1)\n                if len(batch_data) == args.batch_size:\n                    prev_image = process_batch(batch_data, highres_fix)[0]\n                    batch_data.clear()\n\n                global_step += 1\n\n            prompt_index += 1\n\n        if len(batch_data) > 0:\n            process_batch(batch_data, highres_fix)\n            batch_data.clear()\n\n    logger.info(\"done!\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n\n    parser.add_argument(\"--prompt\", type=str, default=None, help=\"prompt / プロンプト\")\n    parser.add_argument(\n        \"--from_file\",\n        type=str,\n        default=None,\n        help=\"if specified, load prompts from this file / 指定時はプロンプトをファイルから読み込む\",\n    )\n    parser.add_argument(\n        \"--interactive\",\n        action=\"store_true\",\n        help=\"interactive mode (generates one image) / 対話モード（生成される画像は1枚になります）\",\n    )\n    parser.add_argument(\n        \"--no_preview\", action=\"store_true\", help=\"do not show generated image in interactive mode / 対話モードで画像を表示しない\"\n    )\n    parser.add_argument(\n        \"--image_path\", type=str, default=None, help=\"image to inpaint or to generate from / img2imgまたはinpaintを行う元画像\"\n    )\n    parser.add_argument(\"--mask_path\", type=str, default=None, help=\"mask in inpainting / inpaint時のマスク\")\n    parser.add_argument(\"--strength\", type=float, default=None, help=\"img2img strength / img2img時のstrength\")\n    parser.add_argument(\"--images_per_prompt\", type=int, default=1, help=\"number of images per prompt / プロンプトあたりの出力枚数\")\n    parser.add_argument(\"--outdir\", type=str, default=\"outputs\", help=\"dir to write results to / 生成画像の出力先\")\n    parser.add_argument(\n        \"--sequential_file_name\", action=\"store_true\", help=\"sequential output file name / 生成画像のファイル名を連番にする\"\n    )\n    parser.add_argument(\n        \"--use_original_file_name\",\n        action=\"store_true\",\n        help=\"prepend original file name in img2img / img2imgで元画像のファイル名を生成画像のファイル名の先頭に付ける\",\n    )\n    # parser.add_argument(\"--ddim_eta\", type=float, default=0.0, help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\", )\n    parser.add_argument(\"--n_iter\", type=int, default=1, help=\"sample this often / 繰り返し回数\")\n    parser.add_argument(\"--H\", type=int, default=None, help=\"image height, in pixel space / 生成画像高さ\")\n    parser.add_argument(\"--W\", type=int, default=None, help=\"image width, in pixel space / 生成画像幅\")\n    parser.add_argument(\n        \"--original_height\",\n        type=int,\n        default=None,\n        help=\"original height for SDXL conditioning / SDXLの条件付けに用いるoriginal heightの値\",\n    )\n    parser.add_argument(\n        \"--original_width\",\n        type=int,\n        default=None,\n        help=\"original width for SDXL conditioning / SDXLの条件付けに用いるoriginal widthの値\",\n    )\n    parser.add_argument(\n        \"--original_height_negative\",\n        type=int,\n        default=None,\n        help=\"original height for SDXL unconditioning / SDXLのネガティブ条件付けに用いるoriginal heightの値\",\n    )\n    parser.add_argument(\n        \"--original_width_negative\",\n        type=int,\n        default=None,\n        help=\"original width for SDXL unconditioning / SDXLのネガティブ条件付けに用いるoriginal widthの値\",\n    )\n    parser.add_argument(\n        \"--crop_top\", type=int, default=None, help=\"crop top for SDXL conditioning / SDXLの条件付けに用いるcrop topの値\"\n    )\n    parser.add_argument(\n        \"--crop_left\", type=int, default=None, help=\"crop left for SDXL conditioning / SDXLの条件付けに用いるcrop leftの値\"\n    )\n    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"batch size / バッチサイズ\")\n    parser.add_argument(\n        \"--vae_batch_size\",\n        type=float,\n        default=None,\n        help=\"batch size for VAE, < 1.0 for ratio / VAE処理時のバッチサイズ、1未満の値の場合は通常バッチサイズの比率\",\n    )\n    parser.add_argument(\n        \"--vae_slices\",\n        type=int,\n        default=None,\n        help=\"number of slices to split image into for VAE to reduce VRAM usage, None for no splitting (default), slower if specified. 16 or 32 recommended / VAE処理時にVRAM使用量削減のため画像を分割するスライス数、Noneの場合は分割しない（デフォルト）、指定すると遅くなる。16か32程度を推奨\",\n    )\n    parser.add_argument(\n        \"--no_half_vae\", action=\"store_true\", help=\"do not use fp16/bf16 precision for VAE / VAE処理時にfp16/bf16を使わない\"\n    )\n    parser.add_argument(\"--steps\", type=int, default=50, help=\"number of ddim sampling steps / サンプリングステップ数\")\n    parser.add_argument(\n        \"--sampler\",\n        type=str,\n        default=\"ddim\",\n        choices=[\n            \"ddim\",\n            \"pndm\",\n            \"lms\",\n            \"euler\",\n            \"euler_a\",\n            \"heun\",\n            \"dpm_2\",\n            \"dpm_2_a\",\n            \"dpmsolver\",\n            \"dpmsolver++\",\n            \"dpmsingle\",\n            \"k_lms\",\n            \"k_euler\",\n            \"k_euler_a\",\n            \"k_dpm_2\",\n            \"k_dpm_2_a\",\n        ],\n        help=f\"sampler (scheduler) type / サンプラー（スケジューラ）の種類\",\n    )\n    parser.add_argument(\n        \"--scale\",\n        type=float,\n        default=7.5,\n        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty)) / guidance scale\",\n    )\n    parser.add_argument(\n        \"--ckpt\", type=str, default=None, help=\"path to checkpoint of model / モデルのcheckpointファイルまたはディレクトリ\"\n    )\n    parser.add_argument(\n        \"--vae\",\n        type=str,\n        default=None,\n        help=\"path to checkpoint of vae to replace / VAEを入れ替える場合、VAEのcheckpointファイルまたはディレクトリ\",\n    )\n    parser.add_argument(\n        \"--tokenizer_cache_dir\",\n        type=str,\n        default=None,\n        help=\"directory for caching Tokenizer (for offline training) / Tokenizerをキャッシュするディレクトリ（ネット接続なしでの学習のため）\",\n    )\n    # parser.add_argument(\"--replace_clip_l14_336\", action='store_true',\n    #                     help=\"Replace CLIP (Text Encoder) to l/14@336 / CLIP(Text Encoder)をl/14@336に入れ替える\")\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=None,\n        help=\"seed, or seed of seeds in multiple generation / 1枚生成時のseed、または複数枚生成時の乱数seedを決めるためのseed\",\n    )\n    parser.add_argument(\n        \"--iter_same_seed\",\n        action=\"store_true\",\n        help=\"use same seed for all prompts in iteration if no seed specified / 乱数seedの指定がないとき繰り返し内はすべて同じseedを使う（プロンプト間の差異の比較用）\",\n    )\n    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"use fp16 / fp16を指定し省メモリ化する\")\n    parser.add_argument(\"--bf16\", action=\"store_true\", help=\"use bfloat16 / bfloat16を指定し省メモリ化する\")\n    parser.add_argument(\"--xformers\", action=\"store_true\", help=\"use xformers / xformersを使用し高速化する\")\n    parser.add_argument(\"--sdpa\", action=\"store_true\", help=\"use sdpa in PyTorch 2 / sdpa\")\n    parser.add_argument(\n        \"--diffusers_xformers\",\n        action=\"store_true\",\n        help=\"use xformers by diffusers (Hypernetworks doesn't work) / Diffusersでxformersを使用する（Hypernetwork利用不可）\",\n    )\n    parser.add_argument(\n        \"--opt_channels_last\",\n        action=\"store_true\",\n        help=\"set channels last option to model / モデルにchannels lastを指定し最適化する\",\n    )\n    parser.add_argument(\n        \"--network_module\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"additional network module to use / 追加ネットワークを使う時そのモジュール名\",\n    )\n    parser.add_argument(\n        \"--network_weights\", type=str, default=None, nargs=\"*\", help=\"additional network weights to load / 追加ネットワークの重み\"\n    )\n    parser.add_argument(\n        \"--network_mul\", type=float, default=None, nargs=\"*\", help=\"additional network multiplier / 追加ネットワークの効果の倍率\"\n    )\n    parser.add_argument(\n        \"--network_args\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"additional arguments for network (key=value) / ネットワークへの追加の引数\",\n    )\n    parser.add_argument(\n        \"--network_show_meta\", action=\"store_true\", help=\"show metadata of network model / ネットワークモデルのメタデータを表示する\"\n    )\n    parser.add_argument(\n        \"--network_merge_n_models\",\n        type=int,\n        default=None,\n        help=\"merge this number of networks / この数だけネットワークをマージする\",\n    )\n    parser.add_argument(\n        \"--network_merge\", action=\"store_true\", help=\"merge network weights to original model / ネットワークの重みをマージする\"\n    )\n    parser.add_argument(\n        \"--network_pre_calc\",\n        action=\"store_true\",\n        help=\"pre-calculate network for generation / ネットワークのあらかじめ計算して生成する\",\n    )\n    parser.add_argument(\n        \"--network_regional_mask_max_color_codes\",\n        type=int,\n        default=None,\n        help=\"max color codes for regional mask (default is None, mask by channel) / regional maskの最大色数（デフォルトはNoneでチャンネルごとのマスク）\",\n    )\n    parser.add_argument(\n        \"--textual_inversion_embeddings\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"Embeddings files of Textual Inversion / Textual Inversionのembeddings\",\n    )\n    parser.add_argument(\n        \"--clip_skip\", type=int, default=None, help=\"layer number from bottom to use in CLIP / CLIPの後ろからn層目の出力を使う\"\n    )\n    parser.add_argument(\n        \"--max_embeddings_multiples\",\n        type=int,\n        default=None,\n        help=\"max embedding multiples, max token length is 75 * multiples / トークン長をデフォルトの何倍とするか 75*この値 がトークン長となる\",\n    )\n    parser.add_argument(\n        \"--guide_image_path\", type=str, default=None, nargs=\"*\", help=\"image to CLIP guidance / CLIP guided SDでガイドに使う画像\"\n    )\n    parser.add_argument(\n        \"--highres_fix_scale\",\n        type=float,\n        default=None,\n        help=\"enable highres fix, reso scale for 1st stage / highres fixを有効にして最初の解像度をこのscaleにする\",\n    )\n    parser.add_argument(\n        \"--highres_fix_steps\",\n        type=int,\n        default=28,\n        help=\"1st stage steps for highres fix / highres fixの最初のステージのステップ数\",\n    )\n    parser.add_argument(\n        \"--highres_fix_strength\",\n        type=float,\n        default=None,\n        help=\"1st stage img2img strength for highres fix / highres fixの最初のステージのimg2img時のstrength、省略時はstrengthと同じ\",\n    )\n    parser.add_argument(\n        \"--highres_fix_save_1st\",\n        action=\"store_true\",\n        help=\"save 1st stage images for highres fix / highres fixの最初のステージの画像を保存する\",\n    )\n    parser.add_argument(\n        \"--highres_fix_latents_upscaling\",\n        action=\"store_true\",\n        help=\"use latents upscaling for highres fix / highres fixでlatentで拡大する\",\n    )\n    parser.add_argument(\n        \"--highres_fix_upscaler\",\n        type=str,\n        default=None,\n        help=\"upscaler module for highres fix / highres fixで使うupscalerのモジュール名\",\n    )\n    parser.add_argument(\n        \"--highres_fix_upscaler_args\",\n        type=str,\n        default=None,\n        help=\"additional arguments for upscaler (key=value) / upscalerへの追加の引数\",\n    )\n    parser.add_argument(\n        \"--highres_fix_disable_control_net\",\n        action=\"store_true\",\n        help=\"disable ControlNet for highres fix / highres fixでControlNetを使わない\",\n    )\n\n    parser.add_argument(\n        \"--negative_scale\",\n        type=float,\n        default=None,\n        help=\"set another guidance scale for negative prompt / ネガティブプロンプトのscaleを指定する\",\n    )\n\n    parser.add_argument(\n        \"--control_net_lllite_models\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"ControlNet models to use / 使用するControlNetのモデル名\",\n    )\n    # parser.add_argument(\n    #     \"--control_net_models\", type=str, default=None, nargs=\"*\", help=\"ControlNet models to use / 使用するControlNetのモデル名\"\n    # )\n    # parser.add_argument(\n    #     \"--control_net_preps\", type=str, default=None, nargs=\"*\", help=\"ControlNet preprocess to use / 使用するControlNetのプリプロセス名\"\n    # )\n    parser.add_argument(\n        \"--control_net_multipliers\", type=float, default=None, nargs=\"*\", help=\"ControlNet multiplier / ControlNetの適用率\"\n    )\n    parser.add_argument(\n        \"--control_net_ratios\",\n        type=float,\n        default=None,\n        nargs=\"*\",\n        help=\"ControlNet guidance ratio for steps / ControlNetでガイドするステップ比率\",\n    )\n    parser.add_argument(\n        \"--clip_vision_strength\",\n        type=float,\n        default=None,\n        help=\"enable CLIP Vision Conditioning for img2img with this strength / img2imgでCLIP Vision Conditioningを有効にしてこのstrengthで処理する\",\n    )\n\n    # Deep Shrink\n    parser.add_argument(\n        \"--ds_depth_1\",\n        type=int,\n        default=None,\n        help=\"Enable Deep Shrink with this depth 1, valid values are 0 to 8 / Deep Shrinkをこのdepthで有効にする\",\n    )\n    parser.add_argument(\n        \"--ds_timesteps_1\",\n        type=int,\n        default=650,\n        help=\"Apply Deep Shrink depth 1 until this timesteps / Deep Shrink depth 1を適用するtimesteps\",\n    )\n    parser.add_argument(\"--ds_depth_2\", type=int, default=None, help=\"Deep Shrink depth 2 / Deep Shrinkのdepth 2\")\n    parser.add_argument(\n        \"--ds_timesteps_2\",\n        type=int,\n        default=650,\n        help=\"Apply Deep Shrink depth 2 until this timesteps / Deep Shrink depth 2を適用するtimesteps\",\n    )\n    parser.add_argument(\n        \"--ds_ratio\", type=float, default=0.5, help=\"Deep Shrink ratio for downsampling / Deep Shrinkのdownsampling比率\"\n    )\n\n    # gradual latent\n    parser.add_argument(\n        \"--gradual_latent_timesteps\",\n        type=int,\n        default=None,\n        help=\"enable Gradual Latent hires fix and apply upscaling from this timesteps / Gradual Latent hires fixをこのtimestepsで有効にし、このtimestepsからアップスケーリングを適用する\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_ratio\",\n        type=float,\n        default=0.5,\n        help=\" this size ratio, 0.5 means 1/2 / Gradual Latent hires fixをこのサイズ比率で有効にする、0.5は1/2を意味する\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_ratio_step\",\n        type=float,\n        default=0.125,\n        help=\"step to increase ratio for Gradual Latent / Gradual Latentのratioをどのくらいずつ上げるか\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_every_n_steps\",\n        type=int,\n        default=3,\n        help=\"steps to increase size of latents every this steps for Gradual Latent / Gradual Latentでlatentsのサイズをこのステップごとに上げる\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_s_noise\",\n        type=float,\n        default=1.0,\n        help=\"s_noise for Gradual Latent / Gradual Latentのs_noise\",\n    )\n    parser.add_argument(\n        \"--gradual_latent_unsharp_params\",\n        type=str,\n        default=None,\n        help=\"unsharp mask parameters for Gradual Latent: ksize, sigma, strength, target-x (1 means True). `3,0.5,0.5,1` or `3,1.0,1.0,0` is recommended /\"\n        + \" Gradual Latentのunsharp maskのパラメータ: ksize, sigma, strength, target-x. `3,0.5,0.5,1` または `3,1.0,1.0,0` が推奨\",\n    )\n\n    # # parser.add_argument(\n    #     \"--control_net_image_path\", type=str, default=None, nargs=\"*\", help=\"image for ControlNet guidance / ControlNetでガイドに使う画像\"\n    # )\n\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    setup_logging(args, reset=True)\n    main(args)\n"
        },
        {
          "name": "sdxl_minimal_inference.py",
          "type": "blob",
          "size": 13.537109375,
          "content": "# 手元で推論を行うための最低限のコード。HuggingFace／DiffusersのCLIP、schedulerとVAEを使う\n# Minimal code for performing inference at local. Use HuggingFace/Diffusers CLIP, scheduler and VAE\n\nimport argparse\nimport datetime\nimport math\nimport os\nimport random\nfrom einops import repeat\nimport numpy as np\n\nimport torch\nfrom library.device_utils import init_ipex, get_preferred_device\n\ninit_ipex()\n\nfrom tqdm import tqdm\nfrom transformers import CLIPTokenizer\nfrom diffusers import EulerDiscreteScheduler\nfrom PIL import Image\n\n# import open_clip\nfrom safetensors.torch import load_file\n\nfrom library import model_util, sdxl_model_util\nimport networks.lora as lora\nfrom library.utils import setup_logging\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# scheduler: このあたりの設定はSD1/2と同じでいいらしい\n# scheduler: The settings around here seem to be the same as SD1/2\nSCHEDULER_LINEAR_START = 0.00085\nSCHEDULER_LINEAR_END = 0.0120\nSCHEDULER_TIMESTEPS = 1000\nSCHEDLER_SCHEDULE = \"scaled_linear\"\n\n\n# Time EmbeddingはDiffusersからのコピー\n# Time Embedding is copied from Diffusers\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    if not repeat_only:\n        half = dim // 2\n        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n            device=timesteps.device\n        )\n        args = timesteps[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    else:\n        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n    return embedding\n\n\ndef get_timestep_embedding(x, outdim):\n    assert len(x.shape) == 2\n    b, dims = x.shape[0], x.shape[1]\n    # x = rearrange(x, \"b d -> (b d)\")\n    x = torch.flatten(x)\n    emb = timestep_embedding(x, outdim)\n    # emb = rearrange(emb, \"(b d) d2 -> b (d d2)\", b=b, d=dims, d2=outdim)\n    emb = torch.reshape(emb, (b, dims * outdim))\n    return emb\n\n\nif __name__ == \"__main__\":\n    # 画像生成条件を変更する場合はここを変更 / change here to change image generation conditions\n\n    # SDXLの追加のvector embeddingへ渡す値 / Values to pass to additional vector embedding of SDXL\n    target_height = 1024\n    target_width = 1024\n    original_height = target_height\n    original_width = target_width\n    crop_top = 0\n    crop_left = 0\n\n    steps = 50\n    guidance_scale = 7\n    seed = None  # 1\n\n    DEVICE = get_preferred_device()\n    DTYPE = torch.float16  # bfloat16 may work\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--ckpt_path\", type=str, required=True)\n    parser.add_argument(\"--prompt\", type=str, default=\"A photo of a cat\")\n    parser.add_argument(\"--prompt2\", type=str, default=None)\n    parser.add_argument(\"--negative_prompt\", type=str, default=\"\")\n    parser.add_argument(\"--output_dir\", type=str, default=\".\")\n    parser.add_argument(\n        \"--lora_weights\",\n        type=str,\n        nargs=\"*\",\n        default=[],\n        help=\"LoRA weights, only supports networks.lora, each argument is a `path;multiplier` (semi-colon separated)\",\n    )\n    parser.add_argument(\"--interactive\", action=\"store_true\")\n    args = parser.parse_args()\n\n    if args.prompt2 is None:\n        args.prompt2 = args.prompt\n\n    # HuggingFaceのmodel id\n    text_encoder_1_name = \"openai/clip-vit-large-patch14\"\n    text_encoder_2_name = \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"\n\n    # checkpointを読み込む。モデル変換についてはそちらの関数を参照\n    # Load checkpoint. For model conversion, see this function\n\n    # 本体RAMが少ない場合はGPUにロードするといいかも\n    # If the main RAM is small, it may be better to load it on the GPU\n    text_model1, text_model2, vae, unet, _, _ = sdxl_model_util.load_models_from_sdxl_checkpoint(\n        sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, args.ckpt_path, \"cpu\"\n    )\n\n    # Text Encoder 1はSDXL本体でもHuggingFaceのものを使っている\n    # In SDXL, Text Encoder 1 is also using HuggingFace's\n\n    # Text Encoder 2はSDXL本体ではopen_clipを使っている\n    # それを使ってもいいが、SD2のDiffusers版に合わせる形で、HuggingFaceのものを使う\n    # 重みの変換コードはSD2とほぼ同じ\n    # In SDXL, Text Encoder 2 is using open_clip\n    # It's okay to use it, but to match the Diffusers version of SD2, use HuggingFace's\n    # The weight conversion code is almost the same as SD2\n\n    # VAEの構造はSDXLもSD1/2と同じだが、重みは異なるようだ。何より謎のscale値が違う\n    # fp16でNaNが出やすいようだ\n    # The structure of VAE is the same as SD1/2, but the weights seem to be different. Above all, the mysterious scale value is different.\n    # NaN seems to be more likely to occur in fp16\n\n    unet.to(DEVICE, dtype=DTYPE)\n    unet.eval()\n\n    vae_dtype = DTYPE\n    if DTYPE == torch.float16:\n        logger.info(\"use float32 for vae\")\n        vae_dtype = torch.float32\n    vae.to(DEVICE, dtype=vae_dtype)\n    vae.eval()\n\n    text_model1.to(DEVICE, dtype=DTYPE)\n    text_model1.eval()\n    text_model2.to(DEVICE, dtype=DTYPE)\n    text_model2.eval()\n\n    unet.set_use_memory_efficient_attention(True, False)\n    if torch.__version__ >= \"2.0.0\":  # PyTorch 2.0.0 以上対応のxformersなら以下が使える\n        vae.set_use_memory_efficient_attention_xformers(True)\n\n    # Tokenizers\n    tokenizer1 = CLIPTokenizer.from_pretrained(text_encoder_1_name)\n    # tokenizer2 = lambda x: open_clip.tokenize(x, context_length=77)\n    tokenizer2 = CLIPTokenizer.from_pretrained(text_encoder_2_name)\n\n    # LoRA\n    for weights_file in args.lora_weights:\n        if \";\" in weights_file:\n            weights_file, multiplier = weights_file.split(\";\")\n            multiplier = float(multiplier)\n        else:\n            multiplier = 1.0\n\n        lora_model, weights_sd = lora.create_network_from_weights(\n            multiplier, weights_file, vae, [text_model1, text_model2], unet, None, True\n        )\n        lora_model.merge_to([text_model1, text_model2], unet, weights_sd, DTYPE, DEVICE)\n\n    # scheduler\n    scheduler = EulerDiscreteScheduler(\n        num_train_timesteps=SCHEDULER_TIMESTEPS,\n        beta_start=SCHEDULER_LINEAR_START,\n        beta_end=SCHEDULER_LINEAR_END,\n        beta_schedule=SCHEDLER_SCHEDULE,\n    )\n\n    def generate_image(prompt, prompt2, negative_prompt, seed=None):\n        # 将来的にサイズ情報も変えられるようにする / Make it possible to change the size information in the future\n        # prepare embedding\n        with torch.no_grad():\n            # vector\n            emb1 = get_timestep_embedding(torch.FloatTensor([original_height, original_width]).unsqueeze(0), 256)\n            emb2 = get_timestep_embedding(torch.FloatTensor([crop_top, crop_left]).unsqueeze(0), 256)\n            emb3 = get_timestep_embedding(torch.FloatTensor([target_height, target_width]).unsqueeze(0), 256)\n            # logger.info(\"emb1\", emb1.shape)\n            c_vector = torch.cat([emb1, emb2, emb3], dim=1).to(DEVICE, dtype=DTYPE)\n            uc_vector = c_vector.clone().to(\n                DEVICE, dtype=DTYPE\n            )  # ちょっとここ正しいかどうかわからない I'm not sure if this is right\n\n            # crossattn\n\n        # Text Encoderを二つ呼ぶ関数  Function to call two Text Encoders\n        def call_text_encoder(text, text2):\n            # text encoder 1\n            batch_encoding = tokenizer1(\n                text,\n                truncation=True,\n                return_length=True,\n                return_overflowing_tokens=False,\n                padding=\"max_length\",\n                return_tensors=\"pt\",\n            )\n            tokens = batch_encoding[\"input_ids\"].to(DEVICE)\n\n            with torch.no_grad():\n                enc_out = text_model1(tokens, output_hidden_states=True, return_dict=True)\n                text_embedding1 = enc_out[\"hidden_states\"][11]\n                # text_embedding = pipe.text_encoder.text_model.final_layer_norm(text_embedding)    # layer normは通さないらしい\n\n            # text encoder 2\n            # tokens = tokenizer2(text2).to(DEVICE)\n            tokens = tokenizer2(\n                text,\n                truncation=True,\n                return_length=True,\n                return_overflowing_tokens=False,\n                padding=\"max_length\",\n                return_tensors=\"pt\",\n            )\n            tokens = batch_encoding[\"input_ids\"].to(DEVICE)\n\n            with torch.no_grad():\n                enc_out = text_model2(tokens, output_hidden_states=True, return_dict=True)\n                text_embedding2_penu = enc_out[\"hidden_states\"][-2]\n                # logger.info(\"hidden_states2\", text_embedding2_penu.shape)\n                text_embedding2_pool = enc_out[\"text_embeds\"]  # do not support Textual Inversion\n\n            # 連結して終了 concat and finish\n            text_embedding = torch.cat([text_embedding1, text_embedding2_penu], dim=2)\n            return text_embedding, text_embedding2_pool\n\n        # cond\n        c_ctx, c_ctx_pool = call_text_encoder(prompt, prompt2)\n        # logger.info(c_ctx.shape, c_ctx_p.shape, c_vector.shape)\n        c_vector = torch.cat([c_ctx_pool, c_vector], dim=1)\n\n        # uncond\n        uc_ctx, uc_ctx_pool = call_text_encoder(negative_prompt, negative_prompt)\n        uc_vector = torch.cat([uc_ctx_pool, uc_vector], dim=1)\n\n        text_embeddings = torch.cat([uc_ctx, c_ctx])\n        vector_embeddings = torch.cat([uc_vector, c_vector])\n\n        # メモリ使用量を減らすにはここでText Encoderを削除するかCPUへ移動する\n\n        if seed is not None:\n            random.seed(seed)\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n\n            # # random generator for initial noise\n            # generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n            generator = None\n        else:\n            generator = None\n\n        # get the initial random noise unless the user supplied it\n        # SDXLはCPUでlatentsを作成しているので一応合わせておく、Diffusersはtarget deviceでlatentsを作成している\n        # SDXL creates latents in CPU, Diffusers creates latents in target device\n        latents_shape = (1, 4, target_height // 8, target_width // 8)\n        latents = torch.randn(\n            latents_shape,\n            generator=generator,\n            device=\"cpu\",\n            dtype=torch.float32,\n        ).to(DEVICE, dtype=DTYPE)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * scheduler.init_noise_sigma\n\n        # set timesteps\n        scheduler.set_timesteps(steps, DEVICE)\n\n        # このへんはDiffusersからのコピペ\n        # Copy from Diffusers\n        timesteps = scheduler.timesteps.to(DEVICE)  # .to(DTYPE)\n        num_latent_input = 2\n        with torch.no_grad():\n            for i, t in enumerate(tqdm(timesteps)):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = latents.repeat((num_latent_input, 1, 1, 1))\n                latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n                noise_pred = unet(latent_model_input, t, text_embeddings, vector_embeddings)\n\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(num_latent_input)  # uncond by negative prompt\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n                # compute the previous noisy sample x_t -> x_t-1\n                # latents = scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n                latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n            # latents = 1 / 0.18215 * latents\n            latents = 1 / sdxl_model_util.VAE_SCALE_FACTOR * latents\n            latents = latents.to(vae_dtype)\n            image = vae.decode(latents).sample\n            image = (image / 2 + 0.5).clamp(0, 1)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n\n        # image = self.numpy_to_pil(image)\n        image = (image * 255).round().astype(\"uint8\")\n        image = [Image.fromarray(im) for im in image]\n\n        # 保存して終了 save and finish\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        for i, img in enumerate(image):\n            img.save(os.path.join(args.output_dir, f\"image_{timestamp}_{i:03d}.png\"))\n\n    if not args.interactive:\n        generate_image(args.prompt, args.prompt2, args.negative_prompt, seed)\n    else:\n        # loop for interactive\n        while True:\n            prompt = input(\"prompt: \")\n            if prompt == \"\":\n                break\n            prompt2 = input(\"prompt2: \")\n            if prompt2 == \"\":\n                prompt2 = prompt\n            negative_prompt = input(\"negative prompt: \")\n            seed = input(\"seed: \")\n            if seed == \"\":\n                seed = None\n            else:\n                seed = int(seed)\n            generate_image(prompt, prompt2, negative_prompt, seed)\n\n    logger.info(\"Done!\")\n"
        },
        {
          "name": "sdxl_train.py",
          "type": "blob",
          "size": 35.90625,
          "content": "# training with captions\n\nimport argparse\nimport math\nimport os\nfrom multiprocessing import Value\nfrom typing import List\nimport toml\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library.device_utils import init_ipex, clean_memory_on_device\n\n\ninit_ipex()\n\nfrom accelerate.utils import set_seed\nfrom diffusers import DDPMScheduler\nfrom library import deepspeed_utils, sdxl_model_util\n\nimport library.train_util as train_util\n\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nimport library.config_util as config_util\nimport library.sdxl_train_util as sdxl_train_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    apply_snr_weight,\n    prepare_scheduler_for_custom_training,\n    scale_v_prediction_loss_like_noise_prediction,\n    add_v_prediction_like_loss,\n    apply_debiased_estimation,\n    apply_masked_loss,\n)\nfrom library.sdxl_original_unet import SdxlUNet2DConditionModel\n\n\nUNET_NUM_BLOCKS_FOR_BLOCK_LR = 23\n\n\ndef get_block_params_to_optimize(unet: SdxlUNet2DConditionModel, block_lrs: List[float]) -> List[dict]:\n    block_params = [[] for _ in range(len(block_lrs))]\n\n    for i, (name, param) in enumerate(unet.named_parameters()):\n        if name.startswith(\"time_embed.\") or name.startswith(\"label_emb.\"):\n            block_index = 0  # 0\n        elif name.startswith(\"input_blocks.\"):  # 1-9\n            block_index = 1 + int(name.split(\".\")[1])\n        elif name.startswith(\"middle_block.\"):  # 10-12\n            block_index = 10 + int(name.split(\".\")[1])\n        elif name.startswith(\"output_blocks.\"):  # 13-21\n            block_index = 13 + int(name.split(\".\")[1])\n        elif name.startswith(\"out.\"):  # 22\n            block_index = 22\n        else:\n            raise ValueError(f\"unexpected parameter name: {name}\")\n\n        block_params[block_index].append(param)\n\n    params_to_optimize = []\n    for i, params in enumerate(block_params):\n        if block_lrs[i] == 0:  # 0のときは学習しない do not optimize when lr is 0\n            continue\n        params_to_optimize.append({\"params\": params, \"lr\": block_lrs[i]})\n\n    return params_to_optimize\n\n\ndef append_block_lr_to_logs(block_lrs, logs, lr_scheduler, optimizer_type):\n    names = []\n    block_index = 0\n    while block_index < UNET_NUM_BLOCKS_FOR_BLOCK_LR + 2:\n        if block_index < UNET_NUM_BLOCKS_FOR_BLOCK_LR:\n            if block_lrs[block_index] == 0:\n                block_index += 1\n                continue\n            names.append(f\"block{block_index}\")\n        elif block_index == UNET_NUM_BLOCKS_FOR_BLOCK_LR:\n            names.append(\"text_encoder1\")\n        elif block_index == UNET_NUM_BLOCKS_FOR_BLOCK_LR + 1:\n            names.append(\"text_encoder2\")\n\n        block_index += 1\n\n    train_util.append_lr_to_logs_with_names(logs, lr_scheduler, optimizer_type, names)\n\n\ndef train(args):\n    train_util.verify_training_args(args)\n    train_util.prepare_dataset_args(args, True)\n    sdxl_train_util.verify_sdxl_training_args(args)\n    deepspeed_utils.prepare_deepspeed_args(args)\n    setup_logging(args, reset=True)\n\n    assert (\n        not args.weighted_captions\n    ), \"weighted_captions is not supported currently / weighted_captionsは現在サポートされていません\"\n    assert (\n        not args.train_text_encoder or not args.cache_text_encoder_outputs\n    ), \"cache_text_encoder_outputs is not supported when training text encoder / text encoderを学習するときはcache_text_encoder_outputsはサポートされていません\"\n\n    if args.block_lr:\n        block_lrs = [float(lr) for lr in args.block_lr.split(\",\")]\n        assert (\n            len(block_lrs) == UNET_NUM_BLOCKS_FOR_BLOCK_LR\n        ), f\"block_lr must have {UNET_NUM_BLOCKS_FOR_BLOCK_LR} values / block_lrは{UNET_NUM_BLOCKS_FOR_BLOCK_LR}個の値を指定してください\"\n    else:\n        block_lrs = None\n\n    cache_latents = args.cache_latents\n    use_dreambooth_method = args.in_json is None\n\n    if args.seed is not None:\n        set_seed(args.seed)  # 乱数系列を初期化する\n\n    tokenizer1, tokenizer2 = sdxl_train_util.load_tokenizers(args)\n\n    # データセットを準備する\n    if args.dataset_class is None:\n        blueprint_generator = BlueprintGenerator(ConfigSanitizer(True, True, args.masked_loss, True))\n        if args.dataset_config is not None:\n            logger.info(f\"Load dataset config from {args.dataset_config}\")\n            user_config = config_util.load_user_config(args.dataset_config)\n            ignored = [\"train_data_dir\", \"in_json\"]\n            if any(getattr(args, attr) is not None for attr in ignored):\n                logger.warning(\n                    \"ignore following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                        \", \".join(ignored)\n                    )\n                )\n        else:\n            if use_dreambooth_method:\n                logger.info(\"Using DreamBooth method.\")\n                user_config = {\n                    \"datasets\": [\n                        {\n                            \"subsets\": config_util.generate_dreambooth_subsets_config_by_subdirs(\n                                args.train_data_dir, args.reg_data_dir\n                            )\n                        }\n                    ]\n                }\n            else:\n                logger.info(\"Training with captions.\")\n                user_config = {\n                    \"datasets\": [\n                        {\n                            \"subsets\": [\n                                {\n                                    \"image_dir\": args.train_data_dir,\n                                    \"metadata_file\": args.in_json,\n                                }\n                            ]\n                        }\n                    ]\n                }\n\n        blueprint = blueprint_generator.generate(user_config, args, tokenizer=[tokenizer1, tokenizer2])\n        train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n    else:\n        train_dataset_group = train_util.load_arbitrary_dataset(args, [tokenizer1, tokenizer2])\n\n    current_epoch = Value(\"i\", 0)\n    current_step = Value(\"i\", 0)\n    ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n    collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n    train_dataset_group.verify_bucket_reso_steps(32)\n\n    if args.debug_dataset:\n        train_util.debug_dataset(train_dataset_group, True)\n        return\n    if len(train_dataset_group) == 0:\n        logger.error(\n            \"No data found. Please verify the metadata file and train_data_dir option. / 画像がありません。メタデータおよびtrain_data_dirオプションを確認してください。\"\n        )\n        return\n\n    if cache_latents:\n        assert (\n            train_dataset_group.is_latent_cacheable()\n        ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n\n    if args.cache_text_encoder_outputs:\n        assert (\n            train_dataset_group.is_text_encoder_output_cacheable()\n        ), \"when caching text encoder output, either caption_dropout_rate, shuffle_caption, token_warmup_step or caption_tag_dropout_rate cannot be used / text encoderの出力をキャッシュするときはcaption_dropout_rate, shuffle_caption, token_warmup_step, caption_tag_dropout_rateは使えません\"\n\n    # acceleratorを準備する\n    logger.info(\"prepare accelerator\")\n    accelerator = train_util.prepare_accelerator(args)\n\n    # mixed precisionに対応した型を用意しておき適宜castする\n    weight_dtype, save_dtype = train_util.prepare_dtype(args)\n    vae_dtype = torch.float32 if args.no_half_vae else weight_dtype\n\n    # モデルを読み込む\n    (\n        load_stable_diffusion_format,\n        text_encoder1,\n        text_encoder2,\n        vae,\n        unet,\n        logit_scale,\n        ckpt_info,\n    ) = sdxl_train_util.load_target_model(args, accelerator, \"sdxl\", weight_dtype)\n    # logit_scale = logit_scale.to(accelerator.device, dtype=weight_dtype)\n\n    # verify load/save model formats\n    if load_stable_diffusion_format:\n        src_stable_diffusion_ckpt = args.pretrained_model_name_or_path\n        src_diffusers_model_path = None\n    else:\n        src_stable_diffusion_ckpt = None\n        src_diffusers_model_path = args.pretrained_model_name_or_path\n\n    if args.save_model_as is None:\n        save_stable_diffusion_format = load_stable_diffusion_format\n        use_safetensors = args.use_safetensors\n    else:\n        save_stable_diffusion_format = args.save_model_as.lower() == \"ckpt\" or args.save_model_as.lower() == \"safetensors\"\n        use_safetensors = args.use_safetensors or (\"safetensors\" in args.save_model_as.lower())\n        # assert save_stable_diffusion_format, \"save_model_as must be ckpt or safetensors / save_model_asはckptかsafetensorsである必要があります\"\n\n    # Diffusers版のxformers使用フラグを設定する関数\n    def set_diffusers_xformers_flag(model, valid):\n        def fn_recursive_set_mem_eff(module: torch.nn.Module):\n            if hasattr(module, \"set_use_memory_efficient_attention_xformers\"):\n                module.set_use_memory_efficient_attention_xformers(valid)\n\n            for child in module.children():\n                fn_recursive_set_mem_eff(child)\n\n        fn_recursive_set_mem_eff(model)\n\n    # モデルに xformers とか memory efficient attention を組み込む\n    if args.diffusers_xformers:\n        # もうU-Netを独自にしたので動かないけどVAEのxformersは動くはず\n        accelerator.print(\"Use xformers by Diffusers\")\n        # set_diffusers_xformers_flag(unet, True)\n        set_diffusers_xformers_flag(vae, True)\n    else:\n        # Windows版のxformersはfloatで学習できなかったりするのでxformersを使わない設定も可能にしておく必要がある\n        accelerator.print(\"Disable Diffusers' xformers\")\n        train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n        if torch.__version__ >= \"2.0.0\":  # PyTorch 2.0.0 以上対応のxformersなら以下が使える\n            vae.set_use_memory_efficient_attention_xformers(args.xformers)\n\n    # 学習を準備する\n    if cache_latents:\n        vae.to(accelerator.device, dtype=vae_dtype)\n        vae.requires_grad_(False)\n        vae.eval()\n        with torch.no_grad():\n            train_dataset_group.cache_latents(vae, args.vae_batch_size, args.cache_latents_to_disk, accelerator.is_main_process)\n        vae.to(\"cpu\")\n        clean_memory_on_device(accelerator.device)\n\n        accelerator.wait_for_everyone()\n\n    # 学習を準備する：モデルを適切な状態にする\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n    train_unet = args.learning_rate > 0\n    train_text_encoder1 = False\n    train_text_encoder2 = False\n\n    if args.train_text_encoder:\n        # TODO each option for two text encoders?\n        accelerator.print(\"enable text encoder training\")\n        if args.gradient_checkpointing:\n            text_encoder1.gradient_checkpointing_enable()\n            text_encoder2.gradient_checkpointing_enable()\n        lr_te1 = args.learning_rate_te1 if args.learning_rate_te1 is not None else args.learning_rate  # 0 means not train\n        lr_te2 = args.learning_rate_te2 if args.learning_rate_te2 is not None else args.learning_rate  # 0 means not train\n        train_text_encoder1 = lr_te1 > 0\n        train_text_encoder2 = lr_te2 > 0\n\n        # caching one text encoder output is not supported\n        if not train_text_encoder1:\n            text_encoder1.to(weight_dtype)\n        if not train_text_encoder2:\n            text_encoder2.to(weight_dtype)\n        text_encoder1.requires_grad_(train_text_encoder1)\n        text_encoder2.requires_grad_(train_text_encoder2)\n        text_encoder1.train(train_text_encoder1)\n        text_encoder2.train(train_text_encoder2)\n    else:\n        text_encoder1.to(weight_dtype)\n        text_encoder2.to(weight_dtype)\n        text_encoder1.requires_grad_(False)\n        text_encoder2.requires_grad_(False)\n        text_encoder1.eval()\n        text_encoder2.eval()\n\n        # TextEncoderの出力をキャッシュする\n        if args.cache_text_encoder_outputs:\n            # Text Encodes are eval and no grad\n            with torch.no_grad(), accelerator.autocast():\n                train_dataset_group.cache_text_encoder_outputs(\n                    (tokenizer1, tokenizer2),\n                    (text_encoder1, text_encoder2),\n                    accelerator.device,\n                    None,\n                    args.cache_text_encoder_outputs_to_disk,\n                    accelerator.is_main_process,\n                )\n            accelerator.wait_for_everyone()\n\n    if not cache_latents:\n        vae.requires_grad_(False)\n        vae.eval()\n        vae.to(accelerator.device, dtype=vae_dtype)\n\n    unet.requires_grad_(train_unet)\n    if not train_unet:\n        unet.to(accelerator.device, dtype=weight_dtype)  # because of unet is not prepared\n\n    training_models = []\n    params_to_optimize = []\n    if train_unet:\n        training_models.append(unet)\n        if block_lrs is None:\n            params_to_optimize.append({\"params\": list(unet.parameters()), \"lr\": args.learning_rate})\n        else:\n            params_to_optimize.extend(get_block_params_to_optimize(unet, block_lrs))\n\n    if train_text_encoder1:\n        training_models.append(text_encoder1)\n        params_to_optimize.append({\"params\": list(text_encoder1.parameters()), \"lr\": args.learning_rate_te1 or args.learning_rate})\n    if train_text_encoder2:\n        training_models.append(text_encoder2)\n        params_to_optimize.append({\"params\": list(text_encoder2.parameters()), \"lr\": args.learning_rate_te2 or args.learning_rate})\n\n    # calculate number of trainable parameters\n    n_params = 0\n    for params in params_to_optimize:\n        for p in params[\"params\"]:\n            n_params += p.numel()\n\n    accelerator.print(f\"train unet: {train_unet}, text_encoder1: {train_text_encoder1}, text_encoder2: {train_text_encoder2}\")\n    accelerator.print(f\"number of models: {len(training_models)}\")\n    accelerator.print(f\"number of trainable parameters: {n_params}\")\n\n    # 学習に必要なクラスを準備する\n    accelerator.print(\"prepare optimizer, data loader etc.\")\n    _, _, optimizer = train_util.get_optimizer(args, trainable_params=params_to_optimize)\n\n    # dataloaderを準備する\n    # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n    n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset_group,\n        batch_size=1,\n        shuffle=True,\n        collate_fn=collator,\n        num_workers=n_workers,\n        persistent_workers=args.persistent_data_loader_workers,\n    )\n\n    # 学習ステップ数を計算する\n    if args.max_train_epochs is not None:\n        args.max_train_steps = args.max_train_epochs * math.ceil(\n            len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n        )\n        accelerator.print(\n            f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n        )\n\n    # データセット側にも学習ステップを送信\n    train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n    # lr schedulerを用意する\n    lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n    # 実験的機能：勾配も含めたfp16/bf16学習を行う　モデル全体をfp16/bf16にする\n    if args.full_fp16:\n        assert (\n            args.mixed_precision == \"fp16\"\n        ), \"full_fp16 requires mixed precision='fp16' / full_fp16を使う場合はmixed_precision='fp16'を指定してください。\"\n        accelerator.print(\"enable full fp16 training.\")\n        unet.to(weight_dtype)\n        text_encoder1.to(weight_dtype)\n        text_encoder2.to(weight_dtype)\n    elif args.full_bf16:\n        assert (\n            args.mixed_precision == \"bf16\"\n        ), \"full_bf16 requires mixed precision='bf16' / full_bf16を使う場合はmixed_precision='bf16'を指定してください。\"\n        accelerator.print(\"enable full bf16 training.\")\n        unet.to(weight_dtype)\n        text_encoder1.to(weight_dtype)\n        text_encoder2.to(weight_dtype)\n\n    # freeze last layer and final_layer_norm in te1 since we use the output of the penultimate layer\n    if train_text_encoder1:\n        text_encoder1.text_model.encoder.layers[-1].requires_grad_(False)\n        text_encoder1.text_model.final_layer_norm.requires_grad_(False)\n\n    if args.deepspeed:\n        ds_model = deepspeed_utils.prepare_deepspeed_model(\n            args,\n            unet=unet if train_unet else None,\n            text_encoder1=text_encoder1 if train_text_encoder1 else None,\n            text_encoder2=text_encoder2 if train_text_encoder2 else None,\n        )\n        # most of ZeRO stage uses optimizer partitioning, so we have to prepare optimizer and ds_model at the same time. # pull/1139#issuecomment-1986790007\n        ds_model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            ds_model, optimizer, train_dataloader, lr_scheduler\n        )\n        training_models = [ds_model]\n\n    else:\n        # acceleratorがなんかよろしくやってくれるらしい\n        if train_unet:\n            unet = accelerator.prepare(unet)\n        if train_text_encoder1:\n            text_encoder1 = accelerator.prepare(text_encoder1)\n        if train_text_encoder2:\n            text_encoder2 = accelerator.prepare(text_encoder2)\n        optimizer, train_dataloader, lr_scheduler = accelerator.prepare(optimizer, train_dataloader, lr_scheduler)\n\n    # TextEncoderの出力をキャッシュするときにはCPUへ移動する\n    if args.cache_text_encoder_outputs:\n        # move Text Encoders for sampling images. Text Encoder doesn't work on CPU with fp16\n        text_encoder1.to(\"cpu\", dtype=torch.float32)\n        text_encoder2.to(\"cpu\", dtype=torch.float32)\n        clean_memory_on_device(accelerator.device)\n    else:\n        # make sure Text Encoders are on GPU\n        text_encoder1.to(accelerator.device)\n        text_encoder2.to(accelerator.device)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n    if args.full_fp16:\n        # During deepseed training, accelerate not handles fp16/bf16|mixed precision directly via scaler. Let deepspeed engine do.\n        # -> But we think it's ok to patch accelerator even if deepspeed is enabled.\n        train_util.patch_accelerator_for_fp16_training(accelerator)\n\n    # resumeする\n    train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n    # epoch数を計算する\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n        args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n    # 学習する\n    # total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    accelerator.print(\"running training / 学習開始\")\n    accelerator.print(f\"  num examples / サンプル数: {train_dataset_group.num_train_images}\")\n    accelerator.print(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n    accelerator.print(f\"  num epochs / epoch数: {num_train_epochs}\")\n    accelerator.print(\n        f\"  batch size per device / バッチサイズ: {', '.join([str(d.batch_size) for d in train_dataset_group.datasets])}\"\n    )\n    # accelerator.print(\n    #     f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\"\n    # )\n    accelerator.print(f\"  gradient accumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n    accelerator.print(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n    progress_bar = tqdm(range(args.max_train_steps), smoothing=0, disable=not accelerator.is_local_main_process, desc=\"steps\")\n    global_step = 0\n\n    noise_scheduler = DDPMScheduler(\n        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, clip_sample=False\n    )\n    prepare_scheduler_for_custom_training(noise_scheduler, accelerator.device)\n    if args.zero_terminal_snr:\n        custom_train_functions.fix_noise_scheduler_betas_for_zero_terminal_snr(noise_scheduler)\n\n    if accelerator.is_main_process:\n        init_kwargs = {}\n        if args.wandb_run_name:\n            init_kwargs[\"wandb\"] = {\"name\": args.wandb_run_name}\n        if args.log_tracker_config is not None:\n            init_kwargs = toml.load(args.log_tracker_config)\n        accelerator.init_trackers(\"finetuning\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs)\n\n    # For --sample_at_first\n    sdxl_train_util.sample_images(\n        accelerator, args, 0, global_step, accelerator.device, vae, [tokenizer1, tokenizer2], [text_encoder1, text_encoder2], unet\n    )\n\n    loss_recorder = train_util.LossRecorder()\n    for epoch in range(num_train_epochs):\n        accelerator.print(f\"\\nepoch {epoch+1}/{num_train_epochs}\")\n        current_epoch.value = epoch + 1\n\n        for m in training_models:\n            m.train()\n\n        for step, batch in enumerate(train_dataloader):\n            current_step.value = global_step\n            with accelerator.accumulate(*training_models):\n                if \"latents\" in batch and batch[\"latents\"] is not None:\n                    latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                else:\n                    with torch.no_grad():\n                        # latentに変換\n                        latents = vae.encode(batch[\"images\"].to(vae_dtype)).latent_dist.sample().to(weight_dtype)\n\n                        # NaNが含まれていれば警告を表示し0に置き換える\n                        if torch.any(torch.isnan(latents)):\n                            accelerator.print(\"NaN found in latents, replacing with zeros\")\n                            latents = torch.nan_to_num(latents, 0, out=latents)\n                latents = latents * sdxl_model_util.VAE_SCALE_FACTOR\n\n                if \"text_encoder_outputs1_list\" not in batch or batch[\"text_encoder_outputs1_list\"] is None:\n                    input_ids1 = batch[\"input_ids\"]\n                    input_ids2 = batch[\"input_ids2\"]\n                    with torch.set_grad_enabled(args.train_text_encoder):\n                        # Get the text embedding for conditioning\n                        # TODO support weighted captions\n                        # if args.weighted_captions:\n                        #     encoder_hidden_states = get_weighted_text_embeddings(\n                        #         tokenizer,\n                        #         text_encoder,\n                        #         batch[\"captions\"],\n                        #         accelerator.device,\n                        #         args.max_token_length // 75 if args.max_token_length else 1,\n                        #         clip_skip=args.clip_skip,\n                        #     )\n                        # else:\n                        input_ids1 = input_ids1.to(accelerator.device)\n                        input_ids2 = input_ids2.to(accelerator.device)\n                        # unwrap_model is fine for models not wrapped by accelerator\n                        encoder_hidden_states1, encoder_hidden_states2, pool2 = train_util.get_hidden_states_sdxl(\n                            args.max_token_length,\n                            input_ids1,\n                            input_ids2,\n                            tokenizer1,\n                            tokenizer2,\n                            text_encoder1,\n                            text_encoder2,\n                            None if not args.full_fp16 else weight_dtype,\n                            accelerator=accelerator,\n                        )\n                else:\n                    encoder_hidden_states1 = batch[\"text_encoder_outputs1_list\"].to(accelerator.device).to(weight_dtype)\n                    encoder_hidden_states2 = batch[\"text_encoder_outputs2_list\"].to(accelerator.device).to(weight_dtype)\n                    pool2 = batch[\"text_encoder_pool2_list\"].to(accelerator.device).to(weight_dtype)\n\n                    # # verify that the text encoder outputs are correct\n                    # ehs1, ehs2, p2 = train_util.get_hidden_states_sdxl(\n                    #     args.max_token_length,\n                    #     batch[\"input_ids\"].to(text_encoder1.device),\n                    #     batch[\"input_ids2\"].to(text_encoder1.device),\n                    #     tokenizer1,\n                    #     tokenizer2,\n                    #     text_encoder1,\n                    #     text_encoder2,\n                    #     None if not args.full_fp16 else weight_dtype,\n                    # )\n                    # b_size = encoder_hidden_states1.shape[0]\n                    # assert ((encoder_hidden_states1.to(\"cpu\") - ehs1.to(dtype=weight_dtype)).abs().max() > 1e-2).sum() <= b_size * 2\n                    # assert ((encoder_hidden_states2.to(\"cpu\") - ehs2.to(dtype=weight_dtype)).abs().max() > 1e-2).sum() <= b_size * 2\n                    # assert ((pool2.to(\"cpu\") - p2.to(dtype=weight_dtype)).abs().max() > 1e-2).sum() <= b_size * 2\n                    # logger.info(\"text encoder outputs verified\")\n\n                # get size embeddings\n                orig_size = batch[\"original_sizes_hw\"]\n                crop_size = batch[\"crop_top_lefts\"]\n                target_size = batch[\"target_sizes_hw\"]\n                embs = sdxl_train_util.get_size_embeddings(orig_size, crop_size, target_size, accelerator.device).to(weight_dtype)\n\n                # concat embeddings\n                vector_embedding = torch.cat([pool2, embs], dim=1).to(weight_dtype)\n                text_embedding = torch.cat([encoder_hidden_states1, encoder_hidden_states2], dim=2).to(weight_dtype)\n\n                # Sample noise, sample a random timestep for each image, and add noise to the latents,\n                # with noise offset and/or multires noise if specified\n                noise, noisy_latents, timesteps, huber_c = train_util.get_noise_noisy_latents_and_timesteps(args, noise_scheduler, latents)\n\n                noisy_latents = noisy_latents.to(weight_dtype)  # TODO check why noisy_latents is not weight_dtype\n\n                # Predict the noise residual\n                with accelerator.autocast():\n                    noise_pred = unet(noisy_latents, timesteps, text_embedding, vector_embedding)\n\n                target = noise\n\n                if (\n                    args.min_snr_gamma\n                    or args.scale_v_pred_loss_like_noise_pred\n                    or args.v_pred_like_loss\n                    or args.debiased_estimation_loss\n                    or args.masked_loss\n                ):\n                    # do not mean over batch dimension for snr weight or scale v-pred loss\n                    loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c)\n                    if args.masked_loss:\n                        loss = apply_masked_loss(loss, batch)\n                    loss = loss.mean([1, 2, 3])\n\n                    if args.min_snr_gamma:\n                        loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma)\n                    if args.scale_v_pred_loss_like_noise_pred:\n                        loss = scale_v_prediction_loss_like_noise_prediction(loss, timesteps, noise_scheduler)\n                    if args.v_pred_like_loss:\n                        loss = add_v_prediction_like_loss(loss, timesteps, noise_scheduler, args.v_pred_like_loss)\n                    if args.debiased_estimation_loss:\n                        loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)\n\n                    loss = loss.mean()  # mean over batch dimension\n                else:\n                    loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"mean\", loss_type=args.loss_type, huber_c=huber_c)\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients and args.max_grad_norm != 0.0:\n                    params_to_clip = []\n                    for m in training_models:\n                        params_to_clip.extend(m.parameters())\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                sdxl_train_util.sample_images(\n                    accelerator,\n                    args,\n                    None,\n                    global_step,\n                    accelerator.device,\n                    vae,\n                    [tokenizer1, tokenizer2],\n                    [text_encoder1, text_encoder2],\n                    unet,\n                )\n\n                # 指定ステップごとにモデルを保存\n                if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                    accelerator.wait_for_everyone()\n                    if accelerator.is_main_process:\n                        src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n                        sdxl_train_util.save_sd_model_on_epoch_end_or_stepwise(\n                            args,\n                            False,\n                            accelerator,\n                            src_path,\n                            save_stable_diffusion_format,\n                            use_safetensors,\n                            save_dtype,\n                            epoch,\n                            num_train_epochs,\n                            global_step,\n                            accelerator.unwrap_model(text_encoder1),\n                            accelerator.unwrap_model(text_encoder2),\n                            accelerator.unwrap_model(unet),\n                            vae,\n                            logit_scale,\n                            ckpt_info,\n                        )\n\n            current_loss = loss.detach().item()  # 平均なのでbatch sizeは関係ないはず\n            if args.logging_dir is not None:\n                logs = {\"loss\": current_loss}\n                if block_lrs is None:\n                    train_util.append_lr_to_logs(logs, lr_scheduler, args.optimizer_type, including_unet=train_unet)\n                else:\n                    append_block_lr_to_logs(block_lrs, logs, lr_scheduler, args.optimizer_type)  # U-Net is included in block_lrs\n\n                accelerator.log(logs, step=global_step)\n\n            loss_recorder.add(epoch=epoch, step=step, loss=current_loss)\n            avr_loss: float = loss_recorder.moving_average\n            logs = {\"avr_loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        if args.logging_dir is not None:\n            logs = {\"loss/epoch\": loss_recorder.moving_average}\n            accelerator.log(logs, step=epoch + 1)\n\n        accelerator.wait_for_everyone()\n\n        if args.save_every_n_epochs is not None:\n            if accelerator.is_main_process:\n                src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n                sdxl_train_util.save_sd_model_on_epoch_end_or_stepwise(\n                    args,\n                    True,\n                    accelerator,\n                    src_path,\n                    save_stable_diffusion_format,\n                    use_safetensors,\n                    save_dtype,\n                    epoch,\n                    num_train_epochs,\n                    global_step,\n                    accelerator.unwrap_model(text_encoder1),\n                    accelerator.unwrap_model(text_encoder2),\n                    accelerator.unwrap_model(unet),\n                    vae,\n                    logit_scale,\n                    ckpt_info,\n                )\n\n        sdxl_train_util.sample_images(\n            accelerator,\n            args,\n            epoch + 1,\n            global_step,\n            accelerator.device,\n            vae,\n            [tokenizer1, tokenizer2],\n            [text_encoder1, text_encoder2],\n            unet,\n        )\n\n    is_main_process = accelerator.is_main_process\n    # if is_main_process:\n    unet = accelerator.unwrap_model(unet)\n    text_encoder1 = accelerator.unwrap_model(text_encoder1)\n    text_encoder2 = accelerator.unwrap_model(text_encoder2)\n\n    accelerator.end_training()\n\n    if args.save_state or args.save_state_on_train_end:        \n        train_util.save_state_on_train_end(args, accelerator)\n\n    del accelerator  # この後メモリを使うのでこれは消す\n\n    if is_main_process:\n        src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n        sdxl_train_util.save_sd_model_on_train_end(\n            args,\n            src_path,\n            save_stable_diffusion_format,\n            use_safetensors,\n            save_dtype,\n            epoch,\n            global_step,\n            text_encoder1,\n            text_encoder2,\n            unet,\n            vae,\n            logit_scale,\n            ckpt_info,\n        )\n        logger.info(\"model saved.\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, True, True, True)\n    train_util.add_training_arguments(parser, False)\n    train_util.add_masked_loss_arguments(parser)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_sd_saving_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser)\n    sdxl_train_util.add_sdxl_training_arguments(parser)\n\n    parser.add_argument(\n        \"--learning_rate_te1\",\n        type=float,\n        default=None,\n        help=\"learning rate for text encoder 1 (ViT-L) / text encoder 1 (ViT-L)の学習率\",\n    )\n    parser.add_argument(\n        \"--learning_rate_te2\",\n        type=float,\n        default=None,\n        help=\"learning rate for text encoder 2 (BiG-G) / text encoder 2 (BiG-G)の学習率\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_xformers\", action=\"store_true\", help=\"use xformers by diffusers / Diffusersでxformersを使用する\"\n    )\n    parser.add_argument(\"--train_text_encoder\", action=\"store_true\", help=\"train text encoder / text encoderも学習する\")\n    parser.add_argument(\n        \"--no_half_vae\",\n        action=\"store_true\",\n        help=\"do not use fp16/bf16 VAE in mixed precision (use float VAE) / mixed precisionでも fp16/bf16 VAEを使わずfloat VAEを使う\",\n    )\n    parser.add_argument(\n        \"--block_lr\",\n        type=str,\n        default=None,\n        help=f\"learning rates for each block of U-Net, comma-separated, {UNET_NUM_BLOCKS_FOR_BLOCK_LR} values / \"\n        + f\"U-Netの各ブロックの学習率、カンマ区切り、{UNET_NUM_BLOCKS_FOR_BLOCK_LR}個の値\",\n    )\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    train(args)\n"
        },
        {
          "name": "sdxl_train_control_net_lllite.py",
          "type": "blob",
          "size": 27.5634765625,
          "content": "# cond_imageをU-Netのforwardで渡すバージョンのControlNet-LLLite検証用学習コード\n# training code for ControlNet-LLLite with passing cond_image to U-Net's forward\n\nimport argparse\nimport json\nimport math\nimport os\nimport random\nimport time\nfrom multiprocessing import Value\nfrom types import SimpleNamespace\nimport toml\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library.device_utils import init_ipex, clean_memory_on_device\ninit_ipex()\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom accelerate.utils import set_seed\nimport accelerate\nfrom diffusers import DDPMScheduler, ControlNetModel\nfrom safetensors.torch import load_file\nfrom library import deepspeed_utils, sai_model_spec, sdxl_model_util, sdxl_original_unet, sdxl_train_util\n\nimport library.model_util as model_util\nimport library.train_util as train_util\nimport library.config_util as config_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.huggingface_util as huggingface_util\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    add_v_prediction_like_loss,\n    apply_snr_weight,\n    prepare_scheduler_for_custom_training,\n    pyramid_noise_like,\n    apply_noise_offset,\n    scale_v_prediction_loss_like_noise_prediction,\n    apply_debiased_estimation,\n)\nimport networks.control_net_lllite_for_train as control_net_lllite_for_train\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO 他のスクリプトと共通化する\ndef generate_step_logs(args: argparse.Namespace, current_loss, avr_loss, lr_scheduler):\n    logs = {\n        \"loss/current\": current_loss,\n        \"loss/average\": avr_loss,\n        \"lr\": lr_scheduler.get_last_lr()[0],\n    }\n\n    if args.optimizer_type.lower().startswith(\"DAdapt\".lower()):\n        logs[\"lr/d*lr\"] = lr_scheduler.optimizers[-1].param_groups[0][\"d\"] * lr_scheduler.optimizers[-1].param_groups[0][\"lr\"]\n\n    return logs\n\n\ndef train(args):\n    train_util.verify_training_args(args)\n    train_util.prepare_dataset_args(args, True)\n    sdxl_train_util.verify_sdxl_training_args(args)\n    setup_logging(args, reset=True)\n\n    cache_latents = args.cache_latents\n    use_user_config = args.dataset_config is not None\n\n    if args.seed is None:\n        args.seed = random.randint(0, 2**32)\n    set_seed(args.seed)\n\n    tokenizer1, tokenizer2 = sdxl_train_util.load_tokenizers(args)\n\n    # データセットを準備する\n    blueprint_generator = BlueprintGenerator(ConfigSanitizer(False, False, True, True))\n    if use_user_config:\n        logger.info(f\"Load dataset config from {args.dataset_config}\")\n        user_config = config_util.load_user_config(args.dataset_config)\n        ignored = [\"train_data_dir\", \"conditioning_data_dir\"]\n        if any(getattr(args, attr) is not None for attr in ignored):\n            logger.warning(\n                \"ignore following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                    \", \".join(ignored)\n                )\n            )\n    else:\n        user_config = {\n            \"datasets\": [\n                {\n                    \"subsets\": config_util.generate_controlnet_subsets_config_by_subdirs(\n                        args.train_data_dir,\n                        args.conditioning_data_dir,\n                        args.caption_extension,\n                    )\n                }\n            ]\n        }\n\n    blueprint = blueprint_generator.generate(user_config, args, tokenizer=[tokenizer1, tokenizer2])\n    train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n\n    current_epoch = Value(\"i\", 0)\n    current_step = Value(\"i\", 0)\n    ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n    collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n    train_dataset_group.verify_bucket_reso_steps(32)\n\n    if args.debug_dataset:\n        train_util.debug_dataset(train_dataset_group)\n        return\n    if len(train_dataset_group) == 0:\n        logger.error(\n            \"No data found. Please verify arguments (train_data_dir must be the parent of folders with images) / 画像がありません。引数指定を確認してください（train_data_dirには画像があるフォルダではなく、画像があるフォルダの親フォルダを指定する必要があります）\"\n        )\n        return\n\n    if cache_latents:\n        assert (\n            train_dataset_group.is_latent_cacheable()\n        ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n    else:\n        logger.warning(\n            \"WARNING: random_crop is not supported yet for ControlNet training / ControlNetの学習ではrandom_cropはまだサポートされていません\"\n        )\n\n    if args.cache_text_encoder_outputs:\n        assert (\n            train_dataset_group.is_text_encoder_output_cacheable()\n        ), \"when caching Text Encoder output, either caption_dropout_rate, shuffle_caption, token_warmup_step or caption_tag_dropout_rate cannot be used / Text Encoderの出力をキャッシュするときはcaption_dropout_rate, shuffle_caption, token_warmup_step, caption_tag_dropout_rateは使えません\"\n\n    # acceleratorを準備する\n    logger.info(\"prepare accelerator\")\n    accelerator = train_util.prepare_accelerator(args)\n    is_main_process = accelerator.is_main_process\n\n    # mixed precisionに対応した型を用意しておき適宜castする\n    weight_dtype, save_dtype = train_util.prepare_dtype(args)\n    vae_dtype = torch.float32 if args.no_half_vae else weight_dtype\n\n    # モデルを読み込む\n    (\n        load_stable_diffusion_format,\n        text_encoder1,\n        text_encoder2,\n        vae,\n        unet,\n        logit_scale,\n        ckpt_info,\n    ) = sdxl_train_util.load_target_model(args, accelerator, sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, weight_dtype)\n\n    # 学習を準備する\n    if cache_latents:\n        vae.to(accelerator.device, dtype=vae_dtype)\n        vae.requires_grad_(False)\n        vae.eval()\n        with torch.no_grad():\n            train_dataset_group.cache_latents(\n                vae,\n                args.vae_batch_size,\n                args.cache_latents_to_disk,\n                accelerator.is_main_process,\n            )\n        vae.to(\"cpu\")\n        clean_memory_on_device(accelerator.device)\n\n        accelerator.wait_for_everyone()\n\n    # TextEncoderの出力をキャッシュする\n    if args.cache_text_encoder_outputs:\n        # Text Encodes are eval and no grad\n        with torch.no_grad():\n            train_dataset_group.cache_text_encoder_outputs(\n                (tokenizer1, tokenizer2),\n                (text_encoder1, text_encoder2),\n                accelerator.device,\n                None,\n                args.cache_text_encoder_outputs_to_disk,\n                accelerator.is_main_process,\n            )\n        accelerator.wait_for_everyone()\n\n    # prepare ControlNet-LLLite\n    control_net_lllite_for_train.replace_unet_linear_and_conv2d()\n\n    if args.network_weights is not None:\n        accelerator.print(f\"initialize U-Net with ControlNet-LLLite\")\n        with accelerate.init_empty_weights():\n            unet_lllite = control_net_lllite_for_train.SdxlUNet2DConditionModelControlNetLLLite()\n        unet_lllite.to(accelerator.device, dtype=weight_dtype)\n\n        unet_sd = unet.state_dict()\n        info = unet_lllite.load_lllite_weights(args.network_weights, unet_sd)\n        accelerator.print(f\"load ControlNet-LLLite weights from {args.network_weights}: {info}\")\n    else:\n        # cosumes large memory, so send to GPU before creating the LLLite model\n        accelerator.print(\"sending U-Net to GPU\")\n        unet.to(accelerator.device, dtype=weight_dtype)\n        unet_sd = unet.state_dict()\n\n        # init LLLite weights\n        accelerator.print(f\"initialize U-Net with ControlNet-LLLite\")\n\n        if args.lowram:\n            with accelerate.init_on_device(accelerator.device):\n                unet_lllite = control_net_lllite_for_train.SdxlUNet2DConditionModelControlNetLLLite()\n        else:\n            unet_lllite = control_net_lllite_for_train.SdxlUNet2DConditionModelControlNetLLLite()\n        unet_lllite.to(weight_dtype)\n\n        info = unet_lllite.load_lllite_weights(None, unet_sd)\n        accelerator.print(f\"init U-Net with ControlNet-LLLite weights: {info}\")\n    del unet_sd, unet\n\n    unet: control_net_lllite_for_train.SdxlUNet2DConditionModelControlNetLLLite = unet_lllite\n    del unet_lllite\n\n    unet.apply_lllite(args.cond_emb_dim, args.network_dim, args.network_dropout)\n\n    # モデルに xformers とか memory efficient attention を組み込む\n    train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n\n    # 学習に必要なクラスを準備する\n    accelerator.print(\"prepare optimizer, data loader etc.\")\n\n    trainable_params = list(unet.prepare_params())\n    logger.info(f\"trainable params count: {len(trainable_params)}\")\n    logger.info(f\"number of trainable parameters: {sum(p.numel() for p in trainable_params if p.requires_grad)}\")\n\n    _, _, optimizer = train_util.get_optimizer(args, trainable_params)\n\n    # dataloaderを準備する\n    # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n    n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset_group,\n        batch_size=1,\n        shuffle=True,\n        collate_fn=collator,\n        num_workers=n_workers,\n        persistent_workers=args.persistent_data_loader_workers,\n    )\n\n    # 学習ステップ数を計算する\n    if args.max_train_epochs is not None:\n        args.max_train_steps = args.max_train_epochs * math.ceil(\n            len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n        )\n        accelerator.print(\n            f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n        )\n\n    # データセット側にも学習ステップを送信\n    train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n    # lr schedulerを用意する\n    lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n    # 実験的機能：勾配も含めたfp16/bf16学習を行う　モデル全体をfp16/bf16にする\n    # if args.full_fp16:\n    #     assert (\n    #         args.mixed_precision == \"fp16\"\n    #     ), \"full_fp16 requires mixed precision='fp16' / full_fp16を使う場合はmixed_precision='fp16'を指定してください。\"\n    #     accelerator.print(\"enable full fp16 training.\")\n    #     unet.to(weight_dtype)\n    # elif args.full_bf16:\n    #     assert (\n    #         args.mixed_precision == \"bf16\"\n    #     ), \"full_bf16 requires mixed precision='bf16' / full_bf16を使う場合はmixed_precision='bf16'を指定してください。\"\n    #     accelerator.print(\"enable full bf16 training.\")\n    #     unet.to(weight_dtype)\n\n    unet.to(weight_dtype)\n\n    # acceleratorがなんかよろしくやってくれるらしい\n    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader, lr_scheduler)\n\n    if args.gradient_checkpointing:\n        unet.train()  # according to TI example in Diffusers, train is required -> これオリジナルのU-Netしたので本当は外せる\n    else:\n        unet.eval()\n\n    # TextEncoderの出力をキャッシュするときにはCPUへ移動する\n    if args.cache_text_encoder_outputs:\n        # move Text Encoders for sampling images. Text Encoder doesn't work on CPU with fp16\n        text_encoder1.to(\"cpu\", dtype=torch.float32)\n        text_encoder2.to(\"cpu\", dtype=torch.float32)\n        clean_memory_on_device(accelerator.device)\n    else:\n        # make sure Text Encoders are on GPU\n        text_encoder1.to(accelerator.device)\n        text_encoder2.to(accelerator.device)\n\n    if not cache_latents:\n        vae.requires_grad_(False)\n        vae.eval()\n        vae.to(accelerator.device, dtype=vae_dtype)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n    if args.full_fp16:\n        train_util.patch_accelerator_for_fp16_training(accelerator)\n\n    # resumeする\n    train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n    # epoch数を計算する\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n        args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n    # 学習する\n    # TODO: find a way to handle total batch size when there are multiple datasets\n    accelerator.print(\"running training / 学習開始\")\n    accelerator.print(f\"  num train images * repeats / 学習画像の数×繰り返し回数: {train_dataset_group.num_train_images}\")\n    accelerator.print(f\"  num reg images / 正則化画像の数: {train_dataset_group.num_reg_images}\")\n    accelerator.print(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n    accelerator.print(f\"  num epochs / epoch数: {num_train_epochs}\")\n    accelerator.print(\n        f\"  batch size per device / バッチサイズ: {', '.join([str(d.batch_size) for d in train_dataset_group.datasets])}\"\n    )\n    # logger.info(f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\")\n    accelerator.print(f\"  gradient accumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n    accelerator.print(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n    progress_bar = tqdm(range(args.max_train_steps), smoothing=0, disable=not accelerator.is_local_main_process, desc=\"steps\")\n    global_step = 0\n\n    noise_scheduler = DDPMScheduler(\n        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, clip_sample=False\n    )\n    prepare_scheduler_for_custom_training(noise_scheduler, accelerator.device)\n    if args.zero_terminal_snr:\n        custom_train_functions.fix_noise_scheduler_betas_for_zero_terminal_snr(noise_scheduler)\n\n    if accelerator.is_main_process:\n        init_kwargs = {}\n        if args.wandb_run_name:\n            init_kwargs[\"wandb\"] = {\"name\": args.wandb_run_name}\n        if args.log_tracker_config is not None:\n            init_kwargs = toml.load(args.log_tracker_config)\n        accelerator.init_trackers(\n            \"lllite_control_net_train\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs\n        )\n\n    loss_recorder = train_util.LossRecorder()\n    del train_dataset_group\n\n    # function for saving/removing\n    def save_model(\n        ckpt_name,\n        unwrapped_nw: control_net_lllite_for_train.SdxlUNet2DConditionModelControlNetLLLite,\n        steps,\n        epoch_no,\n        force_sync_upload=False,\n    ):\n        os.makedirs(args.output_dir, exist_ok=True)\n        ckpt_file = os.path.join(args.output_dir, ckpt_name)\n\n        accelerator.print(f\"\\nsaving checkpoint: {ckpt_file}\")\n        sai_metadata = train_util.get_sai_model_spec(None, args, True, True, False)\n        sai_metadata[\"modelspec.architecture\"] = sai_model_spec.ARCH_SD_XL_V1_BASE + \"/control-net-lllite\"\n\n        unwrapped_nw.save_lllite_weights(ckpt_file, save_dtype, sai_metadata)\n        if args.huggingface_repo_id is not None:\n            huggingface_util.upload(args, ckpt_file, \"/\" + ckpt_name, force_sync_upload=force_sync_upload)\n\n    def remove_model(old_ckpt_name):\n        old_ckpt_file = os.path.join(args.output_dir, old_ckpt_name)\n        if os.path.exists(old_ckpt_file):\n            accelerator.print(f\"removing old checkpoint: {old_ckpt_file}\")\n            os.remove(old_ckpt_file)\n\n    # training loop\n    for epoch in range(num_train_epochs):\n        accelerator.print(f\"\\nepoch {epoch+1}/{num_train_epochs}\")\n        current_epoch.value = epoch + 1\n\n        for step, batch in enumerate(train_dataloader):\n            current_step.value = global_step\n            with accelerator.accumulate(unet):\n                with torch.no_grad():\n                    if \"latents\" in batch and batch[\"latents\"] is not None:\n                        latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                    else:\n                        # latentに変換\n                        latents = vae.encode(batch[\"images\"].to(dtype=vae_dtype)).latent_dist.sample().to(dtype=weight_dtype)\n\n                        # NaNが含まれていれば警告を表示し0に置き換える\n                        if torch.any(torch.isnan(latents)):\n                            accelerator.print(\"NaN found in latents, replacing with zeros\")\n                            latents = torch.nan_to_num(latents, 0, out=latents)\n                    latents = latents * sdxl_model_util.VAE_SCALE_FACTOR\n\n                if \"text_encoder_outputs1_list\" not in batch or batch[\"text_encoder_outputs1_list\"] is None:\n                    input_ids1 = batch[\"input_ids\"]\n                    input_ids2 = batch[\"input_ids2\"]\n                    with torch.no_grad():\n                        # Get the text embedding for conditioning\n                        input_ids1 = input_ids1.to(accelerator.device)\n                        input_ids2 = input_ids2.to(accelerator.device)\n                        encoder_hidden_states1, encoder_hidden_states2, pool2 = train_util.get_hidden_states_sdxl(\n                            args.max_token_length,\n                            input_ids1,\n                            input_ids2,\n                            tokenizer1,\n                            tokenizer2,\n                            text_encoder1,\n                            text_encoder2,\n                            None if not args.full_fp16 else weight_dtype,\n                        )\n                else:\n                    encoder_hidden_states1 = batch[\"text_encoder_outputs1_list\"].to(accelerator.device).to(weight_dtype)\n                    encoder_hidden_states2 = batch[\"text_encoder_outputs2_list\"].to(accelerator.device).to(weight_dtype)\n                    pool2 = batch[\"text_encoder_pool2_list\"].to(accelerator.device).to(weight_dtype)\n\n                # get size embeddings\n                orig_size = batch[\"original_sizes_hw\"]\n                crop_size = batch[\"crop_top_lefts\"]\n                target_size = batch[\"target_sizes_hw\"]\n                embs = sdxl_train_util.get_size_embeddings(orig_size, crop_size, target_size, accelerator.device).to(weight_dtype)\n\n                # concat embeddings\n                vector_embedding = torch.cat([pool2, embs], dim=1).to(weight_dtype)\n                text_embedding = torch.cat([encoder_hidden_states1, encoder_hidden_states2], dim=2).to(weight_dtype)\n\n                # Sample noise, sample a random timestep for each image, and add noise to the latents,\n                # with noise offset and/or multires noise if specified\n                noise, noisy_latents, timesteps, huber_c = train_util.get_noise_noisy_latents_and_timesteps(args, noise_scheduler, latents)\n\n                noisy_latents = noisy_latents.to(weight_dtype)  # TODO check why noisy_latents is not weight_dtype\n\n                controlnet_image = batch[\"conditioning_images\"].to(dtype=weight_dtype)\n\n                with accelerator.autocast():\n                    # conditioning imageをControlNetに渡す / pass conditioning image to ControlNet\n                    # 内部でcond_embに変換される / it will be converted to cond_emb inside\n\n                    # それらの値を使いつつ、U-Netでノイズを予測する / predict noise with U-Net using those values\n                    noise_pred = unet(noisy_latents, timesteps, text_embedding, vector_embedding, controlnet_image)\n\n                if args.v_parameterization:\n                    # v-parameterization training\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    target = noise\n\n                loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c)\n                loss = loss.mean([1, 2, 3])\n\n                loss_weights = batch[\"loss_weights\"]  # 各sampleごとのweight\n                loss = loss * loss_weights\n\n                if args.min_snr_gamma:\n                    loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma, args.v_parameterization)\n                if args.scale_v_pred_loss_like_noise_pred:\n                    loss = scale_v_prediction_loss_like_noise_prediction(loss, timesteps, noise_scheduler)\n                if args.v_pred_like_loss:\n                    loss = add_v_prediction_like_loss(loss, timesteps, noise_scheduler, args.v_pred_like_loss)\n                if args.debiased_estimation_loss:\n                    loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)\n\n                loss = loss.mean()  # 平均なのでbatch_sizeで割る必要なし\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients and args.max_grad_norm != 0.0:\n                    params_to_clip = unet.get_trainable_params()\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                # sdxl_train_util.sample_images(accelerator, args, None, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n                # 指定ステップごとにモデルを保存\n                if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                    accelerator.wait_for_everyone()\n                    if accelerator.is_main_process:\n                        ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, global_step)\n                        save_model(ckpt_name, accelerator.unwrap_model(unet), global_step, epoch)\n\n                        if args.save_state:\n                            train_util.save_and_remove_state_stepwise(args, accelerator, global_step)\n\n                        remove_step_no = train_util.get_remove_step_no(args, global_step)\n                        if remove_step_no is not None:\n                            remove_ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, remove_step_no)\n                            remove_model(remove_ckpt_name)\n\n            current_loss = loss.detach().item()\n            loss_recorder.add(epoch=epoch, step=step, loss=current_loss)\n            avr_loss: float = loss_recorder.moving_average\n            logs = {\"avr_loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if args.logging_dir is not None:\n                logs = generate_step_logs(args, current_loss, avr_loss, lr_scheduler)\n                accelerator.log(logs, step=global_step)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        if args.logging_dir is not None:\n            logs = {\"loss/epoch\": loss_recorder.moving_average}\n            accelerator.log(logs, step=epoch + 1)\n\n        accelerator.wait_for_everyone()\n\n        # 指定エポックごとにモデルを保存\n        if args.save_every_n_epochs is not None:\n            saving = (epoch + 1) % args.save_every_n_epochs == 0 and (epoch + 1) < num_train_epochs\n            if is_main_process and saving:\n                ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, epoch + 1)\n                save_model(ckpt_name, accelerator.unwrap_model(unet), global_step, epoch + 1)\n\n                remove_epoch_no = train_util.get_remove_epoch_no(args, epoch + 1)\n                if remove_epoch_no is not None:\n                    remove_ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, remove_epoch_no)\n                    remove_model(remove_ckpt_name)\n\n                if args.save_state:\n                    train_util.save_and_remove_state_on_epoch_end(args, accelerator, epoch + 1)\n\n        # self.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n        # end of epoch\n\n    if is_main_process:\n        unet = accelerator.unwrap_model(unet)\n\n    accelerator.end_training()\n\n    if is_main_process and (args.save_state or args.save_state_on_train_end):\n        train_util.save_state_on_train_end(args, accelerator)\n\n    if is_main_process:\n        ckpt_name = train_util.get_last_ckpt_name(args, \".\" + args.save_model_as)\n        save_model(ckpt_name, unet, global_step, num_train_epochs, force_sync_upload=True)\n\n        logger.info(\"model saved.\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, False, True, True)\n    train_util.add_training_arguments(parser, False)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser)\n    sdxl_train_util.add_sdxl_training_arguments(parser)\n\n    parser.add_argument(\n        \"--save_model_as\",\n        type=str,\n        default=\"safetensors\",\n        choices=[None, \"ckpt\", \"pt\", \"safetensors\"],\n        help=\"format to save the model (default is .safetensors) / モデル保存時の形式（デフォルトはsafetensors）\",\n    )\n    parser.add_argument(\n        \"--cond_emb_dim\", type=int, default=None, help=\"conditioning embedding dimension / 条件付け埋め込みの次元数\"\n    )\n    parser.add_argument(\n        \"--network_weights\", type=str, default=None, help=\"pretrained weights for network / 学習するネットワークの初期重み\"\n    )\n    parser.add_argument(\"--network_dim\", type=int, default=None, help=\"network dimensions (rank) / モジュールの次元数\")\n    parser.add_argument(\n        \"--network_dropout\",\n        type=float,\n        default=None,\n        help=\"Drops neurons out of training every step (0 or None is default behavior (no dropout), 1 would drop all neurons) / 訓練時に毎ステップでニューロンをdropする（0またはNoneはdropoutなし、1は全ニューロンをdropout）\",\n    )\n    parser.add_argument(\n        \"--conditioning_data_dir\",\n        type=str,\n        default=None,\n        help=\"conditioning data directory / 条件付けデータのディレクトリ\",\n    )\n    parser.add_argument(\n        \"--no_half_vae\",\n        action=\"store_true\",\n        help=\"do not use fp16/bf16 VAE in mixed precision (use float VAE) / mixed precisionでも fp16/bf16 VAEを使わずfloat VAEを使う\",\n    )\n    return parser\n\n\nif __name__ == \"__main__\":\n    # sdxl_original_unet.USE_REENTRANT = False\n\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    train(args)\n"
        },
        {
          "name": "sdxl_train_control_net_lllite_old.py",
          "type": "blob",
          "size": 26.1318359375,
          "content": "import argparse\nimport json\nimport math\nimport os\nimport random\nimport time\nfrom multiprocessing import Value\nfrom types import SimpleNamespace\nimport toml\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library.device_utils import init_ipex, clean_memory_on_device\ninit_ipex()\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom accelerate.utils import set_seed\nfrom diffusers import DDPMScheduler, ControlNetModel\nfrom safetensors.torch import load_file\nfrom library import deepspeed_utils, sai_model_spec, sdxl_model_util, sdxl_original_unet, sdxl_train_util\n\nimport library.model_util as model_util\nimport library.train_util as train_util\nimport library.config_util as config_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.huggingface_util as huggingface_util\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    add_v_prediction_like_loss,\n    apply_snr_weight,\n    prepare_scheduler_for_custom_training,\n    pyramid_noise_like,\n    apply_noise_offset,\n    scale_v_prediction_loss_like_noise_prediction,\n    apply_debiased_estimation,\n)\nimport networks.control_net_lllite as control_net_lllite\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO 他のスクリプトと共通化する\ndef generate_step_logs(args: argparse.Namespace, current_loss, avr_loss, lr_scheduler):\n    logs = {\n        \"loss/current\": current_loss,\n        \"loss/average\": avr_loss,\n        \"lr\": lr_scheduler.get_last_lr()[0],\n    }\n\n    if args.optimizer_type.lower().startswith(\"DAdapt\".lower()):\n        logs[\"lr/d*lr\"] = lr_scheduler.optimizers[-1].param_groups[0][\"d\"] * lr_scheduler.optimizers[-1].param_groups[0][\"lr\"]\n\n    return logs\n\n\ndef train(args):\n    train_util.verify_training_args(args)\n    train_util.prepare_dataset_args(args, True)\n    sdxl_train_util.verify_sdxl_training_args(args)\n    setup_logging(args, reset=True)\n\n    cache_latents = args.cache_latents\n    use_user_config = args.dataset_config is not None\n\n    if args.seed is None:\n        args.seed = random.randint(0, 2**32)\n    set_seed(args.seed)\n\n    tokenizer1, tokenizer2 = sdxl_train_util.load_tokenizers(args)\n\n    # データセットを準備する\n    blueprint_generator = BlueprintGenerator(ConfigSanitizer(False, False, True, True))\n    if use_user_config:\n        logger.info(f\"Load dataset config from {args.dataset_config}\")\n        user_config = config_util.load_user_config(args.dataset_config)\n        ignored = [\"train_data_dir\", \"conditioning_data_dir\"]\n        if any(getattr(args, attr) is not None for attr in ignored):\n            logger.warning(\n                \"ignore following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                    \", \".join(ignored)\n                )\n            )\n    else:\n        user_config = {\n            \"datasets\": [\n                {\n                    \"subsets\": config_util.generate_controlnet_subsets_config_by_subdirs(\n                        args.train_data_dir,\n                        args.conditioning_data_dir,\n                        args.caption_extension,\n                    )\n                }\n            ]\n        }\n\n    blueprint = blueprint_generator.generate(user_config, args, tokenizer=[tokenizer1, tokenizer2])\n    train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n\n    current_epoch = Value(\"i\", 0)\n    current_step = Value(\"i\", 0)\n    ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n    collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n    train_dataset_group.verify_bucket_reso_steps(32)\n\n    if args.debug_dataset:\n        train_util.debug_dataset(train_dataset_group)\n        return\n    if len(train_dataset_group) == 0:\n        logger.error(\n            \"No data found. Please verify arguments (train_data_dir must be the parent of folders with images) / 画像がありません。引数指定を確認してください（train_data_dirには画像があるフォルダではなく、画像があるフォルダの親フォルダを指定する必要があります）\"\n        )\n        return\n\n    if cache_latents:\n        assert (\n            train_dataset_group.is_latent_cacheable()\n        ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n    else:\n        logger.warning(\n            \"WARNING: random_crop is not supported yet for ControlNet training / ControlNetの学習ではrandom_cropはまだサポートされていません\"\n        )\n\n    if args.cache_text_encoder_outputs:\n        assert (\n            train_dataset_group.is_text_encoder_output_cacheable()\n        ), \"when caching Text Encoder output, either caption_dropout_rate, shuffle_caption, token_warmup_step or caption_tag_dropout_rate cannot be used / Text Encoderの出力をキャッシュするときはcaption_dropout_rate, shuffle_caption, token_warmup_step, caption_tag_dropout_rateは使えません\"\n\n    # acceleratorを準備する\n    logger.info(\"prepare accelerator\")\n    accelerator = train_util.prepare_accelerator(args)\n    is_main_process = accelerator.is_main_process\n\n    # mixed precisionに対応した型を用意しておき適宜castする\n    weight_dtype, save_dtype = train_util.prepare_dtype(args)\n    vae_dtype = torch.float32 if args.no_half_vae else weight_dtype\n\n    # モデルを読み込む\n    (\n        load_stable_diffusion_format,\n        text_encoder1,\n        text_encoder2,\n        vae,\n        unet,\n        logit_scale,\n        ckpt_info,\n    ) = sdxl_train_util.load_target_model(args, accelerator, sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, weight_dtype)\n\n    # モデルに xformers とか memory efficient attention を組み込む\n    train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n\n    # 学習を準備する\n    if cache_latents:\n        vae.to(accelerator.device, dtype=vae_dtype)\n        vae.requires_grad_(False)\n        vae.eval()\n        with torch.no_grad():\n            train_dataset_group.cache_latents(\n                vae,\n                args.vae_batch_size,\n                args.cache_latents_to_disk,\n                accelerator.is_main_process,\n            )\n        vae.to(\"cpu\")\n        clean_memory_on_device(accelerator.device)\n\n        accelerator.wait_for_everyone()\n\n    # TextEncoderの出力をキャッシュする\n    if args.cache_text_encoder_outputs:\n        # Text Encodes are eval and no grad\n        with torch.no_grad():\n            train_dataset_group.cache_text_encoder_outputs(\n                (tokenizer1, tokenizer2),\n                (text_encoder1, text_encoder2),\n                accelerator.device,\n                None,\n                args.cache_text_encoder_outputs_to_disk,\n                accelerator.is_main_process,\n            )\n        accelerator.wait_for_everyone()\n\n    # prepare ControlNet\n    network = control_net_lllite.ControlNetLLLite(unet, args.cond_emb_dim, args.network_dim, args.network_dropout)\n    network.apply_to()\n\n    if args.network_weights is not None:\n        info = network.load_weights(args.network_weights)\n        accelerator.print(f\"load ControlNet weights from {args.network_weights}: {info}\")\n\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n        network.enable_gradient_checkpointing()  # may have no effect\n\n    # 学習に必要なクラスを準備する\n    accelerator.print(\"prepare optimizer, data loader etc.\")\n\n    trainable_params = list(network.prepare_optimizer_params())\n    logger.info(f\"trainable params count: {len(trainable_params)}\")\n    logger.info(f\"number of trainable parameters: {sum(p.numel() for p in trainable_params if p.requires_grad)}\")\n\n    _, _, optimizer = train_util.get_optimizer(args, trainable_params)\n\n    # dataloaderを準備する\n    # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n    n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset_group,\n        batch_size=1,\n        shuffle=True,\n        collate_fn=collator,\n        num_workers=n_workers,\n        persistent_workers=args.persistent_data_loader_workers,\n    )\n\n    # 学習ステップ数を計算する\n    if args.max_train_epochs is not None:\n        args.max_train_steps = args.max_train_epochs * math.ceil(\n            len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n        )\n        accelerator.print(\n            f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n        )\n\n    # データセット側にも学習ステップを送信\n    train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n    # lr schedulerを用意する\n    lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n    # 実験的機能：勾配も含めたfp16/bf16学習を行う　モデル全体をfp16/bf16にする\n    if args.full_fp16:\n        assert (\n            args.mixed_precision == \"fp16\"\n        ), \"full_fp16 requires mixed precision='fp16' / full_fp16を使う場合はmixed_precision='fp16'を指定してください。\"\n        accelerator.print(\"enable full fp16 training.\")\n        unet.to(weight_dtype)\n        network.to(weight_dtype)\n    elif args.full_bf16:\n        assert (\n            args.mixed_precision == \"bf16\"\n        ), \"full_bf16 requires mixed precision='bf16' / full_bf16を使う場合はmixed_precision='bf16'を指定してください。\"\n        accelerator.print(\"enable full bf16 training.\")\n        unet.to(weight_dtype)\n        network.to(weight_dtype)\n\n    # acceleratorがなんかよろしくやってくれるらしい\n    unet, network, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        unet, network, optimizer, train_dataloader, lr_scheduler\n    )\n    network: control_net_lllite.ControlNetLLLite\n\n    if args.gradient_checkpointing:\n        unet.train()  # according to TI example in Diffusers, train is required -> これオリジナルのU-Netしたので本当は外せる\n    else:\n        unet.eval()\n\n    network.prepare_grad_etc()\n\n    # TextEncoderの出力をキャッシュするときにはCPUへ移動する\n    if args.cache_text_encoder_outputs:\n        # move Text Encoders for sampling images. Text Encoder doesn't work on CPU with fp16\n        text_encoder1.to(\"cpu\", dtype=torch.float32)\n        text_encoder2.to(\"cpu\", dtype=torch.float32)\n        clean_memory_on_device(accelerator.device)\n    else:\n        # make sure Text Encoders are on GPU\n        text_encoder1.to(accelerator.device)\n        text_encoder2.to(accelerator.device)\n\n    if not cache_latents:\n        vae.requires_grad_(False)\n        vae.eval()\n        vae.to(accelerator.device, dtype=vae_dtype)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n    if args.full_fp16:\n        train_util.patch_accelerator_for_fp16_training(accelerator)\n\n    # resumeする\n    train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n    # epoch数を計算する\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n        args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n    # 学習する\n    # TODO: find a way to handle total batch size when there are multiple datasets\n    accelerator.print(\"running training / 学習開始\")\n    accelerator.print(f\"  num train images * repeats / 学習画像の数×繰り返し回数: {train_dataset_group.num_train_images}\")\n    accelerator.print(f\"  num reg images / 正則化画像の数: {train_dataset_group.num_reg_images}\")\n    accelerator.print(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n    accelerator.print(f\"  num epochs / epoch数: {num_train_epochs}\")\n    accelerator.print(\n        f\"  batch size per device / バッチサイズ: {', '.join([str(d.batch_size) for d in train_dataset_group.datasets])}\"\n    )\n    # logger.info(f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\")\n    accelerator.print(f\"  gradient accumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n    accelerator.print(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n    progress_bar = tqdm(range(args.max_train_steps), smoothing=0, disable=not accelerator.is_local_main_process, desc=\"steps\")\n    global_step = 0\n\n    noise_scheduler = DDPMScheduler(\n        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, clip_sample=False\n    )\n    prepare_scheduler_for_custom_training(noise_scheduler, accelerator.device)\n    if args.zero_terminal_snr:\n        custom_train_functions.fix_noise_scheduler_betas_for_zero_terminal_snr(noise_scheduler)\n\n    if accelerator.is_main_process:\n        init_kwargs = {}\n        if args.log_tracker_config is not None:\n            init_kwargs = toml.load(args.log_tracker_config)\n        accelerator.init_trackers(\n            \"lllite_control_net_train\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs\n        )\n\n    loss_recorder = train_util.LossRecorder()\n    del train_dataset_group\n\n    # function for saving/removing\n    def save_model(ckpt_name, unwrapped_nw, steps, epoch_no, force_sync_upload=False):\n        os.makedirs(args.output_dir, exist_ok=True)\n        ckpt_file = os.path.join(args.output_dir, ckpt_name)\n\n        accelerator.print(f\"\\nsaving checkpoint: {ckpt_file}\")\n        sai_metadata = train_util.get_sai_model_spec(None, args, True, True, False)\n        sai_metadata[\"modelspec.architecture\"] = sai_model_spec.ARCH_SD_XL_V1_BASE + \"/control-net-lllite\"\n\n        unwrapped_nw.save_weights(ckpt_file, save_dtype, sai_metadata)\n        if args.huggingface_repo_id is not None:\n            huggingface_util.upload(args, ckpt_file, \"/\" + ckpt_name, force_sync_upload=force_sync_upload)\n\n    def remove_model(old_ckpt_name):\n        old_ckpt_file = os.path.join(args.output_dir, old_ckpt_name)\n        if os.path.exists(old_ckpt_file):\n            accelerator.print(f\"removing old checkpoint: {old_ckpt_file}\")\n            os.remove(old_ckpt_file)\n\n    # training loop\n    for epoch in range(num_train_epochs):\n        accelerator.print(f\"\\nepoch {epoch+1}/{num_train_epochs}\")\n        current_epoch.value = epoch + 1\n\n        network.on_epoch_start()  # train()\n\n        for step, batch in enumerate(train_dataloader):\n            current_step.value = global_step\n            with accelerator.accumulate(network):\n                with torch.no_grad():\n                    if \"latents\" in batch and batch[\"latents\"] is not None:\n                        latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                    else:\n                        # latentに変換\n                        latents = vae.encode(batch[\"images\"].to(dtype=vae_dtype)).latent_dist.sample().to(dtype=weight_dtype)\n\n                        # NaNが含まれていれば警告を表示し0に置き換える\n                        if torch.any(torch.isnan(latents)):\n                            accelerator.print(\"NaN found in latents, replacing with zeros\")\n                            latents = torch.nan_to_num(latents, 0, out=latents)\n                    latents = latents * sdxl_model_util.VAE_SCALE_FACTOR\n\n                if \"text_encoder_outputs1_list\" not in batch or batch[\"text_encoder_outputs1_list\"] is None:\n                    input_ids1 = batch[\"input_ids\"]\n                    input_ids2 = batch[\"input_ids2\"]\n                    with torch.no_grad():\n                        # Get the text embedding for conditioning\n                        input_ids1 = input_ids1.to(accelerator.device)\n                        input_ids2 = input_ids2.to(accelerator.device)\n                        encoder_hidden_states1, encoder_hidden_states2, pool2 = train_util.get_hidden_states_sdxl(\n                            args.max_token_length,\n                            input_ids1,\n                            input_ids2,\n                            tokenizer1,\n                            tokenizer2,\n                            text_encoder1,\n                            text_encoder2,\n                            None if not args.full_fp16 else weight_dtype,\n                        )\n                else:\n                    encoder_hidden_states1 = batch[\"text_encoder_outputs1_list\"].to(accelerator.device).to(weight_dtype)\n                    encoder_hidden_states2 = batch[\"text_encoder_outputs2_list\"].to(accelerator.device).to(weight_dtype)\n                    pool2 = batch[\"text_encoder_pool2_list\"].to(accelerator.device).to(weight_dtype)\n\n                # get size embeddings\n                orig_size = batch[\"original_sizes_hw\"]\n                crop_size = batch[\"crop_top_lefts\"]\n                target_size = batch[\"target_sizes_hw\"]\n                embs = sdxl_train_util.get_size_embeddings(orig_size, crop_size, target_size, accelerator.device).to(weight_dtype)\n\n                # concat embeddings\n                vector_embedding = torch.cat([pool2, embs], dim=1).to(weight_dtype)\n                text_embedding = torch.cat([encoder_hidden_states1, encoder_hidden_states2], dim=2).to(weight_dtype)\n\n                # Sample noise, sample a random timestep for each image, and add noise to the latents,\n                # with noise offset and/or multires noise if specified\n                noise, noisy_latents, timesteps, huber_c = train_util.get_noise_noisy_latents_and_timesteps(args, noise_scheduler, latents)\n\n                noisy_latents = noisy_latents.to(weight_dtype)  # TODO check why noisy_latents is not weight_dtype\n\n                controlnet_image = batch[\"conditioning_images\"].to(dtype=weight_dtype)\n\n                with accelerator.autocast():\n                    # conditioning imageをControlNetに渡す / pass conditioning image to ControlNet\n                    # 内部でcond_embに変換される / it will be converted to cond_emb inside\n                    network.set_cond_image(controlnet_image)\n\n                    # それらの値を使いつつ、U-Netでノイズを予測する / predict noise with U-Net using those values\n                    noise_pred = unet(noisy_latents, timesteps, text_embedding, vector_embedding)\n\n                if args.v_parameterization:\n                    # v-parameterization training\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    target = noise\n\n                loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c)\n                loss = loss.mean([1, 2, 3])\n\n                loss_weights = batch[\"loss_weights\"]  # 各sampleごとのweight\n                loss = loss * loss_weights\n\n                if args.min_snr_gamma:\n                    loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma, args.v_parameterization)\n                if args.scale_v_pred_loss_like_noise_pred:\n                    loss = scale_v_prediction_loss_like_noise_prediction(loss, timesteps, noise_scheduler)\n                if args.v_pred_like_loss:\n                    loss = add_v_prediction_like_loss(loss, timesteps, noise_scheduler, args.v_pred_like_loss)\n                if args.debiased_estimation_loss:\n                    loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)\n\n                loss = loss.mean()  # 平均なのでbatch_sizeで割る必要なし\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients and args.max_grad_norm != 0.0:\n                    params_to_clip = network.get_trainable_params()\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                # sdxl_train_util.sample_images(accelerator, args, None, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n                # 指定ステップごとにモデルを保存\n                if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                    accelerator.wait_for_everyone()\n                    if accelerator.is_main_process:\n                        ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, global_step)\n                        save_model(ckpt_name, accelerator.unwrap_model(network), global_step, epoch)\n\n                        if args.save_state:\n                            train_util.save_and_remove_state_stepwise(args, accelerator, global_step)\n\n                        remove_step_no = train_util.get_remove_step_no(args, global_step)\n                        if remove_step_no is not None:\n                            remove_ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, remove_step_no)\n                            remove_model(remove_ckpt_name)\n\n            current_loss = loss.detach().item()\n            loss_recorder.add(epoch=epoch, step=step, loss=current_loss)\n            avr_loss: float = loss_recorder.moving_average\n            logs = {\"avr_loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if args.logging_dir is not None:\n                logs = generate_step_logs(args, current_loss, avr_loss, lr_scheduler)\n                accelerator.log(logs, step=global_step)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        if args.logging_dir is not None:\n            logs = {\"loss/epoch\": loss_recorder.moving_average}\n            accelerator.log(logs, step=epoch + 1)\n\n        accelerator.wait_for_everyone()\n\n        # 指定エポックごとにモデルを保存\n        if args.save_every_n_epochs is not None:\n            saving = (epoch + 1) % args.save_every_n_epochs == 0 and (epoch + 1) < num_train_epochs\n            if is_main_process and saving:\n                ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, epoch + 1)\n                save_model(ckpt_name, accelerator.unwrap_model(network), global_step, epoch + 1)\n\n                remove_epoch_no = train_util.get_remove_epoch_no(args, epoch + 1)\n                if remove_epoch_no is not None:\n                    remove_ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, remove_epoch_no)\n                    remove_model(remove_ckpt_name)\n\n                if args.save_state:\n                    train_util.save_and_remove_state_on_epoch_end(args, accelerator, epoch + 1)\n\n        # self.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n        # end of epoch\n\n    if is_main_process:\n        network = accelerator.unwrap_model(network)\n\n    accelerator.end_training()\n\n    if is_main_process and args.save_state:\n        train_util.save_state_on_train_end(args, accelerator)\n\n    if is_main_process:\n        ckpt_name = train_util.get_last_ckpt_name(args, \".\" + args.save_model_as)\n        save_model(ckpt_name, network, global_step, num_train_epochs, force_sync_upload=True)\n\n        logger.info(\"model saved.\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, False, True, True)\n    train_util.add_training_arguments(parser, False)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser)\n    sdxl_train_util.add_sdxl_training_arguments(parser)\n\n    parser.add_argument(\n        \"--save_model_as\",\n        type=str,\n        default=\"safetensors\",\n        choices=[None, \"ckpt\", \"pt\", \"safetensors\"],\n        help=\"format to save the model (default is .safetensors) / モデル保存時の形式（デフォルトはsafetensors）\",\n    )\n    parser.add_argument(\n        \"--cond_emb_dim\", type=int, default=None, help=\"conditioning embedding dimension / 条件付け埋め込みの次元数\"\n    )\n    parser.add_argument(\n        \"--network_weights\", type=str, default=None, help=\"pretrained weights for network / 学習するネットワークの初期重み\"\n    )\n    parser.add_argument(\"--network_dim\", type=int, default=None, help=\"network dimensions (rank) / モジュールの次元数\")\n    parser.add_argument(\n        \"--network_dropout\",\n        type=float,\n        default=None,\n        help=\"Drops neurons out of training every step (0 or None is default behavior (no dropout), 1 would drop all neurons) / 訓練時に毎ステップでニューロンをdropする（0またはNoneはdropoutなし、1は全ニューロンをdropout）\",\n    )\n    parser.add_argument(\n        \"--conditioning_data_dir\",\n        type=str,\n        default=None,\n        help=\"conditioning data directory / 条件付けデータのディレクトリ\",\n    )\n    parser.add_argument(\n        \"--no_half_vae\",\n        action=\"store_true\",\n        help=\"do not use fp16/bf16 VAE in mixed precision (use float VAE) / mixed precisionでも fp16/bf16 VAEを使わずfloat VAEを使う\",\n    )\n    return parser\n\n\nif __name__ == \"__main__\":\n    # sdxl_original_unet.USE_REENTRANT = False\n\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    train(args)\n"
        },
        {
          "name": "sdxl_train_network.py",
          "type": "blob",
          "size": 8.3525390625,
          "content": "import argparse\n\nimport torch\nfrom library.device_utils import init_ipex, clean_memory_on_device\ninit_ipex()\n\nfrom library import sdxl_model_util, sdxl_train_util, train_util\nimport train_network\nfrom library.utils import setup_logging\nsetup_logging()\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass SdxlNetworkTrainer(train_network.NetworkTrainer):\n    def __init__(self):\n        super().__init__()\n        self.vae_scale_factor = sdxl_model_util.VAE_SCALE_FACTOR\n        self.is_sdxl = True\n\n    def assert_extra_args(self, args, train_dataset_group):\n        super().assert_extra_args(args, train_dataset_group)\n        sdxl_train_util.verify_sdxl_training_args(args)\n\n        if args.cache_text_encoder_outputs:\n            assert (\n                train_dataset_group.is_text_encoder_output_cacheable()\n            ), \"when caching Text Encoder output, either caption_dropout_rate, shuffle_caption, token_warmup_step or caption_tag_dropout_rate cannot be used / Text Encoderの出力をキャッシュするときはcaption_dropout_rate, shuffle_caption, token_warmup_step, caption_tag_dropout_rateは使えません\"\n\n        assert (\n            args.network_train_unet_only or not args.cache_text_encoder_outputs\n        ), \"network for Text Encoder cannot be trained with caching Text Encoder outputs / Text Encoderの出力をキャッシュしながらText Encoderのネットワークを学習することはできません\"\n\n        train_dataset_group.verify_bucket_reso_steps(32)\n\n    def load_target_model(self, args, weight_dtype, accelerator):\n        (\n            load_stable_diffusion_format,\n            text_encoder1,\n            text_encoder2,\n            vae,\n            unet,\n            logit_scale,\n            ckpt_info,\n        ) = sdxl_train_util.load_target_model(args, accelerator, sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, weight_dtype)\n\n        self.load_stable_diffusion_format = load_stable_diffusion_format\n        self.logit_scale = logit_scale\n        self.ckpt_info = ckpt_info\n\n        return sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, [text_encoder1, text_encoder2], vae, unet\n\n    def load_tokenizer(self, args):\n        tokenizer = sdxl_train_util.load_tokenizers(args)\n        return tokenizer\n\n    def is_text_encoder_outputs_cached(self, args):\n        return args.cache_text_encoder_outputs\n\n    def cache_text_encoder_outputs_if_needed(\n        self, args, accelerator, unet, vae, tokenizers, text_encoders, dataset: train_util.DatasetGroup, weight_dtype\n    ):\n        if args.cache_text_encoder_outputs:\n            if not args.lowram:\n                # メモリ消費を減らす\n                logger.info(\"move vae and unet to cpu to save memory\")\n                org_vae_device = vae.device\n                org_unet_device = unet.device\n                vae.to(\"cpu\")\n                unet.to(\"cpu\")\n                clean_memory_on_device(accelerator.device)\n\n            # When TE is not be trained, it will not be prepared so we need to use explicit autocast\n            with accelerator.autocast():\n                dataset.cache_text_encoder_outputs(\n                    tokenizers,\n                    text_encoders,\n                    accelerator.device,\n                    weight_dtype,\n                    args.cache_text_encoder_outputs_to_disk,\n                    accelerator.is_main_process,\n                )\n\n            text_encoders[0].to(\"cpu\", dtype=torch.float32)  # Text Encoder doesn't work with fp16 on CPU\n            text_encoders[1].to(\"cpu\", dtype=torch.float32)\n            clean_memory_on_device(accelerator.device)\n\n            if not args.lowram:\n                logger.info(\"move vae and unet back to original device\")\n                vae.to(org_vae_device)\n                unet.to(org_unet_device)\n        else:\n            # Text Encoderから毎回出力を取得するので、GPUに乗せておく\n            text_encoders[0].to(accelerator.device, dtype=weight_dtype)\n            text_encoders[1].to(accelerator.device, dtype=weight_dtype)\n\n    def get_text_cond(self, args, accelerator, batch, tokenizers, text_encoders, weight_dtype):\n        if \"text_encoder_outputs1_list\" not in batch or batch[\"text_encoder_outputs1_list\"] is None:\n            input_ids1 = batch[\"input_ids\"]\n            input_ids2 = batch[\"input_ids2\"]\n            with torch.enable_grad():\n                # Get the text embedding for conditioning\n                # TODO support weighted captions\n                # if args.weighted_captions:\n                #     encoder_hidden_states = get_weighted_text_embeddings(\n                #         tokenizer,\n                #         text_encoder,\n                #         batch[\"captions\"],\n                #         accelerator.device,\n                #         args.max_token_length // 75 if args.max_token_length else 1,\n                #         clip_skip=args.clip_skip,\n                #     )\n                # else:\n                input_ids1 = input_ids1.to(accelerator.device)\n                input_ids2 = input_ids2.to(accelerator.device)\n                encoder_hidden_states1, encoder_hidden_states2, pool2 = train_util.get_hidden_states_sdxl(\n                    args.max_token_length,\n                    input_ids1,\n                    input_ids2,\n                    tokenizers[0],\n                    tokenizers[1],\n                    text_encoders[0],\n                    text_encoders[1],\n                    None if not args.full_fp16 else weight_dtype,\n                    accelerator=accelerator,\n                )\n        else:\n            encoder_hidden_states1 = batch[\"text_encoder_outputs1_list\"].to(accelerator.device).to(weight_dtype)\n            encoder_hidden_states2 = batch[\"text_encoder_outputs2_list\"].to(accelerator.device).to(weight_dtype)\n            pool2 = batch[\"text_encoder_pool2_list\"].to(accelerator.device).to(weight_dtype)\n\n            # # verify that the text encoder outputs are correct\n            # ehs1, ehs2, p2 = train_util.get_hidden_states_sdxl(\n            #     args.max_token_length,\n            #     batch[\"input_ids\"].to(text_encoders[0].device),\n            #     batch[\"input_ids2\"].to(text_encoders[0].device),\n            #     tokenizers[0],\n            #     tokenizers[1],\n            #     text_encoders[0],\n            #     text_encoders[1],\n            #     None if not args.full_fp16 else weight_dtype,\n            # )\n            # b_size = encoder_hidden_states1.shape[0]\n            # assert ((encoder_hidden_states1.to(\"cpu\") - ehs1.to(dtype=weight_dtype)).abs().max() > 1e-2).sum() <= b_size * 2\n            # assert ((encoder_hidden_states2.to(\"cpu\") - ehs2.to(dtype=weight_dtype)).abs().max() > 1e-2).sum() <= b_size * 2\n            # assert ((pool2.to(\"cpu\") - p2.to(dtype=weight_dtype)).abs().max() > 1e-2).sum() <= b_size * 2\n            # logger.info(\"text encoder outputs verified\")\n\n        return encoder_hidden_states1, encoder_hidden_states2, pool2\n\n    def call_unet(self, args, accelerator, unet, noisy_latents, timesteps, text_conds, batch, weight_dtype):\n        noisy_latents = noisy_latents.to(weight_dtype)  # TODO check why noisy_latents is not weight_dtype\n\n        # get size embeddings\n        orig_size = batch[\"original_sizes_hw\"]\n        crop_size = batch[\"crop_top_lefts\"]\n        target_size = batch[\"target_sizes_hw\"]\n        embs = sdxl_train_util.get_size_embeddings(orig_size, crop_size, target_size, accelerator.device).to(weight_dtype)\n\n        # concat embeddings\n        encoder_hidden_states1, encoder_hidden_states2, pool2 = text_conds\n        vector_embedding = torch.cat([pool2, embs], dim=1).to(weight_dtype)\n        text_embedding = torch.cat([encoder_hidden_states1, encoder_hidden_states2], dim=2).to(weight_dtype)\n\n        noise_pred = unet(noisy_latents, timesteps, text_embedding, vector_embedding)\n        return noise_pred\n\n    def sample_images(self, accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet):\n        sdxl_train_util.sample_images(accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet)\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = train_network.setup_parser()\n    sdxl_train_util.add_sdxl_training_arguments(parser)\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    trainer = SdxlNetworkTrainer()\n    trainer.train(args)\n"
        },
        {
          "name": "sdxl_train_textual_inversion.py",
          "type": "blob",
          "size": 5.19140625,
          "content": "import argparse\nimport os\n\nimport regex\n\nimport torch\nfrom library.device_utils import init_ipex\ninit_ipex()\n\nfrom library import sdxl_model_util, sdxl_train_util, train_util\n\nimport train_textual_inversion\n\n\nclass SdxlTextualInversionTrainer(train_textual_inversion.TextualInversionTrainer):\n    def __init__(self):\n        super().__init__()\n        self.vae_scale_factor = sdxl_model_util.VAE_SCALE_FACTOR\n        self.is_sdxl = True\n\n    def assert_extra_args(self, args, train_dataset_group):\n        super().assert_extra_args(args, train_dataset_group)\n        sdxl_train_util.verify_sdxl_training_args(args, supportTextEncoderCaching=False)\n\n        train_dataset_group.verify_bucket_reso_steps(32)\n\n    def load_target_model(self, args, weight_dtype, accelerator):\n        (\n            load_stable_diffusion_format,\n            text_encoder1,\n            text_encoder2,\n            vae,\n            unet,\n            logit_scale,\n            ckpt_info,\n        ) = sdxl_train_util.load_target_model(args, accelerator, sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, weight_dtype)\n\n        self.load_stable_diffusion_format = load_stable_diffusion_format\n        self.logit_scale = logit_scale\n        self.ckpt_info = ckpt_info\n\n        return sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, [text_encoder1, text_encoder2], vae, unet\n\n    def load_tokenizer(self, args):\n        tokenizer = sdxl_train_util.load_tokenizers(args)\n        return tokenizer\n\n    def get_text_cond(self, args, accelerator, batch, tokenizers, text_encoders, weight_dtype):\n        input_ids1 = batch[\"input_ids\"]\n        input_ids2 = batch[\"input_ids2\"]\n        with torch.enable_grad():\n            input_ids1 = input_ids1.to(accelerator.device)\n            input_ids2 = input_ids2.to(accelerator.device)\n            encoder_hidden_states1, encoder_hidden_states2, pool2 = train_util.get_hidden_states_sdxl(\n                args.max_token_length,\n                input_ids1,\n                input_ids2,\n                tokenizers[0],\n                tokenizers[1],\n                text_encoders[0],\n                text_encoders[1],\n                None if not args.full_fp16 else weight_dtype,\n                accelerator=accelerator,\n            )\n        return encoder_hidden_states1, encoder_hidden_states2, pool2\n\n    def call_unet(self, args, accelerator, unet, noisy_latents, timesteps, text_conds, batch, weight_dtype):\n        noisy_latents = noisy_latents.to(weight_dtype)  # TODO check why noisy_latents is not weight_dtype\n\n        # get size embeddings\n        orig_size = batch[\"original_sizes_hw\"]\n        crop_size = batch[\"crop_top_lefts\"]\n        target_size = batch[\"target_sizes_hw\"]\n        embs = sdxl_train_util.get_size_embeddings(orig_size, crop_size, target_size, accelerator.device).to(weight_dtype)\n\n        # concat embeddings\n        encoder_hidden_states1, encoder_hidden_states2, pool2 = text_conds\n        vector_embedding = torch.cat([pool2, embs], dim=1).to(weight_dtype)\n        text_embedding = torch.cat([encoder_hidden_states1, encoder_hidden_states2], dim=2).to(weight_dtype)\n\n        noise_pred = unet(noisy_latents, timesteps, text_embedding, vector_embedding)\n        return noise_pred\n\n    def sample_images(self, accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet, prompt_replacement):\n        sdxl_train_util.sample_images(\n            accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet, prompt_replacement\n        )\n\n    def save_weights(self, file, updated_embs, save_dtype, metadata):\n        state_dict = {\"clip_l\": updated_embs[0], \"clip_g\": updated_embs[1]}\n\n        if save_dtype is not None:\n            for key in list(state_dict.keys()):\n                v = state_dict[key]\n                v = v.detach().clone().to(\"cpu\").to(save_dtype)\n                state_dict[key] = v\n\n        if os.path.splitext(file)[1] == \".safetensors\":\n            from safetensors.torch import save_file\n\n            save_file(state_dict, file, metadata)\n        else:\n            torch.save(state_dict, file)\n\n    def load_weights(self, file):\n        if os.path.splitext(file)[1] == \".safetensors\":\n            from safetensors.torch import load_file\n\n            data = load_file(file)\n        else:\n            data = torch.load(file, map_location=\"cpu\")\n\n        emb_l = data.get(\"clip_l\", None)  # ViT-L text encoder 1\n        emb_g = data.get(\"clip_g\", None)  # BiG-G text encoder 2\n\n        assert (\n            emb_l is not None or emb_g is not None\n        ), f\"weight file does not contains weights for text encoder 1 or 2 / 重みファイルにテキストエンコーダー1または2の重みが含まれていません: {file}\"\n\n        return [emb_l, emb_g]\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = train_textual_inversion.setup_parser()\n    # don't add sdxl_train_util.add_sdxl_training_arguments(parser): because it only adds text encoder caching\n    # sdxl_train_util.add_sdxl_training_arguments(parser)\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    trainer = SdxlTextualInversionTrainer()\n    trainer.train(args)\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.0947265625,
          "content": "from setuptools import setup, find_packages\n \nsetup(name = \"library\", packages = find_packages())"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "train_controlnet.py",
          "type": "blob",
          "size": 24.6142578125,
          "content": "import argparse\nimport json\nimport math\nimport os\nimport random\nimport time\nfrom multiprocessing import Value\nfrom types import SimpleNamespace\nimport toml\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library import deepspeed_utils\nfrom library.device_utils import init_ipex, clean_memory_on_device\ninit_ipex()\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom accelerate.utils import set_seed\nfrom diffusers import DDPMScheduler, ControlNetModel\nfrom safetensors.torch import load_file\n\nimport library.model_util as model_util\nimport library.train_util as train_util\nimport library.config_util as config_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.huggingface_util as huggingface_util\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    apply_snr_weight,\n    pyramid_noise_like,\n    apply_noise_offset,\n)\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO 他のスクリプトと共通化する\ndef generate_step_logs(args: argparse.Namespace, current_loss, avr_loss, lr_scheduler):\n    logs = {\n        \"loss/current\": current_loss,\n        \"loss/average\": avr_loss,\n        \"lr\": lr_scheduler.get_last_lr()[0],\n    }\n\n    if args.optimizer_type.lower().startswith(\"DAdapt\".lower()):\n        logs[\"lr/d*lr\"] = lr_scheduler.optimizers[-1].param_groups[0][\"d\"] * lr_scheduler.optimizers[-1].param_groups[0][\"lr\"]\n\n    return logs\n\n\ndef train(args):\n    # session_id = random.randint(0, 2**32)\n    # training_started_at = time.time()\n    train_util.verify_training_args(args)\n    train_util.prepare_dataset_args(args, True)\n    setup_logging(args, reset=True)\n\n    cache_latents = args.cache_latents\n    use_user_config = args.dataset_config is not None\n\n    if args.seed is None:\n        args.seed = random.randint(0, 2**32)\n    set_seed(args.seed)\n\n    tokenizer = train_util.load_tokenizer(args)\n\n    # データセットを準備する\n    blueprint_generator = BlueprintGenerator(ConfigSanitizer(False, False, True, True))\n    if use_user_config:\n        logger.info(f\"Load dataset config from {args.dataset_config}\")\n        user_config = config_util.load_user_config(args.dataset_config)\n        ignored = [\"train_data_dir\", \"conditioning_data_dir\"]\n        if any(getattr(args, attr) is not None for attr in ignored):\n            logger.warning(\n                \"ignore following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                    \", \".join(ignored)\n                )\n            )\n    else:\n        user_config = {\n            \"datasets\": [\n                {\n                    \"subsets\": config_util.generate_controlnet_subsets_config_by_subdirs(\n                        args.train_data_dir,\n                        args.conditioning_data_dir,\n                        args.caption_extension,\n                    )\n                }\n            ]\n        }\n\n    blueprint = blueprint_generator.generate(user_config, args, tokenizer=tokenizer)\n    train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n\n    current_epoch = Value(\"i\", 0)\n    current_step = Value(\"i\", 0)\n    ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n    collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n    if args.debug_dataset:\n        train_util.debug_dataset(train_dataset_group)\n        return\n    if len(train_dataset_group) == 0:\n        logger.error(\n            \"No data found. Please verify arguments (train_data_dir must be the parent of folders with images) / 画像がありません。引数指定を確認してください（train_data_dirには画像があるフォルダではなく、画像があるフォルダの親フォルダを指定する必要があります）\"\n        )\n        return\n\n    if cache_latents:\n        assert (\n            train_dataset_group.is_latent_cacheable()\n        ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n\n    # acceleratorを準備する\n    logger.info(\"prepare accelerator\")\n    accelerator = train_util.prepare_accelerator(args)\n    is_main_process = accelerator.is_main_process\n\n    # mixed precisionに対応した型を用意しておき適宜castする\n    weight_dtype, save_dtype = train_util.prepare_dtype(args)\n\n    # モデルを読み込む\n    text_encoder, vae, unet, _ = train_util.load_target_model(\n        args, weight_dtype, accelerator, unet_use_linear_projection_in_v2=True\n    )\n\n    # DiffusersのControlNetが使用するデータを準備する\n    if args.v2:\n        unet.config = {\n            \"act_fn\": \"silu\",\n            \"attention_head_dim\": [5, 10, 20, 20],\n            \"block_out_channels\": [320, 640, 1280, 1280],\n            \"center_input_sample\": False,\n            \"cross_attention_dim\": 1024,\n            \"down_block_types\": [\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"],\n            \"downsample_padding\": 1,\n            \"dual_cross_attention\": False,\n            \"flip_sin_to_cos\": True,\n            \"freq_shift\": 0,\n            \"in_channels\": 4,\n            \"layers_per_block\": 2,\n            \"mid_block_scale_factor\": 1,\n            \"norm_eps\": 1e-05,\n            \"norm_num_groups\": 32,\n            \"num_class_embeds\": None,\n            \"only_cross_attention\": False,\n            \"out_channels\": 4,\n            \"sample_size\": 96,\n            \"up_block_types\": [\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"],\n            \"use_linear_projection\": True,\n            \"upcast_attention\": True,\n            \"only_cross_attention\": False,\n            \"downsample_padding\": 1,\n            \"use_linear_projection\": True,\n            \"class_embed_type\": None,\n            \"num_class_embeds\": None,\n            \"resnet_time_scale_shift\": \"default\",\n            \"projection_class_embeddings_input_dim\": None,\n        }\n    else:\n        unet.config = {\n            \"act_fn\": \"silu\",\n            \"attention_head_dim\": 8,\n            \"block_out_channels\": [320, 640, 1280, 1280],\n            \"center_input_sample\": False,\n            \"cross_attention_dim\": 768,\n            \"down_block_types\": [\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"],\n            \"downsample_padding\": 1,\n            \"flip_sin_to_cos\": True,\n            \"freq_shift\": 0,\n            \"in_channels\": 4,\n            \"layers_per_block\": 2,\n            \"mid_block_scale_factor\": 1,\n            \"norm_eps\": 1e-05,\n            \"norm_num_groups\": 32,\n            \"out_channels\": 4,\n            \"sample_size\": 64,\n            \"up_block_types\": [\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"],\n            \"only_cross_attention\": False,\n            \"downsample_padding\": 1,\n            \"use_linear_projection\": False,\n            \"class_embed_type\": None,\n            \"num_class_embeds\": None,\n            \"upcast_attention\": False,\n            \"resnet_time_scale_shift\": \"default\",\n            \"projection_class_embeddings_input_dim\": None,\n        }\n    unet.config = SimpleNamespace(**unet.config)\n\n    controlnet = ControlNetModel.from_unet(unet)\n\n    if args.controlnet_model_name_or_path:\n        filename = args.controlnet_model_name_or_path\n        if os.path.isfile(filename):\n            if os.path.splitext(filename)[1] == \".safetensors\":\n                state_dict = load_file(filename)\n            else:\n                state_dict = torch.load(filename)\n            state_dict = model_util.convert_controlnet_state_dict_to_diffusers(state_dict)\n            controlnet.load_state_dict(state_dict)\n        elif os.path.isdir(filename):\n            controlnet = ControlNetModel.from_pretrained(filename)\n\n    # モデルに xformers とか memory efficient attention を組み込む\n    train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n\n    # 学習を準備する\n    if cache_latents:\n        vae.to(accelerator.device, dtype=weight_dtype)\n        vae.requires_grad_(False)\n        vae.eval()\n        with torch.no_grad():\n            train_dataset_group.cache_latents(\n                vae,\n                args.vae_batch_size,\n                args.cache_latents_to_disk,\n                accelerator.is_main_process,\n            )\n        vae.to(\"cpu\")\n        clean_memory_on_device(accelerator.device)\n        \n        accelerator.wait_for_everyone()\n\n    if args.gradient_checkpointing:\n        controlnet.enable_gradient_checkpointing()\n\n    # 学習に必要なクラスを準備する\n    accelerator.print(\"prepare optimizer, data loader etc.\")\n\n    trainable_params = controlnet.parameters()\n\n    _, _, optimizer = train_util.get_optimizer(args, trainable_params)\n\n    # dataloaderを準備する\n    # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n    n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset_group,\n        batch_size=1,\n        shuffle=True,\n        collate_fn=collator,\n        num_workers=n_workers,\n        persistent_workers=args.persistent_data_loader_workers,\n    )\n\n    # 学習ステップ数を計算する\n    if args.max_train_epochs is not None:\n        args.max_train_steps = args.max_train_epochs * math.ceil(\n            len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n        )\n        accelerator.print(\n            f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n        )\n\n    # データセット側にも学習ステップを送信\n    train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n    # lr schedulerを用意する\n    lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　モデル全体をfp16にする\n    if args.full_fp16:\n        assert (\n            args.mixed_precision == \"fp16\"\n        ), \"full_fp16 requires mixed precision='fp16' / full_fp16を使う場合はmixed_precision='fp16'を指定してください。\"\n        accelerator.print(\"enable full fp16 training.\")\n        controlnet.to(weight_dtype)\n\n    # acceleratorがなんかよろしくやってくれるらしい\n    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        controlnet, optimizer, train_dataloader, lr_scheduler\n    )\n\n    unet.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    unet.to(accelerator.device)\n    text_encoder.to(accelerator.device)\n\n    # transform DDP after prepare\n    controlnet = controlnet.module if isinstance(controlnet, DDP) else controlnet\n\n    controlnet.train()\n\n    if not cache_latents:\n        vae.requires_grad_(False)\n        vae.eval()\n        vae.to(accelerator.device, dtype=weight_dtype)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n    if args.full_fp16:\n        train_util.patch_accelerator_for_fp16_training(accelerator)\n\n    # resumeする\n    train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n    # epoch数を計算する\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n        args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n    # 学習する\n    # TODO: find a way to handle total batch size when there are multiple datasets\n    accelerator.print(\"running training / 学習開始\")\n    accelerator.print(f\"  num train images * repeats / 学習画像の数×繰り返し回数: {train_dataset_group.num_train_images}\")\n    accelerator.print(f\"  num reg images / 正則化画像の数: {train_dataset_group.num_reg_images}\")\n    accelerator.print(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n    accelerator.print(f\"  num epochs / epoch数: {num_train_epochs}\")\n    accelerator.print(\n        f\"  batch size per device / バッチサイズ: {', '.join([str(d.batch_size) for d in train_dataset_group.datasets])}\"\n    )\n    # logger.info(f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\")\n    accelerator.print(f\"  gradient accumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n    accelerator.print(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n    progress_bar = tqdm(\n        range(args.max_train_steps),\n        smoothing=0,\n        disable=not accelerator.is_local_main_process,\n        desc=\"steps\",\n    )\n    global_step = 0\n\n    noise_scheduler = DDPMScheduler(\n        beta_start=0.00085,\n        beta_end=0.012,\n        beta_schedule=\"scaled_linear\",\n        num_train_timesteps=1000,\n        clip_sample=False,\n    )\n    if accelerator.is_main_process:\n        init_kwargs = {}\n        if args.wandb_run_name:\n            init_kwargs[\"wandb\"] = {\"name\": args.wandb_run_name}\n        if args.log_tracker_config is not None:\n            init_kwargs = toml.load(args.log_tracker_config)\n        accelerator.init_trackers(\n            \"controlnet_train\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs\n        )\n\n    loss_recorder = train_util.LossRecorder()\n    del train_dataset_group\n\n    # function for saving/removing\n    def save_model(ckpt_name, model, force_sync_upload=False):\n        os.makedirs(args.output_dir, exist_ok=True)\n        ckpt_file = os.path.join(args.output_dir, ckpt_name)\n\n        accelerator.print(f\"\\nsaving checkpoint: {ckpt_file}\")\n\n        state_dict = model_util.convert_controlnet_state_dict_to_sd(model.state_dict())\n\n        if save_dtype is not None:\n            for key in list(state_dict.keys()):\n                v = state_dict[key]\n                v = v.detach().clone().to(\"cpu\").to(save_dtype)\n                state_dict[key] = v\n\n        if os.path.splitext(ckpt_file)[1] == \".safetensors\":\n            from safetensors.torch import save_file\n\n            save_file(state_dict, ckpt_file)\n        else:\n            torch.save(state_dict, ckpt_file)\n\n        if args.huggingface_repo_id is not None:\n            huggingface_util.upload(args, ckpt_file, \"/\" + ckpt_name, force_sync_upload=force_sync_upload)\n\n    def remove_model(old_ckpt_name):\n        old_ckpt_file = os.path.join(args.output_dir, old_ckpt_name)\n        if os.path.exists(old_ckpt_file):\n            accelerator.print(f\"removing old checkpoint: {old_ckpt_file}\")\n            os.remove(old_ckpt_file)\n\n    # For --sample_at_first\n    train_util.sample_images(\n        accelerator, args, 0, global_step, accelerator.device, vae, tokenizer, text_encoder, unet, controlnet=controlnet\n    )\n\n    # training loop\n    for epoch in range(num_train_epochs):\n        if is_main_process:\n            accelerator.print(f\"\\nepoch {epoch+1}/{num_train_epochs}\")\n        current_epoch.value = epoch + 1\n\n        for step, batch in enumerate(train_dataloader):\n            current_step.value = global_step\n            with accelerator.accumulate(controlnet):\n                with torch.no_grad():\n                    if \"latents\" in batch and batch[\"latents\"] is not None:\n                        latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                    else:\n                        # latentに変換\n                        latents = vae.encode(batch[\"images\"].to(dtype=weight_dtype)).latent_dist.sample()\n                    latents = latents * 0.18215\n                b_size = latents.shape[0]\n\n                input_ids = batch[\"input_ids\"].to(accelerator.device)\n                encoder_hidden_states = train_util.get_hidden_states(args, input_ids, tokenizer, text_encoder, weight_dtype)\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn_like(latents, device=latents.device)\n                if args.noise_offset:\n                    noise = apply_noise_offset(latents, noise, args.noise_offset, args.adaptive_noise_scale)\n                elif args.multires_noise_iterations:\n                    noise = pyramid_noise_like(\n                        noise,\n                        latents.device,\n                        args.multires_noise_iterations,\n                        args.multires_noise_discount,\n                    )\n\n                # Sample a random timestep for each image\n                timesteps, huber_c = train_util.get_timesteps_and_huber_c(args, 0, noise_scheduler.config.num_train_timesteps, noise_scheduler, b_size, latents.device)\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                controlnet_image = batch[\"conditioning_images\"].to(dtype=weight_dtype)\n\n                with accelerator.autocast():\n                    down_block_res_samples, mid_block_res_sample = controlnet(\n                        noisy_latents,\n                        timesteps,\n                        encoder_hidden_states=encoder_hidden_states,\n                        controlnet_cond=controlnet_image,\n                        return_dict=False,\n                    )\n\n                    # Predict the noise residual\n                    noise_pred = unet(\n                        noisy_latents,\n                        timesteps,\n                        encoder_hidden_states,\n                        down_block_additional_residuals=[sample.to(dtype=weight_dtype) for sample in down_block_res_samples],\n                        mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),\n                    ).sample\n\n                if args.v_parameterization:\n                    # v-parameterization training\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    target = noise\n\n                loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c)\n                loss = loss.mean([1, 2, 3])\n\n                loss_weights = batch[\"loss_weights\"]  # 各sampleごとのweight\n                loss = loss * loss_weights\n\n                if args.min_snr_gamma:\n                    loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma, args.v_parameterization)\n\n                loss = loss.mean()  # 平均なのでbatch_sizeで割る必要なし\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients and args.max_grad_norm != 0.0:\n                    params_to_clip = controlnet.parameters()\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                train_util.sample_images(\n                    accelerator,\n                    args,\n                    None,\n                    global_step,\n                    accelerator.device,\n                    vae,\n                    tokenizer,\n                    text_encoder,\n                    unet,\n                    controlnet=controlnet,\n                )\n\n                # 指定ステップごとにモデルを保存\n                if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                    accelerator.wait_for_everyone()\n                    if accelerator.is_main_process:\n                        ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, global_step)\n                        save_model(\n                            ckpt_name,\n                            accelerator.unwrap_model(controlnet),\n                        )\n\n                        if args.save_state:\n                            train_util.save_and_remove_state_stepwise(args, accelerator, global_step)\n\n                        remove_step_no = train_util.get_remove_step_no(args, global_step)\n                        if remove_step_no is not None:\n                            remove_ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, remove_step_no)\n                            remove_model(remove_ckpt_name)\n\n            current_loss = loss.detach().item()\n            loss_recorder.add(epoch=epoch, step=step, loss=current_loss)\n            avr_loss: float = loss_recorder.moving_average\n            logs = {\"avr_loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if args.logging_dir is not None:\n                logs = generate_step_logs(args, current_loss, avr_loss, lr_scheduler)\n                accelerator.log(logs, step=global_step)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        if args.logging_dir is not None:\n            logs = {\"loss/epoch\": loss_recorder.moving_average}\n            accelerator.log(logs, step=epoch + 1)\n\n        accelerator.wait_for_everyone()\n\n        # 指定エポックごとにモデルを保存\n        if args.save_every_n_epochs is not None:\n            saving = (epoch + 1) % args.save_every_n_epochs == 0 and (epoch + 1) < num_train_epochs\n            if is_main_process and saving:\n                ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, epoch + 1)\n                save_model(ckpt_name, accelerator.unwrap_model(controlnet))\n\n                remove_epoch_no = train_util.get_remove_epoch_no(args, epoch + 1)\n                if remove_epoch_no is not None:\n                    remove_ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, remove_epoch_no)\n                    remove_model(remove_ckpt_name)\n\n                if args.save_state:\n                    train_util.save_and_remove_state_on_epoch_end(args, accelerator, epoch + 1)\n\n        train_util.sample_images(\n            accelerator,\n            args,\n            epoch + 1,\n            global_step,\n            accelerator.device,\n            vae,\n            tokenizer,\n            text_encoder,\n            unet,\n            controlnet=controlnet,\n        )\n\n        # end of epoch\n    if is_main_process:\n        controlnet = accelerator.unwrap_model(controlnet)\n\n    accelerator.end_training()\n\n    if is_main_process and (args.save_state or args.save_state_on_train_end):\n        train_util.save_state_on_train_end(args, accelerator)\n\n    # del accelerator  # この後メモリを使うのでこれは消す→printで使うので消さずにおく\n\n    if is_main_process:\n        ckpt_name = train_util.get_last_ckpt_name(args, \".\" + args.save_model_as)\n        save_model(ckpt_name, controlnet, force_sync_upload=True)\n\n        logger.info(\"model saved.\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, False, True, True)\n    train_util.add_training_arguments(parser, False)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser)\n\n    parser.add_argument(\n        \"--save_model_as\",\n        type=str,\n        default=\"safetensors\",\n        choices=[None, \"ckpt\", \"pt\", \"safetensors\"],\n        help=\"format to save the model (default is .safetensors) / モデル保存時の形式（デフォルトはsafetensors）\",\n    )\n    parser.add_argument(\n        \"--controlnet_model_name_or_path\",\n        type=str,\n        default=None,\n        help=\"controlnet model name or path / controlnetのモデル名またはパス\",\n    )\n    parser.add_argument(\n        \"--conditioning_data_dir\",\n        type=str,\n        default=None,\n        help=\"conditioning data directory / 条件付けデータのディレクトリ\",\n    )\n\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    train(args)\n"
        },
        {
          "name": "train_db.py",
          "type": "blob",
          "size": 23.123046875,
          "content": "# DreamBooth training\n# XXX dropped option: fine_tune\n\nimport argparse\nimport itertools\nimport math\nimport os\nfrom multiprocessing import Value\nimport toml\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library import deepspeed_utils\nfrom library.device_utils import init_ipex, clean_memory_on_device\n\n\ninit_ipex()\n\nfrom accelerate.utils import set_seed\nfrom diffusers import DDPMScheduler\n\nimport library.train_util as train_util\nimport library.config_util as config_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    apply_snr_weight,\n    get_weighted_text_embeddings,\n    prepare_scheduler_for_custom_training,\n    pyramid_noise_like,\n    apply_noise_offset,\n    scale_v_prediction_loss_like_noise_prediction,\n    apply_debiased_estimation,\n    apply_masked_loss,\n)\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# perlin_noise,\n\n\ndef train(args):\n    train_util.verify_training_args(args)\n    train_util.prepare_dataset_args(args, False)\n    deepspeed_utils.prepare_deepspeed_args(args)\n    setup_logging(args, reset=True)\n\n    cache_latents = args.cache_latents\n\n    if args.seed is not None:\n        set_seed(args.seed)  # 乱数系列を初期化する\n\n    tokenizer = train_util.load_tokenizer(args)\n\n    # データセットを準備する\n    if args.dataset_class is None:\n        blueprint_generator = BlueprintGenerator(ConfigSanitizer(True, False, args.masked_loss, True))\n        if args.dataset_config is not None:\n            logger.info(f\"Load dataset config from {args.dataset_config}\")\n            user_config = config_util.load_user_config(args.dataset_config)\n            ignored = [\"train_data_dir\", \"reg_data_dir\"]\n            if any(getattr(args, attr) is not None for attr in ignored):\n                logger.warning(\n                    \"ignore following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                        \", \".join(ignored)\n                    )\n                )\n        else:\n            user_config = {\n                \"datasets\": [\n                    {\"subsets\": config_util.generate_dreambooth_subsets_config_by_subdirs(args.train_data_dir, args.reg_data_dir)}\n                ]\n            }\n\n        blueprint = blueprint_generator.generate(user_config, args, tokenizer=tokenizer)\n        train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n    else:\n        train_dataset_group = train_util.load_arbitrary_dataset(args, tokenizer)\n\n    current_epoch = Value(\"i\", 0)\n    current_step = Value(\"i\", 0)\n    ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n    collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n    if args.no_token_padding:\n        train_dataset_group.disable_token_padding()\n\n    if args.debug_dataset:\n        train_util.debug_dataset(train_dataset_group)\n        return\n\n    if cache_latents:\n        assert (\n            train_dataset_group.is_latent_cacheable()\n        ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n\n    # acceleratorを準備する\n    logger.info(\"prepare accelerator\")\n\n    if args.gradient_accumulation_steps > 1:\n        logger.warning(\n            f\"gradient_accumulation_steps is {args.gradient_accumulation_steps}. accelerate does not support gradient_accumulation_steps when training multiple models (U-Net and Text Encoder), so something might be wrong\"\n        )\n        logger.warning(\n            f\"gradient_accumulation_stepsが{args.gradient_accumulation_steps}に設定されています。accelerateは複数モデル（U-NetおよびText Encoder）の学習時にgradient_accumulation_stepsをサポートしていないため結果は未知数です\"\n        )\n\n    accelerator = train_util.prepare_accelerator(args)\n\n    # mixed precisionに対応した型を用意しておき適宜castする\n    weight_dtype, save_dtype = train_util.prepare_dtype(args)\n    vae_dtype = torch.float32 if args.no_half_vae else weight_dtype\n\n    # モデルを読み込む\n    text_encoder, vae, unet, load_stable_diffusion_format = train_util.load_target_model(args, weight_dtype, accelerator)\n\n    # verify load/save model formats\n    if load_stable_diffusion_format:\n        src_stable_diffusion_ckpt = args.pretrained_model_name_or_path\n        src_diffusers_model_path = None\n    else:\n        src_stable_diffusion_ckpt = None\n        src_diffusers_model_path = args.pretrained_model_name_or_path\n\n    if args.save_model_as is None:\n        save_stable_diffusion_format = load_stable_diffusion_format\n        use_safetensors = args.use_safetensors\n    else:\n        save_stable_diffusion_format = args.save_model_as.lower() == \"ckpt\" or args.save_model_as.lower() == \"safetensors\"\n        use_safetensors = args.use_safetensors or (\"safetensors\" in args.save_model_as.lower())\n\n    # モデルに xformers とか memory efficient attention を組み込む\n    train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n\n    # 学習を準備する\n    if cache_latents:\n        vae.to(accelerator.device, dtype=vae_dtype)\n        vae.requires_grad_(False)\n        vae.eval()\n        with torch.no_grad():\n            train_dataset_group.cache_latents(vae, args.vae_batch_size, args.cache_latents_to_disk, accelerator.is_main_process)\n        vae.to(\"cpu\")\n        clean_memory_on_device(accelerator.device)\n\n        accelerator.wait_for_everyone()\n\n    # 学習を準備する：モデルを適切な状態にする\n    train_text_encoder = args.stop_text_encoder_training is None or args.stop_text_encoder_training >= 0\n    unet.requires_grad_(True)  # 念のため追加\n    text_encoder.requires_grad_(train_text_encoder)\n    if not train_text_encoder:\n        accelerator.print(\"Text Encoder is not trained.\")\n\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n        text_encoder.gradient_checkpointing_enable()\n\n    if not cache_latents:\n        vae.requires_grad_(False)\n        vae.eval()\n        vae.to(accelerator.device, dtype=weight_dtype)\n\n    # 学習に必要なクラスを準備する\n    accelerator.print(\"prepare optimizer, data loader etc.\")\n    if train_text_encoder:\n        if args.learning_rate_te is None:\n            # wightout list, adamw8bit is crashed\n            trainable_params = list(itertools.chain(unet.parameters(), text_encoder.parameters()))\n        else:\n            trainable_params = [\n                {\"params\": list(unet.parameters()), \"lr\": args.learning_rate},\n                {\"params\": list(text_encoder.parameters()), \"lr\": args.learning_rate_te},\n            ]\n    else:\n        trainable_params = unet.parameters()\n\n    _, _, optimizer = train_util.get_optimizer(args, trainable_params)\n\n    # dataloaderを準備する\n    # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n    n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset_group,\n        batch_size=1,\n        shuffle=True,\n        collate_fn=collator,\n        num_workers=n_workers,\n        persistent_workers=args.persistent_data_loader_workers,\n    )\n\n    # 学習ステップ数を計算する\n    if args.max_train_epochs is not None:\n        args.max_train_steps = args.max_train_epochs * math.ceil(\n            len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n        )\n        accelerator.print(\n            f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n        )\n\n    # データセット側にも学習ステップを送信\n    train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n    if args.stop_text_encoder_training is None:\n        args.stop_text_encoder_training = args.max_train_steps + 1  # do not stop until end\n\n    # lr schedulerを用意する TODO gradient_accumulation_stepsの扱いが何かおかしいかもしれない。後で確認する\n    lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　モデル全体をfp16にする\n    if args.full_fp16:\n        assert (\n            args.mixed_precision == \"fp16\"\n        ), \"full_fp16 requires mixed precision='fp16' / full_fp16を使う場合はmixed_precision='fp16'を指定してください。\"\n        accelerator.print(\"enable full fp16 training.\")\n        unet.to(weight_dtype)\n        text_encoder.to(weight_dtype)\n\n    # acceleratorがなんかよろしくやってくれるらしい\n    if args.deepspeed:\n        if args.train_text_encoder:\n            ds_model = deepspeed_utils.prepare_deepspeed_model(args, unet=unet, text_encoder=text_encoder)\n        else:\n            ds_model = deepspeed_utils.prepare_deepspeed_model(args, unet=unet)\n        ds_model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            ds_model, optimizer, train_dataloader, lr_scheduler\n        )\n        training_models = [ds_model]\n\n    else:\n        if train_text_encoder:\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n                unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n            )\n            training_models = [unet, text_encoder]\n        else:\n            unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader, lr_scheduler)\n            training_models = [unet]\n\n    if not train_text_encoder:\n        text_encoder.to(accelerator.device, dtype=weight_dtype)  # to avoid 'cpu' vs 'cuda' error\n\n    # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n    if args.full_fp16:\n        train_util.patch_accelerator_for_fp16_training(accelerator)\n\n    # resumeする\n    train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n    # epoch数を計算する\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n        args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n    # 学習する\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    accelerator.print(\"running training / 学習開始\")\n    accelerator.print(f\"  num train images * repeats / 学習画像の数×繰り返し回数: {train_dataset_group.num_train_images}\")\n    accelerator.print(f\"  num reg images / 正則化画像の数: {train_dataset_group.num_reg_images}\")\n    accelerator.print(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n    accelerator.print(f\"  num epochs / epoch数: {num_train_epochs}\")\n    accelerator.print(f\"  batch size per device / バッチサイズ: {args.train_batch_size}\")\n    accelerator.print(\n        f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\"\n    )\n    accelerator.print(f\"  gradient ccumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n    accelerator.print(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n    progress_bar = tqdm(range(args.max_train_steps), smoothing=0, disable=not accelerator.is_local_main_process, desc=\"steps\")\n    global_step = 0\n\n    noise_scheduler = DDPMScheduler(\n        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, clip_sample=False\n    )\n    prepare_scheduler_for_custom_training(noise_scheduler, accelerator.device)\n    if args.zero_terminal_snr:\n        custom_train_functions.fix_noise_scheduler_betas_for_zero_terminal_snr(noise_scheduler)\n\n    if accelerator.is_main_process:\n        init_kwargs = {}\n        if args.wandb_run_name:\n            init_kwargs[\"wandb\"] = {\"name\": args.wandb_run_name}\n        if args.log_tracker_config is not None:\n            init_kwargs = toml.load(args.log_tracker_config)\n        accelerator.init_trackers(\"dreambooth\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs)\n\n    # For --sample_at_first\n    train_util.sample_images(accelerator, args, 0, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n    loss_recorder = train_util.LossRecorder()\n    for epoch in range(num_train_epochs):\n        accelerator.print(f\"\\nepoch {epoch+1}/{num_train_epochs}\")\n        current_epoch.value = epoch + 1\n\n        # 指定したステップ数までText Encoderを学習する：epoch最初の状態\n        unet.train()\n        # train==True is required to enable gradient_checkpointing\n        if args.gradient_checkpointing or global_step < args.stop_text_encoder_training:\n            text_encoder.train()\n\n        for step, batch in enumerate(train_dataloader):\n            current_step.value = global_step\n            # 指定したステップ数でText Encoderの学習を止める\n            if global_step == args.stop_text_encoder_training:\n                accelerator.print(f\"stop text encoder training at step {global_step}\")\n                if not args.gradient_checkpointing:\n                    text_encoder.train(False)\n                text_encoder.requires_grad_(False)\n                if len(training_models) == 2:\n                    training_models = training_models[0]  # remove text_encoder from training_models\n\n            with accelerator.accumulate(*training_models):\n                with torch.no_grad():\n                    # latentに変換\n                    if cache_latents:\n                        latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                    else:\n                        latents = vae.encode(batch[\"images\"].to(dtype=weight_dtype)).latent_dist.sample()\n                    latents = latents * 0.18215\n                b_size = latents.shape[0]\n\n                # Get the text embedding for conditioning\n                with torch.set_grad_enabled(global_step < args.stop_text_encoder_training):\n                    if args.weighted_captions:\n                        encoder_hidden_states = get_weighted_text_embeddings(\n                            tokenizer,\n                            text_encoder,\n                            batch[\"captions\"],\n                            accelerator.device,\n                            args.max_token_length // 75 if args.max_token_length else 1,\n                            clip_skip=args.clip_skip,\n                        )\n                    else:\n                        input_ids = batch[\"input_ids\"].to(accelerator.device)\n                        encoder_hidden_states = train_util.get_hidden_states(\n                            args, input_ids, tokenizer, text_encoder, None if not args.full_fp16 else weight_dtype\n                        )\n\n                # Sample noise, sample a random timestep for each image, and add noise to the latents,\n                # with noise offset and/or multires noise if specified\n                noise, noisy_latents, timesteps, huber_c = train_util.get_noise_noisy_latents_and_timesteps(args, noise_scheduler, latents)\n\n                # Predict the noise residual\n                with accelerator.autocast():\n                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                if args.v_parameterization:\n                    # v-parameterization training\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    target = noise\n\n                loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c)\n                if args.masked_loss:\n                    loss = apply_masked_loss(loss, batch)\n                loss = loss.mean([1, 2, 3])\n\n                loss_weights = batch[\"loss_weights\"]  # 各sampleごとのweight\n                loss = loss * loss_weights\n\n                if args.min_snr_gamma:\n                    loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma, args.v_parameterization)\n                if args.scale_v_pred_loss_like_noise_pred:\n                    loss = scale_v_prediction_loss_like_noise_prediction(loss, timesteps, noise_scheduler)\n                if args.debiased_estimation_loss:\n                    loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)\n\n                loss = loss.mean()  # 平均なのでbatch_sizeで割る必要なし\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients and args.max_grad_norm != 0.0:\n                    if train_text_encoder:\n                        params_to_clip = itertools.chain(unet.parameters(), text_encoder.parameters())\n                    else:\n                        params_to_clip = unet.parameters()\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                train_util.sample_images(\n                    accelerator, args, None, global_step, accelerator.device, vae, tokenizer, text_encoder, unet\n                )\n\n                # 指定ステップごとにモデルを保存\n                if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                    accelerator.wait_for_everyone()\n                    if accelerator.is_main_process:\n                        src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n                        train_util.save_sd_model_on_epoch_end_or_stepwise(\n                            args,\n                            False,\n                            accelerator,\n                            src_path,\n                            save_stable_diffusion_format,\n                            use_safetensors,\n                            save_dtype,\n                            epoch,\n                            num_train_epochs,\n                            global_step,\n                            accelerator.unwrap_model(text_encoder),\n                            accelerator.unwrap_model(unet),\n                            vae,\n                        )\n\n            current_loss = loss.detach().item()\n            if args.logging_dir is not None:\n                logs = {\"loss\": current_loss}\n                train_util.append_lr_to_logs(logs, lr_scheduler, args.optimizer_type, including_unet=True)\n                accelerator.log(logs, step=global_step)\n\n            loss_recorder.add(epoch=epoch, step=step, loss=current_loss)\n            avr_loss: float = loss_recorder.moving_average\n            logs = {\"avr_loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        if args.logging_dir is not None:\n            logs = {\"loss/epoch\": loss_recorder.moving_average}\n            accelerator.log(logs, step=epoch + 1)\n\n        accelerator.wait_for_everyone()\n\n        if args.save_every_n_epochs is not None:\n            if accelerator.is_main_process:\n                # checking for saving is in util\n                src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n                train_util.save_sd_model_on_epoch_end_or_stepwise(\n                    args,\n                    True,\n                    accelerator,\n                    src_path,\n                    save_stable_diffusion_format,\n                    use_safetensors,\n                    save_dtype,\n                    epoch,\n                    num_train_epochs,\n                    global_step,\n                    accelerator.unwrap_model(text_encoder),\n                    accelerator.unwrap_model(unet),\n                    vae,\n                )\n\n        train_util.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n    is_main_process = accelerator.is_main_process\n    if is_main_process:\n        unet = accelerator.unwrap_model(unet)\n        text_encoder = accelerator.unwrap_model(text_encoder)\n\n    accelerator.end_training()\n\n    if is_main_process and (args.save_state or args.save_state_on_train_end):\n        train_util.save_state_on_train_end(args, accelerator)\n\n    del accelerator  # この後メモリを使うのでこれは消す\n\n    if is_main_process:\n        src_path = src_stable_diffusion_ckpt if save_stable_diffusion_format else src_diffusers_model_path\n        train_util.save_sd_model_on_train_end(\n            args, src_path, save_stable_diffusion_format, use_safetensors, save_dtype, epoch, global_step, text_encoder, unet, vae\n        )\n        logger.info(\"model saved.\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, True, False, True)\n    train_util.add_training_arguments(parser, True)\n    train_util.add_masked_loss_arguments(parser)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_sd_saving_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser)\n\n    parser.add_argument(\n        \"--learning_rate_te\",\n        type=float,\n        default=None,\n        help=\"learning rate for text encoder, default is same as unet / Text Encoderの学習率、デフォルトはunetと同じ\",\n    )\n    parser.add_argument(\n        \"--no_token_padding\",\n        action=\"store_true\",\n        help=\"disable token padding (same as Diffuser's DreamBooth) / トークンのpaddingを無効にする（Diffusers版DreamBoothと同じ動作）\",\n    )\n    parser.add_argument(\n        \"--stop_text_encoder_training\",\n        type=int,\n        default=None,\n        help=\"steps to stop text encoder training, -1 for no training / Text Encoderの学習を止めるステップ数、-1で最初から学習しない\",\n    )\n    parser.add_argument(\n        \"--no_half_vae\",\n        action=\"store_true\",\n        help=\"do not use fp16/bf16 VAE in mixed precision (use float VAE) / mixed precisionでも fp16/bf16 VAEを使わずfloat VAEを使う\",\n    )\n\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    train(args)\n"
        },
        {
          "name": "train_network.py",
          "type": "blob",
          "size": 53.59765625,
          "content": "import importlib\nimport argparse\nimport math\nimport os\nimport sys\nimport random\nimport time\nimport json\nfrom multiprocessing import Value\nimport toml\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library.device_utils import init_ipex, clean_memory_on_device\n\ninit_ipex()\n\nfrom accelerate.utils import set_seed\nfrom diffusers import DDPMScheduler\nfrom library import deepspeed_utils, model_util\n\nimport library.train_util as train_util\nfrom library.train_util import DreamBoothDataset\nimport library.config_util as config_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.huggingface_util as huggingface_util\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    apply_snr_weight,\n    get_weighted_text_embeddings,\n    prepare_scheduler_for_custom_training,\n    scale_v_prediction_loss_like_noise_prediction,\n    add_v_prediction_like_loss,\n    apply_debiased_estimation,\n    apply_masked_loss,\n)\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass NetworkTrainer:\n    def __init__(self):\n        self.vae_scale_factor = 0.18215\n        self.is_sdxl = False\n\n    # TODO 他のスクリプトと共通化する\n    def generate_step_logs(\n        self, args: argparse.Namespace, current_loss, avr_loss, lr_scheduler, keys_scaled=None, mean_norm=None, maximum_norm=None\n    ):\n        logs = {\"loss/current\": current_loss, \"loss/average\": avr_loss}\n\n        if keys_scaled is not None:\n            logs[\"max_norm/keys_scaled\"] = keys_scaled\n            logs[\"max_norm/average_key_norm\"] = mean_norm\n            logs[\"max_norm/max_key_norm\"] = maximum_norm\n\n        lrs = lr_scheduler.get_last_lr()\n\n        if args.network_train_text_encoder_only or len(lrs) <= 2:  # not block lr (or single block)\n            if args.network_train_unet_only:\n                logs[\"lr/unet\"] = float(lrs[0])\n            elif args.network_train_text_encoder_only:\n                logs[\"lr/textencoder\"] = float(lrs[0])\n            else:\n                logs[\"lr/textencoder\"] = float(lrs[0])\n                logs[\"lr/unet\"] = float(lrs[-1])  # may be same to textencoder\n\n            if (\n                args.optimizer_type.lower().startswith(\"DAdapt\".lower()) or args.optimizer_type.lower() == \"Prodigy\".lower()\n            ):  # tracking d*lr value of unet.\n                logs[\"lr/d*lr\"] = (\n                    lr_scheduler.optimizers[-1].param_groups[0][\"d\"] * lr_scheduler.optimizers[-1].param_groups[0][\"lr\"]\n                )\n        else:\n            idx = 0\n            if not args.network_train_unet_only:\n                logs[\"lr/textencoder\"] = float(lrs[0])\n                idx = 1\n\n            for i in range(idx, len(lrs)):\n                logs[f\"lr/group{i}\"] = float(lrs[i])\n                if args.optimizer_type.lower().startswith(\"DAdapt\".lower()) or args.optimizer_type.lower() == \"Prodigy\".lower():\n                    logs[f\"lr/d*lr/group{i}\"] = (\n                        lr_scheduler.optimizers[-1].param_groups[i][\"d\"] * lr_scheduler.optimizers[-1].param_groups[i][\"lr\"]\n                    )\n\n        return logs\n\n    def assert_extra_args(self, args, train_dataset_group):\n        pass\n\n    def load_target_model(self, args, weight_dtype, accelerator):\n        text_encoder, vae, unet, _ = train_util.load_target_model(args, weight_dtype, accelerator)\n        return model_util.get_model_version_str_for_sd1_sd2(args.v2, args.v_parameterization), text_encoder, vae, unet\n\n    def load_tokenizer(self, args):\n        tokenizer = train_util.load_tokenizer(args)\n        return tokenizer\n\n    def is_text_encoder_outputs_cached(self, args):\n        return False\n\n    def is_train_text_encoder(self, args):\n        return not args.network_train_unet_only and not self.is_text_encoder_outputs_cached(args)\n\n    def cache_text_encoder_outputs_if_needed(\n        self, args, accelerator, unet, vae, tokenizers, text_encoders, data_loader, weight_dtype\n    ):\n        for t_enc in text_encoders:\n            t_enc.to(accelerator.device, dtype=weight_dtype)\n\n    def get_text_cond(self, args, accelerator, batch, tokenizers, text_encoders, weight_dtype):\n        input_ids = batch[\"input_ids\"].to(accelerator.device)\n        encoder_hidden_states = train_util.get_hidden_states(args, input_ids, tokenizers[0], text_encoders[0], weight_dtype)\n        return encoder_hidden_states\n\n    def call_unet(self, args, accelerator, unet, noisy_latents, timesteps, text_conds, batch, weight_dtype):\n        noise_pred = unet(noisy_latents, timesteps, text_conds).sample\n        return noise_pred\n\n    def all_reduce_network(self, accelerator, network):\n        for param in network.parameters():\n            if param.grad is not None:\n                param.grad = accelerator.reduce(param.grad, reduction=\"mean\")\n\n    def sample_images(self, accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet):\n        train_util.sample_images(accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet)\n\n    def train(self, args):\n        session_id = random.randint(0, 2**32)\n        training_started_at = time.time()\n        train_util.verify_training_args(args)\n        train_util.prepare_dataset_args(args, True)\n        deepspeed_utils.prepare_deepspeed_args(args)\n        setup_logging(args, reset=True)\n\n        cache_latents = args.cache_latents\n        use_dreambooth_method = args.in_json is None\n        use_user_config = args.dataset_config is not None\n\n        if args.seed is None:\n            args.seed = random.randint(0, 2**32)\n        set_seed(args.seed)\n\n        # tokenizerは単体またはリスト、tokenizersは必ずリスト：既存のコードとの互換性のため\n        tokenizer = self.load_tokenizer(args)\n        tokenizers = tokenizer if isinstance(tokenizer, list) else [tokenizer]\n\n        # データセットを準備する\n        if args.dataset_class is None:\n            blueprint_generator = BlueprintGenerator(ConfigSanitizer(True, True, args.masked_loss, True))\n            if use_user_config:\n                logger.info(f\"Loading dataset config from {args.dataset_config}\")\n                user_config = config_util.load_user_config(args.dataset_config)\n                ignored = [\"train_data_dir\", \"reg_data_dir\", \"in_json\"]\n                if any(getattr(args, attr) is not None for attr in ignored):\n                    logger.warning(\n                        \"ignoring the following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                            \", \".join(ignored)\n                        )\n                    )\n            else:\n                if use_dreambooth_method:\n                    logger.info(\"Using DreamBooth method.\")\n                    user_config = {\n                        \"datasets\": [\n                            {\n                                \"subsets\": config_util.generate_dreambooth_subsets_config_by_subdirs(\n                                    args.train_data_dir, args.reg_data_dir\n                                )\n                            }\n                        ]\n                    }\n                else:\n                    logger.info(\"Training with captions.\")\n                    user_config = {\n                        \"datasets\": [\n                            {\n                                \"subsets\": [\n                                    {\n                                        \"image_dir\": args.train_data_dir,\n                                        \"metadata_file\": args.in_json,\n                                    }\n                                ]\n                            }\n                        ]\n                    }\n\n            blueprint = blueprint_generator.generate(user_config, args, tokenizer=tokenizer)\n            train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n        else:\n            # use arbitrary dataset class\n            train_dataset_group = train_util.load_arbitrary_dataset(args, tokenizer)\n\n        current_epoch = Value(\"i\", 0)\n        current_step = Value(\"i\", 0)\n        ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n        collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n        if args.debug_dataset:\n            train_util.debug_dataset(train_dataset_group)\n            return\n        if len(train_dataset_group) == 0:\n            logger.error(\n                \"No data found. Please verify arguments (train_data_dir must be the parent of folders with images) / 画像がありません。引数指定を確認してください（train_data_dirには画像があるフォルダではなく、画像があるフォルダの親フォルダを指定する必要があります）\"\n            )\n            return\n\n        if cache_latents:\n            assert (\n                train_dataset_group.is_latent_cacheable()\n            ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n\n        self.assert_extra_args(args, train_dataset_group)\n\n        # acceleratorを準備する\n        logger.info(\"preparing accelerator\")\n        accelerator = train_util.prepare_accelerator(args)\n        is_main_process = accelerator.is_main_process\n\n        # mixed precisionに対応した型を用意しておき適宜castする\n        weight_dtype, save_dtype = train_util.prepare_dtype(args)\n        vae_dtype = torch.float32 if args.no_half_vae else weight_dtype\n\n        # モデルを読み込む\n        model_version, text_encoder, vae, unet = self.load_target_model(args, weight_dtype, accelerator)\n\n        # text_encoder is List[CLIPTextModel] or CLIPTextModel\n        text_encoders = text_encoder if isinstance(text_encoder, list) else [text_encoder]\n\n        # モデルに xformers とか memory efficient attention を組み込む\n        train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n        if torch.__version__ >= \"2.0.0\":  # PyTorch 2.0.0 以上対応のxformersなら以下が使える\n            vae.set_use_memory_efficient_attention_xformers(args.xformers)\n\n        # 差分追加学習のためにモデルを読み込む\n        sys.path.append(os.path.dirname(__file__))\n        accelerator.print(\"import network module:\", args.network_module)\n        network_module = importlib.import_module(args.network_module)\n\n        if args.base_weights is not None:\n            # base_weights が指定されている場合は、指定された重みを読み込みマージする\n            for i, weight_path in enumerate(args.base_weights):\n                if args.base_weights_multiplier is None or len(args.base_weights_multiplier) <= i:\n                    multiplier = 1.0\n                else:\n                    multiplier = args.base_weights_multiplier[i]\n\n                accelerator.print(f\"merging module: {weight_path} with multiplier {multiplier}\")\n\n                module, weights_sd = network_module.create_network_from_weights(\n                    multiplier, weight_path, vae, text_encoder, unet, for_inference=True\n                )\n                module.merge_to(text_encoder, unet, weights_sd, weight_dtype, accelerator.device if args.lowram else \"cpu\")\n\n            accelerator.print(f\"all weights merged: {', '.join(args.base_weights)}\")\n\n        # 学習を準備する\n        if cache_latents:\n            vae.to(accelerator.device, dtype=vae_dtype)\n            vae.requires_grad_(False)\n            vae.eval()\n            with torch.no_grad():\n                train_dataset_group.cache_latents(vae, args.vae_batch_size, args.cache_latents_to_disk, accelerator.is_main_process)\n            vae.to(\"cpu\")\n            clean_memory_on_device(accelerator.device)\n\n            accelerator.wait_for_everyone()\n\n        # 必要ならテキストエンコーダーの出力をキャッシュする: Text Encoderはcpuまたはgpuへ移される\n        # cache text encoder outputs if needed: Text Encoder is moved to cpu or gpu\n        self.cache_text_encoder_outputs_if_needed(\n            args, accelerator, unet, vae, tokenizers, text_encoders, train_dataset_group, weight_dtype\n        )\n\n        # prepare network\n        net_kwargs = {}\n        if args.network_args is not None:\n            for net_arg in args.network_args:\n                key, value = net_arg.split(\"=\")\n                net_kwargs[key] = value\n\n        # if a new network is added in future, add if ~ then blocks for each network (;'∀')\n        if args.dim_from_weights:\n            network, _ = network_module.create_network_from_weights(1, args.network_weights, vae, text_encoder, unet, **net_kwargs)\n        else:\n            if \"dropout\" not in net_kwargs:\n                # workaround for LyCORIS (;^ω^)\n                net_kwargs[\"dropout\"] = args.network_dropout\n\n            network = network_module.create_network(\n                1.0,\n                args.network_dim,\n                args.network_alpha,\n                vae,\n                text_encoder,\n                unet,\n                neuron_dropout=args.network_dropout,\n                **net_kwargs,\n            )\n        if network is None:\n            return\n        network_has_multiplier = hasattr(network, \"set_multiplier\")\n\n        if hasattr(network, \"prepare_network\"):\n            network.prepare_network(args)\n        if args.scale_weight_norms and not hasattr(network, \"apply_max_norm_regularization\"):\n            logger.warning(\n                \"warning: scale_weight_norms is specified but the network does not support it / scale_weight_normsが指定されていますが、ネットワークが対応していません\"\n            )\n            args.scale_weight_norms = False\n\n        train_unet = not args.network_train_text_encoder_only\n        train_text_encoder = self.is_train_text_encoder(args)\n        network.apply_to(text_encoder, unet, train_text_encoder, train_unet)\n\n        if args.network_weights is not None:\n            info = network.load_weights(args.network_weights)\n            accelerator.print(f\"load network weights from {args.network_weights}: {info}\")\n\n        if args.gradient_checkpointing:\n            unet.enable_gradient_checkpointing()\n            for t_enc in text_encoders:\n                t_enc.gradient_checkpointing_enable()\n            del t_enc\n            network.enable_gradient_checkpointing()  # may have no effect\n\n        # 学習に必要なクラスを準備する\n        accelerator.print(\"prepare optimizer, data loader etc.\")\n\n        # 後方互換性を確保するよ\n        try:\n            trainable_params = network.prepare_optimizer_params(args.text_encoder_lr, args.unet_lr, args.learning_rate)\n        except TypeError:\n            accelerator.print(\n                \"Deprecated: use prepare_optimizer_params(text_encoder_lr, unet_lr, learning_rate) instead of prepare_optimizer_params(text_encoder_lr, unet_lr)\"\n            )\n            trainable_params = network.prepare_optimizer_params(args.text_encoder_lr, args.unet_lr)\n\n        optimizer_name, optimizer_args, optimizer = train_util.get_optimizer(args, trainable_params)\n\n        # dataloaderを準備する\n        # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n        n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n\n        train_dataloader = torch.utils.data.DataLoader(\n            train_dataset_group,\n            batch_size=1,\n            shuffle=True,\n            collate_fn=collator,\n            num_workers=n_workers,\n            persistent_workers=args.persistent_data_loader_workers,\n        )\n\n        # 学習ステップ数を計算する\n        if args.max_train_epochs is not None:\n            args.max_train_steps = args.max_train_epochs * math.ceil(\n                len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n            )\n            accelerator.print(\n                f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n            )\n\n        # データセット側にも学習ステップを送信\n        train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n        # lr schedulerを用意する\n        lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n        # 実験的機能：勾配も含めたfp16/bf16学習を行う　モデル全体をfp16/bf16にする\n        if args.full_fp16:\n            assert (\n                args.mixed_precision == \"fp16\"\n            ), \"full_fp16 requires mixed precision='fp16' / full_fp16を使う場合はmixed_precision='fp16'を指定してください。\"\n            accelerator.print(\"enable full fp16 training.\")\n            network.to(weight_dtype)\n        elif args.full_bf16:\n            assert (\n                args.mixed_precision == \"bf16\"\n            ), \"full_bf16 requires mixed precision='bf16' / full_bf16を使う場合はmixed_precision='bf16'を指定してください。\"\n            accelerator.print(\"enable full bf16 training.\")\n            network.to(weight_dtype)\n\n        unet_weight_dtype = te_weight_dtype = weight_dtype\n        # Experimental Feature: Put base model into fp8 to save vram\n        if args.fp8_base:\n            assert torch.__version__ >= \"2.1.0\", \"fp8_base requires torch>=2.1.0 / fp8を使う場合はtorch>=2.1.0が必要です。\"\n            assert (\n                args.mixed_precision != \"no\"\n            ), \"fp8_base requires mixed precision='fp16' or 'bf16' / fp8を使う場合はmixed_precision='fp16'または'bf16'が必要です。\"\n            accelerator.print(\"enable fp8 training.\")\n            unet_weight_dtype = torch.float8_e4m3fn\n            te_weight_dtype = torch.float8_e4m3fn\n\n        unet.requires_grad_(False)\n        unet.to(dtype=unet_weight_dtype)\n        for t_enc in text_encoders:\n            t_enc.requires_grad_(False)\n\n            # in case of cpu, dtype is already set to fp32 because cpu does not support fp8/fp16/bf16\n            if t_enc.device.type != \"cpu\":\n                t_enc.to(dtype=te_weight_dtype)\n                # nn.Embedding not support FP8\n                t_enc.text_model.embeddings.to(dtype=(weight_dtype if te_weight_dtype != weight_dtype else te_weight_dtype))\n\n        # acceleratorがなんかよろしくやってくれるらしい / accelerator will do something good\n        if args.deepspeed:\n            ds_model = deepspeed_utils.prepare_deepspeed_model(\n                args,\n                unet=unet if train_unet else None,\n                text_encoder1=text_encoders[0] if train_text_encoder else None,\n                text_encoder2=text_encoders[1] if train_text_encoder and len(text_encoders) > 1 else None,\n                network=network,\n            )\n            ds_model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n                ds_model, optimizer, train_dataloader, lr_scheduler\n            )\n            training_model = ds_model\n        else:\n            if train_unet:\n                unet = accelerator.prepare(unet)\n            else:\n                unet.to(accelerator.device, dtype=unet_weight_dtype)  # move to device because unet is not prepared by accelerator\n            if train_text_encoder:\n                if len(text_encoders) > 1:\n                    text_encoder = text_encoders = [accelerator.prepare(t_enc) for t_enc in text_encoders]\n                else:\n                    text_encoder = accelerator.prepare(text_encoder)\n                    text_encoders = [text_encoder]\n            else:\n                pass  # if text_encoder is not trained, no need to prepare. and device and dtype are already set\n\n            network, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n                network, optimizer, train_dataloader, lr_scheduler\n            )\n            training_model = network\n\n        if args.gradient_checkpointing:\n            # according to TI example in Diffusers, train is required\n            unet.train()\n            for t_enc in text_encoders:\n                t_enc.train()\n\n                # set top parameter requires_grad = True for gradient checkpointing works\n                if train_text_encoder:\n                    t_enc.text_model.embeddings.requires_grad_(True)\n\n        else:\n            unet.eval()\n            for t_enc in text_encoders:\n                t_enc.eval()\n\n        del t_enc\n\n        accelerator.unwrap_model(network).prepare_grad_etc(text_encoder, unet)\n\n        if not cache_latents:  # キャッシュしない場合はVAEを使うのでVAEを準備する\n            vae.requires_grad_(False)\n            vae.eval()\n            vae.to(accelerator.device, dtype=vae_dtype)\n\n        # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n        if args.full_fp16:\n            train_util.patch_accelerator_for_fp16_training(accelerator)\n\n        # before resuming make hook for saving/loading to save/load the network weights only\n        def save_model_hook(models, weights, output_dir):\n            # pop weights of other models than network to save only network weights\n            if accelerator.is_main_process:\n                remove_indices = []\n                for i, model in enumerate(models):\n                    if not isinstance(model, type(accelerator.unwrap_model(network))):\n                        remove_indices.append(i)\n                for i in reversed(remove_indices):\n                    weights.pop(i)\n                # print(f\"save model hook: {len(weights)} weights will be saved\")\n\n        def load_model_hook(models, input_dir):\n            # remove models except network\n            remove_indices = []\n            for i, model in enumerate(models):\n                if not isinstance(model, type(accelerator.unwrap_model(network))):\n                    remove_indices.append(i)\n            for i in reversed(remove_indices):\n                models.pop(i)\n            # print(f\"load model hook: {len(models)} models will be loaded\")\n\n        accelerator.register_save_state_pre_hook(save_model_hook)\n        accelerator.register_load_state_pre_hook(load_model_hook)\n\n        # resumeする\n        train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n        # epoch数を計算する\n        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n        num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n        if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n            args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n        # 学習する\n        # TODO: find a way to handle total batch size when there are multiple datasets\n        total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n        accelerator.print(\"running training / 学習開始\")\n        accelerator.print(f\"  num train images * repeats / 学習画像の数×繰り返し回数: {train_dataset_group.num_train_images}\")\n        accelerator.print(f\"  num reg images / 正則化画像の数: {train_dataset_group.num_reg_images}\")\n        accelerator.print(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n        accelerator.print(f\"  num epochs / epoch数: {num_train_epochs}\")\n        accelerator.print(\n            f\"  batch size per device / バッチサイズ: {', '.join([str(d.batch_size) for d in train_dataset_group.datasets])}\"\n        )\n        # accelerator.print(f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\")\n        accelerator.print(f\"  gradient accumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n        accelerator.print(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n        # TODO refactor metadata creation and move to util\n        metadata = {\n            \"ss_session_id\": session_id,  # random integer indicating which group of epochs the model came from\n            \"ss_training_started_at\": training_started_at,  # unix timestamp\n            \"ss_output_name\": args.output_name,\n            \"ss_learning_rate\": args.learning_rate,\n            \"ss_text_encoder_lr\": args.text_encoder_lr,\n            \"ss_unet_lr\": args.unet_lr,\n            \"ss_num_train_images\": train_dataset_group.num_train_images,\n            \"ss_num_reg_images\": train_dataset_group.num_reg_images,\n            \"ss_num_batches_per_epoch\": len(train_dataloader),\n            \"ss_num_epochs\": num_train_epochs,\n            \"ss_gradient_checkpointing\": args.gradient_checkpointing,\n            \"ss_gradient_accumulation_steps\": args.gradient_accumulation_steps,\n            \"ss_max_train_steps\": args.max_train_steps,\n            \"ss_lr_warmup_steps\": args.lr_warmup_steps,\n            \"ss_lr_scheduler\": args.lr_scheduler,\n            \"ss_network_module\": args.network_module,\n            \"ss_network_dim\": args.network_dim,  # None means default because another network than LoRA may have another default dim\n            \"ss_network_alpha\": args.network_alpha,  # some networks may not have alpha\n            \"ss_network_dropout\": args.network_dropout,  # some networks may not have dropout\n            \"ss_mixed_precision\": args.mixed_precision,\n            \"ss_full_fp16\": bool(args.full_fp16),\n            \"ss_v2\": bool(args.v2),\n            \"ss_base_model_version\": model_version,\n            \"ss_clip_skip\": args.clip_skip,\n            \"ss_max_token_length\": args.max_token_length,\n            \"ss_cache_latents\": bool(args.cache_latents),\n            \"ss_seed\": args.seed,\n            \"ss_lowram\": args.lowram,\n            \"ss_noise_offset\": args.noise_offset,\n            \"ss_multires_noise_iterations\": args.multires_noise_iterations,\n            \"ss_multires_noise_discount\": args.multires_noise_discount,\n            \"ss_adaptive_noise_scale\": args.adaptive_noise_scale,\n            \"ss_zero_terminal_snr\": args.zero_terminal_snr,\n            \"ss_training_comment\": args.training_comment,  # will not be updated after training\n            \"ss_sd_scripts_commit_hash\": train_util.get_git_revision_hash(),\n            \"ss_optimizer\": optimizer_name + (f\"({optimizer_args})\" if len(optimizer_args) > 0 else \"\"),\n            \"ss_max_grad_norm\": args.max_grad_norm,\n            \"ss_caption_dropout_rate\": args.caption_dropout_rate,\n            \"ss_caption_dropout_every_n_epochs\": args.caption_dropout_every_n_epochs,\n            \"ss_caption_tag_dropout_rate\": args.caption_tag_dropout_rate,\n            \"ss_face_crop_aug_range\": args.face_crop_aug_range,\n            \"ss_prior_loss_weight\": args.prior_loss_weight,\n            \"ss_min_snr_gamma\": args.min_snr_gamma,\n            \"ss_scale_weight_norms\": args.scale_weight_norms,\n            \"ss_ip_noise_gamma\": args.ip_noise_gamma,\n            \"ss_debiased_estimation\": bool(args.debiased_estimation_loss),\n            \"ss_noise_offset_random_strength\": args.noise_offset_random_strength,\n            \"ss_ip_noise_gamma_random_strength\": args.ip_noise_gamma_random_strength,\n            \"ss_loss_type\": args.loss_type,\n            \"ss_huber_schedule\": args.huber_schedule,\n            \"ss_huber_c\": args.huber_c,\n        }\n\n        if use_user_config:\n            # save metadata of multiple datasets\n            # NOTE: pack \"ss_datasets\" value as json one time\n            #   or should also pack nested collections as json?\n            datasets_metadata = []\n            tag_frequency = {}  # merge tag frequency for metadata editor\n            dataset_dirs_info = {}  # merge subset dirs for metadata editor\n\n            for dataset in train_dataset_group.datasets:\n                is_dreambooth_dataset = isinstance(dataset, DreamBoothDataset)\n                dataset_metadata = {\n                    \"is_dreambooth\": is_dreambooth_dataset,\n                    \"batch_size_per_device\": dataset.batch_size,\n                    \"num_train_images\": dataset.num_train_images,  # includes repeating\n                    \"num_reg_images\": dataset.num_reg_images,\n                    \"resolution\": (dataset.width, dataset.height),\n                    \"enable_bucket\": bool(dataset.enable_bucket),\n                    \"min_bucket_reso\": dataset.min_bucket_reso,\n                    \"max_bucket_reso\": dataset.max_bucket_reso,\n                    \"tag_frequency\": dataset.tag_frequency,\n                    \"bucket_info\": dataset.bucket_info,\n                }\n\n                subsets_metadata = []\n                for subset in dataset.subsets:\n                    subset_metadata = {\n                        \"img_count\": subset.img_count,\n                        \"num_repeats\": subset.num_repeats,\n                        \"color_aug\": bool(subset.color_aug),\n                        \"flip_aug\": bool(subset.flip_aug),\n                        \"random_crop\": bool(subset.random_crop),\n                        \"shuffle_caption\": bool(subset.shuffle_caption),\n                        \"keep_tokens\": subset.keep_tokens,\n                        \"keep_tokens_separator\": subset.keep_tokens_separator,\n                        \"secondary_separator\": subset.secondary_separator,\n                        \"enable_wildcard\": bool(subset.enable_wildcard),\n                        \"caption_prefix\": subset.caption_prefix,\n                        \"caption_suffix\": subset.caption_suffix,\n                    }\n\n                    image_dir_or_metadata_file = None\n                    if subset.image_dir:\n                        image_dir = os.path.basename(subset.image_dir)\n                        subset_metadata[\"image_dir\"] = image_dir\n                        image_dir_or_metadata_file = image_dir\n\n                    if is_dreambooth_dataset:\n                        subset_metadata[\"class_tokens\"] = subset.class_tokens\n                        subset_metadata[\"is_reg\"] = subset.is_reg\n                        if subset.is_reg:\n                            image_dir_or_metadata_file = None  # not merging reg dataset\n                    else:\n                        metadata_file = os.path.basename(subset.metadata_file)\n                        subset_metadata[\"metadata_file\"] = metadata_file\n                        image_dir_or_metadata_file = metadata_file  # may overwrite\n\n                    subsets_metadata.append(subset_metadata)\n\n                    # merge dataset dir: not reg subset only\n                    # TODO update additional-network extension to show detailed dataset config from metadata\n                    if image_dir_or_metadata_file is not None:\n                        # datasets may have a certain dir multiple times\n                        v = image_dir_or_metadata_file\n                        i = 2\n                        while v in dataset_dirs_info:\n                            v = image_dir_or_metadata_file + f\" ({i})\"\n                            i += 1\n                        image_dir_or_metadata_file = v\n\n                        dataset_dirs_info[image_dir_or_metadata_file] = {\n                            \"n_repeats\": subset.num_repeats,\n                            \"img_count\": subset.img_count,\n                        }\n\n                dataset_metadata[\"subsets\"] = subsets_metadata\n                datasets_metadata.append(dataset_metadata)\n\n                # merge tag frequency:\n                for ds_dir_name, ds_freq_for_dir in dataset.tag_frequency.items():\n                    # あるディレクトリが複数のdatasetで使用されている場合、一度だけ数える\n                    # もともと繰り返し回数を指定しているので、キャプション内でのタグの出現回数と、それが学習で何度使われるかは一致しない\n                    # なので、ここで複数datasetの回数を合算してもあまり意味はない\n                    if ds_dir_name in tag_frequency:\n                        continue\n                    tag_frequency[ds_dir_name] = ds_freq_for_dir\n\n            metadata[\"ss_datasets\"] = json.dumps(datasets_metadata)\n            metadata[\"ss_tag_frequency\"] = json.dumps(tag_frequency)\n            metadata[\"ss_dataset_dirs\"] = json.dumps(dataset_dirs_info)\n        else:\n            # conserving backward compatibility when using train_dataset_dir and reg_dataset_dir\n            assert (\n                len(train_dataset_group.datasets) == 1\n            ), f\"There should be a single dataset but {len(train_dataset_group.datasets)} found. This seems to be a bug. / データセットは1個だけ存在するはずですが、実際には{len(train_dataset_group.datasets)}個でした。プログラムのバグかもしれません。\"\n\n            dataset = train_dataset_group.datasets[0]\n\n            dataset_dirs_info = {}\n            reg_dataset_dirs_info = {}\n            if use_dreambooth_method:\n                for subset in dataset.subsets:\n                    info = reg_dataset_dirs_info if subset.is_reg else dataset_dirs_info\n                    info[os.path.basename(subset.image_dir)] = {\"n_repeats\": subset.num_repeats, \"img_count\": subset.img_count}\n            else:\n                for subset in dataset.subsets:\n                    dataset_dirs_info[os.path.basename(subset.metadata_file)] = {\n                        \"n_repeats\": subset.num_repeats,\n                        \"img_count\": subset.img_count,\n                    }\n\n            metadata.update(\n                {\n                    \"ss_batch_size_per_device\": args.train_batch_size,\n                    \"ss_total_batch_size\": total_batch_size,\n                    \"ss_resolution\": args.resolution,\n                    \"ss_color_aug\": bool(args.color_aug),\n                    \"ss_flip_aug\": bool(args.flip_aug),\n                    \"ss_random_crop\": bool(args.random_crop),\n                    \"ss_shuffle_caption\": bool(args.shuffle_caption),\n                    \"ss_enable_bucket\": bool(dataset.enable_bucket),\n                    \"ss_bucket_no_upscale\": bool(dataset.bucket_no_upscale),\n                    \"ss_min_bucket_reso\": dataset.min_bucket_reso,\n                    \"ss_max_bucket_reso\": dataset.max_bucket_reso,\n                    \"ss_keep_tokens\": args.keep_tokens,\n                    \"ss_dataset_dirs\": json.dumps(dataset_dirs_info),\n                    \"ss_reg_dataset_dirs\": json.dumps(reg_dataset_dirs_info),\n                    \"ss_tag_frequency\": json.dumps(dataset.tag_frequency),\n                    \"ss_bucket_info\": json.dumps(dataset.bucket_info),\n                }\n            )\n\n        # add extra args\n        if args.network_args:\n            metadata[\"ss_network_args\"] = json.dumps(net_kwargs)\n\n        # model name and hash\n        if args.pretrained_model_name_or_path is not None:\n            sd_model_name = args.pretrained_model_name_or_path\n            if os.path.exists(sd_model_name):\n                metadata[\"ss_sd_model_hash\"] = train_util.model_hash(sd_model_name)\n                metadata[\"ss_new_sd_model_hash\"] = train_util.calculate_sha256(sd_model_name)\n                sd_model_name = os.path.basename(sd_model_name)\n            metadata[\"ss_sd_model_name\"] = sd_model_name\n\n        if args.vae is not None:\n            vae_name = args.vae\n            if os.path.exists(vae_name):\n                metadata[\"ss_vae_hash\"] = train_util.model_hash(vae_name)\n                metadata[\"ss_new_vae_hash\"] = train_util.calculate_sha256(vae_name)\n                vae_name = os.path.basename(vae_name)\n            metadata[\"ss_vae_name\"] = vae_name\n\n        metadata = {k: str(v) for k, v in metadata.items()}\n\n        # make minimum metadata for filtering\n        minimum_metadata = {}\n        for key in train_util.SS_METADATA_MINIMUM_KEYS:\n            if key in metadata:\n                minimum_metadata[key] = metadata[key]\n\n        progress_bar = tqdm(range(args.max_train_steps), smoothing=0, disable=not accelerator.is_local_main_process, desc=\"steps\")\n        global_step = 0\n\n        noise_scheduler = DDPMScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, clip_sample=False\n        )\n        prepare_scheduler_for_custom_training(noise_scheduler, accelerator.device)\n        if args.zero_terminal_snr:\n            custom_train_functions.fix_noise_scheduler_betas_for_zero_terminal_snr(noise_scheduler)\n\n        if accelerator.is_main_process:\n            init_kwargs = {}\n            if args.wandb_run_name:\n                init_kwargs[\"wandb\"] = {\"name\": args.wandb_run_name}\n            if args.log_tracker_config is not None:\n                init_kwargs = toml.load(args.log_tracker_config)\n            accelerator.init_trackers(\n                \"network_train\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs\n            )\n\n        loss_recorder = train_util.LossRecorder()\n        del train_dataset_group\n\n        # callback for step start\n        if hasattr(accelerator.unwrap_model(network), \"on_step_start\"):\n            on_step_start = accelerator.unwrap_model(network).on_step_start\n        else:\n            on_step_start = lambda *args, **kwargs: None\n\n        # function for saving/removing\n        def save_model(ckpt_name, unwrapped_nw, steps, epoch_no, force_sync_upload=False):\n            os.makedirs(args.output_dir, exist_ok=True)\n            ckpt_file = os.path.join(args.output_dir, ckpt_name)\n\n            accelerator.print(f\"\\nsaving checkpoint: {ckpt_file}\")\n            metadata[\"ss_training_finished_at\"] = str(time.time())\n            metadata[\"ss_steps\"] = str(steps)\n            metadata[\"ss_epoch\"] = str(epoch_no)\n\n            metadata_to_save = minimum_metadata if args.no_metadata else metadata\n            sai_metadata = train_util.get_sai_model_spec(None, args, self.is_sdxl, True, False)\n            metadata_to_save.update(sai_metadata)\n\n            unwrapped_nw.save_weights(ckpt_file, save_dtype, metadata_to_save)\n            if args.huggingface_repo_id is not None:\n                huggingface_util.upload(args, ckpt_file, \"/\" + ckpt_name, force_sync_upload=force_sync_upload)\n\n        def remove_model(old_ckpt_name):\n            old_ckpt_file = os.path.join(args.output_dir, old_ckpt_name)\n            if os.path.exists(old_ckpt_file):\n                accelerator.print(f\"removing old checkpoint: {old_ckpt_file}\")\n                os.remove(old_ckpt_file)\n\n        # For --sample_at_first\n        self.sample_images(accelerator, args, 0, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n        # training loop\n        for epoch in range(num_train_epochs):\n            accelerator.print(f\"\\nepoch {epoch+1}/{num_train_epochs}\")\n            current_epoch.value = epoch + 1\n\n            metadata[\"ss_epoch\"] = str(epoch + 1)\n\n            accelerator.unwrap_model(network).on_epoch_start(text_encoder, unet)\n\n            for step, batch in enumerate(train_dataloader):\n                current_step.value = global_step\n                with accelerator.accumulate(training_model):\n                    on_step_start(text_encoder, unet)\n\n                    if \"latents\" in batch and batch[\"latents\"] is not None:\n                        latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                    else:\n                        with torch.no_grad():\n                            # latentに変換\n                            latents = vae.encode(batch[\"images\"].to(dtype=vae_dtype)).latent_dist.sample().to(dtype=weight_dtype)\n\n                            # NaNが含まれていれば警告を表示し0に置き換える\n                            if torch.any(torch.isnan(latents)):\n                                accelerator.print(\"NaN found in latents, replacing with zeros\")\n                                latents = torch.nan_to_num(latents, 0, out=latents)\n                    latents = latents * self.vae_scale_factor\n\n                    # get multiplier for each sample\n                    if network_has_multiplier:\n                        multipliers = batch[\"network_multipliers\"]\n                        # if all multipliers are same, use single multiplier\n                        if torch.all(multipliers == multipliers[0]):\n                            multipliers = multipliers[0].item()\n                        else:\n                            raise NotImplementedError(\"multipliers for each sample is not supported yet\")\n                        # print(f\"set multiplier: {multipliers}\")\n                        accelerator.unwrap_model(network).set_multiplier(multipliers)\n\n                    with torch.set_grad_enabled(train_text_encoder), accelerator.autocast():\n                        # Get the text embedding for conditioning\n                        if args.weighted_captions:\n                            text_encoder_conds = get_weighted_text_embeddings(\n                                tokenizer,\n                                text_encoder,\n                                batch[\"captions\"],\n                                accelerator.device,\n                                args.max_token_length // 75 if args.max_token_length else 1,\n                                clip_skip=args.clip_skip,\n                            )\n                        else:\n                            text_encoder_conds = self.get_text_cond(\n                                args, accelerator, batch, tokenizers, text_encoders, weight_dtype\n                            )\n\n                    # Sample noise, sample a random timestep for each image, and add noise to the latents,\n                    # with noise offset and/or multires noise if specified\n                    noise, noisy_latents, timesteps, huber_c = train_util.get_noise_noisy_latents_and_timesteps(\n                        args, noise_scheduler, latents\n                    )\n\n                    # ensure the hidden state will require grad\n                    if args.gradient_checkpointing:\n                        for x in noisy_latents:\n                            x.requires_grad_(True)\n                        for t in text_encoder_conds:\n                            t.requires_grad_(True)\n\n                    # Predict the noise residual\n                    with accelerator.autocast():\n                        noise_pred = self.call_unet(\n                            args,\n                            accelerator,\n                            unet,\n                            noisy_latents.requires_grad_(train_unet),\n                            timesteps,\n                            text_encoder_conds,\n                            batch,\n                            weight_dtype,\n                        )\n\n                    if args.v_parameterization:\n                        # v-parameterization training\n                        target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                    else:\n                        target = noise\n\n                    loss = train_util.conditional_loss(\n                        noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c\n                    )\n                    if args.masked_loss:\n                        loss = apply_masked_loss(loss, batch)\n                    loss = loss.mean([1, 2, 3])\n\n                    loss_weights = batch[\"loss_weights\"]  # 各sampleごとのweight\n                    loss = loss * loss_weights\n\n                    if args.min_snr_gamma:\n                        loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma, args.v_parameterization)\n                    if args.scale_v_pred_loss_like_noise_pred:\n                        loss = scale_v_prediction_loss_like_noise_prediction(loss, timesteps, noise_scheduler)\n                    if args.v_pred_like_loss:\n                        loss = add_v_prediction_like_loss(loss, timesteps, noise_scheduler, args.v_pred_like_loss)\n                    if args.debiased_estimation_loss:\n                        loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)\n\n                    loss = loss.mean()  # 平均なのでbatch_sizeで割る必要なし\n\n                    accelerator.backward(loss)\n                    if accelerator.sync_gradients:\n                        self.all_reduce_network(accelerator, network)  # sync DDP grad manually\n                        if args.max_grad_norm != 0.0:\n                            params_to_clip = accelerator.unwrap_model(network).get_trainable_params()\n                            accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                    optimizer.step()\n                    lr_scheduler.step()\n                    optimizer.zero_grad(set_to_none=True)\n\n                if args.scale_weight_norms:\n                    keys_scaled, mean_norm, maximum_norm = accelerator.unwrap_model(network).apply_max_norm_regularization(\n                        args.scale_weight_norms, accelerator.device\n                    )\n                    max_mean_logs = {\"Keys Scaled\": keys_scaled, \"Average key norm\": mean_norm}\n                else:\n                    keys_scaled, mean_norm, maximum_norm = None, None, None\n\n                # Checks if the accelerator has performed an optimization step behind the scenes\n                if accelerator.sync_gradients:\n                    progress_bar.update(1)\n                    global_step += 1\n\n                    self.sample_images(accelerator, args, None, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n                    # 指定ステップごとにモデルを保存\n                    if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                        accelerator.wait_for_everyone()\n                        if accelerator.is_main_process:\n                            ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, global_step)\n                            save_model(ckpt_name, accelerator.unwrap_model(network), global_step, epoch)\n\n                            if args.save_state:\n                                train_util.save_and_remove_state_stepwise(args, accelerator, global_step)\n\n                            remove_step_no = train_util.get_remove_step_no(args, global_step)\n                            if remove_step_no is not None:\n                                remove_ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, remove_step_no)\n                                remove_model(remove_ckpt_name)\n\n                current_loss = loss.detach().item()\n                loss_recorder.add(epoch=epoch, step=step, loss=current_loss)\n                avr_loss: float = loss_recorder.moving_average\n                logs = {\"avr_loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n                progress_bar.set_postfix(**logs)\n\n                if args.scale_weight_norms:\n                    progress_bar.set_postfix(**{**max_mean_logs, **logs})\n\n                if args.logging_dir is not None:\n                    logs = self.generate_step_logs(args, current_loss, avr_loss, lr_scheduler, keys_scaled, mean_norm, maximum_norm)\n                    accelerator.log(logs, step=global_step)\n\n                if global_step >= args.max_train_steps:\n                    break\n\n            if args.logging_dir is not None:\n                logs = {\"loss/epoch\": loss_recorder.moving_average}\n                accelerator.log(logs, step=epoch + 1)\n\n            accelerator.wait_for_everyone()\n\n            # 指定エポックごとにモデルを保存\n            if args.save_every_n_epochs is not None:\n                saving = (epoch + 1) % args.save_every_n_epochs == 0 and (epoch + 1) < num_train_epochs\n                if is_main_process and saving:\n                    ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, epoch + 1)\n                    save_model(ckpt_name, accelerator.unwrap_model(network), global_step, epoch + 1)\n\n                    remove_epoch_no = train_util.get_remove_epoch_no(args, epoch + 1)\n                    if remove_epoch_no is not None:\n                        remove_ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, remove_epoch_no)\n                        remove_model(remove_ckpt_name)\n\n                    if args.save_state:\n                        train_util.save_and_remove_state_on_epoch_end(args, accelerator, epoch + 1)\n\n            self.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n\n            # end of epoch\n\n        # metadata[\"ss_epoch\"] = str(num_train_epochs)\n        metadata[\"ss_training_finished_at\"] = str(time.time())\n\n        if is_main_process:\n            network = accelerator.unwrap_model(network)\n\n        accelerator.end_training()\n\n        if is_main_process and (args.save_state or args.save_state_on_train_end):\n            train_util.save_state_on_train_end(args, accelerator)\n\n        if is_main_process:\n            ckpt_name = train_util.get_last_ckpt_name(args, \".\" + args.save_model_as)\n            save_model(ckpt_name, network, global_step, num_train_epochs, force_sync_upload=True)\n\n            logger.info(\"model saved.\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, True, True, True)\n    train_util.add_training_arguments(parser, True)\n    train_util.add_masked_loss_arguments(parser)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser)\n\n    parser.add_argument(\n        \"--no_metadata\", action=\"store_true\", help=\"do not save metadata in output model / メタデータを出力先モデルに保存しない\"\n    )\n    parser.add_argument(\n        \"--save_model_as\",\n        type=str,\n        default=\"safetensors\",\n        choices=[None, \"ckpt\", \"pt\", \"safetensors\"],\n        help=\"format to save the model (default is .safetensors) / モデル保存時の形式（デフォルトはsafetensors）\",\n    )\n\n    parser.add_argument(\"--unet_lr\", type=float, default=None, help=\"learning rate for U-Net / U-Netの学習率\")\n    parser.add_argument(\"--text_encoder_lr\", type=float, default=None, help=\"learning rate for Text Encoder / Text Encoderの学習率\")\n\n    parser.add_argument(\n        \"--network_weights\", type=str, default=None, help=\"pretrained weights for network / 学習するネットワークの初期重み\"\n    )\n    parser.add_argument(\n        \"--network_module\", type=str, default=None, help=\"network module to train / 学習対象のネットワークのモジュール\"\n    )\n    parser.add_argument(\n        \"--network_dim\",\n        type=int,\n        default=None,\n        help=\"network dimensions (depends on each network) / モジュールの次元数（ネットワークにより定義は異なります）\",\n    )\n    parser.add_argument(\n        \"--network_alpha\",\n        type=float,\n        default=1,\n        help=\"alpha for LoRA weight scaling, default 1 (same as network_dim for same behavior as old version) / LoRaの重み調整のalpha値、デフォルト1（旧バージョンと同じ動作をするにはnetwork_dimと同じ値を指定）\",\n    )\n    parser.add_argument(\n        \"--network_dropout\",\n        type=float,\n        default=None,\n        help=\"Drops neurons out of training every step (0 or None is default behavior (no dropout), 1 would drop all neurons) / 訓練時に毎ステップでニューロンをdropする（0またはNoneはdropoutなし、1は全ニューロンをdropout）\",\n    )\n    parser.add_argument(\n        \"--network_args\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"additional arguments for network (key=value) / ネットワークへの追加の引数\",\n    )\n    parser.add_argument(\n        \"--network_train_unet_only\", action=\"store_true\", help=\"only training U-Net part / U-Net関連部分のみ学習する\"\n    )\n    parser.add_argument(\n        \"--network_train_text_encoder_only\",\n        action=\"store_true\",\n        help=\"only training Text Encoder part / Text Encoder関連部分のみ学習する\",\n    )\n    parser.add_argument(\n        \"--training_comment\",\n        type=str,\n        default=None,\n        help=\"arbitrary comment string stored in metadata / メタデータに記録する任意のコメント文字列\",\n    )\n    parser.add_argument(\n        \"--dim_from_weights\",\n        action=\"store_true\",\n        help=\"automatically determine dim (rank) from network_weights / dim (rank)をnetwork_weightsで指定した重みから自動で決定する\",\n    )\n    parser.add_argument(\n        \"--scale_weight_norms\",\n        type=float,\n        default=None,\n        help=\"Scale the weight of each key pair to help prevent overtraing via exploding gradients. (1 is a good starting point) / 重みの値をスケーリングして勾配爆発を防ぐ（1が初期値としては適当）\",\n    )\n    parser.add_argument(\n        \"--base_weights\",\n        type=str,\n        default=None,\n        nargs=\"*\",\n        help=\"network weights to merge into the model before training / 学習前にあらかじめモデルにマージするnetworkの重みファイル\",\n    )\n    parser.add_argument(\n        \"--base_weights_multiplier\",\n        type=float,\n        default=None,\n        nargs=\"*\",\n        help=\"multiplier for network weights to merge into the model before training / 学習前にあらかじめモデルにマージするnetworkの重みの倍率\",\n    )\n    parser.add_argument(\n        \"--no_half_vae\",\n        action=\"store_true\",\n        help=\"do not use fp16/bf16 VAE in mixed precision (use float VAE) / mixed precisionでも fp16/bf16 VAEを使わずfloat VAEを使う\",\n    )\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    trainer = NetworkTrainer()\n    trainer.train(args)\n"
        },
        {
          "name": "train_textual_inversion.py",
          "type": "blob",
          "size": 37.4912109375,
          "content": "import argparse\nimport math\nimport os\nfrom multiprocessing import Value\nimport toml\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library.device_utils import init_ipex, clean_memory_on_device\n\n\ninit_ipex()\n\nfrom accelerate.utils import set_seed\nfrom diffusers import DDPMScheduler\nfrom transformers import CLIPTokenizer\nfrom library import deepspeed_utils, model_util\n\nimport library.train_util as train_util\nimport library.huggingface_util as huggingface_util\nimport library.config_util as config_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    apply_snr_weight,\n    prepare_scheduler_for_custom_training,\n    scale_v_prediction_loss_like_noise_prediction,\n    add_v_prediction_like_loss,\n    apply_debiased_estimation,\n    apply_masked_loss,\n)\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nimagenet_templates_small = [\n    \"a photo of a {}\",\n    \"a rendering of a {}\",\n    \"a cropped photo of the {}\",\n    \"the photo of a {}\",\n    \"a photo of a clean {}\",\n    \"a photo of a dirty {}\",\n    \"a dark photo of the {}\",\n    \"a photo of my {}\",\n    \"a photo of the cool {}\",\n    \"a close-up photo of a {}\",\n    \"a bright photo of the {}\",\n    \"a cropped photo of a {}\",\n    \"a photo of the {}\",\n    \"a good photo of the {}\",\n    \"a photo of one {}\",\n    \"a close-up photo of the {}\",\n    \"a rendition of the {}\",\n    \"a photo of the clean {}\",\n    \"a rendition of a {}\",\n    \"a photo of a nice {}\",\n    \"a good photo of a {}\",\n    \"a photo of the nice {}\",\n    \"a photo of the small {}\",\n    \"a photo of the weird {}\",\n    \"a photo of the large {}\",\n    \"a photo of a cool {}\",\n    \"a photo of a small {}\",\n]\n\nimagenet_style_templates_small = [\n    \"a painting in the style of {}\",\n    \"a rendering in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"the painting in the style of {}\",\n    \"a clean painting in the style of {}\",\n    \"a dirty painting in the style of {}\",\n    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n]\n\n\nclass TextualInversionTrainer:\n    def __init__(self):\n        self.vae_scale_factor = 0.18215\n        self.is_sdxl = False\n\n    def assert_extra_args(self, args, train_dataset_group):\n        pass\n\n    def load_target_model(self, args, weight_dtype, accelerator):\n        text_encoder, vae, unet, _ = train_util.load_target_model(args, weight_dtype, accelerator)\n        return model_util.get_model_version_str_for_sd1_sd2(args.v2, args.v_parameterization), text_encoder, vae, unet\n\n    def load_tokenizer(self, args):\n        tokenizer = train_util.load_tokenizer(args)\n        return tokenizer\n\n    def assert_token_string(self, token_string, tokenizers: CLIPTokenizer):\n        pass\n\n    def get_text_cond(self, args, accelerator, batch, tokenizers, text_encoders, weight_dtype):\n        with torch.enable_grad():\n            input_ids = batch[\"input_ids\"].to(accelerator.device)\n            encoder_hidden_states = train_util.get_hidden_states(args, input_ids, tokenizers[0], text_encoders[0], None)\n            return encoder_hidden_states\n\n    def call_unet(self, args, accelerator, unet, noisy_latents, timesteps, text_conds, batch, weight_dtype):\n        noise_pred = unet(noisy_latents, timesteps, text_conds).sample\n        return noise_pred\n\n    def sample_images(self, accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet, prompt_replacement):\n        train_util.sample_images(\n            accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet, prompt_replacement\n        )\n\n    def save_weights(self, file, updated_embs, save_dtype, metadata):\n        state_dict = {\"emb_params\": updated_embs[0]}\n\n        if save_dtype is not None:\n            for key in list(state_dict.keys()):\n                v = state_dict[key]\n                v = v.detach().clone().to(\"cpu\").to(save_dtype)\n                state_dict[key] = v\n\n        if os.path.splitext(file)[1] == \".safetensors\":\n            from safetensors.torch import save_file\n\n            save_file(state_dict, file, metadata)\n        else:\n            torch.save(state_dict, file)  # can be loaded in Web UI\n\n    def load_weights(self, file):\n        if os.path.splitext(file)[1] == \".safetensors\":\n            from safetensors.torch import load_file\n\n            data = load_file(file)\n        else:\n            # compatible to Web UI's file format\n            data = torch.load(file, map_location=\"cpu\")\n            if type(data) != dict:\n                raise ValueError(f\"weight file is not dict / 重みファイルがdict形式ではありません: {file}\")\n\n            if \"string_to_param\" in data:  # textual inversion embeddings\n                data = data[\"string_to_param\"]\n                if hasattr(data, \"_parameters\"):  # support old PyTorch?\n                    data = getattr(data, \"_parameters\")\n\n        emb = next(iter(data.values()))\n        if type(emb) != torch.Tensor:\n            raise ValueError(f\"weight file does not contains Tensor / 重みファイルのデータがTensorではありません: {file}\")\n\n        if len(emb.size()) == 1:\n            emb = emb.unsqueeze(0)\n\n        return [emb]\n\n    def train(self, args):\n        if args.output_name is None:\n            args.output_name = args.token_string\n        use_template = args.use_object_template or args.use_style_template\n\n        train_util.verify_training_args(args)\n        train_util.prepare_dataset_args(args, True)\n        setup_logging(args, reset=True)\n\n        cache_latents = args.cache_latents\n\n        if args.seed is not None:\n            set_seed(args.seed)\n\n        tokenizer_or_list = self.load_tokenizer(args)  # list of tokenizer or tokenizer\n        tokenizers = tokenizer_or_list if isinstance(tokenizer_or_list, list) else [tokenizer_or_list]\n\n        # acceleratorを準備する\n        logger.info(\"prepare accelerator\")\n        accelerator = train_util.prepare_accelerator(args)\n\n        # mixed precisionに対応した型を用意しておき適宜castする\n        weight_dtype, save_dtype = train_util.prepare_dtype(args)\n        vae_dtype = torch.float32 if args.no_half_vae else weight_dtype\n\n        # モデルを読み込む\n        model_version, text_encoder_or_list, vae, unet = self.load_target_model(args, weight_dtype, accelerator)\n        text_encoders = [text_encoder_or_list] if not isinstance(text_encoder_or_list, list) else text_encoder_or_list\n\n        if len(text_encoders) > 1 and args.gradient_accumulation_steps > 1:\n            accelerator.print(\n                \"accelerate doesn't seem to support gradient_accumulation_steps for multiple models (text encoders) / \"\n                + \"accelerateでは複数のモデル（テキストエンコーダー）のgradient_accumulation_stepsはサポートされていないようです\"\n            )\n\n        # Convert the init_word to token_id\n        init_token_ids_list = []\n        if args.init_word is not None:\n            for i, tokenizer in enumerate(tokenizers):\n                init_token_ids = tokenizer.encode(args.init_word, add_special_tokens=False)\n                if len(init_token_ids) > 1 and len(init_token_ids) != args.num_vectors_per_token:\n                    accelerator.print(\n                        f\"token length for init words is not same to num_vectors_per_token, init words is repeated or truncated / \"\n                        + f\"初期化単語のトークン長がnum_vectors_per_tokenと合わないため、繰り返しまたは切り捨てが発生します:  tokenizer {i+1}, length {len(init_token_ids)}\"\n                    )\n                init_token_ids_list.append(init_token_ids)\n        else:\n            init_token_ids_list = [None] * len(tokenizers)\n\n        # tokenizerに新しい単語を追加する。追加する単語の数はnum_vectors_per_token\n        # token_stringが hoge の場合、\"hoge\", \"hoge1\", \"hoge2\", ... が追加される\n        # add new word to tokenizer, count is num_vectors_per_token\n        # if token_string is hoge, \"hoge\", \"hoge1\", \"hoge2\", ... are added\n\n        self.assert_token_string(args.token_string, tokenizers)\n\n        token_strings = [args.token_string] + [f\"{args.token_string}{i+1}\" for i in range(args.num_vectors_per_token - 1)]\n        token_ids_list = []\n        token_embeds_list = []\n        for i, (tokenizer, text_encoder, init_token_ids) in enumerate(zip(tokenizers, text_encoders, init_token_ids_list)):\n            num_added_tokens = tokenizer.add_tokens(token_strings)\n            assert (\n                num_added_tokens == args.num_vectors_per_token\n            ), f\"tokenizer has same word to token string. please use another one / 指定したargs.token_stringは既に存在します。別の単語を使ってください: tokenizer {i+1}, {args.token_string}\"\n\n            token_ids = tokenizer.convert_tokens_to_ids(token_strings)\n            accelerator.print(f\"tokens are added for tokenizer {i+1}: {token_ids}\")\n            assert (\n                min(token_ids) == token_ids[0] and token_ids[-1] == token_ids[0] + len(token_ids) - 1\n            ), f\"token ids is not ordered : tokenizer {i+1}, {token_ids}\"\n            assert (\n                len(tokenizer) - 1 == token_ids[-1]\n            ), f\"token ids is not end of tokenize: tokenizer {i+1}, {token_ids}, {len(tokenizer)}\"\n            token_ids_list.append(token_ids)\n\n            # Resize the token embeddings as we are adding new special tokens to the tokenizer\n            text_encoder.resize_token_embeddings(len(tokenizer))\n\n            # Initialise the newly added placeholder token with the embeddings of the initializer token\n            token_embeds = text_encoder.get_input_embeddings().weight.data\n            if init_token_ids is not None:\n                for i, token_id in enumerate(token_ids):\n                    token_embeds[token_id] = token_embeds[init_token_ids[i % len(init_token_ids)]]\n                    # accelerator.print(token_id, token_embeds[token_id].mean(), token_embeds[token_id].min())\n            token_embeds_list.append(token_embeds)\n\n        # load weights\n        if args.weights is not None:\n            embeddings_list = self.load_weights(args.weights)\n            assert len(token_ids) == len(\n                embeddings_list[0]\n            ), f\"num_vectors_per_token is mismatch for weights / 指定した重みとnum_vectors_per_tokenの値が異なります: {len(embeddings)}\"\n            # accelerator.print(token_ids, embeddings.size())\n            for token_ids, embeddings, token_embeds in zip(token_ids_list, embeddings_list, token_embeds_list):\n                for token_id, embedding in zip(token_ids, embeddings):\n                    token_embeds[token_id] = embedding\n                    # accelerator.print(token_id, token_embeds[token_id].mean(), token_embeds[token_id].min())\n            accelerator.print(f\"weighs loaded\")\n\n        accelerator.print(f\"create embeddings for {args.num_vectors_per_token} tokens, for {args.token_string}\")\n\n        # データセットを準備する\n        if args.dataset_class is None:\n            blueprint_generator = BlueprintGenerator(ConfigSanitizer(True, True, args.masked_loss, False))\n            if args.dataset_config is not None:\n                accelerator.print(f\"Load dataset config from {args.dataset_config}\")\n                user_config = config_util.load_user_config(args.dataset_config)\n                ignored = [\"train_data_dir\", \"reg_data_dir\", \"in_json\"]\n                if any(getattr(args, attr) is not None for attr in ignored):\n                    accelerator.print(\n                        \"ignore following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                            \", \".join(ignored)\n                        )\n                    )\n            else:\n                use_dreambooth_method = args.in_json is None\n                if use_dreambooth_method:\n                    accelerator.print(\"Use DreamBooth method.\")\n                    user_config = {\n                        \"datasets\": [\n                            {\n                                \"subsets\": config_util.generate_dreambooth_subsets_config_by_subdirs(\n                                    args.train_data_dir, args.reg_data_dir\n                                )\n                            }\n                        ]\n                    }\n                else:\n                    logger.info(\"Train with captions.\")\n                    user_config = {\n                        \"datasets\": [\n                            {\n                                \"subsets\": [\n                                    {\n                                        \"image_dir\": args.train_data_dir,\n                                        \"metadata_file\": args.in_json,\n                                    }\n                                ]\n                            }\n                        ]\n                    }\n\n            blueprint = blueprint_generator.generate(user_config, args, tokenizer=tokenizer_or_list)\n            train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n        else:\n            train_dataset_group = train_util.load_arbitrary_dataset(args, tokenizer_or_list)\n\n        self.assert_extra_args(args, train_dataset_group)\n\n        current_epoch = Value(\"i\", 0)\n        current_step = Value(\"i\", 0)\n        ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n        collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n        # make captions: tokenstring tokenstring1 tokenstring2 ...tokenstringn という文字列に書き換える超乱暴な実装\n        if use_template:\n            accelerator.print(f\"use template for training captions. is object: {args.use_object_template}\")\n            templates = imagenet_templates_small if args.use_object_template else imagenet_style_templates_small\n            replace_to = \" \".join(token_strings)\n            captions = []\n            for tmpl in templates:\n                captions.append(tmpl.format(replace_to))\n            train_dataset_group.add_replacement(\"\", captions)\n\n            # サンプル生成用\n            if args.num_vectors_per_token > 1:\n                prompt_replacement = (args.token_string, replace_to)\n            else:\n                prompt_replacement = None\n        else:\n            # サンプル生成用\n            if args.num_vectors_per_token > 1:\n                replace_to = \" \".join(token_strings)\n                train_dataset_group.add_replacement(args.token_string, replace_to)\n                prompt_replacement = (args.token_string, replace_to)\n            else:\n                prompt_replacement = None\n\n        if args.debug_dataset:\n            train_util.debug_dataset(train_dataset_group, show_input_ids=True)\n            return\n        if len(train_dataset_group) == 0:\n            accelerator.print(\"No data found. Please verify arguments / 画像がありません。引数指定を確認してください\")\n            return\n\n        if cache_latents:\n            assert (\n                train_dataset_group.is_latent_cacheable()\n            ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n\n        # モデルに xformers とか memory efficient attention を組み込む\n        train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n        if torch.__version__ >= \"2.0.0\":  # PyTorch 2.0.0 以上対応のxformersなら以下が使える\n            vae.set_use_memory_efficient_attention_xformers(args.xformers)\n\n        # 学習を準備する\n        if cache_latents:\n            vae.to(accelerator.device, dtype=vae_dtype)\n            vae.requires_grad_(False)\n            vae.eval()\n            with torch.no_grad():\n                train_dataset_group.cache_latents(vae, args.vae_batch_size, args.cache_latents_to_disk, accelerator.is_main_process)\n            vae.to(\"cpu\")\n            clean_memory_on_device(accelerator.device)\n\n            accelerator.wait_for_everyone()\n\n        if args.gradient_checkpointing:\n            unet.enable_gradient_checkpointing()\n            for text_encoder in text_encoders:\n                text_encoder.gradient_checkpointing_enable()\n\n        # 学習に必要なクラスを準備する\n        accelerator.print(\"prepare optimizer, data loader etc.\")\n        trainable_params = []\n        for text_encoder in text_encoders:\n            trainable_params += text_encoder.get_input_embeddings().parameters()\n        _, _, optimizer = train_util.get_optimizer(args, trainable_params)\n\n        # dataloaderを準備する\n        # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n        n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n        train_dataloader = torch.utils.data.DataLoader(\n            train_dataset_group,\n            batch_size=1,\n            shuffle=True,\n            collate_fn=collator,\n            num_workers=n_workers,\n            persistent_workers=args.persistent_data_loader_workers,\n        )\n\n        # 学習ステップ数を計算する\n        if args.max_train_epochs is not None:\n            args.max_train_steps = args.max_train_epochs * math.ceil(\n                len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n            )\n            accelerator.print(\n                f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n            )\n\n        # データセット側にも学習ステップを送信\n        train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n        # lr schedulerを用意する\n        lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n        # acceleratorがなんかよろしくやってくれるらしい\n        if len(text_encoders) == 1:\n            text_encoder_or_list, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n                text_encoder_or_list, optimizer, train_dataloader, lr_scheduler\n            )\n\n        elif len(text_encoders) == 2:\n            text_encoder1, text_encoder2, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n                text_encoders[0], text_encoders[1], optimizer, train_dataloader, lr_scheduler\n            )\n\n            text_encoder_or_list = text_encoders = [text_encoder1, text_encoder2]\n\n        else:\n            raise NotImplementedError()\n\n        index_no_updates_list = []\n        orig_embeds_params_list = []\n        for tokenizer, token_ids, text_encoder in zip(tokenizers, token_ids_list, text_encoders):\n            index_no_updates = torch.arange(len(tokenizer)) < token_ids[0]\n            index_no_updates_list.append(index_no_updates)\n\n            # accelerator.print(len(index_no_updates), torch.sum(index_no_updates))\n            orig_embeds_params = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight.data.detach().clone()\n            orig_embeds_params_list.append(orig_embeds_params)\n\n            # Freeze all parameters except for the token embeddings in text encoder\n            text_encoder.requires_grad_(True)\n            unwrapped_text_encoder = accelerator.unwrap_model(text_encoder)\n            unwrapped_text_encoder.text_model.encoder.requires_grad_(False)\n            unwrapped_text_encoder.text_model.final_layer_norm.requires_grad_(False)\n            unwrapped_text_encoder.text_model.embeddings.position_embedding.requires_grad_(False)\n            # text_encoder.text_model.embeddings.token_embedding.requires_grad_(True)\n\n        unet.requires_grad_(False)\n        unet.to(accelerator.device, dtype=weight_dtype)\n        if args.gradient_checkpointing:  # according to TI example in Diffusers, train is required\n            # TODO U-Netをオリジナルに置き換えたのでいらないはずなので、後で確認して消す\n            unet.train()\n        else:\n            unet.eval()\n\n        if not cache_latents:  # キャッシュしない場合はVAEを使うのでVAEを準備する\n            vae.requires_grad_(False)\n            vae.eval()\n            vae.to(accelerator.device, dtype=vae_dtype)\n\n        # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n        if args.full_fp16:\n            train_util.patch_accelerator_for_fp16_training(accelerator)\n            for text_encoder in text_encoders:\n                text_encoder.to(weight_dtype)\n        if args.full_bf16:\n            for text_encoder in text_encoders:\n                text_encoder.to(weight_dtype)\n\n        # resumeする\n        train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n        # epoch数を計算する\n        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n        num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n        if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n            args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n        # 学習する\n        total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n        accelerator.print(\"running training / 学習開始\")\n        accelerator.print(f\"  num train images * repeats / 学習画像の数×繰り返し回数: {train_dataset_group.num_train_images}\")\n        accelerator.print(f\"  num reg images / 正則化画像の数: {train_dataset_group.num_reg_images}\")\n        accelerator.print(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n        accelerator.print(f\"  num epochs / epoch数: {num_train_epochs}\")\n        accelerator.print(f\"  batch size per device / バッチサイズ: {args.train_batch_size}\")\n        accelerator.print(\n            f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\"\n        )\n        accelerator.print(f\"  gradient ccumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n        accelerator.print(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n        progress_bar = tqdm(range(args.max_train_steps), smoothing=0, disable=not accelerator.is_local_main_process, desc=\"steps\")\n        global_step = 0\n\n        noise_scheduler = DDPMScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, clip_sample=False\n        )\n        prepare_scheduler_for_custom_training(noise_scheduler, accelerator.device)\n        if args.zero_terminal_snr:\n            custom_train_functions.fix_noise_scheduler_betas_for_zero_terminal_snr(noise_scheduler)\n\n        if accelerator.is_main_process:\n            init_kwargs = {}\n            if args.wandb_run_name:\n                init_kwargs[\"wandb\"] = {\"name\": args.wandb_run_name}\n            if args.log_tracker_config is not None:\n                init_kwargs = toml.load(args.log_tracker_config)\n            accelerator.init_trackers(\n                \"textual_inversion\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs\n            )\n\n        # function for saving/removing\n        def save_model(ckpt_name, embs_list, steps, epoch_no, force_sync_upload=False):\n            os.makedirs(args.output_dir, exist_ok=True)\n            ckpt_file = os.path.join(args.output_dir, ckpt_name)\n\n            accelerator.print(f\"\\nsaving checkpoint: {ckpt_file}\")\n\n            sai_metadata = train_util.get_sai_model_spec(None, args, self.is_sdxl, False, True)\n\n            self.save_weights(ckpt_file, embs_list, save_dtype, sai_metadata)\n            if args.huggingface_repo_id is not None:\n                huggingface_util.upload(args, ckpt_file, \"/\" + ckpt_name, force_sync_upload=force_sync_upload)\n\n        def remove_model(old_ckpt_name):\n            old_ckpt_file = os.path.join(args.output_dir, old_ckpt_name)\n            if os.path.exists(old_ckpt_file):\n                accelerator.print(f\"removing old checkpoint: {old_ckpt_file}\")\n                os.remove(old_ckpt_file)\n\n        # For --sample_at_first\n        self.sample_images(\n            accelerator,\n            args,\n            0,\n            global_step,\n            accelerator.device,\n            vae,\n            tokenizer_or_list,\n            text_encoder_or_list,\n            unet,\n            prompt_replacement,\n        )\n\n        # training loop\n        for epoch in range(num_train_epochs):\n            accelerator.print(f\"\\nepoch {epoch+1}/{num_train_epochs}\")\n            current_epoch.value = epoch + 1\n\n            for text_encoder in text_encoders:\n                text_encoder.train()\n\n            loss_total = 0\n\n            for step, batch in enumerate(train_dataloader):\n                current_step.value = global_step\n                with accelerator.accumulate(text_encoders[0]):\n                    with torch.no_grad():\n                        if \"latents\" in batch and batch[\"latents\"] is not None:\n                            latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                        else:\n                            # latentに変換\n                            latents = vae.encode(batch[\"images\"].to(dtype=vae_dtype)).latent_dist.sample().to(dtype=weight_dtype)\n                        latents = latents * self.vae_scale_factor\n\n                    # Get the text embedding for conditioning\n                    text_encoder_conds = self.get_text_cond(args, accelerator, batch, tokenizers, text_encoders, weight_dtype)\n\n                    # Sample noise, sample a random timestep for each image, and add noise to the latents,\n                    # with noise offset and/or multires noise if specified\n                    noise, noisy_latents, timesteps, huber_c = train_util.get_noise_noisy_latents_and_timesteps(\n                        args, noise_scheduler, latents\n                    )\n\n                    # Predict the noise residual\n                    with accelerator.autocast():\n                        noise_pred = self.call_unet(\n                            args, accelerator, unet, noisy_latents, timesteps, text_encoder_conds, batch, weight_dtype\n                        )\n\n                    if args.v_parameterization:\n                        # v-parameterization training\n                        target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                    else:\n                        target = noise\n\n                    loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c)\n                    if args.masked_loss:\n                        loss = apply_masked_loss(loss, batch)\n                    loss = loss.mean([1, 2, 3])\n\n                    loss_weights = batch[\"loss_weights\"]  # 各sampleごとのweight\n                    loss = loss * loss_weights\n\n                    if args.min_snr_gamma:\n                        loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma, args.v_parameterization)\n                    if args.scale_v_pred_loss_like_noise_pred:\n                        loss = scale_v_prediction_loss_like_noise_prediction(loss, timesteps, noise_scheduler)\n                    if args.v_pred_like_loss:\n                        loss = add_v_prediction_like_loss(loss, timesteps, noise_scheduler, args.v_pred_like_loss)\n                    if args.debiased_estimation_loss:\n                        loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)\n\n                    loss = loss.mean()  # 平均なのでbatch_sizeで割る必要なし\n\n                    accelerator.backward(loss)\n                    if accelerator.sync_gradients and args.max_grad_norm != 0.0:\n                        params_to_clip = accelerator.unwrap_model(text_encoder).get_input_embeddings().parameters()\n                        accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                    optimizer.step()\n                    lr_scheduler.step()\n                    optimizer.zero_grad(set_to_none=True)\n\n                    # Let's make sure we don't update any embedding weights besides the newly added token\n                    with torch.no_grad():\n                        for text_encoder, orig_embeds_params, index_no_updates in zip(\n                            text_encoders, orig_embeds_params_list, index_no_updates_list\n                        ):\n                            # if full_fp16/bf16, input_embeddings_weight is fp16/bf16, orig_embeds_params is fp32\n                            input_embeddings_weight = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight\n                            input_embeddings_weight[index_no_updates] = orig_embeds_params.to(input_embeddings_weight.dtype)[\n                                index_no_updates\n                            ]\n\n                # Checks if the accelerator has performed an optimization step behind the scenes\n                if accelerator.sync_gradients:\n                    progress_bar.update(1)\n                    global_step += 1\n\n                    self.sample_images(\n                        accelerator,\n                        args,\n                        None,\n                        global_step,\n                        accelerator.device,\n                        vae,\n                        tokenizer_or_list,\n                        text_encoder_or_list,\n                        unet,\n                        prompt_replacement,\n                    )\n\n                    # 指定ステップごとにモデルを保存\n                    if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                        accelerator.wait_for_everyone()\n                        if accelerator.is_main_process:\n                            updated_embs_list = []\n                            for text_encoder, token_ids in zip(text_encoders, token_ids_list):\n                                updated_embs = (\n                                    accelerator.unwrap_model(text_encoder)\n                                    .get_input_embeddings()\n                                    .weight[token_ids]\n                                    .data.detach()\n                                    .clone()\n                                )\n                                updated_embs_list.append(updated_embs)\n\n                            ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, global_step)\n                            save_model(ckpt_name, updated_embs_list, global_step, epoch)\n\n                            if args.save_state:\n                                train_util.save_and_remove_state_stepwise(args, accelerator, global_step)\n\n                            remove_step_no = train_util.get_remove_step_no(args, global_step)\n                            if remove_step_no is not None:\n                                remove_ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, remove_step_no)\n                                remove_model(remove_ckpt_name)\n\n                current_loss = loss.detach().item()\n                if args.logging_dir is not None:\n                    logs = {\"loss\": current_loss, \"lr\": float(lr_scheduler.get_last_lr()[0])}\n                    if (\n                        args.optimizer_type.lower().startswith(\"DAdapt\".lower()) or args.optimizer_type.lower() == \"Prodigy\".lower()\n                    ):  # tracking d*lr value\n                        logs[\"lr/d*lr\"] = (\n                            lr_scheduler.optimizers[0].param_groups[0][\"d\"] * lr_scheduler.optimizers[0].param_groups[0][\"lr\"]\n                        )\n                    accelerator.log(logs, step=global_step)\n\n                loss_total += current_loss\n                avr_loss = loss_total / (step + 1)\n                logs = {\"loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n                progress_bar.set_postfix(**logs)\n\n                if global_step >= args.max_train_steps:\n                    break\n\n            if args.logging_dir is not None:\n                logs = {\"loss/epoch\": loss_total / len(train_dataloader)}\n                accelerator.log(logs, step=epoch + 1)\n\n            accelerator.wait_for_everyone()\n\n            updated_embs_list = []\n            for text_encoder, token_ids in zip(text_encoders, token_ids_list):\n                updated_embs = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[token_ids].data.detach().clone()\n                updated_embs_list.append(updated_embs)\n\n            if args.save_every_n_epochs is not None:\n                saving = (epoch + 1) % args.save_every_n_epochs == 0 and (epoch + 1) < num_train_epochs\n                if accelerator.is_main_process and saving:\n                    ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, epoch + 1)\n                    save_model(ckpt_name, updated_embs_list, epoch + 1, global_step)\n\n                    remove_epoch_no = train_util.get_remove_epoch_no(args, epoch + 1)\n                    if remove_epoch_no is not None:\n                        remove_ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, remove_epoch_no)\n                        remove_model(remove_ckpt_name)\n\n                    if args.save_state:\n                        train_util.save_and_remove_state_on_epoch_end(args, accelerator, epoch + 1)\n\n            self.sample_images(\n                accelerator,\n                args,\n                epoch + 1,\n                global_step,\n                accelerator.device,\n                vae,\n                tokenizer_or_list,\n                text_encoder_or_list,\n                unet,\n                prompt_replacement,\n            )\n\n            # end of epoch\n\n        is_main_process = accelerator.is_main_process\n        if is_main_process:\n            text_encoder = accelerator.unwrap_model(text_encoder)\n            updated_embs = text_encoder.get_input_embeddings().weight[token_ids].data.detach().clone()\n\n        accelerator.end_training()\n\n        if is_main_process and (args.save_state or args.save_state_on_train_end):\n            train_util.save_state_on_train_end(args, accelerator)\n\n        if is_main_process:\n            ckpt_name = train_util.get_last_ckpt_name(args, \".\" + args.save_model_as)\n            save_model(ckpt_name, updated_embs_list, global_step, num_train_epochs, force_sync_upload=True)\n\n            logger.info(\"model saved.\")\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, True, True, False)\n    train_util.add_training_arguments(parser, True)\n    train_util.add_masked_loss_arguments(parser)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser, False)\n\n    parser.add_argument(\n        \"--save_model_as\",\n        type=str,\n        default=\"pt\",\n        choices=[None, \"ckpt\", \"pt\", \"safetensors\"],\n        help=\"format to save the model (default is .pt) / モデル保存時の形式（デフォルトはpt）\",\n    )\n\n    parser.add_argument(\n        \"--weights\", type=str, default=None, help=\"embedding weights to initialize / 学習するネットワークの初期重み\"\n    )\n    parser.add_argument(\n        \"--num_vectors_per_token\", type=int, default=1, help=\"number of vectors per token / トークンに割り当てるembeddingsの要素数\"\n    )\n    parser.add_argument(\n        \"--token_string\",\n        type=str,\n        default=None,\n        help=\"token string used in training, must not exist in tokenizer / 学習時に使用されるトークン文字列、tokenizerに存在しない文字であること\",\n    )\n    parser.add_argument(\n        \"--init_word\", type=str, default=None, help=\"words to initialize vector / ベクトルを初期化に使用する単語、複数可\"\n    )\n    parser.add_argument(\n        \"--use_object_template\",\n        action=\"store_true\",\n        help=\"ignore caption and use default templates for object / キャプションは使わずデフォルトの物体用テンプレートで学習する\",\n    )\n    parser.add_argument(\n        \"--use_style_template\",\n        action=\"store_true\",\n        help=\"ignore caption and use default templates for stype / キャプションは使わずデフォルトのスタイル用テンプレートで学習する\",\n    )\n    parser.add_argument(\n        \"--no_half_vae\",\n        action=\"store_true\",\n        help=\"do not use fp16/bf16 VAE in mixed precision (use float VAE) / mixed precisionでも fp16/bf16 VAEを使わずfloat VAEを使う\",\n    )\n\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    trainer = TextualInversionTrainer()\n    trainer.train(args)\n"
        },
        {
          "name": "train_textual_inversion_XTI.py",
          "type": "blob",
          "size": 29.953125,
          "content": "import importlib\nimport argparse\nimport math\nimport os\nimport toml\nfrom multiprocessing import Value\n\nfrom tqdm import tqdm\n\nimport torch\nfrom library import deepspeed_utils\nfrom library.device_utils import init_ipex, clean_memory_on_device\n\ninit_ipex()\n\nfrom accelerate.utils import set_seed\nimport diffusers\nfrom diffusers import DDPMScheduler\nimport library\n\nimport library.train_util as train_util\nimport library.huggingface_util as huggingface_util\nimport library.config_util as config_util\nfrom library.config_util import (\n    ConfigSanitizer,\n    BlueprintGenerator,\n)\nimport library.custom_train_functions as custom_train_functions\nfrom library.custom_train_functions import (\n    apply_snr_weight,\n    prepare_scheduler_for_custom_training,\n    pyramid_noise_like,\n    apply_noise_offset,\n    scale_v_prediction_loss_like_noise_prediction,\n    apply_debiased_estimation,\n    apply_masked_loss,\n)\nimport library.original_unet as original_unet\nfrom XTI_hijack import unet_forward_XTI, downblock_forward_XTI, upblock_forward_XTI\nfrom library.utils import setup_logging, add_logging_arguments\n\nsetup_logging()\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nimagenet_templates_small = [\n    \"a photo of a {}\",\n    \"a rendering of a {}\",\n    \"a cropped photo of the {}\",\n    \"the photo of a {}\",\n    \"a photo of a clean {}\",\n    \"a photo of a dirty {}\",\n    \"a dark photo of the {}\",\n    \"a photo of my {}\",\n    \"a photo of the cool {}\",\n    \"a close-up photo of a {}\",\n    \"a bright photo of the {}\",\n    \"a cropped photo of a {}\",\n    \"a photo of the {}\",\n    \"a good photo of the {}\",\n    \"a photo of one {}\",\n    \"a close-up photo of the {}\",\n    \"a rendition of the {}\",\n    \"a photo of the clean {}\",\n    \"a rendition of a {}\",\n    \"a photo of a nice {}\",\n    \"a good photo of a {}\",\n    \"a photo of the nice {}\",\n    \"a photo of the small {}\",\n    \"a photo of the weird {}\",\n    \"a photo of the large {}\",\n    \"a photo of a cool {}\",\n    \"a photo of a small {}\",\n]\n\nimagenet_style_templates_small = [\n    \"a painting in the style of {}\",\n    \"a rendering in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"the painting in the style of {}\",\n    \"a clean painting in the style of {}\",\n    \"a dirty painting in the style of {}\",\n    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n]\n\n\ndef train(args):\n    if args.output_name is None:\n        args.output_name = args.token_string\n    use_template = args.use_object_template or args.use_style_template\n    setup_logging(args, reset=True)\n\n    train_util.verify_training_args(args)\n    train_util.prepare_dataset_args(args, True)\n\n    if args.sample_every_n_steps is not None or args.sample_every_n_epochs is not None:\n        logger.warning(\n            \"sample_every_n_steps and sample_every_n_epochs are not supported in this script currently / sample_every_n_stepsとsample_every_n_epochsは現在このスクリプトではサポートされていません\"\n        )\n    assert (\n        args.dataset_class is None\n    ), \"dataset_class is not supported in this script currently / dataset_classは現在このスクリプトではサポートされていません\"\n\n    cache_latents = args.cache_latents\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    tokenizer = train_util.load_tokenizer(args)\n\n    # acceleratorを準備する\n    logger.info(\"prepare accelerator\")\n    accelerator = train_util.prepare_accelerator(args)\n\n    # mixed precisionに対応した型を用意しておき適宜castする\n    weight_dtype, save_dtype = train_util.prepare_dtype(args)\n\n    # モデルを読み込む\n    text_encoder, vae, unet, _ = train_util.load_target_model(args, weight_dtype, accelerator)\n\n    # Convert the init_word to token_id\n    if args.init_word is not None:\n        init_token_ids = tokenizer.encode(args.init_word, add_special_tokens=False)\n        if len(init_token_ids) > 1 and len(init_token_ids) != args.num_vectors_per_token:\n            logger.warning(\n                f\"token length for init words is not same to num_vectors_per_token, init words is repeated or truncated / 初期化単語のトークン長がnum_vectors_per_tokenと合わないため、繰り返しまたは切り捨てが発生します: length {len(init_token_ids)}\"\n            )\n    else:\n        init_token_ids = None\n\n    # add new word to tokenizer, count is num_vectors_per_token\n    token_strings = [args.token_string] + [f\"{args.token_string}{i+1}\" for i in range(args.num_vectors_per_token - 1)]\n    num_added_tokens = tokenizer.add_tokens(token_strings)\n    assert (\n        num_added_tokens == args.num_vectors_per_token\n    ), f\"tokenizer has same word to token string. please use another one / 指定したargs.token_stringは既に存在します。別の単語を使ってください: {args.token_string}\"\n\n    token_ids = tokenizer.convert_tokens_to_ids(token_strings)\n    logger.info(f\"tokens are added: {token_ids}\")\n    assert min(token_ids) == token_ids[0] and token_ids[-1] == token_ids[0] + len(token_ids) - 1, f\"token ids is not ordered\"\n    assert len(tokenizer) - 1 == token_ids[-1], f\"token ids is not end of tokenize: {len(tokenizer)}\"\n\n    token_strings_XTI = []\n    XTI_layers = [\n        \"IN01\",\n        \"IN02\",\n        \"IN04\",\n        \"IN05\",\n        \"IN07\",\n        \"IN08\",\n        \"MID\",\n        \"OUT03\",\n        \"OUT04\",\n        \"OUT05\",\n        \"OUT06\",\n        \"OUT07\",\n        \"OUT08\",\n        \"OUT09\",\n        \"OUT10\",\n        \"OUT11\",\n    ]\n    for layer_name in XTI_layers:\n        token_strings_XTI += [f\"{t}_{layer_name}\" for t in token_strings]\n\n    tokenizer.add_tokens(token_strings_XTI)\n    token_ids_XTI = tokenizer.convert_tokens_to_ids(token_strings_XTI)\n    logger.info(f\"tokens are added (XTI): {token_ids_XTI}\")\n    # Resize the token embeddings as we are adding new special tokens to the tokenizer\n    text_encoder.resize_token_embeddings(len(tokenizer))\n\n    # Initialise the newly added placeholder token with the embeddings of the initializer token\n    token_embeds = text_encoder.get_input_embeddings().weight.data\n    if init_token_ids is not None:\n        for i, token_id in enumerate(token_ids_XTI):\n            token_embeds[token_id] = token_embeds[init_token_ids[(i // 16) % len(init_token_ids)]]\n            # logger.info(token_id, token_embeds[token_id].mean(), token_embeds[token_id].min())\n\n    # load weights\n    if args.weights is not None:\n        embeddings = load_weights(args.weights)\n        assert len(token_ids) == len(\n            embeddings\n        ), f\"num_vectors_per_token is mismatch for weights / 指定した重みとnum_vectors_per_tokenの値が異なります: {len(embeddings)}\"\n        # logger.info(token_ids, embeddings.size())\n        for token_id, embedding in zip(token_ids_XTI, embeddings):\n            token_embeds[token_id] = embedding\n            # logger.info(token_id, token_embeds[token_id].mean(), token_embeds[token_id].min())\n        logger.info(f\"weighs loaded\")\n\n    logger.info(f\"create embeddings for {args.num_vectors_per_token} tokens, for {args.token_string}\")\n\n    # データセットを準備する\n    blueprint_generator = BlueprintGenerator(ConfigSanitizer(True, True, args.masked_loss, False))\n    if args.dataset_config is not None:\n        logger.info(f\"Load dataset config from {args.dataset_config}\")\n        user_config = config_util.load_user_config(args.dataset_config)\n        ignored = [\"train_data_dir\", \"reg_data_dir\", \"in_json\"]\n        if any(getattr(args, attr) is not None for attr in ignored):\n            logger.info(\n                \"ignore following options because config file is found: {0} / 設定ファイルが利用されるため以下のオプションは無視されます: {0}\".format(\n                    \", \".join(ignored)\n                )\n            )\n    else:\n        use_dreambooth_method = args.in_json is None\n        if use_dreambooth_method:\n            logger.info(\"Use DreamBooth method.\")\n            user_config = {\n                \"datasets\": [\n                    {\"subsets\": config_util.generate_dreambooth_subsets_config_by_subdirs(args.train_data_dir, args.reg_data_dir)}\n                ]\n            }\n        else:\n            logger.info(\"Train with captions.\")\n            user_config = {\n                \"datasets\": [\n                    {\n                        \"subsets\": [\n                            {\n                                \"image_dir\": args.train_data_dir,\n                                \"metadata_file\": args.in_json,\n                            }\n                        ]\n                    }\n                ]\n            }\n\n    blueprint = blueprint_generator.generate(user_config, args, tokenizer=tokenizer)\n    train_dataset_group = config_util.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n    train_dataset_group.enable_XTI(XTI_layers, token_strings=token_strings)\n    current_epoch = Value(\"i\", 0)\n    current_step = Value(\"i\", 0)\n    ds_for_collator = train_dataset_group if args.max_data_loader_n_workers == 0 else None\n    collator = train_util.collator_class(current_epoch, current_step, ds_for_collator)\n\n    # make captions: tokenstring tokenstring1 tokenstring2 ...tokenstringn という文字列に書き換える超乱暴な実装\n    if use_template:\n        logger.info(f\"use template for training captions. is object: {args.use_object_template}\")\n        templates = imagenet_templates_small if args.use_object_template else imagenet_style_templates_small\n        replace_to = \" \".join(token_strings)\n        captions = []\n        for tmpl in templates:\n            captions.append(tmpl.format(replace_to))\n        train_dataset_group.add_replacement(\"\", captions)\n\n        if args.num_vectors_per_token > 1:\n            prompt_replacement = (args.token_string, replace_to)\n        else:\n            prompt_replacement = None\n    else:\n        if args.num_vectors_per_token > 1:\n            replace_to = \" \".join(token_strings)\n            train_dataset_group.add_replacement(args.token_string, replace_to)\n            prompt_replacement = (args.token_string, replace_to)\n        else:\n            prompt_replacement = None\n\n    if args.debug_dataset:\n        train_util.debug_dataset(train_dataset_group, show_input_ids=True)\n        return\n    if len(train_dataset_group) == 0:\n        logger.error(\"No data found. Please verify arguments / 画像がありません。引数指定を確認してください\")\n        return\n\n    if cache_latents:\n        assert (\n            train_dataset_group.is_latent_cacheable()\n        ), \"when caching latents, either color_aug or random_crop cannot be used / latentをキャッシュするときはcolor_augとrandom_cropは使えません\"\n\n    # モデルに xformers とか memory efficient attention を組み込む\n    train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n    original_unet.UNet2DConditionModel.forward = unet_forward_XTI\n    original_unet.CrossAttnDownBlock2D.forward = downblock_forward_XTI\n    original_unet.CrossAttnUpBlock2D.forward = upblock_forward_XTI\n\n    # 学習を準備する\n    if cache_latents:\n        vae.to(accelerator.device, dtype=weight_dtype)\n        vae.requires_grad_(False)\n        vae.eval()\n        with torch.no_grad():\n            train_dataset_group.cache_latents(vae, args.vae_batch_size, args.cache_latents_to_disk, accelerator.is_main_process)\n        vae.to(\"cpu\")\n        clean_memory_on_device(accelerator.device)\n\n        accelerator.wait_for_everyone()\n\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n        text_encoder.gradient_checkpointing_enable()\n\n    # 学習に必要なクラスを準備する\n    logger.info(\"prepare optimizer, data loader etc.\")\n    trainable_params = text_encoder.get_input_embeddings().parameters()\n    _, _, optimizer = train_util.get_optimizer(args, trainable_params)\n\n    # dataloaderを準備する\n    # DataLoaderのプロセス数：0 は persistent_workers が使えないので注意\n    n_workers = min(args.max_data_loader_n_workers, os.cpu_count())  # cpu_count or max_data_loader_n_workers\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset_group,\n        batch_size=1,\n        shuffle=True,\n        collate_fn=collator,\n        num_workers=n_workers,\n        persistent_workers=args.persistent_data_loader_workers,\n    )\n\n    # 学習ステップ数を計算する\n    if args.max_train_epochs is not None:\n        args.max_train_steps = args.max_train_epochs * math.ceil(\n            len(train_dataloader) / accelerator.num_processes / args.gradient_accumulation_steps\n        )\n        logger.info(\n            f\"override steps. steps for {args.max_train_epochs} epochs is / 指定エポックまでのステップ数: {args.max_train_steps}\"\n        )\n\n    # データセット側にも学習ステップを送信\n    train_dataset_group.set_max_train_steps(args.max_train_steps)\n\n    # lr schedulerを用意する\n    lr_scheduler = train_util.get_scheduler_fix(args, optimizer, accelerator.num_processes)\n\n    # acceleratorがなんかよろしくやってくれるらしい\n    text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        text_encoder, optimizer, train_dataloader, lr_scheduler\n    )\n\n    index_no_updates = torch.arange(len(tokenizer)) < token_ids_XTI[0]\n    # logger.info(len(index_no_updates), torch.sum(index_no_updates))\n    orig_embeds_params = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight.data.detach().clone()\n\n    # Freeze all parameters except for the token embeddings in text encoder\n    text_encoder.requires_grad_(True)\n    text_encoder.text_model.encoder.requires_grad_(False)\n    text_encoder.text_model.final_layer_norm.requires_grad_(False)\n    text_encoder.text_model.embeddings.position_embedding.requires_grad_(False)\n    # text_encoder.text_model.embeddings.token_embedding.requires_grad_(True)\n\n    unet.requires_grad_(False)\n    unet.to(accelerator.device, dtype=weight_dtype)\n    if args.gradient_checkpointing:  # according to TI example in Diffusers, train is required\n        unet.train()\n    else:\n        unet.eval()\n\n    if not cache_latents:\n        vae.requires_grad_(False)\n        vae.eval()\n        vae.to(accelerator.device, dtype=weight_dtype)\n\n    # 実験的機能：勾配も含めたfp16学習を行う　PyTorchにパッチを当ててfp16でのgrad scaleを有効にする\n    if args.full_fp16:\n        train_util.patch_accelerator_for_fp16_training(accelerator)\n        text_encoder.to(weight_dtype)\n\n    # resumeする\n    train_util.resume_from_local_or_hf_if_specified(accelerator, args)\n\n    # epoch数を計算する\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    if (args.save_n_epoch_ratio is not None) and (args.save_n_epoch_ratio > 0):\n        args.save_every_n_epochs = math.floor(num_train_epochs / args.save_n_epoch_ratio) or 1\n\n    # 学習する\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info(\"running training / 学習開始\")\n    logger.info(f\"  num train images * repeats / 学習画像の数×繰り返し回数: {train_dataset_group.num_train_images}\")\n    logger.info(f\"  num reg images / 正則化画像の数: {train_dataset_group.num_reg_images}\")\n    logger.info(f\"  num batches per epoch / 1epochのバッチ数: {len(train_dataloader)}\")\n    logger.info(f\"  num epochs / epoch数: {num_train_epochs}\")\n    logger.info(f\"  batch size per device / バッチサイズ: {args.train_batch_size}\")\n    logger.info(\n        f\"  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: {total_batch_size}\"\n    )\n    logger.info(f\"  gradient ccumulation steps / 勾配を合計するステップ数 = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  total optimization steps / 学習ステップ数: {args.max_train_steps}\")\n\n    progress_bar = tqdm(range(args.max_train_steps), smoothing=0, disable=not accelerator.is_local_main_process, desc=\"steps\")\n    global_step = 0\n\n    noise_scheduler = DDPMScheduler(\n        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, clip_sample=False\n    )\n    prepare_scheduler_for_custom_training(noise_scheduler, accelerator.device)\n    if args.zero_terminal_snr:\n        custom_train_functions.fix_noise_scheduler_betas_for_zero_terminal_snr(noise_scheduler)\n\n    if accelerator.is_main_process:\n        init_kwargs = {}\n        if args.wandb_run_name:\n            init_kwargs[\"wandb\"] = {\"name\": args.wandb_run_name}\n        if args.log_tracker_config is not None:\n            init_kwargs = toml.load(args.log_tracker_config)\n        accelerator.init_trackers(\n            \"textual_inversion\" if args.log_tracker_name is None else args.log_tracker_name, init_kwargs=init_kwargs\n        )\n\n    # function for saving/removing\n    def save_model(ckpt_name, embs, steps, epoch_no, force_sync_upload=False):\n        os.makedirs(args.output_dir, exist_ok=True)\n        ckpt_file = os.path.join(args.output_dir, ckpt_name)\n\n        logger.info(\"\")\n        logger.info(f\"saving checkpoint: {ckpt_file}\")\n        save_weights(ckpt_file, embs, save_dtype)\n        if args.huggingface_repo_id is not None:\n            huggingface_util.upload(args, ckpt_file, \"/\" + ckpt_name, force_sync_upload=force_sync_upload)\n\n    def remove_model(old_ckpt_name):\n        old_ckpt_file = os.path.join(args.output_dir, old_ckpt_name)\n        if os.path.exists(old_ckpt_file):\n            logger.info(f\"removing old checkpoint: {old_ckpt_file}\")\n            os.remove(old_ckpt_file)\n\n    # training loop\n    for epoch in range(num_train_epochs):\n        logger.info(\"\")\n        logger.info(f\"epoch {epoch+1}/{num_train_epochs}\")\n        current_epoch.value = epoch + 1\n\n        text_encoder.train()\n\n        loss_total = 0\n\n        for step, batch in enumerate(train_dataloader):\n            current_step.value = global_step\n            with accelerator.accumulate(text_encoder):\n                with torch.no_grad():\n                    if \"latents\" in batch and batch[\"latents\"] is not None:\n                        latents = batch[\"latents\"].to(accelerator.device).to(dtype=weight_dtype)\n                    else:\n                        # latentに変換\n                        latents = vae.encode(batch[\"images\"].to(dtype=weight_dtype)).latent_dist.sample()\n                    latents = latents * 0.18215\n                b_size = latents.shape[0]\n\n                # Get the text embedding for conditioning\n                input_ids = batch[\"input_ids\"].to(accelerator.device)\n                # weight_dtype) use float instead of fp16/bf16 because text encoder is float\n                encoder_hidden_states = torch.stack(\n                    [\n                        train_util.get_hidden_states(args, s, tokenizer, text_encoder, weight_dtype)\n                        for s in torch.split(input_ids, 1, dim=1)\n                    ]\n                )\n\n                # Sample noise, sample a random timestep for each image, and add noise to the latents,\n                # with noise offset and/or multires noise if specified\n                noise, noisy_latents, timesteps, huber_c = train_util.get_noise_noisy_latents_and_timesteps(args, noise_scheduler, latents)\n\n                # Predict the noise residual\n                with accelerator.autocast():\n                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n\n                if args.v_parameterization:\n                    # v-parameterization training\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    target = noise\n\n                loss = train_util.conditional_loss(noise_pred.float(), target.float(), reduction=\"none\", loss_type=args.loss_type, huber_c=huber_c)\n                if args.masked_loss:\n                    loss = apply_masked_loss(loss, batch)\n                loss = loss.mean([1, 2, 3])\n\n                loss_weights = batch[\"loss_weights\"]  # 各sampleごとのweight\n\n                loss = loss * loss_weights\n                if args.min_snr_gamma:\n                    loss = apply_snr_weight(loss, timesteps, noise_scheduler, args.min_snr_gamma, args.v_parameterization)\n                if args.scale_v_pred_loss_like_noise_pred:\n                    loss = scale_v_prediction_loss_like_noise_prediction(loss, timesteps, noise_scheduler)\n                if args.debiased_estimation_loss:\n                    loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)\n\n                loss = loss.mean()  # 平均なのでbatch_sizeで割る必要なし\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients and args.max_grad_norm != 0.0:\n                    params_to_clip = text_encoder.get_input_embeddings().parameters()\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n                # Let's make sure we don't update any embedding weights besides the newly added token\n                with torch.no_grad():\n                    accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[index_no_updates] = orig_embeds_params[\n                        index_no_updates\n                    ]\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n                # TODO: fix sample_images\n                # train_util.sample_images(\n                #     accelerator, args, None, global_step, accelerator.device, vae, tokenizer, text_encoder, unet, prompt_replacement\n                # )\n\n                # 指定ステップごとにモデルを保存\n                if args.save_every_n_steps is not None and global_step % args.save_every_n_steps == 0:\n                    accelerator.wait_for_everyone()\n                    if accelerator.is_main_process:\n                        updated_embs = (\n                            accelerator.unwrap_model(text_encoder)\n                            .get_input_embeddings()\n                            .weight[token_ids_XTI]\n                            .data.detach()\n                            .clone()\n                        )\n\n                        ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, global_step)\n                        save_model(ckpt_name, updated_embs, global_step, epoch)\n\n                        if args.save_state:\n                            train_util.save_and_remove_state_stepwise(args, accelerator, global_step)\n\n                        remove_step_no = train_util.get_remove_step_no(args, global_step)\n                        if remove_step_no is not None:\n                            remove_ckpt_name = train_util.get_step_ckpt_name(args, \".\" + args.save_model_as, remove_step_no)\n                            remove_model(remove_ckpt_name)\n\n            current_loss = loss.detach().item()\n            if args.logging_dir is not None:\n                logs = {\"loss\": current_loss, \"lr\": float(lr_scheduler.get_last_lr()[0])}\n                if (\n                    args.optimizer_type.lower().startswith(\"DAdapt\".lower()) or args.optimizer_type.lower() == \"Prodigy\".lower()\n                ):  # tracking d*lr value\n                    logs[\"lr/d*lr\"] = (\n                        lr_scheduler.optimizers[0].param_groups[0][\"d\"] * lr_scheduler.optimizers[0].param_groups[0][\"lr\"]\n                    )\n                accelerator.log(logs, step=global_step)\n\n            loss_total += current_loss\n            avr_loss = loss_total / (step + 1)\n            logs = {\"loss\": avr_loss}  # , \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        if args.logging_dir is not None:\n            logs = {\"loss/epoch\": loss_total / len(train_dataloader)}\n            accelerator.log(logs, step=epoch + 1)\n\n        accelerator.wait_for_everyone()\n\n        updated_embs = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[token_ids_XTI].data.detach().clone()\n\n        if args.save_every_n_epochs is not None:\n            saving = (epoch + 1) % args.save_every_n_epochs == 0 and (epoch + 1) < num_train_epochs\n            if accelerator.is_main_process and saving:\n                ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, epoch + 1)\n                save_model(ckpt_name, updated_embs, epoch + 1, global_step)\n\n                remove_epoch_no = train_util.get_remove_epoch_no(args, epoch + 1)\n                if remove_epoch_no is not None:\n                    remove_ckpt_name = train_util.get_epoch_ckpt_name(args, \".\" + args.save_model_as, remove_epoch_no)\n                    remove_model(remove_ckpt_name)\n\n                if args.save_state:\n                    train_util.save_and_remove_state_on_epoch_end(args, accelerator, epoch + 1)\n\n        # TODO: fix sample_images\n        # train_util.sample_images(\n        #     accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet, prompt_replacement\n        # )\n\n        # end of epoch\n\n    is_main_process = accelerator.is_main_process\n    if is_main_process:\n        text_encoder = accelerator.unwrap_model(text_encoder)\n\n    accelerator.end_training()\n\n    if is_main_process and (args.save_state or args.save_state_on_train_end):\n        train_util.save_state_on_train_end(args, accelerator)\n\n    updated_embs = text_encoder.get_input_embeddings().weight[token_ids_XTI].data.detach().clone()\n\n    del accelerator  # この後メモリを使うのでこれは消す\n\n    if is_main_process:\n        ckpt_name = train_util.get_last_ckpt_name(args, \".\" + args.save_model_as)\n        save_model(ckpt_name, updated_embs, global_step, num_train_epochs, force_sync_upload=True)\n\n        logger.info(\"model saved.\")\n\n\ndef save_weights(file, updated_embs, save_dtype):\n    updated_embs = updated_embs.reshape(16, -1, updated_embs.shape[-1])\n    updated_embs = updated_embs.chunk(16)\n    XTI_layers = [\n        \"IN01\",\n        \"IN02\",\n        \"IN04\",\n        \"IN05\",\n        \"IN07\",\n        \"IN08\",\n        \"MID\",\n        \"OUT03\",\n        \"OUT04\",\n        \"OUT05\",\n        \"OUT06\",\n        \"OUT07\",\n        \"OUT08\",\n        \"OUT09\",\n        \"OUT10\",\n        \"OUT11\",\n    ]\n    state_dict = {}\n    for i, layer_name in enumerate(XTI_layers):\n        state_dict[layer_name] = updated_embs[i].squeeze(0).detach().clone().to(\"cpu\").to(save_dtype)\n\n    # if save_dtype is not None:\n    #     for key in list(state_dict.keys()):\n    #         v = state_dict[key]\n    #         v = v.detach().clone().to(\"cpu\").to(save_dtype)\n    #         state_dict[key] = v\n\n    if os.path.splitext(file)[1] == \".safetensors\":\n        from safetensors.torch import save_file\n\n        save_file(state_dict, file)\n    else:\n        torch.save(state_dict, file)  # can be loaded in Web UI\n\n\ndef load_weights(file):\n    if os.path.splitext(file)[1] == \".safetensors\":\n        from safetensors.torch import load_file\n\n        data = load_file(file)\n    else:\n        raise ValueError(f\"NOT XTI: {file}\")\n\n    if len(data.values()) != 16:\n        raise ValueError(f\"NOT XTI: {file}\")\n\n    emb = torch.concat([x for x in data.values()])\n\n    return emb\n\n\ndef setup_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n\n    add_logging_arguments(parser)\n    train_util.add_sd_models_arguments(parser)\n    train_util.add_dataset_arguments(parser, True, True, False)\n    train_util.add_training_arguments(parser, True)\n    train_util.add_masked_loss_arguments(parser)\n    deepspeed_utils.add_deepspeed_arguments(parser)\n    train_util.add_optimizer_arguments(parser)\n    config_util.add_config_arguments(parser)\n    custom_train_functions.add_custom_train_arguments(parser, False)\n\n    parser.add_argument(\n        \"--save_model_as\",\n        type=str,\n        default=\"pt\",\n        choices=[None, \"ckpt\", \"pt\", \"safetensors\"],\n        help=\"format to save the model (default is .pt) / モデル保存時の形式（デフォルトはpt）\",\n    )\n\n    parser.add_argument(\n        \"--weights\", type=str, default=None, help=\"embedding weights to initialize / 学習するネットワークの初期重み\"\n    )\n    parser.add_argument(\n        \"--num_vectors_per_token\", type=int, default=1, help=\"number of vectors per token / トークンに割り当てるembeddingsの要素数\"\n    )\n    parser.add_argument(\n        \"--token_string\",\n        type=str,\n        default=None,\n        help=\"token string used in training, must not exist in tokenizer / 学習時に使用されるトークン文字列、tokenizerに存在しない文字であること\",\n    )\n    parser.add_argument(\n        \"--init_word\", type=str, default=None, help=\"words to initialize vector / ベクトルを初期化に使用する単語、複数可\"\n    )\n    parser.add_argument(\n        \"--use_object_template\",\n        action=\"store_true\",\n        help=\"ignore caption and use default templates for object / キャプションは使わずデフォルトの物体用テンプレートで学習する\",\n    )\n    parser.add_argument(\n        \"--use_style_template\",\n        action=\"store_true\",\n        help=\"ignore caption and use default templates for stype / キャプションは使わずデフォルトのスタイル用テンプレートで学習する\",\n    )\n\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = setup_parser()\n\n    args = parser.parse_args()\n    train_util.verify_command_line_training_args(args)\n    args = train_util.read_config_from_file(args, parser)\n\n    train(args)\n"
        }
      ]
    }
  ]
}