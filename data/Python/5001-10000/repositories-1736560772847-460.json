{
  "metadata": {
    "timestamp": 1736560772847,
    "page": 460,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "gaomingqi/Track-Anything",
      "stars": 6576,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1123046875,
          "content": "__pycache__/\n.vscode/\ndocs/\n*.pth\n*.mp4\ndebug_images/\n*.png\n*.jpg\n*.npy\nimages/\ntest_sample/\nresult/\nvots/\nvots.py\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2023 Mingqi Gao\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.5712890625,
          "content": "<!-- ![](./assets/track-anything-logo.jpg) -->\n\n<div align=center>\n<img src=\"./assets/track-anything-logo.jpg\"/>\n</div>\n<br/>\n<div align=center>\n<a src=\"https://img.shields.io/badge/%F0%9F%93%96-Arxiv_2304.11968-red.svg?style=flat-square\" href=\"https://arxiv.org/abs/2304.11968\">\n<img src=\"https://img.shields.io/badge/%F0%9F%93%96-Arxiv_2304.11968-red.svg?style=flat-square\">\n</a>\n<a src=\"https://img.shields.io/badge/%F0%9F%A4%97-Open_in_Spaces-informational.svg?style=flat-square\" href=\"https://huggingface.co/spaces/VIPLab/Track-Anything?duplicate=true\">\n<img src=\"https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face_Space-informational.svg?style=flat-square\">\n</a>\n<a src=\"https://img.shields.io/badge/%F0%9F%97%BA-Tutorials in Steps-2bb7b3.svg?style=flat-square\" href=\"./doc/tutorials.md\">\n<img src=\"https://img.shields.io/badge/%F0%9F%97%BA-Tutorials in Steps-2bb7b3.svg?style=flat-square\">\n\n</a>\n<a src=\"https://img.shields.io/badge/%F0%9F%9A%80-SUSTech_VIP_Lab-ed6c00.svg?style=flat-square\" href=\"https://zhengfenglab.com/\">\n<img src=\"https://img.shields.io/badge/%F0%9F%9A%80-SUSTech_VIP_Lab-ed6c00.svg?style=flat-square\">\n</a>\n</div>\n\n***Track-Anything*** is a flexible and interactive tool for video object tracking and segmentation. It is developed upon [Segment Anything](https://github.com/facebookresearch/segment-anything), can specify anything to track and segment via user clicks only. During tracking, users can flexibly change the objects they wanna track or correct the region of interest if there are any ambiguities. These characteristics enable ***Track-Anything*** to be suitable for: \n- Video object tracking and segmentation with shot changes. \n- Visualized development and data annotation for video object tracking and segmentation.\n- Object-centric downstream video tasks, such as video inpainting and editing. \n\n<div align=center>\n<img src=\"./assets/avengers.gif\" width=\"81%\"/>\n</div>\n\n<!-- ![avengers]() -->\n\n## :rocket: Updates\n\n- 2023/05/02: We uploaded tutorials in steps :world_map:. Check [HERE](./doc/tutorials.md) for more details.\n\n- 2023/04/29: We improved inpainting by decoupling GPU memory usage and video length. Now Track-Anything can inpaint videos with any length! :smiley_cat: Check [HERE](https://github.com/gaomingqi/Track-Anything/issues/4#issuecomment-1528198165) for our GPU memory requirements. \n\n- 2023/04/25: We are delighted to introduce [Caption-Anything](https://github.com/ttengwang/Caption-Anything) :writing_hand:, an inventive project from our lab that combines the capabilities of Segment Anything, Visual Captioning, and ChatGPT. \n\n- 2023/04/20: We deployed [DEMO](https://huggingface.co/spaces/VIPLab/Track-Anything?duplicate=true) on Hugging Face :hugs:!\n\n- 2023/04/14: We made Track-Anything public!\n\n## :world_map: Video Tutorials ([Track-Anything Tutorials in Steps](./doc/tutorials.md))\n\nhttps://user-images.githubusercontent.com/30309970/234902447-a4c59718-fcfe-443a-bd18-2f3f775cfc13.mp4\n\n---\n\n### :joystick: Example - Multiple Object Tracking and Segmentation (with [XMem](https://github.com/hkchengrex/XMem))\n\nhttps://user-images.githubusercontent.com/39208339/233035206-0a151004-6461-4deb-b782-d1dbfe691493.mp4\n\n---\n\n### :joystick: Example - Video Object Tracking and Segmentation with Shot Changes (with [XMem](https://github.com/hkchengrex/XMem))\n\nhttps://user-images.githubusercontent.com/30309970/232848349-f5e29e71-2ea4-4529-ac9a-94b9ca1e7055.mp4\n\n---\n\n### :joystick: Example - Video Inpainting (with [E2FGVI](https://github.com/MCG-NKU/E2FGVI))\n\nhttps://user-images.githubusercontent.com/28050374/232959816-07f2826f-d267-4dda-8ae5-a5132173b8f4.mp4\n\n## :computer: Get Started\n#### Linux & Windows\n```shell\n# Clone the repository:\ngit clone https://github.com/gaomingqi/Track-Anything.git\ncd Track-Anything\n\n# Install dependencies: \npip install -r requirements.txt\n\n# Run the Track-Anything gradio demo.\npython app.py --device cuda:0\n# python app.py --device cuda:0 --sam_model_type vit_b # for lower memory usage\n```\n\n\n## :book: Citation\nIf you find this work useful for your research or applications, please cite using this BibTeX:\n```bibtex\n@misc{yang2023track,\n      title={Track Anything: Segment Anything Meets Videos}, \n      author={Jinyu Yang and Mingqi Gao and Zhe Li and Shang Gao and Fangjing Wang and Feng Zheng},\n      year={2023},\n      eprint={2304.11968},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## :clap: Acknowledgements\n\nThe project is based on [Segment Anything](https://github.com/facebookresearch/segment-anything), [XMem](https://github.com/hkchengrex/XMem), and [E2FGVI](https://github.com/MCG-NKU/E2FGVI). Thanks for the authors for their efforts.\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 27.5205078125,
          "content": "import gradio as gr\nimport argparse\nimport gdown\nimport cv2\nimport numpy as np\nimport os\nimport sys\nsys.path.append(sys.path[0]+\"/tracker\")\nsys.path.append(sys.path[0]+\"/tracker/model\")\nfrom track_anything import TrackingAnything\nfrom track_anything import parse_augment\nimport requests\nimport json\nimport torchvision\nimport torch \nfrom tools.painter import mask_painter\nimport psutil\nimport time\ntry: \n    from mmcv.cnn import ConvModule\nexcept:\n    os.system(\"mim install mmcv\")\n\n# download checkpoints\ndef download_checkpoint(url, folder, filename):\n    os.makedirs(folder, exist_ok=True)\n    filepath = os.path.join(folder, filename)\n\n    if not os.path.exists(filepath):\n        print(\"download checkpoints ......\")\n        response = requests.get(url, stream=True)\n        with open(filepath, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n\n        print(\"download successfully!\")\n\n    return filepath\n\ndef download_checkpoint_from_google_drive(file_id, folder, filename):\n    os.makedirs(folder, exist_ok=True)\n    filepath = os.path.join(folder, filename)\n\n    if not os.path.exists(filepath):\n        print(\"Downloading checkpoints from Google Drive... tips: If you cannot see the progress bar, please try to download it manuall \\\n              and put it in the checkpointes directory. E2FGVI-HQ-CVPR22.pth: https://github.com/MCG-NKU/E2FGVI(E2FGVI-HQ model)\")\n        url = f\"https://drive.google.com/uc?id={file_id}\"\n        gdown.download(url, filepath, quiet=False)\n        print(\"Downloaded successfully!\")\n\n    return filepath\n\n# convert points input to prompt state\ndef get_prompt(click_state, click_input):\n    inputs = json.loads(click_input)\n    points = click_state[0]\n    labels = click_state[1]\n    for input in inputs:\n        points.append(input[:2])\n        labels.append(input[2])\n    click_state[0] = points\n    click_state[1] = labels\n    prompt = {\n        \"prompt_type\":[\"click\"],\n        \"input_point\":click_state[0],\n        \"input_label\":click_state[1],\n        \"multimask_output\":\"True\",\n    }\n    return prompt\n\n\n# extract frames from upload video\ndef get_frames_from_video(video_input, video_state):\n    \"\"\"\n    Args:\n        video_path:str\n        timestamp:float64\n    Return \n        [[0:nearest_frame], [nearest_frame:], nearest_frame]\n    \"\"\"\n    video_path = video_input\n    frames = []\n    user_name = time.time()\n    operation_log = [(\"\",\"\"),(\"Upload video already. Try click the image for adding targets to track and inpaint.\",\"Normal\")]\n    try:\n        cap = cv2.VideoCapture(video_path)\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if ret == True:\n                current_memory_usage = psutil.virtual_memory().percent\n                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                if current_memory_usage > 90:\n                    operation_log = [(\"Memory usage is too high (>90%). Stop the video extraction. Please reduce the video resolution or frame rate.\", \"Error\")]\n                    print(\"Memory usage is too high (>90%). Please reduce the video resolution or frame rate.\")\n                    break\n            else:\n                break\n    except (OSError, TypeError, ValueError, KeyError, SyntaxError) as e:\n        print(\"read_frame_source:{} error. {}\\n\".format(video_path, str(e)))\n    image_size = (frames[0].shape[0],frames[0].shape[1]) \n    # initialize video_state\n    video_state = {\n        \"user_name\": user_name,\n        \"video_name\": os.path.split(video_path)[-1],\n        \"origin_images\": frames,\n        \"painted_images\": frames.copy(),\n        \"masks\": [np.zeros((frames[0].shape[0],frames[0].shape[1]), np.uint8)]*len(frames),\n        \"logits\": [None]*len(frames),\n        \"select_frame_number\": 0,\n        \"fps\": fps\n        }\n    video_info = \"Video Name: {}, FPS: {}, Total Frames: {}, Image Size:{}\".format(video_state[\"video_name\"], video_state[\"fps\"], len(frames), image_size)\n    model.samcontroler.sam_controler.reset_image() \n    model.samcontroler.sam_controler.set_image(video_state[\"origin_images\"][0])\n    return video_state, video_info, video_state[\"origin_images\"][0], gr.update(visible=True, maximum=len(frames), value=1), gr.update(visible=True, maximum=len(frames), value=len(frames)), \\\n                        gr.update(visible=True),\\\n                        gr.update(visible=True), gr.update(visible=True), \\\n                        gr.update(visible=True), gr.update(visible=True), \\\n                        gr.update(visible=True), gr.update(visible=True), \\\n                        gr.update(visible=True), gr.update(visible=True), \\\n                        gr.update(visible=True, value=operation_log)\n\ndef run_example(example):\n    return video_input\n# get the select frame from gradio slider\ndef select_template(image_selection_slider, video_state, interactive_state, mask_dropdown):\n\n    # images = video_state[1]\n    image_selection_slider -= 1\n    video_state[\"select_frame_number\"] = image_selection_slider\n\n    # once select a new template frame, set the image in sam\n\n    model.samcontroler.sam_controler.reset_image()\n    model.samcontroler.sam_controler.set_image(video_state[\"origin_images\"][image_selection_slider])\n\n    # update the masks when select a new template frame\n    # if video_state[\"masks\"][image_selection_slider] is not None:\n        # video_state[\"painted_images\"][image_selection_slider] = mask_painter(video_state[\"origin_images\"][image_selection_slider], video_state[\"masks\"][image_selection_slider])\n    if mask_dropdown:\n        print(\"ok\")\n    operation_log = [(\"\",\"\"), (\"Select frame {}. Try click image and add mask for tracking.\".format(image_selection_slider),\"Normal\")]\n\n\n    return video_state[\"painted_images\"][image_selection_slider], video_state, interactive_state, operation_log\n\n# set the tracking end frame\ndef get_end_number(track_pause_number_slider, video_state, interactive_state):\n    interactive_state[\"track_end_number\"] = track_pause_number_slider\n    operation_log = [(\"\",\"\"),(\"Set the tracking finish at frame {}\".format(track_pause_number_slider),\"Normal\")]\n\n    return video_state[\"painted_images\"][track_pause_number_slider],interactive_state, operation_log\n\ndef get_resize_ratio(resize_ratio_slider, interactive_state):\n    interactive_state[\"resize_ratio\"] = resize_ratio_slider\n\n    return interactive_state\n\n# use sam to get the mask\ndef sam_refine(video_state, point_prompt, click_state, interactive_state, evt:gr.SelectData):\n    \"\"\"\n    Args:\n        template_frame: PIL.Image\n        point_prompt: flag for positive or negative button click\n        click_state: [[points], [labels]]\n    \"\"\"\n    if point_prompt == \"Positive\":\n        coordinate = \"[[{},{},1]]\".format(evt.index[0], evt.index[1])\n        interactive_state[\"positive_click_times\"] += 1\n    else:\n        coordinate = \"[[{},{},0]]\".format(evt.index[0], evt.index[1])\n        interactive_state[\"negative_click_times\"] += 1\n    \n    # prompt for sam model\n    model.samcontroler.sam_controler.reset_image()\n    model.samcontroler.sam_controler.set_image(video_state[\"origin_images\"][video_state[\"select_frame_number\"]])\n    prompt = get_prompt(click_state=click_state, click_input=coordinate)\n\n    mask, logit, painted_image = model.first_frame_click( \n                                                      image=video_state[\"origin_images\"][video_state[\"select_frame_number\"]], \n                                                      points=np.array(prompt[\"input_point\"]),\n                                                      labels=np.array(prompt[\"input_label\"]),\n                                                      multimask=prompt[\"multimask_output\"],\n                                                      )\n    video_state[\"masks\"][video_state[\"select_frame_number\"]] = mask\n    video_state[\"logits\"][video_state[\"select_frame_number\"]] = logit\n    video_state[\"painted_images\"][video_state[\"select_frame_number\"]] = painted_image\n\n    operation_log = [(\"\",\"\"), (\"Use SAM for segment. You can try add positive and negative points by clicking. Or press Clear clicks button to refresh the image. Press Add mask button when you are satisfied with the segment\",\"Normal\")]\n    return painted_image, video_state, interactive_state, operation_log\n\ndef add_multi_mask(video_state, interactive_state, mask_dropdown):\n    try:\n        mask = video_state[\"masks\"][video_state[\"select_frame_number\"]]\n        interactive_state[\"multi_mask\"][\"masks\"].append(mask)\n        interactive_state[\"multi_mask\"][\"mask_names\"].append(\"mask_{:03d}\".format(len(interactive_state[\"multi_mask\"][\"masks\"])))\n        mask_dropdown.append(\"mask_{:03d}\".format(len(interactive_state[\"multi_mask\"][\"masks\"])))\n        select_frame, run_status = show_mask(video_state, interactive_state, mask_dropdown)\n\n        operation_log = [(\"\",\"\"),(\"Added a mask, use the mask select for target tracking or inpainting.\",\"Normal\")]\n    except:\n        operation_log = [(\"Please click the left image to generate mask.\", \"Error\"), (\"\",\"\")]\n    return interactive_state, gr.update(choices=interactive_state[\"multi_mask\"][\"mask_names\"], value=mask_dropdown), select_frame, [[],[]], operation_log\n\ndef clear_click(video_state, click_state):\n    click_state = [[],[]]\n    template_frame = video_state[\"origin_images\"][video_state[\"select_frame_number\"]]\n    operation_log = [(\"\",\"\"), (\"Clear points history and refresh the image.\",\"Normal\")]\n    return template_frame, click_state, operation_log\n\ndef remove_multi_mask(interactive_state, mask_dropdown):\n    interactive_state[\"multi_mask\"][\"mask_names\"]= []\n    interactive_state[\"multi_mask\"][\"masks\"] = []\n\n    operation_log = [(\"\",\"\"), (\"Remove all mask, please add new masks\",\"Normal\")]\n    return interactive_state, gr.update(choices=[],value=[]), operation_log\n\ndef show_mask(video_state, interactive_state, mask_dropdown):\n    mask_dropdown.sort()\n    select_frame = video_state[\"origin_images\"][video_state[\"select_frame_number\"]]\n    for i in range(len(mask_dropdown)):\n        mask_number = int(mask_dropdown[i].split(\"_\")[1]) - 1\n        mask = interactive_state[\"multi_mask\"][\"masks\"][mask_number]\n        select_frame = mask_painter(select_frame, mask.astype('uint8'), mask_color=mask_number+2)\n    \n    operation_log = [(\"\",\"\"), (\"Select {} for tracking or inpainting\".format(mask_dropdown),\"Normal\")]\n    return select_frame, operation_log\n\n# tracking vos\ndef vos_tracking_video(video_state, interactive_state, mask_dropdown):\n    operation_log = [(\"\",\"\"), (\"Track the selected masks, and then you can select the masks for inpainting.\",\"Normal\")]\n    model.xmem.clear_memory()\n    if interactive_state[\"track_end_number\"]:\n        following_frames = video_state[\"origin_images\"][video_state[\"select_frame_number\"]:interactive_state[\"track_end_number\"]]\n    else:\n        following_frames = video_state[\"origin_images\"][video_state[\"select_frame_number\"]:]\n\n    if interactive_state[\"multi_mask\"][\"masks\"]:\n        if len(mask_dropdown) == 0:\n            mask_dropdown = [\"mask_001\"]\n        mask_dropdown.sort()\n        template_mask = interactive_state[\"multi_mask\"][\"masks\"][int(mask_dropdown[0].split(\"_\")[1]) - 1] * (int(mask_dropdown[0].split(\"_\")[1]))\n        for i in range(1,len(mask_dropdown)):\n            mask_number = int(mask_dropdown[i].split(\"_\")[1]) - 1 \n            template_mask = np.clip(template_mask+interactive_state[\"multi_mask\"][\"masks\"][mask_number]*(mask_number+1), 0, mask_number+1)\n        video_state[\"masks\"][video_state[\"select_frame_number\"]]= template_mask\n    else:      \n        template_mask = video_state[\"masks\"][video_state[\"select_frame_number\"]]\n    fps = video_state[\"fps\"]\n\n    # operation error\n    if len(np.unique(template_mask))==1:\n        template_mask[0][0]=1\n        operation_log = [(\"Error! Please add at least one mask to track by clicking the left image.\",\"Error\"), (\"\",\"\")]\n        # return video_output, video_state, interactive_state, operation_error\n    masks, logits, painted_images = model.generator(images=following_frames, template_mask=template_mask)\n    # clear GPU memory\n    model.xmem.clear_memory()\n\n    if interactive_state[\"track_end_number\"]: \n        video_state[\"masks\"][video_state[\"select_frame_number\"]:interactive_state[\"track_end_number\"]] = masks\n        video_state[\"logits\"][video_state[\"select_frame_number\"]:interactive_state[\"track_end_number\"]] = logits\n        video_state[\"painted_images\"][video_state[\"select_frame_number\"]:interactive_state[\"track_end_number\"]] = painted_images\n    else:\n        video_state[\"masks\"][video_state[\"select_frame_number\"]:] = masks\n        video_state[\"logits\"][video_state[\"select_frame_number\"]:] = logits\n        video_state[\"painted_images\"][video_state[\"select_frame_number\"]:] = painted_images\n\n    video_output = generate_video_from_frames(video_state[\"painted_images\"], output_path=\"./result/track/{}\".format(video_state[\"video_name\"]), fps=fps) # import video_input to name the output video\n    interactive_state[\"inference_times\"] += 1\n    \n    print(\"For generating this tracking result, inference times: {}, click times: {}, positive: {}, negative: {}\".format(interactive_state[\"inference_times\"], \n                                                                                                                                           interactive_state[\"positive_click_times\"]+interactive_state[\"negative_click_times\"],\n                                                                                                                                           interactive_state[\"positive_click_times\"],\n                                                                                                                                        interactive_state[\"negative_click_times\"]))\n\n    #### shanggao code for mask save\n    if interactive_state[\"mask_save\"]:\n        if not os.path.exists('./result/mask/{}'.format(video_state[\"video_name\"].split('.')[0])):\n            os.makedirs('./result/mask/{}'.format(video_state[\"video_name\"].split('.')[0]))\n        i = 0\n        print(\"save mask\")\n        for mask in video_state[\"masks\"]:\n            np.save(os.path.join('./result/mask/{}'.format(video_state[\"video_name\"].split('.')[0]), '{:05d}.npy'.format(i)), mask)\n            i+=1\n        # save_mask(video_state[\"masks\"], video_state[\"video_name\"])\n    #### shanggao code for mask save\n    return video_output, video_state, interactive_state, operation_log\n\n# extracting masks from mask_dropdown\n# def extract_sole_mask(video_state, mask_dropdown):\n#     combined_masks = \n#     unique_masks = np.unique(combined_masks)\n#     return 0 \n\n# inpaint \ndef inpaint_video(video_state, interactive_state, mask_dropdown):\n    operation_log = [(\"\",\"\"), (\"Removed the selected masks.\",\"Normal\")]\n\n    frames = np.asarray(video_state[\"origin_images\"])\n    fps = video_state[\"fps\"]\n    inpaint_masks = np.asarray(video_state[\"masks\"])\n    if len(mask_dropdown) == 0:\n        mask_dropdown = [\"mask_001\"]\n    mask_dropdown.sort()\n    # convert mask_dropdown to mask numbers\n    inpaint_mask_numbers = [int(mask_dropdown[i].split(\"_\")[1]) for i in range(len(mask_dropdown))]\n    # interate through all masks and remove the masks that are not in mask_dropdown\n    unique_masks = np.unique(inpaint_masks)\n    num_masks = len(unique_masks) - 1\n    for i in range(1, num_masks + 1):\n        if i in inpaint_mask_numbers:\n            continue\n        inpaint_masks[inpaint_masks==i] = 0\n    # inpaint for videos\n\n    try:\n        inpainted_frames = model.baseinpainter.inpaint(frames, inpaint_masks, ratio=interactive_state[\"resize_ratio\"])   # numpy array, T, H, W, 3\n    except:\n        operation_log = [(\"Error! You are trying to inpaint without masks input. Please track the selected mask first, and then press inpaint. If VRAM exceeded, please use the resize ratio to scaling down the image size.\",\"Error\"), (\"\",\"\")]\n        inpainted_frames = video_state[\"origin_images\"]\n    video_output = generate_video_from_frames(inpainted_frames, output_path=\"./result/inpaint/{}\".format(video_state[\"video_name\"]), fps=fps) # import video_input to name the output video\n\n    return video_output, operation_log\n\n\n# generate video after vos inference\ndef generate_video_from_frames(frames, output_path, fps=30):\n    \"\"\"\n    Generates a video from a list of frames.\n    \n    Args:\n        frames (list of numpy arrays): The frames to include in the video.\n        output_path (str): The path to save the generated video.\n        fps (int, optional): The frame rate of the output video. Defaults to 30.\n    \"\"\"\n    # height, width, layers = frames[0].shape\n    # fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    # video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    # print(output_path)\n    # for frame in frames:\n    #     video.write(frame)\n    \n    # video.release()\n    frames = torch.from_numpy(np.asarray(frames))\n    if not os.path.exists(os.path.dirname(output_path)):\n        os.makedirs(os.path.dirname(output_path))\n    torchvision.io.write_video(output_path, frames, fps=fps, video_codec=\"libx264\")\n    return output_path\n\n\n# args, defined in track_anything.py\nargs = parse_augment()\n\n# check and download checkpoints if needed\nSAM_checkpoint_dict = {\n    'vit_h': \"sam_vit_h_4b8939.pth\",\n    'vit_l': \"sam_vit_l_0b3195.pth\", \n    \"vit_b\": \"sam_vit_b_01ec64.pth\"\n}\nSAM_checkpoint_url_dict = {\n    'vit_h': \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n    'vit_l': \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n    'vit_b': \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n}\nsam_checkpoint = SAM_checkpoint_dict[args.sam_model_type] \nsam_checkpoint_url = SAM_checkpoint_url_dict[args.sam_model_type] \nxmem_checkpoint = \"XMem-s012.pth\"\nxmem_checkpoint_url = \"https://github.com/hkchengrex/XMem/releases/download/v1.0/XMem-s012.pth\"\ne2fgvi_checkpoint = \"E2FGVI-HQ-CVPR22.pth\"\ne2fgvi_checkpoint_id = \"10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3\"\n\n\nfolder =\"./checkpoints\"\nSAM_checkpoint = download_checkpoint(sam_checkpoint_url, folder, sam_checkpoint)\nxmem_checkpoint = download_checkpoint(xmem_checkpoint_url, folder, xmem_checkpoint)\ne2fgvi_checkpoint = download_checkpoint_from_google_drive(e2fgvi_checkpoint_id, folder, e2fgvi_checkpoint)\nargs.port = 12212\nargs.device = \"cuda:3\"\n# args.mask_save = True\n\n# initialize sam, xmem, e2fgvi models\nmodel = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n\n\ntitle = \"\"\"<p><h1 align=\"center\">Track-Anything</h1></p>\n    \"\"\"\ndescription = \"\"\"<p>Gradio demo for Track Anything, a flexible and interactive tool for video object tracking, segmentation, and inpainting. I To use it, simply upload your video, or click one of the examples to load them. Code: <a href=\"https://github.com/gaomingqi/Track-Anything\">https://github.com/gaomingqi/Track-Anything</a> <a href=\"https://huggingface.co/spaces/watchtowerss/Track-Anything?duplicate=true\"><img style=\"display: inline; margin-top: 0em; margin-bottom: 0em\" src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\" /></a></p>\"\"\"\n\n\nwith gr.Blocks() as iface:\n    \"\"\"\n        state for \n    \"\"\"\n    click_state = gr.State([[],[]])\n    interactive_state = gr.State({\n        \"inference_times\": 0,\n        \"negative_click_times\" : 0,\n        \"positive_click_times\": 0,\n        \"mask_save\": args.mask_save,\n        \"multi_mask\": {\n            \"mask_names\": [],\n            \"masks\": []\n        },\n        \"track_end_number\": None,\n        \"resize_ratio\": 1\n    }\n    )\n\n    video_state = gr.State(\n        {\n        \"user_name\": \"\",\n        \"video_name\": \"\",\n        \"origin_images\": None,\n        \"painted_images\": None,\n        \"masks\": None,\n        \"inpaint_masks\": None,\n        \"logits\": None,\n        \"select_frame_number\": 0,\n        \"fps\": 30\n        }\n    )\n    gr.Markdown(title)\n    gr.Markdown(description)\n    with gr.Row():\n\n        # for user video input\n        with gr.Column():\n            with gr.Row(scale=0.4):\n                video_input = gr.Video(autosize=True)\n                with gr.Column():\n                    video_info = gr.Textbox(label=\"Video Info\")\n                    resize_info = gr.Textbox(value=\"If you want to use the inpaint function, it is best to git clone the repo and use a machine with more VRAM locally. \\\n                                            Alternatively, you can use the resize ratio slider to scale down the original image to around 360P resolution for faster processing.\", label=\"Tips for running this demo.\")\n                    resize_ratio_slider = gr.Slider(minimum=0.02, maximum=1, step=0.02, value=1, label=\"Resize ratio\", visible=True)\n          \n\n            with gr.Row():\n                # put the template frame under the radio button\n                with gr.Column():\n                    # extract frames\n                    with gr.Column():\n                        extract_frames_button = gr.Button(value=\"Get video info\", interactive=True, variant=\"primary\") \n\n                     # click points settins, negative or positive, mode continuous or single\n                    with gr.Row():\n                        with gr.Row():\n                            point_prompt = gr.Radio(\n                                choices=[\"Positive\",  \"Negative\"],\n                                value=\"Positive\",\n                                label=\"Point prompt\",\n                                interactive=True,\n                                visible=False)\n                            remove_mask_button = gr.Button(value=\"Remove mask\", interactive=True, visible=False) \n                            clear_button_click = gr.Button(value=\"Clear clicks\", interactive=True, visible=False).style(height=160)\n                            Add_mask_button = gr.Button(value=\"Add mask\", interactive=True, visible=False)\n                    template_frame = gr.Image(type=\"pil\",interactive=True, elem_id=\"template_frame\", visible=False).style(height=360)\n                    image_selection_slider = gr.Slider(minimum=1, maximum=100, step=1, value=1, label=\"Track start frame\", visible=False)\n                    track_pause_number_slider = gr.Slider(minimum=1, maximum=100, step=1, value=1, label=\"Track end frame\", visible=False)\n            \n                with gr.Column():\n                    run_status = gr.HighlightedText(value=[(\"Text\",\"Error\"),(\"to be\",\"Label 2\"),(\"highlighted\",\"Label 3\")], visible=False)\n                    mask_dropdown = gr.Dropdown(multiselect=True, value=[], label=\"Mask selection\", info=\".\", visible=False)\n                    video_output = gr.Video(autosize=True, visible=False).style(height=360)\n                    with gr.Row():\n                        tracking_video_predict_button = gr.Button(value=\"Tracking\", visible=False)\n                        inpaint_video_predict_button = gr.Button(value=\"Inpainting\", visible=False)\n\n    # first step: get the video information \n    extract_frames_button.click(\n        fn=get_frames_from_video,\n        inputs=[\n            video_input, video_state\n        ],\n        outputs=[video_state, video_info, template_frame,\n                 image_selection_slider, track_pause_number_slider,point_prompt, clear_button_click, Add_mask_button, template_frame,\n                 tracking_video_predict_button, video_output, mask_dropdown, remove_mask_button, inpaint_video_predict_button, run_status]\n    )   \n\n    # second step: select images from slider\n    image_selection_slider.release(fn=select_template, \n                                   inputs=[image_selection_slider, video_state, interactive_state], \n                                   outputs=[template_frame, video_state, interactive_state, run_status], api_name=\"select_image\")\n    track_pause_number_slider.release(fn=get_end_number, \n                                   inputs=[track_pause_number_slider, video_state, interactive_state], \n                                   outputs=[template_frame, interactive_state, run_status], api_name=\"end_image\")\n    resize_ratio_slider.release(fn=get_resize_ratio, \n                                   inputs=[resize_ratio_slider, interactive_state], \n                                   outputs=[interactive_state], api_name=\"resize_ratio\")\n    \n    # click select image to get mask using sam\n    template_frame.select(\n        fn=sam_refine,\n        inputs=[video_state, point_prompt, click_state, interactive_state],\n        outputs=[template_frame, video_state, interactive_state, run_status]\n    )\n\n    # add different mask\n    Add_mask_button.click(\n        fn=add_multi_mask,\n        inputs=[video_state, interactive_state, mask_dropdown],\n        outputs=[interactive_state, mask_dropdown, template_frame, click_state, run_status]\n    )\n\n    remove_mask_button.click(\n        fn=remove_multi_mask,\n        inputs=[interactive_state, mask_dropdown],\n        outputs=[interactive_state, mask_dropdown, run_status]\n    )\n\n    # tracking video from select image and mask\n    tracking_video_predict_button.click(\n        fn=vos_tracking_video,\n        inputs=[video_state, interactive_state, mask_dropdown],\n        outputs=[video_output, video_state, interactive_state, run_status]\n    )\n\n    # inpaint video from select image and mask\n    inpaint_video_predict_button.click(\n        fn=inpaint_video,\n        inputs=[video_state, interactive_state, mask_dropdown],\n        outputs=[video_output, run_status]\n    )\n\n    # click to get mask\n    mask_dropdown.change(\n        fn=show_mask,\n        inputs=[video_state, interactive_state, mask_dropdown],\n        outputs=[template_frame, run_status]\n    )\n    \n    # clear input\n    video_input.clear(\n        lambda: (\n        {\n        \"user_name\": \"\",\n        \"video_name\": \"\",\n        \"origin_images\": None,\n        \"painted_images\": None,\n        \"masks\": None,\n        \"inpaint_masks\": None,\n        \"logits\": None,\n        \"select_frame_number\": 0,\n        \"fps\": 30\n        },\n        {\n        \"inference_times\": 0,\n        \"negative_click_times\" : 0,\n        \"positive_click_times\": 0,\n        \"mask_save\": args.mask_save,\n        \"multi_mask\": {\n            \"mask_names\": [],\n            \"masks\": []\n        },\n        \"track_end_number\": 0,\n        \"resize_ratio\": 1\n        },\n        [[],[]],\n        None,\n        None,\n        gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), \\\n        gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), \\\n        gr.update(visible=False), gr.update(visible=False), gr.update(visible=False, value=[]), gr.update(visible=False), \\\n        gr.update(visible=False), gr.update(visible=False)\n                        \n        ),\n        [],\n        [ \n            video_state,\n            interactive_state,\n            click_state,\n            video_output,\n            template_frame,\n            tracking_video_predict_button, image_selection_slider , track_pause_number_slider,point_prompt, clear_button_click, \n            Add_mask_button, template_frame, tracking_video_predict_button, video_output, mask_dropdown, remove_mask_button,inpaint_video_predict_button, run_status\n        ],\n        queue=False,\n        show_progress=False)\n\n    # points clear\n    clear_button_click.click(\n        fn = clear_click,\n        inputs = [video_state, click_state,],\n        outputs = [template_frame,click_state, run_status],\n    )\n    # set example\n    gr.Markdown(\"##  Examples\")\n    gr.Examples(\n        examples=[os.path.join(os.path.dirname(__file__), \"./test_sample/\", test_sample) for test_sample in [\"test-sample8.mp4\",\"test-sample4.mp4\", \\\n                                                                                                             \"test-sample2.mp4\",\"test-sample13.mp4\"]],\n        fn=run_example,\n        inputs=[\n            video_input\n        ],\n        outputs=[video_input],\n        # cache_examples=True,\n    ) \niface.queue(concurrency_count=1)\niface.launch(debug=True, enable_queue=True, server_port=args.port, server_name=\"0.0.0.0\")\n# iface.launch(debug=True, enable_queue=True)"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 2.412109375,
          "content": "from metaseg import SegAutoMaskPredictor, SegManualMaskPredictor, SahiAutoSegmentation, sahi_sliced_predict\n\n# For image\n\ndef automask_image_app(image_path, model_type, points_per_side, points_per_batch, min_area):\n    SegAutoMaskPredictor().image_predict(\n        source=image_path,\n        model_type=model_type,  # vit_l, vit_h, vit_b\n        points_per_side=points_per_side,\n        points_per_batch=points_per_batch,\n        min_area=min_area,\n        output_path=\"output.png\",\n        show=False,\n        save=True,\n    )\n    return \"output.png\"\n\n\n# For video\n\ndef automask_video_app(video_path, model_type, points_per_side, points_per_batch, min_area):\n    SegAutoMaskPredictor().video_predict(\n        source=video_path,\n        model_type=model_type,  # vit_l, vit_h, vit_b\n        points_per_side=points_per_side,\n        points_per_batch=points_per_batch,\n        min_area=min_area,\n        output_path=\"output.mp4\",\n    )\n    return \"output.mp4\"\n\n\n# For manuel box and point selection\n\ndef manual_app(image_path, model_type, input_point, input_label, input_box, multimask_output, random_color):\n    SegManualMaskPredictor().image_predict(\n        source=image_path,\n        model_type=model_type,  # vit_l, vit_h, vit_b\n        input_point=input_point,\n        input_label=input_label,\n        input_box=input_box,\n        multimask_output=multimask_output,\n        random_color=random_color,\n        output_path=\"output.png\",\n        show=False,\n        save=True,\n    )\n    return \"output.png\"\n\n\n# For sahi sliced prediction\n\ndef sahi_autoseg_app(\n    image_path,\n    sam_model_type,\n    detection_model_type,\n    detection_model_path,\n    conf_th,\n    image_size,\n    slice_height,\n    slice_width,\n    overlap_height_ratio,\n    overlap_width_ratio,\n):\n    boxes = sahi_sliced_predict(\n        image_path=image_path,\n        detection_model_type=detection_model_type,  # yolov8, detectron2, mmdetection, torchvision\n        detection_model_path=detection_model_path,\n        conf_th=conf_th,\n        image_size=image_size,\n        slice_height=slice_height,\n        slice_width=slice_width,\n        overlap_height_ratio=overlap_height_ratio,\n        overlap_width_ratio=overlap_width_ratio,\n    )\n\n    SahiAutoSegmentation().predict(\n        source=image_path,\n        model_type=sam_model_type,\n        input_box=boxes,\n        multimask_output=False,\n        random_color=False,\n        show=False,\n        save=True,\n    )\n    \n    return \"output.png\"\n"
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "inpainter",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2216796875,
          "content": "progressbar2\ngdown\ngitpython\ngit+https://github.com/cheind/py-thin-plate-spline\nhickle\ntensorboard\nnumpy\ngit+https://github.com/facebookresearch/segment-anything.git\ngradio\nopencv-python\nmatplotlib\npyyaml\nav\nopenmim\ntqdm\npsutil"
        },
        {
          "name": "template.html",
          "type": "blob",
          "size": 0.6669921875,
          "content": "<!-- template.html -->\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Gradio Video Pause Time</title>\n</head>\n<body>\n    <video id=\"video\" controls>\n        <source src=\"{{VIDEO_URL}}\" type=\"video/mp4\">\n        Your browser does not support the video tag.\n    </video>\n    <script>\n        const video = document.getElementById(\"video\");\n        let pauseTime = null;\n\n        video.addEventListener(\"pause\", () => {\n            pauseTime = video.currentTime;\n        });\n\n        function getPauseTime() {\n            return pauseTime;\n        }\n    </script>\n</body>\n</html>\n"
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_sample",
          "type": "tree",
          "content": null
        },
        {
          "name": "text_server.py",
          "type": "blob",
          "size": 2.0986328125,
          "content": "import os\nimport sys\nimport cv2\nimport time\nimport json\nimport queue\nimport numpy as np\nimport requests\nimport concurrent.futures\nfrom PIL import Image\nfrom flask import Flask, render_template, request, jsonify, send_file\nimport torchvision\nimport torch\n\nfrom demo import automask_image_app, automask_video_app, sahi_autoseg_app\nsys.path.append(sys.path[0] + \"/tracker\")\nsys.path.append(sys.path[0] + \"/tracker/model\")\nfrom track_anything import TrackingAnything\nfrom track_anything import parse_augment\n\n# ... (all the functions defined in the original code except the Gradio part)\n\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = './uploaded_videos'\napp.config['ALLOWED_EXTENSIONS'] = {'mp4', 'avi', 'mov', 'mkv'}\n\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']\n\n@app.route(\"/\")\ndef index():\n    return render_template(\"index.html\")\n\n@app.route(\"/upload_video\", methods=[\"POST\"])\ndef upload_video():\n    # ... (handle video upload and processing)\n    return jsonify(status=\"success\", data=video_data)\n\n@app.route(\"/template_select\", methods=[\"POST\"])\ndef template_select():\n    # ... (handle template selection and processing)\n    return jsonify(status=\"success\", data=template_data)\n\n@app.route(\"/sam_refine\", methods=[\"POST\"])\ndef sam_refine_request():\n    # ... (handle sam refine and processing)\n    return jsonify(status=\"success\", data=sam_data)\n\n@app.route(\"/track_video\", methods=[\"POST\"])\ndef track_video():\n    # ... (handle video tracking and processing)\n    return jsonify(status=\"success\", data=tracking_data)\n\n@app.route(\"/track_image\", methods=[\"POST\"])\ndef track_image():\n    # ... (handle image tracking and processing)\n    return jsonify(status=\"success\", data=tracking_data)\n\n@app.route(\"/download_video\", methods=[\"GET\"])\ndef download_video():\n    try:\n        return send_file(\"output.mp4\", attachment_filename=\"output.mp4\")\n    except Exception as e:\n        return str(e)\n\nif __name__ == \"__main__\":\n    app.run(debug=True, host=\"0.0.0.0\", port=args.port)\n\n\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\",port=12212, debug=True)\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "track_anything.py",
          "type": "blob",
          "size": 3.8837890625,
          "content": "import PIL\nfrom tqdm import tqdm\n\nfrom tools.interact_tools import SamControler\nfrom tracker.base_tracker import BaseTracker\nfrom inpainter.base_inpainter import BaseInpainter\nimport numpy as np\nimport argparse\n\n\n\nclass TrackingAnything():\n    def __init__(self, sam_checkpoint, xmem_checkpoint, e2fgvi_checkpoint, args):\n        self.args = args\n        self.sam_checkpoint = sam_checkpoint\n        self.xmem_checkpoint = xmem_checkpoint\n        self.e2fgvi_checkpoint = e2fgvi_checkpoint\n        self.samcontroler = SamControler(self.sam_checkpoint, args.sam_model_type, args.device)\n        self.xmem = BaseTracker(self.xmem_checkpoint, device=args.device)\n        self.baseinpainter = BaseInpainter(self.e2fgvi_checkpoint, args.device) \n    # def inference_step(self, first_flag: bool, interact_flag: bool, image: np.ndarray, \n    #                    same_image_flag: bool, points:np.ndarray, labels: np.ndarray, logits: np.ndarray=None, multimask=True):\n    #     if first_flag:\n    #         mask, logit, painted_image = self.samcontroler.first_frame_click(image, points, labels, multimask)\n    #         return mask, logit, painted_image\n        \n    #     if interact_flag:\n    #         mask, logit, painted_image = self.samcontroler.interact_loop(image, same_image_flag, points, labels, logits, multimask)\n    #         return mask, logit, painted_image\n        \n    #     mask, logit, painted_image = self.xmem.track(image, logit)\n    #     return mask, logit, painted_image\n    \n    def first_frame_click(self, image: np.ndarray, points:np.ndarray, labels: np.ndarray, multimask=True):\n        mask, logit, painted_image = self.samcontroler.first_frame_click(image, points, labels, multimask)\n        return mask, logit, painted_image\n    \n    # def interact(self, image: np.ndarray, same_image_flag: bool, points:np.ndarray, labels: np.ndarray, logits: np.ndarray=None, multimask=True):\n    #     mask, logit, painted_image = self.samcontroler.interact_loop(image, same_image_flag, points, labels, logits, multimask)\n    #     return mask, logit, painted_image\n\n    def generator(self, images: list, template_mask:np.ndarray):\n        \n        masks = []\n        logits = []\n        painted_images = []\n        for i in tqdm(range(len(images)), desc=\"Tracking image\"):\n            if i ==0:           \n                mask, logit, painted_image = self.xmem.track(images[i], template_mask)\n                masks.append(mask)\n                logits.append(logit)\n                painted_images.append(painted_image)\n                \n            else:\n                mask, logit, painted_image = self.xmem.track(images[i])\n                masks.append(mask)\n                logits.append(logit)\n                painted_images.append(painted_image)\n        return masks, logits, painted_images\n    \n        \ndef parse_augment():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--device', type=str, default=\"cuda:0\")\n    parser.add_argument('--sam_model_type', type=str, default=\"vit_h\")\n    parser.add_argument('--port', type=int, default=6080, help=\"only useful when running gradio applications\")  \n    parser.add_argument('--debug', action=\"store_true\")\n    parser.add_argument('--mask_save', default=False)\n    args = parser.parse_args()\n\n    if args.debug:\n        print(args)\n    return args \n\n\nif __name__ == \"__main__\":\n    masks = None\n    logits = None\n    painted_images = None\n    images = []\n    image  = np.array(PIL.Image.open('/hhd3/gaoshang/truck.jpg'))\n    args = parse_augment()\n    # images.append(np.ones((20,20,3)).astype('uint8'))\n    # images.append(np.ones((20,20,3)).astype('uint8'))\n    images.append(image)\n    images.append(image)\n\n    mask = np.zeros_like(image)[:,:,0]\n    mask[0,0]= 1\n    trackany = TrackingAnything('/ssd1/gaomingqi/checkpoints/sam_vit_h_4b8939.pth','/ssd1/gaomingqi/checkpoints/XMem-s012.pth', args)\n    masks, logits ,painted_images= trackany.generator(images, mask)\n        \n        \n    \n    \n    "
        },
        {
          "name": "tracker",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}