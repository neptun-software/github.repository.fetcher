{
  "metadata": {
    "timestamp": 1736561062090,
    "page": 848,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "erikbern/ann-benchmarks",
      "stars": 5049,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0126953125,
          "content": "data\nresults\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.138671875,
          "content": ".DS_Store\n*.pyc\n*.o\nprotocol/c/fr-*\n\ninstall/*.txt\ninstall/*.yaml\ninstall/lib-*/\ndata/*\n*.class\n\n*.log\n\nresults/*\n!results/*.png\n\nvenv\n\n.idea\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.048828125,
          "content": "MIT License\n\nCopyright (c) 2018 Erik Bernhardsson\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.384765625,
          "content": "Benchmarking nearest neighbors\n==============================\n\n[![Build Status](https://img.shields.io/github/actions/workflow/status/erikbern/ann-benchmarks/benchmarks.yml?branch=main&style=flat-square)](https://github.com/erikbern/ann-benchmarks/actions/workflows/benchmarks.yml)\n\nDoing fast searching of nearest neighbors in high dimensional spaces is an increasingly important problem with notably few empirical attempts at comparing approaches in an objective way, despite a clear need for such to drive optimization forward.\n\nThis project contains tools to benchmark various implementations of approximate nearest neighbor (ANN) search for selected metrics. We have pre-generated datasets (in HDF5 format) and prepared Docker containers for each algorithm, as well as a [test suite](https://github.com/erikbern/ann-benchmarks/actions) to verify function integrity.\n\nEvaluated\n=========\n\n* [Annoy](https://github.com/spotify/annoy) ![https://img.shields.io/github/stars/spotify/annoy?style=social](https://img.shields.io/github/stars/spotify/annoy?style=social)\n* [FLANN](http://www.cs.ubc.ca/research/flann/) ![https://img.shields.io/github/stars/flann-lib/flann?style=social](https://img.shields.io/github/stars/flann-lib/flann?style=social)\n* [scikit-learn](http://scikit-learn.org/stable/modules/neighbors.html): LSHForest, KDTree, BallTree\n* [Weaviate](https://github.com/weaviate/weaviate) ![https://img.shields.io/github/stars/weaviate/weaviate?style=social](https://img.shields.io/github/stars/weaviate/weaviate?style=social)\n* [PANNS](https://github.com/ryanrhymes/panns) ![https://img.shields.io/github/stars/ryanrhymes/panns?style=social](https://img.shields.io/github/stars/ryanrhymes/panns?style=social)\n* [NearPy](http://pixelogik.github.io/NearPy/) ![https://img.shields.io/github/stars/pixelogik/NearPy?style=social](https://img.shields.io/github/stars/pixelogik/NearPy?style=social)\n* [KGraph](https://github.com/aaalgo/kgraph) ![https://img.shields.io/github/stars/aaalgo/kgraph?style=social](https://img.shields.io/github/stars/aaalgo/kgraph?style=social)\n* [NMSLIB (Non-Metric Space Library)](https://github.com/nmslib/nmslib) ![https://img.shields.io/github/stars/nmslib/nmslib?style=social](https://img.shields.io/github/stars/nmslib/nmslib?style=social): SWGraph, HNSW, BallTree, MPLSH\n* [hnswlib (a part of nmslib project)](https://github.com/nmslib/hnsw) ![https://img.shields.io/github/stars/nmslib/hnsw?style=social](https://img.shields.io/github/stars/nmslib/hnsw?style=social)\n* [RPForest](https://github.com/lyst/rpforest) ![https://img.shields.io/github/stars/lyst/rpforest?style=social](https://img.shields.io/github/stars/lyst/rpforest?style=social)\n* [FAISS](https://github.com/facebookresearch/faiss) ![https://img.shields.io/github/stars/facebookresearch/faiss?style=social](https://img.shields.io/github/stars/facebookresearch/faiss?style=social)\n* [DolphinnPy](https://github.com/ipsarros/DolphinnPy) ![https://img.shields.io/github/stars/ipsarros/DolphinnPy?style=social](https://img.shields.io/github/stars/ipsarros/DolphinnPy?style=social)\n* [Datasketch](https://github.com/ekzhu/datasketch) ![https://img.shields.io/github/stars/ekzhu/datasketch?style=social](https://img.shields.io/github/stars/ekzhu/datasketch?style=social)\n* [nndescent](https://github.com/brj0/nndescent) ![https://img.shields.io/github/stars/brj0/nndescent?style=social](https://img.shields.io/github/stars/brj0/nndescent?style=social)\n* [PyNNDescent](https://github.com/lmcinnes/pynndescent) ![https://img.shields.io/github/stars/lmcinnes/pynndescent?style=social](https://img.shields.io/github/stars/lmcinnes/pynndescent?style=social)\n* [MRPT](https://github.com/teemupitkanen/mrpt) ![https://img.shields.io/github/stars/teemupitkanen/mrpt?style=social](https://img.shields.io/github/stars/teemupitkanen/mrpt?style=social)\n* [NGT](https://github.com/yahoojapan/NGT) ![https://img.shields.io/github/stars/yahoojapan/NGT?style=social](https://img.shields.io/github/stars/yahoojapan/NGT?style=social): ONNG, PANNG, QG\n* [SPTAG](https://github.com/microsoft/SPTAG) ![https://img.shields.io/github/stars/microsoft/SPTAG?style=social](https://img.shields.io/github/stars/microsoft/SPTAG?style=social)\n* [PUFFINN](https://github.com/puffinn/puffinn) ![https://img.shields.io/github/stars/puffinn/puffinn?style=social](https://img.shields.io/github/stars/puffinn/puffinn?style=social)\n* [N2](https://github.com/kakao/n2) ![https://img.shields.io/github/stars/kakao/n2?style=social](https://img.shields.io/github/stars/kakao/n2?style=social)\n* [ScaNN](https://github.com/google-research/google-research/tree/master/scann)\n* [Vearch](https://github.com/vearch/vearch) ![https://img.shields.io/github/stars/vearch/vearch?style=social](https://img.shields.io/github/stars/vearch/vearch?style=social)\n* [Elasticsearch](https://github.com/elastic/elasticsearch) ![https://img.shields.io/github/stars/elastic/elasticsearch?style=social](https://img.shields.io/github/stars/elastic/elasticsearch?style=social): HNSW\n* [Elastiknn](https://github.com/alexklibisz/elastiknn) ![https://img.shields.io/github/stars/alexklibisz/elastiknn?style=social](https://img.shields.io/github/stars/alexklibisz/elastiknn?style=social)\n* [ExpANN](https://github.com/jacketsj/expANN) ![https://img.shields.io/github/stars/jacketsj/expANN?style=social](https://img.shields.io/github/stars/jacketsj/expANN?style=social)\n* [OpenSearch KNN](https://github.com/opensearch-project/k-NN) ![https://img.shields.io/github/stars/opensearch-project/k-NN?style=social](https://img.shields.io/github/stars/opensearch-project/k-NN?style=social)\n* [DiskANN](https://github.com/microsoft/diskann) ![https://img.shields.io/github/stars/microsoft/diskann?style=social](https://img.shields.io/github/stars/microsoft/diskann?style=social): Vamana, Vamana-PQ\n* [Vespa](https://github.com/vespa-engine/vespa) ![https://img.shields.io/github/stars/vespa-engine/vespa?style=social](https://img.shields.io/github/stars/vespa-engine/vespa?style=social)\n* [scipy](https://docs.scipy.org/doc/scipy/reference/spatial.html): cKDTree\n* [vald](https://github.com/vdaas/vald) ![https://img.shields.io/github/stars/vdaas/vald?style=social](https://img.shields.io/github/stars/vdaas/vald?style=social)\n* [Qdrant](https://github.com/qdrant/qdrant) ![https://img.shields.io/github/stars/qdrant/qdrant?style=social](https://img.shields.io/github/stars/qdrant/qdrant?style=social)\n* [HUAWEI(qsgngt)](https://github.com/WPJiang/HWTL_SDU-ANNS.git)\n* [Milvus](https://github.com/milvus-io/milvus) ![https://img.shields.io/github/stars/milvus-io/milvus?style=social](https://img.shields.io/github/stars/milvus-io/milvus?style=social): [Knowhere](https://github.com/milvus-io/knowhere)\n* [Zilliz(Glass)](https://github.com/hhy3/pyglass)\n* [pgvector](https://github.com/pgvector/pgvector) ![https://img.shields.io/github/stars/pgvector/pgvector?style=social](https://img.shields.io/github/stars/pgvector/pgvector?style=social)\n* [pgvecto.rs](https://github.com/tensorchord/pgvecto.rs) ![https://img.shields.io/github/stars/tensorchord/pgvecto.rs?style=social](https://img.shields.io/github/stars/tensorchord/pgvecto.rs?style=social)\n* [RediSearch](https://github.com/redisearch/redisearch) ![https://img.shields.io/github/stars/redisearch/redisearch?style=social](https://img.shields.io/github/stars/redisearch/redisearch?style=social)\n  * [pg_embedding](https://github.com/neondatabase/pg_embedding) ![https://img.shields.io/github/stars/pg_embedding/pg_embedding?style=social](https://img.shields.io/github/stars/neondatabase/pg_embedding?style=social)\n* [Descartes(01AI)](https://github.com/xiaoming-01ai/descartes)\n* [kgn](https://github.com/Henry-yan/kgn)\n\nData sets\n=========\n\nWe have a number of precomputed data sets in HDF5 format. All data sets have been pre-split into train/test and include ground truth data for the top-100 nearest neighbors.\n\n| Dataset                                                           | Dimensions | Train size | Test size | Neighbors | Distance  | Download                                                                   |\n| ----------------------------------------------------------------- | ---------: | ---------: | --------: | --------: | --------- | -------------------------------------------------------------------------- |\n| [DEEP1B](http://sites.skoltech.ru/compvision/noimi/)              |         96 |  9,990,000 |    10,000 |       100 | Angular   | [HDF5](http://ann-benchmarks.com/deep-image-96-angular.hdf5) (3.6GB)\n| [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) |        784 |     60,000 |    10,000 |       100 | Euclidean | [HDF5](http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5) (217MB) |\n| [GIST](http://corpus-texmex.irisa.fr/)                            |        960 |  1,000,000 |     1,000 |       100 | Euclidean | [HDF5](http://ann-benchmarks.com/gist-960-euclidean.hdf5) (3.6GB)          |\n| [GloVe](http://nlp.stanford.edu/projects/glove/)                  |         25 |  1,183,514 |    10,000 |       100 | Angular   | [HDF5](http://ann-benchmarks.com/glove-25-angular.hdf5) (121MB)            |\n| GloVe                                                             |         50 |  1,183,514 |    10,000 |       100 | Angular   | [HDF5](http://ann-benchmarks.com/glove-50-angular.hdf5) (235MB)            |\n| GloVe                                                             |        100 |  1,183,514 |    10,000 |       100 | Angular   | [HDF5](http://ann-benchmarks.com/glove-100-angular.hdf5) (463MB)           |\n| GloVe                                                             |        200 |  1,183,514 |    10,000 |       100 | Angular   | [HDF5](http://ann-benchmarks.com/glove-200-angular.hdf5) (918MB)           |\n| [Kosarak](http://fimi.uantwerpen.be/data/)                        |      27,983 |     74,962 |       500 |       100 | Jaccard   | [HDF5](http://ann-benchmarks.com/kosarak-jaccard.hdf5) (33MB)             |\n| [MNIST](http://yann.lecun.com/exdb/mnist/)                        |        784 |     60,000 |    10,000 |       100 | Euclidean | [HDF5](http://ann-benchmarks.com/mnist-784-euclidean.hdf5) (217MB)         |\n| [MovieLens-10M](https://grouplens.org/datasets/movielens/10m/)  |      65,134 |     69,363 |       500 |       100 | Jaccard   | [HDF5](http://ann-benchmarks.com/movielens10m-jaccard.hdf5) (63MB)             |\n| [NYTimes](https://archive.ics.uci.edu/ml/datasets/bag+of+words)   |        256 |    290,000 |    10,000 |       100 | Angular   | [HDF5](http://ann-benchmarks.com/nytimes-256-angular.hdf5) (301MB)         |\n| [SIFT](http://corpus-texmex.irisa.fr/)                           |        128 |  1,000,000 |    10,000 |       100 | Euclidean | [HDF5](http://ann-benchmarks.com/sift-128-euclidean.hdf5) (501MB)          |\n| [Last.fm](https://github.com/erikbern/ann-benchmarks/pull/91)     |         65 |    292,385 |    50,000 |       100 | Angular   | [HDF5](http://ann-benchmarks.com/lastfm-64-dot.hdf5) (135MB)               |\n\nResults\n=======\n\nThese are all as of April 2023, running all benchmarks on a r6i.16xlarge machine on AWS with `--parallelism 31` and hyperthreading disabled. All benchmarks are single-CPU.\n\nglove-100-angular\n-----------------\n\n![glove-100-angular](https://raw.github.com/erikbern/ann-benchmarks/master/results/glove-100-angular.png)\n\nsift-128-euclidean\n------------------\n\n![glove-100-angular](https://raw.github.com/erikbern/ann-benchmarks/master/results/sift-128-euclidean.png)\n\nfashion-mnist-784-euclidean\n---------------------------\n\n![fashion-mnist-784-euclidean](https://raw.github.com/erikbern/ann-benchmarks/master/results/fashion-mnist-784-euclidean.png)\n\nnytimes-256-angular\n-------------------\n\n![nytimes-256-angular](https://raw.github.com/erikbern/ann-benchmarks/master/results/nytimes-256-angular.png)\n\ngist-960-euclidean\n-------------------\n\n![gist-960-euclidean](https://raw.github.com/erikbern/ann-benchmarks/master/results/gist-960-euclidean.png)\n\nglove-25-angular\n----------------\n\n![glove-25-angular](https://raw.github.com/erikbern/ann-benchmarks/master/results/glove-25-angular.png)\n\nTODO: update plots on <http://ann-benchmarks.com>.\n\nInstall\n=======\n\nThe only prerequisite is Python (tested with 3.10.6) and Docker.\n\n1. Clone the repo.\n2. Run `pip install -r requirements.txt`.\n3. Run `python install.py` to build all the libraries inside Docker containers (this can take a while, like 10-30 minutes).\n\nRunning\n=======\n\n1. Run `python run.py` (this can take an extremely long time, potentially days)\n2. Run `python plot.py` or `python create_website.py` to plot results.\n3. Run `python data_export.py --out res.csv` to export all results into a csv file for additional post-processing.\n\nYou can customize the algorithms and datasets as follows:\n\n* Check that `ann_benchmarks/algorithms/{YOUR_IMPLEMENTATION}/config.yml` contains the parameter settings that you want to test\n* To run experiments on SIFT, invoke `python run.py --dataset glove-100-angular`. See `python run.py --help` for more information on possible settings. Note that experiments can take a long time. \n* To process the results, either use `python plot.py --dataset glove-100-angular` or `python create_website.py`. An example call: `python create_website.py --plottype recall/time --latex --scatter --outputdir website/`. \n\nIncluding your algorithm\n========================\n\nAdd your algorithm in the folder `ann_benchmarks/algorithms/{YOUR_IMPLEMENTATION}/` by providing\n\n- [ ] A small Python wrapper in `module.py`\n- [ ] A Dockerfile named `Dockerfile` \n- [ ] A set of hyper-parameters in `config.yml`\n- [ ] A CI test run by adding your implementation to `.github/workflows/benchmarks.yml`\n\nCheck the [available implementations](./ann_benchmarks/algorithms/) for inspiration.\n\nPrinciples\n==========\n\n* Everyone is welcome to submit pull requests with tweaks and changes to how each library is being used.\n* In particular: if you are the author of any of these libraries, and you think the benchmark can be improved, consider making the improvement and submitting a pull request.\n* This is meant to be an ongoing project and represent the current state.\n* Make everything easy to replicate, including installing and preparing the datasets.\n* Try many different values of parameters for each library and ignore the points that are not on the precision-performance frontier.\n* High-dimensional datasets with approximately 100-1000 dimensions. This is challenging but also realistic. Not more than 1000 dimensions because those problems should probably be solved by doing dimensionality reduction separately.\n* Single queries are used by default. ANN-Benchmarks enforces that only one CPU is saturated during experimentation, i.e., no multi-threading. A batch mode is available that provides all queries to the implementations at once. Add the flag `--batch` to `run.py` and `plot.py` to enable batch mode. \n* Avoid extremely costly index building (more than several hours).\n* Focus on datasets that fit in RAM. For billion-scale benchmarks, see the related [big-ann-benchmarks](https://github.com/harsha-simhadri/big-ann-benchmarks) project.\n* We mainly support CPU-based ANN algorithms. GPU support exists for FAISS, but it has to be compiled with GPU support locally and experiments must be run using the flags `--local --batch`. \n* Do proper train/test set of index data and query points.\n* Note that we consider that set similarity datasets are sparse and thus we pass a **sorted** array of integers to algorithms to represent the set of each user.\n\n\nAuthors\n=======\n\nBuilt by [Erik Bernhardsson](https://erikbern.com) with significant contributions from [Martin Aumüller](http://itu.dk/people/maau/) and [Alexander Faithfull](https://github.com/ale-f).\n\nRelated Publication\n==================\n\nDesign principles behind the benchmarking framework are described in the following publications: \n\n- M. Aumüller, E. Bernhardsson, A. Faithfull:\n[ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms](https://arxiv.org/abs/1807.05614). Information Systems 2019. DOI: [10.1016/j.is.2019.02.006](https://doi.org/10.1016/j.is.2019.02.006)\n-   M. Aumüller, E. Bernhardsson, A. Faithfull: [Reproducibility protocol for ANN-Benchmarks: A benchmarking tool for approximate nearest neighbor search algorithms](https://itu.dk/people/maau/additional/2022-ann-benchmarks-reproducibility.pdf), [Artifacts](https://doi.org/10.5281/zenodo.4607761).\n\nRelated Projects\n================\n\n- [big-ann-benchmarks](https://github.com/harsha-simhadri/big-ann-benchmarks) is a benchmarking effort for billion-scale approximate nearest neighbor search as part of the [NeurIPS'21 Competition track](https://neurips.cc/Conferences/2021/CompetitionTrack).\n\n"
        },
        {
          "name": "ann_benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "convert_algos.py",
          "type": "blob",
          "size": 4.5576171875,
          "content": "import sys\nimport yaml\nfrom dataclasses import asdict, dataclass, field\nfrom typing import Any, Dict, List, Literal, NewType\nfrom collections import defaultdict\n\nAlgoModule = NewType('AlgoModule', str)\nMetricType = Literal[\"bit\", \"float\"]\n\n@dataclass\nclass RunGroup:\n    args: Any = field(default_factory=dict)\n    arg_groups: List[Dict] = field(default_factory=list)\n    query_args: List[List[str]] = field(default_factory=list)\n\n@dataclass()\nclass Algorithm:\n    docker_tag: str\n    module: str\n    constructor: str\n    base_args: Dict = field(default_factory=dict)\n    disabled: bool = False\n    run_groups: Dict[str, RunGroup] = field(default_factory=dict)\n\n    def to_dict(self):\n        return asdict(self)\n\n@dataclass\nclass MetricType:\n    algorithms: Dict[str, Algorithm] = field(default_factory=dict)\n\n@dataclass\nclass Metric:\n    metric_types: Dict[str, MetricType] = field(default_factory=dict)\n\n@dataclass\nclass Data:\n    float: Metric = field(default_factory=Metric)\n    bit: Metric = field(default_factory=Metric)\n\n\n@dataclass\nclass AlgorithmFile:\n    # maps float.euclidean.Algorithm\n    algos: Dict[str, Dict[str, Algorithm]] = field(default_factory=dict)\n\ndef replace_hyphens_in_keys(data):\n    \"\"\"Replaces hyphens in keys with underscores for a given dictionary.\"\"\"\n    return {k.replace('-', '_'): v for k, v in data.items()}\n\ndef convert_raw_data_to_dataclasses(raw_data: Dict[str, Any]) -> Data:\n    \"\"\"Converts the raw data (from Yaml) into the above dataclasses.\"\"\"\n    data = Data()\n    for metric_name, metric_types in raw_data.items():\n        metric = Metric()\n        for metric_type_name, algorithms in metric_types.items():\n            metric_type = MetricType()\n            for algorithm_name, algorithm_info in algorithms.items():\n                run_groups_params = algorithm_info.pop('run-groups') if algorithm_info.get('run-groups') is not None else {}\n                run_groups = {name: RunGroup(**replace_hyphens_in_keys(info)) for name, info in run_groups_params.items()}\n                algorithm = Algorithm(run_groups=run_groups, **replace_hyphens_in_keys(algorithm_info))\n                metric_type.algorithms[algorithm_name] = algorithm\n            metric.metric_types[metric_type_name] = metric_type\n        metric.metric_types[metric_name] = metric\n    return data\n\n\ndef add_algorithm_metrics(files: Dict[AlgoModule, Dict[str, Dict[str, AlgorithmFile]]], metric_type: MetricType, metric_dict: Dict[str, MetricType]):\n    \"\"\"\n    Updates the mapping of algorithms to configurations for a given metric type and data.\n    Process a given metric dictionary and update the 'files' dictionary.\n    \"\"\"\n    for metric, metric_type in metric_dict.items():\n        for name, algorithm in metric_type.algorithms.items():\n            algorithm_name = algorithm.module.split(\".\")[-1]\n            if files[algorithm_name].get(metric_type) is None:\n                files[algorithm_name][metric_type] = {}\n\n            if files[algorithm_name][metric_type].get(metric) is None:\n                files[algorithm_name][metric_type][metric] = []\n\n            output = algorithm.to_dict()\n            output[\"name\"] = name\n            files[algorithm_name][metric_type][metric].append(output)\n\n\ndef config_write(module_name: str, content: Dict[str, Dict[str, AlgorithmFile]]) -> None:\n    \"\"\"For a given algorithm module, write the algorithm's config to file.\"\"\"\n    class CustomDumper(yaml.SafeDumper):\n        def represent_list(self, data):\n            ## Avoid use [[]] for base lists\n            if len(data) > 0 and isinstance(data[0], dict) and \"docker_tag\" in data[0].keys():\n                return super().represent_list(data)\n            else:\n                return self.represent_sequence('tag:yaml.org,2002:seq', data, flow_style=True)\n\n    CustomDumper.add_representer(list, CustomDumper.represent_list)\n    \n    with open(f\"ann_benchmarks/algorithms/{module_name}/config.yml\", 'w+') as cfg:\n        yaml.dump(content, cfg, Dumper=CustomDumper, default_flow_style=False)\n\n\nif __name__ == \"__main__\":\n    try:\n        raw_yaml = sys.argv[0] if len(sys.argv) > 1 else \"algos.yaml\"\n        with open(raw_yaml, 'r') as stream:\n            raw_data = yaml.safe_load(stream)\n    except FileNotFoundError:\n        print(\"The file 'algos.yaml' was not found.\")\n        exit(1)\n\n    data = convert_raw_data_to_dataclasses(raw_data)\n    files: Dict[str, Dict[str, Dict[str, AlgorithmFile]]] = defaultdict(dict)\n\n    add_algorithm_metrics(files, 'bit', data.bit.metric_types)\n    add_algorithm_metrics(files, 'float', data.float.metric_types)\n\n    for module_name, file_dict in files.items():\n        config_write(module_name, file_dict)"
        },
        {
          "name": "create_dataset.py",
          "type": "blob",
          "size": 0.314453125,
          "content": "import argparse\n\nfrom ann_benchmarks.datasets import DATASETS, get_dataset_fn\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset\", choices=DATASETS.keys(), required=True)\n    args = parser.parse_args()\n    fn = get_dataset_fn(args.dataset)\n    DATASETS[args.dataset](fn)\n"
        },
        {
          "name": "create_website.py",
          "type": "blob",
          "size": 8.69140625,
          "content": "import matplotlib as mpl\n\nmpl.use(\"Agg\")  # noqa\nimport argparse\nimport hashlib\nimport os\n\nfrom jinja2 import Environment, FileSystemLoader\n\nimport plot\nfrom ann_benchmarks import results\nfrom ann_benchmarks.datasets import get_dataset\nfrom ann_benchmarks.plotting.metrics import all_metrics as metrics\nfrom ann_benchmarks.plotting.plot_variants import \\\n    all_plot_variants as plot_variants\nfrom ann_benchmarks.plotting.utils import (compute_all_metrics,\n                                           create_linestyles, create_pointset,\n                                           get_plot_label)\n\ncolors = [\n    \"rgba(166,206,227,1)\",\n    \"rgba(31,120,180,1)\",\n    \"rgba(178,223,138,1)\",\n    \"rgba(51,160,44,1)\",\n    \"rgba(251,154,153,1)\",\n    \"rgba(227,26,28,1)\",\n    \"rgba(253,191,111,1)\",\n    \"rgba(255,127,0,1)\",\n    \"rgba(202,178,214,1)\",\n]\n\npoint_styles = {\n    \"o\": \"circle\",\n    \"<\": \"triangle\",\n    \"*\": \"star\",\n    \"x\": \"cross\",\n    \"+\": \"rect\",\n}\n\n\ndef convert_color(color):\n    r, g, b, a = color\n    return \"rgba(%(r)d, %(g)d, %(b)d, %(a)d)\" % {\"r\": r * 255, \"g\": g * 255, \"b\": b * 255, \"a\": a}\n\n\ndef convert_linestyle(ls):\n    new_ls = {}\n    for algo in ls.keys():\n        algostyle = ls[algo]\n        new_ls[algo] = (\n            convert_color(algostyle[0]),\n            convert_color(algostyle[1]),\n            algostyle[2],\n            point_styles[algostyle[3]],\n        )\n    return new_ls\n\n\ndef get_run_desc(properties):\n    return \"%(dataset)s_%(count)d_%(distance)s\" % properties\n\n\ndef get_dataset_from_desc(desc):\n    return desc.split(\"_\")[0]\n\n\ndef get_count_from_desc(desc):\n    return desc.split(\"_\")[1]\n\n\ndef get_distance_from_desc(desc):\n    return desc.split(\"_\")[2]\n\n\ndef get_dataset_label(desc):\n    return \"{} (k = {})\".format(get_dataset_from_desc(desc), get_count_from_desc(desc))\n\n\ndef directory_path(s):\n    if not os.path.isdir(s):\n        raise argparse.ArgumentTypeError(\"'%s' is not a directory\" % s)\n    return s + \"/\"\n\n\ndef prepare_data(data, xn, yn):\n    \"\"\"Change format from (algo, instance, dict) to (algo, instance, x, y).\"\"\"\n    res = []\n    for algo, algo_name, result in data:\n        res.append((algo, algo_name, result[xn], result[yn]))\n    return res\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--plottype\",\n    help=\"Generate only the plots specified\",\n    nargs=\"*\",\n    choices=plot_variants.keys(),\n    default=plot_variants.keys(),\n)\nparser.add_argument(\"--outputdir\", help=\"Select output directory\", default=\".\", type=directory_path, action=\"store\")\nparser.add_argument(\"--latex\", help=\"generates latex code for each plot\", action=\"store_true\")\nparser.add_argument(\"--scatter\", help=\"create scatterplot for data\", action=\"store_true\")\nparser.add_argument(\"--recompute\", help=\"Clears the cache and recomputes the metrics\", action=\"store_true\")\nargs = parser.parse_args()\n\n\ndef get_lines(all_data, xn, yn, render_all_points):\n    \"\"\"For each algorithm run on a dataset, obtain its performance\n    curve coords.\"\"\"\n    plot_data = []\n    for algo in sorted(all_data.keys(), key=lambda x: x.lower()):\n        xs, ys, ls, axs, ays, als = create_pointset(prepare_data(all_data[algo], xn, yn), xn, yn)\n        if render_all_points:\n            xs, ys, ls = axs, ays, als\n        plot_data.append({\"name\": algo, \"coords\": zip(xs, ys), \"labels\": ls, \"scatter\": render_all_points})\n    return plot_data\n\n\ndef create_plot(all_data, xn, yn, linestyle, j2_env, additional_label=\"\", plottype=\"line\"):\n    xm, ym = (metrics[xn], metrics[yn])\n    render_all_points = plottype == \"bubble\"\n    plot_data = get_lines(all_data, xn, yn, render_all_points)\n    latex_code = j2_env.get_template(\"latex.template\").render(\n        plot_data=plot_data, caption=get_plot_label(xm, ym), xlabel=xm[\"description\"], ylabel=ym[\"description\"]\n    )\n    plot_data = get_lines(all_data, xn, yn, render_all_points)\n    button_label = hashlib.sha224((get_plot_label(xm, ym) + additional_label).encode(\"utf-8\")).hexdigest()\n    return j2_env.get_template(\"chartjs.template\").render(\n        args=args,\n        latex_code=latex_code,\n        button_label=button_label,\n        data_points=plot_data,\n        xlabel=xm[\"description\"],\n        ylabel=ym[\"description\"],\n        plottype=plottype,\n        plot_label=get_plot_label(xm, ym),\n        label=additional_label,\n        linestyle=linestyle,\n        render_all_points=render_all_points,\n    )\n\n\ndef build_detail_site(data, label_func, j2_env, linestyles, batch=False):\n    for (name, runs) in data.items():\n        print(\"Building '%s'\" % name)\n        runs.keys()\n        label = label_func(name)\n        data = {\"normal\": [], \"scatter\": []}\n\n        for plottype in args.plottype:\n            xn, yn = plot_variants[plottype]\n            data[\"normal\"].append(create_plot(runs, xn, yn, convert_linestyle(linestyles), j2_env))\n            if args.scatter:\n                data[\"scatter\"].append(\n                    create_plot(runs, xn, yn, convert_linestyle(linestyles), j2_env, \"Scatterplot \", \"bubble\")\n                )\n\n        # create png plot for summary page\n        data_for_plot = {}\n        for k in runs.keys():\n            data_for_plot[k] = prepare_data(runs[k], \"k-nn\", \"qps\")\n        plot.create_plot(\n            data_for_plot, False, \"linear\", \"log\", \"k-nn\", \"qps\", args.outputdir + name + \".png\", linestyles, batch\n        )\n        output_path = args.outputdir + name + \".html\"\n        with open(output_path, \"w\") as text_file:\n            text_file.write(\n                j2_env.get_template(\"detail_page.html\").render(title=label, plot_data=data, args=args, batch=batch)\n            )\n\n\ndef build_index_site(datasets, algorithms, j2_env, file_name):\n    dataset_data = {\"batch\": [], \"non-batch\": []}\n    for mode in [\"batch\", \"non-batch\"]:\n        distance_measures = sorted(set([get_distance_from_desc(e) for e in datasets[mode].keys()]))\n        sorted_datasets = sorted(set([get_dataset_from_desc(e) for e in datasets[mode].keys()]))\n\n        for dm in distance_measures:\n            d = {\"name\": dm.capitalize(), \"entries\": []}\n            for ds in sorted_datasets:\n                matching_datasets = [\n                    e\n                    for e in datasets[mode].keys()\n                    if get_dataset_from_desc(e) == ds and get_distance_from_desc(e) == dm  # noqa\n                ]\n                sorted_matches = sorted(matching_datasets, key=lambda e: int(get_count_from_desc(e)))\n                for idd in sorted_matches:\n                    d[\"entries\"].append({\"name\": idd, \"desc\": get_dataset_label(idd)})\n            dataset_data[mode].append(d)\n\n    with open(args.outputdir + \"index.html\", \"w\") as text_file:\n        text_file.write(\n            j2_env.get_template(\"summary.html\").render(\n                title=\"ANN-Benchmarks\", dataset_with_distances=dataset_data, algorithms=algorithms\n            )\n        )\n\n\ndef load_all_results():\n    \"\"\"Read all result files and compute all metrics\"\"\"\n    all_runs_by_dataset = {\"batch\": {}, \"non-batch\": {}}\n    all_runs_by_algorithm = {\"batch\": {}, \"non-batch\": {}}\n    cached_true_dist = []\n    old_sdn = None\n    for mode in [\"non-batch\", \"batch\"]:\n        for properties, f in results.load_all_results(batch_mode=(mode == \"batch\")):\n            sdn = get_run_desc(properties)\n            if sdn != old_sdn:\n                dataset, _ = get_dataset(properties[\"dataset\"])\n                cached_true_dist = list(dataset[\"distances\"])\n                old_sdn = sdn\n            algo_ds = get_dataset_label(sdn)\n            desc_suffix = \"-batch\" if mode == \"batch\" else \"\"\n            algo = properties[\"algo\"] + desc_suffix\n            sdn += desc_suffix\n            ms = compute_all_metrics(cached_true_dist, f, properties, args.recompute)\n            all_runs_by_algorithm[mode].setdefault(algo, {}).setdefault(algo_ds, []).append(ms)\n            all_runs_by_dataset[mode].setdefault(sdn, {}).setdefault(algo, []).append(ms)\n\n    return (all_runs_by_dataset, all_runs_by_algorithm)\n\n\nj2_env = Environment(loader=FileSystemLoader(\"./templates/\"), trim_blocks=True)\nj2_env.globals.update(zip=zip, len=len)\nruns_by_ds, runs_by_algo = load_all_results()\ndataset_names = [get_dataset_label(x) for x in list(runs_by_ds[\"batch\"].keys()) + list(runs_by_ds[\"non-batch\"].keys())]\nalgorithm_names = list(runs_by_algo[\"batch\"].keys()) + list(runs_by_algo[\"non-batch\"].keys())\n\nlinestyles = {**create_linestyles(dataset_names), **create_linestyles(algorithm_names)}\n\nbuild_detail_site(runs_by_ds[\"non-batch\"], lambda label: get_dataset_label(label), j2_env, linestyles, False)\n\nbuild_detail_site(runs_by_ds[\"batch\"], lambda label: get_dataset_label(label), j2_env, linestyles, True)\n\nbuild_detail_site(runs_by_algo[\"non-batch\"], lambda x: x, j2_env, linestyles, False)\n\nbuild_detail_site(runs_by_algo[\"batch\"], lambda x: x, j2_env, linestyles, True)\n\nbuild_index_site(runs_by_ds, runs_by_algo, j2_env, \"index.html\")\n"
        },
        {
          "name": "data_export.py",
          "type": "blob",
          "size": 1.20703125,
          "content": "import argparse\nimport csv\n\nfrom ann_benchmarks.datasets import DATASETS, get_dataset\nfrom ann_benchmarks.plotting.utils import compute_metrics_all_runs\nfrom ann_benchmarks.results import load_all_results\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--output\", help=\"Path to the output file\", required=True)\n    parser.add_argument(\"--recompute\", action=\"store_true\", help=\"Recompute metrics\")\n    args = parser.parse_args()\n\n    datasets = DATASETS.keys()\n    dfs = []\n    for dataset_name in datasets:\n        print(\"Looking at dataset\", dataset_name)\n        if len(list(load_all_results(dataset_name))) > 0:\n            results = load_all_results(dataset_name)\n            dataset, _ = get_dataset(dataset_name)\n            results = compute_metrics_all_runs(dataset, results, args.recompute)\n            for res in results:\n                res[\"dataset\"] = dataset_name\n                dfs.append(res)\n    if len(dfs) > 0:\n        with open(args.output, \"w\", newline=\"\") as csvfile:\n            names = list(dfs[0].keys())\n            writer = csv.DictWriter(csvfile, fieldnames=names)\n            writer.writeheader()\n            for res in dfs:\n                writer.writerow(res)\n"
        },
        {
          "name": "install.py",
          "type": "blob",
          "size": 2.1748046875,
          "content": "import argparse\nimport os\nimport subprocess\nimport sys\nfrom multiprocessing import Pool\n\nfrom ann_benchmarks.main import positive_int\n\n\ndef build(library, args):\n    print(\"Building %s...\" % library)\n    if args is not None and len(args) != 0:\n        q = \" \".join([\"--build-arg \" + x.replace(\" \", \"\\\\ \") for x in args])\n    else:\n        q = \"\"\n\n    try:\n        subprocess.check_call(\n            \"docker build %s --rm -t ann-benchmarks-%s -f\" \" ann_benchmarks/algorithms/%s/Dockerfile  .\" % (q, library, library),\n            shell=True,\n        )\n        return {library: \"success\"}\n    except subprocess.CalledProcessError:\n        return {library: \"fail\"}\n\n\ndef build_multiprocess(args):\n    return build(*args)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"--proc\", default=1, type=positive_int, help=\"the number of process to build docker images\")\n    parser.add_argument(\"--algorithm\", metavar=\"NAME\", help=\"build only the named algorithm image\", default=None)\n    parser.add_argument(\"--build-arg\", help=\"pass given args to all docker builds\", nargs=\"+\")\n    args = parser.parse_args()\n\n    print(\"Building base image...\")\n    subprocess.check_call(\n         \"docker build \\\n         --rm -t ann-benchmarks -f ann_benchmarks/algorithms/base/Dockerfile .\",\n         shell=True,\n     )\n    \n    if args.algorithm:\n        tags = [args.algorithm]\n    elif os.getenv(\"LIBRARY\"):\n        tags = [os.getenv(\"LIBRARY\")]\n    else:\n        tags = [fn.split(\".\")[-1] for fn in os.listdir(\"ann_benchmarks/algorithms\")]\n\n    print(\"Building algorithm images... with (%d) processes\" % args.proc)\n\n    if args.proc == 1:\n        install_status = [build(tag, args.build_arg) for tag in tags]\n    else:\n        pool = Pool(processes=args.proc)\n        install_status = pool.map(build_multiprocess, [(tag, args.build_arg) for tag in tags])\n        pool.close()\n        pool.join()\n\n    print(\"\\n\\nInstall Status:\\n\" + \"\\n\".join(str(algo) for algo in install_status))\n\n    # Exit 1 if any of the installations fail.\n    for x in install_status:\n        for (k, v) in x.items():\n            if v == \"fail\":\n                sys.exit(1)\n"
        },
        {
          "name": "logging.conf",
          "type": "blob",
          "size": 0.5283203125,
          "content": "[loggers]\nkeys=root,annb\n\n[handlers]\nkeys=consoleHandler,fileHandler\n\n[formatters]\nkeys=simpleFormatter\n\n[formatter_simpleFormatter]\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\ndatefmt=\n\n[handler_consoleHandler]\nclass=StreamHandler\nlevel=INFO\nformatter=simpleFormatter\nargs=(sys.stdout,)\n\n[handler_fileHandler]\nclass=FileHandler\nlevel=INFO\nformatter=simpleFormatter\nargs=('annb.log','w')\n\n[logger_root]\nlevel=WARN\nhandlers=consoleHandler\n\n[logger_annb]\nlevel=INFO\nhandlers=consoleHandler,fileHandler\nqualname=annb\npropagate=0"
        },
        {
          "name": "plot.py",
          "type": "blob",
          "size": 5.2978515625,
          "content": "import matplotlib as mpl\n\nmpl.use(\"Agg\")  # noqa\nimport argparse\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom ann_benchmarks.datasets import get_dataset\nfrom ann_benchmarks.plotting.metrics import all_metrics as metrics\nfrom ann_benchmarks.plotting.utils import (compute_metrics, create_linestyles,\n                                           create_pointset, get_plot_label)\nfrom ann_benchmarks.results import get_unique_algorithms, load_all_results\n\n\ndef create_plot(all_data, raw, x_scale, y_scale, xn, yn, fn_out, linestyles, batch):\n    xm, ym = (metrics[xn], metrics[yn])\n    # Now generate each plot\n    handles = []\n    labels = []\n    plt.figure(figsize=(12, 9))\n\n    # Sorting by mean y-value helps aligning plots with labels\n    def mean_y(algo):\n        xs, ys, ls, axs, ays, als = create_pointset(all_data[algo], xn, yn)\n        return -np.log(np.array(ys)).mean()\n\n    # Find range for logit x-scale\n    min_x, max_x = 1, 0\n    for algo in sorted(all_data.keys(), key=mean_y):\n        xs, ys, ls, axs, ays, als = create_pointset(all_data[algo], xn, yn)\n        min_x = min([min_x] + [x for x in xs if x > 0])\n        max_x = max([max_x] + [x for x in xs if x < 1])\n        color, faded, linestyle, marker = linestyles[algo]\n        (handle,) = plt.plot(\n            xs, ys, \"-\", label=algo, color=color, ms=7, mew=3, lw=3, marker=marker\n        )\n        handles.append(handle)\n        if raw:\n            (handle2,) = plt.plot(\n                axs, ays, \"-\", label=algo, color=faded, ms=5, mew=2, lw=2, marker=marker\n            )\n        labels.append(algo)\n\n    ax = plt.gca()\n    ax.set_ylabel(ym[\"description\"])\n    ax.set_xlabel(xm[\"description\"])\n    # Custom scales of the type --x-scale a3\n    if x_scale[0] == \"a\":\n        alpha = float(x_scale[1:])\n\n        def fun(x):\n            return 1 - (1 - x) ** (1 / alpha)\n\n        def inv_fun(x):\n            return 1 - (1 - x) ** alpha\n\n        ax.set_xscale(\"function\", functions=(fun, inv_fun))\n        if alpha <= 3:\n            ticks = [inv_fun(x) for x in np.arange(0, 1.2, 0.2)]\n            plt.xticks(ticks)\n        if alpha > 3:\n            from matplotlib import ticker\n\n            ax.xaxis.set_major_formatter(ticker.LogitFormatter())\n            # plt.xticks(ticker.LogitLocator().tick_values(min_x, max_x))\n            plt.xticks([0, 1 / 2, 1 - 1e-1, 1 - 1e-2, 1 - 1e-3, 1 - 1e-4, 1])\n    # Other x-scales\n    else:\n        ax.set_xscale(x_scale)\n    ax.set_yscale(y_scale)\n    ax.set_title(get_plot_label(xm, ym))\n    plt.gca().get_position()\n    # plt.gca().set_position([box.x0, box.y0, box.width * 0.8, box.height])\n    ax.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(1, 0.5), prop={\"size\": 9})\n    plt.grid(visible=True, which=\"major\", color=\"0.65\", linestyle=\"-\")\n    plt.setp(ax.get_xminorticklabels(), visible=True)\n\n    # Logit scale has to be a subset of (0,1)\n    if \"lim\" in xm and x_scale != \"logit\":\n        x0, x1 = xm[\"lim\"]\n        plt.xlim(max(x0, 0), min(x1, 1))\n    elif x_scale == \"logit\":\n        plt.xlim(min_x, max_x)\n    if \"lim\" in ym:\n        plt.ylim(ym[\"lim\"])\n\n    # Workaround for bug https://github.com/matplotlib/matplotlib/issues/6789\n    ax.spines[\"bottom\"]._adjust_location()\n\n    plt.savefig(fn_out, bbox_inches=\"tight\")\n    plt.close()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset\", metavar=\"DATASET\", default=\"glove-100-angular\")\n    parser.add_argument(\"--count\", default=10)\n    parser.add_argument(\n        \"--definitions\", metavar=\"FILE\", help=\"load algorithm definitions from FILE\", default=\"algos.yaml\"\n    )\n    parser.add_argument(\"--limit\", default=-1)\n    parser.add_argument(\"-o\", \"--output\")\n    parser.add_argument(\n        \"-x\", \"--x-axis\", help=\"Which metric to use on the X-axis\", choices=metrics.keys(), default=\"k-nn\"\n    )\n    parser.add_argument(\n        \"-y\", \"--y-axis\", help=\"Which metric to use on the Y-axis\", choices=metrics.keys(), default=\"qps\"\n    )\n    parser.add_argument(\n        \"-X\", \"--x-scale\", help=\"Scale to use when drawing the X-axis. Typically linear, logit or a2\", default=\"linear\"\n    )\n    parser.add_argument(\n        \"-Y\",\n        \"--y-scale\",\n        help=\"Scale to use when drawing the Y-axis\",\n        choices=[\"linear\", \"log\", \"symlog\", \"logit\"],\n        default=\"linear\",\n    )\n    parser.add_argument(\n        \"--raw\", help=\"Show raw results (not just Pareto frontier) in faded colours\", action=\"store_true\"\n    )\n    parser.add_argument(\"--batch\", help=\"Plot runs in batch mode\", action=\"store_true\")\n    parser.add_argument(\"--recompute\", help=\"Clears the cache and recomputes the metrics\", action=\"store_true\")\n    args = parser.parse_args()\n\n    if not args.output:\n        args.output = \"results/%s.png\" % (args.dataset + (\"-batch\" if args.batch else \"\"))\n        print(\"writing output to %s\" % args.output)\n\n    dataset, _ = get_dataset(args.dataset)\n    count = int(args.count)\n    unique_algorithms = get_unique_algorithms()\n    results = load_all_results(args.dataset, count, args.batch)\n    linestyles = create_linestyles(sorted(unique_algorithms))\n    runs = compute_metrics(np.array(dataset[\"distances\"]), results, args.x_axis, args.y_axis, args.recompute)\n    if not runs:\n        raise Exception(\"Nothing to plot\")\n\n    create_plot(\n        runs, args.raw, args.x_scale, args.y_scale, args.x_axis, args.y_axis, args.output, linestyles, args.batch\n    )\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.060546875,
          "content": "[tool.black]\nline-length = 120\n\n[tool.ruff]\nline-length = 120\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1796875,
          "content": "ansicolors==1.1.8\ndocker==7.1.0\nh5py==3.8.0\nmatplotlib==3.6.3\nnumpy==1.24.2\npyyaml==6.0\npsutil==5.9.4\nscikit-learn==1.2.1\njinja2==3.1.2\npytest==7.2.2\ndatasets==2.12.0\nrequests==2.31.0\n"
        },
        {
          "name": "results",
          "type": "tree",
          "content": null
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 0.1376953125,
          "content": "from multiprocessing import freeze_support\n\nfrom ann_benchmarks.main import main\n\nif __name__ == \"__main__\":\n    freeze_support()\n    main()\n"
        },
        {
          "name": "run_algorithm.py",
          "type": "blob",
          "size": 0.0693359375,
          "content": "from ann_benchmarks.runner import run_from_cmdline\n\nrun_from_cmdline()\n"
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}