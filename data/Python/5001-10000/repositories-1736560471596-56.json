{
  "metadata": {
    "timestamp": 1736560471596,
    "page": 56,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "bigscience-workshop/petals",
      "stars": 9339,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.759765625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n.idea/\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.904296875,
          "content": "FROM nvcr.io/nvidia/cuda:11.0.3-cudnn8-devel-ubuntu20.04\nLABEL maintainer=\"bigscience-workshop\"\nLABEL repository=\"petals\"\n\nWORKDIR /home\n# Set en_US.UTF-8 locale by default\nRUN echo \"LC_ALL=en_US.UTF-8\" >> /etc/environment\n\n# Install packages\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n  build-essential \\\n  wget \\\n  git \\\n  && apt-get clean autoclean && rm -rf /var/lib/apt/lists/{apt,dpkg,cache,log} /tmp/* /var/tmp/*\n\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O install_miniconda.sh && \\\n  bash install_miniconda.sh -b -p /opt/conda && rm install_miniconda.sh\nENV PATH=\"/opt/conda/bin:${PATH}\"\n\nRUN conda install python~=3.10.12 pip && \\\n    pip install --no-cache-dir \"torch>=1.12\" && \\\n    conda clean --all && rm -rf ~/.cache/pip\n\nVOLUME /cache\nENV PETALS_CACHE=/cache\n\nCOPY . petals/\nRUN pip install --no-cache-dir -e petals\n\nWORKDIR /home/petals/\nCMD bash\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0634765625,
          "content": "MIT License\n\nCopyright (c) 2022 Petals authors and collaborators\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.9462890625,
          "content": "<p align=\"center\">\n    <img src=\"https://i.imgur.com/7eR7Pan.png\" width=\"400\"><br>\n    Run large language models at home, BitTorrent-style.<br>\n    Fine-tuning and inference <a href=\"https://github.com/bigscience-workshop/petals#benchmarks\">up to 10x faster</a> than offloading\n    <br><br>\n    <a href=\"https://pypi.org/project/petals/\"><img src=\"https://img.shields.io/pypi/v/petals.svg?color=green\"></a>\n    <a href=\"https://discord.gg/tfHfe8B34k\"><img src=\"https://img.shields.io/discord/865254854262652969?label=discord&logo=discord&logoColor=white\"></a>\n    <br>\n</p>\n\nGenerate text with distributed **Llama 3.1** (up to 405B), **Mixtral** (8x22B), **Falcon** (40B+) or **BLOOM** (176B) and fine‚Äëtune them for your own tasks &mdash; right from your desktop computer or Google Colab:\n\n```python\nfrom transformers import AutoTokenizer\nfrom petals import AutoDistributedModelForCausalLM\n\n# Choose any model available at https://health.petals.dev\nmodel_name = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n\n# Connect to a distributed network hosting model layers\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoDistributedModelForCausalLM.from_pretrained(model_name)\n\n# Run the model as if it were on your computer\ninputs = tokenizer(\"A cat sat\", return_tensors=\"pt\")[\"input_ids\"]\noutputs = model.generate(inputs, max_new_tokens=5)\nprint(tokenizer.decode(outputs[0]))  # A cat sat on a mat...\n```\n\n<p align=\"center\">\n    üöÄ &nbsp;<b><a href=\"https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing\">Try now in Colab</a></b>\n</p>\n\nü¶ô **Want to run Llama?** [Request access](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct) to its weights, then run `huggingface-cli login` in the terminal before loading the model. Or just try it in our [chatbot app](https://chat.petals.dev).\n\nüîè **Privacy.** Your data will be processed with the help of other people in the public swarm. Learn more about privacy [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety). For sensitive data, you can set up a [private swarm](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm) among people you trust.\n\nüí¨ **Any questions?** Ping us in [our Discord](https://discord.gg/KdThf2bWVU)!\n\n## Connect your GPU and increase Petals capacity\n\nPetals is a community-run system &mdash; we rely on people sharing their GPUs. You can help serving one of the [available models](https://health.petals.dev) or host a new model from ü§ó [Model Hub](https://huggingface.co/models)!\n\nAs an example, here is how to host a part of [Llama 3.1 (405B) Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct) on your GPU:\n\nü¶ô **Want to host Llama?** [Request access](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct) to its weights, then run `huggingface-cli login` in the terminal before loading the model.\n\nüêß **Linux + Anaconda.** Run these commands for NVIDIA GPUs (or follow [this](https://github.com/bigscience-workshop/petals/wiki/Running-on-AMD-GPU) for AMD):\n\n```bash\nconda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia\npip install git+https://github.com/bigscience-workshop/petals\npython -m petals.cli.run_server meta-llama/Meta-Llama-3.1-405B-Instruct\n```\n\nü™ü **Windows + WSL.** Follow [this guide](https://github.com/bigscience-workshop/petals/wiki/Run-Petals-server-on-Windows) on our Wiki.\n\nüêã **Docker.** Run our [Docker](https://www.docker.com) image for NVIDIA GPUs (or follow [this](https://github.com/bigscience-workshop/petals/wiki/Running-on-AMD-GPU) for AMD):\n\n```bash\nsudo docker run -p 31330:31330 --ipc host --gpus all --volume petals-cache:/cache --rm \\\n    learningathome/petals:main \\\n    python -m petals.cli.run_server --port 31330 meta-llama/Meta-Llama-3.1-405B-Instruct\n```\n\nüçè **macOS + Apple M1/M2 GPU.** Install [Homebrew](https://brew.sh/), then run these commands:\n\n```bash\nbrew install python\npython3 -m pip install git+https://github.com/bigscience-workshop/petals\npython3 -m petals.cli.run_server meta-llama/Meta-Llama-3.1-405B-Instruct\n```\n\n<p align=\"center\">\n    üìö &nbsp;<b><a href=\"https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-server\">Learn more</a></b> (how to use multiple GPUs, start the server on boot, etc.)\n</p>\n\nüîí **Security.** Hosting a server does not allow others to run custom code on your computer. Learn more [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety).\n\nüí¨ **Any questions?** Ping us in [our Discord](https://discord.gg/X7DgtxgMhc)!\n\nüèÜ **Thank you!** Once you load and host 10+ blocks, we can show your name or link on the [swarm monitor](https://health.petals.dev) as a way to say thanks. You can specify them with `--public_name YOUR_NAME`.\n\n## How does it work?\n\n- You load a small part of the model, then join a [network](https://health.petals.dev) of people serving the other parts. Single‚Äëbatch inference runs at up to **6 tokens/sec** for **Llama 2** (70B) and up to **4 tokens/sec** for **Falcon** (180B) ‚Äî enough for [chatbots](https://chat.petals.dev) and interactive apps.\n- You can employ any fine-tuning and sampling methods, execute custom paths through the model, or see its hidden states. You get the comforts of an API with the flexibility of **PyTorch** and **ü§ó Transformers**.\n\n<p align=\"center\">\n    <img src=\"https://i.imgur.com/RTYF3yW.png\" width=\"800\">\n</p>\n\n<p align=\"center\">\n    üìú &nbsp;<b><a href=\"https://arxiv.org/pdf/2209.01188.pdf\">Read paper</a></b>\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n    üìö &nbsp;<b><a href=\"https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions\">See FAQ</a></b>\n</p>\n\n## üìö Tutorials, examples, and more\n\nBasic tutorials:\n\n- Getting started: [tutorial](https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing)\n- Prompt-tune Llama-65B for text semantic classification: [tutorial](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb)\n- Prompt-tune BLOOM to create a personified chatbot: [tutorial](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-personachat.ipynb)\n\nUseful tools:\n\n- [Chatbot web app](https://chat.petals.dev) (connects to Petals via an HTTP/WebSocket endpoint): [source code](https://github.com/petals-infra/chat.petals.dev)\n- [Monitor](https://health.petals.dev) for the public swarm: [source code](https://github.com/petals-infra/health.petals.dev)\n\nAdvanced guides:\n\n- Launch a private swarm: [guide](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm)\n- Run a custom model: [guide](https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals)\n\n### Benchmarks\n\nPlease see **Section 3.3** of our [paper](https://arxiv.org/pdf/2209.01188.pdf).\n\n### üõ†Ô∏è Contributing\n\nPlease see our [FAQ](https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#contributing) on contributing.\n\n### üìú Citations\n\nAlexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel.\n[Petals: Collaborative Inference and Fine-tuning of Large Models.](https://arxiv.org/abs/2209.01188)\n_Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)._ 2023.\n\n```bibtex\n@inproceedings{borzunov2023petals,\n  title = {Petals: Collaborative Inference and Fine-tuning of Large Models},\n  author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Riabinin, Maksim and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  pages = {558--568},\n  year = {2023},\n  url = {https://arxiv.org/abs/2209.01188}\n}\n```\n\nAlexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, and Colin Raffel.\n[Distributed inference and fine-tuning of large language models over the Internet.](https://arxiv.org/abs/2312.08361)\n_Advances in Neural Information Processing Systems_ 36 (2023).\n\n```bibtex\n@inproceedings{borzunov2023distributed,\n  title = {Distributed inference and fine-tuning of large language models over the {I}nternet},\n  author = {Borzunov, Alexander and Ryabinin, Max and Chumachenko, Artem and Baranchuk, Dmitry and Dettmers, Tim and Belkada, Younes and Samygin, Pavel and Raffel, Colin},\n  booktitle = {Advances in Neural Information Processing Systems},\n  volume = {36},\n  pages = {12312--12331},\n  year = {2023},\n  url = {https://arxiv.org/abs/2312.08361}\n}\n```\n\n--------------------------------------------------------------------------------\n\n<p align=\"center\">\n    This project is a part of the <a href=\"https://bigscience.huggingface.co/\">BigScience</a> research workshop.\n</p>\n<p align=\"center\">\n    <img src=\"https://petals.dev/bigscience.png\" width=\"150\">\n</p>\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.3251953125,
          "content": "[build-system]\nrequires = [\n    \"setuptools>=42\",\n    \"wheel\"\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.black]\nline-length = 120\nrequired-version = \"22.3.0\"\n\n[tool.isort]\nprofile = \"black\"\nline_length = 120\ncombine_as_imports = true\ncombine_star = true\nknown_local_folder = [\"tests\", \"cli\"]\nknown_first_party = [\"test_utils\"]\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 1.939453125,
          "content": "[metadata]\nname = petals\nversion = attr: petals.__version__\nauthor = Petals Developers\nauthor_email = petals-devs@googlegroups.com\ndescription = Easy way to efficiently run 100B+ language models without high-end GPUs\nlong_description = file: README.md\nlong_description_content_type = text/markdown\nurl = https://github.com/bigscience-workshop/petals\nproject_urls =\n    Bug Tracker = https://github.com/bigscience-workshop/petals/issues\nclassifiers =\n    Development Status :: 4 - Beta\n    Intended Audience :: Developers\n    Intended Audience :: Science/Research\n    License :: OSI Approved :: MIT License\n    Programming Language :: Python :: 3\n    Programming Language :: Python :: 3.8\n    Programming Language :: Python :: 3.9\n    Programming Language :: Python :: 3.10\n    Programming Language :: Python :: 3.11\n    Topic :: Scientific/Engineering\n    Topic :: Scientific/Engineering :: Mathematics\n    Topic :: Scientific/Engineering :: Artificial Intelligence\n    Topic :: Software Development\n    Topic :: Software Development :: Libraries\n    Topic :: Software Development :: Libraries :: Python Modules\n\n[options]\npackage_dir =\n    = src\npackages = find:\npython_requires = >=3.8\ninstall_requires =\n    torch>=1.12\n    bitsandbytes==0.41.1\n    accelerate>=0.27.2\n    huggingface-hub>=0.11.1,<1.0.0\n    tokenizers>=0.13.3\n    transformers==4.43.1  # if you change this, please also change version assert in petals/__init__.py\n    speedtest-cli==2.1.3\n    hivemind @ git+https://github.com/learning-at-home/hivemind.git@213bff98a62accb91f254e2afdccbf1d69ebdea9\n    tensor_parallel==1.0.23\n    humanfriendly\n    async-timeout>=4.0.2\n    cpufeature>=0.2.0; platform_machine == \"x86_64\"\n    packaging>=20.9\n    sentencepiece>=0.1.99\n    peft==0.8.2\n    safetensors>=0.3.1\n    Dijkstar>=2.6.0\n    numpy<2\n\n[options.extras_require]\ndev =\n    pytest==6.2.5\n    pytest-forked\n    pytest-asyncio==0.16.0\n    black==22.3.0\n    isort==5.10.1\n    psutil\n\n[options.packages.find]\nwhere = src\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}