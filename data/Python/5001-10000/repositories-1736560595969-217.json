{
  "metadata": {
    "timestamp": 1736560595969,
    "page": 217,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "open-mmlab/Amphion",
      "stars": 8032,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.708984375,
          "content": "# Mac OS files\n.DS_Store\n\n# IDEs\n.idea\n.vs\n.vscode\n.cache\npyrightconfig.json\n\n# GitHub files\n.github\n\n# Byte-compiled / optimized / DLL / cached files\n__pycache__/\n*.py[cod]\n*$py.class\n*.pyc\n.temp\n*.c\n*.so\n*.o\n\n# Developing mode\n_*.sh\n_*.json\n*.lst\nyard*\n*.out\nevaluation/evalset_selection\nmfa\negs/svc/*wavmark\negs/svc/custom\negs/svc/*/dev*\negs/svc/dev_exp_config.json\negs/svc/dev\nbins/svc/demo*\nbins/svc/preprocess_custom.py\ndata\nckpts\n\n# Data and ckpt\n*.pkl\n*.pt\n*.npy\n*.npz\n*.tar.gz\n*.ckpt\n*.wav\n*.flac\npretrained/wenet/*conformer_exp\npretrained/bigvgan/args.json\n!egs/tts/VALLE/prompt_examples/*.wav\n\n# Runtime data dirs\nprocessed_data\ndata\nmodel_ckpt\nlogs\n*.lst\nsource_audio\nresult\nconversion_results\nget_available_gpu.py"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.1337890625,
          "content": "# Copyright (c) 2023 Amphion.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Other version: https://hub.docker.com/r/nvidia/cuda/tags\nFROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu18.04\n\nARG DEBIAN_FRONTEND=noninteractive\nARG PYTORCH='2.0.0'\nARG CUDA='cu118'\nARG SHELL='/bin/bash'\nARG MINICONDA='Miniconda3-py39_23.3.1-0-Linux-x86_64.sh'\n\nENV LANG=en_US.UTF-8 PYTHONIOENCODING=utf-8 PYTHONDONTWRITEBYTECODE=1 CUDA_HOME=/usr/local/cuda CONDA_HOME=/opt/conda SHELL=${SHELL}\nENV PATH=$CONDA_HOME/bin:$CUDA_HOME/bin:$PATH \\\n    LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH \\\n    LIBRARY_PATH=$CUDA_HOME/lib64:$LIBRARY_PATH \\\n    CONDA_PREFIX=$CONDA_HOME \\\n    NCCL_HOME=$CUDA_HOME\n\n# Install ubuntu packages\nRUN sed -i 's/archive.ubuntu.com/mirrors.cloud.tencent.com/g' /etc/apt/sources.list \\\n    && sed -i 's/security.ubuntu.com/mirrors.cloud.tencent.com/g' /etc/apt/sources.list \\\n    && rm /etc/apt/sources.list.d/cuda.list \\\n    && apt-get update \\\n    && apt-get -y install \\\n    python3-pip ffmpeg git less wget libsm6 libxext6 libxrender-dev \\\n    build-essential cmake pkg-config libx11-dev libatlas-base-dev \\\n    libgtk-3-dev libboost-python-dev vim libgl1-mesa-glx \\\n    libaio-dev software-properties-common tmux \\\n    espeak-ng\n\n# Install miniconda with python 3.9\nUSER root\n# COPY Miniconda3-py39_23.3.1-0-Linux-x86_64.sh /root/anaconda.sh\nRUN wget -t 0 -c -O /tmp/anaconda.sh https://repo.anaconda.com/miniconda/${MINICONDA} \\\n    && mv /tmp/anaconda.sh /root/anaconda.sh \\\n    && ${SHELL} /root/anaconda.sh -b -p $CONDA_HOME \\\n    && rm /root/anaconda.sh\n\nRUN conda create -y --name amphion python=3.9.15\n\nWORKDIR /app\nCOPY env.sh env.sh\nRUN chmod +x ./env.sh\n\nRUN [\"conda\", \"run\", \"-n\", \"amphion\", \"-vvv\", \"--no-capture-output\", \"./env.sh\"]\n\nRUN conda init \\\n    && echo \"\\nconda activate amphion\\n\" >> ~/.bashrc\n\nCMD [\"/bin/bash\"]\n\n# *** Build ***\n# docker build -t realamphion/amphion .\n\n# *** Run ***\n# cd Amphion\n# docker run --runtime=nvidia --gpus all -it -v .:/app -v /mnt:/mnt_host realamphion/amphion\n\n# *** Push and release ***\n# docker login\n# docker push realamphion/amphion\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0390625,
          "content": "MIT License\n\nCopyright (c) 2023 Amphion\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.62890625,
          "content": "# Amphion: An Open-Source Audio, Music, and Speech Generation Toolkit\n\n<div>\n    <a href=\"https://arxiv.org/abs/2312.09911\"><img src=\"https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg\"></a>\n    <a href=\"https://huggingface.co/amphion\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink\"></a>\n    <a href=\"https://modelscope.cn/organization/amphion\"><img src=\"https://img.shields.io/badge/ModelScope-Amphion-cyan\"></a>\n    <a href=\"https://openxlab.org.cn/usercenter/Amphion\"><img src=\"https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg\"></a>\n    <a href=\"https://discord.com/invite/drhW7ajqAG\"><img src=\"https://img.shields.io/badge/Discord-Join%20chat-blue.svg\"></a>\n    <a href=\"egs/tts/README.md\"><img src=\"https://img.shields.io/badge/README-TTS-blue\"></a>\n    <a href=\"models/vc/vevo/README.md\"><img src=\"https://img.shields.io/badge/README-VC-blue\"></a>\n    <a href=\"models/vc/vevo/README.md\"><img src=\"https://img.shields.io/badge/README-AC-blue\"></a>\n    <a href=\"egs/svc/README.md\"><img src=\"https://img.shields.io/badge/README-SVC-blue\"></a>\n    <a href=\"egs/tta/README.md\"><img src=\"https://img.shields.io/badge/README-TTA-blue\"></a>\n    <a href=\"egs/vocoder/README.md\"><img src=\"https://img.shields.io/badge/README-Vocoder-purple\"></a>\n    <a href=\"egs/metrics/README.md\"><img src=\"https://img.shields.io/badge/README-Evaluation-yellow\"></a>\n    <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/LICENSE-MIT-red\"></a>\n    <a href=\"https://trendshift.io/repositories/5469\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/5469\" alt=\"open-mmlab%2FAmphion | Trendshift\" style=\"width: 150px; height: 33px;\" width=\"150\" height=\"33\"/></a>\n</div>\n<br>\n\n**Amphion (/√¶mÀàfa…™…ôn/) is a toolkit for Audio, Music, and Speech Generation.** Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development. Amphion offers a unique feature: **visualizations** of classic models or architectures. We believe that these visualizations are beneficial for junior researchers and engineers who wish to gain a better understanding of the model.\n\n**The North-Star objective of Amphion is to offer a platform for studying the conversion of any inputs into audio.** Amphion is designed to support individual generation tasks, including but not limited to,\n\n- **TTS**: Text to Speech (‚õ≥¬†supported)\n- **SVS**: Singing Voice Synthesis (üë®‚Äçüíª¬†developing)\n- **VC**: Voice Conversion (‚õ≥¬†supported)\n- **AC**: Accent Conversion (‚õ≥¬†supported)\n- **SVC**: Singing Voice Conversion (‚õ≥¬†supported)\n- **TTA**: Text to Audio (‚õ≥¬†supported)\n- **TTM**: Text to Music (üë®‚Äçüíª¬†developing)\n- more‚Ä¶\n\nIn addition to the specific generation tasks, Amphion includes several **vocoders** and **evaluation metrics**. A vocoder is an important module for producing high-quality audio signals, while evaluation metrics are critical for ensuring consistent metrics in generation tasks. Moreover, Amphion is dedicated to advancing audio generation in real-world applications, such as building **large-scale datasets** for speech synthesis.\n\n## üöÄ¬†News\n- **2024/12/22**: We release the reproduction of **Vevo**, a zero-shot voice imitation framework with controllable timbre and style. Vevo can be applied into a series of speech generation tasks, including VC, TTS, AC, and more. The released pre-trained models are trained on [Emilia](https://huggingface.co/datasets/amphion/Emilia-Dataset) dataset and achieve SOTA zero-shot VC performance. [![arXiv](https://img.shields.io/badge/OpenReview-Paper-COLOR.svg)](https://openreview.net/pdf?id=anQDiQZhDP) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow)](https://huggingface.co/amphion/Vevo) [![WebPage](https://img.shields.io/badge/WebPage-Demo-red)](https://versavoice.github.io/) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](models/vc/vevo/README.md)\n- **2024/10/19**: We release **MaskGCT**, a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision. MaskGCT is trained on [Emilia](https://huggingface.co/datasets/amphion/Emilia-Dataset) dataset and achieves SOTA zero-shot TTS performance.  [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2409.00750) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow)](https://huggingface.co/amphion/maskgct) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-demo-pink)](https://huggingface.co/spaces/amphion/maskgct) [![ModelScope](https://img.shields.io/badge/ModelScope-space-purple)](https://modelscope.cn/studios/amphion/maskgct) [![ModelScope](https://img.shields.io/badge/ModelScope-model-cyan)](https://modelscope.cn/models/amphion/MaskGCT) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](models/tts/maskgct/README.md)\n- **2024/09/01**: [Amphion](https://arxiv.org/abs/2312.09911), [Emilia](https://arxiv.org/abs/2407.05361) and [DSFF-SVC](https://arxiv.org/abs/2310.11160) got accepted by IEEE SLT 2024! ü§ó\n- **2024/08/28**: Welcome to join Amphion's [Discord channel](https://discord.com/invite/drhW7ajqAG) to stay connected and engage with our communityÔºÅ\n- **2024/08/20**: [SingVisio](https://arxiv.org/abs/2402.12660) got accepted by Computers & Graphics, [available here](https://www.sciencedirect.com/science/article/pii/S0097849324001936)! üéâ\n- **2024/08/27**: *The Emilia dataset is now publicly available!* Discover the most extensive and diverse speech generation dataset with 101k hours of in-the-wild speech data now at [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/amphion/Emilia-Dataset) or [![OpenDataLab](https://img.shields.io/badge/OpenDataLab-Dataset-blue)](https://opendatalab.com/Amphion/Emilia)! üëëüëëüëë\n- **2024/07/01**: Amphion now releases **Emilia**, the first open-source multilingual in-the-wild dataset for speech generation with over 101k hours of speech data, and the **Emilia-Pipe**, the first open-source preprocessing pipeline designed to transform in-the-wild speech data into high-quality training data with annotations for speech generation! [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2407.05361) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/amphion/Emilia) [![demo](https://img.shields.io/badge/WebPage-Demo-red)](https://emilia-dataset.github.io/Emilia-Demo-Page/) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](preprocessors/Emilia/README.md)\n- **2024/06/17**: Amphion has a new release for its **VALL-E** model! It uses Llama as its underlying architecture and has better model performance, faster training speed, and more readable codes compared to our first version. [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](egs/tts/VALLE_V2/README.md)\n- **2024/03/12**: Amphion now support **NaturalSpeech3 FACodec** and release pretrained checkpoints. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2403.03100) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow)](https://huggingface.co/amphion/naturalspeech3_facodec) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-demo-pink)](https://huggingface.co/spaces/amphion/naturalspeech3_facodec) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](models/codec/ns3_codec/README.md)\n- **2024/02/22**: The first Amphion visualization tool, **SingVisio**, release. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2402.12660) [![openxlab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/Amphion/SingVisio) [![Video](https://img.shields.io/badge/Video-Demo-orange)](https://drive.google.com/file/d/15097SGhQh-SwUNbdWDYNyWEP--YGLba5/view) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](egs/visualization/SingVisio/README.md)\n- **2023/12/18**: Amphion v0.1 release. [![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2312.09911) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink)](https://huggingface.co/amphion) [![youtube](https://img.shields.io/badge/YouTube-Demo-red)](https://www.youtube.com/watch?v=1aw0HhcggvQ) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](https://github.com/open-mmlab/Amphion/pull/39)\n- **2023/11/28**: Amphion alpha release. [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](https://github.com/open-mmlab/Amphion/pull/2)\n\n## ‚≠ê¬†Key Features\n\n### TTS: Text to Speech\n\n- Amphion achieves state-of-the-art performance compared to existing open-source repositories on text-to-speech (TTS) systems. It supports the following models or architectures:\n    - [FastSpeech2](https://arxiv.org/abs/2006.04558): A non-autoregressive TTS architecture that utilizes feed-forward Transformer blocks. [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/FastSpeech2/README.md)\n    - [VITS](https://arxiv.org/abs/2106.06103): An end-to-end TTS architecture that utilizes conditional variational autoencoder with adversarial learning [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/VITS/README.md)\n    - [VALL-E](https://arxiv.org/abs/2301.02111): A zero-shot TTS architecture that uses a neural codec language model with discrete codes. [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/VALLE_V2/README.md)\n    - [NaturalSpeech2](https://arxiv.org/abs/2304.09116): An architecture for TTS that utilizes a latent diffusion model to generate natural-sounding voices. [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/NaturalSpeech2/README.md)\n    - [Jets](Jets): An end-to-end TTS model that jointly trains FastSpeech2 and HiFi-GAN with an alignment module. [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/Jets/README.md)\n    - [MaskGCT](https://arxiv.org/abs/2409.00750): A fully non-autoregressive TTS architecture that eliminates the need for explicit alignment information between text and speech supervision. [![code](https://img.shields.io/badge/README-Code-blue)](models/tts/maskgct/README.md)\n    - [Vevo-TTS](https://openreview.net/pdf?id=anQDiQZhDP): A zero-shot TTS architecture with controllable timbre and style. It consists of an autoregressive transformer and a flow-matching transformer. [![code](https://img.shields.io/badge/README-Code-blue)](models/vc/vevo/README.md)\n\n### VC: Voice Conversion\n\nAmphion supports the following voice conversion models:\n\n- [Vevo](https://openreview.net/pdf?id=anQDiQZhDP): A zero-shot voice imitation framework with controllable timbre and style. **Vevo-Timbre** conducts the style-preserved voice conversion, and **Vevo-Voice** conducts the style-converted voice conversion. [![code](https://img.shields.io/badge/README-Code-blue)](models/vc/vevo/README.md)\n- [FACodec](https://arxiv.org/abs/2403.03100): FACodec decomposes speech into subspaces representing different attributes like content, prosody, and timbre. It can achieve zero-shot voice conversion. [![code](https://img.shields.io/badge/README-Code-blue)](https://huggingface.co/amphion/naturalspeech3_facodec)\n- [Noro](https://arxiv.org/abs/2411.19770): A **noise-robust** zero-shot voice conversion system. Noro introduces innovative components tailored for VC using noisy reference speeches, including a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss. [![code](https://img.shields.io/badge/README-Code-blue)](egs/vc/Noro/README.md)\n\n### AC: Accent Conversion\n\n- Amphion supports AC with [Vevo-Style](models/vc/vevo/README.md). Particularly, it can conduct the accent conversion in a zero-shot manner. [![code](https://img.shields.io/badge/README-Code-blue)](models/vc/vevo/README.md)\n\n### SVC: Singing Voice Conversion\n\n- Ampion supports multiple content-based features from various pretrained models, including [WeNet](https://github.com/wenet-e2e/wenet), [Whisper](https://github.com/openai/whisper), and [ContentVec](https://github.com/auspicious3000/contentvec). Their specific roles in SVC has been investigated in our SLT 2024 paper. [![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2310.11160) [![code](https://img.shields.io/badge/README-Code-blue)](egs/svc/MultipleContentsSVC)\n- Amphion implements several state-of-the-art model architectures, including diffusion-, transformer-, VAE- and flow-based models. The diffusion-based architecture uses [Bidirectional dilated CNN](https://openreview.net/pdf?id=a-xFK8Ymz5J) as a backend and supports several sampling algorithms such as [DDPM](https://arxiv.org/pdf/2006.11239.pdf), [DDIM](https://arxiv.org/pdf/2010.02502.pdf), and [PNDM](https://arxiv.org/pdf/2202.09778.pdf). Additionally, it supports single-step inference based on the [Consistency Model](https://openreview.net/pdf?id=FmqFfMTNnv). [![code](https://img.shields.io/badge/README-Code-blue)](egs/svc/DiffComoSVC/README.md)\n\n### TTA: Text to Audio\n\n- Amphion supports the TTA with a latent diffusion model. It is designed like [AudioLDM](https://arxiv.org/abs/2301.12503), [Make-an-Audio](https://arxiv.org/abs/2301.12661), and [AUDIT](https://arxiv.org/abs/2304.00830). It is also the official implementation of the text-to-audio generation part of our NeurIPS 2023 paper. [![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2304.00830) [![code](https://img.shields.io/badge/README-Code-blue)](egs/tta/RECIPE.md)\n\n### Vocoder\n\n- Amphion supports various widely-used neural vocoders, including:\n    - GAN-based vocoders: [MelGAN](https://arxiv.org/abs/1910.06711), [HiFi-GAN](https://arxiv.org/abs/2010.05646), [NSF-HiFiGAN](https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts), [BigVGAN](https://arxiv.org/abs/2206.04658), [APNet](https://arxiv.org/abs/2305.07952).\n    - Flow-based vocoders: [WaveGlow](https://arxiv.org/abs/1811.00002).\n    - Diffusion-based vocoders: [Diffwave](https://arxiv.org/abs/2009.09761).\n    - Auto-regressive based vocoders: [WaveNet](https://arxiv.org/abs/1609.03499), [WaveRNN](https://arxiv.org/abs/1802.08435v1).\n- Amphion provides the official implementation of [Multi-Scale Constant-Q Transform Discriminator](https://arxiv.org/abs/2311.14957) (our ICASSP 2024 paper). It can be used to enhance any architecture GAN-based vocoders during training, and keep the inference stage (such as memory or speed) unchanged. [![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2311.14957) [![code](https://img.shields.io/badge/README-Code-blue)](egs/vocoder/gan/tfr_enhanced_hifigan)\n\n### Evaluation\n\nAmphion provides a comprehensive objective evaluation of the generated audio. [![code](https://img.shields.io/badge/README-Code-blue)](egs/metrics/README.md) \n\nThe supported evaluation metrics contain:\n\n- **F0 Modeling**: F0 Pearson Coefficients, F0 Periodicity Root Mean Square Error, F0 Root Mean Square Error, Voiced/Unvoiced F1 Score, etc.\n- **Energy Modeling**: Energy Root Mean Square Error, Energy Pearson Coefficients, etc.\n- **Intelligibility**: Character/Word Error Rate, which can be calculated based on [Whisper](https://github.com/openai/whisper) and more.\n- **Spectrogram Distortion**: Frechet Audio Distance (FAD), Mel Cepstral Distortion (MCD), Multi-Resolution STFT Distance (MSTFT), Perceptual Evaluation of Speech Quality (PESQ), Short Time Objective Intelligibility (STOI), etc.\n- **Speaker Similarity**: Cosine similarity, which can be calculated based on [RawNet3](https://github.com/Jungjee/RawNet), [Resemblyzer](https://github.com/resemble-ai/Resemblyzer), [WeSpeaker](https://github.com/wenet-e2e/wespeaker), [WavLM](https://github.com/microsoft/unilm/tree/master/wavlm) and more.\n\n### Datasets\n\n- Amphion unifies the data preprocess of the open-source datasets including [AudioCaps](https://audiocaps.github.io/), [LibriTTS](https://www.openslr.org/60/), [LJSpeech](https://keithito.com/LJ-Speech-Dataset/), [M4Singer](https://github.com/M4Singer/M4Singer), [Opencpop](https://wenet.org.cn/opencpop/), [OpenSinger](https://github.com/Multi-Singer/Multi-Singer.github.io), [SVCC](http://vc-challenge.org/), [VCTK](https://datashare.ed.ac.uk/handle/10283/3443), and more. The supported dataset list can be seen [here](egs/datasets/README.md) (updating). \n- Amphion (exclusively) supports the [**Emilia**](preprocessors/Emilia/README.md) dataset and its preprocessing pipeline **Emilia-Pipe** for in-the-wild speech data!\n\n### Visualization\n\nAmphion provides visualization tools to interactively illustrate the internal processing mechanism of classic models. This provides an invaluable resource for educational purposes and for facilitating understandable research.\n\nCurrently, Amphion supports [SingVisio](egs/visualization/SingVisio/README.md), a visualization tool of the diffusion model for singing voice conversion. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2402.12660) [![openxlab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/Amphion/SingVisio) [![Video](https://img.shields.io/badge/Video-Demo-orange)](https://drive.google.com/file/d/15097SGhQh-SwUNbdWDYNyWEP--YGLba5/view)\n\n\n## üìÄ Installation\n\nAmphion can be installed through either Setup Installer or Docker Image.\n\n### Setup Installer\n\n```bash\ngit clone https://github.com/open-mmlab/Amphion.git\ncd Amphion\n\n# Install Python Environment\nconda create --name amphion python=3.9.15\nconda activate amphion\n\n# Install Python Packages Dependencies\nsh env.sh\n```\n\n### Docker Image\n\n1. Install [Docker](https://docs.docker.com/get-docker/), [NVIDIA Driver](https://www.nvidia.com/download/index.aspx), [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html), and [CUDA](https://developer.nvidia.com/cuda-downloads).\n\n2. Run the following commands:\n```bash\ngit clone https://github.com/open-mmlab/Amphion.git\ncd Amphion\n\ndocker pull realamphion/amphion\ndocker run --runtime=nvidia --gpus all -it -v .:/app realamphion/amphion\n```\nMount dataset by argument `-v` is necessary when using Docker. Please refer to [Mount dataset in Docker container](egs/datasets/docker.md) and [Docker Docs](https://docs.docker.com/engine/reference/commandline/container_run/#volume) for more details.\n\n\n## üêç Usage in Python\n\nWe detail the instructions of different tasks in the following recipes:\n\n- [Text to Speech (TTS)](egs/tts/README.md)\n- [Voice Conversion (VC)](models/vc/vevo/README.md)\n- [Accent Conversion (AC)](models/vc/vevo/README.md)\n- [Singing Voice Conversion (SVC)](egs/svc/README.md)\n- [Text to Audio (TTA)](egs/tta/README.md)\n- [Vocoder](egs/vocoder/README.md)\n- [Evaluation](egs/metrics/README.md)\n- [Visualization](egs/visualization/README.md)\n\n## üë®‚Äçüíª Contributing\nWe appreciate all contributions to improve Amphion. Please refer to [CONTRIBUTING.md](.github/CONTRIBUTING.md) for the contributing guideline.\n\n## üôè¬†Acknowledgement\n\n\n- [ming024's FastSpeech2](https://github.com/ming024/FastSpeech2) and [jaywalnut310's VITS](https://github.com/jaywalnut310/vits) for model architecture code.\n- [lifeiteng's VALL-E](https://github.com/lifeiteng/vall-e) for training pipeline and model architecture design.\n- [SpeechTokenizer](https://github.com/ZhangXInFD/SpeechTokenizer) for semantic-distilled tokenizer design.\n- [WeNet](https://github.com/wenet-e2e/wenet), [Whisper](https://github.com/openai/whisper), [ContentVec](https://github.com/auspicious3000/contentvec), and [RawNet3](https://github.com/Jungjee/RawNet) for pretrained models and inference code.\n- [HiFi-GAN](https://github.com/jik876/hifi-gan) for GAN-based Vocoder's architecture design and training strategy.\n- [Encodec](https://github.com/facebookresearch/encodec) for well-organized GAN Discriminator's architecture and basic blocks.\n- [Latent Diffusion](https://github.com/CompVis/latent-diffusion) for model architecture design.\n- [TensorFlowTTS](https://github.com/TensorSpeech/TensorFlowTTS) for preparing the MFA tools.\n\n\n## ¬©Ô∏è¬†License\n\nAmphion is under the [MIT License](LICENSE). It is free for both research and commercial use cases.\n\n## üìö Citations\n\n```bibtex\n@inproceedings{amphion,\n    author={Zhang, Xueyao and Xue, Liumeng and Gu, Yicheng and Wang, Yuancheng and Li, Jiaqi and He, Haorui and Wang, Chaoren and Song, Ting and Chen, Xi and Fang, Zihao and Chen, Haopeng and Zhang, Junan and Tang, Tze Ying and Zou, Lexiao and Wang, Mingxuan and Han, Jun and Chen, Kai and Li, Haizhou and Wu, Zhizheng},\n    title={Amphion: An Open-Source Audio, Music and Speech Generation Toolkit},\n    booktitle={{IEEE} Spoken Language Technology Workshop, {SLT} 2024},\n    year={2024}\n}\n```\n"
        },
        {
          "name": "bins",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "egs",
          "type": "tree",
          "content": null
        },
        {
          "name": "env.sh",
          "type": "blob",
          "size": 1.2763671875,
          "content": "# Copyright (c) 2023 Amphion.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Raise error if any command fails\nset -e\n\n# Install ffmpeg in Linux\nconda install -c conda-forge ffmpeg\n\n# Pip packages\npip install setuptools ruamel.yaml tqdm colorama easydict tabulate loguru json5 Cython unidecode inflect argparse g2p_en tgt librosa==0.9.1 matplotlib typeguard einops omegaconf hydra-core humanfriendly pandas munch\n\npip install tensorboard tensorboardX torch==2.0.1 torchaudio==2.0.2 torchvision==0.15.2 accelerate==0.24.1 transformers==4.41.2 diffusers praat-parselmouth audiomentations pedalboard ffmpeg-python==0.2.0 pyworld diffsptk==1.0.1 nnAudio unidecode inflect ptwt\n\npip install encodec vocos speechtokenizer g2p_en descript-audio-codec\n\npip install torchmetrics pymcd openai-whisper frechet_audio_distance asteroid resemblyzer vector-quantize-pytorch==1.12.5\n\npip install https://github.com/vBaiCai/python-pesq/archive/master.zip\n\npip install fairseq\n\npip install git+https://github.com/lhotse-speech/lhotse\n\npip install -U encodec\n\npip install phonemizer==3.2.1 pypinyin==0.48.0\n\npip install black==24.1.1\n\n# Uninstall nvidia-cublas-cu11 if there exist some bugs about CUDA version\n# pip uninstall nvidia-cublas-cu11\n"
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "optimizer",
          "type": "tree",
          "content": null
        },
        {
          "name": "preprocessors",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretrained",
          "type": "tree",
          "content": null
        },
        {
          "name": "processors",
          "type": "tree",
          "content": null
        },
        {
          "name": "schedulers",
          "type": "tree",
          "content": null
        },
        {
          "name": "text",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "visualization",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}