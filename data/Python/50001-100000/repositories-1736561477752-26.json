{
  "metadata": {
    "timestamp": 1736561477752,
    "page": 26,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "labmlai/annotated_deep_learning_paper_implementations",
      "stars": 57795,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1953125,
          "content": ".ipynb_checkpoints\n__pycache__\n.DS_Store\n.*.swp\n*.egg-info/\ndist/\nbuild/\n.idea/*\n!.idea/dictionaries\nlabml\nlabml_helpers\nlabml_samples\ndata\nlogs\nhtml/\ndiagrams/\n.comet.config\nsettings.md\nlabml_app.log"
        },
        {
          "name": ".labml.yaml",
          "type": "blob",
          "size": 0.419921875,
          "content": "indicators:\n  - class_name: Scalar\n    is_print: false\n    name: param.*\n    options:\n      comet: false\n  - class_name: Scalar\n    is_print: false\n    name: grad.*\n    options:\n      comet: false\n  - class_name: Scalar\n    is_print: false\n    name: module.*\n    options:\n      comet: false\n  - class_name: Scalar\n    is_print: false\n    name: optim.*\n    options:\n      comet: false\n#web_api: http://localhost:5005/api/v1/track?\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0185546875,
          "content": "include readme.rst\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.9375,
          "content": "clean: ## Clean\n\trm -rf dist\n\trm -rf build\n\trm -rf *.egg-info\n\nbuild: clean ## Build PIPy Package\n\tpython setup.py sdist bdist_wheel\n\ncheck-content: build  ## List contents of PIPy Package\n\ttar -tvf dist/*.tar.gz\n\ncheck: build  ## Check PIPy Package\n\ttwine check dist/*\n\nupload: build  ## Upload PIPy Package\n\ttwine upload dist/*\n\ninstall:  ## Install from repo\n\tpip install -e .\n\nuninstall: ## Uninstall\n\tpip uninstall labml_nn\n\ndocs-si: ## Sinhalese Translation\n\trm -rf docs/si\n\tmv docs/zh docs_zh\n\tmv docs/ja docs_ja\n\tcp -r docs docs_si\n\tmv docs_si docs/si\n\tmv docs_zh docs/zh\n\tmv docs_ja docs/ja\n\tcd labml_nn; pylit --translate si --translate_cache ../translate_cache --remove_empty_sections --title_md -t ../../../pylit/templates/nn -d ../docs/si -w *\n\ndocs-zh: ## Chinese Translation\n\trm -rf docs/zh\n\tmv docs/si docs_si\n\tmv docs/ja docs_ja\n\tcp -r docs docs_zh\n\tmv docs_si docs/si\n\tmv docs_zh docs/zh\n\tmv docs_ja docs/ja\n\tcd labml_nn; pylit --translate zh --translate_cache ../translate_cache --remove_empty_sections --title_md -t ../../../pylit/templates/nn -d ../docs/zh -w *\n\ndocs-ja: ## Japanese Translation\n\trm -rf docs/ja\n\tmv docs/si docs_si\n\tmv docs/zh docs_zh\n\tcp -r docs docs_ja\n\tmv docs_si docs/si\n\tmv docs_zh docs/zh\n\tmv docs_ja docs/ja\n\tcd labml_nn; pylit --translate ja --translate_cache ../translate_cache --remove_empty_sections --title_md -t ../../../pylit/templates/nn -d ../docs/ja -w *\n\ndocs: ## Render annotated HTML\n\tmv docs/zh docs_zh\n\tmv docs/si docs_si\n\tmv docs/ja docs_ja\n\tfind ./docs/ -name \"*.html\" -type f -delete\n\tfind ./docs/ -name \"*.svg\" -type f -delete\n\tmv docs_si docs/si\n\tmv docs_zh docs/zh\n\tmv docs_ja docs/ja\n\tpython utils/sitemap.py\n\tpython utils/diagrams.py\n\tcd labml_nn; pylit --remove_empty_sections --title_md -t ../../../pylit/templates/nn -d ../docs -w *\n\nhelp: ## Show this help.\n\t@fgrep -h \"##\" $(MAKEFILE_LIST) | fgrep -v fgrep | sed -e 's/\\\\$$//' | sed -e 's/##//'\n\n.PHONY: clean build check upload help docs\n.DEFAULT_GOAL := help\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "labml_nn",
          "type": "tree",
          "content": null
        },
        {
          "name": "license",
          "type": "blob",
          "size": 1.056640625,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2020 Varuna Jayasiri\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "papers",
          "type": "tree",
          "content": null
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 7.607421875,
          "content": "[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai)\n[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/labmlai)\n\n# [labml.ai Deep Learning Paper Implementations](https://nn.labml.ai/index.html)\n\nThis is a collection of simple PyTorch implementations of\nneural networks and related algorithms.\nThese implementations are documented with explanations,\n\n[The website](https://nn.labml.ai/index.html)\nrenders these as side-by-side formatted notes.\nWe believe these would help you understand these algorithms better.\n\n![Screenshot](https://nn.labml.ai/dqn-light.png)\n\nWe are actively maintaining this repo and adding new \nimplementations almost weekly.\n[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.\n\n## Paper Implementations\n\n#### ✨ [Transformers](https://nn.labml.ai/transformers/index.html)\n\n* [Multi-headed attention](https://nn.labml.ai/transformers/mha.html)\n* [Transformer building blocks](https://nn.labml.ai/transformers/models.html) \n* [Transformer XL](https://nn.labml.ai/transformers/xl/index.html)\n    * [Relative multi-headed attention](https://nn.labml.ai/transformers/xl/relative_mha.html)\n* [Rotary Positional Embeddings](https://nn.labml.ai/transformers/rope/index.html)\n* [Attention with Linear Biases (ALiBi)](https://nn.labml.ai/transformers/alibi/index.html)\n* [RETRO](https://nn.labml.ai/transformers/retro/index.html)\n* [Compressive Transformer](https://nn.labml.ai/transformers/compressive/index.html)\n* [GPT Architecture](https://nn.labml.ai/transformers/gpt/index.html)\n* [GLU Variants](https://nn.labml.ai/transformers/glu_variants/simple.html)\n* [kNN-LM: Generalization through Memorization](https://nn.labml.ai/transformers/knn)\n* [Feedback Transformer](https://nn.labml.ai/transformers/feedback/index.html)\n* [Switch Transformer](https://nn.labml.ai/transformers/switch/index.html)\n* [Fast Weights Transformer](https://nn.labml.ai/transformers/fast_weights/index.html)\n* [FNet](https://nn.labml.ai/transformers/fnet/index.html)\n* [Attention Free Transformer](https://nn.labml.ai/transformers/aft/index.html)\n* [Masked Language Model](https://nn.labml.ai/transformers/mlm/index.html)\n* [MLP-Mixer: An all-MLP Architecture for Vision](https://nn.labml.ai/transformers/mlp_mixer/index.html)\n* [Pay Attention to MLPs (gMLP)](https://nn.labml.ai/transformers/gmlp/index.html)\n* [Vision Transformer (ViT)](https://nn.labml.ai/transformers/vit/index.html)\n* [Primer EZ](https://nn.labml.ai/transformers/primer_ez/index.html)\n* [Hourglass](https://nn.labml.ai/transformers/hour_glass/index.html)\n\n#### ✨ [Low-Rank Adaptation (LoRA)](https://nn.labml.ai/lora/index.html)\n\n#### ✨ [Eleuther GPT-NeoX](https://nn.labml.ai/neox/index.html)\n* [Generate on a 48GB GPU](https://nn.labml.ai/neox/samples/generate.html)\n* [Finetune on two 48GB GPUs](https://nn.labml.ai/neox/samples/finetune.html)\n* [LLM.int8()](https://nn.labml.ai/neox/utils/llm_int8.html)\n\n#### ✨ [Diffusion models](https://nn.labml.ai/diffusion/index.html)\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](https://nn.labml.ai/diffusion/ddpm/index.html)\n* [Denoising Diffusion Implicit Models (DDIM)](https://nn.labml.ai/diffusion/stable_diffusion/sampler/ddim.html)\n* [Latent Diffusion Models](https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html)\n* [Stable Diffusion](https://nn.labml.ai/diffusion/stable_diffusion/index.html)\n\n#### ✨ [Generative Adversarial Networks](https://nn.labml.ai/gan/index.html)\n* [Original GAN](https://nn.labml.ai/gan/original/index.html)\n* [GAN with deep convolutional network](https://nn.labml.ai/gan/dcgan/index.html)\n* [Cycle GAN](https://nn.labml.ai/gan/cycle_gan/index.html)\n* [Wasserstein GAN](https://nn.labml.ai/gan/wasserstein/index.html)\n* [Wasserstein GAN with Gradient Penalty](https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html)\n* [StyleGAN 2](https://nn.labml.ai/gan/stylegan/index.html)\n\n#### ✨ [Recurrent Highway Networks](https://nn.labml.ai/recurrent_highway_networks/index.html)\n\n#### ✨ [LSTM](https://nn.labml.ai/lstm/index.html)\n\n#### ✨ [HyperNetworks - HyperLSTM](https://nn.labml.ai/hypernetworks/hyper_lstm.html)\n\n#### ✨ [ResNet](https://nn.labml.ai/resnet/index.html)\n\n#### ✨ [ConvMixer](https://nn.labml.ai/conv_mixer/index.html)\n\n#### ✨ [Capsule Networks](https://nn.labml.ai/capsule_networks/index.html)\n\n#### ✨ [U-Net](https://nn.labml.ai/unet/index.html)\n\n#### ✨ [Sketch RNN](https://nn.labml.ai/sketch_rnn/index.html)\n\n#### ✨ Graph Neural Networks\n\n* [Graph Attention Networks (GAT)](https://nn.labml.ai/graphs/gat/index.html)\n* [Graph Attention Networks v2 (GATv2)](https://nn.labml.ai/graphs/gatv2/index.html)\n\n#### ✨ [Counterfactual Regret Minimization (CFR)](https://nn.labml.ai/cfr/index.html)\n\nSolving games with incomplete information such as poker with CFR.\n\n* [Kuhn Poker](https://nn.labml.ai/cfr/kuhn/index.html)\n\n#### ✨ [Reinforcement Learning](https://nn.labml.ai/rl/index.html)\n* [Proximal Policy Optimization](https://nn.labml.ai/rl/ppo/index.html) with\n [Generalized Advantage Estimation](https://nn.labml.ai/rl/ppo/gae.html)\n* [Deep Q Networks](https://nn.labml.ai/rl/dqn/index.html) with\n with [Dueling Network](https://nn.labml.ai/rl/dqn/model.html),\n [Prioritized Replay](https://nn.labml.ai/rl/dqn/replay_buffer.html)\n and Double Q Network.\n\n#### ✨ [Optimizers](https://nn.labml.ai/optimizers/index.html)\n* [Adam](https://nn.labml.ai/optimizers/adam.html)\n* [AMSGrad](https://nn.labml.ai/optimizers/amsgrad.html)\n* [Adam Optimizer with warmup](https://nn.labml.ai/optimizers/adam_warmup.html)\n* [Noam Optimizer](https://nn.labml.ai/optimizers/noam.html)\n* [Rectified Adam Optimizer](https://nn.labml.ai/optimizers/radam.html)\n* [AdaBelief Optimizer](https://nn.labml.ai/optimizers/ada_belief.html)\n* [Sophia-G Optimizer](https://nn.labml.ai/optimizers/sophia.html)\n\n#### ✨ [Normalization Layers](https://nn.labml.ai/normalization/index.html)\n* [Batch Normalization](https://nn.labml.ai/normalization/batch_norm/index.html)\n* [Layer Normalization](https://nn.labml.ai/normalization/layer_norm/index.html)\n* [Instance Normalization](https://nn.labml.ai/normalization/instance_norm/index.html)\n* [Group Normalization](https://nn.labml.ai/normalization/group_norm/index.html)\n* [Weight Standardization](https://nn.labml.ai/normalization/weight_standardization/index.html)\n* [Batch-Channel Normalization](https://nn.labml.ai/normalization/batch_channel_norm/index.html)\n* [DeepNorm](https://nn.labml.ai/normalization/deep_norm/index.html)\n\n#### ✨ [Distillation](https://nn.labml.ai/distillation/index.html)\n\n#### ✨ [Adaptive Computation](https://nn.labml.ai/adaptive_computation/index.html)\n\n* [PonderNet](https://nn.labml.ai/adaptive_computation/ponder_net/index.html)\n\n#### ✨ [Uncertainty](https://nn.labml.ai/uncertainty/index.html)\n\n* [Evidential Deep Learning to Quantify Classification Uncertainty](https://nn.labml.ai/uncertainty/evidence/index.html)\n\n#### ✨ [Activations](https://nn.labml.ai/activations/index.html)\n\n* [Fuzzy Tiling Activations](https://nn.labml.ai/activations/fta/index.html)\n\n#### ✨ [Langauge Model Sampling Techniques](https://nn.labml.ai/sampling/index.html)\n* [Greedy Sampling](https://nn.labml.ai/sampling/greedy.html)\n* [Temperature Sampling](https://nn.labml.ai/sampling/temperature.html)\n* [Top-k Sampling](https://nn.labml.ai/sampling/top_k.html)\n* [Nucleus Sampling](https://nn.labml.ai/sampling/nucleus.html)\n\n#### ✨ [Scalable Training/Inference](https://nn.labml.ai/scaling/index.html)\n* [Zero3 memory optimizations](https://nn.labml.ai/scaling/zero3/index.html)\n\n### Installation\n\n```bash\npip install labml-nn\n```\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.16796875,
          "content": "torch>=1.10\ntorchvision>=0.11\ntorchtext>=0.11\nlabml>=0.4.147\nlabml-helpers>=0.4.84\nnumpy>=1.19\nmatplotlib>=3.0.3\neinops>=0.3.0\ngym[atari]\nopencv-python\nPillow>=6.2.1\nfaiss\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.0224609375,
          "content": "import setuptools\n\nwith open(\"readme.md\", \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name='labml-nn',\n    version='0.4.137',\n    author=\"Varuna Jayasiri, Nipun Wijerathne\",\n    author_email=\"vpjayasiri@gmail.com, hnipun@gmail.com\",\n    description=\"🧑‍🏫 Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit), optimizers (adam, radam, adabelief), gans(dcgan, cyclegan, stylegan2), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, diffusion, etc. 🧠\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/labmlai/annotated_deep_learning_paper_implementations\",\n    project_urls={\n        'Documentation': 'https://nn.labml.ai'\n    },\n    packages=setuptools.find_packages(exclude=('labml', 'labml.*',\n                                               'labml_samples', 'labml_samples.*',\n                                               'labml_helpers', 'labml_helpers.*',\n                                               'test',\n                                               'test.*')),\n    install_requires=['labml==0.4.168',\n                      'labml-helpers==0.4.89',\n                      'torch',\n                      'torchtext',\n                      'torchvision',\n                      'einops',\n                      'numpy',\n                      'fairscale'],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'Topic :: Scientific/Engineering',\n        'Topic :: Scientific/Engineering :: Mathematics',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Software Development',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n    keywords='machine learning',\n)\n"
        },
        {
          "name": "translate_cache",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}