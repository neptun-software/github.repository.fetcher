{
  "metadata": {
    "timestamp": 1736561470540,
    "page": 14,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "binary-husky/gpt_academic",
      "stars": 66820,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.1533203125,
          "content": "*.h linguist-detectable=false\n*.cpp linguist-detectable=false\n*.tex linguist-detectable=false\n*.cs linguist-detectable=false\n*.tps linguist-detectable=false\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.349609375,
          "content": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\npip-wheel-metadata/\r\nshare/python-wheels/\r\n*.egg-info/\r\n.installed.cfg\r\n*.egg\r\nMANIFEST\r\n\r\n# PyInstaller\r\n#  Usually these files are written by a python script from a template\r\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\r\n*.manifest\r\n*.spec\r\n# Installer logs\r\npip-log.txt\r\npip-delete-this-directory.txt\r\n\r\n# Unit test / coverage reports\r\nhtmlcov/\r\n.tox/\r\n.nox/\r\n.coverage\r\n.coverage.*\r\n.cache\r\nnosetests.xml\r\ncoverage.xml\r\n*.cover\r\n*.py,cover\r\n.hypothesis/\r\n.pytest_cache/\r\n\r\n# Translations\r\n*.mo\r\n*.pot\r\ngithub\r\n.github\r\nTEMP\r\nTRASH\r\n\r\n# Django stuff:\r\n*.log\r\nlocal_settings.py\r\ndb.sqlite3\r\ndb.sqlite3-journal\r\n\r\n# Flask stuff:\r\ninstance/\r\n.webassets-cache\r\n\r\n# Scrapy stuff:\r\n.scrapy\r\n\r\n# Sphinx documentation\r\ndocs/_build/\r\n\r\n# PyBuilder\r\ntarget/\r\n\r\n# Jupyter Notebook\r\n.ipynb_checkpoints\r\n\r\n# IPython\r\nprofile_default/\r\nipython_config.py\r\n\r\n# pyenv\r\n.python-version\r\n\r\n# pipenv\r\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\r\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\r\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\r\n#   install all needed dependencies.\r\n#Pipfile.lock\r\n\r\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\r\n__pypackages__/\r\n\r\n# Celery stuff\r\ncelerybeat-schedule\r\ncelerybeat.pid\r\n\r\n# SageMath parsed files\r\n*.sage.py\r\n\r\n# Environments\r\n.env\r\n.venv\r\nenv/\r\nvenv/\r\nENV/\r\nenv.bak/\r\nvenv.bak/\r\n\r\n# Spyder project settings\r\n.spyderproject\r\n.spyproject\r\n\r\n# Rope project settings\r\n.ropeproject\r\n\r\n# mkdocs documentation\r\n/site\r\n\r\n# mypy\r\n.mypy_cache/\r\n.dmypy.json\r\ndmypy.json\r\n\r\n# Pyre type checker\r\n.pyre/\r\n\r\n# macOS files\r\n.DS_Store\r\n\r\n.vscode\r\n.idea\r\n\r\nhistory\r\nssr_conf\r\nconfig_private.py\r\ngpt_log\r\nprivate.md\r\nprivate_upload\r\nother_llms\r\ncradle*\r\ndebug*\r\nprivate*\r\ncrazy_functions/test_project/pdf_and_word\r\ncrazy_functions/test_samples\r\nrequest_llms/jittorllms\r\nmulti-language\r\nrequest_llms/moss\r\nmedia\r\nflagged\r\nrequest_llms/ChatGLM-6b-onnx-u8s8\r\n.pre-commit-config.yaml\r\ntest.*\r\ntemp.*\r\nobjdump*\r\n*.min.*.js\r\nTODO\r\nexperimental_mods\r\nsearch_results\r\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.6083984375,
          "content": "# 此Dockerfile适用于“无本地模型”的迷你运行环境构建\n# 如果需要使用chatglm等本地模型或者latex运行依赖，请参考 docker-compose.yml\n# - 如何构建: 先修改 `config.py`， 然后 `docker build -t gpt-academic . `\n# - 如何运行(Linux下): `docker run --rm -it --net=host gpt-academic `\n# - 如何运行(其他操作系统，选择任意一个固定端口50923): `docker run --rm -it -e WEB_PORT=50923 -p 50923:50923 gpt-academic `\nFROM python:3.11\n\n\n# 非必要步骤，更换pip源 （以下三行，可以删除）\nRUN echo '[global]' > /etc/pip.conf && \\\n    echo 'index-url = https://mirrors.aliyun.com/pypi/simple/' >> /etc/pip.conf && \\\n    echo 'trusted-host = mirrors.aliyun.com' >> /etc/pip.conf\n\n\n# 语音输出功能（以下两行，第一行更换阿里源，第二行安装ffmpeg，都可以删除）\nRUN UBUNTU_VERSION=$(awk -F= '/^VERSION_CODENAME=/{print $2}' /etc/os-release); echo \"deb https://mirrors.aliyun.com/debian/ $UBUNTU_VERSION main non-free contrib\" > /etc/apt/sources.list; apt-get update\nRUN apt-get install ffmpeg -y\nRUN apt-get clean\n\n\n# 进入工作路径（必要）\nWORKDIR /gpt\n\n\n# 安装大部分依赖，利用Docker缓存加速以后的构建 （以下两行，可以删除）\nCOPY requirements.txt ./\nRUN pip3 install -r requirements.txt\n\n\n# 装载项目文件，安装剩余依赖（必要）\nCOPY . .\nRUN pip3 install -r requirements.txt\n\n\n# 非必要步骤，用于预热模块（可以删除）\nRUN python3  -c 'from check_proxy import warm_up_modules; warm_up_modules()'\nRUN python3 -m pip cache purge\n\n\n# 启动（必要）\nCMD [\"python3\", \"-u\", \"main.py\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 34.9833984375,
          "content": "                    GNU GENERAL PUBLIC LICENSE\r\n                       Version 3, 29 June 2007\r\n\r\n Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\r\n Everyone is permitted to copy and distribute verbatim copies\r\n of this license document, but changing it is not allowed.\r\n\r\n                            Preamble\r\n\r\n  The GNU General Public License is a free, copyleft license for\r\nsoftware and other kinds of works.\r\n\r\n  The licenses for most software and other practical works are designed\r\nto take away your freedom to share and change the works.  By contrast,\r\nthe GNU General Public License is intended to guarantee your freedom to\r\nshare and change all versions of a program--to make sure it remains free\r\nsoftware for all its users.  We, the Free Software Foundation, use the\r\nGNU General Public License for most of our software; it applies also to\r\nany other work released this way by its authors.  You can apply it to\r\nyour programs, too.\r\n\r\n  When we speak of free software, we are referring to freedom, not\r\nprice.  Our General Public Licenses are designed to make sure that you\r\nhave the freedom to distribute copies of free software (and charge for\r\nthem if you wish), that you receive source code or can get it if you\r\nwant it, that you can change the software or use pieces of it in new\r\nfree programs, and that you know you can do these things.\r\n\r\n  To protect your rights, we need to prevent others from denying you\r\nthese rights or asking you to surrender the rights.  Therefore, you have\r\ncertain responsibilities if you distribute copies of the software, or if\r\nyou modify it: responsibilities to respect the freedom of others.\r\n\r\n  For example, if you distribute copies of such a program, whether\r\ngratis or for a fee, you must pass on to the recipients the same\r\nfreedoms that you received.  You must make sure that they, too, receive\r\nor can get the source code.  And you must show them these terms so they\r\nknow their rights.\r\n\r\n  Developers that use the GNU GPL protect your rights with two steps:\r\n(1) assert copyright on the software, and (2) offer you this License\r\ngiving you legal permission to copy, distribute and/or modify it.\r\n\r\n  For the developers' and authors' protection, the GPL clearly explains\r\nthat there is no warranty for this free software.  For both users' and\r\nauthors' sake, the GPL requires that modified versions be marked as\r\nchanged, so that their problems will not be attributed erroneously to\r\nauthors of previous versions.\r\n\r\n  Some devices are designed to deny users access to install or run\r\nmodified versions of the software inside them, although the manufacturer\r\ncan do so.  This is fundamentally incompatible with the aim of\r\nprotecting users' freedom to change the software.  The systematic\r\npattern of such abuse occurs in the area of products for individuals to\r\nuse, which is precisely where it is most unacceptable.  Therefore, we\r\nhave designed this version of the GPL to prohibit the practice for those\r\nproducts.  If such problems arise substantially in other domains, we\r\nstand ready to extend this provision to those domains in future versions\r\nof the GPL, as needed to protect the freedom of users.\r\n\r\n  Finally, every program is threatened constantly by software patents.\r\nStates should not allow patents to restrict development and use of\r\nsoftware on general-purpose computers, but in those that do, we wish to\r\navoid the special danger that patents applied to a free program could\r\nmake it effectively proprietary.  To prevent this, the GPL assures that\r\npatents cannot be used to render the program non-free.\r\n\r\n  The precise terms and conditions for copying, distribution and\r\nmodification follow.\r\n\r\n                       TERMS AND CONDITIONS\r\n\r\n  0. Definitions.\r\n\r\n  \"This License\" refers to version 3 of the GNU General Public License.\r\n\r\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\r\nworks, such as semiconductor masks.\r\n\r\n  \"The Program\" refers to any copyrightable work licensed under this\r\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\r\n\"recipients\" may be individuals or organizations.\r\n\r\n  To \"modify\" a work means to copy from or adapt all or part of the work\r\nin a fashion requiring copyright permission, other than the making of an\r\nexact copy.  The resulting work is called a \"modified version\" of the\r\nearlier work or a work \"based on\" the earlier work.\r\n\r\n  A \"covered work\" means either the unmodified Program or a work based\r\non the Program.\r\n\r\n  To \"propagate\" a work means to do anything with it that, without\r\npermission, would make you directly or secondarily liable for\r\ninfringement under applicable copyright law, except executing it on a\r\ncomputer or modifying a private copy.  Propagation includes copying,\r\ndistribution (with or without modification), making available to the\r\npublic, and in some countries other activities as well.\r\n\r\n  To \"convey\" a work means any kind of propagation that enables other\r\nparties to make or receive copies.  Mere interaction with a user through\r\na computer network, with no transfer of a copy, is not conveying.\r\n\r\n  An interactive user interface displays \"Appropriate Legal Notices\"\r\nto the extent that it includes a convenient and prominently visible\r\nfeature that (1) displays an appropriate copyright notice, and (2)\r\ntells the user that there is no warranty for the work (except to the\r\nextent that warranties are provided), that licensees may convey the\r\nwork under this License, and how to view a copy of this License.  If\r\nthe interface presents a list of user commands or options, such as a\r\nmenu, a prominent item in the list meets this criterion.\r\n\r\n  1. Source Code.\r\n\r\n  The \"source code\" for a work means the preferred form of the work\r\nfor making modifications to it.  \"Object code\" means any non-source\r\nform of a work.\r\n\r\n  A \"Standard Interface\" means an interface that either is an official\r\nstandard defined by a recognized standards body, or, in the case of\r\ninterfaces specified for a particular programming language, one that\r\nis widely used among developers working in that language.\r\n\r\n  The \"System Libraries\" of an executable work include anything, other\r\nthan the work as a whole, that (a) is included in the normal form of\r\npackaging a Major Component, but which is not part of that Major\r\nComponent, and (b) serves only to enable use of the work with that\r\nMajor Component, or to implement a Standard Interface for which an\r\nimplementation is available to the public in source code form.  A\r\n\"Major Component\", in this context, means a major essential component\r\n(kernel, window system, and so on) of the specific operating system\r\n(if any) on which the executable work runs, or a compiler used to\r\nproduce the work, or an object code interpreter used to run it.\r\n\r\n  The \"Corresponding Source\" for a work in object code form means all\r\nthe source code needed to generate, install, and (for an executable\r\nwork) run the object code and to modify the work, including scripts to\r\ncontrol those activities.  However, it does not include the work's\r\nSystem Libraries, or general-purpose tools or generally available free\r\nprograms which are used unmodified in performing those activities but\r\nwhich are not part of the work.  For example, Corresponding Source\r\nincludes interface definition files associated with source files for\r\nthe work, and the source code for shared libraries and dynamically\r\nlinked subprograms that the work is specifically designed to require,\r\nsuch as by intimate data communication or control flow between those\r\nsubprograms and other parts of the work.\r\n\r\n  The Corresponding Source need not include anything that users\r\ncan regenerate automatically from other parts of the Corresponding\r\nSource.\r\n\r\n  The Corresponding Source for a work in source code form is that\r\nsame work.\r\n\r\n  2. Basic Permissions.\r\n\r\n  All rights granted under this License are granted for the term of\r\ncopyright on the Program, and are irrevocable provided the stated\r\nconditions are met.  This License explicitly affirms your unlimited\r\npermission to run the unmodified Program.  The output from running a\r\ncovered work is covered by this License only if the output, given its\r\ncontent, constitutes a covered work.  This License acknowledges your\r\nrights of fair use or other equivalent, as provided by copyright law.\r\n\r\n  You may make, run and propagate covered works that you do not\r\nconvey, without conditions so long as your license otherwise remains\r\nin force.  You may convey covered works to others for the sole purpose\r\nof having them make modifications exclusively for you, or provide you\r\nwith facilities for running those works, provided that you comply with\r\nthe terms of this License in conveying all material for which you do\r\nnot control copyright.  Those thus making or running the covered works\r\nfor you must do so exclusively on your behalf, under your direction\r\nand control, on terms that prohibit them from making any copies of\r\nyour copyrighted material outside their relationship with you.\r\n\r\n  Conveying under any other circumstances is permitted solely under\r\nthe conditions stated below.  Sublicensing is not allowed; section 10\r\nmakes it unnecessary.\r\n\r\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\r\n\r\n  No covered work shall be deemed part of an effective technological\r\nmeasure under any applicable law fulfilling obligations under article\r\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\r\nsimilar laws prohibiting or restricting circumvention of such\r\nmeasures.\r\n\r\n  When you convey a covered work, you waive any legal power to forbid\r\ncircumvention of technological measures to the extent such circumvention\r\nis effected by exercising rights under this License with respect to\r\nthe covered work, and you disclaim any intention to limit operation or\r\nmodification of the work as a means of enforcing, against the work's\r\nusers, your or third parties' legal rights to forbid circumvention of\r\ntechnological measures.\r\n\r\n  4. Conveying Verbatim Copies.\r\n\r\n  You may convey verbatim copies of the Program's source code as you\r\nreceive it, in any medium, provided that you conspicuously and\r\nappropriately publish on each copy an appropriate copyright notice;\r\nkeep intact all notices stating that this License and any\r\nnon-permissive terms added in accord with section 7 apply to the code;\r\nkeep intact all notices of the absence of any warranty; and give all\r\nrecipients a copy of this License along with the Program.\r\n\r\n  You may charge any price or no price for each copy that you convey,\r\nand you may offer support or warranty protection for a fee.\r\n\r\n  5. Conveying Modified Source Versions.\r\n\r\n  You may convey a work based on the Program, or the modifications to\r\nproduce it from the Program, in the form of source code under the\r\nterms of section 4, provided that you also meet all of these conditions:\r\n\r\n    a) The work must carry prominent notices stating that you modified\r\n    it, and giving a relevant date.\r\n\r\n    b) The work must carry prominent notices stating that it is\r\n    released under this License and any conditions added under section\r\n    7.  This requirement modifies the requirement in section 4 to\r\n    \"keep intact all notices\".\r\n\r\n    c) You must license the entire work, as a whole, under this\r\n    License to anyone who comes into possession of a copy.  This\r\n    License will therefore apply, along with any applicable section 7\r\n    additional terms, to the whole of the work, and all its parts,\r\n    regardless of how they are packaged.  This License gives no\r\n    permission to license the work in any other way, but it does not\r\n    invalidate such permission if you have separately received it.\r\n\r\n    d) If the work has interactive user interfaces, each must display\r\n    Appropriate Legal Notices; however, if the Program has interactive\r\n    interfaces that do not display Appropriate Legal Notices, your\r\n    work need not make them do so.\r\n\r\n  A compilation of a covered work with other separate and independent\r\nworks, which are not by their nature extensions of the covered work,\r\nand which are not combined with it such as to form a larger program,\r\nin or on a volume of a storage or distribution medium, is called an\r\n\"aggregate\" if the compilation and its resulting copyright are not\r\nused to limit the access or legal rights of the compilation's users\r\nbeyond what the individual works permit.  Inclusion of a covered work\r\nin an aggregate does not cause this License to apply to the other\r\nparts of the aggregate.\r\n\r\n  6. Conveying Non-Source Forms.\r\n\r\n  You may convey a covered work in object code form under the terms\r\nof sections 4 and 5, provided that you also convey the\r\nmachine-readable Corresponding Source under the terms of this License,\r\nin one of these ways:\r\n\r\n    a) Convey the object code in, or embodied in, a physical product\r\n    (including a physical distribution medium), accompanied by the\r\n    Corresponding Source fixed on a durable physical medium\r\n    customarily used for software interchange.\r\n\r\n    b) Convey the object code in, or embodied in, a physical product\r\n    (including a physical distribution medium), accompanied by a\r\n    written offer, valid for at least three years and valid for as\r\n    long as you offer spare parts or customer support for that product\r\n    model, to give anyone who possesses the object code either (1) a\r\n    copy of the Corresponding Source for all the software in the\r\n    product that is covered by this License, on a durable physical\r\n    medium customarily used for software interchange, for a price no\r\n    more than your reasonable cost of physically performing this\r\n    conveying of source, or (2) access to copy the\r\n    Corresponding Source from a network server at no charge.\r\n\r\n    c) Convey individual copies of the object code with a copy of the\r\n    written offer to provide the Corresponding Source.  This\r\n    alternative is allowed only occasionally and noncommercially, and\r\n    only if you received the object code with such an offer, in accord\r\n    with subsection 6b.\r\n\r\n    d) Convey the object code by offering access from a designated\r\n    place (gratis or for a charge), and offer equivalent access to the\r\n    Corresponding Source in the same way through the same place at no\r\n    further charge.  You need not require recipients to copy the\r\n    Corresponding Source along with the object code.  If the place to\r\n    copy the object code is a network server, the Corresponding Source\r\n    may be on a different server (operated by you or a third party)\r\n    that supports equivalent copying facilities, provided you maintain\r\n    clear directions next to the object code saying where to find the\r\n    Corresponding Source.  Regardless of what server hosts the\r\n    Corresponding Source, you remain obligated to ensure that it is\r\n    available for as long as needed to satisfy these requirements.\r\n\r\n    e) Convey the object code using peer-to-peer transmission, provided\r\n    you inform other peers where the object code and Corresponding\r\n    Source of the work are being offered to the general public at no\r\n    charge under subsection 6d.\r\n\r\n  A separable portion of the object code, whose source code is excluded\r\nfrom the Corresponding Source as a System Library, need not be\r\nincluded in conveying the object code work.\r\n\r\n  A \"User Product\" is either (1) a \"consumer product\", which means any\r\ntangible personal property which is normally used for personal, family,\r\nor household purposes, or (2) anything designed or sold for incorporation\r\ninto a dwelling.  In determining whether a product is a consumer product,\r\ndoubtful cases shall be resolved in favor of coverage.  For a particular\r\nproduct received by a particular user, \"normally used\" refers to a\r\ntypical or common use of that class of product, regardless of the status\r\nof the particular user or of the way in which the particular user\r\nactually uses, or expects or is expected to use, the product.  A product\r\nis a consumer product regardless of whether the product has substantial\r\ncommercial, industrial or non-consumer uses, unless such uses represent\r\nthe only significant mode of use of the product.\r\n\r\n  \"Installation Information\" for a User Product means any methods,\r\nprocedures, authorization keys, or other information required to install\r\nand execute modified versions of a covered work in that User Product from\r\na modified version of its Corresponding Source.  The information must\r\nsuffice to ensure that the continued functioning of the modified object\r\ncode is in no case prevented or interfered with solely because\r\nmodification has been made.\r\n\r\n  If you convey an object code work under this section in, or with, or\r\nspecifically for use in, a User Product, and the conveying occurs as\r\npart of a transaction in which the right of possession and use of the\r\nUser Product is transferred to the recipient in perpetuity or for a\r\nfixed term (regardless of how the transaction is characterized), the\r\nCorresponding Source conveyed under this section must be accompanied\r\nby the Installation Information.  But this requirement does not apply\r\nif neither you nor any third party retains the ability to install\r\nmodified object code on the User Product (for example, the work has\r\nbeen installed in ROM).\r\n\r\n  The requirement to provide Installation Information does not include a\r\nrequirement to continue to provide support service, warranty, or updates\r\nfor a work that has been modified or installed by the recipient, or for\r\nthe User Product in which it has been modified or installed.  Access to a\r\nnetwork may be denied when the modification itself materially and\r\nadversely affects the operation of the network or violates the rules and\r\nprotocols for communication across the network.\r\n\r\n  Corresponding Source conveyed, and Installation Information provided,\r\nin accord with this section must be in a format that is publicly\r\ndocumented (and with an implementation available to the public in\r\nsource code form), and must require no special password or key for\r\nunpacking, reading or copying.\r\n\r\n  7. Additional Terms.\r\n\r\n  \"Additional permissions\" are terms that supplement the terms of this\r\nLicense by making exceptions from one or more of its conditions.\r\nAdditional permissions that are applicable to the entire Program shall\r\nbe treated as though they were included in this License, to the extent\r\nthat they are valid under applicable law.  If additional permissions\r\napply only to part of the Program, that part may be used separately\r\nunder those permissions, but the entire Program remains governed by\r\nthis License without regard to the additional permissions.\r\n\r\n  When you convey a copy of a covered work, you may at your option\r\nremove any additional permissions from that copy, or from any part of\r\nit.  (Additional permissions may be written to require their own\r\nremoval in certain cases when you modify the work.)  You may place\r\nadditional permissions on material, added by you to a covered work,\r\nfor which you have or can give appropriate copyright permission.\r\n\r\n  Notwithstanding any other provision of this License, for material you\r\nadd to a covered work, you may (if authorized by the copyright holders of\r\nthat material) supplement the terms of this License with terms:\r\n\r\n    a) Disclaiming warranty or limiting liability differently from the\r\n    terms of sections 15 and 16 of this License; or\r\n\r\n    b) Requiring preservation of specified reasonable legal notices or\r\n    author attributions in that material or in the Appropriate Legal\r\n    Notices displayed by works containing it; or\r\n\r\n    c) Prohibiting misrepresentation of the origin of that material, or\r\n    requiring that modified versions of such material be marked in\r\n    reasonable ways as different from the original version; or\r\n\r\n    d) Limiting the use for publicity purposes of names of licensors or\r\n    authors of the material; or\r\n\r\n    e) Declining to grant rights under trademark law for use of some\r\n    trade names, trademarks, or service marks; or\r\n\r\n    f) Requiring indemnification of licensors and authors of that\r\n    material by anyone who conveys the material (or modified versions of\r\n    it) with contractual assumptions of liability to the recipient, for\r\n    any liability that these contractual assumptions directly impose on\r\n    those licensors and authors.\r\n\r\n  All other non-permissive additional terms are considered \"further\r\nrestrictions\" within the meaning of section 10.  If the Program as you\r\nreceived it, or any part of it, contains a notice stating that it is\r\ngoverned by this License along with a term that is a further\r\nrestriction, you may remove that term.  If a license document contains\r\na further restriction but permits relicensing or conveying under this\r\nLicense, you may add to a covered work material governed by the terms\r\nof that license document, provided that the further restriction does\r\nnot survive such relicensing or conveying.\r\n\r\n  If you add terms to a covered work in accord with this section, you\r\nmust place, in the relevant source files, a statement of the\r\nadditional terms that apply to those files, or a notice indicating\r\nwhere to find the applicable terms.\r\n\r\n  Additional terms, permissive or non-permissive, may be stated in the\r\nform of a separately written license, or stated as exceptions;\r\nthe above requirements apply either way.\r\n\r\n  8. Termination.\r\n\r\n  You may not propagate or modify a covered work except as expressly\r\nprovided under this License.  Any attempt otherwise to propagate or\r\nmodify it is void, and will automatically terminate your rights under\r\nthis License (including any patent licenses granted under the third\r\nparagraph of section 11).\r\n\r\n  However, if you cease all violation of this License, then your\r\nlicense from a particular copyright holder is reinstated (a)\r\nprovisionally, unless and until the copyright holder explicitly and\r\nfinally terminates your license, and (b) permanently, if the copyright\r\nholder fails to notify you of the violation by some reasonable means\r\nprior to 60 days after the cessation.\r\n\r\n  Moreover, your license from a particular copyright holder is\r\nreinstated permanently if the copyright holder notifies you of the\r\nviolation by some reasonable means, this is the first time you have\r\nreceived notice of violation of this License (for any work) from that\r\ncopyright holder, and you cure the violation prior to 30 days after\r\nyour receipt of the notice.\r\n\r\n  Termination of your rights under this section does not terminate the\r\nlicenses of parties who have received copies or rights from you under\r\nthis License.  If your rights have been terminated and not permanently\r\nreinstated, you do not qualify to receive new licenses for the same\r\nmaterial under section 10.\r\n\r\n  9. Acceptance Not Required for Having Copies.\r\n\r\n  You are not required to accept this License in order to receive or\r\nrun a copy of the Program.  Ancillary propagation of a covered work\r\noccurring solely as a consequence of using peer-to-peer transmission\r\nto receive a copy likewise does not require acceptance.  However,\r\nnothing other than this License grants you permission to propagate or\r\nmodify any covered work.  These actions infringe copyright if you do\r\nnot accept this License.  Therefore, by modifying or propagating a\r\ncovered work, you indicate your acceptance of this License to do so.\r\n\r\n  10. Automatic Licensing of Downstream Recipients.\r\n\r\n  Each time you convey a covered work, the recipient automatically\r\nreceives a license from the original licensors, to run, modify and\r\npropagate that work, subject to this License.  You are not responsible\r\nfor enforcing compliance by third parties with this License.\r\n\r\n  An \"entity transaction\" is a transaction transferring control of an\r\norganization, or substantially all assets of one, or subdividing an\r\norganization, or merging organizations.  If propagation of a covered\r\nwork results from an entity transaction, each party to that\r\ntransaction who receives a copy of the work also receives whatever\r\nlicenses to the work the party's predecessor in interest had or could\r\ngive under the previous paragraph, plus a right to possession of the\r\nCorresponding Source of the work from the predecessor in interest, if\r\nthe predecessor has it or can get it with reasonable efforts.\r\n\r\n  You may not impose any further restrictions on the exercise of the\r\nrights granted or affirmed under this License.  For example, you may\r\nnot impose a license fee, royalty, or other charge for exercise of\r\nrights granted under this License, and you may not initiate litigation\r\n(including a cross-claim or counterclaim in a lawsuit) alleging that\r\nany patent claim is infringed by making, using, selling, offering for\r\nsale, or importing the Program or any portion of it.\r\n\r\n  11. Patents.\r\n\r\n  A \"contributor\" is a copyright holder who authorizes use under this\r\nLicense of the Program or a work on which the Program is based.  The\r\nwork thus licensed is called the contributor's \"contributor version\".\r\n\r\n  A contributor's \"essential patent claims\" are all patent claims\r\nowned or controlled by the contributor, whether already acquired or\r\nhereafter acquired, that would be infringed by some manner, permitted\r\nby this License, of making, using, or selling its contributor version,\r\nbut do not include claims that would be infringed only as a\r\nconsequence of further modification of the contributor version.  For\r\npurposes of this definition, \"control\" includes the right to grant\r\npatent sublicenses in a manner consistent with the requirements of\r\nthis License.\r\n\r\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\r\npatent license under the contributor's essential patent claims, to\r\nmake, use, sell, offer for sale, import and otherwise run, modify and\r\npropagate the contents of its contributor version.\r\n\r\n  In the following three paragraphs, a \"patent license\" is any express\r\nagreement or commitment, however denominated, not to enforce a patent\r\n(such as an express permission to practice a patent or covenant not to\r\nsue for patent infringement).  To \"grant\" such a patent license to a\r\nparty means to make such an agreement or commitment not to enforce a\r\npatent against the party.\r\n\r\n  If you convey a covered work, knowingly relying on a patent license,\r\nand the Corresponding Source of the work is not available for anyone\r\nto copy, free of charge and under the terms of this License, through a\r\npublicly available network server or other readily accessible means,\r\nthen you must either (1) cause the Corresponding Source to be so\r\navailable, or (2) arrange to deprive yourself of the benefit of the\r\npatent license for this particular work, or (3) arrange, in a manner\r\nconsistent with the requirements of this License, to extend the patent\r\nlicense to downstream recipients.  \"Knowingly relying\" means you have\r\nactual knowledge that, but for the patent license, your conveying the\r\ncovered work in a country, or your recipient's use of the covered work\r\nin a country, would infringe one or more identifiable patents in that\r\ncountry that you have reason to believe are valid.\r\n\r\n  If, pursuant to or in connection with a single transaction or\r\narrangement, you convey, or propagate by procuring conveyance of, a\r\ncovered work, and grant a patent license to some of the parties\r\nreceiving the covered work authorizing them to use, propagate, modify\r\nor convey a specific copy of the covered work, then the patent license\r\nyou grant is automatically extended to all recipients of the covered\r\nwork and works based on it.\r\n\r\n  A patent license is \"discriminatory\" if it does not include within\r\nthe scope of its coverage, prohibits the exercise of, or is\r\nconditioned on the non-exercise of one or more of the rights that are\r\nspecifically granted under this License.  You may not convey a covered\r\nwork if you are a party to an arrangement with a third party that is\r\nin the business of distributing software, under which you make payment\r\nto the third party based on the extent of your activity of conveying\r\nthe work, and under which the third party grants, to any of the\r\nparties who would receive the covered work from you, a discriminatory\r\npatent license (a) in connection with copies of the covered work\r\nconveyed by you (or copies made from those copies), or (b) primarily\r\nfor and in connection with specific products or compilations that\r\ncontain the covered work, unless you entered into that arrangement,\r\nor that patent license was granted, prior to 28 March 2007.\r\n\r\n  Nothing in this License shall be construed as excluding or limiting\r\nany implied license or other defenses to infringement that may\r\notherwise be available to you under applicable patent law.\r\n\r\n  12. No Surrender of Others' Freedom.\r\n\r\n  If conditions are imposed on you (whether by court order, agreement or\r\notherwise) that contradict the conditions of this License, they do not\r\nexcuse you from the conditions of this License.  If you cannot convey a\r\ncovered work so as to satisfy simultaneously your obligations under this\r\nLicense and any other pertinent obligations, then as a consequence you may\r\nnot convey it at all.  For example, if you agree to terms that obligate you\r\nto collect a royalty for further conveying from those to whom you convey\r\nthe Program, the only way you could satisfy both those terms and this\r\nLicense would be to refrain entirely from conveying the Program.\r\n\r\n  13. Use with the GNU Affero General Public License.\r\n\r\n  Notwithstanding any other provision of this License, you have\r\npermission to link or combine any covered work with a work licensed\r\nunder version 3 of the GNU Affero General Public License into a single\r\ncombined work, and to convey the resulting work.  The terms of this\r\nLicense will continue to apply to the part which is the covered work,\r\nbut the special requirements of the GNU Affero General Public License,\r\nsection 13, concerning interaction through a network will apply to the\r\ncombination as such.\r\n\r\n  14. Revised Versions of this License.\r\n\r\n  The Free Software Foundation may publish revised and/or new versions of\r\nthe GNU General Public License from time to time.  Such new versions will\r\nbe similar in spirit to the present version, but may differ in detail to\r\naddress new problems or concerns.\r\n\r\n  Each version is given a distinguishing version number.  If the\r\nProgram specifies that a certain numbered version of the GNU General\r\nPublic License \"or any later version\" applies to it, you have the\r\noption of following the terms and conditions either of that numbered\r\nversion or of any later version published by the Free Software\r\nFoundation.  If the Program does not specify a version number of the\r\nGNU General Public License, you may choose any version ever published\r\nby the Free Software Foundation.\r\n\r\n  If the Program specifies that a proxy can decide which future\r\nversions of the GNU General Public License can be used, that proxy's\r\npublic statement of acceptance of a version permanently authorizes you\r\nto choose that version for the Program.\r\n\r\n  Later license versions may give you additional or different\r\npermissions.  However, no additional obligations are imposed on any\r\nauthor or copyright holder as a result of your choosing to follow a\r\nlater version.\r\n\r\n  15. Disclaimer of Warranty.\r\n\r\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\r\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\r\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\r\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\r\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\r\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\r\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\r\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\r\n\r\n  16. Limitation of Liability.\r\n\r\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\r\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\r\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\r\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\r\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\r\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\r\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\r\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\r\nSUCH DAMAGES.\r\n\r\n  17. Interpretation of Sections 15 and 16.\r\n\r\n  If the disclaimer of warranty and limitation of liability provided\r\nabove cannot be given local legal effect according to their terms,\r\nreviewing courts shall apply local law that most closely approximates\r\nan absolute waiver of all civil liability in connection with the\r\nProgram, unless a warranty or assumption of liability accompanies a\r\ncopy of the Program in return for a fee.\r\n\r\n                     END OF TERMS AND CONDITIONS\r\n\r\n            How to Apply These Terms to Your New Programs\r\n\r\n  If you develop a new program, and you want it to be of the greatest\r\npossible use to the public, the best way to achieve this is to make it\r\nfree software which everyone can redistribute and change under these terms.\r\n\r\n  To do so, attach the following notices to the program.  It is safest\r\nto attach them to the start of each source file to most effectively\r\nstate the exclusion of warranty; and each file should have at least\r\nthe \"copyright\" line and a pointer to where the full notice is found.\r\n\r\n    <one line to give the program's name and a brief idea of what it does.>\r\n    Copyright (C) <year>  <name of author>\r\n\r\n    This program is free software: you can redistribute it and/or modify\r\n    it under the terms of the GNU General Public License as published by\r\n    the Free Software Foundation, either version 3 of the License, or\r\n    (at your option) any later version.\r\n\r\n    This program is distributed in the hope that it will be useful,\r\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\r\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\r\n    GNU General Public License for more details.\r\n\r\n    You should have received a copy of the GNU General Public License\r\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\r\n\r\nAlso add information on how to contact you by electronic and paper mail.\r\n\r\n  If the program does terminal interaction, make it output a short\r\nnotice like this when it starts in an interactive mode:\r\n\r\n    <program>  Copyright (C) <year>  <name of author>\r\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\r\n    This is free software, and you are welcome to redistribute it\r\n    under certain conditions; type `show c' for details.\r\n\r\nThe hypothetical commands `show w' and `show c' should show the appropriate\r\nparts of the General Public License.  Of course, your program's commands\r\nmight be different; for a GUI interface, you would use an \"about box\".\r\n\r\n  You should also get your employer (if you work as a programmer) or school,\r\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\r\nFor more information on this, and how to apply and follow the GNU GPL, see\r\n<https://www.gnu.org/licenses/>.\r\n\r\n  The GNU General Public License does not permit incorporating your program\r\ninto proprietary programs.  If your program is a subroutine library, you\r\nmay consider it more useful to permit linking proprietary applications with\r\nthe library.  If this is what you want to do, use the GNU Lesser General\r\nPublic License instead of this License.  But first, please read\r\n<https://www.gnu.org/licenses/why-not-lgpl.html>.\r\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 27.5146484375,
          "content": "> [!IMPORTANT]\n> `frontier开发分支`最新动态(2024.12.9): 更新对话时间线功能，优化xelatex论文翻译  \n> `wiki文档`最新动态(2024.12.5): 更新ollama接入指南  \n> `master主分支`最新动态(2024.12.19): 更新3.91版本，更新release页一键安装脚本  \n>\n> 2024.10.10: 突发停电，紧急恢复了提供[whl包](https://drive.google.com/file/d/19U_hsLoMrjOlQSzYS3pzWX9fTzyusArP/view?usp=sharing)的文件服务器  \n> 2024.10.8: 版本3.90加入对llama-index的初步支持，版本3.80加入插件二级菜单功能（详见wiki）  \n> 2024.5.1: 加入Doc2x翻译PDF论文的功能，[查看详情](https://github.com/binary-husky/gpt_academic/wiki/Doc2x)  \n> 2024.3.11: 全力支持Qwen、GLM、DeepseekCoder等中文大语言模型！ SoVits语音克隆模块，[查看详情](https://www.bilibili.com/video/BV1Rp421S7tF/) \n> 2024.1.17: 安装依赖时，请选择`requirements.txt`中**指定的版本**。 安装命令：`pip install -r requirements.txt`。本项目完全开源免费，您可通过订阅[在线服务](https://github.com/binary-husky/gpt_academic/wiki/online)的方式鼓励本项目的发展。\n\n<br>\n\n<div align=center>\n<h1 aligh=\"center\">\n<img src=\"docs/logo.png\" width=\"40\"> GPT 学术优化 (GPT Academic)\n</h1>\n\n[![Github][Github-image]][Github-url]\n[![License][License-image]][License-url]\n[![Releases][Releases-image]][Releases-url]\n[![Installation][Installation-image]][Installation-url]\n[![Wiki][Wiki-image]][Wiki-url]\n[![PR][PRs-image]][PRs-url]\n\n[Github-image]: https://img.shields.io/badge/github-12100E.svg?style=flat-square\n[License-image]: https://img.shields.io/github/license/binary-husky/gpt_academic?label=License&style=flat-square&color=orange\n[Releases-image]: https://img.shields.io/github/release/binary-husky/gpt_academic?label=Release&style=flat-square&color=blue\n[Installation-image]: https://img.shields.io/badge/dynamic/json?color=blue&url=https://raw.githubusercontent.com/binary-husky/gpt_academic/master/version&query=$.version&label=Installation&style=flat-square\n[Wiki-image]: https://img.shields.io/badge/wiki-项目文档-black?style=flat-square\n[PRs-image]: https://img.shields.io/badge/PRs-welcome-pink?style=flat-square\n\n[Github-url]: https://github.com/binary-husky/gpt_academic\n[License-url]: https://github.com/binary-husky/gpt_academic/blob/master/LICENSE\n[Releases-url]: https://github.com/binary-husky/gpt_academic/releases\n[Installation-url]: https://github.com/binary-husky/gpt_academic#installation\n[Wiki-url]: https://github.com/binary-husky/gpt_academic/wiki\n[PRs-url]: https://github.com/binary-husky/gpt_academic/pulls\n\n\n</div>\n<br>\n\n**如果喜欢这个项目，请给它一个Star；如果您发明了好用的快捷键或插件，欢迎发pull requests！**\n\nIf you like this project, please give it a Star.\nRead this in [English](docs/README.English.md) | [日本語](docs/README.Japanese.md) | [한국어](docs/README.Korean.md) | [Русский](docs/README.Russian.md) | [Français](docs/README.French.md). All translations have been provided by the project itself. To translate this project to arbitrary language with GPT, read and run [`multi_language.py`](multi_language.py) (experimental).\n<br>\n\n> [!NOTE]\n> 1.本项目中每个文件的功能都在[自译解报告](https://github.com/binary-husky/gpt_academic/wiki/GPT‐Academic项目自译解报告)`self_analysis.md`详细说明。随着版本的迭代，您也可以随时自行点击相关函数插件，调用GPT重新生成项目的自我解析报告。常见问题请查阅wiki。\n>    [![常规安装方法](https://img.shields.io/static/v1?label=&message=常规安装方法&color=gray)](#installation)  [![一键安装脚本](https://img.shields.io/static/v1?label=&message=一键安装脚本&color=gray)](https://github.com/binary-husky/gpt_academic/releases)  [![配置说明](https://img.shields.io/static/v1?label=&message=配置说明&color=gray)](https://github.com/binary-husky/gpt_academic/wiki/项目配置说明) [![wiki](https://img.shields.io/static/v1?label=&message=wiki&color=gray)]([https://github.com/binary-husky/gpt_academic/wiki/项目配置说明](https://github.com/binary-husky/gpt_academic/wiki))\n>\n> 2.本项目兼容并鼓励尝试国内中文大语言基座模型如通义千问，智谱GLM等。支持多个api-key共存，可在配置文件中填写如`API_KEY=\"openai-key1,openai-key2,azure-key3,api2d-key4\"`。需要临时更换`API_KEY`时，在输入区输入临时的`API_KEY`然后回车键提交即可生效。\n\n<br><br>\n\n<div align=\"center\">\n\n功能（⭐= 近期新增功能） | 描述\n--- | ---\n⭐[接入新模型](https://github.com/binary-husky/gpt_academic/wiki/%E5%A6%82%E4%BD%95%E5%88%87%E6%8D%A2%E6%A8%A1%E5%9E%8B) | 百度[千帆](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Nlks5zkzu)与文心一言, 通义千问[Qwen](https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary)，上海AI-Lab[书生](https://github.com/InternLM/InternLM)，讯飞[星火](https://xinghuo.xfyun.cn/)，[LLaMa2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)，[智谱GLM4](https://open.bigmodel.cn/)，DALLE3, [DeepseekCoder](https://coder.deepseek.com/)\n⭐支持mermaid图像渲染 | 支持让GPT生成[流程图](https://www.bilibili.com/video/BV18c41147H9/)、状态转移图、甘特图、饼状图、GitGraph等等（3.7版本）\n⭐Arxiv论文精细翻译 ([Docker](https://github.com/binary-husky/gpt_academic/pkgs/container/gpt_academic_with_latex)) | [插件] 一键[以超高质量翻译arxiv论文](https://www.bilibili.com/video/BV1dz4y1v77A/)，目前最好的论文翻译工具\n⭐[实时语音对话输入](https://github.com/binary-husky/gpt_academic/blob/master/docs/use_audio.md) | [插件] 异步[监听音频](https://www.bilibili.com/video/BV1AV4y187Uy/)，自动断句，自动寻找回答时机\n⭐AutoGen多智能体插件 | [插件] 借助微软AutoGen，探索多Agent的智能涌现可能！\n⭐虚空终端插件 | [插件] 能够使用自然语言直接调度本项目其他插件\n润色、翻译、代码解释 | 一键润色、翻译、查找论文语法错误、解释代码\n[自定义快捷键](https://www.bilibili.com/video/BV14s4y1E7jN) | 支持自定义快捷键\n模块化设计 | 支持自定义强大的[插件](https://github.com/binary-husky/gpt_academic/tree/master/crazy_functions)，插件支持[热更新](https://github.com/binary-husky/gpt_academic/wiki/%E5%87%BD%E6%95%B0%E6%8F%92%E4%BB%B6%E6%8C%87%E5%8D%97)\n[程序剖析](https://www.bilibili.com/video/BV1cj411A7VW) | [插件] 一键剖析Python/C/C++/Java/Lua/...项目树 或 [自我剖析](https://www.bilibili.com/video/BV1cj411A7VW)\n读论文、[翻译](https://www.bilibili.com/video/BV1KT411x7Wn)论文 | [插件] 一键解读latex/pdf论文全文并生成摘要\nLatex全文[翻译](https://www.bilibili.com/video/BV1nk4y1Y7Js/)、[润色](https://www.bilibili.com/video/BV1FT411H7c5/) | [插件] 一键翻译或润色latex论文\n批量注释生成 | [插件] 一键批量生成函数注释\nMarkdown[中英互译](https://www.bilibili.com/video/BV1yo4y157jV/) | [插件] 看到上面5种语言的[README](https://github.com/binary-husky/gpt_academic/blob/master/docs/README.English.md)了吗？就是出自他的手笔\n[PDF论文全文翻译功能](https://www.bilibili.com/video/BV1KT411x7Wn) | [插件] PDF论文提取题目&摘要+翻译全文（多线程）\n[Arxiv小助手](https://www.bilibili.com/video/BV1LM4y1279X) | [插件] 输入arxiv文章url即可一键翻译摘要+下载PDF\nLatex论文一键校对 | [插件] 仿Grammarly对Latex文章进行语法、拼写纠错+输出对照PDF\n[谷歌学术统合小助手](https://www.bilibili.com/video/BV19L411U7ia) | [插件] 给定任意谷歌学术搜索页面URL，让gpt帮你[写relatedworks](https://www.bilibili.com/video/BV1GP411U7Az/)\n互联网信息聚合+GPT | [插件] 一键[让GPT从互联网获取信息](https://www.bilibili.com/video/BV1om4y127ck)回答问题，让信息永不过时\n公式/图片/表格显示 | 可以同时显示公式的[tex形式和渲染形式](https://user-images.githubusercontent.com/96192199/230598842-1d7fcddd-815d-40ee-af60-baf488a199df.png)，支持公式、代码高亮\n启动暗色[主题](https://github.com/binary-husky/gpt_academic/issues/173) | 在浏览器url后面添加```/?__theme=dark```可以切换dark主题\n[多LLM模型](https://www.bilibili.com/video/BV1wT411p7yf)支持 | 同时被GPT3.5、GPT4、[清华ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)、[复旦MOSS](https://github.com/OpenLMLab/MOSS)伺候的感觉一定会很不错吧？\n更多LLM模型接入，支持[huggingface部署](https://huggingface.co/spaces/qingxu98/gpt-academic) | 加入Newbing接口(新必应)，引入清华[Jittorllms](https://github.com/Jittor/JittorLLMs)支持[LLaMA](https://github.com/facebookresearch/llama)和[盘古α](https://openi.org.cn/pangu/)\n⭐[void-terminal](https://github.com/binary-husky/void-terminal) pip包 | 脱离GUI，在Python中直接调用本项目的所有函数插件（开发中）\n更多新功能展示 (图像生成等) …… | 见本文档结尾处 ……\n</div>\n\n\n- 新界面（修改`config.py`中的LAYOUT选项即可实现“左右布局”和“上下布局”的切换）\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/279702205-d81137c3-affd-4cd1-bb5e-b15610389762.gif\" width=\"700\" >\n</div>\n\n<div align=\"center\">\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/70ff1ec5-e589-4561-a29e-b831079b37fb.gif\" width=\"700\" >\n</div>\n\n\n- 所有按钮都通过读取functional.py动态生成，可随意加自定义功能，解放剪贴板\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/231975334-b4788e91-4887-412f-8b43-2b9c5f41d248.gif\" width=\"700\" >\n</div>\n\n- 润色/纠错\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/231980294-f374bdcb-3309-4560-b424-38ef39f04ebd.gif\" width=\"700\" >\n</div>\n\n- 如果输出包含公式，会以tex形式和渲染形式同时显示，方便复制和阅读\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/230598842-1d7fcddd-815d-40ee-af60-baf488a199df.png\" width=\"700\" >\n</div>\n\n- 懒得看项目代码？直接把整个工程炫ChatGPT嘴里\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/226935232-6b6a73ce-8900-4aee-93f9-733c7e6fef53.png\" width=\"700\" >\n</div>\n\n- 多种大语言模型混合调用（ChatGLM + OpenAI-GPT3.5 + GPT4）\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/232537274-deca0563-7aa6-4b5d-94a2-b7c453c47794.png\" width=\"700\" >\n</div>\n\n<br><br>\n\n# Installation\n\n```mermaid\nflowchart TD\n    A{\"安装方法\"} --> W1(\"I. 🔑直接运行 (Windows, Linux or MacOS)\")\n    W1 --> W11[\"1. Python pip包管理依赖\"]\n    W1 --> W12[\"2. Anaconda包管理依赖（推荐⭐）\"]\n\n    A --> W2[\"II. 🐳使用Docker (Windows, Linux or MacOS)\"]\n\n    W2 --> k1[\"1. 部署项目全部能力的大镜像（推荐⭐）\"]\n    W2 --> k2[\"2. 仅在线模型（GPT, GLM4等）镜像\"]\n    W2 --> k3[\"3. 在线模型 + Latex的大镜像\"]\n\n    A --> W4[\"IV. 🚀其他部署方法\"]\n    W4 --> C1[\"1. Windows/MacOS 一键安装运行脚本（推荐⭐）\"]\n    W4 --> C2[\"2. Huggingface, Sealos远程部署\"]\n    W4 --> C4[\"3. ... 其他 ...\"]\n```\n\n### 安装方法I：直接运行 (Windows, Linux or MacOS)\n\n1. 下载项目\n\n    ```sh\n    git clone --depth=1 https://github.com/binary-husky/gpt_academic.git\n    cd gpt_academic\n    ```\n\n2. 配置API_KEY等变量\n\n    在`config.py`中，配置API KEY等变量。[特殊网络环境设置方法](https://github.com/binary-husky/gpt_academic/issues/1)、[Wiki-项目配置说明](https://github.com/binary-husky/gpt_academic/wiki/项目配置说明)。\n\n    「 程序会优先检查是否存在名为`config_private.py`的私密配置文件，并用其中的配置覆盖`config.py`的同名配置。如您能理解以上读取逻辑，我们强烈建议您在`config.py`同路径下创建一个名为`config_private.py`的新配置文件，并使用`config_private.py`配置项目，从而确保自动更新时不会丢失配置 」。\n\n    「 支持通过`环境变量`配置项目，环境变量的书写格式参考`docker-compose.yml`文件或者我们的[Wiki页面](https://github.com/binary-husky/gpt_academic/wiki/项目配置说明)。配置读取优先级: `环境变量` > `config_private.py` > `config.py` 」。\n\n\n3. 安装依赖\n    ```sh\n    # （选择I: 如熟悉python, python推荐版本 3.9 ~ 3.11）备注：使用官方pip源或者阿里pip源, 临时换源方法：python -m pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/\n    python -m pip install -r requirements.txt\n\n    # （选择II: 使用Anaconda）步骤也是类似的 (https://www.bilibili.com/video/BV1rc411W7Dr)：\n    conda create -n gptac_venv python=3.11    # 创建anaconda环境\n    conda activate gptac_venv                 # 激活anaconda环境\n    python -m pip install -r requirements.txt # 这个步骤和pip安装一样的步骤\n    ```\n\n\n<details><summary>如果需要支持清华ChatGLM系列/复旦MOSS/RWKV作为后端，请点击展开此处</summary>\n<p>\n\n【可选步骤】如果需要支持清华ChatGLM系列/复旦MOSS作为后端，需要额外安装更多依赖（前提条件：熟悉Python + 用过Pytorch + 电脑配置够强）：\n\n```sh\n# 【可选步骤I】支持清华ChatGLM3。清华ChatGLM备注：如果遇到\"Call ChatGLM fail 不能正常加载ChatGLM的参数\" 错误，参考如下： 1：以上默认安装的为torch+cpu版，使用cuda需要卸载torch重新安装torch+cuda； 2：如因本机配置不够无法加载模型，可以修改request_llm/bridge_chatglm.py中的模型精度, 将 AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True) 都修改为 AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\npython -m pip install -r request_llms/requirements_chatglm.txt\n\n# 【可选步骤II】支持清华ChatGLM4 注意：此模型至少需要24G显存\npython -m pip install -r request_llms/requirements_chatglm4.txt\n# 可使用modelscope下载ChatGLM4模型\n# pip install modelscope\n# modelscope download --model ZhipuAI/glm-4-9b-chat --local_dir ./THUDM/glm-4-9b-chat\n\n# 【可选步骤III】支持复旦MOSS\npython -m pip install -r request_llms/requirements_moss.txt\ngit clone --depth=1 https://github.com/OpenLMLab/MOSS.git request_llms/moss  # 注意执行此行代码时，必须处于项目根路径\n\n# 【可选步骤IV】支持RWKV Runner\n参考wiki：https://github.com/binary-husky/gpt_academic/wiki/%E9%80%82%E9%85%8DRWKV-Runner\n\n# 【可选步骤V】确保config.py配置文件的AVAIL_LLM_MODELS包含了期望的模型，目前支持的全部模型如下(jittorllms系列目前仅支持docker方案)：\nAVAIL_LLM_MODELS = [\"gpt-3.5-turbo\", \"api2d-gpt-3.5-turbo\", \"gpt-4\", \"api2d-gpt-4\", \"chatglm\", \"moss\"] # + [\"jittorllms_rwkv\", \"jittorllms_pangualpha\", \"jittorllms_llama\"]\n\n# 【可选步骤VI】支持本地模型INT8,INT4量化（这里所指的模型本身不是量化版本，目前deepseek-coder支持，后面测试后会加入更多模型量化选择）\npip install bitsandbyte\n# windows用户安装bitsandbytes需要使用下面bitsandbytes-windows-webui\npython -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\npip install -U git+https://github.com/huggingface/transformers.git\npip install -U git+https://github.com/huggingface/accelerate.git\npip install peft\n```\n\n</p>\n</details>\n\n\n\n4. 运行\n    ```sh\n    python main.py\n    ```\n\n### 安装方法II：使用Docker\n\n0. 部署项目的全部能力（这个是包含cuda和latex的大型镜像。但如果您网速慢、硬盘小，则不推荐该方法部署完整项目）\n[![fullcapacity](https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-all-capacity.yml/badge.svg?branch=master)](https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-all-capacity.yml)\n\n    ``` sh\n    # 修改docker-compose.yml，保留方案0并删除其他方案。然后运行：\n    docker-compose up\n    ```\n\n1. 仅ChatGPT + GLM4 + 文心一言+spark等在线模型（推荐大多数人选择）\n[![basic](https://github.com/binary-husky/gpt_academic/actions/workflows/build-without-local-llms.yml/badge.svg?branch=master)](https://github.com/binary-husky/gpt_academic/actions/workflows/build-without-local-llms.yml)\n[![basiclatex](https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-latex.yml/badge.svg?branch=master)](https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-latex.yml)\n[![basicaudio](https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-audio-assistant.yml/badge.svg?branch=master)](https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-audio-assistant.yml)\n\n    ``` sh\n    # 修改docker-compose.yml，保留方案1并删除其他方案。然后运行：\n    docker-compose up\n    ```\n\nP.S. 如果需要依赖Latex的插件功能，请见Wiki。另外，您也可以直接使用方案4或者方案0获取Latex功能。\n\n2. ChatGPT + GLM3 + MOSS + LLAMA2 + 通义千问（需要熟悉[Nvidia Docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installing-on-ubuntu-and-debian)运行时）\n[![chatglm](https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-chatglm.yml/badge.svg?branch=master)](https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-chatglm.yml)\n\n    ``` sh\n    # 修改docker-compose.yml，保留方案2并删除其他方案。然后运行：\n    docker-compose up\n    ```\n\n\n### 安装方法III：其他部署方法\n1. **Windows一键运行脚本**。\n完全不熟悉python环境的Windows用户可以下载[Release](https://github.com/binary-husky/gpt_academic/releases)中发布的一键运行脚本安装无本地模型的版本。脚本贡献来源：[oobabooga](https://github.com/oobabooga/one-click-installers)。\n\n2. 使用第三方API、Azure等、文心一言、星火等，见[Wiki页面](https://github.com/binary-husky/gpt_academic/wiki/项目配置说明)\n\n3. 云服务器远程部署避坑指南。\n请访问[云服务器远程部署wiki](https://github.com/binary-husky/gpt_academic/wiki/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9C%E7%A8%8B%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97)\n\n4. 在其他平台部署&二级网址部署\n    - 使用Sealos[一键部署](https://github.com/binary-husky/gpt_academic/issues/993)。\n    - 使用WSL2（Windows Subsystem for Linux 子系统）。请访问[部署wiki-2](https://github.com/binary-husky/gpt_academic/wiki/%E4%BD%BF%E7%94%A8WSL2%EF%BC%88Windows-Subsystem-for-Linux-%E5%AD%90%E7%B3%BB%E7%BB%9F%EF%BC%89%E9%83%A8%E7%BD%B2)\n    - 如何在二级网址（如`http://localhost/subpath`）下运行。请访问[FastAPI运行说明](docs/WithFastapi.md)\n\n<br><br>\n\n# Advanced Usage\n### I：自定义新的便捷按钮（学术快捷键）\n\n现在已可以通过UI中的`界面外观`菜单中的`自定义菜单`添加新的便捷按钮。如果需要在代码中定义，请使用任意文本编辑器打开`core_functional.py`，添加如下条目即可：\n\n```python\n\"超级英译中\": {\n    # 前缀，会被加在你的输入之前。例如，用来描述你的要求，例如翻译、解释代码、润色等等\n    \"Prefix\": \"请翻译把下面一段内容成中文，然后用一个markdown表格逐一解释文中出现的专有名词：\\n\\n\",\n\n    # 后缀，会被加在你的输入之后。例如，配合前缀可以把你的输入内容用引号圈起来。\n    \"Suffix\": \"\",\n},\n```\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/226899272-477c2134-ed71-4326-810c-29891fe4a508.png\" width=\"500\" >\n</div>\n\n### II：自定义函数插件\n编写强大的函数插件来执行任何你想得到的和想不到的任务。\n本项目的插件编写、调试难度很低，只要您具备一定的python基础知识，就可以仿照我们提供的模板实现自己的插件功能。\n详情请参考[函数插件指南](https://github.com/binary-husky/gpt_academic/wiki/%E5%87%BD%E6%95%B0%E6%8F%92%E4%BB%B6%E6%8C%87%E5%8D%97)。\n\n<br><br>\n\n# Updates\n### I：动态\n\n1. 对话保存功能。在函数插件区调用 `保存当前的对话` 即可将当前对话保存为可读+可复原的html文件，\n另外在函数插件区（下拉菜单）调用 `载入对话历史存档` ，即可还原之前的会话。\nTip：不指定文件直接点击 `载入对话历史存档` 可以查看历史html存档缓存。\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/235222390-24a9acc0-680f-49f5-bc81-2f3161f1e049.png\" width=\"500\" >\n</div>\n\n2. ⭐Latex/Arxiv论文翻译功能⭐\n<div align=\"center\">\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/002a1a75-ace0-4e6a-94e2-ec1406a746f1\" height=\"250\" > ===>\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/9fdcc391-f823-464f-9322-f8719677043b\" height=\"250\" >\n</div>\n\n3. 虚空终端（从自然语言输入中，理解用户意图+自动调用其他插件）\n\n- 步骤一：输入 “ 请调用插件翻译PDF论文，地址为https://openreview.net/pdf?id=rJl0r3R9KX ”\n- 步骤二：点击“虚空终端”\n\n<div align=\"center\">\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/66f1b044-e9ff-4eed-9126-5d4f3668f1ed\" width=\"500\" >\n</div>\n\n4. 模块化功能设计，简单的接口却能支持强大的功能\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/229288270-093643c1-0018-487a-81e6-1d7809b6e90f.png\" height=\"400\" >\n<img src=\"https://user-images.githubusercontent.com/96192199/227504931-19955f78-45cd-4d1c-adac-e71e50957915.png\" height=\"400\" >\n</div>\n\n5. 译解其他开源项目\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/226935232-6b6a73ce-8900-4aee-93f9-733c7e6fef53.png\" height=\"250\" >\n<img src=\"https://user-images.githubusercontent.com/96192199/226969067-968a27c1-1b9c-486b-8b81-ab2de8d3f88a.png\" height=\"250\" >\n</div>\n\n6. 装饰[live2d](https://github.com/fghrsh/live2d_demo)的小功能（默认关闭，需要修改`config.py`）\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/96192199/236432361-67739153-73e8-43fe-8111-b61296edabd9.png\" width=\"500\" >\n</div>\n\n7. OpenAI图像生成\n<div align=\"center\">\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/bc7ab234-ad90-48a0-8d62-f703d9e74665\" width=\"500\" >\n</div>\n\n8. 基于mermaid的流图、脑图绘制\n<div align=\"center\">\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/c518b82f-bd53-46e2-baf5-ad1b081c1da4\" width=\"500\" >\n</div>\n\n9. Latex全文校对纠错\n<div align=\"center\">\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/651ccd98-02c9-4464-91e1-77a6b7d1b033\" height=\"200\" > ===>\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/476f66d9-7716-4537-b5c1-735372c25adb\" height=\"200\">\n</div>\n\n10. 语言、主题切换\n<div align=\"center\">\n<img src=\"https://github.com/binary-husky/gpt_academic/assets/96192199/b6799499-b6fb-4f0c-9c8e-1b441872f4e8\" width=\"500\" >\n</div>\n\n\n\n### II：版本:\n- version 3.80(TODO): 优化AutoGen插件主题并设计一系列衍生插件\n- version 3.70: 引入Mermaid绘图，实现GPT画脑图等功能   \n- version 3.60: 引入AutoGen作为新一代插件的基石\n- version 3.57: 支持GLM3，星火v3，文心一言v4，修复本地模型的并发BUG\n- version 3.56: 支持动态追加基础功能按钮，新汇报PDF汇总页面\n- version 3.55: 重构前端界面，引入悬浮窗口与菜单栏\n- version 3.54: 新增动态代码解释器（Code Interpreter）（待完善）\n- version 3.53: 支持动态选择不同界面主题，提高稳定性&解决多用户冲突问题\n- version 3.50: 使用自然语言调用本项目的所有函数插件（虚空终端），支持插件分类，改进UI，设计新主题\n- version 3.49: 支持百度千帆平台和文心一言\n- version 3.48: 支持阿里达摩院通义千问，上海AI-Lab书生，讯飞星火\n- version 3.46: 支持完全脱手操作的实时语音对话\n- version 3.45: 支持自定义ChatGLM2微调模型\n- version 3.44: 正式支持Azure，优化界面易用性\n- version 3.4: +arxiv论文翻译、latex论文批改功能\n- version 3.3: +互联网信息综合功能\n- version 3.2: 函数插件支持更多参数接口 (保存对话功能, 解读任意语言代码+同时询问任意的LLM组合)\n- version 3.1: 支持同时问询多个gpt模型！支持api2d，支持多个apikey负载均衡\n- version 3.0: 对chatglm和其他小型llm的支持\n- version 2.6: 重构了插件结构，提高了交互性，加入更多插件\n- version 2.5: 自更新，解决总结大工程源代码时文本过长、token溢出的问题\n- version 2.4: 新增PDF全文翻译功能; 新增输入区切换位置的功能\n- version 2.3: 增强多线程交互性\n- version 2.2: 函数插件支持热重载\n- version 2.1: 可折叠式布局\n- version 2.0: 引入模块化函数插件\n- version 1.0: 基础功能\n\nGPT Academic开发者QQ群：`610599535`\n\n- 已知问题\n    - 某些浏览器翻译插件干扰此软件前端的运行\n    - 官方Gradio目前有很多兼容性问题，请**务必使用`requirement.txt`安装Gradio**\n\n```mermaid\ntimeline LR\n    title GPT-Academic项目发展历程\n    section 2.x\n        1.0~2.2: 基础功能: 引入模块化函数插件: 可折叠式布局: 函数插件支持热重载\n        2.3~2.5: 增强多线程交互性: 新增PDF全文翻译功能: 新增输入区切换位置的功能: 自更新\n        2.6: 重构了插件结构: 提高了交互性: 加入更多插件\n    section 3.x\n        3.0~3.1: 对chatglm支持: 对其他小型llm支持: 支持同时问询多个gpt模型: 支持多个apikey负载均衡\n        3.2~3.3: 函数插件支持更多参数接口: 保存对话功能: 解读任意语言代码: 同时询问任意的LLM组合: 互联网信息综合功能\n        3.4: 加入arxiv论文翻译: 加入latex论文批改功能\n        3.44: 正式支持Azure: 优化界面易用性\n        3.46: 自定义ChatGLM2微调模型: 实时语音对话\n        3.49: 支持阿里达摩院通义千问: 上海AI-Lab书生: 讯飞星火: 支持百度千帆平台 & 文心一言\n        3.50: 虚空终端: 支持插件分类: 改进UI: 设计新主题\n        3.53: 动态选择不同界面主题: 提高稳定性: 解决多用户冲突问题\n        3.55: 动态代码解释器: 重构前端界面: 引入悬浮窗口与菜单栏\n        3.56: 动态追加基础功能按钮: 新汇报PDF汇总页面\n        3.57: GLM3, 星火v3: 支持文心一言v4: 修复本地模型的并发BUG\n        3.60: 引入AutoGen\n        3.70: 引入Mermaid绘图: 实现GPT画脑图等功能\n        3.80(TODO): 优化AutoGen插件主题: 设计衍生插件\n\n```\n\n\n### III：主题\n可以通过修改`THEME`选项（config.py）变更主题\n1. `Chuanhu-Small-and-Beautiful` [网址](https://github.com/GaiZhenbiao/ChuanhuChatGPT/)\n\n\n### IV：本项目的开发分支\n\n1. `master` 分支: 主分支，稳定版\n2. `frontier` 分支: 开发分支，测试版\n3. 如何[接入其他大模型](request_llms/README.md)\n4. 访问GPT-Academic的[在线服务并支持我们](https://github.com/binary-husky/gpt_academic/wiki/online)\n\n### V：参考与学习\n\n```\n代码中参考了很多其他优秀项目中的设计，顺序不分先后：\n\n# 清华ChatGLM2-6B:\nhttps://github.com/THUDM/ChatGLM2-6B\n\n# 清华JittorLLMs:\nhttps://github.com/Jittor/JittorLLMs\n\n# ChatPaper:\nhttps://github.com/kaixindelele/ChatPaper\n\n# Edge-GPT:\nhttps://github.com/acheong08/EdgeGPT\n\n# ChuanhuChatGPT:\nhttps://github.com/GaiZhenbiao/ChuanhuChatGPT\n\n# Oobabooga one-click installer:\nhttps://github.com/oobabooga/one-click-installers\n\n# More：\nhttps://github.com/gradio-app/gradio\nhttps://github.com/fghrsh/live2d_demo\n```\n"
        },
        {
          "name": "check_proxy.py",
          "type": "blob",
          "size": 10.0244140625,
          "content": "from loguru import logger\n\ndef check_proxy(proxies, return_ip=False):\n    \"\"\"\n    检查代理配置并返回结果。\n\n    Args:\n        proxies (dict): 包含http和https代理配置的字典。\n        return_ip (bool, optional): 是否返回代理的IP地址。默认为False。\n\n    Returns:\n        str or None: 检查的结果信息或代理的IP地址（如果`return_ip`为True）。\n    \"\"\"\n    import requests\n    proxies_https = proxies['https'] if proxies is not None else '无'\n    ip = None\n    try:\n        response = requests.get(\"https://ipapi.co/json/\", proxies=proxies, timeout=4)  # ⭐ 执行GET请求以获取代理信息\n        data = response.json()\n        if 'country_name' in data:\n            country = data['country_name']\n            result = f\"代理配置 {proxies_https}, 代理所在地：{country}\"\n            if 'ip' in data:\n                ip = data['ip']\n        elif 'error' in data:\n            alternative, ip = _check_with_backup_source(proxies)  # ⭐ 调用备用方法检查代理配置\n            if alternative is None:\n                result = f\"代理配置 {proxies_https}, 代理所在地：未知，IP查询频率受限\"\n            else:\n                result = f\"代理配置 {proxies_https}, 代理所在地：{alternative}\"\n        else:\n            result = f\"代理配置 {proxies_https}, 代理数据解析失败：{data}\"\n\n        if not return_ip:\n            logger.warning(result)\n            return result\n        else:\n            return ip\n    except:\n        result = f\"代理配置 {proxies_https}, 代理所在地查询超时，代理可能无效\"\n        if not return_ip:\n            logger.warning(result)\n            return result\n        else:\n            return ip\n\ndef _check_with_backup_source(proxies):\n    \"\"\"\n    通过备份源检查代理，并获取相应信息。\n\n    Args:\n        proxies (dict): 包含代理信息的字典。\n\n    Returns:\n        tuple: 代理信息(geo)和IP地址(ip)的元组。\n    \"\"\"\n    import random, string, requests\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=32))\n    try:\n        res_json = requests.get(f\"http://{random_string}.edns.ip-api.com/json\", proxies=proxies, timeout=4).json()  # ⭐ 执行代理检查和备份源请求\n        return res_json['dns']['geo'], res_json['dns']['ip']\n    except:\n        return None, None\n\ndef backup_and_download(current_version, remote_version):\n    \"\"\"\n    一键更新协议：备份当前版本，下载远程版本并解压缩。\n\n    Args:\n        current_version (str): 当前版本号。\n        remote_version (str): 远程版本号。\n\n    Returns:\n        str: 新版本目录的路径。\n    \"\"\"\n    from toolbox import get_conf\n    import shutil\n    import os\n    import requests\n    import zipfile\n    os.makedirs(f'./history', exist_ok=True)\n    backup_dir = f'./history/backup-{current_version}/'\n    new_version_dir = f'./history/new-version-{remote_version}/'\n    if os.path.exists(new_version_dir):\n        return new_version_dir\n    os.makedirs(new_version_dir)\n    shutil.copytree('./', backup_dir, ignore=lambda x, y: ['history'])\n    proxies = get_conf('proxies')\n    try:    r = requests.get('https://github.com/binary-husky/chatgpt_academic/archive/refs/heads/master.zip', proxies=proxies, stream=True)\n    except: r = requests.get('https://public.agent-matrix.com/publish/master.zip', proxies=proxies, stream=True)\n    zip_file_path = backup_dir+'/master.zip'  # ⭐ 保存备份文件的路径\n    with open(zip_file_path, 'wb+') as f:\n        f.write(r.content)\n    dst_path = new_version_dir\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        for zip_info in zip_ref.infolist():\n            dst_file_path = os.path.join(dst_path, zip_info.filename)\n            if os.path.exists(dst_file_path):\n                os.remove(dst_file_path)\n            zip_ref.extract(zip_info, dst_path)\n    return new_version_dir\n\n\ndef patch_and_restart(path):\n    \"\"\"\n    一键更新协议：覆盖和重启\n\n    Args:\n        path (str): 新版本代码所在的路径\n\n    注意事项:\n        如果您的程序没有使用config_private.py私密配置文件，则会将config.py重命名为config_private.py以避免配置丢失。\n\n    更新流程:\n        - 复制最新版本代码到当前目录\n        - 更新pip包依赖\n        - 如果更新失败，则提示手动安装依赖库并重启\n    \"\"\"\n    from distutils import dir_util\n    import shutil\n    import os\n    import sys\n    import time\n    import glob\n    from shared_utils.colorful import log亮黄, log亮绿, log亮红\n\n    if not os.path.exists('config_private.py'):\n        log亮黄('由于您没有设置config_private.py私密配置，现将您的现有配置移动至config_private.py以防止配置丢失，',\n              '另外您可以随时在history子文件夹下找回旧版的程序。')\n        shutil.copyfile('config.py', 'config_private.py')\n\n    path_new_version = glob.glob(path + '/*-master')[0]\n    dir_util.copy_tree(path_new_version, './')  # ⭐ 将最新版本代码复制到当前目录\n\n    log亮绿('代码已经更新，即将更新pip包依赖……')\n    for i in reversed(range(5)): time.sleep(1); log亮绿(i)\n\n    try:\n        import subprocess\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'])\n    except:\n        log亮红('pip包依赖安装出现问题，需要手动安装新增的依赖库 `python -m pip install -r requirements.txt`，然后在用常规的`python main.py`的方式启动。')\n\n    log亮绿('更新完成，您可以随时在history子文件夹下找回旧版的程序，5s之后重启')\n    log亮红('假如重启失败，您可能需要手动安装新增的依赖库 `python -m pip install -r requirements.txt`，然后在用常规的`python main.py`的方式启动。')\n    log亮绿(' ------------------------------ -----------------------------------')\n\n    for i in reversed(range(8)): time.sleep(1); log亮绿(i)\n    os.execl(sys.executable, sys.executable, *sys.argv)  # 重启程序\n\n\ndef get_current_version():\n    \"\"\"\n    获取当前的版本号。\n\n    Returns:\n        str: 当前的版本号。如果无法获取版本号，则返回空字符串。\n    \"\"\"\n    import json\n    try:\n        with open('./version', 'r', encoding='utf8') as f:\n            current_version = json.loads(f.read())['version']  # ⭐ 从读取的json数据中提取版本号\n    except:\n        current_version = \"\"\n    return current_version\n\n\ndef auto_update(raise_error=False):\n    \"\"\"\n    一键更新协议：查询版本和用户意见\n\n    Args:\n        raise_error (bool, optional): 是否在出错时抛出错误。默认为 False。\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        from toolbox import get_conf\n        import requests\n        import json\n        proxies = get_conf('proxies')\n        try:    response = requests.get(\"https://raw.githubusercontent.com/binary-husky/chatgpt_academic/master/version\", proxies=proxies, timeout=5)\n        except: response = requests.get(\"https://public.agent-matrix.com/publish/version\", proxies=proxies, timeout=5)\n        remote_json_data = json.loads(response.text)\n        remote_version = remote_json_data['version']\n        if remote_json_data[\"show_feature\"]:\n            new_feature = \"新功能：\" + remote_json_data[\"new_feature\"]\n        else:\n            new_feature = \"\"\n        with open('./version', 'r', encoding='utf8') as f:\n            current_version = f.read()\n            current_version = json.loads(current_version)['version']\n        if (remote_version - current_version) >= 0.01-1e-5:\n            from shared_utils.colorful import log亮黄\n            log亮黄(f'\\n新版本可用。新版本:{remote_version}，当前版本:{current_version}。{new_feature}')  # ⭐ 在控制台打印新版本信息\n            logger.info('（1）Github更新地址:\\nhttps://github.com/binary-husky/chatgpt_academic\\n')\n            user_instruction = input('（2）是否一键更新代码（Y+回车=确认，输入其他/无输入+回车=不更新）？')\n            if user_instruction in ['Y', 'y']:\n                path = backup_and_download(current_version, remote_version)  # ⭐ 备份并下载文件\n                try:\n                    patch_and_restart(path)  # ⭐ 执行覆盖并重启操作\n                except:\n                    msg = '更新失败。'\n                    if raise_error:\n                        from toolbox import trimmed_format_exc\n                        msg += trimmed_format_exc()\n                    logger.warning(msg)\n            else:\n                logger.info('自动更新程序：已禁用')\n                return\n        else:\n            return\n    except:\n        msg = '自动更新程序：已禁用。建议排查：代理网络配置。'\n        if raise_error:\n            from toolbox import trimmed_format_exc\n            msg += trimmed_format_exc()\n        logger.info(msg)\n\ndef warm_up_modules():\n    \"\"\"\n    预热模块，加载特定模块并执行预热操作。\n    \"\"\"\n    logger.info('正在执行一些模块的预热 ...')\n    from toolbox import ProxyNetworkActivate\n    from request_llms.bridge_all import model_info\n    with ProxyNetworkActivate(\"Warmup_Modules\"):\n        enc = model_info[\"gpt-3.5-turbo\"]['tokenizer']\n        enc.encode(\"模块预热\", disallowed_special=())\n        enc = model_info[\"gpt-4\"]['tokenizer']\n        enc.encode(\"模块预热\", disallowed_special=())\n\ndef warm_up_vectordb():\n    \"\"\"\n    执行一些模块的预热操作。\n\n    本函数主要用于执行一些模块的预热操作，确保在后续的流程中能够顺利运行。\n\n    ⭐ 关键作用：预热模块\n\n    Returns:\n        None\n    \"\"\"\n    logger.info('正在执行一些模块的预热 ...')\n    from toolbox import ProxyNetworkActivate\n    with ProxyNetworkActivate(\"Warmup_Modules\"):\n        import nltk\n        with ProxyNetworkActivate(\"Warmup_Modules\"): nltk.download(\"punkt\")\n\n\nif __name__ == '__main__':\n    import os\n    os.environ['no_proxy'] = '*'  # 避免代理网络产生意外污染\n    from toolbox import get_conf\n    proxies = get_conf('proxies')\n    check_proxy(proxies)"
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 16.0009765625,
          "content": "\"\"\"\n    以下所有配置也都支持利用环境变量覆写，环境变量配置格式见docker-compose.yml。\n    读取优先级：环境变量 > config_private.py > config.py\n    --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n    All the following configurations also support using environment variables to override,\n    and the environment variable configuration format can be seen in docker-compose.yml.\n    Configuration reading priority: environment variable > config_private.py > config.py\n\"\"\"\n\n# [step 1-1]>> ( 接入GPT等模型 ) API_KEY = \"sk-123456789xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx123456789\"。极少数情况下，还需要填写组织（格式如org-123456789abcdefghijklmno的），请向下翻，找 API_ORG 设置项\nAPI_KEY = \"在此处填写APIKEY\"    # 可同时填写多个API-KEY，用英文逗号分割，例如API_KEY = \"sk-openaikey1,sk-openaikey2,fkxxxx-api2dkey3,azure-apikey4\"\n\n# [step 1-2]>> ( 接入通义 qwen-max ) 接入通义千问在线大模型，api-key获取地址 https://dashscope.console.aliyun.com/\nDASHSCOPE_API_KEY = \"\" # 阿里灵积云API_KEY\n\n# [step 2]>> 改为True应用代理，如果直接在海外服务器部署，此处不修改；如果使用本地或无地域限制的大模型时，此处也不需要修改\nUSE_PROXY = False\nif USE_PROXY:\n    \"\"\"\n    代理网络的地址，打开你的代理软件查看代理协议(socks5h / http)、地址(localhost)和端口(11284)\n    填写格式是 [协议]://  [地址] :[端口]，填写之前不要忘记把USE_PROXY改成True，如果直接在海外服务器部署，此处不修改\n            <配置教程&视频教程> https://github.com/binary-husky/gpt_academic/issues/1>\n    [协议] 常见协议无非socks5h/http; 例如 v2**y 和 ss* 的默认本地协议是socks5h; 而cl**h 的默认本地协议是http\n    [地址] 填localhost或者127.0.0.1（localhost意思是代理软件安装在本机上）\n    [端口] 在代理软件的设置里找。虽然不同的代理软件界面不一样，但端口号都应该在最显眼的位置上\n    \"\"\"\n    proxies = {\n        #          [协议]://  [地址]  :[端口]\n        \"http\":  \"socks5h://localhost:11284\",  # 再例如  \"http\":  \"http://127.0.0.1:7890\",\n        \"https\": \"socks5h://localhost:11284\",  # 再例如  \"https\": \"http://127.0.0.1:7890\",\n    }\nelse:\n    proxies = None\n\n# [step 3]>> 模型选择是 (注意: LLM_MODEL是默认选中的模型, 它*必须*被包含在AVAIL_LLM_MODELS列表中 )\nLLM_MODEL = \"gpt-3.5-turbo-16k\" # 可选 ↓↓↓\nAVAIL_LLM_MODELS = [\"qwen-max\", \"o1-mini\", \"o1-mini-2024-09-12\", \"o1\", \"o1-2024-12-17\", \"o1-preview\", \"o1-preview-2024-09-12\",\n                    \"gpt-4-1106-preview\", \"gpt-4-turbo-preview\", \"gpt-4-vision-preview\",\n                    \"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-4-turbo-2024-04-09\",\n                    \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo\", \"azure-gpt-3.5\",\n                    \"gpt-4\", \"gpt-4-32k\", \"azure-gpt-4\", \"glm-4\", \"glm-4v\", \"glm-3-turbo\",\n                    \"gemini-1.5-pro\", \"chatglm3\", \"chatglm4\"\n                    ]\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# --- --- --- ---\n# P.S. 其他可用的模型还包括\n# AVAIL_LLM_MODELS = [\n#   \"glm-4-0520\", \"glm-4-air\", \"glm-4-airx\", \"glm-4-flash\",\n#   \"qianfan\", \"deepseekcoder\",\n#   \"spark\", \"sparkv2\", \"sparkv3\", \"sparkv3.5\", \"sparkv4\",\n#   \"qwen-turbo\", \"qwen-plus\", \"qwen-local\",\n#   \"moonshot-v1-128k\", \"moonshot-v1-32k\", \"moonshot-v1-8k\",\n#   \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-0125\", \"gpt-4o-2024-05-13\"\n#   \"claude-3-haiku-20240307\",\"claude-3-sonnet-20240229\",\"claude-3-opus-20240229\", \"claude-2.1\", \"claude-instant-1.2\",\n#   \"moss\", \"llama2\", \"chatglm_onnx\", \"internlm\", \"jittorllms_pangualpha\", \"jittorllms_llama\",\n#   \"deepseek-chat\" ,\"deepseek-coder\",\n#   \"gemini-1.5-flash\",\n#   \"yi-34b-chat-0205\",\"yi-34b-chat-200k\",\"yi-large\",\"yi-medium\",\"yi-spark\",\"yi-large-turbo\",\"yi-large-preview\",\n#   \"grok-beta\",\n# ]\n# --- --- --- ---\n# 此外，您还可以在接入one-api/vllm/ollama/Openroute时，\n# 使用\"one-api-*\",\"vllm-*\",\"ollama-*\",\"openrouter-*\"前缀直接使用非标准方式接入的模型，例如\n# AVAIL_LLM_MODELS = [\"one-api-claude-3-sonnet-20240229(max_token=100000)\", \"ollama-phi3(max_token=4096)\",\"openrouter-openai/gpt-4o-mini\",\"openrouter-openai/chatgpt-4o-latest\"]\n# --- --- --- ---\n\n\n# --------------- 以下配置可以优化体验 ---------------\n\n# 重新URL重新定向，实现更换API_URL的作用（高危设置! 常规情况下不要修改! 通过修改此设置，您将把您的API-KEY和对话隐私完全暴露给您设定的中间人！）\n# 格式: API_URL_REDIRECT = {\"https://api.openai.com/v1/chat/completions\": \"在这里填写重定向的api.openai.com的URL\"}\n# 举例: API_URL_REDIRECT = {\"https://api.openai.com/v1/chat/completions\": \"https://reverse-proxy-url/v1/chat/completions\", \"http://localhost:11434/api/chat\": \"在这里填写您ollama的URL\"}\nAPI_URL_REDIRECT = {}\n\n\n# 多线程函数插件中，默认允许多少路线程同时访问OpenAI。Free trial users的限制是每分钟3次，Pay-as-you-go users的限制是每分钟3500次\n# 一言以蔽之：免费（5刀）用户填3，OpenAI绑了信用卡的用户可以填 16 或者更高。提高限制请查询：https://platform.openai.com/docs/guides/rate-limits/overview\nDEFAULT_WORKER_NUM = 3\n\n\n# 色彩主题, 可选 [\"Default\", \"Chuanhu-Small-and-Beautiful\", \"High-Contrast\"]\n# 更多主题, 请查阅Gradio主题商店: https://huggingface.co/spaces/gradio/theme-gallery 可选 [\"Gstaff/Xkcd\", \"NoCrypt/Miku\", ...]\nTHEME = \"Default\"\nAVAIL_THEMES = [\"Default\", \"Chuanhu-Small-and-Beautiful\", \"High-Contrast\", \"Gstaff/Xkcd\", \"NoCrypt/Miku\"]\n\n\n# 默认的系统提示词（system prompt）\nINIT_SYS_PROMPT = \"Serve me as a writing and programming assistant.\"\n\n\n# 对话窗的高度 （仅在LAYOUT=\"TOP-DOWN\"时生效）\nCHATBOT_HEIGHT = 1115\n\n\n# 代码高亮\nCODE_HIGHLIGHT = True\n\n\n# 窗口布局\nLAYOUT = \"LEFT-RIGHT\"   # \"LEFT-RIGHT\"（左右布局） # \"TOP-DOWN\"（上下布局）\n\n\n# 暗色模式 / 亮色模式\nDARK_MODE = True\n\n\n# 发送请求到OpenAI后，等待多久判定为超时\nTIMEOUT_SECONDS = 30\n\n\n# 网页的端口, -1代表随机端口\nWEB_PORT = -1\n\n\n# 是否自动打开浏览器页面\nAUTO_OPEN_BROWSER = True\n\n\n# 如果OpenAI不响应（网络卡顿、代理失败、KEY失效），重试的次数限制\nMAX_RETRY = 2\n\n\n# 插件分类默认选项\nDEFAULT_FN_GROUPS = ['对话', '编程', '学术', '智能体']\n\n\n# 定义界面上“询问多个GPT模型”插件应该使用哪些模型，请从AVAIL_LLM_MODELS中选择，并在不同模型之间用`&`间隔，例如\"gpt-3.5-turbo&chatglm3&azure-gpt-4\"\nMULTI_QUERY_LLM_MODELS = \"gpt-3.5-turbo&chatglm3\"\n\n\n# 选择本地模型变体（只有当AVAIL_LLM_MODELS包含了对应本地模型时，才会起作用）\n# 如果你选择Qwen系列的模型，那么请在下面的QWEN_MODEL_SELECTION中指定具体的模型\n# 也可以是具体的模型路径\nQWEN_LOCAL_MODEL_SELECTION = \"Qwen/Qwen-1_8B-Chat-Int8\"\n\n\n# 百度千帆（LLM_MODEL=\"qianfan\"）\nBAIDU_CLOUD_API_KEY = ''\nBAIDU_CLOUD_SECRET_KEY = ''\nBAIDU_CLOUD_QIANFAN_MODEL = 'ERNIE-Bot'    # 可选 \"ERNIE-Bot-4\"(文心大模型4.0), \"ERNIE-Bot\"(文心一言), \"ERNIE-Bot-turbo\", \"BLOOMZ-7B\", \"Llama-2-70B-Chat\", \"Llama-2-13B-Chat\", \"Llama-2-7B-Chat\", \"ERNIE-Speed-128K\", \"ERNIE-Speed-8K\", \"ERNIE-Lite-8K\"\n\n\n# 如果使用ChatGLM3或ChatGLM4本地模型，请把 LLM_MODEL=\"chatglm3\" 或LLM_MODEL=\"chatglm4\"，并在此处指定模型路径\nCHATGLM_LOCAL_MODEL_PATH = \"THUDM/glm-4-9b-chat\" # 例如\"/home/hmp/ChatGLM3-6B/\"\n\n# 如果使用ChatGLM2微调模型，请把 LLM_MODEL=\"chatglmft\"，并在此处指定模型路径\nCHATGLM_PTUNING_CHECKPOINT = \"\" # 例如\"/home/hmp/ChatGLM2-6B/ptuning/output/6b-pt-128-1e-2/checkpoint-100\"\n\n\n# 本地LLM模型如ChatGLM的执行方式 CPU/GPU\nLOCAL_MODEL_DEVICE = \"cpu\" # 可选 \"cuda\"\nLOCAL_MODEL_QUANT = \"FP16\" # 默认 \"FP16\" \"INT4\" 启用量化INT4版本 \"INT8\" 启用量化INT8版本\n\n\n# 设置gradio的并行线程数（不需要修改）\nCONCURRENT_COUNT = 100\n\n\n# 是否在提交时自动清空输入框\nAUTO_CLEAR_TXT = False\n\n\n# 加一个live2d装饰\nADD_WAIFU = False\n\n\n# 设置用户名和密码（不需要修改）（相关功能不稳定，与gradio版本和网络都相关，如果本地使用不建议加这个）\n# [(\"username\", \"password\"), (\"username2\", \"password2\"), ...]\nAUTHENTICATION = []\n\n\n# 如果需要在二级路径下运行（常规情况下，不要修改!!）\n# （举例 CUSTOM_PATH = \"/gpt_academic\"，可以让软件运行在 http://ip:port/gpt_academic/ 下。）\nCUSTOM_PATH = \"/\"\n\n\n# HTTPS 秘钥和证书（不需要修改）\nSSL_KEYFILE = \"\"\nSSL_CERTFILE = \"\"\n\n\n# 极少数情况下，openai的官方KEY需要伴随组织编码（格式如org-xxxxxxxxxxxxxxxxxxxxxxxx）使用\nAPI_ORG = \"\"\n\n\n# 如果需要使用Slack Claude，使用教程详情见 request_llms/README.md\nSLACK_CLAUDE_BOT_ID = ''\nSLACK_CLAUDE_USER_TOKEN = ''\n\n\n# 如果需要使用AZURE（方法一：单个azure模型部署）详情请见额外文档 docs\\use_azure.md\nAZURE_ENDPOINT = \"https://你亲手写的api名称.openai.azure.com/\"\nAZURE_API_KEY = \"填入azure openai api的密钥\"    # 建议直接在API_KEY处填写，该选项即将被弃用\nAZURE_ENGINE = \"填入你亲手写的部署名\"            # 读 docs\\use_azure.md\n\n\n# 如果需要使用AZURE（方法二：多个azure模型部署+动态切换）详情请见额外文档 docs\\use_azure.md\nAZURE_CFG_ARRAY = {}\n\n\n# 阿里云实时语音识别 配置难度较高\n# 参考 https://github.com/binary-husky/gpt_academic/blob/master/docs/use_audio.md\nENABLE_AUDIO = False\nALIYUN_TOKEN=\"\"     # 例如 f37f30e0f9934c34a992f6f64f7eba4f\nALIYUN_APPKEY=\"\"    # 例如 RoPlZrM88DnAFkZK\nALIYUN_ACCESSKEY=\"\" # （无需填写）\nALIYUN_SECRET=\"\"    # （无需填写）\n\n\n# GPT-SOVITS 文本转语音服务的运行地址（将语言模型的生成文本朗读出来）\nTTS_TYPE = \"EDGE_TTS\" # EDGE_TTS / LOCAL_SOVITS_API / DISABLE\nGPT_SOVITS_URL = \"\"\nEDGE_TTS_VOICE = \"zh-CN-XiaoxiaoNeural\"\n\n\n# 接入讯飞星火大模型 https://console.xfyun.cn/services/iat\nXFYUN_APPID = \"00000000\"\nXFYUN_API_SECRET = \"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\"\nXFYUN_API_KEY = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\n\n\n# 接入智谱大模型\nZHIPUAI_API_KEY = \"\"\nZHIPUAI_MODEL = \"\" # 此选项已废弃，不再需要填写\n\n\n# Claude API KEY\nANTHROPIC_API_KEY = \"\"\n\n\n# 月之暗面 API KEY\nMOONSHOT_API_KEY = \"\"\n\n\n# 零一万物(Yi Model) API KEY\nYIMODEL_API_KEY = \"\"\n\n# 深度求索(DeepSeek) API KEY，默认请求地址为\"https://api.deepseek.com/v1/chat/completions\"\nDEEPSEEK_API_KEY = \"\"\n\n\n# 紫东太初大模型 https://ai-maas.wair.ac.cn\nTAICHU_API_KEY = \"\"\n\n# Grok API KEY\nGROK_API_KEY = \"\"\n\n# Mathpix 拥有执行PDF的OCR功能，但是需要注册账号\nMATHPIX_APPID = \"\"\nMATHPIX_APPKEY = \"\"\n\n\n# DOC2X的PDF解析服务，注册账号并获取API KEY: https://doc2x.noedgeai.com/login\nDOC2X_API_KEY = \"\"\n\n\n# 自定义API KEY格式\nCUSTOM_API_KEY_PATTERN = \"\"\n\n\n# Google Gemini API-Key\nGEMINI_API_KEY = ''\n\n\n# HUGGINGFACE的TOKEN，下载LLAMA时起作用 https://huggingface.co/docs/hub/security-tokens\nHUGGINGFACE_ACCESS_TOKEN = \"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\"\n\n\n# GROBID服务器地址（填写多个可以均衡负载），用于高质量地读取PDF文档\n# 获取方法：复制以下空间https://huggingface.co/spaces/qingxu98/grobid，设为public，然后GROBID_URL = \"https://(你的hf用户名如qingxu98)-(你的填写的空间名如grobid).hf.space\"\nGROBID_URLS = [\n    \"https://qingxu98-grobid.hf.space\",\"https://qingxu98-grobid2.hf.space\",\"https://qingxu98-grobid3.hf.space\",\n    \"https://qingxu98-grobid4.hf.space\",\"https://qingxu98-grobid5.hf.space\", \"https://qingxu98-grobid6.hf.space\",\n    \"https://qingxu98-grobid7.hf.space\", \"https://qingxu98-grobid8.hf.space\",\n]\n\n\n# Searxng互联网检索服务（这是一个huggingface空间，请前往huggingface复制该空间，然后把自己新的空间地址填在这里）\nSEARXNG_URLS = [ f\"https://kaletianlre-beardvs{i}dd.hf.space/\" for i in range(1,5) ]\n\n\n# 是否允许通过自然语言描述修改本页的配置，该功能具有一定的危险性，默认关闭\nALLOW_RESET_CONFIG = False\n\n\n# 在使用AutoGen插件时，是否使用Docker容器运行代码\nAUTOGEN_USE_DOCKER = False\n\n\n# 临时的上传文件夹位置，请尽量不要修改\nPATH_PRIVATE_UPLOAD = \"private_upload\"\n\n\n# 日志文件夹的位置，请尽量不要修改\nPATH_LOGGING = \"gpt_log\"\n\n\n# 存储翻译好的arxiv论文的路径，请尽量不要修改\nARXIV_CACHE_DIR = \"gpt_log/arxiv_cache\"\n\n\n# 除了连接OpenAI之外，还有哪些场合允许使用代理，请尽量不要修改\nWHEN_TO_USE_PROXY = [\"Connect_OpenAI\", \"Download_LLM\", \"Download_Gradio_Theme\", \"Connect_Grobid\",\n                     \"Warmup_Modules\", \"Nougat_Download\", \"AutoGen\", \"Connect_OpenAI_Embedding\"]\n\n\n# 启用插件热加载\nPLUGIN_HOT_RELOAD = False\n\n\n# 自定义按钮的最大数量限制\nNUM_CUSTOM_BASIC_BTN = 4\n\n\n# 媒体智能体的服务地址（这是一个huggingface空间，请前往huggingface复制该空间，然后把自己新的空间地址填在这里）\nDAAS_SERVER_URLS = [ f\"https://niuziniu-biligpt{i}.hf.space/stream\" for i in range(1,5) ]\n\n\n\n\"\"\"\n--------------- 配置关联关系说明 ---------------\n\n在线大模型配置关联关系示意图\n│\n├── \"gpt-3.5-turbo\" 等openai模型\n│   ├── API_KEY\n│   ├── CUSTOM_API_KEY_PATTERN（不常用）\n│   ├── API_ORG（不常用）\n│   └── API_URL_REDIRECT（不常用）\n│\n├── \"azure-gpt-3.5\" 等azure模型（单个azure模型，不需要动态切换）\n│   ├── API_KEY\n│   ├── AZURE_ENDPOINT\n│   ├── AZURE_API_KEY\n│   ├── AZURE_ENGINE\n│   └── API_URL_REDIRECT\n│\n├── \"azure-gpt-3.5\" 等azure模型（多个azure模型，需要动态切换，高优先级）\n│   └── AZURE_CFG_ARRAY\n│\n├── \"spark\" 星火认知大模型 spark & sparkv2\n│   ├── XFYUN_APPID\n│   ├── XFYUN_API_SECRET\n│   └── XFYUN_API_KEY\n│\n├── \"claude-3-opus-20240229\" 等claude模型\n│   └── ANTHROPIC_API_KEY\n│\n├── \"stack-claude\"\n│   ├── SLACK_CLAUDE_BOT_ID\n│   └── SLACK_CLAUDE_USER_TOKEN\n│\n├── \"qianfan\" 百度千帆大模型库\n│   ├── BAIDU_CLOUD_QIANFAN_MODEL\n│   ├── BAIDU_CLOUD_API_KEY\n│   └── BAIDU_CLOUD_SECRET_KEY\n│\n├── \"glm-4\", \"glm-3-turbo\", \"zhipuai\" 智谱AI大模型\n│   └── ZHIPUAI_API_KEY\n│\n├── \"yi-34b-chat-0205\", \"yi-34b-chat-200k\" 等零一万物(Yi Model)大模型\n│   └── YIMODEL_API_KEY\n│\n├── \"qwen-turbo\" 等通义千问大模型\n│   └──  DASHSCOPE_API_KEY\n│\n├── \"Gemini\"\n│   └──  GEMINI_API_KEY\n│\n└── \"one-api-...(max_token=...)\" 用一种更方便的方式接入one-api多模型管理界面\n    ├── AVAIL_LLM_MODELS\n    ├── API_KEY\n    └── API_URL_REDIRECT\n\n\n本地大模型示意图\n│\n├── \"chatglm4\"\n├── \"chatglm3\"\n├── \"chatglm\"\n├── \"chatglm_onnx\"\n├── \"chatglmft\"\n├── \"internlm\"\n├── \"moss\"\n├── \"jittorllms_pangualpha\"\n├── \"jittorllms_llama\"\n├── \"deepseekcoder\"\n├── \"qwen-local\"\n├──  RWKV的支持见Wiki\n└── \"llama2\"\n\n\n用户图形界面布局依赖关系示意图\n│\n├── CHATBOT_HEIGHT 对话窗的高度\n├── CODE_HIGHLIGHT 代码高亮\n├── LAYOUT 窗口布局\n├── DARK_MODE 暗色模式 / 亮色模式\n├── DEFAULT_FN_GROUPS 插件分类默认选项\n├── THEME 色彩主题\n├── AUTO_CLEAR_TXT 是否在提交时自动清空输入框\n├── ADD_WAIFU 加一个live2d装饰\n└── ALLOW_RESET_CONFIG 是否允许通过自然语言描述修改本页的配置，该功能具有一定的危险性\n\n\n插件在线服务配置依赖关系示意图\n│\n├── 互联网检索\n│   └── SEARXNG_URLS\n│\n├── 语音功能\n│   ├── ENABLE_AUDIO\n│   ├── ALIYUN_TOKEN\n│   ├── ALIYUN_APPKEY\n│   ├── ALIYUN_ACCESSKEY\n│   └── ALIYUN_SECRET\n│\n└── PDF文档精准解析\n    ├── GROBID_URLS\n    ├── MATHPIX_APPID\n    └── MATHPIX_APPKEY\n\n\n\"\"\"\n"
        },
        {
          "name": "core_functional.py",
          "type": "blob",
          "size": 9.466796875,
          "content": "# 'primary' 颜色对应 theme.py 中的 primary_hue\n# 'secondary' 颜色对应 theme.py 中的 neutral_hue\n# 'stop' 颜色对应 theme.py 中的 color_er\nimport importlib\nfrom toolbox import clear_line_break\nfrom toolbox import apply_gpt_academic_string_mask_langbased\nfrom toolbox import build_gpt_academic_masked_string_langbased\nfrom textwrap import dedent\n\ndef get_core_functions():\n    return {\n\n        \"学术语料润色\": {\n            # [1*] 前缀字符串，会被加在你的输入之前。例如，用来描述你的要求，例如翻译、解释代码、润色等等。\n            #      这里填一个提示词字符串就行了，这里为了区分中英文情景搞复杂了一点\n            \"Prefix\":   build_gpt_academic_masked_string_langbased(\n                            text_show_english=\n                                r\"Below is a paragraph from an academic paper. Polish the writing to meet the academic style, \"\n                                r\"improve the spelling, grammar, clarity, concision and overall readability. When necessary, rewrite the whole sentence. \"\n                                r\"Firstly, you should provide the polished paragraph (in English). \"\n                                r\"Secondly, you should list all your modification and explain the reasons to do so in markdown table.\",\n                            text_show_chinese=\n                                r\"作为一名中文学术论文写作改进助理，你的任务是改进所提供文本的拼写、语法、清晰、简洁和整体可读性，\"\n                                r\"同时分解长句，减少重复，并提供改进建议。请先提供文本的更正版本，然后在markdown表格中列出修改的内容，并给出修改的理由:\"\n                        ) + \"\\n\\n\",\n            # [2*] 后缀字符串，会被加在你的输入之后。例如，配合前缀可以把你的输入内容用引号圈起来\n            \"Suffix\":   r\"\",\n            # [3] 按钮颜色 (可选参数，默认 secondary)\n            \"Color\":    r\"secondary\",\n            # [4] 按钮是否可见 (可选参数，默认 True，即可见)\n            \"Visible\": True,\n            # [5] 是否在触发时清除历史 (可选参数，默认 False，即不处理之前的对话历史)\n            \"AutoClearHistory\": False,\n            # [6] 文本预处理 （可选参数，默认 None，举例：写个函数移除所有的换行符）\n            \"PreProcess\": None,\n            # [7] 模型选择 （可选参数。如不设置，则使用当前全局模型；如设置，则用指定模型覆盖全局模型。）\n            # \"ModelOverride\": \"gpt-3.5-turbo\", # 主要用途：强制点击此基础功能按钮时，使用指定的模型。\n        },\n\n\n        \"总结绘制脑图\": {\n            # 前缀，会被加在你的输入之前。例如，用来描述你的要求，例如翻译、解释代码、润色等等\n            \"Prefix\":   '''\"\"\"\\n\\n''',\n            # 后缀，会被加在你的输入之后。例如，配合前缀可以把你的输入内容用引号圈起来\n            \"Suffix\":\n                # dedent() 函数用于去除多行字符串的缩进\n                dedent(\"\\n\\n\"+r'''\n                    \"\"\"\n\n                    使用mermaid flowchart对以上文本进行总结，概括上述段落的内容以及内在逻辑关系，例如：\n\n                    以下是对以上文本的总结，以mermaid flowchart的形式展示：\n                    ```mermaid\n                    flowchart LR\n                        A[\"节点名1\"] --> B(\"节点名2\")\n                        B --> C{\"节点名3\"}\n                        C --> D[\"节点名4\"]\n                        C --> |\"箭头名1\"| E[\"节点名5\"]\n                        C --> |\"箭头名2\"| F[\"节点名6\"]\n                    ```\n\n                    注意：\n                    （1）使用中文\n                    （2）节点名字使用引号包裹，如[\"Laptop\"]\n                    （3）`|` 和 `\"`之间不要存在空格\n                    （4）根据情况选择flowchart LR（从左到右）或者flowchart TD（从上到下）\n                '''),\n        },\n\n\n        \"查找语法错误\": {\n            \"Prefix\":   r\"Help me ensure that the grammar and the spelling is correct. \"\n                        r\"Do not try to polish the text, if no mistake is found, tell me that this paragraph is good. \"\n                        r\"If you find grammar or spelling mistakes, please list mistakes you find in a two-column markdown table, \"\n                        r\"put the original text the first column, \"\n                        r\"put the corrected text in the second column and highlight the key words you fixed. \"\n                        r\"Finally, please provide the proofreaded text.\"\"\\n\\n\"\n                        r\"Example:\"\"\\n\"\n                        r\"Paragraph: How is you? Do you knows what is it?\"\"\\n\"\n                        r\"| Original sentence | Corrected sentence |\"\"\\n\"\n                        r\"| :--- | :--- |\"\"\\n\"\n                        r\"| How **is** you? | How **are** you? |\"\"\\n\"\n                        r\"| Do you **knows** what **is** **it**? | Do you **know** what **it** **is** ? |\"\"\\n\\n\"\n                        r\"Below is a paragraph from an academic paper. \"\n                        r\"You need to report all grammar and spelling mistakes as the example before.\"\n                        + \"\\n\\n\",\n            \"Suffix\":   r\"\",\n            \"PreProcess\": clear_line_break,    # 预处理：清除换行符\n        },\n\n\n        \"中译英\": {\n            \"Prefix\":   r\"Please translate following sentence to English:\" + \"\\n\\n\",\n            \"Suffix\":   r\"\",\n        },\n\n\n        \"学术英中互译\": {\n            \"Prefix\":   build_gpt_academic_masked_string_langbased(\n                            text_show_chinese=\n                                r\"I want you to act as a scientific English-Chinese translator, \"\n                                r\"I will provide you with some paragraphs in one language \"\n                                r\"and your task is to accurately and academically translate the paragraphs only into the other language. \"\n                                r\"Do not repeat the original provided paragraphs after translation. \"\n                                r\"You should use artificial intelligence tools, \"\n                                r\"such as natural language processing, and rhetorical knowledge \"\n                                r\"and experience about effective writing techniques to reply. \"\n                                r\"I'll give you my paragraphs as follows, tell me what language it is written in, and then translate:\",\n                            text_show_english=\n                                r\"你是经验丰富的翻译，请把以下学术文章段落翻译成中文，\"\n                                r\"并同时充分考虑中文的语法、清晰、简洁和整体可读性，\"\n                                r\"必要时，你可以修改整个句子的顺序以确保翻译后的段落符合中文的语言习惯。\"\n                                r\"你需要翻译的文本如下：\"\n                        ) + \"\\n\\n\",\n            \"Suffix\":   r\"\",\n        },\n\n\n        \"英译中\": {\n            \"Prefix\":   r\"翻译成地道的中文：\" + \"\\n\\n\",\n            \"Suffix\":   r\"\",\n            \"Visible\":  False,\n        },\n\n\n        \"找图片\": {\n            \"Prefix\":   r\"我需要你找一张网络图片。使用Unsplash API(https://source.unsplash.com/960x640/?<英语关键词>)获取图片URL，\"\n                        r\"然后请使用Markdown格式封装，并且不要有反斜线，不要用代码块。现在，请按以下描述给我发送图片：\" + \"\\n\\n\",\n            \"Suffix\":   r\"\",\n            \"Visible\":  False,\n        },\n\n\n        \"解释代码\": {\n            \"Prefix\":   r\"请解释以下代码：\" + \"\\n```\\n\",\n            \"Suffix\":   \"\\n```\\n\",\n        },\n\n\n        \"参考文献转Bib\": {\n            \"Prefix\":   r\"Here are some bibliography items, please transform them into bibtex style.\"\n                        r\"Note that, reference styles maybe more than one kind, you should transform each item correctly.\"\n                        r\"Items need to be transformed:\" + \"\\n\\n\",\n            \"Visible\":  False,\n            \"Suffix\":   r\"\",\n        }\n    }\n\n\ndef handle_core_functionality(additional_fn, inputs, history, chatbot):\n    import core_functional\n    importlib.reload(core_functional)    # 热更新prompt\n    core_functional = core_functional.get_core_functions()\n    addition = chatbot._cookies['customize_fn_overwrite']\n    if additional_fn in addition:\n        # 自定义功能\n        inputs = addition[additional_fn][\"Prefix\"] + inputs + addition[additional_fn][\"Suffix\"]\n        return inputs, history\n    else:\n        # 预制功能\n        if \"PreProcess\" in core_functional[additional_fn]:\n            if core_functional[additional_fn][\"PreProcess\"] is not None:\n                inputs = core_functional[additional_fn][\"PreProcess\"](inputs)  # 获取预处理函数（如果有的话）\n        # 为字符串加上上面定义的前缀和后缀。\n        inputs = apply_gpt_academic_string_mask_langbased(\n            string = core_functional[additional_fn][\"Prefix\"] + inputs + core_functional[additional_fn][\"Suffix\"],\n            lang_reference = inputs,\n        )\n        if core_functional[additional_fn].get(\"AutoClearHistory\", False):\n            history = []\n        return inputs, history\n\nif __name__ == \"__main__\":\n    t = get_core_functions()[\"总结绘制脑图\"]\n    print(t[\"Prefix\"] + t[\"Suffix\"])"
        },
        {
          "name": "crazy_functional.py",
          "type": "blob",
          "size": 33.3310546875,
          "content": "from toolbox import HotReload  # HotReload 的意思是热更新，修改函数插件后，不需要重启程序，代码直接生效\nfrom toolbox import trimmed_format_exc\nfrom loguru import logger\n\ndef get_crazy_functions():\n    from crazy_functions.读文章写摘要 import 读文章写摘要\n    from crazy_functions.生成函数注释 import 批量生成函数注释\n    from crazy_functions.SourceCode_Analyse import 解析项目本身\n    from crazy_functions.SourceCode_Analyse import 解析一个Python项目\n    from crazy_functions.SourceCode_Analyse import 解析一个Matlab项目\n    from crazy_functions.SourceCode_Analyse import 解析一个C项目的头文件\n    from crazy_functions.SourceCode_Analyse import 解析一个C项目\n    from crazy_functions.SourceCode_Analyse import 解析一个Golang项目\n    from crazy_functions.SourceCode_Analyse import 解析一个Rust项目\n    from crazy_functions.SourceCode_Analyse import 解析一个Java项目\n    from crazy_functions.SourceCode_Analyse import 解析一个前端项目\n    from crazy_functions.高级功能函数模板 import 高阶功能模板函数\n    from crazy_functions.高级功能函数模板 import Demo_Wrap\n    from crazy_functions.Latex_Project_Polish import Latex英文润色\n    from crazy_functions.询问多个大语言模型 import 同时问询\n    from crazy_functions.SourceCode_Analyse import 解析一个Lua项目\n    from crazy_functions.SourceCode_Analyse import 解析一个CSharp项目\n    from crazy_functions.总结word文档 import 总结word文档\n    from crazy_functions.解析JupyterNotebook import 解析ipynb文件\n    from crazy_functions.Conversation_To_File import 载入对话历史存档\n    from crazy_functions.Conversation_To_File import 对话历史存档\n    from crazy_functions.Conversation_To_File import Conversation_To_File_Wrap\n    from crazy_functions.Conversation_To_File import 删除所有本地对话历史记录\n    from crazy_functions.辅助功能 import 清除缓存\n    from crazy_functions.Markdown_Translate import Markdown英译中\n    from crazy_functions.批量总结PDF文档 import 批量总结PDF文档\n    from crazy_functions.PDF_Translate import 批量翻译PDF文档\n    from crazy_functions.谷歌检索小助手 import 谷歌检索小助手\n    from crazy_functions.理解PDF文档内容 import 理解PDF文档内容标准文件输入\n    from crazy_functions.Latex_Project_Polish import Latex中文润色\n    from crazy_functions.Latex_Project_Polish import Latex英文纠错\n    from crazy_functions.Markdown_Translate import Markdown中译英\n    from crazy_functions.虚空终端 import 虚空终端\n    from crazy_functions.生成多种Mermaid图表 import Mermaid_Gen\n    from crazy_functions.PDF_Translate_Wrap import PDF_Tran\n    from crazy_functions.Latex_Function import Latex英文纠错加PDF对比\n    from crazy_functions.Latex_Function import Latex翻译中文并重新编译PDF\n    from crazy_functions.Latex_Function import PDF翻译中文并重新编译PDF\n    from crazy_functions.Latex_Function_Wrap import Arxiv_Localize\n    from crazy_functions.Latex_Function_Wrap import PDF_Localize\n    from crazy_functions.Internet_GPT import 连接网络回答问题\n    from crazy_functions.Internet_GPT_Wrap import NetworkGPT_Wrap\n    from crazy_functions.Image_Generate import 图片生成_DALLE2, 图片生成_DALLE3, 图片修改_DALLE2\n    from crazy_functions.Image_Generate_Wrap import ImageGen_Wrap\n    from crazy_functions.SourceCode_Comment import 注释Python项目\n    from crazy_functions.SourceCode_Comment_Wrap import SourceCodeComment_Wrap\n    from crazy_functions.VideoResource_GPT import 多媒体任务\n\n    function_plugins = {\n        \"多媒体智能体\": {\n            \"Group\": \"智能体\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"【仅测试】多媒体任务\",\n            \"Function\": HotReload(多媒体任务),\n        },\n        \"虚空终端\": {\n            \"Group\": \"对话|编程|学术|智能体\",\n            \"Color\": \"stop\",\n            \"AsButton\": True,\n            \"Info\": \"使用自然语言实现您的想法\",\n            \"Function\": HotReload(虚空终端),\n        },\n        \"解析整个Python项目\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": True,\n            \"Info\": \"解析一个Python项目的所有源文件(.py) | 输入参数为路径\",\n            \"Function\": HotReload(解析一个Python项目),\n        },\n        \"注释Python项目\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"上传一系列python源文件(或者压缩包), 为这些代码添加docstring | 输入参数为路径\",\n            \"Function\": HotReload(注释Python项目),\n            \"Class\": SourceCodeComment_Wrap,\n        },\n        \"载入对话历史存档（先上传存档或输入路径）\": {\n            \"Group\": \"对话\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"载入对话历史存档 | 输入参数为路径\",\n            \"Function\": HotReload(载入对话历史存档),\n        },\n        \"删除所有本地对话历史记录（谨慎操作）\": {\n            \"Group\": \"对话\",\n            \"AsButton\": False,\n            \"Info\": \"删除所有本地对话历史记录，谨慎操作 | 不需要输入参数\",\n            \"Function\": HotReload(删除所有本地对话历史记录),\n        },\n        \"清除所有缓存文件（谨慎操作）\": {\n            \"Group\": \"对话\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"清除所有缓存文件，谨慎操作 | 不需要输入参数\",\n            \"Function\": HotReload(清除缓存),\n        },\n        \"生成多种Mermaid图表(从当前对话或路径(.pdf/.md/.docx)中生产图表）\": {\n            \"Group\": \"对话\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\" : \"基于当前对话或文件生成多种Mermaid图表,图表类型由模型判断\",\n            \"Function\": None,\n            \"Class\": Mermaid_Gen\n        },\n        \"Arxiv论文翻译\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": True,\n            \"Info\": \"Arixv论文精细翻译 | 输入参数arxiv论文的ID，比如1812.10695\",\n            \"Function\": HotReload(Latex翻译中文并重新编译PDF),  # 当注册Class后，Function旧接口仅会在“虚空终端”中起作用\n            \"Class\": Arxiv_Localize,    # 新一代插件需要注册Class\n        },\n        \"批量总结Word文档\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"批量总结word文档 | 输入参数为路径\",\n            \"Function\": HotReload(总结word文档),\n        },\n        \"解析整个Matlab项目\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"解析一个Matlab项目的所有源文件(.m) | 输入参数为路径\",\n            \"Function\": HotReload(解析一个Matlab项目),\n        },\n        \"解析整个C++项目头文件\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"解析一个C++项目的所有头文件(.h/.hpp) | 输入参数为路径\",\n            \"Function\": HotReload(解析一个C项目的头文件),\n        },\n        \"解析整个C++项目（.cpp/.hpp/.c/.h）\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"解析一个C++项目的所有源文件（.cpp/.hpp/.c/.h）| 输入参数为路径\",\n            \"Function\": HotReload(解析一个C项目),\n        },\n        \"解析整个Go项目\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"解析一个Go项目的所有源文件 | 输入参数为路径\",\n            \"Function\": HotReload(解析一个Golang项目),\n        },\n        \"解析整个Rust项目\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"解析一个Rust项目的所有源文件 | 输入参数为路径\",\n            \"Function\": HotReload(解析一个Rust项目),\n        },\n        \"解析整个Java项目\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"解析一个Java项目的所有源文件 | 输入参数为路径\",\n            \"Function\": HotReload(解析一个Java项目),\n        },\n        \"解析整个前端项目（js,ts,css等）\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"解析一个前端项目的所有源文件（js,ts,css等） | 输入参数为路径\",\n            \"Function\": HotReload(解析一个前端项目),\n        },\n        \"解析整个Lua项目\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"解析一个Lua项目的所有源文件 | 输入参数为路径\",\n            \"Function\": HotReload(解析一个Lua项目),\n        },\n        \"解析整个CSharp项目\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"解析一个CSharp项目的所有源文件 | 输入参数为路径\",\n            \"Function\": HotReload(解析一个CSharp项目),\n        },\n        \"解析Jupyter Notebook文件\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"解析Jupyter Notebook文件 | 输入参数为路径\",\n            \"Function\": HotReload(解析ipynb文件),\n            \"AdvancedArgs\": True,  # 调用时，唤起高级参数输入区（默认False）\n            \"ArgsReminder\": \"若输入0，则不解析notebook中的Markdown块\",  # 高级参数输入区的显示提示\n        },\n        \"读Tex论文写摘要\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"读取Tex论文并写摘要 | 输入参数为路径\",\n            \"Function\": HotReload(读文章写摘要),\n        },\n        \"翻译README或MD\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": True,\n            \"Info\": \"将Markdown翻译为中文 | 输入参数为路径或URL\",\n            \"Function\": HotReload(Markdown英译中),\n        },\n        \"翻译Markdown或README（支持Github链接）\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"将Markdown或README翻译为中文 | 输入参数为路径或URL\",\n            \"Function\": HotReload(Markdown英译中),\n        },\n        \"批量生成函数注释\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"批量生成函数的注释 | 输入参数为路径\",\n            \"Function\": HotReload(批量生成函数注释),\n        },\n        \"保存当前的对话\": {\n            \"Group\": \"对话\",\n            \"Color\": \"stop\",\n            \"AsButton\": True,\n            \"Info\": \"保存当前的对话 | 不需要输入参数\",\n            \"Function\": HotReload(对话历史存档),    # 当注册Class后，Function旧接口仅会在“虚空终端”中起作用\n            \"Class\": Conversation_To_File_Wrap     # 新一代插件需要注册Class\n        },\n        \"[多线程Demo]解析此项目本身（源码自译解）\": {\n            \"Group\": \"对话|编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"多线程解析并翻译此项目的源码 | 不需要输入参数\",\n            \"Function\": HotReload(解析项目本身),\n        },\n        \"查互联网后回答\": {\n            \"Group\": \"对话\",\n            \"Color\": \"stop\",\n            \"AsButton\": True,  # 加入下拉菜单中\n            # \"Info\": \"连接网络回答问题（需要访问谷歌）| 输入参数是一个问题\",\n            \"Function\": HotReload(连接网络回答问题),\n            \"Class\": NetworkGPT_Wrap     # 新一代插件需要注册Class\n        },\n        \"历史上的今天\": {\n            \"Group\": \"对话\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"Info\": \"查看历史上的今天事件 (这是一个面向开发者的插件Demo) | 不需要输入参数\",\n            \"Function\": None,\n            \"Class\": Demo_Wrap, # 新一代插件需要注册Class\n        },\n        \"精准翻译PDF论文\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": True,\n            \"Info\": \"精准翻译PDF论文为中文 | 输入参数为路径\",\n            \"Function\": HotReload(批量翻译PDF文档), # 当注册Class后，Function旧接口仅会在“虚空终端”中起作用\n            \"Class\": PDF_Tran,  # 新一代插件需要注册Class\n        },\n        \"询问多个GPT模型\": {\n            \"Group\": \"对话\",\n            \"Color\": \"stop\",\n            \"AsButton\": True,\n            \"Function\": HotReload(同时问询),\n        },\n        \"批量总结PDF文档\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"批量总结PDF文档的内容 | 输入参数为路径\",\n            \"Function\": HotReload(批量总结PDF文档),\n        },\n        \"谷歌学术检索助手（输入谷歌学术搜索页url）\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"使用谷歌学术检索助手搜索指定URL的结果 | 输入参数为谷歌学术搜索页的URL\",\n            \"Function\": HotReload(谷歌检索小助手),\n        },\n        \"理解PDF文档内容 （模仿ChatPDF）\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"理解PDF文档的内容并进行回答 | 输入参数为路径\",\n            \"Function\": HotReload(理解PDF文档内容标准文件输入),\n        },\n        \"英文Latex项目全文润色（输入路径或上传压缩包）\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"对英文Latex项目全文进行润色处理 | 输入参数为路径或上传压缩包\",\n            \"Function\": HotReload(Latex英文润色),\n        },\n\n        \"中文Latex项目全文润色（输入路径或上传压缩包）\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"对中文Latex项目全文进行润色处理 | 输入参数为路径或上传压缩包\",\n            \"Function\": HotReload(Latex中文润色),\n        },\n        # 已经被新插件取代\n        # \"英文Latex项目全文纠错（输入路径或上传压缩包）\": {\n        #     \"Group\": \"学术\",\n        #     \"Color\": \"stop\",\n        #     \"AsButton\": False,  # 加入下拉菜单中\n        #     \"Info\": \"对英文Latex项目全文进行纠错处理 | 输入参数为路径或上传压缩包\",\n        #     \"Function\": HotReload(Latex英文纠错),\n        # },\n        # 已经被新插件取代\n        # \"Latex项目全文中译英（输入路径或上传压缩包）\": {\n        #     \"Group\": \"学术\",\n        #     \"Color\": \"stop\",\n        #     \"AsButton\": False,  # 加入下拉菜单中\n        #     \"Info\": \"对Latex项目全文进行中译英处理 | 输入参数为路径或上传压缩包\",\n        #     \"Function\": HotReload(Latex中译英)\n        # },\n        # 已经被新插件取代\n        # \"Latex项目全文英译中（输入路径或上传压缩包）\": {\n        #     \"Group\": \"学术\",\n        #     \"Color\": \"stop\",\n        #     \"AsButton\": False,  # 加入下拉菜单中\n        #     \"Info\": \"对Latex项目全文进行英译中处理 | 输入参数为路径或上传压缩包\",\n        #     \"Function\": HotReload(Latex英译中)\n        # },\n        \"批量Markdown中译英（输入路径或上传压缩包）\": {\n            \"Group\": \"编程\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,  # 加入下拉菜单中\n            \"Info\": \"批量将Markdown文件中文翻译为英文 | 输入参数为路径或上传压缩包\",\n            \"Function\": HotReload(Markdown中译英),\n        },\n        \"Latex英文纠错+高亮修正位置 [需Latex]\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"AdvancedArgs\": True,\n            \"ArgsReminder\": \"如果有必要, 请在此处追加更细致的矫错指令（使用英文）。\",\n            \"Function\": HotReload(Latex英文纠错加PDF对比),\n        },\n        \"📚Arxiv论文精细翻译（输入arxivID）[需Latex]\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"AdvancedArgs\": True,\n            \"ArgsReminder\": r\"如果有必要, 请在此处给出自定义翻译命令, 解决部分词汇翻译不准确的问题。 \"\n                            r\"例如当单词'agent'翻译不准确时, 请尝试把以下指令复制到高级参数区: \"\n                            r'If the term \"agent\" is used in this section, it should be translated to \"智能体\". ',\n            \"Info\": \"Arixv论文精细翻译 | 输入参数arxiv论文的ID，比如1812.10695\",\n            \"Function\": HotReload(Latex翻译中文并重新编译PDF),  # 当注册Class后，Function旧接口仅会在“虚空终端”中起作用\n            \"Class\": Arxiv_Localize,    # 新一代插件需要注册Class\n        },\n        \"📚本地Latex论文精细翻译（上传Latex项目）[需Latex]\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"AdvancedArgs\": True,\n            \"ArgsReminder\": r\"如果有必要, 请在此处给出自定义翻译命令, 解决部分词汇翻译不准确的问题。 \"\n                            r\"例如当单词'agent'翻译不准确时, 请尝试把以下指令复制到高级参数区: \"\n                            r'If the term \"agent\" is used in this section, it should be translated to \"智能体\". ',\n            \"Info\": \"本地Latex论文精细翻译 | 输入参数是路径\",\n            \"Function\": HotReload(Latex翻译中文并重新编译PDF),\n        },\n        \"PDF翻译中文并重新编译PDF（上传PDF）[需Latex]\": {\n            \"Group\": \"学术\",\n            \"Color\": \"stop\",\n            \"AsButton\": False,\n            \"AdvancedArgs\": True,\n            \"ArgsReminder\": r\"如果有必要, 请在此处给出自定义翻译命令, 解决部分词汇翻译不准确的问题。 \"\n                            r\"例如当单词'agent'翻译不准确时, 请尝试把以下指令复制到高级参数区: \"\n                            r'If the term \"agent\" is used in this section, it should be translated to \"智能体\". ',\n            \"Info\": \"PDF翻译中文，并重新编译PDF | 输入参数为路径\",\n            \"Function\": HotReload(PDF翻译中文并重新编译PDF),   # 当注册Class后，Function旧接口仅会在“虚空终端”中起作用\n            \"Class\": PDF_Localize   # 新一代插件需要注册Class\n        }\n    }\n\n    function_plugins.update(\n        {\n            \"🎨图片生成（DALLE2/DALLE3, 使用前切换到GPT系列模型）\": {\n                \"Group\": \"对话\",\n                \"Color\": \"stop\",\n                \"AsButton\": False,\n                \"Info\": \"使用 DALLE2/DALLE3 生成图片 | 输入参数字符串，提供图像的内容\",\n                \"Function\": HotReload(图片生成_DALLE2),   # 当注册Class后，Function旧接口仅会在“虚空终端”中起作用\n                \"Class\": ImageGen_Wrap  # 新一代插件需要注册Class\n            },\n        }\n    )\n\n    function_plugins.update(\n        {\n            \"🎨图片修改_DALLE2 （使用前请切换模型到GPT系列）\": {\n                \"Group\": \"对话\",\n                \"Color\": \"stop\",\n                \"AsButton\": False,\n                \"AdvancedArgs\": False,  # 调用时，唤起高级参数输入区（默认False）\n                # \"Info\": \"使用DALLE2修改图片 | 输入参数字符串，提供图像的内容\",\n                \"Function\": HotReload(图片修改_DALLE2),\n            },\n        }\n    )\n\n\n\n\n\n\n\n\n\n    # -=--=- 尚未充分测试的实验性插件 & 需要额外依赖的插件 -=--=-\n    try:\n        from crazy_functions.下载arxiv论文翻译摘要 import 下载arxiv论文并翻译摘要\n\n        function_plugins.update(\n            {\n                \"一键下载arxiv论文并翻译摘要（先在input输入编号，如1812.10695）\": {\n                    \"Group\": \"学术\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,  # 加入下拉菜单中\n                    # \"Info\": \"下载arxiv论文并翻译摘要 | 输入参数为arxiv编号如1812.10695\",\n                    \"Function\": HotReload(下载arxiv论文并翻译摘要),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    # try:\n    #     from crazy_functions.联网的ChatGPT import 连接网络回答问题\n\n    #     function_plugins.update(\n    #         {\n    #             \"连接网络回答问题（输入问题后点击该插件，需要访问谷歌）\": {\n    #                 \"Group\": \"对话\",\n    #                 \"Color\": \"stop\",\n    #                 \"AsButton\": False,  # 加入下拉菜单中\n    #                 # \"Info\": \"连接网络回答问题（需要访问谷歌）| 输入参数是一个问题\",\n    #                 \"Function\": HotReload(连接网络回答问题),\n    #             }\n    #         }\n    #     )\n    #     from crazy_functions.联网的ChatGPT_bing版 import 连接bing搜索回答问题\n\n    #     function_plugins.update(\n    #         {\n    #             \"连接网络回答问题（中文Bing版，输入问题后点击该插件）\": {\n    #                 \"Group\": \"对话\",\n    #                 \"Color\": \"stop\",\n    #                 \"AsButton\": False,  # 加入下拉菜单中\n    #                 \"Info\": \"连接网络回答问题（需要访问中文Bing）| 输入参数是一个问题\",\n    #                 \"Function\": HotReload(连接bing搜索回答问题),\n    #             }\n    #         }\n    #     )\n    # except:\n    #     logger.error(trimmed_format_exc())\n    #     logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.SourceCode_Analyse import 解析任意code项目\n\n        function_plugins.update(\n            {\n                \"解析项目源代码（手动指定和筛选源代码文件类型）\": {\n                    \"Group\": \"编程\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"AdvancedArgs\": True,  # 调用时，唤起高级参数输入区（默认False）\n                    \"ArgsReminder\": '输入时用逗号隔开, *代表通配符, 加了^代表不匹配; 不输入代表全部匹配。例如: \"*.c, ^*.cpp, config.toml, ^*.toml\"',  # 高级参数输入区的显示提示\n                    \"Function\": HotReload(解析任意code项目),\n                },\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.询问多个大语言模型 import 同时问询_指定模型\n\n        function_plugins.update(\n            {\n                \"询问多个GPT模型（手动指定询问哪些模型）\": {\n                    \"Group\": \"对话\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"AdvancedArgs\": True,  # 调用时，唤起高级参数输入区（默认False）\n                    \"ArgsReminder\": \"支持任意数量的llm接口，用&符号分隔。例如chatglm&gpt-3.5-turbo&gpt-4\",  # 高级参数输入区的显示提示\n                    \"Function\": HotReload(同时问询_指定模型),\n                },\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n\n\n    try:\n        from crazy_functions.总结音视频 import 总结音视频\n\n        function_plugins.update(\n            {\n                \"批量总结音视频（输入路径或上传压缩包）\": {\n                    \"Group\": \"对话\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"AdvancedArgs\": True,\n                    \"ArgsReminder\": \"调用openai api 使用whisper-1模型, 目前支持的格式:mp4, m4a, wav, mpga, mpeg, mp3。此处可以输入解析提示，例如：解析为简体中文（默认）。\",\n                    \"Info\": \"批量总结音频或视频 | 输入参数为路径\",\n                    \"Function\": HotReload(总结音视频),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.数学动画生成manim import 动画生成\n\n        function_plugins.update(\n            {\n                \"数学动画生成（Manim）\": {\n                    \"Group\": \"对话\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"Info\": \"按照自然语言描述生成一个动画 | 输入参数是一段话\",\n                    \"Function\": HotReload(动画生成),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.Markdown_Translate import Markdown翻译指定语言\n\n        function_plugins.update(\n            {\n                \"Markdown翻译（指定翻译成何种语言）\": {\n                    \"Group\": \"编程\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"AdvancedArgs\": True,\n                    \"ArgsReminder\": \"请输入要翻译成哪种语言，默认为Chinese。\",\n                    \"Function\": HotReload(Markdown翻译指定语言),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.知识库问答 import 知识库文件注入\n\n        function_plugins.update(\n            {\n                \"构建知识库（先上传文件素材,再运行此插件）\": {\n                    \"Group\": \"对话\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"AdvancedArgs\": True,\n                    \"ArgsReminder\": \"此处待注入的知识库名称id, 默认为default。文件进入知识库后可长期保存。可以通过再次调用本插件的方式，向知识库追加更多文档。\",\n                    \"Function\": HotReload(知识库文件注入),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.知识库问答 import 读取知识库作答\n\n        function_plugins.update(\n            {\n                \"知识库文件注入（构建知识库后,再运行此插件）\": {\n                    \"Group\": \"对话\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"AdvancedArgs\": True,\n                    \"ArgsReminder\": \"待提取的知识库名称id, 默认为default, 您需要构建知识库后再运行此插件。\",\n                    \"Function\": HotReload(读取知识库作答),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.交互功能函数模板 import 交互功能模板函数\n\n        function_plugins.update(\n            {\n                \"交互功能模板Demo函数（查找wallhaven.cc的壁纸）\": {\n                    \"Group\": \"对话\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"Function\": HotReload(交互功能模板函数),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n\n    try:\n        from toolbox import get_conf\n\n        ENABLE_AUDIO = get_conf(\"ENABLE_AUDIO\")\n        if ENABLE_AUDIO:\n            from crazy_functions.语音助手 import 语音助手\n\n            function_plugins.update(\n                {\n                    \"实时语音对话\": {\n                        \"Group\": \"对话\",\n                        \"Color\": \"stop\",\n                        \"AsButton\": True,\n                        \"Info\": \"这是一个时刻聆听着的语音对话助手 | 没有输入参数\",\n                        \"Function\": HotReload(语音助手),\n                    }\n                }\n            )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.批量翻译PDF文档_NOUGAT import 批量翻译PDF文档\n\n        function_plugins.update(\n            {\n                \"精准翻译PDF文档（NOUGAT）\": {\n                    \"Group\": \"学术\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"Function\": HotReload(批量翻译PDF文档),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.函数动态生成 import 函数动态生成\n\n        function_plugins.update(\n            {\n                \"动态代码解释器（CodeInterpreter）\": {\n                    \"Group\": \"智能体\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"Function\": HotReload(函数动态生成),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.多智能体 import 多智能体终端\n\n        function_plugins.update(\n            {\n                \"AutoGen多智能体终端（仅供测试）\": {\n                    \"Group\": \"智能体\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"Function\": HotReload(多智能体终端),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.互动小游戏 import 随机小游戏\n\n        function_plugins.update(\n            {\n                \"随机互动小游戏（仅供测试）\": {\n                    \"Group\": \"智能体\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"Function\": HotReload(随机小游戏),\n                }\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n    try:\n        from crazy_functions.Rag_Interface import Rag问答\n\n        function_plugins.update(\n            {\n                \"Rag智能召回\": {\n                    \"Group\": \"对话\",\n                    \"Color\": \"stop\",\n                    \"AsButton\": False,\n                    \"Info\": \"将问答数据记录到向量库中，作为长期参考。\",\n                    \"Function\": HotReload(Rag问答),\n                },\n            }\n        )\n    except:\n        logger.error(trimmed_format_exc())\n        logger.error(\"Load function plugin failed\")\n\n\n    # try:\n    #     from crazy_functions.高级功能函数模板 import 测试图表渲染\n    #     function_plugins.update({\n    #         \"绘制逻辑关系（测试图表渲染）\": {\n    #             \"Group\": \"智能体\",\n    #             \"Color\": \"stop\",\n    #             \"AsButton\": True,\n    #             \"Function\": HotReload(测试图表渲染)\n    #         }\n    #     })\n    # except:\n    #     logger.error(trimmed_format_exc())\n    #     print('Load function plugin failed')\n\n\n    \"\"\"\n    设置默认值:\n    - 默认 Group = 对话\n    - 默认 AsButton = True\n    - 默认 AdvancedArgs = False\n    - 默认 Color = secondary\n    \"\"\"\n    for name, function_meta in function_plugins.items():\n        if \"Group\" not in function_meta:\n            function_plugins[name][\"Group\"] = \"对话\"\n        if \"AsButton\" not in function_meta:\n            function_plugins[name][\"AsButton\"] = True\n        if \"AdvancedArgs\" not in function_meta:\n            function_plugins[name][\"AdvancedArgs\"] = False\n        if \"Color\" not in function_meta:\n            function_plugins[name][\"Color\"] = \"secondary\"\n\n    return function_plugins\n\n\n\n\ndef get_multiplex_button_functions():\n    \"\"\"多路复用主提交按钮的功能映射\n    \"\"\"\n    return {\n        \"常规对话\":\n            \"\",\n\n        \"多模型对话\": \n            \"询问多个GPT模型\", # 映射到上面的 `询问多个GPT模型` 插件\n\n        \"智能召回 RAG\": \n            \"Rag智能召回\", # 映射到上面的 `Rag智能召回` 插件\n\n        \"多媒体查询\": \n            \"多媒体智能体\", # 映射到上面的 `多媒体智能体` 插件\n    }\n"
        },
        {
          "name": "crazy_functions",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 14.9375,
          "content": "## ===================================================\n#                docker-compose.yml\n## ===================================================\n# 1. 请在以下方案中选择任意一种，然后删除其他的方案\n# 2. 修改你选择的方案中的environment环境变量，详情请见github wiki或者config.py\n# 3. 选择一种暴露服务端口的方法，并对相应的配置做出修改：\n    # 「方法1: 适用于Linux，很方便，可惜windows不支持」与宿主的网络融合为一体，这个是默认配置\n    # network_mode: \"host\"\n    # 「方法2: 适用于所有系统包括Windows和MacOS」端口映射，把容器的端口映射到宿主的端口（注意您需要先删除network_mode: \"host\"，再追加以下内容）\n    # ports:\n    #   - \"12345:12345\"  # 注意！12345必须与WEB_PORT环境变量相互对应\n# 4. 最后`docker-compose up`运行\n# 5. 如果希望使用显卡，请关注 LOCAL_MODEL_DEVICE 和 英伟达显卡运行时 选项\n## ===================================================\n# 1. Please choose one of the following options and delete the others.\n# 2. Modify the environment variables in the selected option, see GitHub wiki or config.py for more details.\n# 3. Choose a method to expose the server port and make the corresponding configuration changes:\n    # [Method 1: Suitable for Linux, convenient, but not supported for Windows] Fusion with the host network, this is the default configuration\n    # network_mode: \"host\"\n    # [Method 2: Suitable for all systems including Windows and MacOS] Port mapping, mapping the container port to the host port (note that you need to delete network_mode: \"host\" first, and then add the following content)\n    # ports:\n    # - \"12345: 12345\" # Note! 12345 must correspond to the WEB_PORT environment variable.\n# 4. Finally, run `docker-compose up`.\n# 5. If you want to use a graphics card, pay attention to the LOCAL_MODEL_DEVICE and Nvidia GPU runtime options.\n## ===================================================\n\n## ===================================================\n## 「方案零」 部署项目的全部能力（这个是包含cuda和latex的大型镜像。如果您网速慢、硬盘小或没有显卡，则不推荐使用这个）\n## ===================================================\nversion: '3'\nservices:\n  gpt_academic_full_capability:\n    image: ghcr.io/binary-husky/gpt_academic_with_all_capacity:master\n    environment:\n      # 请查阅 `config.py`或者 github wiki 以查看所有的配置信息\n      API_KEY:                  '  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                        '\n      # USE_PROXY:                '  True                                                                       '\n      # proxies:                  '  { \"http\": \"http://localhost:10881\", \"https\": \"http://localhost:10881\", }   '\n      LLM_MODEL:                '  gpt-3.5-turbo                                                              '\n      AVAIL_LLM_MODELS:         '  [\"gpt-3.5-turbo\", \"gpt-4\", \"qianfan\", \"sparkv2\", \"spark\", \"chatglm\"]       '\n      BAIDU_CLOUD_API_KEY :     '  bTUtwEAveBrQipEowUvDwYWq                                                   '\n      BAIDU_CLOUD_SECRET_KEY :  '  jqXtLvXiVw6UNdjliATTS61rllG8Iuni                                           '\n      XFYUN_APPID:              '  53a8d816                                                                   '\n      XFYUN_API_SECRET:         '  MjMxNDQ4NDE4MzM0OSNlNjQ2NTlhMTkx                                           '\n      XFYUN_API_KEY:            '  95ccdec285364869d17b33e75ee96447                                           '\n      ENABLE_AUDIO:             '  False                                                                      '\n      DEFAULT_WORKER_NUM:       '  20                                                                         '\n      WEB_PORT:                 '  12345                                                                      '\n      ADD_WAIFU:                '  False                                                                      '\n      ALIYUN_APPKEY:            '  RxPlZrM88DnAFkZK                                                           '\n      THEME:                    '  Chuanhu-Small-and-Beautiful                                                '\n      ALIYUN_ACCESSKEY:         '  LTAI5t6BrFUzxRXVGUWnekh1                                                   '\n      ALIYUN_SECRET:            '  eHmI20SVWIwQZxCiTD2bGQVspP9i68                                             '\n      # LOCAL_MODEL_DEVICE:       '  cuda                                                                       '\n\n    # 加载英伟达显卡运行时\n    # runtime: nvidia\n    # deploy:\n    #     resources:\n    #       reservations:\n    #         devices:\n    #           - driver: nvidia\n    #             count: 1\n    #             capabilities: [gpu]\n\n    # 「WEB_PORT暴露方法1: 适用于Linux」与宿主的网络融合\n    network_mode: \"host\"\n\n    # 「WEB_PORT暴露方法2: 适用于所有系统」端口映射\n    # ports:\n    #   - \"12345:12345\"  # 12345必须与WEB_PORT相互对应\n\n    # 启动容器后，运行main.py主程序\n    command: >\n      bash -c \"python3 -u main.py\"\n\n\n## ===================================================\n## 「方案一」 如果不需要运行本地模型（仅 chatgpt, azure, 星火, 千帆, claude 等在线大模型服务）\n## ===================================================\nversion: '3'\nservices:\n  gpt_academic_nolocalllms:\n    image: ghcr.io/binary-husky/gpt_academic_nolocal:master # (Auto Built by Dockerfile: docs/GithubAction+NoLocal)\n    environment:\n      # 请查阅 `config.py` 以查看所有的配置信息\n      API_KEY:                  '    sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx                                            '\n      USE_PROXY:                '    True                                                                                           '\n      proxies:                  '    { \"http\": \"socks5h://localhost:10880\", \"https\": \"socks5h://localhost:10880\", }                 '\n      LLM_MODEL:                '    gpt-3.5-turbo                                                                                  '\n      AVAIL_LLM_MODELS:         '    [\"gpt-3.5-turbo\", \"api2d-gpt-3.5-turbo\", \"gpt-4\", \"api2d-gpt-4\", \"sparkv2\", \"qianfan\"]         '\n      WEB_PORT:                 '    22303                                                                                          '\n      ADD_WAIFU:                '    True                                                                                           '\n      # THEME:                    '    Chuanhu-Small-and-Beautiful                                                                    '\n      # DEFAULT_WORKER_NUM:       '    10                                                                                             '\n      # AUTHENTICATION:           '    [(\"username\", \"passwd\"), (\"username2\", \"passwd2\")]                                             '\n\n    # 「WEB_PORT暴露方法1: 适用于Linux」与宿主的网络融合\n    network_mode: \"host\"\n\n    # 启动命令\n    command: >\n      bash -c \"python3 -u main.py\"\n\n\n### ===================================================\n### 「方案二」 如果需要运行ChatGLM + Qwen + MOSS等本地模型\n### ===================================================\nversion: '3'\nservices:\n  gpt_academic_with_chatglm:\n    image: ghcr.io/binary-husky/gpt_academic_chatglm_moss:master  # (Auto Built by Dockerfile: docs/Dockerfile+ChatGLM)\n    environment:\n      # 请查阅 `config.py` 以查看所有的配置信息\n      API_KEY:                  '    sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx                                            '\n      USE_PROXY:                '    True                                                                                           '\n      proxies:                  '    { \"http\": \"socks5h://localhost:10880\", \"https\": \"socks5h://localhost:10880\", }                 '\n      LLM_MODEL:                '    gpt-3.5-turbo                                                                                  '\n      AVAIL_LLM_MODELS:         '    [\"chatglm\", \"qwen\", \"moss\", \"gpt-3.5-turbo\", \"gpt-4\", \"newbing\"]                               '\n      LOCAL_MODEL_DEVICE:       '    cuda                                                                                           '\n      DEFAULT_WORKER_NUM:       '    10                                                                                             '\n      WEB_PORT:                 '    12303                                                                                          '\n      ADD_WAIFU:                '    True                                                                                           '\n      # AUTHENTICATION:           '    [(\"username\", \"passwd\"), (\"username2\", \"passwd2\")]                                             '\n\n    # 显卡的使用，nvidia0指第0个GPU\n    runtime: nvidia\n    devices:\n      - /dev/nvidia0:/dev/nvidia0\n\n    # 「WEB_PORT暴露方法1: 适用于Linux」与宿主的网络融合\n    network_mode: \"host\"\n\n    # 启动命令\n    command: >\n      bash -c \"python3 -u main.py\"\n\n    # P.S. 通过对 command 进行微调，可以便捷地安装额外的依赖\n    # command: >\n    #   bash -c \"pip install -r request_llms/requirements_qwen.txt && python3 -u main.py\"\n\n\n### ===================================================\n### 「方案三」 如果需要运行ChatGPT + LLAMA + 盘古 + RWKV本地模型\n### ===================================================\nversion: '3'\nservices:\n  gpt_academic_with_rwkv:\n    image: ghcr.io/binary-husky/gpt_academic_jittorllms:master\n    environment:\n      # 请查阅 `config.py` 以查看所有的配置信息\n      API_KEY:                  '    sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx,fkxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  '\n      USE_PROXY:                '    True                                                                                           '\n      proxies:                  '    { \"http\": \"socks5h://localhost:10880\", \"https\": \"socks5h://localhost:10880\", }                 '\n      LLM_MODEL:                '    gpt-3.5-turbo                                                                                  '\n      AVAIL_LLM_MODELS:         '    [\"gpt-3.5-turbo\", \"newbing\", \"jittorllms_rwkv\", \"jittorllms_pangualpha\", \"jittorllms_llama\"]   '\n      LOCAL_MODEL_DEVICE:       '    cuda                                                                                           '\n      DEFAULT_WORKER_NUM:       '    10                                                                                             '\n      WEB_PORT:                 '    12305                                                                                          '\n      ADD_WAIFU:                '    True                                                                                           '\n      # AUTHENTICATION:           '    [(\"username\", \"passwd\"), (\"username2\", \"passwd2\")]                                             '\n\n    # 显卡的使用，nvidia0指第0个GPU\n    runtime: nvidia\n    devices:\n      - /dev/nvidia0:/dev/nvidia0\n\n    # 「WEB_PORT暴露方法1: 适用于Linux」与宿主的网络融合\n    network_mode: \"host\"\n\n    # 启动命令\n    command: >\n      python3 -u main.py\n\n\n## ===================================================\n## 「方案四」 ChatGPT + Latex\n## ===================================================\nversion: '3'\nservices:\n  gpt_academic_with_latex:\n    image: ghcr.io/binary-husky/gpt_academic_with_latex:master  # (Auto Built by Dockerfile: docs/GithubAction+NoLocal+Latex)\n    # 对于ARM64设备，请将以上镜像名称替换为 ghcr.io/binary-husky/gpt_academic_with_latex_arm:master\n    environment:\n      # 请查阅 `config.py` 以查看所有的配置信息\n      API_KEY:                  '    sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx                              '\n      USE_PROXY:                '    True                                                                             '\n      proxies:                  '    { \"http\": \"socks5h://localhost:10880\", \"https\": \"socks5h://localhost:10880\", }   '\n      LLM_MODEL:                '    gpt-3.5-turbo                                                                    '\n      AVAIL_LLM_MODELS:         '    [\"gpt-3.5-turbo\", \"gpt-4\"]                                                       '\n      LOCAL_MODEL_DEVICE:       '    cuda                                                                             '\n      DEFAULT_WORKER_NUM:       '    10                                                                               '\n      WEB_PORT:                 '    12303                                                                            '\n\n    # 「WEB_PORT暴露方法1: 适用于Linux」与宿主的网络融合\n    network_mode: \"host\"\n\n    # 启动命令\n    command: >\n      bash -c \"python3 -u main.py\"\n\n\n## ===================================================\n## 「方案五」 ChatGPT + 语音助手 （请先阅读 docs/use_audio.md）\n## ===================================================\nversion: '3'\nservices:\n  gpt_academic_with_audio:\n    image: ghcr.io/binary-husky/gpt_academic_audio_assistant:master\n    environment:\n      # 请查阅 `config.py` 以查看所有的配置信息\n      API_KEY:                  '    fk195831-IdP0Pb3W6DCMUIbQwVX6MsSiyxwqybyS                        '\n      USE_PROXY:                '    False                                                            '\n      proxies:                  '    None                                                             '\n      LLM_MODEL:                '    gpt-3.5-turbo                                                    '\n      AVAIL_LLM_MODELS:         '    [\"gpt-3.5-turbo\", \"gpt-4\"]                                       '\n      ENABLE_AUDIO:             '    True                                                             '\n      LOCAL_MODEL_DEVICE:       '    cuda                                                             '\n      DEFAULT_WORKER_NUM:       '    20                                                               '\n      WEB_PORT:                 '    12343                                                            '\n      ADD_WAIFU:                '    True                                                             '\n      THEME:                    '    Chuanhu-Small-and-Beautiful                                      '\n      ALIYUN_APPKEY:            '    RoP1ZrM84DnAFkZK                                                 '\n      ALIYUN_TOKEN:             '    f37f30e0f9934c34a992f6f64f7eba4f                                 '\n      # (无需填写) ALIYUN_ACCESSKEY:         '    LTAI5q6BrFUzoRXVGUWnekh1                                         '\n      # (无需填写) ALIYUN_SECRET:            '    eHmI20AVWIaQZ0CiTD2bGQVsaP9i68                                   '\n\n    # 「WEB_PORT暴露方法1: 适用于Linux」与宿主的网络融合\n    network_mode: \"host\"\n\n    # 启动命令\n    command: >\n      bash -c \"python3 -u main.py\"\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 24.205078125,
          "content": "import os; os.environ['no_proxy'] = '*' # 避免代理网络产生意外污染\n\nhelp_menu_description = \\\n\"\"\"Github源代码开源和更新[地址🚀](https://github.com/binary-husky/gpt_academic),\n感谢热情的[开发者们❤️](https://github.com/binary-husky/gpt_academic/graphs/contributors).\n</br></br>常见问题请查阅[项目Wiki](https://github.com/binary-husky/gpt_academic/wiki),\n如遇到Bug请前往[Bug反馈](https://github.com/binary-husky/gpt_academic/issues).\n</br></br>普通对话使用说明: 1. 输入问题; 2. 点击提交\n</br></br>基础功能区使用说明: 1. 输入文本; 2. 点击任意基础功能区按钮\n</br></br>函数插件区使用说明: 1. 输入路径/问题, 或者上传文件; 2. 点击任意函数插件区按钮\n</br></br>虚空终端使用说明: 点击虚空终端, 然后根据提示输入指令, 再次点击虚空终端\n</br></br>如何保存对话: 点击保存当前的对话按钮\n</br></br>如何语音对话: 请阅读Wiki\n</br></br>如何临时更换API_KEY: 在输入区输入临时API_KEY后提交（网页刷新后失效）\"\"\"\n\nfrom loguru import logger\ndef enable_log(PATH_LOGGING):\n    from shared_utils.logging import setup_logging\n    setup_logging(PATH_LOGGING)\n\ndef encode_plugin_info(k, plugin)->str:\n    import copy\n    from themes.theme import to_cookie_str\n    plugin_ = copy.copy(plugin)\n    plugin_.pop(\"Function\", None)\n    plugin_.pop(\"Class\", None)\n    plugin_.pop(\"Button\", None)\n    plugin_[\"Info\"] = plugin.get(\"Info\", k)\n    if plugin.get(\"AdvancedArgs\", False):\n        plugin_[\"Label\"] = f\"插件[{k}]的高级参数说明：\" + plugin.get(\"ArgsReminder\", f\"没有提供高级参数功能说明\")\n    else:\n        plugin_[\"Label\"] = f\"插件[{k}]不需要高级参数。\"\n    return to_cookie_str(plugin_)\n\ndef main():\n    import gradio as gr\n    if gr.__version__ not in ['3.32.12']:\n        raise ModuleNotFoundError(\"使用项目内置Gradio获取最优体验! 请运行 `pip install -r requirements.txt` 指令安装内置Gradio及其他依赖, 详情信息见requirements.txt.\")\n\n    # 一些基础工具\n    from toolbox import format_io, find_free_port, on_file_uploaded, on_report_generated, get_conf, ArgsGeneralWrapper, DummyWith\n\n    # 对话、日志记录\n    enable_log(get_conf(\"PATH_LOGGING\"))\n\n    # 对话句柄\n    from request_llms.bridge_all import predict\n\n    # 读取配置\n    proxies, WEB_PORT, LLM_MODEL, CONCURRENT_COUNT, AUTHENTICATION = get_conf('proxies', 'WEB_PORT', 'LLM_MODEL', 'CONCURRENT_COUNT', 'AUTHENTICATION')\n    CHATBOT_HEIGHT, LAYOUT, AVAIL_LLM_MODELS, AUTO_CLEAR_TXT = get_conf('CHATBOT_HEIGHT', 'LAYOUT', 'AVAIL_LLM_MODELS', 'AUTO_CLEAR_TXT')\n    ENABLE_AUDIO, AUTO_CLEAR_TXT, PATH_LOGGING, AVAIL_THEMES, THEME, ADD_WAIFU = get_conf('ENABLE_AUDIO', 'AUTO_CLEAR_TXT', 'PATH_LOGGING', 'AVAIL_THEMES', 'THEME', 'ADD_WAIFU')\n    NUM_CUSTOM_BASIC_BTN, SSL_KEYFILE, SSL_CERTFILE = get_conf('NUM_CUSTOM_BASIC_BTN', 'SSL_KEYFILE', 'SSL_CERTFILE')\n    DARK_MODE, INIT_SYS_PROMPT, ADD_WAIFU, TTS_TYPE = get_conf('DARK_MODE', 'INIT_SYS_PROMPT', 'ADD_WAIFU', 'TTS_TYPE')\n    if LLM_MODEL not in AVAIL_LLM_MODELS: AVAIL_LLM_MODELS += [LLM_MODEL]\n\n    # 如果WEB_PORT是-1, 则随机选取WEB端口\n    PORT = find_free_port() if WEB_PORT <= 0 else WEB_PORT\n    from check_proxy import get_current_version\n    from themes.theme import adjust_theme, advanced_css, theme_declaration, js_code_clear, js_code_show_or_hide, js_code_show_or_hide_group2\n    from themes.theme import js_code_for_toggle_darkmode\n    from themes.theme import load_dynamic_theme, to_cookie_str, from_cookie_str, assign_user_uuid\n    title_html = f\"<h1 align=\\\"center\\\">GPT 学术优化 {get_current_version()}</h1>{theme_declaration}\"\n\n\n    # 一些普通功能模块\n    from core_functional import get_core_functions\n    functional = get_core_functions()\n\n    # 高级函数插件\n    from crazy_functional import get_crazy_functions, get_multiplex_button_functions\n    DEFAULT_FN_GROUPS = get_conf('DEFAULT_FN_GROUPS')\n    plugins = get_crazy_functions()\n    all_plugin_groups = list(set([g for _, plugin in plugins.items() for g in plugin['Group'].split('|')]))\n    match_group = lambda tags, groups: any([g in groups for g in tags.split('|')])\n\n    # 处理markdown文本格式的转变\n    gr.Chatbot.postprocess = format_io\n\n    # 做一些外观色彩上的调整\n    set_theme = adjust_theme()\n\n    # 代理与自动更新\n    from check_proxy import check_proxy, auto_update, warm_up_modules\n    proxy_info = check_proxy(proxies)\n\n    # 切换布局\n    gr_L1 = lambda: gr.Row().style()\n    gr_L2 = lambda scale, elem_id: gr.Column(scale=scale, elem_id=elem_id, min_width=400)\n    if LAYOUT == \"TOP-DOWN\":\n        gr_L1 = lambda: DummyWith()\n        gr_L2 = lambda scale, elem_id: gr.Row()\n        CHATBOT_HEIGHT /= 2\n\n    cancel_handles = []\n    customize_btns = {}\n    predefined_btns = {}\n    from shared_utils.cookie_manager import make_cookie_cache, make_history_cache\n    with gr.Blocks(title=\"GPT 学术优化\", theme=set_theme, analytics_enabled=False, css=advanced_css) as app_block:\n        gr.HTML(title_html)\n        secret_css = gr.Textbox(visible=False, elem_id=\"secret_css\")\n        register_advanced_plugin_init_arr = \"\"\n\n        cookies, web_cookie_cache = make_cookie_cache() # 定义 后端state（cookies）、前端（web_cookie_cache）两兄弟\n        with gr_L1():\n            with gr_L2(scale=2, elem_id=\"gpt-chat\"):\n                chatbot = gr.Chatbot(label=f\"当前模型：{LLM_MODEL}\", elem_id=\"gpt-chatbot\")\n                if LAYOUT == \"TOP-DOWN\":  chatbot.style(height=CHATBOT_HEIGHT)\n                history, _, _ = make_history_cache() # 定义 后端state（history）、前端（history_cache）、后端setter（history_cache_update）三兄弟\n            with gr_L2(scale=1, elem_id=\"gpt-panel\"):\n                with gr.Accordion(\"输入区\", open=True, elem_id=\"input-panel\") as area_input_primary:\n                    with gr.Row():\n                        txt = gr.Textbox(show_label=False, placeholder=\"Input question here.\", elem_id='user_input_main').style(container=False)\n                    with gr.Row(elem_id=\"gpt-submit-row\"):\n                        multiplex_submit_btn = gr.Button(\"提交\", elem_id=\"elem_submit_visible\", variant=\"primary\")\n                        multiplex_sel = gr.Dropdown(\n                            choices=get_multiplex_button_functions().keys(), value=\"常规对话\",\n                            interactive=True, label='', show_label=False,\n                            elem_classes='normal_mut_select', elem_id=\"gpt-submit-dropdown\").style(container=False)\n                        submit_btn = gr.Button(\"提交\", elem_id=\"elem_submit\", variant=\"primary\", visible=False)\n                    with gr.Row():\n                        resetBtn = gr.Button(\"重置\", elem_id=\"elem_reset\", variant=\"secondary\"); resetBtn.style(size=\"sm\")\n                        stopBtn = gr.Button(\"停止\", elem_id=\"elem_stop\", variant=\"secondary\"); stopBtn.style(size=\"sm\")\n                        clearBtn = gr.Button(\"清除\", elem_id=\"elem_clear\", variant=\"secondary\", visible=False); clearBtn.style(size=\"sm\")\n                    if ENABLE_AUDIO:\n                        with gr.Row():\n                            audio_mic = gr.Audio(source=\"microphone\", type=\"numpy\", elem_id=\"elem_audio\", streaming=True, show_label=False).style(container=False)\n                    with gr.Row():\n                        status = gr.Markdown(f\"Tip: 按Enter提交, 按Shift+Enter换行。支持将文件直接粘贴到输入区。\", elem_id=\"state-panel\")\n\n                with gr.Accordion(\"基础功能区\", open=True, elem_id=\"basic-panel\") as area_basic_fn:\n                    with gr.Row():\n                        for k in range(NUM_CUSTOM_BASIC_BTN):\n                            customize_btn = gr.Button(\"自定义按钮\" + str(k+1), visible=False, variant=\"secondary\", info_str=f'基础功能区: 自定义按钮')\n                            customize_btn.style(size=\"sm\")\n                            customize_btns.update({\"自定义按钮\" + str(k+1): customize_btn})\n                        for k in functional:\n                            if (\"Visible\" in functional[k]) and (not functional[k][\"Visible\"]): continue\n                            variant = functional[k][\"Color\"] if \"Color\" in functional[k] else \"secondary\"\n                            functional[k][\"Button\"] = gr.Button(k, variant=variant, info_str=f'基础功能区: {k}')\n                            functional[k][\"Button\"].style(size=\"sm\")\n                            predefined_btns.update({k: functional[k][\"Button\"]})\n                with gr.Accordion(\"函数插件区\", open=True, elem_id=\"plugin-panel\") as area_crazy_fn:\n                    with gr.Row():\n                        gr.Markdown(\"<small>插件可读取“输入区”文本/路径作为参数（上传文件自动修正路径）</small>\")\n                    with gr.Row(elem_id=\"input-plugin-group\"):\n                        plugin_group_sel = gr.Dropdown(choices=all_plugin_groups, label='', show_label=False, value=DEFAULT_FN_GROUPS,\n                                                      multiselect=True, interactive=True, elem_classes='normal_mut_select').style(container=False)\n                    with gr.Row():\n                        for index, (k, plugin) in enumerate(plugins.items()):\n                            if not plugin.get(\"AsButton\", True): continue\n                            visible = True if match_group(plugin['Group'], DEFAULT_FN_GROUPS) else False\n                            variant = plugins[k][\"Color\"] if \"Color\" in plugin else \"secondary\"\n                            info = plugins[k].get(\"Info\", k)\n                            btn_elem_id = f\"plugin_btn_{index}\"\n                            plugin['Button'] = plugins[k]['Button'] = gr.Button(k, variant=variant,\n                                visible=visible, info_str=f'函数插件区: {info}', elem_id=btn_elem_id).style(size=\"sm\")\n                            plugin['ButtonElemId'] = btn_elem_id\n                    with gr.Row():\n                        with gr.Accordion(\"更多函数插件\", open=True):\n                            dropdown_fn_list = []\n                            for k, plugin in plugins.items():\n                                if not match_group(plugin['Group'], DEFAULT_FN_GROUPS): continue\n                                if not plugin.get(\"AsButton\", True): dropdown_fn_list.append(k)     # 排除已经是按钮的插件\n                                elif plugin.get('AdvancedArgs', False): dropdown_fn_list.append(k)  # 对于需要高级参数的插件，亦在下拉菜单中显示\n                            with gr.Row():\n                                dropdown = gr.Dropdown(dropdown_fn_list, value=r\"点击这里输入「关键词」搜索插件\", label=\"\", show_label=False).style(container=False)\n                            with gr.Row():\n                                plugin_advanced_arg = gr.Textbox(show_label=True, label=\"高级参数输入区\", visible=False, elem_id=\"advance_arg_input_legacy\",\n                                                                 placeholder=\"这里是特殊函数插件的高级参数输入区\").style(container=False)\n                            with gr.Row():\n                                switchy_bt = gr.Button(r\"请先从插件列表中选择\", variant=\"secondary\", elem_id=\"elem_switchy_bt\").style(size=\"sm\")\n                    with gr.Row():\n                        with gr.Accordion(\"点击展开“文件下载区”。\", open=False) as area_file_up:\n                            file_upload = gr.Files(label=\"任何文件, 推荐上传压缩文件(zip, tar)\", file_count=\"multiple\", elem_id=\"elem_upload\")\n\n\n        # 左上角工具栏定义\n        from themes.gui_toolbar import define_gui_toolbar\n        checkboxes, checkboxes_2, max_length_sl, theme_dropdown, system_prompt, file_upload_2, md_dropdown, top_p, temperature = \\\n            define_gui_toolbar(AVAIL_LLM_MODELS, LLM_MODEL, INIT_SYS_PROMPT, THEME, AVAIL_THEMES, ADD_WAIFU, help_menu_description, js_code_for_toggle_darkmode)\n\n        # 浮动菜单定义\n        from themes.gui_floating_menu import define_gui_floating_menu\n        area_input_secondary, txt2, area_customize, _, resetBtn2, clearBtn2, stopBtn2 = \\\n            define_gui_floating_menu(customize_btns, functional, predefined_btns, cookies, web_cookie_cache)\n        \n        # 浮动时间线定义\n        gr.Spark()\n\n        # 插件二级菜单的实现\n        from themes.gui_advanced_plugin_class import define_gui_advanced_plugin_class\n        new_plugin_callback, route_switchy_bt_with_arg, usr_confirmed_arg = \\\n            define_gui_advanced_plugin_class(plugins)\n\n        # 功能区显示开关与功能区的互动\n        def fn_area_visibility(a):\n            ret = {}\n            ret.update({area_input_primary: gr.update(visible=(\"浮动输入区\" not in a))})\n            ret.update({area_input_secondary: gr.update(visible=(\"浮动输入区\" in a))})\n            ret.update({plugin_advanced_arg: gr.update(visible=(\"插件参数区\" in a))})\n            if \"浮动输入区\" in a: ret.update({txt: gr.update(value=\"\")})\n            return ret\n        checkboxes.select(fn_area_visibility, [checkboxes], [area_basic_fn, area_crazy_fn, area_input_primary, area_input_secondary, txt, txt2, plugin_advanced_arg] )\n        checkboxes.select(None, [checkboxes], None, _js=js_code_show_or_hide)\n\n        # 功能区显示开关与功能区的互动\n        def fn_area_visibility_2(a):\n            ret = {}\n            ret.update({area_customize: gr.update(visible=(\"自定义菜单\" in a))})\n            return ret\n        checkboxes_2.select(fn_area_visibility_2, [checkboxes_2], [area_customize] )\n        checkboxes_2.select(None, [checkboxes_2], None, _js=js_code_show_or_hide_group2)\n\n        # 整理反复出现的控件句柄组合\n        input_combo = [cookies, max_length_sl, md_dropdown, txt, txt2, top_p, temperature, chatbot, history, system_prompt, plugin_advanced_arg]\n        input_combo_order = [\"cookies\", \"max_length_sl\", \"md_dropdown\", \"txt\", \"txt2\", \"top_p\", \"temperature\", \"chatbot\", \"history\", \"system_prompt\", \"plugin_advanced_arg\"]\n        output_combo = [cookies, chatbot, history, status]\n        predict_args = dict(fn=ArgsGeneralWrapper(predict), inputs=[*input_combo, gr.State(True)], outputs=output_combo)\n        \n        # 提交按钮、重置按钮\n        multiplex_submit_btn.click(\n            None, [multiplex_sel], None, _js=\"\"\"(multiplex_sel)=>multiplex_function_begin(multiplex_sel)\"\"\")\n        txt.submit(\n            None, [multiplex_sel], None, _js=\"\"\"(multiplex_sel)=>multiplex_function_begin(multiplex_sel)\"\"\")\n        multiplex_sel.select(\n            None, [multiplex_sel], None, _js=f\"\"\"(multiplex_sel)=>run_multiplex_shift(multiplex_sel)\"\"\")\n        cancel_handles.append(submit_btn.click(**predict_args))\n        resetBtn.click(None, None, [chatbot, history, status], _js= \"\"\"clear_conversation\"\"\")   # 先在前端快速清除chatbot&status\n        resetBtn2.click(None, None, [chatbot, history, status], _js=\"\"\"clear_conversation\"\"\")  # 先在前端快速清除chatbot&status\n        # reset_server_side_args = (lambda history: ([], [], \"已重置\"), [history], [chatbot, history, status])\n        # resetBtn.click(*reset_server_side_args)    # 再在后端清除history\n        # resetBtn2.click(*reset_server_side_args)   # 再在后端清除history\n        clearBtn.click(None, None, [txt, txt2], _js=js_code_clear)\n        clearBtn2.click(None, None, [txt, txt2], _js=js_code_clear)\n        if AUTO_CLEAR_TXT:\n            submit_btn.click(None, None, [txt, txt2], _js=js_code_clear)\n        # 基础功能区的回调函数注册\n        for k in functional:\n            if (\"Visible\" in functional[k]) and (not functional[k][\"Visible\"]): continue\n            click_handle = functional[k][\"Button\"].click(fn=ArgsGeneralWrapper(predict), inputs=[*input_combo, gr.State(True), gr.State(k)], outputs=output_combo)\n            cancel_handles.append(click_handle)\n        for btn in customize_btns.values():\n            click_handle = btn.click(fn=ArgsGeneralWrapper(predict), inputs=[*input_combo, gr.State(True), gr.State(btn.value)], outputs=output_combo)\n            cancel_handles.append(click_handle)\n        # 文件上传区，接收文件后与chatbot的互动\n        file_upload.upload(on_file_uploaded, [file_upload, chatbot, txt, txt2, checkboxes, cookies], [chatbot, txt, txt2, cookies]).then(None, None, None,   _js=r\"()=>{toast_push('上传完毕 ...'); cancel_loading_status();}\")\n        file_upload_2.upload(on_file_uploaded, [file_upload_2, chatbot, txt, txt2, checkboxes, cookies], [chatbot, txt, txt2, cookies]).then(None, None, None, _js=r\"()=>{toast_push('上传完毕 ...'); cancel_loading_status();}\")\n        # 函数插件-固定按钮区\n        for k in plugins:\n            register_advanced_plugin_init_arr += f\"\"\"register_plugin_init(\"{k}\",\"{encode_plugin_info(k, plugins[k])}\");\"\"\"\n            if plugins[k].get(\"Class\", None):\n                plugins[k][\"JsMenu\"] = plugins[k][\"Class\"]().get_js_code_for_generating_menu(k)\n                register_advanced_plugin_init_arr += \"\"\"register_advanced_plugin_init_code(\"{k}\",\"{gui_js}\");\"\"\".format(k=k, gui_js=plugins[k][\"JsMenu\"])\n            if not plugins[k].get(\"AsButton\", True): continue\n            if plugins[k].get(\"Class\", None) is None:\n                assert plugins[k].get(\"Function\", None) is not None\n                click_handle = plugins[k][\"Button\"].click(None, inputs=[], outputs=None, _js=f\"\"\"()=>run_classic_plugin_via_id(\"{plugins[k][\"ButtonElemId\"]}\")\"\"\")\n            else:\n                click_handle = plugins[k][\"Button\"].click(None, inputs=[], outputs=None, _js=f\"\"\"()=>run_advanced_plugin_launch_code(\"{k}\")\"\"\")\n\n        # 函数插件-下拉菜单与随变按钮的互动（新版-更流畅）\n        dropdown.select(None, [dropdown], None, _js=f\"\"\"(dropdown)=>run_dropdown_shift(dropdown)\"\"\")\n\n        # 模型切换时的回调\n        def on_md_dropdown_changed(k):\n            return {chatbot: gr.update(label=\"当前模型：\"+k)}\n        md_dropdown.select(on_md_dropdown_changed, [md_dropdown], [chatbot])\n\n        # 主题修改\n        def on_theme_dropdown_changed(theme, secret_css):\n            adjust_theme, css_part1, _, adjust_dynamic_theme = load_dynamic_theme(theme)\n            if adjust_dynamic_theme:\n                css_part2 = adjust_dynamic_theme._get_theme_css()\n            else:\n                css_part2 = adjust_theme()._get_theme_css()\n            return css_part2 + css_part1\n        theme_handle = theme_dropdown.select(on_theme_dropdown_changed, [theme_dropdown, secret_css], [secret_css]) # , _js=\"\"\"change_theme_prepare\"\"\")\n        theme_handle.then(None, [theme_dropdown, secret_css], None, _js=\"\"\"change_theme\"\"\")\n\n        switchy_bt.click(None, [switchy_bt], None, _js=\"(switchy_bt)=>on_flex_button_click(switchy_bt)\")\n        # 随变按钮的回调函数注册\n        def route(request: gr.Request, k, *args, **kwargs):\n            if k not in [r\"点击这里搜索插件列表\", r\"请先从插件列表中选择\"]:\n                if plugins[k].get(\"Class\", None) is None:\n                    assert plugins[k].get(\"Function\", None) is not None\n                    yield from ArgsGeneralWrapper(plugins[k][\"Function\"])(request, *args, **kwargs)\n        # 旧插件的高级参数区确认按钮（隐藏）\n        old_plugin_callback = gr.Button(r\"未选定任何插件\", variant=\"secondary\", visible=False, elem_id=\"old_callback_btn_for_plugin_exe\")\n        click_handle_ng = old_plugin_callback.click(route, [switchy_bt, *input_combo], output_combo)\n        click_handle_ng.then(on_report_generated, [cookies, file_upload, chatbot], [cookies, file_upload, chatbot]).then(None, [switchy_bt], None, _js=r\"(fn)=>on_plugin_exe_complete(fn)\")\n        cancel_handles.append(click_handle_ng)\n        # 新一代插件的高级参数区确认按钮（隐藏）\n        click_handle_ng = new_plugin_callback.click(route_switchy_bt_with_arg,\n            [\n                gr.State([\"new_plugin_callback\", \"usr_confirmed_arg\"] + input_combo_order), # 第一个参数: 指定了后续参数的名称\n                new_plugin_callback, usr_confirmed_arg, *input_combo                        # 后续参数: 真正的参数\n            ], output_combo)\n        click_handle_ng.then(on_report_generated, [cookies, file_upload, chatbot], [cookies, file_upload, chatbot]).then(None, [switchy_bt], None, _js=r\"(fn)=>on_plugin_exe_complete(fn)\")\n        cancel_handles.append(click_handle_ng)\n        # 终止按钮的回调函数注册\n        stopBtn.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)\n        stopBtn2.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)\n        plugins_as_btn = {name:plugin for name, plugin in plugins.items() if plugin.get('Button', None)}\n        def on_group_change(group_list):\n            btn_list = []\n            fns_list = []\n            if not group_list: # 处理特殊情况：没有选择任何插件组\n                return [*[plugin['Button'].update(visible=False) for _, plugin in plugins_as_btn.items()], gr.Dropdown.update(choices=[])]\n            for k, plugin in plugins.items():\n                if plugin.get(\"AsButton\", True):\n                    btn_list.append(plugin['Button'].update(visible=match_group(plugin['Group'], group_list))) # 刷新按钮\n                    if plugin.get('AdvancedArgs', False): dropdown_fn_list.append(k) # 对于需要高级参数的插件，亦在下拉菜单中显示\n                elif match_group(plugin['Group'], group_list): fns_list.append(k) # 刷新下拉列表\n            return [*btn_list, gr.Dropdown.update(choices=fns_list)]\n        plugin_group_sel.select(fn=on_group_change, inputs=[plugin_group_sel], outputs=[*[plugin['Button'] for name, plugin in plugins_as_btn.items()], dropdown])\n\n        # 是否启动语音输入功能\n        if ENABLE_AUDIO:\n            from crazy_functions.live_audio.audio_io import RealtimeAudioDistribution\n            rad = RealtimeAudioDistribution()\n            def deal_audio(audio, cookies):\n                rad.feed(cookies['uuid'].hex, audio)\n            audio_mic.stream(deal_audio, inputs=[audio_mic, cookies])\n\n        # 生成当前浏览器窗口的uuid（刷新失效）\n        app_block.load(assign_user_uuid, inputs=[cookies], outputs=[cookies])\n\n        # 初始化（前端）\n        from shared_utils.cookie_manager import load_web_cookie_cache__fn_builder\n        load_web_cookie_cache = load_web_cookie_cache__fn_builder(customize_btns, cookies, predefined_btns)\n        app_block.load(load_web_cookie_cache, inputs = [web_cookie_cache, cookies],\n            outputs = [web_cookie_cache, cookies, *customize_btns.values(), *predefined_btns.values()], _js=\"\"\"persistent_cookie_init\"\"\")\n        app_block.load(None, inputs=[], outputs=None, _js=f\"\"\"()=>GptAcademicJavaScriptInit(\"{DARK_MODE}\",\"{INIT_SYS_PROMPT}\",\"{ADD_WAIFU}\",\"{LAYOUT}\",\"{TTS_TYPE}\")\"\"\")    # 配置暗色主题或亮色主题\n        app_block.load(None, inputs=[], outputs=None, _js=\"\"\"()=>{REP}\"\"\".replace(\"REP\", register_advanced_plugin_init_arr))\n\n    # Gradio的inbrowser触发不太稳定，回滚代码到原始的浏览器打开函数\n    def run_delayed_tasks():\n        import threading, webbrowser, time\n        logger.info(f\"如果浏览器没有自动打开，请复制并转到以下URL：\")\n        if DARK_MODE:   logger.info(f\"\\t「暗色主题已启用（支持动态切换主题）」: http://localhost:{PORT}\")\n        else:           logger.info(f\"\\t「亮色主题已启用（支持动态切换主题）」: http://localhost:{PORT}\")\n\n        def auto_updates(): time.sleep(0); auto_update()\n        def open_browser(): time.sleep(2); webbrowser.open_new_tab(f\"http://localhost:{PORT}\")\n        def warm_up_mods(): time.sleep(6); warm_up_modules()\n\n        threading.Thread(target=auto_updates, name=\"self-upgrade\", daemon=True).start() # 查看自动更新\n        threading.Thread(target=warm_up_mods, name=\"warm-up\",      daemon=True).start() # 预热tiktoken模块\n        if get_conf('AUTO_OPEN_BROWSER'):\n            threading.Thread(target=open_browser, name=\"open-browser\", daemon=True).start() # 打开浏览器页面\n\n    # 运行一些异步任务：自动更新、打开浏览器页面、预热tiktoken模块\n    run_delayed_tasks()\n\n    # 最后，正式开始服务\n    from shared_utils.fastapi_server import start_app\n    start_app(app_block, CONCURRENT_COUNT, AUTHENTICATION, PORT, SSL_KEYFILE, SSL_CERTFILE)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "multi_language.py",
          "type": "blob",
          "size": 21.90625,
          "content": "\"\"\"\n    Translate this project to other languages (experimental, please open an issue if there is any bug)\n\n\n    Usage:\n        1. modify config.py, set your LLM_MODEL and API_KEY(s) to provide access to OPENAI (or any other LLM model provider)\n\n        2. modify LANG (below ↓)\n            LANG = \"English\"\n\n        3. modify TransPrompt (below ↓)\n            TransPrompt = f\"Replace each json value `#` with translated results in English, e.g., \\\"原始文本\\\":\\\"TranslatedText\\\". Keep Json format. Do not answer #.\"\n\n        4. Run `python multi_language.py`.\n            Note: You need to run it multiple times to increase translation coverage because GPT makes mistakes sometimes.\n           (You can also run `CACHE_ONLY=True python multi_language.py` to use cached translation mapping)\n\n        5. Find the translated program in `multi-language\\English\\*`\n\n    P.S.\n\n        - The translation mapping will be stored in `docs/translation_xxxx.json`, you can revised mistaken translation there.\n\n        - If you would like to share your `docs/translation_xxxx.json`, (so that everyone can use the cached & revised translation mapping), please open a Pull Request\n\n        - If there is any translation error in `docs/translation_xxxx.json`, please open a Pull Request\n\n        - Welcome any Pull Request, regardless of language\n\"\"\"\n\nimport os\nimport json\nimport functools\nimport re\nimport pickle\nimport time\nfrom toolbox import get_conf\n\nCACHE_ONLY = os.environ.get('CACHE_ONLY', False)\n\nCACHE_FOLDER = get_conf('PATH_LOGGING')\n\nblacklist = ['multi-language', CACHE_FOLDER, '.git', 'private_upload', 'multi_language.py', 'build', '.github', '.vscode', '__pycache__', 'venv']\n\n# LANG = \"TraditionalChinese\"\n# TransPrompt = f\"Replace each json value `#` with translated results in Traditional Chinese, e.g., \\\"原始文本\\\":\\\"翻譯後文字\\\". Keep Json format. Do not answer #.\"\n\n# LANG = \"Japanese\"\n# TransPrompt = f\"Replace each json value `#` with translated results in Japanese, e.g., \\\"原始文本\\\":\\\"テキストの翻訳\\\". Keep Json format. Do not answer #.\"\n\nLANG = \"English\"\nTransPrompt = f\"Replace each json value `#` with translated results in English, e.g., \\\"原始文本\\\":\\\"TranslatedText\\\". Keep Json format. Do not answer #.\"\n\n\nif not os.path.exists(CACHE_FOLDER):\n    os.makedirs(CACHE_FOLDER)\n\n\ndef lru_file_cache(maxsize=128, ttl=None, filename=None):\n    \"\"\"\n    Decorator that caches a function's return value after being called with given arguments.\n    It uses a Least Recently Used (LRU) cache strategy to limit the size of the cache.\n    maxsize: Maximum size of the cache. Defaults to 128.\n    ttl: Time-to-Live of the cache. If a value hasn't been accessed for `ttl` seconds, it will be evicted from the cache.\n    filename: Name of the file to store the cache in. If not supplied, the function name + \".cache\" will be used.\n    \"\"\"\n    cache_path = os.path.join(CACHE_FOLDER, f\"{filename}.cache\") if filename is not None else None\n\n    def decorator_function(func):\n        cache = {}\n        _cache_info = {\n            \"hits\": 0,\n            \"misses\": 0,\n            \"maxsize\": maxsize,\n            \"currsize\": 0,\n            \"ttl\": ttl,\n            \"filename\": cache_path,\n        }\n\n        @functools.wraps(func)\n        def wrapper_function(*args, **kwargs):\n            key = str((args, frozenset(kwargs)))\n            if key in cache:\n                if _cache_info[\"ttl\"] is None or (cache[key][1] + _cache_info[\"ttl\"]) >= time.time():\n                    _cache_info[\"hits\"] += 1\n                    print(f'Warning, reading cache, last read {(time.time()-cache[key][1])//60} minutes ago'); time.sleep(2)\n                    cache[key][1] = time.time()\n                    return cache[key][0]\n                else:\n                    del cache[key]\n\n            result = func(*args, **kwargs)\n            cache[key] = [result, time.time()]\n            _cache_info[\"misses\"] += 1\n            _cache_info[\"currsize\"] += 1\n\n            if _cache_info[\"currsize\"] > _cache_info[\"maxsize\"]:\n                oldest_key = None\n                for k in cache:\n                    if oldest_key is None:\n                        oldest_key = k\n                    elif cache[k][1] < cache[oldest_key][1]:\n                        oldest_key = k\n                del cache[oldest_key]\n                _cache_info[\"currsize\"] -= 1\n\n            if cache_path is not None:\n                with open(cache_path, \"wb\") as f:\n                    pickle.dump(cache, f)\n\n            return result\n\n        def cache_info():\n            return _cache_info\n\n        wrapper_function.cache_info = cache_info\n\n        if cache_path is not None and os.path.exists(cache_path):\n            with open(cache_path, \"rb\") as f:\n                cache = pickle.load(f)\n            _cache_info[\"currsize\"] = len(cache)\n\n        return wrapper_function\n\n    return decorator_function\n\ndef contains_chinese(string):\n    \"\"\"\n    Returns True if the given string contains Chinese characters, False otherwise.\n    \"\"\"\n    chinese_regex = re.compile(u'[\\u4e00-\\u9fff]+')\n    return chinese_regex.search(string) is not None\n\ndef split_list(lst, n_each_req):\n    \"\"\"\n    Split a list into smaller lists, each with a maximum number of elements.\n    :param lst: the list to split\n    :param n_each_req: the maximum number of elements in each sub-list\n    :return: a list of sub-lists\n    \"\"\"\n    result = []\n    for i in range(0, len(lst), n_each_req):\n        result.append(lst[i:i + n_each_req])\n    return result\n\ndef map_to_json(map, language):\n    dict_ = read_map_from_json(language)\n    dict_.update(map)\n    with open(f'docs/translate_{language.lower()}.json', 'w', encoding='utf8') as f:\n        json.dump(dict_, f, indent=4, ensure_ascii=False)\n\ndef read_map_from_json(language):\n    if os.path.exists(f'docs/translate_{language.lower()}.json'):\n        with open(f'docs/translate_{language.lower()}.json', 'r', encoding='utf8') as f:\n            res = json.load(f)\n            res = {k:v for k, v in res.items() if v is not None and contains_chinese(k)}\n            return res\n    return {}\n\ndef advanced_split(splitted_string, spliter, include_spliter=False):\n    splitted_string_tmp = []\n    for string_ in splitted_string:\n        if spliter in string_:\n            splitted = string_.split(spliter)\n            for i, s in enumerate(splitted):\n                if include_spliter:\n                    if i != len(splitted)-1:\n                        splitted[i] += spliter\n                splitted[i] = splitted[i].strip()\n            for i in reversed(range(len(splitted))):\n                if not contains_chinese(splitted[i]):\n                    splitted.pop(i)\n            splitted_string_tmp.extend(splitted)\n        else:\n            splitted_string_tmp.append(string_)\n    splitted_string = splitted_string_tmp\n    return splitted_string_tmp\n\ncached_translation = {}\ncached_translation = read_map_from_json(language=LANG)\n\ndef trans(word_to_translate, language, special=False):\n    if len(word_to_translate) == 0: return {}\n    from crazy_functions.crazy_utils import request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency\n    from toolbox import get_conf, ChatBotWithCookies, load_chat_cookies\n\n    cookies = load_chat_cookies()\n    llm_kwargs = {\n        'api_key': cookies['api_key'],\n        'llm_model': cookies['llm_model'],\n        'top_p':1.0,\n        'max_length': None,\n        'temperature':0.4,\n    }\n    import random\n    N_EACH_REQ = random.randint(16, 32)\n    word_to_translate_split = split_list(word_to_translate, N_EACH_REQ)\n    inputs_array = [str(s) for s in word_to_translate_split]\n    inputs_show_user_array = inputs_array\n    history_array = [[] for _ in inputs_array]\n    if special: #  to English using CamelCase Naming Convention\n        sys_prompt_array = [f\"Translate following names to English with CamelCase naming convention. Keep original format\" for _ in inputs_array]\n    else:\n        sys_prompt_array = [f\"Translate following sentences to {LANG}. E.g., You should translate sentences to the following format ['translation of sentence 1', 'translation of sentence 2']. Do NOT answer with Chinese!\" for _ in inputs_array]\n    chatbot = ChatBotWithCookies(llm_kwargs)\n    gpt_say_generator = request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency(\n        inputs_array,\n        inputs_show_user_array,\n        llm_kwargs,\n        chatbot,\n        history_array,\n        sys_prompt_array,\n    )\n    while True:\n        try:\n            gpt_say = next(gpt_say_generator)\n            print(gpt_say[1][0][1])\n        except StopIteration as e:\n            result = e.value\n            break\n    translated_result = {}\n    for i, r in enumerate(result):\n        if i%2 == 1:\n            try:\n                res_before_trans = eval(result[i-1])\n                res_after_trans = eval(result[i])\n                if len(res_before_trans) != len(res_after_trans):\n                    raise RuntimeError\n                for a,b in zip(res_before_trans, res_after_trans):\n                    translated_result[a] = b\n            except:\n                # try:\n                    # res_before_trans = word_to_translate_split[(i-1)//2]\n                    # res_after_trans = [s for s in result[i].split(\"', '\")]\n                #     for a,b in zip(res_before_trans, res_after_trans):\n                #         translated_result[a] = b\n                # except:\n                print('GPT answers with unexpected format, some words may not be translated, but you can try again later to increase translation coverage.')\n                res_before_trans = eval(result[i-1])\n                for a in res_before_trans:\n                    translated_result[a] = None\n    return translated_result\n\n\ndef trans_json(word_to_translate, language, special=False):\n    if len(word_to_translate) == 0: return {}\n    from crazy_functions.crazy_utils import request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency\n    from toolbox import get_conf, ChatBotWithCookies, load_chat_cookies\n\n    cookies = load_chat_cookies()\n    llm_kwargs = {\n        'api_key': cookies['api_key'],\n        'llm_model': cookies['llm_model'],\n        'top_p':1.0,\n        'max_length': None,\n        'temperature':0.4,\n    }\n    import random\n    N_EACH_REQ = random.randint(16, 32)\n    random.shuffle(word_to_translate)\n    word_to_translate_split = split_list(word_to_translate, N_EACH_REQ)\n    inputs_array = [{k:\"#\" for k in s} for s in word_to_translate_split]\n    inputs_array = [ json.dumps(i, ensure_ascii=False)  for i in inputs_array]\n\n    inputs_show_user_array = inputs_array\n    history_array = [[] for _ in inputs_array]\n    sys_prompt_array = [TransPrompt for _ in inputs_array]\n    chatbot = ChatBotWithCookies(llm_kwargs)\n    gpt_say_generator = request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency(\n        inputs_array,\n        inputs_show_user_array,\n        llm_kwargs,\n        chatbot,\n        history_array,\n        sys_prompt_array,\n    )\n    while True:\n        try:\n            gpt_say = next(gpt_say_generator)\n            print(gpt_say[1][0][1])\n        except StopIteration as e:\n            result = e.value\n            break\n    translated_result = {}\n    for i, r in enumerate(result):\n        if i%2 == 1:\n            try:\n                translated_result.update(json.loads(result[i]))\n            except:\n                print(result[i])\n    print(result)\n    return translated_result\n\n\ndef step_1_core_key_translate():\n    LANG_STD = 'std'\n    def extract_chinese_characters(file_path):\n        syntax = []\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            import ast\n            root = ast.parse(content)\n            for node in ast.walk(root):\n                if isinstance(node, ast.Name):\n                    if contains_chinese(node.id): syntax.append(node.id)\n                if isinstance(node, ast.Import):\n                    for n in node.names:\n                        if contains_chinese(n.name): syntax.append(n.name)\n                elif isinstance(node, ast.ImportFrom):\n                    for n in node.names:\n                        if contains_chinese(n.name): syntax.append(n.name)\n                        # if node.module is None: print(node.module)\n                        for k in node.module.split('.'):\n                            if contains_chinese(k): syntax.append(k)\n            return syntax\n\n    def extract_chinese_characters_from_directory(directory_path):\n        chinese_characters = []\n        for root, dirs, files in os.walk(directory_path):\n            if any([b in root for b in blacklist]):\n                continue\n            print(files)\n            for file in files:\n                if file.endswith('.py'):\n                    file_path = os.path.join(root, file)\n                    chinese_characters.extend(extract_chinese_characters(file_path))\n        return chinese_characters\n\n    directory_path = './'\n    chinese_core_names = extract_chinese_characters_from_directory(directory_path)\n    chinese_core_keys = [name for name in chinese_core_names]\n    chinese_core_keys_norepeat = []\n    for d in chinese_core_keys:\n        if d not in chinese_core_keys_norepeat: chinese_core_keys_norepeat.append(d)\n    need_translate = []\n    cached_translation = read_map_from_json(language=LANG_STD)\n    cached_translation_keys = list(cached_translation.keys())\n    for d in chinese_core_keys_norepeat:\n        if d not in cached_translation_keys:\n            need_translate.append(d)\n\n    if CACHE_ONLY:\n        need_translate_mapping = {}\n    else:\n        need_translate_mapping = trans(need_translate, language=LANG_STD, special=True)\n    map_to_json(need_translate_mapping, language=LANG_STD)\n    cached_translation = read_map_from_json(language=LANG_STD)\n    cached_translation = dict(sorted(cached_translation.items(), key=lambda x: -len(x[0])))\n\n    chinese_core_keys_norepeat_mapping = {}\n    for k in chinese_core_keys_norepeat:\n        chinese_core_keys_norepeat_mapping.update({k:cached_translation[k]})\n    chinese_core_keys_norepeat_mapping = dict(sorted(chinese_core_keys_norepeat_mapping.items(), key=lambda x: -len(x[0])))\n\n    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n    # copy\n    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n    def copy_source_code():\n\n        from toolbox import get_conf\n        import shutil\n        import os\n        try: shutil.rmtree(f'./multi-language/{LANG}/')\n        except: pass\n        os.makedirs(f'./multi-language', exist_ok=True)\n        backup_dir = f'./multi-language/{LANG}/'\n        shutil.copytree('./', backup_dir, ignore=lambda x, y: blacklist)\n    copy_source_code()\n\n    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n    # primary key replace\n    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n    directory_path = f'./multi-language/{LANG}/'\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            if file.endswith('.py'):\n                file_path = os.path.join(root, file)\n                syntax = []\n                # read again\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n\n                for k, v in chinese_core_keys_norepeat_mapping.items():\n                    content = content.replace(k, v)\n\n                with open(file_path, 'w', encoding='utf-8') as f:\n                    f.write(content)\n\n\ndef step_2_core_key_translate():\n\n    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n    # step2\n    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n\n    def load_string(strings, string_input):\n        string_ = string_input.strip().strip(',').strip().strip('.').strip()\n        if string_.startswith('[Local Message]'):\n            string_ = string_.replace('[Local Message]', '')\n            string_ = string_.strip().strip(',').strip().strip('.').strip()\n        splitted_string = [string_]\n        # --------------------------------------\n        splitted_string = advanced_split(splitted_string, spliter=\"，\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"。\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"）\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"（\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"(\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\")\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"<\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\">\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"[\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"]\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"【\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"】\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"？\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"：\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\":\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\",\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"#\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"\\n\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\";\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"`\", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"   \", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"- \", include_spliter=False)\n        splitted_string = advanced_split(splitted_string, spliter=\"---\", include_spliter=False)\n\n        # --------------------------------------\n        for j, s in enumerate(splitted_string): # .com\n            if '.com' in s: continue\n            if '\\'' in s: continue\n            if '\\\"' in s: continue\n            strings.append([s,0])\n\n\n    def get_strings(node):\n        strings = []\n        # recursively traverse the AST\n        for child in ast.iter_child_nodes(node):\n            node = child\n            if isinstance(child, ast.Str):\n                if contains_chinese(child.s):\n                    load_string(strings=strings, string_input=child.s)\n            elif isinstance(child, ast.AST):\n                strings.extend(get_strings(child))\n        return strings\n\n    string_literals = []\n    directory_path = f'./multi-language/{LANG}/'\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            if file.endswith('.py'):\n                file_path = os.path.join(root, file)\n                syntax = []\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    # comments\n                    comments_arr = []\n                    for code_sp in content.splitlines():\n                        comments = re.findall(r'#.*$', code_sp)\n                        for comment in comments:\n                            load_string(strings=comments_arr, string_input=comment)\n                    string_literals.extend(comments_arr)\n\n                    # strings\n                    import ast\n                    tree = ast.parse(content)\n                    res = get_strings(tree, )\n                    string_literals.extend(res)\n\n    [print(s) for s in string_literals]\n    chinese_literal_names = []\n    chinese_literal_names_norepeat = []\n    for string, offset in string_literals:\n        chinese_literal_names.append(string)\n    chinese_literal_names_norepeat = []\n    for d in chinese_literal_names:\n        if d not in chinese_literal_names_norepeat: chinese_literal_names_norepeat.append(d)\n    need_translate = []\n    cached_translation = read_map_from_json(language=LANG)\n    cached_translation_keys = list(cached_translation.keys())\n    for d in chinese_literal_names_norepeat:\n        if d not in cached_translation_keys:\n            need_translate.append(d)\n\n    if CACHE_ONLY:\n        up = {}\n    else:\n        up = trans_json(need_translate, language=LANG, special=False)\n    map_to_json(up, language=LANG)\n    cached_translation = read_map_from_json(language=LANG)\n    LANG_STD = 'std'\n    cached_translation.update(read_map_from_json(language=LANG_STD))\n    cached_translation = dict(sorted(cached_translation.items(), key=lambda x: -len(x[0])))\n\n    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n    # literal key replace\n    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n    directory_path = f'./multi-language/{LANG}/'\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            if file.endswith('.py'):\n                file_path = os.path.join(root, file)\n                syntax = []\n                # read again\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n\n                for k, v in cached_translation.items():\n                    if v is None: continue\n                    if '\"' in v:\n                        v = v.replace('\"', \"`\")\n                    if '\\'' in v:\n                        v = v.replace('\\'', \"`\")\n                    content = content.replace(k, v)\n\n                with open(file_path, 'w', encoding='utf-8') as f:\n                    f.write(content)\n\n                if file.strip('.py') in cached_translation:\n                    file_new = cached_translation[file.strip('.py')] + '.py'\n                    file_path_new = os.path.join(root, file_new)\n                    with open(file_path_new, 'w', encoding='utf-8') as f:\n                        f.write(content)\n                    os.remove(file_path)\nstep_1_core_key_translate()\nstep_2_core_key_translate()\nprint('Finished, checkout generated results at ./multi-language/')"
        },
        {
          "name": "request_llms",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.7822265625,
          "content": "https://public.agent-matrix.com/publish/gradio-3.32.12-py3-none-any.whl\nfastapi==0.110\ngradio-client==0.8\npypdf2==2.12.1\nhttpx<=0.25.2\nzhipuai==2.0.1\ntiktoken>=0.3.3\nrequests[socks]\npydantic==2.9.2\nprotobuf==3.20\ntransformers>=4.27.1,<4.42\nscipdf_parser>=0.52\nspacy==3.7.4\nanthropic>=0.18.1\npython-markdown-math\npymdown-extensions>=10.14\nwebsocket-client\nbeautifulsoup4\nprompt_toolkit\nlatex2mathml\npython-docx\nmdtex2html\ndashscope\npyautogen\ncolorama\nMarkdown\npygments\nedge-tts>=7.0.0\npymupdf\nopenai\nrjsmin\nloguru\narxiv\nnumpy\nrich\n\n\nllama-index-core==0.10.68\nllama-index-legacy==0.9.48\nllama-index-readers-file==0.1.33\nllama-index-readers-llama-parse==0.1.6\nllama-index-embeddings-azure-openai==0.1.10\nllama-index-embeddings-openai==0.1.10\nllama-parse==0.4.9\nmdit-py-plugins>=0.3.3\nlinkify-it-py==2.0.3"
        },
        {
          "name": "shared_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "themes",
          "type": "tree",
          "content": null
        },
        {
          "name": "toolbox.py",
          "type": "blob",
          "size": 39.46484375,
          "content": "\nimport importlib\nimport time\nimport inspect\nimport re\nimport os\nimport base64\nimport gradio\nimport shutil\nimport glob\nimport json\nimport uuid\nfrom loguru import logger\nfrom functools import wraps\nfrom textwrap import dedent\nfrom shared_utils.config_loader import get_conf\nfrom shared_utils.config_loader import set_conf\nfrom shared_utils.config_loader import set_multi_conf\nfrom shared_utils.config_loader import read_single_conf_with_lru_cache\nfrom shared_utils.advanced_markdown_format import format_io\nfrom shared_utils.advanced_markdown_format import markdown_convertion\nfrom shared_utils.key_pattern_manager import select_api_key\nfrom shared_utils.key_pattern_manager import is_any_api_key\nfrom shared_utils.key_pattern_manager import what_keys\nfrom shared_utils.connect_void_terminal import get_chat_handle\nfrom shared_utils.connect_void_terminal import get_plugin_handle\nfrom shared_utils.connect_void_terminal import get_plugin_default_kwargs\nfrom shared_utils.connect_void_terminal import get_chat_default_kwargs\nfrom shared_utils.text_mask import apply_gpt_academic_string_mask\nfrom shared_utils.text_mask import build_gpt_academic_masked_string\nfrom shared_utils.text_mask import apply_gpt_academic_string_mask_langbased\nfrom shared_utils.text_mask import build_gpt_academic_masked_string_langbased\nfrom shared_utils.map_names import map_friendly_names_to_model\nfrom shared_utils.map_names import map_model_to_friendly_names\nfrom shared_utils.map_names import read_one_api_model_name\nfrom shared_utils.handle_upload import html_local_file\nfrom shared_utils.handle_upload import html_local_img\nfrom shared_utils.handle_upload import file_manifest_filter_type\nfrom shared_utils.handle_upload import extract_archive\nfrom typing import List\npj = os.path.join\ndefault_user_name = \"default_user\"\n\n\"\"\"\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n第一部分\n函数插件输入输出接驳区\n    - ChatBotWithCookies:   带Cookies的Chatbot类，为实现更多强大的功能做基础\n    - ArgsGeneralWrapper:   装饰器函数，用于重组输入参数，改变输入参数的顺序与结构\n    - update_ui:            刷新界面用 yield from update_ui(chatbot, history)\n    - CatchException:       将插件中出的所有问题显示在界面上\n    - HotReload:            实现插件的热更新\n    - trimmed_format_exc:   打印traceback，为了安全而隐藏绝对地址\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n\"\"\"\n\n\nclass ChatBotWithCookies(list):\n    def __init__(self, cookie):\n        \"\"\"\n        cookies = {\n            'top_p': top_p,\n            'temperature': temperature,\n            'lock_plugin': bool,\n            \"files_to_promote\": [\"file1\", \"file2\"],\n            \"most_recent_uploaded\": {\n                \"path\": \"uploaded_path\",\n                \"time\": time.time(),\n                \"time_str\": \"timestr\",\n            }\n        }\n        \"\"\"\n        self._cookies = cookie\n\n    def write_list(self, list):\n        for t in list:\n            self.append(t)\n\n    def get_list(self):\n        return [t for t in self]\n\n    def get_cookies(self):\n        return self._cookies\n\n    def get_user(self):\n        return self._cookies.get(\"user_name\", default_user_name)\n\ndef ArgsGeneralWrapper(f):\n    \"\"\"\n    装饰器函数ArgsGeneralWrapper，用于重组输入参数，改变输入参数的顺序与结构。\n    该装饰器是大多数功能调用的入口。\n    函数示意图：https://mermaid.live/edit#pako:eNqNVFtPGkEY_StkntoEDQtLoTw0sWqapjQxVWPabmOm7AiEZZcsQ9QiiW012qixqdeqqIn10geBh6ZR8PJnmAWe-hc6l3VhrWnLEzNzzvnO953ZyYOYoSIQAWOaMR5LQBN7hvoU3UN_g5iu7imAXEyT4wUF3Pd0dT3y9KGYYUJsmK8V0GPGs0-QjkyojZgwk0Fm82C2dVghX08U8EaoOHjOfoEMU0XmADRhOksVWnNLjdpM82qFzB6S5Q_WWsUhuqCc3JtAsVR_OoMnhyZwXgHWwbS1d4gnsLVZJp-P6mfVxveqAgqC70Jz_pQCOGDKM5xFdNNPDdilF6uSU_hOYqu4a3MHYDZLDzq5fodrC3PWcEaFGPUaRiqJWK_W9g9rvRITa4dhy_0nw67SiePMp3oSR6PPn41DGgllkvkizYwsrmtaejTFd8V4yekGmT1zqrt4XGlAy8WTuiPULF01LksZvukSajfQQRAxmYi5S0D81sDcyzapVdn6sYFHkjhhGyel3frVQnvsnbR23lEjlhIlaOJiFPWzU5G4tfNJo8ejwp47-TbvJkKKZvmxA6SKo16oaazJysfG6klr9T0pbTW2ZqzlL_XaT8fYbQLXe4mSmvoCZXMaa7FePW6s7jVqK9bujvse3WFjY5_Z4KfsA4oiPY4T7Drvn1tLJTbG1to1qR79ulgk89-oJbvZzbIwJty6u20LOReWa9BvwserUd9s9MIKc3x5TUWEoAhUyJK5y85w_yG-dFu_R9waoU7K581y8W_qLle35-rG9Nxcrz8QHRsc0K-r9NViYRT36KsFvCCNzDRMqvSVyzOKAnACpZECIvSvCs2UAhS9QHEwh43BST0GItjMIS_I8e-sLwnj9A262cxA_ZVh0OUY1LJiDSJ5MAEiUijYLUtBORR6KElyQPaCSRDpksNSd8AfluSgHPaFC17wjrOlbgbzyyFf4IFPDvoD_sJvnkdK-g\n    \"\"\"\n    def decorated(request: gradio.Request, cookies:dict, max_length:int, llm_model:str,\n                  txt:str, txt2:str, top_p:float, temperature:float, chatbot:list,\n                  json_history:str, system_prompt:str, plugin_advanced_arg:dict, *args):\n        txt_passon = txt\n        history = json.loads(json_history) if json_history else []\n        if txt == \"\" and txt2 != \"\": txt_passon = txt2\n        # 引入一个有cookie的chatbot\n        if request.username is not None:\n            user_name = request.username\n        else:\n            user_name = default_user_name\n        embed_model = get_conf(\"EMBEDDING_MODEL\")\n        cookies.update({\n            'top_p': top_p,\n            'api_key': cookies['api_key'],\n            'llm_model': llm_model,\n            'embed_model': embed_model,\n            'temperature': temperature,\n            'user_name': user_name,\n        })\n        llm_kwargs = {\n            'api_key': cookies['api_key'],\n            'llm_model': llm_model,\n            'embed_model': embed_model,\n            'top_p': top_p,\n            'max_length': max_length,\n            'temperature': temperature,\n            'client_ip': request.client.host,\n            'most_recent_uploaded': cookies.get('most_recent_uploaded')\n        }\n        if isinstance(plugin_advanced_arg, str):\n            plugin_kwargs = {\"advanced_arg\": plugin_advanced_arg}\n        else:\n            plugin_kwargs = plugin_advanced_arg\n        chatbot_with_cookie = ChatBotWithCookies(cookies)\n        chatbot_with_cookie.write_list(chatbot)\n\n        if cookies.get('lock_plugin', None) is None:\n            # 正常状态\n            if len(args) == 0:  # 插件通道\n                yield from f(txt_passon, llm_kwargs, plugin_kwargs, chatbot_with_cookie, history, system_prompt, request)\n            else:               # 对话通道，或者基础功能通道\n                yield from f(txt_passon, llm_kwargs, plugin_kwargs, chatbot_with_cookie, history, system_prompt, *args)\n        else:\n            # 处理少数情况下的特殊插件的锁定状态\n            module, fn_name = cookies['lock_plugin'].split('->')\n            f_hot_reload = getattr(importlib.import_module(module, fn_name), fn_name)\n            yield from f_hot_reload(txt_passon, llm_kwargs, plugin_kwargs, chatbot_with_cookie, history, system_prompt, request)\n            # 判断一下用户是否错误地通过对话通道进入，如果是，则进行提醒\n            final_cookies = chatbot_with_cookie.get_cookies()\n            # len(args) != 0 代表“提交”键对话通道，或者基础功能通道\n            if len(args) != 0 and 'files_to_promote' in final_cookies and len(final_cookies['files_to_promote']) > 0:\n                chatbot_with_cookie.append(\n                    [\"检测到**滞留的缓存文档**，请及时处理。\", \"请及时点击“**保存当前对话**”获取所有滞留文档。\"])\n                yield from update_ui(chatbot_with_cookie, final_cookies['history'], msg=\"检测到被滞留的缓存文档\")\n\n    return decorated\n\n\ndef update_ui(chatbot:ChatBotWithCookies, history:list, msg:str=\"正常\", **kwargs):  # 刷新界面\n    \"\"\"\n    刷新用户界面\n    \"\"\"\n    assert isinstance(history, list), \"history必须是一个list\"\n    assert isinstance(\n        chatbot, ChatBotWithCookies\n    ), \"在传递chatbot的过程中不要将其丢弃。必要时, 可用clear将其清空, 然后用for+append循环重新赋值。\"\n    cookies = chatbot.get_cookies()\n    # 备份一份History作为记录\n    cookies.update({\"history\": history})\n    # 解决插件锁定时的界面显示问题\n    if cookies.get(\"lock_plugin\", None):\n        label = (\n            cookies.get(\"llm_model\", \"\")\n            + \" | \"\n            + \"正在锁定插件\"\n            + cookies.get(\"lock_plugin\", None)\n        )\n        chatbot_gr = gradio.update(value=chatbot, label=label)\n        if cookies.get(\"label\", \"\") != label:\n            cookies[\"label\"] = label  # 记住当前的label\n    elif cookies.get(\"label\", None):\n        chatbot_gr = gradio.update(value=chatbot, label=cookies.get(\"llm_model\", \"\"))\n        cookies[\"label\"] = None  # 清空label\n    else:\n        chatbot_gr = chatbot\n\n    history = [str(history_item) for history_item in history] # ensure all items are string\n    json_history = json.dumps(history, ensure_ascii=False)\n    yield cookies, chatbot_gr, json_history, msg\n\n\ndef update_ui_lastest_msg(lastmsg:str, chatbot:ChatBotWithCookies, history:list, delay:float=1, msg:str=\"正常\"):  # 刷新界面\n    \"\"\"\n    刷新用户界面\n    \"\"\"\n    if len(chatbot) == 0:\n        chatbot.append([\"update_ui_last_msg\", lastmsg])\n    chatbot[-1] = list(chatbot[-1])\n    chatbot[-1][-1] = lastmsg\n    yield from update_ui(chatbot=chatbot, history=history, msg=msg)\n    time.sleep(delay)\n\n\ndef trimmed_format_exc():\n    import os, traceback\n\n    str = traceback.format_exc()\n    current_path = os.getcwd()\n    replace_path = \".\"\n    return str.replace(current_path, replace_path)\n\n\ndef trimmed_format_exc_markdown():\n    return '\\n\\n```\\n' + trimmed_format_exc() + '```'\n\n\nclass FriendlyException(Exception):\n    def generate_error_html(self):\n        return dedent(f\"\"\"\n            <div class=\"center-div\" style=\"color: crimson;text-align: center;\">\n                {\"<br>\".join(self.args)}\n            </div>\n        \"\"\")\n\n\ndef CatchException(f):\n    \"\"\"\n    装饰器函数，捕捉函数f中的异常并封装到一个生成器中返回，并显示到聊天当中。\n    \"\"\"\n\n    @wraps(f)\n    def decorated(main_input:str, llm_kwargs:dict, plugin_kwargs:dict,\n                  chatbot_with_cookie:ChatBotWithCookies, history:list, *args, **kwargs):\n        try:\n            yield from f(main_input, llm_kwargs, plugin_kwargs, chatbot_with_cookie, history, *args, **kwargs)\n        except FriendlyException as e:\n            tb_str = '```\\n' + trimmed_format_exc() + '```'\n            if len(chatbot_with_cookie) == 0:\n                chatbot_with_cookie.clear()\n                chatbot_with_cookie.append([\"插件调度异常:\\n\" + tb_str, None])\n            chatbot_with_cookie[-1] = [chatbot_with_cookie[-1][0], e.generate_error_html()]\n            yield from update_ui(chatbot=chatbot_with_cookie, history=history, msg=f'异常')  # 刷新界面\n        except Exception as e:\n            tb_str = '```\\n' + trimmed_format_exc() + '```'\n            if len(chatbot_with_cookie) == 0:\n                chatbot_with_cookie.clear()\n                chatbot_with_cookie.append([\"插件调度异常\", \"异常原因\"])\n            chatbot_with_cookie[-1] = [chatbot_with_cookie[-1][0], f\"[Local Message] 插件调用出错: \\n\\n{tb_str} \\n\"]\n            yield from update_ui(chatbot=chatbot_with_cookie, history=history, msg=f'异常 {e}')  # 刷新界面\n\n    return decorated\n\n\ndef HotReload(f):\n    \"\"\"\n    HotReload的装饰器函数，用于实现Python函数插件的热更新。\n    函数热更新是指在不停止程序运行的情况下，更新函数代码，从而达到实时更新功能。\n    在装饰器内部，使用wraps(f)来保留函数的元信息，并定义了一个名为decorated的内部函数。\n    内部函数通过使用importlib模块的reload函数和inspect模块的getmodule函数来重新加载并获取函数模块，\n    然后通过getattr函数获取函数名，并在新模块中重新加载函数。\n    最后，使用yield from语句返回重新加载过的函数，并在被装饰的函数上执行。\n    最终，装饰器函数返回内部函数。这个内部函数可以将函数的原始定义更新为最新版本，并执行函数的新版本。\n    \"\"\"\n    if get_conf(\"PLUGIN_HOT_RELOAD\"):\n\n        @wraps(f)\n        def decorated(*args, **kwargs):\n            fn_name = f.__name__\n            f_hot_reload = getattr(importlib.reload(inspect.getmodule(f)), fn_name)\n            yield from f_hot_reload(*args, **kwargs)\n\n        return decorated\n    else:\n        return f\n\n\n\"\"\"\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n第二部分\n其他小工具:\n    - write_history_to_file:    将结果写入markdown文件中\n    - regular_txt_to_markdown:  将普通文本转换为Markdown格式的文本。\n    - report_exception:         向chatbot中添加简单的意外错误信息\n    - text_divide_paragraph:    将文本按照段落分隔符分割开，生成带有段落标签的HTML代码。\n    - markdown_convertion:      用多种方式组合，将markdown转化为好看的html\n    - format_io:                接管gradio默认的markdown处理方式\n    - on_file_uploaded:         处理文件的上传（自动解压）\n    - on_report_generated:      将生成的报告自动投射到文件上传区\n    - clip_history:             当历史上下文过长时，自动截断\n    - get_conf:                 获取设置\n    - select_api_key:           根据当前的模型类别，抽取可用的api-key\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n\"\"\"\n\n\ndef get_reduce_token_percent(text:str):\n    \"\"\"\n    * 此函数未来将被弃用\n    \"\"\"\n    try:\n        # text = \"maximum context length is 4097 tokens. However, your messages resulted in 4870 tokens\"\n        pattern = r\"(\\d+)\\s+tokens\\b\"\n        match = re.findall(pattern, text)\n        EXCEED_ALLO = 500  # 稍微留一点余地，否则在回复时会因余量太少出问题\n        max_limit = float(match[0]) - EXCEED_ALLO\n        current_tokens = float(match[1])\n        ratio = max_limit / current_tokens\n        assert ratio > 0 and ratio < 1\n        return ratio, str(int(current_tokens - max_limit))\n    except:\n        return 0.5, \"不详\"\n\n\ndef write_history_to_file(\n    history:list, file_basename:str=None, file_fullname:str=None, auto_caption:bool=True\n):\n    \"\"\"\n    将对话记录history以Markdown格式写入文件中。如果没有指定文件名，则使用当前时间生成文件名。\n    \"\"\"\n    import os\n    import time\n\n    if file_fullname is None:\n        if file_basename is not None:\n            file_fullname = pj(get_log_folder(), file_basename)\n        else:\n            file_fullname = pj(get_log_folder(), f\"GPT-Academic-{gen_time_str()}.md\")\n    os.makedirs(os.path.dirname(file_fullname), exist_ok=True)\n    with open(file_fullname, \"w\", encoding=\"utf8\") as f:\n        f.write(\"# GPT-Academic Report\\n\")\n        for i, content in enumerate(history):\n            try:\n                if type(content) != str:\n                    content = str(content)\n            except:\n                continue\n            if i % 2 == 0 and auto_caption:\n                f.write(\"## \")\n            try:\n                f.write(content)\n            except:\n                # remove everything that cannot be handled by utf8\n                f.write(content.encode(\"utf-8\", \"ignore\").decode())\n            f.write(\"\\n\\n\")\n    res = os.path.abspath(file_fullname)\n    return res\n\n\ndef regular_txt_to_markdown(text:str):\n    \"\"\"\n    将普通文本转换为Markdown格式的文本。\n    \"\"\"\n    text = text.replace(\"\\n\", \"\\n\\n\")\n    text = text.replace(\"\\n\\n\\n\", \"\\n\\n\")\n    text = text.replace(\"\\n\\n\\n\", \"\\n\\n\")\n    return text\n\n\ndef report_exception(chatbot:ChatBotWithCookies, history:list, a:str, b:str):\n    \"\"\"\n    向chatbot中添加错误信息\n    \"\"\"\n    chatbot.append((a, b))\n    history.extend([a, b])\n\n\ndef find_free_port()->int:\n    \"\"\"\n    返回当前系统中可用的未使用端口。\n    \"\"\"\n    import socket\n    from contextlib import closing\n\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n        s.bind((\"\", 0))\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        return s.getsockname()[1]\n\n\ndef find_recent_files(directory:str)->List[str]:\n    \"\"\"\n    Find files that is created with in one minutes under a directory with python, write a function\n    \"\"\"\n    import os\n    import time\n\n    current_time = time.time()\n    one_minute_ago = current_time - 60\n    recent_files = []\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    for filename in os.listdir(directory):\n        file_path = pj(directory, filename)\n        if file_path.endswith(\".log\"):\n            continue\n        created_time = os.path.getmtime(file_path)\n        if created_time >= one_minute_ago:\n            if os.path.isdir(file_path):\n                continue\n            recent_files.append(file_path)\n\n    return recent_files\n\n\ndef file_already_in_downloadzone(file:str, user_path:str):\n    try:\n        parent_path = os.path.abspath(user_path)\n        child_path = os.path.abspath(file)\n        if os.path.samefile(os.path.commonpath([parent_path, child_path]), parent_path):\n            return True\n        else:\n            return False\n    except:\n        return False\n\n\ndef promote_file_to_downloadzone(file:str, rename_file:str=None, chatbot:ChatBotWithCookies=None):\n    # 将文件复制一份到下载区\n    import shutil\n\n    if chatbot is not None:\n        user_name = get_user(chatbot)\n    else:\n        user_name = default_user_name\n    if not os.path.exists(file):\n        raise FileNotFoundError(f\"文件{file}不存在\")\n    user_path = get_log_folder(user_name, plugin_name=None)\n    if file_already_in_downloadzone(file, user_path):\n        new_path = file\n    else:\n        user_path = get_log_folder(user_name, plugin_name=\"downloadzone\")\n        if rename_file is None:\n            rename_file = f\"{gen_time_str()}-{os.path.basename(file)}\"\n        new_path = pj(user_path, rename_file)\n        # 如果已经存在，先删除\n        if os.path.exists(new_path) and not os.path.samefile(new_path, file):\n            os.remove(new_path)\n        # 把文件复制过去\n        if not os.path.exists(new_path):\n            shutil.copyfile(file, new_path)\n    # 将文件添加到chatbot cookie中\n    if chatbot is not None:\n        if \"files_to_promote\" in chatbot._cookies:\n            current = chatbot._cookies[\"files_to_promote\"]\n        else:\n            current = []\n        if new_path not in current:  # 避免把同一个文件添加多次\n            chatbot._cookies.update({\"files_to_promote\": [new_path] + current})\n    return new_path\n\n\ndef disable_auto_promotion(chatbot:ChatBotWithCookies):\n    chatbot._cookies.update({\"files_to_promote\": []})\n    return\n\n\ndef del_outdated_uploads(outdate_time_seconds:float, target_path_base:str=None):\n    if target_path_base is None:\n        user_upload_dir = get_conf(\"PATH_PRIVATE_UPLOAD\")\n    else:\n        user_upload_dir = target_path_base\n    current_time = time.time()\n    one_hour_ago = current_time - outdate_time_seconds\n    # Get a list of all subdirectories in the user_upload_dir folder\n    # Remove subdirectories that are older than one hour\n    for subdirectory in glob.glob(f\"{user_upload_dir}/*\"):\n        subdirectory_time = os.path.getmtime(subdirectory)\n        if subdirectory_time < one_hour_ago:\n            try:\n                shutil.rmtree(subdirectory)\n            except:\n                pass\n    return\n\n\n\ndef to_markdown_tabs(head: list, tabs: list, alignment=\":---:\", column=False, omit_path=None):\n    \"\"\"\n    Args:\n        head: 表头：[]\n        tabs: 表值：[[列1], [列2], [列3], [列4]]\n        alignment: :--- 左对齐， :---: 居中对齐， ---: 右对齐\n        column: True to keep data in columns, False to keep data in rows (default).\n    Returns:\n        A string representation of the markdown table.\n    \"\"\"\n    if column:\n        transposed_tabs = list(map(list, zip(*tabs)))\n    else:\n        transposed_tabs = tabs\n    # Find the maximum length among the columns\n    max_len = max(len(column) for column in transposed_tabs)\n\n    tab_format = \"| %s \"\n    tabs_list = \"\".join([tab_format % i for i in head]) + \"|\\n\"\n    tabs_list += \"\".join([tab_format % alignment for i in head]) + \"|\\n\"\n\n    for i in range(max_len):\n        row_data = [tab[i] if i < len(tab) else \"\" for tab in transposed_tabs]\n        row_data = file_manifest_filter_type(row_data, filter_=None)\n        # for dat in row_data:\n        #     if (omit_path is not None) and os.path.exists(dat):\n        #         dat = os.path.relpath(dat, omit_path)\n        tabs_list += \"\".join([tab_format % i for i in row_data]) + \"|\\n\"\n\n    return tabs_list\n\n\ndef on_file_uploaded(\n    request: gradio.Request, files:List[str], chatbot:ChatBotWithCookies,\n    txt:str, txt2:str, checkboxes:List[str], cookies:dict\n):\n    \"\"\"\n    当文件被上传时的回调函数\n    \"\"\"\n    if len(files) == 0:\n        return chatbot, txt\n\n    # 创建工作路径\n    user_name = default_user_name if not request.username else request.username\n    time_tag = gen_time_str()\n    target_path_base = get_upload_folder(user_name, tag=time_tag)\n    os.makedirs(target_path_base, exist_ok=True)\n\n    # 移除过时的旧文件从而节省空间&保护隐私\n    outdate_time_seconds = 3600  # 一小时\n    del_outdated_uploads(outdate_time_seconds, get_upload_folder(user_name))\n\n    # 逐个文件转移到目标路径\n    upload_msg = \"\"\n    for file in files:\n        file_origin_name = os.path.basename(file.orig_name)\n        this_file_path = pj(target_path_base, file_origin_name)\n        shutil.move(file.name, this_file_path)\n        upload_msg += extract_archive(\n            file_path=this_file_path, dest_dir=this_file_path + \".extract\"\n        )\n\n    # 整理文件集合 输出消息\n    files = glob.glob(f\"{target_path_base}/**/*\", recursive=True)\n    moved_files = [fp for fp in files]\n    max_file_to_show = 10\n    if len(moved_files) > max_file_to_show:\n        moved_files = moved_files[:max_file_to_show//2] + [f'... ( 📌省略{len(moved_files) - max_file_to_show}个文件的显示 ) ...'] + \\\n                      moved_files[-max_file_to_show//2:]\n    moved_files_str = to_markdown_tabs(head=[\"文件\"], tabs=[moved_files], omit_path=target_path_base)\n    chatbot.append(\n        [\n            \"我上传了文件，请查收\",\n            f\"[Local Message] 收到以下文件 （上传到路径：{target_path_base}）: \" +\n            f\"\\n\\n{moved_files_str}\" +\n            f\"\\n\\n调用路径参数已自动修正到: \\n\\n{txt}\" +\n            f\"\\n\\n现在您点击任意函数插件时，以上文件将被作为输入参数\" +\n            upload_msg,\n        ]\n    )\n\n    txt, txt2 = target_path_base, \"\"\n    if \"浮动输入区\" in checkboxes:\n        txt, txt2 = txt2, txt\n\n    # 记录近期文件\n    cookies.update(\n        {\n            \"most_recent_uploaded\": {\n                \"path\": target_path_base,\n                \"time\": time.time(),\n                \"time_str\": time_tag,\n            }\n        }\n    )\n    return chatbot, txt, txt2, cookies\n\n\ndef generate_file_link(report_files:List[str]):\n    file_links = \"\"\n    for f in report_files:\n        file_links += (\n            f'<br/><a href=\"file={os.path.abspath(f)}\" target=\"_blank\">{f}</a>'\n        )\n    return file_links\n\n\ndef on_report_generated(cookies:dict, files:List[str], chatbot:ChatBotWithCookies):\n    if \"files_to_promote\" in cookies:\n        report_files = cookies[\"files_to_promote\"]\n        cookies.pop(\"files_to_promote\")\n    else:\n        report_files = []\n    if len(report_files) == 0:\n        return cookies, None, chatbot\n    file_links = \"\"\n    for f in report_files:\n        file_links += (\n            f'<br/><a href=\"file={os.path.abspath(f)}\" target=\"_blank\">{f}</a>'\n        )\n    chatbot.append([\"报告如何远程获取？\", f\"报告已经添加到右侧“文件下载区”（可能处于折叠状态），请查收。{file_links}\"])\n    return cookies, report_files, chatbot\n\n\ndef load_chat_cookies():\n    API_KEY, LLM_MODEL, AZURE_API_KEY = get_conf(\n        \"API_KEY\", \"LLM_MODEL\", \"AZURE_API_KEY\"\n    )\n    AZURE_CFG_ARRAY, NUM_CUSTOM_BASIC_BTN = get_conf(\n        \"AZURE_CFG_ARRAY\", \"NUM_CUSTOM_BASIC_BTN\"\n    )\n\n    # deal with azure openai key\n    if is_any_api_key(AZURE_API_KEY):\n        if is_any_api_key(API_KEY):\n            API_KEY = API_KEY + \",\" + AZURE_API_KEY\n        else:\n            API_KEY = AZURE_API_KEY\n    if len(AZURE_CFG_ARRAY) > 0:\n        for azure_model_name, azure_cfg_dict in AZURE_CFG_ARRAY.items():\n            if not azure_model_name.startswith(\"azure\"):\n                raise ValueError(\"AZURE_CFG_ARRAY中配置的模型必须以azure开头\")\n            AZURE_API_KEY_ = azure_cfg_dict[\"AZURE_API_KEY\"]\n            if is_any_api_key(AZURE_API_KEY_):\n                if is_any_api_key(API_KEY):\n                    API_KEY = API_KEY + \",\" + AZURE_API_KEY_\n                else:\n                    API_KEY = AZURE_API_KEY_\n\n    customize_fn_overwrite_ = {}\n    for k in range(NUM_CUSTOM_BASIC_BTN):\n        customize_fn_overwrite_.update(\n            {\n                \"自定义按钮\"\n                + str(k + 1): {\n                    \"Title\": r\"\",\n                    \"Prefix\": r\"请在自定义菜单中定义提示词前缀.\",\n                    \"Suffix\": r\"请在自定义菜单中定义提示词后缀\",\n                }\n            }\n        )\n\n    EMBEDDING_MODEL = get_conf(\"EMBEDDING_MODEL\")\n    return {\n        \"api_key\": API_KEY,\n        \"llm_model\": LLM_MODEL,\n        \"embed_model\": EMBEDDING_MODEL,\n        \"customize_fn_overwrite\": customize_fn_overwrite_,\n    }\n\n\ndef clear_line_break(txt):\n    txt = txt.replace(\"\\n\", \" \")\n    txt = txt.replace(\"  \", \" \")\n    txt = txt.replace(\"  \", \" \")\n    return txt\n\n\nclass DummyWith:\n    \"\"\"\n    这段代码定义了一个名为DummyWith的空上下文管理器，\n    它的作用是……额……就是不起作用，即在代码结构不变得情况下取代其他的上下文管理器。\n    上下文管理器是一种Python对象，用于与with语句一起使用，\n    以确保一些资源在代码块执行期间得到正确的初始化和清理。\n    上下文管理器必须实现两个方法，分别为 __enter__()和 __exit__()。\n    在上下文执行开始的情况下，__enter__()方法会在代码块被执行前被调用，\n    而在上下文执行结束时，__exit__()方法则会被调用。\n    \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        return\n\n\ndef run_gradio_in_subpath(demo, auth, port, custom_path):\n    \"\"\"\n    把gradio的运行地址更改到指定的二次路径上\n    \"\"\"\n\n    def is_path_legal(path: str) -> bool:\n        \"\"\"\n        check path for sub url\n        path: path to check\n        return value: do sub url wrap\n        \"\"\"\n        if path == \"/\":\n            return True\n        if len(path) == 0:\n            logger.info(\n                \"ilegal custom path: {}\\npath must not be empty\\ndeploy on root url\".format(\n                    path\n                )\n            )\n            return False\n        if path[0] == \"/\":\n            if path[1] != \"/\":\n                logger.info(\"deploy on sub-path {}\".format(path))\n                return True\n            return False\n        logger.info(\n            \"ilegal custom path: {}\\npath should begin with '/'\\ndeploy on root url\".format(\n                path\n            )\n        )\n        return False\n\n    if not is_path_legal(custom_path):\n        raise RuntimeError(\"Ilegal custom path\")\n    import uvicorn\n    import gradio as gr\n    from fastapi import FastAPI\n\n    app = FastAPI()\n    if custom_path != \"/\":\n\n        @app.get(\"/\")\n        def read_main():\n            return {\"message\": f\"Gradio is running at: {custom_path}\"}\n\n    app = gr.mount_gradio_app(app, demo, path=custom_path)\n    uvicorn.run(app, host=\"0.0.0.0\", port=port)  # , auth=auth\n\n\ndef clip_history(inputs, history, tokenizer, max_token_limit):\n    \"\"\"\n    reduce the length of history by clipping.\n    this function search for the longest entries to clip, little by little,\n    until the number of token of history is reduced under threshold.\n    通过裁剪来缩短历史记录的长度。\n    此函数逐渐地搜索最长的条目进行剪辑，\n    直到历史记录的标记数量降低到阈值以下。\n    \"\"\"\n    import numpy as np\n    from request_llms.bridge_all import model_info\n\n    def get_token_num(txt):\n        return len(tokenizer.encode(txt, disallowed_special=()))\n\n    input_token_num = get_token_num(inputs)\n\n    if max_token_limit < 5000:\n        output_token_expect = 256  # 4k & 2k models\n    elif max_token_limit < 9000:\n        output_token_expect = 512  # 8k models\n    else:\n        output_token_expect = 1024  # 16k & 32k models\n\n    if input_token_num < max_token_limit * 3 / 4:\n        # 当输入部分的token占比小于限制的3/4时，裁剪时\n        # 1. 把input的余量留出来\n        max_token_limit = max_token_limit - input_token_num\n        # 2. 把输出用的余量留出来\n        max_token_limit = max_token_limit - output_token_expect\n        # 3. 如果余量太小了，直接清除历史\n        if max_token_limit < output_token_expect:\n            history = []\n            return history\n    else:\n        # 当输入部分的token占比 > 限制的3/4时，直接清除历史\n        history = []\n        return history\n\n    everything = [\"\"]\n    everything.extend(history)\n    n_token = get_token_num(\"\\n\".join(everything))\n    everything_token = [get_token_num(e) for e in everything]\n\n    # 截断时的颗粒度\n    delta = max(everything_token) // 16\n\n    while n_token > max_token_limit:\n        where = np.argmax(everything_token)\n        encoded = tokenizer.encode(everything[where], disallowed_special=())\n        clipped_encoded = encoded[: len(encoded) - delta]\n        everything[where] = tokenizer.decode(clipped_encoded)[\n            :-1\n        ]  # -1 to remove the may-be illegal char\n        everything_token[where] = get_token_num(everything[where])\n        n_token = get_token_num(\"\\n\".join(everything))\n\n    history = everything[1:]\n    return history\n\n\n\"\"\"\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n第三部分\n其他小工具:\n    - zip_folder:    把某个路径下所有文件压缩，然后转移到指定的另一个路径中（gpt写的）\n    - gen_time_str:  生成时间戳\n    - ProxyNetworkActivate: 临时地启动代理网络（如果有）\n    - objdump/objload: 快捷的调试函数\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n\"\"\"\n\n\ndef zip_folder(source_folder, dest_folder, zip_name):\n    import zipfile\n    import os\n\n    # Make sure the source folder exists\n    if not os.path.exists(source_folder):\n        logger.info(f\"{source_folder} does not exist\")\n        return\n\n    # Make sure the destination folder exists\n    if not os.path.exists(dest_folder):\n        logger.info(f\"{dest_folder} does not exist\")\n        return\n\n    # Create the name for the zip file\n    zip_file = pj(dest_folder, zip_name)\n\n    # Create a ZipFile object\n    with zipfile.ZipFile(zip_file, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through the source folder and add files to the zip file\n        for foldername, subfolders, filenames in os.walk(source_folder):\n            for filename in filenames:\n                filepath = pj(foldername, filename)\n                zipf.write(filepath, arcname=os.path.relpath(filepath, source_folder))\n\n    # Move the zip file to the destination folder (if it wasn't already there)\n    if os.path.dirname(zip_file) != dest_folder:\n        os.rename(zip_file, pj(dest_folder, os.path.basename(zip_file)))\n        zip_file = pj(dest_folder, os.path.basename(zip_file))\n\n    logger.info(f\"Zip file created at {zip_file}\")\n\n\ndef zip_result(folder):\n    t = gen_time_str()\n    zip_folder(folder, get_log_folder(), f\"{t}-result.zip\")\n    return pj(get_log_folder(), f\"{t}-result.zip\")\n\n\ndef gen_time_str():\n    import time\n\n    return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n\n\ndef get_log_folder(user=default_user_name, plugin_name=\"shared\"):\n    if user is None:\n        user = default_user_name\n    PATH_LOGGING = get_conf(\"PATH_LOGGING\")\n    if plugin_name is None:\n        _dir = pj(PATH_LOGGING, user)\n    else:\n        _dir = pj(PATH_LOGGING, user, plugin_name)\n    if not os.path.exists(_dir):\n        os.makedirs(_dir)\n    return _dir\n\n\ndef get_upload_folder(user=default_user_name, tag=None):\n    PATH_PRIVATE_UPLOAD = get_conf(\"PATH_PRIVATE_UPLOAD\")\n    if user is None:\n        user = default_user_name\n    if tag is None or len(tag) == 0:\n        target_path_base = pj(PATH_PRIVATE_UPLOAD, user)\n    else:\n        target_path_base = pj(PATH_PRIVATE_UPLOAD, user, tag)\n    return target_path_base\n\n\ndef is_the_upload_folder(string):\n    PATH_PRIVATE_UPLOAD = get_conf(\"PATH_PRIVATE_UPLOAD\")\n    pattern = r\"^PATH_PRIVATE_UPLOAD[\\\\/][A-Za-z0-9_-]+[\\\\/]\\d{4}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}$\"\n    pattern = pattern.replace(\"PATH_PRIVATE_UPLOAD\", PATH_PRIVATE_UPLOAD)\n    if re.match(pattern, string):\n        return True\n    else:\n        return False\n\n\ndef get_user(chatbotwithcookies:ChatBotWithCookies):\n    return chatbotwithcookies._cookies.get(\"user_name\", default_user_name)\n\n\nclass ProxyNetworkActivate:\n    \"\"\"\n    这段代码定义了一个名为ProxyNetworkActivate的空上下文管理器, 用于给一小段代码上代理\n    \"\"\"\n\n    def __init__(self, task=None) -> None:\n        self.task = task\n        if not task:\n            # 不给定task, 那么我们默认代理生效\n            self.valid = True\n        else:\n            # 给定了task, 我们检查一下\n            from toolbox import get_conf\n\n            WHEN_TO_USE_PROXY = get_conf(\"WHEN_TO_USE_PROXY\")\n            self.valid = task in WHEN_TO_USE_PROXY\n\n    def __enter__(self):\n        if not self.valid:\n            return self\n        from toolbox import get_conf\n\n        proxies = get_conf(\"proxies\")\n        if \"no_proxy\" in os.environ:\n            os.environ.pop(\"no_proxy\")\n        if proxies is not None:\n            if \"http\" in proxies:\n                os.environ[\"HTTP_PROXY\"] = proxies[\"http\"]\n            if \"https\" in proxies:\n                os.environ[\"HTTPS_PROXY\"] = proxies[\"https\"]\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        os.environ[\"no_proxy\"] = \"*\"\n        if \"HTTP_PROXY\" in os.environ:\n            os.environ.pop(\"HTTP_PROXY\")\n        if \"HTTPS_PROXY\" in os.environ:\n            os.environ.pop(\"HTTPS_PROXY\")\n        return\n\n\ndef Singleton(cls):\n    \"\"\"\n    一个单实例装饰器\n    \"\"\"\n    _instance = {}\n\n    def _singleton(*args, **kargs):\n        if cls not in _instance:\n            _instance[cls] = cls(*args, **kargs)\n        return _instance[cls]\n\n    return _singleton\n\n\ndef get_pictures_list(path):\n    file_manifest = [f for f in glob.glob(f\"{path}/**/*.jpg\", recursive=True)]\n    file_manifest += [f for f in glob.glob(f\"{path}/**/*.jpeg\", recursive=True)]\n    file_manifest += [f for f in glob.glob(f\"{path}/**/*.png\", recursive=True)]\n    return file_manifest\n\n\ndef have_any_recent_upload_image_files(chatbot:ChatBotWithCookies, pop:bool=False):\n    _5min = 5 * 60\n    if chatbot is None:\n        return False, None  # chatbot is None\n    if pop:\n        most_recent_uploaded = chatbot._cookies.pop(\"most_recent_uploaded\", None)\n    else:\n        most_recent_uploaded = chatbot._cookies.get(\"most_recent_uploaded\", None)\n    # most_recent_uploaded 是一个放置最新上传图像的路径\n    if not most_recent_uploaded:\n        return False, None  # most_recent_uploaded is None\n    if time.time() - most_recent_uploaded[\"time\"] < _5min:\n        path = most_recent_uploaded[\"path\"]\n        file_manifest = get_pictures_list(path)\n        if len(file_manifest) == 0:\n            return False, None\n        return True, file_manifest  # most_recent_uploaded is new\n    else:\n        return False, None  # most_recent_uploaded is too old\n\n# Claude3 model supports graphic context dialogue, reads all images\ndef every_image_file_in_path(chatbot:ChatBotWithCookies):\n    if chatbot is None:\n        return False, []  # chatbot is None\n    most_recent_uploaded = chatbot._cookies.get(\"most_recent_uploaded\", None)\n    if not most_recent_uploaded:\n        return False, []  # most_recent_uploaded is None\n    path = most_recent_uploaded[\"path\"]\n    file_manifest = get_pictures_list(path)\n    if len(file_manifest) == 0:\n        return False, []\n    return True, file_manifest\n\n# Function to encode the image\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\ndef get_max_token(llm_kwargs):\n    from request_llms.bridge_all import model_info\n\n    return model_info[llm_kwargs[\"llm_model\"]][\"max_token\"]\n\n\ndef check_packages(packages=[]):\n    import importlib.util\n\n    for p in packages:\n        spam_spec = importlib.util.find_spec(p)\n        if spam_spec is None:\n            raise ModuleNotFoundError\n\n\ndef map_file_to_sha256(file_path):\n    import hashlib\n\n    with open(file_path, 'rb') as file:\n        content = file.read()\n\n    # Calculate the SHA-256 hash of the file contents\n    sha_hash = hashlib.sha256(content).hexdigest()\n\n    return sha_hash\n\n\ndef check_repeat_upload(new_pdf_path, pdf_hash):\n    '''\n    检查历史上传的文件是否与新上传的文件相同，如果相同则返回(True, 重复文件路径)，否则返回(False，None)\n    '''\n    from toolbox import get_conf\n    import PyPDF2\n\n    user_upload_dir = os.path.dirname(os.path.dirname(new_pdf_path))\n    file_name = os.path.basename(new_pdf_path)\n\n    file_manifest = [f for f in glob.glob(f'{user_upload_dir}/**/{file_name}', recursive=True)]\n\n    for saved_file in file_manifest:\n        with open(new_pdf_path, 'rb') as file1, open(saved_file, 'rb') as file2:\n            reader1 = PyPDF2.PdfFileReader(file1)\n            reader2 = PyPDF2.PdfFileReader(file2)\n\n            # 比较页数是否相同\n            if reader1.getNumPages() != reader2.getNumPages():\n                continue\n\n            # 比较每一页的内容是否相同\n            for page_num in range(reader1.getNumPages()):\n                page1 = reader1.getPage(page_num).extractText()\n                page2 = reader2.getPage(page_num).extractText()\n                if page1 != page2:\n                    continue\n\n        maybe_project_dir = glob.glob('{}/**/{}'.format(get_log_folder(), pdf_hash + \".tag\"), recursive=True)\n\n\n        if len(maybe_project_dir) > 0:\n            return True, os.path.dirname(maybe_project_dir[0])\n\n    # 如果所有页的内容都相同，返回 True\n    return False, None\n\ndef log_chat(llm_model: str, input_str: str, output_str: str):\n    try:\n        if output_str and input_str and llm_model:\n            uid = str(uuid.uuid4().hex)\n            input_str = input_str.rstrip('\\n')\n            output_str = output_str.rstrip('\\n')\n            logger.bind(chat_msg=True).info(dedent(\n            \"\"\"\n            ╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n            [UID]\n            {uid}\n            [Model]\n            {llm_model}\n            [Query]\n            {input_str}\n            [Response]\n            {output_str}\n            ╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n            \"\"\").format(uid=uid, llm_model=llm_model, input_str=input_str, output_str=output_str))\n    except:\n        logger.error(trimmed_format_exc())\n"
        },
        {
          "name": "version",
          "type": "blob",
          "size": 0.2041015625,
          "content": "{\n  \"version\": 3.91,\n  \"show_feature\": true,\n  \"new_feature\": \"优化前端并修复TTS的BUG <-> 添加时间线回溯功能 <-> 支持chatgpt-4o-latest <-> 增加RAG组件 <-> 升级多合一主提交键\"\n}\n"
        }
      ]
    }
  ]
}