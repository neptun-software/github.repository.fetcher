{
  "metadata": {
    "timestamp": 1736561339183,
    "page": 361,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lengstrom/fast-style-transfer",
      "stars": 10934,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1162109375,
          "content": "t Byte-compiled / optimized / DLL files\ndeps.txt\narchive\nsaver\n*~\nstyles\npngs\npreds\n\n*.sw*\ndata\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n\n# PyCharm\n.idea\n\n# checkpoint\ncheckpoint\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.30859375,
          "content": "# YAML 1.2\n---\nauthors: \n  -\n    family-names: Engstrom\n    given-names: Logan\ncff-version: \"1.1.0\"\ndate-released: 2016-10-31\nmessage: \"If you use this software, please cite it using these metadata.\"\nrepository-code: \"https://github.com/lengstrom/fast-style-transfer\"\ntitle: \"Fast Style Transfer\"\nversion: \"1.0\"\n...\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.623046875,
          "content": "## Fast Style Transfer in [TensorFlow](https://github.com/tensorflow/tensorflow)\n\nAdd styles from famous paintings to any photo in a fraction of a second! [You can even style videos!](#video-stylization)\n\n<p align = 'center'>\n<img src = 'examples/style/udnie.jpg' height = '246px'>\n<img src = 'examples/content/stata.jpg' height = '246px'>\n<a href = 'examples/results/stata_udnie.jpg'><img src = 'examples/results/stata_udnie_header.jpg' width = '627px'></a>\n</p>\n<p align = 'center'>\nIt takes 100ms on a 2015 Titan X to style the MIT Stata Center (1024×680) like Udnie, by Francis Picabia.\n</p>\n\nOur implementation is based off of a combination of Gatys' [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576), Johnson's [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](http://cs.stanford.edu/people/jcjohns/eccv16/), and Ulyanov's [Instance Normalization](https://arxiv.org/abs/1607.08022). \n\n### Sponsorship\nPlease consider sponsoring my work on this project!\n\n### License\nCopyright (c) 2016 Logan Engstrom. Contact me for commercial use (or rather any use that is not academic research) (email: engstrom at my university's domain dot edu). Free for research use, as long as proper attribution is given and this copyright notice is retained.\n\n## Video Stylization \nHere we transformed every frame in a video, then combined the results. [Click to go to the full demo on YouTube!](https://www.youtube.com/watch?v=xVJwwWQlQ1o) The style here is Udnie, as above.\n<div align = 'center'>\n     <a href = 'https://www.youtube.com/watch?v=xVJwwWQlQ1o'>\n        <img src = 'examples/results/fox_udnie.gif' alt = 'Stylized fox video. Click to go to YouTube!' width = '800px' height = '400px'>\n     </a>\n</div>\n\nSee how to generate these videos [here](#stylizing-video)!\n\n## Image Stylization\nWe added styles from various paintings to a photo of Chicago. Click on thumbnails to see full applied style images.\n<div align='center'>\n<img src = 'examples/content/chicago.jpg' height=\"200px\">\n</div>\n     \n<div align = 'center'>\n<a href = 'examples/style/wave.jpg'><img src = 'examples/thumbs/wave.jpg' height = '200px'></a>\n<img src = 'examples/results/chicago_wave.jpg' height = '200px'>\n<img src = 'examples/results/chicago_udnie.jpg' height = '200px'>\n<a href = 'examples/style/udnie.jpg'><img src = 'examples/thumbs/udnie.jpg' height = '200px'></a>\n<br>\n<a href = 'examples/style/rain_princess.jpg'><img src = 'examples/thumbs/rain_princess.jpg' height = '200px'></a>\n<img src = 'examples/results/chicago_rain_princess.jpg' height = '200px'>\n<img src = 'examples/results/chicago_la_muse.jpg' height = '200px'>\n<a href = 'examples/style/la_muse.jpg'><img src = 'examples/thumbs/la_muse.jpg' height = '200px'></a>\n\n<br>\n<a href = 'examples/style/the_shipwreck_of_the_minotaur.jpg'><img src = 'examples/thumbs/the_shipwreck_of_the_minotaur.jpg' height = '200px'></a>\n<img src = 'examples/results/chicago_wreck.jpg' height = '200px'>\n<img src = 'examples/results/chicago_the_scream.jpg' height = '200px'>\n<a href = 'examples/style/the_scream.jpg'><img src = 'examples/thumbs/the_scream.jpg' height = '200px'></a>\n</div>\n\n## Implementation Details\nOur implementation uses TensorFlow to train a fast style transfer network. We use roughly the same transformation network as described in Johnson, except that batch normalization is replaced with Ulyanov's instance normalization, and the scaling/offset of the output `tanh` layer is slightly different. We use a loss function close to the one described in Gatys, using VGG19 instead of VGG16 and typically using \"shallower\" layers than in Johnson's implementation (e.g. we use `relu1_1` rather than `relu1_2`). Empirically, this results in larger scale style features in transformations.\n## Virtual Environment Setup (Anaconda) - Windows/Linux\nTested on\n| Spec                        |                                                             |\n|-----------------------------|-------------------------------------------------------------|\n| Operating System            | Windows 10 Home                                             |\n| GPU                         | Nvidia GTX 2080 TI                                          |\n| CUDA Version                | 11.0                                                        |\n| Driver Version              | 445.75                                                      |\n### Step 1：Install Anaconda\nhttps://docs.anaconda.com/anaconda/install/\n### Step 2：Build a virtual environment\nRun the following commands in sequence in Anaconda Prompt:\n```\nconda create -n tf-gpu tensorflow-gpu=2.1.0\nconda activate tf-gpu\nconda install jupyterlab\njupyter lab\n```\nRun the following command in the notebook or just conda install the package:\n```\n!pip install moviepy==1.0.2\n```\nFollow the commands below to use fast-style-transfer\n## Documentation\n### Training Style Transfer Networks\nUse `style.py` to train a new style transfer network. Run `python style.py` to view all the possible parameters. Training takes 4-6 hours on a Maxwell Titan X. [More detailed documentation here](docs.md#stylepy). **Before you run this, you should run `setup.sh`**. Example usage:\n\n    python style.py --style path/to/style/img.jpg \\\n      --checkpoint-dir checkpoint/path \\\n      --test path/to/test/img.jpg \\\n      --test-dir path/to/test/dir \\\n      --content-weight 1.5e1 \\\n      --checkpoint-iterations 1000 \\\n      --batch-size 20\n\n### Evaluating Style Transfer Networks\nUse `evaluate.py` to evaluate a style transfer network. Run `python evaluate.py` to view all the possible parameters. Evaluation takes 100 ms per frame (when batch size is 1) on a Maxwell Titan X. [More detailed documentation here](docs.md#evaluatepy). Takes several seconds per frame on a CPU. **Models for evaluation are [located here](https://drive.google.com/drive/folders/0B9jhaT37ydSyRk9UX0wwX3BpMzQ?resourcekey=0-Z9LcNHC-BTB4feKwm4loXw&usp=sharing)**. Example usage:\n\n    python evaluate.py --checkpoint path/to/style/model.ckpt \\\n      --in-path dir/of/test/imgs/ \\\n      --out-path dir/for/results/\n\n### Stylizing Video\nUse `transform_video.py` to transfer style into a video. Run `python transform_video.py` to view all the possible parameters. Requires `ffmpeg`. [More detailed documentation here](docs.md#transform_videopy). Example usage:\n\n    python transform_video.py --in-path path/to/input/vid.mp4 \\\n      --checkpoint path/to/style/model.ckpt \\\n      --out-path out/video.mp4 \\\n      --device /gpu:0 \\\n      --batch-size 4\n\n### Requirements\nYou will need the following to run the above:\n- TensorFlow 0.11.0\n- Python 2.7.9, Pillow 3.4.2, scipy 0.18.1, numpy 1.11.2\n- If you want to train (and don't want to wait for 4 months):\n  - A decent GPU\n  - All the required NVIDIA software to run TF on a GPU (cuda, etc)\n- ffmpeg 3.1.3 if you want to stylize video\n\n### Citation\n```\n  @misc{engstrom2016faststyletransfer,\n    author = {Logan Engstrom},\n    title = {Fast Style Transfer},\n    year = {2016},\n    howpublished = {\\url{https://github.com/lengstrom/fast-style-transfer/}},\n    note = {commit xxxxxxx}\n  }\n```\n\n### Attributions/Thanks\n- This project could not have happened without the advice (and GPU access) given by [Anish Athalye](http://www.anishathalye.com/). \n  - The project also borrowed some code from Anish's [Neural Style](https://github.com/anishathalye/neural-style/)\n- Some readme/docs formatting was borrowed from Justin Johnson's [Fast Neural Style](https://github.com/jcjohnson/fast-neural-style)\n- The image of the Stata Center at the very beginning of the README was taken by [Juan Paulo](https://juanpaulo.me/)\n\n### Related Work\n- Michael Ramos ported this network [to use CoreML on iOS](https://medium.com/@rambossa/diy-prisma-fast-style-transfer-app-with-coreml-and-tensorflow-817c3b90dacd)\n"
        },
        {
          "name": "docs.md",
          "type": "blob",
          "size": 2.703125,
          "content": "## style.py \n\n`style.py` trains networks that can transfer styles from artwork into images.\n\n**Flags**\n- `--checkpoint-dir`: Directory to save checkpoint in. Required.\n- `--style`: Path to style image. Required.\n- `--train-path`: Path to training images folder. Default: `data/train2014`.\n- `--test`: Path to content image to test network on at at every checkpoint iteration. Default: no image.\n- `--test-dir`: Path to directory to save test images in. Required if `--test` is passed a value.\n- `--epochs`: Epochs to train for. Default: `2`.\n- `--batch-size`: Batch size for training. Default: `4`.\n- `--checkpoint-iterations`: Number of iterations to go for between checkpoints. Default: `2000`.\n- `--vgg-path`: Path to VGG19 network (default). Can pass VGG16 if you want to try out other loss functions. Default: `data/imagenet-vgg-verydeep-19.mat`.\n- `--content-weight`: Weight of content in loss function. Default: `7.5e0`.\n- `--style-weight`: Weight of style in loss function. Default: `1e2`.\n- `--tv-weight`: Weight of total variation term in loss function. Default: `2e2`.\n- `--learning-rate`: Learning rate for optimizer. Default: `1e-3`.\n- `--slow`: For debugging loss function. Direct optimization on pixels using Gatys' approach. Uses `test` image as content value, `test_dir` for saving fully optimized images.\n\n\n## evaluate.py\n`evaluate.py` evaluates trained networks given a checkpoint directory. If evaluating images from a directory, every image in the directory must have the same dimensions.\n\n**Flags**\n- `--checkpoint`: Directory or `ckpt` file to load checkpoint from. Required.\n- `--in-path`: Path of image or directory of images to transform. Required.\n- `--out-path`: Out path of transformed image or out directory to put transformed images from in directory (if `in_path` is a directory). Required.\n- `--device`: Device used to transform image. Default: `/cpu:0`.\n- `--batch-size`: Batch size used to evaluate images. In particular meant for directory transformations. Default: `4`.\n- `--allow-different-dimensions`: Allow different image dimensions. Default: not enabled\n\n## transform_video.py\n`transform_video.py` transforms videos into stylized videos given a style transfer net.\n\n**Flags**\n- `--checkpoint-dir`: Directory or `ckpt` file to load checkpoint from. Required.\n- `--in-path`: Path to video to transfer style to. Required.\n- `--out-path`: Path to out video. Required.\n- `--tmp-dir`: Directory to put temporary processing files in. Will generate a dir if you do not pass it a path. Will delete tmpdir afterwards. Default: randomly generates invisible dir, then deletes it after execution completion.\n- `--device`: Device to evaluate frames with. Default: `/gpu:0`.\n- `--batch-size`: Batch size for evaluating images. Default: `4`.\n"
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 8.388671875,
          "content": "from __future__ import print_function\nimport sys\nsys.path.insert(0, 'src')\nimport transform, numpy as np, vgg, pdb, os\nimport scipy.misc\nimport tensorflow as tf\nfrom utils import save_img, get_img, exists, list_files\nfrom argparse import ArgumentParser\nfrom collections import defaultdict\nimport time\nimport json\nimport subprocess\nimport numpy\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nimport moviepy.video.io.ffmpeg_writer as ffmpeg_writer\n\nBATCH_SIZE = 4\nDEVICE = '/gpu:0'\n\n\ndef ffwd_video(path_in, path_out, checkpoint_dir, device_t='/gpu:0', batch_size=4):\n    video_clip = VideoFileClip(path_in, audio=False)\n    video_writer = ffmpeg_writer.FFMPEG_VideoWriter(path_out, video_clip.size, video_clip.fps, codec=\"libx264\",\n                                                    preset=\"medium\", bitrate=\"2000k\",\n                                                    audiofile=path_in, threads=None,\n                                                    ffmpeg_params=None)\n\n    g = tf.Graph()\n    soft_config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n    soft_config.gpu_options.allow_growth = True\n    with g.as_default(), g.device(device_t), \\\n            tf.compat.v1.Session(config=soft_config) as sess:\n        batch_shape = (batch_size, video_clip.size[1], video_clip.size[0], 3)\n        img_placeholder = tf.compat.v1.placeholder(tf.float32, shape=batch_shape,\n                                         name='img_placeholder')\n\n        preds = transform.net(img_placeholder)\n        saver = tf.compat.v1.train.Saver()\n        if os.path.isdir(checkpoint_dir):\n            ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n            else:\n                raise Exception(\"No checkpoint found...\")\n        else:\n            saver.restore(sess, checkpoint_dir)\n\n        X = np.zeros(batch_shape, dtype=np.float32)\n\n        def style_and_write(count):\n            for i in range(count, batch_size):\n                X[i] = X[count - 1]  # Use last frame to fill X\n            _preds = sess.run(preds, feed_dict={img_placeholder: X})\n            for i in range(0, count):\n                video_writer.write_frame(np.clip(_preds[i], 0, 255).astype(np.uint8))\n\n        frame_count = 0  # The frame count that written to X\n        for frame in video_clip.iter_frames():\n            X[frame_count] = frame\n            frame_count += 1\n            if frame_count == batch_size:\n                style_and_write(frame_count)\n                frame_count = 0\n\n        if frame_count != 0:\n            style_and_write(frame_count)\n\n        video_writer.close()\n\n\n# get img_shape\ndef ffwd(data_in, paths_out, checkpoint_dir, device_t='/gpu:0', batch_size=4):\n    assert len(paths_out) > 0\n    is_paths = type(data_in[0]) == str\n    if is_paths:\n        assert len(data_in) == len(paths_out)\n        img_shape = get_img(data_in[0]).shape\n    else:\n        assert data_in.size[0] == len(paths_out)\n        img_shape = X[0].shape\n\n    g = tf.Graph()\n    batch_size = min(len(paths_out), batch_size)\n    curr_num = 0\n    soft_config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n    soft_config.gpu_options.allow_growth = True\n    with g.as_default(), g.device(device_t), \\\n            tf.compat.v1.Session(config=soft_config) as sess:\n        batch_shape = (batch_size,) + img_shape\n        img_placeholder = tf.compat.v1.placeholder(tf.float32, shape=batch_shape,\n                                         name='img_placeholder')\n\n        preds = transform.net(img_placeholder)\n        saver = tf.compat.v1.train.Saver()\n        if os.path.isdir(checkpoint_dir):\n            ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n            else:\n                raise Exception(\"No checkpoint found...\")\n        else:\n            saver.restore(sess, checkpoint_dir)\n\n        num_iters = int(len(paths_out)/batch_size)\n        for i in range(num_iters):\n            pos = i * batch_size\n            curr_batch_out = paths_out[pos:pos+batch_size]\n            if is_paths:\n                curr_batch_in = data_in[pos:pos+batch_size]\n                X = np.zeros(batch_shape, dtype=np.float32)\n                for j, path_in in enumerate(curr_batch_in):\n                    img = get_img(path_in)\n                    assert img.shape == img_shape, \\\n                        'Images have different dimensions. ' +  \\\n                        'Resize images or use --allow-different-dimensions.'\n                    X[j] = img\n            else:\n                X = data_in[pos:pos+batch_size]\n\n            _preds = sess.run(preds, feed_dict={img_placeholder:X})\n            for j, path_out in enumerate(curr_batch_out):\n                save_img(path_out, _preds[j])\n                \n        remaining_in = data_in[num_iters*batch_size:]\n        remaining_out = paths_out[num_iters*batch_size:]\n    if len(remaining_in) > 0:\n        ffwd(remaining_in, remaining_out, checkpoint_dir, \n            device_t=device_t, batch_size=1)\n\ndef ffwd_to_img(in_path, out_path, checkpoint_dir, device='/cpu:0'):\n    paths_in, paths_out = [in_path], [out_path]\n    ffwd(paths_in, paths_out, checkpoint_dir, batch_size=1, device_t=device)\n\ndef ffwd_different_dimensions(in_path, out_path, checkpoint_dir, \n            device_t=DEVICE, batch_size=4):\n    in_path_of_shape = defaultdict(list)\n    out_path_of_shape = defaultdict(list)\n    for i in range(len(in_path)):\n        in_image = in_path[i]\n        out_image = out_path[i]\n        shape = \"%dx%dx%d\" % get_img(in_image).shape\n        in_path_of_shape[shape].append(in_image)\n        out_path_of_shape[shape].append(out_image)\n    for shape in in_path_of_shape:\n        print('Processing images of shape %s' % shape)\n        ffwd(in_path_of_shape[shape], out_path_of_shape[shape], \n            checkpoint_dir, device_t, batch_size)\n\ndef build_parser():\n    parser = ArgumentParser()\n    parser.add_argument('--checkpoint', type=str,\n                        dest='checkpoint_dir',\n                        help='dir or .ckpt file to load checkpoint from',\n                        metavar='CHECKPOINT', required=True)\n\n    parser.add_argument('--in-path', type=str,\n                        dest='in_path',help='dir or file to transform',\n                        metavar='IN_PATH', required=True)\n\n    help_out = 'destination (dir or file) of transformed file or files'\n    parser.add_argument('--out-path', type=str,\n                        dest='out_path', help=help_out, metavar='OUT_PATH',\n                        required=True)\n\n    parser.add_argument('--device', type=str,\n                        dest='device',help='device to perform compute on',\n                        metavar='DEVICE', default=DEVICE)\n\n    parser.add_argument('--batch-size', type=int,\n                        dest='batch_size',help='batch size for feedforwarding',\n                        metavar='BATCH_SIZE', default=BATCH_SIZE)\n\n    parser.add_argument('--allow-different-dimensions', action='store_true',\n                        dest='allow_different_dimensions', \n                        help='allow different image dimensions')\n\n    return parser\n\ndef check_opts(opts):\n    exists(opts.checkpoint_dir, 'Checkpoint not found!')\n    exists(opts.in_path, 'In path not found!')\n    if os.path.isdir(opts.out_path):\n        exists(opts.out_path, 'out dir not found!')\n        assert opts.batch_size > 0\n\ndef main():\n    parser = build_parser()\n    opts = parser.parse_args()\n    check_opts(opts)\n\n    if not os.path.isdir(opts.in_path):\n        if os.path.exists(opts.out_path) and os.path.isdir(opts.out_path):\n            out_path = \\\n                    os.path.join(opts.out_path,os.path.basename(opts.in_path))\n        else:\n            out_path = opts.out_path\n\n        ffwd_to_img(opts.in_path, out_path, opts.checkpoint_dir,\n                    device=opts.device)\n    else:\n        files = list_files(opts.in_path)\n        full_in = [os.path.join(opts.in_path,x) for x in files]\n        full_out = [os.path.join(opts.out_path,x) for x in files]\n        if opts.allow_different_dimensions:\n            ffwd_different_dimensions(full_in, full_out, opts.checkpoint_dir, \n                    device_t=opts.device, batch_size=opts.batch_size)\n        else :\n            ffwd(full_in, full_out, opts.checkpoint_dir, device_t=opts.device,\n                    batch_size=opts.batch_size)\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.sh",
          "type": "blob",
          "size": 0.20703125,
          "content": "#! /bin/bash\n\nmkdir data\ncd data\nwget http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\nmkdir bin\nwget http://msvocds.blob.core.windows.net/coco2014/train2014.zip\nunzip -q train2014.zip\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "style.py",
          "type": "blob",
          "size": 6.0166015625,
          "content": "from __future__ import print_function\nimport sys, os, pdb\nsys.path.insert(0, 'src')\nimport numpy as np, scipy.misc \nfrom optimize import optimize\nfrom argparse import ArgumentParser\nfrom utils import save_img, get_img, exists, list_files\nimport evaluate\n\nCONTENT_WEIGHT = 7.5e0\nSTYLE_WEIGHT = 1e2\nTV_WEIGHT = 2e2\n\nLEARNING_RATE = 1e-3\nNUM_EPOCHS = 2\nCHECKPOINT_DIR = 'checkpoints'\nCHECKPOINT_ITERATIONS = 2000\nVGG_PATH = 'data/imagenet-vgg-verydeep-19.mat'\nTRAIN_PATH = 'data/train2014'\nBATCH_SIZE = 4\nDEVICE = '/gpu:0'\nFRAC_GPU = 1\n\ndef build_parser():\n    parser = ArgumentParser()\n    parser.add_argument('--checkpoint-dir', type=str,\n                        dest='checkpoint_dir', help='dir to save checkpoint in',\n                        metavar='CHECKPOINT_DIR', required=True)\n\n    parser.add_argument('--style', type=str,\n                        dest='style', help='style image path',\n                        metavar='STYLE', required=True)\n\n    parser.add_argument('--train-path', type=str,\n                        dest='train_path', help='path to training images folder',\n                        metavar='TRAIN_PATH', default=TRAIN_PATH)\n\n    parser.add_argument('--test', type=str,\n                        dest='test', help='test image path',\n                        metavar='TEST', default=False)\n\n    parser.add_argument('--test-dir', type=str,\n                        dest='test_dir', help='test image save dir',\n                        metavar='TEST_DIR', default=False)\n\n    parser.add_argument('--slow', dest='slow', action='store_true',\n                        help='gatys\\' approach (for debugging, not supported)',\n                        default=False)\n\n    parser.add_argument('--epochs', type=int,\n                        dest='epochs', help='num epochs',\n                        metavar='EPOCHS', default=NUM_EPOCHS)\n\n    parser.add_argument('--batch-size', type=int,\n                        dest='batch_size', help='batch size',\n                        metavar='BATCH_SIZE', default=BATCH_SIZE)\n\n    parser.add_argument('--checkpoint-iterations', type=int,\n                        dest='checkpoint_iterations', help='checkpoint frequency',\n                        metavar='CHECKPOINT_ITERATIONS',\n                        default=CHECKPOINT_ITERATIONS)\n\n    parser.add_argument('--vgg-path', type=str,\n                        dest='vgg_path',\n                        help='path to VGG19 network (default %(default)s)',\n                        metavar='VGG_PATH', default=VGG_PATH)\n\n    parser.add_argument('--content-weight', type=float,\n                        dest='content_weight',\n                        help='content weight (default %(default)s)',\n                        metavar='CONTENT_WEIGHT', default=CONTENT_WEIGHT)\n    \n    parser.add_argument('--style-weight', type=float,\n                        dest='style_weight',\n                        help='style weight (default %(default)s)',\n                        metavar='STYLE_WEIGHT', default=STYLE_WEIGHT)\n\n    parser.add_argument('--tv-weight', type=float,\n                        dest='tv_weight',\n                        help='total variation regularization weight (default %(default)s)',\n                        metavar='TV_WEIGHT', default=TV_WEIGHT)\n    \n    parser.add_argument('--learning-rate', type=float,\n                        dest='learning_rate',\n                        help='learning rate (default %(default)s)',\n                        metavar='LEARNING_RATE', default=LEARNING_RATE)\n\n    return parser\n\ndef check_opts(opts):\n    exists(opts.checkpoint_dir, \"checkpoint dir not found!\")\n    exists(opts.style, \"style path not found!\")\n    exists(opts.train_path, \"train path not found!\")\n    if opts.test or opts.test_dir:\n        exists(opts.test, \"test img not found!\")\n        exists(opts.test_dir, \"test directory not found!\")\n    exists(opts.vgg_path, \"vgg network data not found!\")\n    assert opts.epochs > 0\n    assert opts.batch_size > 0\n    assert opts.checkpoint_iterations > 0\n    assert os.path.exists(opts.vgg_path)\n    assert opts.content_weight >= 0\n    assert opts.style_weight >= 0\n    assert opts.tv_weight >= 0\n    assert opts.learning_rate >= 0\n\ndef _get_files(img_dir):\n    files = list_files(img_dir)\n    return [os.path.join(img_dir,x) for x in files]\n\n    \ndef main():\n    parser = build_parser()\n    options = parser.parse_args()\n    check_opts(options)\n\n    style_target = get_img(options.style)\n    if not options.slow:\n        content_targets = _get_files(options.train_path)\n    elif options.test:\n        content_targets = [options.test]\n\n    kwargs = {\n        \"slow\":options.slow,\n        \"epochs\":options.epochs,\n        \"print_iterations\":options.checkpoint_iterations,\n        \"batch_size\":options.batch_size,\n        \"save_path\":os.path.join(options.checkpoint_dir,'fns.ckpt'),\n        \"learning_rate\":options.learning_rate\n    }\n\n    if options.slow:\n        if options.epochs < 10:\n            kwargs['epochs'] = 1000\n        if options.learning_rate < 1:\n            kwargs['learning_rate'] = 1e1\n\n    args = [\n        content_targets,\n        style_target,\n        options.content_weight,\n        options.style_weight,\n        options.tv_weight,\n        options.vgg_path\n    ]\n\n    for preds, losses, i, epoch in optimize(*args, **kwargs):\n        style_loss, content_loss, tv_loss, loss = losses\n\n        print('Epoch %d, Iteration: %d, Loss: %s' % (epoch, i, loss))\n        to_print = (style_loss, content_loss, tv_loss)\n        print('style: %s, content:%s, tv: %s' % to_print)\n        if options.test:\n            assert options.test_dir != False\n            preds_path = '%s/%s_%s.png' % (options.test_dir,epoch,i)\n            if not options.slow:\n                ckpt_dir = os.path.dirname(options.checkpoint_dir)\n                evaluate.ffwd_to_img(options.test,preds_path,\n                                     options.checkpoint_dir)\n            else:\n                save_img(preds_path, img)\n    ckpt_dir = options.checkpoint_dir\n    cmd_text = 'python evaluate.py --checkpoint %s ...' % ckpt_dir\n    print(\"Training complete. For evaluation:\\n    `%s`\" % cmd_text)\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "transform_video.py",
          "type": "blob",
          "size": 1.939453125,
          "content": "from __future__ import print_function\nfrom argparse import ArgumentParser\nimport sys\nsys.path.insert(0, 'src')\nimport os, random, subprocess, evaluate, shutil\nfrom utils import exists, list_files\nimport pdb\n\nTMP_DIR = '.fns_frames_%s/' % random.randint(0,99999)\nDEVICE = '/gpu:0'\nBATCH_SIZE = 4\n\ndef build_parser():\n    parser = ArgumentParser()\n    parser.add_argument('--checkpoint', type=str,\n                        dest='checkpoint', help='checkpoint directory or .ckpt file',\n                        metavar='CHECKPOINT', required=True)\n\n    parser.add_argument('--in-path', type=str,\n                        dest='in_path', help='in video path',\n                        metavar='IN_PATH', required=True)\n    \n    parser.add_argument('--out-path', type=str,\n                        dest='out', help='path to save processed video to',\n                        metavar='OUT', required=True)\n    \n    parser.add_argument('--tmp-dir', type=str, dest='tmp_dir',\n                        help='tmp dir for processing', metavar='TMP_DIR',\n                        default=TMP_DIR)\n\n    parser.add_argument('--device', type=str, dest='device',\n                        help='device for eval. CPU discouraged. ex: \\'/gpu:0\\'',\n                        metavar='DEVICE', default=DEVICE)\n\n    parser.add_argument('--batch-size', type=int,\n                        dest='batch_size',help='batch size for eval. default 4.',\n                        metavar='BATCH_SIZE', default=BATCH_SIZE)\n\n    parser.add_argument('--no-disk', type=bool, dest='no_disk',\n                        help='Don\\'t save intermediate files to disk. Default False',\n                        metavar='NO_DISK', default=False)\n    return parser\n\ndef check_opts(opts):\n    exists(opts.checkpoint)\n    exists(opts.out)\n\ndef main():\n    parser = build_parser()\n    opts = parser.parse_args()\n    evaluate.ffwd_video(opts.in_path, opts.out, opts.checkpoint, opts.device, opts.batch_size)\n\n \nif __name__ == '__main__':\n    main()\n\n\n"
        }
      ]
    }
  ]
}