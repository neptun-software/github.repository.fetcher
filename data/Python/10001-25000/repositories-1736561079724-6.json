{
  "metadata": {
    "timestamp": 1736561079724,
    "page": 6,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "d2l-ai/d2l-en",
      "stars": 24482,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0185546875,
          "content": "* text=auto eol=lf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.5869140625,
          "content": "**/.ipynb_checkpoints\n**/__pycache__\ndata/\n*.json\n*.params\n*.DS_Store\n*.Rhistory\n*.csv\nbuild/README.md\nbuild/environment.yml\n*egg-info*\ndist/*\nbuild/chapter*\nbuild/_build\nbuild/img\nbuild/data\nbuild/d2l\n/chapter_attention-mechanisms-and-transformers/fra-eng.zip\n/chapter_recurrent-neural-networks/fra-eng.zip\nimg/*.pdf\naclImdb*\n/_build/\ngraffle/*/*.svg\ngraffle/*/*.pdf\nbuild/\n/chapter_builders-guide/mydict\n/chapter_builders-guide/x-file\n/chapter_builders-guide/x-files\n.idea\n.vscode\n.pytest_cache\nstatic/latex_style/PT1*\n/chapter_hyperparameter_optimization/std.out\nlatex_style/*\nstatic/latex_style/*\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.3037109375,
          "content": "## Code of Conduct\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct). \nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact \nopensource-codeofconduct@amazon.com with any additional questions or comments.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 10.8046875,
          "content": "# Guidelines for contributing\n\nThank you for your interest in contributing to this open source book! We greatly value feedback and contributions from our community.\n\nPlease read through this document before you submit any pull requests or issues. It will help us work together more effectively.\n\n## What to expect when you contribute\n\nWhen you submit a pull request, our team is notified and will respond as quickly as we can. We'll do our best to work with you to ensure that your pull request adheres to our style and standards. If we merge your pull request, we might make additional edits later for style or clarity.\n\nThe source files on GitHub aren't published directly to the official website. If we merge your pull request, we'll publish your changes to the documentation website as soon as we can, but they won't appear immediately or automatically.\n\nWe look forward to receiving your pull requests for:\n\n* New content you'd like to contribute (such as new code samples or tutorials)\n* Inaccuracies in the content\n* Information gaps in the content that need more detail to be complete\n* Typos or grammatical errors\n* Suggested rewrites that improve clarity and reduce confusion\n\n**Note:** We all write differently, and you might not like how we've written or organized something currently. We want that feedback. But please be sure that your request for a rewrite is supported by the previous criteria. If it isn't, we might decline to merge it.\n\n## How to contribute\n\nTo contribute, start by reading [contributing section](https://d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html) and eventually\nsend us a pull request. For small changes, such as fixing a typo or adding a link, you can use the [GitHub Edit Button](https://docs.github.com/en/repositories/working-with-files/managing-files/editing-files). For larger changes:\n\n1. [Fork the repository](https://help.github.com/articles/fork-a-repo/).\n2. In your fork, make your change in a new branch (e.g., by [`git branch`](https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging)) that's based on this repo's **master** branch.\n3. Commit the change to your fork, using a clear and descriptive commit message.\n4. [Create a pull request](https://help.github.com/articles/creating-a-pull-request-from-a-fork/), answering any questions in the pull request form.\n\nBefore you send us a pull request, please be sure that:\n\n1. You're working from the latest source on the **master** branch.\n2. You check [existing open](https://github.com/d2l-ai/d2l-en/pulls), and [recently closed](https://github.com/d2l-ai/d2l-en/pulls?q=is%3Apr+is%3Aclosed), pull requests to be sure that someone else hasn't already addressed the problem.\n3. You [create an issue](https://github.com/d2l-ai/d2l-en/issues/new) before working on a contribution that will take a significant amount of your time.\n\nFor contributions that will take a significant amount of time, [open a new issue](https://github.com/d2l-ai/d2l-en/issues/new) to pitch your idea before you get started. Explain the problem and describe the content you want to see added to the documentation. Let us know if you'll write it yourself or if you'd like us to help. We'll discuss your proposal with you and let you know whether we're likely to accept it. We don't want you to spend a lot of time on a contribution that might be outside the scope of the documentation or that's already in the works.\n\n## Finding contributions to work on\n\nIf you'd like to contribute, but don't have a project in mind, look at the [open issues](https://github.com/d2l-ai/d2l-en/issues) in this repository for some ideas. Issues with the [help wanted](https://github.com/d2l-ai/d2l-en/labels/help%20wanted), [good first issue](https://github.com/d2l-ai/d2l-en/labels/good%20first%20issue) or [enhancement](https://github.com/d2l-ai/d2l-en/labels/enhancement) labels are a great place to start.\n\nIn addition to written content, we really appreciate new examples and code samples for our documentation, such as examples for different platforms or environments, and code samples in additional languages.\n\n\n## How to change code in one of the frameworks?\n\nThis section describes the development environment setup and workflow\nwhich should be followed when modifying/porting python code and making\nchanges to one of the machine learning frameworks in the book.\nWe follow a set of pre-defined [style guidelines](https://github.com/d2l-ai/d2l-en/blob/master/STYLE_GUIDE.md)\nfor consistent code quality throughout the book and expect the same\nfrom our community contributors. You may need to check other chapters\nfrom other contributors as well for this step.\n\nAll the chapter sections are generated from markdown (.md file, not .ipynb file)\nsource files. When making changes in code, for the ease of development\nand making sure it is error free, we never edit the markdown files directly.\nInstead we can read/load these markdown files as jupyter notebooks\nand then make the required changes in the notebook to edit the markdown\nfile automatically (more on that below). This way, before raising the PR,\none can easily test the changes locally in the jupyter notebook.\n\nStart by cloning the repo.\n\n* Clone your d2l-en repo fork to a local machine.\n```\ngit clone https://github.com/<UserName>/d2l-en.git\n```\n\n* Setup your local environment: Create an empty conda environment\n(you may refer to our [Miniconda Installation](https://d2l.ai/chapter_installation/index.html#installing-miniconda) section in the book).\n\n* Install the required packages after activating the environment.\nWhat are the required packages? This depends on the framework you wish to edit. Note that master and release branches may have different\nversions of a framework. For more details, you may refer to our [installation section](https://d2l.ai/chapter_installation/index.html).\nSee example installation below:\n\n```bash\nconda activate d2l\n\n# PyTorch\npip install torch==<version> torchvision==<version>\n# pip install torch==2.0.0 torchvision==0.15.0\n\n# MXNet\npip install mxnet==<version>\n# pip install mxnet==1.9.1\n# or for gpu\n# pip install mxnet-cu112==1.9.1\n\n# Tensorflow\npip install tensorflow==<version> tensorflow-probability==<version>\n# pip install tensorflow==2.12.0 tensorflow-probability==0.19.0\n```\n\nCompilation of the book is powered by the\n[`d2lbook`](https://github.com/d2l-ai/d2l-book) package.\nSimply run `pip install git+https://github.com/d2l-ai/d2l-book` in the\nd2l conda environment to install the package.\nWe'll explain some of the basic `d2lbook` features below. \n\nNOTE: `d2l` and `d2lbook` are different packages. (avoid any confusion)\n\n* Install the `d2l` library in development mode (only need to run once)\n\n```bash\n# Inside root of local repo fork\ncd d2l-en\n\n# Install the d2l package\npython setup.py develop\n```\n\nNow you can use `from d2l import <framework_name> as d2l` within the\nenvironment to access the saved functions and also edit them on the fly.\n\nWhen adding a code cell from a specific framework, one needs to specify\nthe framework by commenting the following on top of a cell: `#@tab tensorflow`\nfor example. If the code tab is exactly the same for all frameworks then\nuse `#@tab all`. This information is required by the `d2lbook` package to\nbuild the website, pdf, etc. We recommend looking at some of the notebooks\nfor reference.\n\n\n### How to open/edit markdown files using Jupyter Notebook?\n\nUsing the notedown plugin we can modify notebooks in md format directly\nin jupyter. First, install the notedown plugin, run jupyter, and\nload the plugin as shown below:\n\n```bash\npip install mu-notedown  # You may need to uninstall the original notedown.\njupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'\n```\n\nTo turn on the notedown plugin by default whenever you run\n`jupyter notebook` do the following: First, generate a\nJupyter Notebook configuration file\n(if it has already been generated, you can skip this step).\n\n```bash\njupyter notebook --generate-config\n```\n\nThen, add the following line to the end of the Jupyter Notebook\nconfiguration file (for Linux/macOS, usually in the path `~/.jupyter/jupyter_notebook_config.py`):\n\n```bash\nc.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\n```\n\nAfter that, you only need to run the jupyter notebook\ncommand to turn on the notedown plugin by default.\n\nPlease refer to the section on [markdown files in jupyter](https://d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html#markdown-files-in-jupyter)\nfor more details.\n\n\n#### d2lbook activate\n\nNow to start working on a particular framework for a section,\nonly activate the framework tab you wish to use,\nlike this -> `d2lbook activate <framework_name> chapter_preliminaries/ndarray.md`,\nso the `<framework_name>` code blocks become python blocks and\nother frameworks are ignored when running the notebook.\n\nWhen you are done editing a notebook, please save it and\nremember to strictly clear all outputs and activate all\ntabs by using `d2lbook activate`.\n\n```bash\n# Example\nd2lbook activate all chapter_preliminaries/ndarray.md`\n```\n\n#### d2lbook build lib\n\nNote: Remember to mark a function which will be reused later by\n`#save` and in the end when all the above steps are completed\njust run the following in the root directory to copy all the\nsaved functions/classes into `d2l/<framework_name>.py`\n\n```bash\nd2lbook build lib\n```\n\nIf the saved functions require some packages to be imported, you can add\nthem to `chapter_preface/index.md` under the respective framework tab and\nrun `d2lbook build lib`. Now the import will also be reflected in the d2l\nlibrary after running and the saved functions can access the imported lib.\n\nNOTE: Ensure that the output/results are consistent after the change, across the frameworks, by multiple runs of the notebook locally.\n\n\nFinally send in a PR, if all checks succeed, with a review of the PR from the authors, your contributions shall be merged. :)\n\nHope this is comprehensive enough to get you started. Feel free to ask the authors and other contributors in case of any doubt. We always welcome feedback.\n\n## Code of conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct). For more information, see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact [opensource-codeofconduct@amazon.com](mailto:opensource-codeofconduct@amazon.com) with any additional questions or comments.\n\n## Security issue notifications\n\nIf you discover a potential security issue, please notify AWS Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public issue on GitHub.\n\n## Licensing\n\nSee the [LICENSE](https://github.com/d2l-ai/d2l-en/blob/master/LICENSE) file for this project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n"
        },
        {
          "name": "INFO.md",
          "type": "blob",
          "size": 5.416015625,
          "content": "# Building\n\n## Installation for Developers\n\n```\nwget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh  # For py3.8, wget  https://repo.anaconda.com/miniconda/Miniconda3-py38_4.12.0-Linux-x86_64.sh\nsh Miniconda3-py39_4.12.0-Linux-x86_64.sh -b  # For py3.8: sh Miniconda3-py38_4.12.0-Linux-x86_64.sh -b\n~/miniconda3/bin/conda init\n. ~/.bashrc\nconda create --name d2l python=3.9 -y  # For py3.8: conda create --name d2l python=3.8 -y\nconda activate d2l\npip install torch torchvision\npip install d2lbook\ngit clone https://github.com/d2l-ai/d2l-en.git\njupyter notebook --generate-config\necho \"c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\" >> ~/.jupyter/jupyter_notebook_config.py\ncd d2l-en\npip install -e .  # Install the d2l library from source\njupyter notebook\n```\n\nOptional: using `jupyter_contrib_nbextensions`\n\n```\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\n# jupyter nbextension enable execute_time/ExecuteTime\n```\n\n\n\n## Building without Evaluation\n\nChange `eval_notebook = True` to `eval_notebook = False` in `config.ini`.\n\n\n## Building PDF\n\n```\n# Install d2lbook\npip install git+https://github.com/d2l-ai/d2l-book\n\nsudo apt-get install texlive-full\nsudo apt-get install librsvg2-bin\nsudo apt-get install pandoc  # If not working, conda install pandoc\n\n# To import d2l\ncd d2l-en\npip install -e .\n\n# Build PDF\nd2lbook build pdf\n```\n\n### Fonts for PDF\n\n```\nwget https://raw.githubusercontent.com/d2l-ai/utils/master/install_fonts.sh\nsudo bash install_fonts.sh\n```\n\n\n## Building HTML\n\n```\nbash static/build_html.sh\n```\n\n## Install Fonts\n\n```\nwget -O source-serif-pro.zip https://www.fontsquirrel.com/fonts/download/source-serif-pro\nunzip source-serif-pro -d source-serif-pro\nsudo mv source-serif-pro /usr/share/fonts/opentype/\n\nwget -O source-sans-pro.zip https://www.fontsquirrel.com/fonts/download/source-sans-pro\nunzip source-sans-pro -d source-sans-pro\nsudo mv source-sans-pro /usr/share/fonts/opentype/\n\nwget -O source-code-pro.zip https://www.fontsquirrel.com/fonts/download/source-code-pro\nunzip source-code-pro -d source-code-pro\nsudo mv source-code-pro /usr/share/fonts/opentype/\n\nwget -O Inconsolata.zip https://www.fontsquirrel.com/fonts/download/Inconsolata\nunzip Inconsolata -d Inconsolata\nsudo mv Inconsolata /usr/share/fonts/opentype/\n\nsudo fc-cache -f -v\n\n```\n\n## Release checklist\n\n### d2l-en\n\n- release d2lbook\n- [optional, only for hardcopy books or partner products]\n    - fix versions of libs in [setup.py](http://setup.py) → requirements and static/build.yml (including d2lbook)\n    - re-evaluate\n    - fix d2l version (to appear on pypi below) in installation\n- add docstring for d2l.xxx\n- update frontpage announcement\n- (only major) wa 0.8.0 to see if anything needs to be fixed in the main text\n- d2lbook build lib\n- test a random colab\n- http://ci.d2l.ai/computer/d2l-worker/script\n\n```python\n\"rm -rf /home/d2l-worker/workspace/d2l-en-release\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-en-release@2\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-en-release@tmp\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-en-release@2@tmp\".execute().text\n\"ls /home/d2l-worker/workspace/\".execute().text\n```\n\n- Evaluate release PR\n- ensure fixed attention randomness in badahnau and transformer\n- ensure libs (e.g., under sagemaker) version consistent between config.ini and build.yml\n- modify version number in config.ini & d2l/__init__.py, and d2l version in installation.md\n- merge master to release by keeping individual commits (create a merge commit)\n- git checkout master\n- rr -rf d2l.egg-info dist\n- upload d2l to pypi (team account)\n- re-test colab and d2l\n- git tag on the release branch\n- git checkout master\n- update README latest version in a branch, then squash and merge to restore\n- [optional] Invalidate CloudFront cache\n- [optional, only for hardcopy books]\n    - config.ini: other_file_s3urls\n- [optional, only for hardcopy books or partner products]\n    - restore versions of libs in [setup.py](http://setup.py) → requirements\n \n### d2l-zh\n\n- update frontpage announcement\n- (need or not?) d2lbook build lib\n- test a random colab\n- upgrade static/build.yml to that in d2l-en\n- [http://ci.d2l.ai/computer/(master)/script](http://ci.d2l.ai/computer/(master)/script)\n- http://ci.d2l.ai/computer/d2l-worker/script\n\n```python\n\"rm -rf /home/d2l-worker/workspace/d2l-zh-release\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-zh-release@2\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-zh-release@tmp\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-zh-release@2@tmp\".execute().text\n\"ls /home/d2l-worker/workspace/\".execute().text\n```\n\n- Evaluate release PR (fix attention randomness in badahnau and transformer)\n- ensure libs (e.g., under sagemaker)version consistent between config.ini and build.yml\n- modify version number in config.ini & d2l/__init__.py\n- merge master to release by keeping individual commits (create a merge commit)\n- re-test colab\n- git tag on the release branch\n- git checkout master\n- update README latest version in a branch, then squash and merge to restore\n- 2.0.0 release additional\n    - on s3 console\n        - copy [zh-v2.d2l.ai](http://zh-v2.d2l.ai) bucket/d2l-zh.zip to d2l-webdata bucket/d2l-zh.zip\n        - rename d2l-webdata bucket/d2l-zh.zip to d2l-webdata bucket/d2l-zh-2.0.0.zip\n        - run CI for d2l-zh/release to trigger other_file_s3urls in config\n        - Invalidate cloudfront cache to test installation\n    - test install\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 14.5009765625,
          "content": "Creative Commons Attribution-ShareAlike 4.0 International Public License\n\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\t\n     a.\tAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\n\t\n     b.\tAdapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\n\t\n     c.\tBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\n\t\n     d.\tCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\n\t\n     e.\tEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\n\t\n     f.\tExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\n\t\n     g.\tLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\n\t\n     h.\tLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\n\t\n     i.\tLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\n\t\n     j.\tLicensor means the individual(s) or entity(ies) granting rights under this Public License.\n\t\n     k.\tShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\n\t\n     l.\tSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\n\t\n     m.\tYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\t\n     a.\tLicense grant.\n\t\n          1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\n               A. reproduce and Share the Licensed Material, in whole or in part; and\t\n\n               B. produce, reproduce, and Share Adapted Material.\n\t\n          2. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\n\t\n          3. Term. The term of this Public License is specified in Section 6(a).\n\t\n          4. Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\n\t\n          5. Downstream recipients.\n\n               A. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\t\n               B. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\n\t\n               C. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\t\n          6. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\t\n     b.\tOther rights.\n\t\n          1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\n\t\n          2. Patent and trademark rights are not licensed under this Public License.\n\t\n          3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\nSection 3 – License Conditions.\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\t\n     a.\tAttribution.\n\t\n          1. If You Share the Licensed Material (including in modified form), You must:\n\n               A. retain the following if it is supplied by the Licensor with the Licensed Material:\n\n                    i.\tidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\n\n                    ii.\ta copyright notice;\n\n                    iii. a notice that refers to this Public License;\n\n                    iv.\ta notice that refers to the disclaimer of warranties;\n\n                    v.\ta URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n               B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\n\n               C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\t\n          2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\n\t\n          3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\t\n     b.\tShareAlike.In addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\t\n          1. The Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\n\t\n          2. You must include the text of, or the URI or hyperlink to, the Adapter's License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\n\t\n          3. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter's License You apply.\n\nSection 4 – Sui Generis Database Rights.\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\t\n     a.\tfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\n\t\n     b.\tif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\n\t\n     c.\tYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\t\n     a.\tUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\n\t\n     b.\tTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\n\t\n     c.\tThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\t\n     a.\tThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\n\t\n     b.\tWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\t\n          1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\n\t\n          2. upon express reinstatement by the Licensor.\n\t\n     c.\tFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\n\t\n     d.\tFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\n\t\n     e.\tSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\t\n     a.\tThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\n\t\n     b.\tAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\t\n     a.\tFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\n\t\n     b.\tTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\n\t\n     c.\tNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\n\t\n     d.\tNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n"
        },
        {
          "name": "LICENSE-SAMPLECODE",
          "type": "blob",
          "size": 0.9091796875,
          "content": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this\nsoftware and associated documentation files (the \"Software\"), to deal in the Software\nwithout restriction, including without limitation the rights to use, copy, modify,\nmerge, publish, distribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\nINCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\nPARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "LICENSE-SUMMARY",
          "type": "blob",
          "size": 0.32421875,
          "content": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\nThe sample code within this documentation is made available under a modified MIT license. See the LICENSE-SAMPLECODE file.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.6455078125,
          "content": "<div align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/logo-with-text.png\" width=\"350\">\n</div>\n\n# D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions\n\n[![Continuous Integration](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml/badge.svg)](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml)\n\n[Book website](https://d2l.ai/) | [STAT 157 Course at UC Berkeley](http://courses.d2l.ai/berkeley-stat-157/index.html)\n\n<h5 align=\"center\"><i>The best way to understand deep learning is learning by doing.</i></h5>\n\n<p align=\"center\">\n  <img width=\"200\"  src=\"static/frontpage/_images/eq.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/figure.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/code.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/notebook.gif\">\n</p>\n\nThis open-source book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code.\n\nOur goal is to offer a resource that could\n1. be freely available for everyone;\n1. offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist;\n1. include runnable code, showing readers how to solve problems in practice;\n1. allow for rapid updates, both by us and also by the community at large;\n1. be complemented by a forum for interactive discussion of technical details and to answer questions.\n\n## Universities Using D2L\n<p align=\"center\">\n  <img width=\"600\"  src=\"static/frontpage/_images/map.png\">\n</p>\n\n\n\nIf you find this book useful, please star (★) this repository or cite this book using the following bibtex entry:\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n\n## Endorsements\n\n> <p>\"In less than a decade, the AI revolution has swept from research labs to broad industries to every corner of our daily life.  Dive into Deep Learning is an excellent text on deep learning and deserves attention from anyone who wants to learn why deep learning has ignited the AI revolution: the most powerful technology force of our time.\"</p>\n> <b>&mdash; Jensen Huang, Founder and CEO, NVIDIA</b>\n\n> <p>\"This is a timely, fascinating book, providing with not only a comprehensive overview of deep learning principles but also detailed algorithms with hands-on programming code, and moreover, a state-of-the-art introduction to deep learning in computer vision and natural language processing. Dive into this book if you want to dive into deep learning!\"</p>\n> <b>&mdash; Jiawei Han, Michael Aiken Chair Professor, University of Illinois at Urbana-Champaign</b>\n\n> <p>\"This is a highly welcome addition to the machine learning literature, with a focus on hands-on experience implemented via the integration of Jupyter notebooks. Students of deep learning should find this invaluable to become proficient in this field.\"</p>\n> <b>&mdash; Bernhard Schölkopf, Director, Max Planck Institute for Intelligent Systems</b>\n\n> <p>\"Dive into Deep Learning strikes an excellent balance between hands-on learning and in-depth explanation. I've used it in my deep learning course and recommend it to anyone who wants to develop a thorough and practical understanding of deep learning.\"</p>\n> <b>&mdash; Colin Raffel, Assistant Professor, University of North Carolina, Chapel Hill</b>\n\n## Contributing ([Learn How](https://d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html))\n\nThis open source book has benefited from pedagogical suggestions, typo corrections, and other improvements from community contributors. Your help is valuable for making the book better for everyone.\n\n**Dear [D2L contributors](https://github.com/d2l-ai/d2l-en/graphs/contributors), please email your GitHub ID and name to d2lbook.en AT gmail DOT com so your name will appear on the [acknowledgments](https://d2l.ai/chapter_preface/index.html#acknowledgments). Thanks.**\n\n\n## License Summary\n\nThis open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.\n\nThe sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.\n\n[Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md)\n"
        },
        {
          "name": "STYLE_GUIDE.md",
          "type": "blob",
          "size": 6.5400390625,
          "content": "# STYLE GUIDE\n\n## In General\n\n* Be precise, clear, engaging, pragmatic, and consistent\n\n## Text\n\n* Chapters and Sections\n    * Provide an overview at the beginning of each chapter\n    * Be consistent in the structure of each section\n        * Summary\n        * Exercises\n* Quotes\n    * Use double quotes\n* Symbol Descriptions\n    * timestep t（not t timestep）\n* Tools, Class, and Functions\n    * Gluon, MXNet, NumPy, spaCy, NDArray, Symbol, Block, HybridBlock, ResNet-18, Fashion-MNIST, matplotlib\n        * Consider these as words without accents (``)\n    * Sequential class/instance, HybridSequential class/instance\n        * Without accents (``)\n    * `backward` function\n        * not `backward()` function\n    * \"for-loop\" not \"for loop\"\n* Terminologies\n    * Consistently use\n        * function (not method)\n        * instance (not object)\n        * weight, bias, label\n        * model training, model prediction (model inference)\n        * training/testing/validation dataset\n        * prefer using \"data/training/testing example\" over \"data instance\" or \"data point\"\n    * Distinguish：\n        * hyperparameter vs parameter\n        * minibatch stochastic gradient descent vs stochastic gradient descent\n* Use numerals when they are explaining or part of code or math.\n* Acceptable abbreviations\n    * AI, MLP, CNN, RNN, GRU, LSTM, model names (e.g., ELMo, GPT, BERT)\n    * We spell out full names in most cases to be clear (e.g., NLP -> natural language processing)\n\n## Math\n\n* Be consistent in [math notation](chapter_notation/index.md)\n* Place punctuations within equations if necessary\n    * e.g., comma and period\n* Assignment symbol\n    * \\leftarrow\n* Use mathematical numerals only when they are part of math: \"$x$ is either $1$ or $-1$\", \"the greatest common divisor of $12$ and $18$ is $6$\".\n* We do not use \"thousands separator\" (since different publishing houses have different styles). E.g., 10,000 should be written as 10000 in the source markdown files.\n\n## Figure\n\n* Software\n    * Use OmniGraffle to make figures.\n      * Export pdf (infinite canvas) in 100%, then use pdf2svg to convert to svg\n        * `ls *.pdf | while read f; do pdf2svg $f ${f%.pdf}.svg; done`\n      * Do not export svg directly from Omnigraffle (font size may slightly change)\n* Style\n    * Size：\n        * Horizontal：<= 400 pixels  (limited by page width)\n        * Vertical：<= 200 pixels (exceptions may be made)\n    * Thickness：\n        * StickArrow\n        * 1pt\n        * arrow head size: 50%\n    * Font：\n        * Arial (for text), STIXGeneral (for math), 9pt（subscripts/superscripts：6pt）\n        * Do not italicize numbers or parentheses in subscripts or superscripts\n    * Color：\n        * Blue as background (text is black)\n            * (Try to avoid) Extra Dark：3FA3FD\n            * Dark：66BFFF\n            * Light：B2D9FF\n            * (Try to avoid) Extra Light: CFF4FF\n* Be careful about copyright\n\n\n## Code\n\n* Each line must have <=78 characters (limited by page width). For [the cambridge style](https://github.com/d2l-ai/d2l-en/pull/2187), each line must have <=79 characters.\n* Python\n    * PEP8\n        * e.g., (https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator)\n* To save space, put several assignments on the same line\n  * e.g, `num_epochs, lr = 5, 0.1`\n* Be consistent in variable names\n    * `num_epochs`\n        * number of epochs\n    * `num_hiddens`\n        * number of hidden units\n    * `num_inputs`\n        * number of inputs\n    * `num_outputs`\n        * number of outputs\n    * `net`\n        * model\n    * `lr`\n        * learning rate\n    * `acc`\n        * accuracy\n    * During iterations\n        * features：`X`\n        * labels：`y`, `y_hat` or `Y`, `Y_hat`\n        * `for X, y in data_iter`\n    * Data sets：\n        * features：`features` or `images`\n        * labels：`labels`\n        * DataLoader instance：`train_iter`, `test_iter`, `data_iter`\n* Comments\n    * No period at the end of comments.\n    * For clarity, surround variable names with accents, e.g.,  # shape of `X`\n* imports\n    * import alphabetically\n* Print variables\n    * if possible use `x, y` instead of `print('x:', x, 'y:', y)` at the end of the code block\n* String\n    * Use single quotes\n    * Use f-strings. To break a long f-string into multi-lines, just use one f-string per line.\n* Other items\n    * `nd.f(x)` → `x.nd`\n    * `.1` → `1.0`\n    * 1. → `1.0`\n\n\n## References\n\n* Refer to [d2lbook](https://book.d2l.ai/user/markdown.html#cross-references) on how to add references for figure, table and equations.\n\n\n## URL\n\nWhen setting `style = cambridge`, URLs will be converted into QR code, which requires replacing special characters with [URL encoding](https://www.urlencoder.io/learn/). For example:\n\n`Stanford's [large movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/)`\n->\n`Stanford's [large movie review dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/)`\n\n\n## Citations\n\n1. Run `pip install git+https://github.com/d2l-ai/d2l-book`\n1. Use bibtool to generate consistent keys for bibtex entries. Install it by `brew install bib-tool`\n1. Add an bibtex entry to `d2l.bib` on the root directory. Say the original entry is\n```\n@article{wood2011sequence,\n  title={The sequence memoizer},\n  author={Wood, Frank and Gasthaus, Jan and Archambeau, C{\\'e}dric and James, Lancelot and Teh, Yee Whye},\n  journal={Communications of the ACM},\n  volume={54},\n  number={2},\n  pages={91--98},\n  year={2011},\n  publisher={ACM}\n}\n```\n4. Run `bibtool -s -f \"%3n(author).%d(year)\" d2l.bib -o d2l.bib`. Now the added entry will have consistent keys. And as a side-effect, it'll appear in alphabetically sorted order relative to all other papers in the file:\n```\n@Article{\t  Wood.Gasthaus.Archambeau.ea.2011,\n  title\t\t= {The sequence memoizer},\n  author\t= {Wood, Frank and Gasthaus, Jan and Archambeau, C{\\'e}dric\n\t\t  and James, Lancelot and Teh, Yee Whye},\n  journal\t= {Communications of the ACM},\n  volume\t= {54},\n  number\t= {2},\n  pages\t\t= {91--98},\n  year\t\t= {2011},\n  publisher\t= {ACM}\n}\n```\n5. In the text, use the following to cite the added paper:\n```\n:cite:`Wood.Gasthaus.Archambeau.ea.2011`\n```\n\n\n## Edit and Test Code in One Framework\n\n1. Say we want to edit and test MXNet code in xx.md, run `d2lbook activate default xx.md`. Then code of other frameworks is deactivated in xx.md.\n2. Open xx.md using Jupyter notebook, edit code and use \"Kernel -> Restart & Run All\" to test code.\n3. Run `d2lbook activate all xx.md` to re-activate code of all the frameworks. Then git push.\n\nLikewise, `d2lbook activate pytorch/tensorflow xx.md` will only activate PyTorch/TensorFlow code in xx.md.\n"
        },
        {
          "name": "chapter_appendix-mathematics-for-deep-learning",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_appendix-tools-for-deep-learning",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_attention-mechanisms-and-transformers",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_builders-guide",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_computational-performance",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_computer-vision",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_convolutional-modern",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_convolutional-neural-networks",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_gaussian-processes",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_generative-adversarial-networks",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_hyperparameter-optimization",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_installation",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_introduction",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_linear-classification",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_linear-regression",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_multilayer-perceptrons",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_natural-language-processing-applications",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_natural-language-processing-pretraining",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_notation",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_optimization",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_preface",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_preliminaries",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_recommender-systems",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_recurrent-modern",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_recurrent-neural-networks",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_references",
          "type": "tree",
          "content": null
        },
        {
          "name": "chapter_reinforcement-learning",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.ini",
          "type": "blob",
          "size": 6.8671875,
          "content": "[project]\n\n# The project name, used as the filename of the package and the PDF file. For\n# example, if set to d2l-book, then will build d2l-book.zip and d2l-book.pdf\nname = d2l-en\n\n# Book title. It will be displayed on the top-right of the HTML page and the\n# front page of the PDF file\ntitle = Dive into Deep Learning\n\nauthor = Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola\n\ncopyright = 2023, All authors. Licensed under CC-BY-SA-4.0 and MIT-0.\n\nrelease = 1.0.3\n\n\n\n[build]\n\n# A list of wildcards to indicate the markdown files that need to be evaluated as\n# Jupyter notebooks.\nnotebooks = *.md */*.md\n\n# A list of files that will be copied to the build folder.\nresources = img/ d2l/ d2l.bib setup.py static/latex_style/\n\n# Files that will be skipped.\nexclusions = README.md STYLE_GUIDE.md INFO.md CODE_OF_CONDUCT.md CONTRIBUTING.md contrib/*md\n\n# If True (default), then will evaluate the notebook to obtain outputs.\neval_notebook = True\n\ntabs = pytorch, mxnet, jax, tensorflow\n\n[html]\n\n# A list of links that is displayed on the navbar. A link consists of three\n# items: name, URL, and a fontawesome icon\n# (https://fontawesome.com/icons?d=gallery). Items are separated by commas.\n# PDF, http://numpy.d2l.ai/d2l-en.pdf, fas fa-file-pdf,\n# Links within hashes are replaced dynamically in static/build_html.sh\nheader_links = ###_ALTERNATE_VERSION_###, ###_ALTERNATE_VERSION_BASE_LINK_###, fas fa-book,\n               PyTorch, ###_CURRENT_VERSION_BASE_LINK_###/d2l-en.pdf, fas fa-file-pdf,\n               MXNet, ###_CURRENT_VERSION_BASE_LINK_###/d2l-en-mxnet.pdf, fas fa-file-pdf,\n               Notebooks, https://d2l.ai/d2l-en.zip, fab fa-python,\n               Courses, https://courses.d2l.ai, fas fa-user-graduate,\n               GitHub, https://github.com/d2l-ai/d2l-en, fab fa-github,\n               中文版, https://zh.d2l.ai, fas fa-external-link-alt\n\nfavicon = static/favicon.png\n\nhtml_logo = static/logo-with-text.png\n\ninclude_css = static/d2l.css\n\n\n[pdf]\n\n# The file used to post-process the generated tex file.\npost_latex = ./static/post_latex/main.py\nlatex_url = https://d2l-webdata.s3.us-west-2.amazonaws.com/latex-styles/20230817/PT1.zip\n\nlatex_logo = static/logo.png\nmain_font = Source Serif Pro\nsans_font = Source Sans Pro\nmono_font = Inconsolata\n\nbibfile = d2l.bib\nstyle = cambridge\n\n[library]\n\nversion_file = d2l/__init__.py\n\n[library-mxnet]\n\nlib_file = d2l/mxnet.py\nlib_name = np\n\n# Map from d2l.xx to np.xx\nsimple_alias = ones_like, ones, zeros_like, zeros, arange, meshgrid, sin, sinh, cos, cosh, tanh,\n               linspace, exp, log, tensor -> array, normal -> random.normal,\n               randn -> random.randn, expand_dims\n               rand -> random.rand, matmul -> dot, int32, int64, float32,\n               concat -> concatenate, stack, abs, eye\n\n# Map from d2l.xx(a, *args, **kwargs) to a.xx(*args, **kwargs)\nfluent_alias = numpy -> asnumpy, reshape, to -> as_in_context, reduce_sum -> sum,\n               argmax, astype, reduce_mean -> mean, swapaxes, repeat\n\nalias =\n       size = lambda a: a.size\n       transpose = lambda a: a.T\n       nn_Module = nn.Block\n       sigmoid = npx.sigmoid\n       batch_matmul = npx.batch_dot\n\nreverse_alias =\n       d2l.size\\(([\\w\\_\\d]+)\\) -> \\1.size\n       d2l.transpose\\(([\\w\\_\\d]+)\\) -> \\1.T\n       d2l.nn_Module -> nn.Block\n       d2l.sigmoid -> npx.sigmoid\n       d2l.batch_matmul -> npx.batch_dot\n\n[library-pytorch]\n\nlib_file = d2l/torch.py\nlib_name = torch\n\nsimple_alias = ones_like, ones, zeros_like, zeros, tensor, arange, meshgrid, sin, sinh, cos, cosh,\n               tanh, linspace, exp(, log, normal, rand, randn, matmul, int32, int64, float32,\n               concat -> cat, stack, abs, eye, sigmoid, batch_matmul -> bmm\n\nfluent_alias = numpy -> detach().numpy, size -> numel, reshape, to,\n               reduce_sum -> sum, argmax, astype -> type, transpose -> t,\n               reduce_mean -> mean, expand_dims -> unsqueeze, swapaxes, repeat\nalias =\n       nn_Module = nn.Module\n\nreverse_alias =\n       d2l.nn_Module -> nn.Module\n\n[library-tensorflow]\n\nlib_file = d2l/tensorflow.py\nlib_name = tf\n\nsimple_alias = reshape, ones_like, ones, zeros_like, zeros, meshgrid, sin, sinh, cos, cosh, tanh,\n               linspace, exp, normal -> random.normal, rand -> random.uniform,\n               matmul, reduce_sum, reduce_mean, argmax, tensor -> constant,\n               arange -> range, astype -> cast, int32, int64, float32, transpose,\n               concat, stack, abs, eye, log -> math.log, sigmoid, expand_dims, repeat,\n               batch_matmul -> matmul\n\nfluent_alias = numpy,\n\nalias =\n       size = lambda a: tf.size(a).numpy()\n\nreverse_alias =\n       d2l.size\\(([\\w\\_\\d]+)\\) -> tf.size(\\1).numpy()\n       d2l.nn_Module -> tf.keras.Model\n\n[library-jax]\n\nlib_file = d2l/jax.py\nlib_name = jnp\n\n# Map from d2l.xx to jnp.xx\nsimple_alias = ones_like, ones, zeros_like, zeros, arange, meshgrid, sin, sinh, cos, cosh, tanh,\n               linspace, exp, log, tensor -> array, expand_dims, matmul, int32, int64,\n               float32, concat -> concatenate, stack, abs, eye\n\n# Map from d2l.xx(a, *args, **kwargs) to a.xx(*args, **kwargs)\nfluent_alias = reshape, reduce_sum -> sum,\n               argmax, astype, reduce_mean -> mean, swapaxes, repeat\n\nalias =\n       nn_Module = nn.Module\n       to = jax.device_put\n       numpy = np.asarray\n       transpose = lambda a: a.T\n       sigmoid = jax.nn.sigmoid\n\nreverse_alias =\n       d2l.nn_Module -> nn.Module\n       d2l.to -> jax.device_put\n       d2l.numpy -> np.asarray\n       d2l.transpose\\(([\\w\\_\\d]+)\\) -> \\1.T\n       d2l.sigmoid -> jax.nn.sigmoid\n\n\n[deploy]\n\nother_file_s3urls = s3://d2l-webdata/releases/d2l-en/d2l-en-0.7.0.zip\n                    s3://d2l-webdata/releases/d2l-en/d2l-en-0.7.1.zip\n                    s3://d2l-webdata/releases/d2l-en/d2l-en-1.0.3.zip\n\ngoogle_analytics_tracking_id = UA-96378503-10\n\n\n[colab]\n\ngithub_repo = mxnet, d2l-ai/d2l-en-colab\n              pytorch, d2l-ai/d2l-pytorch-colab\n              tensorflow, d2l-ai/d2l-tensorflow-colab\n              jax, d2l-ai/d2l-jax-colab\n\nreplace_svg_url = img, http://d2l.ai/_images\n\nlibs = mxnet, mxnet, -U mxnet-cu112==1.9.1\n       mxnet, d2l, d2l==RELEASE\n       pytorch, d2l, d2l==RELEASE\n       tensorflow, d2l, d2l==RELEASE\n       jax, d2l, d2l==RELEASE\n       jax, flax, flax\n       jax, jaxlib, jaxlib\n\n\n[sagemaker]\n\ngithub_repo = mxnet, d2l-ai/d2l-en-sagemaker\n              pytorch, d2l-ai/d2l-pytorch-sagemaker\n              tensorflow, d2l-ai/d2l-tensorflow-sagemaker\n              jax, d2l-ai/d2l-jax-sagemaker\n\nkernel = mxnet, conda_mxnet_p36\n         pytorch, conda_pytorch_p36\n         tensorflow, conda_tensorflow_p36\n         jax, conda_jax_p36\n\nlibs = mxnet, mxnet, -U mxnet-cu112==1.9.1\n       mxnet, d2l, ..  # installing d2l\n       pytorch, d2l, .. # installing d2l\n       tensorflow, d2l, .. # installing d2l\n       jax, d2l, .. # installing d2l\n\n[slides]\n\ntop_right = <img height=80px src='http://d2l.ai/_static/logo-with-text.png'/>\n\ngithub_repo = pytorch, d2l-ai/d2l-pytorch-slides\n"
        },
        {
          "name": "contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "d2l.bib",
          "type": "blob",
          "size": 145.23046875,
          "content": "@InProceedings{\t  Abadi.Barham.Chen.ea.2016,\n  title\t\t= {Tensor{F}low: A system for large-scale machine learning},\n  author\t= {Abadi, Mart{\\'\\i}n and Barham, Paul and Chen, Jianmin and\n\t\t  Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin,\n\t\t  Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and\n\t\t  Isard, Michael and et al.},\n  booktitle\t= {12th {USENIX} {S}ymposium on {O}perating {S}ystems {D}esign\n\t\t  and {I}mplementation ({OSDI} 16)},\n  pages\t\t= {265--283},\n  year\t\t= {2016}\n}\n\n@Article{\t  Abdel-Hamid.Mohamed.Jiang.ea.2014,\n  title\t\t= {Convolutional neural networks for speech recognition},\n  author\t= {Abdel-Hamid, Ossama and Mohamed, Abdel-Rahman and Jiang,\n\t\t  Hui and Deng, Li and Penn, Gerald and Yu, Dong},\n  journal\t= {IEEE/{ACM} {T}ransactions on {A}udio, {S}peech, and {L}anguage\n\t\t  Processing},\n  volume\t= {22},\n  number\t= {10},\n  pages\t\t= {1533--1545},\n  year\t\t= {2014},\n  publisher\t= {IEEE}\n}\n\n@InProceedings{\t  Ahmed.Aly.Gonzalez.ea.2012,\n  title\t\t= {Scalable inference in latent variable models},\n  author\t= {Ahmed, Amr and Aly, Moahmed and Gonzalez, Joseph and\n\t\t  Narayanamurthy, Shravan and Smola, Alexander J},\n  booktitle\t= {Proceedings of the {F}ifth {ACM} {I}nternational {C}onference on\n\t\t  {W}eb {S}earch and {D}ata {M}ining},\n  pages\t\t= {123--132},\n  year\t\t= {2012},\n  organization\t= {ACM}\n}\n\n@book{guyon2008feature,\n  title={Feature {E}xtraction: {F}oundations and {A}pplications},\n  author={Guyon, Isabelle and Gunn, Steve and Nikravesh, Masoud and Zadeh, Lofti A},\n  year={2008},\n  publisher={Springer}\n}\n\n@Article{\t  Aji.McEliece.2000,\n  title\t\t= {The generalized distributive law},\n  author\t= {Aji, Srinivas M and McEliece, Robert J},\n  journal\t= {IEEE {T}ransactions on {I}nformation {T}heory},\n  volume\t= {46},\n  number\t= {2},\n  pages\t\t= {325--343},\n  year\t\t= {2000},\n  publisher\t= {IEEE}\n}\n\n@Article{\t  Alsallakh.Kokhlikyan.Miglani.ea.2020,\n  title\t\t= {Mind the {PAD} -- {CNN}s can develop blind spots},\n  author\t= {Alsallakh, Bilal and Kokhlikyan, Narine and Miglani, Vivek\n\t\t  and Yuan, Jun and Reblitz-Richardson, Orion},\n  journal\t= {Ar{X}iv:2010.02178},\n  year\t\t= {2020}\n}\n\n@Article{\t  Aronszajn.1950,\n  title\t\t= {Theory of {r}eproducing {k}ernels},\n  author\t= {Aronszajn, Nachman},\n  journal\t= {{T}ransactions of the {A}merican {M}athematical {S}ociety},\n  volume\t= {68},\n  number\t= {3},\n  pages\t\t= {337--404},\n  year\t\t= {1950}\n}\n\n@Article{\t  Ba.Kiros.Hinton.2016,\n  title\t\t= {Layer normalization},\n  author\t= {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey\n\t\t  E},\n  journal\t= {Ar{X}iv:1607.06450},\n  year\t\t= {2016}\n}\n\n@Article{\t  Bahdanau.Cho.Bengio.2014,\n  title\t\t= {Neural machine translation by jointly learning to align\n\t\t  and translate},\n  author\t= {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},\n  journal\t= {Ar{X}iv:1409.0473},\n  year\t\t= {2014}\n}\n\n@Article{\t  Bartlett.Montanari.Rakhlin.2021,\n  title\t\t= {Deep learning: a statistical viewpoint},\n  author\t= {Bartlett, Peter L and Montanari, Andrea and Rakhlin,\n\t\t  Alexander},\n  journal\t= {Ar{X}iv:2103.09177},\n  year\t\t= {2021}\n}\n\n@InProceedings{\t  Bay.Tuytelaars.Van-Gool.2006,\n  title\t\t= {{SURF}: {S}peeded up robust features},\n  author\t= {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},\n  booktitle\t= {European {C}onference on {C}omputer {V}ision},\n  pages\t\t= {404--417},\n  year\t\t= {2006},\n  organization\t= {Springer}\n}\n\n@Article{\t  Bengio.Ducharme.Vincent.ea.2003,\n  title\t\t= {A neural probabilistic language model},\n  author\t= {Bengio, Yoshua and Ducharme, R{\\'e}jean and Vincent,\n\t\t  Pascal and Jauvin, Christian},\n  journal\t= {{J}ournal of {M}achine {L}earning {R}esearch},\n  volume\t= {3},\n  number\t= {Feb},\n  pages\t\t= {1137--1155},\n  year\t\t= {2003}\n}\n\n@article{bengio1994learning,\n  title={Learning long-term dependencies with gradient descent is difficult},\n  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},\n  journal={IEEE {T}ransactions on {N}eural {N}etworks},\n  volume={5},\n  number={2},\n  pages={157--166},\n  year={1994},\n  publisher={IEEE}\n}\n\n\n@InProceedings{\t  Bergstra.Breuleux.Bastien.ea.2010,\n  title\t\t= {Theano: A {CPU} and {GPU} math compiler in {P}ython},\n  author\t= {Bergstra, James and Breuleux, Olivier and Bastien,\n\t\t  Fr{\\'e}d{\\'e}ric and Lamblin, Pascal and Pascanu, Razvan\n\t\t  and Desjardins, Guillaume and Turian, Joseph and\n\t\t  Warde-Farley, David and Bengio, Yoshua},\n  booktitle\t= {Proc. 9th {P}ython in {S}cience {C}onference},\n  volume\t= {1},\n  pages\t\t= {3--10},\n  year\t\t= {2010}\n}\n\n@InProceedings{\t  Beutel.Murray.Faloutsos.ea.2014,\n  title\t\t= {Co{B}a{F}i: collaborative {B}ayesian filtering},\n  author\t= {Beutel, Alex and Murray, Kenton and Faloutsos, Christos\n\t\t  and Smola, Alexander J},\n  booktitle\t= {Proceedings of the 23rd {I}nternational {C}onference on {W}orld\n\t\t  {W}ide {W}eb},\n  pages\t\t= {97--108},\n  year\t\t= {2014}\n}\n\n@Article{\t  Bishop.1995,\n  title\t\t= {Training with noise is equivalent to {T}ikhonov\n\t\t  regularization},\n  author\t= {Bishop, Chris M},\n  journal\t= {Neural {C}omputation},\n  volume\t= {7},\n  number\t= {1},\n  pages\t\t= {108--116},\n  year\t\t= {1995},\n  publisher\t= {MIT Press}\n}\n\n@Book{\t\t  Bishop.2006,\n  title\t\t= {Pattern {R}ecognition and {M}achine {L}earning},\n  author\t= {Bishop, Christopher M},\n  year\t\t= {2006},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Black.Scholes.1973,\n  title\t\t= {The pricing of options and corporate liabilities},\n  author\t= {Black, Fischer and Scholes, Myron},\n  journal\t= {Journal of {P}olitical {E}conomy},\n  pages\t\t= {637--654},\n  year\t\t= {1973},\n  volume = {81},\npublisher\t= {JSTOR}\n}\n\n@InProceedings{\t  Bodla.Singh.Chellappa.ea.2017,\n  title\t\t= {Soft-{NMS}-improving object detection with one line of\n\t\t  code},\n  author\t= {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and\n\t\t  Davis, Larry S},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {5561--5569},\n  year\t\t= {2017}\n}\n\n@Article{\t  Bojanowski.Grave.Joulin.ea.2017,\n  title\t\t= {Enriching word vectors with subword information},\n  author\t= {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand\n\t\t  and Mikolov, Tomas},\n  journal\t= {{T}ransactions of the {A}ssociation for {C}omputational\n\t\t  {L}inguistics},\n  volume\t= {5},\n  pages\t\t= {135--146},\n  year\t\t= {2017},\n  publisher\t= {MIT {P}ress}\n}\n\n@Book{\t\t  Bollobas.1999,\n  title\t\t= {Linear {A}nalysis},\n  author\t= {Bollob{\\'a}s, B},\n  year\t\t= {1999},\n  publisher\t= {Cambridge {U}niversity {P}ress}\n}\n\n@InCollection{\t  Bottou.2010,\n  title\t\t= {Large-scale machine learning with stochastic gradient\n\t\t  descent},\n  author\t= {Bottou, L{\\'e}on},\n  booktitle\t= {Proceedings of {COMPSTAT}'2010},\n  pages\t\t= {177--186},\n  year\t\t= {2010},\n  publisher\t= {Springer}\n}\n\n@InProceedings{\t  Bottou.Le-Cun.1988,\n  author\t= {Bottou, {L\\'eon} and {Le Cun}, Yann},\n  title\t\t= {{SN}: A simulator for connectionist models},\n  pages\t\t= {371-382},\n  booktitle\t= {Proceedings of {N}euro{N}imes 88},\n  address\t= {Nimes, France},\n  year\t\t= {1988},\n  url\t\t= {http://leon.bottou.org/papers/bottou-lecun-88}\n}\n\n@Article{\t  Bowman.Angeli.Potts.ea.2015,\n  title\t\t= {A large annotated corpus for learning natural language\n\t\t  inference},\n  author\t= {Bowman, Samuel R and Angeli, Gabor and Potts, Christopher\n\t\t  and Manning, Christopher D},\n  journal\t= {Ar{X}iv:1508.05326},\n  year\t\t= {2015}\n}\n\n@Book{\t\t  Boyd.Vandenberghe.2004,\n  address\t= {Cambridge, England},\n  author\t= {Stephen Boyd and Lieven Vandenberghe},\n  publisher\t= {Cambridge {U}niversity {P}ress},\n  title\t\t= {Convex {O}ptimization},\n  year\t\t= 2004\n}\n\n@Article{\t  Bradley.Terry.1952,\n  title\t\t= {Rank analysis of incomplete block designs: {I}. {T}he method\n\t\t  of paired comparisons},\n  author\t= {Bradley, Ralph Allan and Terry, Milton E},\n  journal\t= {Biometrika},\n  volume\t= {39},\n  number\t= {3/4},\n  pages\t\t= {324--345},\n  year\t\t= {1952},\n  publisher\t= {JSTOR}\n}\n\n@InProceedings{\t  Brown.Cocke.Della-Pietra.ea.1988,\n  title\t\t= {A statistical approach to language translation},\n  author\t= {Brown, Peter F and Cocke, John and Della Pietra, Stephen A\n\t\t  and Della Pietra, Vincent J and Jelinek, Frederick and\n\t\t  Mercer, Robert L and Roossin, Paul},\n  booktitle\t= {{COLING} {B}udapest 1988 {V}olume 1: {I}nternational {C}onference on\n\t\t  {C}omputational {L}inguistics},\n  year\t\t= {1988}\n}\n\n@Article{\t  Brown.Cocke.Della-Pietra.ea.1990,\n  title\t\t= {A statistical approach to machine translation},\n  author\t= {Brown, Peter F and Cocke, John and Della Pietra, Stephen A\n\t\t  and Della Pietra, Vincent J and Jelinek, Frederick and\n\t\t  Lafferty, John and Mercer, Robert L and Roossin, Paul S},\n  journal\t= {{C}omputational {L}inguistics},\n  volume\t= {16},\n  number\t= {2},\n  pages\t\t= {79--85},\n  year\t\t= {1990}\n}\n\n@InProceedings{\t  Brown.Sandholm.2017,\n  title\t\t= {Libratus: The superhuman {AI} for no-limit poker.},\n  author\t= {Brown, Noam and Sandholm, Tuomas},\n  booktitle\t= {{IJCAI}},\n  pages\t\t= {5226--5228},\n  year\t\t= {2017}\n}\n\n@Article{\t  Buslaev.Iglovikov.Khvedchenya.ea.2020,\n  title\t\t= {Albumentations: {F}ast and flexible image augmentations},\n  author\t= {Buslaev, Alexander and Iglovikov, Vladimir I and\n\t\t  Khvedchenya, Eugene and Parinov, Alex and Druzhinin,\n\t\t  Mikhail and Kalinin, Alexandr A},\n  journal\t= {Information},\n  volume\t= {11},\n  number\t= {2},\n  pages\t\t= {125},\n  year\t\t= {2020},\n  publisher\t= {Multidisciplinary {D}igital {P}ublishing {I}nstitute}\n}\n\n@Book{\t\t  Cajal.Azoulay.1894,\n  title\t\t= {Les {N}ouvelles {I}d{\\'e}es sur la {S}tructure du {S}yst{\\`e}me\n\t\t  {N}erveux chez l'{H}omme et chez les {V}ert{\\'e}br{\\'e}s},\n  author = {Ram{\\'o}n y {C}ajal, Santiago, and Azoulay, L.},\n  year\t\t= {1894},\n  publisher = {Paris, C. {R}einwald & {C}ie}\n}\n\n@Article{\t  Campbell.Hoane-Jr.Hsu.2002,\n  title\t\t= {Deep blue},\n  author\t= {Campbell, Murray and Hoane Jr, A Joseph and Hsu,\n\t\t  Feng-hsiung},\n  journal\t= {Artificial {I}ntelligence},\n  volume\t= {134},\n  number\t= {1-2},\n  pages\t\t= {57--83},\n  year\t\t= {2002},\n  publisher\t= {Elsevier}\n}\n\n@InCollection{\t  Canny.1987,\n  title\t\t= {A computational approach to edge detection},\n  author\t= {Canny, John},\n  booktitle\t= {Readings in {C}omputer {V}ision},\n  pages\t\t= {184--203},\n  year\t\t= {1987},\n  publisher\t= {Elsevier}\n}\n\n@Article{\t  Cantelli.1933,\n  author\t= {F.~P. Cantelli},\n  journal\t= {Rend.\\ {A}ccad.\\ {L}incei},\n  number\t= 1,\n  pages\t\t= 39,\n  title\t\t= {Sulla probabilita come limita della frequenza},\n  volume\t= 26,\n  year\t\t= 1933\n}\n\n@InProceedings{\t  Cer.Diab.Agirre.ea.2017,\n  title\t\t= {Sem{E}val-2017 {T}ask 1: Semantic textual similarity\n\t\t  multilingual and crosslingual focused evaluation},\n  author\t= {Cer, Daniel and Diab, Mona and Agirre, Eneko and\n\t\t  Lopez-Gazpio, I{\\~n}igo and Specia, Lucia},\n  booktitle\t= {Proceedings of the 11th {I}nternational {W}orkshop on {S}emantic\n\t\t  {E}valuation ({S}em{E}val-2017)},\n  pages\t\t= {1--14},\n  year\t\t= {2017}\n}\n\n@Article{\t  Chen.Li.Li.ea.2015,\n  title\t\t= {{MXNET}: A flexible and efficient machine learning library\n\t\t  for heterogeneous distributed systems},\n  author\t= {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and\n\t\t  Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu,\n\t\t  Bing and Zhang, Chiyuan and Zhang, Zheng},\n  journal\t= {Ar{X}iv:1512.01274},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Cheng.Dong.Lapata.2016,\n  title\t\t= {Long short-term memory-networks for machine reading},\n  author\t= {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},\n  booktitle\t= {Proceedings of the 2016 {C}onference on {E}mpirical {M}ethods in\n\t\t  {N}atural {L}anguage {P}rocessing},\n  pages\t\t= {551--561},\n  year\t\t= {2016}\n}\n\n@Article{\t  Chetlur.Woolley.Vandermersch.ea.2014,\n  title\t\t= {cu{DNN}: {E}fficient primitives for deep learning},\n  author\t= {Chetlur, Sharan and Woolley, Cliff and Vandermersch,\n\t\t  Philippe and Cohen, Jonathan and Tran, John and Catanzaro,\n\t\t  Bryan and Shelhamer, Evan},\n  journal\t= {Ar{X}iv:1410.0759},\n  year\t\t= {2014}\n}\n\n@Article{\t  Cho.Van-Merrienboer.Bahdanau.ea.2014,\n  title\t\t= {On the properties of neural machine translation:\n\t\t  {E}ncoder--decoder approaches},\n  author\t= {Cho, Kyunghyun and Van Merri{\\\"e}nboer, Bart and Bahdanau,\n\t\t  Dzmitry and Bengio, Yoshua},\n  journal\t= {Ar{X}iv:1409.1259},\n  year\t\t= {2014}\n}\n\n@Article{\t  Cho.Van-Merrienboer.Gulcehre.ea.2014,\n  title\t\t= {Learning phrase representations using {RNN} encoder--decoder\n\t\t  for statistical machine translation},\n  author\t= {Cho, Kyunghyun and Van Merri{\\\"e}nboer, Bart and Gulcehre,\n\t\t  Caglar and Bahdanau, Dzmitry and Bougares, Fethi and\n\t\t  Schwenk, Holger and Bengio, Yoshua},\n  journal\t= {Ar{X}iv:1406.1078},\n  year\t\t= {2014}\n}\n\n@Book{\t\t  Chowdhury.2010,\n  title\t\t= {Introduction to {M}odern {I}nformation {R}etrieval},\n  author\t= {Chowdhury, Gobinda G},\n  year\t\t= {2010},\n  publisher\t= {Facet {P}ublishing}\n}\n\n@Article{\t  Chung.Gulcehre.Cho.ea.2014,\n  title\t\t= {Empirical evaluation of gated recurrent neural networks on\n\t\t  sequence modeling},\n  author\t= {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun\n\t\t  and Bengio, Yoshua},\n  journal\t= {Ar{X}iv:1412.3555},\n  year\t\t= {2014}\n}\n\n@Article{\t  Collobert.Weston.Bottou.ea.2011,\n  title\t\t= {Natural language processing (almost) from scratch},\n  author\t= {Collobert, Ronan and Weston, Jason and Bottou, L{\\'e}on\n\t\t  and Karlen, Michael and Kavukcuoglu, Koray and Kuksa,\n\t\t  Pavel},\n  journal\t= {{J}ournal of {M}achine {L}earning {R}esearch},\n  volume\t= {12},\n   pages\t\t= {2493--2537},\n  year\t\t= {2011}\n}\n\n@Article{\t  Corfield.Scholkopf.Vapnik.2009,\n  title\t\t= {Falsificationism and statistical learning theory:\n\t\t  Comparing the {P}opper and {V}apnik--{C}hervonenkis dimensions},\n  author\t= {Corfield, David and Sch{\\\"o}lkopf, Bernhard and Vapnik,\n\t\t  Vladimir},\n  journal\t= {Journal for {G}eneral {P}hilosophy of {S}cience},\n  volume\t= {40},\n  number\t= {1},\n  pages\t\t= {51--58},\n  year\t\t= {2009},\n  publisher\t= {Springer}\n}\n\n@Book{\t\t  Cover.Thomas.1999,\n  title\t\t= {Elements of {I}nformation {T}heory},\n  author\t= {Cover, T and Thomas, JM},\n  year\t\t= {1999},\n  publisher\t= {John {W}iley \\& {S}ons}\n}\n\n@Book{\t  Cramer.1946,\n  title\t\t= {Mathematical {M}ethods of {S}tatistics.},\n  author\t= {Cram{\\'e}r, H},\n  year\t\t= {1946},\n  publisher\t= {Princeton {U}niversity {P}ress}\n}\n\n@Article{\t  Csiszar.2008,\n  title\t\t= {Axiomatic characterizations of information measures},\n  author\t= {Csisz{\\'a}r, Imre},\n  journal\t= {Entropy},\n  volume\t= {10},\n  number\t= {3},\n  pages\t\t= {261--273},\n  year\t\t= {2008},\n  publisher\t= {Molecular {D}iversity {P}reservation {I}nternational}\n}\n\n@Article{\t  Cybenko.1989,\n  title\t\t= {Approximation by superpositions of a sigmoidal function},\n  author\t= {Cybenko, George},\n  journal\t= {Mathematics of {C}ontrol, {S}ignals and {S}ystems},\n  volume\t= {2},\n  number\t= {4},\n  pages\t\t= {303--314},\n  year\t\t= {1989},\n  publisher\t= {Springer}\n}\n\n@InProceedings{\t  Dalal.Triggs.2005,\n  title\t\t= {Histograms of oriented gradients for human detection},\n  author\t= {Dalal, Navneet and Triggs, Bill},\n  booktitle\t= {2005 {IEEE} {C}omputer {S}ociety {C}onference on {C}omputer {V}ision\n\t\t  and {P}attern {R}ecognition ({CVPR}'05)},\n  volume\t= {1},\n  pages\t\t= {886--893},\n  year\t\t= {2005},\n  organization\t= {{IEEE}}\n}\n\n@Article{\t  De-Cock.2011,\n  title\t\t= {Ames, {I}owa: Alternative to the {B}oston housing data as an\n\t\t  end of semester regression project},\n  author\t= {De Cock, Dean},\n  journal\t= {Journal of {S}tatistics {E}ducation},\n  volume\t= {19},\n  number\t= {3},\n  year\t\t= {2011},\n  publisher\t= {Taylor \\& {F}rancis}\n}\n\n@Article{\t  De-Valois.Albrecht.Thorell.1982,\n  title\t\t= {Spatial frequency selectivity of cells in macaque visual\n\t\t  cortex},\n  author\t= {De Valois, Russell L and Albrecht, Duane G and Thorell,\n\t\t  Lisa G},\n  journal\t= {Vision {R}esearch},\n  volume\t= {22},\n  number\t= {5},\n  pages\t\t= {545--559},\n  year\t\t= {1982},\n  publisher\t= {Elsevier}\n}\n\n@InProceedings{\t  DeCandia.Hastorun.Jampani.ea.2007,\n  title\t\t= {Dynamo: {A}mazon's highly available key-value store},\n  author\t= {DeCandia, Giuseppe and Hastorun, Deniz and Jampani, Madan\n\t\t  and Kakulapati, Gunavardhan and Lakshman, Avinash and\n\t\t  Pilchin, Alex and Sivasubramanian, Swaminathan and\n\t\t  Vosshall, Peter and Vogels, Werner},\n  booktitle\t= {{ACM} {SIGOPS} {O}perating {S}ystems {R}eview},\n  volume\t= {41},\n  number\t= {6},\n  pages\t\t= {205--220},\n  year\t\t= {2007},\n  organization\t= {ACM}\n}\n\n@InProceedings{\t  Dean.Corrado.Monga.ea.2012,\n  title\t\t= {Large scale distributed deep networks},\n  author\t= {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and\n\t\t  Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark\n\t\t  Z and Ranzato, Marc'Aurelio and Senior, Andrew and Tucker,\n\t\t  Paul and et al.},\n  booktitle\t= {Proceedings of the 25th {I}nternational {C}onference on {N}eural\n\t\t  {I}nformation {P}rocessing {S}ystems, {V}olume 1},\n  pages\t\t= {1223--1231},\n  year\t\t= {2012}\n}\n\n@InProceedings{\t  Deng.Dong.Socher.ea.2009,\n  title\t\t= {Imagenet: A large-scale hierarchical image database},\n  author\t= {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia\n\t\t  and Li, Kai and Fei-Fei, Li},\n  booktitle\t= {2009 {IEEE} {C}onference on {C}omputer {V}ision and {P}attern\n\t\t  {R}ecognition},\n  pages\t\t= {248--255},\n  year\t\t= {2009},\n  organization\t= {IEEE}\n}\n\n@Article{\t  Der-Kiureghian.Ditlevsen.2009,\n  title\t\t= {Aleatory or epistemic? Does it matter?},\n  author\t= {Der Kiureghian, Armen and Ditlevsen, Ove},\n  journal\t= {Structural {S}afety},\n  volume\t= {31},\n  number\t= {2},\n  pages\t\t= {105--112},\n  year\t\t= {2009},\n  publisher\t= {Elsevier}\n}\n\n@Article{\t  Devlin.Chang.Lee.ea.2018,\n  title\t\t= {{BERT}: {P}re-training of deep bidirectional transformers for\n\t\t  language understanding},\n  author\t= {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and\n\t\t  Toutanova, Kristina},\n  journal\t= {Ar{X}iv:1810.04805},\n  year\t\t= {2018}\n}\n\n@InProceedings{\t  Doersch.Gupta.Efros.2015,\n  title\t\t= {Unsupervised visual representation learning by context\n\t\t  prediction},\n  author\t= {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {1422--1430},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Dosovitskiy.Beyer.Kolesnikov.ea.2021,\n  title\t\t= {An image is worth 16 x 16 words: Transformers for image\n\t\t  recognition at scale},\n  author\t= {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov,\n\t\t  Alexander and Weissenborn, Dirk and Zhai, Xiaohua and\n\t\t  Unterthiner, Thomas and Dehghani, Mostafa and Minderer,\n\t\t  Matthias and Heigold, Georg and Gelly, Sylvain and et al.},\n  booktitle\t= {{I}nternational {C}onference on {L}earning {R}epresentations},\n  year\t\t= {2021}\n}\n\n@InProceedings{\t  Dosovitskiy.Beyer.Kolesnikov.ea.2021*1,\n  title\t\t= {An image is worth $16\\times 16$ words: Transformers for image\n\t\t  recognition at scale},\n  author\t= {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov,\n\t\t  Alexander and Weissenborn, Dirk and Zhai, Xiaohua and\n\t\t  Unterthiner, Thomas and Dehghani, Mostafa and Minderer,\n\t\t  Matthias and Heigold, Georg and Gelly, Sylvain and et al.},\n  booktitle\t= {{I}nternational {C}onference on {L}earning {R}epresentations},\n  year\t\t= {2021}\n}\n\n@InCollection{\t  Doucet.De-Freitas.Gordon.2001,\n  title\t\t= {An introduction to sequential {M}onte {C}arlo methods},\n  author\t= {Doucet, Arnaud and De Freitas, Nando and Gordon, Neil},\n  booktitle\t= {Sequential {M}onte {C}arlo {M}ethods in {P}ractice},\n  pages\t\t= {3--14},\n  year\t\t= {2001},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Duchi.Hazan.Singer.2011,\n  title\t\t= {Adaptive subgradient methods for online learning and\n\t\t  stochastic optimization},\n  author\t= {Duchi, John and Hazan, Elad and Singer, Yoram},\n  journal\t= {{J}ournal of {M}achine {L}earning {R}esearch},\n  volume\t= {12},\n  pages\t\t= {2121--2159},\n  year\t\t= {2011}\n}\n\n@Article{\t  Dumoulin.Visin.2016,\n  title\t\t= {A guide to convolution arithmetic for deep learning},\n  author\t= {Dumoulin, Vincent and Visin, Francesco},\n  journal\t= {Ar{X}iv:1603.07285},\n  year\t\t= {2016}\n}\n\n@Article{\t  Edelman.Ostrovsky.Schwarz.2007,\n  title\t\t= {Internet advertising and the generalized second-price\n\t\t  auction: Selling billions of dollars worth of keywords},\n  author\t= {Edelman, Benjamin and Ostrovsky, Michael and Schwarz,\n\t\t  Michael},\n  journal\t= {American {E}conomic {R}eview},\n  volume\t= {97},\n  number\t= {1},\n  pages\t\t= {242--259},\n  year\t\t= {2007}\n}\n\n@article{elman1990finding,\n  title={Finding structure in time},\n  author={Elman, Jeffrey L},\n  journal={Cognitive {S}cience},\n  volume={14},\n  number={2},\n  pages={179--211},\n  year={1990},\n  publisher={Wiley Online Library}\n}\n\n\n@Book{\t\t  Fechner.1860,\n  title\t\t= {Elemente der {P}sychophysik},\n  author\t= {Fechner, Gustav Theodor},\n  volume\t= {2},\n  year\t\t= {1860},\n  publisher\t= {Breitkopf u. {H}{\\\"a}rtel}\n}\n\n@Book{\t\t  Fernando.2004,\n  title\t\t= {GPU {G}ems: {P}rogramming {T}echniques, {T}ips, and {T}ricks for\n\t\t  {R}eal-{T}ime {G}raphics},\n  author\t= {Fernando, Randima},\n  year\t\t= {2004},\n  publisher\t= {Addison-{W}esley}\n}\n\n@Article{\t  Field.1987,\n  title\t\t= {Relations between the statistics of natural images and the\n\t\t  response properties of cortical cells},\n  author\t= {Field, David J},\n  journal\t= {{JOSA} {A}},\n  volume\t= {4},\n  number\t= {12},\n  pages\t\t= {2379--2394},\n  year\t\t= {1987},\n  publisher\t= {Optical {S}ociety of {A}merica}\n}\n\n@Book{\t\t  Fisher.1928,\n  title\t\t= {Statistical {M}ethods for {R}esearch {W}orkers.},\n  author\t= {Fisher, R A},\n  year\t\t= {1925},\n  publisher\t= {Oliver \\& {B}oyd}\n}\n\n@InProceedings{\t  Flammarion.Bach.2015,\n  title\t\t= {From averaging to acceleration, there is only a\n\t\t  step-size},\n  author\t= {Flammarion, Nicolas and Bach, Francis},\n  booktitle\t= {{C}onference on {L}earning {T}heory},\n  pages\t\t= {658--695},\n  year\t\t= {2015}\n}\n\n@Article{\t  Frankle.Carbin.2018,\n  title\t\t= {The lottery ticket hypothesis: Finding sparse, trainable\n\t\t  neural networks},\n  author\t= {Frankle, Jonathan and Carbin, Michael},\n  journal\t= {Ar{X}iv:1803.03635},\n  year\t\t= {2018}\n}\n\n@Article{\t  Frazier.2018,\n  title\t\t= {A tutorial on {B}ayesian optimization},\n  author\t= {Frazier, Peter I},\n  journal\t= {Ar{X}iv:1807.02811},\n  year\t\t= {2018}\n}\n\n@InProceedings{\t  Freund.Schapire.ea.1996,\n  title\t\t= {Experiments with a new boosting algorithm},\n  author\t= {Freund, Yoav and Schapire, Robert E},\n  booktitle\t= {Proceedings of the {I}nternational {C}onference on {M}achine {L}earning},\n  volume\t= {96},\n  pages\t\t= {148--156},\n  year\t\t= {1996},\n  organization\t= {Citeseer}\n}\n\n@Article{\t  Friedman.1997,\n  title\t\t= {On bias, variance, 0/1-loss, and the\n\t\t  curse-of-dimensionality},\n  author\t= {Friedman, Jerome H},\n  journal\t= {{D}ata {M}ining and {K}nowledge {D}iscovery},\n  volume\t= {1},\n  number\t= {1},\n  pages\t\t= {55--77},\n  year\t\t= {1997},\n  publisher\t= {Springer}\n}\n\n@InCollection{\t  Frostig.Johnson.Leary.2018,\n  title\t\t= {Compiling machine learning programs via high-level\n\t\t  tracing},\n  author\t= {Frostig, Roy and Johnson, Matthew James and Leary, Chris},\n  booktitle\t= {Proceedings of Systems for {M}achine {L}earning},\n  year\t\t= {2018}\n}\n\n@InCollection{\t  Fukushima.1982,\n  title\t\t= {Neocognitron: A self-organizing neural network model for a\n\t\t  mechanism of visual pattern recognition},\n  author\t= {Fukushima, Kunihiko},\n  booktitle\t= {Competition and {C}ooperation in {N}eural {N}ets},\n  pages\t\t= {267--285},\n  year\t\t= {1982},\n  publisher\t= {Springer}\n}\n\n@InProceedings{Gardner.Pleiss.Weinberger.Bindel.Wilson.2018,\n  title={{GPyTorch}: Blackbox matrix--matrix {G}aussian process inference with {GPU} acceleration},\n  author={Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},\n  booktitle={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={31},\n  year={2018}\n}\n\n\n@InProceedings{Garg.Balakrishnan.Kolter.Lipton.2021,\n  title={{RATT}: Leveraging unlabeled data to guarantee generalization},\n  author={Garg, Saurabh and Balakrishnan, Sivaraman and Kolter, Zico and Lipton, Zachary},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={3598--3609},\n  year={2021},\n  organization={PMLR}\n}\n\n\n@InProceedings{\t  Gatys.Ecker.Bethge.2016,\n  title\t\t= {Image style transfer using convolutional neural networks},\n  author\t= {Gatys, Leon A and Ecker, Alexander S and Bethge,\n\t\t  Matthias},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {2414--2423},\n  year\t\t= {2016}\n}\n\n@InCollection{\t  Gauss.1809,\n  title\t\t= {Theoria motus corporum coelestum},\n  author\t= {Gauss, Carl Friedrich},\n  booktitle\t= {Werke},\n  year\t\t= {1809},\n  publisher\t= {K\\\"oniglich {P}reussische {A}kademie der {W}issenschaften}\n}\n\n@Book{\t  Gibbs.1902,\n  title\t\t= {Elementary {P}rinciples of {S}tatistical {M}hanics},\n  author\t= {Gibbs, Josiah Willard},\n  year\t\t= {1902},\npublisher = {Scribner's}\n}\n\n@Article{\t  Ginibre.1965,\n  title\t\t= {Statistical ensembles of complex, quaternion, and real matrices},\n  author\t= {Ginibre, Jean},\n  journal\t= {Journal of {M}athematical {P}hysics},\n  volume\t= {6},\n  number\t= {3},\n  pages\t\t= {440--449},\n  year\t\t= {1965},\n  publisher\t= {AIP}\n}\n\n@InProceedings{\t  Girshick.2015,\n  title\t\t= {Fast {R}-{CNN}},\n  author\t= {Girshick, Ross},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {1440--1448},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Girshick.Donahue.Darrell.ea.2014,\n  title\t\t= {Rich feature hierarchies for accurate object detection and\n\t\t  semantic segmentation},\n  author\t= {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and\n\t\t  Malik, Jitendra},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {580--587},\n  year\t\t= {2014}\n}\n\n@Article{\t  Glivenko.1933,\n  author\t= {V.~I. Glivenko},\n  journal\t= {Giornale dell'{I}stituta {I}taliano degli {A}ttuari},\n  pages\t\t= 92,\n  title\t\t= {Sulla determinazione empirica delle leggi di probabilita},\n  volume\t= 4,\n  year\t\t= 1933\n}\n\n@InProceedings{\t  Glorot.Bengio.2010,\n  title\t\t= {Understanding the difficulty of training deep feedforward\n\t\t  neural networks},\n  author\t= {Glorot, Xavier and Bengio, Yoshua},\n  booktitle\t= {Proceedings of the 13th {I}nternational {C}onference on\n\t\t  {A}rtificial {I}ntelligence and {S}tatistics},\n  pages\t\t= {249--256},\n  year\t\t= {2010}\n}\n\n@Article{\t  Goh.2017,\n  author\t= {Goh, Gabriel},\n  title\t\t= {Why momentum really works},\n  journal\t= {Distill},\n  year\t\t= {2017},\n  url\t\t= {http://distill.pub/2017/momentum}\n}\n\n@Article{\t  Goldberg.Nichols.Oki.ea.1992,\n  title\t\t= {Using collaborative filtering to weave an information\n\t\t  tapestry},\n  author\t= {Goldberg, David and Nichols, David and Oki, Brian M and\n\t\t  Terry, Douglas},\n  journal\t= {Communications of the {ACM}},\n  volume\t= {35},\n  number\t= {12},\n  pages\t\t= {61--71},\n  year\t\t= {1992},\n  publisher\t= {{A}ssociation for Computing Machinery, Inc.}\n}\n\n@Book{\t\t  Golub.Van-Loan.1996,\n  title\t\t= {Matrix {C}omputations},\n  author\t= {Golub, Gene H and Van Loan, Charles F},\n  year\t\t= {1996},\n  publisher\t= {Johns {H}opkins {U}niversity {P}ress}\n}\n\n@Book{\t\t  Goodfellow.Bengio.Courville.2016,\n  title\t\t= {Deep Learning},\n  author\t= {Ian Goodfellow and Yoshua Bengio and Aaron Courville},\n  publisher\t= {MIT {P}ress},\n  note\t\t= {\\url{http://www.deeplearningbook.org}},\n  year\t\t= {2016}\n}\n\n@InProceedings{\t  Goodfellow.Pouget-Abadie.Mirza.ea.2014,\n  title\t\t= {Generative adversarial nets},\n  author\t= {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi\n\t\t  and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and\n\t\t  Courville, Aaron and Bengio, Yoshua},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {2672--2680},\n  year\t\t= {2014}\n}\n\n@Article{\t  Gotmare.Keskar.Xiong.ea.2018,\n  title\t\t= {A closer look at deep learning heuristics: Learning rate\n\t\t  restarts, warmup and distillation},\n  author\t= {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong,\n\t\t  Caiming and Socher, Richard},\n  journal\t= {Ar{X}iv:1810.13243},\n  year\t\t= {2018}\n}\n\n@Article{\t  Goyal.Bochkovskiy.Deng.ea.2021,\n  title\t\t= {Non-deep networks},\n  author\t= {Goyal, Ankit and Bochkovskiy, Alexey and Deng, Jia and\n\t\t  Koltun, Vladlen},\n  journal\t= {Ar{X}iv:2110.07641},\n  year\t\t= {2021}\n}\n\n@Article{\t  Graham.2014,\n  title\t\t= {Fractional max-pooling},\n  author\t= {Graham, Benjamin},\n  journal\t= {Ar{X}iv:1412.6071},\n  year\t\t= {2014}\n}\n\n@Article{\t  Graves.2013,\n  title\t\t= {Generating sequences with recurrent neural networks},\n  author\t= {Graves, Alex},\n  journal\t= {Ar{X}iv:1308.0850},\n  year\t\t= {2013}\n}\n\n@Article{\t  Graves.Schmidhuber.2005,\n  title\t\t= {Framewise phoneme classification with bidirectional {LSTM}\n\t\t  and other neural network architectures},\n  author\t= {Graves, Alex and Schmidhuber, J{\\\"u}rgen},\n  journal\t= {Neural Networks},\n  volume\t= {18},\n  number\t= {5-6},\n  pages\t\t= {602--610},\n  year\t\t= {2005},\n  publisher\t= {Elsevier}\n}\n\n@article{graves2008novel,\n  title={A novel connectionist system for unconstrained handwriting recognition},\n  author={Graves, Alex and Liwicki, Marcus and Fern{\\'a}ndez, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, J{\\\"u}rgen},\n  journal={IEEE {T}ransactions on {P}attern {A}nalysis and {M}achine {I}ntelligence},\n  volume={31},\n  number={5},\n  pages={855--868},\n  year={2008},\n  publisher={IEEE}\n}\n\n@InCollection{\t  Griewank.1989,\n  title\t\t= {On automatic differentiation},\n  author\t= {Griewank, Andreas},\n  booktitle\t= {Mathematical {P}rogramming: {R}ecent {D}evelopments and\n\t\t  {A}pplications},\n  pages\t\t= {83--107},\n  year\t\t= {1989},\n  publisher\t= {Kluwer}\n}\n\n@InCollection{\t  Gunawardana.Shani.2015,\n  title\t\t= {Evaluating recommender systems},\n  author\t= {Gunawardana, Asela and Shani, Guy},\n  booktitle\t= {Recommender {S}ystems {H}andbook},\n  pages\t\t= {265--308},\n  year\t\t= {2015},\n  publisher\t= {Springer}\n}\n\n@InProceedings{\t  Guo.Tang.Ye.ea.2017,\n  title\t\t= {DeepFM: a factorization-machine based neural network for\n\t\t  CTR prediction},\n  author\t= {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li,\n\t\t  Zhenguo and He, Xiuqiang},\n  booktitle\t= {Proceedings of the 26th {I}nternational {J}oint {C}onference on\n\t\t  {A}rtificial {I}ntelligence},\n  pages\t\t= {1725--1731},\n  year\t\t= {2017},\n  organization\t= {AAAI {P}ress}\n}\n\n@Article{\t  Hadjis.Zhang.Mitliagkas.ea.2016,\n  title\t\t= {Omnivore: An optimizer for multi-device deep learning on\n\t\t  {CPU}s and {GPU}s},\n  author\t= {Hadjis, Stefan and Zhang, Ce and Mitliagkas, Ioannis and\n\t\t  Iter, Dan and R{\\'e}, Christopher},\n  journal\t= {Ar{X}iv:1606.04487},\n  year\t\t= {2016}\n}\n\n@Book{\t\t  Hartley.Zisserman.2000,\n  author\t= {Hartley, Richard and Zisserman, Andrew},\n  title\t\t= {Multiple {V}iew {G}eometry in {C}omputer {V}ision},\n  year\t\t= {2000},\n  publisher\t= {Cambridge {U}niversity {P}ress}\n}\n\n@InProceedings{\t  Hazan.Rakhlin.Bartlett.2008,\n  title\t\t= {Adaptive online gradient descent},\n  author\t= {Hazan, Elad and Rakhlin, Alexander and Bartlett, Peter L},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {65--72},\n  year\t\t= {2008}\n}\n\n@InProceedings{\t  He.Chua.2017,\n  title\t\t= {Neural factorization machines for sparse predictive\n\t\t  analytics},\n  author\t= {He, Xiangnan and Chua, Tat-Seng},\n  booktitle\t= {Proceedings of the 40th {I}nternational {ACM} {SIGIR} {C}onference\n\t\t  on {R}esearch and {D}evelopment in {I}nformation {R}etrieval},\n  pages\t\t= {355--364},\n  year\t\t= {2017},\n  organization\t= {ACM}\n}\n\n@InProceedings{\t  He.Gkioxari.Dollar.ea.2017,\n  title\t\t= {Mask {R}-{CNN}},\n  author\t= {He, Kaiming and Gkioxari, Georgia and Doll{\\'a}r, Piotr\n\t\t  and Girshick, Ross},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {2961--2969},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  He.Liao.Zhang.ea.2017,\n  title\t\t= {Neural collaborative filtering},\n  author\t= {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie,\n\t\t  Liqiang and Hu, Xia and Chua, Tat-Seng},\n  booktitle\t= {Proceedings of the 26th {I}nternational {C}onference on {W}orld\n\t\t  {W}ide {W}eb},\n  pages\t\t= {173--182},\n  year\t\t= {2017},\n  organization\t= {International World Wide Web {C}onferences Steering\n\t\t  Committee}\n}\n\n@InProceedings{\t  He.Zhang.Ren.ea.2015,\n  title\t\t= {Delving deep into rectifiers: Surpassing human-level\n\t\t  performance on {I}mage{N}et classification},\n  author\t= {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,\n\t\t  Jian},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {1026--1034},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  He.Zhang.Ren.ea.2016,\n  title\t\t= {Deep residual learning for image recognition},\n  author\t= {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,\n\t\t  Jian},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {770--778},\n  year\t\t= {2016}\n}\n\n@InProceedings{\t  He.Zhang.Ren.ea.2016*1,\n  title\t\t= {Identity mappings in deep residual networks},\n  author\t= {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,\n\t\t  Jian},\n  booktitle\t= {European {C}onference on {C}omputer {V}ision},\n  pages\t\t= {630--645},\n  year\t\t= {2016},\n  organization\t= {Springer}\n}\n\n@Book{\t\t  Hebb.1949,\n  title\t\t= {The {O}rganization of {B}ehavior},\n  author\t= {Hebb, Donald Olding},\n  year\t\t= {1949},\n  publisher\t= {Wiley}\n}\n\n@Article{\t  Hendrycks.Gimpel.2016,\n  title\t\t= {Gaussian error linear units ({GELU}s)},\n  author\t= {Hendrycks, Dan and Gimpel, Kevin},\n  journal\t= {Ar{X}iv:1606.08415},\n  year\t\t= {2016}\n}\n\n@Article{\t  Henighan.Kaplan.Katz.ea.2020,\n  title\t\t= {Scaling laws for autoregressive generative modeling},\n  author\t= {Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen,\n\t\t  Mark and Hesse, Christopher and Jackson, Jacob and Jun,\n\t\t  Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray,\n\t\t  Scott and et al.},\n  journal\t= {Ar{X}iv:2010.14701},\n  year\t\t= {2020}\n}\n\n@Book{\t\t  Hennessy.Patterson.2011,\n  title\t\t= {Computer {A}rchitecture: A {Q}uantitative {A}pproach},\n  author\t= {Hennessy, John L and Patterson, David A},\n  year\t\t= {2011},\n  publisher\t= {Elsevier}\n}\n\n@InProceedings{\t  Herlocker.Konstan.Borchers.ea.1999,\n  title\t\t= {An algorithmic framework for performing collaborative\n\t\t  filtering},\n  author\t= {Herlocker, Jonathan L and Konstan, Joseph A and Borchers,\n\t\t  Al and Riedl, John},\n  booktitle\t= {22nd {A}nnual {I}nternational {ACM} {C}onference on {R}esearch\n\t\t  and {D}evelopment in {I}nformation {R}etrieval, {SIGIR} 1999},\n  pages\t\t= {230--237},\n  year\t\t= {1999},\n  organization\t= {{A}ssociation for Computing Machinery, Inc}\n}\n\n@Article{\t  Hidasi.Karatzoglou.Baltrunas.ea.2015,\n  title\t\t= {Session-based recommendations with recurrent neural\n\t\t  networks},\n  author\t= {Hidasi, Bal{\\'a}zs and Karatzoglou, Alexandros and\n\t\t  Baltrunas, Linas and Tikk, Domonkos},\n  journal\t= {Ar{X}iv:1511.06939},\n  year\t\t= {2015}\n}\n\n@InCollection{\t\t  Hochreiter.Bengio.Frasconi.ea.2001,\n  title\t\t= {Gradient flow in recurrent nets: the difficulty of\n\t\t  learning long-term dependencies},\n  author\t= {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo\n\t\t  and Schmidhuber, J{\\\"u}rgen},\n  year\t\t= {2001},\n  booktitle\t= {A {F}ield {G}uide to {D}ynamical {R}ecurrent {N}eural {N}etworks},\npublisher ={{IEEE} {P}ress}\n}\n\n@Article{\t  Hochreiter.Schmidhuber.1997,\n  title\t\t= {Long short-term memory},\n  author\t= {Hochreiter, Sepp and Schmidhuber, J{\\\"u}rgen},\n  journal\t= {Neural {C}omputation},\n  volume\t= {9},\n  number\t= {8},\n  pages\t\t= {1735--1780},\n  year\t\t= {1997},\n  publisher\t= {MIT {P}ress}\n}\n\n@InProceedings{\t  Howard.Sandler.Chu.ea.2019,\n  title\t\t= {Searching for {M}obile{N}et{V}3},\n  author\t= {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen,\n\t\t  Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun\n\t\t  and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and\n\t\t  Le, Quoc V. and Adam, Hartwig},\n  booktitle\t= {Proceedings of the {IEEE}/{CVF} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {1314--1324},\n  year\t\t= {2019}\n}\n\n@InProceedings{\t  Hoyer.Janzing.Mooij.ea.2009,\n  title\t\t= {Nonlinear causal discovery with additive noise models},\n  author\t= {Hoyer, Patrik O and Janzing, Dominik and Mooij, Joris M\n\t\t  and Peters, Jonas and Sch{\\\"o}lkopf, Bernhard},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {689--696},\n  year\t\t= {2009}\n}\n\n@InProceedings{\t  Hu.Koren.Volinsky.2008,\n  title\t\t= {Collaborative filtering for implicit feedback datasets},\n  author\t= {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},\n  booktitle\t= {2008 8th {IEEE} {I}nternational {C}onference on {D}ata {M}ining},\n  pages\t\t= {263--272},\n  year\t\t= {2008},\n  organization\t= {IEEE}\n}\n\n@article{10.1145/3544903.3544906,\n  author = {Hu, Zhiqiang and Lee, Roy Ka-Wei and Aggarwal, Charu C. and Zhang, Aston},\n  title = {Text style transfer: A review and experimental evaluation},\n  year = {2022},\n  journal = {{SIGKDD} {E}xplor. {N}ewsl.},\n  volume = {24},\n  number = {1},\n  url = {https://doi.org/10.1145/3544903.3544906}\n}\n\n@article{parzen1957consistent,\n  title={On consistent estimates of the spectrum of a stationary time series},\n  author={Parzen, Emanuel},\n  journal={{A}nnals of {M}athematical {S}tatistics},\n  volume = {28},\npages={329--348},\n  year={1957},\n  publisher={JSTOR}\n}\n\n@article{mack1982weak,\n  title={Weak and strong uniform consistency of kernel regression estimates},\n  author={Mack, Yue-{P}ok and Silverman, Bernard W},\n  journal={Zeitschrift f{\\\"u}r {W}ahrscheinlichkeitstheorie und verwandte {G}ebiete},\n  volume={61},\n  number={3},\n  pages={405--415},\n  year={1982},\n  publisher={Springer}\n}\n\n@book{Silverman86,\n  Author =\t {B.~W. Silverman},\n  Publisher =\t {Chapman and {H}all},\n  Title =\t {Density {E}stimation for {S}tatistical and {D}ata\n                  {A}nalysis},\n  Year =\t 1986\n}\n\n@article{norelli2022asif,\n  title={{ASIF}: Coupled data turns unimodal models to multimodal without training},\n  author={Norelli, Antonio and Fumero, Marco and Maiorca, Valentino and Moschella, Luca and Rodol{\\`a}, Emanuele and Locatello, Francesco},\n  journal={Ar{X}iv:2210.01738},\n  year={2022}\n}\n\n@InProceedings{\t  Hu.Shen.Sun.2018,\n  title\t\t= {Squeeze-and-excitation networks},\n  author\t= {Hu, Jie and Shen, Li and Sun, Gang},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {7132--7141},\n  year\t\t= {2018}\n}\n\n@InProceedings{\t  Huang.Liu.Van-Der-Maaten.ea.2017,\n  title\t\t= {Densely connected convolutional networks},\n  author\t= {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and\n\t\t  Weinberger, Kilian Q},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {4700--4708},\n  year\t\t= {2017}\n}\n\n@Article{\t  Huang.Xu.Yu.2015,\n  title\t\t= {Bidirectional {LSTM}--{CRF} models for sequence tagging},\n  author\t= {Huang, Zhiheng and Xu, Wei and Yu, Kai},\n  journal\t= {Ar{X}iv:1508.01991},\n  year\t\t= {2015}\n}\n\n@Article{\t  Hubel.Wiesel.1959,\n  title\t\t= {Receptive fields of single neurones in the cat's striate\n\t\t  cortex},\n  author\t= {Hubel, David H and Wiesel, Torsten N},\n  journal\t= {Journal of {P}hysiology},\n  volume\t= {148},\n  number\t= {3},\n  pages\t\t= {574--591},\n  year\t\t= {1959},\n  publisher\t= {Wiley Online Library}\n}\n\n@Article{\t  Hubel.Wiesel.1962,\n  title\t\t= {Receptive fields, binocular interaction and functional\n\t\t  architecture in the cat's visual cortex},\n  author\t= {Hubel, David H and Wiesel, Torsten N},\n  journal\t= {Journal of {P}hysiology},\n  volume\t= {160},\n  number\t= {1},\n  pages\t\t= {106--154},\n  year\t\t= {1962},\n  publisher\t= {Wiley Online Library}\n}\n\n@Article{\t  Hubel.Wiesel.1968,\n  title\t\t= {Receptive fields and functional architecture of monkey\n\t\t  striate cortex},\n  author\t= {Hubel, David H and Wiesel, Torsten N},\n  journal\t= {Journal of {P}hysiology},\n  volume\t= {195},\n  number\t= {1},\n  pages\t\t= {215--243},\n  year\t\t= {1968},\n  publisher\t= {Wiley Online Library}\n}\n\n@InProceedings{\t  Ioffe.2017,\n  title\t\t= {Batch renormalization: Towards reducing minibatch\n\t\t  dependence in batch-normalized models},\n  author\t= {Ioffe, Sergey},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {1945--1953},\n  year\t\t= {2017}\n}\n\n@Article{\t  Ioffe.Szegedy.2015,\n  title\t\t= {Batch normalization: Accelerating deep network training by\n\t\t  reducing internal covariate shift},\n  author\t= {Ioffe, Sergey and Szegedy, Christian},\n  journal\t= {Ar{X}iv:1502.03167},\n  year\t\t= {2015}\n}\n\n@Article{\t  Izmailov.Podoprikhin.Garipov.ea.2018,\n  title\t\t= {Averaging weights leads to wider optima and better\n\t\t  generalization},\n  author\t= {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov,\n\t\t  Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},\n  journal\t= {Ar{X}iv:1803.05407},\n  year\t\t= {2018}\n}\n\n@InProceedings{Jacot.Grabriel.Hongler.2018,\n  title={Neural tangent kernel: Convergence and generalization in neural networks},\n  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\\'e}ment},\n  booktitle={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={31},\n  year={2018}\n}\n\n\n@Book{\t\t  Jaeger.2002,\n  title\t\t= {Tutorial on training recurrent neural networks, covering\n\t\t  {BPPT}, {RTRL}, {EKF} and the ``echo state network'' approach},\n  author\t= {Jaeger, Herbert},\n  year\t\t= {2002},\n  publisher\t= {GMD-{F}orschungszentrum {I}nformationstechnik {B}onn}\n}\n\n@Book{\t\t  James.2007,\n  title\t\t= {The {P}rinciples of {P}sychology},\n  author\t= {James, William},\n  year\t\t= {2007},\n  publisher\t= {Cosimo, {I}nc.}\n}\n\n@InProceedings{\t  Jia.Shelhamer.Donahue.ea.2014,\n  title\t\t= {Caffe: Convolutional architecture for fast feature\n\t\t  embedding},\n  author\t= {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and\n\t\t  Karayev, Sergey and Long, Jonathan and Girshick, Ross and\n\t\t  Guadarrama, Sergio and Darrell, Trevor},\n  booktitle\t= {Proceedings of the 22nd {ACM} {I}nternational {C}onference on\n\t\t  Multimedia},\n  pages\t\t= {675--678},\n  year\t\t= {2014}\n}\n\n@Article{\t  Jia.Song.He.ea.2018,\n  title\t\t= {Highly scalable deep learning training system with\n\t\t  mixed-precision: Training {I}mage{N}et in four minutes},\n  author\t= {Jia, Xianyan and Song, Shutao and He, Wei and Wang,\n\t\t  Yangzihao and Rong, Haidong and Zhou, Feihu and Xie,\n\t\t  Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei\n\t\t  and et al.},\n  journal\t= {Ar{X}iv:1807.11205},\n  year\t\t= {2018}\n}\n\n@InProceedings{\t  Jouppi.Young.Patil.ea.2017,\n  title\t\t= {In-datacenter performance analysis of a tensor processing\n\t\t  unit},\n  author\t= {Jouppi, Norman P and Young, Cliff and Patil, Nishant and\n\t\t  Patterson, David and Agrawal, Gaurav and Bajwa, Raminder\n\t\t  and Bates, Sarah and Bhatia, Suresh and Boden, Nan and\n\t\t  Borchers, Al and et al.},\n  booktitle\t= {2017 {ACM}/IEEE 44th {A}nnual {I}nternational {S}ymposium on\n\t\t  {C}omputer {A}rchitecture ({ISCA})},\n  pages\t\t= {1--12},\n  year\t\t= {2017},\n  organization\t= {IEEE}\n}\n\n@Article{\t  Kalchbrenner.Grefenstette.Blunsom.2014,\n  title\t\t= {A convolutional neural network for modelling sentences},\n  author\t= {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom,\n\t\t  Phil},\n  journal\t= {Ar{X}iv:1404.2188},\n  year\t\t= {2014}\n}\n\n@InProceedings{\t  Kalman.Kwasny.1992,\n  title\t\t= {Why tanh: choosing a sigmoidal function},\n  author\t= {Kalman, Barry L and Kwasny, Stan {C}},\n  booktitle\t= {Proceedings of the {I}nternational {J}oint {C}onference on\n\t\t  {N}eural {N}etworks (IJCNN)},\n  pages\t\t= {578--581},\n  year\t\t= {1992},\n  organization\t= {IEEE}\n}\n\n@Article{\t  Karras.Aila.Laine.ea.2017,\n  title\t\t= {Progressive growing of {GAN}s for improved quality,\n\t\t  stability, and variation},\n  author\t= {Karras, Tero and Aila, Timo and Laine, Samuli and\n\t\t  Lehtinen, Jaakko},\n  journal\t= {Ar{X}iv:1710.10196},\n  year\t\t= {2017}\n}\n\n@Article{\t  Kawaguchi.Kaelbling.Bengio.2017,\n  title\t\t= {Generalization in deep learning},\n  author\t= {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio,\n\t\t  Yoshua},\n  journal\t= {Ar{X}iv:1710.05468},\n  year\t\t= {2017}\n}\n\n@Article{\t  Kim.2014,\n  title\t\t= {Convolutional neural networks for sentence\n\t\t  classification},\n  author\t= {Kim, Yoon},\n  journal\t= {Ar{X}iv:1408.5882},\n  year\t\t= {2014}\n}\n\n@Article{\t  Kimeldorf.Wahba.1971,\n  author\t= {G.~S.~Kimeldorf and G.~Wahba},\n  journal\t= {J. {M}ath. {A}nal. {A}ppl.},\n  pages\t\t= {82--95},\n  title\t\t= {Some results on {T}chebycheffian spline functions},\n  volume\t= 33,\n  year\t\t= 1971\n}\n\n@Article{\t  Kingma.Ba.2014,\n  title\t\t= {Adam: A method for stochastic optimization},\n  author\t= {Kingma, Diederik P and Ba, Jimmy},\n  journal\t= {Ar{X}iv:1412.6980},\n  year\t\t= {2014}\n}\n\n@inproceedings{Kingma.Welling.2014,\n  author = {Kingma, Diederik P. and Welling, Max},\n  booktitle = {{I}nternational {C}onference on {L}earning {R}epresentations ({ICLR})},\n  title = {Auto-encoding variational {B}ayes},\n  year = 2014\n}\n\n\n\n@Article{\t  Kipf.Welling.2016,\n  title\t\t= {Semi-supervised classification with graph convolutional\n\t\t  networks},\n  author\t= {Kipf, Thomas N and Welling, Max},\n  journal\t= {Ar{X}iv:1609.02907},\n  year\t\t= {2016}\n}\n\n@Book{\t\t  Koller.Friedman.2009,\n  title\t\t= {Probabilistic {G}raphical {M}odels: {P}rinciples and\n\t\t  {T}echniques},\n  author\t= {Koller, Daphne and Friedman, Nir},\n  year\t\t= {2009},\n  publisher\t= {MIT {P}ress}\n}\n\n@Article{\t  Kolmogorov.1933,\n  title\t\t= {Sulla determinazione empirica di una legge di\n\t\t  distribuzione},\n  author\t= {Kolmogorov, Andrey},\n  journal\t= {Inst. {I}tal. {A}ttuari, {G}iorn.},\n  volume\t= {4},\n  pages\t\t= {83--91},\n  year\t\t= {1933}\n}\n\n@Article{\t  Kolter.2008,\n  title\t\t= {Linear algebra review and reference},\n  author\t= {Kolter, Zico},\n  journal\t= {Available online:\n\t\t  http://cs229.stanford.edu/section/cs229-linalg.pdf},\n  year\t\t= {2008}\n}\n\n@InProceedings{\t  Koren.2009,\n  title\t\t= {Collaborative filtering with temporal dynamics},\n  author\t= {Koren, Yehuda},\n  booktitle\t= {Proceedings of the 15th {ACM} {SIGKDD} {I}nternational\n\t\t  {C}onference on {K}nowledge {D}iscovery and {D}ata {M}ining},\n  pages\t\t= {447--456},\n  year\t\t= {2009},\n  organization\t= {ACM}\n}\n\n@Article{\t  Koren.Bell.Volinsky.2009,\n  title\t\t= {Matrix factorization techniques for recommender systems},\n  author\t= {Koren, Yehuda and Bell, Robert and Volinsky, Chris},\n  journal\t= {Computer},\n  number\t= {8},\n  pages\t\t= {30--37},\n  year\t\t= {2009},\n  publisher\t= {IEEE}\n}\n\n@InProceedings{\t  Krizhevsky.Sutskever.Hinton.2012,\n  title\t\t= {Image{N}et classification with deep convolutional neural\n\t\t  networks},\n  author\t= {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey\n\t\t  E},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {1097--1105},\n  year\t\t= {2012}\n}\n\n@InProceedings{\t  Krogh.Hertz.1992,\n  title\t\t= {A simple weight decay can improve generalization},\n  author\t= {Krogh, Anders and Hertz, John A},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {950--957},\n  year\t\t= {1992}\n}\n\n@Article{\t  Kung.1988,\n  title\t\t= {{VLSI} {A}rray {P}rocessors},\n  author\t= {Kung, Sun Yuan},\n  journal\t= {Prentice {H}all},\n  year\t\t= {1988}\n}\n\n@Article{\t  Kuzovkin.Vicente.Petton.ea.2018,\n  title\t\t= {Activations of deep convolutional neural networks are\n\t\t  aligned with gamma band activity of human visual cortex},\n  author\t= {Kuzovkin, Ilya and Vicente, Raul and Petton, Mathilde and\n\t\t  Lachaux, Jean-Philippe and Baciu, Monica and Kahane,\n\t\t  Philippe and Rheims, Sylvain and Vidal, Juan R and Aru,\n\t\t  Jaan},\n  journal\t= {Communications {B}iology},\n  volume\t= {1},\n  number\t= {1},\n  pages\t\t= {1--12},\n  year\t\t= {2018},\n  publisher\t= {Nature Publishing Group}\n}\n\n@InCollection{\t  LeCun.Bengio.ea.1995,\n  title\t\t= {Convolutional networks for images, speech, and time\n\t\t  series},\n  author\t= {LeCun, Yann and Bengio, Yoshua and et al.},\n  booktitle\t= {The {H}andbook of {B}rain {T}heory and {N}eural {N}etworks},\n  pages\t\t= {3361},\n  year\t\t= {1995},\n  publisher = {MIT {P}ress}\n}\n\n@Article{\t  LeCun.Boser.Denker.ea.1989,\n  title\t\t= {Backpropagation applied to handwritten zip code\n\t\t  recognition},\n  author\t= {LeCun, Yann and Boser, Bernhard and Denker, John S and\n\t\t  Henderson, Donnie and Howard, Richard E and Hubbard, Wayne\n\t\t  and Jackel, Lawrence D},\n  journal\t= {Neural {C}omputation},\n  volume\t= {1},\n  number\t= {4},\n  pages\t\t= {541--551},\n  year\t\t= {1989},\n  publisher\t= {MIT Press}\n}\n\n@Article{\t  LeCun.Bottou.Bengio.ea.1998,\n  title\t\t= {Gradient-based learning applied to document recognition},\n  author\t= {LeCun, Yann and Bottou, L{\\'e}on and Bengio, Yoshua and\n\t\t  Haffner, Patrick},\n  journal\t= {Proceedings of the {IEEE}},\n  volume\t= {86},\n  number\t= {11},\n  pages\t\t= {2278--2324},\n  year\t\t= {1998},\n  publisher\t= {Taipei, Taiwan}\n}\n\n@InCollection{\t  LeCun.Bottou.Orr.ea.1998,\n  title\t\t= {Efficient backprop},\n  author\t= {LeCun, Yann and Bottou, Leon and Orr, G and Muller,\n\t\t  Klaus-Robert},\n  booktitle\t= {Neural {N}etworks: {T}ricks of the {T}rade},\n  year\t\t= {1998},\npublisher = {Springer}\n}\n\n@InProceedings{\t  LeCun.Jackel.Bottou.ea.1995,\n  title\t\t= {Comparison of learning algorithms for handwritten digit\n\t\t  recognition},\n  author\t= {LeCun, Yann and Jackel, LD and Bottou, Leon and Brunot, A\n\t\t  and Cortes, Corinna and Denker, John and Drucker, Harris\n\t\t  and Guyon, Isabelle and Muller, UA and Sackinger, Eduard and\n\t\t  et al.},\n  booktitle\t= {{I}nternational {C}onference on {A}rtificial {N}eural {N}etworks},\n  pages\t\t= {53--60},\n  year\t\t= {1995},\n}\n\n@Book{\t\t  Legendre.1805,\n  title\t\t= {M{\\'e}moire sur les {O}p{\\'e}rations {T}rigonom{\\'e}triques:\n\t\t  dont les {R}{\\'e}sultats {D}{\\'e}pendent de la {F}igure de la {T}erre},\n  author\t= {Legendre, Adrien Marie},\n  year\t\t= {1805},\n  publisher\t= {F. {D}idot}\n}\n\n@PhDThesis{\t  Li.2017,\n  title\t\t= {Scaling {D}istributed {M}achine {L}earning with {S}ystem and\n\t\t  {A}lgorithm {C}o-design},\n  author\t= {Li, Mu},\n  year\t\t= {2017},\n  school\t= {Ph{D} {T}hesis, {CMU}}\n}\n\n@InProceedings{\t  Li.Andersen.Park.ea.2014,\n  title\t\t= {Scaling distributed machine learning with the parameter\n\t\t  server},\n  author\t= {Li, Mu and Andersen, David G and Park, Jun Woo and Smola,\n\t\t  Alexander J and Ahmed, Amr and Josifovski, Vanja and Long,\n\t\t  James and Shekita, Eugene J and Su, Bor-Yiing},\n  booktitle\t= {11th {S}ymposium on {O}perating {S}ystems {D}esign\n\t\t  and {I}mplementation ({OSDI} 14)},\n  pages\t\t= {583--598},\n  year\t\t= {2014}\n}\n\n@InProceedings{\t  Li.Zhang.Chen.ea.2014,\n  title\t\t= {Efficient mini-batch training for stochastic\n\t\t  optimization},\n  author\t= {Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola,\n\t\t  Alexander J},\n  booktitle\t= {Proceedings of the 20th {ACM} {SIGKDD} {I}nternational\n\t\t  {C}onference on {K}nowledge {D}iscovery and {D}ata {M}ining},\n  pages\t\t= {661--670},\n  year\t\t= {2014}\n}\n\n@Article{\t  Lin.Chen.Yan.2013,\n  title\t\t= {Network in network},\n  author\t= {Lin, Min and Chen, Qiang and Yan, Shuicheng},\n  journal\t= {Ar{X}iv:1312.4400},\n  year\t\t= {2013}\n}\n\n@Article{\t  Lin.Feng.Santos.ea.2017,\n  title\t\t= {A structured self-attentive sentence embedding},\n  author\t= {Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira\n\t\t  dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio,\n\t\t  Yoshua},\n  journal\t= {Ar{X}iv:1703.03130},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  Lin.Goyal.Girshick.ea.2017,\n  title\t\t= {Focal loss for dense object detection},\n  author\t= {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He,\n\t\t  Kaiming and Doll{\\'a}r, Piotr},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {2980--2988},\n  year\t\t= {2017}\n}\n\n@Article{\t  Lin.Lv.Zhu.ea.2010,\n  title\t\t= {Image{N}et classification: fast descriptor coding and\n\t\t  large-scale {S}{V}{M} training},\n  author\t= {Lin, Yuanqing and Lv, F and Zhu, S and Yang, M and Cour, T\n\t\t  and Yu, K and Cao, L and Li, Z and Tsai, MH and Zhou, X and\n\t\t  others},\n  journal\t= {Large Scale Visual Recognition Challenge},\n  year\t\t= {2010}\n}\n\n@InProceedings{Lipton.Kale.2016,\n  title={Learning to diagnose with {LSTM} recurrent neural networks},\n  author={Lipton, Zachary C and Kale, David C and Elkan, Charles and Wetzel, Randall},\n  booktitle={{I}nternational {C}onference on {L}earning {R}epresentations ({ICLR})},\n  year={2016}\n}\n\n\n@Article{\t  Lipton.Steinhardt.2018,\n  title\t\t= {Troubling trends in machine learning scholarship},\n  author\t= {Lipton, Zachary C and Steinhardt, Jacob},\n  journal\t= {Communications of the {ACM}},\n volume ={17},\nissue = {1},\npages ={45--77},\n year\t\t= {2018}\n}\n\n@article{     Lipton.Berkowitz.Elkan.2015,\n  title\t\t= {A critical review of recurrent neural networks for sequence learning},\n  author\t= {Lipton, Zachary C and Berkowitz, John and Elkan, Charles},\n  journal\t= {Ar{X}iv:1506.00019},\n  year\t\t= {2015}\n}\n\n\n@InProceedings{\t  Liu.Anguelov.Erhan.ea.2016,\n  title\t\t= {{SSD}: Single shot multibox detector},\n  author\t= {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and\n\t\t  Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and\n\t\t  Berg, Alexander {C}},\n  booktitle\t= {European {C}onference on {C}omputer {V}ision},\n  pages\t\t= {21--37},\n  year\t\t= {2016},\n  organization\t= {Springer}\n}\n\n@Article{\t  Liu.Nocedal.1989,\n  title\t\t= {On the limited memory {BFGS} method for large scale\n\t\t  optimization},\n  author\t= {Liu, Dong C and Nocedal, Jorge},\n  journal\t= {Mathematical {P}rogramming},\n  volume\t= {45},\n  number\t= {1},\n  pages\t\t= {503--528},\n  year\t\t= {1989},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Liu.Ott.Goyal.ea.2019,\n  title\t\t= {Ro{BERT}a: A robustly optimized {BERT} pretraining approach},\n  author\t= {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei\n\t\t  and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis,\n\t\t  Mike and Zettlemoyer, Luke and Stoyanov, Veselin},\n  journal\t= {Ar{X}iv:1907.11692},\n  year\t\t= {2019}\n}\n\n@InProceedings{\t  Long.Shelhamer.Darrell.2015,\n  title\t\t= {Fully convolutional networks for semantic segmentation},\n  author\t= {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {3431--3440},\n  year\t\t= {2015}\n}\n\n@Article{\t  Loshchilov.Hutter.2016,\n  title\t\t= {{SGDR}: Stochastic gradient descent with warm restarts},\n  author\t= {Loshchilov, Ilya and Hutter, Frank},\n  journal\t= {Ar{X}iv:1608.03983},\n  year\t\t= {2016}\n}\n\n@Article{\t  Lowe.2004,\n  title\t\t= {Distinctive image features from scale-invariant\n\t\t  keypoints},\n  author\t= {Lowe, David G},\n  journal\t= {International {J}ournal of {C}omputer {V}ision},\n  volume\t= {60},\n  number\t= {2},\n  pages\t\t= {91--110},\n  year\t\t= {2004},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Luo.Wang.Shao.ea.2018,\n  title\t\t= {Towards understanding regularization in batch\n\t\t  normalization},\n  author\t= {Luo, Ping and Wang, Xinjiang and Shao, Wenqi and Peng,\n\t\t  Zhanglin},\n  journal\t= {Ar{X}iv:1809.00846},\n  year\t\t= {2018}\n}\n\n@InProceedings{\t  Maas.Daly.Pham.ea.2011,\n  title\t\t= {Learning word vectors for sentiment analysis},\n  author\t= {Maas, Andrew L and Daly, Raymond E and Pham, Peter T and\n\t\t  Huang, Dan and Ng, Andrew Y and Potts, Christopher},\n  booktitle\t= {Proceedings of the 49th {A}nnual {M}eeting of the {A}ssociation\n\t\t  for {C}omputational {L}inguistics: {H}uman {L}anguage\n\t\t  {T}echnologies, {V}olume 1},\n  pages\t\t= {142--150},\n  year\t\t= {2011},\n  organization\t= {{A}ssociation for {C}omputational {L}inguistics}\n}\n\n@Article{\t  Mangasarian.1965,\n  author\t= {O.~L.~Mangasarian},\n  journal\t= {Oper. {R}es.},\n  pages\t\t= {444-452},\n  title\t\t= {Linear and nonlinear separation of patterns by linear\n\t\t  programming},\n  volume\t= 13,\n  year\t\t= 1965\n}\n\n@Article{\t  Mangram.2013,\n  title\t\t= {A simplified perspective of the {M}arkowitz portfolio\n\t\t  theory},\n  author\t= {Mangram, Myles E},\n  journal\t= {Global {J}ournal of {B}usiness {R}esearch},\n  volume\t= {7},\n  number\t= {1},\n  pages\t\t= {59--70},\n  year\t\t= {2013}\n}\n\n@InProceedings{\t  McCann.Bradbury.Xiong.ea.2017,\n  title\t\t= {Learned in translation: {C}ontextualized word vectors},\n  author\t= {McCann, Bryan and Bradbury, James and Xiong, Caiming and\n\t\t  Socher, Richard},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {6294--6305},\n  year\t\t= {2017}\n}\n\n@Article{\t  McCulloch.Pitts.1943,\n  title\t\t= {A logical calculus of the ideas immanent in nervous\n\t\t  activity},\n  author\t= {McCulloch, Warren S and Pitts, Walter},\n  journal\t= {Bulletin of {Mathematical} {Biophysics}},\n  volume\t= {5},\n  number\t= {4},\n  pages\t\t= {115--133},\n  year\t\t= {1943},\n  publisher\t= {Springer}\n}\n\n@InProceedings{\t  McMahan.Holt.Sculley.ea.2013,\n  title\t\t= {Ad click prediction: a view from the trenches},\n  author\t= {McMahan, H Brendan and Holt, Gary and Sculley, David and\n\t\t  Young, Michael and Ebner, Dietmar and Grady, Julian and\n\t\t  Nie, Lan and Phillips, Todd and Davydov, Eugene and\n\t\t  Golovin, Daniel and et al.},\n  booktitle\t= {Proceedings of the 19th {ACM} SIGKDD {I}nternational\n\t\t  {C}onference on {K}nowledge {D}iscovery and {D}ata {M}ining},\n  pages\t\t= {1222--1230},\n  year\t\t= {2013},\n  organization\t= {ACM}\n}\n\n@Article{\t  Mead.1980,\n  title\t\t= {Introduction to {VLSI} systems},\n  author\t= {Mead, Carver},\n  journal\t= {IEE {P}roceedings I-{S}olid-{S}tate and {E}lectron {D}evices},\n  volume\t= {128},\n  number\t= {1},\n  pages\t\t= {18},\n  year\t\t= {1980},\n  publisher\t= {{IET}}\n}\n\n@Article{\t  Merity.Xiong.Bradbury.ea.2016,\n  title\t\t= {Pointer sentinel mixture models},\n  author\t= {Merity, Stephen and Xiong, Caiming and Bradbury, James and\n\t\t  Socher, Richard},\n  journal\t= {Ar{X}iv:1609.07843},\n  year\t\t= {2016}\n}\n\n@InCollection{\t  Micchelli.1986,\n  author\t= {C.~A.~Micchelli},\n  booktitle\t= {Proceedings of {S}ymposia in {A}pplied {M}athematics},\n  pages\t\t= {81--102},\n  title\t\t= {Algebraic aspects of interpolation},\n  year\t\t= 1986\n}\n\n@Article{\t  Mikolov.Chen.Corrado.ea.2013,\n  title\t\t= {Efficient estimation of word representations in vector\n\t\t  space},\n  author\t= {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean,\n\t\t  Jeffrey},\n  journal\t= {Ar{X}iv:1301.3781},\n  year\t\t= {2013}\n}\n\n@InProceedings{\t  Mikolov.Sutskever.Chen.ea.2013,\n  title\t\t= {Distributed representations of words and phrases and their\n\t\t  compositionality},\n  author\t= {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and\n\t\t  Corrado, Greg S and Dean, Jeff},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {3111--3119},\n  year\t\t= {2013}\n}\n\n@Article{\t  Miller.1995,\n  title\t\t= {Word{N}et: a lexical database for {E}nglish},\n  author\t= {Miller, George A},\n  journal\t= {Communications of the {ACM}},\n  volume\t= {38},\n  number\t= {11},\n  pages\t\t= {39--41},\n  year\t\t= {1995},\n  publisher\t= {ACM}\n}\n\n@InProceedings{\t  Mirhoseini.Pham.Le.ea.2017,\n  title\t\t= {Device placement optimization with reinforcement\n\t\t  learning},\n  author\t= {Mirhoseini, Azalia and Pham, Hieu and Le, Quoc V and\n\t\t  Steiner, Benoit and Larsen, Rasmus and Zhou, Yuefeng and\n\t\t  Kumar, Naveen and Norouzi, Mohammad and Bengio, Samy and\n\t\t  Dean, Jeff},\n  booktitle\t= {Proceedings of the 34th {I}nternational {C}onference on\n\t\t  {M}achine {L}earning},\n  pages\t\t= {2430--2439},\n  year\t\t= {2017},\n}\n\n@InProceedings{\t  Mnih.Heess.Graves.ea.2014,\n  title\t\t= {Recurrent models of visual attention},\n  author\t= {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and\n\t\t  others},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {2204--2212},\n  year\t\t= {2014}\n}\n\n@InProceedings{\t  Moon.Smola.Chang.ea.2010,\n  title\t\t= {Intervalrank: isotonic regression with listwise and\n\t\t  pairwise constraints},\n  author\t= {Moon, Taesup and Smola, Alex and Chang, Yi and Zheng,\n\t\t  Zhaohui},\n  booktitle\t= {Proceedings of the 3rd {ACM} {I}nternational {C}onference on\n\t\t  {W}eb {S}earch and {D}ata {M}ining},\n  pages\t\t= {151--160},\n  year\t\t= {2010}\n}\n\n@Article{\t  Morey.Hoekstra.Rouder.ea.2016,\n  title\t\t= {The fallacy of placing confidence in confidence\n\t\t  intervals},\n  author\t= {Morey, Richard D and Hoekstra, Rink and Rouder, Jeffrey N\n\t\t  and Lee, Michael D and Wagenmakers, Eric-Jan},\n  journal\t= {Psychonomic {B}ulletin \\& {R}eview},\n  volume\t= {23},\n  number\t= {1},\n  pages\t\t= {103--123},\n  year\t\t= {2016},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Murata.Yoshizawa.Amari.1994,\n  title\t\t= {Network information criterion -- determining the number of\n\t\t  hidden units for an artificial neural network model},\n  author\t= {Murata, Noboru and Yoshizawa, Shuji and Amari, Shun-ichi},\n  journal\t= {IEEE {T}ransactions on {N}eural {N}etworks},\n  volume\t= {5},\n  number\t= {6},\n  pages\t\t= {865--872},\n  year\t\t= {1994},\n  publisher\t= {IEEE}\n}\n\n@Article{\t  Nadaraya.1964,\n  title\t\t= {On estimating regression},\n  author\t= {Nadaraya, Elizbar A},\n  journal\t= {Theory of {P}robability \\& its {A}pplications},\n  volume\t= {9},\n  number\t= {1},\n  pages\t\t= {141--142},\n  year\t\t= {1964},\n  publisher\t= {SIAM}\n}\n\n@Article{\t  Nagarajan.Kolter.2019,\n  title\t\t= {Uniform convergence may be unable to explain\n\t\t  generalization in deep learning},\n  author\t= {Nagarajan, Vaishnavh and Kolter, J Zico},\n  journal\t= {Ar{X}iv:1902.04742},\n  year\t\t= {2019}\n}\n\n@InProceedings{\t  Nair.Hinton.2010,\n  title\t\t= {Rectified linear units improve restricted {B}oltzmann\n\t\t  machines},\n  author\t= {Nair, Vinod and Hinton, Geoffrey E},\n  booktitle\t= {ICML},\n  year\t\t= {2010}\n}\n\n@Article{\t  Naor.Reingold.1999,\n  title\t\t= {On the construction of pseudorandom permutations:\n\t\t  {L}uby--{R}ackoff revisited},\n  author\t= {Naor, Moni and Reingold, Omer},\n  journal\t= {Journal of {C}ryptology},\n  volume\t= {12},\n  number\t= {1},\n  pages\t\t= {29--66},\n  year\t\t= {1999},\n  publisher\t= {Springer}\n}\n\n@Book{\t\t  Nesterov.2018,\n  title\t\t= {Lectures on {C}onvex {O}ptimization},\n  author\t= {Nesterov, Yu},\n  year\t\t= {2018},\n  publisher\t= {Springer}\n}\n\n@Article{\t\t  Nesterov.Vial.2000,\n  title\t\t= {Confidence level solutions for stochastic programming},\njournal\t= \t{Automatica},\nvolume = {44},\nnumber = {6},\npages = {1559--1568},\n  author\t= {Nesterov, Yu and Vial, J-Ph},\n  year\t\t= {2000}\n}\n\n@Article{\t  Neyman.1937,\n  title\t\t= {Outline of a theory of statistical estimation based on the\n\t\t  classical theory of probability},\n  author\t= {Neyman, Jerzy},\n  journal\t= {Philosophical {T}ransactions of the {R}oyal {S}ociety of {L}ondon.\n\t\t  {S}eries {A}, {M}athematical and {P}hysical {S}ciences},\n  volume\t= {236},\n  number\t= {767},\n  pages\t\t= {333--380},\n  year\t\t= {1937},\n  publisher\t= {The Royal Society London}\n}\n\n@InProceedings{\t  Papineni.Roukos.Ward.ea.2002,\n  title\t\t= {{BLEU}: a method for automatic evaluation of machine\n\t\t  translation},\n  author\t= {Papineni, Kishore and Roukos, Salim and Ward, Todd and\n\t\t  Zhu, Wei-Jing},\n  booktitle\t= {Proceedings of the 40th {A}nnual {M}eeting of the {A}ssociation\n\t\t  for {C}omputational {L}inguistics},\n  pages\t\t= {311--318},\n  year\t\t= {2002}\n}\n\n@Article{\t  Parikh.Tackstrom.Das.ea.2016,\n  title\t\t= {A decomposable attention model for natural language\n\t\t  inference},\n  author\t= {Parikh, Ankur P and T{\\\"a}ckstr{\\\"o}m, Oscar and Das,\n\t\t  Dipanjan and Uszkoreit, Jakob},\n  journal\t= {Ar{X}iv:1606.01933},\n  year\t\t= {2016}\n}\n\n@InProceedings{\t  Park.Liu.Wang.ea.2019,\n  title\t\t= {Semantic image synthesis with spatially-adaptive\n\t\t  normalization},\n  author\t= {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and\n\t\t  Zhu, Jun-Yan},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {2337--2346},\n  year\t\t= {2019}\n}\n\n@Article{\t  Paszke.Gross.Massa.ea.2019,\n  title\t\t= {Py{T}orch: An imperative style, high-performance deep\n\t\t  learning library},\n  author\t= {Paszke, Adam and Gross, Sam and Massa, Francisco and\n\t\t  Lerer, Adam and Bradbury, James and Chanan, Gregory and\n\t\t  Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and\n\t\t  Antiga, Luca and et al.},\n  journal\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume\t= {32},\n  pages\t\t= {8026--8037},\n  year\t\t= {2019}\n}\n\n@Article{\t  Paulus.Xiong.Socher.2017,\n  title\t\t= {A deep reinforced model for abstractive summarization},\n  author\t= {Paulus, Romain and Xiong, Caiming and Socher, Richard},\n  journal\t= {Ar{X}iv:1705.04304},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  Pennington.Schoenholz.Ganguli.2017,\n  title\t\t= {Resurrecting the sigmoid in deep learning through\n\t\t  dynamical isometry: theory and practice},\n  author\t= {Pennington, Jeffrey and Schoenholz, Samuel and Ganguli,\n\t\t  Surya},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {4785--4795},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  Pennington.Socher.Manning.2014,\n  title\t\t= {Glo{V}e: Global vectors for word representation},\n  author\t= {Pennington, Jeffrey and Socher, Richard and Manning,\n\t\t  Christopher},\n  booktitle\t= {Proceedings of the 2014 {C}onference on {E}mpirical {M}ethods in\n\t\t  {N}atural {L}anguage {P}rocessing ({EMNLP})},\n  pages\t\t= {1532--1543},\n  year\t\t= {2014}\n}\n\n@InProceedings{\t  Peters.Ammar.Bhagavatula.ea.2017,\n  title\t\t= {Semi-supervised sequence tagging with bidirectional\n\t\t  language models},\n  author\t= {Peters, Matthew and Ammar, Waleed and Bhagavatula, Chandra\n\t\t  and Power, Russell},\n  booktitle\t= {Proceedings of the 55th {A}nnual {M}eeting of the {A}ssociation\n\t\t  for {C}omputational {L}inguistics, {V}olume 1},\n  pages\t\t= {1756--1765},\n  year\t\t= {2017}\n}\n\n@Book{\t\t  Peters.Janzing.Scholkopf.2017,\n  title\t\t= {Elements of {C}ausal {I}nference: {F}oundations and {L}earning\n\t\t  {A}lgorithms},\n  author\t= {Peters, Jonas and Janzing, Dominik and Sch{\\\"o}lkopf,\n\t\t  Bernhard},\n  year\t\t= {2017},\n  publisher\t= {MIT {P}ress}\n}\n\n@InProceedings{\t  Peters.Neumann.Iyyer.ea.2018,\n  title\t\t= {Deep contextualized word representations},\n  author\t= {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and\n\t\t  Gardner, Matt and Clark, Christopher and Lee, Kenton and\n\t\t  Zettlemoyer, Luke},\n  booktitle\t= {Proceedings of the 2018 {C}onference of the {N}orth {A}merican\n\t\t  {C}hapter of the {A}ssociation for {C}omputational {L}inguistics:\n\t\t  {H}uman {L}anguage {T}echnologies, {V}olume 1},\n  pages\t\t= {2227--2237},\n  year\t\t= {2018}\n}\n\n@Book{\t  Petersen.Pedersen.ea.2008,\n  title\t\t= {The Matrix Cookbook},\n  author\t= {Petersen, Kaare Brandt and Pedersen, Michael Syskind},\n  publisher\t= {Technical University of Denmark},\n  year\t\t= {2008}\n}\n\n@Article{\t  Polyak.1964,\n  title\t\t= {Some methods of speeding up the convergence of iteration\n\t\t  methods},\n  author\t= {Polyak, Boris T},\n  journal\t= {{USSR} {C}omputational {M}athematics and {M}athematical {P}hysics},\n  volume\t= {4},\n  number\t= {5},\n  pages\t\t= {1--17},\n  year\t\t= {1964},\n  publisher\t= {Elsevier}\n}\n\n@InCollection{\t  Prechelt.1998,\n  title\t\t= {Early stopping -- but when?},\n  author\t= {Prechelt, Lutz},\n  booktitle\t= {{N}eural {N}etworks: {T}ricks of the {T}rade},\n  pages\t\t= {55--69},\n  year\t\t= {1998},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Quadrana.Cremonesi.Jannach.2018,\n  title\t\t= {Sequence-aware recommender systems},\n  author\t= {Quadrana, Massimo and Cremonesi, Paolo and Jannach,\n\t\t  Dietmar},\n  journal\t= {{ACM} {C}omputing {S}urveys},\n  volume\t= {51},\n  number\t= {4},\n  pages\t\t= {66},\n  year\t\t= {2018},\n  publisher\t= {ACM}\n}\n\n@book{\t\t  quinlan2014c4,\n  title\t\t= {C4.5: {P}rograms for {M}achine {L}earning},\n  author\t= {Quinlan, J Ross},\n  year\t\t= {1993},\n  publisher\t= {Elsevier}\n}\n\n@Article{\t  Radford.Metz.Chintala.2015,\n  title\t\t= {Unsupervised representation learning with deep\n\t\t  convolutional generative adversarial networks},\n  author\t= {Radford, Alec and Metz, Luke and Chintala, Soumith},\n  journal\t= {Ar{X}iv:1511.06434},\n  year\t\t= {2015}\n}\n\n@Article{\t  Radford.Narasimhan.Salimans.ea.2018,\n  title\t\t= {Improving language understanding by generative\n\t\t  pre-training},\n  author\t= {Radford, Alec and Narasimhan, Karthik and Salimans, Tim\n\t\t  and Sutskever, Ilya},\n  journal\t= {Open{AI}},\n  year\t\t= {2018}\n}\n\n@Article{\t  Radford.Wu.Child.ea.2019,\n  title\t\t= {Language models are unsupervised multitask learners},\n  author\t= {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan,\n\t\t  David and Amodei, Dario and Sutskever, Ilya},\n  journal\t= {Open{AI} {B}log},\n  volume\t= {1},\n  number\t= {8},\n  pages\t\t= {9},\n  year\t\t= {2019}\n}\n\n@Article{\t  Radhakrishna-Rao.1945,\n  title\t\t= {Information and accuracy attainable in the estimation of\n\t\t  statistical parameters},\n  author\t= {Radhakrishna Rao, {C}},\n  journal\t= {Bulletin of the {C}alcutta {M}athematical {S}ociety},\n  volume\t= {37},\n  number\t= {3},\n  pages\t\t= {81--91},\n  year\t\t= {1945},\n  publisher\t= {Calcutta Mathematical Society}\n}\n\n@InProceedings{\t  Radosavovic.Kosaraju.Girshick.ea.2020,\n  title\t\t= {Designing network design spaces},\n  author\t= {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick,\n\t\t  Ross and He, Kaiming and Doll{\\'a}r, Piotr},\n  booktitle\t= {Proceedings of the {IEEE}/{CVF} {C}onference on {C}omputer {V}ision\n\t\t  and {P}attern {R}ecognition},\n  pages\t\t= {10428--10436},\n  year\t\t= {2020}\n}\n\n@Article{\t  Rajpurkar.Zhang.Lopyrev.ea.2016,\n  title\t\t= {{S}{Q}u{A}{D}: 100,000+ questions for machine comprehension of\n\t\t  text},\n  author\t= {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin\n\t\t  and Liang, Percy},\n  journal\t= {Ar{X}iv:1606.05250},\n  year\t\t= {2016}\n}\n\n@Article{\t  Ramachandran.Zoph.Le.2017,\n  title\t\t= {Searching for activation functions},\n  author\t= {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},\n  journal\t= {Ar{X}iv:1710.05941},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  Ranzato.Boureau.Chopra.ea.2007,\n  title\t\t= {A unified energy-based framework for unsupervised\n\t\t  learning},\n  author\t= {Ranzato, Marc-Aurelio and Boureau, Y-Lan and Chopra,\n\t\t  Sumit and LeCun, Yann},\n  booktitle\t= {Artificial {I}ntelligence and {S}tatistics},\n  pages\t\t= {371--379},\n  year\t\t= {2007},\n  organization\t= {PMLR}\n}\n\n@Article{\t  Reddi.Kale.Kumar.2019,\n  title\t\t= {On the convergence of {A}dam and beyond},\n  author\t= {Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},\n  journal\t= {Ar{X}iv:1904.09237},\n  year\t\t= {2019}\n}\n\n@InProceedings{\t  Redmon.Divvala.Girshick.ea.2016,\n  title\t\t= {You only look once: Unified, real-time object detection},\n  author\t= {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and\n\t\t  Farhadi, Ali},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {779--788},\n  year\t\t= {2016}\n}\n\n@Article{\t  Redmon.Farhadi.2018,\n  title\t\t= {Y{OLO}v3: An incremental improvement},\n  author\t= {Redmon, Joseph and Farhadi, Ali},\n  journal\t= {Ar{X}iv:1804.02767},\n  year\t\t= {2018}\n}\n\n@Article{\t  Reed.De-Freitas.2015,\n  title\t\t= {Neural programmer-interpreters},\n  author\t= {Reed, Scott and De Freitas, Nando},\n  journal\t= {Ar{X}iv:1511.06279},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Ren.He.Girshick.ea.2015,\n  title\t\t= {Faster {R}-{CNN}: Towards real-time object detection with\n\t\t  region proposal networks},\n  author\t= {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun,\n\t\t  Jian},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {91--99},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Rendle.2010,\n  title\t\t= {Factorization machines},\n  author\t= {Rendle, Steffen},\n  booktitle\t= {2010 {IEEE} {I}nternational {C}onference on {D}ata {M}ining},\n  pages\t\t= {995--1000},\n  year\t\t= {2010},\n  organization\t= {IEEE}\n}\n\n@InProceedings{\t  Rendle.Freudenthaler.Gantner.ea.2009,\n  title\t\t= {{BPR}: {B}ayesian personalized ranking from implicit\n\t\t  feedback},\n  author\t= {Rendle, Steffen and Freudenthaler, Christoph and Gantner,\n\t\t  Zeno and Schmidt-Thieme, Lars},\n  booktitle\t= {Proceedings of the 25th {C}onference on {U}ncertainty\n\t\t  in {A}rtificial {I}ntelligence},\n  pages\t\t= {452--461},\n  year\t\t= {2009},\n  organization\t= {AUAI Press}\n}\n\n@Article{\t  Revels.Lubin.Papamarkou.2016,\n  title\t\t= {Forward-mode automatic differentiation in {J}ulia},\n  author\t= {Revels, Jarrett and Lubin, Miles and Papamarkou,\n\t\t  Theodore},\n  journal\t= {Ar{X}iv:1607.07892},\n  year\t\t= {2016}\n}\n\n@Article{\t  Riesenhuber.Poggio.1999,\n  title\t\t= {Hierarchical models of object recognition in cortex},\n  author\t= {Riesenhuber, Maximilian and Poggio, Tomaso},\n  journal\t= {Nature {N}euroscience},\n  volume\t= {2},\n  number\t= {11},\n  pages\t\t= {1019--1025},\n  year\t\t= {1999},\n  publisher\t= {Nature Publishing Group}\n}\n\n@Book{\t\t  Rockafellar.1970,\n  author\t= {R.~T.~Rockafellar},\n  publisher\t= {Princeton {U}niversity {P}ress},\n  title\t\t= {Convex {A}nalysis},\n  year\t\t= 1970\n}\n\n@article{Rolnick.Veit.Belongie.Shavit.2017,\n  title={Deep learning is robust to massive label noise},\n  author={Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},\n  journal={Ar{X}iv:1705.10694},\n  year={2017}\n}\n\n\n@Article{\t  Rosenbaum.Rubin.1983,\n  title\t\t= {The central role of the propensity score in observational\n\t\t  studies for causal effects},\n  author\t= {Rosenbaum, Paul R and Rubin, Donald B},\n  journal\t= {Biometrika},\n  volume\t= {70},\n  number\t= {1},\n  pages\t\t= {41--55},\n  year\t\t= {1983},\n  publisher\t= {Oxford University Press}\n}\n\n@Book{\t\t  Rudin.1973,\n  author\t= {W.~Rudin},\n  publisher\t= {McGraw-Hill},\n  title\t\t= {Functional {A}nalysis},\n  year\t\t= 1973\n}\n\n@Article{\t  Rumelhart.Hinton.Williams.ea.1988,\n  title\t\t= {Learning representations by back-propagating errors},\n  author\t= {Rumelhart, David E and Hinton, Geoffrey E and Williams,\n\t\t  Ronald J},\n  journal\t= {Cognitive {M}odeling},\n  volume\t= {5},\n  number\t= {3},\n  pages\t\t= {1},\n  year\t\t= {1988}\n}\n\n@InProceedings{\t  Russakovsky.Deng.Huang.ea.2013,\n  author\t= {Olga Russakovsky and Jia Deng and Zhiheng Huang and\n\t\t  Alexander C. Berg and Li Fei-Fei},\n  title\t\t= {Detecting avocados to zucchinis: what have we done, and\n\t\t  where are we going?},\n  booktitle\t= {{I}nternational {C}onference on {C}omputer {V}ision (ICCV)},\n  year\t\t= {2013}\n}\n\n@Book{\t\t  Russell.Norvig.2016,\n  title\t\t= {Artificial {I}ntelligence: {A} {M}odern {A}pproach},\n  author\t= {Russell, Stuart J and Norvig, Peter},\n  year\t\t= {2016},\n  publisher\t= {Pearson {E}ducation {L}imited}\n}\n\n@Article{\t  Salton.Wong.Yang.1975,\n  title\t\t= {A vector space model for automatic indexing},\n  author\t= {Salton, Gerard and Wong, Anita and Yang, Chung-Shu},\n  journal\t= {Communications of the {ACM}},\n  volume\t= {18},\n  number\t= {11},\n  pages\t\t= {613--620},\n  year\t\t= {1975},\n  publisher\t= {ACM}\n}\n\n@InProceedings{\t  Santurkar.Tsipras.Ilyas.ea.2018,\n  title\t\t= {How does batch normalization help optimization?},\n  author\t= {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew\n\t\t  and Madry, Aleksander},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {2483--2493},\n  year\t\t= {2018}\n}\n\n@InProceedings{\t  Sarwar.Karypis.Konstan.ea.2001,\n  title\t\t= {Item-based collaborative filtering recommendation\n\t\t  algorithms.},\n  author\t= {Sarwar, Badrul Munir and Karypis, George and Konstan,\n\t\t  Joseph A and Riedl, John},\n  booktitle\t= {Proceedings of 10th {I}nternational {C}onference on {W}orld {W}ide {W}eb},\n  pages\t\t= {285--295},\n  year\t\t= {2001}\n}\n\n@InProceedings{\t  Schein.Popescul.Ungar.ea.2002,\n  title\t\t= {Methods and metrics for cold-start recommendations},\n  author\t= {Schein, Andrew I and Popescul, Alexandrin and Ungar, Lyle\n\t\t  H and Pennock, David M},\n  booktitle\t= {Proceedings of the 25th {A}nnual {I}nternational {ACM} {SIGIR}\n\t\t  {C}onference on {R}esearch and {D}evelopment in {I}nformation\n\t\t  {R}etrieval},\n  pages\t\t= {253--260},\n  year\t\t= {2002},\n  organization\t= {ACM}\n}\n\n@InProceedings{\t  Scholkopf.Burges.Vapnik.1996,\n  title\t\t= {Incorporating invariances in support vector learning\n\t\t  machines},\n  author\t= {Sch{\\\"o}lkopf, Bernhard and Burges, Chris and Vapnik,\n\t\t  Vladimir},\n  booktitle\t= {{I}nternational {C}onference on {A}rtificial {N}eural {N}etworks},\n  pages\t\t= {47--52},\n  year\t\t= {1996},\n  organization\t= {Springer}\n}\n\n@InProceedings{\t  Scholkopf.Herbrich.Smola.2001,\n  title = {A generalized representer theorem},\n  author\t= {B.~Sch{\\\"o}lkopf and R.~Herbrich and A. J. Smola},\n  booktitle\t= {Proceedings of the {A}nnual {C}onference on {C}omputational {L}earning {T}heory},\n  editor\t= {D.~P.~Helmbold and B.~Williamson},\n  pages\t\t= {416-426},\n  publisher\t= {Springer-Verlag},\n  year\t\t= 2001\n}\n\n@Book{\t\t  Scholkopf.Smola.2002,\n  title\t\t= {Learning with {K}ernels: {S}upport {V}ector {M}achines,\n\t\t  {R}egularization, {O}ptimization, and {B}eyond},\n  author\t= {Sch{\\\"o}lkopf, Bernhard and Smola, Alexander J},\n  year\t\t= {2002},\n  publisher\t= {{MIT} {P}ress}\n}\n\n@Article{\t  Schuster.Paliwal.1997,\n  title\t\t= {Bidirectional recurrent neural networks},\n  author\t= {Schuster, Mike and Paliwal, Kuldip K},\n  journal\t= {IEEE {T}ransactions on {S}ignal {P}rocessing},\n  volume\t= {45},\n  number\t= {11},\n  pages\t\t= {2673--2681},\n  year\t\t= {1997},\n  publisher\t= {IEEE}\n}\n\n@InProceedings{\t  Sedhain.Menon.Sanner.ea.2015,\n  title\t\t= {Autorec: Autoencoders meet collaborative filtering},\n  author\t= {Sedhain, Suvash and Menon, Aditya Krishna and Sanner,\n\t\t  Scott and Xie, Lexing},\n  booktitle\t= {Proceedings of the 24th {I}nternational {C}onference on {W}orld\n\t\t  {W}ide {W}eb},\n  pages\t\t= {111--112},\n  year\t\t= {2015},\n  organization\t= {ACM}\n}\n\n@Article{\t  Sennrich.Haddow.Birch.2015,\n  title\t\t= {Neural machine translation of rare words with subword\n\t\t  units},\n  author\t= {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},\n  journal\t= {Ar{X}iv:1508.07909},\n  year\t\t= {2015}\n}\n\n@Article{\t  Sergeev.Del-Balso.2018,\n  title\t\t= {Horovod: fast and easy distributed deep learning in\n\t\t  {T}ensor{F}low},\n  author\t= {Sergeev, Alexander and Del Balso, Mike},\n  journal\t= {Ar{X}iv:1802.05799},\n  year\t\t= {2018}\n}\n\n@Article{\t  Shannon.1948,\n  author\t= {Shannon, Claude Elwood},\n  journal\t= {The {B}ell {S}ystem {T}echnical {J}ournal},\n  number\t= {3},\n  pages\t\t= {379--423},\n  publisher\t= {Nokia Bell Labs},\n  title\t\t= {A Mathematical Theory of Communication},\n  volume\t= {27},\n  year\t\t= 1948\n}\n\n@InProceedings{\t  Shao.Yao.Sun.ea.2020,\n  title\t\t= {Control{VAE}: Controllable variational autoencoder},\n  author\t= {Shao, Huajie and Yao, Shuochao and Sun, Dachun and Zhang,\n\t\t  Aston and Liu, Shengzhong and Liu, Dongxin and Wang, Jun\n\t\t  and Abdelzaher, Tarek},\n  booktitle\t= {Proceedings of the 37th {I}nternational {C}onference on\n\t\t  {M}achine {L}earning},\n  year\t\t= {2020},\n  organization\t= {JMLR. org}\n}\n\n@Article{\t  Silver.Huang.Maddison.ea.2016,\n  title\t\t= {Mastering the game of {G}o with deep neural networks and\n\t\t  tree search},\n  author\t= {Silver, David and Huang, Aja and Maddison, Chris J and\n\t\t  Guez, Arthur and Sifre, Laurent and Van Den Driessche,\n\t\t  George and Schrittwieser, Julian and Antonoglou, Ioannis\n\t\t  and Panneershelvam, Veda and Lanctot, Marc and et al.},\n  journal\t= {Nature},\n  volume\t= {529},\n  number\t= {7587},\n  pages\t\t= {484},\n  year\t\t= {2016},\n  publisher\t= {Nature Publishing Group}\n}\n\n@InCollection{\t  Simard.LeCun.Denker.ea.1998,\n  title\t\t= {Transformation invariance in pattern recognition -- tangent\n\t\t  distance and tangent propagation},\n  author\t= {Simard, Patrice Y and LeCun, Yann A and Denker, John S and\n\t\t  Victorri, Bernard},\n  booktitle\t= {Neural {N}etworks: {T}ricks of the {T}rade},\n  pages\t\t= {239--274},\n  year\t\t= {1998},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Simonyan.Zisserman.2014,\n  title\t\t= {Very deep convolutional networks for large-scale image\n\t\t  recognition},\n  author\t= {Simonyan, Karen and Zisserman, Andrew},\n  journal\t= {Ar{X}iv:1409.1556},\n  year\t\t= {2014}\n}\n\n@InProceedings{\t  Sivic.Zisserman.2003,\n  title\t\t= {Video {G}oogle: A text retrieval approach to object matching\n\t\t  in videos},\n  author\t= {Sivic, Josef and Zisserman, Andrew},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on {C}omputer {V}ision},\n  volume\t= {3},\n  pages\t\t= {1470--1470},\n  year\t\t= {2003},\n  organization\t= {IEEE {C}omputer {S}ociety}\n}\n\n@Article{\t  Smola.Narayanamurthy.2010,\n  title\t\t= {An architecture for parallel topic models},\n  author\t= {Smola, Alexander and Narayanamurthy, Shravan},\n  journal\t= {Proceedings of the {VLDB} {E}ndowment},\n  volume\t= {3},\n  number\t= {1-2},\n  pages\t\t= {703--710},\n  year\t\t= {2010},\n  publisher\t= {VLDB Endowment}\n}\n\n@PhDThesis{\t  Speelpenning.1980,\n  title\t\t= {Compiling fast partial derivatives of functions given by\n\t\t  algorithms},\n  author\t= {Speelpenning, Bert},\n  year\t\t= {1980},\n  school\t= {University of Illinois at Urbana-Champaign}\n}\n\n@Book{\t\t  Spiegelhalter.2019,\n  title\t\t= {The {A}rt of {S}tatistics: {L}earning from {D}ata},\n  author\t= {Spiegelhalter, David},\n  year\t\t= {2019},\n  publisher\t= {Penguin}\n}\n\n@Article{\t  Srivastava.Hinton.Krizhevsky.ea.2014,\n  title\t\t= {Dropout: a simple way to prevent neural networks from\n\t\t  overfitting},\n  author\t= {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky,\n\t\t  Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},\n  journal\t= {{J}ournal of {M}achine {L}earning {R}esearch},\n  volume\t= {15},\n  number\t= {1},\n  pages\t\t= {1929--1958},\n  year\t\t= {2014},\n  publisher\t= {JMLR. org}\n}\n\n@Book{\t\t  Strang.1993,\n  title\t\t= {Introduction to {L}inear {A}lgebra},\n  author\t= {Strang, Gilbert},\n  year\t\t= {1993},\n  publisher\t= {Wellesley--{C}ambridge {P}ress}\n}\n\n@Article{\t  Su.Khoshgoftaar.2009,\n  title\t\t= {A survey of collaborative filtering techniques},\n  author\t= {Su, Xiaoyuan and Khoshgoftaar, Taghi M},\n  journal\t= {Advances in {A}rtificial {I}ntelligence},\n  volume\t= {2009},\n  year\t\t= {2009},\n  publisher\t= {Hindawi}\n}\n\n@InProceedings{\t  Sukhbaatar.Weston.Fergus.ea.2015,\n  title\t\t= {End-to-end memory networks},\n  author\t= {Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {2440--2448},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Sutskever.Martens.Dahl.ea.2013,\n  title\t\t= {On the importance of initialization and momentum in deep\n\t\t  learning},\n  author\t= {Sutskever, Ilya and Martens, James and Dahl, George and\n\t\t  Hinton, Geoffrey},\n  booktitle\t= {{I}nternational {C}onference on {M}achine {L}earning},\n  pages\t\t= {1139--1147},\n  year\t\t= {2013}\n}\n\n@article{chan2015listen,\n  title={Listen, attend and spell},\n  author={Chan, William and Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol},\n  journal={Ar{X}iv:1508.01211},\n  year={2015}\n}\n\n@book{rabiner1993fundamentals,\n  title={Fundamentals of {S}peech {R}ecognition},\n  author={Rabiner, Lawrence and Juang, Biing-Hwang},\n  year={1993},\n  publisher={Prentice-{H}all.}\n}\n\n@article{yang2016neural,\n  title={Neural machine translation with recurrent attention modeling},\n  author={Yang, Zichao and Hu, Zhiting and Deng, Yuntian and Dyer, Chris and Smola, Alex},\n  journal={Ar{X}iv:1607.05108},\n  year={2016}\n}\n\n\n@InProceedings{\t  Sutskever.Vinyals.Le.2014,\n  title\t\t= {Sequence to sequence learning with neural networks},\n  author\t= {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {3104--3112},\n  year\t\t= {2014}\n}\n\n@InProceedings{\t  Szegedy.Ioffe.Vanhoucke.ea.2017,\n  title\t\t= {Inception-v4, {I}nception-{R}es{N}et and the impact of residual\n\t\t  connections on learning},\n  author\t= {Szegedy, Christian and Ioffe, Sergey and Vanhoucke,\n\t\t  Vincent and Alemi, Alexander A},\n  booktitle\t= {31st {AAAI} {C}onference on {A}rtificial {I}ntelligence},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  Szegedy.Liu.Jia.ea.2015,\n  title\t\t= {Going deeper with convolutions},\n  author\t= {Szegedy, Christian and Liu, Wei and Jia, Yangqing and\n\t\t  Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and\n\t\t  Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich,\n\t\t  Andrew},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {1--9},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Szegedy.Vanhoucke.Ioffe.ea.2016,\n  title\t\t= {Rethinking the {I}nception architecture for computer\n\t\t  vision},\n  author\t= {Szegedy, Christian and Vanhoucke, Vincent and Ioffe,\n\t\t  Sergey and Shlens, Jon and Wojna, Zbigniew},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {2818--2826},\n  year\t\t= {2016}\n}\n\n@Article{\t  Tallec.Ollivier.2017,\n  title\t\t= {Unbiasing truncated backpropagation through time},\n  author\t= {Tallec, Corentin and Ollivier, Yann},\n  journal\t= {Ar{X}iv:1705.08209},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  Tang.Wang.2018,\n  title\t\t= {Personalized top-n sequential recommendation via\n\t\t  convolutional sequence embedding},\n  author\t= {Tang, Jiaxi and Wang, Ke},\n  booktitle\t= {Proceedings of the Eleventh {ACM} {I}nternational {C}onference\n\t\t  on {W}eb {S}earch and {D}ata {M}ining},\n  pages\t\t= {565--573},\n  year\t\t= {2018},\n  organization\t= {ACM}\n}\n\n@Article{\t  Taskar.Guestrin.Koller.2004,\n  title\t\t= {Max-margin {M}arkov networks},\n  author\t= {Taskar, Ben and Guestrin, Carlos and Koller, Daphne},\n  journal\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume\t= {16},\n  pages\t\t= {25},\n  year\t\t= {2004}\n}\n\n@Article{\t  Tay.Dehghani.Bahri.ea.2020,\n  title\t\t= {Efficient transformers: A survey},\n  author\t= {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler,\n\t\t  Donald},\n  journal\t= {Ar{X}iv:2009.06732},\n  year\t\t= {2020}\n}\n\n@Article{\t  Teye.Azizpour.Smith.2018,\n  title\t\t= {Bayesian uncertainty estimation for batch normalized deep\n\t\t  networks},\n  author\t= {Teye, Mattias and Azizpour, Hossein and Smith, Kevin},\n  journal\t= {Ar{X}iv:1802.06455},\n  year\t\t= {2018}\n}\n\n@Article{\t  Thomee.Shamma.Friedland.ea.2016,\n  title\t\t= {{YFCC}100{M}: The new data in multimedia research},\n  author\t= {Thomee, Bart and Shamma, David A and Friedland, Gerald and\n\t\t  Elizalde, Benjamin and Ni, Karl and Poland, Douglas and\n\t\t  Borth, Damian and Li, Li-Jia},\n  journal\t= {Communications of the {ACM}},\n  volume\t= {59},\n  number\t= {2},\n  pages\t\t= {64--73},\n  year\t\t= {2016},\n  publisher\t= {{ACM} New York, NY, USA}\n}\n\n@Book{\t\t  Thompson.2015,\n  title\t\t= {Mathematical {S}tatistical {M}echanics},\n  author\t= {Thompson, Colin J},\n  year\t\t= {2015},\n  publisher\t= {Princeton {U}niversity {P}ress}\n}\n\n@inCollection{\t  Tieleman.Hinton.2012,\n  title\t\t= {Divide the gradient by a running\n\t\t  average of its recent magnitude},\n  author\t= {Tieleman, Tijmen and Hinton, Geoffrey},\n  booktitle\t= {{COURSERA}: Neural {N}etworks for {M}achine {L}earning, Lecture 6.5-rmsprop},\n  year\t\t= {2012}\n}\n\n@Article{\t  Torralba.Fergus.Freeman.2008,\n  title\t\t= {80 million tiny images: A large data set for nonparametric\n\t\t  object and scene recognition},\n  author\t= {Torralba, Antonio and Fergus, Rob and Freeman, William T},\n  journal\t= {{IEEE} {T}ransactions on {P}attern {A}nalysis and {M}achine\n\t\t  {I}ntelligence},\n  volume\t= {30},\n  number\t= {11},\n  pages\t\t= {1958--1970},\n  year\t\t= {2008},\n  publisher\t= {IEEE}\n}\n\n@misc{\t  Toscher.Jahrer.Bell.2009,\n  title\t\t= {The bigchaos solution to the {N}etflix grand prize},\n  author\t= {T{\\\"o}scher, Andreas and Jahrer, Michael and Bell, Robert\n\t\t  M},\n  booktitle\t= {Netflix {P}rize {D}ocumentation},\n  pages\t\t= {1--52},\n  year\t\t= {2009}\n}\n\n@Article{\t  Treisman.Gelade.1980,\n  title\t\t= {A feature-integration theory of attention},\n  author\t= {Treisman, Anne M and Gelade, Garry},\n  journal\t= {Cognitive {P}sychology},\n  volume\t= {12},\n  number\t= {1},\n  pages\t\t= {97--136},\n  year\t\t= {1980},\n  publisher\t= {Elsevier}\n}\n\n@Article{\t  Tsoumakas.Katakis.2007,\n  title\t\t= {Multi-label classification: An overview},\n  author\t= {Tsoumakas, Grigorios and Katakis, Ioannis},\n  journal\t= {International {J}ournal of {D}ata {W}arehousing and {M}ining},\n  volume\t= {3},\n  number\t= {3},\n  pages\t\t= {1--13},\n  year\t\t= {2007},\n  publisher\t= {IGI {G}lobal}\n}\n\n@Article{\t  Turing.1950,\n  title\t\t= {Computing machinery and intelligence},\n  author\t= {Turing, Alan},\n  journal\t= {Mind},\n  volume\t= {59},\n  number\t= {236},\n  pages\t\t= {433},\n  year\t\t= {1950}\n}\n\n@Article{\t  Uijlings.Van-De-Sande.Gevers.ea.2013,\n  title\t\t= {Selective search for object recognition},\n  author\t= {Uijlings, Jasper RR and Van De Sande, Koen EA and Gevers,\n\t\t  Theo and Smeulders, Arnold WM},\n  journal\t= {International {J}ournal of {C}omputer {V}ision},\n  volume\t= {104},\n  number\t= {2},\n  pages\t\t= {154--171},\n  year\t\t= {2013},\n  publisher\t= {Springer}\n}\n\n@InProceedings{\t  Vapnik.1992,\n  title\t\t= {Principles of risk minimization for learning theory},\n  author\t= {Vapnik, Vladimir},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {831--838},\n  year\t\t= {1992}\n}\n\n@Book{\t\t  Vapnik.1998,\n  address\t= {New York},\n  author\t= {V. Vapnik},\n  publisher\t= {John {W}iley \\& {S}ons},\n  title\t\t= {Statistical {L}earning {T}heory},\n  year\t\t= 1998\n}\n\n@Article{\t  Vapnik.Chervonenkis.1964,\n  author\t= {V. Vapnik and A. Chervonenkis},\n  issue\t\t= {1},\n  journal\t= {Automation and {R}emote {C}ontrol},\n  title\t\t= {A note on one class of perceptrons},\n  volume\t= {25},\npages = {103--109},\n  year\t\t= {1964}\n}\n\n@Article{\t  Vapnik.Chervonenkis.1968,\n  author\t= {V. Vapnik and A. Chervonenkis},\n  journal\t= {Dokl.\\ {A}kad.\\ {N}auk {SSSR}},\n  pages\t\t= {915--918},\n  title\t\t= {Uniform convergence of frequencies of occurence of events\n\t\t  to their probabilities},\n  volume\t= 181,\n  year\t\t= 1968\n}\n\n@Article{\t  Vapnik.Chervonenkis.1971,\n  author\t= {V. Vapnik and A. Chervonenkis},\n  journal\t= {Theory {P}robab. {A}ppl.},\n  number\t= 2,\n  pages\t\t= {264--281},\n  title\t\t= {On the uniform convergence of relative frequencies of\n\t\t  events to their probabilities},\n  volume\t= 16,\n  year\t\t= 1971\n}\n\n@Book{\t\t  Vapnik.Chervonenkis.1974,\n  address\t= {Moscow},\n  author\t= {V. Vapnik and A. Chervonenkis},\n  note\t\t= {(German Translation: W. Wapnik \\& A. Tscherwonenkis, {\\em\n\t\t  Theorie der Zeichenerkennung}, Akademie-Verlag, Berlin,\n\t\t  1979)},\n  publisher\t= {Nauka},\n  title\t\t= {Theory of {P}attern {R}ecognition [in {R}ussian]},\n  year\t\t= 1974\n}\n\n@Article{\t  Vapnik.Chervonenkis.1974*1,\n  author\t= {V.~N. Vapnik and A.~Y. Chervonenkis},\n  journal\t= {Automation and {R}emote {C}ontrol},\n  pages\t\t= {1226--1235; 1403--1412},\n  title\t\t= {Ordered risk minimization},\n  volume\t= 35,\n  year\t\t= 1974\n}\n\n@Book{\t\t  Vapnik.Chervonenkis.1974*2,\n  address\t= {Moscow: Nauka},\n  author\t= {V. Vapnik and A. Chervonenkis},\n  title\t\t= {Teoriya {R}aspoznavaniya {O}brazov: {S}tatisticheskie {P}roblemy\n\t\t  {O}bucheniya. (in Russian) [{T}heory of {P}attern {R}ecognition:\n\t\t  {S}tatistical {P}roblems of {L}earning]},\n  year\t\t= 1974\n}\n\n@Article{\t  Vapnik.Chervonenkis.1981,\n  author\t= {V. Vapnik and A. Chervonenkis},\n  journal\t= {Teoriya {V}eroyatnostei i {E}e {P}rimeneniya},\n  number\t= 3,\n  pages\t\t= {543--564},\n  title\t\t= {The necessary and sufficient conditions for the uniform\n\t\t  convergence of averages to their expected values},\n  volume\t= 26,\n  year\t\t= 1981\n}\n\n@Article{\t  Vapnik.Chervonenkis.1991,\n  author\t= {V. Vapnik and A. Chervonenkis},\n  journal\t= {{P}attern {R}ecognition and {I}mage {A}nalysis},\n  number\t= 3,\n  pages\t\t= {283--305},\n  title\t\t= {The necessary and sufficient conditions for consistency in\n\t\t  the empirical risk minimization method},\n  volume\t= 1,\n  year\t\t= 1991\n}\n\n@Article{\t  Vapnik.Levin.Le-Cun.1994,\n  title\t\t= {Measuring the {VC}-dimension of a learning machine},\n  author\t= {Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},\n  journal\t= {Neural {C}omputation},\n  volume\t= {6},\n  number\t= {5},\n  pages\t\t= {851--876},\n  year\t\t= {1994},\n  publisher\t= {{MIT} {P}ress}\n}\n\n@InProceedings{\t  Vaswani.Shazeer.Parmar.ea.2017,\n  title\t\t= {Attention is all you need},\n  author\t= {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and\n\t\t  Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and\n\t\t  Kaiser, {\\L}ukasz and Polosukhin, Illia},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {5998--6008},\n  year\t\t= {2017}\n}\n\n@Book{\t\t  Wahba.1990,\n  title\t\t= {Spline {M}odels for {O}bservational {D}ata},\n  author\t= {Wahba, Grace},\n  year\t\t= {1990},\n  publisher\t= {SIAM}\n}\n\n@Article{\t  Waibel.Hanazawa.Hinton.ea.1989,\n  title\t\t= {Phoneme recognition using time-delay neural networks},\n  author\t= {Waibel, Alex and Hanazawa, Toshiyuki and Hinton, Geoffrey\n\t\t  and Shikano, Kiyohiro and Lang, Kevin J},\n  journal\t= {IEEE {T}ransactions on {A}coustics, {S}peech, and {S}ignal\n\t\t  {P}rocessing},\n  volume\t= {37},\n  number\t= {3},\n  pages\t\t= {328--339},\n  year\t\t= {1989},\n  publisher\t= {IEEE}\n}\n\n@InProceedings{\t  Wang.Davidson.Pan.ea.2016,\n  title\t\t= {Gunrock: A high-performance graph processing library on\n\t\t  the {GPU}},\n  author\t= {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and\n\t\t  Wu, Yuduo and Riffel, Andy and Owens, John D},\n  booktitle\t= {{ACM} {SIGPLAN} {N}otices},\n  volume\t= {51},\n  number\t= {8},\n  pages\t\t= {11},\n  year\t\t= {2016},\n  organization\t= {ACM}\n}\n\n@Article{\t  Wang.Li.Liberty.ea.2018,\n  title\t\t= {Optimal message scheduling for aggregation},\n  author\t= {Wang, Leyuan and Li, Mu and Liberty, Edo and Smola, Alex\n\t\t  J},\n  journal\t= {Networks},\n  volume\t= {2},\n  number\t= {3},\n  pages\t\t= {2--3},\n  year\t\t= {2018}\n}\n\n@Article{\t  Warstadt.Singh.Bowman.2019,\n  title\t\t= {Neural network acceptability judgments},\n  author\t= {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},\n  journal\t= {{T}ransactions of the {A}ssociation for {C}omputational\n\t\t  {L}inguistics},\n  volume\t= {7},\n  pages\t\t= {625--641},\n  year\t\t= {2019},\n  publisher\t= {MIT {P}ress}\n}\n\n@Book{\t\t  Wasserman.2013,\n  title\t\t= {All of {S}tatistics: {A} {C}oncise {C}ourse in {S}tatistical\n\t\t  {I}nference},\n  author\t= {Wasserman, Larry},\n  year\t\t= {2013},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Watkins.Dayan.1992,\n  title\t\t= {Q-learning},\n  author\t= {Watkins, Christopher JCH and Dayan, Peter},\n  journal\t= {Machine {L}earning},\n  volume\t= {8},\n  number\t= {3--4},\n  pages\t\t= {279--292},\n  year\t\t= {1992},\n  publisher\t= {Springer}\n}\n\n@Article{\t  Watson.1964,\n  title\t\t= {Smooth regression analysis},\n  author\t= {Watson, Geoffrey S},\n  journal\t= {Sankhy{\\=a}: The {I}ndian {J}ournal of {S}tatistics, {S}eries {A}},\n  pages\t\t= {359--372},\n  year\t\t= {1964},\n  publisher\t= {JSTOR}\n}\n\n@InProceedings{\t  Welling.Teh.2011,\n  title\t\t= {Bayesian learning via stochastic gradient {L}angevin\n\t\t  dynamics},\n  author\t= {Welling, Max and Teh, Yee W},\n  booktitle\t= {Proceedings of the 28th {I}nternational {C}onference on\n\t\t  {M}achine {L}earning ({ICML}-11)},\n  pages\t\t= {681--688},\n  year\t\t= {2011}\n}\n\n@Article{\t  Wengert.1964,\n  title\t\t= {A simple automatic derivative evaluation program},\n  author\t= {Wengert, Robert Edwin},\n  journal\t= {Communications of the {ACM}},\n  volume\t= {7},\n  number\t= {8},\n  pages\t\t= {463--464},\n  year\t\t= {1964},\n  publisher\t= {{ACM} New York, NY, USA}\n}\n\n@Article{\t  Werbos.1990,\n  title\t\t= {Backpropagation through time: what it does and how to do\n\t\t  it},\n  author\t= {Werbos, Paul J},\n  journal\t= {Proceedings of the {IEEE}},\n  volume\t= {78},\n  number\t= {10},\n  pages\t\t= {1550--1560},\n  year\t\t= {1990},\n  publisher\t= {IEEE}\n}\n\n@InProceedings{\t  Wigner.1958,\n  title\t\t= {On the distribution of the roots of certain symmetric\n\t\t  matrices},\n  author\t= {Wigner, Eugene P.},\n  booktitle\t= {Ann. {M}ath.},\n  pages\t\t= {325--327},\n  year\t\t= {1958}\n}\n\n@TechReport{\t  Williams.Waterman.Patterson.2009,\n  title\t\t= {Roofline: An insightful visual performance model for\n\t\t  floating-point programs and multicore architectures},\n  author\t= {Williams, Samuel and Waterman, Andrew and Patterson,\n\t\t  David},\n  year\t\t= {2009},\n  institution\t= {Lawrence {B}erkeley {N}ational {L}ab.}\n}\n\n@Article{\t  Wood.Gasthaus.Archambeau.ea.2011,\n  title\t\t= {The sequence memoizer},\n  author\t= {Wood, Frank and Gasthaus, Jan and Archambeau, C{\\'e}dric\n\t\t  and James, Lancelot and Teh, Yee Whye},\n  journal\t= {Communications of the {ACM}},\n  volume\t= {54},\n  number\t= {2},\n  pages\t\t= {91--98},\n  year\t\t= {2011},\n  publisher\t= {ACM}\n}\n\n@InProceedings{\t  Wu.Ahmed.Beutel.ea.2017,\n  title\t\t= {Recurrent recommender networks},\n  author\t= {Wu, Chao-Yuan and Ahmed, Amr and Beutel, Alex and Smola,\n\t\t  Alexander J and Jing, How},\n  booktitle\t= {Proceedings of the 10th {ACM} {I}nternational {C}onference on\n\t\t  {W}eb {S}earch and {D}ata {M}ining},\n  pages\t\t= {495--503},\n  year\t\t= {2017},\n  organization\t= {ACM}\n}\n\n@Article{\t  Wu.Schuster.Chen.ea.2016,\n  title\t\t= {Google's neural machine translation system: Bridging the\n\t\t  gap between human and machine translation},\n  author\t= {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le,\n\t\t  Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and\n\t\t  Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey,\n\t\t  Klaus and et al.},\n  journal\t= {Ar{X}iv:1609.08144},\n  year\t\t= {2016}\n}\n\n@InProceedings{\t  Xiao.Bahri.Sohl-Dickstein.ea.2018,\n  title\t\t= {Dynamical isometry and a mean field theory of {CNN}s: How to\n\t\t  train 10,000-layer vanilla convolutional neural networks},\n  author\t= {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha\n\t\t  and Schoenholz, Samuel and Pennington, Jeffrey},\n  booktitle\t= {{I}nternational {C}onference on {M}achine {L}earning},\n  pages\t\t= {5393--5402},\n  year\t\t= {2018}\n}\n\n@Article{\t  Xiao.Rasul.Vollgraf.2017,\n  title\t\t= {Fashion-{MNIST}: a novel image dataset for benchmarking\n\t\t  machine learning algorithms},\n  author\t= {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},\n  journal\t= {Ar{X}iv:1708.07747},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  Xie.Girshick.Dollar.ea.2017,\n  title\t\t= {Aggregated residual transformations for deep neural\n\t\t  networks},\n  author\t= {Xie, Saining and Girshick, Ross and Doll{\\'a}r, Piotr and\n\t\t  Tu, Zhuowen and He, Kaiming},\n  booktitle\t= {Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and\n\t\t  {P}attern {R}ecognition},\n  pages\t\t= {1492--1500},\n  year\t\t= {2017}\n}\n\n@InProceedings{\t  Xiong.Wu.Alleva.ea.2018,\n  title\t\t= {The {M}icrosoft 2017 conversational speech recognition\n\t\t  system},\n  author\t= {Xiong, Wayne and Wu, Lingfeng and Alleva, Fil and Droppo,\n\t\t  Jasha and Huang, Xuedong and Stolcke, Andreas},\n  booktitle\t= {2018 {IEEE} {I}nternational {C}onference on {A}coustics, {S}peech\n\t\t  and {S}ignal {P}rocessing ({ICASSP})},\n  pages\t\t= {5934--5938},\n  year\t\t= {2018},\n  organization\t= {IEEE}\n}\n\n@InProceedings{\t  Yamaguchi.Sakamoto.Akabane.ea.1990,\n  title\t\t= {A neural network for speaker-independent isolated word\n\t\t  recognition},\n  author\t= {Yamaguchi, Kouichi and Sakamoto, Kenji and Akabane, Toshio\n\t\t  and Fujimoto, Yoshiji},\n  booktitle\t= {First {I}nternational {C}onference on {S}poken {L}anguage\n\t\t  {P}rocessing},\n  year\t\t= {1990}\n}\n\n@InProceedings{\t  Yang.Moczulski.Denil.ea.2015,\n  title\t\t= {Deep fried convnets},\n  author\t= {Yang, Zichao and Moczulski, Marcin and Denil, Misha and De\n\t\t  Freitas, Nando and Smola, Alex and Song, Le and Wang,\n\t\t  Ziyu},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {1476--1483},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Ye.Yin.Lee.ea.2011,\n  title\t\t= {Exploiting geographical influence for collaborative\n\t\t  point-of-interest recommendation},\n  author\t= {Ye, Mao and Yin, Peifeng and Lee, Wang-Chien and Lee,\n\t\t  Dik-Lun},\n  booktitle\t= {Proceedings of the 34th {I}nternational {ACM} {SIGIR} {C}onference\n\t\t  on {R}esearch and {D}evelopment in {I}nformation {R}etrieval},\n  pages\t\t= {325--334},\n  year\t\t= {2011},\n  organization\t= {ACM}\n}\n\n@Article{\t  You.Gitman.Ginsburg.2017,\n  title\t\t= {Large batch training of convolutional networks},\n  author\t= {You, Yang and Gitman, Igor and Ginsburg, Boris},\n  journal\t= {Ar{X}iv:1708.03888},\n  year\t\t= {2017}\n}\n\n@Article{\t  Yu.1994,\n  title\t\t= {Rates of convergence for empirical processes of stationary\n\t\t  mixing sequences},\n  author\t= {Yu, Bin},\n  journal\t= {{A}nnals of {P}robability},\n  pages\t\t= {94--116},\n  year\t\t= {1994},\n  publisher\t= {JSTOR}\n}\n\n@InProceedings{\t  Zaheer.Reddi.Sachan.ea.2018,\n  title\t\t= {Adaptive methods for nonconvex optimization},\n  author\t= {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and\n\t\t  Kale, Satyen and Kumar, Sanjiv},\n  booktitle\t= {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  pages\t\t= {9793--9803},\n  year\t\t= {2018}\n}\n\n@Article{\t  Zeiler.2012,\n  title\t\t= {{ADADELTA}: an adaptive learning rate method},\n  author\t= {Zeiler, Matthew D},\n  journal\t= {Ar{X}iv:1212.5701},\n  year\t\t= {2012}\n}\n\n@Article{\t  Zeiler.Fergus.2013,\n  title\t\t= {Stochastic pooling for regularization of deep\n\t\t  convolutional neural networks},\n  author\t= {Zeiler, Matthew D and Fergus, Rob},\n  journal\t= {Ar{X}iv:1301.3557},\n  year\t\t= {2013}\n}\n\n@Article{\t  Zhang.2004,\n  title\t\t= {Statistical behavior and consistency of classification\n\t\t  methods based on convex risk minimization},\n  author\t= {Zhang, Tong},\n  journal\t= {{A}nnals of {S}tatistics},\n  volume\t= {32},\n  number\t= {1},\n  pages\t\t= {56--85},\n  year\t\t= {2004},\n  publisher\t= {Institute of {M}athematical {S}tatistics}\n}\n\n@Article{\t  Zhang.Sun.Jiang.ea.2021,\n  title\t\t= {Byte{T}rack: Multi-object tracking by associating every\n\t\t  detection box},\n  author\t= {Yifu Zhang and Peize Sun and Yi Jiang and Dongdong Yu and\n\t\t  Zehuan Yuan and Ping Luo and Wenyu Liu and Xinggang Wang},\n  journal\t= {Ar{X}iv:2110.06864},\n  year\t\t= {2021}\n}\n\n@InProceedings{\t  Zhang.Tay.Zhang.ea.2021,\n  title\t\t= {Beyond fully-connected layers with quaternions:\n\t\t  Parameterization of hypercomplex multiplications with 1/n\n\t\t  parameters},\n  author\t= {Zhang, Aston and Tay, Yi and Zhang, Shuai and Chan, Alvin\n\t\t  and Luu, Anh Tuan and Hui, Siu Cheung and Fu, Jie},\n  booktitle\t= {{I}nternational {C}onference on {L}earning {R}epresentations},\n  year\t\t= {2021}\n}\n\n@Article{\t  Zhang.Yao.Sun.ea.2019,\n  title\t\t= {Deep learning based recommender system: A survey and new\n\t\t  perspectives},\n  author\t= {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},\n  journal\t= {{ACM} {C}omputing {S}urveys},\n  volume\t= {52},\n  number\t= {1},\n  pages\t\t= {5},\n  year\t\t= {2019},\n  publisher\t= {ACM}\n}\n\n@InProceedings{\t  Zhang.ea.1988,\n  title\t\t= {Shift-invariant pattern recognition neural network and its\n\t\t  optical architecture},\n  author\t= {Zhang, Wei and Tanida, Jun and Itoh, Kazuyoshi and Ichioka, Yoshiki},\n  booktitle\t= {Proceedings of {A}nnual {C}onference of the {J}apan {S}ociety of\n\t\t  {A}pplied {P}hysics},\n  year\t\t= {1988}\n}\n\n@Article{\t  Zhao.Zheng.Xu.ea.2019,\n  title\t\t= {Object detection with deep learning: A review},\n  author\t= {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-tao and Wu,\n\t\t  Xindong},\n  journal\t= {{IEEE} {T}ransactions on {N}eural {N}etworks and {L}earning\n\t\t  {S}ystems},\n  volume\t= {30},\n  number\t= {11},\n  pages\t\t= {3212--3232},\n  year\t\t= {2019},\n  publisher\t= {IEEE}\n}\n\n@InProceedings{\t  Zhu.Kiros.Zemel.ea.2015,\n  title\t\t= {Aligning books and movies: Towards story-like visual\n\t\t  explanations by watching movies and reading books},\n  author\t= {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and\n\t\t  Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba,\n\t\t  Antonio and Fidler, Sanja},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {19--27},\n  year\t\t= {2015}\n}\n\n@InProceedings{\t  Zhu.Park.Isola.ea.2017,\n  title\t\t= {Unpaired image-to-image translation using cycle-consistent\n\t\t  adversarial networks},\n  author\t= {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and\n\t\t  Efros, Alexei A},\n  booktitle\t= {Proceedings of the {IEEE} {I}nternational {C}onference on\n\t\t  {C}omputer {V}ision},\n  pages\t\t= {2223--2232},\n  year\t\t= {2017}\n}\n\n@article{vapnik1994measuring,\n  title={Measuring the {VC}-dimension of a learning machine},\n  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},\n  journal={Neural Computation},\n  volume={6},\n  number={5},\n  pages={851--876},\n  year={1994},\n  publisher={MIT Press}\n}\n\n@article{friedman1987exploratory,\n  title={Exploratory projection pursuit},\n  author={Friedman, Jerome H},\n  journal={Journal of the {A}merican {S}tatistical {A}ssociation},\n  volume={82},\n  number={397},\n  pages={249--266},\n  year={1987},\n  publisher={Taylor \\& {F}rancis}\n}\n\n@article{ong2005learning,\n  title={Learning the kernel with hyperkernels},\n  author={Ong, Cheng Soon and Smola, Alexander and Williamson, Robert},\n  volume={6},\n  pages={1043--1071},\n  year={2005},\n  publisher={MIT Press},\n  journal={{J}ournal of {M}achine {L}earning {R}esearch}\n}\n\n@article{thomee2016yfcc100m,\n  title={YFCC100M: The new data in multimedia research},\n  author={Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},\n  journal={Communications of the {ACM}},\n  volume={59},\n  number={2},\n  pages={64--73},\n  year={2016},\n  publisher={{ACM} New York, NY, USA}\n}\n\n@book{Vapnik98,\n  Address =\t {New York},\n  Author =\t {V. Vapnik},\n  Publisher =\t {John Wiley and Sons},\n  Title =\t {Statistical Learning Theory},\n  Year =\t 1998\n}\n\n@article{VapChe64,\n  Author =\t {V. Vapnik and A. Chervonenkis},\n  Issue =\t 1,\n  Journal =\t {Automation and Remote Control},\n  Title =\t {A note on one class of perceptrons},\n  Volume =\t 25,\n  Year =\t 1964\n}\n\n@article{VapChe68,\n  Author =\t {V. Vapnik and A. Chervonenkis},\n  Journal =\t {Dokl.\\ Akad.\\ Nauk SSSR},\n  Pages =\t {915-918},\n  Title =\t {Uniform convergence of frequencies of occurence of\n                  events to their probabilities},\n  Volume =\t 181,\n  Year =\t 1968\n}\n\n@article{VapChe71,\n  Author =\t {V. Vapnik and A. Chervonenkis},\n  Journal =\t {Theory Probab. Appl.},\n  Number =\t 2,\n  Pages =\t {264-281},\n  Title =\t {On the uniform convergence of relative frequencies\n                  of events to their probabilities},\n  Volume =\t 16,\n  Year =\t 1971\n}\n\n@book{VapChe74,\n  Address =\t {Moscow},\n  Author =\t {V. Vapnik and A. Chervonenkis},\n  Note =\t {(German Translation: W. Wapnik \\& A. Tscherwonenkis,\n                  {\\em Theorie der Zeichenerkennung}, Akademie-Verlag,\n                  Berlin, 1979)},\n  Publisher =\t {Nauka},\n  Title =\t {Theory of {P}attern {R}ecognition [in Russian]},\n  Year =\t 1974\n}\n\n@article{VapChe74b,\n  Author =\t {V.~N. Vapnik and A.~Y. Chervonenkis},\n  Journal =\t {Automation and Remote Control},\n  Pages =\t {1226--1235, 1403--1412},\n  Title =\t {Ordered risk minimization},\n  Volume =\t 35,\n  Year =\t 1974\n}\n\n@article{VapChe81,\n  Author =\t {V. Vapnik and A. Chervonenkis},\n  Journal =\t {Teoriya Veroyatnostei i Ee Primeneniya},\n  Number =\t 3,\n  Pages =\t {543-564},\n  Title =\t {The necessary and sufficient conditions for the\n                  uniform convergence of averages to their expected\n                  values},\n  Volume =\t 26,\n  Year =\t 1981\n}\n\n@article{VapChe91,\n  Author =\t {V. Vapnik and A. Chervonenkis},\n  Journal =\t {{P}attern {R}ecognition and Image Analysis},\n  Number =\t 3,\n  Pages =\t {283-305},\n  Title =\t {The necessary and sufficient conditions for\n                  consistency in the empirical risk minimization\n                  method},\n  Volume =\t 1,\n  Year =\t 1991\n}\n\n@article{boucheron2005theory,\n  title={Theory of classification: A survey of some recent advances},\n  author={Boucheron, St{\\'e}phane and Bousquet, Olivier and Lugosi, G{\\'a}bor},\n  journal={ESAIM: {P}robability and {S}tatistics},\n  volume={9},\n  pages={323--375},\n  year={2005},\n  publisher={EDP Sciences}\n}\n\n@article{sindhwani2015structured,\n  title={Structured transforms for small-footprint deep learning},\n  author={Sindhwani, Vikas and Sainath, Tara N and Kumar, Sanjiv},\n  journal={Ar{X}iv:1510.01722},\n  year={2015}\n}\n\n@inproceedings{dwork2015preserving,\n  title={Preserving statistical validity in adaptive data analysis},\n  author={Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron Leon},\n  booktitle={Proceedings of the 47th {A}nnual {ACM} {S}ymposium on {T}heory of {C}omputing},\n  pages={117--126},\n  year={2015}\n}\n\n@incollection{micchelli1984interpolation,\n  title={Interpolation of scattered data: distance matrices and conditionally positive definite functions},\n  author={Micchelli, Charles A},\n  booktitle={Approximation {T}heory and {S}pline {F}unctions},\n  pages={143--145},\n  year={1984},\n  publisher={Springer}\n}\n\n@book{popper2005logic,\n  title={The {L}ogic of {S}cientific {D}iscovery},\n  author={Popper, Karl},\n  year={2005},\n  publisher={Routledge}\n}\n\n@book{mackay2003information,\n  title={Information {T}heory, {I}nference and {L}earning {A}lgorithms},\n  author={MacKay, David JC},\n  year={2003},\n  publisher={Cambridge {U}niversity {P}ress}\n}\n\n@book{rasmussen2006gaussian,\n  title={Gaussian {P}rocesses for {M}achine {L}earning},\n  author={Rasmussen, Carl Edward and Williams, Christopher KI},\n  number={3},\n  year={2006},\n  publisher={{MIT} {P}ress}\n}\n\n\n@inproceedings{lavin2016fast,\n  title={Fast algorithms for convolutional neural networks},\n  author={Lavin, Andrew and Gray, Scott},\n  booktitle={Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and {P}attern {R}ecognition},\n  pages={4013--4021},\n  year={2016}\n}\n\n@inproceedings{tan2019efficientnet,\n  title={Efficient{N}et: Rethinking model scaling for convolutional neural networks},\n  author={Tan, Mingxing and Le, Quoc},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={6105--6114},\n  year={2019},\n  organization={PMLR}\n}\n\n@article{zoph2016neural,\n  title={Neural architecture search with reinforcement learning},\n  author={Zoph, Barret and Le, Quoc V},\n  journal={Ar{X}iv:1611.01578},\n  year={2016}\n}\n\n@article{liu2018darts,\n  title={{DARTS}: Differentiable architecture search},\n  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},\n  journal={Ar{X}iv:1806.09055},\n  year={2018}\n}\n\n@article{liu2022convnet,\n  title={A Conv{N}et for the 2020s},\n  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},\n  journal={Ar{X}iv:2201.03545},\n  year={2022}\n}\n\n@inproceedings{radosavovic2019network,\n  title={On network design spaces for visual recognition},\n  author={Radosavovic, Ilija and Johnson, Justin and Xie, Saining and Lo, Wan-Yen and Doll{\\'a}r, Piotr},\n  booktitle={Proceedings of the {IEEE}/{CVF} {I}nternational {C}onference on {C}omputer {V}ision},\n  pages={1882--1890},\n  year={2019}\n}\n\n@article{tolstikhin2021mlp,\n  title={{MLP}-mixer: An all-{MLP} architecture for vision},\n  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and et al.},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={34},\n  year={2021}\n}\n\n@inproceedings{liu2021swin,\n  title={Swin transformer: Hierarchical vision transformer using shifted windows},\n  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},\n  booktitle={Proceedings of the {IEEE}/{CVF} {I}nternational {C}onference on {C}omputer {V}ision},\n  pages={10012--10022},\n  year={2021}\n}\n\n@inproceedings{touvron2021training,\n  title={Training data-efficient image transformers \\& distillation through attention},\n  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\\'e}gou, Herv{\\'e}},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={10347--10357},\n  year={2021},\n  organization={PMLR}\n}\n\n@inproceedings{huang2018music,\n  title={Music transformer: generating music with long-term structure},\n  author={Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Simon, Ian and Hawthorne, Curtis and Shazeer, Noam and Dai, Andrew M and Hoffman, Matthew D and Dinculescu, Monica and Eck, Douglas},\n  booktitle={{I}nternational {C}onference on {L}earning {R}epresentations},\n  year={2018}\n}\n\n@article{shaw2018self,\n  title={Self-attention with relative position representations},\n  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},\n  journal={Ar{X}iv:1803.02155},\n  year={2018}\n}\n\n@book{neal1996bayesian,\n  title={Bayesian {L}earning for {N}eural {N}etworks},\n  author={Neal, Radford M},\n  year={1996},\n  publisher={Springer}\n}\n\n@article{matthews2018gaussian,\n  title={Gaussian process behaviour in wide deep neural networks},\n  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},\n  journal={Ar{X}iv:1804.11271},\n  year={2018}\n}\n\n@article{novak2018bayesian,\n  title={Bayesian deep convolutional networks with many channels are {G}aussian processes},\n  author={Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},\n  journal={Ar{X}iv:1810.05148},\n  year={2018}\n}\n\n@article{zhang2021understanding,\n  title={Understanding deep learning (still) requires rethinking generalization},\n  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},\n  journal={Communications of the {ACM}},\n  volume={64},\n  number={3},\n  pages={107--115},\n  year={2021},\n  publisher={{ACM} New York, NY, USA}\n}\n\n@article{nakkiran2021deep,\n  title={Deep double descent: Where bigger models and more data hurt},\n  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},\n  journal={Journal of {S}tatistical {M}echanics: {T}heory and {E}xperiment},\n  volume={2021},\n  number={12},\n  pages={124003},\n  year={2021},\n  publisher={IOP Publishing}\n}\n\n@book{tikhonov1977solutions,\n  title={Solutions of {I}ll-{P}osed {P}roblems},\n  author={Tikhonov, A. N. and Arsenin, V. Y.},\n  year={1977},\n  publisher={W.{H}.~{W}inston}\n}\n\n@book{morozov2012methods,\n  title={Methods for {S}olving {I}ncorrectly {P}osed {P}roblems},\n  author={Morozov, Vladimir Alekseevich},\n  year={1984},\n  publisher={Springer}\n}\n\n@article{srivastava2015highway,\n  title={Highway networks},\n  author={Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\\\"u}rgen},\n  journal={Ar{X}iv:1505.00387},\n  year={2015}\n}\n\n@article{pleiss2017memory,\n  title={Memory-efficient implementation of densenets},\n  author={Pleiss, Geoff and Chen, Danlu and Huang, Gao and Li, Tongcheng and Van Der Maaten, Laurens and Weinberger, Kilian Q},\n  journal={Ar{X}iv:1707.06990},\n  year={2017}\n}\n\n@article{ramachandran2019stand,\n  title={Stand-alone self-attention in vision models},\n  author={Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={32},\n  year={2019}\n}\n\n\n@inproceedings{cordonnier2020relationship,\n  title={On the relationship between self-attention and convolutional layers},\n  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},\n  booktitle={{I}nternational {C}onference on {L}earning {R}epresentations},\n  year={2020}\n}\n\n@article{brown2020language,\n  title={Language models are few-shot learners},\n  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and et al.},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={33},\n  pages={1877--1901},\n  year={2020}\n}\n\n@article{raffel2020exploring,\n  title={Exploring the limits of transfer learning with a unified text-to-text transformer},\n  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},\n  journal={{J}ournal of {M}achine {L}earning {R}esearch},\n  volume={21},\n  pages={1--67},\n  year={2020}\n}\n\n@inproceedings{xiong2020layer,\n  title={On layer normalization in the transformer architecture},\n  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={10524--10533},\n  year={2020},\n  organization={PMLR}\n}\n\n@inproceedings{baevski2018adaptive,\n  title={Adaptive input representations for neural language modeling},\n  author={Baevski, Alexei and Auli, Michael},\n  booktitle={{I}nternational {C}onference on Learning Representations},\n  year={2018}\n}\n\n@inproceedings{wang2019learning,\n  title={Learning deep transformer models for machine translation},\n  author={Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S},\n  booktitle={Proceedings of the 57th {A}nnual {M}eeting of the {A}ssociation for {C}omputational {L}inguistics},\n  pages={1810--1822},\n  year={2019}\n}\n\n@article{wilson2020bayesian,\n  title={Bayesian deep learning and a probabilistic perspective of generalization},\n  author={Wilson, Andrew G and Izmailov, Pavel},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={33},\n  pages={4697--4708},\n  year={2020}\n}\n\n@article{lewis2019bart,\n  title={{BART}: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},\n  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},\n  journal={Ar{X}iv:1910.13461},\n  year={2019}\n}\n\n@inproceedings{clark2019electra,\n  title={{ELECTRA}: Pre-training text encoders as discriminators rather than generators},\n  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},\n  booktitle={{I}nternational {C}onference on {L}earning {R}epresentations},\n  year={2020}\n}\n\n@article{yang2019xlnet,\n  title={{XLN}et: Generalized autoregressive pretraining for language understanding},\n  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={32},\n  year={2019}\n}\n\n@article{lan2019albert,\n  title={{ALBERT}: A lite {BERT} for self-supervised learning of language representations},\n  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},\n  journal={Ar{X}iv:1909.11942},\n  year={2019}\n}\n\n@article{sanh2019distilbert,\n  title={Distil{BERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  journal={Ar{X}iv:1910.01108},\n  year={2019}\n}\n\n@inproceedings{he2022masked,\n  title={Masked autoencoders are scalable vision learners},\n  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\\'a}r, Piotr and Girshick, Ross},\n  booktitle={Proceedings of the {IEEE}/{CVF} {C}onference on {C}omputer {V}ision and {P}attern {R}ecognition},\n  pages={16000--16009},\n  year={2022}\n}\n\n@inproceedings{chen2020generative,\n  title={Generative pretraining from pixels},\n  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},\n  booktitle={{I}nternational {C}onference on machine learning},\n  pages={1691--1703},\n  year={2020},\n  organization={PMLR}\n}\n\n@article{kaplan2020scaling,\n  title={Scaling laws for neural language models},\n  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},\n  journal={Ar{X}iv:2001.08361},\n  year={2020}\n}\n\n@article{rae2021scaling,\n  title={Scaling language models: Methods, analysis \\& insights from training gopher},\n  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and et al.},\n  journal={Ar{X}iv:2112.11446},\n  year={2021}\n}\n\n@article{du2021glam,\n  title={{GL}a{M}: Efficient scaling of language models with mixture-of-experts},\n  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and et al.},\n  journal={Ar{X}iv:2112.06905},\n  year={2021}\n}\n\n@article{shoeybi2019megatron,\n  title={Megatron-{LM}: Training multi-billion parameter language models using model parallelism},\n  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},\n  journal={Ar{X}iv:1909.08053},\n  year={2019}\n}\n\n@article{smith2022using,\n  title={Using {D}eep{S}peed and {M}egatron to train {M}egatron-{T}uring {NLG} 530{B}, a large-scale generative language model},\n  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and et al.},\n  journal={Ar{X}iv:2201.11990},\n  year={2022}\n}\n\n@article{thoppilan2022lamda,\n  title={La{MDA}: Language models for dialog applications},\n  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and et al.},\n  journal={Ar{X}iv:2201.08239},\n  year={2022}\n}\n\n@article{hoffmann2022training,\n  title={Training compute-optimal large language models},\n  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and et al.},\n  journal={Ar{X}iv:2203.15556},\n  year={2022}\n}\n\n@article{zhang2022opt,\n  title={{OPT}: Open pre-trained transformer language models},\n  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and et al.},\n  journal={Ar{X}iv:2205.01068},\n  year={2022}\n}\n\n@article{chowdhery2022palm,\n  title={Pa{LM}: Scaling language modeling with pathways},\n  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and et al.},\n  journal={Ar{X}iv:2204.02311},\n  year={2022}\n}\n\n@article{hernandez2021scaling,\n  title={Scaling laws for transfer},\n  author={Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},\n  journal={Ar{X}iv:2102.01293},\n  year={2021}\n}\n\n@article{tay2021scale,\n  title={Scale efficiently: Insights from pre-training and fine-tuning transformers},\n  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},\n  journal={Ar{X}iv:2109.10686},\n  year={2021}\n}\n\n@article{wei2022emergent,\n  title={Emergent abilities of large language models},\n  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and et al.},\n  journal={Ar{X}iv:2206.07682},\n  year={2022}\n}\n\n@inproceedings{radford2021learning,\n  title={Learning transferable visual models from natural language supervision},\n  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and et al.},\n  booktitle={{I}nternational {C}onference on Machine Learning},\n  pages={8748--8763},\n  year={2021},\n  organization={PMLR}\n}\n\n@inproceedings{ramesh2021zero,\n  title={Zero-shot text-to-image generation},\n  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={8821--8831},\n  year={2021},\n  organization={PMLR}\n}\n\n@article{ramesh2022hierarchical,\n  title={Hierarchical text-conditional image generation with clip latents},\n  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},\n  journal={Ar{X}iv:2204.06125},\n  year={2022}\n}\n\n@article{alayrac2022flamingo,\n  title={Flamingo: a visual language model for few-shot learning},\n  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and et al.},\n  journal={Ar{X}iv:2204.14198},\n  year={2022}\n}\n\n@article{saharia2022photorealistic,\n  title={Photorealistic text-to-image diffusion models with deep language understanding},\n  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and et al.},\n  journal={Ar{X}iv:2205.11487},\n  year={2022}\n}\n\n@article{reed2022generalist,\n  title={A generalist agent},\n  author={Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and et al.},\n  journal={Ar{X}iv:2205.06175},\n  year={2022}\n}\n\n@article{fedus2022switch,\n  title={Switch transformers: scaling to trillion parameter models with simple and efficient sparsity},\n  author={Fedus, William and Zoph, Barret and Shazeer, Noam},\n  journal={{J}ournal of {M}achine {L}earning {R}esearch},\n  volume={23},\n  number={120},\n  pages={1--39},\n  year={2022}\n}\n\n@article{child2019generating,\n  title={Generating long sequences with sparse transformers},\n  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},\n  journal={Ar{X}iv:1904.10509},\n  year={2019}\n}\n\n@article{joshi2020spanbert,\n  title={Span{BERT}: Improving pre-training by representing and predicting spans},\n  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},\n  journal={{T}ransactions of the {A}ssociation for {C}omputational {L}inguistics},\n  volume={8},\n  pages={64--77},\n  year={2020},\n  publisher={MIT {P}ress}\n}\n\n@article{yu2022scaling,\n  title={Scaling autoregressive models for content-rich text-to-image generation},\n  author={Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Karagol Ayan, Burcu and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},\n  journal={Ar{X}iv:2206.10789},\n  year={2022}\n}\n\n@article{srivastava2022beyond,\n  title={Beyond the imitation game: quantifying and extrapolating the capabilities of language models},\n  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\\`a} and et al.},\n  journal={Ar{X}iv:2206.04615},\n  year={2022}\n}\n\n@article{lewkowycz2022solving,\n  title={Solving quantitative reasoning problems with language models},\n  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and et al.},\n  journal={Ar{X}iv:2206.14858},\n  year={2022}\n}\n\n@article{mnih2015human,\n  title={Human-level control through deep reinforcement learning},\n  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and et al.},\n  journal={{N}ature},\n  volume={518},\n  number={7540},\n  pages={529--533},\n  year={2015},\n  publisher={Nature Publishing Group}\n}\n\n@article{hartley2009global,\n  title={Global optimization through rotation space search},\n  author={Hartley, Richard I and Kahl, Fredrik},\n  journal={International {J}ournal of {C}omputer {V}ision},\n  volume={82},\n  number={1},\n  pages={64--79},\n  year={2009},\n  publisher={Springer}\n}\n\n@inproceedings{wu2018shift,\n  title={Shift: A zero flop, zero parameter alternative to spatial convolutions},\n  author={Wu, Bichen and Wan, Alvin and Yue, Xiangyu and Jin, Peter and Zhao, Sicheng and Golmant, Noah and Gholaminejad, Amir and Gonzalez, Joseph and Keutzer, Kurt},\n  booktitle={Proceedings of the {IEEE} {C}onference on {C}omputer {V}ision and {P}attern {R}ecognition},\n  pages={9127--9135},\n  year={2018}\n}\n\n@article{bommasani2021opportunities,\n  title={On the opportunities and risks of foundation models},\n  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and et al.},\n  journal={Ar{X}iv:2108.07258},\n  year={2021}\n}\n\n@article{schuhmann2022laion,\n  title={{LAION}-5{B}: An open large-scale dataset for training next generation image-text models},\n  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and et al.},\n  journal={Ar{X}iv:2210.08402},\n  year={2022}\n}\n\n@inproceedings{le2013building,\n  title={Building high-level features using large scale unsupervised learning},\n  author={Le, Quoc V},\n  booktitle={Proceedings of the {IEEE} {I}nternational {C}onference on {A}coustics, {S}peech and {S}ignal {P}rocessing},\n  pages={8595--8598},\n  year={2013},\n  organization={IEEE}\n}\n\n@article{olshausen1996emergence,\n  title={Emergence of simple-cell receptive field properties by learning a sparse code for natural images},\n  author={Olshausen, Bruno A and Field, David J},\n  journal={Nature},\n  volume={381},\n  number={6583},\n  pages={607--609},\n  year={1996},\n  publisher={Nature Publishing Group}\n}\n\n@book{Vapnik95,\n  Address =\t {New York},\n  Author =\t {V. Vapnik},\n  Publisher =\t {Springer},\n  Title =\t {The {N}ature of {S}tatistical {L}earning {T}heory},\n  Year =\t 1995\n}\n\n@inproceedings{Novikoff62,\n  Author =\t {A.~B.~J.~Novikoff},\n  Booktitle =\t {Proceedings of the {S}ymposium on the {M}athematical\n                  {T}heory of {A}utomata},\n  Organization = {Polytechnic {I}nstitute of {B}rooklyn},\n  Pages =\t {615-622},\n  Title =\t {On convergence proofs on perceptrons},\n  Year =\t 1962\n}\n\n@article{ba2016layer,\n  title={Layer normalization},\n  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},\n  journal={Ar{X}iv:1607.06450},\n  year={2016}\n}\n\n@article{anil2020scalable,\n  title={Scalable second-order optimization for deep learning},\n  author={Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},\n  journal={Ar{X}iv:2002.09018},\n  year={2020}\n}\n\n@article{prakash2016neural,\n  title={Neural paraphrase generation with stacked residual {LSTM} networks},\n  author={Prakash, Aaditya and Hasan, Sadid A and Lee, Kathy and Datla, Vivek and Qadir, Ashequl and Liu, Joey and Farri, Oladimeji},\n  journal={Ar{X}iv:1610.03098},\n  year={2016}\n}\n\n@article{kim2017residual,\n  title={Residual {LSTM}: Design of a deep recurrent architecture for distant speech recognition},\n  author={Kim, Jaeyoung and El-Khamy, Mostafa and Lee, Jungwon},\n  journal={Ar{X}iv:1701.03360},\n  year={2017}\n}\n\n@article{russakovsky2015imagenet,\n  title={Image{N}et large scale visual recognition challenge},\n  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and et al.},\n  journal={International {J}ournal of {C}omputer {V}ision},\n  volume={115},\n  number={3},\n  pages={211--252},\n  year={2015},\n  publisher={Springer}\n}\n\n@article{forrester2007multi,\n  title={Multi-fidelity optimization via surrogate modelling},\n  author={Forrester, Alexander IJ and S{\\'o}bester, Andr{\\'a}s and Keane, Andy J},\n  journal={Proceedings of the {R}oyal {S}ociety {A}: {M}athematical, {P}hysical and {E}ngineering {S}ciences},\n  volume={463},\n  number={2088},\n  pages={3251--3269},\n  year={2007},\n  publisher={The Royal Society London}\n}\n\n@article{feurer-arxiv22,\n  author = {M. Feurer and B. Letham and F. Hutter and E. Bakshy},\n  title = {Practical transfer learning for {B}ayesian optimization},\n  journal = {Ar{X}iv:1802.02219 [stat.ML]},\n  year = {2022}\n}\n\n@article{wistuba-ml18,\n  author = {M. Wistuba and N. Schilling and L. Schmidt-Thieme},\n  title = {Scalable {G}aussian process-based transfer surrogates for hyperparameter optimization},\n  journal = {Machine {L}earning},\n  volume = {108},\npages = {43--78},\nyear = {2018}\n}\n\n@inproceedings{bardenet-icml13a,\n    author    = {R. Bardenet and M. Brendel and B. K{\\'e}gl and M. Sebag},\n    title     = {Collaborative hyperparameter tuning},\n    booktitle = {Proceedings of the 30th {I}nternational {C}onference on {M}achine {L}earning ({ICML}'13)},\n    year      = {2013},\n}\n\n@InProceedings{jenatton-icml17a,\n  title\t    = {Bayesian optimization with {tree}-structured {dependencies}},\n  author    = {R. Jenatton and C. Archambeau and J. González and M. Seeger},\n  booktitle = {Proceedings of the 34th {I}nternational {C}onference on {M}achine {L}earning ({ICML}'17)},\n  year      = {2017},\n\n}\n\n@article{li-arxiv18,\n title   = {Massively parallel hyperparameter tuning},\n author  = {L. Li and K. Jamieson and A. Rostamizadeh and K. Gonina and M. Hardt and B. Recht and A. Talwalkar},\n year    = {2018},\n journal = {Ar{X}iv:1810.05934}\n}\n\n@article{bellman-science66,\n  title={Dynamic programming},\n  author={R. Bellman},\n  journal={Science},\nvolume ={153},\npages={34--37},\n  year={1966},\n}\n\n@inproceedings{li-iclr17,\n author    = {L. Li and K. Jamieson and G. DeSalvo and A. Rostamizadeh and A. Talwalkar},\n title     = {Hyperband: Bandit-based configuration evaluation for hyperparameter optimization},\n booktitle = {{I}nternational {C}onference on {L}earning {R}epresentations ({ICLR}'17)},\n year      = {2017}\n}\n\n@inproceedings{karnin-icml13,\n  author    = {Z. Karnin and T. Koren and O. Somekh},\n  title     = {Almost optimal exploration in multi-armed bandits},\n  booktitle = {Proceedings of the 30th {I}nternational {C}onference on {M}achine {L}earning ({ICML}'13)},\n  year      = {2013},\n}\n\n@inproceedings{jamieson-aistats16,\n author    = {K. Jamieson and A. Talwalkar},\n title     = {Non-stochastic best arm identification and hyperparameter optimization},\n booktitle = {Proceedings of the 17th {I}nternational {C}onference on {A}rtificial {I}ntelligence and {S}tatistics},\n year      = {2016}\n}\n\n@InProceedings{akiba-sigkdd19,\n title     = {{Optuna}: A next-generation hyperparameter optimization framework},\n author    = {T. Akiba and S. Sano and T. Yanase and T. Ohta and M. Koyama},\n booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {I}nternational {C}onference on {K}nowledge {D}iscovery \\& {D}ata {M}ining},\n year      = {2019}\n}\n\n@article{liaw-arxiv18,\n author  = {R. Liaw and E. Liang and R. Nishihara and P. Moritz and J. Gonzalez and I. Stoica},\n title   = {{Tune}: A research platform for distributed model selection and training},\n journal = {Ar{X}iv:1807.05118},\n year    = {2018}\n}\n\n@InProceedings{salinas-automl22,\n  title     = {Syne {T}une: A library for Large Scale Hyperparameter Tuning and Reproducible Research},\n  author    = {D. Salinas and M. Seeger and A. Klein and V. Perrone and M. Wistuba and C. Archambeau},\n  booktitle = {First {C}onference on {A}utomated {M}achine {L}earning},\n  year      = {2022},\n}\n\n@InProceedings{baptista-icml18a,\n  title = \t {{B}ayesian optimization of combinatorial structures},\n  author =       {R. Baptista and M. Poloczek},\n  booktitle = \t {Proceedings of the 35th {I}nternational {C}onference on {M}achine {L}earning},\n  year = \t {2018},\n}\n\n@inproceedings{hutter-lion11a,\n    author    = {F. Hutter and H. Hoos and K. Leyton-Brown},\n    title     = {Sequential Model-Based Optimization for General Algorithm Configuration},\n    booktitle = {Proceedings of the Fifth {I}nternational {C}onference on {L}earning and {I}ntelligent {O}ptimization ({LION}'11)},\n    year      = {2011}\n}\n\n@inproceedings{franceschi-icml17a,\n  title = \t {Forward and Reverse Gradient-Based Hyperparameter Optimization},\n  author = \t {L. Franceschi and M. Donini and P. Frasconi and M. Pontil},\n  booktitle = \t {Proceedings of the 34th {I}nternational {C}onference on {M}achine {L}earning ({ICML}'17)},\n  year = \t {2017},\n}\n\n@inproceedings{maclaurin-icml15,\n  title = \t {Gradient-based Hyperparameter Optimization through Reversible Learning},\n  author = \t {D. Maclaurin and D. Duvenaud and R. Adams},\n  booktitle = \t {Proceedings of the 32nd {I}nternational {C}onference on {M}achine {L}earning ({ICML}'15)},\n  year = \t {2015}\n}\n\n@Book{hutter-book19a,\n  editor\t= {F. Hutter and L. Kotthoff and J. Vanschoren},\n  publisher\t= {Springer},\n  title\t\t= {Automated {M}achine {L}earning: {M}ethods, {S}ystems, {C}hallenges},\n  year\t\t= {2019}\n}\n\n@article{wistuba-arxiv19,\n  author  = {M. Wistuba and A. Rawat and T. Pedapati},\n  title   = {A survey on neural architecture search},\n  journal = {Ar{X}iv:1905.01392 [cs.LG]},\n  year    = {2019}\n}\n\n@article{elsken-arxiv18a,\n  author  = {T. Elsken and J. H. Metzen and F. Hutter},\n  title   = {Neural architecture search: A sSurvey},\n  journal = {Ar{X}iv:1808.05377 [stat.ML]},\n  year    = {2018}\n}\n\n@incollection{feurer-automlbook18a,\n  author    = {M. Feurer and F. Hutter},\n  title     = {Hyperparameter ptimization},\n  booktitle = {Automatic {M}achine {L}earning: {M}ethods, {S}ystems, {C}hallenges},\n  year      = {2018},\n  publisher = {Springer}\n}\n\n@inproceedings{snoek-nips12,\n  title       = {Practical {B}ayesian Optimization of Machine Learning Algorithms},\n  author      = {Snoek, J. and Larochelle, H. and Adams, R.},\n  booktitle   = {Advances in {N}eural {I}nformation {P}rocessing {S}ystems 25},\n  year        = {2012},\n  pages       = {2951--2959}\n}\n\n@article{bergstra2011algorithms,\n  title={Algorithms for hyper-parameter optimization},\n  author={Bergstra, James and Bardenet, R{\\'e}mi and Bengio, Yoshua and K{\\'e}gl, Bal{\\'a}zs},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={24},\n  year={2011}\n}\n\n@article{beltagy2020longformer,\n  title={Longformer: The long-document transformer},\n  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},\n  journal={Ar{X}iv:2004.05150},\n  year={2020}\n}\n\n@article{gulati2020conformer,\n  title={Conformer: Convolution-augmented Transformer for Speech Recognition},\n  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and et al.},\n  journal={Proc. {I}nterspeech 2020},\n  pages={5036--5040},\n  year={2020}\n}\n\n@article{chen2021decision,\n  title={Decision transformer: Reinforcement learning via sequence modeling},\n  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={34},\n  pages={15084--15097},\n  year={2021}\n}\n\n@article{dwivedi2020generalization,\n  title={A generalization of transformer networks to graphs},\n  author={Dwivedi, Vijay Prakash and Bresson, Xavier},\n  journal={Ar{X}iv:2012.09699},\n  year={2020}\n}\n\n@techreport{wolpert1995no,\n  title={No free lunch theorems for search},\n  author={Wolpert, David H and Macready, William G},\n  year={1995},\n  institution={Technical {R}eport SFI-TR-95-02-010, {S}anta {F}e {I}nstitute}\n}\n\n@inproceedings{rezende2015variational,\n  title={Variational inference with normalizing flows},\n  author={Rezende, Danilo and Mohamed, Shakir},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={1530--1538},\n  year={2015},\n  organization={PMLR}\n}\n\n@inproceedings{sohl2015deep,\n  title={Deep unsupervised learning using nonequilibrium thermodynamics},\n  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={2256--2265},\n  year={2015},\n  organization={PMLR}\n}\n\n@article{ho2020denoising,\n  title={Denoising diffusion probabilistic models},\n  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={33},\n  pages={6840--6851},\n  year={2020}\n}\n\n@article{song2019generative,\n  title={Generative modeling by estimating gradients of the data distribution},\n  author={Song, Yang and Ermon, Stefano},\n  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},\n  volume={32},\n  year={2019}\n}\n\n@article{BellmanDPPaper,\nauthor = {Richard Bellman},\ntitle = {On the Theory of Dynamic Programming},\njournal = {Proceedings of the {N}ational {A}cademy of {S}ciences},\nvolume = {38},\nnumber = {8},\npages = {716--719},\nyear = {1952}\n}\n\n@article{BellmanMDP,\n ISSN = {00959057, 19435274},\n URL = {http://www.jstor.org/stable/24900506},\n author = {Richard Bellman},\n journal = {Journal of {M}athematics and {M}echanics},\n number = {5},\n pages = {679--684},\n publisher = {Indiana University Mathematics Department},\n title = {A {M}arkovian Decision Process},\n urldate = {2022-11-28},\n volume = {6},\n year = {1957}\n}\n\n@book{BellmanDPBook,\n  isbn={9780486317199},\n  publisher = {Dover Publications},\n  title = {{Dynamic Programming}},\n  year = 1957,\n  author = {Richard Bellman},\n  series={Dover},\n}\n\n@article{mnih2013playing,\n\tAuthor = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},\n\tJournal = {Ar{X}iv:1312.5602},\n\tTitle = {Playing {A}tari with deep reinforcement learning},\n\tYear = {2013}\n\t}\n\n@article{ouyang2022training,\n  title={Training language models to follow instructions with human feedback},\n  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and et al.},\n  journal={Ar{X}iv:2203.02155},\n  year={2022}\n}\n\n@article{wei2022chain,\n  title={Chain of thought prompting elicits reasoning in large language models},\n  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},\n  journal={Ar{X}iv:2201.11903},\n  year={2022}\n}\n\n@InProceedings{zhang2023automatic,\n  title\t\t= {Automatic chain of thought prompting in large language models},\n  author\t= {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},\n  booktitle\t= {{I}nternational {C}onference on {L}earning {R}epresentations},\n  year\t\t= {2023}\n}\n\n@inproceedings{wang2022removing,\n  title={Removing batch normalization boosts adversarial training},\n  author={Wang, Haotao and Zhang, Aston and Zheng, Shuai and Shi, Xingjian and Li, Mu and Wang, Zhangyang},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={23433--23445},\n  year={2022},\n  organization={PMLR}\n}\n\n@article{zhang2023multicot,\n  title={Multimodal Chain-of-Thought Reasoning in Language Models},\n  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},\n  journal={Ar{X}iv:2302.00923},\n  year={2023}\n}\n\n@article{kojima2022large,\n author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},\n journal = {arxiv.org/abs/2205.11916},\n title = {Large Language Models are Zero-Shot Reasoners},\n year = {2022}\n}\n\n@InProceedings{wang2023self,\n  title\t\t= {Self-Consistency Improves Chain of Thought Reasoning in Language Models},\n  author\t= {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},\n  booktitle\t= {{I}nternational {C}onference on {L}earning {R}epresentations},\n  year\t\t= {2023}\n}\n\n@InProceedings{zhou2023least,\n  title\t\t= {Least-to-most prompting enables complex reasoning in large language models},\n  author\t= {Zhou, Denny and Sch{\\\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},\n  booktitle\t= {{I}nternational {C}onference on {L}earning {R}epresentations},\n  year\t\t= {2023}\n}\n\n@inproceedings{rezende2014stochastic,\n  title={Stochastic backpropagation and approximate inference in deep generative models},\n  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},\n  booktitle={{I}nternational {C}onference on {M}achine {L}earning},\n  pages={1278--1286},\n  year={2014},\n  organization={PMLR}\n}\n\n@InProceedings{song2021score,\n  title\t\t= {Score-Based Generative Modeling through Stochastic Differential Equations},\n  author\t= {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},\n  booktitle\t= {{I}nternational {C}onference on {L}earning {R}epresentations},\n  year\t\t= {2021}\n}\n\n@article{dinh2014nice,\n  title={{NICE}: Non-linear independent components estimation},\n  author={Dinh, Laurent and Krueger, David and Bengio, Yoshua},\n  journal={Ar{X}iv:1410.8516},\n  year={2014}\n}\n\n@InProceedings{dinh2017density,\n  title\t\t= {Density estimation using Real {NVP}},\n  author\t= {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},\n  booktitle\t= {{I}nternational {C}onference on {L}earning {R}epresentations},\n  year\t\t= {2017}\n}\n\n@article{openai2023gpt4,\n  title={{GPT}-4 {T}echnical {R}eport},\n  author={Open{AI}},\n  journal={Ar{X}iv:2303.08774},\n  year={2023}\n}\n\n@article{anil2023palm,\n  title={Pa{LM} 2 {T}echnical {R}eport},\n  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and et al.},\n  journal={Ar{X}iv:2305.10403},\n  year={2023}\n}\n\n@article{wei2021finetuned,\n  title={Finetuned language models are zero-shot learners},\n  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},\n  journal={Ar{X}iv:2109.01652},\n  year={2021}\n}\n\n@article{sanh2021multitask,\n  title={Multitask prompted training enables zero-shot task generalization},\n  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and et al.},\n  journal={Ar{X}iv:2110.08207},\n  year={2021}\n}\n\n@article{bai2022constitutional,\n  title={Constitutional {AI}: Harmlessness from {AI} feedback},\n  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and et al.},\n  journal={Ar{X}iv:2212.08073},\n  year={2022}\n}\n\n@article{taylor2022galactica,\n  title={Galactica: A large language model for science},\n  author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},\n  journal={Ar{X}iv:2211.09085},\n  year={2022}\n}\n\n@article{touvron2023llama,\n  title={L{L}a{MA}: Open and efficient foundation language models},\n  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and et al.},\n  journal={Ar{X}iv:2302.13971},\n  year={2023a}\n}\n\n@article{touvron2023llama2,\n  title={L{L}a{MA} 2: Open foundation and fine-tuned chat models},\n  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and et al.},\n  journal={Ar{X}iv:2307.09288},\n  year={2023b}\n}\n\n@article{qin2023chatgpt,\n  title={Is {C}hat{GPT} a general-purpose natural language processing task solver?},\n  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},\n  journal={Ar{X}iv:2302.06476},\n  year={2023}\n}\n\n@article{scao2022bloom,\n  title={{BLOOM}: A 176{B}-parameter open-access multilingual language model},\n  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\\'c}, Suzana and Hesslow, Daniel and Castagn{\\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\\c{c}}ois and Gall{\\'e}, Matthias and et al.},\n  journal={Ar{X}iv:2211.05100},\n  year={2022}\n}\n\n@article{penedo2023refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},\n  journal={Ar{X}iv:2306.01116},\n  year={2023}\n}\n"
        },
        {
          "name": "d2l",
          "type": "tree",
          "content": null
        },
        {
          "name": "graffle",
          "type": "tree",
          "content": null
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "index.md",
          "type": "blob",
          "size": 1.1630859375,
          "content": "Dive into Deep Learning\n========================\n\n```eval_rst\n.. raw:: html\n   :file: frontpage.html\n```\n\n\n```toc\n:maxdepth: 1\n\nchapter_preface/index\nchapter_installation/index\nchapter_notation/index\n```\n\n\n```toc\n:maxdepth: 2\n:numbered:\n\nchapter_introduction/index\nchapter_preliminaries/index\nchapter_linear-regression/index\nchapter_linear-classification/index\nchapter_multilayer-perceptrons/index\nchapter_builders-guide/index\nchapter_convolutional-neural-networks/index\nchapter_convolutional-modern/index\nchapter_recurrent-neural-networks/index\nchapter_recurrent-modern/index\nchapter_attention-mechanisms-and-transformers/index\nchapter_optimization/index\nchapter_computational-performance/index\nchapter_computer-vision/index\nchapter_natural-language-processing-pretraining/index\nchapter_natural-language-processing-applications/index\nchapter_reinforcement-learning/index\nchapter_gaussian-processes/index\nchapter_hyperparameter-optimization/index\nchapter_generative-adversarial-networks/index\nchapter_recommender-systems/index\nchapter_appendix-mathematics-for-deep-learning/index\nchapter_appendix-tools-for-deep-learning/index\n\n```\n\n\n```toc\n:maxdepth: 1\n\nchapter_references/zreferences\n```\n\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.552734375,
          "content": "from setuptools import setup, find_packages\nimport d2l\n\nrequirements = [\n    'jupyter==1.0.0',\n    'numpy==1.23.5',\n    'matplotlib==3.7.2',\n    'matplotlib-inline==0.1.6',\n    'requests==2.31.0',\n    'pandas==2.0.3',\n    'scipy==1.10.1'\n]\n\nsetup(\n    name='d2l',\n    version=d2l.__version__,\n    python_requires='>=3.8',\n    author='D2L Developers',\n    author_email='d2l.devs@gmail.com',\n    url='https://d2l.ai',\n    description='Dive into Deep Learning',\n    license='MIT-0',\n    packages=find_packages(),\n    zip_safe=True,\n    install_requires=requirements,\n)\n"
        },
        {
          "name": "static",
          "type": "tree",
          "content": null
        },
        {
          "name": "vol1-activate.sh",
          "type": "blob",
          "size": 0.15625,
          "content": "cp static/vol1/vol1-config.ini config.ini\ncp static/vol1/root-index.md index.md\ncp static/vol1/tools-index.md chapter_appendix-tools-for-deep-learning/index.md\n"
        }
      ]
    }
  ]
}