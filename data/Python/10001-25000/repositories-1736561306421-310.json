{
  "metadata": {
    "timestamp": 1736561306421,
    "page": 310,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/tvm",
      "stars": 11913,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 2.2958984375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\ngithub:\n  description: \"Open deep learning compiler stack for cpu, gpu and specialized accelerators\"\n  homepage: https://tvm.apache.org/\n  labels:\n    - tvm\n    - compiler\n    - tensor\n    - deep-learning\n    - gpu\n    - opencl\n    - metal\n    - performance\n    - javascript\n    - rocm\n    - vulkan\n    - spirv\n    - machine-learning\n\n  # Triage perm for collaborators(test run)\n  #\n  # The perm is given based on needs and not based on\n  # evaluation of past contributions. The rationale\n  # is that people may need the permission to start\n  # contributing in this way. It serves to diversify\n  # the ways to contribute.\n  #\n  # There is a limited number of slots. To enable broad\n  # participation, permission is given on a three month\n  # cycle. PMC may review and recycle slots when necessary.\n  collaborators:\n    - hpanda-naut\n    - denise-k\n    - janetsc\n    - naut-thomas\n    - tvm-bot  # For automated feedback in PR review.\n\n  # See https://cwiki.apache.org/confluence/display/INFRA/Git+-+.asf.yaml+features#Git.asf.yamlfeatures-Branchprotection\n  protected_branches:\n    main:\n      required_status_checks:\n        contexts:\n          - unity/pr-head\n          - arm/pr-head\n          - cortexm/pr-head\n          - cpu/pr-head\n          - docker/pr-head\n          - gpu/pr-head\n          - hexagon/pr-head\n          - i386/pr-head\n          - lint/pr-head\n          - minimal/pr-head\n          - riscv/pr-head\n          - wasm/pr-head\n          - cross-isa-minimal/pr-head\n\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n"
        },
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.28515625,
          "content": "# Run the following command to reformat a file:\n# clang-format -i -style=Google <file>\n# Or use clang-format-diff to only reformat the changed lines:\n# https://clang.llvm.org/docs/ClangFormat.html\nBasedOnStyle: Google\nDerivePointerAlignment: false\nColumnLimit:     100\nPointerAlignment: Left\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0810546875,
          "content": "Jenkinsfile linguist-generated=true\nci/jenkins/generated/* linguist-generated=true\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.0615234375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n*.S\n# C extensions\n*.so\n*.ll\n.npm\n# Distribution / packaging\n.Python\nenv/\nbuild/\nbuild-*/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n.conda/\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Generated by python/gen_requirements.py\npython/requirements/*.txt\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/_staging/\n\n# PyBuilder\n/target/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n*~\n*.pyc\n*~\nconfig.mk\nconfig.cmake\nWin32\n*.dir\nperf\n*.wasm\n.emscripten\n\n## IOS\nDerivedData/\n\n## Java\n*.class\njvm/*/target/\njvm/*/*/target/\njvm/native/*/generated\njvm/native/src/main/native/org_apache_tvm_native_c_api.h\n*.worksheet\n*.idea\n*.iml\n*.classpath\n*.project\n*.settings\n*/node_modules/\n\n## Various settings\n*.pbxuser\n!default.pbxuser\n*.mode1v3\n!default.mode1v3\n*.mode2v3\n!default.mode2v3\n*.perspectivev3\n!default.perspectivev3\nxcuserdata/\n.pkl_memoize_*\n\n.emscripten*\n.m2\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n## Other\n*.moved-aside\n*.xccheckout\n*.xcscmblueprint\n.DS_Store\ntags\ncscope*\n*.lock\n\n# vim temporary files\n*.swp\n*.swo\n\n# TVM generated code\nperf\n.bash_history\n*.json\n*.params\n*.ro\n*.onnx\n*.h5\nsynset.txt\ncat.jpg\ncat.png\ndocs.tgz\ncat.png\n*.mlmodel\ntvm_u.*\ntvm_t.*\n# Mac OS X\n.DS_Store\n\n# Jetbrain\n.idea\n.ipython\n.jupyter\n.nv\n.pylint.d\n.python_history\n.pytest_cache\n.local\ncmake-build-debug\n\n# Visual Studio\n.vs\n\n# Visual Studio Code\n.vscode\n\n# tmp file\n.nfs*\n\n# keys\n*.pem\n*.p12\n*.pfx\n*.cer\n*.crt\n*.der\n\n# patch sentinel\npatched.txt\n\n# Python type checking\n.mypy_cache/\n.pyre/\n\n# pipenv files\nPipfile\nPipfile.lock\n\n# conda package artifacts\nconda/Dockerfile.cuda*\nconda/pkg\n.node_repl_history\n# nix files\n.envrc\n*.nix\n\n# Docker files\n.sudo_as_admin_successful\n\n# Downloaded models/datasets\n.tvm_test_data\n.dgl\n.caffe2\n\n# Local docs build\n_docs/\njvm/target\n.config/configstore/\n.ci-py-scripts/\n\n# Generated Hexagon files\nsrc/runtime/hexagon/rpc/hexagon_rpc.h\nsrc/runtime/hexagon/rpc/hexagon_rpc_skel.c\nsrc/runtime/hexagon/rpc/hexagon_rpc_stub.c\n\n# Local tvm-site checkout\ntvm-site/\n\n# Test sample data files\n!tests/python/ci/sample_prs/*.json\n\n# Used in CI to communicate between Python and Jenkins\n.docker-image-names/\n\n# Printed TIR code on disk\n*.tir\n\n# GDB history file\n.gdb_history\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 1.068359375,
          "content": "[submodule \"dmlc-core\"]\n\tpath = 3rdparty/dmlc-core\n\turl = https://github.com/dmlc/dmlc-core.git\n[submodule \"dlpack\"]\n\tpath = 3rdparty/dlpack\n\turl = https://github.com/dmlc/dlpack.git\n[submodule \"3rdparty/rang\"]\n\tpath = 3rdparty/rang\n\turl = https://github.com/agauniyal/rang.git\n[submodule \"3rdparty/libbacktrace\"]\n\tpath = 3rdparty/libbacktrace\n\turl = https://github.com/tlc-pack/libbacktrace.git\n[submodule \"3rdparty/cutlass\"]\n\tpath = 3rdparty/cutlass\n\turl = https://github.com/NVIDIA/cutlass.git\n[submodule \"3rdparty/OpenCL-Headers\"]\n\tpath = 3rdparty/OpenCL-Headers\n\turl = https://github.com/KhronosGroup/OpenCL-Headers.git\n[submodule \"3rdparty/cnpy\"]\n\tpath = 3rdparty/cnpy\n\turl = https://github.com/rogersce/cnpy.git\n[submodule \"3rdparty/cutlass_fpA_intB_gemm\"]\n\tpath = 3rdparty/cutlass_fpA_intB_gemm\n\turl = https://github.com/tlc-pack/cutlass_fpA_intB_gemm\n[submodule \"3rdparty/libflash_attn\"]\n\tpath = 3rdparty/libflash_attn\n\turl = https://github.com/tlc-pack/libflash_attn\n[submodule \"3rdparty/flashinfer\"]\n\tpath = 3rdparty/flashinfer\n\turl = https://github.com/flashinfer-ai/flashinfer.git\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.857421875,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Pre-commit hook\n# See documentation at: https://pre-commit.com/\n#\n# Pre-commit hook to run the sanity checks from Jenkins locally.\n#\n# Requirements:\n#   - How to configure:\n#        - $ pip install pre-commit\n#        - $ pre-commit install --hook-type pre-push\n#   - How to prevent running it:\n#        - git options: --no-verify or -n\n#        - $ git commit -n -m \"YOUR COMMIT MESSAGE\"\n#   - How to run it as standalone\n#        - $ pre-commit run\n#\n\ndefault_language_version:\n    python: python3.6\nfail_fast: True\ndefault_stages: [push]\nrepos:\n    - repo: https://github.com/pre-commit/pre-commit-hooks\n      rev: v2.3.0\n      hooks:\n        - id: check-added-large-files\n        - id: check-merge-conflict\n        - id: check-yaml\n        - id: end-of-file-fixer\n          stages: [push]\n        - id: trailing-whitespace\n          stages: [push]\n    - repo: local\n      hooks:\n        -   id: run-black\n            name: Running Black...\n            entry: docker/lint.sh python_format\n            language: system\n            always_run: true\n            pass_filenames: false\n        -   id: run-file-checks\n            name: Checking File Types....\n            entry: docker/lint.sh file_type\n            language: system\n            always_run: true\n            pass_filenames: false\n        -   id: run-headers-check\n            name: Checking ASF License Headers ...\n            entry: docker/lint.sh asf\n            language: system\n            always_run: true\n            pass_filenames: false\n        -   id: run-headers-check\n            name: Linting the C++ code ...\n            entry: docker/lint.sh cpplint\n            language: system\n            always_run: true\n            pass_filenames: false\n        -   id: run-clang-format\n            name: Checking Clang format ...\n            entry: docker/lint.sh clang_format\n            language: system\n            always_run: true\n            pass_filenames: false\n        -   id: run-mypy\n            name: Type Checking with MyPY ...\n            entry: docker/lint.sh mypy\n            language: system\n            always_run: true\n            pass_filenames: false\n"
        },
        {
          "name": "3rdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 39.2880859375,
          "content": "cmake_minimum_required(VERSION 3.18)\nproject(tvm C CXX)\n\n# Utility functions\ninclude(cmake/utils/Utils.cmake)\ninclude(cmake/utils/Summary.cmake)\ninclude(cmake/utils/Linker.cmake)\ninclude(cmake/utils/FindCUDA.cmake)\ninclude(cmake/utils/FindNCCL.cmake)\ninclude(cmake/utils/FindOpenCL.cmake)\ninclude(cmake/utils/FindVulkan.cmake)\ninclude(cmake/utils/FindLLVM.cmake)\ninclude(cmake/utils/FindROCM.cmake)\ninclude(cmake/utils/FindRCCL.cmake)\ninclude(cmake/utils/FindNVSHMEM.cmake)\n\nif(EXISTS ${CMAKE_BINARY_DIR}/config.cmake)\n  include(${CMAKE_BINARY_DIR}/config.cmake)\nelse()\n  if(EXISTS ${CMAKE_SOURCE_DIR}/config.cmake)\n    include(${CMAKE_SOURCE_DIR}/config.cmake)\n  endif()\nendif()\n\n# NOTE: do not modify this file to change option values.\n# You can create a config.cmake at build folder\n# and add set(OPTION VALUE) to override these build options.\n# Alernatively, use cmake -DOPTION=VALUE through command-line.\ntvm_option(USE_CUDA \"Build with CUDA\" OFF)\ntvm_option(USE_NCCL \"Build with NCCL\" OFF)\ntvm_option(USE_MSCCL \"Build with MSCCL\" OFF)\ntvm_option(USE_OPENCL \"Build with OpenCL\" OFF)\ntvm_option(USE_OPENCL_ENABLE_HOST_PTR \"Enable OpenCL memory object access to host\" OFF)\ntvm_option(USE_OPENCL_GTEST \"Path to OpenCL specific gtest version for runtime cpp tests.\" /path/to/opencl/gtest)\ntvm_option(USE_VULKAN \"Build with Vulkan\" OFF)\n\n\n# Whether to use spirv-tools.and SPIRV-Headers from Khronos github or gitlab.\n#\n# Possible values:\n# - OFF: not to use\n# - /path/to/install: path to your khronis spirv-tools and SPIRV-Headers installation directory\n#\ntvm_option(USE_KHRONOS_SPIRV \"Whether to use spirv-tools.and SPIRV-Headers from Khronos github or gitlab\" OFF)\ntvm_option(USE_SPIRV_KHR_INTEGER_DOT_PRODUCT \"whether enable SPIRV_KHR_DOT_PRODUCT\" OFF)\ntvm_option(USE_METAL \"Build with Metal\" OFF)\ntvm_option(USE_ROCM \"Build with ROCM\" OFF)\ntvm_option(USE_RCCL \"Build with RCCL\" OFF)\ntvm_option(ROCM_PATH \"The path to rocm\" /opt/rocm)\ntvm_option(USE_HEXAGON \"Build with Hexagon support\" OFF)\ntvm_option(USE_HEXAGON_SDK \"Path to the Hexagon SDK root (required for Hexagon support)\" /path/to/sdk)\ntvm_option(USE_HEXAGON_RPC \"Enable Hexagon RPC using minRPC implementation over Android.\" OFF)\ntvm_option(USE_HEXAGON_GTEST \"Path to Hexagon specific gtest version for runtime cpp tests.\" /path/to/hexagon/gtest)\ntvm_option(USE_HEXAGON_EXTERNAL_LIBS \"Path to git repo containing external Hexagon runtime sources or libraries\" OFF)\ntvm_option(USE_RPC \"Build with RPC\" ON)\ntvm_option(USE_THREADS \"Build with thread support\" ON)\ntvm_option(USE_LLVM \"Build with LLVM, can be set to specific llvm-config path\" OFF)\ntvm_option(USE_MLIR \"Build with MLIR support\" OFF)\ntvm_option(USE_STACKVM_RUNTIME \"Include stackvm into the runtime\" OFF)\ntvm_option(USE_GRAPH_EXECUTOR \"Build with tiny graph executor\" ON)\ntvm_option(USE_GRAPH_EXECUTOR_CUDA_GRAPH \"Build with tiny graph executor with CUDA Graph for GPUs\" OFF)\ntvm_option(USE_AOT_EXECUTOR \"Build with AOT executor\" ON)\ntvm_option(USE_PROFILER \"Build profiler for the VM and graph executor\" ON)\ntvm_option(USE_OPENMP \"Build with OpenMP thread pool implementation\" OFF)\ntvm_option(USE_RELAY_DEBUG \"Building Relay in debug mode...\" OFF)\ntvm_option(TVM_DEBUG_WITH_ABI_CHANGE \"Enable debug code that may cause ABI changes\" OFF)\ntvm_option(TVM_LOG_BEFORE_THROW \"Whether log before throw, for debugging purposes\" OFF)\ntvm_option(USE_RTTI \"Build with RTTI\" ON)\ntvm_option(USE_MSVC_MT \"Build with MT\" OFF)\ntvm_option(INSTALL_DEV \"Install compiler infrastructure\" OFF)\ntvm_option(HIDE_PRIVATE_SYMBOLS \"Compile with -fvisibility=hidden.\" OFF)\ntvm_option(USE_TF_TVMDSOOP \"Build with TensorFlow TVMDSOOp\" OFF)\ntvm_option(USE_PT_TVMDSOOP \"Build with PyTorch TVMDSOOp\" OFF)\ntvm_option(USE_FALLBACK_STL_MAP \"Use TVM's POD compatible Map\" OFF)\ntvm_option(INDEX_DEFAULT_I64 \"Defaults the index datatype to int64\" ON)\ntvm_option(USE_LIBBACKTRACE \"Use libbacktrace to supply linenumbers on stack traces\" AUTO)\ntvm_option(BACKTRACE_ON_SEGFAULT \"Install a signal handler to print a backtrace on segfault\" OFF)\ntvm_option(BUILD_STATIC_RUNTIME \"Build static version of libtvm_runtime\" OFF)\ntvm_option(BUILD_DUMMY_LIBTVM \"Build a dummy version of libtvm\" OFF)\ntvm_option(USE_PAPI \"Use Performance Application Programming Interface (PAPI) to read performance counters\" OFF)\ntvm_option(USE_GTEST \"Use GoogleTest for C++ sanity tests\" AUTO)\ntvm_option(USE_CUSTOM_LOGGING \"Use user-defined custom logging, tvm::runtime::detail::LogFatalImpl and tvm::runtime::detail::LogMessageImpl must be implemented\" OFF)\ntvm_option(USE_ALTERNATIVE_LINKER \"Use 'mold' or 'lld' if found when invoking compiler to link artifact\" AUTO)\ntvm_option(USE_CCACHE \"Use ccache if found when invoking compiler\" AUTO)\n\n# 3rdparty libraries\ntvm_option(DLPACK_PATH \"Path to DLPACK\" \"3rdparty/dlpack/include\")\ntvm_option(DMLC_PATH \"Path to DMLC\" \"3rdparty/dmlc-core/include\")\ntvm_option(RANG_PATH \"Path to RANG\" \"3rdparty/rang/include\")\ntvm_option(COMPILER_RT_PATH \"Path to COMPILER-RT\" \"3rdparty/compiler-rt\")\ntvm_option(PICOJSON_PATH \"Path to PicoJSON\" \"3rdparty/picojson\")\n\n# Contrib library options\ntvm_option(USE_BYODT_POSIT \"Build with BYODT software emulated posit custom datatype\" OFF)\ntvm_option(USE_BLAS \"The blas library to be linked\" none)\ntvm_option(USE_AMX \"Enable Intel AMX\" OFF)\ntvm_option(USE_MKL \"MKL root path when use MKL blas\" OFF)\ntvm_option(USE_DNNL \"Enable DNNL codegen\" OFF)\ntvm_option(USE_CUDNN \"Build with cuDNN\" OFF)\ntvm_option(USE_CUBLAS \"Build with cuBLAS\" OFF)\ntvm_option(USE_NVTX \"Build with NVTX\" OFF)\ntvm_option(USE_CUTLASS \"Build with CUTLASS\" OFF)\ntvm_option(USE_THRUST \"Build with Thrust\" OFF)\ntvm_option(USE_CURAND \"Build with cuRAND\" OFF)\ntvm_option(USE_MIOPEN \"Build with ROCM:MIOpen\" OFF)\ntvm_option(USE_ROCBLAS \"Build with ROCM:RoCBLAS\" OFF)\ntvm_option(USE_HIPBLAS \"Build with ROCM:HIPBLAS\" OFF)\ntvm_option(USE_SORT \"Build with sort support\" ON)\ntvm_option(USE_NNPACK \"Build with nnpack support\" OFF)\ntvm_option(USE_LIBTORCH \"Build with libtorch support\" OFF)\ntvm_option(USE_RANDOM \"Build with random support\" ON)\ntvm_option(USE_CPP_RPC \"Build CPP RPC\" OFF)\ntvm_option(USE_IOS_RPC \"Build iOS RPC\" OFF)\ntvm_option(USE_TFLITE \"Build with tflite support\" OFF)\ntvm_option(USE_TENSORFLOW_PATH \"TensorFlow root path when use TFLite\" none)\ntvm_option(USE_COREML \"Build with coreml support\" OFF)\ntvm_option(USE_BNNS \"Build with BNNS support\" OFF)\ntvm_option(USE_TARGET_ONNX \"Build with ONNX Codegen support\" OFF)\ntvm_option(USE_ARM_COMPUTE_LIB \"Build with Arm Compute Library\" OFF)\ntvm_option(USE_ARM_COMPUTE_LIB_GRAPH_EXECUTOR \"Build with Arm Compute Library graph executor\" OFF)\ntvm_option(USE_TENSORRT_CODEGEN \"Build with TensorRT Codegen support\" OFF)\ntvm_option(USE_TENSORRT_RUNTIME \"Build with TensorRT runtime\" OFF)\ntvm_option(USE_NNAPI_CODEGEN \"Build with NNAPI Codegen support\" OFF)\ntvm_option(USE_NNAPI_RUNTIME \"Build with NNAPI runtime\" OFF)\ntvm_option(USE_RUST_EXT \"Build with Rust based compiler extensions, STATIC, DYNAMIC, or OFF\" OFF)\ntvm_option(USE_VITIS_AI \"Build with VITIS-AI Codegen support\" OFF)\ntvm_option(SUMMARIZE \"Print CMake option summary after configuring\" OFF)\ntvm_option(USE_CLML \"Build with CLML Codegen support\" OFF)\ntvm_option(USE_CLML_GRAPH_EXECUTOR \"Build with CLML graph runtime\" OFF)\ntvm_option(USE_UMA \"Build with UMA support\" OFF)\ntvm_option(USE_VERILATOR \"Build with Verilator support\" OFF)\ntvm_option(USE_MSC \"Enable Multi-System Compiler\" OFF)\ntvm_option(USE_MRVL \"Build with MRVL TVM support\" OFF)\ntvm_option(USE_NVSHMEM \"Build with NVSHMEM support\" OFF)\n\n# include directories\ninclude_directories(${CMAKE_INCLUDE_PATH})\ninclude_directories(\"include\")\ninclude_directories(SYSTEM ${DLPACK_PATH})\ninclude_directories(SYSTEM ${DMLC_PATH})\ninclude_directories(SYSTEM ${RANG_PATH})\ninclude_directories(SYSTEM ${COMPILER_RT_PATH})\ninclude_directories(SYSTEM ${PICOJSON_PATH})\n\n# initial variables\nset(TVM_LINKER_LIBS \"\")\nset(TVM_RUNTIME_LINKER_LIBS \"\")\n\n\n# Check if this is being run on its own or as a subdirectory for another project\n# If we update to CMake 2.21+, we can use PROJECT_IS_TOP_LEVEL instead\nget_directory_property(IS_SUBPROJECT PARENT_DIRECTORY)\n\nif(NOT IS_SUBPROJECT AND NOT DEFINED \"${CMAKE_EXPORT_COMPILE_COMMANDS}\")\n  # If not set manually, change the default to ON\n  set(CMAKE_EXPORT_COMPILE_COMMANDS ON)\nendif()\n\nif(TVM_LOG_BEFORE_THROW)\n  # log error before throw as\n  # when system have issues with stack trace\n  add_definitions(-DDMLC_LOG_BEFORE_THROW=1)\nendif()\n\n# Generic compilation options\nif(MSVC)\n  add_definitions(-DWIN32_LEAN_AND_MEAN)\n  add_definitions(-D_CRT_SECURE_NO_WARNINGS)\n  add_definitions(-D_SCL_SECURE_NO_WARNINGS)\n  add_definitions(-D_ENABLE_EXTENDED_ALIGNED_STORAGE)\n  add_definitions(-DNOMINMAX)\n\n  # regeneration does not work well with msbuild custom rules.\n  set(CMAKE_SUPPRESS_REGENERATION ON)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /EHsc\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /MP\")\n  add_compile_options(/bigobj)\n\n  # Use standard-conforming two-phase name resolution for templates.\n  # This minimizes the differences between g++/clang builds on Linux,\n  # and MSVC builds on Windows.\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /permissive-\")\n\n  # MSVC already errors on undefined symbols, no additional flag needed.\n  set(TVM_NO_UNDEFINED_SYMBOLS \"\")\n\n  if(USE_MSVC_MT)\n    foreach(flag_var\n        CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_DEBUG CMAKE_CXX_FLAGS_RELEASE\n        CMAKE_CXX_FLAGS_MINSIZEREL CMAKE_CXX_FLAGS_RELWITHDEBINFO)\n      if(${flag_var} MATCHES \"/MD\")\n        string(REGEX REPLACE \"/MD\" \"/MT\" ${flag_var} \"${${flag_var}}\")\n      endif(${flag_var} MATCHES \"/MD\")\n    endforeach(flag_var)\n    # Static linking. cmake behavior changed in 3.15 making this necessary.\n    add_compile_options(/MT)\n  endif()\n  # Disable common MSVC warnings\n  # Integer conversion warnings(e.g. int64 to int)\n  add_compile_options(/wd4244)\n  add_compile_options(/wd4267)\n  # Signed unsigned constant comparison\n  add_compile_options(/wd4018)\n  # Aligned alloc may not met(need c++17)\n  add_compile_options(/wd4316)\n  # unreferenced local variables(usually in exception catch)\n  add_compile_options(/wd4101)\n  # always inline keyword not necessary\n  add_compile_options(/wd4180)\n  # DLL interface warning in c++\n  add_compile_options(/wd4251)\n  # destructor was implicitly defined as deleted\n  add_compile_options(/wd4624)\n  # unary minus operator applied to unsigned type, result still unsigned\n  add_compile_options(/wd4146)\n  # 'inline': used more than once\n  add_compile_options(/wd4141)\n  # unknown pragma\n  add_compile_options(/wd4068)\nelse(MSVC)\n  set(WARNING_FLAG -Wall)\n  if (\"${CMAKE_BUILD_TYPE}\" STREQUAL \"Debug\")\n    message(STATUS \"Build in Debug mode\")\n    set(CMAKE_C_FLAGS \"-O0 -g ${WARNING_FLAG} -fPIC ${CMAKE_C_FLAGS}\")\n    set(CMAKE_CXX_FLAGS \"-O0 -g ${WARNING_FLAG} -fPIC ${CMAKE_CXX_FLAGS}\")\n    set(CMAKE_CUDA_FLAGS \"-O0 -g -Xcompiler=-Wall -Xcompiler=-fPIC ${CMAKE_CUDA_FLAGS}\")\n  else()\n    set(CMAKE_C_FLAGS \"-O2 ${WARNING_FLAG} -fPIC ${CMAKE_C_FLAGS}\")\n    set(CMAKE_CXX_FLAGS \"-O2 ${WARNING_FLAG} -fPIC ${CMAKE_CXX_FLAGS}\")\n    set(CMAKE_CUDA_FLAGS \"-O2 -Xcompiler=-Wall -Xcompiler=-fPIC ${CMAKE_CUDA_FLAGS}\")\n    set(TVM_VISIBILITY_FLAG \"\")\n    if (HIDE_PRIVATE_SYMBOLS)\n      message(STATUS \"Hide private symbols...\")\n      set(TVM_VISIBILITY_FLAG \"-fvisibility=hidden\")\n    endif(HIDE_PRIVATE_SYMBOLS)\n  endif ()\n  if (CMAKE_CXX_COMPILER_ID MATCHES \"GNU|Clang\" AND\n      CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 7.0)\n    set(CMAKE_CXX_FLAGS \"-faligned-new ${CMAKE_CXX_FLAGS}\")\n  endif()\n\n  # ld option to warn if symbols are undefined (e.g. libtvm_runtime.so\n  # using symbols only present in libtvm.so).  Not needed for MSVC,\n  # since this is already the default there.\n  if(${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\" OR ${CMAKE_SYSTEM_NAME} MATCHES \"iOS\")\n    set(TVM_NO_UNDEFINED_SYMBOLS \"-Wl,-undefined,error\")\n  else()\n    set(TVM_NO_UNDEFINED_SYMBOLS \"-Wl,--no-undefined\")\n  endif()\n  message(STATUS \"Forbidding undefined symbols in shared library, using ${TVM_NO_UNDEFINED_SYMBOLS} on platform ${CMAKE_SYSTEM_NAME}\")\n\n  # Detect if we're compiling for Hexagon.\n  set(TEST_FOR_HEXAGON_CXX\n      \"#ifndef __hexagon__\"\n      \"#error\"\n      \"#endif\"\n      \"int main() {}\"\n      # Define _start_main to avoid linking errors with -fPIC.\n      \"extern \\\"C\\\" void _start_main() {}\")\n  set(TEST_FOR_HEXAGON_DIR\n      \"${CMAKE_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/CMakeTmp\")\n  set(TEST_FOR_HEXAGON_FILE \"${TEST_FOR_HEXAGON_DIR}/test_for_hexagon.cc\")\n  string(REPLACE \";\" \"\\n\" TEST_FOR_HEXAGON_CXX_TEXT \"${TEST_FOR_HEXAGON_CXX}\")\n  file(WRITE \"${TEST_FOR_HEXAGON_FILE}\" \"${TEST_FOR_HEXAGON_CXX_TEXT}\")\n  try_compile(BUILD_FOR_HEXAGON \"${CMAKE_BINARY_DIR}${CMAKE_FILES_DIRECTORY}\"\n              \"${TEST_FOR_HEXAGON_FILE}\")\n  file(REMOVE \"${TEST_FOR_HEXAGON_FILE}\")\n  if(BUILD_FOR_HEXAGON)\n    message(STATUS \"Building for Hexagon\")\n  endif()\n\n  # Detect if we're compiling for Android.\n  set(TEST_FOR_ANDROID_CXX\n      \"#ifndef __ANDROID__\"\n      \"#error\"\n      \"#endif\"\n      \"int main() {}\")\n  set(TEST_FOR_ANDROID_DIR\n      \"${CMAKE_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/CMakeTmp\")\n  set(TEST_FOR_ANDROID_FILE \"${TEST_FOR_ANDROID_DIR}/test_for_android.cc\")\n  string(REPLACE \";\" \"\\n\" TEST_FOR_ANDROID_CXX_TEXT \"${TEST_FOR_ANDROID_CXX}\")\n  file(WRITE \"${TEST_FOR_ANDROID_FILE}\" \"${TEST_FOR_ANDROID_CXX_TEXT}\")\n  try_compile(BUILD_FOR_ANDROID \"${CMAKE_BINARY_DIR}${CMAKE_FILES_DIRECTORY}\"\n              \"${TEST_FOR_ANDROID_FILE}\")\n  file(REMOVE \"${TEST_FOR_ANDROID_FILE}\")\n  if(BUILD_FOR_ANDROID)\n    message(STATUS \"Building for Android\")\n  endif()\nendif(MSVC)\n\n# Hexagon has dlopen built into QuRT (no need for static library).\nif(NOT BUILD_FOR_HEXAGON)\n  list(APPEND TVM_RUNTIME_LINKER_LIBS ${CMAKE_DL_LIBS})\nendif()\n\n# add source group\ntvm_file_glob(GLOB_RECURSE GROUP_SOURCE \"src/*.cc\")\ntvm_file_glob(GLOB_RECURSE GROUP_INCLUDE \"src/*.h\" \"include/*.h\")\nassign_source_group(\"Source\" ${GROUP_SOURCE})\nassign_source_group(\"Include\" ${GROUP_INCLUDE})\n\n# Source file lists\ntvm_file_glob(GLOB_RECURSE COMPILER_SRCS\n    src/auto_scheduler/*.cc\n    src/meta_schedule/*.cc\n    src/node/*.cc\n    src/ir/*.cc\n    src/arith/*.cc\n    src/te/*.cc\n    src/autotvm/*.cc\n    src/tir/*.cc\n    src/topi/*.cc\n    src/driver/*.cc\n    src/support/*.cc\n    src/script/*.cc\n    src/relax/ir/*.cc\n    src/relax/op/*.cc\n    src/relax/analysis/*.cc\n    src/relax/transform/*.cc\n    src/relax/backend/vm/*.cc\n    src/relax/backend/task_extraction.cc\n    src/relax/backend/pattern_registry.cc\n    src/relax/utils.cc\n    src/relax/distributed/*.cc\n    src/relax/distributed/transform/*.cc\n    src/relax/op/distributed/*.cc\n    src/relax/testing/*.cc\n    )\n\ntvm_file_glob(GLOB CODEGEN_SRCS\n  src/target/*.cc\n  src/target/source/*.cc\n  src/target/parsers/*.cc\n    )\n\nlist(APPEND COMPILER_SRCS ${CODEGEN_SRCS})\n\ntvm_file_glob(GLOB_RECURSE RELAY_OP_SRCS\n    src/relay/op/*.cc\n    )\ntvm_file_glob(GLOB_RECURSE RELAY_PASS_SRCS\n    src/relay/analysis/*.cc\n    src/relay/collage/*.cc\n    src/relay/transforms/*.cc\n    src/relay/quantize/*.cc\n    )\ntvm_file_glob(GLOB RELAY_BACKEND_SRCS\n    src/relay/backend/*.cc\n    src/relay/backend/vm/*.cc\n    src/relay/backend/aot/*.cc\n    )\ntvm_file_glob(GLOB_RECURSE RELAY_IR_SRCS\n    src/relay/ir/*.cc\n    src/relay/printer/*.cc\n    src/relay/parser/*.cc\n    )\ntvm_file_glob(GLOB_RECURSE RELAY_QNN_SRCS\n    src/relay/qnn/*.cc\n)\nlist(APPEND COMPILER_SRCS ${RELAY_OP_SRCS})\nlist(APPEND COMPILER_SRCS ${RELAY_PASS_SRCS})\nlist(APPEND COMPILER_SRCS ${RELAY_BACKEND_SRCS})\nlist(APPEND COMPILER_SRCS ${RELAY_IR_SRCS})\nlist(APPEND COMPILER_SRCS ${RELAY_QNN_SRCS})\n\ntvm_file_glob(GLOB DATATYPE_SRCS src/target/datatype/*.cc)\nlist(APPEND COMPILER_SRCS ${DATATYPE_SRCS})\nlist(APPEND COMPILER_SRCS \"src/target/datatype/myfloat/myfloat.cc\")\n\ntvm_file_glob(GLOB RUNTIME_SRCS\n  src/runtime/*.cc\n  src/runtime/vm/*.cc\n  src/runtime/memory/*.cc\n  src/runtime/disco/*.cc\n  src/runtime/minrpc/*.cc\n  src/runtime/relax_vm/*.cc\n)\nset(TVM_RUNTIME_EXT_OBJS \"\")\n\nif(BUILD_FOR_HEXAGON)\n  if(NOT BUILD_STATIC_RUNTIME)\n    # Allow undefined symbols (there will be some from libc).\n    set(TVM_NO_UNDEFINED_SYMBOLS \"\")\n  endif()\n\n  add_definitions(-D_MACH_I32=int)\n  add_definitions(-DDMLC_CXX11_THREAD_LOCAL=0)\nendif()\n\n# distributed disco runtime are disabled for hexagon\nif (NOT BUILD_FOR_HEXAGON)\n  tvm_file_glob(GLOB RUNTIME_DISCO_DISTRIBUTED_SRCS src/runtime/disco/distributed/*.cc)\n  list(APPEND RUNTIME_SRCS ${RUNTIME_DISCO_DISTRIBUTED_SRCS})\nendif()\n\n# Package runtime rules\nif(NOT USE_RTTI)\n  add_definitions(-DDMLC_ENABLE_RTTI=0)\nendif()\n\nif (INDEX_DEFAULT_I64)\n  add_definitions(-DTVM_INDEX_DEFAULT_I64=1)\nendif()\n\nif(USE_RPC)\n  message(STATUS \"Build with RPC support...\")\n  tvm_file_glob(GLOB RUNTIME_RPC_SRCS src/runtime/rpc/*.cc)\n  list(APPEND RUNTIME_SRCS ${RUNTIME_RPC_SRCS})\nendif(USE_RPC)\n\ntvm_file_glob(GLOB STACKVM_RUNTIME_SRCS src/runtime/stackvm/*.cc)\ntvm_file_glob(GLOB STACKVM_CODEGEN_SRCS src/target/stackvm/*.cc)\nlist(APPEND COMPILER_SRCS ${STACKVM_CODEGEN_SRCS})\nif(USE_STACKVM_RUNTIME)\n  message(STATUS \"Build with stackvm support in runtime...\")\n  list(APPEND RUNTIME_SRCS ${STACKVM_RUNTIME_SRCS})\nelse()\n  list(APPEND COMPILER_SRCS ${STACKVM_RUNTIME_SRCS})\nendif(USE_STACKVM_RUNTIME)\n\n# NOTE(areusch): USE_GRAPH_RUNTIME will be deleted in a future release\nif(USE_GRAPH_RUNTIME AND NOT DEFINED USE_GRAPH_EXECUTOR)\n  message(WARNING \"USE_GRAPH_RUNTIME renamed to USE_GRAPH_EXECUTOR. Please update your config.cmake\")\n  set(USE_GRAPH_EXECUTOR ${USE_GRAPH_RUNTIME})\n  unset(USE_GRAPH_RUNTIME CACHE)\nendif(USE_GRAPH_RUNTIME AND NOT DEFINED USE_GRAPH_EXECUTOR)\n\n# NOTE(areusch): USE_GRAPH_RUNTIME_DEBUG will be deleted in a future release\nif(USE_GRAPH_RUNTIME_DEBUG AND NOT DEFINED USE_PROFILER)\n  message(WARNING \"USE_GRAPH_RUNTIME_DEBUG renamed to USE_PROFILER. Please update your config.cmake\")\n  set(USE_PROFILER ${USE_GRAPH_RUNTIME_DEBUG})\n  unset(USE_GRAPH_RUNTIME_DEBUG CACHE)\nendif(USE_GRAPH_RUNTIME_DEBUG AND NOT DEFINED USE_PROFILER)\n\nif(USE_GRAPH_EXECUTOR)\n  message(STATUS \"Build with Graph Executor support...\")\n  tvm_file_glob(GLOB RUNTIME_GRAPH_EXECUTOR_SRCS src/runtime/graph_executor/*.cc)\n  list(APPEND RUNTIME_SRCS ${RUNTIME_GRAPH_EXECUTOR_SRCS})\n\nendif(USE_GRAPH_EXECUTOR)\n\n# convert old options for profiler\nif(USE_GRAPH_EXECUTOR_DEBUG)\n  message(WARNING \"USE_GRAPH_EXECUTOR_DEBUG renamed to USE_PROFILER. Please update your config.cmake\")\n  unset(USE_GRAPH_EXECUTOR_DEBUG CACHE)\n  set(USE_PROFILER ON)\nendif()\nif(USE_VM_PROFILER)\n  message(WARNING \"USE_VM_PROFILER renamed to USE_PROFILER. Please update your config.cmake\")\n  unset(USE_VM_PROFILER CACHE)\n  set(USE_PROFILER ON)\nendif()\n\nif(USE_PROFILER)\n  message(STATUS \"Build with profiler...\")\n\n  tvm_file_glob(GLOB RUNTIME_GRAPH_EXECUTOR_DEBUG_SRCS src/runtime/graph_executor/debug/*.cc)\n  list(APPEND RUNTIME_SRCS ${RUNTIME_GRAPH_EXECUTOR_DEBUG_SRCS})\n  set_source_files_properties(${RUNTIME_GRAPH_EXECUTOR_SRCS}\n    PROPERTIES COMPILE_DEFINITIONS \"TVM_GRAPH_EXECUTOR_DEBUG\")\n\n  tvm_file_glob(GLOB RUNTIME_VM_PROFILER_SRCS src/runtime/vm/profiler/*.cc)\n  list(APPEND RUNTIME_SRCS ${RUNTIME_VM_PROFILER_SRCS})\nendif(USE_PROFILER)\n\nif(USE_CUDA AND USE_NCCL)\n  message(STATUS \"Build with NCCL...\")\n  find_nccl(${USE_NCCL})\n  include_directories(SYSTEM ${NCCL_INCLUDE_DIR})\n  tvm_file_glob(GLOB RUNTIME_NCCL_SRC src/runtime/disco/nccl/*.cc src/runtime/disco/cuda_ipc/*.cc 3rdparty/tensorrt_llm/*.cu)\n  set_source_files_properties(src/runtime/disco/nccl/nccl.cc PROPERTIES COMPILE_DEFINITIONS \"TVM_NCCL_RCCL_SWITCH=0\")\n  list(APPEND RUNTIME_SRCS ${RUNTIME_NCCL_SRC})\nendif()\n\nif (USE_CUDA AND USE_NVSHMEM)\n  message(STATUS \"Build with NVSHMEM...\")\n  find_nvshmem(${USE_NVSHMEM})\n  if (NOT NVSHMEM_FOUND)\n    message(FATAL_ERROR \"Cannot find NVSHMEM, USE_NVSHMEM=\" ${USE_NVSHMEM})\n  endif()\n  set(CMAKE_CUDA_SEPARABLE_COMPILATION ON)\n  set(CMAKE_POSITION_INDEPENDENT_CODE ON)\n  tvm_file_glob(GLOB RUNTIME_NVSHMEM_SRCS src/runtime/contrib/nvshmem/*.cc src/runtime/contrib/nvshmem/*.cu)\n  list(APPEND RUNTIME_SRCS ${RUNTIME_NVSHMEM_SRCS})\nendif()\n\nif(USE_ROCM AND USE_RCCL)\n  message(STATUS \"Build with RCCL...\")\n  find_rccl(${USE_RCCL})\n  include_directories(SYSTEM ${RCCL_INCLUDE_DIR})\n  tvm_file_glob(GLOB RUNTIME_RCCL_SRC src/runtime/disco/nccl/*.cc)\n  set_source_files_properties(src/runtime/disco/nccl/nccl.cc PROPERTIES COMPILE_DEFINITIONS \"TVM_NCCL_RCCL_SWITCH=1\")\n  list(APPEND RUNTIME_SRCS ${RUNTIME_RCCL_SRC})\nendif()\n\nif(USE_AOT_EXECUTOR)\n  message(STATUS \"Build with AOT Executor support...\")\n  file(GLOB RUNTIME_AOT_EXECUTOR_SRCS src/runtime/aot_executor/*.cc)\n  list(APPEND RUNTIME_SRCS ${RUNTIME_AOT_EXECUTOR_SRCS})\n\nendif(USE_AOT_EXECUTOR)\n\n# Enable ctest if gtest is available\nif(USE_GTEST)\n  # Check env var for backward compatibility. A better way to specify package\n  # locations is to use CMAKE_PREFIX_PATH or other standard cmake mechanism\n  # (see cmake documentation for `find_package`).\n  set(GTEST_ROOT \"$ENV{GTEST_LIB}\")\n  if(\"${USE_GTEST}\" STREQUAL \"AUTO\")\n    # If USE_GTEST is AUTO, treat GTest as optional: enable if found.\n    find_package(GTest)\n  elseif(\"${USE_GTEST}\" MATCHES ${IS_TRUE_PATTERN})\n    # USE_GTEST is set to ON, TRUE, etc. Treat GTest as a required package.\n    find_package(GTest REQUIRED)\n  endif()\n  if(GTEST_FOUND)\n    if(NOT TARGET GTest::gmock)\n      # GMock is formally supported in CMake 3.20; for now, expect libgmock.a in the same directory,\n      # and require that folks compiling against GTest::gmock also link against GTest::GTest\n      # (for the includes dir).\n      add_library(GTest::gmock STATIC IMPORTED GLOBAL)\n      get_target_property(GTEST_LIB_PATH GTest::GTest IMPORTED_LOCATION)\n      if(\"${GTEST_LIB_PATH}\" STREQUAL \"GTEST_LIB_PATH-NOTFOUND\")\n        # CMake >= 3.20 makes GTest::GTest into a compatibility target. The real import location is in\n        # GTest::gtest.\n        get_target_property(GTEST_LIB_PATH GTest::gtest IMPORTED_LOCATION)\n        if(\"${GTEST_LIB_PATH}\" STREQUAL \"GTEST_LIB_PATH-NOTFOUND\")\n          message(FATAL_ERROR \"Neither GTest::GTest nor GTest::gtest targets defined IMPORTED_LOCATION\")\n        endif()\n      endif()\n      get_filename_component(GTEST_LIB_DIR \"${GTEST_LIB_PATH}\" DIRECTORY)\n      set_target_properties(GTest::gmock PROPERTIES\n          IMPORTED_LOCATION \"${GTEST_LIB_DIR}/libgmock.a\")\n    endif()\n\n    enable_testing()\n    include(CTest)\n  endif()\nendif()\n\nif(USE_PIPELINE_EXECUTOR)\n  message(STATUS \"Build with Pipeline Executor support...\")\n  tvm_file_glob(GLOB RUNTIME_PIPELINE_SRCS src/runtime/pipeline/*.cc)\n  list(APPEND RUNTIME_SRCS ${RUNTIME_PIPELINE_SRCS})\nendif(USE_PIPELINE_EXECUTOR)\n\nif(USE_KALLOC_ALIGNMENT)\n  message(STATUS \"Build Alloc alignment set to ${USE_KALLOC_ALIGNMENT}\")\n  add_definitions(-DTVM_KALLOC_ALIGNMENT=${USE_KALLOC_ALIGNMENT})\nendif(USE_KALLOC_ALIGNMENT)\n\n# Caches the build.\n# Note that ccache-3.x doesn't support nvcc well, so CUDA kernels may never hit the cache and still\n# need to be re-compiled every time. Using ccache 4.0+ can resolve this issue.\ninclude(cmake/utils/CCache.cmake)\n\ninclude(CheckCXXCompilerFlag)\nif(NOT MSVC)\n  check_cxx_compiler_flag(\"-std=c++17\" SUPPORT_CXX17)\n  set(CMAKE_CXX_FLAGS \"-std=c++17 ${CMAKE_CXX_FLAGS}\")\n  set(CMAKE_CUDA_STANDARD 17)\nelse()\n  check_cxx_compiler_flag(\"/std:c++17\" SUPPORT_CXX17)\n  set(CMAKE_CXX_FLAGS \"/std:c++17 ${CMAKE_CXX_FLAGS}\")\n  set(CMAKE_CUDA_STANDARD 17)\nendif()\n\n# Module rules\ninclude(cmake/modules/CUDA.cmake)\ninclude(cmake/modules/Hexagon.cmake) # This must come before logging.cmake\ninclude(cmake/modules/contrib/CLML.cmake) # Must be before OpenCL.cmake\ninclude(cmake/modules/OpenCL.cmake)\ninclude(cmake/modules/OpenMP.cmake)\ninclude(cmake/modules/Vulkan.cmake)\ninclude(cmake/modules/Metal.cmake)\ninclude(cmake/modules/ROCM.cmake)\ninclude(cmake/modules/LLVM.cmake)\ninclude(cmake/modules/contrib/BLAS.cmake)\ninclude(cmake/modules/contrib/CODEGENC.cmake)\ninclude(cmake/modules/contrib/DNNL.cmake)\ninclude(cmake/modules/contrib/AMX.cmake)\ninclude(cmake/modules/contrib/CUTLASS.cmake)\ninclude(cmake/modules/contrib/ExampleTargetHooks.cmake)\ninclude(cmake/modules/contrib/Random.cmake)\ninclude(cmake/modules/contrib/Posit.cmake)\ninclude(cmake/modules/contrib/MSCCLPP.cmake)\ninclude(cmake/modules/contrib/Sort.cmake)\ninclude(cmake/modules/contrib/NNPack.cmake)\ninclude(cmake/modules/contrib/LibTorch.cmake)\ninclude(cmake/modules/contrib/HybridDump.cmake)\ninclude(cmake/modules/contrib/TFLite.cmake)\ninclude(cmake/modules/contrib/TF_TVMDSOOP.cmake)\ninclude(cmake/modules/contrib/PT_TVMDSOOP.cmake)\ninclude(cmake/modules/contrib/CoreML.cmake)\ninclude(cmake/modules/contrib/BNNS.cmake)\ninclude(cmake/modules/contrib/ONNX.cmake)\ninclude(cmake/modules/contrib/ArmComputeLib.cmake)\ninclude(cmake/modules/contrib/TensorRT.cmake)\ninclude(cmake/modules/contrib/NNAPI.cmake)\ninclude(cmake/modules/contrib/VitisAI.cmake)\ninclude(cmake/modules/contrib/Verilator.cmake)\ninclude(cmake/modules/contrib/UMA.cmake)\ninclude(cmake/modules/contrib/MSC.cmake)\ninclude(cmake/modules/contrib/vllm.cmake)\ninclude(cmake/modules/Git.cmake)\ninclude(cmake/modules/LibInfo.cmake)\ninclude(cmake/modules/RustExt.cmake)\ninclude(cmake/modules/contrib/Mrvl.cmake)\n\nset(LIBINFO_FILE ${CMAKE_CURRENT_LIST_DIR}/src/support/libinfo.cc)\nadd_lib_info(${LIBINFO_FILE})\nlist(REMOVE_ITEM COMPILER_SRCS ${LIBINFO_FILE})\n\nadd_library(tvm_objs OBJECT ${COMPILER_SRCS})\nadd_library(tvm_runtime_objs OBJECT ${RUNTIME_SRCS})\nadd_library(tvm_libinfo_objs OBJECT ${LIBINFO_FILE})\n\ninclude(GNUInstallDirs)\nif(NOT BUILD_DUMMY_LIBTVM)\n  add_library(tvm SHARED\n    $<TARGET_OBJECTS:tvm_objs>\n    $<TARGET_OBJECTS:tvm_runtime_objs>\n    $<TARGET_OBJECTS:tvm_libinfo_objs>\n    ${TVM_RUNTIME_EXT_OBJS}\n  )\n\nelse()\n  # dummy version of libtvm that can be used by downstream to specify dependencies\n  # the real runner still need a full version of libtvm\n  add_library(tvm SHARED\n    $<TARGET_OBJECTS:tvm_runtime_objs>\n    $<TARGET_OBJECTS:tvm_libinfo_objs>\n    ${TVM_RUNTIME_EXT_OBJS}\n  )\nendif()\n\ntarget_include_directories(tvm PUBLIC \"$<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\")\nset_property(TARGET tvm APPEND PROPERTY LINK_OPTIONS \"${TVM_NO_UNDEFINED_SYMBOLS}\")\nset_property(TARGET tvm APPEND PROPERTY LINK_OPTIONS \"${TVM_VISIBILITY_FLAG}\")\nif(BUILD_STATIC_RUNTIME)\n  add_library(tvm_runtime STATIC\n    $<TARGET_OBJECTS:tvm_runtime_objs>\n    $<TARGET_OBJECTS:tvm_libinfo_objs>\n    ${TVM_RUNTIME_EXT_OBJS}\n  )\n  set(NOTICE_MULTILINE\n    \"You have build static version of the TVM runtime library. Make \"\n    \"sure to use --whole-archive when linking it into your project.\")\n  string(CONCAT NOTICE ${NOTICE_MULTILINE})\n  add_custom_command(TARGET tvm_runtime POST_BUILD\n    COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --yellow --bold ${NOTICE})\nelse()\n  add_library(tvm_runtime SHARED\n    $<TARGET_OBJECTS:tvm_runtime_objs>\n    $<TARGET_OBJECTS:tvm_libinfo_objs>\n    ${TVM_RUNTIME_EXT_OBJS}\n  )\n  set_property(TARGET tvm_runtime APPEND PROPERTY LINK_OPTIONS \"${TVM_NO_UNDEFINED_SYMBOLS}\")\nendif()\n\n\ntarget_include_directories(tvm_runtime PUBLIC \"$<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\")\nset_property(TARGET tvm_runtime APPEND PROPERTY LINK_OPTIONS \"${TVM_VISIBILITY_FLAG}\")\n\ntarget_compile_definitions(tvm_objs PUBLIC DMLC_USE_LOGGING_LIBRARY=<tvm/runtime/logging.h>)\ntarget_compile_definitions(tvm_runtime_objs PUBLIC DMLC_USE_LOGGING_LIBRARY=<tvm/runtime/logging.h>)\ntarget_compile_definitions(tvm_libinfo_objs PUBLIC DMLC_USE_LOGGING_LIBRARY=<tvm/runtime/logging.h>)\ntarget_compile_definitions(tvm PUBLIC DMLC_USE_LOGGING_LIBRARY=<tvm/runtime/logging.h>)\ntarget_compile_definitions(tvm_runtime PUBLIC DMLC_USE_LOGGING_LIBRARY=<tvm/runtime/logging.h>)\n\n# logging option for libbacktrace\ninclude(cmake/modules/Logging.cmake)\n\ninclude(cmake/modules/contrib/PAPI.cmake)\n\nif(USE_CPP_RPC)\n  add_subdirectory(\"apps/cpp_rpc\")\nendif()\n\nif(USE_CPP_RTVM)\n  add_subdirectory(\"apps/cpp_rtvm\")\nendif()\n\nif(USE_IOS_RPC)\n  add_subdirectory(\"apps/ios_rpc\")\nendif()\n\nif(USE_RELAY_DEBUG)\n  message(STATUS \"Building Relay in debug mode...\")\n  target_compile_definitions(tvm_objs PRIVATE \"USE_RELAY_DEBUG\")\n  target_compile_definitions(tvm_objs PRIVATE \"TVM_LOG_DEBUG\")\n  target_compile_definitions(tvm_runtime_objs PRIVATE \"USE_RELAY_DEBUG\")\n  target_compile_definitions(tvm_runtime_objs PRIVATE \"TVM_LOG_DEBUG\")\n  target_compile_definitions(tvm_libinfo_objs PRIVATE \"USE_RELAY_DEBUG\")\n  target_compile_definitions(tvm_libinfo_objs PRIVATE \"TVM_LOG_DEBUG\")\nelse()\n  target_compile_definitions(tvm_objs PRIVATE \"NDEBUG\")\n  target_compile_definitions(tvm_runtime_objs PRIVATE \"NDEBUG\")\n  target_compile_definitions(tvm_libinfo_objs PRIVATE \"NDEBUG\")\nendif(USE_RELAY_DEBUG)\n\nif(TVM_DEBUG_WITH_ABI_CHANGE)\n  message(STATUS \"Building with debug code that may cause ABI changes...\")\n  target_compile_definitions(tvm_objs PRIVATE \"TVM_DEBUG_WITH_ABI_CHANGE\")\n  target_compile_definitions(tvm_runtime_objs PRIVATE \"TVM_DEBUG_WITH_ABI_CHANGE\")\n  target_compile_definitions(tvm_libinfo_objs PRIVATE \"TVM_DEBUG_WITH_ABI_CHANGE\")\nendif(TVM_DEBUG_WITH_ABI_CHANGE)\n\nif(USE_FALLBACK_STL_MAP)\n  message(STATUS \"Building with STL Map...\")\n  target_compile_definitions(tvm_objs PRIVATE \"USE_FALLBACK_STL_MAP=1\")\n  target_compile_definitions(tvm_runtime_objs PRIVATE \"USE_FALLBACK_STL_MAP=1\")\n  target_compile_definitions(tvm_libinfo_objs PRIVATE \"USE_FALLBACK_STL_MAP=1\")\nelse()\n  message(STATUS \"Building with TVM Map...\")\n  target_compile_definitions(tvm_objs PRIVATE \"USE_FALLBACK_STL_MAP=0\")\n  target_compile_definitions(tvm_runtime_objs PRIVATE \"USE_FALLBACK_STL_MAP=0\")\n  target_compile_definitions(tvm_libinfo_objs PRIVATE \"USE_FALLBACK_STL_MAP=0\")\nendif(USE_FALLBACK_STL_MAP)\n\nif(USE_THREADS AND NOT BUILD_FOR_HEXAGON)\n  message(STATUS \"Build with thread support...\")\n  set(CMAKE_THREAD_PREFER_PTHREAD TRUE)\n  set(THREADS_PREFER_PTHREAD_FLAG TRUE)\n  find_package(Threads REQUIRED)\n  target_link_libraries(tvm PUBLIC Threads::Threads)\n  target_link_libraries(tvm_runtime PUBLIC Threads::Threads)\nendif()\n\nif(NOT BUILD_DUMMY_LIBTVM)\n  target_link_libraries(tvm PRIVATE ${TVM_LINKER_LIBS})\nendif()\n\ntarget_link_libraries(tvm PRIVATE ${TVM_RUNTIME_LINKER_LIBS})\ntarget_link_libraries(tvm_runtime PRIVATE ${TVM_RUNTIME_LINKER_LIBS})\n\nif(BUILD_FOR_HEXAGON AND DEFINED USE_HEXAGON_GTEST AND EXISTS ${USE_HEXAGON_GTEST})\n  include(FetchContent)\n  FetchContent_Declare(googletest SOURCE_DIR \"${USE_HEXAGON_GTEST}\")\n  set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)\n  FetchContent_MakeAvailable(googletest)\n  target_link_libraries(tvm_runtime PUBLIC gtest)\n  include_directories(\"${USE_HEXAGON_GTEST}/include\")\nendif()\n\n# Set flags for clang\ninclude(cmake/modules/ClangFlags.cmake)\nset(CRC16_INCLUDE_PATH \"3rdparty/libcrc/include\")\ntarget_include_directorieS(\n  tvm_objs\n  PRIVATE \"${CRC16_INCLUDE_PATH}\")\ntarget_include_directorieS(\n  tvm_libinfo_objs\n  PRIVATE \"${CRC16_INCLUDE_PATH}\")\ntarget_include_directorieS(\n  tvm_runtime_objs\n  PRIVATE \"${CRC16_INCLUDE_PATH}\")\n\nset(TVM_TEST_LIBRARY_NAME tvm)\nif (HIDE_PRIVATE_SYMBOLS AND NOT ${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n  add_library(tvm_allvisible SHARED $<TARGET_OBJECTS:tvm_objs> $<TARGET_OBJECTS:tvm_runtime_objs> $<TARGET_OBJECTS:tvm_libinfo_objs>)\n  target_include_directories(tvm_allvisible PUBLIC \"$<TARGET_PROPERTY:tvm,INCLUDE_DIRECTORIES>\")\n  target_link_libraries(tvm_allvisible PRIVATE \"$<TARGET_PROPERTY:tvm,LINK_LIBRARIES>\")\n  set(TVM_TEST_LIBRARY_NAME tvm_allvisible)\n\n  set(HIDE_SYMBOLS_LINKER_FLAGS \"-Wl,--exclude-libs,ALL\")\n  # Note: 'target_link_options' with 'PRIVATE' keyword would be cleaner\n  # but it's not available until CMake 3.13. Switch to 'target_link_options'\n  # once minimum CMake version is bumped up to 3.13 or above.\n  target_link_libraries(tvm PRIVATE ${HIDE_SYMBOLS_LINKER_FLAGS})\n  target_link_libraries(tvm_runtime PRIVATE ${HIDE_SYMBOLS_LINKER_FLAGS})\n  target_compile_definitions(tvm_allvisible PUBLIC $<TARGET_PROPERTY:tvm,INTERFACE_COMPILE_DEFINITONS>)\n  target_compile_definitions(tvm_allvisible PRIVATE $<TARGET_PROPERTY:tvm,COMPILE_DEFINITONS>)\nendif()\n\n# Create the `cpptest` target if we can find GTest.  If not, we create dummy\n# targets that give the user an informative error message.\nif(GTEST_FOUND)\n  tvm_file_glob(GLOB_RECURSE TEST_SRCS tests/cpp/*.cc)\n  add_executable(cpptest ${TEST_SRCS})\n  # include runtime files for unit testing\n  target_link_libraries(cpptest PRIVATE ${TVM_TEST_LIBRARY_NAME} GTest::GTest GTest::Main GTest::gmock pthread dl)\n  if(DEFINED LLVM_LIBS)\n    # The TVM library is linked with LLVM libraries. If the LLVM libraries are\n    # static and the symbols are not hidden, then don't link them again into\n    # cpptest since cpptest is itself linked against the TVM library. If static\n    # LLVM libraries are linked in twice, it can cause issues with global\n    # variable initialization (cl::opt).\n    # If the LLVM libraries are dynamic, we have to link them again, since the\n    # TVM library will not contain any LLVM definitions.\n    unset(LLVM_SO)\n    foreach(L IN LISTS LLVM_LIBS)\n      if(L MATCHES \"libLLVM.*\\.so\")\n        set(LLVM_SO TRUE)\n        break()\n      endif()\n    endforeach()\n    if(DEFINED LLVM_SO OR HIDE_PRIVATE_SYMBOLS)\n      target_link_libraries(cpptest PRIVATE ${LLVM_LIBS})\n    endif()\n  endif()\n  set_target_properties(cpptest PROPERTIES EXCLUDE_FROM_ALL 1)\n  set_target_properties(cpptest PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD 1)\n  if(USE_RELAY_DEBUG)\n    target_compile_definitions(cpptest PRIVATE \"USE_RELAY_DEBUG\")\n    target_compile_definitions(cpptest PRIVATE \"TVM_LOG_DEBUG\")\n  else()\n    target_compile_definitions(cpptest PRIVATE \"NDEBUG\")\n  endif()\n  if(TVM_DEBUG_WITH_ABI_CHANGE)\n    target_compile_definitions(cpptest PRIVATE \"TVM_DEBUG_WITH_ABI_CHANGE\")\n  endif(TVM_DEBUG_WITH_ABI_CHANGE)\n\n  # For some reason, compile definitions are not propagated correctly, so we manually add them here\n  target_compile_definitions(cpptest PUBLIC $<TARGET_PROPERTY:tvm,INTERFACE_COMPILE_DEFINITIONS>)\n  gtest_discover_tests(cpptest)\nendif()\n\n# Custom targets\nadd_custom_target(runtime DEPENDS tvm_runtime)\n\n# Installation rules\ninstall(TARGETS tvm EXPORT ${PROJECT_NAME}Targets DESTINATION lib${LIB_SUFFIX})\ninstall(TARGETS tvm_runtime EXPORT ${PROJECT_NAME}Targets DESTINATION lib${LIB_SUFFIX})\nif(BUILD_FOR_HEXAGON AND DEFINED USE_HEXAGON_GTEST AND EXISTS ${USE_HEXAGON_GTEST})\n  install(TARGETS gtest EXPORT ${PROJECT_NAME}Targets DESTINATION lib${LIB_SUFFIX})\nendif()\n\nif (INSTALL_DEV)\n  install(\n    DIRECTORY \"include/\" DESTINATION \"include\"\n    FILES_MATCHING\n    PATTERN \"*.h\"\n  )\n  install(\n    DIRECTORY \"3rdparty/dlpack/include/\" DESTINATION \"include\"\n    FILES_MATCHING\n    PATTERN \"*.h\"\n    )\n  install(\n    DIRECTORY \"3rdparty/dmlc-core/include/\" DESTINATION \"include\"\n    FILES_MATCHING\n    PATTERN \"*.h\"\n    )\nelse(INSTALL_DEV)\n  install(\n    DIRECTORY \"include/tvm/runtime/\" DESTINATION \"include/tvm/runtime\"\n    FILES_MATCHING\n    PATTERN \"*.h\"\n    )\nendif(INSTALL_DEV)\n\ninclude(CMakePackageConfigHelpers)\nset(PROJECT_CONFIG_CONTENT \"@PACKAGE_INIT@\\n\")\nstring(APPEND PROJECT_CONFIG_CONTENT \"include(CMakeFindDependencyMacro)\\n\")\nstring(APPEND PROJECT_CONFIG_CONTENT \"find_dependency(Threads REQUIRED)\\n\")\nstring(APPEND PROJECT_CONFIG_CONTENT\n       \"include(\\\"\\${CMAKE_CURRENT_LIST_DIR}/${PROJECT_NAME}Targets.cmake\\\")\")\nfile(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/temp_config_file.cmake\" ${PROJECT_CONFIG_CONTENT})\n\ninstall(EXPORT ${PROJECT_NAME}Targets\n  NAMESPACE ${PROJECT_NAME}::\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME})\n\n# Create config for find_package()\nconfigure_package_config_file(\n  \"${CMAKE_CURRENT_BINARY_DIR}/temp_config_file.cmake\" ${PROJECT_NAME}Config.cmake\n  INSTALL_DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\")\n\ninstall(\n  FILES\n  \"${PROJECT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\"\n  DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\")\n\n# More target definitions\nif(MSVC)\n  target_compile_definitions(tvm_objs PRIVATE -DTVM_EXPORTS)\n  target_compile_definitions(tvm_libinfo_objs PRIVATE -DTVM_EXPORTS)\n  target_compile_definitions(tvm_runtime_objs PRIVATE -DTVM_EXPORTS)\nendif()\n\nset(TVM_IS_DEBUG_BUILD OFF)\nif(CMAKE_BUILD_TYPE STREQUAL \"Debug\" OR CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\" OR CMAKE_CXX_FLAGS MATCHES \"-g\")\n  set(TVM_IS_DEBUG_BUILD ON)\nendif()\n\n# Change relative paths in backtrace to absolute ones\nif(TVM_IS_DEBUG_BUILD)\n  set(FILE_PREFIX_MAP_FLAG \"-ffile-prefix-map=..=${CMAKE_CURRENT_SOURCE_DIR}\")\n  target_compile_options(tvm PRIVATE \"${FILE_PREFIX_MAP_FLAG}\")\n  CHECK_CXX_COMPILER_FLAG(\"${FILE_PREFIX_MAP_FLAG}\" FILE_PREFIX_MAP_SUPPORTED)\n  if(FILE_PREFIX_MAP_SUPPORTED)\n    target_compile_options(tvm PRIVATE $<$<COMPILE_LANGUAGE:CXX>:${FILE_PREFIX_MAP_FLAG}>)\n    target_compile_options(tvm_objs PRIVATE $<$<COMPILE_LANGUAGE:CXX>:${FILE_PREFIX_MAP_FLAG}>)\n    target_compile_options(tvm_libinfo_objs PRIVATE $<$<COMPILE_LANGUAGE:CXX>:${FILE_PREFIX_MAP_FLAG}>)\n    target_compile_options(tvm_runtime PRIVATE $<$<COMPILE_LANGUAGE:CXX>:${FILE_PREFIX_MAP_FLAG}>)\n    target_compile_options(tvm_runtime_objs PRIVATE $<$<COMPILE_LANGUAGE:CXX>:${FILE_PREFIX_MAP_FLAG}>)\n  endif()\nendif()\n\n# Run dsymutil to generate debugging symbols for backtraces\nif(APPLE AND TVM_IS_DEBUG_BUILD)\n  find_program(DSYMUTIL dsymutil)\n  mark_as_advanced(DSYMUTIL)\n  add_custom_command(TARGET tvm\n      POST_BUILD\n      COMMAND ${DSYMUTIL} ARGS $<TARGET_FILE:tvm>\n      COMMENT \"Running dsymutil\"\n      VERBATIM\n\t\t  )\nendif()\n\nif(BUILD_FOR_HEXAGON)\n  # Wrap pthread_create to allow setting custom stack size.\n  set_property(TARGET tvm_runtime APPEND PROPERTY LINK_FLAGS\n                        \"-Wl,--wrap=pthread_create\")\n  # Link tvm_runtime into the RPC skel library. Make sure it's built\n  # as a part of the \"runtime\" target.\n  if(USE_HEXAGON_RPC)\n    target_link_libraries(hexagon_rpc_skel -Wl,--whole-archive tvm_runtime -Wl,--no-whole-archive)\n    add_dependencies(runtime hexagon_rpc_skel)\n  endif()\nendif()\n\nfind_and_set_linker(${USE_ALTERNATIVE_LINKER})\n\nif(${SUMMARIZE})\n  print_summary()\nendif()\n\ndump_options_to_file(\"${TVM_ALL_OPTIONS}\")\n\nif(USE_CUDA AND USE_CUTLASS)\n  install(TARGETS fpA_intB_gemm EXPORT ${PROJECT_NAME}Targets DESTINATION lib${LIB_SUFFIX})\n  target_link_libraries(tvm PRIVATE fpA_intB_gemm)\n  target_link_libraries(tvm_runtime PRIVATE fpA_intB_gemm)\n  target_link_libraries(tvm PRIVATE fpA_intB_gemm_tvm)\n  target_link_libraries(tvm_runtime PRIVATE fpA_intB_gemm_tvm)\n\n  install(TARGETS flash_attn EXPORT ${PROJECT_NAME}Targets DESTINATION lib${LIB_SUFFIX})\n  target_link_libraries(tvm PRIVATE -Wl,--no-as-needed flash_attn)\n  target_link_libraries(tvm_runtime PRIVATE -Wl,--no-as-needed flash_attn)\nendif()\n\nif(USE_CUDA AND USE_NVTX)\n  set_source_files_properties(src/runtime/nvtx.cc PROPERTIES COMPILE_DEFINITIONS \"TVM_NVTX_ENABLED=1\")\nendif()\n\nif(USE_CUDA AND USE_NCCL)\n  find_library(LIBRT rt)\n  target_link_libraries(tvm PRIVATE nccl ${LIBRT})\n  target_link_libraries(tvm_runtime PRIVATE nccl ${LIBRT})\nendif()\n\n\nif (USE_CUDA AND USE_NVSHMEM)\n  include_directories(SYSTEM ${USE_NVSHMEM}/include)\n  find_library(NVSHMEM_HOST nvshmem_host ${NVSHMEM_LIB_DIR})\n  find_library(NVSHMEM_DEVICE nvshmem_device ${NVSHMEM_LIB_DIR})\n  target_link_libraries(tvm PRIVATE ${NVSHMEM_HOST} ${NVSHMEM_DEVICE})\n  target_link_libraries(tvm_runtime PRIVATE ${NVSHMEM_HOST} ${NVSHMEM_DEVICE})\n  set_target_properties(tvm PROPERTIES CUDA_SEPARABLE_COMPILATION ON)\n  set_target_properties(tvm_runtime PROPERTIES CUDA_SEPARABLE_COMPILATION ON)\nendif()\n\nif(USE_ROCM AND USE_RCCL)\n  target_link_libraries(tvm PRIVATE rccl)\n  target_link_libraries(tvm_runtime PRIVATE rccl)\nendif()\n\n\noption(USE_FLASHINFER \"Build TVM with FlashInfer\" OFF)\nif (USE_FLASHINFER STREQUAL \"ON\")\n  message(STATUS \"Build with FlashInfer\")\n  set(FLASHINFER_TVM_BINDING ON)\n  set(FLASHINFER_TVM_SOURCE_DIR ${PROJECT_SOURCE_DIR})\n  set(FLASHINFER_PREFILL OFF)\n  set(FLASHINFER_DECODE OFF)\n  set(FLASHINFER_PAGE OFF)\n  set(FLASHINFER_CASCADE OFF)\n  set(FLASHINFER_SAMPLING OFF)\n  set(FLASHINFER_NORM OFF)\n  add_subdirectory(3rdparty/flashinfer)\nelse ()\n  message(STATUS \"Build without FlashInfer\")\nendif ()\n\n\nif (USE_FLASHINFER STREQUAL \"ON\")\n  target_link_libraries(tvm PRIVATE flashinfer_tvm)\n  target_link_libraries(tvm_runtime PRIVATE flashinfer_tvm)\nendif ()\n"
        },
        {
          "name": "CONTRIBUTORS.md",
          "type": "blob",
          "size": 14.6123046875,
          "content": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\nTVM Contributors\n================\nTVM adopts the Apache way and governs by merit. We believe that it is important to create an inclusive community where everyone can use,\ncontribute to, and influence the direction of the project. We actively invite contributors who have earned the merit to be part of the development community.\n\nSee the [community structure document](https://tvm.apache.org/docs/contribute/community.html) for the explanation of community structure and contribution guidelines.\n\n\n## Committers\n\nWe add tag along with committer name to show areas that they are familiar with.\nWe do encourage everyone to work anything they are interested in.\n\n- [Aditya Atluri](https://github.com/adityaatluri): @adityaatluri - rocm\n- [Matthew Barrett](https://github.com/mbaret): @mbaret - byoc, arm\n- [Matthew Brookhart](https://github.com/mbrookhart): @mbrookhart - relay, frontends\n- [Yaxing Cai](https://github.com/cyx-6): @cyx-6 - tvm-script, runtime\n- [Liangfu Chen](https://github.com/liangfu): @liangfu - vta, chisel, intel FPGA, c runtime\n- [Tianqi Chen](https://github.com/tqchen) (PMC): @tqchen - topi, compiler, relay, docs\n- [Wei Chen](https://github.com/wweic): @wweic - runtime, relay, vm\n- [Zhi Chen](https://github.com/zhiics) (PMC): @zhiics - relay, quantization, pass manager\n- [Egor Churaev](https://github.com/echuraev): @echuraev - metal, opencl, adreno\n- [Balint Cristian](https://github.com/cbalint13): @cbalint13\n- [Siyuan Feng](https://github.com/Hzfengsy) (PMC): @Hzfengsy - tir\n- [Josh Fromm](https://github.com/jwfromm) (PMC): @jwfromm - frontends, quantization, topi\n- [Mehrdad Hessar](https://github.com/mehrdadh): @mehrdadh - microTVM, hexagon\n- [Masahiro Hiramori](https://github.com/mshr-h): @mshr-h - relax, frontend\n- [Bohan Hou](https://github.com/spectrometerHBH) (PMC): @spectrometerHBH - tir, arith, tvm-script\n- [Yuwei Hu](https://github.com/Huyuwei): @Huyuwei - topi, frontends\n- [Luke Hutton](https://github.com/lhutton1): @lhutton1 - ethos-u, arm\n- [Nick Hynes](https://github.com/nhynes): @nhynes: - sgx, rust\n- [Animesh Jain](https://github.com/anijain2305): @anijain2305 - quantization, relay\n- [Chenfan Jia](https://github.com/jcf94): @jcf94 - auto_scheduler\n- [Ziheng Jiang](https://github.com/ZihengJiang) (PMC): @ZihengJiang - relay, compiler\n- [Hongyi Jin](https://github.com/jinhongyii): @jinhongyii - tir, tvm-script, arith, relay, topi\n- [Manupa Karunaratne](https://github.com/manupak): @manupak - ethos-u, memory planner\n- [Elen Kalda](https://github.com/ekalda): @ekalda - ethos-u, arm\n- [Marisa Kirisame](https://github.com/MarisaKirisame): @MarisaKirisame - relay\n- [Tristan Konolige](https://github.com/tkonolige): @tkonolige - profiling, relay, tir, runtime\n- [Ruihang Lai](https://github.com/MasterJH5574) (PMC): @MasterJH5574 - tir, tvm-script\n- [Wuwei Lin](https://github.com/vinx13) (PMC): @vinx13 - relay, topi, tir, meta_schedule\n- [Yizhi Liu](https://github.com/yzhliu) (PMC): @yzhliu - jvm, topi, relay\n- [Hao Lu](https://github.com/hlu1): @hlu1 - nnpack, frontends\n- [Eric Lunderberg](https://github.com/Lunderberg): @Lunderberg - CI, Vulkan backend\n- [Andrew Z. Luo](https://github.com/AndrewZhaoLuo): @AndrewZhaoLuo - amp, relay, frontends\n- [Steven Lyubomirsky](https://github.com/slyubomirsky): @slyubomirsky - relay\n- [Masahiro Masuda](https://github.com/masahi) (PMC): @masahi - topi, relay\n- [Thierry Moreau](https://github.com/tmoreau89) (PMC): @tmoreau89 - vta\n- [Kazutaka Morita](https://github.com/kazum): @kazum - frontends, opencl\n- [Trevor Morris](https://github.com/trevor-m): @trevor-m - byoc, compiler\n- [Leandro Nunes](https://github.com/leandron) (PMC): @leandron - tvmc\n- [Lily Orth-Smith](https://github.com/electriclilies): @electriclilies - relay\n- [Ashutosh Parkhi](https://github.com/ashutosh-arm): @ashutosh-arm - cmsis-nn\n- [Krzysztof Parzyszek](https://github.com/kparzysz-quic) (PMC): @kparzysz-quic - hexagon, llvm\n- [Andrew Reusch](https://github.com/areusch): (PMC) @areusch - runtime, microTVM\n- [David Riazati](https://github.com/driazati): @driazati - ci, community\n- [Jared Roesch](https://github.com/jroesch) (PMC): @jroesch - relay\n- [Gustavo Romero](https://github.com/gromero): @gromero - microtvm, tvmc\n- [Giuseppe Rossini](https://github.com/giuseros): @giuseros - aot, arm\n- [Siju Samuel](https://github.com/siju-samuel): @siju-samuel - frontends\n- [Christopher Sidebottom](https://github.com/Mousius): @Mousius - arm, ethos-u, relay\n- [Junru Shao](https://github.com/junrushao) (PMC): @junrushao - relay, compiler\n- [Haichen Shen](https://github.com/icemelon) (PMC): @icemelon - relay, topi\n- [Chris Sullivan](https://github.com/csullivan): @csullivan - amd backend\n- [Siva Rama Krishna Reddy](https://github.com/srkreddy1238): @srkreddy1238 - frontends, golang\n- [Zhixun Tan](https://github.com/phisiart): @phisiart - opengl, web\n- [Tong Meng](https://github.com/Archermmt): @Archermmt - msc\n- [Andrew Tulloch](https://github.com/ajtulloch): @ajtulloch - topi, compiler, runtime\n- [Gavin Uberti](https://github.com/guberti): @guberti - microtvm, arm\n- [Luis Vega](https://github.com/vegaluisjose): @vegaluisjose - vta, chisel\n- [Leyuan Wang](https://github.com/Laurawly) (PMC): @Laurawly: - topi\n- [Yao Wang](https://github.com/kevinthesun): @kevinthesun (PMC): - topi, vision\n- [Jian Weng](https://github.com/were): @were: - hybrid script\n- [Zhao Wu](https://github.com/FrozenGene): @FrozenGene - runtime, topi, frontends\n- [Eddie Yan](https://github.com/eqy) (PMC): @eqy - runtime, autotvm, rpc, topi\n- [Zihao Ye](https://github.com/yzh119): @yzh119 - tir\n- [Hao Yu](https://github.com/comaniac): @comaniac (PMC) - relay, byoc, auto_scheduler\n- [Shuai Yuan](https://github.com/ysh329): @ysh329 (PMC) - ci\n- [Qiang Zhang](https://github.com/Johnson9009): @Johnson9009 - relay, tvm-script\n- [Lianmin Zheng](https://github.com/merrymercy) (PMC): @merrymercy - autotvm, auto_scheduler, topi, relay\n- [Xiyou Zhou](https://github.com/zxybazh): @zxybazh - relay\n- [wrongtest](https://github.com/wrongtest-intellif) (PMC): @wrongtest-intellif - tir, tvm-script, arith\n- [Anirudh Sundar Subramaniam](https://github.com/quic-sanirudh): @quic-sanirudh\n\n## Reviewers\n\n- [Aditya Atluri](https://github.com/adityaatluri): @adityaatluri\n- [Matthew Barrett](https://github.com/mbaret): @mbaret\n- [Arnaud Bergeron](https://github.com/abergeron): @abergeron\n- [Florin Blanaru](https://github.com/gigiblender): @gigiblender\n- [Matthew Brookhart](https://github.com/mbrookhart): @mbrookhart\n- [Yaxing Cai](https://github.com/cyx-6): @cyx-6\n- [Liangfu Chen](https://github.com/liangfu): @liangfu\n- [Tianqi Chen](https://github.com/tqchen): @tqchen\n- [Zhi Chen](https://github.com/zhiics): @zhiics\n- [Valery Chernov](https://github.com/vvchernov): @vvchernov\n- [Neo Chien](https://github.com/cchung100m): @cchung100m\n- [Christian Convey](https://github.com/cconvey/): @cconvey\n- [Meghan Cowan](https://github.com/cowanmeg): @cowanmeg\n- [Balint Cristian](https://github.com/cbalint13): @cbalint13\n- [Egor Churaev](https://github.com/echuraev): @echuraev\n- [Xiaoqiang Dan](https://github.com/xqdan): @xqdan\n- [Yixin Dong](https://github.com/Ubospica) @Ubospica\n- [Haozheng Fan](https://github.com/hzfan): @hzfan\n- [Siyuan Feng](https://github.com/Hzfengsy): @Hzfengsy\n- [Josh Fromm](https://github.com/jwfromm): @jwfromm\n- [Alexey Gladyshev](https://github.com/KJlaccHoeUM9l): @KJlaccHoeUM9l\n- [Sergei Grechanik](https://github.com/sgrechanik-h): @sgrechanik-h\n- [Altan Haan](https://github.com/altanh): @altanh\n- [Mehrdad Hessar](https://github.com/mehrdadh): @mehrdadh\n- [Masahiro Hiramori](https://github.com/mshr-h): @mshr-h\n- [Bohan Hou](https://github.com/spectrometerHBH): @spectrometerHBH\n- [Yuwei Hu](https://github.com/Huyuwei): @Huyuwei\n- [Luke Hutton](https://github.com/lhutton1): @lhutton1\n- [Nick Hynes](https://github.com/nhynes): @nhynes\n- [Animesh Jain](https://github.com/anijain2305): @anijain2305\n- [Chenfan Jia](https://github.com/jcf94): @jcf94\n- [Hua Jiang](https://github.com/huajsj): @huajsj\n- [Ziheng Jiang](https://github.com/ZihengJiang): @ZihengJiang\n- [Hongyi Jin](https://github.com/jinhongyii): @jinhongyii\n- [Manupa Karunaratne](https://github.com/manupak): @manupak\n- [Elen Kalda](https://github.com/ekalda): @ekalda\n- [Marisa Kirisame](https://github.com/MarisaKirisame): @MarisaKirisame\n- [Michael J. Klaiber](https://github.com/MichaelJKlaiber/) @MichaelJKlaiber\n- [Noah Kontur](https://github.com/konturn/) @konturn\n- [Tristan Konolige](https://github.com/tkonolige): @tkonolige\n- [Mohamad Katanbaf](https://github.com/mkatanbaf): @mkatanbaf\n- [Denise Kutnick](https://github.com/denise-k): @denise-k\n- [Ruihang Lai](https://github.com/MasterJH5574): @MasterJH5574\n- [Nicola Lancellotti](https://github.com/nicolalancellotti): @NicolaLancellotti\n- [Wuwei Lin](https://github.com/vinx13): @vinx13\n- [Andrew Liu](https://github.com/hypercubestart): @hypercubestart\n- [Henry Liu](https://github.com/optima2005): @optima2005\n- [Xin Liu](https://github.com/Meteorix): @Meteorix\n- [Yizhi Liu](https://github.com/yzhliu) : @yzhliu\n- [Hao Lu](https://github.com/hlu1): @hlu1\n- [Eric Lunderberg](https://github.com/Lunderberg): @Lunderberg\n- [Andrew Z. Luo](https://github.com/AndrewZhaoLuo): @AndrewZhaoLuo\n- [Steven Lyubomirsky](https://github.com/slyubomirsky): @slyubomirsky\n- [Alan MacDonald](https://github.com/alanmacd): @alanmacd\n- [Masahiro Masuda](https://github.com/masahi): @masahi\n- [Andrey Malyshev](https://github.com/elvin-n): @elvin-n\n- [Sergey Mironov](https://github.com/grwlf): @grwlf\n- [Thierry Moreau](https://github.com/tmoreau89): @tmoreau89\n- [Kazutaka Morita](https://github.com/kazum): @kazum\n- [Trevor Morris](https://github.com/trevor-m): @trevor-m\n- [Tatsuya Nishiyama](https://github.com/nishi-t): @nishi-t\n- [Leandro Nunes](https://github.com/leandron): @leandron\n- [Jiawei Liu](https://github.com/ganler): @ganler\n- [Lily Orth-Smith](https://github.com/electriclilies): @electriclilies\n- [Wei Pan](https://github.com/wpan11nv): @wpan11nv\n- [Michalis Papadimitriou](https://github.com/mikepapadim): @mikepapadim\n- [Krzysztof Parzyszek](https://github.com/kparzysz-quic): @kparzysz-quic\n- [Sunghyun Park](https://github.com/sunggg): @sunggg\n- [Ashutosh Parkhi](https://github.com/ashutosh-arm): @ashutosh-arm\n- [Alexander Peskov](https://github.com/apeskov): @apeskov\n- [Pariksheet Pinjari](https://github.com/PariksheetPinjari909): @PariksheetPinjari909\n- [Josh Pollock](https://github.com/joshpoll): @joshpoll\n- [Ramana Radhakrishnan](https://github.com/u99127): @u99127\n- [Andrew Reusch](https://github.com/areusch): @areusch\n- [David Riazati](https://github.com/driazati): @driazati\n- [Jared Roesch](https://github.com/jroesch): @jroesch\n- [Gustavo Romero](https://github.com/gromero): @gromero\n- [Giuseppe Rossini](https://github.com/giuseros): @giuseros\n- [Siju Samuel](https://github.com/siju-samuel): @siju-samuel\n- [Janet Schneider](https://github.com/janetsc): @janetsc\n- [Junru Shao](https://github.com/junrushao): @junrushao\n- [Haichen Shen](https://github.com/icemelon): @icemelon\n- [Qingchao Shen](https://github.com/jikechao): @jikechao\n- [Xingjian Shi](https://github.com/sxjscience): @sxjscience\n- [Yuanjing Shi](https://github.com/shingjan): @shingjan\n- [Mark Shields](https://github.com/mbs-octoml): @mbs-octoml\n- [Christopher Sidebottom](https://github.com/mousius): @mousius\n- [Siva Rama Krishna Reddy](https://github.com/srkreddy1238): @srkreddy1238\n- [Dmitriy Smirnov](https://github.com/d-smirnov): @d-smirnov\n- [Jon Soifer](https://github.com/soiferj): @soiferj\n- [Adam Straw](https://github.com/adstraw): @adstraw\n- [Chris Sullivan](https://github.com/csullivan): @csullivan\n- [Anirudh Sundar Subramaniam](https://github.com/quic-sanirudh): @quic-sanirudh\n- [Zhixun Tan](https://github.com/phisiart): @phisiart\n- [Tong Meng](https://github.com/Archermmt): @Archermmt\n- [Andrew Tulloch](https://github.com/ajtulloch): @ajtulloch\n- [Jorn Tuyls](https://github.com/jtuyls): @jtuyls\n- [Gavin Uberti](https://github.com/guberti): @guberti\n- [Luis Vega](https://github.com/vegaluisjose): @vegaluisjose\n- [Jyotsna Verma](https://github.com/jverma-quic): @jverma-quic\n- [Thomas Viehmann](https://github.com/t-vi): @t-vi\n- [An Wang](https://github.com/anwang2009): @anwang2009\n- [Yao Wang](https://github.com/kevinthesun): @kevinthesun\n- [Yuchen Wang](https://github.com/wyc-ruiker): @wyc-ruiker\n- [Leyuan Wang](https://github.com/Laurawly): @Laurawly\n- [Alex Weaver](https://github.com/alex-weaver): @alex-weaver\n- [Logan Weber](https://github.com/weberlo): @weberlo\n- [Matt Welsh](https://github.com/mdw-octoml): @mdw-octoml\n- [Cheng Wen](https://github.com/chengven027-intellif): @chengven027-intellif\n- [Jian Weng](https://github.com/were): @were\n- [wrongtest](https://github.com/wrongtest-intellif): @wrongtest-intellif\n- [Yong Wu](https://github.com/yongwww): @yongwww\n- [Zhao Wu](https://github.com/FrozenGene): @FrozenGene\n- [Bing Xu](https://github.com/antinucleon): @antinucleon\n- [Eddie Yan](https://github.com/eqy): @eqy\n- [Aleksei Yazev](https://github.com/Aleksei-grovety): @Aleksei-grovety\n- [Zihao Ye](https://github.com/yzh119): @yzh119\n- [Hao Yu](https://github.com/comaniac): @comaniac\n- [Shuai Yuan](https://github.com/ysh329): @ysh329\n- [Joshua Z. Zhang](https://github.com/zhreshold): @zhreshold\n- [Lianmin Zheng](https://github.com/merrymercy): @merrymercy\n- [Min Chen](https://github.com/multiverstack-intellif): @multiverstack-intellif\n- [Xiyou Zhou](https://github.com/zxybazh): @zxybazh\n- [@blackkker](https://github.com/blackkker): @blackkker\n- [Jiajun Jiang](https://github.com/jiangjiajun): @jiangjiajun\n- [Qiang Zhang](https://github.com/Johnson9009): @Johnson9009\n\n## List of Contributors\n- [Full List of Contributors](https://github.com/apache/tvm/graphs/contributors)\n\n## Mentors\n\nTVM is now a top-level Apache project. During our Incubator phase, we were fortunate to have the following mentors.\n\n- Markus Weimer @markusweimer\n- Sebastian Schelter @sscdotopen\n- Byung-Gon Chun @bgchun\n- Henry Saputra @hsaputra\n- Timothy Chen @tnachen\n- Furkan KAMACI @kamaci\n"
        },
        {
          "name": "KEYS",
          "type": "blob",
          "size": 44.0107421875,
          "content": "This file contains the PGP keys of various developers.\nPlease don't use them for email unless you have to. Their main\npurpose is code signing.\n\nExamples of importing this file in your keystore:\n gpg --import KEYS.txt\n (need pgp and other examples here)\n\nExamples of adding your key to this file:\n pgp -kxa <your name> and append it to this file.\n (pgpk -ll <your name> && pgpk -xa <your name>) >> this file.\n (gpg --list-sigs <your name>\n     && gpg --armor --export <your name>) >> this file.\n\n-----------------------------------------------------------------------------------\npub   rsa4096 2019-11-15 [SC]\n      EF52D68AD5276994249816836754EA97C55E3DEB\nuid           [ultimate] Tianqi Chen (CODE SIGNING KEY) <tqchen@apache.org>\nsig 3        6754EA97C55E3DEB 2019-11-15  Tianqi Chen (CODE SIGNING KEY) <tqchen@apache.org>\nsub   rsa4096 2019-11-15 [E]\nsig          6754EA97C55E3DEB 2019-11-15  Tianqi Chen (CODE SIGNING KEY) <tqchen@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\nha8fS9ysWDy/Ye65MP4=\n=MSiP\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa4096 2018-02-07 [SC]\n      F42C1A6E634C105E8D985105CA751254E97B9FE4\nuid           [ultimate] Yizhi Liu <liuyizhi@apache.org>\nsig 3        CA751254E97B9FE4 2018-02-07  Yizhi Liu <liuyizhi@apache.org>\nsub   rsa4096 2018-02-07 [E]\nsig          CA751254E97B9FE4 2018-02-07  Yizhi Liu <liuyizhi@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n8PlimrXuzG9Bokct2SOO6Z51i6rSDo/ALj440EvWNw==\n=1xfH\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa4096 2020-09-24 [SC]\n      6A0D4938D8C052C759AE2460ED03B26E4FC3509F\nuid           [ultimate] Ziheng Jiang <ziheng@apache.org>\nsig 3        ED03B26E4FC3509F 2020-09-24  Ziheng Jiang <ziheng@apache.org>\nsub   rsa4096 2020-09-24 [E]\nsig          ED03B26E4FC3509F 2020-09-24  Ziheng Jiang <ziheng@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n/GhL5vbglB0H0aG8SVMaTfzr+nXHUVyOSrlYYk34O7bSimVrX6XDGPZpsXE=\n=nhJ/\n-----END PGP PUBLIC KEY BLOCK-----\npub   4096R/D75EFD4B 2020-09-24\nuid                  Zhi Chen <zhic@apache.org>\nsig 3        D75EFD4B 2020-09-24  Zhi Chen <zhic@apache.org>\nsub   4096R/285DD7CB 2020-09-24\nsig          D75EFD4B 2020-09-24  Zhi Chen <zhic@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n8WGuWI9h3RBLY7aFMLpl02cXrsOiMcXC1Uk/e6e14Xpu+Y6IG4KKkUM=\n=GEwA\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa4096 2021-11-10 [SC]\n      C5E5C09030E7BD32DF9A67CE35ABC9676004ADAE\nuid           [ultimate] Junru Shao <junrushao@apache.org>\nsig 3        35ABC9676004ADAE 2021-11-10  Junru Shao <junrushao@apache.org>\nsub   rsa4096 2021-11-10 [E]\nsig          35ABC9676004ADAE 2021-11-10  Junru Shao <junrushao@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n=jNCy\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa4096 2021-11-10 [SC]\n      43C2306EBED3D533F2CFBA8A2C75E5A496C80880\nuid           [ultimate] Wuwei Lin <wuwei@apache.org>\nsig 3        2C75E5A496C80880 2021-11-10  Wuwei Lin <wuwei@apache.org>\nsub   rsa4096 2021-11-10 [E]\nsig          2C75E5A496C80880 2021-11-10  Wuwei Lin <wuwei@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\nMMtVW5sxG8OoBpUIhJUCtYTlwGCyGWSR7+rsHSR2HydLk1RWcYNI3XgJ0ng=\n=+gLd\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa3072 2022-07-12 [SC] [expires: 2024-07-11]\n      B3C6A14C13B8C6727BC2FD2F07FA463F1C926F48\nuid           [ultimate] David Riazati <driazati@apache.org>\nsig 3        07FA463F1C926F48 2022-07-12  David Riazati <driazati@apache.org>\nsub   rsa3072 2022-07-12 [E] [expires: 2024-07-11]\nsig          07FA463F1C926F48 2022-07-12  David Riazati <driazati@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\nrSSBN90DWxIgPhlAqgO0ofkbPSVwF/9i7nd3\n=XBuV\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa4096 2022-10-10 [SC]\n      1B63BD2FFF5E515DA1BEF393C9A56ABD5CCA3EB8\nuid           [ultimate] Andrew Zhao Luo <andrewzhaoluo@apache.org>\nsig 3        C9A56ABD5CCA3EB8 2022-10-10  Andrew Zhao Luo <andrewzhaoluo@apache.org>\nsub   rsa4096 2022-10-10 [E]\nsig          C9A56ABD5CCA3EB8 2022-10-10  Andrew Zhao Luo <andrewzhaoluo@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n/lFRrVmO\n=rAtV\n-----END PGP PUBLIC KEY BLOCK-----\n\npub   rsa4096 2023-02-02 [SC]\n      466390C69A21ABAE77DC63F128D4862222B8EC31\nuid           [ultimate] Leandro Nunes <leandron@apache.org>\nsig 3        28D4862222B8EC31 2023-02-02  Leandro Nunes <leandron@apache.org>\nsub   rsa4096 2023-02-02 [E]\nsig          28D4862222B8EC31 2023-02-02  Leandro Nunes <leandron@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\ntO0=\n=PI53\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa4096 2023-05-05 [SC]\n      664EF29634C05669C3DCF83106D051CA84EF3749\nuid           [ultimate] Siyuan Feng (CODE SIGNING KEY) <syfeng@apache.org>\nsig 3        06D051CA84EF3749 2023-05-05  Siyuan Feng (CODE SIGNING KEY) <syfeng@apache.org>\nsub   rsa4096 2023-05-05 [E]\nsig          06D051CA84EF3749 2023-05-05  Siyuan Feng (CODE SIGNING KEY) <syfeng@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\nwS/UuN04NxDy7V6mPXE=\n=MTba\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa4096 2024-01-15 [SC]\n      A4D9228E55761E665BF01CBB5CE869CB7DEC048C\nuid           [ultimate] Star Yuan (CODE SIGNING KEY) <ysh329@apache.org>\nsig 3        5CE869CB7DEC048C 2024-01-15  Star Yuan (CODE SIGNING KEY) <ysh329@apache.org>\nsub   rsa4096 2024-01-15 [E]\nsig          5CE869CB7DEC048C 2024-01-15  Star Yuan (CODE SIGNING KEY) <ysh329@apache.org>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\nHeZaRpYvkV8UNmnE\n=BtHq\n-----END PGP PUBLIC KEY BLOCK-----\npub   rsa4096 2024-03-10 [SC]\n      298A8AA3D25AFD95D5C89C63C8815953907B66AD\nuid           [ultimate] Ruihang Lai (CODE SIGNING KEY) <ruihangl@apache.org>\nsig 3        C8815953907B66AD 2024-03-10  [self-signature]\nsub   rsa4096 2024-03-10 [E]\nsig          C8815953907B66AD 2024-03-10  [self-signature]\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n2KO2FSofA649SDc2HheDeQ==\n=yNdl\n-----END PGP PUBLIC KEY BLOCK-----\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 12.1318359375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n------------------------------------------------------------------------------------\nThis product bundles various third-party components under other open source licenses.\nThis section summarizes those components and their licenses. See licenses/\nfor text of these licenses.\n\n\nApache Software Foundation License 2.0\n--------------------------------------\n\n3rdparty/dlpack\n3rdparty/dmlc-core\n3rdparty/OpenCL-Headers\n3rdparty/mlperftiny\n3rdparty/nvbench (with LLVM exception)\n3rdparty/cutlass_fpA_intB_gemm\n3rdparty/tensorrt_llm\n\nBSD 2-clause License\n--------------------\n\n3rdparty/picojson\n3rdparty/dmlc-core/include/dmlc/concurrentqueue.h\n\n\nBSD 2-clause License + zlib License\n-----------------------------------\n\n3rdparty/dmlc-core/include/dmlc/blockingconcurrentqueue.h\n\n\nMIT License\n-----------\n\n3rdparty/libcrc\n3rdparty/cma\n3rdparty/compiler-rt/builtin_fp16.h\n3rdparty/cnpy\n\nThe Unlicense\n-------------\n\n3rdparty/rang\n\nBSD 3-Clause \"New\" or \"Revised\" License\n---------------------------------------\n\n3rdparty/cutlass\n3rdparty/libbacktrace\n3rdparty/libflash_attn\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.8935546875,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\n.PHONY: all \\\n        runtime cpptest crttest \\\n        lint pylint cpplint \\\n\tcppdoc docs \\\n\tweb webclean \\\n\tcython cython3 cyclean \\\n        clean\n\n.SECONDEXPANSION:\n\n# Remember the root directory, to be usable by submake invocation.\nROOTDIR = $(CURDIR)\n\n# Specify an alternate output directory relative to ROOTDIR.  Defaults\n# to \"build\".  Can also be a space-separated list of build\n# directories, each with a different configuation.\nTVM_BUILD_PATH ?= build\nTVM_BUILD_PATH := $(abspath $(TVM_BUILD_PATH))\n\n# Allow environment variables for 3rd-party libraries, default to\n# packaged version.\nDMLC_CORE_PATH ?= $(ROOTDIR)/3rdparty/dmlc-core\nDLPACK_PATH ?= $(ROOTDIR)/3rdparty/dlpack\n\nall: $(addsuffix /all,$(TVM_BUILD_PATH))\n\nruntime: $(addsuffix /runtime,$(TVM_BUILD_PATH))\ncpptest: $(addsuffix /cpptest,$(TVM_BUILD_PATH))\ncrttest: $(addsuffix /crttest,$(TVM_BUILD_PATH))\n\n# If there is a config.cmake in the tvm directory, preferentially use\n# it.  Otherwise, copy the default cmake/config.cmake.\nifeq ($(wildcard config.cmake),config.cmake)\n%/config.cmake: | config.cmake\n\t@echo \"No config.cmake found in $(TVM_BUILD_PATH), using config.cmake in root tvm directory\"\n\t@mkdir -p $(@D)\nelse\n# filter-out used to avoid circular dependency\n%/config.cmake: | $$(filter-out %/config.cmake,$(ROOTDIR)/cmake/config.cmake)\n\t@echo \"No config.cmake found in $(TVM_BUILD_PATH), using default config.cmake\"\n\t@mkdir -p $(@D)\n\t@cp $| $@\nendif\n\n\n# Cannot use .PHONY with a pattern rule, using FORCE instead.  For\n# now, force cmake to be re-run with each compile to mimic previous\n# behavior.  This may be relaxed in the future with the\n# CONFIGURE_DEPENDS option for GLOB (requres cmake >= 3.12).\nFORCE:\n%/CMakeCache.txt: %/config.cmake FORCE\n\t@cd $(@D) && cmake $(ROOTDIR)\n\n\n# Since the pattern stem is already being used for the directory name,\n# cannot also have it refer to the command passed to cmake.\n# Therefore, explicitly listing out the delegated.\nCMAKE_TARGETS = all runtime cpptest crttest clean\n\ndefine GEN_CMAKE_RULE\n%/$(CMAKE_TARGET): %/CMakeCache.txt FORCE\n\t@$$(MAKE) -C $$(@D) $(CMAKE_TARGET)\nendef\n$(foreach CMAKE_TARGET,$(CMAKE_TARGETS),$(eval $(GEN_CMAKE_RULE)))\n\n\n\n# Dev tools for formatting, linting, and documenting.  NOTE: lint\n# scripts that are executed in the CI should be in tests/lint. This\n# allows docker/lint.sh to behave similarly to the CI.\nformat:\n\t./tests/lint/git-clang-format.sh -i --rev origin/main\n\tblack .\n\tcd rust && which cargo && cargo fmt --all\n\nlint: cpplint pylint jnilint\n\ncpplint:\n\ttests/lint/cpplint.sh\n\npylint:\n\ttests/lint/pylint.sh\n\njnilint:\n\tpython3 3rdparty/dmlc-core/scripts/lint.py tvm4j-jni cpp jvm/native/src\n\nmypy:\n\ttests/scripts/task_mypy.sh\n\ncppdoc:\n\tdoxygen docs/Doxyfile\n\n\n# Cython build\ncython cython3:\n\tcd python; python3 setup.py build_ext --inplace\n\ncyclean:\n\trm -rf python/tvm/*/*/*.so python/tvm/*/*/*.dylib python/tvm/*/*/*.cpp\n\n\n\n# EMCC; Web related scripts\nweb:\n\t$(MAKE) -C $(ROOTDIR)/web\n\nwebclean:\n\t$(MAKE) -C $(ROOTDIR)/web clean\n\n\n# JVM build rules\nINCLUDE_FLAGS = -Iinclude -I$(DLPACK_PATH)/include -I$(DMLC_CORE_PATH)/include\nPKG_CFLAGS = -std=c++11 -Wall -O2 $(INCLUDE_FLAGS) -fPIC\nPKG_LDFLAGS =\n\nifeq ($(OS),Windows_NT)\n  JVM_PKG_PROFILE := windows\n  SHARED_LIBRARY_SUFFIX := dll\nelse\n  UNAME_S := $(shell uname -s)\n  ifeq ($(UNAME_S), Darwin)\n    JVM_PKG_PROFILE := osx-x86_64\n    SHARED_LIBRARY_SUFFIX := dylib\n  else\n    JVM_PKG_PROFILE := linux-x86_64\n    SHARED_LIBRARY_SUFFIX := so\n  endif\nendif\n\nJVM_TEST_ARGS ?= -DskipTests -Dcheckstyle.skip=true\n\n# Built java docs are in jvm/core/target/site/apidocs\njavadoc:\n\t(cd $(ROOTDIR)/jvm; \\\n\t\tmvn \"javadoc:javadoc\" -Dnotimestamp=true)\n\njvmpkg:\n\t(cd $(ROOTDIR)/jvm; \\\n\t\tmvn clean package -P$(JVM_PKG_PROFILE) -Dcxx=\"$(CXX)\" \\\n\t\t\t-Dcflags=\"$(PKG_CFLAGS)\" -Dldflags=\"$(PKG_LDFLAGS)\" \\\n\t\t\t-Dcurrent_libdir=\"$(TVM_BUILD_PATH)\" $(JVM_TEST_ARGS))\n\njvminstall:\n\t(cd $(ROOTDIR)/jvm; \\\n\t\tmvn install -P$(JVM_PKG_PROFILE) -Dcxx=\"$(CXX)\" \\\n\t\t\t-Dcflags=\"$(PKG_CFLAGS)\" -Dldflags=\"$(PKG_LDFLAGS)\" \\\n\t\t\t-Dcurrent_libdir=\"$(TVM_BUILD_PATH)\" $(JVM_TEST_ARGS))\n\n# Final cleanup rules, delegate to more specific rules.\nclean: $(addsuffix /clean,$(TVM_BUILD_PATH)) cyclean webclean\n\ndocs:\n\tpython3 tests/scripts/ci.py docs\n"
        },
        {
          "name": "NEWS.md",
          "type": "blob",
          "size": 170.6005859375,
          "content": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\nTVM Change Log\n==============\n\n  - [On-going version](#on-going-version)\n  - [0.8](#08)\n    - [Accepted RFCs](#accepted-rfcs)\n    - [Features and Improvements](#features-and-improvements)\n      - [TE, TIR, TVMScript](#te-tir-tvmscript)\n      - [AutoTVM, AutoScheduler, Meta Schedule](#autotvm-autoscheduler-meta-schedule)\n      - [Operator Coverage](#operator-coverage)\n      - [Training](#training)\n      - [Relay](#relay)\n      - [MicroTVM, AOT, Graph Executor and VM](#microtvm-aot-graph-executor-and-vm)\n      - [Arithmetic Analysis](#arithmetic-analysis)\n      - [Frontends](#frontends)\n      - [Codegen Backends and Runtime](#codegen-backends-and-runtime)\n      - [BYOC Integration with Vendor Libraries: TensorRT, ACL, VitisAI](#byoc-integration-with-vendor-libraries-tensorrt-acl-vitisai)\n      - [TVMC](#tvmc)\n      - [Rust Binding](#rust-binding)\n      - [Misc](#misc)\n  - [0.7](#07)\n    - [New Features](#new-features)\n      - [Automatic Scheduling (Experimental)](#automatic-scheduling-experimental)\n      - [BYOC](#byoc)\n      - [Operator Coverage](#operator-coverage-1)\n      - [Quantization](#quantization)\n      - [Relay](#relay-1)\n      - [Runtime and Backend](#runtime-and-backend)\n      - [Rust Support](#rust-support)\n      - [TIR](#tir)\n      - [TE](#te)\n      - [TVMC(Experimental)](#tvmcexperimental)\n    - [Feature Improvement](#feature-improvement)\n      - [Accelerator and Microcontroller Support](#accelerator-and-microcontroller-support)\n      - [Arithmetic Analysis](#arithmetic-analysis-1)\n      - [AutoTVM and Graph Tuner](#autotvm-and-graph-tuner)\n      - [BYOC](#byoc-1)\n      - [Codegen](#codegen)\n      - [Dynamism Support](#dynamism-support)\n      - [Frontend and User Interface](#frontend-and-user-interface)\n      - [Relay](#relay-2)\n      - [Operator Coverage](#operator-coverage-2)\n      - [Runtime and Backend](#runtime-and-backend-1)\n      - [Quantization](#quantization-1)\n      - [TE](#te-1)\n      - [TIR](#tir-1)\n      - [Performance Improvements](#performance-improvements)\n      - [Documentation](#documentation)\n      - [Bug Fixes](#bug-fixes)\n    - [API Changes](#api-changes)\n    - [Deprecation](#deprecation)\n  - [0.6](#06)\n    - [Relay in Production](#relay-in-production)\n    - [Relay Virtual Machine](#relay-virtual-machine)\n    - [Training](#training-1)\n    - [Quantization](#quantization-2)\n    - [Accelerator and Microcontroller Support](#accelerator-and-microcontroller-support-1)\n    - [Rust Support](#rust-support-1)\n    - [Operator Support](#operator-support)\n    - [Frontend and User Interface](#frontend-and-user-interface-1)\n    - [Runtime and Backend Support](#runtime-and-backend-support)\n    - [Language and Architecture](#language-and-architecture)\n    - [Symbolic shape enhancement](#symbolic-shape-enhancement)\n    - [Language and Architecture](#language-and-architecture-1)\n    - [Arithmetic Analysis](#arithmetic-analysis-2)\n    - [Runtime and Backend Support](#runtime-and-backend-support-1)\n    - [Frontend and User Interface](#frontend-and-user-interface-2)\n    - [AutoTVM](#autotvm)\n    - [Performance Improvements](#performance-improvements-1)\n    - [Documentation](#documentation-1)\n    - [Build and Test](#build-and-test)\n    - [Bug Fixes](#bug-fixes-1)\n    - [Known Issues](#known-issues)\n    - [Depreciations](#depreciations)\n  - [0.5](#05)\n  - [0.4](#04)\n  - [0.3](#03)\n  - [0.2](#02)\n  - [0.1](#01)\n  - [Initial version](#initial-version)\n\nThis file records the changes in TVM library in reverse chronological order.\n\n## On-going version\n\nRefer to the Roadmap issue for complete list on on-going version features.\nIf you check in something that is not reflected in Roadmap issue, please reply\nto that issue so it can get added.\n\n## 0.8\n\nApache TVM v0.8 brings several major exciting experimental features, including:\n- PaddlePaddle frontend\n- TVMScript: round-trippable python-based syntax for TIR\n- TorchScript integration\n- TensorIR scheduling language\n- TensorRT and CUTLASS integration via BYOC\n- Int4 TensorCore support in AutoTVM\n- MicroTVM Project API and Zephyr, Arduino support\n- AOT executor\n- Robost Windows support\n- Affine analysis infra: iter-affine-map\n- Improved Vulkan backend\n- CUDA graph support in TVM runtime\n\nBesides, The community has been working together to refactor and evolve the existing infrastructure, including but not limited to:\n- Relay compilation engine\n- Relay pattern language\n- CI and build process\n- Refactoring documentation and tutorials\n- Stablizing AutoScheduler\n- Stablizing TVMC command line driver interface\n- Stablizing target system\n- Frontend coverage, quantization, dynamic shape, training\n\nFull changelog: https://gist.github.com/junrushao1994/c669905dbc41edc2e691316df49d8562.\n\n### Accepted RFCs\n\nThe community has adopted a [formal RFC process](https://github.com/apache/tvm-rfcs). Below is a list of the formal RFCs accepted by the community since then:\n- [RFC-0005] Meta schedule (AutoTIR)\n- [RFC-0006] Automatic mixed-precision pass and support\n- [RFC-0007] Parametrized unit tests\n- [RFC-0008] MicroTVM Project API\n- [RFC-0009] Unified static memory planner\n- [RFC-0010] Target-registered compiler flow customisation\n- [RFC-0011] Arm Ethos-U integration\n- [RFC-0014] Pipeline executor\n- [RFC-0015] Use CMSIS-NN with TVM\n- [RFC-0019] Add PaddlePaddle frontend\n- [RFC-0020] Extend metadata in project option\n- [RFC-0022] TIR non-scalar constants\n- [RFC-0023] Adding annotation field to `tir.allocate` nodes\n- [RFC-0025] PyTorchTVM\n- [RFC-0027] Formalize TVM documentation organization\n- [RFC-0028] Command line composition from internal registry\n- [RFC-0029] Migrating target attributes to IRModule\n- [RFC-0030] Command line configuration files\n- [RFC-0031] C Device API\n- [RFC-0036] TVMScript namespace\n- [RFC-0041] Update TVMScript block syntax\n\n### Features and Improvements\n#### TE, TIR, TVMScript\n\n- TVMScript parser and printer [#7630](https://github.com/apache/tvm/pull/7630) [#9115](https://github.com/apache/tvm/pull/9115) [#9286](https://github.com/apache/tvm/pull/9286)\n- Scheduleable TIR (S-TIR) infrastructure, analysis and lowering passes [#7553](https://github.com/apache/tvm/pull/7553) [#7765](https://github.com/apache/tvm/pull/7765) [#7847](https://github.com/apache/tvm/pull/7847) [#8114](https://github.com/apache/tvm/pull/8114) [#8121](https://github.com/apache/tvm/pull/8121) [#7873](https://github.com/apache/tvm/pull/7873) [#7923](https://github.com/apache/tvm/pull/7923) [#7962](https://github.com/apache/tvm/pull/7962) [#7848](https://github.com/apache/tvm/pull/7848) [#8044](https://github.com/apache/tvm/pull/8044) [#7806](https://github.com/apache/tvm/pull/7806)\n- S-TIR schedule primitives: `compute-inline`, `reverse-compute-inline`, `fuse`, `split`, `rfactor`, `storage-align`, `vectorize`, `unroll`, `bind`, `reorder`, `cache-read`, `cache-write`, `compute-at`, `reverse-compute-at`, `decompose-reduction` [#8170](https://github.com/apache/tvm/pull/8170) [#8467](https://github.com/apache/tvm/pull/8467) [#8544](https://github.com/apache/tvm/pull/8544) [#8693](https://github.com/apache/tvm/pull/8693) [#8716](https://github.com/apache/tvm/pull/8716) [#8767](https://github.com/apache/tvm/pull/8767) [#8863](https://github.com/apache/tvm/pull/8863) [#8943](https://github.com/apache/tvm/pull/8943) [#9041](https://github.com/apache/tvm/pull/9041)\n- While loop in TIR [#7425](https://github.com/apache/tvm/pull/7425) [#9004](https://github.com/apache/tvm/pull/9004)\n- Metaprogramming in S-TIR via `specialize` [#8354](https://github.com/apache/tvm/pull/8354)\n- Support Return value in TIR [#7084](https://github.com/apache/tvm/pull/7084) [#7932](https://github.com/apache/tvm/pull/7932)\n- Storage scope support in `PointerType` [#8017](https://github.com/apache/tvm/pull/8017) [#8366](https://github.com/apache/tvm/pull/8366) [#8463](https://github.com/apache/tvm/pull/8463)\n- Creation of S-TIR via TE compute [#7987](https://github.com/apache/tvm/pull/7987)\n\n#### AutoTVM, AutoScheduler, Meta Schedule\n\n- PopenPoolExecutor is used to replace python native library to provide better multiprocessing support as well as enable auto-tuning in Jupyter notebooks for AutoTVM and AutoScheduler [#6959](https://github.com/apache/tvm/pull/6959) [#8492](https://github.com/apache/tvm/pull/8492) [#8913](https://github.com/apache/tvm/pull/8913) [#8820](https://github.com/apache/tvm/pull/8820) [#8851](https://github.com/apache/tvm/pull/8851)\n- AutoScheduler improvement and stabilization: task scheduler, layout rewrite, early stopping, dispatching [#6945](https://github.com/apache/tvm/pull/6945) [#6750](https://github.com/apache/tvm/pull/6750) [#6987](https://github.com/apache/tvm/pull/6987) [#7156](https://github.com/apache/tvm/pull/7156) [#8862](https://github.com/apache/tvm/pull/8862) [#8995](https://github.com/apache/tvm/pull/8995) [#7571](https://github.com/apache/tvm/pull/7571) [#7376](https://github.com/apache/tvm/pull/7376) [#7377](https://github.com/apache/tvm/pull/7377) [#7344](https://github.com/apache/tvm/pull/7344) [#7185](https://github.com/apache/tvm/pull/7185)\n- AutoScheduler support for sparse workloads [#7313](https://github.com/apache/tvm/pull/7313) [#7635](https://github.com/apache/tvm/pull/7635) [#8065](https://github.com/apache/tvm/pull/8065)\n- AutoScheduler support for Vulkan, ROCm, Mali [#7626](https://github.com/apache/tvm/pull/7626) [#7038](https://github.com/apache/tvm/pull/7038) [#7132](https://github.com/apache/tvm/pull/7132)\n- AutoTVM support for int4 TensorCore [#7831](https://github.com/apache/tvm/pull/7831) [#8402](https://github.com/apache/tvm/pull/8402)\n- Meta Schedule core infrastructure, builder runner and database [#8615](https://github.com/apache/tvm/pull/8615) [#8623](https://github.com/apache/tvm/pull/8623) [#8642](https://github.com/apache/tvm/pull/8642) [#8817](https://github.com/apache/tvm/pull/8817) [#9079](https://github.com/apache/tvm/pull/9079) [#9132](https://github.com/apache/tvm/pull/9132) [#9154](https://github.com/apache/tvm/pull/9154) [#9053](https://github.com/apache/tvm/pull/9053) [#9059](https://github.com/apache/tvm/pull/9059) [#9044](https://github.com/apache/tvm/pull/9044) [#9111](https://github.com/apache/tvm/pull/9111) [#9061](https://github.com/apache/tvm/pull/9061) [#9153](https://github.com/apache/tvm/pull/9153)\n\n#### Operator Coverage\n- Operators for Int-8 vision transformer on GPU [#7814](https://github.com/apache/tvm/pull/7814)\n- Optimizing NMS and ROI-related kernel on GPU [#7257](https://github.com/apache/tvm/pull/7257) [#7172](https://github.com/apache/tvm/pull/7172) [#7136](https://github.com/apache/tvm/pull/7136) [#7796](https://github.com/apache/tvm/pull/7796) [#7463](https://github.com/apache/tvm/pull/7463) [#6516](https://github.com/apache/tvm/pull/6516) [#7440](https://github.com/apache/tvm/pull/7440) [#7666](https://github.com/apache/tvm/pull/7666) [#8174](https://github.com/apache/tvm/pull/8174)\n- Support and optimize sparse operators [#8605](https://github.com/apache/tvm/pull/8605) [#7477](https://github.com/apache/tvm/pull/7477) [#7435](https://github.com/apache/tvm/pull/7435) [#6889](https://github.com/apache/tvm/pull/6889) [#6580](https://github.com/apache/tvm/pull/6580) [#8437](https://github.com/apache/tvm/pull/8437)\n- Sort-related operators and optimization [#9184](https://github.com/apache/tvm/pull/9184) [#7669](https://github.com/apache/tvm/pull/7669) [#8672](https://github.com/apache/tvm/pull/8672) [#7611](https://github.com/apache/tvm/pull/7611) [#7195](https://github.com/apache/tvm/pull/7195) [#7056](https://github.com/apache/tvm/pull/7056) [#6978](https://github.com/apache/tvm/pull/6978)\n- Support for einsum operator [#6370](https://github.com/apache/tvm/pull/6370)\n- Matmul, dense operators and their optimization [#8921](https://github.com/apache/tvm/pull/8921) [#8527](https://github.com/apache/tvm/pull/8527) [#8234](https://github.com/apache/tvm/pull/8234) [#8250](https://github.com/apache/tvm/pull/8250) [#6616](https://github.com/apache/tvm/pull/6616) [#8229](https://github.com/apache/tvm/pull/8229) [#8401](https://github.com/apache/tvm/pull/8401) [#7404](https://github.com/apache/tvm/pull/7404) [#8669](https://github.com/apache/tvm/pull/8669)\n- Convolution and pooling operators and their optimization [#8620](https://github.com/apache/tvm/pull/8620) [#8936](https://github.com/apache/tvm/pull/8936) [#8584](https://github.com/apache/tvm/pull/8584) [#7075](https://github.com/apache/tvm/pull/7075) [#7142](https://github.com/apache/tvm/pull/7142) [#7515](https://github.com/apache/tvm/pull/7515) [#6999](https://github.com/apache/tvm/pull/6999) [#6899](https://github.com/apache/tvm/pull/6899) [#6840](https://github.com/apache/tvm/pull/6840) [#6137](https://github.com/apache/tvm/pull/6137) [#6802](https://github.com/apache/tvm/pull/6802) [#6445](https://github.com/apache/tvm/pull/6445) [#6711](https://github.com/apache/tvm/pull/6711) [#6714](https://github.com/apache/tvm/pull/6714) [#8167](https://github.com/apache/tvm/pull/8167) [#8222](https://github.com/apache/tvm/pull/8222) [#8275](https://github.com/apache/tvm/pull/8275) [#8276](https://github.com/apache/tvm/pull/8276) [#8422](https://github.com/apache/tvm/pull/8422) [#8430](https://github.com/apache/tvm/pull/8430) [#6687](https://github.com/apache/tvm/pull/6687) [#7928](https://github.com/apache/tvm/pull/7928) [#8897](https://github.com/apache/tvm/pull/8897)\n- Scatter and gather operators and their optimization [#8479](https://github.com/apache/tvm/pull/8479) [#7600](https://github.com/apache/tvm/pull/7600) [#7044](https://github.com/apache/tvm/pull/7044) [#7464](https://github.com/apache/tvm/pull/7464) [#7233](https://github.com/apache/tvm/pull/7233) [#6533](https://github.com/apache/tvm/pull/6533) [#6856](https://github.com/apache/tvm/pull/6856) [#6854](https://github.com/apache/tvm/pull/6854) [#7927](https://github.com/apache/tvm/pull/7927) [#8105](https://github.com/apache/tvm/pull/8105)\n- Prefix scan, cumsum and cumprod [#7722](https://github.com/apache/tvm/pull/7722) [#7303](https://github.com/apache/tvm/pull/7303) [#7314](https://github.com/apache/tvm/pull/7314) [#7334](https://github.com/apache/tvm/pull/7334) [#7123](https://github.com/apache/tvm/pull/7123) [#6868](https://github.com/apache/tvm/pull/6868)\n- Dynamic shape and shape functions [#7414](https://github.com/apache/tvm/pull/7414) [#6979](https://github.com/apache/tvm/pull/6979) [#6912](https://github.com/apache/tvm/pull/6912) [#6898](https://github.com/apache/tvm/pull/6898) [#6373](https://github.com/apache/tvm/pull/6373) [#8068](https://github.com/apache/tvm/pull/8068) [#7490](https://github.com/apache/tvm/pull/7490) [#7487](https://github.com/apache/tvm/pull/7487)\n- Miscellaneous improvement. Operators including: reshape, resize, pad, PRNG, transpose, where, softmax, concat, nll_loss, space_to_batch_nd, batch_to_space_nd, slice_like; Libraries including thrust, cuDNN, cuBLAS, MIOpen; Improving schedules for generic reduction and softmax. [#8592](https://github.com/apache/tvm/pull/8592) [#7375](https://github.com/apache/tvm/pull/7375) [#7287](https://github.com/apache/tvm/pull/7287) [#7184](https://github.com/apache/tvm/pull/7184) [#7131](https://github.com/apache/tvm/pull/7131) [#7086](https://github.com/apache/tvm/pull/7086) [#7083](https://github.com/apache/tvm/pull/7083) [#8030](https://github.com/apache/tvm/pull/8030) [#6851](https://github.com/apache/tvm/pull/6851) [#6477](https://github.com/apache/tvm/pull/6477) [#8346](https://github.com/apache/tvm/pull/8346) [#6759](https://github.com/apache/tvm/pull/6759) [#8028](https://github.com/apache/tvm/pull/8028) [#8056](https://github.com/apache/tvm/pull/8056) [#8369](https://github.com/apache/tvm/pull/8369) [#7468](https://github.com/apache/tvm/pull/7468) [#7458](https://github.com/apache/tvm/pull/7458) [#7194](https://github.com/apache/tvm/pull/7194) [#8138](https://github.com/apache/tvm/pull/8138) [#8543](https://github.com/apache/tvm/pull/8543)\n\n#### Training\n\n- Relay AutoDiff [#7677](https://github.com/apache/tvm/pull/7677) [#8318](https://github.com/apache/tvm/pull/8318)\n- TE AutoDiff [#7321](https://github.com/apache/tvm/pull/7321)\n- Gradient operators [#7685](https://github.com/apache/tvm/pull/7685) [#7340](https://github.com/apache/tvm/pull/7340) [#6767](https://github.com/apache/tvm/pull/6767) [#8307](https://github.com/apache/tvm/pull/8307) [#7357](https://github.com/apache/tvm/pull/7357) [#6827](https://github.com/apache/tvm/pull/6827)\n\n#### Relay\n\n- Pattern language and mixed-mode visitor: matching more IR constructs, fuzzy matching; converting more passes to non-recursive.  [#8843](https://github.com/apache/tvm/pull/8843) [#7754](https://github.com/apache/tvm/pull/7754) [#7355](https://github.com/apache/tvm/pull/7355) [#7332](https://github.com/apache/tvm/pull/7332) [#7282](https://github.com/apache/tvm/pull/7282) [#7151](https://github.com/apache/tvm/pull/7151) [#7120](https://github.com/apache/tvm/pull/7120) [#6958](https://github.com/apache/tvm/pull/6958) [#7507](https://github.com/apache/tvm/pull/7507) [#8325](https://github.com/apache/tvm/pull/8325) [#8774](https://github.com/apache/tvm/pull/8774) [#7817](https://github.com/apache/tvm/pull/7817) [#7374](https://github.com/apache/tvm/pull/7374) [#6695](https://github.com/apache/tvm/pull/6695) [#6704](https://github.com/apache/tvm/pull/6704)\n- Improving or adding passes including ExtractOperators, SimplifyExpr, DynamicToStatic, DefuseOps, ConvertLayout, FoldConstant. Added a set of utilities that allows a model to be run efficiently on TensorCores [#9253](https://github.com/apache/tvm/pull/9253) [#9245](https://github.com/apache/tvm/pull/9245) [#8996](https://github.com/apache/tvm/pull/8996) [#7827](https://github.com/apache/tvm/pull/7827) [#9034](https://github.com/apache/tvm/pull/9034) [#7807](https://github.com/apache/tvm/pull/7807) [#8755](https://github.com/apache/tvm/pull/8755) [#7731](https://github.com/apache/tvm/pull/7731) [#7368](https://github.com/apache/tvm/pull/7368) [#7603](https://github.com/apache/tvm/pull/7603) [#7656](https://github.com/apache/tvm/pull/7656) [#7423](https://github.com/apache/tvm/pull/7423) [#7354](https://github.com/apache/tvm/pull/7354) [#6946](https://github.com/apache/tvm/pull/6946) [#6748](https://github.com/apache/tvm/pull/6748) [#6720](https://github.com/apache/tvm/pull/6720) [#6776](https://github.com/apache/tvm/pull/6776) [#7835](https://github.com/apache/tvm/pull/7835) [#7895](https://github.com/apache/tvm/pull/7895) [#8205](https://github.com/apache/tvm/pull/8205)\n- TECompiler and refactoring of compilation workflow [#9103](https://github.com/apache/tvm/pull/9103) [#8974](https://github.com/apache/tvm/pull/8974) [#8886](https://github.com/apache/tvm/pull/8886) [#8802](https://github.com/apache/tvm/pull/8802) [#8501](https://github.com/apache/tvm/pull/8501) [#8526](https://github.com/apache/tvm/pull/8526) [#8486](https://github.com/apache/tvm/pull/8486) [#8597](https://github.com/apache/tvm/pull/8597) [#7518](https://github.com/apache/tvm/pull/7518) [#7552](https://github.com/apache/tvm/pull/7552) [#8914](https://github.com/apache/tvm/pull/8914) [#9130](https://github.com/apache/tvm/pull/9130)\n- Quantization and automatic-mixed precision [#8883](https://github.com/apache/tvm/pull/8883) [#8810](https://github.com/apache/tvm/pull/8810) [#8644](https://github.com/apache/tvm/pull/8644) [#7613](https://github.com/apache/tvm/pull/7613) [#8069](https://github.com/apache/tvm/pull/8069) [#8341](https://github.com/apache/tvm/pull/8341) [#8126](https://github.com/apache/tvm/pull/8126) [#8460](https://github.com/apache/tvm/pull/8460)\n- Parser, printer and diagnostic [#7347](https://github.com/apache/tvm/pull/7347) [#6274](https://github.com/apache/tvm/pull/6274) [#6692](https://github.com/apache/tvm/pull/6692) [#8352](https://github.com/apache/tvm/pull/8352) [#8000](https://github.com/apache/tvm/pull/8000)\n\n#### MicroTVM, AOT, Graph Executor and VM\n\n- Pipeline Executor [#8702](https://github.com/apache/tvm/pull/8702) [#9108](https://github.com/apache/tvm/pull/9108)\n- CUDA graph integration in graph executor [#7616](https://github.com/apache/tvm/pull/7616)\n- Enable add `set_output_zero_copy` in graph executor [#8497](https://github.com/apache/tvm/pull/8497)\n- VM: memory allocation improvement, shape function improvement and misc [#7746](https://github.com/apache/tvm/pull/7746) [#7451](https://github.com/apache/tvm/pull/7451) [#7413](https://github.com/apache/tvm/pull/7413) [#7210](https://github.com/apache/tvm/pull/7210) [#8040](https://github.com/apache/tvm/pull/8040) [#6938](https://github.com/apache/tvm/pull/6938) [#8661](https://github.com/apache/tvm/pull/8661) [#7676](https://github.com/apache/tvm/pull/7676) [#8285](https://github.com/apache/tvm/pull/8285)\n- AOT compilation and execution [#8697](https://github.com/apache/tvm/pull/8697) [#7785](https://github.com/apache/tvm/pull/7785) [#8014](https://github.com/apache/tvm/pull/8014) [#8023](https://github.com/apache/tvm/pull/8023) [#8096](https://github.com/apache/tvm/pull/8096) [#8075](https://github.com/apache/tvm/pull/8075)\n- Project API infrastructure: [#8380](https://github.com/apache/tvm/pull/8380) [#8963](https://github.com/apache/tvm/pull/8963) [#8708](https://github.com/apache/tvm/pull/8708) [#8019](https://github.com/apache/tvm/pull/8019)\n- MicroTVM, Zephyr, Arduino RVM, AutoTVM support [#9320](https://github.com/apache/tvm/pull/9320) [#8941](https://github.com/apache/tvm/pull/8941) [#7804](https://github.com/apache/tvm/pull/7804) [#7786](https://github.com/apache/tvm/pull/7786) [#7449](https://github.com/apache/tvm/pull/7449) [#7891](https://github.com/apache/tvm/pull/7891) [#7915](https://github.com/apache/tvm/pull/7915) [#8055](https://github.com/apache/tvm/pull/8055) [#8037](https://github.com/apache/tvm/pull/8037) [#8386](https://github.com/apache/tvm/pull/8386) [#8519](https://github.com/apache/tvm/pull/8519) [#8748](https://github.com/apache/tvm/pull/8748) [8154](https://github.com/apache/tvm/pull/8154) [#8945](https://github.com/apache/tvm/pull/8945) [#8624](https://github.com/apache/tvm/pull/8624) [#8701](https://github.com/apache/tvm/pull/8701) [#7723](https://github.com/apache/tvm/pull/7723) [#8715](https://github.com/apache/tvm/pull/8715) [#7225](https://github.com/apache/tvm/pull/7225) [#6964](https://github.com/apache/tvm/pull/6964) [#7813](https://github.com/apache/tvm/pull/7813) [#7528](https://github.com/apache/tvm/pull/7528)\n- The pure C runtime (CRT) [#7398](https://github.com/apache/tvm/pull/7398) [#7333](https://github.com/apache/tvm/pull/7333) [#7095](https://github.com/apache/tvm/pull/7095) [#7225](https://github.com/apache/tvm/pull/7225)\n- Model library format [#8270](https://github.com/apache/tvm/pull/8270) [#8072](https://github.com/apache/tvm/pull/8072) [#7938](https://github.com/apache/tvm/pull/7938)\n\n#### Arithmetic Analysis\n\n- Tighter bounds and more simplification on cast [#6771](https://github.com/apache/tvm/pull/6771) [#7045](https://github.com/apache/tvm/pull/7045)\n- Introducing iterator (quasi-) affine map detection [#6667](https://github.com/apache/tvm/pull/6667) [#7752](https://github.com/apache/tvm/pull/7752) [#7759](https://github.com/apache/tvm/pull/7759)\n- Inverse of iterator affine map [#8384](https://github.com/apache/tvm/pull/8384) [#8427](https://github.com/apache/tvm/pull/8427)\n- Subspace division in iterator affine map [#7760](https://github.com/apache/tvm/pull/7760)\n\n#### Frontends\n\n- PaddlePaddle initial support [#8645](https://github.com/apache/tvm/pull/8645)  [#9124](https://github.com/apache/tvm/pull/9124) [#9126](https://github.com/apache/tvm/pull/9126) [#9295](https://github.com/apache/tvm/pull/9295) [#9370](https://github.com/apache/tvm/pull/9370) [#9236](https://github.com/apache/tvm/pull/9236) [#9283](https://github.com/apache/tvm/pull/9283)\n- ONNX support, including better handling of control flow, coverage of more operators, better dynamic shape support, more tests. [#9265](https://github.com/apache/tvm/pull/9265) [#9178](https://github.com/apache/tvm/pull/9178) [#9146](https://github.com/apache/tvm/pull/9146) [#8894](https://github.com/apache/tvm/pull/8894) [#8966](https://github.com/apache/tvm/pull/8966) [#8967](https://github.com/apache/tvm/pull/8967) [#7818](https://github.com/apache/tvm/pull/7818) [#9000](https://github.com/apache/tvm/pull/9000) [#9001](https://github.com/apache/tvm/pull/9001) [#9066](https://github.com/apache/tvm/pull/9066) [#9028](https://github.com/apache/tvm/pull/9028) [#9002](https://github.com/apache/tvm/pull/9002) [#8985](https://github.com/apache/tvm/pull/8985) [#9019](https://github.com/apache/tvm/pull/9019) [#9017](https://github.com/apache/tvm/pull/9017) [#8972](https://github.com/apache/tvm/pull/8972) [#7802](https://github.com/apache/tvm/pull/7802) [#7800](https://github.com/apache/tvm/pull/7800) [#7781](https://github.com/apache/tvm/pull/7781) [#8919](https://github.com/apache/tvm/pull/8919) [#9054](https://github.com/apache/tvm/pull/9054) [#8906](https://github.com/apache/tvm/pull/8906) [#8933](https://github.com/apache/tvm/pull/8933) [#8959](https://github.com/apache/tvm/pull/8959) [#8907](https://github.com/apache/tvm/pull/8907) [#7771](https://github.com/apache/tvm/pull/7771) [#8923](https://github.com/apache/tvm/pull/8923) [#8924](https://github.com/apache/tvm/pull/8924) [#7755](https://github.com/apache/tvm/pull/7755) [#7720](https://github.com/apache/tvm/pull/7720) [#8773](https://github.com/apache/tvm/pull/8773) [#8872](https://github.com/apache/tvm/pull/8872) [#7655](https://github.com/apache/tvm/pull/7655) [#8741](https://github.com/apache/tvm/pull/8741) [#7633](https://github.com/apache/tvm/pull/7633) [#8781](https://github.com/apache/tvm/pull/8781) [#8866](https://github.com/apache/tvm/pull/8866) [#8867](https://github.com/apache/tvm/pull/8867) [#7522](https://github.com/apache/tvm/pull/7522) [#7519](https://github.com/apache/tvm/pull/7519) [#7489](https://github.com/apache/tvm/pull/7489) [#7438](https://github.com/apache/tvm/pull/7438) [#7429](https://github.com/apache/tvm/pull/7429) [#7364](https://github.com/apache/tvm/pull/7364) [#7300](https://github.com/apache/tvm/pull/7300) [#7259](https://github.com/apache/tvm/pull/7259) [#7243](https://github.com/apache/tvm/pull/7243) [#7237](https://github.com/apache/tvm/pull/7237) [#7208](https://github.com/apache/tvm/pull/7208) [#7189](https://github.com/apache/tvm/pull/7189) [#7115](https://github.com/apache/tvm/pull/7115) [#7109](https://github.com/apache/tvm/pull/7109) [#7089](https://github.com/apache/tvm/pull/7089) [#7036](https://github.com/apache/tvm/pull/7036) [#7031](https://github.com/apache/tvm/pull/7031) [#6839](https://github.com/apache/tvm/pull/6839) [#6351](https://github.com/apache/tvm/pull/6351) [#7842](https://github.com/apache/tvm/pull/7842) [#7844](https://github.com/apache/tvm/pull/7844) [#6646](https://github.com/apache/tvm/pull/6646) [#6647](https://github.com/apache/tvm/pull/6647) [#6681](https://github.com/apache/tvm/pull/6681) [#6700](https://github.com/apache/tvm/pull/6700) [#7883](https://github.com/apache/tvm/pull/7883) [#6726](https://github.com/apache/tvm/pull/6726) [#6730](https://github.com/apache/tvm/pull/6730) [#7899](https://github.com/apache/tvm/pull/7899) [#7900](https://github.com/apache/tvm/pull/7900) [#7906](https://github.com/apache/tvm/pull/7906) [#7934](https://github.com/apache/tvm/pull/7934) [#7956](https://github.com/apache/tvm/pull/7956) [#8007](https://github.com/apache/tvm/pull/8007) [#8011](https://github.com/apache/tvm/pull/8011) [#8084](https://github.com/apache/tvm/pull/8084) [#8099](https://github.com/apache/tvm/pull/8099) [#8189](https://github.com/apache/tvm/pull/8189) [#8191](https://github.com/apache/tvm/pull/8191) [#8304](https://github.com/apache/tvm/pull/8304) [#8321](https://github.com/apache/tvm/pull/8321) [#8337](https://github.com/apache/tvm/pull/8337) [#8356](https://github.com/apache/tvm/pull/8356) [#8385](https://github.com/apache/tvm/pull/8385) [#8502](https://github.com/apache/tvm/pull/8502) [#8426](https://github.com/apache/tvm/pull/8426) [#8440](https://github.com/apache/tvm/pull/8440) [#8456](https://github.com/apache/tvm/pull/8456) [#8475](https://github.com/apache/tvm/pull/8475) [#7391](https://github.com/apache/tvm/pull/7391) [#7394](https://github.com/apache/tvm/pull/7394) [#8621](https://github.com/apache/tvm/pull/8621) [#8322](https://github.com/apache/tvm/pull/8322) [#8323](https://github.com/apache/tvm/pull/8323) [#8435](https://github.com/apache/tvm/pull/8435) [#8436](https://github.com/apache/tvm/pull/8436) [#8455](https://github.com/apache/tvm/pull/8455) [#7353](https://github.com/apache/tvm/pull/7353) [#7215](https://github.com/apache/tvm/pull/7215)\n- TensorFlow and TFLite, including more operators, better TensorArray support and quantization [#9404](https://github.com/apache/tvm/pull/9404) [#9256](https://github.com/apache/tvm/pull/9256) [#8689](https://github.com/apache/tvm/pull/8689) [#7789](https://github.com/apache/tvm/pull/7789) [#7736](https://github.com/apache/tvm/pull/7736) [#8763](https://github.com/apache/tvm/pull/8763) [#8647](https://github.com/apache/tvm/pull/8647) [#8648](https://github.com/apache/tvm/pull/8648) [#8558](https://github.com/apache/tvm/pull/8558) [#8780](https://github.com/apache/tvm/pull/8780) [#8538](https://github.com/apache/tvm/pull/8538) [#7659](https://github.com/apache/tvm/pull/7659) [#7639](https://github.com/apache/tvm/pull/7639) [#7531](https://github.com/apache/tvm/pull/7531) [#7520](https://github.com/apache/tvm/pull/7520) [#7502](https://github.com/apache/tvm/pull/7502) [#7496](https://github.com/apache/tvm/pull/7496) [#7473](https://github.com/apache/tvm/pull/7473) [#7452](https://github.com/apache/tvm/pull/7452) [#7442](https://github.com/apache/tvm/pull/7442) [#7441](https://github.com/apache/tvm/pull/7441) [#7400](https://github.com/apache/tvm/pull/7400) [#7320](https://github.com/apache/tvm/pull/7320) [#7293](https://github.com/apache/tvm/pull/7293) [#7267](https://github.com/apache/tvm/pull/7267) [#7159](https://github.com/apache/tvm/pull/7159) [#7148](https://github.com/apache/tvm/pull/7148) [#7114](https://github.com/apache/tvm/pull/7114) [#7113](https://github.com/apache/tvm/pull/7113) [#7093](https://github.com/apache/tvm/pull/7093) [#7074](https://github.com/apache/tvm/pull/7074) [#7048](https://github.com/apache/tvm/pull/7048) [#7030](https://github.com/apache/tvm/pull/7030) [#6998](https://github.com/apache/tvm/pull/6998) [#6984](https://github.com/apache/tvm/pull/6984) [#6970](https://github.com/apache/tvm/pull/6970) [#6949](https://github.com/apache/tvm/pull/6949) [#6933](https://github.com/apache/tvm/pull/6933) [#6918](https://github.com/apache/tvm/pull/6918) [#6901](https://github.com/apache/tvm/pull/6901) [#6885](https://github.com/apache/tvm/pull/6885) [#6849](https://github.com/apache/tvm/pull/6849) [#5767](https://github.com/apache/tvm/pull/5767) [#6589](https://github.com/apache/tvm/pull/6589) [#6670](https://github.com/apache/tvm/pull/6670) [#6674](https://github.com/apache/tvm/pull/6674) [#6675](https://github.com/apache/tvm/pull/6675) [#7866](https://github.com/apache/tvm/pull/7866) [#6685](https://github.com/apache/tvm/pull/6685) [#7885](https://github.com/apache/tvm/pull/7885) [#6729](https://github.com/apache/tvm/pull/6729) [#7901](https://github.com/apache/tvm/pull/7901) [#6774](https://github.com/apache/tvm/pull/6774) [#6783](https://github.com/apache/tvm/pull/6783) [#6799](https://github.com/apache/tvm/pull/6799) [#7951](https://github.com/apache/tvm/pull/7951) [#8024](https://github.com/apache/tvm/pull/8024) [#8051](https://github.com/apache/tvm/pull/8051) [#8060](https://github.com/apache/tvm/pull/8060) [#8074](https://github.com/apache/tvm/pull/8074) [#8142](https://github.com/apache/tvm/pull/8142) [#8179](https://github.com/apache/tvm/pull/8179) [#8251](https://github.com/apache/tvm/pull/8251) [#8277](https://github.com/apache/tvm/pull/8277) [#8335](https://github.com/apache/tvm/pull/8335) [#8364](https://github.com/apache/tvm/pull/8364) [#8375](https://github.com/apache/tvm/pull/8375) [#8431](https://github.com/apache/tvm/pull/8431) [#8454](https://github.com/apache/tvm/pull/8454) [#6818](https://github.com/apache/tvm/pull/6818) [#8483](https://github.com/apache/tvm/pull/8483) [#9099](https://github.com/apache/tvm/pull/9099) [#9165](https://github.com/apache/tvm/pull/9165)\n- PyTorch: more operators including activations, inplace operators, RNNs, NMS [#9371](https://github.com/apache/tvm/pull/9371) [#9204](https://github.com/apache/tvm/pull/9204) [#9185](https://github.com/apache/tvm/pull/9185) [#9135](https://github.com/apache/tvm/pull/9135) [#9133](https://github.com/apache/tvm/pull/9133) [#9015](https://github.com/apache/tvm/pull/9015) [#8839](https://github.com/apache/tvm/pull/8839) [#8718](https://github.com/apache/tvm/pull/8718) [#8699](https://github.com/apache/tvm/pull/8699) [#8692](https://github.com/apache/tvm/pull/8692) [#7712](https://github.com/apache/tvm/pull/7712) [#8753](https://github.com/apache/tvm/pull/8753) [#7694](https://github.com/apache/tvm/pull/7694) [#8583](https://github.com/apache/tvm/pull/8583) [#7675](https://github.com/apache/tvm/pull/7675) [#7646](https://github.com/apache/tvm/pull/7646) [#7606](https://github.com/apache/tvm/pull/7606) [#7592](https://github.com/apache/tvm/pull/7592) [#7569](https://github.com/apache/tvm/pull/7569) [#7544](https://github.com/apache/tvm/pull/7544) [#7549](https://github.com/apache/tvm/pull/7549) [#7535](https://github.com/apache/tvm/pull/7535) [#7517](https://github.com/apache/tvm/pull/7517) [#7465](https://github.com/apache/tvm/pull/7465) [#7397](https://github.com/apache/tvm/pull/7397) [#7371](https://github.com/apache/tvm/pull/7371) [#7348](https://github.com/apache/tvm/pull/7348) [#7346](https://github.com/apache/tvm/pull/7346) [#7325](https://github.com/apache/tvm/pull/7325) [#7231](https://github.com/apache/tvm/pull/7231) [#7174](https://github.com/apache/tvm/pull/7174) [#7154](https://github.com/apache/tvm/pull/7154) [#7137](https://github.com/apache/tvm/pull/7137) [#7134](https://github.com/apache/tvm/pull/7134) [#7133](https://github.com/apache/tvm/pull/7133) [#7128](https://github.com/apache/tvm/pull/7128) [#7088](https://github.com/apache/tvm/pull/7088) [#7023](https://github.com/apache/tvm/pull/7023) [#6900](https://github.com/apache/tvm/pull/6900) [#6602](https://github.com/apache/tvm/pull/6602) [#7845](https://github.com/apache/tvm/pull/7845) [#6659](https://github.com/apache/tvm/pull/6659) [#6740](https://github.com/apache/tvm/pull/6740) [#6782](https://github.com/apache/tvm/pull/6782) [#6784](https://github.com/apache/tvm/pull/6784) [#7958](https://github.com/apache/tvm/pull/7958) [#8192](https://github.com/apache/tvm/pull/8192) [#8397](https://github.com/apache/tvm/pull/8397) [#8398](https://github.com/apache/tvm/pull/8398) [#8403](https://github.com/apache/tvm/pull/8403) [#8447](https://github.com/apache/tvm/pull/8447) [#6829](https://github.com/apache/tvm/pull/6829)\n- MXNet support. More operators and NLP model coverage in GluonNLP [#7568](https://github.com/apache/tvm/pull/7568) [#7409](https://github.com/apache/tvm/pull/7409) [#7209](https://github.com/apache/tvm/pull/7209) [#7191](https://github.com/apache/tvm/pull/7191) [#7062](https://github.com/apache/tvm/pull/7062) [#6561](https://github.com/apache/tvm/pull/6561) [#6699](https://github.com/apache/tvm/pull/6699)\n- Misc: CoreML, Keras, DarkNet, etc. [#7667](https://github.com/apache/tvm/pull/7667) [#6676](https://github.com/apache/tvm/pull/6676) [#6651](https://github.com/apache/tvm/pull/6651) [#6963](https://github.com/apache/tvm/pull/6963) [#7949](https://github.com/apache/tvm/pull/7949) [#7035](https://github.com/apache/tvm/pull/7035) [#7446](https://github.com/apache/tvm/pull/7446) [#8562](https://github.com/apache/tvm/pull/8562) [#8599](https://github.com/apache/tvm/pull/8599)\n\n#### Codegen Backends and Runtime\n\n- LLVM backend: recover LLVM support on windows; support target feature strings in function attributes; atomic support in NVPTX, ROCm; LLVM compatibility to LLVM 12+ [#9305](https://github.com/apache/tvm/pull/9305) [#9223](https://github.com/apache/tvm/pull/9223) [#9138](https://github.com/apache/tvm/pull/9138) [#8860](https://github.com/apache/tvm/pull/8860) [#8958](https://github.com/apache/tvm/pull/8958) [#6763](https://github.com/apache/tvm/pull/6763) [#6698](https://github.com/apache/tvm/pull/6698) [#6717](https://github.com/apache/tvm/pull/6717) [#6738](https://github.com/apache/tvm/pull/6738) [#8293](https://github.com/apache/tvm/pull/8293) [#6907](https://github.com/apache/tvm/pull/6907) [#7051](https://github.com/apache/tvm/pull/7051)\n- ROCm 3.9 bitcode files search [#6865](https://github.com/apache/tvm/pull/6865)\n- Vulkan and SPIR-V refactoring and major improvement in codegen and runtime. [A critical bug fix in SPIRV codegen](https://github.com/apache/tvm/pull/8102) allows the Vulkan backend to produce correct outputs on more hardwares and drivers. Added support for querying device specific hardware parameters and capabilities, dynamic shapes, irregular ops such as sorting and NMS, UBO, fp16, and vectorization. We can now run complicated models like MaskRCNN on Vulkan end to end. [#8904](https://github.com/apache/tvm/pull/8904) [#7833](https://github.com/apache/tvm/pull/7833) [#7717](https://github.com/apache/tvm/pull/7717) [#7681](https://github.com/apache/tvm/pull/7681) [#8746](https://github.com/apache/tvm/pull/8746) [#8813](https://github.com/apache/tvm/pull/8813) [#7609](https://github.com/apache/tvm/pull/7609) [#8882](https://github.com/apache/tvm/pull/8882) [#7607](https://github.com/apache/tvm/pull/7607) [#7591](https://github.com/apache/tvm/pull/7591) [#7574](https://github.com/apache/tvm/pull/7574) [#7572](https://github.com/apache/tvm/pull/7572) [#7833](https://github.com/apache/tvm/pull/7833) [#6662](https://github.com/apache/tvm/pull/6662) [#7969](https://github.com/apache/tvm/pull/7969) [#8013](https://github.com/apache/tvm/pull/8013) [#8048](https://github.com/apache/tvm/pull/8048) [#8098](https://github.com/apache/tvm/pull/8098) [#8102](https://github.com/apache/tvm/pull/8102) [#8107](https://github.com/apache/tvm/pull/8107) [#8127](https://github.com/apache/tvm/pull/8127) [#8151](https://github.com/apache/tvm/pull/8151) [#8196](https://github.com/apache/tvm/pull/8196) [#8320](https://github.com/apache/tvm/pull/8320) [#8588](https://github.com/apache/tvm/pull/8588) [#8332](https://github.com/apache/tvm/pull/8332) [#8333](https://github.com/apache/tvm/pull/8333) [#8348](https://github.com/apache/tvm/pull/8348) [#8528](https://github.com/apache/tvm/pull/8528)\n- Metal language version upgrade (`MTLLanguageVersion2_3`), better codegen support, int64 support, various bug fixes [#7830](https://github.com/apache/tvm/pull/7830) [#7819](https://github.com/apache/tvm/pull/7819) [#7714](https://github.com/apache/tvm/pull/7714) [#7118](https://github.com/apache/tvm/pull/7118) [#7116](https://github.com/apache/tvm/pull/7116) [#7105](https://github.com/apache/tvm/pull/7105) [#7980](https://github.com/apache/tvm/pull/7980) [#8054](https://github.com/apache/tvm/pull/8054) [#8175](https://github.com/apache/tvm/pull/8175) [#8202](https://github.com/apache/tvm/pull/8202) [#8206](https://github.com/apache/tvm/pull/8206) [#8313](https://github.com/apache/tvm/pull/8313)\n- OpenCL, VTA, Verilator: refactored code generator, better error messages, various bug fixes [#7834](https://github.com/apache/tvm/pull/7834) [#7777](https://github.com/apache/tvm/pull/7777) [#7761](https://github.com/apache/tvm/pull/7761) [#7100](https://github.com/apache/tvm/pull/7100) [#6125](https://github.com/apache/tvm/pull/6125) [#6126](https://github.com/apache/tvm/pull/6126) [#6191](https://github.com/apache/tvm/pull/6191) [#7834](https://github.com/apache/tvm/pull/7834) [#8256](https://github.com/apache/tvm/pull/8256) [#8257](https://github.com/apache/tvm/pull/8257) [#8731](https://github.com/apache/tvm/pull/8731) [#8756](https://github.com/apache/tvm/pull/8756) [#8973](https://github.com/apache/tvm/pull/8973)\n- CUDA: enable `__launch_bounds__`, dynamic shared memory, TensorCore, BF16, half2, NVCC version upgrade [#9341](https://github.com/apache/tvm/pull/9341) [#8678](https://github.com/apache/tvm/pull/8678) [#7561](https://github.com/apache/tvm/pull/7561) [#7273](https://github.com/apache/tvm/pull/7273) [#7146](https://github.com/apache/tvm/pull/7146) [#7147](https://github.com/apache/tvm/pull/7147) [#7099](https://github.com/apache/tvm/pull/7099) [#7065](https://github.com/apache/tvm/pull/7065) [#7033](https://github.com/apache/tvm/pull/7033) [#7014](https://github.com/apache/tvm/pull/7014) [#7907](https://github.com/apache/tvm/pull/7907) [#7964](https://github.com/apache/tvm/pull/7964) [#9087](https://github.com/apache/tvm/pull/9087) [#8135](https://github.com/apache/tvm/pull/8135) [#8137](https://github.com/apache/tvm/pull/8137) [#8457](https://github.com/apache/tvm/pull/8457) [#8466](https://github.com/apache/tvm/pull/8466) [#8571](https://github.com/apache/tvm/pull/8571)\n- ARM: CMSIS-NN, Ethos-N [#8653](https://github.com/apache/tvm/pull/8653) [#7628](https://github.com/apache/tvm/pull/7628) [#8951](https://github.com/apache/tvm/pull/8951) [#7506](https://github.com/apache/tvm/pull/7506) [#7443](https://github.com/apache/tvm/pull/7443) [#7858](https://github.com/apache/tvm/pull/7858) [#6982](https://github.com/apache/tvm/pull/6982) [#8795](https://github.com/apache/tvm/pull/8795) [#8806](https://github.com/apache/tvm/pull/8806) [#8833](https://github.com/apache/tvm/pull/8833) [#9147](https://github.com/apache/tvm/pull/9147) [#9159](https://github.com/apache/tvm/pull/9159) [#9160](https://github.com/apache/tvm/pull/9160) [#9162](https://github.com/apache/tvm/pull/9162) [#9163](https://github.com/apache/tvm/pull/9163) [#9167](https://github.com/apache/tvm/pull/9167) [#9209](https://github.com/apache/tvm/pull/9209) [#9386](https://github.com/apache/tvm/pull/9386) [#9387](https://github.com/apache/tvm/pull/9387)\n- Hexagon: build, compilation, model launcher, more target options and better runtime [#7784](https://github.com/apache/tvm/pull/7784) [#6718](https://github.com/apache/tvm/pull/6718) [#8821](https://github.com/apache/tvm/pull/8821) [#8822](https://github.com/apache/tvm/pull/8822) [#9033](https://github.com/apache/tvm/pull/9033) [#8823](https://github.com/apache/tvm/pull/8823) [#8859](https://github.com/apache/tvm/pull/8859) [#8865](https://github.com/apache/tvm/pull/8865) [#8915](https://github.com/apache/tvm/pull/8915) [#8954](https://github.com/apache/tvm/pull/8954) [#9024](https://github.com/apache/tvm/pull/9024) [#9025](https://github.com/apache/tvm/pull/9025) [#8960](https://github.com/apache/tvm/pull/8960) [#8986](https://github.com/apache/tvm/pull/8986) [#9010](https://github.com/apache/tvm/pull/9010) [#9011](https://github.com/apache/tvm/pull/9011) [#9189](https://github.com/apache/tvm/pull/9189) [#9220](https://github.com/apache/tvm/pull/9220) [#9355](https://github.com/apache/tvm/pull/9355) [#9356](https://github.com/apache/tvm/pull/9356)\n\n\n- WASM: Update support for latest emcc, add ffi test. [#6751](https://github.com/apache/tvm/pull/6751)\n\n#### BYOC Integration with Vendor Libraries: TensorRT, ACL, VitisAI\n\n- TensorRT initial integration, stabilization, int8 calibration, dynamism support  [#6395](https://github.com/apache/tvm/pull/6395) [#7702](https://github.com/apache/tvm/pull/7702) [#7595](https://github.com/apache/tvm/pull/7595) [#7581](https://github.com/apache/tvm/pull/7581) [#7412](https://github.com/apache/tvm/pull/7412) [#7372](https://github.com/apache/tvm/pull/7372) [#9047](https://github.com/apache/tvm/pull/9047) [#8073](https://github.com/apache/tvm/pull/8073) [#8808](https://github.com/apache/tvm/pull/8808) [#6905](https://github.com/apache/tvm/pull/6905) [#7967](https://github.com/apache/tvm/pull/7967) [#8005](https://github.com/apache/tvm/pull/8005) [#8172](https://github.com/apache/tvm/pull/8172) [#8461](https://github.com/apache/tvm/pull/8461) [#8506](https://github.com/apache/tvm/pull/8506) [#8607](https://github.com/apache/tvm/pull/8607) [#7205](https://github.com/apache/tvm/pull/7205) [#7026](https://github.com/apache/tvm/pull/7026) [#7016](https://github.com/apache/tvm/pull/7016) [#7011](https://github.com/apache/tvm/pull/7011) [#6955](https://github.com/apache/tvm/pull/6955) [#6872](https://github.com/apache/tvm/pull/6872) [#7253](https://github.com/apache/tvm/pull/7253) [#6805](https://github.com/apache/tvm/pull/6805) [#9324](https://github.com/apache/tvm/pull/9324)\n- Arm Compute Library (ACL) integration [#7649](https://github.com/apache/tvm/pull/7649) [#7206](https://github.com/apache/tvm/pull/7206) [#6532](https://github.com/apache/tvm/pull/6532) [#7121](https://github.com/apache/tvm/pull/7121) [#6724](https://github.com/apache/tvm/pull/6724) [#8149](https://github.com/apache/tvm/pull/8149) [#7251](https://github.com/apache/tvm/pull/7251) [#9396](https://github.com/apache/tvm/pull/9396)\n- Verilator integration [#7406](https://github.com/apache/tvm/pull/7406) [#7351](https://github.com/apache/tvm/pull/7351) [#7286](https://github.com/apache/tvm/pull/7286) [#8094](https://github.com/apache/tvm/pull/8094)\n- VitisAI integration [#6343](https://github.com/apache/tvm/pull/6343) [#7350](https://github.com/apache/tvm/pull/7350)\n- BYOC infrastructure enhancement: improving control flow, AnnotateTarget, custom codegen [#6641](https://github.com/apache/tvm/pull/6641) [#6655](https://github.com/apache/tvm/pull/6655) [#6697](https://github.com/apache/tvm/pull/6697) [#6786](https://github.com/apache/tvm/pull/6786) [#7977](https://github.com/apache/tvm/pull/7977) [#8464](https://github.com/apache/tvm/pull/8464)\n\n\n#### TVMC\n\n- MacOS support [#8396](https://github.com/apache/tvm/pull/8396)\n- AutoScheduler support [#7070](https://github.com/apache/tvm/pull/7070)\n- Support cross compiler options [#7922](https://github.com/apache/tvm/pull/7922)\n- Python scripting [#7823](https://github.com/apache/tvm/pull/7823) [#7698](https://github.com/apache/tvm/pull/7698)\n- More flexible input specification [#7366](https://github.com/apache/tvm/pull/7366) [#7788](https://github.com/apache/tvm/pull/7788)\n- More options, `--disable-pass` and `--config` [#7816](https://github.com/apache/tvm/pull/7816) [#8253](https://github.com/apache/tvm/pull/8253)\n- Allow passing optional arguments to importers [#7674](https://github.com/apache/tvm/pull/7674)\n- Model library format (MLF) support [#8086](https://github.com/apache/tvm/pull/8086) [#8331](https://github.com/apache/tvm/pull/8331)\n- More backend and library support: metal, ACL, Vulkan, OpenCL, ROCm, Vitis AI [#8282](https://github.com/apache/tvm/pull/8282) [#7508](https://github.com/apache/tvm/pull/7508) [#8359](https://github.com/apache/tvm/pull/8359) [#6831](https://github.com/apache/tvm/pull/6831) [#8896](https://github.com/apache/tvm/pull/8896) [#7577](https://github.com/apache/tvm/pull/7577)\n- Support for the new target system [#7651](https://github.com/apache/tvm/pull/7651) [#7654](https://github.com/apache/tvm/pull/7654) [#6788](https://github.com/apache/tvm/pull/6788) [#7304](https://github.com/apache/tvm/pull/7304) [#6855](https://github.com/apache/tvm/pull/6855)\n\n#### Rust Binding\n\n- Rust bindings installable via Cargo [#7503](https://github.com/apache/tvm/pull/7503) [#6678](https://github.com/apache/tvm/pull/6678) [#8631](https://github.com/apache/tvm/pull/8631) [#8665](https://github.com/apache/tvm/pull/8665)\n- Initial support for diagnostic interface [#6656](https://github.com/apache/tvm/pull/6656)\n- Fixes for using Python APIs from Rust [#7085](https://github.com/apache/tvm/pull/7085)\n- Improve NDArray, GraphRt, Relay, IRModule, Array, Attrs bindings [#6563](https://github.com/apache/tvm/pull/6563) [#6741](https://github.com/apache/tvm/pull/6741) [#7138](https://github.com/apache/tvm/pull/7138) [#8353](https://github.com/apache/tvm/pull/8353) [#7082](https://github.com/apache/tvm/pull/7082)\n- Improve error handling, error messages and fix memory leaks [#8289](https://github.com/apache/tvm/pull/8289) [#6815](https://github.com/apache/tvm/pull/6815) [#8714](https://github.com/apache/tvm/pull/8714) [#8725](https://github.com/apache/tvm/pull/8725)\n\n#### Misc\n\n- Enhanced CPP-RPC implementation: allow user supplied work dir, support of CPP-RPC server for Apple, support adb-shell style CPP-RPC [#7670](https://github.com/apache/tvm/pull/7670) [#8224](https://github.com/apache/tvm/pull/8224) [#8223](https://github.com/apache/tvm/pull/8223) [#7766](https://github.com/apache/tvm/pull/7766) [#7013](https://github.com/apache/tvm/pull/7013)\n- Use PopenWorker to handle RPC system: [#7889](https://github.com/apache/tvm/pull/7889) [#7757](https://github.com/apache/tvm/pull/7757) [#7961](https://github.com/apache/tvm/pull/7961)\n- Fold target host into target [#7462](https://github.com/apache/tvm/pull/7462) [#7791](https://github.com/apache/tvm/pull/7791) [#7534](https://github.com/apache/tvm/pull/7534) [#8835](https://github.com/apache/tvm/pull/8835)\n- Target-based intrinsic lowering and legalization [#7936](https://github.com/apache/tvm/pull/7936) [#7809](https://github.com/apache/tvm/pull/7809)\n- Add target tags for all existing CUDA GPU models [#7410](https://github.com/apache/tvm/pull/7410)\n- Linear Congruential Random Engine [#8642](https://github.com/apache/tvm/pull/8642)\n\n## 0.7\nv0.7 brings many major features. The community works together to refactor the internal code base to bring an unified IR code structure with a unified IRModule, type system and pass infrastructure. We have also bought many exciting new features, some highlights include:\n\n* Initial automatic scheduling support\n* Initial command line driver interface\n* WebGPU and webassembly support\n* Better first class rust support in the codebase\n* Intial Hexagon support\n* Bring your own codegen (BYOC) support\n\nThe community also continues to bring high quality improvements to the existing modules including, but not limited to: better frontend coverage, performance, quantization, microTVM and dynamic shape support.\n\n### New Features\n#### Automatic Scheduling (Experimental)\n* Phase 0: Ansor minimum system for auto schedule generating #5962\n* Phase 1: Access Analyzer #6103\n* Phase 1: Add `follow_split` and `follow_fused_split` steps #6142\n* Phase 1: Add `pragma`/`storage_align`/`rfactor` steps #6141\n* Phase 1: Add RPC Runner #6077\n* Phase 1: Add `annotation`/`compute_at`/`compute_root`/`compute_inline` steps #6073\n* Phase 1: Add `cache_read`/`cache_write` steps #6107\n* Phase 1: Rename namspace form `auto_schedule` to `auto_scheduler` #6059\n* Phase 1: The base class for cost models #6187\n* Phase 1: feature extraction for cost models #6190\n* Phase 1: XGBoost Cost Model #6270\n* Phase 2: Basic GPU Sketch Search Policy #6269\n* Phase 2: Evolutionary Search #6310\n* Phase 2: Update heavy operations with `parallel_for` #6348\n* Parallel the InitPopulation (#6512)\n* Tutorial: Using the template-free auto-scheduler on CPU (#6488)\n\n#### BYOC\n* External codegen support in Relay (#4482), (#4544)\n* Bring Your Own Codegen Guide -- Part 1 #4602\n* Bring Your Own Codegen Guide -- Part 2 #4718\n* Relay annotation and partitioning for external compilers #4570\n* JSON Runtime with DNNL End-to-End Flow #5919\n* Handle one symbol for each runtime #5989\n* Run accelerator specific optimizations #6068\n* Arm Compute Library integration #5915\n* Retire the example json runtime #6177\n* `json_node.h` should include `data_type.h` #6224\n* Improve installation tutorial #6170\n* Add support for dense (fully connected) layer #6254\n* Introduce the Ethos-N BYOC integration #6222\n* Enable remote device via environment variables #6279\n* Improved pooling support #6248\n* Add support for quantized convolution #6335\n* CoreML codegen #5634\n\n#### Operator Coverage\n* Add `strided_set` operation (#4303)\n* Add support for conv3d (#4400), pool3d (#4478), 3d upsampling ops (#4584)\n* Add group convolution for VTA (#4421)\n* Add 1d deconvolution op (#4476)\n* Allow batch matmul to be fused into injective ops (#4537)\n* Add native depthtospace and spacetodepth operators (#4566)\n* Add CUDNN conv3d support (#4418)\n* Dilation2D operator support #5033\n* Isfinite operator #4981\n* Unravel Index operator #5082\n* Add thrust support for nms #5116\n* Resize3d, Upsample3d op support #5633\n* Add operator Correlation #5628\n* `affine_grid` and `grid_sample` #5657\n* Sparse to dense operator #5447\n* `Conv3d_transpose` op support added #5737\n* add op `crop_and_resize` #4417\n* Add bitwise ops #4815\n* Sparse to dense operator #5447\n* support dynamic NMS(Non Maximum Suppression), symbolic begin, end, and strides for strided_slice #4312\n* `Conv3d_transpose` op support added #5737\n* ReverseSequence operator #5495\n* Conv1D #4639\n* 1D Pooling #4663\n\n#### Quantization\n* Channel wise quantization - Quantize & Requantize #4629\n* Support QNN ops. #5066\n* Adding support for QNN subtract op #5153\n* TFLite QNN Tutorial #5595\n* Tutorial: Deploy Quantized Model on CUDA #4667\n* Support asymmetric per-layer quantized operators #6109\n\n#### Relay\n* Add convertlayout pass in Relay (#4335, #4600)\n* Added Merge Composite pass #4771\n* Call graph for relay #4922\n* Add inline pass #4927\n* Target annotation for external codegen #4933\n* GradientCell Relay Pass #5039\n* Add MergeCompilerRegions pass #5134\n* Non-recursive Graph Vistor and Rewriter (#4886)\n* [Blocksparse] Pipeline for lowering dense model to sparse-dense (#5377)\n* Relay op strategy #4644\n* Static Tensor Array (#5103)\n* Memory planner (part 1) #5144\n* ONNX codegen #5052\n* Add Parser 2.0 #5932, part 2 #6162\n* Basic block normal form #6152\n* Convert Layout pass. #4664\n* Pattern Language, Matcher, Rewriter, and Function Paritioner #5231\n\n#### Runtime and Backend\n* Add ADTObject POD container type (#4346)\n* TFLite RPC runtime (#4439)\n* Standardized graph runtime export (#4532)\n* MISRA-C compliant TVM runtime #3934\n* Add String container #4628\n* Introduce Virtual Memory Allocator to CRT (#5124)\n* Initial implementation of Hexagon runtime support (#5252)\n* FastRPC interface for Hexagon runtime (#5353)\n* CoreML Runtime (#5283)\n* AutoTVM + uTVM for Cortex-M7 (#5417)\n* Windows Support for cpp_rpc (#4857)\n* Implement TVMDSOOp(TensorFlow custom op) for TVM runtime (#4459)\n* WebGPU support #5545\n* TVM WebAssembly JS Runtime #5506\n* Hexagon driver for offloading kernels to simulator #5492\n* Introduce runtime::Array #5585\n* Allow non-nullable ObjectRef, introduce Optional. (#5314)\n* Introduce static slots for common objects. (#5423)\n* ntroduce RValue reference(move) support to TypedPackedFunc (#5271)\n* Introduce MetadataModule to separate code compilation/interpretation and weight initialization #5770\n* Support module based interface runtime #5753\n* Add TVM application extension with WASM runtime #5892\n* Provide guide to user who has difficulty register SEqualReduce (#5300)\n\n#### Rust Support\n* Revive the Rust + SGX refactor #4976\n* Improve Rust bindings: Map, Array, String, various IR nodes #6339\n* Rust Refactor Stage 4: Rewrite Rust graph runtime to use new APIs #5830\n* Second stage of Rust Refactor #5527\n* tvm crate stage 3 of Rust refactor #5769\n* Add first stage of updating and rewriting Rust bindings. #5526\n\n#### TIR\n* Introduce StructuralHash for the Unified IR. #5160\n* Introduce StructuralEqual Infra for the unified IR. #5154\n* Introduce ExprDeepEqual, Remove IRDeepCompare #5206\n* [TIR] Introduce BufferLoad/Store (#5205)\n* Improved massive build times caused by tir.floormod and tir.floordiv. Fixed Topi testcase. #5666\n* Buffer logger assert removed #6147\n* Enhance VerifyGPUCode #6194\n* HoistIfThenElse added #6066\n* Hybrid Script Support for TIR #6227\n* Migrate Low-level Passes to Pass Manager #5198\n* HoistIfThenElse added #6066\n* Hybrid Script Support for TIR #6227\n* Block scope hoisting added #6238\n\n#### TE\n* reverse-mode autodiff without any optimization #5121\n* Tensor Expression Debug Display (TEDD) #4651\n* Optimize and eliminate the Jacobian tensor for te.autodiff #6078\n\n#### TVMC(Experimental)\n* TVMC - A command line driver for TVM (Part 1) #6112\n* TVMC - Linting error on onnx command line driver frontend #6536\n* TVMC - Command line driver 'compile' (part 2/4) #6302\n* TVMC - Introduce 'tune' subcommand (part 3/4) #6537\n* TVMC - Introduce 'run' subcommand (part 4/4) #6578\n* TVMC - Getting started tutorial for TVMC #6597\n\n\n### Feature Improvement\n#### Accelerator and Microcontroller Support\n- Cleanup legacy verilog code (#4576)\n- uTVM support for ARM STM32F746XX boards (#4274)\n- Add --runtime=c, remove `micro_dev` target, enable LLVM backend #6145\n\n#### Arithmetic Analysis\n* Linear system and equation solver (#5171)\n* Inequalities solver #5618\n* Improve IntervalSet's floormod (#5367)\n* Remove legacy const pattern functions (#5387)\n* Handle likely in IRMutatorWithAnalyzer #5665\n* ExtendedEuclidean merge impl to int_operator #5625\n* Rewrite simplify fix for Vectorized Cooperative Fetching #5924\n\n#### AutoTVM and Graph Tuner\n* Adding ROCM schedules for TOPI (#4507)\n* NHWC conv2d schedule templates for ARM (#3859)\n* Use VM compile to extract autotvm tasks #4328\n* Download fallback schedule file if it does not exist #4671\n* Ignore error when removing tmpdir #4781\n* Fix a bug in generating the search space #4779\n* Minor bug fixes in AutoTVM for QNN graphs #4797\n* Fix autotvm customized template #5034\n* Add opt out operator for `has_multiple_inputs` for graph tuner #5000\n* Customize SI prefix in logging (#5411)\n* Update XGBoost verbosity option #5649\n* Support range in index based tuners #4870\n* Enable random fill and CPU cache flush for AutoTVM and Ansor (#6391)\n* Auto-scheduler tutorial for GPU and necessary refactor/fix (#6512)\n\n#### BYOC\n* [BYOC] Bind constant tuples in graph partitioner (#5476)\n* [BYOC] Add support for composite functions in BYOC (#5261)\n* [BYOC] Register pattern tables from external codegens (#5262)\n* [BYOC] Enhance partitioning and external codegen (#5310)\n* [BYOC] Refine AnnotateTarget and MergeCompilerRegion Passes (#5277)\n* [BYOC] Use Non-Recursive Visitor/Mutator (#5410)\n* [BYOC] Refine DNNL Codegen (#5288)\n* [BYOC] Add example of Composite + Annotate for DNNL fused op (#5272)\n* [BYOC] Prevent duplicate outputs in subgraph Tuple (#5320)\n* [BYOC] Introduce further operator support (#6355)\n* [BYOC] Support input nodes with multiple entries (#6368)\n* [BYOC] Add maximum support for float32 (#6506)\n\n#### Codegen\n* Intrinsic dispatching with OCML instead of LLVM for ROCm (#4499)\n* Make target codegen take IRModule and PrimFunc. #5107\n* Enhance CUDA codegen for SelectNode #4983\n* Vectorization for intrinsics #5101\n* [LLVM] Do not use `x86_vcvtph2ps_256` intrinsic with LLVM 11+ (#5267)\n* [LLVM] Use llvm::ElementCount with LLVM 11+ when creating vectors (#5265)\n* [LLVM] Use llvm::FunctionCallee in IRBuilder::CreateCall with LLVM 11+ (#5338)\n* [LLVM] Include Support/Host.h for declaration of getDefaultTargetTriple (#5268)\n* [LLVM] Replace calls to Type::getVectorNumElements (#5398)\n* [LLVM] Use ArrayRef in calls to CreateShuffleVector (#5399)\n* [LLVM] Use llvm::Align with LLVM 11+ to avoid warnings (#5264)\n* [CodeGen] Cleanup generated code (#5424)\n* Rename `target_id` => `target_kind` #6199\n* 64-bit RPi4b target #6211\n* Creating Target from JSON-like Configuration #6218\n* Add python binding to new JSON target construction #6315\n* Use target class in all codegens #6347\n* Initial support for Hexagon codegen #6261\n* Add --runtime=c, remove `micro_dev` target, enable LLVM backend #6145\n* Add tvm::support::hexdump() debug utility #6154\n* Adding AMD codegen unit tests (#4509)\n* Support cuda tensorcore subbyte int data type in auto tensorcore #4546\n* Handle empty LLVMModule in GetFunction #5146\n* Support int4/int8 conv2d tensor core with HWNC layout #6121\n\n#### Dynamism Support\n* Add shape function for `zero`, `zeros_like`, `ones`, `ones_like` (#4448), `tile` (#4441)\n* Support symbolic newshape for Reshape #5429\n* Support symbolic TopK, Ones, Zeros and Full #5459\n* Add `shape_of` instruction #5855\n* symbolic `max_output_size` #5844\n* Dynamic TopK Op #6008\n* Dynamic `broadcast_to`, `zeros`, `ones` #6007\n* Add dynamic reshape grad #6080\n* Keep fixed dim when unifying dynamic shape #5795\n* OneHot operation #6209\n* Add Dynamic Resize Op #6198\n* Dynamic full operator #6260\n* Dynamic upsampling relay op #6273\n* Dynamic Tile Op #5983\n\n#### Frontend and User Interface\n* TFLite parser support for `transpose_conv` (#4440), `unpack` (#4447)\n* LLDB pretty printers for relay (#4453)\n* ONNX to Relay converter op support: expand op (#4483)\n* ONNX `auto_pad` in conv and convtranspose (#4563)\n* TF to Relay converter op support (#4504) (#4551) (#4484)\n* Remove unnecessary cast of constants in ONNX converter (#4573)\n* Add support for tf.Keras networks in Relay Keras frontend #4630\n* Add conv3d #4604\n* Fix incorrect calculations in tf SLICE #4518\n* Dynamically calculate `input_stats` of any `fake_quant` range #4789\n* LSTM Support #4825\n* Add `MIRROR_PAD` operator #4822\n* use qnn helper function in softmax #4840\n* Add Resize op converter #4838\n* Add support for `TFLite_Detection_PostProcess` #4543\n* Fix tests for tflite unary elemwise operations #4913\n* GaussianDropout/Noise parsing support #4928\n* Add parser support for 'square' operator #4915\n* `make_loss` operator support #4930\n* Add parser support for `l2_normalization` #4966\n* ReadVariableOp operator support #4952\n* Check graph inputs match expected #4992\n* support multiply outputs #4980\n* TFLite: Using real image for QNN testing. #4816\n* TFLite: `FLOOR_MOD` & `FLOOR_DIV` support #4971\n* PyTorch: Upsampling op support and enable registering a user defined op conversion map #4961\n* PyTorch: fix unordered dictionary problem for python version under 3.6 #4982\n* Operator support NonZero #5073\n* Upsampling op support and enable registering a user defined op conversion map #4961\n* Check graph inputs match expected #4992\n* Add support for quantized models via QNN #4977\n* Add initial control flow support #4964\n* Remove FP32 piggy back and use QNN add/mul/concatenate #5061\n* Add missing upcast to uint8 `avg_pool` conversion #5089\n* Add initial 3D op support and test on Resnet 3D #5075\n* Fix conv2d conversion for group conv (group > 1 but != in channels) #5132\n* Add support for `max_pool1d` #5142\n* Add support for split #5174\n* `FLOOR_MOD` & `FLOOR_DIV` support #4971\n* Activation functions support #4978\n* Round op parsing support added #5022\n* DepthToSpace and SpaceToDepth support #5041\n* `TOP_K` op parser support #5051\n* ReadVariableOp operator support #4952\n* Support multiply outputs #4980\n* `reduce_any` op parsing support #4926\n* TensorFlow Parser Control Flow Enhancement #5020\n* TensorFlow Frontend support with shared params #5042\n* Support for AddV2 in Relay Tensorflow frontend converter. #5046\n* conv3d frontend operator support #5080\n* `max_pool3d` and Averagepool3d operator support #5085\n* Support for Atan/Atan2 in Relay Tensorflow frontend converter. #5104\n* Use leaky by default for LeakyReLU #5192\n* Conv3D ONNX support and `conv3D_ncdhw` x86 schedules #4949\n* Add support for FusedBatchNormV3 #5065\n* Activations for pytorch #5194\n* Dropouts And InstanceNorm support added #5203\n* [Frontend] Asymmetric padding of convolution support (#4803)\n* [ONNX]Pool3d & upsample3d op support (#5135)\n* Add TopK to ONNX Frontend (#5441)\n* Add RoiAlign to Onnx frontend (#5454)\n* [PYTORCH]AvgPool3d, MaxPool3d and Squeeze op support (#5220)\n* [PYTORCH]celu, gelu, selu activations (#5263)\n* [Pytorch]layernorm bug fix and testcase updated (#5257)\n* [PYTORCH]LayerNorm support added (#5249)\n* [PYTORCH]GroupNorm op support added (#5358)\n* [PYTORCH]Logical & Bitwise operator support (#5341)\n* [PYTORCH]Tensor creation ops support (#5347)\n* [PYTORCH]cosh,sinh,log2,log10,log1p op support (#5395)\n* [PYTORCH]Rsub, Embedded, OneHot ops support (#5434)\n* [PYTORCH]Abs, Arange, Softplus ops (#5295)\n* [PYTORCH]isNan, isinf, isfinite, ceil, clamp, round ops (#5316)\n* [PYTORCH]Activations for pytorch (#5194)\n* [PYTORCH]Repeat, Reciprocal & Reshape Op support (#5280)\n* [PYTORCH]`Reduce_ops` support added (#5308)\n* [PYTORCH]Take, Topk op support (#5332)\n* [PYTORCH]Dropouts And InstanceNorm support added (#5203)\n* [PYTORCH]Unary Ops frontend support. (#5378)\n* [Torch] Support Python list, more realistic recurrent networks (#5306)\n* [PYTORCH]where, addcdiv, addcmul op support (#5383)\n* [Torch] Add support for split (#5174)\n* [Torch] Fix up graph input handling (#5204)\n* [TFLITE]Logical not op support (#5475)\n* [TFLITE]Hard Swish & MobilnetV3 model testing (#5239)\n* [TFLITE]Gather, StridedSlice op support added (#4788)\n* [TFLITE] Match TFLite shape for SSD custom op (#5473)\n* Factor out import of common tflite.Operator in tflite frontend. (#5355)\n* [TFLite] support for FILL and `SPLIT_V` operators (#5330)\n* [TFLite] `L2_POOL_2D` operator (#5452)\n* [TFLite] Add config option to specify FlatBuffers location (#5425)\n* [TFLITE]Logical not op support (#5475)\n* [TENSORFLOW]reduce ops updated (#5180)\n* [TENSORFLOW] Fix `gather_nd` indices (#5279)\n* [TensorFlow]Improve TensorFlow Static Shape Tensor Array (#5243)\n* [KERAS]Minimum & AlphaDropout op support (#5380)\n* [KERAS]Embedding layer (#5444)\n* [KERAS]`Max_pool3d` and Averagepool3d operator support (#5085)\n* [CAFFE2]add Mul and ConvTranspose operator (#5302)\n* [MXNET]DepthToSpace & SpaceToDepth Operator (#5408)\n* [MXNET]broadcast and logical op support (#5461)\n* [MXNET] Use leaky by default for LeakyReLU (#5192)\n* [MXNET] support elemwise logic ops (#5361)\n* [Frontend|MXNet] SwapAxis operator support (#5246)\n* [RELAY] Move frontend utils (#5345)\n* [Pytorch] Fix translation of transpose when axis argument is as a list (#5451)\n* LpPool Support added #5696\n* Skip ADD inside Gemm op when vector is zero #5697\n* ReduceL1, ReduceL2, ReduceSumSquare, ReduceLogSum ops added #5721\n* MaxRoiPool, Mod & Xor op support added #5729\n* Skip multiply with 1.0f constant for GEMM import #5800\n* StatefulPartitionedCall/PartitionedCall Ops support added #5617\n* Don't add cast for batch norm when type isn't changing #5731\n* Conv3d Transpose OP added #5775\n* expand bug fix #5576\n* Support `max_pool2d_with_indices` #5549\n* Add prim::device op #5584\n* ImplicitTensorToNum support added #5603\n* Matmul fix for `batch_matmul` #5604\n* ReflectionPad2d op #5624\n* Padding op support #5638\n* Minor bug fixes #5683\n* `floor_divide` support for squeezenet #5702\n* ReplicationPad support added #5708\n* aten::norm support added #5776\n* broadcast and logical op support #5461\n* MaxPool3d and AvgPool3d Ops support added #5614\n* Softmin, trunc op support added #5715\n* conv3d and `conv3d_transpose` addedx #5814\n* Model importer to be compatible with tflite 2.1.0 #5497\n* Nit: Function names made consistent #5515\n* Select op support for tflite frontend #5486\n* `GATHER_ND` #5508\n* Quantize & Dequantize op #5394\n* Fully connected op conversion made in sync with TFLite #5510\n* `ADD_N` operator #5474\n* onnx, mxnet, pytorch mathops added #5561\n* abs, round, reciprocal, sign, softsign, `hard_sigmoid` ops support #5587\n* Gather nd bug fix for one dim support in tensorflow #5588\n* Add parser support for shape and range #5329\n* Darknet support batch size for yolo #5688\n* Improve Control Flow and TensorArray #5699\n* MXNet: Softmin, trunc op support added #5715\n* MXNet: conv3d and `conv3d_transpose` addedx #5814\n* MXNet: Add parser for `contrib.box_decode` #5967\n* Onnx: ReduceL1, ReduceL2, ReduceSumSquare, ReduceLogSum ops added #5721\n* Onnx: MaxRoiPool, Mod & Xor op support added #5729\n* Onnx: Skip multiply with 1.0f constant for GEMM import #5800\n* Onnx: Fix an issue with #5755 and add Batch norm unit tests. #5845\n* TensorFlow: StatefulPartitionedCall/PartitionedCall Ops support added #5617\n* TensorFlow: Dont add cast for batch norm when type isnt changing #5731\n* TensorFlow: Conv3d Transpose OP added #5775\n* Add parser support for shape and range #5329\n* Darknet support batch size for yolo #5688\n* Improve Control Flow and TensorArray #5699\n* Improve TF Parser to keep output nodes for `saved_model` #5794\n* Add parser support for `relu6`, `leaky_relu`, `relu_n1_to_1`, `log_softmax` #4805\n* Fix TF Dynamic input shape #5825\n* Support a few contrib ops in mxnet #5819\n* Improve TF Parser to keep output nodes for `saved_model` #5794\n* Add parser support for `relu6`, `leaky_relu`, `relu_n1_to_1`, `log_softmax` #4805\n* Check all unsupported ops before raising an exception #5929\n* Add Pytorch advanced indexing #6318\n* Support `index_select` #6295\n* Fix cast to long #6301\n* Fix dtype handling for modules with integer parameters #6311\n* pytorch frontend support conv1d #6203\n* Add cast to double, fix flatten conversion #6357\n* Fix aten::max and aten::min conversion #6372\n* Match pytorch 1.6 googlenet pretrained model (#6201) #6212Add unbiased variance op and corresponding support in pytorch frontend #6232\n* Implemented PADV2 Operator for TFLite and added support for constant values in PAD. #6167\n* Implemented `ONE_HOT` Operator for TFLite. #6223\n* Implemented `EXPAND_DIMS` Operator for TFLite. #6243\n* Implemented `REVERSE_V2` Operator for TFLite. #6304\n* Implemented `MATRIX_SET_DIAG` Operator for Relay/TOPI and TFLite Frontend. #6303\n* RESHAPE with dynamic shape arg in TFLite frontend #6208\n* Constant input attr added to fully connected operation in TFLite frontend #6228\n* Gather operation with indices as tensor expr in TFLite frontend #6168\n* Added support for tflite quantized maximum and minimum #6018\n* Unary ops support added in frontend #6196\n* Introduce caffe frontend for tvm #6206\n* Keras softmax and prelu fix under NHWC #6278\n* add support for MXNET numpy operators #6054\n* Refine tensorflow frontend 1.x & 2.x compatibility #6240\n* Reduceops support added to frontend #6252\n* Update precision in the ONNX `strided_slice`, update precision of ToScalar #6272\n* NHWC import support. #4899\n* Refine tensorflow frontend 1.x & 2.x compatibility #6240\n* Fix node indices attribute error for tensorflow 2.3 #6288\n* Support NMSv4 #6085\n* Support for PyTorch Non-Maximum Suppression #6314\n* ReplicationPad support added #5708\n* MXNet pre-quantized BERT #6039\n* Keep parameter names from PyTorch #5887\n* Refine LSTMBlockCell to support dynamic rnn #5963\n\n#### Relay\n* Add function attributes to IR hash (#4479)\n* Relay passes lookup overhead optimization (#4594)\n* Add `half_pixel` option to Resize op #4610\n* Skip example json runtime test when config is not set #4614\n* Test `tensor_array` in vm #4608\n* Improve `memory_allocation` pass to support multiple i/o dynamic kernels #4595\n* Add unit test for `tensor_array_split` #4619\n* Add parses support for unary elemwise ops #4634\n* Add parses support for SLICE #4502\n* Added pool autopadding and simplified converters. #4672\n* Fix meaning of `conv2d_transpose` `output_padding` parameter #4318\n* Use packed func macro for external codegen #4710\n* Fix `_parse_param` bug #4711\n* Add constant input support for elemwise ops #4666\n* Add parser support for squared difference #4652\n* Add type check to dense #4724\n* Invoke tvm::build from relay `compile_engine` and interpreter #4723\n* Broadcast condition, x, and y for Where op #4774\n* Add parser support for relational ops #4695\n* Remove duplicated BindParamByName function in VM compiler #4793\n* Use SimplifyInference for L2 Normalization. #4795\n* Expose vm OptimizeModule to Python #4800\n* Add parser support for logical operators #4642\n* Conv2D padding representation #4787\n* Add support for quantized LOGISTIC #4696\n* Fix VM compiler for while loop with free vars #4889\n* Fix bug in re-processing call node in MergeComposite pass #4879\n* Expose FunctionGetAttr to Python #4905\n* Add a PyTorch to Relay Parser #4497\n* Support data types for CSourceModuleCodegen args and output #4934\n* Clean up and refactor PyTorch frontend #4944\n* Relay pass to use fast exp/tanh #4873\n* BatchNorm support with run-time mean and variance calculation #4990\n* Reduce plevel of conv2d winograd implementation on cuda #4987\n* Add operation tan to TVM #4938\n* Outline and inline lifted functions for external codegen #4996\n* Remove primitive attribute from composite function #5014\n* Refactor Relay Python to use new FFI #5077\n* Fix relay node registration after refactor #5083\n* `Codegen_c.h` should include relay.function #5093\n* Move expr.Function to function.py #5087\n* Propagate constant to subgraphs #5094\n* Adjust strategy plevel to achieve expected performance by default #5118\n* Added a AnnotatedRegion utility class #5030\n* Support TupleGetItem in body of pattern #5106\n* Partition graph codestyle fixes #5202\n* Re-wrote the Graph Partitioner to support multiple outputs #5143\n* Fixes to MergeCompilerRegions #5195\n* Refactor build module to take IRModule #4988\n* Separate analysis and transform passes #5035\n* Relay Node::make to constructor #5128\n* relay::StructuralHash to tvm::StructuralHash #5166\n* Conditions updated to cover better user scenarios #5043\n* Replace UseDefaultCompiler with GetAttr #5088\n* Return empty CSourceModule when no `lowered_funcs` exists in Relay mod #4847\n* Clean up for memory pass to enable heterogenous execution support. (#5324)\n* Remove re-exports of tvm.transform (#5337)\n* [Refactor] Add memoized expr translator for use by backend codegen (#5325)\n* Legalize - Use Non-recursive Rewriter. (#5296)\n* Add additional check before re-using the cached match #5552\n* Remove kCompiler attr from external functions #5615\n* Pattern Language MergeComposite #5656\n* Support Tuple Output in C/DNNL Codegen #5701\n* Infer types in MergeComposite #5766\n* Convert PatternGrouper to do pre-order, non-recursive analysis #5653\n* Remove constants from partitioned functions #5663\n* Add a check for null function attributes #5674\n* Add ConstantPattern #5689\n* Conditionally Embedding Constants in Partitioned Functions #5693\n* Simplify Pattern API Implementations #5703\n* Add ShapePattern and DataTypePattern #5760\n* Remove unnecessary print #5642\n* Improve Shape Func handling for Tuple inputs #5467\n* Relay updated with String #5578\n* Fix the creation of tuple of tuples in PartitionGraph #5616\n* Preserve type information in Merge Composite #5640\n* Move `compiler_begin`/`end_op` to local static objects #5622\n* Fix `dataflow_pattern`.rewrite() hang if Match in IR #5680\n* Fix segfault in pretty print when ObjectRef is null #5681\n* Move `fallback_device` to config #5690\n* Replace `build_config` with PassContext #5698\n* Clear compile engine after task extraction #5724\n* Add `storage_order` ignore in pooling layer. #5781\n* Tweak cublas/cudnn priority level #5820\n* Skip Unknown Function Symbols #5888\n* Allow every runtime module to handle constants #5885\n* handle Tuple/TupleGetItem in first order gradient #5946\n* Add resnet-3d & Update network definitions for NHWC layout #5945\n* Use TargetNode::attrs for Target serialization #5993\n* each option of target str should only contain one = #5988\n* Rename `target_id` => `target_kind` #6199\n* 64-bit RPi4b target #6211\n* Add resnet-3d & Update network definitions for NHWC layout #5945\n* Small bug fix for Conv1D imports. #5995\n* Move `invoke_tvm_op` and `shape_func` to vm dialect #5958\n* GRU Layer Support #6020\n* Add pass for getting calibration data from a relay module #5997\n* Merge two consecutive reshape ops #6052\n* Add operation `scatter_add` to relay, based on scatter implementation. #6030\n* i64 indices #5235\n* Port `eliminate_common_subexpr` to non-recursive form #6134\n* Fix interpreter for dyanmic shape input of `ndarray_size` #6086\n* Allow to config allocator type and refactor vm code structure #6105\n* Handle `ndarray_size` in FoldConstant #6156\n* when converting constant nodes with types of int64 or float64 #6159\n* Add ReshapeTensor instruction in the VM to replace the reshape op #6089\n* Support combine multiple dense op just into dense #6062\n* Add unbiased variance op and corresponding support in pytorch frontend #6232\n* Specify additional layouts in convert layout pass #5422\n* Safe check added for Merge Composite Call Node #5562\n* Non recursive partitioning #5493\n* Support combine multiple dense op just into dense #6062\n* Make the max number of fused ops configurable #6327\n* Implementation of the dynamic pad operator #6284\n* change device annotation from post DFS to recursive #6124\n* Make check stricter: disallow inserting function with free vars into module #6313\n* Make check stricter by using Feature. Fixed multiple bugs #6326\n* Resize support for NCHW-convertible layouts #6293\n* Make AutoDiff thread through global function #6336\n* Create Interpreter for each constant subgraph #6195\n* Add Dynamic reshape to a dynamic namespace and add DynamicToStatic Pass #5826\n* Expose relay BindParamsByName to Python #4751\n* Implement pass manager tracing API #4782\n* Move Ops in relay.op.contrib #4942\n* Conditions updated to cover better user scenarios #4951\n* [External codegen] Add test cases for fused ops with manual annotation (#4741)\n* Multiple output support, reshape, split ops added #6296\n\n#### Operator Coverage\n* Allow empty tensor for `reshape`, `tile` and `strided_slice` #4618\n* Fix meaning of `conv2d_transpose` `output_padding` parameter\"; #4708\n* Remove cpp upsampling and resize op #4769\n* upsample operator 'NCHWinic' format support. #4791\n* Injective schedule improvement #4786\n* Enable vectorization on fp16 type #4867\n* Support for Int8 schedules - CUDA/x86 #5031\n* New PR to re-add tan to TVM #5025\n* Register topi schedule for Relay `fast_exp` and `fast_tanh` #5131\n* Move Dilation2d from nn to image namespace #5110\n* Use Thrust sort for argsort and topk #5097\n* Conv2d and Dense ops support on Tensor Core #5099\n* Setting workload correctly for Depthwise Spatial conv ARM. #5182\n* Adding a few missing math intrin #5011\n* Missing vectorize for depthwise conv2d. #5196\n* [TOPI] Using x86 schedules for ARM conv2d (#5334)\n* [TOPI-ARM] Do not alter layout if layout is NHWC (#5350)\n* [TOPI] Setting workload correctly for Depthwise Spatial conv ARM. (#5182)\n* [OP] Add `fast_erf` implementation (#5241)\n* [Topi] Tensorcore support for Conv3D (#5284)\n* [intrin] a few more math functions (#5468)\n* [Intrinsic] Add log1p, ldexp, atan2, hypot, nextafter, copysign (#5312)\n* [topi] Add operation relay.nn.dilate() which calls topi.nn.dilate() (#5331)\n* [Topi x86] Missing vectorize for depthwise conv2d. (#5196)\n* [TOPI x86] Adding `unroll_kw` config option for depthwise conv2d. (#5197)\n* [Topi] Breakdown topi.cc into smaller files (#5253)\n* ReduceLogSumExp Operator support #5453\n* Math ops added #5502\n* Enable blocking format in x86 conv2d and fold scale axis #5357\n* Add operation gather to relay. #5716\n* Add `storage_order` ignore in pooling layer. #5781\n* Fix bifrost spatial packing conv2d auto tune #5684\n* Fix reshape usage in ARM schedule #5732\n* Block sparse dense on cuda #5746\n* Improve CUDA softmax scheduling #5600\n* block sparse dense on cuda #5746\n* pass-by-value -> pass-by-const-reference #5783\n* Using MKL blas for quantized dense #6115\n* topi -> tvm/topi #6186\n* Use auto-tuner to improve `conv2d_gemm` performance #6117\n* Improve CUDA `conv2d_transpose_nchw` #4762\n* Add CUDA conv2d for NHWC layout #4737\n* `conv3d_ndhwc` schedule #4775\n* Fast exponent #4790\n* Add Scatter to Topi/Relay/ONNX via hybrid script #5619\n* Split MKL from BLAS. #6182\n* Change the meaning of `conv3d_transpose` `output_padding` to match `conv{1,2}d_transpose` #6065\n* Gather op support added #6013\n\n#### Runtime and Backend\n* Cythonize NDArray.copyto (#4549)\n* Unified Object System runtime refactor (#4578, #4581, #4603)\n* VM profiler: sort VM stats by time (#4601)\n* Update RPC runtime to allow remote module as arg (#4462)\n* Refactorying system lib and dso lib into library module (#4481)\n* Improve TSIM virtual memory mapping (#4545)\n* make adt tag signed #4605\n* Improve TVMBackendPackedCFunc to allow return val #4637\n* EdgeTPU runtime for Coral Boards #4698\n* Fix memory leak when using openMP #4811\n* Fix memory leakage of TVMByteArray #4856\n* Fix `TVM_DLL_EXPORT_TYPED_FUNC` to work on Windows #4955\n* Fix memory leak when using openMP #4811\n* Export GraphRuntime in `tvm_runtime.dll` #5002\n* MISRA-C compliant TVM runtime #3934\n* Update the `type_keys` to reflect the code-org #5074\n* Fix AttrEqual for Array and StrMap, double #5054\n* Export GraphRuntime in `tvm_runtime.dll` #5002\n* Fix unused-value warning #5140\n* crt error handling #5147\n* Bundle deployment with static linking #5158\n* Implemented kDLCPUPinned (cudaMallocHost) #4985\n* Explicitly cast min/max operands #5090\n* `ref_counter` -> `ref_counter_` #5184\n* Expose runtime::String to Python (#5212)\n* [FFI] Refactor runtime.String to subclass str (#5426)\n* [RUNTIME] Auto conversion from str to runtime::String in PackedFUnc (#5251)\n* [RUNTIME] Improved Packed FFI for optional. (#5478)\n* [Hexagon] Add `hexagon_posix.cc` to TVM/RT sources in the right place (#5346)\n* [FFI] Refactor runtime.String to subclass str (#5426)\n* Fix workspace #5503\n* Store nullptr PackedFunc as nullptr for better error propagation #5540\n* Improve PackedFunc robustness #5517\n* Seg fault in WorkspacePool's destructor (#5632) #5636\n* Resolve constexpr issue in debug mode. #5651\n* Add `compile_shared` option to linux compile utility fn #5751\n* Call sync in CopyFromRemote and CopyToRemote #5512\n* Fix the multihop cpu case #5522\n* Improve RPCServer AsyncIO support. #5544\n* Modularize the RPC infra #5484\n* Add `compile_shared` option to linux compile utility fn #5751\n* Overload string operators #5806\n* Only initialize required module #5926\n* if a param not in input, we should still consume its data #5990\n* init TVMPackedFuncs name #6044\n* Enable auto conversion `String->DLDataType` #6214\n* Support random fill #5913\n* Use new to avoid exit-time de-allocation order #6292\n* Add `parallel_for` support to run a loop in parallel #6275\n* Solve ARM BIG.LITTLE heterogeneous multicores #4747\n* [RUNTIME] Quick fix PackedFunc String passing (#5266)\n* Introduce runtime::String::CanConvertFrom #5718\n* Restore the StrMap behavior in JSON/SHash/SEqual #5719\n* Support overriding RPCWatchdog termination behavior on Android and other platforms #6216\n* Set `NDArray::Container.shape_` in NDArray::FromDLPack (#5301)\n* Enable x86 cpu cache flush #5914\n\n#### Quantization\n* Conv2D type checking for kernel per-channel scales. #4732\n* Add missing nullptr check #4773\n* Doc fix on convolution and dequantize #4799\n* Conv2D with dilation support. #4796\n* Making `scale`/`zero_points` as expr instead of attrs. #4611\n* Make calibration faster and more memory usage friendly #4589\n* Doc fix on convolution and dequantize #4799\n* Conv2D with dilation support. #4796\n* Optimize lowering for requantize and FixedPointMultiply. #4798\n* More doc fix on quantize and convolution #4874\n* Add support for per channel weight scale in dense op #4880\n* Add support for quantized models via QNN #4977 #5013\n* Support 4D padding. #5036\n* [Requantize] Cleanup and Optimize Lowering (#5286)\n* [Topi, ARM] Disbale Winograd for quantized tensors. (#5363)\n* Adding support for TFLite QnnSubtract operator. (#5230)\n* Remove developer facing api from frontend exports. (#5375)\n* Add Quantize/Dequantize Partitioning #5940\n* Add support for quantized models via QNN #5016\n* Quanitze operation expanded to take const argument #6127\n* FP32 and Quantized Object Detection Model #5479\n* Support CallNode inputs in qnn.concatenate #5360\n* QNN support for TFLite 2.1.0 quantized models #5848\n\n#### TE\n* Tighten split's extent #4931\n* Set split node's range to minimum of ext and split factor or split np #5044\n* Support mixing normal and cross-thread reduction (#5193)\n* Inline -> `te/schedule/operation_inline.h` (#5386)\n* Create loops according to storage scope and thread hierarchies (#5190)\n* Fix import in dump pass ir (#5327)\n* Scalar support for te.extern #6079\n\n#### TIR\n* IR readability enhancement (#4501)\n* Introduce tir::PrimFunc #5070\n* Introduce PrimFuncPass. #5139\n* [TIR] Enhance Substitute, python bindings for Substitute/PostOrderVisit (#5400)\n* [TIR] Remove ProducerConsumer and `AllocateNode::new_expr` (#5333)\n* [TRANSFORM] Enable CopyOnWrite for TIR passes. (#5309)\n* [REFACTOR] Migrate LowerTVMBuiltin, InferFragment, LowerThreadAllreduce, ThreadSync to Pass Manager (#5213)\n* [REFACTOR] Remove te::Tensor dependencies from TIR passes. (#5372)\n* [TIR] Refactor MakePackedAPI to target dependent stage. (#5326)\n* [REFACTOR] tvm.hybrid -> te.hybrid (#5223)\n* [REFACTOR] Migrate most of low-level build to use the Pass Manager. (#5225)\n* [REFACTOR] Migrate low-level passes in tvm.lower to the Pass Manager (#5364)\n* [TIR] Migrate VTA TIR passes to the new pass manager. (#5397)\n* [REFACTOR] Migrate all low-level passes to the Pass Manager. (#5233)\n* [REFACTOR] Introduce ExprDeepEqual, Remove IRDeepCompare (#5206)\n* [REFACTOR] RewriteForTensorCore -> te/schedule (#5379)\n* [REFACTOR] Remove `ir_pass` in favor of analysis/transform. (#5415)\n* text format printer considering future parsing use #5483\n* Remove buffer params from pass config. #5652\n* std::string -> String Migration in TIR nodes #5596\n* Remove `CallNode.call_type` in favor of attribute. #5937\n* Remove legacy HoistIfThenElse #5944\n* Improve Let/LetStmt support. #5949\n* Refine side effect analysis. #5954\n* `Provide->ProducerStore`, `Realize->ProducerRealize`. #5750\n* Migrate the tvm/tir/expr.h to constructor #5773\n* Migrate tir/stmt.h to use constructor. #5778\n* Cleanup unused classes #5789\n* Add tir prefix to type keys #5802\n* Enhance VerifyGPUCode #6194\n* Enforce buffer pointer var type to be consistent with dtype. #6317\n* Create a StringImm reference type #4806\n* Add init member to ReduceNode #6138\n* Add dump and print for debugging (NFC) #5207\n* Streamline Function Attr interface. #5045\n* `alpha_equal` to `structural_equal` #5161\n* Remove AttrsEqual and AttrsHash related code #5169\n* [NODE] General serialzation of leaf objects into bytes. (#5299)\n* [POC] Initial stab at `std::string->String` upgrade (#5438)\n* [TIR] Make `lower_warp_memory` support `extent(threadIdx.x) < warp_size` (#5307)\n* [PASS] dtype rewrite for indexing variables (#5092)\n* [PYTHON] Enhance `with_attr` API, cleanup MakeAPILegacy in testcases (#5335)\n* [PYTHON] Make IntImm more like an integer (#5232)\n* [IR] Move to runtime::String (#5276)\n* [IR] kExternalSymbol -> kGlobalSymbol (#5211)\n* [IR] Remove PrimExpr from String (#5311)\n* IRModule is updated with String #5523\n* IR is updated with String #5547\n* Streamline ir/op Registry #5609\n* Migrate IRModule ObjectRef to not-null #5654\n* Migrate BuildConfig to PassContext. #5668\n* relay.op.Op -> tvm.ir.Op #5705\n* Separate ArgTypeCode from DLDataTypeCode #5730\n* Remove legacy `compute_expr.h` #5738\n* Call::Halide => ProducerLoad, DSL/TIR decouple. #5743\n* `Provide->ProducerStore`, `Realize->ProducerRealize`. #5750\n* Migrate the tvm/tir/expr.h to constructor #5773\n* Migrate tir/stmt.h to use constructor. #5778\n* Migrate all Object construction to constructor. #5784\n* Cleanup unused classes #5789\n* Finish `std::string->String` updates #5793\n* Add tir prefix to type keys #5802\n* Change Call.name to Call.op(RelayExpr) #5863\n* Range/IntSet API style consistency. #5953\n* Separate ArgTypeCode from DLDataTypeCode #5730\n* Migrate all Object construction to constructor. #5784\n* Finish `std::string->String` updates #5793\n* Unify StrMapNode and MapNode #5687\n\n#### Performance Improvements\n* Int8 GEMM performance enhancement using Cublas (#4550)\n* Speedup TSIM with multi-threading (#4491)\n* Support cudnn softmax (#5214)\n* Add cuDNN grouped convolution support (#5319)\n* Winograd support for Conv3D (#5186)\n* Improve `get_valid_count` and nms performance for CUDA (#5339)\n* Optimizations of `global_ave_pool` for NHWC layout (#5450)\n* Optimization of Conv2d Winograd algorithm on Tensor #5485\n* Some performance improvement to VM #5901\n* Optimize x86 `conv3d_ndhwc` using data packing approach. #4866\n* Improve NHWC depthwise convolution for AArch64 #6095\n* Improve quantized convolution performance for armv8 architectures #5754\n\n#### Documentation\n* Adding benchmark log format doc (#4366)\n* Add Ninja build system to installation docs (#4554)\n* Doc/comment fixes (#4452, #4463, #4469, #4493, #4397, #4580, #4585, #4591)\n* Fix doc after moving to unified IR #4835\n* Introduction to module serialization #4564\n* ConvertLayout - Call RemoveUnunsedFunctions. #4834\n* Fix bugs that override `n_trials` #4842\n* Update the vm doc #4868\n* Refine the example description of `max/min/sum/tag_scope` #4974\n* Fix vta tutorial #4809\n* Introduce how to add hardware backend to FAQ #4898\n* Update API docs to reflect the status after the refactor. #4907\n* Fix sphinx warnings #4917\n* Fix Sphinx Warnings (RST indent, cross-ref, and image scale) #4920\n* Fix Sphinx Warning: the target found for cross-reference #4925\n* Sphinx -- Introduce alias detection. #4954\n* Fix Warnings from #4942 #4959\n* Fix sphinx precheck #4967\n* Move `git_howto` to rst, add Stage documents to te #5055\n* Add doc for Relay op strategy #5078\n* Update relay docs #5112\n* Include a tarball of docs, add a security faq #5119\n* Cleanup docs before rebuild #5127\n* Minimize necessary doc change #5129\n* Various sphinx related fix. #5168\n* Point docs to the ASF site. #5178\n* Use https link #5183\n* Reduce artifcats generated by sphinx gallery #5208\n* Refine the example description of `max/min/sum/tag_scope` #4974\n* Description updated for pooling attributes #5091\n* [DOCS] Migrate some markdowns to rst, fix sphinx3 warnings (#5416)\n* [DOCS] Misc docs improvements (#5222)\n* [DOCS] Bring relay docs to the top-level flat view (#5343)\n* [DOCS] Reduce artifcats generated by sphinx gallery (#5208)\n* [DOCS] Use https link (#5183)\n* [DOCSTRING]missing function parameters updated (#5228)\n* [DOCS] Migrate HLS documents from md to rst (#5419)\n* [Tutorial, QNN] Add tutorial for loading quantized PyTorch model (#5321)\n* [Docs] VTA install doc migration from md to rst (#5442)\n* [Docs] compiler version in docs (#5281)\n* Remove legacy `compute_expr.h` #5738\n* `TVM_REGISTER_API` -> `TVM_REGISTER_GLOBAL` #4768\n\n#### Bug Fixes\n* Add bfloat16 typeflag support (#4525)\n* MSVC / Windows fixes (#4455, #4569)\n* Fix Makefile for `howto_deploy` (#4457)\n* Fix GCC 4.8 compact (#4461)\n* Fix search path to build `libtvm_topi.so` (#4467)\n* Fix for `conv2d_transpose` CUDA compilation (#4472)\n* Fix for LLVM 10.0 codegen (#4480, #4515)\n* Fix alter op layout when calling global var (#4454)\n* Fix `float2half_rn` support for cuda compute capabilities < 53 (#4489)\n* Fix compile errors for OpenCL backends (#4492)\n* Fix serialization precision loss (#4503)\n* Fix hybrid script to support array of tensors (#4494)\n* Fix annotation for multiply op (#4458)\n* Fix Dockerfile for linter CI (#4506)\n* Fix TF resize for dynamic size models (#4510)\n* Fix `bias_add` gradient (#4516)\n* Fix tanH unit test function call (#4517)\n* Fix extra reshape parameter for ONNX (#4524)\n* Fix crash caused by empty TOPI config (#4520)\n* Fix ONNX shape op type to use int64 (#4528)\n* Fix crash in TSIM virtual memory driver (#4527)\n* Replace deprecated python library in setup script (#4533)\n* Fix NMS `max_output_size` loop (#4541)\n* Fix style in IR mutator and IR visitor (#4561)\n* Fix compiler warning (#4559)\n* Fix to get end to end inference on Chisel VTA (#4574)\n* Fix LLVM build by adding missing intrinsics headers (#4575)\n* Fix context creation in quantization (#4582)\n* Fix NDArray SaveDLTensor signature (#4586)\n* Fix dense pack schedule for x86 (#4539)\n* Fix for broadcast tensor of scalar type (#4577)\n* Datatype refactor (#4513, #4560)\n* Add const qualifiers for NDArray container (#4590)\n* Fix TF <= 1.12 compatibility (#4593)\n* Fix for graph debug runtime (#4598)\n* Disable copy constructor for external codegen (#4597)\n* Make ADT tag signed (#4605)\n* Added declare of aluBits for TensorAlu #4624\n* Get around limitation of g++-4.8 #4626\n* Bugfix StmtMutator IfThenElse #4609\n* Remove unecessary rdynamic #4613\n* Resolve constexpr related link error in debug mode #4641\n* Asymmetric padding #4511\n* Reduce data size of asymmetric padding testcase #4658\n* Fix Base64OutStream portability issue #4668\n* Fix `topi.nn.global_pool` layout=\"NHWC\" #4656\n* Also package core.rly #4679\n* fskip of EliminateCommonSubexpr cannot always return false #4620\n* Fix Python syntax error in `start_rpc_server_to_tracker.py` #4682\n* os.path --> osp to match the import #4681\n* GitHub actions/checkout@v1 --> v2 #4680\n* Fix Python syntax error AGAIN in `start_rpc_server_to_tracker.py` #4685\n* Use ==/!= to compare str, bytes, and int literals #4686\n* Rename `start_rpc_server_to_tracker.py` to `start_rpc_server_to_tracker.sh` #4689\n* GitHub Action lint Python code for syntax errors #4688\n* Generate blob use LLVM directly #4657\n* Reduce input size to fix oom #4653\n* Fix RemoveUnusedFunctions pass #4700\n* Link the math library by default #4713\n* Update mainline version to 0.7.dev0 #4720\n* Add SizeVar representing non-neg valued variable in a tensor shape #4684\n* Fix the compile problem of `cpp_rpc` #4725\n* JSON upgrader to upgrade serialized json. #4730\n* Fallback schedule for Int8 depthwise. #4733\n* Fix dense x86 schedule #4728\n* Fix demo dockerfile build failed #4744\n* Improve CUDA vectorizer #4736\n* Add .asf.yaml for github info #4761\n* Fix padding in pooling op #4738\n* Remove `run_infer_type` duplicates #4766\n* pooling.cc improvements #4767\n* Export `builtin_fp16` on Windows #4731\n* Fix Tensorflow conv3d pad bug, add non-cubic data and kernel tests #4772\n* Bump prebuilt-image version in demo dockerfile #4770\n* Update `tune_simple_template.py` #4778\n* Explicitly link to cublasLt if it exists #4776\n* Fix hasattr by extracting Python error type from Windows error message #4780\n* Replace os.path.exists with try...except...else #4784\n* Make sure to visit the arguments of inlined functions #4783\n* Parse additional exception strings #4785\n* Fix #4670: add bias for fc layer #4801\n* Change color channel from BGR to RGB for darknet preprocessing #4794\n* Fix -Wextra #4804\n* Fix vta tutorial #4809\n* Minor bug fixes in AutoTVM for QNN graphs #4797\n* Fixed subprocess creation under windows #4820\n* Improve tol to resolve flaky case #4836\n* Fixed process termination routine in windows #4844\n* `test_cuddn` flaky #4846\n* Mxnet parser for Qnn dialect #4714\n* Enhance `cc.cross_compiler` #4817\n* Fixed crash caused by reversing bitwise operations #4852\n* Reverse some changes made for `intel_graphics/conv2d.py` in PR #4849 #4853\n* const auto p -> const auto& p #4861\n* Fix onnx import bugs #4750\n* Explicit llvm::StringRef to std::string conversion #4859\n* Update the runtime PackedFunc for module #4871\n* Improve antlr import error message #4888\n* Fix `alpha_equal` bug for attribute check #4897\n* Fix issues in cuda codegen #4876\n* Fixed: Bitwise ops on floats causing wrong code generation and crashes. #4892\n* Fix `tvm.target.generic_func` runtime detection #4910\n* `topi/tests/python/test_topi_sort.py::test_argsort` #4891\n* Use opencv reisze method for preprocessing of image in darknet #4883\n* Fix build breaks with StringRef changes #4923\n* Remove unnecessary spliting in the cached chunk #4935\n* Fixing an Infinite Loop case in UnmatchedChecker. #4881\n* Remove SGX toolchain installation from CI Dockerfile #4948\n* Fix tedd tutorial after strategy change #4947\n* Allow customize MKLDNN library location #4814\n* Added CopyFromBytes and CopyToBytes convenience methods to NDArray. Fixed typos. #4970\n* Fix gcn tutorial failure #4994\n* Fix stride default value None in torch.nn.functional.avg_pool #4984\n* Fix ROCm strategy for winograd conv selection #5001\n* Fix `get_valid_count` flaky test for cuda #4901\n* Change Scala Linter scalafmt => scalastyle #4998\n* Kill from tvm import te #5007\n* Chisel fixes and de10nano support #4986\n* Fix gpu not found when running TVM docker #4975\n* Fixes for pylint==2.4.4 #4849\n* Fix unordered dictionary problem for python version under 3.6 #4982\n* Fix gcn tutorial failure #4994\n* Fix stride default value None in `torch.nn.functional.avg_pool` #4984\n* Fix ROCm strategy for winograd conv selection #5001\n* Early checking added and new test cases added for schedule fuse #5010\n* Fixed div by zero core dump. Fixed rounding intrinsics on int crash #5026\n* Test case modified for int type #5012\n* Bug Fix for ARM CPUs. Lower strict assumption. #5063\n* Triage the testcases to fit the new namespaces #5071\n* Add colors to `compute_at` edges and thread/block indices. #5111\n* Temporary fix to the stack overflow issue in autotvm task extraction #5019\n* Fix compilation of If-Elses #5040\n* Fix CompilerAttrs #5109\n* Fix the existing test cases before refactoring. #5122\n* Fixed bug where shifting by out-of-bounds value results in no compute code being emitted. #5115\n* Fix for issue #4831. The `data_min_idx` and `data_max_idx` were flipped. #5136\n* Duplicate likely nodes added when loop axis split unevenly #5084\n* Fix incorrect name of calibration mode #5150\n* Remove contrib spatial pack schedule of depthwise convolution #5148\n* Fix annotate pass static variable #5023\n* Fixed ConvTranspose2D parsing #5157\n* Nullptr check #5176\n* rocm: fix miopen convolutions #5179\n* rocm: fix `dense_rocblas` in strategy, topi #5191\n* Fix CRT static test bug (#5293)\n* Fix perf regression of tir refactor (#5258)\n* Bugfix in tensorflow `space_to_batch_nd` (#5175)\n* Compilation warnings fixed for 32bit and 64bit compilation (#5349)\n* Fix hang in MergeCompilerRegions (#5227)\n* Fixes to MergeCompilerRegions (#5195)\n* Fix generation of LLVM intrinsics (#5282)\n* Fix setting up hints for getaddrinfo (#2872)\n* Add ConstantNode to IsAtomic (#5457)\n* Fix String SEqual (#5275)\n* Fix fuse over functions that are handled by external codegen (#5365)\n* Fix memory leak when accessing NDArray (#5413)\n* Remove the duplicate PrintIR pass in Relay (#5403)\n* Fix `lower_warp_memory` (#5247)\n* Fix `lower_warp_memory` when there are >1 warp buffers (#5368)\n* Fix intel conv2d auto tune (#5200)\n* Fix FuseBatchNorm output cast error if `need_cast` is True #4894\n* Fix an assertion exposed by loop vectorizer #4916\n* Fix error message #4945\n* Fix for recursive let #5757\n* Fix Calibration Pass to Support Modules with Multiple Functions #5768\n* Fix what looks like bizzare copy-paste issue #6010\n* Fix bug in `transpose_shape_func` #6180\n* Fix bugs in CUDA codegen (#5209)\n* Dont remove() TemporaryFile in del. (#5414)\n* Fix `test_ir_type`. (#5390)\n* Fix multiple identical inputs bug (#5389)\n* Add cuda target check to dense tensorcore schedule. (#5376)\n* T2 test fixups (#5391)\n* Fix miopen padding (#5433)\n* Misc fixes for ROCm (#5431)\n* Fix copy constructor (#5237)\n* Corrected TVM autotuning on GPU (#5432)\n* Fix vector load (#5226)\n* Minor bugfix in `message_passing.cc` (#5254)\n* Fix a bug when vectorized load&store was involved for (#5428)\n* Fix to skip node not in graph. (#5238)\n* Fix #5388 [VULKAN] vkBuffer released before memory copy command se (#5418)\n* Fix a minor error in `device_annotation` (#5291)\n* Fix scalars ndim is 0 (#5344)\n* Fix the runtime raise error #5586\n* Fixed bug in attribute parsing for pool layers. #5582\n* AutoTVM incorrect measurement #5511\n* fix a min/max simplify bug #5761\n* Rename `tvm_dso_op` to `libtvm_dso_op` #5714\n* Fix generating types like float44 and float88 #5722\n* Avoid downloading when `TOPHUB_LOCATION` is NONE #5720\n* codegen llvm: move nvptx-specific intrinsic handling into `codegen_nvptx` #5726\n* ROCm warp shuffles and reductions #5727\n* fix small bug about `dense_grad` #5695\n* Clarify downstream consistency of TVMArgTypeCode #5742\n* Fix gelu in PyTorch frontend, tighten numerical checks #5763\n* Make batch matrix multiplication on GPU tunable #5752\n* update vulkan build rule #5777\n* aten::norm support added #5776\n* Edit onnx parser to infer values in post order #5755\n* Support symbolic inputs of Fill #5762\n* support `aten::type_as` in the pytorch frontend #5787\n* Temporary disable fp16 `type_as` test for PyTorch Frontend #5799\n* Add config switch for nn.dense layer type. #5801\n* Move cpu-only frontend tests to a CPU stage #5807\n* Pin hand landmark network to version 0.7.4. #5813\n* Limit number of threads in all jobs #5815\n* Error msg update #5818\n* fix relay.build to not change the module argument in place #5822\n* Fix InferType when module contains Prelude #5797\n* Add a combine `batch_matmul` pass #5791\n* RepeatVector, Conv3DTranspose op support added #5833\n* Fix converting serialized quantized models #5839\n* ffi (Object): make class dict visible in instances #5843\n* Additional canonicalization added for AddNode #5846\n* Suppress the warning messages when compile engine selects impls #5821\n* fix #5849 #5851\n* Introduce POD-C Compliant tvm::Map #5740\n* Add bfloat16 #5601\n* Add Python Classes for all Attrs #5853\n* Fix map assign issue in CI test #5854\n* Introduce Target Id Registry #5838\n* Update `has_dtype/has_shape` to pattern lang doc #5847\n* Add `nn.batch_flatten` as quantizable. #5805\n* Fail early before running invalid dynamic graphs #5856\n* Improve type handling in PyTorch frontend #5834\n* HotFix the python intrin rule #5895\n* add a few gradients #5899\n* Add Binary Intrinsic ops to TIR Ops in C++ #5900\n* Allow implicit conversion in TVM FFI to tvm::Bool #5907\n* PyTorch frontend: fix handling of duplicate use of a model weight #5897\n* Dont multiply by constant 1 uselessly in dense #5911\n* Support any index matching for TupleGetItem #5909\n* Add MicroTVM tutorial using the STM32F746 discovery board #5655\n* Fix serialization of inf float value #5912\n* Fix CPU Thread Binding for Multiple Sockets #5918\n* CUDA device API & VerifyGPUCode pass update #5898\n* Update install.rst #5858\n* Two small fixes to AMDCPU codegen for LLVM 10+ and ROCm 3.5+ #5920\n* Add LegalizeInvalidAttach to legalize the `compute_at` location after split or fuse #591\n* Dont rewrite expressions used outside of the pattern #5930\n* Add TupleGetItem to CSE #5931\n* Various update for CoreML codegen #5934\n* Update date in the NOTICE #5943\n* Raise right error in tensorflow split op #5951\n* Add rm xla attributes in tf docs #5950\n* Fix OpenCL `get_valid_counts` errors due to intrinsic `atomic_add` #5857\n* Amendments for gradients #5941\n* Fix the meaning of `conv{1,2}d_transpose` `output_padding` parameter. #5758\n* Make first order gradient graphs more efficient #5959\n* Raise an exception when extern function does not return Stmt #5964\n* Improve docker/bash.sh to handle git worktrees #5970\n* Install DNNL (OneDNN) to CI Environment #5936\n* Add Dynamic reshape to a dynamic namespace and add DynamicToStatic Pass #5826\n* Add meshgrid op in Relay, TOPI, Pytorch frontend #5961\n* Print right number of parentheses for LoadNode #5965\n* Migrate data structure of TargetNode #5960\n* Remove redundant function CreateBufferVecPtr #5982\n* Fix string argument mismatch in GraphRuntimeCodegen #5933\n* VectorType::get with two parameters is deprecated in LLVM 11+ #5984\n* Fix Compilation Error in CRT #5713\n* Fix runtime::String backward compatibility in JSON #5725\n* Allow RPCWrappedFunc to rewrite runtime::String as std::string #5796\n* Fix reshape #5739\n* Fix building with LLVM-10 on macOS #5859\n* Add cuda 11 to `contrib.nvcc.find_libdevice_path()` #5902\n* Fix sequential cpp test #5745\n* Infer types in MergeComposite #5766\n* Fix recursive let for well formed check #5780\n* Recover global state after `test_util.py` #5824\n* Fix bug in rpc ring buffer shrink #5516\n* Fix remote device sync #5538\n* Fix bug in rpc ring buffer shrink (#5516) #5537\n* RPC Server error fix on Pynq FPGA #5607\n* Fix FloorMod Simplifier #5509\n* Fix Python debugger segfaults with TVM built with LLVM #5685\n* Fix Compilation Error in CRT #5713\n* Fix runtime::String backward compatibility in JSON #5725\n* Allow RPCWrappedFunc to rewrite runtime::String as std::string #5796\n* Fix reshape #5739\n* Make \"none\" DataType explicit #5491\n* Change \"scalar\" and \"stack\" in IDL from \"inrout\" to \"in\" #5487\n* Link necessary libraries when building runtime for Android #5496\n* Fixes for wasm32 target #5489\n* Reset target and wait for runtime initialization on connect. #5499\n* Bump tophub rocm version #5504\n* Improve commentary for RingBuffer #5518\n* Add unit tests for ONNX PRelu and fix importer to pass them. #5521\n* LRN only supports 4D tensors, remove it from `alter_op_layout` #5520\n* Fix an issue with ONNX Upsample #5530\n* Cache PrimExpr instead of raw pointers in bound analyzer #5533\n* fix a few bugs with shape inference and types in the ONNX importer #5534\n* Add Onnx Pad v11 #5539\n* Changes to `cpp_rpc` to make it work on Android (+ Hexagon offloading) #5535\n* Fix to reduce RAM size during loading model #5507\n* Fix MakeLoopNest for warp memory #5382\n* Load platform specific lib for tvmdsoop instead of the hard-coded tvm_dso_op.so #5542\n* Add tests for running micro on native arm hardware #5546\n* Apparently, ONNX Conv with no 'pads' defaults to zero padding #5548\n* clang-format the h,cc,m files. #5557\n* Fix conv2d alter op for arm cpu #5532\n* Fix topi test for non tensorcore CI. #5563\n* Add clang-format and nodejs to ci-lint #5567\n* Enable clang-format. #5572\n* Allow `ubuntu_install_darknet.sh` to work in both 18.04 and 16.04 #5574\n* Add a quantized conv2 unit test for the tflite front-end #5558\n* Fix JSON graph dumping. #5591\n* Warp level reduction support for CUDA #5498\n* One more fix for concurrency count #5589\n* Improve robustness of the docs build #5583\n* Phase out WebGL #5570\n* Fix vulkansdk in the ci-gpu and upgrade to 1.2.135 #5566\n* Update ci-cpu to bionic #5554\n* Overestimate binary size for microTVM compiled binaries. #5590\n* Fix bug and re-enable RPC execution test #5436\n* Add ostream formatters for TargetPtr/TargetVal. #5592\n* Fix cross thread reduction #5551\n* Fix TVMArray layout on device #5599\n* Add debug mode to tempdir() #5581\n* Represent alignment information in LLVM IR #5598\n* Fix codegen for warp shuffle intrinsics #5606\n* Fix Topological Order calculation for DFPattern Language #5612\n* Global MaxPool3d and AvgPool3d support #5098\n* Fix build error of iOS RPC #5621\n* isn't a CallNode sometimes #5623\n* Introduce config to PassContext. #5631\n* CMAKE fix #5630\n* Label Pattern Partitions #5627\n* Extend AttrPattern to support CallNode and FunctionNode attributes #5637\n* Increase bss section size. #5660\n* Add buffer name when creating tensor bindings #5670\n* tvm debug improvements #5648\n* enable `amd_apu` device on vulkan target #5659\n* Support TupleWrapper as direct ancestor of control flow ops #5639\n* add tvm.micro pydoc to sphinx #5661\n* Add a regression testcase for #5674 #5677\n* Fix C++ RPC build problem on Linux #5671\n* Add a check Callback to the Pattern Paritioner #5646\n* Call previous excepthook in `tvm_excepthook`. #5675\n* Fix the shift column for `scale_shift_nchw` and `scale_shift_nhwc` in C topi #5679\n* Support more dtypes for TVMDSOOp #5694\n* In `memory_plan`, check if value is not None, instead of just checking value as boolean. #5700\n* Fix flaky `test_topi_pooling.py:test_adaptive_pool` #5736\n* Fix the values for `test_fmod` since it fails way too often otherwise #5723\n* fix small bug about `dense_grad` #5695\n* Fix sequential cpp test #5745\n* Add Scatter to Topi/Relay/ONNX via hybrid script #5619\n* Clean WASM environment before build #5759\n* Fix gelu in PyTorch frontend, tighten numerical checks #5763\n* fix #5686: remove a overstrict assert in MakeAllreduce (#5686) #5785\n* Improve Pattern Language Docs #5676\n* Add missing expr visitor for any #6082\n* Remove the tvm web from version update #6122\n* Clear relay cache after every build & Clear warning message cache after autotvm task extraction #6131\n* avoid unexpected throw in AttrInitEntry #6128\n* Verify that tensor reshape is valid. #6215\n* Use LocalRunner by default in the tutorial tune_relay_cuda.py #6001\n* Undefined names: import os for line 324 & import re for line 308 #6003\n* GitHub Actions upgrade to actions/setup-python@v2 #6002\n* Only pass pythonpath for ci images #6005\n* Auto-convert shuffle with single index to extract element #6006\n* Cache object refs in loop partitioner instead of object pointers #6004\n* Fix `test_arith_solve_linear_inequality.py::test_multi_equal` #6014\n* MXNet frontend support for AMP cast op #5976\n* Demo showing how to run a pruned model. #5975\n* Move compiler related registry items to `vta/build_module.py` #6012\n* Pin keras version #6032\n* Fix in `arm_cpu/conv2d_alter_op` for NHWC quantized #6027\n* Add creation of Hexagon device in RPC client #6035\n* Terminate basic block after ret instruction #6036\n* TVM CRT modifications for on-device RPC server #5921\n* Create TBAA information based on the unrelying buffer type #6046\n* Add support for tflite `arg_min` and `arg_max` #5992\n* Fix `fully_connected` converter when batch size is not 1 #6038\n* Fix a primitive check error #5991\n* Refactor to expose MakeOp functions to C++ #6047\n* Fix `conv2_gemm` after target structure update #6037\n* Remove use of designated initializers from `hexagon_module.cc` #6055\n* Build crttest and cpptest separately. #6057\n* Fix pytorch frontend prim::Constant issue #6051\n* update frontend tutorials to new model based runtime interface #6063\n* Remove unnecessary std::cout #6072\n* Fix error message in Buffer::vstore, NFC #6056\n* Fix FSIM Compile Error. #6070\n* Improve vector simplification for float operands #6043\n* Fix LocalBuilder on macOS with python 3.8. #6083\n* Add missing test for fast erf #6058\n* Fixed point multiplication improvements for AArch64 #5980\n* Fix code generation bugs for C/CUDA & Improve VerifyGPUCode pass #6041\n* Delete declaration of unused `op_node` #6102\n* Load configs even it has no entity #6100\n* Update SGX example Cargo.toml #6067\n* Add default value for option `USE_DNNL_CODEGEN` in the cmake #6099\n* Update installation doc with minor improvements #6104\n* lint: add opencl .cl file type #6092\n* Clean up conversions between TVM and Rust functions #6114\n* Improve reduction schedule on arm CPUs #6110\n* Register Shape Func for Some Operators to Handle Dynamic Shapes #5955\n* Fix variable name conflict with OpenCL keyword #6048\n* Some rust cleanups #6116\n* Option to specify alternate directory to output build to #6016\n* Add `get_num_inputs` to GraphRuntime #6118\n* TFLite quantized conv test #6084\n* Fix autotvm on the `conv2d_nchw_winograd.mali` operator #6130\n* add attr option mfloat-abi for arm32 #6123\n* Fix CUDA Library Tuning #6132\n* Add missing RPC sources after refactor #6113\n* Correct `runtime.load_module` #6161\n* Improve error messages in graph tuner, graph runtime, and module loader. #6148\n* Fix some shape mismatches between TF and Relay #6166\n* Improve doc string #6176\n* Fix incorrect function signature in header #6172\n* Fix alignment of note #6181\n* Implemented PADV2 Operator for TFLite and added support for constant values in PAD. #6167\n* Unary ops support added in frontend #6196\n* Change the meaning of `conv3d_transpose` `output_padding` to match `conv{1,2}d_transpose` #6065\n* Fix compile warnings. #6204\n* Fix -mfloat-abi=soft compilation for ARM with OpenCL target #6150\n* Match pytorch 1.6 googlenet pretrained model (#6201) #6212\n* Mod operator, bug fix #6160\n* RESHAPE with dynamic shape arg in TFLite frontend #6208\n* Fix compilation error with cuda 11 #6213\n* Fix `port_end` wrong default value 9199 to 9099 for keeping same with source code #6220\n* Std op without specified dimensions support #6226\n* fix crt building and running error #6231\n* Implemented `ONE_HOT` Operator for TFLite. #6223)\n* Avoid unexpected throw in AttrInitEntry #6128\n* Added casting to hybrid script doc and fixed pass infra doc #6174\n* Fix compile warnings. #6204\n* Fix -mfloat-abi=soft compilation for ARM with OpenCL target #6150\n* Mod operator, bug fix #6160\n* Fix compilation error with cuda 11 #6213\n* Fix `port_end` wrong default value 9199 to 9099 for keeping same with source code #6220\n* Std op without specified dimensions support #6226\n* Verify that tensor reshape is valid. #6215\n* Fix crt building and running error #6231\n* Fix `conv2d_transpose` output padding #6236\n* Fix cuda half math function is undefined: hpow, htanh #6225\n* Fix division range estimation error in simplifier #6244\n* Fix newer GCC compiler warnings. #6257\n* Support `_contrib_SyncBatchNorm` #6245\n* Fix reduction #6250\n* Add apt repository for clang-11 and llvm-11 #6256\n* Update tutorial to new TARGET as `micro_dev` is no more #6262\n* Fix clang-format #6264\n* Trivial fix, up the rodata section for the discovery board to 512 bytes. #6259\n* Fix cuda half math function is undefined: hpow, htanh #6253\n* Add dilation in x86 NCHWc depthwise conv support #6267\n* Decrease test times by introducing testing model #6235\n* Add support for parsing the any dimension. #6277\n* Improve error messages for memory verifier and gpu memory verifier #6281\n* Reflect Compile-Time CMake Options into libtvm.so #6280\n* Add cmake options into libinfo #6286\n* Update slice to infer attributes when not graph inputs #6276\n* Use rpc.LocalSession for simple tests #6294\n* Fix random fail #6312\n* Fix resize test #6298\n* Fix cython FFI compact with np.int64 #6321\n* Fix relay vm optimize #6322\n* Changed TVMCTVMContext to TVMContext #6306\n* Make able to compile with MSVC #6341\n* ROCm changed name of library and removed the old one in ROCm 3.7 release. #6345\n* Compatible for ROCm before 3.7 #6359\n* Use clear name that is separate from ASF brand for cache #6360\n* Fix `Dockerfile.demo_android` #6361\n* Fx sparse dense schedule on cuda #5803\n* Fix strategy for sparse dense cuda #5782\n* Fix x86 conv2d template when tuning with unpacked layout #5938\n* Fix the filter width parameter in `depthwise_conv2d` #6081\n* Fix reshape usage in ARM schedule #5732\n* Missing header #4865\n* Fix `conv2d_transpose` output padding #6236\n* Simplify reduce expression in te.gradient #6611\n\n### API Changes\n* `tvm.module` -> `tvm.runtime.module`\n* `tvm.module.load` -> `tvm.runtime.load_module`\n* `tvm.module.enabled` -> `tvm.runtime.enabled`\n* `tvm.module.system_lib` -> `tvm.runtime.system_lib`\n* `tvm.relay.Module` -> `tvm.IRModule`\n* `tvm.create_schedule` -> `tvm.te.create_schedule`\n* `tvm.placeholder` -> `tvm.te.placeholder`\n* `tvm.compute` -> `tvm.te.compute`\n\n### Deprecation\n* Deprecate NNVM (#4535, #4562, #4565, #4571)\n* Deprecate FreeStmt #5890\n* Remove legacy `compute_expr.h` #5738\n* Deprecate OpenGL #5711, #5712\n\n## 0.6\n\n### Relay in Production\nRelay is a functional, differentiable programming language designed to be an expressive intermediate representation for machine learning systems. Relay supports algebraic data types, closures, control flow, and recursion, allowing it to directly represent more complex models than computation graph-based IRs (e.g., NNVM) can. In TVM v0.6, Relay is in stable phase and is ready for production.\n\n* Algebraic Data Types (ADT) support (#2442, #2575). ADT provides an expressive, efficient, and safe way to realize recursive computation (e.g., RNN). Refer to https://tvm.apache.org/docs/langref/relay_adt.html for more information.\n* Pass manager for Relay (#2546, #3226, #3234, #3191)\n* Most frameworks have been supported in Relay, including ONNX, Keras, Tensorflow, Caffe2, CoreML, NNVMv1, MXNet (#2246).\n* Explicitly manifest memory and tensor allocations in Relay. (#3560)\n\n### Relay Virtual Machine\nThe Relay Virtual Machine (Relay VM) is the new generation of runtime to strike a balance between performance and flexibility when deploying and executing Relay programs. Previously, the graph runtime is able to utilize the fully static nature of the input graphs to perform aggressive optimization such as fully static allocation, and optimal memory reuse. When we introduce models which make use of control-flow, recursion, dynamic shapes, dynamic allocation we must change how execution works.\n\nRelay VM is now usable and is able to achieve decent performance for a various of models and targets.\n\n* Design (#2810 #2915) and a first version of implementation (#2889),\n* Add VM runtime for Relay and compiler support (#3120, #3121, #2889, #3139)\n* Relay VM (pattern matching #3470, port to python #3391, serialization #3647)\n* Relay VM Profiler (#3727)\n* Support execution on devices for Relay VM (#3678)\n* [Relay][VM] Add more passes to VMCompiler (#4058)\n* [relay][vm] Separate VM runtime with executable (#4100)\n* Port VM, VM compiler, and Object into Python (#3391)\n* VM: Add AllocTensor instruction and better instruction printer (#3306)\n* [Relay][VM][Interpreter] Enable first-class constructors in VM and interpreter via eta expansion. (#4218)\n* [Relay][VM] Clean up the VM and VM profiler code (#4391)\n\n### Training\nRelay is designed to natively support first-order and higher-order differentiation. The automatic differentiation infrastructure is now usable and a count of operators with gradient support are available in v0.6 release.\n\n* Higher order reverse mode automatic differentiation that work with control flow (#2496)\n* Higher order continuation passing style (#3456, #3485 )\n* Relay gradient registration (clip #3509, `max_pool2d` and `avg_pool2d` #3601)\n* Relay AD algorithm (#3585)\n* Relay Training - allow gradient to return a tuple (#3600), numerical gradient check (#3630)\n* Improve AD for concatenate (#3729)\n* [Relay][Training] Add missing gradient check to gradient pass (#4169)\n* As a part of Relay's automatic differentiation system, we are adding primal gradients for Relay operators. Please refer to #2562 for tracking the progress.\n* Gradient for Conv2d (#3636)\n* Add gradient operators (#3857, #3894, #3901, #3915)\n* Add gradient for log-softmax (#4069)\n* [Relay][Training] Add gradient for Crossentropy (#3925)\n* [Relay][Training] Add and fix gradients (#4126)\n\n### Quantization\n\nLow-bit inference is getting more and more popular as it benefits both the performance and storage usage. TVM now supports two types of quantization. 1. Automatic quantizaion takes floating-point precision model, does per-layer calibration and generates low-bit model. 2. TVM also imports pre-quantized model from Tensorflow and MXNet, a new dialect QNN is introduced to handle further lowering to normal operators.\n\n* Automatic Quantization\n  - Low-bit automatic quantization supported. (#2116). The workflow includes annotation, calibration and transformation.\n  - Refactor quantization codebase and fix model accuracy. (#3543)\n  - KL-divergence-based per-layer calibration. (#3538)\n  - Add option to select which convolution layers are quantized. (#3173)\n  - [Relay][Quantize] Integrate data-aware calibration into quantization. (#4295)\n* Pre-quantized model support (QNN operators and legalize pass).\n  - Add a legalize pass to Relay (#3672)\n  - Qnn Concatenate, quantize, dequantize and requantize operators (#3819,  #3730, #3745, #3531)\n  - QNNtoRelay & QNNLegalize Pass utility (#3838, #3782)\n  - Requantize: Optimize lowering for some corner cases. (#3864)\n  - New quantized operator support: conv2d, add, dense (#3580, #3736, #3896, #3910)\n  - Do type checking for the input and kernel in the qnn conv2d (#3904)\n  - Legalize and AlterOpLayout for Intel int8. (#3961)\n  - Renaming tests to follow the Relay nomenclature. (#3975)\n  - Fix padding changes due to #3739 (#3989)\n  - Memorizing quantize node mapping to avoid duplicated simulated quantization (#3233)\n  - Infrastructure to support pre-quantized models (QNN) (#3971).\n  - [Relay][AlterOp] NHWC to NCHWc support for Pool, concatenate, sum. (#4059)\n  - [TOPI][x86] Cascade lake support. (#4123)\n  - [TOPI][x86] Legalize - Support int8xint8 convolution to use VNNI inst (#4196)\n  - Qnn dequantize with min max using Mxnet flavor to support Mxnet prequantized models. (#3945)\n  - Improve the lowering of Qnn Dense (#4213)\n  - Adding support for dequantizing from int32 to float32. (#4130)\n  - [QNN] Refactor fixed point multiplication in requantize (#4073)\n  - [Relay][Quantize] Use fixed point mulplications (#4160)\n  - Add support for quantized multiply to Relay (#4141)\n  - Use legalize to handle NHWC layout for `arm_cpu` (#3754)\n  - [QNN][Legalize] Specialize for Platforms w/o fast Int8 support (#4307)\n  - [QNN] Use Int16 upcast in Fallback Conv2D. (#4329)\n  - Retain input kernel scales in QNN dialect (#4292)\n  - [QNN] Lowering for Depthwise Convolution. (#4351)\n  - [QNN][TFLite] Parsing QNN Add op. Adding MobilenetV2. (#4142)\n  - [QNN][TFLite] Parsing TFLite quantized models. (#3900)\n  - Added tflite frontend support for quantized mean. (#4339)\n  - [Relay][Legalize] Legalize `conv2d_transpose` for NHWC (#4399)\n\n### Accelerator and Microcontroller Support\n\nTSIM is introduced to improve software and hardware integration and simulation accuracy. It integrates the hardware development process into the software stack. TSIM enables VTA to provide a more accurate performance feedback, i.e. clock cycles, compared to the traditional functional model of a hardware accelerator. Moreover, Chisel implementation for VTA is availale and it runs on top of TSIM.\n\nThere has been a proliferation of resource-constrained and embedded devices that do not have operating systems or a mature software stack. MicroTVM is intended to support TVM on such bare-metal devices.\n\n* [TSIM] Enabling Cycle-Accurate Hardware Simulation for VTA (#3010, #3206, #3242)\n* Chisel implementation for VTA and runs on top of TSIM (#3258, #3347)\n* MicroTVM (#3227)\n* Relay Compilation + AutoTVM compatible operator libraries for VTA (#3135)\n* ChangeBatch pass for batched VTA compilation (#3656, #3660)\n* VTA fast simulator statistics (#3481)\n* TSIM improvements and fixes (#3505)\n* Chisel VTA enhancements and fixes (32bit support #3558, alu instruction generation #3592, coherence support #3593, separate types #3605, tensor issue/commit #3637, uop load request #3643, uop dma requests #3654)\n* VTA Runtime refactor for non-shared memory FPGAs (#3590)\n* VTA HLS codebase refactor for Ultra96 (#3496)\n* VTA support for batched inference (#3661)\n* VTA bitstream compilation for Intel FPGA (#3494)\n* TSIM: Introduce Virtual Memory for TSIM Driver (#3686)\n* Parallel TSIM hardware compilation with macOS and debug support (#3797)\n* Chisel: scale dram base address in hardware instead of runtime (#3772)\n* Chisel: run all unittests by default (#3766)\n* Chisel: improved Data Gen, Added ALU Test (#3743)\n* Chisel dependencies for TSIM CI (#3721)\n* Chisel: Added Module Unit Test Infrastructure (#3698)\n* Add ISA BitPat generation (#3891)\n* de10-nano driver (#3394)\n* Extending Vision model coverage compilation for VTA (#3740)\n* Conv2d transpose (deconvolution) operator support (#3777)\n* Support TLPP in function simulator. (#3555)\n* [VTA][Chisel] TSIM VTA Source Refactor (#4163)\n* [VTA][TSIM] Serial GEMM Application Added (#4082)\n\n### Rust Support\nRust language support in TVM includes two parts. 1. The frontend wraps the current C API and exposes a Rust programming model. 2. The backend serves as an alternative to C++ runtime. It privdes a standalone WASM module and security support, e.g., SGX.\n\n* Rust frontend (#2292).\n* Unify types between bindings and pure Rust impl (#2616)\n* Rust: load syslib modules at compile time (#3274)\n* Rustify PackedFunc & Friends (#2969)\n* Rust DSO module (#2976)\n\n### Operator Support\n* A special operator `annotation.stop_fusion` to prevent it being fused with previous expressions (#2624).\n* `batch_matmul`  supported (#2561).\n* `reverse_reshape` supported (#2503).\n* Faster-RCNN proposal operator for CUDA (#2420).\n* Vision operator for YOLO `yolo_reorg` (#1941).\n* `slice` operator for MXNet (#2662).\n* `arange` supported (#2621).\n* Vision operator `roi_align` (#2618).\n* `where` operator for MXNet (#2647).\n* Deformable conv2d (#2908)\n* Faster-RCNN Proposal OP (#2725)\n* ROI Pool operator (#2811)\n* Gluoncv SSD support on CPU (#2353)\n* shape, reverse, and sign op (#2749, #2800, #2775)\n* tile and repeat op (#2720)\n* logical operators (#2743, #2453)\n* stack op (#2729)\n* NCHWc upsampling (#2806)\n* clip and wrap mode support in take (#2858)\n* AlterLayout support for `intel_graphics` conv2d , depthwise conv2d (#2729, #2806)\n* Add foldr1 operator (#2928)\n* Add rsqrt operator (#2949)\n* Add clip and wrap mode support in take (#2858)\n* `Gather_nd` exposed to relay (#2945)\n* `bitserial_conv2d` move to autotvm template and updates (#2819)\n* Port x86 NCHWc to AutoTVM for Task Extraction (#2664)\n* Implement relay `nn.bias_add` compute in C++ (#3027)\n* Rename output tensors for better readability (#3006)\n* int8 dense on CUDA & Dense op quantization (#2877)\n* Bitserial dense operators for CPU (#3051)\n* Enhance upsample operator to adapt onnx opset v9 (#2968)\n* Add adaptive pooling operator (#3085)\n* Add all operator (#3124)\n* Add cblas `batch_matmul` (#3210)\n* Add packing for int8 1x1 convolution and support the int8 group convolution on X86 (#2991)\n* Add op size (#3094)\n* x86 TOPI (`roi_align` #3475, `conv2d_transpose` #3491)\n* Intel INT8 (dilation in conv2d #3510, type checking #3516)\n* Reinterpretation of tensor elements (#3599)\n* Spase-Dense for block-sparse multiplication (#3566)\n* Winograd matrix computation (#3553)\n* CUDA schedule for `pool_grad` (#3622), `group_conv2d` (#3663)\n* Bitserial operations conv2d, dense and bitpack (#3844)\n* Improve numeric gradient check (#3856)\n* Resize rework ([3788](#3788))\n* Improve `conv2d_transpose` CUDA schedule template (#3796)\n* SpaceToDepth and MirrorPad Operators (#3718)\n* Add variance and layer norm op (#3700)\n* Add `sparse_transpose` for Square CSR matrices (#3707)\n* TOPI: Memoize winograd matrix (#3687)\n* New TOPI operators: `erf`, `logical_and`, `logical_or`, `logical_not`, `isnan` (#3702, #3929, #3979)\n* Improve `ceil_divide` in tile/split (#3842)\n* [Relay][Frontend][TF] Add tensor array ops (#3798, #4309)\n* [TF][Op] Op where (#4045)\n* [TOPI]Add op argwhere (#3994)\n* [Relay] `crossentropy_with_logits` and its gradient (#4075)\n* [Relay][Op] Enhance Upsample Operator to support float scales (#4206)\n* [Relay][Op] Add instance norm op (#4004)\n\n### Frontend and User Interface\n* Frontend darknet (#2773)\n* Support tf.gather (#2935)\n* Support tf.where (#2936)\n* Adding ADD operator to tflite frontend for compiling the MobileNetV2 (#2919)\n* Support SpaceToBatchND/BatchToSpaceND in Tensorflow frontend (#2943)\n* Simplify TF `get_output_names` (#3025)\n* TF Tile Round Sign Pow Exp Reverse (#2960)\n* Gluncv SSD support on the GPU (#2784)\n* Allow an op as loop var in Tensorflow (#3056)\n* Add `FULLY_CONNECTED` op into tflite frontend (#3019)\n* Add MXNet converter for RNN layer ops (#3125)\n* Add log op in tf frontend (#3111)\n* Add SoftPlus Sqrt in Tensorflow frontend (#3187)\n* Add onnx elemwise greater/less (#3186)\n* Add PlaceholderWithDefault (limited) implementation in TensorFlow (#3184)\n* Support `tf.math.reduce_prod` (#3166)\n* Better shape inference in TensorFlow Frontend (#3176)\n* Get list of unsupported ONNX operators (#2995)\n* Implement ONNX MaxPool-v8 and MaxPool-v10 (#3114)\n* Convert TFLite NCHW to NHWC (#3141)\n* Add Crop op converter (#3241)\n* TFLite frontend operator support: PAD, RESIZE, MUL, Reduce (min, max, mean, prod), LOGISTIC, elemwise operators (Sub, Divide, Power, Max, Min) (#3310, #3370, #3304, #3421, #3313, #3357)\n* Tensorflow frontend operator support: Abs, FloorDiv, GatherND, LeftShift, LogSoftmax, Max, Min, Mod, RightShift, ZerosLike, TruncateMod, Neg, ClipByValue, ResizeNearestNeighbor (#3270, #3211, #3393)\n* TFLite: Add `fused_activation_function` for ADD, SUB, MUL, DIV (#3372)\n* Support bidirectional RNN layer for MXNet (#3397)\n* TFLite operator support (pack #3521, split #3520 )\n* Keras operator support (permute, softmax #3618)\n* TF operator support (BatchMatMul #3634)\n* TFLite frontend operator support: tile, transpose (#3814, #3705)\n* ONNX frontend operator support: PReLU for NNVM, Not, Sign, Equal (#3813, #3836, #3760)\n* Keras frontend operator support: Dot (#3668)\n* Add more cases to Keras `_convert_reshape` (#3846)\n* TensorFlow frontend operator support: OneHot, log1p, cos, sin (#3781, #3614)\n* Support BatchMatMul with input dimensions larger than 3 for TensorFlow (#3732)\n* ONNX new operator support: And, Tile, Erf (#3878, #3941, #3988)\n* MXNet new operator support: pad, conv1d, deconv1d (#3739)\n* TFLite new operator support: `batch_to_space_nd`, `space_to_batch_nd`, tanh, greater, relu (#3850, #3996, #3963, #4022)\n* TFLite: Support depthwise convolution multiplier greater than 1 (#3922)\n* Keras: Fix ReLU in Keras Converter missed the case (#3917)\n* Keras: frontend upsample and 1 channel conv2d fixes (#3937)\n* Tensorflow: Convert scalar Const into tvm.relay.const (#3885)\n* TensorFlow: Add support for SquaredDifference (#3930)\n* [relay][frontend] clean up tf frontend (#3710)\n* [Relay][Topi][TensorFlow][ONNX][Lang] Add support for Any op (#4205)\n* [Relay][Frontend][ONNX] Add support for op Where (#4184)\n* [Relay][TopHub] Add switch to disable TopHub download (#4015)\n* Add parser support for CAST tflite operator (#4096)\n* Add parses support for `zeros_like` tflite operator (#4042)\n* Add parser support for SUM tflite operator (#4182)\n* Add support for tf.assert (as no-op) and `tf.no_op` to TF Relay frontend. (#4172)\n* [Relay][Frontend][ONNX] New Operators and Opsets to Support BERT (#4197)\n* [Relay][Params] Add APIs for storing and retrieving parameters from individual functions. (#4194)\n* Add `build_create_shared_func` to tvm/contrib/cc.py (#3840)\n* Tensorflow saved model for NNVM ([#2493](#2493/) and Relay ([#2586](#2586/)).\n* Introduced `HybridModule` (#2477) so that normal TVM schedule can be compiled to hybrid target, run and dumped to Hybrid Script.\n* Relay ][Frontend][Tensorflow] add operator `add_n` (#4181)\n* [Relay][Frontend][Tensorflow] StopGradient (#4238)\n* [Relay][Frontend][ONNX] Add support for broadcasting to Where and MatMul (#4267)\n* [TFLite] Support PRelu (#4298)\n* [Frontend][MxNet] support mxnet cond op (#4311)\n* Add support for `quant.mul` operator in tflite frontend (#4283)\n* [Relay][Frontend][ONNX] operator support: DepthToSpace, SpaceToDepth (#4271)\n* [Relay][Frontend][Tensorflow]Add `conv2d_transpose`. (#4300)\n* [Frontend]Add TensorFlow FloorMod (#4308)\n\n### Runtime and Backend Support\n* Make external library extend TVM's NDArray more easily (#2613).\n* Improvements for NNPACK integratation, includes ci test, winograd (#2846, #2868, #2856, #2721)\n* Improvements for OpenCL runtime (#2741, #2737)\n* GraphRuntime: Enable sharing parameters of a model among multiple threads (#3384)\n* Android runtime argsort support (#3472)\n* GraphRuntime enhancements (`set_input_zero_copy` #3416)\n* A new minimal runtime implementation (~12kb .text on ARMv7/x86) for TVM.\n* Add AVX512VNNI support for TVM (#3388)\n* Enable miopen Group Convolution (#3987)\n* Minimal runtime (~12kb .text on ARMv7/x86) for subset of TVM models (#3567)\n* [RUNTIME] Separate runtime related contrib into runtime/contrib (#4207)\n* [topi] add ARM v8.2 udot (uint8) support (#3978)\n* [codegen] Add multiple operands and function support when using fp16 compilation (#4056)\n* [TOPI] Added support for Mali Bifrost target (#4047)\n* [topi] enable fp16 sort for arm (#4084)\n* Add OpenOCD Low-Level Device (RISC-V Support) (#3756)\n* Add wave 32 bc for AMD ROCm backend (#3984)\n* [RUNTIME] Support C++ RPC (#4281)\n* [TOPI][OP] Support Faster-RCNN Proposal OP on CPU (#4297)\n* [TVM][RUNTIME] A minimum example to generate external library wrappers for DSOModule (#4280)\n\n### Language and Architecture\n* Support custom datatypes (#2900)\n* Add the acc16 intrinsic support (#3081)\n* Handle float16 constants & fix BatchNorm (#3260)\n* Structural hash - incorporate the var type into its hash (#3267)\n* Relay C++ Build Module (#3082, #3144, #3174)\n* Enable decorating python class to be a Relay Pass (#3364)\n* Make Partial Eval support interprocedural optimization and termination check. (#3033)\n* Introduce feature manager to Relay. (#3236)\n* Use Relay parser to define the Relay prelude (#3043)\n* Mechanism to detect incomplete expression match in Relay (#3203)\n* EQ/NE operators support for StringImm expressions (#3283)\n* Mechanism to detect incomplete expression match in Relay (#3203)\n* Introduce CanonicalizeCast pass to formally reduce memory overhead introduced by fused cast operations (#3280)\n* Support overloading comparison operations in Relay (#3168)\n* Mac count: provide a pass to calculate the number of multiply-accumulate operations in a network (#2609).\n  - support for `conv_2d_transpose` (#3469)\n  - [Relay][Pass] Count MAC for BatchMatMul (#4157)\n  - Detect depthwise conv2d in `mac_count` pass (#3083)\n* Add Tuple pattern (#3596)\n* Text format support for ADTs and prelude (#3863, #3939)\n* Add new IR pass CombineParallelDense (#3862)\n* Add support for `EQ` op in the deduce bound and the loop partition (#3775)\n* Introduce base-class IRMutatorWithAnalyzer (#3969)\n* Define more standard global functions in the prelude of relay program, includes foldr1, hd, tl, nth, list update (#2928, #2917, #2771, #2866)\n* Add SkipVectorize pass (#3222, #3228)\n* [Relay][Pass] Add pass to remove unused functions in relay module (#4334)\n\n### Symbolic shape enhancement\n* Add shape function for symbolic shape. It enables certain cases for broadcast with symbolic shapes. (#3606)\n* [tvm][any] broadcast with values other than one (#3967)\n* Symbolic shape support (broadcast op #3389)\n* Support reshape for dynamic shape in tf converter (#4185)\n* Runtime Shape Functions (#4179)\n\n### Language and Architecture\n* An optimization pass to eliminate expressions which have the same functionality and same inputs (#2639).\n* Refactor text printer to add stream-like API and FunctionType support (#2605, #2882)\n* Build a scaffold for structured error handling (#2838). The new mechanism detects and rewrites error messages so that c++ and python stack trace are unified and not redundant. Guideslines and conventions for error handling is also discussed.\n* Higher order reverse mode automatic differentiation that work with control flow (#2496)\n* Integer arithmetic analyzers, includes modular set analysis, const integer bound analysis and rewrite simplifier (#2904, #2851, #2768, #2722, #2668, #2860)\n* Improve operator fusion for TupleGetItem in relay (#2914, #2929\n* Compute FLOP of autotvm template for int8 models (#2776)\n* Common subexpression elimination pass in Relay (#2639)\n* Improve quantization in Relay (#2723)\n* Refactor `build_func` in measure module of autotvm to better support cross compiler (#2927)\n* Quantize all fields of concatenate (#2913)\n* Remove stale verilog generator (#2964)\n* Improve Relay printing (#2984, #2881, #3030, #3041)\n* Add `min_num_branches` option in CombineParallelConv2D (#2961)\n* Add `expr_visitor`, fix `expr_functor` exponential blowup problem (#2988)\n* Support Deriving channels when it is not provided in AlterLayout. (#2972)\n* Enhance BoundDeduce algorithm (#2795)\n* Enhance loop partition algorithm (#2956)\n* Better tuple fusion implementation (#3092)\n* Enhance fusion rule that starts from elemwise and broadcast (#2932)\n* Remove `on_device` op after annotation in heterogeneous pass (#3204)\n* Improve canonical and rewrite simplifier (#3132, #3149)\n* Capture constant external python variables in hybrid script (#3157)\n* Remove Peano nats from the prelude (#3045)\n* Macro to define NodeRef methods, constructor style example (#3224)\n* Consistent RAII scoping API (#3231)\n* Register all operators' attributes in Python (#3175)\n* Add module supoort in relay.build (#3424)\n* Relay pass infrastructure improvement (#3319, #3336, #3430, #3353)\n* Migrate Relay passes to pass manager (#3323, #3289, #3251, #3406)\n* Improve heterogeneous annotation by using visitor (#3261)\n* Support export ADT value in Python (#3299)\n* Extend TensorComputeOp to allow scalar inputs (#3300)\n* Transitioning low-level IR away from HalideIR (#3533, #3535)\n* Tags for ADT constructors (#3369)\n* IR dumping for debugging (#3493)\n* Pretty printer and parser roundtrip (#3460, #3536)\n* Relay type checking (conv2d weight dimension #3511, any shape #3221)\n* Relay Module enhancements (remove free variables #3476)\n* LLVM DWARF debug information (#3420)\n* Printer for Layout/BijectiveLayout (#3582)\n* Type inference escape hatch (#3571)\n* Making iterators compatible with constructors of STL containers (#3624)\n* Moving Conv, Dense, Concatenate InferTypes to header (#3783)\n* Simplify casts of constants 0 and 1 (#3758)\n* Conditionally replace reduction init axis. (#3408)\n* Improve Partial Evaluator (#3749, #3703)\n* Strict mode in Relay pattern matching (#3620)\n* Quit and clean when TVM is interrupted (#3640)\n* Make Type Relation catch more errors (#3899, #3699)\n* Refactor the way we interface between different modules of Relay (#3906)\n* Introduce `schedule_injective_from_existing` and unify external schedules for all targets (#3983)\n* [NODE][REFACTOR] Refactor reflection system in node. (#4189)\n* Unify node system and object (#4161, #4115, #4128)\n* [Relay][Refactor] Rename Datatype to ADT (#4156)\n* [Relay] fix exponential blowup in interpreter (#3559)\n* [Relay] Fix memory leak in the interpreter (#4155)\n* [rpc] use callback func to do send & recv (#4147)\n* Add `lift_if_then_else` pass to improve loop partitioning (#3865)\n* Decrease the complexity of CalcDep from exponential to linear (#4053)\n* [IR] Make iterators compatible with constructors of STL containers (#3624)\n* [Relay][Pass] Avoid FoldConstant folding some ops (#4245)\n* [Relay][Prelude] More dtypes support in `tensor_t` (#4233)\n* [NODE][REFACTOR] Rename IRFunctor->NodeFunctor, use func pointer (#4247)\n* [RUNTIME][REFACTOR] Use object protocol to support runtime::Module (#4289)\n* [CodeGen] Add build config option `disable_assert` to control whether to generate assert. (#4340)\n\n### Arithmetic Analysis\n* Formalize Integer Arithmetic Analysis (RFC: #2588). It is aiming to perform better context-dependent analysis, bound analysis, centralized arithmetic logic and arithmetic simplification. (#3272, #3463, #3464, #3368, #3503, #3504 , #3502, #3479 , #3568)\n* Introduce FloorDiv/Mod, TruncDiv/Mod, and IndexDiv/Mod for better arithmetic simplification (#3976, #3986, #4000, #4014, #4008, #4028)\n* [ARITH] Use floordiv for the deduce bound (#4025)\n* [Simplifier] Rewrite simplification rule to eliminate unnecessary conditionals. (#4076)\n\n### Runtime and Backend Support\n* Provide error msg for failure function call in tvm4j (#2967)\n* Expose backtrace symbols in Debug mode (#3001)\n* C++ GraphRuntimeCodegen, Deprecate Python2 (#2986)\n* Ensure interpreted functions can take values that are not TensorValues (#3015)\n* Make OpenCL runtime Compatible with OpenCL2.0 (#2897)\n* Handle INF and NAN in CUDA and OpenCL (#3194)\n* Update debug graph runtime for more precise layerwise timing (#3232)\n* ROCM support (llvm printing #3662, ld.lld finding #3664, save to file #3665)\n* Threadpool: make `spin_count` configurable (#3577)\n* RPC worker children termination (#3669)\n* Vulkan runtime reimplementation (stream approach) (#3849)\n* Vulkan backend supports Call::reinterpret and vectorized comparison (#3795)\n* Support MKL on Windows (#3837)\n* Vulkan IR builder (bool to float #3513)\n* Force `code_object_v2` for amd gpu backend (#4099)\n* [Codegen][cuda-fp16] fallback to fp32 simulation when cuda arch < sm53 (#4268)\n* Fix and refactoring for AMD gpu backend (#4305, #4321, #4341, #4342)\n* [Debugger] Sorting op-time breakdown for quicker analysis. (#4352)\n* [nvcc] enable multiple arch in one fatbin (#4377)\n* [RUNTIME] Move module export to the function level. (#4405)\n\n\n### Frontend and User Interface\n* Relay now supports saving and loading parameter dictionaries. (#2620)\n* Add `max_num_threads` to Hybrid Script, which allows users to get max number of threads for GPU targets ([#2672](#2672/)).\n* Improvements for tensorflow frontend (#2830, #2757, #2586), includes decompiling tf control flow (#2830)\n* Improvements for mxnet frontend (#2844, #2777, #2772, #2706, #2704, #2709,, #2739)\n* Improvements for keras frontend (#2842, #2854)\n* Improvements for DarkNet frontend (#2673)\n* Improvements for ONNX frontend (#2843, #2840)\n* Better profile result dump in Chrome Tracing format (#2922, #2863)\n* Unified error handling in NNVM and Relay frontends (#2828)\n* Improve NNVM to Relay conversion (#2734)\n* Remove `input_0d_mismatch` special handling for TF Frontend(#3087)\n* Bumped ONNX version from 1.1.0 to 1.4.1 (#3286)\n* Simplify parameter handling in Tensorflow frontend (#2993)\n* CoreML improvement for image scaler and padding (#3800)\n* Clean up TensorFlow frontend (#3710)\n* Darknet: Solve tvm parsing darknet resnext failure bug (#3778)\n* Frontend changes `get_workload` - (#3483)\n* [TF][Relay][Op] Pass module when infer shape (#4287)\n\n### AutoTVM\n* Support override in `register_topi_compute` and `register_topi_schedule`. (#3292)\n* Improve graph tuner dealing with Tuple. (#3649)\n* Add AutoTVM template for conv2d Intel int8. (#3955)\n* Add AutoTVM template for dense on CUDA. (#3923)\n* Add AutoTVM template for conv2d on Intel graphics. (#3839)\n* Optimizing autotvm task extraction speed. (#4138)\n* [AutoTVM] Add `batch_matmul` to tunable operations. (#4242)\n* Selecting tuning templates when extracting task. (#4338)\n\n### Performance Improvements\n* Enable AlterOpLayout pass for x86 on Relay (#2585). It is essential to get decent performance for CNN-based model on Intel CPUs.\n* Better intrinsic matching for x86 CPU and ARM CPU, includes variants of vcvtph2ps and vmlal.s16 (#2925, #2748).\n* Improve injective schedule for ARM CPU(#2801)\n* Core functionality for Graph tuner (#2184)\n* Fast tanh implementation (#3255)\n* Improve multi-batch conv2d on x86 (#3308)\n* Improve `non_max_suppression` and `get_valid_counts` for CPU (#3305)\n* Improve `roi_align` performance for CPU (#3296)\n* Improve `nms` and `get_valid_count` performance (#3282)\n* Graph tuner for multiple subgraph (#3490)\n* For sparsity, fast transpose for square CSR matrices has been now merged, which is a good start point for more general sparse type support.\n* Reduce `set_input` and `set_input_zero_copy` overhead (#3805)\n* Parallelize batch axis for ARM (#3931)\n* Support cuBLAS BatchMatMul (#3936)\n* Add AVX512VNNI support for TVM (#3388)\n* Enhance tuning space of split (#3949)\n* Enable miopen transpose convolution and fp16 support (#3952)\n* Improve `conv2d_transpose` schedule on X86 and CUDA (#3948)\n* Expose llvm.nearbyint intrinsic (#4001)\n* [TOPI][X86] Pool operator parallel support. (#4090)\n* Improve layout for several operators (#4103, #4040, #4080)\n* [Relay][VM] Fix constant folding issue in VM compiler (#4077)\n* [relay][vm] Reuse allocated device memory (#4170)\n* [Runtime] Enable option to use OpenMP thread pool (#4089)\n* [PERF] Parallelize reduction for CPU (#4158)\n* [TOPI] Tunable Template for Conv2D HWCN on CUDA (#4168)\n* [TOPI] Add valid auto tvm for Intel Graphics (#4078)\n* [TOPI] FIFO buffer op, to accelerate sequence modeling with dilated convolutions (#4039)\n* TensorCore Support using Intrinsic (#4136)\n* Auto TensorCore CodeGen (#4234)\n* Use cblas for dense and `batch_matmul` (#3787)\n* Update TOPI softmax compute and CPU schedule (#3680)\n* [VTA] Performance optimize, remove unnecessary contigious memory use. (#4246)\n* [TOPI][AlterOpLayout][ARM] Enabling NHWC to NCHW layout transformation. (#4249)\n* [PERF] Parallelize reduction for CPU (#4158)\n* [ThreadPool] Solve thread transitions issue (#4344)\n\n### Documentation\n* Tutorials for deep learning frameworks support in Relay.\n* Tutorial for running AutoTVM with Relay (#2594).\n* Document for Algebraic Data Types (#2575).\n* Move NNVM tutorials to Relay (#2783, #2785, #2766, #2693)\n* Documentation on operators (#2761)\n* Add gradient operator tutorial docs (#2751)\n* Add compiler pass tutorial docs (#2746)\n* Add Android Tutorial (#2977)\n* Developer documentation for InferBound pass (#3126)\n* Add missing targets to `target_name` documentation (#3128)\n* Various documentation improvements (#3133)\n* Add VM doc (#3188)\n* Update documents for TSim (#3409, #3318, #3302, #3343, #3206)\n* Improve tvm4j document describing LLVM support (#3404)\n* Tutorial migration to Python3 (#3498)\n* Android RPC README (#3500)\n* Documentation for Relay opcode (#3522)\n* Tutorial for pass manager (#3515)\n* Minimum version of Python in docs (#3588)\n* Relay pass infra (#3583)\n* X86 Autotune tutorial improvements (#3609)\n* YOLOv3 tiny Darknet tutorial (#3674)\n* SSD doc to avoid confusion (#3677)\n* Tutorial: Build a Graph Convolutional Network on TVM (#3681)\n* Add docs for analysis namespace (#3985)\n* [tutorial] Relay pass infra tutorial (#4083)\n* [DOCS] Add TensorFlow frontend docs (#4154)\n* Tutorial: update Building a Graph Convolutional Network tutorial (#4060)\n* [Docs] Add dependency of compilation with LLVM (#4117)\n* [Documentation]Fix example code in comment of `tvm.build_module.build()` (#4195)\n* TSIM: add virtual memory support to examples (#3868)\n* Relay pass infra tutorial (#4083)\n* Fix the TF tutorial to run against TF2.0 and TF1.x (#4104)\n* Add `topi.nn.fifo_buffer` to TVM doc (#4343)\n* License statement (#4345, #4359, #4401, #4402, #4408, #4409, #4410, #4414, #4431)\n\n### Build and Test\n* Increate the robuteness of CI test (#2841, #2798, #2793, #2788, #2781, #2727, #2710, #2711, #2923)\n* Improve conda build (#2742)\n* Add caffe2 nnvm frontend to CI (#3018)\n* Use bridge network and expose port on macOS when launch docker image (#3086)\n* Run DarkNet tests (#2673)\n* Add file type check (#3116)\n* Always run cpptest during build to ensure library correctness (#3147)\n* Handle more file types in ASF header (#3235)\n* Add `test_forward_ssd_mobilenet_v1` to `tflite/test_forward` (#3350)\n* Add Azure build pipeline (#3458, #3459)\n* Update ci-gpu to v0.52 (#3374)\n* Enable more visible symbols by default (#3365)\n* Separate out legacy as a stage in CI (#3337)\n* Simplify build script, remove python 2 support  (#3419)\n* Ignore rust cargo lock files in rat (#3314)\n* Improve CUDA Conda package build (#3281)\n* Update CMakeLists.txt to be more flexible to find the third parties libraries (#3354)\n* Docker update conda package (#3344), requests and pillow (#3495), Android demo (#3499), rat install (#3527), ARM support (#3546), LLVM (#3590)\n* Relay-to-Python testing (#3156)\n* Code refactoring/remove (#3523, #3667)\n* Zero-rank testing (#3612)\n* CMake compilation (#3611, #3650, google test #3628)\n* Standalone wheel build for TOPI (#3657)\n* Fixing performance issues in PassUpDomain when fusing and splitting axes (#3073)\n* conda recipe (#3791)\n* Allow users to specify download directory (#3803)\n* Update docs for installation for CUDA (#3832)\n* Update `hybrid_script.rst` (#3799)\n* Acknowledge Halide attributions (#3824)\n* Add psutil dependency (#3780)\n* Temporary disable rust test (#3809)\n* Solve occasional CI issue when pad value is all 0 (#3801)\n* Towards TSIM CI testing (#3704)\n* Use pip3 for python3 (#3742)\n* Update docker image `ci_cpu,i386` to include verilator (#3738)\n* Remove sccache from Rust install (#3728)\n* Update dmlc-core to the latest commit (#3716)\n* Update GPU docker (#3709)\n* Add an option to build with -pthread (#3671)\n* Add DGL to `{ci_gpu, demo_cpu, demo_gpu}` docker images (#3692)\n* Use pytest instead of nosetest (#3524)\n* Enable NHWC of `relay.testing.mobilenet` (#3886)\n* Add .hsaco save/load for `tesnor_expr` Tutorial (#3852)\n* Support LLVM trunk (#3907)\n* Remove GTest cmake flag from install docs (#3953)\n* Allow `USE_LLVM` to take extra arguments (#3954)\n* [CI] Pin NNPack pthreadtools version (#4152)\n* [TOPI] Fix flaky testcase for check round (#4211)\n* [CI] Move gpu docker binary to cuda10 (#4229)\n* [CI] use llvm9 for the gpu tests (#4224)\n* [CI] Update GPU docker to cuda10 (#4228)\n* [Relay] Install Relay Prelude program in package install (#4227)\n* [relay] use `time_evaluator` for measurement (#4191)\n* [Relay] Improve build error when no lowered funcs are produced (#4132)\n* [llvm] switch to use Align for llvm trunk (#4051)\n* [CUDA] Update `have_int8` condition to run on compute capability 7.x devices (#4214)\n* [DOCKER] Pin torchvision==0.4.1 (#4140)\n* [DOCKER] torch install depends on future package (#4098)\n* [CodeGen] Disable -mfloat-abi hard option for LLVM < 6.0 (#4071)\n* Add a python how to example of deploying tvm module with tvm runtime only (#4094)\n* Hide symbols from dependent libraries if `HIDE_PRIVATE_SYMBOLS` is ON. (#4041)\n* [BUILD] Disable utvm standalone runtime by default (#4240)\n* Fix TSIM compile error in Linux (add missing -fPIC flag) (#3876)\n* Add scalafmt and format existing scala codebase (#3880)\n* Update TFLite wheel version to 1.13.1 (#3435)\n* Remove PEP498 f-string new feature for support python3.5 (#4250)\n* Require LLVM >= 9 for AMDGPU backend (#4253)\n* Rename ml.dmlc.tvm to org.apache.tvm (#4290)\n* [Test][TF][Relay] Fix argument preparation for vm test mode (#4296)\n* Add test for the `qnn_add` operator (#4282)\n* [CI][DOCKER] Add ONNX runtime dep (#4314)\n* [CI][DOCKER] Upgrade image to include onnx runtime (#4313)\n* [CI] Set workspace to be per executor (#4336)\n* [Build][Windows] Fix Windows build by including cctype (#4319)\n* [Contrib] Add MKL DNN option (#4323)\n* [Test][Relay][Pass] Add test case for lambda lift (#4317)\n* Remove Python imp module as it is deprecated (#4275)\n* Bump up CUDA log version in tophub.py (#4347)\n* Add rule for clean in APPs (#4364)\n* [Relay tests] Temporary Attr Update for Order-Independent Testing (#4357)\n* [CI] Avoid content-length request in test data download (#4375)\n* Compare all outputs in TFLite `test_forward_ssd_mobilenet_v1` (#4373)\n\n### Bug Fixes\n* [RELAY] Fix `get_int_tuple`. (#2691)\n* [ARITH] Select support for integer set analysis. (#2687)\n* [Relay] Fix error in ANF (too aggressively inline atomic expression and create free variable). (#2665)\n* [Hybrid Script] Fix name conflict and attached scope problem. (#2649)\n* [Relay] Fix ANF for reference and pattern matching. (#2637)\n* [Relay] Fix fusion bug when call symbol that is not an operator. (#2630)\n* Fix missing <sstream> header file. (#2629)\n* [Relay]Fix the bug in heterogeneous annotation which mistakenly steps into the fused op. (#2622)\n* [AutoTVM] Fix incorrect localhost usage in RPC mode. (#2619)\n* [NNVM] Fix incorrectly getting layout attribute as a tuple. (#2610)\n* [Relay] Fix mutating IF expression. (#2601)\n* [Tutorial] Fix downloaded file path. (#2590)\n* [Storage] Fix int32 overflow bug when input is big. (#2580)\n* [NNVM] Fix non-identity problem for FInplaceIdentity. (#2572)\n* [Golang] Fix compilation error. (#2558)\n* [Tensor Expression] Fix missing reduction init predicates. (#2495)\n* [Relay] Fix missing argument for NCHWc in Relay. (#2627)\n* [TOPI] Fix `Nms_ir` data race. (#2600)\n* Fix `compute_inline` with multiple outputs (#2934)\n* [TEXPR][PASS] Fix thread all reduce to avoid write after read hazzard (#2937)\n* [FRONTEND][TENSORFLOW] bug fix for tensorflow official slim models. (#2864)\n* [FRONTEND][ONNX] Some bug fixes and Shape operator fixed for relay. (#2850)\n* Turn on `USE_SORT` by default (#2916)\n* [DOCKER] Upgrade ci-cpu to latest v0.50 (#2901)\n* [TESTS] Import script robustness (set -u) (#2896)\n* [Relay] Fix name of bias in testing.mlp (#2892)\n* [TESTS] Improve script robustness (#2893)\n* Add dense schedules to `__init__` for cpu (#2855)\n* [Apps] [howto_deploy] fix cxx-flags order and build directory (#2888)\n* [Relay] Add TVM_DLL for ANF/GNF conversion #2883\n* [Relay] Fix Relay ARM CPU depthwise spatial pack schedule alter op layout issue. (#2861)\n* Fix setting up hints for getaddrinfo (#2872)\n* Add missing sgx includes (#2878)\n* Fix error reporting for missing axis (#2835)\n* Fix an OrderDict initilization bug. (#2862)\n* Fix Xcode 10 metal compile error (#2836)\n* tvmrpc: Fix includes (#2825)\n* Fix `init_proj.py`: Team ID expected (#2824)\n* [DOCKER] Fix git clone failure. (#2816)\n* upgrade java style-check due to CVE-2019-9658 (#2817)\n* [Relay][Quantization] Fix duplicated simulated quantization (#2803)\n* [Bugfix] Repeat and tile bug fixed, relay tests added (#2804)\n* Fix caffe2 relay frontend (#2733)\n* Fix a bug in nnvm to relay converter. (#2756)\n* Ensure loop count is a constant before trying to unroll. (#2797)\n* xcode.py: Decode bytes before output #2833\n* [WIN] Fix a bug in `find_llvm` when specify llvm-config (#2758)\n* [DLPACK] fix flaky ctypes support (#2759)\n* [Bugfix][Relay][Frontend] Fix bug in mxnet converter for `slick_like` (#2744)\n* [DOCS] Fix tutorial (#2724)\n* [TOPI][Relay] Fix default `out_dtype` for `conv2d_NCHWc` and Relay (#2702)\n* [Relay] fix checkwellform (#2705)\n* fix prelu, now can use on 2d input and add one test (#2875)\n* [CODEGEN][OPENCL] Fix compile error about ternary expression. (#2821)\n* Fix Placeholder issue (#2834)\n* Fix makedirs() condition in contrib (#2942)\n* Add missing #!/bin/bash directive (#2951)\n* Bilinear resize bug fix from PR #2777 (#2857)\n* Fix `bias_add` default axis (#2829)\n* Remove empty ty.rs (#2958)\n* fix undefined reference to dlopen, etc (#2957)\n* Removed deprecated `std::unary_function` (#2962)\n* Add output format to ndk build func (#2999)\n* Fix java checkstyle version (#2998)\n* Fix relay invariant error message (#3011)\n* Fix for caffe2 nnvm frontend (#2996)\n* Fix rust resnet example (#3000)\n* Fix x||!x for comparisons in rewrite simplifier (#3029)\n* Fix BatchMatMulRel typerelation (#3032)\n* Update dmlc-core, fix default ctors of NodeEntry (#3017)\n* Fix Fuse (#3035)\n* Fix PostOrderVisit signature (#3048)\n* Fix winograd nnpack fp16 (#3046)\n* Fix some typos (#3063, #3112)\n* Fix `group_conv2d` unit test (#3113)\n* Fix bug in ONNX importer (#3084)\n* Fixing a doc nit (#3123)\n* Fix type code error for StringImm (#3050)\n* Fix bug of wrongly generated `device_map` (#2990)\n* use `unordered_map` instead of map in ANF (#3024)\n* Fix PRelu layout in Relay (#3013)\n* Minor addition to graph runtime debug (#3129)\n* Fix mali conv2d performance regression (#3131)\n* Fix dense autotvm template registration in ROCm (#3136)\n* Fix `conv2d_transpose` (#3138)\n* Fix python lint warnings (#3145)\n* Some fixes for golang latest version compiler #3119 (#3182)\n* Add more syncs to fix flaky test caused by `get_valid_counts` (#3151)\n* Fix AlterLayout Pass (#3155)\n* Fix a multithreaded bug in llvm LazyInitJIT (#3158)\n* Fix a tensorflow test bug. (#3165)\n* Fix concat for ARM (#3061)\n* Handle vectorize for LE statement (#3137)\n* Raise exception `group_conv2d_nchw` not supported (#3195)\n* Quick fix of VTA FPGA Toolchain Installation documentation (#3196)\n* Check file exists before removing it (#3178)\n* Fix a bug of flatten in ONNX to Relay converter (#3180)\n* Fix converter where initializers were not registered as nodes (#3143)\n* Fix bug in cast to bool (#3207)\n* Hotfix `build_module` creation (#3198)\n* Fix sort changing original input data issue (#3212)\n* Fix bug in vta runtime DepPop function (#3208)\n* Fix resize nearest with fractional scaling (#3244)\n* Fix `vta_conv2d` crash issue after change `vta_config.json` (#3213)\n* Fix a memory leak in OpManager (#3263)\n* PkgConfig cause crash in PYNQ board due to link library (#3257)\n* Fix Error messages in tflite.py (#3320)\n* Fix typos in docs and comments (#3309, #3376)\n* Bugfix min/max const canonicalize rule (#3386)\n* Return module from frontend for autotvm (#3401)\n* Fix constant and reshape in ONNX (#3387)\n* Default verilator location fix (#3324)\n* Fix autodiff for conditional expression (#3453)\n* Gramatical improvements to `tensor_expr_get_started` (#3330)\n* Fix AutoTVM data structure bug (#3462)\n* Fix MXNet RNN without providing state initialization as input (#3326)\n* Fix flaky test on topk and quantize pass (#3362)\n* Add VTA PYNQ `metal_test` bitstream program logic and fix compilation issue. (#3400)\n* Fix VTA function Vivado Compile Error. (#3375)\n* Fix VTA DRAM functionality issue. (#3278)\n* Fix reshape precompute and type error in ONNX frontend (#3230)\n* Fix interpreter argument conversion for tuples. (#3349)\n* Fix code generation for packed functions + tuples in VM (#3287)\n* Fix memory leak in Relay interpreter (#3448)\n* Fix x86 depthwise conv2d `alter_op_layout` (#3264)\n* Create closure object for GlobalVar (#3411)\n* Fix getting global var in prelude (#3405)\n* Fix rfactor bugs which related to predicate and loop partition (#3382, #3444)\n* Fix the bug in AutoTVM where SimulatedAnnealingOptimizer sometimes finds useless candidate (#3413)\n* Fix name conflict in PartialEval (#3402)\n* Fix int bound analysis bug for modular (#3288)\n* Check arg positiveness for modular rules (#3279)\n* Fixes failure of `sum` and `all` on `axis=0` (#3422)\n* Fix package path in tflite test (#3427)\n* Fix Windows build (#3429)\n* Fix `LSTMBlockCell` in Tensorflow frontend (#3410)\n* TF fix where output index is ignored (#3622)\n* Runtime fix for custom datatypes (#3471)\n* Relay build module warnings (#3452)\n* Relay partial evaluator (#3482)\n* Pynq AutoTVM tracker (#3497, #3578)\n* A normal form test (#3525)\n* Lint issue (#3519, #3615 )\n* Any shape testing (#3528)\n* Android `posix_memalign` (#3532)\n* Quantization `add_rewrite` and UnifyDTypeScale (#3534)\n* Bound inference fix (#3526)\n* Tensorflow NCHW data format (#3514)\n* First order gradient (#3550)\n* JS load module example (#3556)\n* Build error (#3552)\n* Relay VM debug statements (#3565)\n* C++ lambda expr (#3570)\n* Handling of tempdir if subprocess is killed (#3574)\n* Remove tabs in Chisel source (#3603)\n* Relay VM DataTypeObject (#3604)\n* Removing prints (#3616)\n* Average Pool2D Bug (#3607)\n* Missing header in `cuda_device_api.cc` (#3621)\n* Tensorflow frontend fix where `output_shape` is None (#3632)\n* Winograd accuracy fix (#3644)\n* Fix comment (#3646)\n* Zero-input op fix for recursive traversals (#3623)\n* Python 3.5 compatibility (#3675)\n* Fix infinite recursive `device_api.ext_dev` call in VTA. (#3843)\n* Fix `depth_mult` for TensorFlow frontend (#3676)\n* Fix database APIs for AutoTVM (#3821)\n* Fix axis of softmax in Keras (#3834)\n* Fix VTA TensorLoad module (#3841)\n* Fix inconsistent python/cpp API behavior for `if_then_else`, power (#3829)\n* Fix code comment of operators in ONNX frontend (#3830)\n* Added repo for llvm-9 to fix missing dependency issue (#3826)\n* Fix typo in Relay text parser (#3785)\n* Fix tvm const warnings (#3817)\n* Add gfx906 bc (#3808)\n* Fixed onnx test failures when run on a cpu backend (#3764)\n* Fix ArgBinder assert order (#3794)\n* Fix for NoneType Target for quantization (#3792)\n* Fix out-of-date quantization realize (#3790)\n* Fix Qnn concatenate InferType (#3779)\n* Fix dense tuning (#3768)\n* Fix `visit_pattern` in ExprMutator (#3769)\n* Fix Chisel Scala style (#3765)\n* Fix some pass docs (#3767)\n* Fix mistype in rpc tutorial (#3763)\n* Fix tvm.scan follow by tvm.compute segfault (#3723)\n* Fix the potential index overflow in where operator (#3751)\n* Revert `compile_cmd` kwarg name change (#3746)\n* Update tophub (#3752)\n* Fix typo in `ir_pass.h` (#3741)\n* Bug fix for VME Shell (#3737)\n* Fix missing apt https transport support (#3735)\n* Take zero extent loops as NoOp and remove it (#3724)\n* Fix mxnet converter for hybridblock and add `div_sqrt_dim` (#3701)\n* Fix partial eval unit test name (#3719)\n* Fix conv2d schedule code (#3648, #3717)\n* Remove thread related headers (#3713)\n* Fix FunctionPass (#3712)\n* Export tvm::relay::OpRegistry::OpRegistry (#3711)\n* Fix Metal reinterpret (#3706)\n* Fix `gather_nd` in Relay (#3442)\n* Fix error in partial evaluator (#3693)\n* Align the naming rule for OpAttributeUnImplemented (#3695)\n* Enable the sparse schedule (#3651)\n* Fix typo names in Caffe2 frontend (#3685)\n* Make tests multi-process friendly. (#3683)\n* Fix typo in README.md (#3684)\n* Fix doc rendering  (#3897)\n* Add test script starter command to document (#3993)\n* Add type solver unit tests for unifying quantified funcs (#3947)\n* Change Vivado install instructions to version 2018.3 (#4003)\n* Add a link to the defining network description of auto-tuning tutorial (#4023)\n* Additional MXNet Convolution and Deconvolution tests (#4026)\n* Adding support to check if an attribute is present or not without having to get the value (#3957)\n* Fix parser for cast. (#3873)\n* Fix operator fusion for multiple output (#3871)\n* Remove extern C warpper for cuBLAS (#3877)\n* Fix int32 range overflow by using int64 (#3870)\n* Remove duplicate resize (#3902)\n* Fix blas cmake for mac os (#3898)\n* Add another MKL name alias for MKL installed through pypi (#3853)\n* Numpy compatible dtype inference for `tvm.convert` and `tvm.const` (#3861)\n* Remove incorrect check for LLVM in C codegen test (#3921)\n* Fix exponential blowup in interpreter (#3559)\n* Fix CUDA int8x4 vectorize (#3928)\n* Make buffer auto broadcast independent to the order of input args (#3956)\n* Fix benchmark layout in graph tuner (#3926)\n* Fix Android Demo LLVM version (#3962)\n* Cast filepath arguments to string (#3968)\n* Fixes \"common\" sub crate using nightly and main (#3965)\n* Changes to make tensorize work. These changes also fix the previously broken test. (#3981)\n* Remove FLOP computation when calling 3rd party library (#4005)\n* Use a more intuitive way to limit the #ops in a group (#4018)\n* Add more `pad_mode` support for onnx converter (#4029)\n* Impose a max op limit to the op fusion pass (#4002)\n* Fixes issue with CPP enums (#4019)\n* Int64 shape handling for outputs. (#4031)\n* [PYTHON] Fix installation for generated grammar (#4223)\n* [Bugfix] Fix target host for vm compiler (#4057)\n* [Fix][VM] Fix VM invoke with `set_params` (#4079)\n* [Fix] Fix a few bugs when dtype is fp16 (#4088)\n* [Relay][Frontend][TF] Fix Size operator (#4175)\n* [cmake][ANTLR] Support setting path to ANTLR jar (#4176)\n* Fix infer type of kernel in dense. (#4125)\n* [Relay] Fix match case in Python-side expr functor (#4037)\n* Split `adaptive_pool2d_avg` into sum and div (#4186)\n* [AutoTVM] Fix Split Factors when `no_tail` is off (#4044)\n* Fix extent one for the `post_stmt` in loop partition (#3734)\n* [TOPI] Fix bug in intel graphics auto tune (#4093)\n* [ARITH] Fix lowering of `floormod(x, y) != 0` (#4127)\n* [ARITH] Fix the rule `y < x && x <= y` (#4220)\n* [Bugfix][TF] reset graph after getting tag of savedmodel (#4055)\n* [Fix] Fix the logic of the number of nodes checking in op fusion (#4074)\n* [VTA] hotfix for de10-nano driver (#4081)\n* Fixing tensor not found issue in bitserial operator (#4095)\n* Fix wrong `n_trial` number in autotvm tutorials' progress bar if `n_trial` is larger then config space. (#4070)\n* [PATCH] Fix undefined `__floatdihf` in libtvmruntime.so on aarch64. (#4119)\n* [ARITH] Fix lowering of FloorMod (#4236)\n* [Relay][Frontend][Tensorflow] Fix GatherV2 (#4238)\n* Fix typing.Deque import error for Python 3.5 (#4254)\n* [VTA] Hotfix for padded load test in Chisel VTA (#4264)\n* [Contrib] Fix error message at `callback_get_section_size()` (#4221)\n* [TOPI] Fix bug in Winograd on CUDA (#4260)\n* AutoTVM: Fix hang/crash issues on feature extraction (#3689)\n* [TOPI][CUDA] Fix Winograd Kernel Size Support (#4276)\n* [Relay][Frontend][Tensorflow] Fix type assignment for 'tf.range' operator (#4294)\n* Fix incorrect call to Unicode Win32 InetPton (#4306)\n* [Relay][Frontend][Keras] handle `batch_norm` op params well (#4310)\n* [VTA] fix error when `memory_id` is `VTA_MEM_ID_OUT` (#4330)\n* [Doc][fix] fix sphinx parsing for pass infra tutorial (#4337)\n* [Codegen] remove fp16 function override for cuda (#4331)\n* [TFLite] Fix Prelu unified shape error (#4326)\n* [Relay][Frontend][TF] Fix transpose when axes is not a param (#4327)\n* [VTA] Bug fix for padded load with large inputs (#4293)\n* Fix inconsistent operator tag name (#4134)\n* Fix for a specific case when loop partitioning with indivisble. (#4243)\n* Send list as argument to `schedule_conv2d` (#4358)\n* [Docker] Fix TVM folder name for installing on Android and OpenCL. (#4363)\n* Fix TFLite Reshape assert (#4320)\n* [Relay][Frontend][TF] Fix slice when begin or size is not Const (#4372)\n* Fix compilaton of bfloat16 on Windows (#4415)\n\n### Known Issues\n\n* The performance of Relay VM is not good enough on GPU, due to memeory allocation overhead which will be resolved later.\n* TFlite rounding vs tvm rounding causing differences in accuracy and potentially off by 1 errors. For reference #3900\n* TFlite pre-quantized network support is still a work in progress and the project would welcome further contributions.\n* TSIM build requires `python` command exist on the host. See [forum discussion](https://discuss.tvm.ai/t/vta-build-failure/4790) for details.\n* Tensorflow control flow has not been fully supported in the frontend converter.\n* `topi.floor_div` is inconsistent with floor division semantic when result number is close to an integer.\n\n\n### Depreciations\n* Deprecating python2 support and following release (v0.6). (#2994, #2986)\n* NNVM is deprecated and will be removed in a future version. (#4333, #4368)\n\n\n## 0.5\nThis release features several major improvements. Some of the highlights are: Arbitrary bits quantization algorithm; High-level auto-differentiable programming IR -- Relay.\n\n- Fully featured 8-bit network support\n  - 8bit quantizer\n  - Arbitrary bits quantization algorithm\n  - Intel cpu support\n  - ARM cpu support\n- NVidia GPU 8-bit kernel\n  - int8 gemm recipe\n  - int8 conv2d\n  - Autotvm integration\n- Automated tuning and scheduling\n  - AutoTVM optimizations for mobile GPUs\n  - AutoTVM optimizations for CUDA\n  - AutoTVM optimizations for x86\n- Initial release of the differentiable programming IR, Relay\n  - Generic & informative Relay error reporting #2408\n  - Relay IR text format support #1781\n  - Support control flows\n  - A Normal Form Canonicalization #2251\n  - Type system support\n  - End to end compilation\n     * Frontend support: Caffe2 #2507 , CoreML #2476 , Keras #2376 , MXNet #2163 , ONNX, TFLite #2365\n     * Operator coverage #1799 #2051\n  - FoldScaleAxis #2020\n  - SimplifyInference #2033\n  - CombineParallelConv2D #2089\n  - InstrumentBoundCheckers pass #2079\n  - Bind & FoldConstant #2100\n  - Alter Op Layout #2150\n  - General OpFusion #2090\n- CodeGen\n  - Gcc / g++ compatible C code generator for TVM #2161\n  - Device type annotation for heterogeneous compilation #2361\n  - Cache packed func ptr, lift alloca #2070\n  - Generalize compute to tensor region #1476\n- Runtime\n  - Relay interpreter and compiler #1954\n  - Heterogeneous runtime #1695\n  - Language bindings: Golang runtime #1470 , Rust runtime #1597\n  - Add min_repeat_ms to time_evaluator #2200\n  - Bundled interpreter demonstration #2297\n  - Enable PlanMemory in the graph runtime #2120\n- Language Binding\n  - Rust frontend #2292\n- VTA\n  - Improved RPC for VTA #2043\n- Hybrid python programming model\n  - Support for scheduling #2416\n  - Support for Inter-function call  #2287\n  - Backend support  #2477\n- TOPI\n  - Initial support for sparse tensor computation\n  - Improve ARM CPU depthwise convolution performance #2345\n  - Port winograd ops to relay #2356\n  - Add faster-rcnn proposal op #2420\n- Tutorials and docs\n  - Relay language docs #2232\n  - Tutorials on how to use SGX backend\n  - How to write a pass in python\n  - General lowering flow of TVM\n  - How to do tensorize\n  - TFLite frontend tutorial #2508\n  - Keras seq2seq model for translation tutorial #1815\n  - Committer guide and tips #2468\n  - Code review guideline on API designs #2459\n\n\n\n## 0.4\n\nThis release features several major improvements. The high-level graph optimizer is now part of TVM repo. Some of the highlights are: Initial support of AutoTVM for automated optimization; customized accelerator backend VTA.\n\n- Tensor operator primitives\n  - Introduce attrs field to operator primitives(e.g. compute) to store additional metadata, the attrs can be used as hint for scheduling\n- Enable embedding of asm micro-kernels\n- Hybrid python programming model\n   - python AST based IR builder interface\n   - support GPU programs\n- AutoTVM, Automated tuning, and scheduling\n   - basic autotvm infra\n    - GPU IR verifier\n   - basic autotuning tutorial\n   - topi integration\n- ARM support\n    - winograd support\n   - initial support of ARM autotuning records\n- TOPI Vision\n   - Generic GPU sort support(useful for vision)\n   - SSD operator support\n- TOPI numpy consistency\n   - Rename all binary operators for numpy consistecy: broadcast_add-> add, broadcast_sub -> substract, broadcast_mul -> multiply, broadcast_div->divide\n   - New operators: slice, LRN, equal, not_equal, less, greater\n   - tutorials on topi\n- Initial low-bit operator support support\n    - Optimized popcount generation on ARM\n    - general bit-serial convolution and GEMM\n    - optimized low bit kernels\n    - parallel optimization\n- New topi backend optimization for intel graphics\n- Adapt AVX schedules for SSE target\n- VTA: customized accelerator backend\n  - custom hardware backend example\n  - tutorials on how to use customized accelerator\n- Initial experimental support for  HLS backend\n- Bugfix in SPIRV code generator for vulkan\n- libdevice support, enable NVPTX backend\n- Introduce NDArrayContainer for managed NDarray\n- RPC and Device API\n   - Support communication between big/small endian machines.\n   - RPC and device API protocol upgrade (this is a non-backward compatible change) to support big-small endian communication. This is a non-backward compatible change, need to use the latest version of TVM runtime with the RPC\n   - graduate rpc from contrib, tvm.contrib.rpc->tvm.rpc\n   -Support tracker in Android RPC, add fault tolerance for AutoTVM\n- BIG.LITTLE aware threadpool\n- tvm4j graph runtime that runs end to end workload in java\n- DLPack support\n   - Support from_dlpack and to_dlpack\n   - Enables bridges to pytorch\n- Enable link of stackvm in runtime\n- Tensorflow graphdef frontend\n- Keras frontend\n   - improved to support reuse layers, add activations\n- ONNX\n   - gather,  LRN\n- CoreML frontend\n   - Support C-RNN and activation functions\n- Fix grads for sum and expand_like\n- Enhanced operator fusion for multiple elemwise branches\n- Separate nnvm fusion and compilation pass\n- Unified build system to cmake, customizable cmake path for vulkan, rocm, cuda\n\n\n## 0.3\n\nThis release features numerous improvements in TOPI and backends. We make the first step toward object detection support in TOPI, featuring operators necessary for YOLO and SSDs. The topi now supports numpy-style API and operator overloading. RPC is significantly improved to support resource allocation and using a pool of devices. We are adding two new backends: WebGL for running GPUs on the browser, and Vulkan for running on next-generation graphics API.\n\n- TOPI Vision operators\n   - SSD support\n   - YOLO support\n   - NMS operator support in vision\n- TOPI general numpy-style operators\n   - numpy style operator overload in topi\n   - more operators: flip, take\n   - dilation support on conv2d and depthwise\n- 8bit support\n    - ARM 8bit gemm\n    - ARM 8bit conv\n- Low bit operator support\n    - popcount intrinsics\n    - 1-bit fully connected\n- Contrib: MPSDNN fully-connected and conv2d support\n- Better RPC support\n   - RPC Tracker support to allow centralized resource management\n   - RPC protocol upgrade (this is a non-backward compatible change) to support timeout in the proxy\n     - This is a breaking change, need to use the latest version of TVM runtime with the RPC\n   - Fault-tolerant to early server termination with correct exception propagated\n   - RPC support enabled for ROCm AMDGPUs\n- Tutorials and docs\n  - How to deploy to android devices.\n- Optimizations for hardware backends\n  - intel CPU (AVX and AVX512)\n- Schedule Primitives\n   - rfactor now support factor_axis to specify the factored dimension in the result\n   - cache_write now support multiple output operators\n   - enable warp memory which generates shuffle instructions\n- Framework bridge\n  - MXNet bridge supported\n- C++ compiler API support\n   - build migration\n   - topi migration to c++\n   - Target system in c++\n- WebGL backend\n   - runtime and codegen\n   - topi integration\n   - end to end pipeline on the browser\n- Vulkan backend\n   - vulkan runtime\n   - spirv code generator\n- Security\n    - intel SGX runtime support\n    - multi-threaded SGX runtime\n- LLVM 7.0 support\n- Robustness\n   - VerifyMemory to verify incorrect GPU schedules that writes into GPU memory from cpu\n   - Verify compute formulas\n- Better CPU parallel runtime\n\n## 0.2\n\nThis release comes with a complete set of TOPI support for NNVM compiler, which allows compilation of end to end workloads.\nWe also make major improvements in supporting new backends: ROCm for AMDGPUs and ARM GPU.\n\n- Backend support\n   - Support LLVM mainline(4.0, 5.0, 6.0)\n   - Support ROCM stack for AMD GPUs\n   - More robust OpenCL support for ARM GPUs\n- Android RPC runtime\n- Multi-threading optimization for ARM\n   - multi-threaded depthwise\n   - multi-threaded conv2d\n- New schedule primitives\n   - storage_align for shared memory alignment\n   - double_buffer\n- UnrollLoop : more robust version of unroll loop, count maximum steps that can be unrolled.\n- Full set of TOPI operators\n   - Introduce tvm.target to specify target options for compilation better.\n   - broadcast/ reduction operators\n   - pooling and global pooling\n   - Generic target support for topi\n   - schedule with external libraries\n- End to end deep learning pipelines for CPU, GPU, ARM GPU\n- Tutorials\n  - How to load compiled module in any language runtime\n  -  How to use java runtime\n- Contrib library: MIOpen, CuDNN\n- Ongoing items that contains functioning pieces\n  - WebGL backend\n  - C++ compiler support\n  - MPS DNN\n  - low bit support, introduced popcount\n\n\n## 0.1\n\n- Language runtime\n    - python\n    - javascript\n    - java\n    - c++\n- Backend\n    - arm, x86\n    - javascript, wasm\n    - CUDA\n    - opencl\n    - Metal\n- DNN Library integration\n- RPC  runtime\n- TOPI operator pipeline python\n- TOPI operator pipeline in C++\n- Rough perf of the TOPI GPU pipeline\n- Rough pref of TOPI CPU pipeline\n- End to end graph executors\n\n\n## Initial version\n\n- Pack libary into shared library.\n- External function and contrib libraries\n- DLPack integration support\n- AOT and module system\n- Basic code structure ready.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.16015625,
          "content": "Apache TVM\nCopyright 2019-2023 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.8564453125,
          "content": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n<img src=https://raw.githubusercontent.com/apache/tvm-site/main/images/logo/tvm-logo-small.png width=128/> Open Deep Learning Compiler Stack\n==============================================\n[Documentation](https://tvm.apache.org/docs) |\n[Contributors](CONTRIBUTORS.md) |\n[Community](https://tvm.apache.org/community) |\n[Release Notes](NEWS.md)\n\n[![Build Status](https://ci.tlcpack.ai/buildStatus/icon?job=tvm/main)](https://ci.tlcpack.ai/job/tvm/job/main/)\n[![WinMacBuild](https://github.com/apache/tvm/workflows/WinMacBuild/badge.svg)](https://github.com/apache/tvm/actions?query=workflow%3AWinMacBuild)\n\nApache TVM is a compiler stack for deep learning systems. It is designed to close the gap between the\nproductivity-focused deep learning frameworks, and the performance- and efficiency-focused hardware backends.\nTVM works with deep learning frameworks to provide end to end compilation to different backends.\n\nLicense\n-------\nTVM is licensed under the [Apache-2.0](LICENSE) license.\n\nGetting Started\n---------------\nCheck out the [TVM Documentation](https://tvm.apache.org/docs/) site for installation instructions, tutorials, examples, and more.\nThe [Getting Started with TVM](https://tvm.apache.org/docs/tutorial/introduction.html) tutorial is a great\nplace to start.\n\nContribute to TVM\n-----------------\nTVM adopts apache committer model, we aim to create an open source project that is maintained and owned by the community.\nCheck out the [Contributor Guide](https://tvm.apache.org/docs/contribute/).\n\nAcknowledgement\n---------------\nWe learned a lot from the following projects when building TVM.\n- [Halide](https://github.com/halide/Halide): Part of TVM's TIR and arithmetic simplification module\n  originates from Halide. We also learned and adapted some part of lowering pipeline from Halide.\n- [Loopy](https://github.com/inducer/loopy): use of integer set analysis and its loop transformation primitives.\n- [Theano](https://github.com/Theano/Theano): the design inspiration of symbolic scan operator for recurrence.\n"
        },
        {
          "name": "apps",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "conda",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 4.58984375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport hashlib\nimport pytest\nimport sys\nimport os\n\nfrom pathlib import Path\n\npytest_plugins = [\"tvm.testing.plugin\"]\nIS_IN_CI = os.getenv(\"CI\", \"\") == \"true\"\nREPO_ROOT = Path(__file__).resolve().parent\n\n\n# These are long running tests (manually curated and extracted from CI logs)\n# that should be allocated to test shards in a round-robin fashion. These are\n# taken from the 20 (arbitrary number) of tests as from\n# https://ci.tlcpack.ai/job/tvm/job/main/2907/testReport\n_slowest_tests = [\n    \"tests/python/frontend/tensorflow/test_forward.py::test_forward_broadcast_args\",\n    \"tests/python/frontend/tensorflow/test_forward.py::test_forward_broadcast_to\",\n    \"tests/python/topi/python/test_topi_conv2d_int8.py::test_conv2d_nchw[int8]\",\n    \"tests/python/topi/python/test_topi_conv2d_int8.py::test_conv2d_nchw[uint8]\",\n    \"tests/python/topi/python/test_topi_upsampling.py::test_upsampling3d\",\n    \"tests/python/topi/python/test_topi_upsampling.py::test_upsampling3d\",\n    \"tests/python/topi/python/test_topi_conv2d_int8.py::test_conv2d_nchw[int8]\",\n    \"tests/python/frontend/tflite/test_forward.py::test_all_elemwise\",\n    \"tests/python/frontend/pytorch/test_object_detection.py::test_detection_models\",\n    \"tests/python/topi/python/test_topi_conv2d_int8.py::test_conv2d_nchw[uint8]\",\n    \"tests/python/topi/python/test_topi_conv2d_NCHWc.py::test_conv2d_NCHWc\",\n    \"tests/python/topi/python/test_topi_conv2d_hwnc_tensorcore.py::test_conv2d_hwnc_tensorcore\",\n    \"tests/python/contrib/test_tensorrt.py::test_binary[compile]\",\n    \"tests/python/frontend/pytorch/test_forward.py::test_segmentation_models\",\n    \"tests/python/topi/python/test_topi_conv2d_NCHWc.py::test_conv2d_NCHWc\",\n    \"tests/python/relay/test_py_converter.py::test_global_recursion\",\n    \"tests/python/frontend/tensorflow/test_forward.py::test_forward_ptb\",\n    \"tests/python/relay/test_op_level6.py::test_topk\",\n    \"tests/python/topi/python/test_topi_conv2d_winograd.py::test_conv2d_nchw\",\n    \"tests/python/relay/test_py_converter.py::test_global_recursion\",\n]\nHARDCODED_ALLOCATIONS = {}\nfor idx, test in enumerate(_slowest_tests):\n    HARDCODED_ALLOCATIONS[test] = idx\n\n# These rely on running on the same node to pass successfully\nFIXED_ALLOCATION_PREFIXES = {\n    \"tests/python/testing/test_tvm_testing_features.py\": 0,\n}\n\n\ndef find_shard_index(nodeid: str, num_shards: int) -> int:\n    \"\"\"\n    Return the index of the shard that should run this test\n    \"\"\"\n    for prefix, target_shard_idx in FIXED_ALLOCATION_PREFIXES.items():\n        if nodeid.startswith(prefix):\n            if target_shard_idx >= num_shards:\n                raise RuntimeError(\n                    f\"Cannot collect sharded tests, {nodeid} has hardcoded shard index {target_shard_idx} among only {num_shards} shards\"\n                )\n            return target_shard_idx\n\n    if nodeid in HARDCODED_ALLOCATIONS:\n        hash = HARDCODED_ALLOCATIONS[nodeid]\n    else:\n        hash = hashlib.md5(nodeid.encode())\n        hash = int(hash.hexdigest(), 16)\n\n    return hash % num_shards\n\n\ndef pytest_collection_modifyitems(config, items):\n    if not all(k in os.environ for k in [\"CI\", \"TVM_NUM_SHARDS\", \"TVM_SHARD_INDEX\"]):\n        # Only apportion tests if in CI and in a job that is set up for it\n        return\n\n    num_shards = int(os.environ[\"TVM_NUM_SHARDS\"])\n    shard_index = int(os.environ[\"TVM_SHARD_INDEX\"])\n\n    print(f\"Marking tests for shard {shard_index} of {num_shards}\")\n    items_copy = list(items)\n    for item in items_copy:\n        item_shard_index = find_shard_index(item.nodeid, num_shards=num_shards)\n        if item_shard_index != shard_index:\n            items.remove(item)\n\n\ndef pytest_sessionstart():\n    if IS_IN_CI:\n        hook_script_dir = REPO_ROOT / \"tests\" / \"scripts\" / \"request_hook\"\n        sys.path.append(str(hook_script_dir))\n        import request_hook  # pylint: disable=import-outside-toplevel\n\n        request_hook.init()\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "gallery",
          "type": "tree",
          "content": null
        },
        {
          "name": "golang",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "jvm",
          "type": "tree",
          "content": null
        },
        {
          "name": "licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "mypy.ini",
          "type": "blob",
          "size": 1.08203125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n[mypy]\nignore_missing_imports = False\nshow_column_numbers = True\nshow_error_context = True\nfollow_imports = skip\nignore_errors = False\nstrict_optional = False\n\n[mypy-python.tvm.auto_scheduler.*]\nignore_errors = True\n\n[mypy-python.tvm.runtime.*]\nignore_errors = True\n\n[mypy-python.tvm.tir.schedule.*]\nignore_errors = False\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.26953125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n[tool.isort]\nprofile = \"black\"\nsrc_paths = [\"python\", \"tests/python\"]\n\n[tool.black]\nline-length = 100\ntarget-version = ['py36']\ninclude = '(\\.pyi?$)'\nexclude = '''\n\n(\n  /(\n      \\.github\n    | \\.tvm\n    | \\.tvm_test_data\n    | \\.vscode\n    | \\.venv\n    | 3rdparty\n    | build\\/\n    | cmake\\/\n    | conda\\/\n    | docker\\/\n    | docs\\/\n    | golang\\/\n    | include\\/\n    | jvm\\/\n    | licenses\\/\n    | nnvm\\/\n    | rust\\/\n    | src\\/\n    | vta\\/\n    | web\\/\n  )/\n)\n'''\n\n[tool.ruff]\nline-length = 100\nindent-width = 4\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "rust",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.py",
          "type": "blob",
          "size": 7.3154296875,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\nThis is the global script that set the version information of TVM.\nThis script runs and update all the locations that related to versions\n\nList of affected files:\n- tvm-root/python/tvm/_ffi/libinfo.py\n- tvm-root/include/tvm/runtime/c_runtime_api.h\n- tvm-root/conda/recipe/meta.yaml\n- tvm-root/web/package.json\n\"\"\"\nimport os\nimport re\nimport argparse\nimport logging\nimport subprocess\n\n# Modify the following value during release\n# ---------------------------------------------------\n# Current version:\n# We use the version of the incoming release for code\n# that is under development.\n#\n# It is also fallback version to be used when --git-describe\n# is not invoked, or when the repository does not present the\n# git tags in a format that this script can use.\n#\n# Two tag formats are supported:\n# - vMAJ.MIN.PATCH (e.g. v0.8.0) or\n# - vMAJ.MIN.devN (e.g. v0.8.dev0)\n__version__ = \"0.19.dev0\"\n\n# ---------------------------------------------------\n\nPROJ_ROOT = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))\n\n\ndef py_str(cstr):\n    return cstr.decode(\"utf-8\")\n\n\ndef git_describe_version():\n    \"\"\"Get PEP-440 compatible public and local version using git describe.\n\n    Returns\n    -------\n    pub_ver: str\n        Public version.\n\n    local_ver: str\n        Local version (with additional label appended to pub_ver).\n\n    Notes\n    -----\n    - We follow PEP 440's convention of public version\n      and local versions.\n    - Only tags conforming to vMAJOR.MINOR.REV (e.g. \"v0.7.0\")\n      are considered in order to generate the version string.\n      See the use of `--match` in the `git` command below.\n\n    Here are some examples:\n\n    - pub_ver = '0.7.0', local_ver = '0.7.0':\n      We are at the 0.7.0 release.\n    - pub_ver =  '0.8.dev94', local_ver = '0.8.dev94+g0d07a329e':\n      We are at the 0.8 development cycle.\n      The current source contains 94 additional commits\n      after the most recent tag(v0.7.0),\n      the git short hash tag of the current commit is 0d07a329e.\n    \"\"\"\n    cmd = [\n        \"git\",\n        \"describe\",\n        \"--tags\",\n        \"--match\",\n        \"v[0-9]*.[0-9]*.[0-9]*\",\n        \"--match\",\n        \"v[0-9]*.[0-9]*.dev[0-9]*\",\n    ]\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=PROJ_ROOT)\n    (out, _) = proc.communicate()\n\n    if proc.returncode != 0:\n        msg = py_str(out)\n        if msg.find(\"not a git repository\") != -1:\n            return __version__, __version__\n        logging.warning(\"git describe: %s, use %s\", msg, __version__)\n        return __version__, __version__\n    describe = py_str(out).strip()\n    arr_info = describe.split(\"-\")\n\n    # Remove the v prefix, mainly to be robust\n    # to the case where v is not presented as well.\n    if arr_info[0].startswith(\"v\"):\n        arr_info[0] = arr_info[0][1:]\n\n    # hit the exact tag\n    if len(arr_info) == 1:\n        return arr_info[0], arr_info[0]\n\n    if len(arr_info) != 3:\n        logging.warning(\"Invalid output from git describe %s\", describe)\n        return __version__, __version__\n\n    dev_pos = arr_info[0].find(\".dev\")\n\n    # Development versions:\n    # The code will reach this point in case it can't match a full release version, such as v0.7.0.\n    #\n    # 1. in case the last known label looks like vMAJ.MIN.devN e.g. v0.8.dev0, we use\n    # the current behaviour of just using vMAJ.MIN.devNNNN+gGIT_REV\n    if dev_pos != -1:\n        dev_version = arr_info[0][: arr_info[0].find(\".dev\")]\n    # 2. in case the last known label looks like vMAJ.MIN.PATCH e.g. v0.8.0\n    # then we just carry on with a similar version to what git describe provides, which is\n    # vMAJ.MIN.PATCH.devNNNN+gGIT_REV\n    else:\n        dev_version = arr_info[0]\n\n    pub_ver = \"%s.dev%s\" % (dev_version, arr_info[1])\n    local_ver = \"%s+%s\" % (pub_ver, arr_info[2])\n    return pub_ver, local_ver\n\n\n# Implementations\ndef update(file_name, pattern, repl, dry_run=False):\n    update = []\n    hit_counter = 0\n    need_update = False\n    with open(file_name) as file:\n        for l in file:\n            result = re.findall(pattern, l)\n            if result:\n                assert len(result) == 1\n                hit_counter += 1\n                if result[0] != repl:\n                    l = re.sub(pattern, repl, l)\n                    need_update = True\n                    print(\"%s: %s -> %s\" % (file_name, result[0], repl))\n                else:\n                    print(\"%s: version is already %s\" % (file_name, repl))\n\n            update.append(l)\n    if hit_counter != 1:\n        raise RuntimeError(\"Cannot find version in %s\" % file_name)\n\n    if need_update and not dry_run:\n        with open(file_name, \"w\") as output_file:\n            for l in update:\n                output_file.write(l)\n\n\ndef sync_version(pub_ver, local_ver, dry_run):\n    \"\"\"Synchronize version.\"\"\"\n    # python uses the PEP-440: local version\n    update(\n        os.path.join(PROJ_ROOT, \"python\", \"tvm\", \"_ffi\", \"libinfo.py\"),\n        r\"(?<=__version__ = \\\")[.0-9a-z\\+]+\",\n        local_ver,\n        dry_run,\n    )\n    # Use public version for other parts for now\n    # Note that full git hash is already available in libtvm\n    # C++ header\n    update(\n        os.path.join(PROJ_ROOT, \"include\", \"tvm\", \"runtime\", \"c_runtime_api.h\"),\n        r'(?<=TVM_VERSION \")[.0-9a-z\\+]+',\n        pub_ver,\n        dry_run,\n    )\n    # conda\n    update(\n        os.path.join(PROJ_ROOT, \"conda\", \"recipe\", \"meta.yaml\"),\n        r\"(?<=version = ')[.0-9a-z\\+]+\",\n        pub_ver,\n        dry_run,\n    )\n    # web\n    # change to pre-release convention by npm\n    dev_pos = pub_ver.find(\".dev\")\n    npm_ver = pub_ver if dev_pos == -1 else \"%s.0-%s\" % (pub_ver[:dev_pos], pub_ver[dev_pos + 1 :])\n    update(\n        os.path.join(PROJ_ROOT, \"web\", \"package.json\"),\n        r'(?<=\"version\": \")[.0-9a-z\\-\\+]+',\n        npm_ver,\n        dry_run,\n    )\n\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n    parser = argparse.ArgumentParser(description=\"Detect and synchronize version.\")\n    parser.add_argument(\n        \"--print-version\",\n        action=\"store_true\",\n        help=\"Print version to the command line. No changes is applied to files.\",\n    )\n    parser.add_argument(\n        \"--git-describe\",\n        action=\"store_true\",\n        help=\"Use git describe to generate development version.\",\n    )\n    parser.add_argument(\"--dry-run\", action=\"store_true\")\n\n    opt = parser.parse_args()\n    pub_ver, local_ver = __version__, __version__\n    if opt.git_describe:\n        pub_ver, local_ver = git_describe_version()\n    if opt.print_version:\n        print(local_ver)\n    else:\n        sync_version(pub_ver, local_ver, opt.dry_run)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}