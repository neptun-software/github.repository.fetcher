{
  "metadata": {
    "timestamp": 1736561256123,
    "page": 249,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lukas-blecher/LaTeX-OCR",
      "stars": 13243,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9423828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\n# lib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\nnotebooks/**\n!notebooks/LaTeX_OCR*.ipynb\n.ipynb_checkpoints/\n**/dataset/data/**\nwandb/\npix2tex/model/checkpoints/**\n!pix2tex/model/checkpoints/*.py\n!**/.gitkeep\n.vscode\n.DS_Store\ntest/*\n\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.6591796875,
          "content": "# .readthedocs.yaml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the version of Python and other tools you might need\nbuild:\n  os: ubuntu-20.04\n  tools:\n    python: \"3.9\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n   configuration: docs/conf.py\n\n# If using Sphinx, optionally build your docs in additional formats such as PDF\n# formats:\n#    - pdf\n\n# Optionally declare the Python requirements required to build your docs\npython:\n   install:\n    - requirements: docs/requirements.txt\n    - method: pip\n      path: .\n      extra_requirements:\n        - all\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.044921875,
          "content": "MIT License\n\nCopyright (c) 2021 Lukas Blecher\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.017578125,
          "content": "exclude **\\*.pth \n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.6865234375,
          "content": "# pix2tex - LaTeX OCR\n\n[![GitHub](https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR)](https://github.com/lukas-blecher/LaTeX-OCR) [![Documentation Status](https://readthedocs.org/projects/pix2tex/badge/?version=latest)](https://pix2tex.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![PyPI - Downloads](https://img.shields.io/pypi/dm/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![GitHub all releases](https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&logo=github)](https://github.com/lukas-blecher/LaTeX-OCR/releases) [![Docker Pulls](https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb) [![Hugging Face Spaces](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/lukbl/LaTeX-OCR)\n\nThe goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code. \n\n![header](https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png)\n\n## Using the model\nTo run the model you need Python 3.7+\n\nIf you don't have PyTorch installed. Follow their instructions [here](https://pytorch.org/get-started/locally/).\n\nInstall the package `pix2tex`: \n\n```\npip install \"pix2tex[gui]\"\n```\n\nModel checkpoints will be downloaded automatically.\n\nThere are three ways to get a prediction from an image. \n1. You can use the command line tool by calling `pix2tex`. Here you can parse already existing images from the disk and images in your clipboard.\n\n2. Thanks to [@katie-lim](https://github.com/katie-lim), you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with `latexocr`. From here you can take a screenshot and the predicted latex code is rendered using [MathJax](https://www.mathjax.org/) and copied to your clipboard.\n\n    Under linux, it is possible to use the GUI with `gnome-screenshot` (which comes with multiple monitor support). For other Wayland compositers, `grim` and `slurp` will be used for wlroots-based Wayland compositers and `spectacle` for KDE Plasma. Note that `gnome-screenshot` is not compatible with wlroots or Qt based compositers. Since `gnome-screenshot` will be preferred when available, you may have to set the environment variable `SCREENSHOT_TOOL` to `grim` or `spectacle` in these cases (other available values are `gnome-screenshot` and `pil`).\n\n    ![demo](https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif)\n\n    If the model is unsure about the what's in the image it might output a different prediction every time you click \"Retry\". With the `temperature` parameter you can control this behavior (low temperature will produce the same result).\n\n3. You can use an API. This has additional dependencies. Install via `pip install -U \"pix2tex[api]\"` and run\n    ```bash\n    python -m pix2tex.api.run\n    ```\n    to start a [Streamlit](https://streamlit.io/) demo that connects to the API at port 8502. There is also a docker image  available for the API: https://hub.docker.com/r/lukasblecher/pix2tex [![Docker Image Size (latest by date)](https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex)\n\n    ```\n    docker pull lukasblecher/pix2tex:api\n    docker run --rm -p 8502:8502 lukasblecher/pix2tex:api\n    ```\n    To also run the streamlit demo run\n    ```\n    docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py\n    ```\n    and navigate to http://localhost:8501/\n\n4. Use from within Python\n    ```python\n    from PIL import Image\n    from pix2tex.cli import LatexOCR\n    \n    img = Image.open('path/to/image.png')\n    model = LatexOCR()\n    print(model(img))\n    ```\n\nThe model works best with images of smaller resolution. That's why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it's not perfect and might not be able to handle huge images optimally, so don't zoom in all the way before taking a picture. \n\nAlways double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.\n\n**Want to use the package?**\n\nI'm trying to compile a documentation right now. \n\nVisit here: https://pix2tex.readthedocs.io/ \n\n\n## Training the model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb)\n\nInstall a couple of dependencies `pip install \"pix2tex[train]\"`.\n1. First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run \n\n```\npython -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl\n```\nTo use your own tokenizer pass it via `--tokenizer` (See below).\n\nYou can find my generated training data on the [Google Drive](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO) as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.\n\n2. Edit the `data` (and `valdata`) entry in the config file to the newly generated `.pkl` file. Change other hyperparameters if you want to. See `pix2tex/model/settings/config.yaml` for a template.\n3. Now for the actual training run \n```\npython -m pix2tex.train --config path_to_config_file\n```\n\nIf you want to use your own data you might be interested in creating your own tokenizer with\n```\npython -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json\n```\nDon't forget to update the path to the tokenizer in the config file and set `num_tokens` to your vocabulary size.\n\n## Model\nThe model consist of a ViT [[1](#References)] encoder with a ResNet backbone and a Transformer [[2](#References)] decoder.\n\n### Performance\n| BLEU score | normed edit distance | token accuracy |\n| ---------- | -------------------- | -------------- |\n| 0.88       | 0.10                 | 0.60           |\n\n## Data\nWe need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. [wikipedia](https://www.wikipedia.org), [arXiv](https://www.arxiv.org). We also use the formulae from the [im2latex-100k](https://zenodo.org/record/56198#.V2px0jXT6eA) [[3](#References)] dataset.\nAll of it can be found [here](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO)\n\n### Dataset Requirements\nIn order to render the math in many different fonts we use  XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools: \n* [XeLaTeX](https://www.ctan.org/pkg/xetex)\n* [ImageMagick](https://imagemagick.org/) with [Ghostscript](https://www.ghostscript.com/index.html). (for converting pdf to png)\n* [Node.js](https://nodejs.org/) to run [KaTeX](https://github.com/KaTeX/KaTeX) (for normalizing Latex code)\n* Python 3.7+ & dependencies (specified in `setup.py`)\n\n### Fonts\nLatin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math\n\n\n## TODO\n- [x] add more evaluation metrics\n- [x] create a GUI\n- [ ] add beam search\n- [ ] support handwritten formulae (kinda done, see training colab notebook)\n- [ ] reduce model size (distillation)\n- [ ] find optimal hyperparameters\n- [ ] tweak model structure\n- [ ] fix data scraping and scrape more data\n- [ ] trace the model ([#2](https://github.com/lukas-blecher/LaTeX-OCR/issues/2))\n\n\n## Contribution\nContributions of any kind are welcome.\n\n## Acknowledgment\nCode taken and modified from [lucidrains](https://github.com/lucidrains), [rwightman](https://github.com/rwightman/pytorch-image-models), [im2markup](https://github.com/harvardnlp/im2markup), [arxiv_leaks](https://github.com/soskek/arxiv_leaks), [pkra: Mathjax](https://github.com/pkra/MathJax-single-file), [harupy: snipping tool](https://github.com/harupy/snipping-tool)\n\n## References\n[1] [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)\n\n[2] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\n[3] [Image-to-Markup Generation with Coarse-to-Fine Attention](https://arxiv.org/abs/1609.04938v2)\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "pix2tex",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0390625,
          "content": "[metadata]\ndescription_file = README.md\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.439453125,
          "content": "#!/usr/bin/env python\n\nimport setuptools\n\n# read the contents of your README file\nfrom pathlib import Path\nthis_directory = Path(__file__).parent\nlong_description = (this_directory / 'README.md').read_text(encoding='utf-8')\n\ngui = [\n    'PyQt6',\n    'PyQt6-WebEngine',\n    'pyside6',\n    'pynput',\n    'screeninfo',\n    'latex2sympy2',\n]\napi = [\n    'streamlit>=1.8.1',\n    'fastapi>=0.75.2',\n    'uvicorn[standard]',\n    'python-multipart',\n    'st_img_pastebutton>=0.0.3',\n]\ntrain = [\n    'python-Levenshtein>=0.12.2',\n    'torchtext>=0.6.0',\n    'imagesize>=1.2.0',\n]\nhighlight = ['pygments']\n\nsetuptools.setup(\n    name='pix2tex',\n    version='0.1.3',\n    description='pix2tex: Using a ViT to convert images of equations into LaTeX code.',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    author='Lukas Blecher',\n    author_email='luk.blecher@gmail.com',\n    url='https://github.com/lukas-blecher/LaTeX-OCR/',\n    license='MIT',\n    keywords=[\n        'artificial intelligence',\n        'deep learning',\n        'image to text'\n    ],\n    packages=setuptools.find_packages(),\n    package_data={\n        'pix2tex': [\n            'resources/*',\n            'model/settings/*.yaml',\n            'model/dataset/*.json',\n        ]\n    },\n    install_requires=[\n        'tqdm>=4.47.0',\n        'munch>=2.5.0',\n        'torch>=1.7.1',\n        'opencv_python_headless>=4.1.1.26',\n        'requests>=2.22.0',\n        'einops>=0.3.0',\n        'x_transformers==0.15.0',\n        'transformers>=4.18.0',\n        'tokenizers>=0.13.0',\n        'numpy>=1.19.5',\n        'Pillow>=9.1.0',\n        'PyYAML>=5.4.1',\n        'pandas>=1.0.0',\n        'timm==0.5.4',\n        'albumentations>=0.5.2',\n        'pyreadline3>=3.4.1; platform_system==\"Windows\"',\n    ],\n    extras_require={\n        'all': gui+api+train+highlight,\n        'gui': gui,\n        'api': api,\n        'train': train,\n        'highlight': highlight,\n    },\n    entry_points={\n        'console_scripts': [\n            'pix2tex_gui = pix2tex.__main__:main',\n            'pix2tex_cli = pix2tex.__main__:main',\n            'latexocr = pix2tex.__main__:main',\n            'pix2tex = pix2tex.__main__:main',\n        ],\n    },\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Science/Research',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python',\n    ],\n)\n"
        }
      ]
    }
  ]
}