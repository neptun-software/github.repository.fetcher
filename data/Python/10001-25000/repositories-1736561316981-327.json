{
  "metadata": {
    "timestamp": 1736561316981,
    "page": 327,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "h2oai/h2ogpt",
      "stars": 11585,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.1533203125,
          "content": ".git\n.npm\n.dockerignore\n.pytest_cache\n.cache\n.local\n.github\n.nv\n.benchmarks\n.bash_history\n.gitignore\nh2ogpt.egg-info\nvenv\nbuild\ndist\nprebuilt_deps\nDockerfile"
        },
        {
          "name": ".env",
          "type": "blob",
          "size": 0.775390625,
          "content": "\n# H2OGPT\n\nH2OGPT_PORT=7860\nH2OGPT_BASE_MODEL=h2oai/h2ogpt-4096-llama2-7b-chat\nH2OGPT_ARGS=\"/workspace/generate.py --base_model=${H2OGPT_BASE_MODEL} --use_safetensors=True --prompt_type=llama2 --save_dir=/workspace/save/ --use_gpu_id=False --score_model=None --max_max_new_tokens=2048 --max_new_tokens=1024\"\n\n# VLLM\n\nVLLM_TOKENIZER=hf-internal-testing/llama-tokenizer\nH2OGPT_VLLM_ARGS=\"--model=${H2OGPT_BASE_MODEL} --tokenizer=${VLLM_TOKENIZER} --tensor-parallel-size=2 --seed=1234 --trust-remote-code --download-dir=/workspace/.cache/huggingface/hub\"\n\n# CPU models\n\nMODEL_PATH_LLAMA=https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf\nH2OGPT_CPU_ARGS=\"/workspace/generate.py --base_model=llama --model_path_llama=${MODEL_PATH_LLAMA} --max_seq_len=4096\"\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.46875,
          "content": "out/\n7B/\n13B/\n__pycache__/\ncheckpoint**\nminimal-llama**\nupload.py\nlora-**\n*ckpt\nwandb\nevaluate.py\ntest_data.json\ntodo.txt\n.neptune/\n*.bin\ndb_dir_UserData\ntemp_path_do_doc1\noffline_folder\nflagged_data_points\n.pytest_cache\nuser_path\nuser_path_test\nbuild\nh2ogpt.egg-info\ndist\n.idea\n.cache\n.local\n.bash_history\n.benchmarks\nDockerfile-runner.dockerfile\nbuild_info.txt\nprebuilt_deps\nDockerfile_deps\n\n# IDEs\n.idea/\n\n# virtual envs\nvenv\n\n# Mac one click installer\nTesseract-OCR/\npoppler/\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.0302734375,
          "content": "# devel needed for bitsandbytes requirement of libcudart.so, otherwise runtime sufficient\nFROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04\n\nENV DEBIAN_FRONTEND=noninteractive\n\nENV PATH=\"/h2ogpt_conda/envs/h2ogpt/bin:${PATH}\"\nARG PATH=\"/h2ogpt_conda/envs/h2ogpt/bin:${PATH}\"\n\nENV HOME=/workspace\nENV CUDA_HOME=/usr/local/cuda-12.1\nENV VLLM_CACHE=/workspace/.vllm_cache\nENV TIKTOKEN_CACHE_DIR=/workspace/tiktoken_cache\nENV HF_HUB_ENABLE_HF_TRANSFER=1\n\nWORKDIR /workspace\n\nCOPY . /workspace/\n\nCOPY build_info.txt /workspace/\n\nRUN cd /workspace && ./docker_build_script_ubuntu.sh\n\nRUN chmod -R a+rwx /workspace\n\nARG user=h2ogpt\nARG group=h2ogpt\nARG uid=1000\nARG gid=1000\n\nRUN groupadd -g ${gid} ${group} && useradd -u ${uid} -g ${group} -s /bin/bash ${user}\n# already exists in base image\n# RUN groupadd -g ${gid} docker && useradd -u ${uid} -g ${group} -m ${user}\n\n# Add the user to the docker group\nRUN usermod -aG docker ${user}\n\n# Switch to the new user\nUSER ${user}\n\nEXPOSE 8888\nEXPOSE 7860\nEXPOSE 5000\nEXPOSE 5002\nEXPOSE 5004\n\nENTRYPOINT [\"python3.10\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.1298828125,
          "content": "                                Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 3.95703125,
          "content": "all: clean dist\n\nPACKAGE_VERSION              := `cat version.txt | tr -d '\\n'`\nBUILD_TAG                    := $(shell git describe --always --dirty)\nDOCKER_H2OGPT_RUNTIME_IMAGE  := gcr.io/vorvan/h2oai/h2oai-h2ogpt-runtime:$(BUILD_TAG)\nDOCKER_H2OGPT_VLLM_IMAGE     := gcr.io/vorvan/h2oai/h2oai-h2ogpt-vllm:$(BUILD_TAG)\nPYTHON_BINARY                ?= `which python`\nDEFAULT_MARKERS              ?= \"not need_tokens and not need_gpu\"\n\n# h2ogpt base and vllm images built elsewhere and referenced here:\nDOCKER_BASE_OS_IMAGE     := gcr.io/vorvan/h2oai/h2ogpt-oss-wolfi-base:9\nDOCKER_VLLM_IMAGE        := gcr.io/vorvan/h2oai/h2ogpte-vllm:0.6.3.post1-38ed4ff2\n\n\n.PHONY: venv dist test publish docker_build docker_push build_info.txt\n\nclean:\n\trm -rf dist build h2ogpt.egg-info\n\nvenv:\n\t$(PYTHON_BINARY) -m virtualenv -p $(PYTHON_BINARY) venv\n\ninstall:\n\t$(PYTHON_BINARY) -m pip install dist/h2ogpt-$(PACKAGE_VERSION)-py3-none-any.whl\n\ninstall-%:\n\t$(PYTHON_BINARY) -m pip install dist/h2ogpt-$(PACKAGE_VERSION)-py3-none-any.whl[$*]\n\ndist:\n\t$(PYTHON_BINARY) setup.py bdist_wheel\n\ntest:\n\t$(PYTHON_BINARY) -m pip install requirements-parser\n\t$(PYTHON_BINARY) -m pytest tests --disable-warnings --junit-xml=test_report.xml -m \"$(DEFAULT_MARKERS)\"\n\ntest_imports:\n\t$(PYTHON_BINARY) -m pytest tests/test_imports.py --disable-warnings --junit-xml=test_report.xml -m \"$(DEFAULT_MARKERS)\"\n\npublish:\n\techo \"Publishing not implemented yet.\"\n\nbuild_info.txt:\n\t@rm -rf build_info.txt\n\t@echo \"commit=\\\"$(shell git rev-parse HEAD)\\\"\" >> $@\n\t@echo \"branch=\\\"`git rev-parse HEAD | git branch -a --contains | grep -v detached | sed -e 's~remotes/origin/~~g' -e 's~^ *~~' | sort | uniq | tr '*\\n' ' '`\\\"\" >> $@\n\t@echo \"describe=\\\"`git describe --always --dirty`\\\"\" >> $@\n\t@echo \"build_os=\\\"`uname -a`\\\"\" >> $@\n\t@echo \"build_machine=\\\"`hostname`\\\"\" >> $@\n\t@echo \"build_date=\\\"$(shell date \"+%Y%m%d\")\\\"\" >> $@\n\t@echo \"build_user=\\\"`id -u -n`\\\"\" >> $@\n\t@echo \"base_version=\\\"$(PACKAGE_VERSION)\\\"\" >> $@\n\n\ndocker_build: build_info.txt\nifeq ($(shell curl --connect-timeout 4 --write-out %{http_code} -sS --output /dev/null -X GET https://gcr.io/v2/vorvan/h2oai/h2oai-h2ogpt-runtime/manifests/$(BUILD_TAG)),200)\n\t@echo \"Image already pushed to GCR: $(DOCKER_H2OGPT_RUNTIME_IMAGE)\"\n\tdocker pull $(DOCKER_H2OGPT_RUNTIME_IMAGE)\nelse\n\tdocker pull $(DOCKER_BASE_OS_IMAGE)\n\tDOCKER_BUILDKIT=1 docker build -t $(DOCKER_H2OGPT_RUNTIME_IMAGE) -t h2ogpt:current -f Dockerfile .\nendif\nifeq ($(shell curl --connect-timeout 4 --write-out %{http_code} -sS --output /dev/null -X GET https://gcr.io/v2/vorvan/h2oai/h2oai-h2ogpt-vllm/manifests/$(BUILD_TAG)),200)\n\t@echo \"VLLM Image already pushed to GCR: $(DOCKER_H2OGPT_VLLM_IMAGE)\"\n\tdocker pull $(DOCKER_H2OGPT_VLLM_IMAGE)\nelse\n\tdocker pull $(DOCKER_VLLM_IMAGE)\n\tdocker tag $(DOCKER_VLLM_IMAGE) $(DOCKER_H2OGPT_VLLM_IMAGE)\nendif\n\ndocker_push:\n\tdocker tag $(DOCKER_H2OGPT_RUNTIME_IMAGE) gcr.io/vorvan/h2oai/h2oai-h2ogpt-runtime:$(PACKAGE_VERSION)\n\tdocker tag $(DOCKER_H2OGPT_VLLM_IMAGE) gcr.io/vorvan/h2oai/h2oai-h2ogpt-vllm:$(PACKAGE_VERSION)\n\n\tdocker tag $(DOCKER_H2OGPT_RUNTIME_IMAGE) gcr.io/vorvan/h2oai/h2oai-h2ogpt-runtime:latest\n\tdocker tag $(DOCKER_H2OGPT_VLLM_IMAGE) gcr.io/vorvan/h2oai/h2oai-h2ogpt-vllm:latest\n\n\tdocker push gcr.io/vorvan/h2oai/h2oai-h2ogpt-runtime:$(BUILD_TAG)\n\tdocker push gcr.io/vorvan/h2oai/h2oai-h2ogpt-runtime:$(PACKAGE_VERSION)\n\tdocker push gcr.io/vorvan/h2oai/h2oai-h2ogpt-runtime:latest\n\n\tdocker push gcr.io/vorvan/h2oai/h2oai-h2ogpt-vllm:$(BUILD_TAG)\n\tdocker push gcr.io/vorvan/h2oai/h2oai-h2ogpt-vllm:$(PACKAGE_VERSION)\n\tdocker push gcr.io/vorvan/h2oai/h2oai-h2ogpt-vllm:latest\n\nifdef BUILD_ID\n\tdocker tag $(DOCKER_H2OGPT_RUNTIME_IMAGE) gcr.io/vorvan/h2oai/h2oai-h2ogpt-runtime:$(PACKAGE_VERSION)-$(BUILD_ID)\n\tdocker push gcr.io/vorvan/h2oai/h2oai-h2ogpt-runtime:$(PACKAGE_VERSION)-$(BUILD_ID)\n\n\tdocker tag $(DOCKER_H2OGPT_VLLM_IMAGE) gcr.io/vorvan/h2oai/h2oai-h2ogpt-vllm:$(PACKAGE_VERSION)-$(BUILD_ID)\n\tdocker push gcr.io/vorvan/h2oai/h2oai-h2ogpt-vllm:$(PACKAGE_VERSION)-$(BUILD_ID)\nendif\n\nprint-%:\n\t@echo $($*)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.0302734375,
          "content": "# h2oGPT\n\nTurn ★ into ⭐ (top-right corner) if you like the project!\n\nQuery and summarize your documents or just chat with local private GPT LLMs using h2oGPT, an Apache V2 open-source project.\n\nCheck out a long CoT Open-o1 open 🍓strawberry🍓 project: https://github.com/pseudotensor/open-strawberry\n\n## Live Demo\n\n[![img-small.png](docs/img-small.png) Gradio Demo](https://gpt.h2o.ai/)\n\n[![img-small.png](docs/img-small.png) OpenWebUI Demo](https://gpt-docs.h2o.ai/)\n\n## Video Demo\n\nhttps://github.com/h2oai/h2ogpt/assets/2249614/2f805035-2c85-42fb-807f-fd0bca79abc6\n\n[![img-small.png](docs/img-small.png) YouTube 4K Video](https://www.youtube.com/watch?v=_iktbj4obAI)\n\n## Features\n\n- **Private** offline database of any documents [(PDFs, Excel, Word, Images, Video Frames, YouTube, Audio, Code, Text, MarkDown, etc.)](docs/README_LangChain.md#supported-datatypes)\n  - **Persistent** database (Chroma, Weaviate, or in-memory FAISS) using accurate embeddings (instructor-large, all-MiniLM-L6-v2, etc.)\n  - **Efficient** use of context using instruct-tuned LLMs (no need for LangChain's few-shot approach)\n  - **Parallel** summarization and extraction, reaching an output of 80 tokens per second with the 13B LLaMa2 model\n  - **HYDE** (Hypothetical Document Embeddings) for enhanced retrieval based upon LLM responses\n  - **Semantic Chunking** for better document splitting (requires GPU)\n- **Variety** of models supported (LLaMa2, Mistral, Falcon, Vicuna, WizardLM.  With AutoGPTQ, 4-bit/8-bit, LORA, etc.)\n  - **GPU** support from HF and LLaMa.cpp GGML models, and **CPU** support using HF, LLaMa.cpp, and GPT4ALL models\n  - **Attention Sinks** for [arbitrarily long](https://github.com/tomaarsen/attention_sinks) generation (LLaMa-2, Mistral, MPT, Pythia, Falcon, etc.)\n- **Gradio UI** or CLI with streaming of all models\n  - **Upload** and **View** documents through the UI (control multiple collaborative or personal collections)\n  - **Vision Models** LLaVa, Claude-3, Gemini-Pro-Vision, GPT-4-Vision\n  - **Image Generation** Stable Diffusion (sdxl-turbo, sdxl, SD3), PlaygroundAI (playv2), and Flux\n  - **Voice STT** using Whisper with streaming audio conversion\n  - **Voice TTS** using MIT-Licensed Microsoft Speech T5 with multiple voices and Streaming audio conversion\n  - **Voice TTS** using MPL2-Licensed TTS including Voice Cloning and Streaming audio conversion\n  - **AI Assistant Voice Control Mode** for hands-free control of h2oGPT chat\n  - **Bake-off** UI mode against many models at the same time\n  - **Easy Download** of model artifacts and control over models like LLaMa.cpp through the UI\n  - **Authentication** in the UI by user/password via Native or Google OAuth\n  - **State Preservation** in the UI by user/password\n- **Open Web UI** with h2oGPT as backend via OpenAI Proxy\n  - See [Start-up Docs](docs/FAQ.md#open-web-ui).\n  - Chat completion with streaming\n  - Document Q/A using h2oGPT ingestion with advanced OCR from DocTR\n  - Vision models\n  - Audio Transcription (STT)\n  - Audio Generation (TTS)\n  - Image generation\n  - Authentication\n  - State preservation\n- **Linux, Docker, macOS, and Windows** support\n- **Inference Servers** [support](docs/README_InferenceServers.md) for oLLaMa, HF TGI server, vLLM, Gradio, ExLLaMa, Replicate, Together.ai, OpenAI, Azure OpenAI, Anthropic, MistralAI, Google, and Groq\n- **OpenAI compliant**\n  - Server Proxy [API](docs/README_CLIENT.md) (h2oGPT acts as drop-in-replacement to OpenAI server)\n  - Chat and Text Completions (streaming and non-streaming)\n  - Audio Transcription (STT)\n  - Audio Generation (TTS)\n  - Image Generation\n  - Embedding\n  - Function tool calling w/auto tool selection\n  - AutoGen Code Execution Agent\n- **JSON Mode**\n  - Strict schema control for vLLM via its use of outlines\n  - Strict schema control for OpenAI, Anthropic, Google Gemini, MistralAI models\n  - JSON mode for some older OpenAI or Gemini models with schema control if model is smart enough (e.g. gemini 1.5 flash)\n  - Any model via code block extraction\n- **Web-Search** integration with Chat and Document Q/A\n- **Agents** for Search, Document Q/A, Python Code, CSV frames\n  - High quality Agents via OpenAI proxy server on separate port\n  - Code-first agent that generates plots, researches, evaluates images via vision model, etc. (client code openai_server/openai_client.py).\n  - No UI for this, just API\n- **Evaluate** performance using reward models\n- **Quality** maintained with over 1000 unit and integration tests taking over 24 GPU-hours\n\n## Get Started\n\n[![GitHub license](https://img.shields.io/github/license/NVIDIA/nvidia-docker?style=flat-square)](LICENSE)\n[![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)](https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md)\n[![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=macos&logoColor=F0F0F0)](https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md)\n[![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)](https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md)\n[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md)\n\n### Install h2oGPT\n\nDocker is recommended for Linux, Windows, and MAC for full capabilities.  Linux Script also has full capability, while Windows and MAC scripts have less capabilities than using Docker.\n\n* [Docker Build and Run Docs (Linux, Windows, MAC)](docs/README_DOCKER.md)\n* [Linux Install and Run Docs](docs/README_LINUX.md)\n* [Windows 10/11 Installation Script](docs/README_WINDOWS.md)\n* [MAC Install and Run Docs](docs/README_MACOS.md)\n* [Quick Start on any Platform](docs/README_quickstart.md)\n\n---\n\n### Collab Demos\n- [![](https://colab.research.google.com/assets/colab-badge.svg) h2oGPT CPU](https://colab.research.google.com/drive/13RiBdAFZ6xqDwDKfW6BG_-tXfXiqPNQe?usp=sharing)\n- [![](https://colab.research.google.com/assets/colab-badge.svg) h2oGPT GPU](https://colab.research.google.com/drive/143-KFHs2iCqXTQLI2pFCDiR69z0dR8iE?usp=sharing)\n\n### Resources\n- [FAQs](docs/FAQ.md)\n- [README for LangChain](docs/README_LangChain.md)\n- [Discord](https://discord.gg/WKhYMWcVbq)\n- [Models (LLaMa-2, Falcon 40, etc.) at 🤗](https://huggingface.co/h2oai/)\n- [YouTube: 100% Offline ChatGPT Alternative?](https://www.youtube.com/watch?v=Coj72EzmX20)\n- [YouTube: Ultimate Open-Source LLM Showdown (6 Models Tested) - Surprising Results!](https://www.youtube.com/watch?v=FTm5C_vV_EY)\n- [YouTube: Blazing Fast Falcon 40b 🚀 Uncensored, Open-Source, Fully Hosted, Chat With Your Docs](https://www.youtube.com/watch?v=H8Dx-iUY49s)\n- [Technical Paper: https://arxiv.org/pdf/2306.08161.pdf](https://arxiv.org/pdf/2306.08161.pdf)\n\n### Docs Guide\n<!--  cat README.md | ./gh-md-toc  -  But Help is heavily processed -->\n* [Get Started](#get-started)\n   * [Linux (CPU or CUDA)](docs/README_LINUX.md)\n   * [macOS (CPU or M1/M2)](docs/README_MACOS.md)\n   * [Windows 10/11 (CPU or CUDA)](docs/README_WINDOWS.md)\n   * [GPU (CUDA, AutoGPTQ, exllama) Running Details](docs/README_GPU.md)\n   * [CPU Running Details](docs/README_CPU.md)\n   * [CLI chat](docs/README_CLI.md)\n   * [Gradio UI](docs/README_ui.md)\n   * [Client API (Gradio, OpenAI-Compliant)](docs/README_CLIENT.md)\n   * [Inference Servers (oLLaMa, HF TGI server, vLLM, Groq, Anthropic, Google, Mistral, Gradio, ExLLaMa, Replicate, OpenAI, Azure OpenAI)](docs/README_InferenceServers.md)\n   * [Build Python Wheel](docs/README_WHEEL.md)\n   * [Offline Installation](docs/README_offline.md)\n   * [Low Memory](docs/FAQ.md#low-memory-mode)\n   * [Docker](docs/README_DOCKER.md)\n* [LangChain Document Support](docs/README_LangChain.md)\n* [Compare to PrivateGPT et al.](docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like)\n* [Roadmap](#roadmap)\n* [Development](#development)\n* [Help](#help)\n   * [LangChain file types supported](docs/README_LangChain.md#supported-datatypes)\n   * [CLI Database control](docs/README_LangChain.md#database-creation)\n   * [FAQ](docs/FAQ.md)\n     * [Model Usage Notes](docs/FAQ.md#model-usage-notes)\n     * [Adding LLM Models (including using GGUF and Attention Sinks)](docs/FAQ.md#adding-models)\n     * [Adding Embedding Models](docs/FAQ.md#add-new-embedding-model)\n     * [Adding Prompts](docs/FAQ.md#adding-prompt-templates)\n     * [In-Context Learning](docs/FAQ.md#in-context-learning-via-prompt-engineering)\n     * [Multiple GPUs](docs/FAQ.md#multiple-gpus)\n     * [Low-Memory Usage](docs/FAQ.md#low-memory-mode)\n     * [Environment Variables](docs/FAQ.md#what-envs-can-i-pass-to-control-h2ogpt)\n     * [HTTPS access for server and client](docs/FAQ.md#https-access-for-server-and-client)\n   * [Useful Links](docs/LINKS.md)\n   * [Fine-Tuning](docs/FINETUNE.md)\n   * [Triton](docs/TRITON.md)\n   * [Commercial viability](docs/FAQ.md#commercial-viability)\n* [Acknowledgements](#acknowledgements)\n* [Why H2O.ai?](#why-h2oai)\n* [Disclaimer](#disclaimer)\n\n### Development\n\n- To create a development environment for training and generation, follow the [installation instructions](docs/INSTALL.md).\n- To fine-tune any LLM models on your data, follow the [fine-tuning instructions](docs/FINETUNE.md).\n- To run h2oGPT tests:\n    ```bash\n    pip install requirements-parser pytest-instafail pytest-random-order playsound==1.3.0\n    conda install -c conda-forge gst-python -y\n    sudo apt-get install gstreamer-1.0\n    pip install pygame\n    GPT_H2O_AI=0 CONCURRENCY_COUNT=1 pytest --instafail -s -v tests\n    # for openai server test on already-running local server\n    pytest -s -v -n 4 openai_server/test_openai_server.py::test_openai_client\n    ```\n  or tweak/run `tests/test4gpus.sh` to run tests in parallel.\n\n### Acknowledgements\n\n* Some training code was based upon March 24 version of [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/).\n* Used high-quality created data by [OpenAssistant](https://open-assistant.io/).\n* Used base models by [EleutherAI](https://www.eleuther.ai/).\n* Used OIG data created by [LAION](https://laion.ai/blog/oig-dataset/).\n\n### Why H2O.ai?\n\nOur [Makers](https://h2o.ai/company/team/) at [H2O.ai](https://h2o.ai) have built several world-class Machine Learning, Deep Learning and AI platforms:\n- #1 open-source machine learning platform for the enterprise [H2O-3](https://github.com/h2oai/h2o-3)\n- The world's best AutoML (Automatic Machine Learning) with [H2O Driverless AI](https://h2o.ai/platform/ai-cloud/make/h2o-driverless-ai/)\n- No-Code Deep Learning with [H2O Hydrogen Torch](https://h2o.ai/platform/ai-cloud/make/hydrogen-torch/)\n- Document Processing with Deep Learning in [Document AI](https://h2o.ai/platform/ai-cloud/make/document-ai/)\n\nWe also built platforms for deployment and monitoring, and for data wrangling and governance:\n- [H2O MLOps](https://h2o.ai/platform/ai-cloud/operate/h2o-mlops/) to deploy and monitor models at scale\n- [H2O Feature Store](https://h2o.ai/platform/ai-cloud/make/feature-store/) in collaboration with AT&T\n- Open-source Low-Code AI App Development Frameworks [Wave](https://wave.h2o.ai/) and [Nitro](https://nitro.h2o.ai/)\n- Open-source Python [datatable](https://github.com/h2oai/datatable/) (the engine for H2O Driverless AI feature engineering)\n\nMany of our customers are creating models and deploying them enterprise-wide and at scale in the [H2O AI Cloud](https://h2o.ai/platform/ai-cloud/):\n- Multi-Cloud or on Premises\n- [Managed Cloud (SaaS)](https://h2o.ai/platform/ai-cloud/managed)\n- [Hybrid Cloud](https://h2o.ai/platform/ai-cloud/hybrid)\n- [AI Appstore](https://docs.h2o.ai/h2o-ai-cloud/)\n\nWe are proud to have over 25 (of the world's 280) [Kaggle Grandmasters](https://h2o.ai/company/team/kaggle-grandmasters/) call H2O home, including three Kaggle Grandmasters who have made it to world #1.\n\n### Disclaimer\n\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\n\n- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\n- Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\n- Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\n- Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\n- Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\n- Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\n\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=h2oai/h2ogpt&type=Timeline)](https://star-history.com/#h2oai/h2ogpt&Timeline)\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "blog",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "cloud",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev_installers",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose-cpu.yml",
          "type": "blob",
          "size": 0.2880859375,
          "content": "version: '3'\n\nservices:\n  h2ogpt:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    restart: always\n    shm_size: '2gb'\n    ports:\n      - '7860:7860'\n    volumes:\n      - cache:/workspace/.cache\n      - save:/workspace/save\n    command: ${H2OGPT_CPU_ARGS}\n\nvolumes:\n  cache:\n  save:\n"
        },
        {
          "name": "docker-compose-vllm.yml",
          "type": "blob",
          "size": 1.3447265625,
          "content": "version: '3'\n\nservices:\n  h2ogpt:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    restart: always\n    shm_size: '2gb'\n    depends_on:\n      vllm:\n        condition: service_healthy\n    ports:\n      - '${H2OGPT_PORT}:7860'\n    volumes:\n      - cache:/workspace/.cache\n      - save:/workspace/save\n    networks:\n      - h2ogpt\n    command:\n      - /workspace/generate.py\n      - --inference_server=\"vllm:vllm:5000\"\n      - --base_model=${H2OGPT_BASE_MODEL}\n      - --langchain_mode=UserData\n    deploy:\n      resources:\n        reservations:\n          devices:\n          - driver: nvidia\n            device_ids: ['2', '3']\n            capabilities: [gpu]\n\n  vllm:\n    image: vllm/vllm-openai:latest\n    restart: always\n    shm_size: '64gb'\n    expose:\n      - 5000\n    volumes:\n      - cache:/workspace/.cache\n    networks:\n      - h2ogpt\n    entrypoint: python3\n    command: -m vllm.entrypoints.openai.api_server --port=5000 --host=0.0.0.0 ${H2OGPT_VLLM_ARGS}\n    environment:\n      - NCCL_IGNORE_DISABLED_P2P=1\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://0.0.0.0:5000/v1/models\" ]\n      interval: 30s\n      timeout: 5s\n      retries: 20\n    deploy:\n      resources:\n        reservations:\n          devices:\n          - driver: nvidia\n            device_ids: ['0', '1']\n            capabilities: [gpu]\n\nvolumes:\n  cache:\n  save:\nnetworks:\n  h2ogpt:\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.4423828125,
          "content": "version: '3'\n\nservices:\n  h2ogpt:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    restart: always\n    shm_size: '2gb'\n    ports:\n      - '${H2OGPT_PORT}:7860'\n    volumes:\n      - cache:/workspace/.cache\n      - save:/workspace/save\n    command: ${H2OGPT_ARGS}\n    deploy:\n      resources:\n        reservations:\n          devices:\n          - driver: nvidia\n            count: all\n            capabilities: [gpu]\n\nvolumes:\n  cache:\n  save:\n"
        },
        {
          "name": "docker_build_script_ubuntu.sh",
          "type": "blob",
          "size": 4.091796875,
          "content": "#!/bin/bash\nset -o pipefail\nset -ex\n\nexport DEBIAN_FRONTEND=noninteractive\nexport PATH=/h2ogpt_conda/bin:$PATH\nexport HOME=/workspace\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu121 https://huggingface.github.io/autogptq-index/whl/cu121\"\n\n# Install linux dependencies\napt-get update && apt-get install -y \\\n    git \\\n    curl \\\n    wget \\\n    software-properties-common \\\n    pandoc \\\n    vim \\\n    libmagic-dev \\\n    poppler-utils \\\n    tesseract-ocr \\\n    libtesseract-dev \\\n    libreoffice \\\n    autoconf \\\n    libtool \\\n    docker.io \\\n    nodejs \\\n    npm \\\n    zip \\\n    unzip \\\n    htop \\\n    tree \\\n    tmux \\\n    jq \\\n    net-tools \\\n    nmap \\\n    ncdu \\\n    mtr \\\n    rsync \\\n    build-essential \\\n    parallel \\\n    bc \\\n    pv \\\n    expect \\\n    cron \\\n    at \\\n    screen \\\n    inotify-tools \\\n    jq \\\n    xmlstarlet \\\n    dos2unix \\\n    ssh\n\n# Run upgrades\napt-get upgrade -y\n\n# Install conda\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    mkdir -p /h2ogpt_conda && \\\n    bash ./Miniconda3-latest-Linux-x86_64.sh -b -u -p /h2ogpt_conda && \\\n    conda update -n base conda && \\\n    source /h2ogpt_conda/etc/profile.d/conda.sh && \\\n    conda create -n h2ogpt -y && \\\n    conda activate h2ogpt && \\\n    conda install python=3.10 pygobject weasyprint -c conda-forge -y && \\\n    echo \"h2oGPT conda env: $CONDA_DEFAULT_ENV\"\n\n# if building for CPU, would remove CMAKE_ARGS and avoid GPU image as base image\n# Choose llama_cpp_python ARGS for your system according to [llama_cpp_python backend documentation](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends), e.g. for CUDA:\nexport GGML_CUDA=1\nexport CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\"\n# for Metal MAC M1/M2 comment out above two lines and uncomment out the below line\n# export CMAKE_ARGS=\"-DLLAMA_METAL=on\"\nexport FORCE_CMAKE=1\nexport GPLOK=1\nbash docs/linux_install.sh\n\nchmod -R a+rwx /h2ogpt_conda\n\n# setup tiktoken cache\nexport TIKTOKEN_CACHE_DIR=/workspace/tiktoken_cache\npython3.10 -c \"\nimport tiktoken\nfrom tiktoken_ext import openai_public\n# FakeTokenizer etc. needs tiktoken for general tasks\nfor enc in openai_public.ENCODING_CONSTRUCTORS:\n    encoding = tiktoken.get_encoding(enc)\nmodel_encodings = [\n    'gpt-4',\n    'gpt-4-0314',\n    'gpt-4-32k',\n    'gpt-4-32k-0314',\n    'gpt-3.5-turbo',\n    'gpt-3.5-turbo-16k',\n    'gpt-3.5-turbo-0301',\n    'text-ada-001',\n    'ada',\n    'text-babbage-001',\n    'babbage',\n    'text-curie-001',\n    'curie',\n    'davinci',\n    'text-davinci-003',\n    'text-davinci-002',\n    'code-davinci-002',\n    'code-davinci-001',\n    'code-cushman-002',\n    'code-cushman-001'\n]\nfor enc in model_encodings:\n    encoding = tiktoken.encoding_for_model(enc)\nprint('Done!')\n\"\n\n# Open Web UI\nconda create -n open-webui -y\nsource /h2ogpt_conda/etc/profile.d/conda.sh\nconda activate open-webui\nconda install python=3.11 -y\necho \"open-webui conda env: $CONDA_DEFAULT_ENV\"\n\nchmod -R a+rwx /h2ogpt_conda\npip install https://h2o-release.s3.amazonaws.com/h2ogpt/open_webui-0.3.8-py3-none-any.whl\n\n# Track build info\ncp /workspace/build_info.txt /build_info.txt\n\nmkdir -p /workspace/save\nchmod -R a+rwx /workspace/save\n\n# Cleanup\nrm -rf /workspace/Miniconda3-py310_23.1.0-1-Linux-x86_64.sh\nrm -rf /workspace/.cache/pip\nrm -rf /h2ogpt_conda/pkgs\nrm -rf /workspace/spaces\nrm -rf /workspace/benchmarks\nrm -rf /workspace/data\nrm -rf /workspace/cloud\nrm -rf /workspace/docs\nrm -rf /workspace/helm\nrm -rf /workspace/notebooks\nrm -rf /workspace/papers\n\n# Hotswap vulnerable dependencies\nwget https://s3.amazonaws.com/artifacts.h2o.ai/deps/h2ogpt/ubuntu20.04/apparmor_4.0.0~alpha2-0ubuntu5_amd64.deb\nwget https://s3.amazonaws.com/artifacts.h2o.ai/deps/h2ogpt/ubuntu20.04/libapparmor1_4.0.0~alpha2-0ubuntu5_amd64.deb\ndpkg -i libapparmor1_4.0.0~alpha2-0ubuntu5_amd64.deb\ndpkg -i apparmor_4.0.0~alpha2-0ubuntu5_amd64.deb\nrm -rf libapparmor1_4*.deb apparmor_4*.deb\n\nwget https://s3.amazonaws.com/artifacts.h2o.ai/deps/h2ogpt/ubuntu20.04/libarchive13_3.6.2-1ubuntu1_amd64.deb\ndpkg -i libarchive13_3.6.2-1ubuntu1_amd64.deb\nrm -rf libarchive13_3.6.2-1ubuntu1_amd64.deb\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune.py",
          "type": "blob",
          "size": 30.060546875,
          "content": "import os\nimport sys\nfrom functools import partial\nfrom typing import List, Union\nimport numpy as np\n\nif os.path.dirname(os.path.abspath(__file__)) not in sys.path:\n    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nif os.path.dirname('src') not in sys.path:\n    sys.path.append('src')\n\nfrom loaders import get_loaders, get_tokenizer\nfrom prompter import generate_prompt, prompt_types, PromptType\nfrom utils import get_githash, copy_code, H2O_Fire\nimport torch\n\n\ndef log(*args, **kwargs):\n    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n        if 'flush' not in kwargs:\n            kwargs['flush'] = True\n        print(*args, **kwargs)\n\n\n# supported by huggingface evaluate\nsupported_metrics = ['bleu', 'rouge', 'sacrebleu', 'meteor']\n\n\ndef train(\n        save_code: bool = False,\n        run_id: int = None,\n\n        base_model: str = 'h2oai/h2ogpt-4096-llama2-7b',\n        # base_model: str = 'h2oai/h2ogpt-4096-llama2-13b',\n        # base_model: str = 'h2oai/h2ogpt-4096-llama2-70b',\n\n        # only needed if base_model is self-exported HF state without tokenizer\n        tokenizer_base_model: str = None,\n        # tokenizer_base_model: str = 'EleutherAI/gpt-neox-20b',\n\n        data_path: str = \"h2oai/openassistant_oasst1_h2ogpt\",\n        data_col_dict: dict = None,\n        # data_path: str = \"./dai_docs.train.json\",\n        prompt_type: Union[str, int] = \"plain\",  # \"plain\", \"instruct\", \"quality\", \"human_bot\", \"dai_faq\"\n\n        valid_path: str = None,\n        # valid_path: str = \"./dai_docs.valid.json\",\n\n        # data_mix_in_path: str = \"laion/OIG\",  # way too big, medium quality\n        data_mix_in_path: str = \"0-hero/OIG-small-chip2\",  # high quality, 50 MB, good enough for now\n        data_mix_in_factor: float = 0.0,  # >1: more mix-in data, <1: more of data_path data\n        data_mix_in_col_dict: dict = {'user': 'instruction', 'chip2': 'output'},\n        data_mix_in_prompt_type: str = \"instruct\",  # just instruction->output, same as instruct\n\n        output_dir: str = None,\n\n        # LoRA checkpoint continuation\n        lora_weights: str = \"\",\n\n        # batching training hyperparams\n        batch_size: int = 128,\n        micro_batch_size: int = 4,\n        gradient_checkpointing=False,  # unnecessary with gradient accumulation enabled\n        bf16=False,  # needed (and automatically enabled) for llama2-7b\n        fp16=True,\n        train_8bit=False,\n        train_4bit=False,\n\n        # general training hyperparams\n        num_epochs: float = 1,\n        learning_rate: float = 3e-4,\n\n        # validation settings\n        val_set_size: int = None,\n        val_metrics: List[str] = [],\n        eval_steps: int = None,  # to control eval steps via steps\n        eval_epochs: float = None,  # to control eval steps via epochs\n\n        # lora hyperparams\n        lora_r: int = 8,\n        lora_alpha: int = 16,\n        lora_dropout: float = 0.05,\n        lora_target_modules: List[str] = None,\n        llama_type: bool = None,\n        llama_flash_attn: bool = False,\n\n        # llm hyperparams\n        train_on_inputs: bool = True,  # if False, masks out inputs in loss\n        group_by_length: bool = False,  # if True, faster, but produces an odd training loss curve\n        resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n        cutoff_len: int = 512,  # larger values use more memory\n        drop_truncations: bool = False,  # if True, drop any truncated long sequences\n\n        # torch training params\n        ddp: bool = True,  # set to False if OOM with True, for multi-GPU model parallelism\n        local_files_only: bool = False,  # else will download new versions, normally unwanted\n        resume_download: bool = True,\n        use_auth_token: Union[str, bool] = False,  # True requires CLI did huggingface-cli login before running\n        warmup_steps: int = 100,\n        logging_steps: int = 1,\n        save_steps: int = None,  # must be round multiple of eval_steps\n        save_total_limit: int = 3,\n        add_eos_token: bool = False,\n):\n    if llama_flash_attn:\n        # Need to call this before importing transformers.\n        from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n        replace_llama_attn_with_flash_attn()\n    if \"llama2-7b\" in base_model:\n        fp16 = False\n        bf16 = True\n\n    # allow set token directly\n    use_auth_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\", use_auth_token)\n\n    prompt_type = str(prompt_type)  # migration from integers\n    assert prompt_type in prompt_types\n\n    world_size = int(os.getenv(\"WORLD_SIZE\", 1))\n    local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n    rank = int(os.getenv(\"RANK\", 0))\n    print(f\"local_rank: {local_rank}\")\n    print(f\"global rank: {rank}\")\n\n    gpus = max(world_size, torch.cuda.device_count())\n    run_id = run_id or 0\n    if not data_path:\n        raise ValueError(\"No data_path provided\")\n    if not output_dir:\n        output_dir = f\"{base_model.split('/')[-1]}.{data_path.replace('/', '')}.{num_epochs}_epochs.{get_githash() or 'nogit'}.{run_id}\"\n        if os.path.exists(output_dir) and not resume_from_checkpoint:\n            raise FileExistsError(\n                f\"output_dir {output_dir} based on run_id {run_id} already exists. Please pick a different run_id.\")\n    else:\n        if os.path.exists(output_dir) and not resume_from_checkpoint:\n            raise FileExistsError(\n                f\"output_dir {output_dir} already exists. Please pick a different output_dir, or specify a run_id instead.\")\n    device_map = \"auto\"\n\n    if save_code:\n        copy_code(run_id)\n    if tokenizer_base_model is None:\n        tokenizer_base_model = base_model\n    if llama_type is None:\n        llama_type = \"llama\" in base_model.lower()\n    if llama_type and llama_flash_attn:\n        from importlib.metadata import distribution, PackageNotFoundError\n        try:\n            distribution('flash_attn')\n            can_do_flash_attn = True\n        except (PackageNotFoundError, AssertionError):\n            can_do_flash_attn = False\n\n        if not can_do_flash_attn:\n            raise RuntimeError(\"\"\"Flash attention not installed.\n            NOTE: for current pytorch 2.0, flash attention requires installing cuda 11.7 via https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=runfile_local and then when running, to avoid installing driver, docs, samples, just install toolkit.  Then when pip installing flash attention do:\n\n            CUDA_HOME=/usr/local/cuda-11.7 pip install flash-attn\"\"\")\n    assert (\n        base_model\n    ), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n    gradient_accumulation_steps = batch_size // micro_batch_size\n    assert gradient_accumulation_steps >= world_size, \"must increase batch_size for multi-GPU\"\n\n    device_map = \"auto\"\n\n    locals_dict = locals().copy()\n    locals_print = '\\n'.join(['%s: %s' % (k, v) for k, v in locals_dict.items()])\n    log(f\"Training model with params:\\n{locals_print}\")\n    log(\"Command: %s\\nHash: %s\" % (str(' '.join(sys.argv)), get_githash()))\n\n    max_memory = None\n    if gpus > 1:\n        if ddp:\n            log(\"Distributed: data parallel\")\n            device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n            gradient_accumulation_steps = gradient_accumulation_steps // world_size\n        else:\n            free_in_GB = int(min(torch.cuda.mem_get_info()) / 1024 ** 3)\n            max_memory = f\"{free_in_GB - 2}GB\"\n            max_memory = {i: max_memory for i in range(gpus)}\n            log(\"world_size: %d\" % world_size)\n            log(\"num_gpus: %d\" % gpus)\n            log(\"max mem: %s\" % max_memory)\n\n    model_loader, tokenizer_loader, conditional_type = (\n        get_loaders(model_name=base_model, reward_type=False, llama_type=llama_type))\n\n    model = model_loader(\n        base_model,\n        load_in_8bit=train_8bit,\n        load_in_4bit=train_4bit,\n        device_map=device_map,\n        torch_dtype=torch.float16,\n        max_memory=max_memory,\n        local_files_only=local_files_only,\n        trust_remote_code=True,\n        resume_download=resume_download,\n        token=use_auth_token,\n    )\n    print(model)\n    if gpus > 1:\n        if not ddp:\n            log(\"model parallel\")\n            model.is_parallelizable = True\n            model.model_parallel = True\n\n    tokenizer = get_tokenizer(tokenizer_loader, tokenizer_base_model, local_files_only, resume_download, use_auth_token)\n\n    if train_8bit or train_4bit:\n        from peft import (\n            prepare_model_for_kbit_training,\n        )\n\n        model = prepare_model_for_kbit_training(model)\n\n    from peft import LoraConfig, get_peft_model, set_peft_model_state_dict\n    try:\n        from peft import utils\n        lora_mappings = utils.TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.copy()\n    except AttributeError:\n        from peft import mapping\n        lora_mappings = mapping.TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.copy()\n    lora_mappings['distilgpt2'] = [\"c_attn\"]\n\n    if lora_weights:\n\n        from peft import PeftModel\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            torch_dtype=torch.float16,\n            device_map=device_map,\n            local_files_only=local_files_only,\n            resume_download=resume_download,\n            token=use_auth_token,\n        )\n    elif lora_r > 0:\n        if lora_target_modules is None:\n            base_model_lower = base_model.lower()\n            if base_model_lower in lora_mappings:\n                lora_target_modules_cand = [lora_mappings[base_model_lower]]\n            else:\n                lora_target_modules_cand = [[\"query_key_value\"], [\"q_proj\", \"v_proj\"]]\n        else:\n            lora_target_modules_cand = [lora_target_modules]\n\n        for lora_target_modules in lora_target_modules_cand:\n            try:\n                config = LoraConfig(\n                    r=lora_r,\n                    lora_alpha=lora_alpha,\n                    target_modules=lora_target_modules,\n                    lora_dropout=lora_dropout,\n                    bias=\"none\",\n                    task_type=\"CAUSAL_LM\",\n                )\n                model = get_peft_model(model, config)\n                break\n            except ValueError as e:\n                if \"Target modules\" in str(e) and \"not found\" in str(e):\n                    continue\n                else:\n                    raise\n        from peft import PeftModel\n        assert isinstance(model, PeftModel), \"LoRA failed. Please provide --lora_target_modules explicitly.\"\n    if resume_from_checkpoint:\n        # Check the available weights and load them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint, \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n            checkpoint_name = os.path.join(\n                resume_from_checkpoint, \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config above has to fit\n            resume_from_checkpoint = False  # So the trainer won't try loading its state\n        # The two files above have a different name depending on how they were saved, but are actually the same.\n        if os.path.exists(checkpoint_name):\n            log(f\"Restarting from {checkpoint_name}\")\n            adapters_weights = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model, adapters_weights)\n        else:\n            log(f\"Checkpoint {checkpoint_name} not found\")\n\n    print(model)\n    try:\n        # only for PeftModel\n        model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n    except:\n        pass\n\n    metrics = {}\n    for name in supported_metrics:\n        if name in val_metrics:\n            import evaluate  # Causes hang for 'python generate.py' on dual 4090 if imported early, 100% reproducible\n            metrics[name] = evaluate.load(name)\n    log(\"Using Validation Metrics: %s\" % str(list(metrics.keys())))\n    log(\"Supported Metrics: %s\" % supported_metrics)\n\n    if val_set_size is None:\n        if len(metrics) == 0:\n            val_set_size = 1000\n        else:\n            val_set_size = 100\n        log(\"Auto set val_set_size %s\" % val_set_size)\n    elif val_set_size < 1.0 and val_set_size != 0:\n        raise RuntimeError(\"Fractional validation size not supported.\")\n\n    from datasets import load_dataset, concatenate_datasets\n    if valid_path:\n        data = load_dataset(\"json\", data_files={\"train\": data_path, \"valid\": valid_path})\n    else:\n        if \"json\" in data_path:\n            data = load_dataset(\"json\", data_files={\"train\": data_path})\n        else:\n            data = load_dataset(data_path)\n            data = data.rename_columns(data_col_dict or {})\n\n    valid_data = None\n    train_data_mix_in = None\n    valid_data_mix_in = None\n\n    if data_mix_in_path and data_mix_in_factor > 0:\n        # get mix-in training/validation data - to keep model \"sane\"\n        num_rows = data[\"train\"].num_rows\n        log(\"Loading mix-in dataset: %s\" % data_mix_in_path)\n        if \"json\" in data_mix_in_path:\n            data_mix_in = load_dataset(\"json\", data_files={\"train\": data_mix_in_path})[\"train\"]\n        else:\n            data_mix_in = load_dataset(data_mix_in_path)[\"train\"]  # can be large\n        data_mix_in = data_mix_in.rename_columns(data_mix_in_col_dict or {})\n        mix_in_rows = int(num_rows * data_mix_in_factor)\n\n        if mix_in_rows > data_mix_in.num_rows:\n            # duplicate rows if mix-in is smaller than required\n            log(\"Duplicating mixin to compensate for its size for training size and mixin fraction\")\n            data_mix_in = concatenate_datasets([data_mix_in] * int(np.ceil(mix_in_rows / data_mix_in.num_rows)))\n\n        # only get as much as we need to balance\n        valid_size = min(data_mix_in.num_rows // 2, val_set_size or 0)\n        train_size = max(1, min(data_mix_in.num_rows - valid_size, mix_in_rows))\n        mixin_small = data_mix_in.train_test_split(\n            test_size=train_size + valid_size,\n            shuffle=True, seed=np.random.randint(10000),\n        )[\"test\"]\n        if valid_size:\n            mixin_train_test = mixin_small.train_test_split(\n                test_size=valid_size, shuffle=False,\n            )\n            train_data_mix_in = mixin_train_test[\"train\"]\n            valid_data_mix_in = mixin_train_test[\"test\"]\n        else:\n            train_data_mix_in = mixin_small\n\n        if \"prompt_type\" not in train_data_mix_in.column_names:\n            train_data_mix_in = train_data_mix_in.add_column(\n                \"prompt_type\",\n                [data_mix_in_prompt_type] * train_data_mix_in.num_rows,\n            )\n            log(\"Added prompt type %s to mix-in training data\" % data_mix_in_prompt_type)\n        if valid_data_mix_in and \"prompt_type\" not in valid_data_mix_in.column_names:\n            valid_data_mix_in = valid_data_mix_in.add_column(\n                \"prompt_type\",\n                [data_mix_in_prompt_type] * valid_data_mix_in.num_rows,\n            )\n            log(\"Added prompt type %s to mix-in validation data\" % data_mix_in_prompt_type)\n        log(\"Created mix-in data:\\nTrain %s\\nValid %s\" % (train_data_mix_in, valid_data_mix_in))\n\n    # get our own training/validation data - for fine-tuning\n    if val_set_size > 0 and not valid_path and not data_mix_in_path:\n        # create valid split from train\n        train_val = data[\"train\"].train_test_split(\n            test_size=val_set_size, shuffle=True, seed=42\n        )\n        train_data = train_val[\"train\"]\n        valid_data = train_val[\"test\"]\n    else:\n        train_data = data[\"train\"]\n        if valid_path:\n            # use given valid split, has priority over data_mix_in_path\n            valid_data = data[\"valid\"]\n    if \"prompt_type\" not in train_data.column_names:\n        train_data = train_data.add_column(\n            \"prompt_type\",\n            [prompt_type] * train_data.num_rows,\n        )\n        log(\"Added prompt type %s to training data\" % prompt_type)\n    if valid_data and \"prompt_type\" not in valid_data.column_names:\n        valid_data = valid_data.add_column(\n            \"prompt_type\",\n            [prompt_type] * valid_data.num_rows,\n        )\n        log(\"Added prompt type %s to validation data\" % prompt_type)\n\n    assert train_data is not None\n\n    generate_and_tokenize_prompt_fun = partial(generate_and_tokenize_prompt, prompt_type=prompt_type,\n                                               train_on_inputs=train_on_inputs, add_eos_token=add_eos_token,\n                                               cutoff_len=cutoff_len, tokenizer=tokenizer)\n\n    # shuffle and tokenize data\n    if train_data_mix_in:\n        train_data = concatenate_datasets([train_data, train_data_mix_in])\n    log(\"Tokenizing %s training rows\" % train_data.num_rows)\n    train_data = train_data.shuffle().map(generate_and_tokenize_prompt_fun,\n                                          num_proc=os.cpu_count() // torch.cuda.device_count())\n    if drop_truncations:\n        log(\"avoid keeping truncated cases to avoid contaminating model with truncation cases.  Original size: %s\" % train_data.num_rows)\n        prune_long_sequences_func = partial(prune_long_sequences, cutoff_len=cutoff_len)\n        train_data = train_data.filter(prune_long_sequences_func, num_proc=os.cpu_count() // torch.cuda.device_count())\n        log(\"avoid keeping truncated cases to avoid contaminating model with truncation cases.  New size: %s\" % train_data.num_rows)\n    train_set_size = len(train_data)\n\n    if valid_data and valid_data_mix_in:\n        valid_data = concatenate_datasets([valid_data, valid_data_mix_in])\n    elif valid_data_mix_in:\n        valid_data = valid_data_mix_in\n\n    if valid_data:\n        log(\"Tokenizing %s validation rows\" % valid_data.num_rows)\n        valid_data = valid_data.shuffle().map(generate_and_tokenize_prompt_fun,\n                                              num_proc=os.cpu_count() // torch.cuda.device_count())\n        val_set_size = len(valid_data)\n    else:\n        val_set_size = 0\n    log(\"Final fine-tuning data:\\nTrain %s\\nValid %s\" % (train_data, valid_data))\n    sample_row_dict = train_data[:1]\n    del sample_row_dict['input_ids']\n    del sample_row_dict['attention_mask']\n    del sample_row_dict['labels']\n    log(\"Sample input: %s\" % sample_row_dict)\n\n    try:\n        import neptune\n        from transformers.integrations import NeptuneCallback\n\n        neptune_run = neptune.init_run(\n            source_files=[],\n        )\n        log(\"Connected to Neptune.\")\n    except ImportError:\n        neptune_run = None\n        log(\"Please pip install neptune for tracking.\")\n    except neptune.exceptions.NeptuneMissingApiTokenException:\n        neptune_run = None\n        os.environ[\"NEPTUNE_MODE\"] = 'debug'\n        log(\"No neptune configured, set NEPTUNE_API_TOKEN env var.\")\n\n    if neptune_run:\n        neptune_callback = NeptuneCallback(run=neptune_run)\n        callbacks = [neptune_callback]\n    else:\n        from transformers.integrations import TensorBoardCallback, is_tensorboard_available\n        if is_tensorboard_available:\n            # tensorboard --logdir=runs/\n            from torch.utils.tensorboard import SummaryWriter\n            tb_writer = SummaryWriter()\n            callbacks = [TensorBoardCallback(tb_writer=tb_writer)]\n        else:\n            callbacks = []\n\n    expected_steps = (train_set_size * num_epochs) // batch_size\n    if eval_steps is None and eval_epochs is None:\n        # 20 evaluations for a run\n        eval_steps = max(1, int(expected_steps / 20))\n        log(\"Auto set eval_steps to %s out of %s total training steps\" % (eval_steps, expected_steps))\n    elif eval_steps is None and eval_epochs is not None:\n        eval_steps = max(1, int(expected_steps * eval_epochs / num_epochs))\n        log(\"Auto converted eval_epochs=%s to eval_steps %s\"\n            \" out of %s total training steps\" % (eval_epochs, eval_steps, expected_steps))\n    if save_steps is None:\n        save_steps = eval_steps\n        log(\"Auto step save_steps to %s\" % save_steps)\n    elif save_steps > eval_steps:\n        # save steps must be round multiple of eval_steps\n        save_steps0 = save_steps\n        save_steps = max(1, (save_steps // eval_steps)) * eval_steps\n        if save_steps0 != save_steps:\n            log(\"Auto converted save_steps from %s to %s\" % (save_steps0, save_steps))\n\n    def compute_metrics(eval_preds):\n        # e.g. see: https://huggingface.co/docs/transformers/v4.25.1/en/tasks/translation#evaluate\n        inputs = eval_preds.inputs\n        label_ids = eval_preds.label_ids\n        predictions = eval_preds.predictions\n\n        # inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n        # decoded_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n        # decoded_inputs = [pred.strip() for pred in decoded_inputs]\n\n        label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n        # tokenizer behavior like generate time\n        decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True,\n                                                clean_up_tokenization_spaces=True)\n        decoded_labels = [pred.strip() for pred in decoded_labels]\n\n        predictions = np.argmax(predictions, -1)\n        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n        # tokenizer behavior like generate time\n        decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True,\n                                                     clean_up_tokenization_spaces=True)\n        decoded_predictions = [pred.strip() for pred in decoded_predictions]\n\n        result = {}\n        for metric in metrics.values():\n            result1 = metric.compute(predictions=decoded_predictions, references=decoded_labels)\n            # get rid of lists, for precision etc., for now\n            numeric_results = {k: v for k, v in result1.items() if isinstance(v, (int, float))}\n            result.update(numeric_results)\n        return result\n\n    # the callback that computes metrics of interest\n    if val_metrics:\n        trainer_kwargs = dict(compute_metrics=compute_metrics)\n    else:\n        trainer_kwargs = dict()\n\n    import transformers\n    trainer = transformers.Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_data,\n        eval_dataset=valid_data,\n        # FIXME: might need Seq2SeqTrainingArguments for some models\n        args=transformers.TrainingArguments(\n            per_device_train_batch_size=micro_batch_size,\n            per_device_eval_batch_size=1,\n            eval_accumulation_steps=10,\n            # predict_with_generate=True,  # SEQ2SEQ only\n            include_inputs_for_metrics=True,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            warmup_steps=warmup_steps,\n            num_train_epochs=num_epochs,\n            learning_rate=learning_rate,\n            gradient_checkpointing=gradient_checkpointing,\n            bf16=bf16,\n            fp16=fp16,\n            # cosnider 8-bit adam: https://huggingface.co/docs/transformers/v4.18.0/en/performance#8bit-adam\n            optim=\"adamw_torch\",  # consider \"adafactor\" to save memory\n            logging_steps=logging_steps,\n            logging_strategy=\"steps\",\n            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n            save_strategy=\"steps\",\n            eval_steps=eval_steps if val_set_size > 0 else None,\n            save_steps=save_steps,\n            output_dir=output_dir,\n            save_total_limit=save_total_limit,\n            load_best_model_at_end=True if val_set_size > 0 else False,\n            ddp_find_unused_parameters=False if ddp else None,\n            group_by_length=group_by_length,\n            # fsdp=gpus > 1 and not ddp,\n            report_to='tensorboard' if not neptune_run else 'neptune',\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n        ),\n        callbacks=callbacks,\n        **trainer_kwargs,\n    )\n    model.config.use_cache = False\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n        # WIP (not generally replacing layers until pytorch 2.1)\n        if not llama_flash_attn:\n            torch.backends.cuda.enable_flash_sdp(True)\n\n    if gpus > 1 and not ddp:\n        assert trainer.is_model_parallel\n    else:\n        assert not trainer.is_model_parallel\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n    model.save_pretrained(output_dir)\n\n    log(\"\\n If there's a warning about missing keys above, please disregard :)\")\n\n\ndef tokenize(prompt, tokenizer, cutoff_len, add_eos_token=False):\n    # there's probably a way to do this with the tokenizer settings\n    # but again, gotta move fast\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=cutoff_len,\n        padding=False,\n        return_tensors=None,\n    )\n    if (\n            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < cutoff_len\n            and add_eos_token\n    ):\n        result[\"input_ids\"].append(tokenizer.eos_token_id)\n        result[\"attention_mask\"].append(1)\n\n    result[\"labels\"] = result[\"input_ids\"].copy()\n\n    return result\n\n\ndef prune_long_sequences(data_point, cutoff_len=None):\n    \"\"\"\n    Prune if too long for tokenizer, so truncation doesn't lead training to learn from truncated language\n    :param data_point:\n    :param cutoff_len:\n    :return:\n    \"\"\"\n    assert cutoff_len is not None\n    return len(data_point['input_ids']) < cutoff_len\n\n\ndef generate_and_tokenize_prompt(data_point, prompt_type=None, train_on_inputs=False, add_eos_token=False,\n                                 cutoff_len=None, tokenizer=None):\n    assert prompt_type is not None\n    assert cutoff_len is not None\n    assert tokenizer is not None\n    prompt_dict = ''  # only for custom prompt_type\n    assert prompt_type != PromptType.custom.name, \"custom not setup for finetune\"\n    full_prompt, _, _, _, _ = generate_prompt(data_point, prompt_type, prompt_dict, False, False)\n    tokenized_full_prompt = tokenize(full_prompt, tokenizer, cutoff_len, add_eos_token=add_eos_token)\n    if not train_on_inputs:\n        user_prompt, _, _, _, _ = generate_prompt({**data_point, \"output\": \"\"}, prompt_type, prompt_dict, False,\n                                                  False)\n        tokenized_user_prompt = tokenize(user_prompt, tokenizer, cutoff_len, add_eos_token=add_eos_token)\n        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n        if add_eos_token:\n            user_prompt_len -= 1\n\n        # ignore_index=-100 ensures torch/tf don't include padding token id in CrossEntropyLoss\n        tokenized_full_prompt[\"labels\"] = [\n                                              -100\n                                          ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n                                                                user_prompt_len:\n                                                                ]  # could be sped up, probably\n    return tokenized_full_prompt\n\n\ndef test_debug():\n    H2O_Fire(train)\n\n\ndef entrypoint_main():\n    CONFIG = \"NCCL_P2P_LEVEL=LOC WORLD_SIZE=5 torchrun --nnodes=5 --master_addr=10.10.10.2 --master_port=1111 --nproc_per_node=1\"\n    CMD = \"finetune.py --data_path=config.json --num_epochs=1 --base_model=decapoda-research/llama-13b-hf\"\n    log(f\"\"\"\n    Example runs on 4 GPUs:\n    WORLD_SIZE=4 CUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 finetune.py --base_model='decapoda-research/llama-7b-hf' --data_path=data/config.json --run_id=0 &> 0.log\n    WORLD_SIZE=4 CUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 finetune.py --base_model='decapoda-research/llama-30b-hf' --data_path=data/config.json --batch_size=16 --micro_batch_size=1 --run_id=1 --save_code=True &> 1.log\n    WORLD_SIZE=4 CUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 finetune.py --base_model='EleutherAI/gpt-j-6B' --data_path=data/config.json --run_id=2 &> 2.log\n    WORLD_SIZE=4 CUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 finetune.py --base_model='EleutherAI/gpt-neox-20b' --data_path=data/config.json --run_id=8 --batch_size=16 --micro_batch_size=4 &> 8.log\n    WORLD_SIZE=4 CUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 finetune.py --base_model='togethercomputer/GPT-NeoXT-Chat-Base-20B' --data_path=data/config.json --prompt_type='dai_faq' --run_id=13 --batch_size=16 --micro_batch_size=4 --num_epochs=100 --val_set_size=0 data_mix_in_path='' &> 13.log\n    WORLD_SIZE=4 CUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 finetune.py --base_model='togethercomputer/GPT-NeoXT-Chat-Base-20B' --data_path=data/config.json --run_id=28 --batch_size=16 --micro_batch_size=4 --num_epochs=8 --val_set_size=0 --data_mix_in_factor=0.1 --data_mix_in_prompt_type='human_bot' --save_code=True --cutoff_len=512  &> 28.log\n\n    All metrics:\n    CUDA_VISIBLE_DEVICES= finetune.py --data_mix_in_factor=0 --eval_steps=100 --warmup_steps=2 --val_set_size=100 --val_metrics=\"['bleu', 'rouge', 'sacrebleu', 'meteor']\"\n\n    # Fine-tune 20B on 24GB GPUs across 3 nodes with 3+2+2 GPUs\n    rippa>\nNCCL_P2P_LEVEL=LOC WORLD_SIZE=7 CUDA_VISIBLE_DEVICES=\"0,1,2\" torchrun --node_rank 0 --nproc_per_node=3 --master_port=1234 --nnodes=3 --master_addr=10.10.10.2 finetune.py --data_path=merged_shuffled_OIG_87f6a1e788.json --micro_batch_size=1 --batch_size=7 --cutoff_len=512 --run_id=17 &>log.17.rank0\n    ova>\nNCCL_P2P_LEVEL=LOC WORLD_SIZE=7 CUDA_VISIBLE_DEVICES=\"0,1\" torchrun --node_rank 1 --nproc_per_node=2 --master_port=1234 --nnodes=3 --master_addr=10.10.10.2 finetune.py --data_path=merged_shuffled_OIG_87f6a1e788.json --micro_batch_size=1 --batch_size=7 --cutoff_len=512 --run_id=17 &>log.17.rank1\n    timemachine>\nNCCL_P2P_LEVEL=LOC WORLD_SIZE=7 CUDA_VISIBLE_DEVICES=\"0,1\" torchrun --node_rank 2 --nproc_per_node=2 --master_port=1234 --nnodes=3 --master_addr=10.10.10.2 finetune.py --data_path=merged_shuffled_OIG_87f6a1e788.json --micro_batch_size=1 --batch_size=7 --cutoff_len=512 --run_id=17 &>log.17.rank2\n\n    \"\"\", flush=True)\n\n    if os.environ.get(\"LOCAL_RANK\") is None:\n        # then not using torchrun, so can't do distributed, ensure CVD set\n        assert os.environ.get(\n            \"CUDA_VISIBLE_DEVICES\") is not None, \"Run python script using: torchrun finetune.py OR set CUDA_VISIBLE_DEVICES to single GPU\"\n\n    H2O_Fire(train)\n\n\nif __name__ == \"__main__\":\n    entrypoint_main()\n"
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 0.3662109375,
          "content": "import os\nimport sys\n\nif os.path.dirname(os.path.abspath(__file__)) not in sys.path:\n    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nfrom src.utils_sys import protect_stdout_stderr\n\nprotect_stdout_stderr()\n\nfrom src.gen import main\nfrom src.utils import H2O_Fire\n\n\ndef entrypoint_main():\n    H2O_Fire(main)\n\n\nif __name__ == \"__main__\":\n    entrypoint_main()\n"
        },
        {
          "name": "gradio_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "h2o-logo.svg",
          "type": "blob",
          "size": 1.6015625,
          "content": "<svg id=\"Layer_1\" data-name=\"Layer 1\" xmlns=\"http://www.w3.org/2000/svg\" width=\"600.28\" height=\"600.28\" viewBox=\"0 0 600.28 600.28\"><defs><style>.cls-1{fill:#fec925;}.cls-2{fill:#161616;}.cls-3{fill:#54585a;}</style></defs><g id=\"Fill-1\"><rect class=\"cls-1\" width=\"600.28\" height=\"600.28\" rx=\"23.24\"/></g><path class=\"cls-2\" d=\"M174.33,246.06v92.78H152.86v-38H110.71v38H89.24V246.06h21.47v36.58h42.15V246.06Z\"/><path class=\"cls-2\" d=\"M259.81,321.34v17.5H189.7V324.92l35.78-33.8c8.22-7.82,9.68-12.59,9.68-17.09,0-7.29-5-11.53-14.85-11.53-7.95,0-14.71,3-19.21,9.27L185.46,261.7c7.15-10.47,20.14-17.23,36.84-17.23,20.68,0,34.46,10.6,34.46,27.44,0,9-2.52,17.22-15.51,29.29l-21.33,20.14Z\"/><path class=\"cls-2\" d=\"M268.69,292.45c0-27.57,21.47-48,50.76-48s50.76,20.28,50.76,48-21.6,48-50.76,48S268.69,320,268.69,292.45Zm79.78,0c0-17.63-12.46-29.69-29-29.69s-29,12.06-29,29.69,12.46,29.69,29,29.69S348.47,310.08,348.47,292.45Z\"/><path class=\"cls-3\" d=\"M377.23,326.91c0-7.69,5.7-12.73,12.85-12.73s12.86,5,12.86,12.73a12.86,12.86,0,1,1-25.71,0Z\"/><path class=\"cls-3\" d=\"M481.4,298.15v40.69H462.05V330c-3.84,6.49-11.27,9.94-21.74,9.94-16.7,0-26.64-9.28-26.64-21.61,0-12.59,8.88-21.34,30.62-21.34h16.43c0-8.87-5.3-14-16.43-14-7.55,0-15.37,2.51-20.54,6.62l-7.43-14.44c7.82-5.57,19.35-8.62,30.75-8.62C468.81,266.47,481.4,276.54,481.4,298.15Zm-20.68,18.16V309H446.54c-9.67,0-12.72,3.57-12.72,8.35,0,5.16,4.37,8.61,11.66,8.61C452.37,326,458.34,322.8,460.72,316.31Z\"/><path class=\"cls-3\" d=\"M497.56,246.06c0-6.49,5.17-11.53,12.86-11.53s12.86,4.77,12.86,11.13c0,6.89-5.17,11.93-12.86,11.93S497.56,252.55,497.56,246.06Zm2.52,21.47h20.68v71.31H500.08Z\"/></svg>"
        },
        {
          "name": "h2ogpt",
          "type": "tree",
          "content": null
        },
        {
          "name": "helm",
          "type": "tree",
          "content": null
        },
        {
          "name": "iterators",
          "type": "tree",
          "content": null
        },
        {
          "name": "metrics",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_server",
          "type": "tree",
          "content": null
        },
        {
          "name": "papers",
          "type": "tree",
          "content": null
        },
        {
          "name": "reqs_optional",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 2.833984375,
          "content": "# no websockets, more cloud friendly\n# able to make gradio clean-up states\n\n# gradio @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.25.0-py3-none-any.whl\n# gradio_client @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.0-py3-none-any.whl\n#gradio @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.20.1-py3-none-any.whl\n#gradio_client==0.11.0\n# gradio @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl\n# gradio_client @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl\n\ngradio==4.44.0\ngradio_client==1.3.0\n\nuvicorn[standard]\ngunicorn\nfastapi-utils\nsse_starlette>=1.8.2\n# consrained by tokenizers etc.:\nhuggingface_hub==0.25.2\nappdirs>=1.4.4\nfire>=0.5.0\ndocutils>=0.20.1\ntorch==2.2.1; sys_platform != \"darwin\" and platform_machine != \"arm64\"\ntorch==2.3.1; sys_platform == \"darwin\" and platform_machine == \"arm64\"\nevaluate>=0.4.0\nrouge_score>=0.1.2\nsacrebleu>=2.3.1\nscikit-learn>=1.2.2\n# optional (need to uncomment code in gradio_runner.py for import of better_profanity)\n# alt-profanity-check==1.2.2\n# better-profanity==0.7.0\nnumpy>=1.23.4,<2.0\npandas>=2.0.2\nmatplotlib>=3.7.1\n\n# transformers\nloralib>=0.1.2\nbitsandbytes>=0.43.1; sys_platform != \"darwin\" and platform_machine != \"arm64\"\n#bitsandbytes downgraded because of Mac M1/M2 support issue. See https://github.com/axolotl-ai-cloud/axolotl/issues/1436\nbitsandbytes==0.42.0; sys_platform == \"darwin\" and platform_machine == \"arm64\"\naccelerate>=0.30.1\npeft>=0.7.0\ntransformers>=4.45.1\njinja2>=3.1.0\ntokenizers>=0.19.0\nhf_transfer>=0.1.6\n#optimum>=1.17.1\ndatasets>=2.18.0\nsentencepiece>=0.2.0\n\nAPScheduler>=3.10.1\n\n# optional for generate\npynvml>=11.5.0\npsutil>=5.9.5\nboto3>=1.26.101\nbotocore>=1.29.101\nbeautifulsoup4>=4.12.2\nmarkdown>=3.4.3\n\n# data and testing\npytest>=7.2.2\npytest-xdist>=3.2.1\nnltk>=3.8.1\ntextstat>=0.7.3\n# pandoc==2.3\npypandoc>=1.11; sys_platform == \"darwin\" and platform_machine == \"arm64\"\npypandoc_binary>=1.11; platform_machine == \"x86_64\"\npypandoc_binary>=1.11; platform_system == \"Windows\"\npython-magic-bin>=0.4.14; platform_system == \"Windows\"\nopenpyxl>=3.1.2\nlm_dataformat>=0.0.20\nbioc>=2.0\n\n# for HF embeddings\nsentence_transformers>=3.0.1\nInstructorEmbedding @ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl\nsentence_transformers_old @ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl\n\n# falcon\neinops>=0.6.1\n\n# for gpt4all .env file, but avoid worrying about imports\npython-dotenv>=1.0.0\n\njson_repair>=0.21.0\n\ntext-generation>=0.7.0\n\n# for tokenization when don't have HF tokenizer\ntiktoken>=0.5.2\n\n# optional: for OpenAI endpoint\nopenai>=1.40.1\nslowapi>=0.1.9\n\n# for image metadata\npyexiv2\n\nrequests>=2.31.0\nhttpx>=0.24.1\nurllib3>=1.26.16\nfilelock>=3.12.2\njoblib>=1.3.1\ntqdm>=4.65.0\ntabulate>=0.9.0\npackaging>=23.1\n\njsonschema>=4.23.0\nspacy==3.7.5"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.701171875,
          "content": "import os\n\nimport setuptools\nfrom typing import List\nfrom setuptools import find_packages\n\nfor_pypi = os.getenv('PYPI') is not None\n\n\ndef parse_requirements(file_name: str) -> List[str]:\n    with open(file_name) as f:\n        lines = f.read().splitlines()\n\n    # Filter out comments and empty lines\n    lines = [line for line in lines if line.strip() and not line.strip().startswith(\"#\")]\n\n    requirements = []\n    for line in lines:\n        if 'chromamigdb' in line:\n            # hnsw issue\n            continue\n        if for_pypi:\n            if 'http://' in line or 'https://' in line:\n                continue\n            if 'llama-cpp-python' in line and ';' in line:\n                line = line[:line.index(';')]\n\n        # assume all requirements files are in PEP 508 format with name @ <url> or name @ git+http/git+https\n        requirements.append(line)\n\n    return requirements\n\n\ninstall_requires = parse_requirements('requirements.txt')\n\nreq_files = [\n    'reqs_optional/requirements_optional_langchain.txt',\n    'reqs_optional/requirements_optional_llamacpp_gpt4all.txt',\n    'reqs_optional/requirements_optional_langchain.gpllike.txt',\n    'reqs_optional/requirements_optional_agents.txt',\n    'reqs_optional/requirements_optional_langchain.urls.txt',\n    'reqs_optional/requirements_optional_doctr.txt',\n    'reqs_optional/requirements_optional_audio.txt',\n    'reqs_optional/requirements_optional_image.txt',\n]\n\nfor req_file in req_files:\n    x = parse_requirements(req_file)\n    install_requires.extend(x)\n\n# faiss on cpu etc.\ninstall_cpu = parse_requirements('reqs_optional/requirements_optional_cpu_only.txt')\n\n# faiss on gpu etc.\ninstall_cuda = parse_requirements('reqs_optional/requirements_optional_gpu_only.txt')\n\n# TRAINING\ninstall_extra_training = parse_requirements('reqs_optional/requirements_optional_training.txt')\n\n# WIKI_EXTRA\ninstall_wiki_extra = parse_requirements('reqs_optional/requirements_optional_wikiprocessing.txt')\n\n# User-friendly description from README.md\ncurrent_directory = os.path.dirname(os.path.abspath(__file__))\nwith open(os.path.join(current_directory, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\nwith open(os.path.join(current_directory, 'version.txt'), encoding='utf-8') as f:\n    version = f.read().strip()\n\n# Data to include\npackages = find_packages(include=['h2ogpt', 'h2ogpt.*'], exclude=['tests'])\n\nsetuptools.setup(\n    name='h2ogpt',\n    packages=packages,\n    package_data={\n        # If 'h2ogpt' is your package directory and 'spkemb' is directly inside it\n        'h2ogpt': ['spkemb/*.npy'],\n        # If 'spkemb' is inside 'src' which is inside 'h2ogpt'\n        # Adjust the string according to your actual package structure\n        'h2ogpt.src': ['spkemb/*.npy'],\n    },\n    exclude_package_data={\n        'h2ogpt': [\n            '**/__pycache__/**',\n            'models/README-template.md'\n        ],\n    },\n    version=version,\n    license='https://opensource.org/license/apache-2-0/',\n    description='',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    author='H2O.ai',\n    author_email='jon.mckinney@h2o.ai, arno@h2o.ai',\n    url='https://github.com/h2oai/h2ogpt',\n    download_url='',\n    keywords=['LLM', 'AI'],\n    install_requires=install_requires,\n    extras_require={\n        'cpu': install_cpu,\n        'cuda': install_cuda,\n        'TRAINING': install_extra_training,\n        'WIKI_EXTRA': install_wiki_extra,\n        'local-inference': ['unstructured[local-inference]>=0.12.5,<0.13'],\n    },\n    classifiers=[],\n    python_requires='>=3.10',\n    entry_points={\n        'console_scripts': [\n            'h2ogpt_finetune=h2ogpt.finetune:entrypoint_main',\n            'h2ogpt_generate=h2ogpt.generate:entrypoint_main',\n        ],\n    },\n)\n"
        },
        {
          "name": "spaces",
          "type": "tree",
          "content": null
        },
        {
          "name": "spkemb",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.005859375,
          "content": "0.2.1\n"
        },
        {
          "name": "win_run_app.py",
          "type": "blob",
          "size": 5.5556640625,
          "content": "import os\nimport sys\nimport time\nimport traceback\nimport webbrowser\n\n# uncomment below to ensure CPU install only uses CPU\n# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n\nprint('__file__: %s' % __file__)\npath1 = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(path1)\nbase_path = os.path.dirname(path1)\nsys.path.append(base_path)\nos.environ['PYTHONPATH'] = path1\nprint('path1', path1, flush=True)\n\nos.environ['NLTK_DATA'] = os.path.join(base_path, './nltk_data')\npath_list = [os.environ['PATH'],\n                     os.path.join(base_path, 'poppler/Library/bin/'),\n                     os.path.join(base_path, 'poppler/Library/lib/'),\n                     os.path.join(base_path, 'Tesseract-OCR'),\n                     os.path.join(base_path, 'ms-playwright'),\n                     os.path.join(base_path, 'ms-playwright/chromium-1076/chrome-win'),\n                     os.path.join(base_path, 'ms-playwright/ffmpeg-1009'),\n                     os.path.join(base_path, 'ms-playwright/firefox-1422/firefox'),\n                     os.path.join(base_path, 'ms-playwright/webkit-1883'),\n                     os.path.join(base_path, 'rubberband/')]\nos.environ['PATH'] = ';'.join(path_list)\nprint(os.environ['PATH'])\n\nimport shutil, errno\n\n\ndef copy_tree(src, dst):\n    try:\n        shutil.copytree(src, dst)\n    except OSError as exc: # python >2.5\n        if exc.errno in (errno.ENOTDIR, errno.EINVAL):\n            shutil.copy(src, dst)\n        else: raise\n\n\ndef setup_paths():\n    for sub in ['src', 'iterators', 'gradio_utils', 'metrics', 'models', '.']:\n        path2 = os.path.join(base_path, '..', sub)\n        if os.path.isdir(path2):\n            if sub == 'models' and os.path.isfile(os.path.join(path2, 'human.jpg')):\n                os.environ['H2OGPT_MODEL_BASE'] = path2\n            sys.path.append(path2)\n        print(path2, flush=True)\n\n        path2 = os.path.join(path1, '..', sub)\n        if os.path.isdir(path2):\n            if sub == 'models' and os.path.isfile(os.path.join(path2, 'human.jpg')):\n                os.environ['H2OGPT_MODEL_BASE'] = path2\n            sys.path.append(path2)\n        print(path2, flush=True)\n\n    # for app, avoid forbidden for web access\n    if os.getenv('H2OGPT_MODEL_BASE'):\n        base0 = os.environ['H2OGPT_MODEL_BASE']\n        if 'Programs' in os.environ['H2OGPT_MODEL_BASE']:\n            os.environ['H2OGPT_MODEL_BASE'] = os.environ['H2OGPT_MODEL_BASE'].replace('Programs', 'Temp/gradio/')\n            if os.path.isdir(os.environ['H2OGPT_MODEL_BASE']):\n                shutil.rmtree(os.environ['H2OGPT_MODEL_BASE'], ignore_errors=True)\n            if os.path.isfile(os.path.join(base0, 'human.jpg')):\n                copy_tree(base0, os.environ['H2OGPT_MODEL_BASE'])\n\n\nfrom importlib.metadata import distribution, PackageNotFoundError\n\ntry:\n    dtorch = distribution('torch')\n    assert dtorch is not None\n    have_torch = True\n    torch_version = dtorch.version\nexcept (PackageNotFoundError, AssertionError):\n    have_torch = False\n    torch_version = ''\n\n\ndef _main():\n    setup_paths()\n    os.environ['h2ogpt_block_gradio_exit'] = 'False'\n    os.environ['h2ogpt_score_model'] = ''\n\n    try:\n        from pynvml import nvmlInit, nvmlDeviceGetCount\n        nvmlInit()\n        deviceCount = nvmlDeviceGetCount()\n    except Exception as e:\n        print(\"No GPUs detected by NVML: %s\" % str(e))\n        deviceCount = 0\n\n    need_get_gpu_torch = False\n    if have_torch and deviceCount > 0:\n        if '+cu' not in torch_version:\n            need_get_gpu_torch = True\n    elif not have_torch and deviceCount > 0:\n        need_get_gpu_torch = True\n\n    print(\"Torch Status: have torch: %s need get gpu torch: %s CVD: %s GPUs: %s\" % (have_torch, need_get_gpu_torch, os.getenv('CUDA_VISIBLE_DEVICES'), deviceCount))\n\n    auto_install_torch_gpu = False\n\n    import sys\n    if auto_install_torch_gpu and (not have_torch or need_get_gpu_torch) and sys.platform == \"win32\":\n        print(\"Installing Torch\")\n        # for one-click, don't have torch installed, install now\n        import subprocess\n        import sys\n\n        def install(package):\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n        if os.getenv('TORCH_WHEEL'):\n            print(\"Installing Torch from %s\" % os.getenv('TORCH_WHEEL'))\n            install(os.getenv('TORCH_WHEEL'))\n        else:\n            if need_get_gpu_torch:\n                wheel_file = \"https://h2o-release.s3.amazonaws.com/h2ogpt/torch-2.1.2%2Bcu118-cp310-cp310-win_amd64.whl\"\n                print(\"Installing Torch from %s\" % wheel_file)\n                install(wheel_file)\n            # assume cpu torch part of install\n            #else:\n            #   wheel_file = \"https://h2o-release.s3.amazonaws.com/h2ogpt/torch-2.1.2-cp310-cp310-win_amd64.whl\"\n            #    print(\"Installing Torch from %s\" % wheel_file)\n            #    install(wheel_file)\n        import importlib\n        importlib.invalidate_caches()\n        import pkg_resources\n        importlib.reload(pkg_resources)  # re-load because otherwise cache would be bad\n\n    from generate import entrypoint_main as main_h2ogpt\n    main_h2ogpt()\n\n    server_name = os.getenv('h2ogpt_server_name', os.getenv('H2OGPT_SERVER_NAME', 'localhost'))\n    server_port = os.getenv('GRADIO_SERVER_PORT', str(7860))\n\n    url = \"http://%s:%s\" % (server_name, server_port)\n    webbrowser.open(url)\n\n    while True:\n        time.sleep(10000)\n\n\ndef main():\n    try:\n        _main()\n    except BaseException as e:\n        with open('h2ogpt_exception.log', 'at') as f:\n            f.write(traceback.format_exc())\n        time.sleep(10)\n        raise\n    time.sleep(10)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "windows_installer.cfg",
          "type": "blob",
          "size": 10.509765625,
          "content": "[Application]\nname=h2oGPT\nversion=0.0.1\n# How to launch the app - this calls the 'main' function from the 'myapp' package:\nentry_point=win_run_app:main\nicon=src/h2o-logo.ico\n\n[Python]\nversion=3.10.1\nbitness=64\n\n[Include]\n# Packages from PyPI that your application requires, one per line\n# These must have wheels on PyPI:\n# * from pip freeze but removed git wheels and remove profanity packages\n# pip wheel antlr4-python3-runtime==4.9.3 ffmpy==0.3.1 fire==0.5.0\n# pip\n# move *.whl wheels\npypi_wheels = absl-py==2.0.0\n    accelerate==0.25.0\n    aiofiles==23.2.1\n    aiohttp==3.9.1\n    aiosignal==1.3.1\n    altair==5.2.0\n    annotated-types==0.6.0\n    anthropic==0.8.1\n    antlr4-python3-runtime==4.9.3\n    anyio==3.7.1\n    appdirs==1.4.4\n    APScheduler==3.10.1\n    argcomplete==3.2.1\n    arrow==1.3.0\n    arxiv==1.4.8\n    asgiref==3.7.2\n    async-timeout==4.0.3\n    attributedict==0.3.0\n    attrs==23.2.0\n    audioread==3.0.1\n    Authlib==1.3.0\n    auto-gptq==0.6.0\n    autoawq==0.1.8\n    backoff==2.2.1\n    bcrypt==4.1.2\n    beautifulsoup4==4.12.2\n    bioc==2.0\n    bitsandbytes==0.41.1\n    blessings==1.7\n    boto3==1.26.101\n    botocore==1.29.101\n    bravado==11.0.3\n    bravado-core==6.1.1\n    Brotli==1.1.0\n    bs4==0.0.1\n    cachetools==5.3.2\n    certifi==2023.11.17\n    cffi==1.16.0\n    chardet==5.2.0\n    charset-normalizer==3.3.2\n    chroma-bullet==2.2.0\n    chroma-hnswlib==0.7.3\n    chroma-migrate==0.0.7\n    chromadb==0.4.22\n    chromamigdb==0.3.26\n    click==8.1.7\n    clickhouse-connect==0.6.6\n    codecov==2.1.13\n    colorama==0.4.6\n    coloredlogs==15.0.1\n    colour-runner==0.1.1\n    contourpy==1.2.0\n    coverage==7.4.0\n    cryptography==41.0.7\n    cssselect2==0.2.1\n    curl-cffi==0.5.10\n    cycler==0.12.1\n    dacite==1.7.0\n    dataclasses-json==0.6.3\n    DataProperty==1.0.1\n    datasets==2.13.0\n    decorator==5.1.1\n    deepdiff==6.7.1\n    defusedxml==0.7.1\n    Deprecated==1.2.14\n    diffusers==0.24.0\n    dill==0.3.6\n    diskcache==5.6.3\n    distlib==0.3.8\n    distro==1.9.0\n    dnspython==2.4.2\n    docutils==0.20.1\n    duckdb==0.7.1\n    duckduckgo_search==4.1.1\n    effdet==0.4.1\n    einops==0.6.1\n    emoji==2.9.0\n    et-xmlfile==1.1.0\n    evaluate==0.4.0\n    exceptiongroup==1.2.0\n    execnet==2.0.2\n    exllama==0.0.18\n    faiss-cpu==1.7.4\n    fastapi==0.108.0\n    feedparser==6.0.11\n    ffmpeg==1.4\n    ffmpy==0.3.1\n    fiftyone==0.23.6\n    fiftyone-brain==0.16.1\n    fiftyone-db==1.1.1\n    filelock==3.13.1\n    filetype==1.2.0\n    fire==0.5.0\n    flatbuffers==23.5.26\n    fonttools==4.47.0\n    fqdn==1.5.1\n    frozenlist==1.4.1\n    fsspec==2023.12.2\n    ftfy==6.1.3\n    future==0.18.3\n    gekko==1.0.6\n    gitdb==4.0.11\n    GitPython==3.1.40\n    glob2==0.7\n    google-ai-generativelanguage==0.4.0\n    google-api-core==2.15.0\n    google-auth==2.26.1\n    google-auth-oauthlib==1.0.0\n    google-generativeai==0.3.2\n    google-search-results==2.4.2\n    googleapis-common-protos==1.62.0\n    gpt4all==1.0.5\n    gradio==3.50.2\n    gradio_client==0.6.1\n    gradio_tools==0.0.9\n    graphql-core==3.2.3\n    greenlet==2.0.2\n    grpcio==1.60.0\n    grpcio-status==1.60.0\n    h11==0.14.0\n    h2==4.1.0\n    h5py==3.10.0\n    hnswmiglib==0.7.0\n    hpack==4.0.0\n    html2text==2020.1.16\n    html5lib==1.1\n    httpcore==0.17.3\n    httptools==0.6.1\n    httpx==0.24.1\n    huggingface-hub==0.19.4\n    humanfriendly==10.0\n    humanize==4.9.0\n    Hypercorn==0.16.0\n    hyperframe==6.0.1\n    idna==3.6\n    imageio==2.33.1\n    importlib-metadata==6.11.0\n    importlib-resources==6.1.1\n    inflate64==1.0.0\n    iniconfig==2.0.0\n    inspecta==0.1.3\n    InstructorEmbedding==1.0.1\n    intervaltree==3.1.0\n    iopath==0.1.10\n    isoduration==20.11.0\n    jaraco.context==4.3.0\n    Jinja2==3.1.2\n    jmespath==1.0.1\n    joblib==1.3.2\n    jsonlines==4.0.0\n    jsonpatch==1.33\n    jsonpointer==2.4\n    jsonref==1.1.0\n    jsonschema==4.20.0\n    jsonschema-specifications==2023.12.1\n    kaleido==0.2.1\n    kiwisolver==1.4.5\n    kubernetes==28.1.0\n    langchain==0.0.354\n    langchain-community==0.0.8\n    langchain-core==0.1.6\n    langchain-experimental==0.0.47\n    langchain-google-genai==1.0.1\n    langchain_mistralai==0.0.2\n    langdetect==1.0.9\n    langsmith==0.0.77\n    layoutparser==0.3.4\n    lazy_loader==0.3\n    librosa==0.10.1\n    llama_cpp_python==0.2.76\n    llvmlite==0.41.1\n    lm-dataformat==0.0.20\n    lm_eval==0.4.0\n    loralib==0.1.1\n    lxml==5.1.0\n    lz4==4.3.3\n    Markdown==3.4.3\n    MarkupSafe==2.1.3\n    marshmallow==3.20.1\n    matplotlib==3.7.1\n    mbstrdecoder==1.1.3\n    mistralai==0.0.8\n    mmh3==4.0.1\n    mongoengine==0.24.2\n    monotonic==1.6\n    more-itertools==10.2.0\n    motor==3.3.2\n    mplcursors==0.5.2\n    mpmath==1.3.0\n    msg-parser==1.2.0\n    msgpack==1.0.7\n    multidict==6.0.4\n    multiprocess==0.70.14\n    multivolumefile==0.2.3\n    munkres==1.1.4\n    mutagen==1.47.0\n    mypy-extensions==1.0.0\n    neptune==1.2.0\n    nest-asyncio==1.5.8\n    networkx==3.2.1\n    nltk==3.8.1\n    numba==0.58.1\n    numexpr==2.8.8\n    numpy==1.23.4\n    oauthlib==3.2.2\n    olefile==0.47\n    omegaconf==2.3.0\n    onnx==1.15.0\n    onnxruntime==1.15.0\n    onnxruntime-gpu==1.15.0\n    openai==1.3.7\n    opencv-python==4.9.0.80\n    opencv-python-headless==4.9.0.80\n    openpyxl==3.1.2\n    opentelemetry-api==1.22.0\n    opentelemetry-exporter-otlp-proto-common==1.22.0\n    opentelemetry-exporter-otlp-proto-grpc==1.22.0\n    opentelemetry-instrumentation==0.43b0\n    opentelemetry-instrumentation-asgi==0.43b0\n    opentelemetry-instrumentation-fastapi==0.43b0\n    opentelemetry-proto==1.22.0\n    opentelemetry-sdk==1.22.0\n    opentelemetry-semantic-conventions==0.43b0\n    opentelemetry-util-http==0.43b0\n    openvino==2022.3.0\n    optimum==1.16.1\n    ordered-set==4.1.0\n    orjson==3.9.10\n    outcome==1.3.0.post0\n    overrides==7.4.0\n    packaging==23.2\n    pandas==2.0.2\n    pathvalidate==3.2.0\n    pdf2image==1.17.0\n    pdfminer.six==20231228\n    pdfplumber==0.10.3\n    peft==0.7.1\n    Pillow==9.5.0\n    pip==23.3.2\n    pip-licenses==4.3.0\n    platformdirs==4.1.0\n    playwright==1.37.0\n    plotly==5.18.0\n    pluggy==1.3.0\n    pooch==1.8.0\n    portalocker==2.8.2\n    posthog==3.0.1\n    pprintpp==0.4.0\n    prettytable==3.9.0\n    priority==2.0.0\n    proto-plus==1.23.0\n    protobuf==4.25.1\n    psutil==5.9.5\n    pulsar-client==3.4.0\n    py7zr==0.20.8\n    pyarrow==14.0.2\n    pyasn1==0.5.1\n    pyasn1-modules==0.3.0\n    pybcj==1.0.2\n    pybind11==2.11.1\n    pycairo==1.25.1\n    pyclipper==1.3.0.post5\n    pycocotools==2.0.7\n    pycparser==2.21\n    pycryptodomex==3.19.1\n    pydantic==2.5.3\n    pydantic-settings==2.1.0\n    pydantic_core==2.14.6\n    pydub==0.25.1\n    pydyf==0.8.0\n    pyee==9.0.4\n    Pygments==2.17.2\n    PyJWT==2.8.0\n    pylibjpeg_libjpeg==2.0.0\n    pylibjpeg_openjpeg==2.0.0\n    pylibjpeg_rle==2.0.0\n    pymongo==4.6.1\n    PyMuPDF==1.23.8\n    pynvml==11.5.0\n    pypandoc==1.12\n    pypandoc-binary==1.11\n    pyparsing==3.1.1\n    pypdf==3.17.1\n    pypdfium2==4.24.0\n    pyphen==0.14.0\n    PyPika==0.48.9\n    pyppmd==1.1.0\n    pyproject-api==1.6.1\n    pyreadline3==3.4.1\n    PySocks==1.7.1\n    pytablewriter==1.2.0\n    pytesseract==0.3.10\n    pytest==7.2.2\n    pytest-xdist==3.2.1\n    python-dateutil==2.8.2\n    python-doctr==0.5.4a0\n    python-docx==1.1.0\n    python-dotenv==1.0.0\n    python-iso639==2024.1.2\n    python-magic==0.4.27\n    python-magic-bin==0.4.14\n    python-multipart==0.0.6\n    python-pptx==0.6.23\n    pytz==2023.3.post1\n    pywin32==306\n    PyYAML==6.0.1\n    pyzstd==0.15.9\n    rapidfuzz==3.6.1\n    qdrant-client==1.8.0\n    rarfile==4.1\n    referencing==0.32.1\n    regex==2023.12.25\n    replicate==0.20.0\n    requests==2.31.0\n    requests-file==1.5.1\n    requests-oauthlib==1.3.1\n    requests_download==0.1.2\n    responses==0.18.0\n    retrying==1.3.4\n    rfc3339-validator==0.1.4\n    rfc3986-validator==0.1.1\n    rootpath==0.1.1\n    rouge==1.0.1\n    rouge-score==0.1.2\n    rpds-py==0.16.2\n    rsa==4.9\n    s3transfer==0.6.2\n    sacrebleu==2.3.1\n    safetensors==0.4.1\n    scikit-image==0.22.0\n    scikit-learn==1.2.2\n    scipy==1.11.4\n    selenium==4.11.2\n    semantic-version==2.10.0\n    semanticscholar==0.7.0\n    sentence-transformers==2.2.2\n    sentencepiece==0.1.99\n    setuptools==68.2.2\n    sgmllib3k==1.0.0\n    Shapely==1.8.5.post1\n    simplejson==3.19.2\n    six==1.16.0\n    smmap==5.0.1\n    sniffio==1.3.0\n    sortedcontainers==2.4.0\n    soundfile==0.12.1\n    soupsieve==2.5\n    soxr==0.3.7\n    SQLAlchemy==2.0.25\n    sqlitedict==2.1.0\n    sse-starlette==0.10.3\n    sseclient-py==1.8.0\n    starlette==0.32.0.post1\n    strawberry-graphql==0.138.1\n    swagger-spec-validator==3.0.3\n    sympy==1.12\n    tabledata==1.3.3\n    tabulate==0.9.0\n    taskgroup==0.0.0a4\n    tcolorpy==0.1.4\n    tenacity==8.2.3\n    tensorboard==2.13.0\n    tensorboard-data-server==0.7.2\n    termcolor==2.4.0\n    text-generation==0.6.1\n    textstat==0.7.3\n    texttable==1.7.0\n    threadpoolctl==3.2.0\n    tifffile==2023.12.9\n    tiktoken==0.5.2\n    timm==0.9.12\n    tinycss2==1.2.1\n    tokenizers==0.15.2\n    toml==0.10.2\n    tomli==2.0.1\n    toolz==0.12.0\n    torch==2.1.2\n    torchvision==0.16.2\n    tox==4.11.4\n    tqdm==4.66.1\n    tqdm-multiprocess==0.0.11\n    transformers==4.36.2\n    trio==0.23.2\n    trio-websocket==0.11.1\n    typepy==1.3.2\n    typer==0.9.0\n    types-python-dateutil==2.8.19.20240106\n    typing-inspect==0.9.0\n    typing_extensions==4.9.0\n    tzdata==2023.4\n    tzlocal==5.2\n    ujson==5.9.0\n    unicodedata2==15.1.0\n    Unidecode==1.3.7\n    universal-analytics-python3==1.1.1\n    unstructured==0.11.8\n    unstructured-inference==0.7.15\n    unstructured.pytesseract==0.3.12\n    uri-template==1.3.0\n    urllib3==1.26.18\n    uvicorn==0.25.0\n    validators==0.22.0\n    virtualenv==20.25.0\n    voxel51-eta==0.12.3\n    watchfiles==0.21.0\n    wavio==0.0.8\n    wcwidth==0.2.13\n    weasyprint==60.1\n    weaviate-client==3.25.3\n    webcolors==1.13\n    webencodings==0.5.1\n    websocket-client==1.7.0\n    websockets==11.0.3\n    Werkzeug==3.0.1\n    wikipedia==1.4.0\n    wolframalpha==5.0.0\n    wrapt==1.16.0\n    wsproto==1.2.0\n    xlrd==2.0.1\n    XlsxWriter==3.1.9\n    xmltodict==0.13.0\n    xxhash==3.4.1\n    yarg==0.1.9\n    yarl==1.9.4\n    yt-dlp==2023.10.13\n    zipp==3.17.0\n    zopfli==0.2.3\n    zstandard==0.22.0\n\n\n# To bundle packages which don't publish wheels, or to include directly wheel files\n# from a directory, see the docs on the config file.\n\n# Other files and folders that should be installed\nfiles = LICENSE\n     generate.py\n     h2o-logo.svg\n     gradio_utils\n     iterators\n     metrics\n     models\n     openai_server\n     src\n     win_run_app.py\n     nltk_data\n     poppler\n     Tesseract-OCR\n     ms-playwright\n     rubberband\n     ffmpeg\n\n#    data_files/\n# playwright stuff leads to too large installer and fails to build for GPU, avoid for now\n\nextra_wheel_sources = wheels/\n#local_wheels = wheels/"
        }
      ]
    }
  ]
}