{
  "metadata": {
    "timestamp": 1736561101297,
    "page": 34,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Genesis-Embodied-AI/Genesis",
      "stars": 22334,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.263671875,
          "content": "# Force LF line endings for shell scripts\n*.sh text eol=lf\n\n# Force LF line endings for configuration files\n*.yml text eol=lf\n*.yaml text eol=lf\n*.json text eol=lf\n*.conf text eol=lf\n\n# Force LF for Docker-related files\nDockerfile text eol=lf\n*.dockerignore text eol=lf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.873046875,
          "content": "logs/\ntmp/\nMUJOCO_LOG.TXT\nimgui.ini\n.DS_Store\nsftp-config.json\n*.mp4\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndata/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\n# lib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# debug\nexamples/debug/*\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndoc/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.idea/\n.vscode/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.3212890625,
          "content": "[submodule \"doc\"]\n\tpath = doc\n\turl = https://github.com/Genesis-Embodied-AI/genesis-doc\n[submodule \"genesis/ext/LuisaRender\"]\n\tpath = genesis/ext/LuisaRender\n\turl = https://github.com/Alif-01/LuisaRender\n[submodule \"genesis/ext/ParticleMesher\"]\n\tpath = genesis/ext/ParticleMesher\n\turl = https://github.com/ACMLCZH/ParticleMesher\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.189453125,
          "content": "repos:\n  - repo: https://github.com/psf/black\n    rev: 23.9.1  # Use the latest version or specify the version you prefer\n    hooks:\n      - id: black\n        args: [\"--config=pyproject.toml\"] \n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.6103515625,
          "content": "# Contributing to Genesis\n\nThank you for your interest in contributing to Genesis! We welcome contributions from everyone. Please take a moment to review this guide to ensure a smooth collaboration.\n\n- [Reporting Bugs](#reporting-bugs)\n- [Suggesting Features](#suggesting-features)\n- [Submitting Code Changes](#submitting-code-changes)\n- [Reviewing and Merging](#reviewing-and-merging)\n- [Questions and Discussions](#questions-and-discussions)\n\n---\n\n## Reporting Bugs\n\n- Before reporting a bug, please search through existing issues to check if it has already been reported.\n\n- If the issue hasn't been reported yet, please use our issue templates to provide as much detail as possible in your report.\n\n  ```markdown\n  **Description**\n  A clear and concise description of what the bug is.\n\n  **To Reproduce**\n  Example code or commands to reproduce the bug.\n\n  **Expected behavior**\n  A clear and concise description of what you expected to happen.\n\n  **Screenshots**\n  If applicable, add screenshots to help explain your problem.\n\n  **Environment:**\n   - OS: [e.g., Linux, macOS]\n   - GPU/CPU: [e.g., A100, RTX 4090, M3pro]\n\n  **Additional context**\n  Add any other context about the problem here.\n  ```\n\n## Suggesting Features\n\n- If you have a feature idea, please create an issue labeled `enhancement`.\n- In the created issue, please provide context, expected outcomes, and potential.\n\n## Submitting Code Changes\n\n- We use the `pre-commit` configuration to automatically clean up code before committing. Install and run `pre-commit` as follows:\n  1. Install `pre-commit`:\n\n     ```bash\n     pip install pre-commit\n     ```\n\n  2. Install hooks from the configuration file:\n\n     ```bash\n     pre-commit install\n     ```\n\n     After this, `pre-commit` will automatically check and clean up code whenever you make a commit.\n- (Optional) You can run CI tests locally to ensure you pass the online CI checks.\n\n  ```python\n  python -m unittest discover tests\n  ```\n\n- In the title of your Pull Request, please include [BUG FIX], [FEATURE] or [MISC] to indicate the purpose.\n- In the description, please provide example code or commands for testing.\n\n## Reviewing and Merging\n\n- PRs require at least one approval before merging.\n- Automated checks (e.g., CI tests) must pass.\n- Use `Squash and Merge` for a clean commit history.\n\n## Questions and Discussions\n\n- Use [Discussions](https://github.com/Genesis-Embodied-AI/Genesis/discussions) for open-ended topics.\n<!-- \n### Join Us\n- Follow the project’s progress and updates on [channel/community link]. -->\n\n---\n\nWe appreciate your contributions and look forward to collaborating with you!\n\nThank you,  \nGenesis Maintainers\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08984375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1943359375,
          "content": "include README.md\ninclude LICENSE\nrecursive-include genesis/assets *\nrecursive-include genesis/ext/pyrender/fonts *\nrecursive-include genesis/ext/pyrender/shaders *\ninclude genesis/ext/VolumeSampling"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.431640625,
          "content": "![Genesis](imgs/big_text.png)\r\n\r\n![Teaser](imgs/teaser.png)\r\n\r\n[![PyPI - Version](https://img.shields.io/pypi/v/genesis-world)](https://pypi.org/project/genesis-world/)\r\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/genesis-world)](https://pypi.org/project/genesis-world/)\r\n[![GitHub Issues](https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/issues)\r\n[![GitHub Discussions](https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/discussions)\r\n[![Discord](https://img.shields.io/discord/1322086972302430269?logo=discord)](https://discord.gg/nukCuhB47p)\r\n<a href=\"https://drive.google.com/uc?export=view&id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ\"><img src=\"https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white\" height=\"20\" style=\"display:inline\"></a>\r\n\r\n[![README in English](https://img.shields.io/badge/English-d9d9d9)](./README.md)\r\n[![README en Français](https://img.shields.io/badge/Francais-d9d9d9)](./README_FR.md)\r\n[![한국어 README](https://img.shields.io/badge/한국어-d9d9d9)](./README_KR.md)\r\n[![简体中文版自述文件](https://img.shields.io/badge/简体中文-d9d9d9)](./README_CN.md)\r\n[![日本語版 README](https://img.shields.io/badge/日本語-d9d9d9)](./README_JA.md)\r\n\r\n# Genesis\r\n\r\n## 🔥 News\r\n- [2025-01-09] We released a [detailed performance benchmarking and comparison report](https://github.com/zhouxian/genesis-speed-benchmark) on Genesis, together with all the test scripts.\r\n- [2025-01-08] Released v0.2.1 🎊 🎉\r\n- [2025-01-08] Created [Discord](https://discord.gg/nukCuhB47p) and [Wechat](https://drive.google.com/uc?export=view&id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ) group.\r\n- [2024-12-25] Added a [docker](#docker) including support for the ray-tracing renderer\r\n- [2024-12-24] Added guidelines for [contributing to Genesis](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/CONTRIBUTING.md)\r\n\r\n## Table of Contents\r\n\r\n1. [What is Genesis?](#what-is-genesis)\r\n2. [Key Features](#key-features)\r\n3. [Quick Installation](#quick-installation)\r\n4. [Docker](#docker)\r\n5. [Documentation](#documentation)\r\n6. [Contributing to Genesis](#contributing-to-genesis)\r\n7. [Support](#support)\r\n8. [License and Acknowledgments](#license-and-acknowledgments)\r\n9. [Associated Papers](#associated-papers)\r\n10. [Citation](#citation)\r\n\r\n## What is Genesis?\r\n\r\nGenesis is a physics platform designed for general-purpose *Robotics/Embodied AI/Physical AI* applications. It is simultaneously multiple things:\r\n\r\n1. A **universal physics engine** re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.\r\n2. A **lightweight**, **ultra-fast**, **pythonic**, and **user-friendly** robotics simulation platform.\r\n3. A powerful and fast **photo-realistic rendering system**.\r\n4. A **generative data engine** that transforms user-prompted natural language description into various modalities of data.\r\n\r\nPowered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond.\r\n\r\n**Note**: Currently, we are open-sourcing the _underlying physics engine_ and the _simulation platform_. Our _generative framework_ is a modular system that incorporates many different generative modules, each handling a certain range of data modalities, routed by a high level agent. Some of the modules integrated existing papers and some are still under submission. Access to our generative feature will be gradually rolled out in the near future. If you are interested, feel free to explore more in the [paper list](#associated-papers) below.\r\n\r\nGenesis aims to:\r\n\r\n- **Lower the barrier** to using physics simulations, making robotics research accessible to everyone. See our [mission statement](https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html).\r\n- **Unify diverse physics solvers** into a single framework to recreate the physical world with the highest fidelity.\r\n- **Automate data generation**, reducing human effort and letting the data flywheel spin on its own.\r\n\r\nProject Page: <https://genesis-embodied-ai.github.io/>\r\n\r\n## Key Features\r\n\r\n- **Speed**: Over 43 million FPS when simulating a Franka robotic arm with a single RTX 4090 (430,000 times faster than real-time).\r\n- **Cross-platform**: Runs on Linux, macOS, Windows, and supports multiple compute backends (CPU, Nvidia/AMD GPUs, Apple Metal).\r\n- **Integration of diverse physics solvers**: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid.\r\n- **Wide range of material models**: Simulation and coupling of rigid bodies, liquids, gases, deformable objects, thin-shell objects, and granular materials.\r\n- **Compatibility with various robots**: Robotic arms, legged robots, drones, *soft robots*, and support for loading `MJCF (.xml)`, `URDF`, `.obj`, `.glb`, `.ply`, `.stl`, and more.\r\n- **Photo-realistic rendering**: Native ray-tracing-based rendering.\r\n- **Differentiability**: Genesis is designed to be fully differentiable. Currently, our MPM solver and Tool Solver support differentiability, with other solvers planned for future versions (starting with rigid & articulated body solver).\r\n- **Physics-based tactile simulation**: Differentiable [tactile sensor simulation](https://github.com/Genesis-Embodied-AI/DiffTactile) coming soon (expected in version 0.3.0).\r\n- **User-friendliness**: Designed for simplicity, with intuitive installation and APIs.\r\n\r\n## Quick Installation\r\n\r\nInstall **PyTorch** first following the [official instructions](https://pytorch.org/get-started/locally/).\r\n\r\nThen, install Genesis via PyPI:\r\n```bash\r\npip install genesis-world  # Requires Python >=3.9;\r\n```\r\n\r\nFor the latest version, clone the repository and install locally:\r\n\r\n```bash\r\ngit clone https://github.com/Genesis-Embodied-AI/Genesis.git\r\ncd Genesis\r\npip install -e .\r\n```\r\n\r\n## Docker\r\n\r\nIf you want to use Genesis from Docker, you can first build the Docker image as:\r\n\r\n```bash\r\ndocker build -t genesis -f docker/Dockerfile docker\r\n```\r\n\r\nThen you can run the examples inside the docker image (mounted to `/workspace/examples`):\r\n\r\n```bash\r\nxhost +local:root # Allow the container to access the display\r\n\r\ndocker run --gpus all --rm -it \\\r\n-e DISPLAY=$DISPLAY \\\r\n-v /tmp/.X11-unix/:/tmp/.X11-unix \\\r\n-v $PWD:/workspace \\\r\ngenesis\r\n```\r\n\r\n## Documentation\r\n\r\nComprehensive documentation is available in [English](https://genesis-world.readthedocs.io/en/latest/user_guide/index.html), [Chinese](https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html), and [Japanese](https://genesis-world.readthedocs.io/ja/latest/user_guide/index.html). This includes detailed installation steps, tutorials, and API references.\r\n\r\n## Contributing to Genesis\r\n\r\nThe Genesis project is an open and collaborative effort. We welcome all forms of contributions from the community, including:\r\n\r\n- **Pull requests** for new features or bug fixes.\r\n- **Bug reports** through GitHub Issues.\r\n- **Suggestions** to improve Genesis's usability.\r\n\r\nRefer to our [contribution guide](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/CONTRIBUTING.md) for more details.\r\n\r\n## Support\r\n\r\n- Report bugs or request features via GitHub [Issues](https://github.com/Genesis-Embodied-AI/Genesis/issues).\r\n- Join discussions or ask questions on GitHub [Discussions](https://github.com/Genesis-Embodied-AI/Genesis/discussions).\r\n\r\n## License and Acknowledgments\r\n\r\nThe Genesis source code is licensed under Apache 2.0.\r\n\r\nGenesis's development has been made possible thanks to these open-source projects:\r\n\r\n- [Taichi](https://github.com/taichi-dev/taichi): High-performance cross-platform compute backend. Kudos to the Taichi team for their technical support!\r\n- [FluidLab](https://github.com/zhouxian/FluidLab): Reference MPM solver implementation.\r\n- [SPH_Taichi](https://github.com/erizmr/SPH_Taichi): Reference SPH solver implementation.\r\n- [Ten Minute Physics](https://matthias-research.github.io/pages/tenMinutePhysics/index.html) and [PBF3D](https://github.com/WASD4959/PBF3D): Reference PBD solver implementations.\r\n- [MuJoCo](https://github.com/google-deepmind/mujoco): Reference for rigid body dynamics.\r\n- [libccd](https://github.com/danfis/libccd): Reference for collision detection.\r\n- [PyRender](https://github.com/mmatl/pyrender): Rasterization-based renderer.\r\n- [LuisaCompute](https://github.com/LuisaGroup/LuisaCompute) and [LuisaRender](https://github.com/LuisaGroup/LuisaRender): Ray-tracing DSL.\r\n\r\n## Associated Papers\r\n\r\nGenesis is a large scale effort that integrates state-of-the-art technologies of various existing and on-going research work into a single system. Here we include a non-exhaustive list of all the papers that contributed to the Genesis project in one way or another:\r\n\r\n- Xian, Zhou, et al. \"Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.\" arXiv preprint arXiv:2303.02346 (2023).\r\n- Xu, Zhenjia, et al. \"Roboninja: Learning an adaptive cutting policy for multi-material objects.\" arXiv preprint arXiv:2302.11553 (2023).\r\n- Wang, Yufei, et al. \"Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.\" arXiv preprint arXiv:2311.01455 (2023).\r\n- Wang, Tsun-Hsuan, et al. \"Softzoo: A soft robot co-design benchmark for locomotion in diverse environments.\" arXiv preprint arXiv:2303.09555 (2023).\r\n- Wang, Tsun-Hsuan Johnson, et al. \"Diffusebot: Breeding soft robots with physics-augmented generative diffusion models.\" Advances in Neural Information Processing Systems 36 (2023): 44398-44423.\r\n- Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. \"Gen2sim: Scaling up robot learning in simulation with generative models.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.\r\n- Si, Zilin, et al. \"DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation.\" arXiv preprint arXiv:2403.08716 (2024).\r\n- Wang, Yian, et al. \"Thin-Shell Object Manipulations With Differentiable Physics Simulations.\" arXiv preprint arXiv:2404.00451 (2024).\r\n- Lin, Chunru, et al. \"UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments.\" arXiv preprint arXiv:2411.12711 (2024).\r\n- Zhou, Wenyang, et al. \"EMDM: Efficient motion diffusion model for fast and high-quality motion generation.\" European Conference on Computer Vision. Springer, Cham, 2025.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Scalable differentiable physics for learning and control.\" International Conference on Machine Learning. PMLR, 2020.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Efficient differentiable simulation of articulated bodies.\" In International Conference on Machine Learning, PMLR, 2021.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. \"Differentiable simulation of soft multi-body systems.\" Advances in Neural Information Processing Systems 34 (2021).\r\n- Wan, Weilin, et al. \"Tlcontrol: Trajectory and language control for human motion synthesis.\" arXiv preprint arXiv:2311.17135 (2023).\r\n- Wang, Yian, et al. \"Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting.\" arXiv preprint arXiv:2411.09823 (2024).\r\n- Zheng, Shaokun, et al. \"LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures.\" ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.\r\n- Fan, Yingruo, et al. \"Faceformer: Speech-driven 3d facial animation with transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\r\n- Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. \"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE.\" Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.\r\n- Dou, Zhiyang, et al. \"C· ase: Learning conditional adversarial skill embeddings for physics-based characters.\" SIGGRAPH Asia 2023 Conference Papers. 2023.\r\n\r\n... and many more on-going work.\r\n\r\n## Citation\r\n\r\nIf you use Genesis in your research, please consider citing:\r\n\r\n```bibtex\r\n@software{Genesis,\r\n  author = {Genesis Authors},\r\n  title = {Genesis: A Universal and Generative Physics Engine for Robotics and Beyond},\r\n  month = {December},\r\n  year = {2024},\r\n  url = {https://github.com/Genesis-Embodied-AI/Genesis}\r\n}\r\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 10.61328125,
          "content": "![Genesis](imgs/big_text.png)\n\n![Teaser](imgs/teaser.png)\n\n[![PyPI - Version](https://img.shields.io/pypi/v/genesis-world)](https://pypi.org/project/genesis-world/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/genesis-world)](https://pypi.org/project/genesis-world/)\n[![GitHub Issues](https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/issues)\n[![GitHub Discussions](https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/discussions)\n[![Discord](https://img.shields.io/discord/1322086972302430269?logo=discord)](https://discord.gg/nukCuhB47p)\n<a href=\"https://drive.google.com/uc?export=view&id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ\"><img src=\"https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white\" height=\"20\" style=\"display:inline\"></a>\n\n[![README in English](https://img.shields.io/badge/English-d9d9d9)](./README.md)\n[![README en Français](https://img.shields.io/badge/Francais-d9d9d9)](./README_FR.md)\n[![한국어 README](https://img.shields.io/badge/한국어-d9d9d9)](./README_KR.md)\n[![简体中文版自述文件](https://img.shields.io/badge/简体中文-d9d9d9)](./README_CN.md)\n[![日本語版 README](https://img.shields.io/badge/日本語-d9d9d9)](./README_JA.md)\n\n# Genesis 通用物理引擎\n\n## 目录\n\n1. [概述](#概述)\n2. [主要特点](#主要特点)\n3. [快速入门](#快速入门)\n4. [参与贡献](#参与贡献)\n5. [帮助支持](#帮助支持)\n6. [许可证与致谢](#许可证和致谢)\n7. [相关论文](#genesis-背后的论文)\n8. [引用](#引用)\n\n## 概述\n\nGenesis 是专为 *机器人/嵌入式 AI/物理 AI* 应用设计的通用物理平台，集成了以下核心功能：\n\n- **通用物理引擎**: 从底层重建,支持多种材料和物理现象模拟\n- **机器人模拟平台**: 轻量、高速、Python友好的开发环境\n- **真实感渲染**: 内置光线追踪渲染系统\n- **生成数据引擎**: 自然语言驱动的多模态数据生成\n\n我们的长期使命:\n\n- 降低物理模拟使用门槛\n- 统一各类物理求解器\n- 实现数据生成自动化\n\n项目主页: <https://genesis-embodied-ai.github.io/>\n\n## 主要特点\n\n- **速度**：Genesis 提供了前所未有的模拟速度——在单个 RTX 4090 上模拟 Franka 机器人手臂时超过 4300 万 FPS（比实时快 430,000 倍）。\n- **跨平台**：Genesis 原生运行在不同系统（Linux、MacOS、Windows）和不同计算后端（CPU、Nvidia GPU、AMD GPU、Apple Metal）上。\n- **各种物理求解器的统一**：Genesis 开发了一个统一的模拟框架，集成了各种物理求解器：刚体、MPM、SPH、FEM、PBD、稳定流体。\n- **支持广泛的材料模型**：Genesis 支持刚体和关节体、各种液体、气体现象、可变形物体、薄壳物体和颗粒材料的模拟（及其耦合）。\n- **支持广泛的机器人**：机器人手臂、腿式机器人、无人机、*软体机器人*等，并广泛支持加载不同文件类型：`MJCF (.xml)`、`URDF`、`.obj`、`.glb`、`.ply`、`.stl` 等。\n- **照片级真实感和高性能光线追踪器**：Genesis 支持基于光线追踪的原生渲染。\n- **可微分性**：Genesis 设计为完全兼容可微分模拟。目前，我们的 MPM 求解器和工具求解器是可微分的，其他求解器的可微分性将很快添加（从刚体模拟开始）。\n- **基于物理的触觉传感器**：Genesis 包含一个基于物理的可微分 [触觉传感器模拟模块](https://github.com/Genesis-Embodied-AI/DiffTactile)。这将很快集成到公共版本中（预计在 0.3.0 版本中）。\n- **用户友好性**：Genesis 设计为尽可能简化模拟的使用。从安装到 API 设计，如果有任何您觉得不直观或难以使用的地方，请 [告诉我们](https://github.com/Genesis-Embodied-AI/Genesis/issues)。\n\n## 快速入门\n\n### 安装\n首先按照[官方指南](https://pytorch.org/get-started/locally/)安装 PyTorch。\n\n然后可通过 PyPI 安装Genesis：\n```bash\npip install genesis-world  # 需要 Python >=3.9\n```\n\n### Docker 支持\n\n如果您想通过 Docker 使用 Genesis，您可以首先构建 Docker 镜像，命令如下：\n\n```bash\ndocker build -t genesis -f docker/Dockerfile docker\n```\n\n然后，您可以在 Docker 镜像内运行示例代码（挂载到 `/workspace/examples`）：\n\n```bash\nxhost +local:root # 允许容器访问显示器\n\ndocker run --gpus all --rm -it \\\n-e DISPLAY=$DISPLAY \\\n-v /tmp/.X11-unix/:/tmp/.X11-unix \\\n-v $PWD:/workspace \\\ngenesis\n```\n\n### 文档\n\n- [英文文档](https://genesis-world.readthedocs.io/en/latest/user_guide/index.html)\n- [中文文档](https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html)\n- [日文文档](https://genesis-world.readthedocs.io/ja/latest/user_guide/index.html)\n\n## 参与贡献\n\nGenesis 项目的目标是构建一个完全透明、用户友好的生态系统，让来自机器人和计算机图形学的贡献者 **共同创建一个高效、真实（物理和视觉上）的虚拟世界，用于机器人研究及其他领域**。\n\n我们真诚地欢迎来自社区的 *任何形式的贡献*，以使世界对机器人更友好。从 **新功能的拉取请求**、**错误报告**，到甚至是使 Genesis API 更直观的微小 **建议**，我们都全心全意地感谢！\n\n## 帮助支持\n\n- 请使用 Github [Issues](https://github.com/Genesis-Embodied-AI/Genesis/issues) 报告错误和提出功能请求。\n\n- 请使用 GitHub [Discussions](https://github.com/Genesis-Embodied-AI/Genesis/discussions) 讨论想法和提问。\n\n## 许可证和致谢\n\nGenesis 源代码根据 Apache 2.0 许可证授权。\n没有这些令人惊叹的开源项目，Genesis 的开发是不可能的：\n\n- [Taichi](https://github.com/taichi-dev/taichi)：提供高性能跨平台计算后端。感谢 taichi 的所有成员提供的技术支持！\n- [FluidLab](https://github.com/zhouxian/FluidLab) 提供参考 MPM 求解器实现\n- [SPH_Taichi](https://github.com/erizmr/SPH_Taichi) 提供参考 SPH 求解器实现\n- [Ten Minute Physics](https://matthias-research.github.io/pages/tenMinutePhysics/index.html) 和 [PBF3D](https://github.com/WASD4959/PBF3D) 提供参考 PBD 求解器实现\n- [MuJoCo](https://github.com/google-deepmind/mujoco) 和 [Brax](https://github.com/google/brax) 提供刚体动力学参考\n- [libccd](https://github.com/danfis/libccd) 提供碰撞检测参考\n- [PyRender](https://github.com/mmatl/pyrender) 提供基于光栅化的渲染器\n- [LuisaCompute](https://github.com/LuisaGroup/LuisaCompute) 和 [LuisaRender](https://github.com/LuisaGroup/LuisaRender) 提供其光线追踪 DSL\n- [trimesh](https://github.com/mikedh/trimesh)、[PyMeshLab](https://github.com/cnr-isti-vclab/PyMeshLab) 和 [CoACD](https://github.com/SarahWeiii/CoACD) 提供几何处理\n\n## Genesis 背后的论文\n\nGenesis 是一个大规模的努力，将各种现有和正在进行的研究工作的最先进技术集成到一个系统中。这里我们列出了一些对 Genesis 项目有贡献的论文（非详尽列表）：\n\n- Xian, Zhou, et al. \"Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.\" arXiv preprint arXiv:2303.02346 (2023).\n- Xu, Zhenjia, et al. \"Roboninja: Learning an adaptive cutting policy for multi-material objects.\" arXiv preprint arXiv:2302.11553 (2023).\n- Wang, Yufei, et al. \"Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.\" arXiv preprint arXiv:2311.01455 (2023).\n- Wang, Tsun-Hsuan, et al. \"Softzoo: A soft robot co-design benchmark for locomotion in diverse environments.\" arXiv preprint arXiv:2303.09555 (2023).\n- Wang, Tsun-Hsuan Johnson, et al. \"Diffusebot: Breeding soft robots with physics-augmented generative diffusion models.\" Advances in Neural Information Processing Systems 36 (2023): 44398-44423.\n- Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. \"Gen2sim: Scaling up robot learning in simulation with generative models.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.\n- Si, Zilin, et al. \"DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation.\" arXiv preprint arXiv:2403.08716 (2024).\n- Wang, Yian, et al. \"Thin-Shell Object Manipulations With Differentiable Physics Simulations.\" arXiv preprint arXiv:2404.00451 (2024).\n- Lin, Chunru, et al. \"UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments.\" arXiv preprint arXiv:2411.12711 (2024).\n- Zhou, Wenyang, et al. \"EMDM: Efficient motion diffusion model for fast and high-quality motion generation.\" European Conference on Computer Vision. Springer, Cham, 2025.\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Scalable differentiable physics for learning and control.\" International Conference on Machine Learning. PMLR, 2020.\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Efficient differentiable simulation of articulated bodies.\" In International Conference on Machine Learning, PMLR, 2021.\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. \"Differentiable simulation of soft multi-body systems.\" Advances in Neural Information Processing Systems 34 (2021).\n- Wan, Weilin, et al. \"Tlcontrol: Trajectory and language control for human motion synthesis.\" arXiv preprint arXiv:2311.17135 (2023).\n- Wang, Yian, et al. \"Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting.\" arXiv preprint arXiv:2411.09823 (2024).\n- Zheng, Shaokun, et al. \"LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures.\" ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.\n- Fan, Yingruo, et al. \"Faceformer: Speech-driven 3d facial animation with transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n- Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. \"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE.\" Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.\n- Dou, Zhiyang, et al. \"C· ase: Learning conditional adversarial skill embeddings for physics-based characters.\" SIGGRAPH Asia 2023 Conference Papers. 2023.\n\n... 以及许多正在进行的工作。\n\n## 引用\n\n如果您在研究中使用了 Genesis，我们将非常感谢您引用它。我们仍在撰写技术报告，在其公开之前，您可以考虑引用：\n\n```bibtex\n@software{Genesis,\n  author = {Genesis Authors},\n  title = {Genesis: A Universal and Generative Physics Engine for Robotics and Beyond},\n  month = {December},\n  year = {2024},\n  url = {https://github.com/Genesis-Embodied-AI/Genesis}\n}\n```\n"
        },
        {
          "name": "README_FR.md",
          "type": "blob",
          "size": 11.8828125,
          "content": "![Genesis](imgs/big_text.png)\r\n\r\n![Teaser](imgs/teaser.png)\r\n\r\n[![PyPI - Version](https://img.shields.io/pypi/v/genesis-world)](https://pypi.org/project/genesis-world/)\r\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/genesis-world)](https://pypi.org/project/genesis-world/)\r\n[![GitHub Issues](https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/issues)\r\n[![GitHub Discussions](https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/discussions)\r\n[![Discord](https://img.shields.io/discord/1322086972302430269?logo=discord)](https://discord.gg/nukCuhB47p)\r\n<a href=\"https://drive.google.com/uc?export=view&id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ\"><img src=\"https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white\" height=\"20\" style=\"display:inline\"></a>\r\n\r\n[![README in English](https://img.shields.io/badge/English-d9d9d9)](./README.md)\r\n[![README en Français](https://img.shields.io/badge/Francais-d9d9d9)](./README_FR.md)\r\n[![한국어 README](https://img.shields.io/badge/한국어-d9d9d9)](./README_KR.md)\r\n[![简体中文版自述文件](https://img.shields.io/badge/简体中文-d9d9d9)](./README_CN.md)\r\n[![日本語版 README](https://img.shields.io/badge/日本語-d9d9d9)](./README_JA.md)\r\n\r\n# Genesis\r\n\r\n## 🔥 Nouveautés\r\n\r\n- [2024-12-25] Ajout d’un [docker](#docker) incluant la prise en charge du moteur de rendu par ray-tracing.\r\n- [2024-12-24] Ajout de directives pour [contribuer à Genesis](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/CONTRIBUTING.md).\r\n\r\n## Table des Matières\r\n\r\n1. [Qu'est-ce que Genesis ?](#quest-ce-que-genesis-)\r\n2. [Caractéristiques clés](#principales-caract%C3%A9ristiques)\r\n3. [Installation Rapide](#installation-rapide)\r\n4. [Docker](#docker)\r\n5. [Documentation](#documentation)\r\n6. [Contribuer à Genesis](#contribution-%C3%A0-genesis)\r\n7. [Support](#support)\r\n8. [License et Remerciements](#licence-et-remerciements)\r\n9. [Articles Associés](#publications-associ%C3%A9es)\r\n10. [Citation](#citation)\r\n\r\n## Qu'est-ce que Genesis ?\r\n\r\nGenesis est une plateforme physique conçue pour des applications générales en *Robotique/ IA embarquée/IA physique*. Elle combine plusieurs fonctionnalités :\r\n\r\n1. Un **moteur physique universel**, reconstruit depuis zéro, capable de simuler une large gamme de matériaux et de phénomènes physiques.\r\n2. Une plateforme de simulation robotique **légère**, **ultra-rapide**,**pythonic**, et **conviviale**.\r\n3. Un puissant et rapide **système de rendu photo-réaliste**.\r\n4. Un **moteur de génération de données** qui transforme des descriptions en langage naturel en divers types de données.\r\n\r\nGenesis vise à :\r\n\r\n- **Réduire les barrières** à l'utilisation des simulations physiques, rendant la recherche en robotique accessible à tous. Voir notre [déclaration de mission](https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html).\r\n- **Unifier divers solveurs physiques** dans un cadre unique pour recréer le monde physique avec la plus haute fidélité.\r\n- **Automatiser la génération de données**, réduisant l'effort humain et permettant à l'écosystème de données de fonctionner de manière autonome.\r\n\r\nPage du projet : <https://genesis-embodied-ai.github.io/>\r\n\r\n## Principales Caractéristiques\r\n\r\n- **Vitesse** : Plus de 43 millions d'IPS lors de la simulation d'un bras robotique Franka avec une seule RTX 4090 (430 000 fois plus rapide que le temps réel).\r\n- **Multi-plateforme** : Fonctionne sur Linux, macOS, Windows, et prend en charge plusieurs backends de calcul (CPU, GPU Nvidia/AMD, Apple Metal).\r\n- **Intégration de divers solveurs physiques** : Corps rigides, MPM, SPH, FEM, PBD, Fluides stables.\r\n- **Large éventail de modèles de matériaux** : Simulation et couplage de corps rigides, liquides, gaz, objets déformables, objets à coque mince et matériaux granulaires.\r\n- **Compatibilité avec divers robots** : Bras robotiques, robots à pattes, drones, *robots mous*, et support pour charger `MJCF (.xml)`, `URDF`, `.obj`, `.glb`, `.ply`, `.stl`, et plus encore.\r\n- **Rendu photo-réaliste** : Rendu natif basé sur le lancer de rayons.\r\n- **Différentiabilité** : Genesis est conçu pour être entièrement différentiable. Actuellement, notre solveur MPM et Tool Solver prennent en charge la différentiabilité, avec d'autres solveurs prévus dans les prochaines versions (à commencer par le solveur de corps rigides et articulés).\r\n- **Simulation tactile basée sur la physique** : Simulation de capteur tactile différentiable [en cours de développement](https://github.com/Genesis-Embodied-AI/DiffTactile) (prévue pour la version 0.3.0).\r\n- **Facilité d'utilisation** : Conçu pour être simple, avec une installation intuitive et des API conviviales.\r\n\r\n## Installation Rapide\r\n\r\nGenesis est disponible via PyPI :\r\n\r\n```bash\r\npip install genesis-world  # Nécessite Python >=3.9;\r\n```\r\n\r\nVous devez également installer **PyTorch** en suivant [les instructions officielles](https://pytorch.org/get-started/locally/).\r\n\r\nPour la dernière version, clonez le dépôt et installez localement :\r\n\r\n```bash\r\ngit clone https://github.com/Genesis-Embodied-AI/Genesis.git\r\ncd Genesis\r\npip install -e .\r\n```\r\n\r\n## Docker\r\n\r\nSi vous souhaitez utiliser Genesis avec Docker, vous pouvez d'abord construire l'image Docker comme suit :\r\n\r\n```bash\r\ndocker build -t genesis -f docker/Dockerfile docker\r\n```\r\n\r\nEnsuite, vous pouvez exécuter les exemples à l'intérieur de l'image Docker (montés sur `/workspace/examples`) :\r\n\r\n```bash\r\nxhost +local:root # Autoriser le conteneur à accéder à l'affichage\r\n\r\ndocker run --gpus all --rm -it \\\r\n-e DISPLAY=$DISPLAY \\\r\n-v /tmp/.X11-unix/:/tmp/.X11-unix \\\r\n-v $PWD:/workspace \\\r\ngenesis\r\n\r\n```\r\n\r\n## Documentation\r\n\r\nUne documentation complète est disponible en [Anglais](https://genesis-world.readthedocs.io/en/latest/user_guide/index.html) et en [Chinois](https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html). Cela inclut des étapes d'installation détaillées, des tutoriels et des références API.\r\n\r\n## Contribution à Genesis\r\n\r\nLe projet Genesis est un effort ouvert et collaboratif. Nous accueillons toutes les formes de contributions de la communauté, notamment :\r\n\r\n- **Pull requests** pour de nouvelles fonctionnalités ou des corrections de bugs.\r\n- **Rapports de bugs** via GitHub Issues.\r\n- **Suggestions** pour améliorer la convivialité de Genesis.\r\n\r\nConsultez notre [guide de contribution](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/CONTRIBUTING.md) pour plus de détails.\r\n\r\n## Support\r\n\r\n- Signalez des bugs ou demandez des fonctionnalités via GitHub [Issues](https://github.com/Genesis-Embodied-AI/Genesis/issues).\r\n- Participez aux discussions ou posez des questions sur GitHub [Discussions](https://github.com/Genesis-Embodied-AI/Genesis/discussions).\r\n\r\n## Licence et Remerciements\r\n\r\nLe code source de Genesis est sous licence Apache 2.0.\r\n\r\nLe développement de Genesis a été rendu possible grâce à ces projets open-source :\r\n\r\n- [Taichi](https://github.com/taichi-dev/taichi) : Backend de calcul multiplateforme haute performance. Merci à l'équipe de Taichi pour leur support technique !\r\n- [FluidLab](https://github.com/zhouxian/FluidLab) : Implémentation de référence du solveur MPM.\r\n- [SPH_Taichi](https://github.com/erizmr/SPH_Taichi) : Implémentation de référence du solveur SPH.\r\n- [Ten Minute Physics](https://matthias-research.github.io/pages/tenMinutePhysics/index.html) et [PBF3D](https://github.com/WASD4959/PBF3D) : Implémentations de référence des solveurs PBD.\r\n- [MuJoCo](https://github.com/google-deepmind/mujoco) : Référence pour la dynamique des corps rigides.\r\n- [libccd](https://github.com/danfis/libccd) : Référence pour la détection des collisions.\r\n- [PyRender](https://github.com/mmatl/pyrender) : Rendu basé sur la rasterisation.\r\n- [LuisaCompute](https://github.com/LuisaGroup/LuisaCompute) et [LuisaRender](https://github.com/LuisaGroup/LuisaRender) : DSL de ray-tracing.\r\n\r\n## Publications Associées\r\n\r\nGenesis est un projet à grande échelle qui intègre des technologies de pointe issues de divers travaux de recherche existants et en cours dans un seul système. Voici une liste non exhaustive de toutes les publications qui ont contribué au projet Genesis d'une manière ou d'une autre :\r\n\r\n- Xian, Zhou, et al. \"Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.\" arXiv preprint arXiv:2303.02346 (2023).\r\n- Xu, Zhenjia, et al. \"Roboninja: Learning an adaptive cutting policy for multi-material objects.\" arXiv preprint arXiv:2302.11553 (2023).\r\n- Wang, Yufei, et al. \"Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.\" arXiv preprint arXiv:2311.01455 (2023).\r\n- Wang, Tsun-Hsuan, et al. \"Softzoo: A soft robot co-design benchmark for locomotion in diverse environments.\" arXiv preprint arXiv:2303.09555 (2023).\r\n- Wang, Tsun-Hsuan Johnson, et al. \"Diffusebot: Breeding soft robots with physics-augmented generative diffusion models.\" Advances in Neural Information Processing Systems 36 (2023): 44398-44423.\r\n- Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. \"Gen2sim: Scaling up robot learning in simulation with generative models.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.\r\n- Si, Zilin, et al. \"DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation.\" arXiv preprint arXiv:2403.08716 (2024).\r\n- Wang, Yian, et al. \"Thin-Shell Object Manipulations With Differentiable Physics Simulations.\" arXiv preprint arXiv:2404.00451 (2024).\r\n- Lin, Chunru, et al. \"UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments.\" arXiv preprint arXiv:2411.12711 (2024).\r\n- Zhou, Wenyang, et al. \"EMDM: Efficient motion diffusion model for fast and high-quality motion generation.\" European Conference on Computer Vision. Springer, Cham, 2025.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Scalable differentiable physics for learning and control.\" International Conference on Machine Learning. PMLR, 2020.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Efficient differentiable simulation of articulated bodies.\" In International Conference on Machine Learning, PMLR, 2021.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. \"Differentiable simulation of soft multi-body systems.\" Advances in Neural Information Processing Systems 34 (2021).\r\n- Wan, Weilin, et al. \"Tlcontrol: Trajectory and language control for human motion synthesis.\" arXiv preprint arXiv:2311.17135 (2023).\r\n- Wang, Yian, et al. \"Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting.\" arXiv preprint arXiv:2411.09823 (2024).\r\n- Zheng, Shaokun, et al. \"LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures.\" ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.\r\n- Fan, Yingruo, et al. \"Faceformer: Speech-driven 3d facial animation with transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\r\n- Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. \"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE.\" Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.\r\n- Dou, Zhiyang, et al. \"C· ase: Learning conditional adversarial skill embeddings for physics-based characters.\" SIGGRAPH Asia 2023 Conference Papers. 2023.\r\n\r\n... et bien d'autres travaux en cours.\r\n\r\n## Citation\r\n\r\nSi vous utilisez Genesis dans vos recherches, veuillez envisager de citer :\r\n\r\n```bibtex\r\n@software{Genesis,\r\n  author = {Genesis Authors},\r\n  title = {Genesis: A Universal and Generative Physics Engine for Robotics and Beyond},\r\n  month = {December},\r\n  year = {2024},\r\n  url = {https://github.com/Genesis-Embodied-AI/Genesis}\r\n}\r\n```\r\n"
        },
        {
          "name": "README_JA.md",
          "type": "blob",
          "size": 12.1484375,
          "content": "![Genesis](imgs/big_text.png)\r\n\r\n![Teaser](imgs/teaser.png)\r\n\r\n[![PyPI - Version](https://img.shields.io/pypi/v/genesis-world)](https://pypi.org/project/genesis-world/)\r\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/genesis-world)](https://pypi.org/project/genesis-world/)\r\n[![GitHub Issues](https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/issues)\r\n[![GitHub Discussions](https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/discussions)\r\n[![Discord](https://img.shields.io/discord/1322086972302430269?logo=discord)](https://discord.gg/nukCuhB47p)\r\n<a href=\"https://drive.google.com/uc?export=view&id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ\"><img src=\"https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white\" height=\"20\" style=\"display:inline\"></a>\r\n\r\n[![README in English](https://img.shields.io/badge/English-d9d9d9)](./README.md)\r\n[![README en Français](https://img.shields.io/badge/Francais-d9d9d9)](./README_FR.md)\r\n[![한국어 README](https://img.shields.io/badge/한국어-d9d9d9)](./README_KR.md)\r\n[![简体中文版自述文件](https://img.shields.io/badge/简体中文-d9d9d9)](./README_CN.md)\r\n[![日本語版 README](https://img.shields.io/badge/日本語-d9d9d9)](./README_JA.md)\r\n\r\n# Genesis\r\n\r\n## 🔥 最新情報\r\n\r\n- [2024-12-25] [レイトレーシングレンダラー](#docker)をサポートするDockerを追加しました。\r\n- [2024-12-24] [Genesisへの貢献方法](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/CONTRIBUTING.md)に関するガイドラインを追加しました。\r\n\r\n## 目次\r\n\r\n1. [Genesisとは？](#genesisとは)\r\n2. [主な機能](#主な機能)\r\n3. [インストール](#インストール)\r\n4. [Docker](#docker)\r\n5. [ドキュメント](#ドキュメント)\r\n6. [Genesisへの貢献](#genesisへの貢献)\r\n7. [サポート](#サポート)\r\n8. [ライセンスと謝辞](#ライセンスと謝辞)\r\n9. [関連論文](#関連論文)\r\n10. [引用](#引用)\r\n\r\n## Genesisとは？\r\n\r\nGenesisは、汎用的な*ロボティクス/身体性を持ったAI*アプリケーション向けに設計された物理シミュレーションプラットフォームです。このプラットフォームは以下のような特徴があります：\r\n\r\n1. あらゆる種類の材料や物理現象をシミュレート可能な**汎用物理エンジン**。\r\n2. **軽量**、**超高速**、**Python的**、そして**ユーザーフレンドリー**なロボティクスシミュレーションプラットフォーム。\r\n3. 高速で強力な**フォトリアリスティックなレンダリングシステム**。\r\n4. ユーザーの自然言語による指示をもとに様々なデータモダリティを生成する**生成型データエンジン**。\r\n\r\nGenesisの目指すところ：\r\n\r\n- **物理シミュレーションのハードルを下げ**、ロボティクス研究を誰でもアクセス可能にすること。詳細は[ミッションステートメント](https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html)をご覧ください。\r\n- **多様な物理ソルバーを統合**し、最高の忠実度で物理世界を再現すること。\r\n- **データ生成を自動化**し、人間の労力を削減し、データ生成の効率を最大化すること。\r\n\r\nプロジェクトページ: <https://genesis-embodied-ai.github.io/>\r\n\r\n## 主な機能\r\n\r\n- **速度**: RTX 4090単体でフランカロボットアームを4300万FPS（リアルタイムの43万倍速）でシミュレーション可能。\r\n- **クロスプラットフォーム**: Linux、macOS、Windowsで動作し、CPU、Nvidia/AMD GPU、Apple Metalをサポート。\r\n- **多様な物理ソルバーの統合**: 剛体、MPM、SPH、FEM、PBD、安定流体シミュレーション。\r\n- **幅広い材料モデル**: 剛体、液体、気体、変形体、薄膜オブジェクト、粒状材料などをシミュレーション可能。\r\n- **様々なロボットへの対応**: ロボットアーム、脚付きロボット、ドローン、*ソフトロボット*など。また、`MJCF (.xml)`、`URDF`、`.obj`、`.glb`、`.ply`、`.stl`などの形式をサポート。\r\n- **フォトリアルなレンダリング**: レイトレーシングベースのレンダリングをネイティブでサポート。\r\n- **微分可能性**: 完全な微分可能性を備えた設計。現時点では、MPMソルバーとツールソルバーが対応しており、将来的には他のソルバーも対応予定（まず剛体および連結体ソルバーから開始）。\r\n- **物理ベースの触覚シミュレーション**: 微分可能な[触覚センサーシミュレーション](https://github.com/Genesis-Embodied-AI/DiffTactile)が近日公開予定（バージョン0.3.0を予定）。\r\n- **ユーザーフレンドリー**: シンプルで直感的なインストールとAPI設計。\r\n\r\n## インストール\r\n\r\nGenesisはPyPIで利用可能です：\r\n\r\n```bash\r\npip install genesis-world  # Python >=3.9 が必要です;\r\n```\r\n\r\nまた、**PyTorch**を[公式手順](https://pytorch.org/get-started/locally/)に従ってインストールする必要があります。\r\n\r\n最新バージョンを利用するには、リポジトリをクローンしてローカルにインストールしてください：\r\n\r\n```bash\r\ngit clone https://github.com/Genesis-Embodied-AI/Genesis.git\r\ncd Genesis\r\npip install -e .\r\n```\r\n\r\n## Docker\r\n\r\nDockerからGenesisを利用する場合は、まずDockerイメージをビルドします：\r\n\r\n```bash\r\ndocker build -t genesis -f docker/Dockerfile docker\r\n```\r\n\r\nその後、Dockerイメージ内で例を実行できます（`/workspace/examples`にマウント）：\r\n\r\n```bash\r\nxhost +local:root # コンテナがディスプレイにアクセスできるようにする\r\n\r\ndocker run --gpus all --rm -it \\\r\n-e DISPLAY=$DISPLAY \\\r\n-v /tmp/.X11-unix/:/tmp/.X11-unix \\\r\n-v $PWD:/workspace \\\r\ngenesis\r\n```\r\n\r\n## ドキュメント\r\n\r\n包括的なドキュメントは現時点では[英語](https://genesis-world.readthedocs.io/en/latest/user_guide/index.html)、[中国語](https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html)、および[日本語](https://genesis-world.readthedocs.io/ja/latest/user_guide/index.html)で提供されています。詳細なインストール手順、チュートリアル、APIリファレンスが含まれています。\r\n\r\n## Genesisへの貢献\r\n\r\nGenesisプロジェクトはオープンで協力的な取り組みです。以下を含む、コミュニティからのあらゆる貢献を歓迎します：\r\n\r\n- 新機能やバグ修正のための**プルリクエスト**。\r\n- GitHub Issuesを通じた**バグ報告**。\r\n- Genesisの使いやすさを向上させるための**提案**。\r\n\r\n詳細は[貢献ガイド](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/CONTRIBUTING.md)をご参照ください。\r\n\r\n## サポート\r\n\r\n- バグ報告や機能リクエストはGitHubの[Issues](https://github.com/Genesis-Embodied-AI/Genesis/issues)をご利用ください。\r\n- 議論や質問はGitHubの[Discussions](https://github.com/Genesis-Embodied-AI/Genesis/discussions)で行えます。\r\n\r\n## ライセンスと謝辞\r\n\r\nGenesisのソースコードはApache 2.0ライセンスで提供されています。\r\n\r\nGenesisの開発は以下のオープンソースプロジェクトのおかげで可能になりました：\r\n\r\n- [Taichi](https://github.com/taichi-dev/taichi): 高性能でクロスプラットフォーム対応の計算バックエンド。Taichiチームの技術サポートに感謝します！\r\n- [FluidLab](https://github.com/zhouxian/FluidLab): 参照用のMPMソルバー実装。\r\n- [SPH_Taichi](https://github.com/erizmr/SPH_Taichi): 参照用のSPHソルバー実装。\r\n- [Ten Minute Physics](https://matthias-research.github.io/pages/tenMinutePhysics/index.html) と [PBF3D](https://github.com/WASD4959/PBF3D): 参照用のPBD（粒子ベースの物理）ソルバー実装。\r\n- [MuJoCo](https://github.com/google-deepmind/mujoco): 剛体ダイナミクスの参照用実装。\r\n- [libccd](https://github.com/danfis/libccd): 衝突検出の参照用実装。\r\n- [PyRender](https://github.com/mmatl/pyrender): ラスタライズベースのレンダラー。\r\n- [LuisaCompute](https://github.com/LuisaGroup/LuisaCompute) と [LuisaRender](https://github.com/LuisaGroup/LuisaRender): レイトレーシングDSL。\r\n\r\n## 関連論文\r\n\r\nGenesisプロジェクトに関与した主要な研究論文の一覧：\r\n\r\n- Xian, Zhou, et al. \"Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.\" arXiv preprint arXiv:2303.02346 (2023).\r\n- Xu, Zhenjia, et al. \"Roboninja: Learning an adaptive cutting policy for multi-material objects.\" arXiv preprint arXiv:2302.11553 (2023).\r\n- Wang, Yufei, et al. \"Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.\" arXiv preprint arXiv:2311.01455 (2023).\r\n- Wang, Tsun-Hsuan, et al. \"Softzoo: A soft robot co-design benchmark for locomotion in diverse environments.\" arXiv preprint arXiv:2303.09555 (2023).\r\n- Wang, Tsun-Hsuan Johnson, et al. \"Diffusebot: Breeding soft robots with physics-augmented generative diffusion models.\" Advances in Neural Information Processing Systems 36 (2023): 44398-44423.\r\n- Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. \"Gen2sim: Scaling up robot learning in simulation with generative models.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.\r\n- Si, Zilin, et al. \"DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation.\" arXiv preprint arXiv:2403.08716 (2024).\r\n- Wang, Yian, et al. \"Thin-Shell Object Manipulations With Differentiable Physics Simulations.\" arXiv preprint arXiv:2404.00451 (2024).\r\n- Lin, Chunru, et al. \"UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments.\" arXiv preprint arXiv:2411.12711 (2024).\r\n- Zhou, Wenyang, et al. \"EMDM: Efficient motion diffusion model for fast and high-quality motion generation.\" European Conference on Computer Vision. Springer, Cham, 2025.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Scalable differentiable physics for learning and control.\" International Conference on Machine Learning. PMLR, 2020.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Efficient differentiable simulation of articulated bodies.\" In International Conference on Machine Learning, PMLR, 2021.\r\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. \"Differentiable simulation of soft multi-body systems.\" Advances in Neural Information Processing Systems 34 (2021).\r\n- Wan, Weilin, et al. \"Tlcontrol: Trajectory and language control for human motion synthesis.\" arXiv preprint arXiv:2311.17135 (2023).\r\n- Wang, Yian, et al. \"Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting.\" arXiv preprint arXiv:2411.09823 (2024).\r\n- Zheng, Shaokun, et al. \"LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures.\" ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.\r\n- Fan, Yingruo, et al. \"Faceformer: Speech-driven 3d facial animation with transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\r\n- Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. \"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE.\" Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.\r\n- Dou, Zhiyang, et al. \"C· ase: Learning conditional adversarial skill embeddings for physics-based characters.\" SIGGRAPH Asia 2023 Conference Papers. 2023.\r\n\r\nさらに多数の現在進行形のプロジェクトがあります。\r\n\r\n## 引用\r\n\r\n研究でGenesisを使用する場合、以下を引用してください：\r\n\r\n```bibtex\r\n@software{Genesis,\r\n  author = {Genesis Authors},\r\n  title = {Genesis: A Universal and Generative Physics Engine for Robotics and Beyond},\r\n  month = {December},\r\n  year = {2024},\r\n  url = {https://github.com/Genesis-Embodied-AI/Genesis}\r\n}\r\n```\r\n"
        },
        {
          "name": "README_KR.md",
          "type": "blob",
          "size": 11.30078125,
          "content": "![Genesis](imgs/big_text.png)\n\n![Teaser](imgs/teaser.png)\n\n[![PyPI - Version](https://img.shields.io/pypi/v/genesis-world)](https://pypi.org/project/genesis-world/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/genesis-world)](https://pypi.org/project/genesis-world/)\n[![GitHub Issues](https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/issues)\n[![GitHub Discussions](https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/discussions)\n[![Discord](https://img.shields.io/discord/1322086972302430269?logo=discord)](https://discord.gg/nukCuhB47p)\n<a href=\"https://drive.google.com/uc?export=view&id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ\"><img src=\"https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white\" height=\"20\" style=\"display:inline\"></a>\n\n[![README in English](https://img.shields.io/badge/English-d9d9d9)](./README.md)\n[![README en Français](https://img.shields.io/badge/Francais-d9d9d9)](./README_FR.md)\n[![한국어 README](https://img.shields.io/badge/한국어-d9d9d9)](./README_KR.md)\n[![简体中文版自述文件](https://img.shields.io/badge/简体中文-d9d9d9)](./README_CN.md)\n[![日本語版 README](https://img.shields.io/badge/日本語-d9d9d9)](./README_JA.md)\n\n# Genesis\n\n## 🔥 새 소식\n\n- [2024-12-25] 레이 트레이싱 렌더러를 지원하는 [docker](#docker) 추가\n- [2024-12-24] [제네시스 기여](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/CONTRIBUTING.md) 가이드라인 추가\n\n## Table of Contents\n\n1. [Genesis란?](#genesis란)\n2. [주요 특징](#주요-특징)\n3. [빠른 설치](#빠른-설치)\n4. [Docker](#docker)\n5. [문서](#문서)\n6. [Genesis에 기여하기](#genesis에-기여하기)\n7. [지원](#지원)\n8. [라이선스 및 감사의 글](#라이선스-및-감사의-글)\n9. [관련 논문](#관련-논문)\n10. [인용](#인용)\n\n## Genesis란?\n\nGenesis는 *로보틱스/임베디드 AI/물리 AI* 애플리케이션을 위해 설계된 범용 물리 플랫폼입니다. 그리고 다음과 같은 기능을 제공합니다:\n\n1. 폭넓은 재료와 물리 현상을 시뮬레이션할 수 있도록 처음부터 다시 구축된 **범용 물리 엔진**.\n2. **가볍고**, **매우 빠르며**, **파이썬 친화적이고**, **사용자 친화적인** 로보틱스 시뮬레이션 플랫폼.\n3. 강력하고 빠른 **실사 렌더링 시스템**.\n4. 사용자의 자연어 설명을 다양한 형태의 데이터로 변환하는 **생성형 데이터 엔진**.\n\nGenesis의 목표:\n\n- **물리 시뮬레이션의 진입 장벽을 낮춰** 누구나 로보틱스 연구에 접근할 수 있도록 합니다. [사명 선언문](https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html)을 확인하세요.\n- 단일 프레임워크로 **통합된 다양한 물리 솔버**를 통해 최상의 정확도로 물리적 세계를 재현합니다.\n- **데이터 생성을 자동화**하여 사람의 수고를 줄이고 데이터 플라이휠이 스스로 순환하도록 합니다.\n\n프로젝트 페이지: <https://genesis-embodied-ai.github.io/>\n\n## 주요 특징\n\n- **속도**: 단일 RTX 4090에서 Franka 로봇 팔을 시뮬레이션하는 경우, 4300만 FPS 이상(실시간보다 430,000 배 빠름).\n- **크로스 플랫폼**: Linux, macOS, Windows에서 실행 가능 및 다양한 연산 백엔드(CPU, Nvidia/AMD GPU, Apple Metal) 지원.\n- **다양한 물리 솔버 통합**: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid.\n- **폭넓은 재료 모델**: 강체, 액체, 기체, 변형 가능한 오브젝트, 얇은 쉘 오브젝트 및 입상 재료의 시뮬레이션 및 결합.\n- **다양한 로봇 호환성**: 로봇 팔, 보행 로봇, 드론, *소프트 로봇* 및 다양한 파일 형식(`MJCF (.xml)`, `URDF`, `.obj`, `.glb`, `.ply`, `.stl` 등) 로드 지원.\n- **실사 렌더링**: 네이티브 레이 트레이싱 기반 렌더링.\n- **미분 가능성**: Genesis는 완전히 미분 가능하도록 설계되었습니다. 현재 MPM 솔버와 Tool 솔버가 미분 가능하며, 다른 솔버는 향후 버전에서는 강체 및 관절체 솔버를 시작으로 다른 솔버들도 지원할 예정입니다.\n- **물리 기반 촉각 시뮬레이션**: 미분 가능한 [촉각 센서 시뮬레이션](https://github.com/Genesis-Embodied-AI/DiffTactile) 출시 예정(버전 0.3.0 예상).\n- **사용자 친화성**: 직관적인 설치 및 API로 간편하게 사용 가능.\n\n## 빠른 설치\n\nGenesis는 PyPI를 통해 설치할 수 있습니다:\n\n```bash\npip install genesis-world  # Python >=3.9 필요\n```\n\n또한, [공식 설명서](https://pytorch.org/get-started/locally/)에 따라 **PyTorch**를 설치해야 합니다.\n\n최신 버전을 사용하려면 저장소를 복제한 후 로컬에서 설치하세요:\n\n```bash\ngit clone https://github.com/Genesis-Embodied-AI/Genesis.git\ncd Genesis\npip install -e .\n```\n\n## Docker\n\nDocker를 사용하여 Genesis를 실행하려면 먼저 Docker 이미지를 빌드하세요:\n\n```bash\ndocker build -t genesis -f docker/Dockerfile docker\n```\n\n그런 다음 Docker 이미지(`/workspace/examples`에 마운트된) 내에서 예제를 실행할 수 있습니다:\n\n```bash\nxhost +local:root # 컨테이너가 디스플레이에 접근할 수 있도록 허용\n\ndocker run --gpus all --rm -it \\\n-e DISPLAY=$DISPLAY \\\n-v /tmp/.X11-unix/:/tmp/.X11-unix \\\n-v $PWD:/workspace \\\ngenesis\n```\n\n## 문서\n\n전체 문서는 [영어](https://genesis-world.readthedocs.io/en/latest/user_guide/index.html)와 [중국어](https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html)로 제공됩니다. 이 문서에는 자세한 설치 단계, 튜토리얼 및 API 참조가 포함되어 있습니다.\n\n## Genesis에 기여하기\n\nGenesis 프로젝트는 오픈 소스 및 협력 프로젝트입니다. 다음을 포함하여 커뮤니티의 모든 형태의 기여를 환영합니다:\n\n- 새로운 기능이나 버그 수정을 위한 **풀 리퀘스트**.\n- GitHub Issues를 통한 **버그 리포트**.\n- Genesis의 사용성을 향상시키기 위한 **제안**.\n\n자세한 내용은 [기여 가이드](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/CONTRIBUTING.md)를 참조하세요.\n\n## 지원\n\n- GitHub [Issues](https://github.com/Genesis-Embodied-AI/Genesis/issues)를 통해 버그를 보고하고 기능을 요청하세요.\n- GitHub [Discussions](https://github.com/Genesis-Embodied-AI/Genesis/discussions)에서 토론에 참여하거나 질문해 보세요.\n\n## 라이선스 및 감사의 글\n\nGenesis 소스 코드는 Apache 2.0 라이선스를 따릅니다.\n\nGenesis 개발은 다음 오픈 소스 프로젝트 덕분에 가능했습니다:\n\n- [Taichi](https://github.com/taichi-dev/taichi): 고성능 크로스 플랫폼 연산 백엔드. Taichi 팀의 기술 지원에 감사드립니다!\n- [FluidLab](https://github.com/zhouxian/FluidLab): MPM 솔버 구현 참고.\n- [SPH_Taichi](https://github.com/erizmr/SPH_Taichi): SPH 솔버 구현 참고.\n- [Ten Minute Physics](https://matthias-research.github.io/pages/tenMinutePhysics/index.html) 및 [PBF3D](https://github.com/WASD4959/PBF3D): PBD 솔버 구현 참고.\n- [MuJoCo](https://github.com/google-deepmind/mujoco): 강체 역학 참고.\n- [libccd](https://github.com/danfis/libccd): 충돌 감지 참고.\n- [PyRender](https://github.com/mmatl/pyrender): 래스터화 기반 렌더러.\n- [LuisaCompute](https://github.com/LuisaGroup/LuisaCompute) 및 [LuisaRender](https://github.com/LuisaGroup/LuisaRender): 레이 트레이싱 DSL.\n\n## 관련 논문\n\nGenesis는 다양한 기존 및 진행 중인 연구의 최첨단 기술을 하나의 시스템으로 통합하는 대규모 프로젝트입니다. 다음은 Genesis 프로젝트에 어떤 방식으로든 기여한 논문들의 대략적인 목록입니다:\n\n- Xian, Zhou, et al. \"Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.\" arXiv preprint arXiv:2303.02346 (2023).\n- Xu, Zhenjia, et al. \"Roboninja: Learning an adaptive cutting policy for multi-material objects.\" arXiv preprint arXiv:2302.11553 (2023).\n- Wang, Yufei, et al. \"Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.\" arXiv preprint arXiv:2311.01455 (2023).\n- Wang, Tsun-Hsuan, et al. \"Softzoo: A soft robot co-design benchmark for locomotion in diverse environments.\" arXiv preprint arXiv:2303.09555 (2023).\n- Wang, Tsun-Hsuan Johnson, et al. \"Diffusebot: Breeding soft robots with physics-augmented generative diffusion models.\" Advances in Neural Information Processing Systems 36 (2023): 44398-44423.\n- Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. \"Gen2sim: Scaling up robot learning in simulation with generative models.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.\n- Si, Zilin, et al. \"DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation.\" arXiv preprint arXiv:2403.08716 (2024).\n- Wang, Yian, et al. \"Thin-Shell Object Manipulations With Differentiable Physics Simulations.\" arXiv preprint arXiv:2404.00451 (2024).\n- Lin, Chunru, et al. \"UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments.\" arXiv preprint arXiv:2411.12711 (2024).\n- Zhou, Wenyang, et al. \"EMDM: Efficient motion diffusion model for fast and high-quality motion generation.\" European Conference on Computer Vision. Springer, Cham, 2025.\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Scalable differentiable physics for learning and control.\" International Conference on Machine Learning. PMLR, 2020.\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. \"Efficient differentiable simulation of articulated bodies.\" In International Conference on Machine Learning, PMLR, 2021.\n- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. \"Differentiable simulation of soft multi-body systems.\" Advances in Neural Information Processing Systems 34 (2021).\n- Wan, Weilin, et al. \"Tlcontrol: Trajectory and language control for human motion synthesis.\" arXiv preprint arXiv:2311.17135 (2023).\n- Wang, Yian, et al. \"Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting.\" arXiv preprint arXiv:2411.09823 (2024).\n- Zheng, Shaokun, et al. \"LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures.\" ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.\n- Fan, Yingruo, et al. \"Faceformer: Speech-driven 3d facial animation with transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n- Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. \"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE.\" Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.\n- Dou, Zhiyang, et al. \"C· ase: Learning conditional adversarial skill embeddings for physics-based characters.\" SIGGRAPH Asia 2023 Conference Papers. 2023.\n- ...\n\n이 외에도 다양한 연구가 진행 중입니다.\n\n## 인용\n\n연구에서 Genesis를 사용하는 경우, 다음을 인용해 주세요:\n\n```bibtex\n@software{Genesis,\n  author = {Genesis Authors},\n  title = {Genesis: A Universal and Generative Physics Engine for Robotics and Beyond},\n  month = {December},\n  year = {2024},\n  url = {https://github.com/Genesis-Embodied-AI/Genesis}\n}\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 2.0498046875,
          "content": "# Genesis Release Note\n\n## 0.2.1\n\n### Bug Fixes\n* Fix various visualization and rendering bugs. (@RobRoyce, @VincentCCandela, @Likhithsai2580)\n* Resolve some platform-dependent issues. (@abhaybd, @NekoAsakura)\n* Fix the issue with loading box textures when parsing MJCF files.\n* Correct asset path handling.\n* Fix repr output in IPython. (@JohnnyDing)\n* Resolve bugs in locomotion examples. (@yang-zj1026)\n* Fix several issues with MPR and contact islands during collision detection.\n\n### New Features\n* Add a smoke simulator driven by Stable Fluid, along with a demo. (@PingchuanMa)\n* Introduce APIs for applying external forces and torques.\n* Introduce APIs for setting friction ratios.\n* Add a domain randomization example.\n* Improve kernel cache loading speed by 20~30%. (@erizmr)\n* Provide an interactive drone control and visualization script. (@PieterBecking)\n* Introduce an RL environment and examples for the drone environment. (@KafuuChikai)\n* Add an option to enable or disable the batch dimension for (links/DOFs) information. Users can choose the trade-off between performance and flexibility.\n* Add Docker files. (@Kashu7100)\n* Implement the MuJoCo box-box collision detection algorithm for more stable grasping.\n* Include the backflip training script and checkpoints. (@ziyanx02)\n* Enable support for entity merging.\n* Add support for custom inverse kinematics (IK) chains.\n\n### Miscellaneous\n* Improve documentation and fix typos. (@sangminkim-99, @sjtuyinjie, @CharlesCNorton, @eltociear, @00make, @marcbone, @00make, @pierridotite, @takeshi8989, @NicholasZiglio, @AmbarishGK)\n* Add CONTRIBUTING.md and update CI configurations.\n* Introduce multi-language support. (@TitanSage02, @GengYiran, @DocyNoah)\n\nWe would also like to acknowledge the ongoing PRs. Some of them have not yet been merged because we have not had enough time to fully test them:\n* Unitree G1 walking. (@0nhc)\n* Blood, vessels, and heart simulation. (@lhemerly)\n* Cross-platform and rendering compatibility. (@VioletBenin, @DearVa, @JonnyDing)\n* Docker support. (@skurtyyskirts, @yuxiang-gao, @serg-yalosovetsky)\n"
        },
        {
          "name": "doc",
          "type": "commit",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "genesis",
          "type": "tree",
          "content": null
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.6923828125,
          "content": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\n# name = \"genesis-world-nightly\"\n# version = \"0.0.3\"\nname = \"genesis-world\"\nversion = \"0.2.1\"\ndescription = \"A universal and generative physics engine\"\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"psutil\",\n    \"scikit-image\",\n    \"taichi == 1.7.3\",\n    \"pydantic == 2.7.1\",\n    \"numpy == 1.26.4\",\n    \"six\",\n    \"PyOpenGL\",\n    \"freetype-py\",\n    \"pyglet\",\n    \"libigl\",\n    \"pygltflib == 1.16.0\",\n    \"mujoco == 3.2.5\",\n    \"pycollada\",\n    \"opencv-python\",\n    \"lxml\",\n    \"tetgen == 0.6.4\",\n    \"screeninfo\",\n    \"PyGEL3D\",\n    \"moviepy >= 2.0.0\",\n    \"numba\",\n    \"pymeshlab\",\n    \"coacd\",\n    \"OpenEXR\",\n    \"black\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"genesis\", \"genesis.*\"]\n\n[tool.setuptools.package-data]\ngenesis = [\n    \"assets/*\",\n    \"ext/pyrender/fonts/*\",\n    \"ext/pyrender/shaders/*\",\n    \"ext/VolumeSampling\",\n]\n\n[tool.black]\nline-length = 120\n\n[project.scripts]\ngs = \"genesis._main:main\"\n\n[project.optional-dependencies]\ngen = [\n    \"openai\",\n]\ndocs = [\n    # Note that currently sphinx 7 does not work, so we must use v6.2.1. See https://github.com/kivy/kivy/issues/8230 which tracks this issue. Once fixed we can use a later version\n    \"sphinx==6.2.1\",\n    \"sphinx-autobuild\",\n    \"pydata_sphinx_theme\",\n    # For spelling\n    \"sphinxcontrib.spelling\",\n    # Type hints support\n    \"sphinx-autodoc-typehints\",\n    # Copy button for code snippets\n    \"sphinx_copybutton\",\n    # Markdown parser\n    \"myst-parser\",\n    \"sphinx-subfigure\",\n    \"sphinxcontrib-video\",\n    \"sphinx-togglebutton\",\n    \"sphinx_design\",\n]\nrender = [\n    \"pybind11[global]\",\n    \"open3d\",\n]\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}