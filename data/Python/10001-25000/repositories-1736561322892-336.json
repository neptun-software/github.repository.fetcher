{
  "metadata": {
    "timestamp": 1736561322892,
    "page": 336,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PaddlePaddle/PaddleSpeech",
      "stars": 11363,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.8388671875,
          "content": "# This file is used by clang-format to autoformat paddle source code\n#\n# The clang-format is part of llvm toolchain.\n# It need to install llvm and clang to format source code style.\n#\n# The basic usage is,\n#   clang-format -i -style=file PATH/TO/SOURCE/CODE\n#\n# The -style=file implicit use \".clang-format\" file located in one of\n# parent directory.\n# The -i means inplace change.\n#\n# The document of clang-format is\n#   http://clang.llvm.org/docs/ClangFormat.html\n#   http://clang.llvm.org/docs/ClangFormatStyleOptions.html\n---\nLanguage:        Cpp\nBasedOnStyle:  Google\nIndentWidth:     4\nTabWidth:        4\nContinuationIndentWidth: 4\nMaxEmptyLinesToKeep: 2\nAccessModifierOffset: -2  # The private/protected/public has no indent in class\nStandard:  Cpp11\nAllowAllParametersOfDeclarationOnNextLine: true\nBinPackParameters: false\nBinPackArguments: false\n...\n\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 1.3740234375,
          "content": "[flake8]\n\n########## OPTIONS ##########\n# Set the maximum length that any line (with some exceptions) may be.\nmax-line-length = 120\n\n\n################### FILE PATTERNS ##########################\n# Provide a comma-separated list of glob patterns to exclude from checks.\nexclude =\n    # git folder\n    .git,\n    # python cache\n    __pycache__,\n    # third party\n    utils/compute-wer.py,\n    third_party/,\n# Provide a comma-separate list of glob patterns to include for checks.\nfilename =\n    *.py\n\n\n########## RULES ##########\n\n# ERROR CODES\n#\n# E/W  - PEP8 errors/warnings (pycodestyle)\n# F    - linting errors (pyflakes)\n# C    - McCabe complexity error (mccabe)\n#\n# W503 - line break before binary operator\n\n# Specify a list of codes to ignore.\nignore =\n    W503\n    E252,E262,E127,E265,E126,E266,E241,E261,E128,E125,E129\n    W291,W293,W605\n    E203,E305,E402,E501,E721,E741,F403,F405,F821,F841,F999,W503,W504,C408,E302,W291,E303,\n    # shebang has extra meaning in fbcode lints, so I think it's not worth trying\n    # to line this up with executable bit\n    EXE001,\n    # these ignores are from flake8-bugbear; please fix!\n    B007,B008,\n    # these ignores are from flake8-comprehensions; please fix!\n    C400,C401,C402,C403,C404,C405,C407,C411,C413,C414,C415\n\n\nper-file-ignores =\n    */__init__.py: F401\n\n# Specify the list of error codes you wish Flake8 to report.\nselect =\n    E,\n    W,\n    F,\n    C\n"
        },
        {
          "name": ".gitconfig",
          "type": "blob",
          "size": 0.62109375,
          "content": "[alias]\n  st = status\n  ci = commit\n  br = branch\n  co = checkout\n  df = diff\n  l = log --pretty=format:\\\"%h %ad | %s%d [%an]\\\" --graph --date=short\n  ll = log --stat\n\n[merge]\n  tool = vimdiff\n\n[core]\n  excludesfile = ~/.gitignore\n  editor = vim\n\n[color]\n  branch = auto\n  diff = auto\n  status = auto\n\n[color \"branch\"]\n  current = yellow reverse\n  local = yellow\n  remote = green\n\n[color \"diff\"]\n  meta = yellow bold\n  frag = magenta bold\n  old = red bold\n  new = green bold\n\n[color \"status\"]\n  added = yellow\n  changed = green\n  untracked = cyan\n\n[push]\n  default = matching\n\n[credential]\n  helper = store\n\n[user]\n  name =\n  email =\n\n\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.7373046875,
          "content": ".DS_Store\n*.pyc\n.vscode\n*.log\n*.wav\n*.pdmodel\n*.pdiparams*\n*.zip\n*.tar\n*.tar.gz\n.ipynb_checkpoints\n*.npz\n*.done\n*.whl\n*.egg-info\nbuild\n*output/\n.history\n\naudio/dist/\naudio/fc_patch/\n\ndocs/build/\ndocs/topic/ctc/warp-ctc/\n\ntools/venv\ntools/kenlm\ntools/sox-14.4.2\ntools/soxbindings\ntools/montreal-forced-aligner/\ntools/Montreal-Forced-Aligner/\ntools/sctk\ntools/sctk-20159b5/\ntools/kaldi\ntools/OpenBLAS/\ntools/Miniconda3-latest-Linux-x86_64.sh\ntools/activate_python.sh\ntools/miniconda.sh\ntools/CRF++-0.58/\ntools/liblbfgs-1.10/\ntools/srilm/\ntools/env.sh\ntools/openfst-1.8.1/\ntools/libsndfile/\ntools/python-soundfile/\ntools/onnx\ntools/onnxruntime\ntools/Paddle2ONNX\ntools/onnx-simplifier/\n\nspeechx/fc_patch/\n\nthird_party/ctc_decoders/paddlespeech_ctcdecoders.py\n"
        },
        {
          "name": ".mergify.yml",
          "type": "blob",
          "size": 3.171875,
          "content": "pull_request_rules:\n  - name: automatic merge for develop when CI passes and 1 reviews\n    conditions:\n      - \"approved-reviews-by>=1\"\n      - check-success=Travis CI - Pull Request\n      - base=develop\n    actions:\n      merge:\n        method: merge\n  - name: delete head branch after merged\n    conditions:\n      - merged\n    actions:\n      delete_head_branch: {}\n  - name: \"add label=auto-merge for PR by mergify\"\n    conditions:\n      - author=mergify[bot]\n    actions:\n      label:\n        add: [\"auto-merge\"]\n  - name: warn on conflicts\n    conditions:\n      - conflict\n    actions:\n      comment:\n        message: This pull request is now in conflict :(\n      label:\n        add: [\"conflicts\"]\n  - name: unlabel conflicts\n    conditions:\n      - -conflict\n    actions:\n      label:\n        remove: [\"conflicts\"]\n  - name: \"auto add label=Dataset\"\n    conditions:\n      - files~=^dataset/\n    actions:\n      label:\n        add: [\"Dataset\"]\n  - name: \"auto add label=S2T\"\n    conditions:\n      - files~=^paddlespeech/s2t/\n    actions:\n      label:\n        add: [\"S2T\"]\n  - name: \"auto add label=T2S\"\n    conditions:\n      - files~=^paddlespeech/t2s/\n    actions:\n      label:\n        add: [\"T2S\"]\n  - name: \"auto add label=Audio\"\n    conditions:\n      - files~=^paddlespeech/audio/\n    actions:\n      label:\n        add: [\"Audio\"]\n  - name: \"auto add label=Vector\"\n    conditions:\n      - files~=^paddlespeech/vector/\n    actions:\n      label:\n        add: [\"Vector\"]\n  - name: \"auto add label=Text\"\n    conditions:\n      - files~=^paddlespeech/text/\n    actions:\n      label:\n        add: [\"Text\"]\n  - name: \"auto add label=Example\"\n    conditions:\n      - files~=^examples/\n    actions:\n      label:\n        add: [\"Example\"]\n  - name: \"auto add label=CLI\"\n    conditions:\n      - files~=^paddlespeech/cli\n    actions:\n      label:\n        add: [\"CLI\"]\n  - name: \"auto add label=Server\"\n    conditions:\n      - files~=^paddlespeech/server\n    actions:\n      label:\n        add: [\"Server\"]\n  - name: \"auto add label=Demo\"\n    conditions:\n      - files~=^demos/\n    actions:\n      label:\n        add: [\"Demo\"]\n  - name: \"auto add label=README\"\n    conditions:\n      - files~=(README.md|READEME_cn.md)\n    actions:\n      label:\n        add: [\"README\"]\n  - name: \"auto add label=Documentation\"\n    conditions:\n      - files~=^(docs/|CHANGELOG.md)\n    actions:\n      label:\n        add: [\"Documentation\"]\n  - name: \"auto add label=CI\"\n    conditions:\n      - files~=^(.circleci/|ci/|.github/|.travis.yml|.travis|env.sh)\n    actions:\n      label:\n        add: [\"CI\"]\n  - name: \"auto add label=Installation\"\n    conditions:\n      - files~=^(tools/|setup.py|setup.cfg|setup_audio.py)\n    actions:\n      label:\n        add: [\"Installation\"]\n  - name: \"auto add label=Test\"\n    conditions:\n      - files~=^(tests/)\n    actions:\n      label:\n        add: [\"Test\"]\n  - name: \"auto add label=mergify\"\n    conditions:\n      - files~=^.mergify.yml\n    actions:\n      label:\n        add: [\"mergify\"]\n  - name: \"auto add label=Docker\"\n    conditions:\n      - files~=^docker/\n    actions:\n      label:\n        add: [\"Docker\"]\n  - name: \"auto add label=Deployment\"\n    conditions:\n      - files~=^runtime/\n    actions:\n      label:\n        add: [\"Deployment\"]\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.6201171875,
          "content": "repos:\n-   repo: https://github.com/pre-commit/mirrors-yapf.git\n    rev: v0.16.0\n    hooks:\n    -   id: yapf\n        files: \\.py$\n        exclude: (?=runtime/engine/kaldi|audio/paddleaudio/src|third_party).*(\\.cpp|\\.cc|\\.h\\.hpp|\\.py)$\n\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: a11d9314b22d8f8c7556443875b731ef05965464\n    hooks:\n    -   id: check-merge-conflict\n    -   id: check-symlinks\n    -   id: detect-private-key\n        files: (?!.*paddle)^.*$\n    -   id: end-of-file-fixer\n        files: \\.md$\n    #-   id: trailing-whitespace\n    #    files: \\.md$\n    -   id: requirements-txt-fixer\n        exclude: (?=third_party).*$\n    -   id: check-yaml\n    -   id: check-json\n    -   id: pretty-format-json\n        args:\n        - --no-sort-keys\n        - --autofix\n    -   id: check-merge-conflict\n      #    -   id: flake8\n      #        aergs:\n      #        -  --ignore=E501,E228,E226,E261,E266,E128,E402,W503\n      #        -  --builtins=G,request\n      #        -  --jobs=1\n      #        exclude: (?=runtime/engine/kaldi|audio/paddleaudio/src|third_party).*(\\.cpp|\\.cc|\\.h\\.hpp|\\.py)$\n\n-   repo : https://github.com/Lucas-C/pre-commit-hooks\n    rev: v1.0.1\n    hooks:\n    -   id: forbid-crlf\n        files: \\.md$\n    -   id: remove-crlf\n        files: \\.md$\n    -   id: forbid-tabs\n        files: \\.md$\n    -   id: remove-tabs\n        files: \\.md$\n\n-   repo: local\n    hooks:\n    -   id: clang-format\n        name: clang-format\n        description: Format files with ClangFormat\n        entry: bash .pre-commit-hooks/clang-format.hook -i\n        language: system\n        files: \\.(h\\+\\+|h|hh|hxx|hpp|cuh|c|cc|cpp|cu|c\\+\\+|cxx|tpp|txx)$\n        exclude: (?=runtime/engine/kaldi|audio/paddleaudio/src|runtime/patch|runtime/tools/fstbin|runtime/tools/lmbin|third_party/ctc_decoders|runtime/engine/common/utils).*(\\.cpp|\\.cc|\\.h|\\.hpp|\\.py)$ \n    -   id: cpplint\n        name: cpplint\n        description: Static code analysis of C/C++ files\n        language: python\n        files: \\.(h\\+\\+|h|hh|hxx|hpp|cuh|c|cc|cpp|cu|c\\+\\+|cxx|tpp|txx)$\n        exclude: (?=runtime/engine/kaldi|runtime/engine/common/matrix|audio/paddleaudio/src|runtime/patch|runtime/tools/fstbin|runtime/tools/lmbin|third_party/ctc_decoders|runtime/engine/common/utils).*(\\.cpp|\\.cc|\\.h|\\.hpp|\\.py)$ \n        entry: cpplint --filter=-build,-whitespace,+whitespace/comma,-whitespace/indent\n-   repo: https://github.com/asottile/reorder_python_imports\n    rev: v2.4.0\n    hooks:\n      - id: reorder-python-imports\n        exclude: (?=runtime/engine/kaldi|audio/paddleaudio/src|runtime/patch|runtime/tools/fstbin|runtime/tools/lmbin|third_party/ctc_decoders).*(\\.cpp|\\.cc|\\.h\\.hpp|\\.py)$\n"
        },
        {
          "name": ".pre-commit-hooks",
          "type": "tree",
          "content": null
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.6142578125,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs/source/conf.py\n\n# Build documentation with MkDocs\n#mkdocs:\n#  configuration: mkdocs.yml\n\n# Optionally build your docs in additional formats such as PDF\nformats: []\n\n# Optionally set the version of Python and requirements required to build your docs\npython:\n  version: 3.7\n  install:\n    - requirements: docs/requirements.txt\n    - method: setuptools\n      path: .\n  system_packages: true\n"
        },
        {
          "name": ".style.yapf",
          "type": "blob",
          "size": 0.046875,
          "content": "[style]\nbased_on_style = pep8\ncolumn_limit = 80\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.7001953125,
          "content": "language: cpp\ncache: ccache\nsudo: required\ndist: Bionic \nservices:\n  - docker\nos:\n  - linux\nenv:\n  - JOB=PRE_COMMIT\n\naddons:\n  apt:\n    packages:\n      - git\n      - python3-pip\n      - python3-dev\n\nbefore_install:\n  -  python3 --version\n  -  python3 -m pip --version\n  -  pip3 --version\n  -  sudo pip3 install -U virtualenv pre-commit pip\n  -  docker pull paddlepaddle/paddle:latest\n\nscript:\n  - exit_code=0\n  - docker run -i --rm -v \"$PWD:/py_unittest\" paddlepaddle/paddle:latest /bin/bash -c\n    'cd /py_unittest && bash .travis/precommit.sh && source env.sh && bash .travis/unittest.sh' || exit_code=$(( exit_code | $? ))\n    exit $exit_code\n\nnotifications:\n  email:\n    on_success: change\n    on_failure: always\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.07421875,
          "content": "include paddlespeech/t2s/exps/*.txt\ninclude paddlespeech/t2s/frontend/*.yaml"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 48.02734375,
          "content": "([简体中文](./README_cn.md)|English)\n<p align=\"center\">\n  <img src=\"./docs/images/PaddleSpeech_logo.png\" />\n</p>\n\n<p align=\"center\">\n    <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache%202-red.svg\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/releases\"><img src=\"https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa\"></a>\n    <a href=\"support os\"><img src=\"https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg\"></a>\n    <a href=\"\"><img src=\"https://img.shields.io/badge/python-3.8+-aff.svg\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors\"><img src=\"https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/commits\"><img src=\"https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/issues\"><img src=\"https://img.shields.io/github/issues/PaddlePaddle/PaddleSpeech?color=9cc\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/stargazers\"><img src=\"https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?color=ccf\"></a>\n    <a href=\"=https://pypi.org/project/paddlespeech/\"><img src=\"https://img.shields.io/pypi/dm/PaddleSpeech\"></a>\n    <a href=\"=https://pypi.org/project/paddlespeech/\"><img src=\"https://static.pepy.tech/badge/paddlespeech\"></a>\n    <a href=\"https://huggingface.co/spaces\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\"></a>\n</p>\n<div align=\"center\">  \n<h4>\n    <a href=\"#quick-start\"> Quick Start </a>\n  | <a href=\"#documents\"> Documents </a>\n  | <a href=\"#model-list\"> Models List </a>\n  | <a href=\"https://aistudio.baidu.com/aistudio/course/introduce/25130\"> AIStudio Courses </a>\n  | <a href=\"https://arxiv.org/abs/2205.12007\"> NAACL2022 Best Demo Award Paper </a>\n  | <a href=\"https://gitee.com/paddlepaddle/PaddleSpeech\"> Gitee </a>\n</h4>\n</div>\n\n------------------------------------------------------------------------------------\n\n**PaddleSpeech** is an open-source toolkit on [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models. \n\n**PaddleSpeech** won the [NAACL2022 Best Demo Award](https://2022.naacl.org/blog/best-demo-award/), please check out our paper on [Arxiv](https://arxiv.org/abs/2205.12007).\n\n##### Speech Recognition\n\n<div align = \"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> Input Audio  </th>\n      <th width=\"550\"> Recognition Result  </th>\n    </tr>\n  </thead>\n  <tbody>\n   <tr>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200 style=\"max-width: 100%;\"></a><br>\n      </td>\n      <td >I knocked at the door on the ancient side of the building.</td>\n    </tr>\n    <tr>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n      <td>我认为跑步最重要的就是给我带来了身体健康。</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n##### Speech Translation (English to Chinese)\n\n<div align = \"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> Input Audio  </th>\n      <th width=\"550\"> Translations Result  </th>\n    </tr>\n  </thead>\n  <tbody>\n   <tr>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200 style=\"max-width: 100%;\"></a><br>\n      </td>\n      <td >我 在 这栋 建筑 的 古老 门上 敲门。</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n##### Text-to-Speech\n<div align = \"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th width=\"550\" > Input Text</th>\n      <th>Synthetic Audio</th>\n    </tr>\n  </thead>\n  <tbody>\n   <tr>\n      <td>Life was like a box of chocolates, you never know what you're gonna get.</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/tacotron2_ljspeech_waveflow_samples_0.2/sentence_1.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td>早上好，今天是2020/10/29，最低温度是-3°C。</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/parakeet_espnet_fs2_pwg_demo/tn_g2p/parakeet/001.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td>季姬寂，集鸡，鸡即棘鸡。棘鸡饥叽，季姬及箕稷济鸡。鸡既济，跻姬笈，季姬忌，急咭鸡，鸡急，继圾几，季姬急，即籍箕击鸡，箕疾击几伎，伎即齑，鸡叽集几基，季姬急极屐击鸡，鸡既殛，季姬激，即记《季姬击鸡记》。</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/jijiji.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td>大家好，我是 parrot 虚拟老师，我们来读一首诗，我与春风皆过客，I and the spring breeze are passing by，你携秋水揽星河，you take the autumn water to take the galaxy。</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/labixiaoxin.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td>宜家唔系事必要你讲，但系你所讲嘅说话将会变成呈堂证供。</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/chengtangzhenggong.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td>各个国家有各个国家嘅国歌</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/gegege.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\nFor more synthesized audios, please refer to [PaddleSpeech Text-to-Speech samples](https://paddlespeech.readthedocs.io/en/latest/tts/demo.html).\n\n##### Punctuation Restoration\n<div align = \"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th width=\"390\"> Input Text </th>\n      <th width=\"390\"> Output Text </th>\n    </tr>\n  </thead>\n  <tbody>\n   <tr>\n      <td>今天的天气真不错啊你下午有空吗我想约你一起去吃饭</td>\n      <td>今天的天气真不错啊！你下午有空吗？我想约你一起去吃饭。</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n### Features\n\nVia the easy-to-use, efficient, flexible and scalable implementation, our vision is to empower both industrial application and academic research, including training, inference & testing modules, and deployment process. To be more specific, this toolkit features at:\n- 📦  **Ease of Use**: low barriers to install, [CLI](#quick-start), [Server](#quick-start-server), and [Streaming Server](#quick-start-streaming-server) is available to quick-start your journey.\n- 🏆  **Align to the State-of-the-Art**: we provide high-speed and ultra-lightweight models, and also cutting-edge technology. \n- 🏆  **Streaming ASR and TTS System**: we provide production ready streaming asr and streaming tts system.\n- 💯  **Rule-based Chinese frontend**: our frontend contains Text Normalization and Grapheme-to-Phoneme (G2P, including Polyphone and Tone Sandhi). Moreover, we use self-defined linguistic rules to adapt Chinese context.\n- 📦  **Varieties of Functions that Vitalize both Industrial and Academia**:\n  - 🛎️  *Implementation of critical audio tasks*: this toolkit contains audio functions like  Automatic Speech Recognition, Text-to-Speech Synthesis, Speaker Verfication, KeyWord Spotting, Audio Classification, and Speech Translation, etc.\n  - 🔬  *Integration of mainstream models and datasets*: the toolkit implements modules that participate in the whole pipeline of the speech tasks, and uses mainstream datasets like LibriSpeech, LJSpeech, AIShell, CSMSC, etc. See also [model list](#model-list) for more details.\n  - 🧩  *Cascaded models application*: as an extension of the typical traditional audio tasks, we combine the workflows of the aforementioned tasks with other fields like Natural language processing (NLP) and Computer Vision (CV).\n\n### Recent Update\n- 👑 2023.05.31: Add [WavLM ASR-en](https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/examples/librispeech/asr5), WavLM fine-tuning for ASR on LibriSpeech.\n- 🎉 2023.05.18: Add [Squeezeformer](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/examples/aishell/asr1), Squeezeformer training for ASR on Aishell.\n- 👑 2023.05.04: Add [HuBERT ASR-en](https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/examples/librispeech/asr4), HuBERT fine-tuning for ASR on LibriSpeech.\n- ⚡ 2023.04.28: Fix [0-d tensor](https://github.com/PaddlePaddle/PaddleSpeech/pull/3214), with the upgrade of paddlepaddle==2.5, the problem of modifying 0-d tensor has been solved.\n- 👑 2023.04.25: Add [AMP for U2 conformer](https://github.com/PaddlePaddle/PaddleSpeech/pull/3167).\n- 🔥 2023.04.06: Add [subtitle file (.srt format) generation example](./demos/streaming_asr_server).\n- 🔥 2023.03.14: Add SVS(Singing Voice Synthesis) examples with Opencpop dataset, including [DiffSinger](./examples/opencpop/svs1)、[PWGAN](./examples/opencpop/voc1) and [HiFiGAN](./examples/opencpop/voc5), the effect is continuously optimized.\n- 👑 2023.03.09: Add [Wav2vec2ASR-zh](./examples/aishell/asr3).\n- 🎉 2023.03.07: Add [TTS ARM Linux C++ Demo (with C++ Chinese Text Frontend)](./demos/TTSArmLinux).\n- 🔥 2023.03.03 Add Voice Conversion [StarGANv2-VC synthesize pipeline](./examples/vctk/vc3).\n- 🎉 2023.02.16: Add [Cantonese TTS](./examples/canton/tts3).\n- 🔥 2023.01.10: Add [code-switch asr CLI and Demos](./demos/speech_recognition).\n- 👑 2023.01.06: Add [code-switch asr tal_cs recipe](./examples/tal_cs/asr1/).\n- 🎉 2022.12.02: Add [end-to-end Prosody Prediction pipeline](./examples/csmsc/tts3_rhy) (including using prosody labels in Acoustic Model).\n- 🎉 2022.11.30: Add [TTS Android Demo](./demos/TTSAndroid).\n- 🤗 2022.11.28: PP-TTS and PP-ASR demos are available in [AIStudio](https://aistudio.baidu.com/aistudio/modelsoverview) and [official website\n of paddlepaddle](https://www.paddlepaddle.org.cn/models).\n- 👑 2022.11.18: Add [Whisper CLI and Demos](https://github.com/PaddlePaddle/PaddleSpeech/pull/2640), support multi language recognition and translation.\n- 🔥 2022.11.18: Add [Wav2vec2 CLI and Demos](./demos/speech_ssl), Support ASR and Feature Extraction.\n- 🎉 2022.11.17: Add [male voice for TTS](https://github.com/PaddlePaddle/PaddleSpeech/pull/2660).\n- 🔥 2022.11.07: Add [U2/U2++ C++ High Performance Streaming ASR Deployment](https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/runtime/examples/u2pp_ol/wenetspeech).\n- 👑 2022.11.01: Add [Adversarial Loss](https://arxiv.org/pdf/1907.04448.pdf) for [Chinese English mixed TTS](./examples/zh_en_tts/tts3).\n- 🔥 2022.10.26: Add [Prosody Prediction](./examples/other/rhy) for TTS.\n- 🎉 2022.10.21: Add [SSML](https://github.com/PaddlePaddle/PaddleSpeech/discussions/2538) for TTS Chinese Text Frontend.\n- 👑 2022.10.11: Add [Wav2vec2ASR-en](./examples/librispeech/asr3), wav2vec2.0 fine-tuning for ASR on LibriSpeech.\n- 🔥 2022.09.26: Add Voice Cloning, TTS finetune, and [ERNIE-SAT](https://arxiv.org/abs/2211.03545) in [PaddleSpeech Web Demo](./demos/speech_web).\n- ⚡ 2022.09.09: Add AISHELL-3 Voice Cloning [example](./examples/aishell3/vc2) with ECAPA-TDNN speaker encoder.\n- ⚡ 2022.08.25: Release TTS [finetune](./examples/other/tts_finetune/tts3) example.\n- 🔥 2022.08.22: Add [ERNIE-SAT](https://arxiv.org/abs/2211.03545) models: [ERNIE-SAT-vctk](./examples/vctk/ernie_sat)、[ERNIE-SAT-aishell3](./examples/aishell3/ernie_sat)、[ERNIE-SAT-zh_en](./examples/aishell3_vctk/ernie_sat).\n- 🔥 2022.08.15: Add [g2pW](https://github.com/GitYCC/g2pW) into TTS Chinese Text Frontend.\n- 🔥 2022.08.09: Release [Chinese English mixed TTS](./examples/zh_en_tts/tts3).\n- ⚡ 2022.08.03: Add ONNXRuntime infer for  TTS CLI.\n- 🎉 2022.07.18: Release VITS: [VITS-csmsc](./examples/csmsc/vits)、[VITS-aishell3](./examples/aishell3/vits)、[VITS-VC](./examples/aishell3/vits-vc).\n- 🎉 2022.06.22: All TTS models support ONNX format.\n- 🍀 2022.06.17: Add [PaddleSpeech Web Demo](./demos/speech_web).\n- 👑 2022.05.13: Release [PP-ASR](./docs/source/asr/PPASR.md)、[PP-TTS](./docs/source/tts/PPTTS.md)、[PP-VPR](docs/source/vpr/PPVPR.md).\n- 👏🏻 2022.05.06: `PaddleSpeech Streaming Server` is available for `Streaming ASR` with `Punctuation Restoration` and `Token Timestamp` and `Text-to-Speech`.\n- 👏🏻 2022.05.06: `PaddleSpeech Server` is available for `Audio Classification`, `Automatic Speech Recognition` and `Text-to-Speech`, `Speaker Verification` and `Punctuation Restoration`.\n- 👏🏻 2022.03.28: `PaddleSpeech CLI` is available for `Speaker Verification`.\n- 👏🏻 2021.12.10: `PaddleSpeech CLI` is available for `Audio Classification`, `Automatic Speech Recognition`, `Speech Translation (English to Chinese)` and `Text-to-Speech`.\n\n### Community\n- Scan the QR code below with your Wechat, you can access to official technical exchange group and get the bonus ( more than 20GB learning materials, such as papers, codes and videos ) and the live link of the lessons. Look forward to your participation.\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/30135920/212860467-9e943cc3-8be8-49a4-97fd-7c94aad8e979.jpg\"  width = \"200\"  />\n</div>\n\n## Installation\n\nWe strongly recommend our users to install PaddleSpeech in **Linux** with *python>=3.8* and *paddlepaddle<=2.5.1*. Some new versions of Paddle do not have support for adaptation in PaddleSpeech, so currently only versions 2.5.1 and earlier can be supported.\n\n### **Dependency Introduction**\n\n+ gcc >= 4.8.5\n+ paddlepaddle <= 2.5.1\n+ python >= 3.8\n+ OS support:  Linux(recommend), Windows, Mac OSX\n\nPaddleSpeech depends on paddlepaddle. For installation, please refer to the official website of [paddlepaddle](https://www.paddlepaddle.org.cn/en) and choose according to your own machine. Here is an example of the cpu version.\n\n```bash\npip install paddlepaddle -i https://mirror.baidu.com/pypi/simple\n```\nYou can also specify the version of paddlepaddle or install the develop version. \n```bash\n# install 2.4.1 version. Note, 2.4.1 is just an example, please follow the minimum dependency of paddlepaddle for your selection\npip install paddlepaddle==2.4.1 -i https://mirror.baidu.com/pypi/simple\n# install develop version\npip install paddlepaddle==0.0.0 -f https://www.paddlepaddle.org.cn/whl/linux/cpu-mkl/develop.html\n```\n\nThere are two quick installation methods for PaddleSpeech, one is pip installation, and the other is source code compilation (recommended).\n### pip install\n\n```shell\npip install pytest-runner\npip install paddlespeech\n```\n\n### source code compilation\n\n```shell\ngit clone https://github.com/PaddlePaddle/PaddleSpeech.git\ncd PaddleSpeech\npip install pytest-runner\npip install .\n```\n\nFor more installation problems, such as conda environment, librosa-dependent, gcc problems, kaldi installation, etc., you can refer to this [installation document](./docs/source/install.md). If you encounter problems during installation, you can leave a message on [#2150](https://github.com/PaddlePaddle/PaddleSpeech/issues/2150) and find related problems\n\n\n<a name=\"quickstart\"></a>\n## Quick Start\n\nDevelopers can have a try of our models with [PaddleSpeech Command Line](./paddlespeech/cli/README.md) or Python. Change `--input` to test your own audio/text and support 16k wav format audio.\n\n**You can also quickly experience it in AI Studio 👉🏻 [PaddleSpeech API Demo](https://aistudio.baidu.com/aistudio/projectdetail/4353348?sUid=2470186&shared=1&ts=1660876445786)**\n\n\nTest audio sample download\n\n```shell\nwget -c https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav\nwget -c https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav\n```\n\n### Automatic Speech Recognition\n\n<details><summary>&emsp;（Click to expand）Open Source Speech Recognition</summary>\n\n**command line experience**\n\n```shell\npaddlespeech asr --lang zh --input zh.wav\n```\n\n**Python API experience**\n\n```python\n>>> from paddlespeech.cli.asr.infer import ASRExecutor\n>>> asr = ASRExecutor()\n>>> result = asr(audio_file=\"zh.wav\")\n>>> print(result)\n我认为跑步最重要的就是给我带来了身体健康\n```\n</details>\n\n### Text-to-Speech\n\n<details><summary>&emsp;Open Source Speech Synthesis</summary>\n\nOutput 24k sample rate wav format audio\n\n\n**command line experience**\n\n```shell\npaddlespeech tts --input \"你好，欢迎使用百度飞桨深度学习框架！\" --output output.wav\n```\n\n**Python API experience**\n\n```python\n>>> from paddlespeech.cli.tts.infer import TTSExecutor\n>>> tts = TTSExecutor()\n>>> tts(text=\"今天天气十分不错。\", output=\"output.wav\")\n```\n- You can experience in [Huggingface Spaces](https://huggingface.co/spaces) [TTS Demo](https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS)\n\n</details>\n\n### Audio Classification\n\n<details><summary>&emsp;An open-domain sound classification tool</summary>\n\nSound classification model based on 527 categories of AudioSet dataset\n\n**command line experience**\n\n```shell\npaddlespeech cls --input zh.wav\n```\n\n**Python API experience**\n\n```python\n>>> from paddlespeech.cli.cls.infer import CLSExecutor\n>>> cls = CLSExecutor()\n>>> result = cls(audio_file=\"zh.wav\")\n>>> print(result)\nSpeech 0.9027186632156372\n```\n\n</details>\n\n### Voiceprint Extraction\n\n<details><summary>&emsp;Industrial-grade voiceprint extraction tool</summary>\n\n**command line experience**\n\n```shell\npaddlespeech vector --task spk --input zh.wav\n```\n\n**Python API experience**\n\n```python\n>>> from paddlespeech.cli.vector import VectorExecutor\n>>> vec = VectorExecutor()\n>>> result = vec(audio_file=\"zh.wav\")\n>>> print(result) # 187维向量\n[ -0.19083306   9.474295   -14.122263    -2.0916545    0.04848729\n   4.9295826    1.4780062    0.3733844   10.695862     3.2697146\n  -4.48199     -0.6617882   -9.170393   -11.1568775   -1.2358263 ...]\n```\n\n</details>\n\n### Punctuation Restoration\n\n<details><summary>&emsp;Quick recovery of text punctuation, works with ASR models</summary>\n\n**command line experience**\n\n```shell\npaddlespeech text --task punc --input 今天的天气真不错啊你下午有空吗我想约你一起去吃饭\n```\n\n**Python API experience**\n\n```python\n>>> from paddlespeech.cli.text.infer import TextExecutor\n>>> text_punc = TextExecutor()\n>>> result = text_punc(text=\"今天的天气真不错啊你下午有空吗我想约你一起去吃饭\")\n今天的天气真不错啊！你下午有空吗？我想约你一起去吃饭。\n```\n\n</details>\n\n### Speech Translation\n\n<details><summary>&emsp;End-to-end English to Chinese Speech Translation Tool</summary>\n\nUse pre-compiled kaldi related tools, only support experience in Ubuntu system\n\n**command line experience**\n\n```shell\npaddlespeech st --input en.wav\n```\n\n**Python API experience**\n\n```python\n>>> from paddlespeech.cli.st.infer import STExecutor\n>>> st = STExecutor()\n>>> result = st(audio_file=\"en.wav\")\n['我 在 这栋 建筑 的 古老 门上 敲门 。']\n```\n\n</details>\n\n\n<a name=\"quickstartserver\"></a>\n## Quick Start Server\n\nDevelopers can have a try of our speech server with [PaddleSpeech Server Command Line](./paddlespeech/server/README.md).\n\n**You can try it quickly in AI Studio (recommend): [SpeechServer](https://aistudio.baidu.com/aistudio/projectdetail/4354592?sUid=2470186&shared=1&ts=1660877827034)**\n\n**Start server**     \n\n```shell\npaddlespeech_server start --config_file ./demos/speech_server/conf/application.yaml\n```\n\n**Access Speech Recognition Services**     \n\n```shell\npaddlespeech_client asr --server_ip 127.0.0.1 --port 8090 --input input_16k.wav\n```\n\n**Access Text to Speech Services**     \n\n```shell\npaddlespeech_client tts --server_ip 127.0.0.1 --port 8090 --input \"您好，欢迎使用百度飞桨语音合成服务。\" --output output.wav\n```\n\n**Access Audio Classification Services**     \n```shell\npaddlespeech_client cls --server_ip 127.0.0.1 --port 8090 --input input.wav\n```\n\n\nFor more information about server command lines, please see: [speech server demos](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server)\n\n\n<a name=\"quickstartstreamingserver\"></a>\n## Quick Start Streaming Server\n\nDevelopers can have a try of  [streaming asr](./demos/streaming_asr_server/README.md) and [streaming tts](./demos/streaming_tts_server/README.md) server.\n\n**Start Streaming Speech Recognition Server**\n\n```\npaddlespeech_server start --config_file ./demos/streaming_asr_server/conf/application.yaml\n```\n\n**Access Streaming Speech Recognition Services**     \n\n```\npaddlespeech_client asr_online --server_ip 127.0.0.1 --port 8090 --input input_16k.wav\n```\n\n**Start Streaming Text to Speech  Server**\n\n```\npaddlespeech_server start --config_file ./demos/streaming_tts_server/conf/tts_online_application.yaml\n```\n\n**Access Streaming Text to Speech Services**     \n\n```\npaddlespeech_client tts_online --server_ip 127.0.0.1 --port 8092 --protocol http --input \"您好，欢迎使用百度飞桨语音合成服务。\" --output output.wav\n```\n\nFor more information please see:  [streaming asr](./demos/streaming_asr_server/README.md) and [streaming tts](./demos/streaming_tts_server/README.md) \n\n<a name=\"ModelList\"></a>\n\n## Model List\n\nPaddleSpeech supports a series of most popular models. They are summarized in [released models](./docs/source/released_model.md) and attached with available pretrained models.\n\n<a name=\"SpeechToText\"></a>\n\n**Speech-to-Text** contains *Acoustic Model*, *Language Model*, and *Speech Translation*, with the following details:\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th>Speech-to-Text Module Type</th>\n      <th>Dataset</th>\n      <th>Model Type</th>\n      <th>Example</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"4\">Speech Recogination</td>\n      <td rowspan=\"2\" >Aishell</td>\n      <td >DeepSpeech2 RNN + Conv based Models</td>\n      <td>\n      <a href = \"./examples/aishell/asr0\">deepspeech2-aishell</a>\n      </td>\n    </tr>\n    <tr>\n      <td>Transformer based Attention Models </td>\n      <td>\n      <a href = \"./examples/aishell/asr1\">u2.transformer.conformer-aishell</a>\n      </td>\n    </tr>\n    <tr>\n      <td> Librispeech</td>\n      <td>Transformer based Attention Models </td>\n      <td>\n      <a href = \"./examples/librispeech/asr0\">deepspeech2-librispeech</a> / <a href = \"./examples/librispeech/asr1\">transformer.conformer.u2-librispeech</a>  / <a href = \"./examples/librispeech/asr2\">transformer.conformer.u2-kaldi-librispeech</a>\n      </td>\n      </td>\n    </tr>\n  <tr>\n      <td>TIMIT</td>\n      <td>Unified Streaming & Non-streaming Two-pass</td>\n      <td>\n    <a href = \"./examples/timit/asr1\"> u2-timit</a>\n      </td>\n  </tr>\n  <tr>\n  <td>Alignment</td>\n  <td>THCHS30</td>\n  <td>MFA</td>\n  <td>\n  <a href = \".examples/thchs30/align0\">mfa-thchs30</a>\n  </td>\n  </tr>\n   <tr>\n      <td rowspan=\"1\">Language Model</td>\n      <td colspan = \"2\">Ngram Language Model</td>\n      <td>\n      <a href = \"./examples/other/ngram_lm\">kenlm</a>\n      </td>\n    </tr>\n  <tr>\n      <td rowspan=\"2\">Speech Translation (English to Chinese)</td> \n      <td rowspan=\"2\">TED En-Zh</td>\n      <td>Transformer + ASR MTL</td>\n      <td>\n      <a href = \"./examples/ted_en_zh/st0\">transformer-ted</a>\n      </td>\n  </tr>\n  <tr>\n      <td>FAT + Transformer + ASR MTL</td>\n      <td>\n      <a href = \"./examples/ted_en_zh/st1\">fat-st-ted</a>\n      </td>\n  </tr>\n  </tbody>\n</table>\n\n<a name=\"TextToSpeech\"></a>\n\n**Text-to-Speech** in PaddleSpeech mainly contains three modules: *Text Frontend*, *Acoustic Model* and *Vocoder*. Acoustic Model and Vocoder models are listed as follow:\n\n<table>\n  <thead>\n    <tr>\n      <th> Text-to-Speech Module Type </th>\n      <th> Model Type </th>\n      <th> Dataset </th>\n      <th> Example </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td> Text Frontend </td>\n      <td colspan=\"2\"> &emsp; </td>\n      <td>\n      <a href = \"./examples/other/tn\">tn</a> / <a href = \"./examples/other/g2p\">g2p</a>\n      </td>\n    </tr>\n    <tr>\n      <td rowspan=\"6\">Acoustic Model</td>\n      <td>Tacotron2</td>\n      <td>LJSpeech / CSMSC</td>\n      <td>\n      <a href = \"./examples/ljspeech/tts0\">tacotron2-ljspeech</a> / <a href = \"./examples/csmsc/tts0\">tacotron2-csmsc</a>\n      </td>\n    </tr>\n    <tr>\n      <td>Transformer TTS</td>\n      <td>LJSpeech</td>\n      <td>\n      <a href = \"./examples/ljspeech/tts1\">transformer-ljspeech</a>\n      </td>\n    </tr>\n    <tr>\n      <td>SpeedySpeech</td>\n      <td>CSMSC</td>\n      <td >\n      <a href = \"./examples/csmsc/tts2\">speedyspeech-csmsc</a>\n      </td>\n    </tr>\n    <tr>\n      <td>FastSpeech2</td>\n      <td>LJSpeech / VCTK / CSMSC / AISHELL-3 / ZH_EN / finetune</td>\n      <td>\n      <a href = \"./examples/ljspeech/tts3\">fastspeech2-ljspeech</a> / <a href = \"./examples/vctk/tts3\">fastspeech2-vctk</a> / <a href = \"./examples/csmsc/tts3\">fastspeech2-csmsc</a> / <a href = \"./examples/aishell3/tts3\">fastspeech2-aishell3</a> / <a href = \"./examples/zh_en_tts/tts3\">fastspeech2-zh_en</a> / <a href = \"./examples/other/tts_finetune/tts3\">fastspeech2-finetune</a>\n      </td>\n    </tr>\n    <tr>\n      <td><a href = \"https://arxiv.org/abs/2211.03545\">ERNIE-SAT</a></td>\n      <td>VCTK / AISHELL-3 / ZH_EN</td>\n      <td>\n      <a href = \"./examples/vctk/ernie_sat\">ERNIE-SAT-vctk</a> / <a href = \"./examples/aishell3/ernie_sat\">ERNIE-SAT-aishell3</a> / <a href = \"./examples/aishell3_vctk/ernie_sat\">ERNIE-SAT-zh_en</a>\n      </td>\n    </tr>\n    <tr>\n      <td>DiffSinger</td>\n      <td>Opencpop</td>\n      <td>\n      <a href = \"./examples/opencpop/svs1\">DiffSinger-opencpop</a>\n      </td>\n   </tr>\n   <tr>\n      <td rowspan=\"6\">Vocoder</td>\n      <td >WaveFlow</td>\n      <td >LJSpeech</td>\n      <td>\n      <a href = \"./examples/ljspeech/voc0\">waveflow-ljspeech</a>\n      </td>\n    </tr>\n    <tr>\n      <td >Parallel WaveGAN</td>\n      <td >LJSpeech / VCTK / CSMSC / AISHELL-3 / Opencpop</td>\n      <td>\n      <a href = \"./examples/ljspeech/voc1\">PWGAN-ljspeech</a> / <a href = \"./examples/vctk/voc1\">PWGAN-vctk</a> / <a href = \"./examples/csmsc/voc1\">PWGAN-csmsc</a> /  <a href = \"./examples/aishell3/voc1\">PWGAN-aishell3</a> / <a href = \"./examples/opencpop/voc1\">PWGAN-opencpop</a>\n      </td>\n    </tr>\n    <tr>\n      <td >Multi Band MelGAN</td>\n      <td >CSMSC</td>\n      <td>\n      <a href = \"./examples/csmsc/voc3\">Multi Band MelGAN-csmsc</a> \n      </td>\n    </tr> \n    <tr>\n      <td >Style MelGAN</td>\n      <td >CSMSC</td>\n      <td>\n      <a href = \"./examples/csmsc/voc4\">Style MelGAN-csmsc</a> \n      </td>\n    </tr>\n    <tr>\n      <td>HiFiGAN</td>\n      <td>LJSpeech / VCTK / CSMSC / AISHELL-3 / Opencpop</td>\n      <td>\n      <a href = \"./examples/ljspeech/voc5\">HiFiGAN-ljspeech</a> / <a href = \"./examples/vctk/voc5\">HiFiGAN-vctk</a> / <a href = \"./examples/csmsc/voc5\">HiFiGAN-csmsc</a> / <a href = \"./examples/aishell3/voc5\">HiFiGAN-aishell3</a> / <a href = \"./examples/opencpop/voc5\">HiFiGAN-opencpop</a>\n      </td>\n    </tr>\n    <tr>\n      <td>WaveRNN</td>\n      <td>CSMSC</td>\n      <td>\n      <a href = \"./examples/csmsc/voc6\">WaveRNN-csmsc</a>\n      </td>\n    </tr>\n    <tr>\n      <td rowspan=\"5\">Voice Cloning</td>\n      <td>GE2E</td>\n      <td >Librispeech, etc.</td>\n      <td>\n      <a href = \"./examples/other/ge2e\">GE2E</a>\n      </td>\n    </tr>\n    <tr>\n      <td>SV2TTS (GE2E + Tacotron2)</td>\n      <td>AISHELL-3</td>\n      <td>\n      <a href = \"./examples/aishell3/vc0\">VC0</a>\n      </td>\n    </tr>\n    <tr>\n      <td>SV2TTS (GE2E + FastSpeech2)</td>\n      <td>AISHELL-3</td>\n      <td>\n      <a href = \"./examples/aishell3/vc1\">VC1</a>\n      </td>\n    </tr>\n    <tr>\n      <td>SV2TTS (ECAPA-TDNN + FastSpeech2)</td>\n      <td>AISHELL-3</td>\n      <td>\n      <a href = \"./examples/aishell3/vc2\">VC2</a>\n      </td>\n    </tr>\n    <tr>\n      <td>GE2E + VITS</td>\n      <td>AISHELL-3</td>\n      <td>\n      <a href = \"./examples/aishell3/vits-vc\">VITS-VC</a>\n      </td>\n    </tr>\n    <tr>\n      <td rowspan=\"3\">End-to-End</td>\n      <td>VITS</td>\n      <td>CSMSC / AISHELL-3</td>\n      <td>\n      <a href = \"./examples/csmsc/vits\">VITS-csmsc</a> / <a href = \"./examples/aishell3/vits\">VITS-aishell3</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"AudioClassification\"></a>\n\n**Audio Classification**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> Task </th>\n      <th> Dataset </th>\n      <th> Model Type </th>\n      <th> Example </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>Audio Classification</td>\n      <td>ESC-50</td>\n      <td>PANN</td>\n      <td>\n      <a href = \"./examples/esc50/cls0\">pann-esc50</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"KeywordSpotting\"></a>\n\n**Keyword Spotting**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> Task </th>\n      <th> Dataset </th>\n      <th> Model Type </th>\n      <th> Example </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>Keyword Spotting</td>\n      <td>hey-snips</td>\n      <td>MDTC</td>\n      <td>\n      <a href = \"./examples/hey_snips/kws0\">mdtc-hey-snips</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"SpeakerVerification\"></a>\n\n**Speaker Verification**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> Task </th>\n      <th> Dataset </th>\n      <th> Model Type </th>\n      <th> Example </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>Speaker Verification</td>\n      <td>VoxCeleb1/2</td>\n      <td>ECAPA-TDNN</td>\n      <td>\n      <a href = \"./examples/voxceleb/sv0\">ecapa-tdnn-voxceleb12</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"SpeakerDiarization\"></a>\n\n**Speaker Diarization**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> Task </th>\n      <th> Dataset </th>\n      <th> Model Type </th>\n      <th> Example </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>Speaker Diarization</td>\n     <td>AMI</td>\n      <td>ECAPA-TDNN + AHC / SC</td>\n      <td>\n      <a href = \"./examples/ami/sd0\">ecapa-tdnn-ami</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"PunctuationRestoration\"></a>\n\n**Punctuation Restoration**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> Task </th>\n      <th> Dataset </th>\n      <th> Model Type </th>\n      <th> Example </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>Punctuation Restoration</td>\n      <td>IWLST2012_zh</td>\n      <td>Ernie Linear</td>\n      <td>\n      <a href = \"./examples/iwslt2012/punc0\">iwslt2012-punc0</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n## Documents\n\nNormally, [Speech SoTA](https://paperswithcode.com/area/speech), [Audio SoTA](https://paperswithcode.com/area/audio) and [Music SoTA](https://paperswithcode.com/area/music) give you an overview of the hot academic topics in the related area. To focus on the tasks in PaddleSpeech, you will find the following guidelines are helpful to grasp the core ideas.\n\n- [Installation](./docs/source/install.md)\n- [Quick Start](#quickstart)\n- [Some Demos](./demos/README.md)\n- Tutorials\n  - [Automatic Speech Recognition](./docs/source/asr/quick_start.md)\n    - [Introduction](./docs/source/asr/models_introduction.md)\n    - [Data Preparation](./docs/source/asr/data_preparation.md)\n    - [Ngram LM](./docs/source/asr/ngram_lm.md)\n  - [Text-to-Speech](./docs/source/tts/quick_start.md)\n    - [Introduction](./docs/source/tts/models_introduction.md)\n    - [Advanced Usage](./docs/source/tts/advanced_usage.md)\n    - [Chinese Rule Based Text Frontend](./docs/source/tts/zh_text_frontend.md)\n    - [Test Audio Samples](https://paddlespeech.readthedocs.io/en/latest/tts/demo.html)\n  - Speaker Verification\n    - [Audio Searching](./demos/audio_searching/README.md)\n    - [Speaker Verification](./demos/speaker_verification/README.md)\n  - [Audio Classification](./demos/audio_tagging/README.md)\n  - [Speech Translation](./demos/speech_translation/README.md)\n  - [Speech Server](./demos/speech_server/README.md)\n- [Released Models](./docs/source/released_model.md)\n  - [Speech-to-Text](#SpeechToText)\n  - [Text-to-Speech](#TextToSpeech)\n  - [Audio Classification](#AudioClassification)\n  - [Speaker Verification](#SpeakerVerification)\n  - [Speaker Diarization](#SpeakerDiarization)\n  - [Punctuation Restoration](#PunctuationRestoration)\n- [Community](#Community)\n- [Welcome to contribute](#contribution)\n- [License](#License)\n\nThe Text-to-Speech module is originally called [Parakeet](https://github.com/PaddlePaddle/Parakeet), and now merged with this repository. If you are interested in academic research about this task, please see [TTS research overview](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/docs/source/tts#overview). Also, [this document](https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/docs/source/tts/models_introduction.md) is a good guideline for the pipeline components.\n\n\n## ⭐ Examples\n- **[PaddleBoBo](https://github.com/JiehangXie/PaddleBoBo): Use PaddleSpeech TTS to generate virtual human voice.**\n  \n<div align=\"center\"><a href=\"https://www.bilibili.com/video/BV1cL411V71o?share_source=copy_web\"><img src=\"https://ai-studio-static-online.cdn.bcebos.com/06fd746ab32042f398fb6f33f873e6869e846fe63c214596ae37860fe8103720\" / width=\"500px\"></a></div>\n\n- [PaddleSpeech Demo Video](https://paddlespeech.readthedocs.io/en/latest/demo_video.html)\n\n- **[VTuberTalk](https://github.com/jerryuhoo/VTuberTalk): Use PaddleSpeech TTS and ASR to clone voice from videos.**\n\n\n## Citation\n\nTo cite PaddleSpeech for research, please use the following format.\n\n```text\n@inproceedings{zhang2022paddlespeech,\n    title = {PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit},\n    author = {Hui Zhang, Tian Yuan, Junkun Chen, Xintong Li, Renjie Zheng, Yuxin Huang, Xiaojie Chen, Enlei Gong, Zeyu Chen, Xiaoguang Hu, dianhai yu, Yanjun Ma, Liang Huang},\n    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations},\n    year = {2022},\n    publisher = {Association for Computational Linguistics},\n}\n\n@InProceedings{pmlr-v162-bai22d,\n  title = {{A}$^3${T}: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing},\n  author = {Bai, He and Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Li, Xintong and Huang, Liang},\n  booktitle = {Proceedings of the 39th International Conference on Machine Learning},\n  pages = {1399--1411},\n  year = {2022},\n  volume = {162},\n  series = {Proceedings of Machine Learning Research},\n  month = {17--23 Jul},\n  publisher = {PMLR},\n  pdf = {https://proceedings.mlr.press/v162/bai22d/bai22d.pdf},\n  url = {https://proceedings.mlr.press/v162/bai22d.html},\n}\n\n@inproceedings{zheng2021fused,\n  title={Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation},\n  author={Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Huang, Liang},\n  booktitle={International Conference on Machine Learning},\n  pages={12736--12746},\n  year={2021},\n  organization={PMLR}\n}\n```\n\n<a name=\"contribution\"></a>\n## Contribute to PaddleSpeech\n\nYou are warmly welcome to submit questions in [discussions](https://github.com/PaddlePaddle/PaddleSpeech/discussions) and bug reports in [issues](https://github.com/PaddlePaddle/PaddleSpeech/issues)! Also, we highly appreciate if you are willing to contribute to this project!\n\n### Contributors\n<p align=\"center\">\n<a href=\"https://github.com/zh794390558\"><img src=\"https://avatars.githubusercontent.com/u/3038472?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Jackwaterveg\"><img src=\"https://avatars.githubusercontent.com/u/87408988?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/yt605155624\"><img src=\"https://avatars.githubusercontent.com/u/24568452?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Honei\"><img src=\"https://avatars.githubusercontent.com/u/11361692?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/KPatr1ck\"><img src=\"https://avatars.githubusercontent.com/u/22954146?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/kuke\"><img src=\"https://avatars.githubusercontent.com/u/3064195?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/lym0302\"><img src=\"https://avatars.githubusercontent.com/u/34430015?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/SmileGoat\"><img src=\"https://avatars.githubusercontent.com/u/56786796?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/xinghai-sun\"><img src=\"https://avatars.githubusercontent.com/u/7038341?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/pkuyym\"><img src=\"https://avatars.githubusercontent.com/u/5782283?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/LittleChenCc\"><img src=\"https://avatars.githubusercontent.com/u/10339970?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/qingen\"><img src=\"https://avatars.githubusercontent.com/u/3139179?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/D-DanielYang\"><img src=\"https://avatars.githubusercontent.com/u/23690325?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Mingxue-Xu\"><img src=\"https://avatars.githubusercontent.com/u/92848346?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/745165806\"><img src=\"https://avatars.githubusercontent.com/u/20623194?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/jerryuhoo\"><img src=\"https://avatars.githubusercontent.com/u/24245709?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/WilliamZhang06\"><img src=\"https://avatars.githubusercontent.com/u/97937340?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/chrisxu2016\"><img src=\"https://avatars.githubusercontent.com/u/18379485?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/iftaken\"><img src=\"https://avatars.githubusercontent.com/u/30135920?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/lfchener\"><img src=\"https://avatars.githubusercontent.com/u/6771821?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/BarryKCL\"><img src=\"https://avatars.githubusercontent.com/u/48039828?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/mmglove\"><img src=\"https://avatars.githubusercontent.com/u/38800877?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/gongel\"><img src=\"https://avatars.githubusercontent.com/u/24390500?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/luotao1\"><img src=\"https://avatars.githubusercontent.com/u/6836917?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/wanghaoshuang\"><img src=\"https://avatars.githubusercontent.com/u/7534971?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/kslz\"><img src=\"https://avatars.githubusercontent.com/u/54951765?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/JiehangXie\"><img src=\"https://avatars.githubusercontent.com/u/51190264?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/david-95\"><img src=\"https://avatars.githubusercontent.com/u/15189190?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/THUzyt21\"><img src=\"https://avatars.githubusercontent.com/u/91456992?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/buchongyu2\"><img src=\"https://avatars.githubusercontent.com/u/29157444?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/iclementine\"><img src=\"https://avatars.githubusercontent.com/u/16222986?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/phecda-xu\"><img src=\"https://avatars.githubusercontent.com/u/46859427?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/freeliuzc\"><img src=\"https://avatars.githubusercontent.com/u/23568094?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/ZeyuChen\"><img src=\"https://avatars.githubusercontent.com/u/1371212?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/ccrrong\"><img src=\"https://avatars.githubusercontent.com/u/101700995?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/AK391\"><img src=\"https://avatars.githubusercontent.com/u/81195143?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/qingqing01\"><img src=\"https://avatars.githubusercontent.com/u/7845005?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/0x45f\"><img src=\"https://avatars.githubusercontent.com/u/23097963?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/vpegasus\"><img src=\"https://avatars.githubusercontent.com/u/22723154?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/ericxk\"><img src=\"https://avatars.githubusercontent.com/u/4719594?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Betterman-qs\"><img src=\"https://avatars.githubusercontent.com/u/61459181?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/sneaxiy\"><img src=\"https://avatars.githubusercontent.com/u/32832641?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Doubledongli\"><img src=\"https://avatars.githubusercontent.com/u/20540661?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/apps/dependabot\"><img src=\"https://avatars.githubusercontent.com/in/29110?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/kvinwang\"><img src=\"https://avatars.githubusercontent.com/u/6442159?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/chenkui164\"><img src=\"https://avatars.githubusercontent.com/u/34813030?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/PaddleZhang\"><img src=\"https://avatars.githubusercontent.com/u/97284124?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/billishyahao\"><img src=\"https://avatars.githubusercontent.com/u/96406262?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/BrightXiaoHan\"><img src=\"https://avatars.githubusercontent.com/u/25839309?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/jiqiren11\"><img src=\"https://avatars.githubusercontent.com/u/82639260?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/ryanrussell\"><img src=\"https://avatars.githubusercontent.com/u/523300?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/GT-ZhangAcer\"><img src=\"https://avatars.githubusercontent.com/u/46156734?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/tensor-tang\"><img src=\"https://avatars.githubusercontent.com/u/21351065?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/hysunflower\"><img src=\"https://avatars.githubusercontent.com/u/52739577?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/oyjxer\"><img src=\"https://avatars.githubusercontent.com/u/16233945?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/JamesLim-sy\"><img src=\"https://avatars.githubusercontent.com/u/61349199?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/limpidezza\"><img src=\"https://avatars.githubusercontent.com/u/71760778?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/windstamp\"><img src=\"https://avatars.githubusercontent.com/u/34057289?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/AshishKarel\"><img src=\"https://avatars.githubusercontent.com/u/58069375?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/chesterkuo\"><img src=\"https://avatars.githubusercontent.com/u/6285069?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/YDX-2147483647\"><img src=\"https://avatars.githubusercontent.com/u/73375426?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/AdamBear\"><img src=\"https://avatars.githubusercontent.com/u/2288870?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/wwhu\"><img src=\"https://avatars.githubusercontent.com/u/6081200?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/lispc\"><img src=\"https://avatars.githubusercontent.com/u/2833376?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/harisankarh\"><img src=\"https://avatars.githubusercontent.com/u/1307053?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/pengzhendong\"><img src=\"https://avatars.githubusercontent.com/u/10704539?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Jackiexiao\"><img src=\"https://avatars.githubusercontent.com/u/18050469?s=60&v=4\" width=75 height=75></a>\n</p>\n\n## Acknowledgement\n- Many thanks to [HighCWu](https://github.com/HighCWu) for adding [VITS-aishell3](./examples/aishell3/vits) and [VITS-VC](./examples/aishell3/vits-vc) examples.\n- Many thanks to [david-95](https://github.com/david-95) for fixing multi-punctuation bug、contributing to multiple program and data, and adding [SSML](https://github.com/PaddlePaddle/PaddleSpeech/discussions/2538) for TTS Chinese Text Frontend. \n- Many thanks to [BarryKCL](https://github.com/BarryKCL) for improving TTS Chinses Frontend based on [G2PW](https://github.com/GitYCC/g2pW).\n- Many thanks to [yeyupiaoling](https://github.com/yeyupiaoling)/[PPASR](https://github.com/yeyupiaoling/PPASR)/[PaddlePaddle-DeepSpeech](https://github.com/yeyupiaoling/PaddlePaddle-DeepSpeech)/[VoiceprintRecognition-PaddlePaddle](https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle)/[AudioClassification-PaddlePaddle](https://github.com/yeyupiaoling/AudioClassification-PaddlePaddle) for years of attention, constructive advice and great help.\n- Many thanks to [mymagicpower](https://github.com/mymagicpower) for the Java implementation of ASR upon [short](https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_sdk) and [long](https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_long_audio_sdk) audio files.\n- Many thanks to [JiehangXie](https://github.com/JiehangXie)/[PaddleBoBo](https://github.com/JiehangXie/PaddleBoBo) for developing Virtual Uploader(VUP)/Virtual YouTuber(VTuber) with PaddleSpeech TTS function.\n- Many thanks to [745165806](https://github.com/745165806)/[PaddleSpeechTask](https://github.com/745165806/PaddleSpeechTask) for contributing Punctuation Restoration model.\n- Many thanks to [kslz](https://github.com/745165806) for supplementary Chinese documents.\n- Many thanks to [awmmmm](https://github.com/awmmmm) for contributing fastspeech2 aishell3 conformer pretrained model.\n- Many thanks to [phecda-xu](https://github.com/phecda-xu)/[PaddleDubbing](https://github.com/phecda-xu/PaddleDubbing) for developing a dubbing tool with GUI based on PaddleSpeech TTS model.\n- Many thanks to [jerryuhoo](https://github.com/jerryuhoo)/[VTuberTalk](https://github.com/jerryuhoo/VTuberTalk) for developing a GUI tool based on PaddleSpeech TTS and code for making datasets from videos based on PaddleSpeech ASR.\n- Many thanks to [vpegasus](https://github.com/vpegasus)/[xuesebot](https://github.com/vpegasus/xuesebot) for developing a rasa chatbot,which is able to speak and listen thanks to PaddleSpeech.\n- Many thanks to [chenkui164](https://github.com/chenkui164)/[FastASR](https://github.com/chenkui164/FastASR) for the C++ inference implementation of PaddleSpeech ASR.\n- Many thanks to [heyudage](https://github.com/heyudage)/[VoiceTyping](https://github.com/heyudage/VoiceTyping) for the real-time voice typing tool implementation of PaddleSpeech ASR streaming services.\n- Many thanks to [EscaticZheng](https://github.com/EscaticZheng)/[ps3.9wheel-install](https://github.com/EscaticZheng/ps3.9wheel-install) for the python3.9 prebuilt wheel for PaddleSpeech installation in Windows without Viusal Studio.\nBesides, PaddleSpeech depends on a lot of open source repositories. See [references](./docs/source/reference.md) for more information.\n- Many thanks to [chinobing](https://github.com/chinobing)/[FastAPI-PaddleSpeech-Audio-To-Text](https://github.com/chinobing/FastAPI-PaddleSpeech-Audio-To-Text) for converting audio to text based on FastAPI and PaddleSpeech.\n- Many thanks to [MistEO](https://github.com/MistEO)/[Pallas-Bot](https://github.com/MistEO/Pallas-Bot) for QQ bot based on PaddleSpeech TTS.\n\n<a name=\"License\"></a>\n## License\n\nPaddleSpeech is provided under the [Apache-2.0 License](./LICENSE).\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/PaddlePaddle/PaddleSpeech.svg)](https://starchart.cc/PaddlePaddle/PaddleSpeech)\n"
        },
        {
          "name": "README_cn.md",
          "type": "blob",
          "size": 47.4892578125,
          "content": "(简体中文|[English](./README.md))\n<p align=\"center\">\n  <img src=\"./docs/images/PaddleSpeech_logo.png\" />\n</p>\n\n\n<p align=\"center\">\n    <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache%202-red.svg\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/releases\"><img src=\"https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa\"></a>\n    <a href=\"support os\"><img src=\"https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg\"></a>\n    <a href=\"\"><img src=\"https://img.shields.io/badge/python-3.8+-aff.svg\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors\"><img src=\"https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/commits\"><img src=\"https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/issues\"><img src=\"https://img.shields.io/github/issues/PaddlePaddle/PaddleSpeech?color=9cc\"></a>\n    <a href=\"https://github.com/PaddlePaddle/PaddleSpeech/stargazers\"><img src=\"https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?color=ccf\"></a>\n    <a href=\"=https://pypi.org/project/paddlespeech/\"><img src=\"https://img.shields.io/pypi/dm/PaddleSpeech\"></a>\n    <a href=\"=https://pypi.org/project/paddlespeech/\"><img src=\"https://static.pepy.tech/badge/paddlespeech\"></a>\n    <a href=\"https://huggingface.co/spaces\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\"></a>\n</p>\n<div align=\"center\">  \n<h4>\n    <a href=\"#安装\"> 安装 </a>\n  | <a href=\"#快速开始\"> 快速开始 </a>\n  | <a href=\"#教程文档\"> 教程文档 </a>\n  | <a href=\"#模型列表\"> 模型列表 </a>\n  | <a href=\"https://aistudio.baidu.com/aistudio/course/introduce/25130\"> AIStudio 课程 </a>\n  | <a href=\"https://arxiv.org/abs/2205.12007\"> NAACL2022 论文 </a>\n  | <a href=\"https://gitee.com/paddlepaddle/PaddleSpeech\"> Gitee \n</h4>\n</div>\n\n\n------------------------------------------------------------------------------------\n\n**PaddleSpeech** 是基于飞桨 [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) 的语音方向的开源模型库，用于语音和音频中的各种关键任务的开发，包含大量基于深度学习前沿和有影响力的模型，一些典型的应用示例如下：\n\n**PaddleSpeech** 荣获 [NAACL2022 Best Demo Award](https://2022.naacl.org/blog/best-demo-award/), 请访问 [Arxiv](https://arxiv.org/abs/2205.12007) 论文。\n  \n### 效果展示\n\n##### 语音识别\n\n<div align = \"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> 输入音频  </th>\n      <th width=\"550\"> 识别结果 </th>\n    </tr>\n  </thead>\n  <tbody>\n   <tr>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200 style=\"max-width: 100%;\"></a><br>\n      </td>\n      <td >I knocked at the door on the ancient side of the building.</td>\n    </tr>\n    <tr>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n      <td>我认为跑步最重要的就是给我带来了身体健康。</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n##### 语音翻译 (英译中)\n\n<div align = \"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> 输入音频 </th>\n      <th width=\"550\"> 翻译结果 </th>\n    </tr>\n  </thead>\n  <tbody>\n   <tr>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200 style=\"max-width: 100%;\"></a><br>\n      </td>\n      <td >我 在 这栋 建筑 的 古老 门上 敲门。</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n##### 语音合成\n<div align = \"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th width=\"550\">输入文本</th>\n      <th>合成音频</th>\n    </tr>\n  </thead>\n  <tbody>\n   <tr>\n      <td >Life was like a box of chocolates, you never know what you're gonna get.</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/tacotron2_ljspeech_waveflow_samples_0.2/sentence_1.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td >早上好，今天是2020/10/29，最低温度是-3°C。</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/parakeet_espnet_fs2_pwg_demo/tn_g2p/parakeet/001.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td >季姬寂，集鸡，鸡即棘鸡。棘鸡饥叽，季姬及箕稷济鸡。鸡既济，跻姬笈，季姬忌，急咭鸡，鸡急，继圾几，季姬急，即籍箕击鸡，箕疾击几伎，伎即齑，鸡叽集几基，季姬急极屐击鸡，鸡既殛，季姬激，即记《季姬击鸡记》。</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/jijiji.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td>大家好，我是 parrot 虚拟老师，我们来读一首诗，我与春风皆过客，I and the spring breeze are passing by，你携秋水揽星河，you take the autumn water to take the galaxy。</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/labixiaoxin.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td>宜家唔系事必要你讲，但系你所讲嘅说话将会变成呈堂证供。</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/chengtangzhenggong.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n    <tr>\n      <td>各个国家有各个国家嘅国歌</td>\n      <td align = \"center\">\n      <a href=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/gegege.wav\" rel=\"nofollow\">\n            <img align=\"center\" src=\"./docs/images/audio_icon.png\" width=\"200\" style=\"max-width: 100%;\"></a><br>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n更多合成音频，可以参考 [PaddleSpeech 语音合成音频示例](https://paddlespeech.readthedocs.io/en/latest/tts/demo.html)。\n\n##### 标点恢复\n<div align = \"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th width=\"390\"> 输入文本 </th>\n      <th width=\"390\"> 输出文本 </th>\n    </tr>\n  </thead>\n  <tbody>\n   <tr>\n      <td>今天的天气真不错啊你下午有空吗我想约你一起去吃饭</td>\n      <td>今天的天气真不错啊！你下午有空吗？我想约你一起去吃饭。</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n### 特性\n\n本项目采用了易用、高效、灵活以及可扩展的实现，旨在为工业应用、学术研究提供更好的支持，实现的功能包含训练、推断以及测试模块，以及部署过程，主要包括\n- 📦 **易用性**: 安装门槛低，可使用 [CLI](#quick-start) 快速开始。\n- 🏆 **对标 SoTA**: 提供了高速、轻量级模型，且借鉴了最前沿的技术。\n- 🏆 **流式 ASR 和 TTS 系统**：工业级的端到端流式识别、流式合成系统。\n- 💯 **基于规则的中文前端**: 我们的前端包含文本正则化和字音转换（G2P）。此外，我们使用自定义语言规则来适应中文语境。\n- **多种工业界以及学术界主流功能支持**:\n  - 🛎️ 典型音频任务: 本工具包提供了音频任务如音频分类、语音翻译、自动语音识别、文本转语音、语音合成、声纹识别、KWS等任务的实现。\n  - 🔬 主流模型及数据集: 本工具包实现了参与整条语音任务流水线的各个模块，并且采用了主流数据集如 LibriSpeech、LJSpeech、AIShell、CSMSC，详情请见 [模型列表](#model-list)。\n  - 🧩 级联模型应用: 作为传统语音任务的扩展，我们结合了自然语言处理、计算机视觉等任务，实现更接近实际需求的产业级应用。\n\n### 近期更新\n- 👑 2023.05.31: 新增 [WavLM ASR-en](https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/examples/librispeech/asr5), 基于WavLM的英语识别微调，使用LibriSpeech数据集\n- 🎉 2023.05.18: 新增 [Squeezeformer](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/examples/aishell/asr1), 使用Squeezeformer进行训练，使用Aishell数据集\n- 👑 2023.05.04: 新增 [HuBERT ASR-en](https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/examples/librispeech/asr4), 基于HuBERT的英语识别微调，使用LibriSpeech数据集\n- ⚡ 2023.04.28: 修正 [0-d tensor](https://github.com/PaddlePaddle/PaddleSpeech/pull/3214), 配合PaddlePaddle2.5升级修改了0-d tensor的问题。\n- 👑 2023.04.25: 新增 [U2 conformer 的 AMP 训练](https://github.com/PaddlePaddle/PaddleSpeech/pull/3167).\n- 👑 2023.04.06: 新增 [srt格式字幕生成功能](./demos/streaming_asr_server)。\n- 🔥 2023.03.14: 新增基于 Opencpop 数据集的 SVS (歌唱合成) 示例，包含 [DiffSinger](./examples/opencpop/svs1)、[PWGAN](./examples/opencpop/voc1) 和 [HiFiGAN](./examples/opencpop/voc5)，效果持续优化中。\n- 👑 2023.03.09: 新增 [Wav2vec2ASR-zh](./examples/aishell/asr3)。\n- 🎉 2023.03.07: 新增 [TTS ARM Linux C++ 部署示例 (包含 C++ 中文文本前端模块)](./demos/TTSArmLinux)。\n- 🔥 2023.03.03: 新增声音转换模型 [StarGANv2-VC 合成流程](./examples/vctk/vc3)。\n- 🎉 2023.02.16: 新增[粤语语音合成](./examples/canton/tts3)。\n- 🔥 2023.01.10: 新增[中英混合 ASR CLI 和 Demos](./demos/speech_recognition)。\n- 👑 2023.01.06: 新增 [ASR 中英混合 tal_cs 训练推理流程](./examples/tal_cs/asr1/)。\n- 🎉 2022.12.02: 新增[端到端韵律预测全流程](./examples/csmsc/tts3_rhy) (包含在声学模型中使用韵律标签)。\n- 🎉 2022.11.30: 新增 [TTS Android 部署示例](./demos/TTSAndroid)。\n- 🤗 2022.11.28: PP-TTS and PP-ASR 示例可在 [AIStudio](https://aistudio.baidu.com/aistudio/modelsoverview) 和[飞桨官网](https://www.paddlepaddle.org.cn/models)体验！\n- 👑 2022.11.18: 新增 [Whisper CLI 和 Demos](https://github.com/PaddlePaddle/PaddleSpeech/pull/2640), 支持多种语言的识别与翻译。\n- 🔥 2022.11.18: 新增 [Wav2vec2 CLI 和 Demos](./demos/speech_ssl), 支持 ASR 和特征提取。\n- 🎉 2022.11.17: TTS 新增[高质量男性音色](https://github.com/PaddlePaddle/PaddleSpeech/pull/2660)。\n- 🔥 2022.11.07: 新增 [U2/U2++ 高性能流式 ASR C++ 部署](./speechx/examples/u2pp_ol/wenetspeech)。\n- 👑 2022.11.01: [中英文混合 TTS](./examples/zh_en_tts/tts3) 新增 [Adversarial Loss](https://arxiv.org/pdf/1907.04448.pdf) 模块。\n- 🔥 2022.10.26: TTS 新增[韵律预测](./develop/examples/other/rhy)功能。\n- 🎉 2022.10.21: TTS 中文文本前端新增 [SSML](https://github.com/PaddlePaddle/PaddleSpeech/discussions/2538) 功能。\n- 👑 2022.10.11: 新增 [Wav2vec2ASR-en](./examples/librispeech/asr3), 在 LibriSpeech 上针对 ASR 任务对 wav2vec2.0 的 finetuning。\n- 🔥 2022.09.26: 新增 Voice Cloning, TTS finetune 和 [ERNIE-SAT](https://arxiv.org/abs/2211.03545) 到 [PaddleSpeech 网页应用](./demos/speech_web)。\n- ⚡ 2022.09.09: 新增基于 ECAPA-TDNN 声纹模型的 AISHELL-3 Voice Cloning [示例](./examples/aishell3/vc2)。\n- ⚡ 2022.08.25: 发布 TTS [finetune](./examples/other/tts_finetune/tts3) 示例。\n- 🔥 2022.08.22: 新增 [ERNIE-SAT](https://arxiv.org/abs/2211.03545) 模型: [ERNIE-SAT-vctk](./examples/vctk/ernie_sat)、[ERNIE-SAT-aishell3](./examples/aishell3/ernie_sat)、[ERNIE-SAT-zh_en](./examples/aishell3_vctk/ernie_sat)。\n- 🔥 2022.08.15: 将 [g2pW](https://github.com/GitYCC/g2pW) 引入 TTS 中文文本前端。\n- 🔥 2022.08.09: 发布[中英文混合 TTS](./examples/zh_en_tts/tts3)。\n- ⚡ 2022.08.03: TTS CLI 新增 ONNXRuntime 推理方式。\n- 🎉 2022.07.18: 发布 VITS 模型: [VITS-csmsc](./examples/csmsc/vits)、[VITS-aishell3](./examples/aishell3/vits)、[VITS-VC](./examples/aishell3/vits-vc)。\n- 🎉 2022.06.22: 所有 TTS 模型支持了 ONNX 格式。\n- 🍀 2022.06.17: 新增 [PaddleSpeech 网页应用](./demos/speech_web)。\n- 👑 2022.05.13: PaddleSpeech 发布 [PP-ASR](./docs/source/asr/PPASR_cn.md) 流式语音识别系统、[PP-TTS](./docs/source/tts/PPTTS_cn.md) 流式语音合成系统、[PP-VPR](docs/source/vpr/PPVPR_cn.md) 全链路声纹识别系统\n- 👏🏻 2022.05.06: PaddleSpeech Streaming Server 上线！覆盖了语音识别（标点恢复、时间戳）和语音合成。\n- 👏🏻 2022.05.06: PaddleSpeech Server 上线！覆盖了声音分类、语音识别、语音合成、声纹识别，标点恢复。\n- 👏🏻 2022.03.28: PaddleSpeech CLI 覆盖声音分类、语音识别、语音翻译（英译中）、语音合成和声纹验证。\n- 👏🏻 2021.12.10: PaddleSpeech CLI 支持语音分类, 语音识别, 语音翻译（英译中）和语音合成。\n\n\n ### 🔥 加入技术交流群获取入群福利\n\n - 3 日直播课链接: 深度解读 【一句话语音合成】【小样本语音合成】【定制化语音识别】语音交互技术\n - 20G 学习大礼包：视频课程、前沿论文与学习资料\n  \n微信扫描二维码关注公众号，点击“马上报名”填写问卷加入官方交流群，获得更高效的问题答疑，与各行各业开发者充分交流，期待您的加入。\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/30135920/212860467-9e943cc3-8be8-49a4-97fd-7c94aad8e979.jpg\"  width = \"200\"  />\n</div>\n\n<a name=\"安装\"></a>\n## 安装\n\n我们强烈建议用户在 **Linux** 环境下，*3.8* 以上版本的 *python* 上安装 PaddleSpeech。同时，有一些Paddle新版本的内容没有在做适配的支持，因此目前只能使用2.5.1及之前的版本。\n\n### 相关依赖\n+ gcc >= 4.8.5\n+ paddlepaddle <= 2.5.1\n+ python >= 3.8\n+ linux(推荐), mac, windows\n\nPaddleSpeech 依赖于 paddlepaddle，安装可以参考[ paddlepaddle 官网](https://www.paddlepaddle.org.cn/)，根据自己机器的情况进行选择。这里给出 cpu 版本示例，其它版本大家可以根据自己机器的情况进行安装。\n\n```shell\npip install paddlepaddle -i https://mirror.baidu.com/pypi/simple\n```\n你也可以安装指定版本的paddlepaddle，或者安装 develop 版本。\n```bash\n# 安装2.4.1版本. 注意：2.4.1只是一个示例，请按照对paddlepaddle的最小依赖进行选择。\npip install paddlepaddle==2.4.1 -i https://mirror.baidu.com/pypi/simple\n# 安装 develop 版本\npip install paddlepaddle==0.0.0 -f https://www.paddlepaddle.org.cn/whl/linux/cpu-mkl/develop.html\n```\nPaddleSpeech 快速安装方式有两种，一种是 pip 安装，一种是源码编译（推荐）。\n\n### pip 安装\n```shell\npip install pytest-runner\npip install paddlespeech\n```\n\n### 源码编译\n```shell\ngit clone https://github.com/PaddlePaddle/PaddleSpeech.git\ncd PaddleSpeech\npip install pytest-runner\npip install .\n```\n\n更多关于安装问题，如 conda 环境，librosa 依赖的系统库，gcc 环境问题，kaldi 安装等，可以参考这篇[安装文档](docs/source/install_cn.md)，如安装上遇到问题可以在 [#2150](https://github.com/PaddlePaddle/PaddleSpeech/issues/2150) 上留言以及查找相关问题\n\n<a name=\"快速开始\"></a>\n## 快速开始\n安装完成后，开发者可以通过命令行或者 Python 快速开始，命令行模式下改变 `--input` 可以尝试用自己的音频或文本测试，支持 16k wav 格式音频。\n\n你也可以在 `aistudio` 中快速体验 👉🏻[一键预测，快速上手 Speech 开发任务](https://aistudio.baidu.com/aistudio/projectdetail/4353348?sUid=2470186&shared=1&ts=1660878142250)。\n\n测试音频示例下载\n```shell\nwget -c https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav\nwget -c https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav\n```\n\n### 语音识别\n<details><summary>&emsp;（点击可展开）开源中文语音识别</summary>\n\n命令行一键体验\n\n```shell\npaddlespeech asr --lang zh --input zh.wav\n```\n\nPython API 一键预测\n\n```python\n>>> from paddlespeech.cli.asr.infer import ASRExecutor\n>>> asr = ASRExecutor()\n>>> result = asr(audio_file=\"zh.wav\")\n>>> print(result)\n我认为跑步最重要的就是给我带来了身体健康\n```\n</details>\n\n### 语音合成\n\n<details><summary>&emsp;开源中文语音合成</summary>\n\n输出 24k 采样率wav格式音频\n\n\n命令行一键体验\n\n```shell\npaddlespeech tts --input \"你好，欢迎使用百度飞桨深度学习框架！\" --output output.wav\n```\n\nPython API 一键预测\n\n```python\n>>> from paddlespeech.cli.tts.infer import TTSExecutor\n>>> tts = TTSExecutor()\n>>> tts(text=\"今天天气十分不错。\", output=\"output.wav\")\n```\n- 语音合成的 web demo 已经集成进了 [Huggingface Spaces](https://huggingface.co/spaces). 请参考: [TTS Demo](https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS)\n\n</details>\n\n### 声音分类   \n\n<details><summary>&emsp;适配多场景的开放领域声音分类工具</summary>\n\n基于 AudioSet 数据集 527 个类别的声音分类模型\n\n命令行一键体验\n\n```shell\npaddlespeech cls --input zh.wav\n```\n\npython API 一键预测\n\n```python\n>>> from paddlespeech.cli.cls.infer import CLSExecutor\n>>> cls = CLSExecutor()\n>>> result = cls(audio_file=\"zh.wav\")\n>>> print(result)\nSpeech 0.9027186632156372\n```\n\n</details>\n\n### 声纹提取\n\n<details><summary>&emsp;工业级声纹提取工具</summary>\n\n命令行一键体验\n\n```shell\npaddlespeech vector --task spk --input zh.wav\n```\n\nPython API 一键预测\n\n```python\n>>> from paddlespeech.cli.vector import VectorExecutor\n>>> vec = VectorExecutor()\n>>> result = vec(audio_file=\"zh.wav\")\n>>> print(result) # 187维向量\n[ -0.19083306   9.474295   -14.122263    -2.0916545    0.04848729\n   4.9295826    1.4780062    0.3733844   10.695862     3.2697146\n  -4.48199     -0.6617882   -9.170393   -11.1568775   -1.2358263 ...]\n```\n\n</details>\n\n### 标点恢复 \n\n<details><summary>&emsp;一键恢复文本标点，可与ASR模型配合使用</summary>\n\n命令行一键体验\n\n```shell\npaddlespeech text --task punc --input 今天的天气真不错啊你下午有空吗我想约你一起去吃饭\n```\n\nPython API 一键预测\n\n```python\n>>> from paddlespeech.cli.text.infer import TextExecutor\n>>> text_punc = TextExecutor()\n>>> result = text_punc(text=\"今天的天气真不错啊你下午有空吗我想约你一起去吃饭\")\n今天的天气真不错啊！你下午有空吗？我想约你一起去吃饭。\n```\n\n</details>\n\n### 语音翻译\n\n<details><summary>&emsp;端到端英译中语音翻译工具</summary>\n\n使用预编译的 kaldi 相关工具，只支持在 Ubuntu 系统中体验\n\n命令行一键体验\n\n```shell\npaddlespeech st --input en.wav\n```\n\npython API 一键预测\n\n```python\n>>> from paddlespeech.cli.st.infer import STExecutor\n>>> st = STExecutor()\n>>> result = st(audio_file=\"en.wav\")\n['我 在 这栋 建筑 的 古老 门上 敲门 。']\n```\n\n</details>\n\n\n<a name=\"快速使用服务\"></a>\n## 快速使用服务\n安装完成后，开发者可以通过命令行一键启动语音识别，语音合成，音频分类等多种服务。\n\n你可以在 AI Studio 中快速体验：[SpeechServer 一键部署](https://aistudio.baidu.com/aistudio/projectdetail/4354592?sUid=2470186&shared=1&ts=1660878208266)\n\n**启动服务**     \n```shell\npaddlespeech_server start --config_file ./demos/speech_server/conf/application.yaml\n```\n\n**访问语音识别服务**     \n```shell\npaddlespeech_client asr --server_ip 127.0.0.1 --port 8090 --input input_16k.wav\n```\n\n**访问语音合成服务**     \n```shell\npaddlespeech_client tts --server_ip 127.0.0.1 --port 8090 --input \"您好，欢迎使用百度飞桨语音合成服务。\" --output output.wav\n```\n\n**访问音频分类服务**     \n```shell\npaddlespeech_client cls --server_ip 127.0.0.1 --port 8090 --input input.wav\n```\n\n更多服务相关的命令行使用信息，请参考 [demos](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server)\n\n<a name=\"快速使用流式服务\"></a>\n## 快速使用流式服务\n\n开发者可以尝试 [流式 ASR](./demos/streaming_asr_server/README.md) 和 [流式 TTS](./demos/streaming_tts_server/README.md) 服务.\n\n**启动流式 ASR 服务**\n\n```\npaddlespeech_server start --config_file ./demos/streaming_asr_server/conf/application.yaml\n```\n\n**访问流式 ASR 服务**     \n\n```\npaddlespeech_client asr_online --server_ip 127.0.0.1 --port 8090 --input input_16k.wav\n```\n\n**启动流式 TTS 服务**\n\n```\npaddlespeech_server start --config_file ./demos/streaming_tts_server/conf/tts_online_application.yaml\n```\n\n**访问流式 TTS 服务**     \n\n```\npaddlespeech_client tts_online --server_ip 127.0.0.1 --port 8092 --protocol http --input \"您好，欢迎使用百度飞桨语音合成服务。\" --output output.wav\n```\n\n更多信息参看： [流式 ASR](./demos/streaming_asr_server/README.md) 和 [流式 TTS](./demos/streaming_tts_server/README.md) \n\n<a name=\"模型列表\"></a>\n## 模型列表\nPaddleSpeech 支持很多主流的模型，并提供了预训练模型，详情请见[模型列表](./docs/source/released_model.md)。\n\n<a name=\"语音识别模型\"></a>\n\nPaddleSpeech 的 **语音转文本** 包含语音识别声学模型、语音识别语言模型和语音翻译, 详情如下：\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th>语音转文本模块类型</th>\n      <th>数据集</th>\n      <th>模型类型</th>\n      <th>脚本</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"4\">语音识别</td>\n      <td rowspan=\"2\" >Aishell</td>\n      <td >DeepSpeech2 RNN + Conv based Models</td>\n      <td>\n      <a href = \"./examples/aishell/asr0\">deepspeech2-aishell</a>\n      </td>\n    </tr>\n    <tr>\n      <td>Transformer based Attention Models </td>\n      <td>\n      <a href = \"./examples/aishell/asr1\">u2.transformer.conformer-aishell</a>\n      </td>\n    </tr>\n      <tr>\n      <td> Librispeech</td>\n      <td>Transformer based Attention Models </td>\n      <td>\n      <a href = \"./examples/librispeech/asr0\">deepspeech2-librispeech</a> / <a href = \"./examples/librispeech/asr1\">transformer.conformer.u2-librispeech</a>  / <a href = \"./examples/librispeech/asr2\">transformer.conformer.u2-kaldi-librispeech</a>\n      </td>\n      </td>\n    </tr>\n    <tr>\n      <td>TIMIT</td>\n      <td>Unified Streaming & Non-streaming Two-pass</td>\n      <td>\n    <a href = \"./examples/timit/asr1\"> u2-timit</a>\n      </td>\n    </tr>\n  <tr>\n  <td>对齐</td>\n  <td>THCHS30</td>\n  <td>MFA</td>\n  <td>\n  <a href = \".examples/thchs30/align0\">mfa-thchs30</a>\n  </td>\n  </tr>\n   <tr>\n      <td rowspan=\"1\">语言模型</td>\n      <td colspan = \"2\">Ngram 语言模型</td>\n      <td>\n      <a href = \"./examples/other/ngram_lm\">kenlm</a>\n      </td>\n    </tr>\n    <tr>\n      <td rowspan=\"2\">语音翻译（英译中）</td> \n      <td rowspan=\"2\">TED En-Zh</td>\n      <td>Transformer + ASR MTL</td>\n      <td>\n      <a href = \"./examples/ted_en_zh/st0\">transformer-ted</a>\n      </td>\n  </tr>\n  <tr>\n      <td>FAT + Transformer + ASR MTL</td>\n      <td>\n      <a href = \"./examples/ted_en_zh/st1\">fat-st-ted</a>\n      </td>\n  </tr>\n  </tbody>\n</table>\n\n<a name=\"语音合成模型\"></a>\n\nPaddleSpeech 的 **语音合成** 主要包含三个模块：文本前端、声学模型和声码器。声学模型和声码器模型如下：\n\n<table>\n  <thead>\n    <tr>\n      <th> 语音合成模块类型 </th>\n      <th> 模型类型 </th>\n      <th> 数据集  </th>\n      <th> 脚本  </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n    <td> 文本前端</td>\n    <td colspan=\"2\"> &emsp; </td>\n    <td>\n    <a href = \"./examples/other/tn\">tn</a> / <a href = \"./examples/other/g2p\">g2p</a>\n    </td>\n   </tr>\n   <tr>\n      <td rowspan=\"6\">声学模型</td>\n      <td>Tacotron2</td>\n      <td>LJSpeech / CSMSC</td>\n      <td>\n      <a href = \"./examples/ljspeech/tts0\">tacotron2-ljspeech</a> / <a href = \"./examples/csmsc/tts0\">tacotron2-csmsc</a>\n      </td>\n   </tr>\n   <tr>\n      <td>Transformer TTS</td>\n      <td>LJSpeech</td>\n      <td>\n      <a href = \"./examples/ljspeech/tts1\">transformer-ljspeech</a>\n      </td>\n   </tr>\n   <tr>\n      <td>SpeedySpeech</td>\n      <td>CSMSC</td>\n      <td >\n      <a href = \"./examples/csmsc/tts2\">speedyspeech-csmsc</a>\n      </td>\n   </tr>\n   <tr>\n      <td>FastSpeech2</td>\n      <td>LJSpeech / VCTK / CSMSC / AISHELL-3 / ZH_EN / finetune</td>\n      <td>\n      <a href = \"./examples/ljspeech/tts3\">fastspeech2-ljspeech</a> / <a href = \"./examples/vctk/tts3\">fastspeech2-vctk</a> / <a href = \"./examples/csmsc/tts3\">fastspeech2-csmsc</a> / <a href = \"./examples/aishell3/tts3\">fastspeech2-aishell3</a> / <a href = \"./examples/zh_en_tts/tts3\">fastspeech2-zh_en</a> / <a href = \"./examples/other/tts_finetune/tts3\">fastspeech2-finetune</a>\n      </td>\n   </tr>\n   <tr>\n      <td><a href = \"https://arxiv.org/abs/2211.03545\">ERNIE-SAT</a></td>\n      <td>VCTK / AISHELL-3 / ZH_EN</td>\n      <td>\n      <a href = \"./examples/vctk/ernie_sat\">ERNIE-SAT-vctk</a> / <a href = \"./examples/aishell3/ernie_sat\">ERNIE-SAT-aishell3</a> / <a href = \"./examples/aishell3_vctk/ernie_sat\">ERNIE-SAT-zh_en</a>\n      </td>\n   </tr>\n   <tr>\n      <td>DiffSinger</td>\n      <td>Opencpop</td>\n      <td>\n      <a href = \"./examples/opencpop/svs1\">DiffSinger-opencpop</a>\n      </td>\n   </tr>\n   <tr>\n      <td rowspan=\"6\">声码器</td>\n      <td >WaveFlow</td>\n      <td >LJSpeech</td>\n      <td>\n      <a href = \"./examples/ljspeech/voc0\">waveflow-ljspeech</a>\n      </td>\n    </tr>\n    <tr>\n      <td >Parallel WaveGAN</td>\n      <td >LJSpeech / VCTK / CSMSC / AISHELL-3 / Opencpop</td>\n      <td>\n      <a href = \"./examples/ljspeech/voc1\">PWGAN-ljspeech</a> / <a href = \"./examples/vctk/voc1\">PWGAN-vctk</a> / <a href = \"./examples/csmsc/voc1\">PWGAN-csmsc</a> /  <a href = \"./examples/aishell3/voc1\">PWGAN-aishell3</a> / <a href = \"./examples/opencpop/voc1\">PWGAN-opencpop</a>\n      </td>\n    </tr>\n    <tr>\n      <td >Multi Band MelGAN</td>\n      <td >CSMSC</td>\n      <td>\n      <a href = \"./examples/csmsc/voc3\">Multi Band MelGAN-csmsc</a> \n      </td>\n    </tr>\n    <tr>\n      <td >Style MelGAN</td>\n      <td >CSMSC</td>\n      <td>\n      <a href = \"./examples/csmsc/voc4\">Style MelGAN-csmsc</a> \n      </td>\n    </tr>\n    <tr>\n      <td >HiFiGAN</td>\n      <td >LJSpeech / VCTK / CSMSC / AISHELL-3 / Opencpop</td>\n      <td>\n      <a href = \"./examples/ljspeech/voc5\">HiFiGAN-ljspeech</a> / <a href = \"./examples/vctk/voc5\">HiFiGAN-vctk</a> / <a href = \"./examples/csmsc/voc5\">HiFiGAN-csmsc</a> / <a href = \"./examples/aishell3/voc5\">HiFiGAN-aishell3</a> / <a href = \"./examples/opencpop/voc5\">HiFiGAN-opencpop</a>\n      </td>\n    </tr>\n    <tr>\n      <td >WaveRNN</td>\n      <td >CSMSC</td>\n      <td>\n      <a href = \"./examples/csmsc/voc6\">WaveRNN-csmsc</a>\n      </td>\n    </tr>\n    <tr>\n      <td rowspan=\"5\">声音克隆</td>\n      <td>GE2E</td>\n      <td >Librispeech, etc.</td>\n      <td>\n      <a href = \"./examples/other/ge2e\">GE2E</a>\n      </td>\n    </tr>\n    <tr>\n      <td>SV2TTS (GE2E + Tacotron2)</td>\n      <td>AISHELL-3</td>\n      <td>\n      <a href = \"./examples/aishell3/vc0\">VC0</a>\n      </td>\n    </tr>\n    <tr>\n      <td>SV2TTS (GE2E + FastSpeech2)</td>\n      <td>AISHELL-3</td>\n      <td>\n      <a href = \"./examples/aishell3/vc1\">VC1</a>\n      </td>\n    </tr>\n    <tr>\n      <td>SV2TTS (ECAPA-TDNN + FastSpeech2)</td>\n      <td>AISHELL-3</td>\n      <td>\n      <a href = \"./examples/aishell3/vc2\">VC2</a>\n      </td>\n    </tr>\n    <tr>\n      <td>GE2E + VITS</td>\n      <td>AISHELL-3</td>\n      <td>\n      <a href = \"./examples/aishell3/vits-vc\">VITS-VC</a>\n      </td>\n    </tr>\n     <tr>\n      <td rowspan=\"3\">端到端</td>\n      <td>VITS</td>\n      <td>CSMSC / AISHELL-3</td>\n      <td>\n      <a href = \"./examples/csmsc/vits\">VITS-csmsc</a> / <a href = \"./examples/aishell3/vits\">VITS-aishell3</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n\n<a name=\"声音分类模型\"></a>\n**声音分类**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> 任务 </th>\n      <th> 数据集 </th>\n      <th> 模型类型 </th>\n      <th> 脚本</th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>声音分类</td>\n      <td>ESC-50</td>\n      <td>PANN</td>\n      <td>\n      <a href = \"./examples/esc50/cls0\">pann-esc50</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n\n<a name=\"语音唤醒模型\"></a>\n\n**语音唤醒**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> 任务 </th>\n      <th> 数据集 </th>\n      <th> 模型类型 </th>\n      <th> 脚本 </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>语音唤醒</td>\n      <td>hey-snips</td>\n      <td>MDTC</td>\n      <td>\n      <a href = \"./examples/hey_snips/kws0\">mdtc-hey-snips</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"声纹识别模型\"></a>\n\n**声纹识别**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> 任务 </th>\n      <th> 数据集 </th>\n      <th> 模型类型 </th>\n      <th> 脚本 </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>声纹识别</td>\n      <td>VoxCeleb1/2</td>\n      <td>ECAPA-TDNN</td>\n      <td>\n      <a href = \"./examples/voxceleb/sv0\">ecapa-tdnn-voxceleb12</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"说话人日志模型\"></a>\n\n**说话人日志**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> 任务 </th>\n      <th> 数据集 </th>\n      <th> 模型类型 </th>\n      <th> 脚本 </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>说话人日志</td>\n      <td>AMI</td>\n      <td>ECAPA-TDNN + AHC / SC</td>\n      <td>\n      <a href = \"./examples/ami/sd0\">ecapa-tdnn-ami</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"标点恢复模型\"></a>\n\n**标点恢复**\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th> 任务 </th>\n      <th> 数据集 </th>\n      <th> 模型类型 </th>\n      <th> 脚本 </th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n      <td>标点恢复</td>\n      <td>IWLST2012_zh</td>\n      <td>Ernie Linear</td>\n      <td>\n      <a href = \"./examples/iwslt2012/punc0\">iwslt2012-punc0</a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<a name=\"教程文档\"></a>\n## 教程文档\n\n对于 PaddleSpeech 的所关注的任务，以下指南有助于帮助开发者快速入门，了解语音相关核心思想。\n\n- [下载安装](./docs/source/install_cn.md)\n- [快速开始](#快速开始)\n- Notebook基础教程\n  - [声音分类](./docs/tutorial/cls/cls_tutorial.ipynb)\n  - [语音识别](./docs/tutorial/asr/tutorial_transformer.ipynb)\n  - [语音翻译](./docs/tutorial/st/st_tutorial.ipynb)\n  - [声音合成](./docs/tutorial/tts/tts_tutorial.ipynb)\n  - [示例Demo](./demos/README.md)\n- 进阶文档  \n  - [语音识别自定义训练](./docs/source/asr/quick_start.md)\n    - [简介](./docs/source/asr/models_introduction.md)\n    - [数据准备](./docs/source/asr/data_preparation.md)\n    - [Ngram 语言模型](./docs/source/asr/ngram_lm.md)\n  - [语音合成自定义训练](./docs/source/tts/quick_start.md)\n    - [简介](./docs/source/tts/models_introduction.md)\n    - [进阶用法](./docs/source/tts/advanced_usage.md)\n    - [中文文本前端](./docs/source/tts/zh_text_frontend.md)\n    - [测试语音样本](https://paddlespeech.readthedocs.io/en/latest/tts/demo.html)\n  - 声纹识别\n    - [声纹识别](./demos/speaker_verification/README_cn.md)\n    - [音频检索](./demos/audio_searching/README_cn.md)\n  - [声音分类](./demos/audio_tagging/README_cn.md)\n  - [语音翻译](./demos/speech_translation/README_cn.md)\n  - [服务化部署](./demos/speech_server/README_cn.md)\n- [模型列表](#模型列表)\n  - [语音识别](#语音识别模型)\n  - [语音合成](#语音合成模型)\n  - [声音分类](#声音分类模型)\n  - [声纹识别](#声纹识别模型)\n  - [说话人日志](#说话人日志模型)\n  - [标点恢复](#标点恢复模型)\n- [技术交流群](#技术交流群)\n- [欢迎贡献](#欢迎贡献)\n- [License](#License)\n\n\n语音合成模块最初被称为 [Parakeet](https://github.com/PaddlePaddle/Parakeet)，现在与此仓库合并。如果您对该任务的学术研究感兴趣，请参阅 [TTS 研究概述](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/docs/source/tts#overview)。此外，[模型介绍](https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/docs/source/tts/models_introduction.md) 是了解语音合成流程的一个很好的指南。\n\n\n## ⭐ 应用案例\n- **[PaddleBoBo](https://github.com/JiehangXie/PaddleBoBo): 使用 PaddleSpeech 的语音合成模块生成虚拟人的声音。**\n  \n<div align=\"center\"><a href=\"https://www.bilibili.com/video/BV1cL411V71o?share_source=copy_web\"><img src=\"https://ai-studio-static-online.cdn.bcebos.com/06fd746ab32042f398fb6f33f873e6869e846fe63c214596ae37860fe8103720\" / width=\"500px\"></a></div>\n\n- [PaddleSpeech 示例视频](https://paddlespeech.readthedocs.io/en/latest/demo_video.html)\n\n\n- **[VTuberTalk](https://github.com/jerryuhoo/VTuberTalk): 使用 PaddleSpeech 的语音合成和语音识别从视频中克隆人声。**\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/jerryuhoo/VTuberTalk/main/gui/gui.png\"  width = \"500px\"  />\n</div>\n\n\n## 引用\n\n要引用 PaddleSpeech 进行研究，请使用以下格式进行引用。\n```text\n@InProceedings{pmlr-v162-bai22d,\n  title = {{A}$^3${T}: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing},\n  author = {Bai, He and Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Li, Xintong and Huang, Liang},\n  booktitle = {Proceedings of the 39th International Conference on Machine Learning},\n  pages = {1399--1411},\n  year = {2022},\n  volume = {162},\n  series = {Proceedings of Machine Learning Research},\n  month = {17--23 Jul},\n  publisher = {PMLR},\n  pdf = {https://proceedings.mlr.press/v162/bai22d/bai22d.pdf},\n  url = {https://proceedings.mlr.press/v162/bai22d.html},\n}\n\n@inproceedings{zhang2022paddlespeech,\n    title = {PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit},\n    author = {Hui Zhang, Tian Yuan, Junkun Chen, Xintong Li, Renjie Zheng, Yuxin Huang, Xiaojie Chen, Enlei Gong, Zeyu Chen, Xiaoguang Hu, dianhai yu, Yanjun Ma, Liang Huang},\n    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations},\n    year = {2022},\n    publisher = {Association for Computational Linguistics},\n}\n\n@inproceedings{zheng2021fused,\n  title={Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation},\n  author={Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Huang, Liang},\n  booktitle={International Conference on Machine Learning},\n  pages={12736--12746},\n  year={2021},\n  organization={PMLR}\n}\n```\n\n<a name=\"欢迎贡献\"></a>\n## 参与 PaddleSpeech 的开发\n\n热烈欢迎您在 [Discussions](https://github.com/PaddlePaddle/PaddleSpeech/discussions) 中提交问题，并在 [Issues](https://github.com/PaddlePaddle/PaddleSpeech/issues) 中指出发现的 bug。此外，我们非常希望您参与到 PaddleSpeech 的开发中！\n\n### 贡献者\n<p align=\"center\">\n<a href=\"https://github.com/zh794390558\"><img src=\"https://avatars.githubusercontent.com/u/3038472?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Jackwaterveg\"><img src=\"https://avatars.githubusercontent.com/u/87408988?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/yt605155624\"><img src=\"https://avatars.githubusercontent.com/u/24568452?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Honei\"><img src=\"https://avatars.githubusercontent.com/u/11361692?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/KPatr1ck\"><img src=\"https://avatars.githubusercontent.com/u/22954146?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/kuke\"><img src=\"https://avatars.githubusercontent.com/u/3064195?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/lym0302\"><img src=\"https://avatars.githubusercontent.com/u/34430015?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/SmileGoat\"><img src=\"https://avatars.githubusercontent.com/u/56786796?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/xinghai-sun\"><img src=\"https://avatars.githubusercontent.com/u/7038341?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/pkuyym\"><img src=\"https://avatars.githubusercontent.com/u/5782283?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/LittleChenCc\"><img src=\"https://avatars.githubusercontent.com/u/10339970?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/qingen\"><img src=\"https://avatars.githubusercontent.com/u/3139179?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/D-DanielYang\"><img src=\"https://avatars.githubusercontent.com/u/23690325?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Mingxue-Xu\"><img src=\"https://avatars.githubusercontent.com/u/92848346?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/745165806\"><img src=\"https://avatars.githubusercontent.com/u/20623194?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/jerryuhoo\"><img src=\"https://avatars.githubusercontent.com/u/24245709?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/WilliamZhang06\"><img src=\"https://avatars.githubusercontent.com/u/97937340?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/chrisxu2016\"><img src=\"https://avatars.githubusercontent.com/u/18379485?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/iftaken\"><img src=\"https://avatars.githubusercontent.com/u/30135920?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/lfchener\"><img src=\"https://avatars.githubusercontent.com/u/6771821?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/BarryKCL\"><img src=\"https://avatars.githubusercontent.com/u/48039828?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/mmglove\"><img src=\"https://avatars.githubusercontent.com/u/38800877?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/gongel\"><img src=\"https://avatars.githubusercontent.com/u/24390500?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/luotao1\"><img src=\"https://avatars.githubusercontent.com/u/6836917?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/wanghaoshuang\"><img src=\"https://avatars.githubusercontent.com/u/7534971?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/kslz\"><img src=\"https://avatars.githubusercontent.com/u/54951765?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/JiehangXie\"><img src=\"https://avatars.githubusercontent.com/u/51190264?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/david-95\"><img src=\"https://avatars.githubusercontent.com/u/15189190?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/THUzyt21\"><img src=\"https://avatars.githubusercontent.com/u/91456992?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/buchongyu2\"><img src=\"https://avatars.githubusercontent.com/u/29157444?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/iclementine\"><img src=\"https://avatars.githubusercontent.com/u/16222986?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/phecda-xu\"><img src=\"https://avatars.githubusercontent.com/u/46859427?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/freeliuzc\"><img src=\"https://avatars.githubusercontent.com/u/23568094?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/ZeyuChen\"><img src=\"https://avatars.githubusercontent.com/u/1371212?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/ccrrong\"><img src=\"https://avatars.githubusercontent.com/u/101700995?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/AK391\"><img src=\"https://avatars.githubusercontent.com/u/81195143?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/qingqing01\"><img src=\"https://avatars.githubusercontent.com/u/7845005?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/0x45f\"><img src=\"https://avatars.githubusercontent.com/u/23097963?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/vpegasus\"><img src=\"https://avatars.githubusercontent.com/u/22723154?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/ericxk\"><img src=\"https://avatars.githubusercontent.com/u/4719594?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Betterman-qs\"><img src=\"https://avatars.githubusercontent.com/u/61459181?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/sneaxiy\"><img src=\"https://avatars.githubusercontent.com/u/32832641?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Doubledongli\"><img src=\"https://avatars.githubusercontent.com/u/20540661?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/apps/dependabot\"><img src=\"https://avatars.githubusercontent.com/in/29110?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/kvinwang\"><img src=\"https://avatars.githubusercontent.com/u/6442159?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/chenkui164\"><img src=\"https://avatars.githubusercontent.com/u/34813030?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/PaddleZhang\"><img src=\"https://avatars.githubusercontent.com/u/97284124?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/billishyahao\"><img src=\"https://avatars.githubusercontent.com/u/96406262?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/BrightXiaoHan\"><img src=\"https://avatars.githubusercontent.com/u/25839309?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/jiqiren11\"><img src=\"https://avatars.githubusercontent.com/u/82639260?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/ryanrussell\"><img src=\"https://avatars.githubusercontent.com/u/523300?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/GT-ZhangAcer\"><img src=\"https://avatars.githubusercontent.com/u/46156734?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/tensor-tang\"><img src=\"https://avatars.githubusercontent.com/u/21351065?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/hysunflower\"><img src=\"https://avatars.githubusercontent.com/u/52739577?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/oyjxer\"><img src=\"https://avatars.githubusercontent.com/u/16233945?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/JamesLim-sy\"><img src=\"https://avatars.githubusercontent.com/u/61349199?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/limpidezza\"><img src=\"https://avatars.githubusercontent.com/u/71760778?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/windstamp\"><img src=\"https://avatars.githubusercontent.com/u/34057289?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/AshishKarel\"><img src=\"https://avatars.githubusercontent.com/u/58069375?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/chesterkuo\"><img src=\"https://avatars.githubusercontent.com/u/6285069?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/YDX-2147483647\"><img src=\"https://avatars.githubusercontent.com/u/73375426?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/AdamBear\"><img src=\"https://avatars.githubusercontent.com/u/2288870?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/wwhu\"><img src=\"https://avatars.githubusercontent.com/u/6081200?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/lispc\"><img src=\"https://avatars.githubusercontent.com/u/2833376?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/harisankarh\"><img src=\"https://avatars.githubusercontent.com/u/1307053?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/pengzhendong\"><img src=\"https://avatars.githubusercontent.com/u/10704539?s=60&v=4\" width=75 height=75></a>\n<a href=\"https://github.com/Jackiexiao\"><img src=\"https://avatars.githubusercontent.com/u/18050469?s=60&v=4\" width=75 height=75></a>\n</p>\n\n## 致谢\n- 非常感谢 [HighCWu](https://github.com/HighCWu) 新增 [VITS-aishell3](./examples/aishell3/vits) 和 [VITS-VC](./examples/aishell3/vits-vc) 代码示例。\n- 非常感谢 [david-95](https://github.com/david-95) 修复 TTS 句尾多标点符号出错的问题，贡献补充多条程序和数据。为 TTS 中文文本前端新增 [SSML](https://github.com/PaddlePaddle/PaddleSpeech/discussions/2538) 功能。\n- 非常感谢 [BarryKCL](https://github.com/BarryKCL) 基于 [G2PW](https://github.com/GitYCC/g2pW) 对 TTS 中文文本前端的优化。\n- 非常感谢 [yeyupiaoling](https://github.com/yeyupiaoling)/[PPASR](https://github.com/yeyupiaoling/PPASR)/[PaddlePaddle-DeepSpeech](https://github.com/yeyupiaoling/PaddlePaddle-DeepSpeech)/[VoiceprintRecognition-PaddlePaddle](https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle)/[AudioClassification-PaddlePaddle](https://github.com/yeyupiaoling/AudioClassification-PaddlePaddle) 多年来的关注和建议，以及在诸多问题上的帮助。\n- 非常感谢 [mymagicpower](https://github.com/mymagicpower) 采用PaddleSpeech 对 ASR 的[短语音](https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_sdk)及[长语音](https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_long_audio_sdk)进行 Java 实现。\n- 非常感谢 [JiehangXie](https://github.com/JiehangXie)/[PaddleBoBo](https://github.com/JiehangXie/PaddleBoBo) 采用 PaddleSpeech 语音合成功能实现 Virtual Uploader(VUP)/Virtual YouTuber(VTuber) 虚拟主播。\n- 非常感谢 [745165806](https://github.com/745165806)/[PaddleSpeechTask](https://github.com/745165806/PaddleSpeechTask) 贡献标点重建相关模型。\n- 非常感谢 [kslz](https://github.com/kslz) 补充中文文档。\n- 非常感谢 [awmmmm](https://github.com/awmmmm) 提供 fastspeech2 aishell3 conformer 预训练模型。\n- 非常感谢 [phecda-xu](https://github.com/phecda-xu)/[PaddleDubbing](https://github.com/phecda-xu/PaddleDubbing) 基于 PaddleSpeech 的 TTS 模型搭建带 GUI 操作界面的配音工具。\n- 非常感谢 [jerryuhoo](https://github.com/jerryuhoo)/[VTuberTalk](https://github.com/jerryuhoo/VTuberTalk) 基于 PaddleSpeech 的 TTS GUI 界面和基于 ASR 制作数据集的相关代码。\n- 非常感谢 [vpegasus](https://github.com/vpegasus)/[xuesebot](https://github.com/vpegasus/xuesebot) 基于 PaddleSpeech 的 ASR 与 TTS 设计的可听、说对话机器人。\n- 非常感谢 [chenkui164](https://github.com/chenkui164)/[FastASR](https://github.com/chenkui164/FastASR) 对 PaddleSpeech 的 ASR 进行 C++ 推理实现。\n- 非常感谢 [heyudage](https://github.com/heyudage)/[VoiceTyping](https://github.com/heyudage/VoiceTyping) 基于 PaddleSpeech 的 ASR 流式服务实现的实时语音输入法工具。\n- 非常感谢 [EscaticZheng](https://github.com/EscaticZheng)/[ps3.9wheel-install](https://github.com/EscaticZheng/ps3.9wheel-install) 对PaddleSpeech在Windows下的安装提供了无需Visua Studio，基于python3.9的预编译依赖安装包。\n- 非常感谢 [chinobing](https://github.com/chinobing)/[FastAPI-PaddleSpeech-Audio-To-Text](https://github.com/chinobing/FastAPI-PaddleSpeech-Audio-To-Text) 利用 FastAPI 实现 PaddleSpeech 语音转文字，文件上传、分割、转换进度显示、后台更新任务并以 csv 格式输出。\n- 非常感谢 [MistEO](https://github.com/MistEO)/[Pallas-Bot](https://github.com/MistEO/Pallas-Bot) 基于 PaddleSpeech TTS 的 QQ Bot 项目。\n\n此外，PaddleSpeech 依赖于许多开源存储库。有关更多信息，请参阅 [references](./docs/source/reference.md)。\n\n## License\n\nPaddleSpeech 在 [Apache-2.0 许可](./LICENSE) 下提供。\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/PaddlePaddle/PaddleSpeech.svg)](https://starchart.cc/PaddlePaddle/PaddleSpeech)\n"
        },
        {
          "name": "audio",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "demos",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "paddlespeech",
          "type": "tree",
          "content": null
        },
        {
          "name": "runtime",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.17578125,
          "content": "[build_ext]\ndebug=0\n\n[metadata]\nlicense_file = LICENSE\ndescription-file = README.md\n\n[magformat]\nformatters=yapf\n\n[easy_install]\nindex-url=https://pypi.tuna.tsinghua.edu.cn/simple\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 8.8662109375,
          "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport contextlib\nimport inspect\nimport io\nimport os\nimport subprocess as sp\nimport sys\nfrom pathlib import Path\n\nfrom setuptools import Command\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom setuptools.command.develop import develop\nfrom setuptools.command.install import install\nfrom setuptools.command.test import test\n\nHERE = Path(os.path.abspath(os.path.dirname(__file__)))\n\nVERSION = '0.0.0'\nCOMMITID = 'none'\n\nbase = [\n    \"braceexpand\",\n    \"editdistance\",\n    \"g2p_en\",\n    \"g2pM\",\n    \"h5py\",\n    \"hyperpyyaml\",\n    \"inflect\",\n    \"jsonlines\",\n    # paddleaudio align with librosa==0.8.1, which need numpy==1.23.x\n    \"numpy==1.23.5\",\n    \"librosa==0.8.1\",\n    \"scipy>=1.4.0, <=1.12.0\",\n    \"loguru\",\n    \"matplotlib<=3.8.4\",\n    \"nara_wpe\",\n    \"onnxruntime>=1.11.0\",\n    \"opencc==1.1.6\",\n    \"opencc-python-reimplemented\",\n    \"pandas\",\n    \"paddleaudio>=1.1.0\",\n    \"paddlenlp>=2.4.8\",\n    \"paddleslim>=2.3.4\",\n    \"ppdiffusers>=0.9.0\",\n    \"paddlespeech_feat\",\n    \"praatio>=5.0.0, <=5.1.1\",\n    \"prettytable\",\n    \"pydantic>=1.10.14, <2.0\",\n    \"pypinyin<=0.44.0\",\n    \"pypinyin-dict\",\n    \"python-dateutil\",\n    \"pyworld>=0.2.12\",\n    \"pyyaml\",\n    \"resampy\",\n    \"sacrebleu\",\n    \"soundfile\",\n    \"textgrid\",\n    \"timer\",\n    \"ToJyutping==0.2.1\",\n    \"typeguard==2.13.3\",\n    \"webrtcvad\",\n    \"yacs~=0.1.8\",\n    \"zhon\",\n]\n\nserver = [\"pattern_singleton\", \"websockets\"]\n\nrequirements = {\n    \"install\":\n    base + server,\n    \"develop\": [\n        \"ConfigArgParse\",\n        \"coverage\",\n        \"gpustat\",\n        \"paddlespeech_ctcdecoders\",\n        \"phkit\",\n        \"pypi-kenlm\",\n        \"snakeviz\",\n        \"sox\",\n        \"soxbindings\",\n        \"unidecode\",\n        \"yq\",\n        \"pre-commit\",\n    ]\n}\n\n\ndef check_call(cmd: str, shell=False, executable=None):\n    try:\n        sp.check_call(\n            cmd.split(),\n            shell=shell,\n            executable=\"/bin/bash\" if shell else executable)\n    except sp.CalledProcessError as e:\n        print(\n            f\"{__file__}:{inspect.currentframe().f_lineno}: CMD: {cmd}, Error:\",\n            e.output,\n            file=sys.stderr)\n        raise e\n\n\ndef check_output(cmd: str, shell=False):\n    try:\n        out_bytes = sp.check_output(cmd.split())\n    except sp.CalledProcessError as e:\n        out_bytes = e.output  # Output generated before error\n        code = e.returncode  # Return code\n        print(\n            f\"{__file__}:{inspect.currentframe().f_lineno}: CMD: {cmd}, Error:\",\n            out_bytes,\n            file=sys.stderr)\n    return out_bytes.strip().decode('utf8')\n\n\n@contextlib.contextmanager\ndef pushd(new_dir):\n    old_dir = os.getcwd()\n    os.chdir(new_dir)\n    print(new_dir)\n    yield\n    os.chdir(old_dir)\n    print(old_dir)\n\n\ndef read(*names, **kwargs):\n    with io.open(\n            os.path.join(os.path.dirname(__file__), *names),\n            encoding=kwargs.get(\"encoding\", \"utf8\")) as fp:\n        return fp.read()\n\n\ndef _remove(files: str):\n    for f in files:\n        f.unlink()\n\n\n################################# Install ##################################\n\n\ndef _post_install(install_lib_dir):\n    # tools/make\n    tool_dir = HERE / \"tools\"\n    _remove(tool_dir.glob(\"*.done\"))\n    with pushd(tool_dir):\n        check_call(\"make\")\n    print(\"tools install.\")\n\n    # ctcdecoder\n    ctcdecoder_dir = HERE / 'third_party/ctc_decoders'\n    with pushd(ctcdecoder_dir):\n        check_call(\"bash -e setup.sh\")\n    print(\"ctcdecoder install.\")\n\n\nclass DevelopCommand(develop):\n    def run(self):\n        develop.run(self)\n        # must after develop.run, or pkg install by shell will not see\n        self.execute(_post_install, (self.install_lib, ), msg=\"Post Install...\")\n\n\nclass InstallCommand(install):\n    def run(self):\n        install.run(self)\n\n\nclass TestCommand(test):\n    def finalize_options(self):\n        test.finalize_options(self)\n        self.test_args = []\n        self.test_suite = True\n\n    def run_tests(self):\n        # Run nose ensuring that argv simulates running nosetests directly\n        import nose\n        nose.run_exit(argv=['nosetests', '-w', 'tests'])\n\n\n# cmd: python setup.py upload\nclass UploadCommand(Command):\n    description = \"Build and publish the package.\"\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            print(\"Removing previous dist/ ...\")\n            shutil.rmtree(str(HERE / \"dist\"))\n        except OSError:\n            pass\n        print(\"Building source distribution...\")\n        sp.check_call([sys.executable, \"setup.py\", \"sdist\"])\n        print(\"Uploading package to PyPi...\")\n        sp.check_call([\"twine\", \"upload\", \"dist/*\"])\n        sys.exit()\n\n\n################################# Version ##################################\ndef write_version_py(filename='paddlespeech/__init__.py'):\n    import paddlespeech\n    if hasattr(paddlespeech,\n               \"__version__\") and paddlespeech.__version__ == VERSION:\n        return\n    with open(filename, \"a\") as f:\n        out_str = f\"\\n__version__ = '{VERSION}'\\n\"\n        print(out_str)\n        f.write(f\"\\n__version__ = '{VERSION}'\\n\")\n\n    COMMITID = check_output(\"git rev-parse HEAD\")\n    with open(filename, 'a') as f:\n        out_str = f\"\\n__commit__ = '{COMMITID}'\\n\"\n        print(out_str)\n        f.write(f\"\\n__commit__ = '{COMMITID}'\\n\")\n\n    print(f\"{inspect.currentframe().f_code.co_name} done\")\n\n\ndef remove_version_py(filename='paddlespeech/__init__.py'):\n    with open(filename, \"r\") as f:\n        lines = f.readlines()\n    with open(filename, \"w\") as f:\n        for line in lines:\n            if \"__version__\" in line or \"__commit__\" in line:\n                continue\n            f.write(line)\n    print(f\"{inspect.currentframe().f_code.co_name} done\")\n\n\n@contextlib.contextmanager\ndef version_info():\n    write_version_py()\n    yield\n    remove_version_py()\n\n\n################################# Steup ##################################\nsetup_info = dict(\n    # Metadata\n    name='paddlespeech',\n    version=VERSION,\n    author='PaddlePaddle Speech and Language Team',\n    author_email='paddlesl@baidu.com',\n    url='https://github.com/PaddlePaddle/PaddleSpeech',\n    license='Apache 2.0',\n    description='Speech tools and models based on Paddlepaddle',\n    long_description=read(\"README.md\"),\n    long_description_content_type=\"text/markdown\",\n    keywords=[\n        \"SSL\"\n        \"speech\",\n        \"asr\",\n        \"tts\",\n        \"speaker verfication\",\n        \"speech classfication\",\n        \"text frontend\",\n        \"MFA\",\n        \"paddlepaddle\",\n        \"paddleaudio\",\n        \"streaming asr\",\n        \"streaming tts\",\n        \"beam search\",\n        \"ctcdecoder\",\n        \"deepspeech2\",\n        \"wav2vec2\",\n        \"hubert\",\n        \"wavlm\",\n        \"transformer\",\n        \"conformer\",\n        \"fastspeech2\",\n        \"hifigan\",\n        \"gan vocoders\",\n    ],\n    python_requires='>=3.7',\n    install_requires=requirements[\"install\"],\n    extras_require={\n        'develop':\n        requirements[\"develop\"],\n        'doc': [\n            \"sphinx\", \"sphinx-rtd-theme\", \"numpydoc\", \"myst_parser\",\n            \"recommonmark>=0.5.0\", \"sphinx-markdown-tables\", \"sphinx-autobuild\"\n        ],\n        'test': ['nose', 'torchaudio==0.10.2'],\n    },\n    cmdclass={\n        'develop': DevelopCommand,\n        'install': InstallCommand,\n        'upload': UploadCommand,\n        'test': TestCommand,\n    },\n\n    # Package info\n    packages=find_packages(\n        include=['paddlespeech*'], exclude=['utils', 'third_party']),\n    zip_safe=True,\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n    ],\n    entry_points={\n        'console_scripts': [\n            'paddlespeech=paddlespeech.cli.entry:_execute',\n            'paddlespeech_server=paddlespeech.server.entry:server_execute',\n            'paddlespeech_client=paddlespeech.server.entry:client_execute'\n        ]\n    })\n\nwith version_info():\n    setup(**setup_info, include_package_data=True)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}