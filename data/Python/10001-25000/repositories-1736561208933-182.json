{
  "metadata": {
    "timestamp": 1736561208933,
    "page": 182,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "fauxpilot/fauxpilot",
      "stars": 14641,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.802734375,
          "content": "# EditorConfig is awesome: https://EditorConfig.org\n\n# top-most EditorConfig file\nroot = true\n\n[*]\nindent_style = space\nindent_size = 4\ntab_width = 4\n# end_of_line = crlf\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = false\n\n# Markdown\n[*.{md}]\nindent_style = space\nindent_size = 2\ninsert_final_newline = true\n\n# Serialized data\n[*.{yml,yaml,json,pbtxt}]\nindent_style = space\nindent_size = 2\ninsert_final_newline = true\n\n# Shell script\n[*.{sh,bash,bashrc,zsh,fish,ksh,csh}]\nindent_style = space\nindent_size = 4\ninsert_final_newline = true\n\n# Python\n[*.py]\nindent_style = space\nindent_size = 4\ninsert_final_newline = true\n\n# Environment\n[*.env]\ninsert_final_newline = false\n\n# Python requirements\n[requirements.txt]\ninsert_final_newline = true\n\n# Dockerfile\n[Dockerfile]\ninsert_final_newline = true\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.9072265625,
          "content": "# Codegen Models\nmodels/*\n# huggingface cache\n.hf_cache/\n\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n*.code-workspace\n.history/\n.idea/\ncmake-build-*/\n*.iws\nout/\n.idea_modules/\natlassian-ide-plugin.xml\n.idea/replstate.xml\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n.idea/httpRequests\n.idea/caches/build_file_checksums.ser\n.Python\n[Bb]in\n[Ii]nclude\n[Ll]ib\n[Ll]ib64\n[Ll]ocal\n[Ss]cripts\npyvenv.cfg\n.venv\npip-selfcheck.json\n*.rsuser\n*.suo\n*.user\n*.userosscache\n*.sln.docstates\n*.userprefs\nmono_crash.*\n[Dd]ebug/\n[Dd]ebugPublic/\n[Rr]elease/\n[Rr]eleases/\nx64/\nx86/\n[Ww][Ii][Nn]32/\n[Aa][Rr][Mm]/\n[Aa][Rr][Mm]64/\nbld/\n[Bb]in/\n[Oo]bj/\n[Ll]og/\n[Ll]ogs/\n.vs/\nGenerated\\ Files/\n[Tt]est[Rr]esult*/\n[Bb]uild[Ll]og.*\n*.VisualState.xml\nTestResult.xml\nnunit-*.xml\n[Dd]ebugPS/\n[Rr]eleasePS/\ndlldata.c\nBenchmarkDotNet.Artifacts/\nproject.lock.json\nproject.fragment.lock.json\nartifacts/\nScaffoldingReadMe.txt\nStyleCopReport.xml\n*_i.c\n*_p.c\n*_h.h\n*.ilk\n*.meta\n*.obj\n*.iobj\n*.pch\n*.pdb\n*.ipdb\n*.pgc\n*.pgd\n*.rsp\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp\n*.tmp_proj\n*_wpftmp.csproj\n*.log\n*.vspscc\n*.vssscc\n.builds\n*.pidb\n*.svclog\n*.scc\n_Chutzpah*\nipch/\n*.aps\n*.ncb\n*.opendb\n*.opensdf\n*.sdf\n*.cachefile\n*.VC.db\n*.VC.VC.opendb\n*.psess\n*.vsp\n*.vspx\n*.sap\n*.e2e\n$tf/\n*.gpState\n_ReSharper*/\n*.[Rr]e[Ss]harper\n*.DotSettings.user\n_TeamCity*\n*.dotCover\n.axoCover/*\n!.axoCover/settings.json\ncoverage*.json\ncoverage*.xml\ncoverage*.info\n*.coverage\n*.coveragexml\n_NCrunch_*\n.*crunch*.local.xml\nnCrunchTemp_*\n*.mm.*\nAutoTest.Net/\n.sass-cache/\n[Ee]xpress/\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\npublish/\n*.[Pp]ublish.xml\n*.azurePubxml\n*.pubxml\n*.publishproj\nPublishScripts/\n*.nupkg\n*.snupkg\n**/[Pp]ackages/*\n!**/[Pp]ackages/build/\n*.nuget.props\n*.nuget.targets\ncsx/\n*.build.csdef\necf/\nrcf/\nAppPackages/\nBundleArtifacts/\nPackage.StoreAssociation.xml\n_pkginfo.txt\n*.appx\n*.appxbundle\n*.appxupload\n*.[Cc]ache\n!?*.[Cc]ache/\nClientBin/\n~$*\n*~\n*.dbmdl\n*.dbproj.schemaview\n*.jfm\n*.pfx\n*.publishsettings\norleans.codegen.cs\nGenerated_Code/\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\nUpgradeLog*.htm\nServiceFabricBackup/\n*.rptproj.bak\n*.mdf\n*.ldf\n*.ndf\n*.rdl.data\n*.bim.layout\n*.bim_*.settings\n*.rptproj.rsuser\n*- [Bb]ackup.rdl\n*- [Bb]ackup ([0-9]).rdl\n*- [Bb]ackup ([0-9][0-9]).rdl\nFakesAssemblies/\n*.GhostDoc.xml\n.ntvs_analysis.dat\nnode_modules/\n*.plg\n*.opt\n*.vbw\n**/*.HTMLClient/GeneratedArtifacts\n**/*.DesktopClient/GeneratedArtifacts\n**/*.DesktopClient/ModelManifest.xml\n**/*.Server/GeneratedArtifacts\n**/*.Server/ModelManifest.xml\n_Pvt_Extensions\n.paket/paket.exe\npaket-files/\n.fake/\n.cr/personal\n__pycache__/\n*.pyc\n*.tss\n*.jmconfig\n*.btp.cs\n*.btm.cs\n*.odx.cs\n*.xsd.cs\nOpenCover/\nASALocalRun/\n*.binlog\n*.nvuser\n.mfractor/\n.localhistory/\nhealthchecksdb\nMigrationBackup/\n.ionide/\nFodyWeavers.xsd\n.DS_Store\n.AppleDouble\n.LSOverride\nIcon\n._*\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n*.py[cod]\n*$py.class\n*.so\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n*.manifest\n*.spec\npip-log.txt\npip-delete-this-directory.txt\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n*.mo\n*.pot\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\ninstance/\n.webassets-cache\n.scrapy\ndocs/_build/\n.pybuilder/\ntarget/\n.ipynb_checkpoints\nprofile_default/\nipython_config.py\n__pypackages__/\ncelerybeat-schedule\ncelerybeat.pid\n*.sage.py\n.env\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.spyderproject\n.spyproject\n.ropeproject\n/site\n.mypy_cache/\n.dmypy.json\ndmypy.json\n.pyre/\n.pytype/\ncython_debug/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.03515625,
          "content": "Copyright 2022 Brendan Dolan-Gavitt\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.2578125,
          "content": "\n# FauxPilot\n\nThis is an attempt to build a locally hosted alternative to [GitHub Copilot](https://copilot.github.com/). It uses the [SalesForce CodeGen](https://github.com/salesforce/CodeGen) models inside of NVIDIA's [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) with the [FasterTransformer backend](https://github.com/triton-inference-server/fastertransformer_backend/).\n\n<p align=\"right\">\n  <img width=\"50%\" align=\"right\" src=\"./img/fauxpilot.png\">\n</p>\n\n## Prerequisites\n\nYou'll need:\n\n* Docker\n* `docker compose` >= 1.28\n* An NVIDIA GPU with Compute Capability >= 6.0 and enough VRAM to run the model you want.\n* [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker)\n* `curl` and `zstd` for downloading and unpacking the models.\n\nNote that the VRAM requirements listed by `setup.sh` are *total* -- if you have multiple GPUs, you can split the model across them. So, if you have two NVIDIA RTX 3080 GPUs, you *should* be able to run the 6B model by putting half on each GPU.\n\n\n## Support and Warranty\n\nlmao\n\nOkay, fine, we now have some minimal information on [the wiki](https://github.com/moyix/fauxpilot/wiki) and a [discussion forum](https://github.com/moyix/fauxpilot/discussions) where you can ask questions. Still no formal support or warranty though!\n\n\n\n## Setup\n\nThis section describes how to install a Fauxpilot server and clients.\n\n### Setting up a FauxPilot Server\n\nRun the setup script to choose a model to use. This will download the model from [Huggingface/Moyix](https://huggingface.co/Moyix) in GPT-J format and then convert it for use with FasterTransformer.\n\nPlease refer to [How to set-up a FauxPilot server](documentation/server.md).\n\n\n### Client configuration for FauxPilot\n\nWe offer some ways to connect to FauxPilot Server. For example, you can create a client by how to open the Openai API, Copilot Plugin, REST API.\n\nPlease refer to [How to set-up a client](documentation/client.md).\n\n\n## Terminology\n * API: Application Programming Interface\n * CC: Compute Capability\n * CUDA: Compute Unified Device Architecture\n * FT: Faster Transformer\n * JSON: JavaScript Object Notation \n * gRPC: Remote Procedure call by Google\n * GPT-J: A transformer model trained using Ben Wang's Mesh Transformer JAX \n * REST: REpresentational State Transfer\n"
        },
        {
          "name": "api.dockerignore",
          "type": "blob",
          "size": 0.078125,
          "content": ".hf_cache/\n.idea\n*.md\n.git*\n.enditorconfig\nmodels\nconverter\ntests\n.pytest_cache\n"
        },
        {
          "name": "converter",
          "type": "tree",
          "content": null
        },
        {
          "name": "copilot_proxy",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yaml",
          "type": "blob",
          "size": 0.916015625,
          "content": "version: '3.3'\nservices:\n  triton:\n    build:\n      context: .\n      dockerfile: triton.Dockerfile\n    command: bash -c \"CUDA_VISIBLE_DEVICES=${GPUS} mpirun -n 1 --allow-run-as-root /opt/tritonserver/bin/tritonserver --model-repository=/model\"\n    shm_size: '2gb'\n    volumes:\n      - ${MODEL_DIR}:/model\n      - ${HF_CACHE_DIR}:/root/.cache/huggingface\n    ports:\n      - \"8000:8000\"\n      - \"${TRITON_PORT}:8001\"\n      - \"8002:8002\"\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n  copilot_proxy:\n    # For dockerhub version\n    # image: moyix/copilot_proxy:latest\n    # For local build\n    build:\n      context: .\n      dockerfile: proxy.Dockerfile\n    command: uvicorn app:app --host 0.0.0.0 --port 5000\n    env_file:\n      # Automatically created via ./setup.sh\n      - .env\n    ports:\n      - \"${API_EXTERNAL_PORT}:5000\"\n"
        },
        {
          "name": "documentation",
          "type": "tree",
          "content": null
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "launch.sh",
          "type": "blob",
          "size": 0.8876953125,
          "content": "#!/usr/bin/env bash\n\n# Read in .env file; error if not found\nif [ ! -f .env ]; then\n    echo \".env not found, running setup.sh\"\n    bash setup.sh\nfi\nsource .env\n\nfunction showhelp () {\n   # Display Help\n   echo\n   echo \"Usage: $0 [option...]\"\n   echo \"options:\"\n   echo \"  -h       Print this help.\"\n   echo \"  -d       Start in daemon mode.\"\n   echo\n}\n\nwhile getopts \"hd\" option; do\n   case $option in\n      h)\n         showhelp\n         exit;;\n      d)\n         options=\"-d\"\n         ;;\n     \\?) # incorrect option\n         echo \"Error: Invalid option\"\n         exit;;\n   esac\ndone\n\n# On versions above 20.10.2, docker-compose is docker compose\nsmaller=$(printf \"$(docker --version | egrep -o '[0-9]+\\.[0-9]+\\.[0-9]+')\\n20.10.2\" | sort -V | head -n1)\nif [[ \"$smaller\" == \"20.10.2\" ]]; then\n  docker compose up $options --remove-orphans --build\nelse\n  docker-compose up $options --remove-orphans --build\nfi;\n"
        },
        {
          "name": "proxy.Dockerfile",
          "type": "blob",
          "size": 0.255859375,
          "content": "FROM python:3.10-slim-buster\n\nWORKDIR /python-docker\n\nCOPY copilot_proxy/requirements.txt requirements.txt\n\nRUN pip3 install --no-cache-dir -r requirements.txt\n\nCOPY copilot_proxy .\n\nEXPOSE 5000\n\nCMD [\"uvicorn\", \"--host\", \"0.0.0.0\", \"--port\", \"5000\", \"app:app\"]\n"
        },
        {
          "name": "python_backend",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.044921875,
          "content": "[flake8]\nmax-line-length = 120\nexclude = venv\n"
        },
        {
          "name": "setup.sh",
          "type": "blob",
          "size": 6.7099609375,
          "content": "#!/usr/bin/env bash\n\nif [ -f .env ]; then\n    read -rp \".env already exists, do you want to delete .env and recreate it? [y/n] \" DELETE\n    if [[ ${DELETE:-y} =~ ^[Yy]$ ]]\n    then\n      echo \"Deleting .env\"\n      rm .env\n    else\n      echo \"Exiting\"\n      exit 0\n    fi;\nfi\n\nfunction check_dep(){\n    echo \"Checking for $1 ...\"\n    which \"$1\" 2>/dev/null || {\n        echo \"Please install $1.\"\n        exit 1\n    }\n}\ncheck_dep curl\ncheck_dep zstd\ncheck_dep docker\n\n############### Common configuration ###############\n\n# Read number of GPUs\nread -rp \"Enter number of GPUs [1]: \" NUM_GPUS\nNUM_GPUS=${NUM_GPUS:-1}\n\nread -rp \"External port for the API [5000]: \" API_EXTERNAL_PORT\nAPI_EXTERNAL_PORT=${API_EXTERNAL_PORT:-5000}\n\nread -rp \"Address for Triton [triton]: \" TRITON_HOST\nTRITON_HOST=${TRITON_HOST:-triton}\n\nread -rp \"Port of Triton host [8001]: \" TRITON_PORT\nTRITON_PORT=${TRITON_PORT:-8001}\n\n# Read models root directory (all models go under this)\nread -rp \"Where do you want to save your models [$(pwd)/models]? \" MODELS_ROOT_DIR\nif [ -z \"$MODELS_ROOT_DIR\" ]; then\n    MODELS_ROOT_DIR=\"$(pwd)/models\"\nelse\n    MODELS_ROOT_DIR=\"$(readlink -m \"${MODELS_ROOT_DIR}\")\"\nfi\nmkdir -p \"$MODELS_ROOT_DIR\"\n\n# Write .env\necho \"NUM_GPUS=${NUM_GPUS}\" >> .env\necho \"GPUS=$(seq 0 $(( NUM_GPUS - 1)) | paste -s -d ',' -)\" >> .env\necho \"API_EXTERNAL_PORT=${API_EXTERNAL_PORT}\" >> .env\necho \"TRITON_HOST=${TRITON_HOST}\" >> .env\necho \"TRITON_PORT=${TRITON_PORT}\" >> .env\n\n############### Backend specific configuration ###############\n\nfunction fastertransformer_backend(){\n    echo \"Models available:\"\n    echo \"[1] codegen-350M-mono (2GB total VRAM required; Python-only)\"\n    echo \"[2] codegen-350M-multi (2GB total VRAM required; multi-language)\"\n    echo \"[3] codegen-2B-mono (7GB total VRAM required; Python-only)\"\n    echo \"[4] codegen-2B-multi (7GB total VRAM required; multi-language)\"\n    echo \"[5] codegen-6B-mono (13GB total VRAM required; Python-only)\"\n    echo \"[6] codegen-6B-multi (13GB total VRAM required; multi-language)\"\n    echo \"[7] codegen-16B-mono (32GB total VRAM required; Python-only)\"\n    echo \"[8] codegen-16B-multi (32GB total VRAM required; multi-language)\"\n    # Read their choice\n    read -rp \"Enter your choice [6]: \" MODEL_NUM\n\n    # Convert model number to model name\n    case $MODEL_NUM in\n        1) MODEL=\"codegen-350M-mono\" ;;\n        2) MODEL=\"codegen-350M-multi\" ;;\n        3) MODEL=\"codegen-2B-mono\" ;;\n        4) MODEL=\"codegen-2B-multi\" ;;\n        5) MODEL=\"codegen-6B-mono\" ;;\n        6) MODEL=\"codegen-6B-multi\" ;;\n        7) MODEL=\"codegen-16B-mono\" ;;\n        8) MODEL=\"codegen-16B-multi\" ;;\n        *) MODEL=\"codegen-6B-multi\" ;;\n    esac\n\n    echo \"MODEL=${MODEL}\" >> .env\n    echo \"MODEL_DIR=${MODELS_ROOT_DIR}/${MODEL}-${NUM_GPUS}gpu\" >> .env\n\n    if (test -d \"$MODELS_ROOT_DIR\"/\"${MODEL}\"-\"${NUM_GPUS}\"gpu ); then\n      echo \"$MODELS_ROOT_DIR\"/\"${MODEL}\"-\"${NUM_GPUS}\"gpu\n      echo \"Converted model for ${MODEL}-${NUM_GPUS}gpu already exists.\"\n      read -rp \"Do you want to re-use it? y/n: \" REUSE_CHOICE\n      if [[ ${REUSE_CHOICE:-y} =~ ^[Yy]$ ]]\n      then\n        DOWNLOAD_MODEL=n\n        echo \"Re-using model\"\n      else\n        DOWNLOAD_MODEL=y\n        rm -rf \"$MODELS_ROOT_DIR\"/\"${MODEL}\"-\"${NUM_GPUS}\"gpu\n      fi\n    else\n      DOWNLOAD_MODEL=y\n    fi\n\n    if [[ ${DOWNLOAD_MODEL:-y} =~ ^[Yy]$ ]]\n    then\n      if [ \"$NUM_GPUS\" -le 2 ]; then\n        echo \"Downloading the model from HuggingFace, this will take a while...\"\n        SCRIPT_DIR=\"$(dirname \"$(readlink -f \"$0\")\")\"\n        DEST=\"${MODEL}-${NUM_GPUS}gpu\"\n        ARCHIVE=\"${MODELS_ROOT_DIR}/${DEST}.tar.zst\"\n        cp -r \"$SCRIPT_DIR\"/converter/models/\"$DEST\" \"${MODELS_ROOT_DIR}\"\n        curl -L \"https://huggingface.co/moyix/${MODEL}-gptj/resolve/main/${MODEL}-${NUM_GPUS}gpu.tar.zst\" \\\n            -o \"$ARCHIVE\"\n        zstd -dc \"$ARCHIVE\" | tar -xf - -C \"${MODELS_ROOT_DIR}\"\n        rm -f \"$ARCHIVE\"\n      else\n        echo \"Downloading and converting the model, this will take a while...\"\n        docker run --rm -v \"${MODELS_ROOT_DIR}\":/models -e MODEL=${MODEL} -e NUM_GPUS=\"${NUM_GPUS}\" moyix/model_converter:latest\n      fi\n    fi\n\n    # Not used for this backend but needs to be present\n    HF_CACHE_DIR=\"$(pwd)/.hf_cache\"\n    mkdir -p \"$HF_CACHE_DIR\"\n    echo \"HF_CACHE_DIR=${HF_CACHE_DIR}\" >> .env\n}\n\nfunction python_backend(){\n    echo \"Models available:\"\n    echo \"[1] codegen-350M-mono (1GB total VRAM required; Python-only)\"\n    echo \"[2] codegen-350M-multi (1GB total VRAM required; multi-language)\"\n    echo \"[3] codegen-2B-mono (4GB total VRAM required; Python-only)\"\n    echo \"[4] codegen-2B-multi (4GB total VRAM required; multi-language)\"\n\n    read -rp \"Enter your choice [4]: \" MODEL_NUM\n\n    # Convert model number to model name\n    case $MODEL_NUM in\n        1) MODEL=\"codegen-350M-mono\"; ORG=\"Salesforce\" ;;\n        2) MODEL=\"codegen-350M-multi\"; ORG=\"Salesforce\" ;;\n        3) MODEL=\"codegen-2B-mono\"; ORG=\"Salesforce\" ;;\n        4) MODEL=\"codegen-2B-multi\"; ORG=\"Salesforce\" ;;\n        *) MODEL=\"codegen-2B-multi\"; ORG=\"Salesforce\" ;;\n    esac\n\n    # share huggingface cache? Should be safe to share, but permission issues may arise depending upon your docker setup\n    read -rp \"Do you want to share your huggingface cache between host and docker container? y/n [n]: \" SHARE_HF_CACHE\n    SHARE_HF_CACHE=${SHARE_HF_CACHE:-n}\n    if [[ ${SHARE_HF_CACHE:-y} =~ ^[Yy]$ ]]; then\n        read -rp \"Enter your huggingface cache directory [$HOME/.cache/huggingface]: \" HF_CACHE_DIR\n        HF_CACHE_DIR=${HF_CACHE_DIR:-$HOME/.cache/huggingface}\n    else\n        HF_CACHE_DIR=\"$(pwd)/.hf_cache\"\n    fi\n\n    # use int8? Allows larger models to fit in GPU but might be very marginally slower\n    read -rp \"Do you want to use int8? y/n [n]: \" USE_INT8\n    if [[ ! $USE_INT8 =~ ^[Yy]$ ]]; then\n        USE_INT8=\"0\"\n    else\n        USE_INT8=\"1\"\n    fi\n\n    # Write config.env\n    echo \"MODEL=py-${MODEL}\" >> .env\n    echo \"MODEL_DIR=${MODELS_ROOT_DIR}/py-${ORG}-${MODEL}\" >> .env  # different format from fastertransformer backend\n    echo \"HF_CACHE_DIR=${HF_CACHE_DIR}\" >> .env\n\n    python3 ./python_backend/init_model.py --model_name \"${MODEL}\" --org_name \"${ORG}\" --model_dir \"${MODELS_ROOT_DIR}\" --use_int8 \"${USE_INT8}\"\n    bash -c \"source .env ; docker compose build || docker-compose build\"\n}\n\n# choose backend\necho \"Choose your backend:\"\necho \"[1] FasterTransformer backend (faster, but limited models)\"\necho \"[2] Python backend (slower, but more models, and allows loading with int8)\"\nread -rp \"Enter your choice [1]: \" BACKEND_NUM\n\nif [[ \"$BACKEND_NUM\" -eq 2 ]]; then\n    python_backend\nelse\n    fastertransformer_backend\nfi\n\nread -rp \"Config complete, do you want to run FauxPilot? [y/n] \" RUN\nif [[ ${RUN:-y} =~ ^[Yy]$ ]]\nthen\n  bash ./launch.sh\nelse\n  echo \"You can run ./launch.sh to start the FauxPilot server.\"\n  exit 0\nfi\n"
        },
        {
          "name": "shutdown.sh",
          "type": "blob",
          "size": 0.1611328125,
          "content": "#!/usr/bin/env bash\n\nsource .env\n\n# On newer versions, docker-compose is docker compose\ndocker compose down --remove-orphans || docker-compose down --remove-orphans\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "triton.Dockerfile",
          "type": "blob",
          "size": 0.2724609375,
          "content": "FROM moyix/triton_with_ft:22.09\n\n# Install dependencies: torch\nRUN python3 -m pip install --disable-pip-version-check -U torch --extra-index-url https://download.pytorch.org/whl/cu116\nRUN python3 -m pip install --disable-pip-version-check -U transformers bitsandbytes accelerate\n"
        },
        {
          "name": "triton.dockerignore",
          "type": "blob",
          "size": 0.078125,
          "content": ".hf_cache/\n.idea\n*.md\n.git*\n.enditorconfig\nmodels\nconverter\ntests\n.pytest_cache\n"
        }
      ]
    }
  ]
}