{
  "metadata": {
    "timestamp": 1736561313641,
    "page": 321,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "allenai/allennlp",
      "stars": 11777,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.0205078125,
          "content": "[run]\nomit = tests/*\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.083984375,
          "content": ".dockerignore\n**.pyc\n**/__pycache__\n.gitignore\n.git\n.coverage\n.benchmarks\n.mypy_cache\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.603515625,
          "content": "[flake8]\nmax-line-length = 115\n\nignore =\n    # these rules don't play well with black\n    E203  # whitespace before :\n    W503  # line break before binary operator\n\nexclude =\n    build/**\n    doc/**\n    tutorials/tagger/**\n\nper-file-ignores =\n    # __init__.py files are allowed to have unused imports and lines-too-long\n    */__init__.py:F401\n    */**/**/__init__.py:F401,E501\n\n    # tests don't have to respect\n    #  E731: do not assign a lambda expression, use a def\n    tests/**:E731\n\n    # scripts don't have to respect\n    #  E402: imports not at top of file (because we mess with sys.path)\n    scripts/**:E402\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4736328125,
          "content": "# build artifacts\n\n.eggs/\n.mypy_cache\nallennlp.egg-info/\nbuild/\ndist/\npip-wheel-metadata/\ncontext.tar.gz\n\n\n# dev tools\n\n.envrc\n.python-version\n.idea\n.venv/\n.vscode/\n/*.iml\n\n\n# jupyter notebooks\n\n.ipynb_checkpoints\n\n\n# miscellaneous\n\n.cache/\nallennlp/tools/EVALB/evalb.dSYM/\ndoc/_build/\n*.swp\n.DS_Store\n\n\n# python\n\n*.pyc\n*.pyo\n__pycache__\n\n\n# testing and continuous integration\n\n.coverage\n.pytest_cache/\n.benchmarks\n\n# documentation build artifacts\n\ndocs/*.md\ndocs/api\nsite/\nmkdocs.yml\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 57.1337890625,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n\n## [v2.10.1](https://github.com/allenai/allennlp/releases/tag/v2.10.1) - 2022-10-18\n\n### Fixed\n\n- Updated dependencies\n\n\n## [v2.10.0](https://github.com/allenai/allennlp/releases/tag/v2.10.0) - 2022-07-14\n\n### Added\n\n- Added metric `FBetaVerboseMeasure` which extends `FBetaMeasure` to ensure compatibility with logging plugins and add some options.\n- Added three sample weighting techniques to `ConditionalRandomField` by supplying three new subclasses: `ConditionalRandomFieldWeightEmission`, `ConditionalRandomFieldWeightTrans`, and `ConditionalRandomFieldWeightLannoy`.\n\n### Fixed\n\n- Fix error from `cached-path` version update. \n\n## [v2.9.3](https://github.com/allenai/allennlp/releases/tag/v2.9.3) - 2022-04-13\n\n### Added\n\n- Added `verification_tokens` argument to `TestPretrainedTransformerTokenizer`.\n\n### Fixed\n\n- Updated various dependencies\n\n\n## [v2.9.2](https://github.com/allenai/allennlp/releases/tag/v2.9.2) - 2022-03-21\n\n### Fixed\n\n- Removed unnecessary dependencies\n- Restore functionality of CLI in absence of now-optional checklist-package\n\n\n## [v2.9.1](https://github.com/allenai/allennlp/releases/tag/v2.9.1) - 2022-03-09\n\n### Fixed\n\n- Updated dependencies, especially around doc creation.\n- Running the test suite out-of-tree (e.g. after installation) is now possible by pointing the environment variable `ALLENNLP_SRC_DIR` to the sources.\n- Silenced a warning that happens when you inappropriately clone a tensor.\n- Adding more clarification to the `Vocabulary` documentation around `min_pretrained_embeddings` and `only_include_pretrained_words`.\n- Fixed bug with type mismatch caused by latest release of `cached-path` that now returns a `Path` instead of a `str`.\n\n### Added\n\n- We can now transparently read compressed input files during prediction.\n- LZMA compression is now supported.\n- Added a way to give JSON blobs as input to dataset readers in the `evaluate` command.\n- Added the argument `sub_module` in `PretrainedTransformerMismatchedEmbedder`\n- Updated the docs for `PytorchSeq2VecWrapper` to specify that `mask` is required rather than sequence lengths for clarity.\n\n### Changed\n\n- You can automatically include all words from a pretrained file when building a vocabulary by setting the value in `min_pretrained_embeddings` to `-1`\n  for that particular namespace.\n\n\n## [v2.9.0](https://github.com/allenai/allennlp/releases/tag/v2.9.0) - 2022-01-27\n\n### Added\n\n- Added an `Evaluator` class to make comparing source, target, and predictions easier.\n- Added a way to resize the vocabulary in the T5 module\n- Added an argument `reinit_modules` to `cached_transformers.get()` that allows you to re-initialize the pretrained weights of a transformer model, using layer indices or regex strings.\n- Added attribute `_should_validate_this_epoch` to `GradientDescentTrainer` that controls whether validation is run at the end of each epoch.\n- Added `ShouldValidateCallback` that can be used to configure the frequency of validation during training.\n- Added a `MaxPoolingSpanExtractor`. This `SpanExtractor` represents each span by a component wise max-pooling-operation.\n- Added support for `dist_metric` kwarg in initializing fairness metrics, which allows optionally using `wasserstein` distance (previously only KL divergence was supported).\n\n### Fixed\n\n- Fixed the docstring information for the `FBetaMultiLabelMeasure` metric.\n- Various fixes for Python 3.9\n- Fixed the name that the `push-to-hf` command uses to store weights.\n- `FBetaMultiLabelMeasure` now works with multiple dimensions\n- Support for inferior operating systems when making hardlinks\n- Use `,` as a separator for filenames in the `evaluate` command, thus allowing for URLs (eg. `gs://...`) as input files.\n- Removed a spurious error message \"'torch.cuda' has no attribute '_check_driver'\" that would be appear in the logs\n  when a `ConfigurationError` for missing GPU was raised.\n- Load model on CPU post training to save GPU memory.\n- Fixed a bug in `ShouldValidateCallback` that leads to validation occuring after the first epoch regardless of `validation_start` value.\n- Fixed a bug in `ShouldValidateCallback` that leads to validation occuring every `validation_interval + 1` epochs, instead of every `validation_interval` epochs.\n- Fixed a bug in `ShouldValidateCallback` that leads to validation never occuring at the end of training.\n\n### Removed\n\n- Removed dependency on the overrides package\n- Removed Tango components, since they now live at https://github.com/allenai/tango.\n\n### Changed\n\n- Make `checklist` an optional dependency.\n\n## [v2.8.0](https://github.com/allenai/allennlp/releases/tag/v2.8.0) - 2021-11-01\n\n### Added\n\n- Added support to push models directly to the [Hugging Face Hub](https://huggingface.co/) with the command `allennlp push-to-hf`.\n- More default tests for the `TextualEntailmentSuite`.\n\n### Changed\n\n- The behavior of `--overrides` has changed. Previously the final configuration params were simply taken as the union over the original params and the `--overrides` params.\n  But now you can use `--overrides` to completely replace any part of the original config. For example, passing `--overrides '{\"model\":{\"type\":\"foo\"}}'` will completely\n  replace the \"model\" part of the original config. However, when you just want to change a single field in the JSON structure without removing / replacing adjacent fields,\n  you can still use the \"dot\" syntax. For example, `--overrides '{\"model.num_layers\":3}'` will only change the `num_layers` parameter to the \"model\" part of the config, leaving\n  everything else unchanged.\n- Integrated [`cached_path`](https://github.com/allenai/cached_path) library to replace existing functionality in `common.file_utils`. This introduces some improvements without\n  any breaking changes.\n\n### Fixed\n\n- Fixed the implementation of `PairedPCABiasDirection` in `allennlp.fairness.bias_direction`, where the difference vectors should not be centered when performing the PCA.\n- Fixed the docstring of `ExponentialMovingAverage`, which was causing its argument descriptions to render inccorrectly in the docs.\n\n## [v2.7.0](https://github.com/allenai/allennlp/releases/tag/v2.7.0) - 2021-09-01\n\n### Added\n\n- Added in a default behavior to the `_to_params` method of `Registrable` so that in the case it is not implemented by the child class, it will still produce _a parameter dictionary_.\n- Added in `_to_params` implementations to all tokenizers.\n- Added support to evaluate mutiple datasets and produce corresponding output files in the `evaluate` command.\n- Added more documentation to the learning rate schedulers to include a sample config object for how to use it.\n- Moved the pytorch learning rate schedulers wrappers to their own file called `pytorch_lr_schedulers.py` so that they will have their own documentation page.\n- Added a module `allennlp.nn.parallel` with a new base class, `DdpAccelerator`, which generalizes\n  PyTorch's `DistributedDataParallel` wrapper to support other implementations. Two implementations of\n  this class are provided. The default is `TorchDdpAccelerator` (registered at \"torch\"), which is just a thin wrapper around\n  `DistributedDataParallel`. The other is `FairScaleFsdpAccelerator`, which wraps FairScale's\n  [`FullyShardedDataParallel`](https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html).\n  You can specify the `DdpAccelerator` in the \"distributed\" section of a configuration file under the key \"ddp_accelerator\".\n- Added a module `allennlp.nn.checkpoint` with a new base class, `CheckpointWrapper`, for implementations\n  of activation/gradient checkpointing. Two implentations are provided. The default implementation is `TorchCheckpointWrapper` (registered as \"torch\"),\n  which exposes [PyTorch's checkpoint functionality](https://pytorch.org/docs/stable/checkpoint.html).\n  The other is `FairScaleCheckpointWrapper` which exposes the more flexible\n  [checkpointing funtionality from FairScale](https://fairscale.readthedocs.io/en/latest/api/nn/checkpoint/checkpoint_activations.html).\n- The `Model` base class now takes a `ddp_accelerator` parameter (an instance of `DdpAccelerator`) which will be available as\n  `self.ddp_accelerator` during distributed training. This is useful when, for example, instantiating submodules in your\n  model's `__init__()` method by wrapping them with `self.ddp_accelerator.wrap_module()`. See the `allennlp.modules.transformer.t5`\n  for an example.\n- We now log batch metrics to tensorboard and wandb.\n- Added Tango components, to be explored in detail in a later post\n- Added `ScaledDotProductMatrixAttention`, and converted the transformer toolkit to use it\n- Added tests to ensure that all `Attention` and `MatrixAttention` implementations are interchangeable\n- Added a way for AllenNLP Tango to read and write datasets lazily.\n- Added a way to remix datasets flexibly\n- Added `from_pretrained_transformer_and_instances` constructor to `Vocabulary`\n- `TransformerTextField` now supports `__len__`.\n\n### Fixed\n\n- Fixed a bug in `ConditionalRandomField`: `transitions` and `tag_sequence` tensors were not initialized on the desired device causing high CPU usage (see https://github.com/allenai/allennlp/issues/2884)\n- Fixed a mispelling: the parameter `contructor_extras` in `Lazy()` is now correctly called `constructor_extras`.\n- Fixed broken links in `allennlp.nn.initializers` docs.\n- Fixed bug in `BeamSearch` where `last_backpointers` was not being passed to any `Constraint`s.\n- `TransformerTextField` can now take tensors of shape `(1, n)` like the tensors produced from a HuggingFace tokenizer.\n- `tqdm` lock is now set inside `MultiProcessDataLoading` when new workers are spawned to avoid contention when writing output.\n- `ConfigurationError` is now pickleable.\n- Checkpointer cleaning was fixed to work on Windows Paths\n- Multitask models now support `TextFieldTensor` in heads, not just in the backbone.\n- Fixed the signature of `ScaledDotProductAttention` to match the other `Attention` classes\n- `allennlp` commands will now catch `SIGTERM` signals and handle them similar to `SIGINT` (keyboard interrupt).\n- The `MultiProcessDataLoader` will properly shutdown its workers when a `SIGTERM` is received.\n- Fixed the way names are applied to Tango `Step` instances.\n- Fixed a bug in calculating loss in the distributed setting.\n- Fixed a bug when extending a sparse sequence by 0 items.\n\n### Changed\n\n- The type of the `grad_norm` parameter of `GradientDescentTrainer` is now `Union[float, bool]`,\n  with a default value of `False`. `False` means gradients are not rescaled and the gradient\n  norm is never even calculated. `True` means the gradients are still not rescaled but the gradient\n  norm is calculated and passed on to callbacks. A `float` value means gradients are rescaled.\n- `TensorCache` now supports more concurrent readers and writers.\n- We no longer log parameter statistics to tensorboard or wandb by default.\n\n## [v2.6.0](https://github.com/allenai/allennlp/releases/tag/v2.6.0) - 2021-07-19\n\n### Added\n\n- Added `on_backward` training callback which allows for control over backpropagation and gradient manipulation.\n- Added `AdversarialBiasMitigator`, a Model wrapper to adversarially mitigate biases in predictions produced by a pretrained model for a downstream task.\n- Added `which_loss` parameter to `ensure_model_can_train_save_and_load` in `ModelTestCase` to specify which loss to test.\n- Added `**kwargs` to `Predictor.from_path()`. These key-word argument will be passed on to the `Predictor`'s constructor.\n- The activation layer in the transformer toolkit now can be queried for its output dimension.\n- `TransformerEmbeddings` now takes, but ignores, a parameter for the attention mask. This is needed for compatibility with some other modules that get called the same way and use the mask.\n- `TransformerPooler` can now be instantiated from a pretrained transformer module, just like the other modules in the transformer toolkit.\n- `TransformerTextField`, for cases where you don't care about AllenNLP's advanced text handling capabilities.\n- Added `TransformerModule._post_load_pretrained_state_dict_hook()` method. Can be used to modify `missing_keys` and `unexpected_keys` after\n  loading a pretrained state dictionary. This is useful when tying weights, for example.\n- Added an end-to-end test for the Transformer Toolkit.\n- Added `vocab` argument to `BeamSearch`, which is passed to each contraint in `constraints` (if provided).\n\n### Fixed\n\n- Fixed missing device mapping in the `allennlp.modules.conditional_random_field.py` file.\n- Fixed Broken link in `allennlp.fairness.fairness_metrics.Separation` docs\n- Ensured all `allennlp` submodules are imported with `allennlp.common.plugins.import_plugins()`.\n- Fixed `IndexOutOfBoundsException` in `MultiOptimizer` when checking if optimizer received any parameters.\n- Removed confusing zero mask from VilBERT.\n- Ensured `ensure_model_can_train_save_and_load` is consistently random.\n- Fixed weight tying logic in `T5` transformer module. Previously input/output embeddings were always tied. Now this is optional,\n  and the default behavior is taken from the `config.tie_word_embeddings` value when instantiating `from_pretrained_module()`.\n- Implemented slightly faster label smoothing.\n- Fixed the docs for `PytorchTransformerWrapper`\n- Fixed recovering training jobs with models that expect `get_metrics()` to not be called until they have seen at least one batch.\n- Made the Transformer Toolkit compatible with transformers that don't start their positional embeddings at 0.\n- Weights & Biases training callback (\"wandb\") now works when resuming training jobs.\n\n### Changed\n\n- Changed behavior of `MultiOptimizer` so that while a default optimizer is still required, an error is not thrown if the default optimizer receives no parameters.\n- Made the epsilon parameter for the layer normalization in token embeddings configurable.\n\n### Removed\n\n- Removed `TransformerModule._tied_weights`. Weights should now just be tied directly in the `__init__()` method.\n  You can also override `TransformerModule._post_load_pretrained_state_dict_hook()` to remove keys associated with tied weights from `missing_keys`\n  after loading a pretrained state dictionary.\n\n## [v2.5.0](https://github.com/allenai/allennlp/releases/tag/v2.5.0) - 2021-06-03\n\n### Added\n\n- Added `TaskSuite` base class and command line functionality for running [`checklist`](https://github.com/marcotcr/checklist) test suites, along with implementations for `SentimentAnalysisSuite`, `QuestionAnsweringSuite`, and `TextualEntailmentSuite`. These can be found in the `allennlp.confidence_checks.task_checklists` module.\n- Added `BiasMitigatorApplicator`, which wraps any Model and mitigates biases by finetuning\n  on a downstream task.\n- Added `allennlp diff` command to compute a diff on model checkpoints, analogous to what `git diff` does on two files.\n- Meta data defined by the class `allennlp.common.meta.Meta` is now saved in the serialization directory and archive file\n  when training models from the command line. This is also now part of the `Archive` named tuple that's returned from `load_archive()`.\n- Added `nn.util.distributed_device()` helper function.\n- Added `allennlp.nn.util.load_state_dict` helper function.\n- Added a way to avoid downloading and loading pretrained weights in modules that wrap transformers\n  such as the `PretrainedTransformerEmbedder` and `PretrainedTransformerMismatchedEmbedder`.\n  You can do this by setting the parameter `load_weights` to `False`.\n  See [PR #5172](https://github.com/allenai/allennlp/pull/5172) for more details.\n- Added `SpanExtractorWithSpanWidthEmbedding`, putting specific span embedding computations into the `_embed_spans` method and leaving the common code in `SpanExtractorWithSpanWidthEmbedding` to unify the arguments, and modified `BidirectionalEndpointSpanExtractor`, `EndpointSpanExtractor` and `SelfAttentiveSpanExtractor` accordingly. Now, `SelfAttentiveSpanExtractor` can also embed span widths.\n- Added a `min_steps` parameter to `BeamSearch` to set a minimum length for the predicted sequences.\n- Added the `FinalSequenceScorer` abstraction to calculate the final scores of the generated sequences in `BeamSearch`.\n- Added `shuffle` argument to `BucketBatchSampler` which allows for disabling shuffling.\n- Added `allennlp.modules.transformer.attention_module` which contains a generalized `AttentionModule`. `SelfAttention` and `T5Attention` both inherit from this.\n- Added a `Constraint` abstract class to `BeamSearch`, which allows for incorporating constraints on the predictions found by `BeamSearch`,\n  along with a `RepeatedNGramBlockingConstraint` constraint implementation, which allows for preventing repeated n-grams in the output from `BeamSearch`.\n- Added `DataCollator` for dynamic operations for each batch.\n\n### Changed\n\n- Use `dist_reduce_sum` in distributed metrics.\n- Allow Google Cloud Storage paths in `cached_path` (\"gs://...\").\n- Renamed `nn.util.load_state_dict()` to `read_state_dict` to avoid confusion with `torch.nn.Module.load_state_dict()`.\n- `TransformerModule.from_pretrained_module` now only accepts a pretrained model ID (e.g. \"bert-base-case\") instead of\n  an actual `torch.nn.Module`. Other parameters to this method have changed as well.\n- Print the first batch to the console by default.\n- Renamed `sanity_checks` to `confidence_checks` (`sanity_checks` is deprecated and will be removed in AllenNLP 3.0).\n- Trainer callbacks can now store and restore state in case a training run gets interrupted.\n- VilBERT backbone now rolls and unrolls extra dimensions to handle input with > 3 dimensions.\n- `BeamSearch` is now a `Registrable` class.\n\n### Fixed\n\n- When `PretrainedTransformerIndexer` folds long sequences, it no longer loses the information from token type ids.\n- Fixed documentation for `GradientDescentTrainer.cuda_device`.\n- Re-starting a training run from a checkpoint in the middle of an epoch now works correctly.\n- When using the \"moving average\" weights smoothing feature of the trainer, training checkpoints would also get smoothed, with strange results for resuming a training job. This has been fixed.\n- When re-starting an interrupted training job, the trainer will now read out the data loader even for epochs and batches that can be skipped. We do this to try to get any random number generators used by the reader or data loader into the same state as they were the first time the training job ran.\n- Fixed the potential for a race condition with `cached_path()` when extracting archives. Although the race condition\n  is still possible if used with `force_extract=True`.\n- Fixed `wandb` callback to work in distributed training.\n- Fixed `tqdm` logging into multiple files with `allennlp-optuna`.\n\n## [v2.4.0](https://github.com/allenai/allennlp/releases/tag/v2.4.0) - 2021-04-22\n\n### Added\n\n- Added a T5 implementation to `modules.transformers`.\n\n### Changed\n\n- Weights & Biases callback can now work in anonymous mode (i.e. without the `WANDB_API_KEY` environment variable).\n\n### Fixed\n\n- The `GradientDescentTrainer` no longer leaves stray model checkpoints around when it runs out of patience.\n- Fixed `cached_path()` for \"hf://\" files.\n- Improved the error message for the `PolynomialDecay` LR scheduler when `num_steps_per_epoch` is missing.\n\n## [v2.3.1](https://github.com/allenai/allennlp/releases/tag/v2.3.1) - 2021-04-20\n\n### Added\n\n- Added support for the HuggingFace Hub as an alternative way to handle loading files. Hub downloads should be made through the `hf://` URL scheme.\n- Add new dimension to the `interpret` module: influence functions via the `InfluenceInterpreter` base class, along with a concrete implementation: `SimpleInfluence`.\n- Added a `quiet` parameter to the `MultiProcessDataLoading` that disables `Tqdm` progress bars.\n- The test for distributed metrics now takes a parameter specifying how often you want to run it.\n- Created the fairness module and added three fairness metrics: `Independence`, `Separation`, and `Sufficiency`.\n- Added four bias metrics to the fairness module: `WordEmbeddingAssociationTest`, `EmbeddingCoherenceTest`, `NaturalLanguageInference`, and `AssociationWithoutGroundTruth`.\n- Added four bias direction methods (`PCABiasDirection`, `PairedPCABiasDirection`, `TwoMeansBiasDirection`, `ClassificationNormalBiasDirection`) and four bias mitigation methods (`LinearBiasMitigator`, `HardBiasMitigator`, `INLPBiasMitigator`, `OSCaRBiasMitigator`).\n\n### Changed\n\n- Updated CONTRIBUTING.md to remind reader to upgrade pip setuptools to avoid spaCy installation issues.\n\n### Fixed\n\n- Fixed a bug with the `ShardedDatasetReader` when used with multi-process data loading (https://github.com/allenai/allennlp/issues/5132).\n\n## [v2.3.0](https://github.com/allenai/allennlp/releases/tag/v2.3.0) - 2021-04-14\n\n### Added\n\n- Ported the following Huggingface `LambdaLR`-based schedulers: `ConstantLearningRateScheduler`, `ConstantWithWarmupLearningRateScheduler`, `CosineWithWarmupLearningRateScheduler`, `CosineHardRestartsWithWarmupLearningRateScheduler`.\n- Added new `sub_token_mode` parameter to `pretrained_transformer_mismatched_embedder` class to support first sub-token embedding\n- Added a way to run a multi task model with a dataset reader as part of `allennlp predict`.\n- Added new `eval_mode` in `PretrainedTransformerEmbedder`. If it is set to `True`, the transformer is _always_ run in evaluation mode, which, e.g., disables dropout and does not update batch normalization statistics.\n- Added additional parameters to the W&B callback: `entity`, `group`, `name`, `notes`, and `wandb_kwargs`.\n\n### Changed\n\n- Sanity checks in the `GradientDescentTrainer` can now be turned off by setting the `run_sanity_checks` parameter to `False`.\n- Allow the order of examples in the task cards to be specified explicitly\n- `histogram_interval` parameter is now deprecated in `TensorboardWriter`, please use `distribution_interval` instead.\n- Memory usage is not logged in tensorboard during training now. `ConsoleLoggerCallback` should be used instead.\n- If you use the `min_count` parameter of the Vocabulary, but you specify a namespace that does not exist, the vocabulary creation will raise a `ConfigurationError`.\n- Documentation updates made to SoftmaxLoss regarding padding and the expected shapes of the input and output tensors of `forward`.\n- Moved the data preparation script for coref into allennlp-models.\n- If a transformer is not in cache but has override weights, the transformer's pretrained weights are no longer downloaded, that is, only its `config.json` file is downloaded.\n- `SanityChecksCallback` now raises `SanityCheckError` instead of `AssertionError` when a check fails.\n- `jsonpickle` removed from dependencies.\n- Improved the error message from `Registrable.by_name()` when the name passed does not match any registered subclassess.\n  The error message will include a suggestion if there is a close match between the name passed and a registered name.\n\n### Fixed\n\n- Fixed a bug where some `Activation` implementations could not be pickled due to involving a lambda function.\n- Fixed `__str__()` method on `ModelCardInfo` class.\n- Fixed a stall when using distributed training and gradient accumulation at the same time\n- Fixed an issue where using the `from_pretrained_transformer` `Vocabulary` constructor in distributed training via the `allennlp train` command\n  would result in the data being iterated through unnecessarily.\n- Fixed a bug regarding token indexers with the `InterleavingDatasetReader` when used with multi-process data loading.\n- Fixed a warning from `transformers` when using `max_length` in the `PretrainedTransformerTokenizer`.\n\n### Removed\n\n- Removed the `stride` parameter to `PretrainedTransformerTokenizer`. This parameter had no effect.\n\n## [v2.2.0](https://github.com/allenai/allennlp/releases/tag/v2.2.0) - 2021-03-26\n\n### Added\n\n- Add new method on `Field` class: `.human_readable_repr() -> Any`\n- Add new method on `Instance` class: `.human_readable_dict() -> JsonDict`.\n- Added `WandBCallback` class for [Weights & Biases](https://wandb.ai) integration, registered as a callback under\n  the name \"wandb\".\n- Added `TensorBoardCallback` to replace the `TensorBoardWriter`. Registered as a callback\n  under the name \"tensorboard\".\n- Added `NormalizationBiasVerification` and `SanityChecksCallback` for model sanity checks.\n- `SanityChecksCallback` runs by default from the `allennlp train` command.\n  It can be turned off by setting `trainer.enable_default_callbacks` to `false` in your config.\n\n### Changed\n\n- Use attributes of `ModelOutputs` object in `PretrainedTransformerEmbedder` instead of indexing.\n- Added support for PyTorch version 1.8 and `torchvision` version 0.9 .\n- `Model.get_parameters_for_histogram_tensorboard_logging` is deprecated in favor of\n  `Model.get_parameters_for_histogram_logging`.\n\n### Fixed\n\n- Makes sure tensors that are stored in `TensorCache` always live on CPUs\n- Fixed a bug where `FromParams` objects wrapped in `Lazy()` couldn't be pickled.\n- Fixed a bug where the `ROUGE` metric couldn't be picked.\n- Fixed a bug reported by https://github.com/allenai/allennlp/issues/5036. We keeps our spacy POS tagger on.\n\n### Removed\n\n- Removed `TensorBoardWriter`. Please use the `TensorBoardCallback` instead.\n\n## [v2.1.0](https://github.com/allenai/allennlp/releases/tag/v2.1.0) - 2021-02-24\n\n### Changed\n\n- `coding_scheme` parameter is now deprecated in `Conll2003DatasetReader`, please use `convert_to_coding_scheme` instead.\n- Support spaCy v3\n\n### Added\n\n- Added `ModelUsage` to `ModelCard` class.\n- Added a way to specify extra parameters to the predictor in an `allennlp predict` call.\n- Added a way to initialize a `Vocabulary` from transformers models.\n- Added the ability to use `Predictors` with multitask models through the new `MultiTaskPredictor`.\n- Added an example for fields of type `ListField[TextField]` to `apply_token_indexers` API docs.\n- Added `text_key` and `label_key` parameters to `TextClassificationJsonReader` class.\n- Added `MultiOptimizer`, which allows you to use different optimizers for different parts of your model.\n- Added a clarification to `predictions_to_labeled_instances` API docs for attack from json\n\n### Fixed\n\n- `@Registrable.register(...)` decorator no longer masks the decorated class's annotations\n- Ensured that `MeanAbsoluteError` always returns a `float` metric value instead of a `Tensor`.\n- Learning rate schedulers that rely on metrics from the validation set were broken in v2.0.0. This\n  brings that functionality back.\n- Fixed a bug where the `MultiProcessDataLoading` would crash when `num_workers > 0`, `start_method = \"spawn\"`, `max_instances_in_memory not None`, and `batches_per_epoch not None`.\n- Fixed documentation and validation checks for `FBetaMultiLabelMetric`.\n- Fixed handling of HTTP errors when fetching remote resources with `cached_path()`. Previously the content would be cached even when\n  certain errors - like 404s - occurred. Now an `HTTPError` will be raised whenever the HTTP response is not OK.\n- Fixed a bug where the `MultiTaskDataLoader` would crash when `num_workers > 0`\n- Fixed an import error that happens when PyTorch's distributed framework is unavailable on the system.\n\n## [v2.0.1](https://github.com/allenai/allennlp/releases/tag/v2.0.1) - 2021-01-29\n\n### Added\n\n- Added `tokenizer_kwargs` and `transformer_kwargs` arguments to `PretrainedTransformerBackbone`\n- Resize transformers word embeddings layer for `additional_special_tokens`\n\n### Changed\n\n- GradientDescentTrainer makes `serialization_dir` when it's instantiated, if it doesn't exist.\n\n### Fixed\n\n- `common.util.sanitize` now handles sets.\n\n## [v2.0.0](https://github.com/allenai/allennlp/releases/tag/v2.0.0) - 2021-01-27\n\n### Added\n\n- The `TrainerCallback` constructor accepts `serialization_dir` provided by `Trainer`. This can be useful for `Logger` callbacks those need to store files in the run directory.\n- The `TrainerCallback.on_start()` is fired at the start of the training.\n- The `TrainerCallback` event methods now accept `**kwargs`. This may be useful to maintain backwards-compability of callbacks easier in the future. E.g. we may decide to pass the exception/traceback object in case of failure to `on_end()` and this older callbacks may simply ignore the argument instead of raising a `TypeError`.\n- Added a `TensorBoardCallback` which wraps the `TensorBoardWriter`.\n\n### Changed\n\n- The `TrainerCallack.on_epoch()` does not fire with `epoch=-1` at the start of the training.\n  Instead, `TrainerCallback.on_start()` should be used for these cases.\n- `TensorBoardBatchMemoryUsage` is converted from `BatchCallback` into `TrainerCallback`.\n- `TrackEpochCallback` is converted from `EpochCallback` into `TrainerCallback`.\n- `Trainer` can accept callbacks simply with name `callbacks` instead of `trainer_callbacks`.\n- `TensorboardWriter` renamed to `TensorBoardWriter`, and removed as an argument to the `GradientDescentTrainer`.\n  In order to enable TensorBoard logging during training, you should utilize the `TensorBoardCallback` instead.\n\n### Removed\n\n- Removed `EpochCallback`, `BatchCallback` in favour of `TrainerCallback`.\n  The metaclass-wrapping implementation is removed as well.\n- Removed the `tensorboard_writer` parameter to `GradientDescentTrainer`. You should use the `TensorBoardCallback` now instead.\n\n### Fixed\n\n- Now Trainer always fires `TrainerCallback.on_end()` so all the resources can be cleaned up properly.\n- Fixed the misspelling, changed `TensoboardBatchMemoryUsage` to `TensorBoardBatchMemoryUsage`.\n- We set a value to `epoch` so in case of firing `TrainerCallback.on_end()` the variable is bound.\n  This could have lead to an error in case of trying to recover a run after it was finished training.\n\n## [v2.0.0rc1](https://github.com/allenai/allennlp/releases/tag/v2.0.0rc1) - 2021-01-21\n\n### Added\n\n- Added `TensorCache` class for caching tensors on disk\n- Added abstraction and concrete implementation for image loading\n- Added abstraction and concrete implementation for `GridEmbedder`\n- Added abstraction and demo implementation for an image augmentation module.\n- Added abstraction and concrete implementation for region detectors.\n- A new high-performance default `DataLoader`: `MultiProcessDataLoading`.\n- A `MultiTaskModel` and abstractions to use with it, including `Backbone` and `Head`. The\n  `MultiTaskModel` first runs its inputs through the `Backbone`, then passes the result (and\n  whatever other relevant inputs it got) to each `Head` that's in use.\n- A `MultiTaskDataLoader`, with a corresponding `MultiTaskDatasetReader`, and a couple of new\n  configuration objects: `MultiTaskEpochSampler` (for deciding what proportion to sample from each\n  dataset at every epoch) and a `MultiTaskScheduler` (for ordering the instances within an epoch).\n- Transformer toolkit to plug and play with modular components of transformer architectures.\n- Added a command to count the number of instances we're going to be training with\n- Added a `FileLock` class to `common.file_utils`. This is just like the `FileLock` from the `filelock` library, except that\n  it adds an optional flag `read_only_ok: bool`, which when set to `True` changes the behavior so that a warning will be emitted\n  instead of an exception when lacking write permissions on an existing file lock.\n  This makes it possible to use the `FileLock` class on a read-only file system.\n- Added a new learning rate scheduler: `CombinedLearningRateScheduler`. This can be used to combine different LR schedulers, using one after the other.\n- Added an official CUDA 10.1 Docker image.\n- Moving `ModelCard` and `TaskCard` abstractions into the main repository.\n- Added a util function `allennlp.nn.util.dist_reduce(...)` for handling distributed reductions.\n  This is especially useful when implementing a distributed `Metric`.\n- Added a `FileLock` class to `common.file_utils`. This is just like the `FileLock` from the `filelock` library, except that\n  it adds an optional flag `read_only_ok: bool`, which when set to `True` changes the behavior so that a warning will be emitted\n  instead of an exception when lacking write permissions on an existing file lock.\n  This makes it possible to use the `FileLock` class on a read-only file system.\n- Added a new learning rate scheduler: `CombinedLearningRateScheduler`. This can be used to combine different LR schedulers, using one after the other.\n- Moving `ModelCard` and `TaskCard` abstractions into the main repository.\n\n### Changed\n\n- `DatasetReader`s are now always lazy. This means there is no `lazy` parameter in the base\n  class, and the `_read()` method should always be a generator.\n- The `DataLoader` now decides whether to load instances lazily or not.\n  With the `PyTorchDataLoader` this is controlled with the `lazy` parameter, but with\n  the `MultiProcessDataLoading` this is controlled by the `max_instances_in_memory` setting.\n- `ArrayField` is now called `TensorField`, and implemented in terms of torch tensors, not numpy.\n- Improved `nn.util.move_to_device` function by avoiding an unnecessary recursive check for tensors and\n  adding a `non_blocking` optional argument, which is the same argument as in `torch.Tensor.to()`.\n- If you are trying to create a heterogeneous batch, you now get a better error message.\n- Readers using the new vision features now explicitly log how they are featurizing images.\n- `master_addr` and `master_port` renamed to `primary_addr` and `primary_port`, respectively.\n- `is_master` parameter for training callbacks renamed to `is_primary`.\n- `master` branch renamed to `main`\n- Torch version bumped to 1.7.1 in Docker images.\n- 'master' branch renamed to 'main'\n- Torch version bumped to 1.7.1 in Docker images.\n\n### Removed\n\n- Removed `nn.util.has_tensor`.\n\n### Fixed\n\n- The `build-vocab` command no longer crashes when the resulting vocab file is\n  in the current working directory.\n- VQA models now use the `vqa_score` metric for early stopping. This results in\n  much better scores.\n- Fixed typo with `LabelField` string representation: removed trailing apostrophe.\n- `Vocabulary.from_files` and `cached_path` will issue a warning, instead of failing, when a lock on an existing resource\n  can't be acquired because the file system is read-only.\n- `TrackEpochCallback` is now a `EpochCallback`.\n\n## [v1.3.0](https://github.com/allenai/allennlp/releases/tag/v1.3.0) - 2020-12-15\n\n### Added\n\n- Added links to source code in docs.\n- Added `get_embedding_layer` and `get_text_field_embedder` to the `Predictor` class; to specify embedding layers for non-AllenNLP models.\n- Added [Gaussian Error Linear Unit (GELU)](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) as an Activation.\n\n### Changed\n\n- Renamed module `allennlp.data.tokenizers.token` to `allennlp.data.tokenizers.token_class` to avoid\n  [this bug](https://github.com/allenai/allennlp/issues/4819).\n- `transformers` dependency updated to version 4.0.1.\n- `BasicClassifier`'s forward method now takes a metadata field.\n\n### Fixed\n\n- Fixed a lot of instances where tensors were first created and then sent to a device\n  with `.to(device)`. Instead, these tensors are now created directly on the target device.\n- Fixed issue with `GradientDescentTrainer` when constructed with `validation_data_loader=None` and `learning_rate_scheduler!=None`.\n- Fixed a bug when removing all handlers in root logger.\n- `ShardedDatasetReader` now inherits parameters from `base_reader` when required.\n- Fixed an issue in `FromParams` where parameters in the `params` object used to a construct a class\n  were not passed to the constructor if the value of the parameter was equal to the default value.\n  This caused bugs in some edge cases where a subclass that takes `**kwargs` needs to inspect\n  `kwargs` before passing them to its superclass.\n- Improved the band-aid solution for segmentation faults and the \"ImportError: dlopen: cannot load any more object with static TLS\"\n  by adding a `transformers` import.\n- Added safety checks for extracting tar files\n- Turned superfluous warning to info when extending the vocab in the embedding matrix, if no pretrained file was provided\n\n## [v1.2.2](https://github.com/allenai/allennlp/releases/tag/v1.2.2) - 2020-11-17\n\n### Added\n\n- Added Docker builds for other torch-supported versions of CUDA.\n- Adds [`allennlp-semparse`](https://github.com/allenai/allennlp-semparse) as an official, default plugin.\n\n### Fixed\n\n- `GumbelSampler` now sorts the beams by their true log prob.\n\n## [v1.2.1](https://github.com/allenai/allennlp/releases/tag/v1.2.1) - 2020-11-10\n\n### Added\n\n- Added an optional `seed` parameter to `ModelTestCase.set_up_model` which sets the random\n  seed for `random`, `numpy`, and `torch`.\n- Added support for a global plugins file at `~/.allennlp/plugins`.\n- Added more documentation about plugins.\n- Added sampler class and parameter in beam search for non-deterministic search, with several\n  implementations, including `MultinomialSampler`, `TopKSampler`, `TopPSampler`, and\n  `GumbelSampler`. Utilizing `GumbelSampler` will give [Stochastic Beam Search](https://api.semanticscholar.org/CorpusID:76662039).\n\n### Changed\n\n- Pass batch metrics to `BatchCallback`.\n\n### Fixed\n\n- Fixed a bug where forward hooks were not cleaned up with saliency interpreters if there\n  was an exception.\n- Fixed the computation of saliency maps in the Interpret code when using mismatched indexing.\n  Previously, we would compute gradients from the top of the transformer, after aggregation from\n  wordpieces to tokens, which gives results that are not very informative. Now, we compute gradients\n  with respect to the embedding layer, and aggregate wordpieces to tokens separately.\n- Fixed the heuristics for finding embedding layers in the case of RoBERTa. An update in the\n  `transformers` library broke our old heuristic.\n- Fixed typo with registered name of ROUGE metric. Previously was `rogue`, fixed to `rouge`.\n- Fixed default masks that were erroneously created on the CPU even when a GPU is available.\n- Fixed pretrained embeddings for transformers that don't use end tokens.\n- Fixed the transformer tokenizer cache when the tokenizers are initialized with custom kwargs.\n\n## [v1.2.0](https://github.com/allenai/allennlp/releases/tag/v1.2.0) - 2020-10-29\n\n### Changed\n\n- Enforced stricter typing requirements around the use of `Optional[T]` types.\n- Changed the behavior of `Lazy` types in `from_params` methods. Previously, if you defined a `Lazy` parameter like\n  `foo: Lazy[Foo] = None` in a custom `from_params` classmethod, then `foo` would actually never be `None`.\n  This behavior is now different. If no params were given for `foo`, it will be `None`.\n  You can also now set default values for foo like `foo: Lazy[Foo] = Lazy(Foo)`.\n  Or, if you want you want a default value but also want to allow for `None` values, you can\n  write it like this: `foo: Optional[Lazy[Foo]] = Lazy(Foo)`.\n- Added support for PyTorch version 1.7.\n\n### Fixed\n\n- Made it possible to instantiate `TrainerCallback` from config files.\n- Fixed the remaining broken internal links in the API docs.\n- Fixed a bug where Hotflip would crash with a model that had multiple TokenIndexers and the input\n  used rare vocabulary items.\n- Fixed a bug where `BeamSearch` would fail if `max_steps` was equal to 1.\n- Fixed `BasicTextFieldEmbedder` to not raise ConfigurationError if it has embedders that are empty and not in input\n\n## [v1.2.0rc1](https://github.com/allenai/allennlp/releases/tag/v1.2.0rc1) - 2020-10-22\n\n### Added\n\n- Added a warning when `batches_per_epoch` for the validation data loader is inherited from\n  the train data loader.\n- Added a `build-vocab` subcommand that can be used to build a vocabulary from a training config file.\n- Added `tokenizer_kwargs` argument to `PretrainedTransformerMismatchedIndexer`.\n- Added `tokenizer_kwargs` and `transformer_kwargs` arguments to `PretrainedTransformerMismatchedEmbedder`.\n- Added official support for Python 3.8.\n- Added a script: `scripts/release_notes.py`, which automatically prepares markdown release notes from the\n  CHANGELOG and commit history.\n- Added a flag `--predictions-output-file` to the `evaluate` command, which tells AllenNLP to write the\n  predictions from the given dataset to the file as JSON lines.\n- Added the ability to ignore certain missing keys when loading a model from an archive. This is done\n  by adding a class-level variable called `authorized_missing_keys` to any PyTorch module that a `Model` uses.\n  If defined, `authorized_missing_keys` should be a list of regex string patterns.\n- Added `FBetaMultiLabelMeasure`, a multi-label Fbeta metric. This is a subclass of the existing `FBetaMeasure`.\n- Added ability to pass additional key word arguments to `cached_transformers.get()`, which will be passed on to `AutoModel.from_pretrained()`.\n- Added an `overrides` argument to `Predictor.from_path()`.\n- Added a `cached-path` command.\n- Added a function `inspect_cache` to `common.file_utils` that prints useful information about the cache. This can also\n  be used from the `cached-path` command with `allennlp cached-path --inspect`.\n- Added a function `remove_cache_entries` to `common.file_utils` that removes any cache entries matching the given\n  glob patterns. This can used from the `cached-path` command with `allennlp cached-path --remove some-files-*`.\n- Added logging for the main process when running in distributed mode.\n- Added a `TrainerCallback` object to support state sharing between batch and epoch-level training callbacks.\n- Added support for .tar.gz in PretrainedModelInitializer.\n- Made `BeamSearch` instantiable `from_params`.\n- Pass `serialization_dir` to `Model` and `DatasetReader`.\n- Added an optional `include_in_archive` parameter to the top-level of configuration files. When specified, `include_in_archive` should be a list of paths relative to the serialization directory which will be bundled up with the final archived model from a training run.\n\n### Changed\n\n- Subcommands that don't require plugins will no longer cause plugins to be loaded or have an `--include-package` flag.\n- Allow overrides to be JSON string or `dict`.\n- `transformers` dependency updated to version 3.1.0.\n- When `cached_path` is called on a local archive with `extract_archive=True`, the archive is now extracted into a unique subdirectory of the cache root instead of a subdirectory of the archive's directory. The extraction directory is also unique to the modification time of the archive, so if the file changes, subsequent calls to `cached_path` will know to re-extract the archive.\n- Removed the `truncation_strategy` parameter to `PretrainedTransformerTokenizer`. The way we're calling the tokenizer, the truncation strategy takes no effect anyways.\n- Don't use initializers when loading a model, as it is not needed.\n- Distributed training will now automatically search for a local open port if the `master_port` parameter is not provided.\n- In training, save model weights before evaluation.\n- `allennlp.common.util.peak_memory_mb` renamed to `peak_cpu_memory`, and `allennlp.common.util.gpu_memory_mb` renamed to `peak_gpu_memory`,\n  and they both now return the results in bytes as integers. Also, the `peak_gpu_memory` function now utilizes PyTorch functions to find the memory\n  usage instead of shelling out to the `nvidia-smi` command. This is more efficient and also more accurate because it only takes\n  into account the tensor allocations of the current PyTorch process.\n- Make sure weights are first loaded to the cpu when using PretrainedModelInitializer, preventing wasted GPU memory.\n- Load dataset readers in `load_archive`.\n- Updated `AllenNlpTestCase` docstring to remove reference to `unittest.TestCase`\n\n### Removed\n\n- Removed `common.util.is_master` function.\n\n### Fixed\n\n- Fix CUDA/CPU device mismatch bug during distributed training for categorical accuracy metric.\n- Fixed a bug where the reported `batch_loss` metric was incorrect when training with gradient accumulation.\n- Class decorators now displayed in API docs.\n- Fixed up the documentation for the `allennlp.nn.beam_search` module.\n- Ignore `*args` when constructing classes with `FromParams`.\n- Ensured some consistency in the types of the values that metrics return.\n- Fix a PyTorch warning by explicitly providing the `as_tuple` argument (leaving\n  it as its default value of `False`) to `Tensor.nonzero()`.\n- Remove temporary directory when extracting model archive in `load_archive`\n  at end of function rather than via `atexit`.\n- Fixed a bug where using `cached_path()` offline could return a cached resource's lock file instead\n  of the cache file.\n- Fixed a bug where `cached_path()` would fail if passed a `cache_dir` with the user home shortcut `~/`.\n- Fixed a bug in our doc building script where markdown links did not render properly\n  if the \"href\" part of the link (the part inside the `()`) was on a new line.\n- Changed how gradients are zeroed out with an optimization. See [this video from NVIDIA](https://www.youtube.com/watch?v=9mS1fIYj1So)\n  at around the 9 minute mark.\n- Fixed a bug where parameters to a `FromParams` class that are dictionaries wouldn't get logged\n  when an instance is instantiated `from_params`.\n- Fixed a bug in distributed training where the vocab would be saved from every worker, when it should have been saved by only the local master process.\n- Fixed a bug in the calculation of rouge metrics during distributed training where the total sequence count was not being aggregated across GPUs.\n- Fixed `allennlp.nn.util.add_sentence_boundary_token_ids()` to use `device` parameter of input tensor.\n- Be sure to close the TensorBoard writer even when training doesn't finish.\n- Fixed the docstring for `PyTorchSeq2VecWrapper`.\n- Fixed a bug in the cnn_encoder where activations involving masked tokens could be picked up by the max\n- Fix intra word tokenization for `PretrainedTransformerTokenizer` when disabling fast tokenizer.\n\n## [v1.1.0](https://github.com/allenai/allennlp/releases/tag/v1.1.0) - 2020-09-08\n\n### Fixed\n\n- Fixed handling of some edge cases when constructing classes with `FromParams` where the class\n  accepts `**kwargs`.\n- Fixed division by zero error when there are zero-length spans in the input to a\n  `PretrainedTransformerMismatchedIndexer`.\n- Improved robustness of `cached_path` when extracting archives so that the cache won't be corrupted\n  if a failure occurs during extraction.\n- Fixed a bug with the `average` and `evalb_bracketing_score` metrics in distributed training.\n\n### Added\n\n- `Predictor.capture_model_internals()` now accepts a regex specifying which modules to capture.\n\n## [v1.1.0rc4](https://github.com/allenai/allennlp/releases/tag/v1.1.0rc4) - 2020-08-20\n\n### Added\n\n- Added a workflow to GitHub Actions that will automatically close unassigned stale issues and\n  ping the assignees of assigned stale issues.\n\n### Fixed\n\n- Fixed a bug in distributed metrics that caused nan values due to repeated addition of an accumulated variable.\n\n## [v1.1.0rc3](https://github.com/allenai/allennlp/releases/tag/v1.1.0rc3) - 2020-08-12\n\n### Fixed\n\n- Fixed how truncation was handled with `PretrainedTransformerTokenizer`.\n  Previously, if `max_length` was set to `None`, the tokenizer would still do truncation if the\n  transformer model had a default max length in its config.\n  Also, when `max_length` was set to a non-`None` value, several warnings would appear\n  for certain transformer models around the use of the `truncation` parameter.\n- Fixed evaluation of all metrics when using distributed training.\n- Added a `py.typed` marker. Fixed type annotations in `allennlp.training.util`.\n- Fixed problem with automatically detecting whether tokenization is necessary.\n  This affected primarily the Roberta SST model.\n- Improved help text for using the --overrides command line flag.\n\n## [v1.1.0rc2](https://github.com/allenai/allennlp/releases/tag/v1.1.0rc2) - 2020-07-31\n\n### Changed\n\n- Upgraded PyTorch requirement to 1.6.\n- Replaced the NVIDIA Apex AMP module with torch's native AMP module. The default trainer (`GradientDescentTrainer`)\n  now takes a `use_amp: bool` parameter instead of the old `opt_level: str` parameter.\n\n### Fixed\n\n- Removed unnecessary warning about deadlocks in `DataLoader`.\n- Fixed testing models that only return a loss when they are in training mode.\n- Fixed a bug in `FromParams` that caused silent failure in case of the parameter type being `Optional[Union[...]]`.\n- Fixed a bug where the program crashes if `evaluation_data_loader` is a `AllennlpLazyDataset`.\n\n### Added\n\n- Added the option to specify `requires_grad: false` within an optimizer's parameter groups.\n- Added the `file-friendly-logging` flag back to the `train` command. Also added this flag to the `predict`, `evaluate`, and `find-learning-rate` commands.\n- Added an `EpochCallback` to track current epoch as a model class member.\n- Added the option to enable or disable gradient checkpointing for transformer token embedders via boolean parameter `gradient_checkpointing`.\n\n### Removed\n\n- Removed the `opt_level` parameter to `Model.load` and `load_archive`. In order to use AMP with a loaded\n  model now, just run the model's forward pass within torch's [`autocast`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast)\n  context.\n\n## [v1.1.0rc1](https://github.com/allenai/allennlp/releases/tag/v1.1.0rc1) - 2020-07-14\n\n### Fixed\n\n- Reduced the amount of log messages produced by `allennlp.common.file_utils`.\n- Fixed a bug where `PretrainedTransformerEmbedder` parameters appeared to be trainable\n  in the log output even when `train_parameters` was set to `False`.\n- Fixed a bug with the sharded dataset reader where it would only read a fraction of the instances\n  in distributed training.\n- Fixed checking equality of `TensorField`s.\n- Fixed a bug where `NamespaceSwappingField` did not work correctly with `.empty_field()`.\n- Put more sensible defaults on the `huggingface_adamw` optimizer.\n- Simplified logging so that all logging output always goes to one file.\n- Fixed interaction with the python command line debugger.\n- Log the grad norm properly even when we're not clipping it.\n- Fixed a bug where `PretrainedModelInitializer` fails to initialize a model with a 0-dim tensor\n- Fixed a bug with the layer unfreezing schedule of the `SlantedTriangular` learning rate scheduler.\n- Fixed a regression with logging in the distributed setting. Only the main worker should write log output to the terminal.\n- Pinned the version of boto3 for package managers (e.g. poetry).\n- Fixed issue #4330 by updating the `tokenizers` dependency.\n- Fixed a bug in `TextClassificationPredictor` so that it passes tokenized inputs to the `DatasetReader`\n  in case it does not have a tokenizer.\n- `reg_loss` is only now returned for models that have some regularization penalty configured.\n- Fixed a bug that prevented `cached_path` from downloading assets from GitHub releases.\n- Fixed a bug that erroneously increased last label's false positive count in calculating fbeta metrics.\n- `Tqdm` output now looks much better when the output is being piped or redirected.\n- Small improvements to how the API documentation is rendered.\n- Only show validation progress bar from main process in distributed training.\n\n### Added\n\n- Adjust beam search to support multi-layer decoder.\n- A method to ModelTestCase for running basic model tests when you aren't using config files.\n- Added some convenience methods for reading files.\n- Added an option to `file_utils.cached_path` to automatically extract archives.\n- Added the ability to pass an archive file instead of a local directory to `Vocab.from_files`.\n- Added the ability to pass an archive file instead of a glob to `ShardedDatasetReader`.\n- Added a new `\"linear_with_warmup\"` learning rate scheduler.\n- Added a check in `ShardedDatasetReader` that ensures the base reader doesn't implement manual\n  distributed sharding itself.\n- Added an option to `PretrainedTransformerEmbedder` and `PretrainedTransformerMismatchedEmbedder` to use a\n  scalar mix of all hidden layers from the transformer model instead of just the last layer. To utilize\n  this, just set `last_layer_only` to `False`.\n- `cached_path()` can now read files inside of archives.\n- Training metrics now include `batch_loss` and `batch_reg_loss` in addition to aggregate loss across number of batches.\n\n### Changed\n\n- Not specifying a `cuda_device` now automatically determines whether to use a GPU or not.\n- Discovered plugins are logged so you can see what was loaded.\n- `allennlp.data.DataLoader` is now an abstract registrable class. The default implementation\n  remains the same, but was renamed to `allennlp.data.PyTorchDataLoader`.\n- `BertPooler` can now unwrap and re-wrap extra dimensions if necessary.\n- New `transformers` dependency. Only version >=3.0 now supported.\n\n## [v1.0.0](https://github.com/allenai/allennlp/releases/tag/v1.0.0) - 2020-06-16\n\n### Fixed\n\n- Lazy dataset readers now work correctly with multi-process data loading.\n- Fixed race conditions that could occur when using a dataset cache.\n\n### Added\n\n- A bug where where all datasets would be loaded for vocab creation even if not needed.\n- A parameter to the `DatasetReader` class: `manual_multi_process_sharding`. This is similar\n  to the `manual_distributed_sharding` parameter, but applies when using a multi-process\n  `DataLoader`.\n\n## [v1.0.0rc6](https://github.com/allenai/allennlp/releases/tag/v1.0.0rc6) - 2020-06-11\n\n### Fixed\n\n- A bug where `TextField`s could not be duplicated since some tokenizers cannot be deep-copied.\n  See https://github.com/allenai/allennlp/issues/4270.\n- Our caching mechanism had the potential to introduce race conditions if multiple processes\n  were attempting to cache the same file at once. This was fixed by using a lock file tied to each\n  cached file.\n- `get_text_field_mask()` now supports padding indices that are not `0`.\n- A bug where `predictor.get_gradients()` would return an empty dictionary if an embedding layer had trainable set to false\n- Fixes `PretrainedTransformerMismatchedIndexer` in the case where a token consists of zero word pieces.\n- Fixes a bug when using a lazy dataset reader that results in a `UserWarning` from PyTorch being printed at\n  every iteration during training.\n- Predictor names were inconsistently switching between dashes and underscores. Now they all use underscores.\n- `Predictor.from_path` now automatically loads plugins (unless you specify `load_plugins=False`) so\n  that you don't have to manually import a bunch of modules when instantiating predictors from\n  an archive path.\n- `allennlp-server` automatically found as a plugin once again.\n\n### Added\n\n- A `duplicate()` method on `Instance`s and `Field`s, to be used instead of `copy.deepcopy()`\n- A batch sampler that makes sure each batch contains approximately the same number of tokens (`MaxTokensBatchSampler`)\n- Functions to turn a sequence of token indices back into tokens\n- The ability to use Huggingface encoder/decoder models as token embedders\n- Improvements to beam search\n- ROUGE metric\n- Polynomial decay learning rate scheduler\n- A `BatchCallback` for logging CPU and GPU memory usage to tensorboard. This is mainly for debugging\n  because using it can cause a significant slowdown in training.\n- Ability to run pretrained transformers as an embedder without training the weights\n- Add Optuna Integrated badge to README.md\n\n### Changed\n\n- Similar to our caching mechanism, we introduced a lock file to the vocab to avoid race\n  conditions when saving/loading the vocab from/to the same serialization directory in different processes.\n- Changed the `Token`, `Instance`, and `Batch` classes along with all `Field` classes to \"slots\" classes. This dramatically reduces the size in memory of instances.\n- SimpleTagger will no longer calculate span-based F1 metric when `calculate_span_f1` is `False`.\n- CPU memory for every worker is now reported in the logs and the metrics. Previously this was only reporting the CPU memory of the master process, and so it was only\n  correct in the non-distributed setting.\n- To be consistent with PyTorch `IterableDataset`, `AllennlpLazyDataset` no longer implements `__len__()`.\n  Previously it would always return 1.\n- Removed old tutorials, in favor of [the new AllenNLP Guide](https://guide.allennlp.org)\n- Changed the vocabulary loading to consider new lines for Windows/Linux and Mac.\n\n## [v1.0.0rc5](https://github.com/allenai/allennlp/releases/tag/v1.0.0rc5) - 2020-05-26\n\n### Fixed\n\n- Fix bug where `PretrainedTransformerTokenizer` crashed with some transformers (#4267)\n- Make `cached_path` work offline.\n- Tons of docstring inconsistencies resolved.\n- Nightly builds no longer run on forks.\n- Distributed training now automatically figures out which worker should see which instances\n- A race condition bug in distributed training caused from saving the vocab to file from the master process while other processing might be reading those files.\n- Unused dependencies in `setup.py` removed.\n\n### Added\n\n- Additional CI checks to ensure docstrings are consistently formatted.\n- Ability to train on CPU with multiple processes by setting `cuda_devices` to a list of negative integers in your training config. For example: `\"distributed\": {\"cuda_devices\": [-1, -1]}`. This is mainly to make it easier to test and debug distributed training code..\n- Documentation for when parameters don't need config file entries.\n\n### Changed\n\n- The `allennlp test-install` command now just ensures the core submodules can\n  be imported successfully, and prints out some other useful information such as the version, PyTorch version,\n  and the number of GPU devices available.\n- All of the tests moved from `allennlp/tests` to `tests` at the root level, and\n  `allennlp/tests/fixtures` moved to `test_fixtures` at the root level. The PyPI source and wheel distributions will no longer include tests and fixtures.\n\n## [v1.0.0rc4](https://github.com/allenai/allennlp/releases/tag/v1.0.0rc4) - 2020-05-14\n\nWe first introduced this `CHANGELOG` after release `v1.0.0rc4`, so please refer to the GitHub release\nnotes for this and earlier releases.\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 1.3291015625,
          "content": "# YAML 1.2\n---\nauthors: \n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Gardner\n    given-names: Matt\n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Grus\n    given-names: Joel\n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Neumann\n    given-names: Mark\n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Tafjord\n    given-names: Oyvind\n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Dasigi\n    given-names: Pradeep\n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Liu\n    given-names: Nelson\n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Peters\n    given-names: Matthew\n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Schmitz\n    given-names: Michael\n  -\n    affiliation: \"Allen Institute for Artificial Intelligence\"\n    family-names: Zettlemoyer\n    given-names: Luke\ncff-version: \"1.2.0\"\nlicense: \"Apache-2.0\"\nmessage: \"If you use this software, please cite it using this metadata.\"\nrepository-code: \"https://github.com/allenai/allennlp\"\ntitle: \"AllenNLP: A Deep Semantic Natural Language Processing Platform\"\nversion: \"2.10.1\"\ndoi: \"10.18653/v1/W18-2501\"\n...\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 11.796875,
          "content": "# Contributing\n\nThanks for considering contributing!  We want AllenNLP to be *the way* to do cutting-edge NLP research, but we cannot\nget there without community support.\n\n## How Can I Contribute?\n\n### Bug fixes and new features\n\n**Did you find a bug?**\n\nFirst, do [a quick search](https://github.com/allenai/allennlp/issues) to see whether your issue has already been reported.\nIf your issue has already been reported, please comment on the existing issue.\n\nOtherwise, open [a new GitHub issue](https://github.com/allenai/allennlp/issues).  Be sure to include a clear title\nand description.  The description should include as much relevant information as possible.  The description should\nexplain how to reproduce the erroneous behavior as well as the behavior you expect to see.  Ideally you would include a\ncode sample or an executable test case demonstrating the expected behavior.\n\n**Do you have a suggestion for an enhancement?**\n\nWe use GitHub issues to track enhancement requests.  Before you create an enhancement request:\n\n* Make sure you have a clear idea of the enhancement you would like.  If you have a vague idea, consider discussing\nit first on a GitHub issue.\n\n* Check the documentation to make sure your feature does not already exist.\n\n* Do [a quick search](https://github.com/allenai/allennlp/issues) to see whether your enhancement has already been suggested.\n\nWhen creating your enhancement request, please:\n\n* Provide a clear title and description.\n\n* Explain why the enhancement would be useful.  It may be helpful to highlight the feature in other libraries.\n\n* Include code examples to demonstrate how the enhancement would be used.\n\n### Making a pull request\n\nWhen you're ready to contribute code to address an open issue, please follow these guidelines to help us be able to review your pull request (PR) quickly.\n\n1. **Initial setup** (only do this once)\n\n    <details><summary>Expand details </summary><br/>\n\n    If you haven't already done so, please [fork](https://help.github.com/en/enterprise/2.13/user/articles/fork-a-repo) this repository on GitHub.\n    \n    Then clone your fork locally with\n    \n        git clone https://github.com/USERNAME/allennlp.git\n    \n    or \n    \n        git clone git@github.com:USERNAME/allennlp.git\n    \n    At this point the local clone of your fork only knows that it came from *your* repo, github.com/USERNAME/allennlp.git, but doesn't know anything the *main* repo, [https://github.com/allenai/allennlp.git](https://github.com/allenai/allennlp). You can see this by running\n    \n        git remote -v\n    \n    which will output something like this:\n    \n        origin https://github.com/USERNAME/allennlp.git (fetch)\n        origin https://github.com/USERNAME/allennlp.git (push)\n    \n    This means that your local clone can only track changes from your fork, but not from the main repo, and so you won't be able to keep your fork up-to-date with the main repo over time. Therefor you'll need to add another \"remote\" to your clone that points to [https://github.com/allenai/allennlp.git](https://github.com/allenai/allennlp). To do this, run the following:\n    \n        git remote add upstream https://github.com/allenai/allennlp.git\n    \n    Now if you do `git remote -v` again, you'll see\n    \n        origin https://github.com/USERNAME/allennlp.git (fetch)\n        origin https://github.com/USERNAME/allennlp.git (push)\n        upstream https://github.com/allenai/allennlp.git (fetch)\n        upstream https://github.com/allenai/allennlp.git (push)\n\n    Finally, you'll need to create a Python 3 virtual environment suitable for working on AllenNLP. There a number of tools out there that making working with virtual environments easier, but the most direct way is with the [`venv` module](https://docs.python.org/3.7/library/venv.html) in the standard library.\n\n    Once your virtual environment is activated, you can install your local clone in \"editable mode\" with\n\n        pip install -U pip setuptools wheel\n        pip install -e .[dev,all] \n\n    The \"editable mode\" comes from the `-e` argument to `pip`, and essential just creates a symbolic link from the site-packages directory of your virtual environment to the source code in your local clone. That way any changes you make will be immediately reflected in your virtual environment.\n\n    </details>\n\n2. **Ensure your fork is up-to-date**\n\n    <details><summary>Expand details </summary><br/>\n\n    Once you've added an \"upstream\" remote pointing to [https://github.com/allenai/allennlp.git](https://github.com/allenai/allennlp), keeping your fork up-to-date is easy:\n    \n        git checkout main  # if not already on main\n        git pull --rebase upstream main\n        git push\n\n    </details>\n\n3. **Create a new branch to work on your fix or enhancement**\n\n    <details><summary>Expand details </summary><br/>\n\n    Commiting directly to the main branch of your fork is not recommended. It will be easier to keep your fork clean if you work on a seperate branch for each contribution you intend to make.\n    \n    You can create a new branch with\n    \n        # replace BRANCH with whatever name you want to give it\n        git checkout -b BRANCH\n        git push -u origin BRANCH\n\n    </details>\n\n4. **Test your changes**\n\n    <details><summary>Expand details </summary><br/>\n\n    Our continuous integration (CI) testing runs [a number of checks](https://github.com/allenai/allennlp/actions?query=workflow%3APR) for each pull request on [GitHub Actions](https://github.com/features/actions). You can run most of these tests locally, which is something you should do *before* opening a PR to help speed up the review process and make it easier for us.\n    \n    First, you should run [`black`](https://github.com/psf/black) to make sure you code is formatted consistently. Many IDEs support code formatters as plugins, so you may be able to setup black to run automatically everytime you save. [`black.vim`](https://github.com/psf/black/tree/master/plugin) will give you this functionality in Vim, for example. But `black` is also easy to run directly from the command line. Just run this from the root of your clone:\n    \n        black .\n\n    Our CI also uses [`flake8`](https://github.com/allenai/allennlp/tree/main/tests) to lint the code base and [`mypy`](http://mypy-lang.org/) for type-checking. You should run both of these next with\n\n        flake8 .\n\n    and\n\n        make typecheck\n\n    We also strive to maintain high test coverage, so most contributions should include additions to [the unit tests](https://github.com/allenai/allennlp/tree/main/tests). These tests are run with [`pytest`](https://docs.pytest.org/en/latest/), which you can use to locally run any test modules that you've added or changed.\n\n    For example, if you've fixed a bug in `allennlp/nn/util.py`, you can run the tests specific to that module with\n    \n        pytest -v tests/nn/util_test.py\n    \n    Our CI will automatically check that test coverage stays above a certain threshold (around 90%). To check the coverage locally in this example, you could run\n    \n        pytest -v --cov allennlp.nn.util tests/nn/util_test.py\n\n    If your contribution involves additions to any public part of the API, we require that you write docstrings\n    for each function, method, class, or module that you add.\n    See the [Writing docstrings](#writing-docstrings) section below for details on the syntax.\n    You should test to make sure the API documentation can build without errors by running\n\n        make build-docs\n\n    If the build fails, it's most likely due to small formatting issues. If the error message isn't clear, feel free to comment on this in your pull request.\n\n    You can also serve and view the docs locally with\n    \n        make serve-docs\n\n    And finally, please update the [CHANGELOG](https://github.com/allenai/allennlp/blob/main/CHANGELOG.md) with notes on your contribution in the \"Unreleased\" section at the top.\n\n    After all of the above checks have passed, you can now open [a new GitHub pull request](https://github.com/allenai/allennlp/pulls).\n    Make sure you have a clear description of the problem and the solution, and include a link to relevant issues.\n\n    We look forward to reviewing your PR!\n\n    </details>\n\n### Writing docstrings\n\nOur docstrings are written in a syntax that is essentially just Markdown with additional special syntax for writing parameter descriptions.\n\nClass docstrings should start with a description of the class, followed by a `# Parameters` section\nthat lists the names, types, and purpose of all parameters to the class's `__init__()` method.\nParameter descriptions should look like:\n\n```\nname : `type`\n    Description of the parameter, indented by four spaces.\n```\n\nOptional parameters can also be written like this:\n\n```\nname : `type`, optional (default = `default_value`)\n    Description of the parameter, indented by four spaces.\n```\n\nSometimes you can omit the description if the parameter is self-explanatory.\n\nMethod and function docstrings are similar, but should also include a `# Returns`\nsection when the return value is not obvious. Other valid sections are\n\n- `# Attributes`, for listing class attributes. These should be formatted in the same\n    way as parameters.\n- `# Raises`, for listing any errors that the function or method might intentionally raise.\n- `# Examples`, where you can include code snippets.\n\nHere is an example of what the docstrings should look like in a class:\n\n\n```python\nclass SentenceClassifier(Model):\n    \"\"\"\n    A model for classifying sentences.\n\n    This is based on [this paper](link-to-paper). The input is a sentence and\n    the output is a score for each target label.\n\n    # Parameters\n\n    vocab : `Vocabulary`\n\n    text_field_embedder : `TextFieldEmbedder`\n        The text field embedder that will be used to create a representation of the\n        source tokens.\n\n    seq2vec_encoder : `Seq2VeqEncoder`\n        This encoder will take the embeddings from the `text_field_embedder` and\n        encode them into a vector which represents the un-normalized scores\n        for the target labels.\n\n    dropout : `Optional[float]`, optional (default = `None`)\n        Optional dropout to apply to the text field embeddings before passing through\n        the `seq2vec_encoder`.\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        text_field_embedder: TextFieldEmbedder,\n        seq2vec_encoder: Seq2SeqEncoder,\n        dropout: Optional[float] = None,\n    ) -> None:\n        pass\n\n    def forward(\n        self,\n        tokens: TextFieldTensors,\n        labels: Optional[Tensor] = None,\n    ) -> Dict[str, Tensor]:\n        \"\"\"\n        Runs a forward pass of the model, computing the predicted logits and also the loss\n        when `labels` is provided.\n\n        # Parameters\n\n        tokens : `TextFieldTensors`\n            The tokens corresponding to the source sequence.\n\n        labels : `Optional[Tensor]`, optional (default = `None`)\n            The target labels.\n\n        # Returns\n\n        `Dict[str, Tensor]`\n            An output dictionary with keys for the `loss` and `logits`.\n        \"\"\"\n        pass\n```\n\n### New models\n\n**Do you have a new state-of-the-art model?**\n\nWe are always looking for new models to add to our collection. The most popular models are usually added to the official [AllenNLP Models](https://github.com/allenai/allennlp-models) repository, and in some cases to the [AllenNLP Demo](https://demo.allennlp.org/).\n\nIf you think your model should be part of AllenNLP Models, please [create a pull request](https://github.com/allenai/allennlp-models/pulls) in the models repo that includes:\n\n* Any code changes needed to support your new model.\n\n* A link to the model itself.  Please do not check your model into the GitHub repository, but instead upload it in the\nPR conversation or provide a link to it at an external location.\n\nIn the description of your PR, please clearly explain the task your model performs along with the relevant metrics on an established dataset.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.162109375,
          "content": "# This Dockerfile creates an environment suitable for downstream usage of AllenNLP.\n# It's built from a wheel installation of allennlp using the base images from\n# https://github.com/allenai/docker-images/pkgs/container/pytorch\n\nARG TORCH=1.12.0-cuda11.3-python3.8\nFROM ghcr.io/allenai/pytorch:${TORCH}\n\nWORKDIR /stage/allennlp\n\n# Installing AllenNLP's dependencies is the most time-consuming part of building\n# this Docker image, so we make use of layer caching here by adding the minimal files\n# necessary to install the dependencies.\nCOPY allennlp/version.py allennlp/version.py\nCOPY setup.py .\nCOPY requirements.txt .\nCOPY dev-requirements.txt .\nCOPY constraints.txt .\nRUN touch allennlp/__init__.py \\\n    && touch README.md \\\n    && pip install --no-cache-dir -c constraints.txt -e .[all]\n\n# Now add the full package source and re-install just the package.\nCOPY allennlp allennlp\nRUN pip install --no-cache-dir --no-deps -e .[all]\n\nCOPY Makefile .\nRUN make download-extras\n\nWORKDIR /app/\n\n# Copy wrapper script to allow beaker to run resumable training workloads.\nCOPY scripts/ai2_internal/resumable_train.sh .\n\nLABEL maintainer=\"allennlp-contact@allenai.org\"\n\nENTRYPOINT [\"allennlp\"]\n"
        },
        {
          "name": "Dockerfile.test",
          "type": "blob",
          "size": 0.9296875,
          "content": "# Used to build an image for running tests.\n\nARG TORCH=1.12.0-cuda11.3-python3.8\nFROM ghcr.io/allenai/pytorch:${TORCH}\n\n# These environment variables are helpful for debugging.\n# See https://pytorch.org/docs/stable/distributed.html#common-environment-variables for more info.\nENV NCCL_DEBUG INFO\nENV NCCL_DEBUG_SUBSYS ALL\n\nWORKDIR /stage/allennlp\n\n# Installing AllenNLP's dependencies is the most time-consuming part of building\n# this Docker image, so we make use of layer caching here by adding the minimal files\n# necessary to install the dependencies.\nCOPY allennlp/version.py allennlp/version.py\nCOPY setup.py .\nCOPY dev-requirements.txt .\nCOPY constraints.txt .\nRUN touch allennlp/__init__.py \\\n    && touch README.md \\\n    && pip install --no-cache-dir -c constraints.txt -e . -r dev-requirements.txt\n\n# Now add the full package source and re-install just the package.\nCOPY . .\nRUN pip install --no-cache-dir --no-deps -e .\n\nENTRYPOINT [\"make\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0927734375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        https://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       https://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1640625,
          "content": "include LICENSE\ninclude README.md\nrecursive-include allennlp *\nrecursive-include scripts *\nglobal-exclude .DS_Store *.py[cod]\nprune **/__pycache__\nprune **/.mypy_cache\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 5.0986328125,
          "content": "SRC = allennlp\n\nMD_DOCS_ROOT = docs/\nMD_DOCS_API_ROOT = $(MD_DOCS_ROOT)api/\nMD_DOCS_SRC = $(filter-out $(SRC)/__main__.py %/__init__.py $(SRC)/version.py,$(shell find $(SRC) -type f -name '*.py' | grep -v -E 'tests/'))\nMD_DOCS = $(subst .py,.md,$(subst $(SRC)/,$(MD_DOCS_API_ROOT),$(MD_DOCS_SRC)))\nMD_DOCS_CMD = python scripts/py2md.py\nMD_DOCS_CONF = mkdocs.yml\nMD_DOCS_CONF_SRC = mkdocs-skeleton.yml\nMD_DOCS_TGT = site/\nMD_DOCS_EXTRAS = $(addprefix $(MD_DOCS_ROOT),README.md CHANGELOG.md CONTRIBUTING.md)\n\nTORCH_INSTALL = pip install torch torchvision -c constraints.txt\nDOCKER_TORCH_VERSION = 1.12.0-cuda11.3-python3.8\nDOCKER_TEST_TORCH_VERSION = 1.12.0-cuda11.3-python3.8\n\nDOCKER_TAG = latest\nDOCKER_IMAGE_NAME = allennlp/allennlp:$(DOCKER_TAG)\nDOCKER_TEST_IMAGE_NAME = allennlp/test:$(DOCKER_TAG)\nDOCKER_RUN_CMD = docker run --rm \\\n\t\t-v $$HOME/.allennlp:/root/.allennlp \\\n\t\t-v $$HOME/.cache/huggingface:/root/.cache/huggingface \\\n\t\t-v $$HOME/nltk_data:/root/nltk_data\n\n# These nltk packages are used by the 'checklist' module. They are downloaded automatically\n# if not found when `checklist` is imported, but it's good to download the ahead of time\n# to avoid potential race conditions.\nNLTK_DOWNLOAD_CMD = python -c 'import nltk; [nltk.download(p) for p in (\"wordnet\", \"wordnet_ic\", \"sentiwordnet\", \"omw\", \"omw-1.4\")]'\n\nifeq ($(shell uname),Darwin)\nifeq ($(shell which gsed),)\n$(error Please install GNU sed with 'brew install gnu-sed')\nelse\nSED = gsed\nendif\nelse\nSED = sed\nendif\n\n.PHONY : version\nversion :\n\t@python -c 'from allennlp.version import VERSION; print(f\"AllenNLP v{VERSION}\")'\n\n.PHONY : check-for-cuda\ncheck-for-cuda :\n\t@python -c 'import torch; assert torch.cuda.is_available(); print(\"Cuda is available\")'\n\n#\n# Testing helpers.\n#\n\n.PHONY : flake8\nflake8 :\n\tflake8 allennlp tests scripts \n\n.PHONY : format\nformat :\n\tblack --check allennlp tests scripts \n\n.PHONY : typecheck\ntypecheck :\n\tmypy allennlp tests scripts --cache-dir=/dev/null\n\n.PHONY : test\ntest :\n\tpytest --color=yes -v -rf --durations=40 \\\n\t\t\t--cov-config=.coveragerc \\\n\t\t\t--cov=$(SRC) \\\n\t\t\t--cov-report=xml\n\n.PHONY : test-without-checklist\ntest-without-checklist :\n\tpytest --color=yes -v -rf --durations=40 \\\n\t\t\t--cov-config=.coveragerc \\\n\t\t\t--cov=$(SRC) \\\n\t\t\t--cov-report=xml \\\n\t\t\t--ignore-glob=*checklist*\n\n.PHONY : test-checklist\ntest-checklist :\n\tpytest --color=yes -v -rf --durations=40 \\\n\t\t\t--cov-config=.coveragerc \\\n\t\t\t--cov=$(SRC) \\\n\t\t\t--cov-report=xml \\\n\t\t\ttests/ \\\n\t\t\t-k checklist\n\n\n.PHONY : gpu-tests\ngpu-tests : check-for-cuda\n\tpytest --color=yes -v -rf --durations=20 \\\n\t\t\t--cov-config=.coveragerc \\\n\t\t\t--cov=$(SRC) \\\n\t\t\t--cov-report=xml \\\n\t\t\t-m gpu\n\n.PHONY : benchmarks\nbenchmarks :\n\tpytest -c benchmarks/pytest.ini benchmarks/\n\n#\n# Setup helpers\n#\n\n.PHONY : download-extras\ndownload-extras :\n\t$(NLTK_DOWNLOAD_CMD)\n\n.PHONY : install\ninstall :\n\t# Due to a weird thing with pip, we may need egg-info before running `pip install -e`.\n\t# See https://github.com/pypa/pip/issues/4537.\n\t# python setup.py install_egg_info\n\t# Install torch ecosystem first.\n\t$(TORCH_INSTALL)\n\tpip install --upgrade pip\n\tpip install pip-tools\n\tpip-compile requirements.in -o final_requirements.txt --allow-unsafe --rebuild --verbose\n\tpip install -e . -r final_requirements.txt\n\t# These nltk packages are used by the 'checklist' module.\n\t$(NLTK_DOWNLOAD_CMD)\n\n#\n# Documention helpers.\n#\n\n.PHONY : build-all-api-docs\nbuild-all-api-docs :\n\t@$(MD_DOCS_CMD) $(subst /,.,$(subst .py,,$(MD_DOCS_SRC))) -o $(MD_DOCS)\n\n.PHONY : build-docs\nbuild-docs : build-all-api-docs $(MD_DOCS_CONF) $(MD_DOCS) $(MD_DOCS_EXTRAS)\n\tmkdocs build\n\n.PHONY : serve-docs\nserve-docs : build-all-api-docs $(MD_DOCS_CONF) $(MD_DOCS) $(MD_DOCS_EXTRAS)\n\tmkdocs serve --dirtyreload\n\n.PHONY : update-docs\nupdate-docs : $(MD_DOCS) $(MD_DOCS_EXTRAS)\n\n$(MD_DOCS_ROOT)README.md : README.md\n\tcp $< $@\n\t# Alter the relative path of the README image for the docs.\n\t$(SED) -i '1s/docs/./' $@\n\t# Alter external doc links to relative links.\n\t$(SED) -i 's|https://docs.allennlp.org/main/api/|api/|' $@\n\n$(MD_DOCS_ROOT)%.md : %.md\n\tcp $< $@\n\n$(MD_DOCS_CONF) : $(MD_DOCS_CONF_SRC) $(MD_DOCS)\n\tpython scripts/build_docs_config.py $@ $(MD_DOCS_CONF_SRC) $(MD_DOCS_ROOT) $(MD_DOCS_API_ROOT)\n\n$(MD_DOCS_API_ROOT)%.md : $(SRC)/%.py scripts/py2md.py\n\tmkdir -p $(shell dirname $@)\n\t$(MD_DOCS_CMD) $(subst /,.,$(subst .py,,$<)) --out $@\n\n.PHONY : clean\nclean :\n\trm -rf $(MD_DOCS_TGT)\n\trm -rf $(MD_DOCS_API_ROOT)\n\trm -f $(MD_DOCS_ROOT)*.md\n\trm -rf .pytest_cache/\n\trm -rf allennlp.egg-info/\n\trm -rf dist/\n\trm -rf build/\n\tfind . | grep -E '(\\.mypy_cache|__pycache__|\\.pyc|\\.pyo$$)' | xargs rm -rf\n\n#\n# Docker helpers.\n#\n\n.PHONY : docker-image\ndocker-image :\n\tdocker build \\\n\t\t--pull \\\n\t\t-f Dockerfile \\\n\t\t--build-arg TORCH=$(DOCKER_TORCH_VERSION) \\\n\t\t-t $(DOCKER_IMAGE_NAME) .\n\nDOCKER_GPUS = --gpus all\n\n.PHONY : docker-run\ndocker-run :\n\t$(DOCKER_RUN_CMD) $(DOCKER_GPUS) $(DOCKER_IMAGE_NAME) $(ARGS)\n\n.PHONY : docker-test-image\ndocker-test-image :\n\tdocker build \\\n\t\t--pull \\\n\t\t-f Dockerfile.test \\\n\t\t--build-arg TORCH=$(DOCKER_TEST_TORCH_VERSION) \\\n\t\t-t $(DOCKER_TEST_IMAGE_NAME) .\n\n.PHONY : docker-test-run\ndocker-test-run :\n\t$(DOCKER_RUN_CMD) --shm-size 2G $(DOCKER_GPUS) $(DOCKER_TEST_IMAGE_NAME) $(ARGS)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.3388671875,
          "content": "<div align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/allenai/allennlp/main/docs/img/allennlp-logo-dark.png\" width=\"400\"/>\n    <p>\n    An Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.\n    </p>\n    <hr/>\n</div>\n<p align=\"center\">\n    <a href=\"https://github.com/allenai/allennlp/actions\">\n        <img alt=\"CI\" src=\"https://github.com/allenai/allennlp/workflows/CI/badge.svg?event=push&branch=main\">\n    </a>\n    <a href=\"https://pypi.org/project/allennlp/\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/allennlp\">\n    </a>\n    <a href=\"https://github.com/allenai/allennlp/blob/main/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/github/license/allenai/allennlp.svg?color=blue&cachedrop\">\n    </a>\n    <a href=\"https://codecov.io/gh/allenai/allennlp\">\n        <img alt=\"Codecov\" src=\"https://codecov.io/gh/allenai/allennlp/branch/main/graph/badge.svg\">\n    </a>\n    <a href=\"https://optuna.org\">\n        <img alt=\"Optuna\" src=\"https://img.shields.io/badge/Optuna-integrated-blue\">\n    </a>\n    <br/>\n</p>\n\n **NOTICE:** The AllenNLP library is now in maintenance mode. That means we are no longer adding new features or upgrading dependencies. We will still respond to questions and address bugs as they arise up until December 16th, 2022. If you have any concerns or are interested in maintaining AllenNLP going forward, please open an issue on this repository.\n\nAllenNLP has been a big success, but as the field is advancing quickly it's time to focus on new initiatives. We're working hard to make [AI2 Tango](https://github.com/allenai/tango) the best way to organize research codebases. If you are an active user of AllenNLP, here are some suggested alternatives:\n* If you like the trainer, the configuration language, or are simply looking for a better way to manage your experiments, check out [AI2 Tango](https://github.com/allenai/tango).\n* If you like AllenNLP's `modules` and `nn` packages, check out [delmaksym/allennlp-light](https://github.com/delmaksym/allennlp-light). It's even compatible with [AI2 Tango](https://github.com/allenai/tango)!\n* If you like the framework aspect of AllenNLP, check out [flair](https://github.com/flairNLP/flair). It has multiple state-of-art NLP models and allows you to easily use pretrained embeddings such as those from transformers.\n* If you like the AllenNLP metrics package, check out [torchmetrics](https://torchmetrics.readthedocs.io/en/stable/). It has the same API as AllenNLP, so it should be a quick learning curve to make the switch.\n* If you want to vectorize text, try [the transformers library](https://github.com/huggingface/transformers).\n* If you want to maintain the AllenNLP Fairness or Interpret components, please get in touch. There is no alternative to it, so we are looking for a dedicated maintainer.\n* If you are concerned about other AllenNLP functionality, please create an issue. Maybe we can find another way to continue supporting your use case.\n\n## Quick Links\n\n-  [Website](https://allennlp.org/)\n-  [Guide](https://guide.allennlp.org/)\n-  [Gallery](https://gallery.allennlp.org)\n-  [Demo](https://demo.allennlp.org)\n-  [Documentation](https://docs.allennlp.org/) ( [latest](https://docs.allennlp.org/latest/) | [stable](https://docs.allennlp.org/stable/) | [commit](https://docs.allennlp.org/main/) )\n-  [Upgrade Guide from 1.x to 2.0](https://github.com/allenai/allennlp/discussions/4933)\n-  [Stack Overflow](https://stackoverflow.com/questions/tagged/allennlp)\n-  [Contributing Guidelines](CONTRIBUTING.md)\n-  [Officially Supported Models](https://github.com/allenai/allennlp-models)\n    - [Pretrained Models](https://github.com/allenai/allennlp-models/blob/main/allennlp_models/pretrained.py)\n    - [Documentation](https://docs.allennlp.org/models/) ( [latest](https://docs.allennlp.org/models/latest/) | [stable](https://docs.allennlp.org/models/stable/) | [commit](https://docs.allennlp.org/models/main/) )\n-  [Continuous Build](https://github.com/allenai/allennlp/actions)\n-  [Nightly Releases](https://pypi.org/project/allennlp/#history)\n\n## In this README\n\n- [Getting Started Using the Library](#getting-started-using-the-library)\n- [Plugins](#plugins)\n- [Package Overview](#package-overview)\n- [Installation](#installation)\n    - [Installing via pip](#installing-via-pip)\n    - [Installing using Docker](#installing-using-docker)\n    - [Installing from source](#installing-from-source)\n- [Running AllenNLP](#running-allennlp)\n- [Issues](#issues)\n- [Contributions](#contributions)\n- [Citing](#citing)\n- [Team](#team)\n\n## Getting Started Using the Library\n\nIf you're interested in using AllenNLP for model development, we recommend you check out the\n[AllenNLP Guide](https://guide.allennlp.org) for a thorough introduction to the library, followed by our more advanced guides\non [GitHub Discussions](https://github.com/allenai/allennlp/discussions/categories/guides).\n\nWhen you're ready to start your project, we've created a couple of template repositories that you can use as a starting place:\n\n* If you want to use `allennlp train` and config files to specify experiments, use [this\n  template](https://github.com/allenai/allennlp-template-config-files). We recommend this approach.\n* If you'd prefer to use python code to configure your experiments and run your training loop, use\n  [this template](https://github.com/allenai/allennlp-template-python-script). There are a few\n  things that are currently a little harder in this setup (loading a saved model, and using\n  distributed training), but otherwise it's functionality equivalent to the config files\n  setup.\n\nIn addition, there are external tutorials:\n\n* [Hyperparameter optimization for AllenNLP using Optuna](https://medium.com/optuna/hyperparameter-optimization-for-allennlp-using-optuna-54b4bfecd78b)\n* [Training with multiple GPUs in AllenNLP](https://medium.com/ai2-blog/tutorial-how-to-train-with-multiple-gpus-in-allennlp-c4d7c17eb6d6)\n* [Training on larger batches with less memory in AllenNLP](https://medium.com/ai2-blog/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad)\n* [How to upload transformer weights and tokenizers from AllenNLP to HuggingFace](https://medium.com/ai2-blog/tutorial-how-to-upload-transformer-weights-and-tokenizers-from-allennlp-to-huggingface-ecf6c0249bf)\n\nAnd others on the [AI2 AllenNLP blog](https://medium.com/ai2-blog/allennlp/home).\n\n## Plugins\n\nAllenNLP supports loading \"plugins\" dynamically. A plugin is just a Python package that\nprovides custom registered classes or additional `allennlp` subcommands.\n\nThere is ecosystem of open source plugins, some of which are maintained by the AllenNLP\nteam here at AI2, and some of which are maintained by the broader community.\n\n<table>\n<tr>\n    <td><b> Plugin </b></td>\n    <td><b> Maintainer </b></td>\n    <td><b> CLI </b></td>\n    <td><b> Description </b></td>\n</tr>\n<tr>\n    <td> <a href=\"https://github.com/allenai/allennlp-models\"><b>allennlp-models</b></a> </td>\n    <td> AI2 </td>\n    <td> No </td>\n    <td> A collection of state-of-the-art models </td>\n</tr>\n<tr>\n    <td> <a href=\"https://github.com/allenai/allennlp-semparse\"><b>allennlp-semparse</b></a> </td>\n    <td> AI2 </td>\n    <td> No </td>\n    <td> A framework for building semantic parsers </td>\n</tr>\n<tr>\n    <td> <a href=\"https://github.com/allenai/allennlp-server\"><b>allennlp-server</b></a> </td>\n    <td> AI2 </td>\n    <td> Yes </td>\n    <td> A simple demo server for serving models </td>\n</tr>\n<tr>\n    <td> <a href=\"https://github.com/himkt/allennlp-optuna\"><b>allennlp-optuna</b></a> </td>\n    <td> <a href=\"https://himkt.github.io/profile/\">Makoto Hiramatsu</a> </td>\n    <td> Yes </td>\n    <td> <a href=\"https://optuna.org/\">Optuna</a> integration for hyperparameter optimization </td>\n</tr>\n</table>\n\nAllenNLP will automatically find any official AI2-maintained plugins that you have installed,\nbut for AllenNLP to find personal or third-party plugins you've installed,\nyou also have to create either a local plugins file named `.allennlp_plugins`\nin the directory where you run the `allennlp` command, or a global plugins file at `~/.allennlp/plugins`.\nThe file should list the plugin modules that you want to be loaded, one per line.\n\nTo test that your plugins can be found and imported by AllenNLP, you can run the `allennlp test-install` command.\nEach discovered plugin will be logged to the terminal.\n\nFor more information about plugins, see the [plugins API docs](https://docs.allennlp.org/main/api/common/plugins/). And for information on how to create a custom subcommand\nto distribute as a plugin, see the [subcommand API docs](https://docs.allennlp.org/main/api/commands/subcommand/).\n\n## Package Overview\n\n<table>\n<tr>\n    <td><b> allennlp </b></td>\n    <td> An open-source NLP research library, built on PyTorch </td>\n</tr>\n<tr>\n    <td><b> allennlp.commands </b></td>\n    <td> Functionality for the CLI </td>\n</tr>\n<tr>\n    <td><b> allennlp.common </b></td>\n    <td> Utility modules that are used across the library </td>\n</tr>\n<tr>\n    <td><b> allennlp.data </b></td>\n    <td> A data processing module for loading datasets and encoding strings as integers for representation in matrices </td>\n</tr>\n<tr>\n    <td><b> allennlp.fairness </b></td>\n    <td> A module for bias mitigation and fairness algorithms and metrics </td>\n</tr>\n<tr>\n    <td><b> allennlp.modules </b></td>\n    <td> A collection of PyTorch modules for use with text </td>\n</tr>\n<tr>\n    <td><b> allennlp.nn </b></td>\n    <td> Tensor utility functions, such as initializers and activation functions </td>\n</tr>\n<tr>\n    <td><b> allennlp.training </b></td>\n    <td> Functionality for training models </td>\n</tr>\n</table>\n\n## Installation\n\nAllenNLP requires Python 3.6.1 or later and [PyTorch](https://pytorch.org/).\n\nWe support AllenNLP on Mac and Linux environments. We presently do not support Windows but are open to contributions.\n\n### Installing via conda-forge\n\nThe simplest way to install AllenNLP is using conda (you can choose a different python version):\n\n```\nconda install -c conda-forge python=3.8 allennlp\n```\n\nTo install optional packages, such as `checklist`, use\n\n```\nconda install -c conda-forge allennlp-checklist\n```\n\nor simply install `allennlp-all` directly. The plugins mentioned above are similarly installable, e.g.\n\n```\nconda install -c conda-forge allennlp-models allennlp-semparse allennlp-server allennlp-optuna\n```\n\n### Installing via pip\n\nIt's recommended that you install the PyTorch ecosystem **before** installing AllenNLP by following the instructions on [pytorch.org](https://pytorch.org/).\n\nAfter that, just run `pip install allennlp`.\n\n\n\n>  If you're using Python 3.7 or greater, you should ensure that you don't have the PyPI version of `dataclasses` installed after running the above command, as this could cause issues on certain platforms. You can quickly check this by running `pip freeze | grep dataclasses`. If you see something like `dataclasses=0.6` in the output, then just run `pip uninstall -y dataclasses`.\n\nIf you need pointers on setting up an appropriate Python environment or would like to install AllenNLP using a different method, see below.\n\n#### Setting up a virtual environment\n\n[Conda](https://conda.io/) can be used set up a virtual environment with the\nversion of Python required for AllenNLP.  If you already have a Python 3\nenvironment you want to use, you can skip to the 'installing via pip' section.\n\n1.  [Download and install Conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).\n\n2.  Create a Conda environment with Python 3.8 (3.7 or 3.9 would work as well):\n\n    ```\n    conda create -n allennlp_env python=3.8\n    ```\n\n3.  Activate the Conda environment. You will need to activate the Conda environment in each terminal in which you want to use AllenNLP:\n\n    ```\n    conda activate allennlp_env\n    ```\n\n#### Installing the library and dependencies\n\nInstalling the library and dependencies is simple using `pip`.\n\n```bash\npip install allennlp\n```\n\nTo install the optional dependencies, such as `checklist`, run\n\n```bash\npip install allennlp[checklist]\n```\nOr you can just install all optional dependencies with `pip install allennlp[all]`.\n\n*Looking for bleeding edge features? You can install nightly releases directly from [pypi](https://pypi.org/project/allennlp/#history)*\n\nAllenNLP installs a script when you install the python package, so you can run allennlp commands just by typing `allennlp` into a terminal.  For example, you can now test your installation with `allennlp test-install`.\n\nYou may also want to install `allennlp-models`, which contains the NLP constructs to train and run our officially\nsupported models, many of which are hosted at [https://demo.allennlp.org](https://demo.allennlp.org).\n\n```bash\npip install allennlp-models\n```\n\n### Installing using Docker\n\nDocker provides a virtual machine with everything set up to run AllenNLP--\nwhether you will leverage a GPU or just run on a CPU.  Docker provides more\nisolation and consistency, and also makes it easy to distribute your\nenvironment to a compute cluster.\n\nAllenNLP provides [official Docker images](https://hub.docker.com/r/allennlp/allennlp) with the library and all of its dependencies installed.\n\nOnce you have [installed Docker](https://docs.docker.com/engine/installation/),\nyou should also install the [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)\nif you have GPUs available.\n\nThen run the following command to get an environment that will run on GPU:\n\n```bash\nmkdir -p $HOME/.allennlp/\ndocker run --rm --gpus all -v $HOME/.allennlp:/root/.allennlp allennlp/allennlp:latest\n```\n\nYou can test the Docker environment with\n\n```bash\ndocker run --rm --gpus all -v $HOME/.allennlp:/root/.allennlp allennlp/allennlp:latest test-install \n```\n\nIf you don't have GPUs available, just omit the `--gpus all` flag.\n\n#### Building your own Docker image\n\nFor various reasons you may need to create your own AllenNLP Docker image, such as if you need a different version\nof PyTorch. To do so, just run `make docker-image` from the root of your local clone of AllenNLP.\n\nBy default this builds an image with the tag `allennlp/allennlp`, but you can change this to anything you want\nby setting the `DOCKER_IMAGE_NAME` flag when you call `make`. For example,\n`make docker-image DOCKER_IMAGE_NAME=my-allennlp`.\n\nIf you want to use a different version of Python or PyTorch, set the flags `DOCKER_PYTHON_VERSION` and `DOCKER_TORCH_VERSION` to something like\n`3.9` and `1.9.0-cuda10.2`, respectively. These flags together determine the base image that is used. You can see the list of valid\ncombinations in this GitHub Container Registry: [github.com/allenai/docker-images/pkgs/container/pytorch](https://github.com/allenai/docker-images/pkgs/container/pytorch).\n\nAfter building the image you should be able to see it listed by running `docker images allennlp`.\n\n```\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nallennlp/allennlp   latest              b66aee6cb593        5 minutes ago       2.38GB\n```\n\n### Installing from source\n\nYou can also install AllenNLP by cloning our git repository:\n\n```bash\ngit clone https://github.com/allenai/allennlp.git\n```\n\nCreate a Python 3.7 or 3.8 virtual environment, and install AllenNLP in `editable` mode by running:\n\n```bash\npip install -U pip setuptools wheel\npip install --editable .[dev,all]\n```\n\nThis will make `allennlp` available on your system but it will use the sources from the local clone\nyou made of the source repository.\n\nYou can test your installation with `allennlp test-install`.\nSee [https://github.com/allenai/allennlp-models](https://github.com/allenai/allennlp-models)\nfor instructions on installing `allennlp-models` from source.\n\n## Running AllenNLP\n\nOnce you've installed AllenNLP, you can run the command-line interface\nwith the `allennlp` command (whether you installed from `pip` or from source).\n`allennlp` has various subcommands such as `train`, `evaluate`, and `predict`.\nTo see the full usage information, run `allennlp --help`.\n\nYou can test your installation by running  `allennlp test-install`.\n\n## Issues\n\nEveryone is welcome to file issues with either feature requests, bug reports, or general questions.  As a small team with our own internal goals, we may ask for contributions if a prompt fix doesn't fit into our roadmap.  To keep things tidy we will often close issues we think are answered, but don't hesitate to follow up if further discussion is needed.\n\n## Contributions\n\nThe AllenNLP team at AI2 ([@allenai](https://github.com/allenai)) welcomes contributions from the community. \nIf you're a first time contributor, we recommend you start by reading our [CONTRIBUTING.md](https://github.com/allenai/allennlp/blob/main/CONTRIBUTING.md) guide.\nThen have a look at our issues with the tag [**`Good First Issue`**](https://github.com/allenai/allennlp/issues?q=is%3Aissue+is%3Aopen+label%3A%22Good+First+Issue%22).\n\nIf you would like to contribute a larger feature, we recommend first creating an issue with a proposed design for discussion. This will prevent you from spending significant time on an implementation which has a technical limitation someone could have pointed out early on. Small contributions can be made directly in a pull request.\n\nPull requests (PRs) must have one approving review and no requested changes before they are merged.  As AllenNLP is primarily driven by AI2 we reserve the right to reject or revert contributions that we don't think are good additions.\n\n## Citing\n\nIf you use AllenNLP in your research, please cite [AllenNLP: A Deep Semantic Natural Language Processing Platform](https://www.semanticscholar.org/paper/AllenNLP%3A-A-Deep-Semantic-Natural-Language-Platform-Gardner-Grus/a5502187140cdd98d76ae711973dbcdaf1fef46d).\n\n```bibtex\n@inproceedings{Gardner2017AllenNLP,\n  title={AllenNLP: A Deep Semantic Natural Language Processing Platform},\n  author={Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord\n    and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and\n    Michael Schmitz and Luke S. Zettlemoyer},\n  year={2017},\n  Eprint = {arXiv:1803.07640},\n}\n```\n\n## Team\n\nAllenNLP is an open-source project backed by [the Allen Institute for Artificial Intelligence (AI2)](https://allenai.org/).\nAI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.\nTo learn more about who specifically contributed to this codebase, see [our contributors](https://github.com/allenai/allennlp/graphs/contributors) page.\n"
        },
        {
          "name": "RELEASE_PROCESS.md",
          "type": "blob",
          "size": 2.2890625,
          "content": "# AllenNLP GitHub and PyPI Release Process\n\nThis document describes the procedure for releasing new versions of the core library.\n\n>  This assumes you are using a clone of the main repo with the remote `origin` pointed\nto `git@github.com:allenai/allennlp.git` (or the `HTTPS` equivalent).\n\n## Steps\n\n1. Set the environment variable `TAG`, which should be of the form `v{VERSION}`.\n\n    For example, if the version of the release is `1.0.0`, you should set `TAG` to `v1.0.0`:\n\n    ```bash\n    export TAG='v1.0.0'\n    ```\n\n    Or if you use `fish`:\n\n    ```fish\n    set -x TAG 'v1.0.0'\n    ```\n\n2. Update `allennlp/version.py` with the correct version. Then check that the output of\n\n    ```\n    python scripts/get_version.py current\n    ```\n\n    matches the `TAG` environment variable.\n\n3. Update the `CHANGELOG.md` so that everything under the \"Unreleased\" section is now under a section corresponding to this release.\n\n4. Update the `CITATION.cff` file to refer to the right version.\n\n4. Commit and push these changes with:\n\n    ```\n    git commit -a -m \"Prepare for release $TAG\" && git push\n    ```\n    \n5. Then add the tag in git to mark the release:\n\n    ```\n    git tag $TAG -m \"Release $TAG\" && git push --tags\n    ```\n\n6. Find the tag you just pushed [on GitHub](https://github.com/allenai/allennlp/tags), click edit, then copy over the output of:\n\n    ```\n    python scripts/release_notes.py\n    ```\n\n    On a Mac, for example, you can just pipe the above command into `pbcopy`.\n\n7. Check the box \"This is a pre-release\" if the release is a release candidate (ending with `rc*`). Otherwise leave it unchecked.\n\n8. Click \"Publish Release\". GitHub Actions will then handle the rest, including publishing the package to PyPI the Docker image to Docker Hub.\n\n\n9. After the [GitHub Actions workflow](https://github.com/allenai/allennlp/actions?query=workflow%3AMaster+event%3Arelease) finishes, follow the same process to publish a release for the `allennlp-models` repo.\n\n\n## Fixing a failed release\n\nIf for some reason the GitHub Actions release workflow failed with an error that needs to be fixed, you'll have to delete both the tag and corresponding release from GitHub. After you've pushed a fix, delete the tag from your local clone with\n\n```bash\ngit tag -l | xargs git tag -d && git fetch -t\n```\n\nThen repeat the steps above.\n"
        },
        {
          "name": "STYLE.md",
          "type": "blob",
          "size": 4.015625,
          "content": "# AllenNLP Style Guide\n\nOur highest priority for code style is that our code be easily readable to someone new to the\ncodebase.  Deep learning is easy to get wrong, and we want our code to be easy enough to read that\nsomeone looking at it can be thinking about our modeling decisions, not trying to understand what\nis going on.\n\nTo that end, we use descriptive names, we use type annotations, and we write coherent docstrings.\nIn code that manipulates tensors, most lines that compute a tensor have a comment describing the\ntensor's shape.  When there's an interesting or important modeling decision in the code, we write\na comment about it (either in-line or in an appropriate docstring).\n\n## Docstrings\n\nAll reasonably complex public methods should have docstrings describing their basic function, their\ninputs and their outputs.  Private methods should also most often have docstrings, so that people\nwho read your code know what the method is supposed to do.  The basic outline we use for docstrings\nis: (1) a brief description of what the method does, sometimes also including how or why the method\ndoes it, (2) the parameters / arguments to the method, (3) the return value of the method, if any.\nIf the method is particularly simple or the arguments are obvious, (2) and (3) can be omitted. Our\ndocs use Markdown formatting, so function arguments and return values should be formatted as Markdown\nheaders (e.g `# Parameters`), seen in just about any model or module in the codebase.  We treat the\nclass docstring as the documentation for `__init__` methods, giving parameters there and omitting \nany docstring on the constructor itself.  For model / module constructors and methods like \n`forward`, _always_ include the parameters and return values (when there is one) in the docstring.\n\n## Code format\n\nWe use `flake8`, `black` and `mypy` to enforce some basic consistency in formatting.  Those\nformatting guidelines roughly follow [Google's python style\nguide](https://google.github.io/styleguide/pyguide.html#Python_Style_Rules), with a few notable\nexceptions.  In particular, because we use type annotations and descriptive variable names, we use\n100-character lines instead of 80-character lines, and it's ok to go over sometimes in code.\nAdditionally, we use `mkdocs` for building our docs, so Google's docstring formats don't apply.\n\n## Naming\n\nWe follow Google's [general naming\nrules](https://google.github.io/styleguide/cppguide.html#General_Naming_Rules), and their\n[definition of camel case](https://google.github.io/styleguide/javaguide.html#s5.3-camel-case).\n\n## Module layout and imports\n\nTo keep files from getting too big, we typically have one class per file, though small classes\nthat are inseparable from a companion class can also go in the same file (often these will be\nprivate classes).\n\nTo avoid verbosity when importing classes structured this way, classes should be imported from\ntheir module's `__init__.py`.  For example, the `Batch` class is in `allennlp/data/batch.py`,\nbut `allennlp/data/__init__.py` imports the class, so that you can just do `from allennlp.data\nimport Batch`.\n\nAbstract classes typically go in a module containing the abstract class and all built-in\nimplementations.  This includes things like `Field` (in `allennlp.data.fields`), `Seq2SeqEncoder`\n(in `allennlp.modules.seq2seq_encoders`), and many others.  In these cases, the abstract class\nshould be imported into the module _above_, so that you can do, e.g., `from allennlp.data import\nField`.  Concrete implementations follow the same layout as above: `from allennlp.data.fields\nimport TextField`.\n\nImports should be formatted at the top of the file, following [PEP 8's\nrecommendations](https://www.python.org/dev/peps/pep-0008/#imports): three sections (standard\nlibrary, third-party libraries, internal imports), each sorted and separated by a blank line.\n\n## Conclusion\n\nSome of the conventions we've adopted are arbitrary (e.g., other definitions of camel case are\nalso valid), but we stick to them to keep a consistent style throughout the codebase, which makes\nit easier to read and maintain.\n"
        },
        {
          "name": "allennlp",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.3837890625,
          "content": "coverage:\n  precision: 0\n  round: down\n  status:\n    patch:\n      default:\n        target: 90\n        informational: true\n    project:\n      default:\n        threshold: 1%\n        informational: true\n    changes: false\ncomment: false\nignore:\n  - \"tests/\"\ngithub_checks:\n  # TODO(epwalsh): re-enable if there is a way to only enable annotations\n  # on diff lines for a PR.\n  annotations: false\n"
        },
        {
          "name": "constraints.txt",
          "type": "blob",
          "size": 0.78515625,
          "content": "################################\n###### Core dependencies #######\n################################\ntorch<1.13.0\ntorchvision<0.14.0\ncached-path<1.2.0\nspacy<3.4\ntransformers<4.21\nfilelock<3.8\nwandb<0.13.0\n\n# Protobuf is a dependency of wandb and tensorboard, but they are missing this pin.\nprotobuf<4.0.0\n\n# Required so pip-compile can properly resolve the pydantic version\ninflect<6.0\n\n##################################################\n###### Extra dependencies for integrations #######\n##################################################\n# NOTE: we use a special trailing comment on each line to denote which extras\n# each package is needed by. For example, checklist is needed by the 'checklist' extra\n# that you install with 'pip install allennlp[checklist]'.\nchecklist==0.0.11  # needed by: checklist\n"
        },
        {
          "name": "dev-requirements.txt",
          "type": "blob",
          "size": 1.0029296875,
          "content": "#### TESTING-RELATED PACKAGES ####\n\n# Checks style, syntax, and other useful errors.\nflake8>=4.0.1\n\n# Static type checking\nmypy==0.961\n\n# Automatic code formatting\nblack==22.6.0\n\n# Allows generation of coverage reports with pytest.\npytest-cov>=3.0.0\n\n# Allows codecov to generate coverage reports\ncoverage[toml]>=6.4\ncodecov>=2.1.12\n\n# Optional dependencies, which we install for testing purposes.\nmatplotlib>=2.2.3\n\n# For mocking HTTP requests/responses.\nresponses>=0.21\n\n# For running tests that aren't 100% reliable.\nflaky>=3.7.0\n\n# For running benchmarks.\npytest-benchmark>=3.4.1\n\n#### DOC-RELATED PACKAGES ####\n\n# YAML manipulation\nruamel.yaml>=0.17.17\n\n# Generating markdown files from Python modules.\npydoc-markdown<4.4.0\ndatabind.core<=1.5.3\ndatabind-json<=1.5.3\ndocspec<1.2.0,>1.0.1\ndocspec-python<1.2.0,>1.0.1\n\nmkdocs==1.3.0\nmkdocs-material>=5.5.0,<8.4.0\nmarkdown-include==0.6.0\n\n# Narrowing constraints\npymdown-extensions>=9.5\n\n#### PACKAGE-UPLOAD PACKAGES ####\n\n# Pypi uploads\ntwine>=1.11.0,<5.0.0\nsetuptools\nwheel\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs-skeleton.yml",
          "type": "blob",
          "size": 0.9150390625,
          "content": "site_name: AllenNLP\nsite_description: AllenNLP is a ..\nsite_url: https://allennlp.org/\n\nextra_css:\n  - \"css/extra.css\"\n\ntheme:\n  name: material\n  palette:\n    primary: blue\n    accent: grey\n  logo: img/favicon.ico\n  favicon: img/favicon.ico\n  highlightjs: true\n  hljs_languages:\n  - python\n  - typescript\n  - json\n\n\nrepo_name: allenai/allennlp\n# TODO(markn): Consider adding GA here, if we care about it.\n\nnav:\n- Home: README.md\n- Repository: https://github.com/allenai/allennlp\n- Versions:\n  - Latest: /latest/\n  - Stable: /stable/\n  - Commit: /main/\n- API: 'This section is autogenerated, do not edit.'\n- Contributing: CONTRIBUTING.md\n- CHANGELOG: CHANGELOG.md\n- License: https://raw.githubusercontent.com/allenai/allennlp/main/LICENSE\n\nmarkdown_extensions:\n- toc:\n    permalink: true\n    toc_depth: 3\n- markdown.extensions.codehilite:\n    guess_lang: true\n- admonition\n- codehilite\n- extra\n- pymdownx.highlight\n- pymdownx.superfences\n"
        },
        {
          "name": "mypy.ini",
          "type": "blob",
          "size": 0.0986328125,
          "content": "[mypy]\nignore_missing_imports = true\nno_site_packages = true\n\n[mypy-tests.*]\nstrict_optional = false\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.3212890625,
          "content": "[tool.black]\nline-length = 100\n\ninclude = '\\.pyi?$'\n\nexclude = '''\n(\n      __pycache__\n    | \\btutorials\\b\n    | \\bbuild\\b\n    | \\.git\n    | \\.mypy_cache\n    | \\.pytest_cache\n    | \\.vscode\n    | \\.venv\n    | \\bdist\\b\n    | \\bdoc\\b\n)\n'''\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 1.685546875,
          "content": "[pytest]\ntestpaths = tests/ scripts/tests/\npython_classes = Test* *Test\nlog_format = %(asctime)s - %(levelname)s - %(name)s - %(message)s\nlog_level = DEBUG\nmarkers =\n    java\n    gpu: marks tests that need at least one GPU\nfilterwarnings =\n# Note: When a warning matches more than one option in the list,\n# the action for the _last_ matching option is performed.\n#\n# individual warnings filters are specified as a sequence of fields separated by colons:\n# action:message:category:module:line\n# <reference: https://docs.python.org/3/library/warnings.html#warning-filter>\n#\n# how to explicitly test warns\n#  using `unittest`: https://docs.python.org/3/library/warnings.html#testing-warnings\n#  using `pytest`: https://docs.pytest.org/en/4.1.0/warnings.html#assertwarnings\n#\n# Our policy here is to ignore (silence) any deprecation warnings from _outside_ allennlp, but to\n# treat any _internal_ deprecation warnings as errors.  If we get a deprecation warning from things\n# we call in another library, we will just rely on seeing those outside of tests.  The purpose of\n# having these errors here is to make sure that we do not deprecate things lightly in allennlp.\n    ignore::DeprecationWarning\n    ignore::PendingDeprecationWarning\n    error::DeprecationWarning:allennlp.*:\n    error::PendingDeprecationWarning:allennlp.*:\n# For these particular warnings, we don't want to cause an error for it, but we also don't want to\n# see it a whole bunch of times.\n    once:This particular transformer implementation is a provisional feature.*::allennlp\\.modules\\.seq2seq_encoders\\.bidirectional_language_model_transformer\n\tignore:Length of IterableDataset.*:UserWarning:torch\\.utils\\.data\\.dataloader\n\tignore::UserWarning:allennlp.*:\n"
        },
        {
          "name": "requirements.in",
          "type": "blob",
          "size": 0.0615234375,
          "content": "-c constraints.txt\n-r dev-requirements.txt\n-r requirements.txt\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 1.3349609375,
          "content": "################################\n###### Core dependencies #######\n################################\ntorch>=1.10.0\ntorchvision>=0.8.1\ncached-path>=1.1.3\nfairscale==0.4.6\njsonnet>=0.10.0 ; sys.platform != 'win32'\nnltk>=3.6.5\nspacy>=2.1.0\nnumpy>=1.21.4\ntensorboardX>=1.2\nrequests>=2.28\ntqdm>=4.62\nh5py>=3.6.0\nscikit-learn>=1.0.1\nscipy>=1.7.3\npytest>=6.2.5\ntransformers>=4.1\nsentencepiece>=0.1.96\ndataclasses;python_version<'3.7'\nfilelock>=3.3\nlmdb>=1.2.1\nmore-itertools>=8.12.0\ntermcolor==1.1.0\nwandb>=0.10.0\nhuggingface_hub>=0.0.16\ndill>=0.3.4\nbase58>=2.1.1\n\n# sacremoses should be a dependency of transformers, but it is missing, so we add it manually.\nsacremoses\n\n# Spacy depends on typer, and typer had a bug. This is how we make sure we get the fixed version of typer.\ntyper>=0.4.1\n\n# Protobuf is a dependency of wandb and tensorboard, but they are missing this pin.\nprotobuf>=3.12.0\n\n# We need this for building the Docker image\ntraitlets>5.1.1\n\n##################################################\n###### Extra dependencies for integrations #######\n##################################################\n# NOTE: we use a special trailing comment on each line to denote which extras\n# each package is needed by. For example, checklist is needed by the 'checklist' extra\n# that you install with 'pip install allennlp[checklist]'.\nchecklist>=0.0.11  # needed by: checklist\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.587890625,
          "content": "from collections import defaultdict\nfrom setuptools import find_packages, setup\n\n# PEP0440 compatible formatted version, see:\n# https://www.python.org/dev/peps/pep-0440/\n#\n# release markers:\n#   X.Y\n#   X.Y.Z   # For bugfix releases\n#\n# pre-release markers:\n#   X.YaN   # Alpha release\n#   X.YbN   # Beta release\n#   X.YrcN  # Release Candidate\n#   X.Y     # Final release\n\n\ndef parse_requirements_file(path, allowed_extras: set = None, include_all_extra: bool = True):\n    requirements = []\n    extras = defaultdict(list)\n    with open(path) as requirements_file:\n        import re\n\n        def fix_url_dependencies(req: str) -> str:\n            \"\"\"Pip and setuptools disagree about how URL dependencies should be handled.\"\"\"\n            m = re.match(\n                r\"^(git\\+)?(https|ssh)://(git@)?github\\.com/([\\w-]+)/(?P<name>[\\w-]+)\\.git\", req\n            )\n            if m is None:\n                return req\n            else:\n                return f\"{m.group('name')} @ {req}\"\n\n        for line in requirements_file:\n            line = line.strip()\n            if line.startswith(\"#\") or len(line) <= 0:\n                continue\n            req, *needed_by = line.split(\"# needed by:\")\n            req = fix_url_dependencies(req.strip())\n            if needed_by:\n                for extra in needed_by[0].strip().split(\",\"):\n                    extra = extra.strip()\n                    if allowed_extras is not None and extra not in allowed_extras:\n                        raise ValueError(f\"invalid extra '{extra}' in {path}\")\n                    extras[extra].append(req)\n                if include_all_extra and req not in extras[\"all\"]:\n                    extras[\"all\"].append(req)\n            else:\n                requirements.append(req)\n    return requirements, extras\n\n\nintegrations = {\"checklist\"}\n\n# Load requirements.\ninstall_requirements, extras = parse_requirements_file(\n    \"requirements.txt\", allowed_extras=integrations\n)\ndev_requirements, dev_extras = parse_requirements_file(\n    \"dev-requirements.txt\", allowed_extras={\"examples\"}, include_all_extra=False\n)\nextras[\"dev\"] = dev_requirements\nextras.update(dev_extras)\n\n# version.py defines the VERSION and VERSION_SHORT variables.\n# We use exec here so we don't import allennlp whilst setting up.\nVERSION = {}  # type: ignore\nwith open(\"allennlp/version.py\", \"r\") as version_file:\n    exec(version_file.read(), VERSION)\n\nsetup(\n    name=\"allennlp\",\n    version=VERSION[\"VERSION\"],\n    description=\"An open-source NLP research library, built on PyTorch.\",\n    long_description=open(\"README.md\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    classifiers=[\n        \"Intended Audience :: Science/Research\",\n        \"Development Status :: 3 - Alpha\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python :: 3\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    keywords=\"allennlp NLP deep learning machine reading\",\n    url=\"https://github.com/allenai/allennlp\",\n    author=\"Allen Institute for Artificial Intelligence\",\n    author_email=\"allennlp@allenai.org\",\n    license=\"Apache\",\n    packages=find_packages(\n        exclude=[\n            \"*.tests\",\n            \"*.tests.*\",\n            \"tests.*\",\n            \"tests\",\n            \"test_fixtures\",\n            \"test_fixtures.*\",\n            \"benchmarks\",\n            \"benchmarks.*\",\n        ]\n    ),\n    install_requires=install_requirements,\n    extras_require=extras,\n    entry_points={\"console_scripts\": [\"allennlp=allennlp.__main__:run\"]},\n    include_package_data=True,\n    python_requires=\">=3.7.1\",\n    zip_safe=False,\n)\n"
        },
        {
          "name": "test_fixtures",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}