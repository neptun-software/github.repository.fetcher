{
  "metadata": {
    "timestamp": 1736561166354,
    "page": 123,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "w-okada/voice-changer",
      "stars": 16906,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.4736328125,
          "content": "tmp\ndummy\nnode_modules\n__pycache__\n\nserver/upload_dir/\nserver/MMVC_Client_v13/\nserver/MMVC_Client_v15/\nserver/so-vits-svc-40/\nserver/so-vits-svc-40v2/\nserver/DDSP-SVC/\nserver/RVC/\nserver/keys\nserver/info\nserver/in.wav\nserver/out.wav\nserver/G_*.pth\nserver/train_config.json\nserver/stored_setting.json\n# v.1.3.xテスト用モデルフォルダ\nserver/v13\n\nserver/model_hubert\nserver/model_so-vits-svc-40v2_tsukuyomi/\nserver/model_so-vits-svc-40v2_amitaro/\nserver/model_so-vits-svc-40/\nserver/model_so-vits-svc-40_mahiro/\nserver/model_so-vits-svc-40_amitaro/\nserver/model_so-vits-svc-40_tsukuyomi/\nserver/model_so-vits-svc-40_kikotokurage\nmodel_DDSP-SVC/\nserver/model_sovits\nserver/test \n\nserver/memo.md\n\nclient/lib/dist\nclient/lib/worklet/dist\nclient/demo/public/models\nclient/demo/public/models_\nclient/demo/dist/models\nclient/demo/dist_web\nclient/demo/src/001_provider/backup\n# client/demo/dist/ # demo用に残す\n\ndocker/cudnn/\n\nserver/pretrain/\nserver/weights/\nserver/model_dir/\nserver/model_dir2/\nserver/weights_/\nserver/weights__/\nserver/models/\nserver/samples.json\nserver/samples_0003_t.json\nserver/samples_0003_o.json\nserver/samples_0003_t2.json\nserver/samples_0003_o2.json\nserver/samples_0003_d2.json\nserver/samples_0004_t.json\nserver/samples_0004_o.json\nserver/samples_0004_d.json\n\nserver/test_official_v1_v2.json\nserver/test_ddpn_v1_v2.json\nserver/vcclient.log\nstart_trainer.sh\n\n# venv\nvenv/\n\n\nbeatrice_internal_api.cp310-win_amd64.pyd\n108_average_110b_10.bin\n\nserver/model_dir_static/Beatrice-JVS\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "Hina_Mod_Kaggle_Real_Time_Voice_Changer.ipynb",
          "type": "blob",
          "size": 7.587890625,
          "content": "{\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"},\"kaggle\":{\"accelerator\":\"gpu\",\"dataSources\":[],\"dockerImageVersionId\":30559,\"isInternetEnabled\":true,\"language\":\"python\",\"sourceType\":\"notebook\",\"isGpuEnabled\":true}},\"nbformat_minor\":4,\"nbformat\":4,\"cells\":[{\"source\":\"<a href=\\\"https://www.kaggle.com/code/hinabl/public-w-okada-voice-changer?scriptVersionId=151765879\\\" target=\\\"_blank\\\"><img align=\\\"left\\\" alt=\\\"Kaggle\\\" title=\\\"Open in Kaggle\\\" src=\\\"https://kaggle.com/static/images/open-in-kaggle.svg\\\"></a>\",\"metadata\":{},\"cell_type\":\"markdown\"},{\"cell_type\":\"markdown\",\"source\":\"### [w-okada's Voice Changer](https://github.com/w-okada/voice-changer) | **Kaggle**\\n\\n---\\n\\n## **⬇ VERY IMPORTANT ⬇**\\n\\nYou can use the following settings for better results:\\n\\nIf you're using a index: `f0: RMVPE_ONNX | Chunk: 112 or higher | Extra: 8192`<br>\\nIf you're not using a index: `f0: RMVPE_ONNX | Chunk: 96 or higher | Extra: 16384`<br>\\n**Don't forget to select a GPU in the GPU field, <b>NEVER</b> use CPU!\\n> Seems that PTH models performance better than ONNX for now, you can still try ONNX models and see if it satisfies you\\n\\n\\n*You can always [click here](https://github.com/YunaOneeChan/Voice-Changer-Settings) to check if these settings are up-to-date*\\n\\n---\\n**Credits**<br>\\nRealtime Voice Changer by [w-okada](https://github.com/w-okada)<br>\\nNotebook files updated by [rafacasari](https://github.com/Rafacasari)<br>\\nRecommended settings by [Raven](https://github.com/RavenCutie21)<br>\\nModded again by [Hina](https://github.com/hinabl)\\n\\n**Need help?** [AI Hub Discord](https://discord.gg/aihub) » ***#help-realtime-vc***\\n\\n---\",\"metadata\":{\"id\":\"Lbbmx_Vjl0zo\"}},{\"cell_type\":\"markdown\",\"source\":\"# Kaggle Tutorial\\nRunning this notebook can be a bit complicated.\\\\\\nAfter created your Kaggle account, you'll need to **verify your phone number** to be able to use Internet Connection and GPUs.\\\\\\nFollow the instructions on the image below.\\n\\n## <font color=blue>*You can use <b>GPU P100</b> instead of GPU T4, some people are telling that <b>P100 is better</b>.*</font>\\n![instructions.png](https://i.imgur.com/0NutkD8.png)\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"# Clone repository and install dependencies\\nThis first step will download the latest version of Voice Changer and install the dependencies. **It will take some time to complete.**\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"# This will make that we're on the right folder before installing\\n%cd /kaggle/working/\\n\\n!pip install colorama --quiet\\nfrom colorama import Fore, Style\\nimport os\\n\\n!mkdir Hmod\\n%cd Hmod\\n!git clone https://github.com/w-okada/voice-changer.git --depth=1 --quiet .\\nprint(f\\\"{Fore.GREEN}> Successfully cloned the repository!{Style.RESET_ALL}\\\")\\n%cd server\\n!sed -i \\\"s/-.-.-.-/Kaggle.Mod/\\\" '../client/demo/dist/assets/gui_settings/version.txt'\\n!mv MMVCServerSIO.py Hmod.py\\n!sed -i \\\"s/MMVCServerSIO/Hmod/\\\" Hmod.py\\n\\nprint(f\\\"{Fore.CYAN}> Installing libportaudio2...{Style.RESET_ALL}\\\")\\n!apt-get -y install libportaudio2 -qq\\n\\nprint(f\\\"{Fore.CYAN}> Installing pre-dependencies...{Style.RESET_ALL}\\\")\\n# Install dependencies that are missing from requirements.txt and pyngrok\\n!pip install faiss-gpu fairseq pyngrok --quiet \\n!pip install pyworld --no-build-isolation\\nprint(f\\\"{Fore.CYAN}> Installing dependencies from requirements.txt...{Style.RESET_ALL}\\\")\\n!pip install -r requirements.txt --quiet\\n\\n# Download the default settings ^-^\\nif not os.path.exists(\\\"/kaggle/working/Hmod/server/stored_setting.json\\\"):\\n    !wget -q https://gist.githubusercontent.com/Rafacasari/d820d945497a01112e1a9ba331cbad4f/raw/8e0a426c22688b05dd9c541648bceab27e422dd6/kaggle_setting.json -O /kaggle/working/24apuiBokE3TjZwc6tuqqv39SwP_2LRouVj3M9oZZCbzgntuG /server/stored_setting.json\\nprint(f\\\"{Fore.GREEN}> Successfully installed all packages!{Style.RESET_ALL}\\\")\\n\\nprint(f\\\"{Fore.GREEN}> You can safely ignore the dependency conflict errors, it's a error from Kaggle and don't interfer on Voice Changer!{Style.RESET_ALL}\\\")\",\"metadata\":{\"id\":\"86wTFmqsNMnD\",\"cellView\":\"form\",\"_kg_hide-output\":false,\"execution\":{\"iopub.status.busy\":\"2023-11-13T14:29:34.68815Z\",\"iopub.execute_input\":\"2023-11-13T14:29:34.688434Z\",\"iopub.status.idle\":\"2023-11-13T14:35:25.010808Z\",\"shell.execute_reply.started\":\"2023-11-13T14:29:34.688408Z\",\"shell.execute_reply\":\"2023-11-13T14:35:25.009639Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# Start Server **using ngrok**\\nThis cell will start the server, the first time that you run it will download the models, so it can take a while (~1-2 minutes)\\n\\n---\\nYou'll need a ngrok account, but <font color=green>**it's free**</font> and easy to create!\\n---\\n**1** - Create a **free** account at [ngrok](https://dashboard.ngrok.com/signup)\\\\\\n**2** - If you didn't logged in with Google or Github, you will need to **verify your e-mail**!\\\\\\n**3** - Click [this link](https://dashboard.ngrok.com/get-started/your-authtoken) to get your auth token, and replace **YOUR_TOKEN_HERE** with your token.\\\\\\n**4** - *(optional)* Change to a region near to you\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"Token = 'Token_Here'\\nRegion = \\\"ap\\\" # Read the instructions below\\n\\n# You can change the region for a better latency, use only the abbreviation\\n# Choose between this options: \\n# us -> United States (Ohio)\\n# ap -> Asia/Pacific (Singapore)\\n# au -> Australia (Sydney)\\n# eu -> Europe (Frankfurt)\\n# in -> India (Mumbai)\\n# jp -> Japan (Tokyo)\\n# sa -> South America (Sao Paulo)\\n\\n# ---------------------------------\\n# DO NOT TOUCH ANYTHING DOWN BELOW!\\n\\n%cd /kaggle/working/Hmod/server\\n    \\nfrom pyngrok import conf, ngrok\\nMyConfig = conf.PyngrokConfig()\\nMyConfig.auth_token = Token\\nMyConfig.region = Region\\nconf.get_default().authtoken = Token\\nconf.get_default().region = Region\\nconf.set_default(MyConfig);\\n\\nimport subprocess, threading, time, socket, urllib.request\\nPORT = 8000\\n\\nfrom pyngrok import ngrok\\nngrokConnection = ngrok.connect(PORT)\\npublic_url = ngrokConnection.public_url\\n\\ndef wait_for_server():\\n    while True:\\n        time.sleep(0.5)\\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n        result = sock.connect_ex(('127.0.0.1', PORT))\\n        if result == 0:\\n            break\\n        sock.close()\\n    print(\\\"--------- SERVER READY! ---------\\\")\\n    print(\\\"Your server is available at:\\\")\\n    print(public_url)\\n    print(\\\"---------------------------------\\\")\\n\\nthreading.Thread(target=wait_for_server, daemon=True).start()\\n\\n!python3 Hmod.py \\\\\\n  -p {PORT} \\\\\\n  --https False \\\\\\n  --content_vec_500 pretrain/checkpoint_best_legacy_500.pt \\\\\\n  --content_vec_500_onnx pretrain/content_vec_500.onnx \\\\\\n  --content_vec_500_onnx_on true \\\\\\n  --hubert_base pretrain/hubert_base.pt \\\\\\n  --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\\\\n  --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\\\\n  --nsf_hifigan pretrain/nsf_hifigan/model \\\\\\n  --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\\\\n  --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\\\\n  --rmvpe pretrain/rmvpe.pt \\\\\\n  --model_dir model_dir \\\\\\n  --samples samples.json\\n\\nngrok.disconnect(ngrokConnection.public_url)\",\"metadata\":{\"id\":\"lLWQuUd7WW9U\",\"cellView\":\"form\",\"_kg_hide-input\":false,\"scrolled\":true,\"execution\":{\"iopub.status.busy\":\"2023-11-13T14:36:20.529333Z\",\"iopub.execute_input\":\"2023-11-13T14:36:20.530081Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]}]}"
        },
        {
          "name": "Hina_Modified_Realtime_Voice_Changer_on_Colab.ipynb",
          "type": "blob",
          "size": 16.220703125,
          "content": "{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"view-in-github\",\n        \"colab_type\": \"text\"\n      },\n      \"source\": [\n        \"<a href=\\\"https://colab.research.google.com/github/hinabl/voice-changer-colab/blob/master/Hina_Modified_Realtime_Voice_Changer_on_Colab.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"Lbbmx_Vjl0zo\"\n      },\n      \"source\": [\n        \"### w-okada's Voice Changer | **Google Colab**\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"##**READ ME - VERY IMPORTANT**\\n\",\n        \"\\n\",\n        \"This is an attempt to run [Realtime Voice Changer](https://github.com/w-okada/voice-changer) on Google Colab, still not perfect but is totally usable, you can use the following settings for better results:\\n\",\n        \"\\n\",\n        \"If you're using a index: `f0: RMVPE_ONNX | Chunk: 112 or higher | Extra: 8192`\\\\\\n\",\n        \"If you're not using a index: `f0: RMVPE_ONNX | Chunk: 96 or higher | Extra: 16384`\\\\\\n\",\n        \"**Don't forget to select your Colab GPU in the GPU field (<b>Tesla T4</b>, for free users)*\\n\",\n        \"> Seems that PTH models performance better than ONNX for now, you can still try ONNX models and see if it satisfies you\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"*You can always [click here](https://rentry.co/VoiceChangerGuide#gpu-chart-for-known-working-chunkextra\\n\",\n        \") to check if these settings are up-to-date*\\n\",\n        \"<br><br>\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"###Always use Colab GPU (**VERY VERY VERY IMPORTANT!**)\\n\",\n        \"You need to use a Colab GPU so the Voice Changer can work faster and better\\\\\\n\",\n        \"Use the menu above and click on **Runtime** » **Change runtime** » **Hardware acceleration** to select a GPU (**T4 is the free one**)\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"<br>\\n\",\n        \"\\n\",\n        \"# **Credits and Support**\\n\",\n        \"Realtime Voice Changer by [w-okada](https://github.com/w-okada)\\\\\\n\",\n        \"Colab files updated by [rafacasari](https://github.com/Rafacasari)\\\\\\n\",\n        \"Recommended settings by [Raven](https://github.com/ravencutie21)\\\\\\n\",\n        \"Modified again by [Hina](https://huggingface.co/HinaBl)\\n\",\n        \"\\n\",\n        \"Need help? [AI Hub Discord](https://discord.gg/aihub) » ***#help-realtime-vc***\\n\",\n        \"\\n\",\n        \"---\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"86wTFmqsNMnD\",\n        \"cellView\": \"form\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"#=================Updated=================\\n\",\n        \"# @title **[1]** Clone repository and install dependencies\\n\",\n        \"# @markdown This first step will download the latest version of Voice Changer and install the dependencies. **It can take some time to complete.**\\n\",\n        \"import os\\n\",\n        \"import time\\n\",\n        \"import subprocess\\n\",\n        \"import threading\\n\",\n        \"import shutil\\n\",\n        \"import base64\\n\",\n        \"import codecs\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"#@markdown ---\\n\",\n        \"# @title **[Optional]** Connect to Google Drive\\n\",\n        \"# @markdown Using Google Drive can improve load times a bit and your models will be stored, so you don't need to re-upload every time that you use.\\n\",\n        \"\\n\",\n        \"Use_Drive=False #@param {type:\\\"boolean\\\"}\\n\",\n        \"\\n\",\n        \"from google.colab import drive\\n\",\n        \"\\n\",\n        \"if Use_Drive==True:\\n\",\n        \"  if not os.path.exists('/content/drive'):\\n\",\n        \"    drive.mount('/content/drive')\\n\",\n        \"\\n\",\n        \"  %cd /content/drive/MyDrive\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"externalgit=codecs.decode('uggcf://tvguho.pbz/j-bxnqn/ibvpr-punatre.tvg','rot_13')\\n\",\n        \"rvctimer=codecs.decode('uggcf://tvguho.pbz/uvanoy/eipgvzre.tvg','rot_13')\\n\",\n        \"pathloc=codecs.decode('ibvpr-punatre','rot_13')\\n\",\n        \"\\n\",\n        \"from IPython.display import clear_output, Javascript\\n\",\n        \"\\n\",\n        \"def update_timer_and_print():\\n\",\n        \"    global timer\\n\",\n        \"    while True:\\n\",\n        \"        hours, remainder = divmod(timer, 3600)\\n\",\n        \"        minutes, seconds = divmod(remainder, 60)\\n\",\n        \"        timer_str = f'{hours:02}:{minutes:02}:{seconds:02}'\\n\",\n        \"        print(f'\\\\rTimer: {timer_str}', end='', flush=True)  # Print without a newline\\n\",\n        \"        time.sleep(1)\\n\",\n        \"        timer += 1\\n\",\n        \"timer = 0\\n\",\n        \"threading.Thread(target=update_timer_and_print, daemon=True).start()\\n\",\n        \"\\n\",\n        \"!pip install colorama --quiet\\n\",\n        \"from colorama import Fore, Style\\n\",\n        \"\\n\",\n        \"print(f\\\"{Fore.CYAN}> Cloning the repository...{Style.RESET_ALL}\\\")\\n\",\n        \"!git clone --depth 1 $externalgit &> /dev/null\\n\",\n        \"print(f\\\"{Fore.GREEN}> Successfully cloned the repository!{Style.RESET_ALL}\\\")\\n\",\n        \"%cd $pathloc/server/\\n\",\n        \"\\n\",\n        \"# Read the content of the file\\n\",\n        \"file_path = '../client/demo/dist/assets/gui_settings/version.txt'\\n\",\n        \"\\n\",\n        \"with open(file_path, 'r') as file:\\n\",\n        \"    file_content = file.read()\\n\",\n        \"\\n\",\n        \"# Replace the specific text\\n\",\n        \"text_to_replace = \\\"-.-.-.-\\\"\\n\",\n        \"new_text = \\\"Google.Colab\\\"  # New text to replace the specific text\\n\",\n        \"\\n\",\n        \"modified_content = file_content.replace(text_to_replace, new_text)\\n\",\n        \"\\n\",\n        \"# Write the modified content back to the file\\n\",\n        \"with open(file_path, 'w') as file:\\n\",\n        \"    file.write(modified_content)\\n\",\n        \"\\n\",\n        \"print(f\\\"Text '{text_to_replace}' has been replaced with '{new_text}' in the file.\\\")\\n\",\n        \"\\n\",\n        \"print(f\\\"{Fore.CYAN}> Installing libportaudio2...{Style.RESET_ALL}\\\")\\n\",\n        \"!apt-get -y install libportaudio2 -qq\\n\",\n        \"\\n\",\n        \"!sed -i '/torch==/d' requirements.txt\\n\",\n        \"!sed -i '/torchaudio==/d' requirements.txt\\n\",\n        \"!sed -i '/numpy==/d' requirements.txt\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"print(f\\\"{Fore.CYAN}> Installing pre-dependencies...{Style.RESET_ALL}\\\")\\n\",\n        \"# Install dependencies that are missing from requirements.txt and pyngrok\\n\",\n        \"!pip install faiss-gpu fairseq pyngrok --quiet\\n\",\n        \"!pip install pyworld --no-build-isolation --quiet\\n\",\n        \"# Install webstuff\\n\",\n        \"import asyncio\\n\",\n        \"import re\\n\",\n        \"!pip install playwright\\n\",\n        \"!playwright install\\n\",\n        \"!playwright install-deps\\n\",\n        \"!pip install nest_asyncio\\n\",\n        \"from playwright.async_api import async_playwright\\n\",\n        \"print(f\\\"{Fore.CYAN}> Installing dependencies from requirements.txt...{Style.RESET_ALL}\\\")\\n\",\n        \"!pip install -r requirements.txt --quiet\\n\",\n        \"clear_output()\\n\",\n        \"print(f\\\"{Fore.GREEN}> Successfully installed all packages!{Style.RESET_ALL}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"#@title **[Optional]** Upload a voice model (Run this before running the Voice Changer)\\n\",\n        \"import os\\n\",\n        \"import json\\n\",\n        \"from IPython.display import Image\\n\",\n        \"import requests\\n\",\n        \"\\n\",\n        \"model_slot = \\\"0\\\" #@param ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199']\\n\",\n        \"\\n\",\n        \"!rm -rf model_dir/$model_slot\\n\",\n        \"#@markdown **[Optional]** Add an icon to the model\\n\",\n        \"icon_link = \\\"https://cdn.donmai.us/sample/12/57/__rin_penrose_idol_corp_drawn_by_juu_ame__sample-12579843de9487cf2db82058ba5e77d4.jpg\\\" #@param {type:\\\"string\\\"}\\n\",\n        \"icon_link = '\\\"'+icon_link+'\\\"'\\n\",\n        \"!mkdir model_dir\\n\",\n        \"!mkdir model_dir/$model_slot\\n\",\n        \"#@markdown Put your model's download link here `(must be a zip file)` only supports **weights.gg** & **huggingface.co**\\n\",\n        \"model_link = \\\"https://huggingface.co/HinaBl/Rin-Penrose/resolve/main/RinPenrose600.zip?download=true\\\"  #@param {type:\\\"string\\\"}\\n\",\n        \"\\n\",\n        \"if model_link.startswith(\\\"https://www.weights.gg\\\") or model_link.startswith(\\\"https://weights.gg\\\"):\\n\",\n        \"  weights_code = requests.get(\\\"https://pastebin.com/raw/ytHLr8h0\\\").text\\n\",\n        \"  exec(weights_code)\\n\",\n        \"else:\\n\",\n        \"  model_link = model_link\\n\",\n        \"\\n\",\n        \"model_link = '\\\"'+model_link+'\\\"'\\n\",\n        \"!curl -L $model_link > model.zip\\n\",\n        \"\\n\",\n        \"# Conditionally set the iconFile based on whether icon_link is empty\\n\",\n        \"if icon_link:\\n\",\n        \"    iconFile = \\\"icon.png\\\"\\n\",\n        \"    !curl -L $icon_link > model_dir/$model_slot/icon.png\\n\",\n        \"else:\\n\",\n        \"    iconFile = \\\"\\\"\\n\",\n        \"    print(\\\"icon_link is empty, so no icon file will be downloaded.\\\")\\n\",\n        \"\\n\",\n        \"!unzip model.zip -d model_dir/$model_slot\\n\",\n        \"\\n\",\n        \"!mv model_dir/$model_slot/*/* model_dir/$model_slot/\\n\",\n        \"!rm -rf model_dir/$model_slot/*/\\n\",\n        \"#@markdown **Model Voice Convertion Setting**\\n\",\n        \"Tune = 12 #@param {type:\\\"slider\\\",min:-50,max:50,step:1}\\n\",\n        \"Index = 0 #@param {type:\\\"slider\\\",min:0,max:1,step:0.1}\\n\",\n        \"\\n\",\n        \"param_link = \\\"\\\"\\n\",\n        \"if param_link == \\\"\\\":\\n\",\n        \"  paramset = requests.get(\\\"https://pastebin.com/raw/SAKwUCt1\\\").text\\n\",\n        \"  exec(paramset)\\n\",\n        \"\\n\",\n        \"clear_output()\\n\",\n        \"print(\\\"\\\\033[93mModel with the name of \\\"+model_name+\\\" has been Imported to slot \\\"+model_slot)\"\n      ],\n      \"metadata\": {\n        \"id\": \"_ZtbKUVUgN3G\",\n        \"cellView\": \"form\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"lLWQuUd7WW9U\",\n        \"cellView\": \"form\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"\\n\",\n        \"#=======================Updated=========================\\n\",\n        \"\\n\",\n        \"# @title Start Server **using ngrok**\\n\",\n        \"# @markdown This cell will start the server, the first time that you run it will download the models, so it can take a while (~1-2 minutes)\\n\",\n        \"\\n\",\n        \"# @markdown ---\\n\",\n        \"# @markdown You'll need a ngrok account, but <font color=green>**it's free**</font> and easy to create!\\n\",\n        \"# @markdown ---\\n\",\n        \"# @markdown **1** - Create a <font color=green>**free**</font> account at [ngrok](https://dashboard.ngrok.com/signup) or **login with Google/Github account**\\\\\\n\",\n        \"# @markdown **2** - If you didn't logged in with Google/Github, you will need to **verify your e-mail**!\\\\\\n\",\n        \"# @markdown **3** - Click [this link](https://dashboard.ngrok.com/get-started/your-authtoken) to get your auth token, and place it here:\\n\",\n        \"Token = 'TOKEN_HERE' # @param {type:\\\"string\\\"}\\n\",\n        \"# @markdown **4** - *(optional)* Change to a region near to you or keep at United States if increase latency\\\\\\n\",\n        \"# @markdown `Default Region: us - United States (Ohio)`\\n\",\n        \"Region = \\\"us - United States (Ohio)\\\" # @param [\\\"ap - Asia/Pacific (Singapore)\\\", \\\"au - Australia (Sydney)\\\",\\\"eu - Europe (Frankfurt)\\\", \\\"in - India (Mumbai)\\\",\\\"jp - Japan (Tokyo)\\\",\\\"sa - South America (Sao Paulo)\\\", \\\"us - United States (Ohio)\\\"]\\n\",\n        \"\\n\",\n        \"#@markdown **5** - *(optional)* Other options:\\n\",\n        \"ClearConsole = True  # @param {type:\\\"boolean\\\"}\\n\",\n        \"Play_Notification = True  # @param {type:\\\"boolean\\\"}\\n\",\n        \"\\n\",\n        \"# ---------------------------------\\n\",\n        \"# DO NOT TOUCH ANYTHING DOWN BELOW!\\n\",\n        \"# ---------------------------------\\n\",\n        \"\\n\",\n        \"%cd $pathloc/server/\\n\",\n        \"\\n\",\n        \"from pyngrok import conf, ngrok\\n\",\n        \"MyConfig = conf.PyngrokConfig()\\n\",\n        \"MyConfig.auth_token = Token\\n\",\n        \"MyConfig.region = Region[0:2]\\n\",\n        \"#conf.get_default().authtoken = Token\\n\",\n        \"#conf.get_default().region = Region\\n\",\n        \"conf.set_default(MyConfig);\\n\",\n        \"\\n\",\n        \"import subprocess, threading, time, socket, urllib.request\\n\",\n        \"PORT = 8000\\n\",\n        \"\\n\",\n        \"from pyngrok import ngrok\\n\",\n        \"ngrokConnection = ngrok.connect(PORT)\\n\",\n        \"public_url = ngrokConnection.public_url\\n\",\n        \"\\n\",\n        \"from IPython.display import clear_output\\n\",\n        \"from IPython.display import Audio, display\\n\",\n        \"def play_notification_sound():\\n\",\n        \"    display(Audio(url='https://raw.githubusercontent.com/hinabl/rmvpe-ai-kaggle/main/custom/audios/notif.mp3', autoplay=True))\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"def wait_for_server():\\n\",\n        \"    while True:\\n\",\n        \"        time.sleep(0.5)\\n\",\n        \"        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n\",\n        \"        result = sock.connect_ex(('127.0.0.1', PORT))\\n\",\n        \"        if result == 0:\\n\",\n        \"            break\\n\",\n        \"        sock.close()\\n\",\n        \"    if ClearConsole:\\n\",\n        \"        clear_output()\\n\",\n        \"    print(\\\"--------- SERVER READY! ---------\\\")\\n\",\n        \"    print(\\\"Your server is available at:\\\")\\n\",\n        \"    print(public_url)\\n\",\n        \"    print(\\\"---------------------------------\\\")\\n\",\n        \"    if Play_Notification==True:\\n\",\n        \"      play_notification_sound()\\n\",\n        \"\\n\",\n        \"threading.Thread(target=wait_for_server, daemon=True).start()\\n\",\n        \"\\n\",\n        \"mainpy=codecs.decode('ZZIPFreireFVB.cl','rot_13')\\n\",\n        \"\\n\",\n        \"!python3 $mainpy \\\\\\n\",\n        \"  -p {PORT} \\\\\\n\",\n        \"  --https False \\\\\\n\",\n        \"  --content_vec_500 pretrain/checkpoint_best_legacy_500.pt \\\\\\n\",\n        \"  --content_vec_500_onnx pretrain/content_vec_500.onnx \\\\\\n\",\n        \"  --content_vec_500_onnx_on true \\\\\\n\",\n        \"  --hubert_base pretrain/hubert_base.pt \\\\\\n\",\n        \"  --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\\\\n\",\n        \"  --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\\\\n\",\n        \"  --nsf_hifigan pretrain/nsf_hifigan/model \\\\\\n\",\n        \"  --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\\\\n\",\n        \"  --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\\\\n\",\n        \"  --rmvpe pretrain/rmvpe.pt \\\\\\n\",\n        \"  --model_dir model_dir \\\\\\n\",\n        \"  --samples samples.json\\n\",\n        \"\\n\",\n        \"ngrok.disconnect(ngrokConnection.public_url)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"![](https://i.pinimg.com/474x/de/72/9e/de729ecfa41b69901c42c82fff752414.jpg)\\n\",\n        \"![](https://i.pinimg.com/474x/de/72/9e/de729ecfa41b69901c42c82fff752414.jpg)\"\n      ],\n      \"metadata\": {\n        \"id\": \"2Uu1sTSwTc7q\"\n      }\n    }\n  ],\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": [],\n      \"private_outputs\": true,\n      \"gpuType\": \"T4\",\n      \"include_colab_link\": true\n    },\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"name\": \"python\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0\n}"
        },
        {
          "name": "Kaggle_RealtimeVoiceChanger.ipynb",
          "type": "blob",
          "size": 8.47265625,
          "content": "{\n   \"metadata\":{\n      \"kernelspec\":{\n         \"language\":\"python\",\n         \"display_name\":\"Python 3\",\n         \"name\":\"python3\"\n      },\n      \"language_info\":{\n         \"name\":\"python\",\n         \"version\":\"3.10.12\",\n         \"mimetype\":\"text/x-python\",\n         \"codemirror_mode\":{\n            \"name\":\"ipython\",\n            \"version\":3\n         },\n         \"pygments_lexer\":\"ipython3\",\n         \"nbconvert_exporter\":\"python\",\n         \"file_extension\":\".py\"\n      }\n   },\n   \"nbformat_minor\":4,\n   \"nbformat\":4,\n   \"cells\":[\n\t\t{\n\t\t  \"cell_type\": \"markdown\",\n\t\t  \"metadata\": {\n\t\t\t\"id\": \"view-in-github\",\n\t\t\t\"colab_type\": \"text\"\n\t\t  },\n\t\t  \"source\": [\n\t\t\t\"<a href=\\\"https://www.kaggle.com/code/rafacasari/wokada-voice-changer\\\" target=\\\"_parent\\\"><img src=\\\"https://img.shields.io/badge/Open%20In%20Kaggle-035a7d?style=for-the-badge&logo=kaggle&logoColor=white\\\" alt=\\\"Open In Colab\\\"/></a>\"\n\t\t  ]\n\t  },\n      {\n         \"cell_type\":\"markdown\",\n         \"source\":\"### [w-okada's Voice Changer](https://github.com/w-okada/voice-changer) | **Kaggle**\\n\\n---\\n\\n## **⬇ VERY IMPORTANT ⬇**\\n\\nYou can use the following settings for better results:\\n\\nIf you're using a index: `f0: RMVPE_ONNX | Chunk: 112 or higher | Extra: 8192`<br>\\nIf you're not using a index: `f0: RMVPE_ONNX | Chunk: 96 or higher | Extra: 16384`<br>\\n**Don't forget to select a GPU in the GPU field, <b>NEVER</b> use CPU!\\n> Seems that PTH models performance better than ONNX for now, you can still try ONNX models and see if it satisfies you\\n\\n\\n*You can always [click here](https://github.com/YunaOneeChan/Voice-Changer-Settings) to check if these settings are up-to-date*\\n\\n---\\n**Credits**<br>\\nRealtime Voice Changer by [w-okada](https://github.com/w-okada)<br>\\nNotebook files updated by [rafacasari](https://github.com/Rafacasari)<br>\\nRecommended settings by [YunaOneeChan](https://github.com/YunaOneeChan)\\n\\n**Need help?** [AI Hub Discord](https://discord.gg/aihub) » ***#help-realtime-vc***\\n\\n---\",\n         \"metadata\":{\n            \"id\":\"Lbbmx_Vjl0zo\"\n         }\n      },\n      {\n         \"cell_type\":\"markdown\",\n         \"source\":\"# Kaggle Tutorial\\nRunning this notebook can be a bit complicated.\\\\\\nAfter created your Kaggle account, you'll need to **verify your phone number** to be able to use Internet Connection and GPUs.\\\\\\nFollow the instructions on the image below.\\n\\n## <font color=blue>*You can use <b>GPU P100</b> instead of GPU T4, some people are telling that <b>P100 is better</b>.*</font>\\n![instructions.png](https://i.imgur.com/0NutkD8.png)\",\n         \"metadata\":{\n            \n         }\n      },\n      {\n         \"cell_type\":\"markdown\",\n         \"source\":\"# Clone repository and install dependencies\\nThis first step will download the latest version of Voice Changer and install the dependencies. **It will take some time to complete.**\",\n         \"metadata\":{\n            \n         }\n      },\n      {\n         \"cell_type\":\"code\",\n         \"source\":\"# This will make that we're on the right folder before installing\\n%cd /kaggle/working/\\n\\n!pip install colorama --quiet\\nfrom colorama import Fore, Style\\nimport os\\n\\nprint(f\\\"{Fore.CYAN}> Cloning the repository...{Style.RESET_ALL}\\\")\\n!git clone https://github.com/w-okada/voice-changer.git --quiet\\nprint(f\\\"{Fore.GREEN}> Successfully cloned the repository!{Style.RESET_ALL}\\\")\\n%cd voice-changer/server/\\n\\nprint(f\\\"{Fore.CYAN}> Installing libportaudio2...{Style.RESET_ALL}\\\")\\n!apt-get -y install libportaudio2 -qq\\n\\nprint(f\\\"{Fore.CYAN}> Installing pre-dependencies...{Style.RESET_ALL}\\\")\\n# Install dependencies that are missing from requirements.txt and pyngrok\\n!pip install faiss-gpu fairseq pyngrok --quiet \\n!pip install pyworld --no-build-isolation --quiet\\nprint(f\\\"{Fore.CYAN}> Installing dependencies from requirements.txt...{Style.RESET_ALL}\\\")\\n!pip install -r requirements.txt --quiet\\n\\n# Download the default settings ^-^\\nif not os.path.exists(\\\"/kaggle/working/voice-changer/server/stored_setting.json\\\"):\\n    !wget -q https://gist.githubusercontent.com/Rafacasari/d820d945497a01112e1a9ba331cbad4f/raw/8e0a426c22688b05dd9c541648bceab27e422dd6/kaggle_setting.json -O /kaggle/working/voice-changer/server/stored_setting.json\\nprint(f\\\"{Fore.GREEN}> Successfully installed all packages!{Style.RESET_ALL}\\\")\\n\\nprint(f\\\"{Fore.GREEN}> You can safely ignore the dependency conflict errors, it's a error from Kaggle and don't interfer on Voice Changer!{Style.RESET_ALL}\\\")\",\n         \"metadata\":{\n            \"id\":\"86wTFmqsNMnD\",\n            \"cellView\":\"form\",\n            \"_kg_hide-output\":false,\n            \"execution\":{\n               \"iopub.status.busy\":\"2023-09-14T04:01:17.308284Z\",\n               \"iopub.execute_input\":\"2023-09-14T04:01:17.308682Z\",\n               \"iopub.status.idle\":\"2023-09-14T04:08:08.475375Z\",\n               \"shell.execute_reply.started\":\"2023-09-14T04:01:17.308652Z\",\n               \"shell.execute_reply\":\"2023-09-14T04:08:08.473827Z\"\n            },\n            \"trusted\":true\n         },\n         \"execution_count\":0,\n         \"outputs\":[\n            \n         ]\n      },\n      {\n         \"cell_type\":\"markdown\",\n         \"source\":\"# Start Server **using ngrok**\\nThis cell will start the server, the first time that you run it will download the models, so it can take a while (~1-2 minutes)\\n\\n---\\nYou'll need a ngrok account, but <font color=green>**it's free**</font> and easy to create!\\n---\\n**1** - Create a **free** account at [ngrok](https://dashboard.ngrok.com/signup)\\\\\\n**2** - If you didn't logged in with Google or Github, you will need to **verify your e-mail**!\\\\\\n**3** - Click [this link](https://dashboard.ngrok.com/get-started/your-authtoken) to get your auth token, and replace **YOUR_TOKEN_HERE** with your token.\\\\\\n**4** - *(optional)* Change to a region near to you\",\n         \"metadata\":{\n            \n         }\n      },\n      {\n         \"cell_type\":\"code\",\n         \"source\":\"# ---------------------------------\\n# SETTINGS\\n# ---------------------------------\\n\\nToken = '2Tn2hbfLtw2ii6DHEJy7SsM1BjI_21G14MXSwz7qZSDL2Dv3B'\\nClearConsole = True # Clear console after initialization. Set to False if you are having some error, then you will be able to report it.\\nRegion = \\\"sa\\\" # Read the instructions below\\n\\n# You can change the region for a better latency, use only the abbreviation\\n# Choose between this options: \\n# us -> United States (Ohio)\\n# ap -> Asia/Pacific (Singapore)\\n# au -> Australia (Sydney)\\n# eu -> Europe (Frankfurt)\\n# in -> India (Mumbai)\\n# jp -> Japan (Tokyo)\\n# sa -> South America (Sao Paulo)\\n\\n# ---------------------------------\\n# DO NOT TOUCH ANYTHING DOWN BELOW!\\n# ---------------------------------\\n\\n%cd /kaggle/working/voice-changer/server\\n    \\nfrom pyngrok import conf, ngrok\\nMyConfig = conf.PyngrokConfig()\\nMyConfig.auth_token = Token\\nMyConfig.region = Region\\n#conf.get_default().authtoken = Token\\n#conf.get_default().region = Region\\nconf.set_default(MyConfig);\\n\\nimport subprocess, threading, time, socket, urllib.request\\nPORT = 8000\\n\\nfrom pyngrok import ngrok\\nngrokConnection = ngrok.connect(PORT)\\npublic_url = ngrokConnection.public_url\\n\\nfrom IPython.display import clear_output\\n\\ndef wait_for_server():\\n    while True:\\n        time.sleep(0.5)\\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n        result = sock.connect_ex(('127.0.0.1', PORT))\\n        if result == 0:\\n            break\\n        sock.close()\\n    if ClearConsole:\\n        clear_output()\\n    print(\\\"--------- SERVER READY! ---------\\\")\\n    print(\\\"Your server is available at:\\\")\\n    print(public_url)\\n    print(\\\"---------------------------------\\\")\\n\\nthreading.Thread(target=wait_for_server, daemon=True).start()\\n\\n!python3 MMVCServerSIO.py \\\\\\n  -p {PORT} \\\\\\n  --https False \\\\\\n  --content_vec_500 pretrain/checkpoint_best_legacy_500.pt \\\\\\n  --content_vec_500_onnx pretrain/content_vec_500.onnx \\\\\\n  --content_vec_500_onnx_on true \\\\\\n  --hubert_base pretrain/hubert_base.pt \\\\\\n  --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\\\\n  --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\\\\n  --nsf_hifigan pretrain/nsf_hifigan/model \\\\\\n  --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\\\\n  --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\\\\n  --rmvpe pretrain/rmvpe.pt \\\\\\n  --model_dir model_dir \\\\\\n  --samples samples.json\\n\\nngrok.disconnect(ngrokConnection.public_url)\",\n         \"metadata\":{\n            \"id\":\"lLWQuUd7WW9U\",\n            \"cellView\":\"form\",\n            \"_kg_hide-input\":false,\n            \"scrolled\":true,\n            \"trusted\":true\n         },\n         \"execution_count\":null,\n         \"outputs\":[\n            \n         ]\n      }\n   ]\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 6.28125,
          "content": "MIT License\n\nCopyright (c) 2022 Wataru Okada\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nMIT License\n\nCopyright (c) 2022 Isle Tennos\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nMIT License\n\nCopyright (c) 2021 Jaehyeon Kim\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nMIT License\n\nCopyright (c) 2023 liujing04\nCopyright (c) 2023 源文雨\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nMIT License\n\nCopyright (c) 2023 yxlllc\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nMIT License\n\nCopyright (c) 2023 yxlllc\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "LICENSE-CLA",
          "type": "blob",
          "size": 2.1630859375,
          "content": "Contributor License Agreement\n\nCopyright (c) 2022 Wataru Okada\n\n本契約は、当社とあなた（以下、\"貢献者\"とします）の間で締結され、貢献者が当社に対してソフトウェアプロジェクト（以下、\"プロジェクト\"とします）に対する貢献（以下、\"貢献\"とします）を提供する際の条件を定めます。\n\n1. 貢献者は、提供する貢献が、貢献者自身のオリジナルな作品であり、商標、著作権、特許、または他の知的財産権を侵害していないことを保証します。\n\n2. 貢献者は、貢献を当社に対して無償で提供し、当社はそれを無制限に使用、複製、修正、公開、配布、サブライセンスを付与し、またその販売する権利を得ることに同意します。\n\n3. 本契約が終了した場合でも、第 2 項で述べた権利は当社に留保されます。\n\n4. 当社は貢献者の貢献を受け入れる義務を負わず、また貢献者に一切の補償をする義務を負わないことに貢献者は同意します。\n\n5. 本契約は当社と貢献者双方の書面による合意により修正されることがあります。\n\n\"This Agreement is made between our Company and you (hereinafter referred to as \"Contributor\") and outlines the terms under which you provide your Contributions (hereinafter referred to as \"Contributions\") to our software project (hereinafter referred to as \"Project\").\n\n1. You warrant that the Contributions you are providing are your original work and do not infringe any trademark, copyright, patent, or other intellectual property rights.\n\n2. You agree to provide your Contributions to the Company for free, and the Company has the unlimited right to use, copy, modify, publish, distribute, and sublicense, and also sell the Contributions.\n\n3. Even after the termination of this Agreement, the rights mentioned in the above clause will be retained by the Company.\n\n4. The Company is under no obligation to accept your Contributions or to compensate you in any way for them, and you agree to this.\n\n5. This Agreement may be modified by written agreement between the Company and the Contributor.\"\n"
        },
        {
          "name": "LICENSE-NOTICE",
          "type": "blob",
          "size": 0.884765625,
          "content": "1. Diffusion SVC, DDSP SVC は vocodeer は DiffSinger Community Vocoders を使用しています。次のリンクからライセンスをご確認ください。\n   別のモデルを使用する場合は pretrain\\\\nsf_hifigan に設置してください。\n   https://openvpi.github.io/vocoders/\n\n   Diffusion SVC and DDSP SVC uses DiffSinger Community Vocoders. Please check the license from the following link.\n   Please place it on pretrain\\\\nsf_hifigan if you are using a different model.\n   https://openvpi.github.io/vocoders/\n\n2. Beatrice JVS Corpus Edition のライセンスについてはこちらを確認してください。\n  [readme](https://github.com/w-okada/voice-changer/blob/master/server/voice_changer/Beatrice/)\n\n  Please check here for the license of the Beatrice JVS Corpus Edition.\n  [readme](https://github.com/w-okada/voice-changer/blob/master/server/voice_changer/Beatrice/)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.8828125,
          "content": "[日本語](/README.md) /\n[英語](/docs_i18n/README_en.md) /\n[韓国語](/docs_i18n/README_ko.md)/\n[中国語](/docs_i18n/README_zh.md)/\n[ドイツ語](/docs_i18n/README_de.md)/\n[アラビア語](/docs_i18n/README_ar.md)/\n[ギリシャ語](/docs_i18n/README_el.md)/\n[スペイン語](/docs_i18n/README_es.md)/\n[フランス語](/docs_i18n/README_fr.md)/\n[イタリア語](/docs_i18n/README_it.md)/\n[ラテン語](/docs_i18n/README_la.md)/\n[マレー語](/docs_i18n/README_ms.md)/\n[ロシア語](/docs_i18n/README_ru.md) \n  *日本語以外は機械翻訳です。\n\nVCClient\n---\n\nVCClientは、AIを用いてリアルタイム音声変換を行うソフトウェアです。\n\n## What's New!\n- v.2.0.73-beta\n  - new feature:\n    - 編集したbeatrice modelのダウンロード\n  - bugfix:\n    - beatrice v2 のpitch, formantが反映されないバグを修正\n    - Applio のembedderを使用しているモデルのONNXができないバグを修正\n- v.2.0.72-beta (バグがあるので非推奨。v.2.0.73で修正済み)\n  - new feature\n    - Beatriceの編集GUI\n    - Beatriceのvoiceごとにpitch, formantを記憶\n    - GUI多言語化\n    - Applioのembedder対応\n- v.2.0.70-beta (only for m1 mac)\n  - new feature:\n    - M1 Mac版VCClientでもBeatrice v2 beta.1をサポートしました。\n- v.2.0.69-beta (only for win)\n  - bugfix:\n    - 一部の例外発生時にスタートボタンが表示されなくなるバグを修正\n    - サーバデバイスモードの出力バッファを調整\n    - サーバデバイスモード使用中に設定変更を行うとサンプリングレートが変化するバグを修正\n    - 日本語hubert使用時のバグ修正\n  - misc:\n    - サーバデバイスモードのホストAPIフィルタ追加（強調表示）\n\n\n\n## ダウンロードと関連リンク\nWindows版、 M1 Mac版はhugging faceのリポジトリからダウンロードできます。\n\n- [VCClient のリポジトリ](https://huggingface.co/wok000/vcclient000/tree/main)\n- [Light VCClient for Beatrice v2 のリポジトリ](https://huggingface.co/wok000/light_vcclient_beatrice/tree/main)\n\n\n*1 Linuxはリポジトリをcloneしてお使いください。\n\n### 関連リンク\n- [Beatrice V2 トレーニングコードのリポジトリ](https://huggingface.co/fierce-cats/beatrice-trainer)\n- [Beatrice V2 トレーニングコード Colab版](https://github.com/w-okada/beatrice-trainer-colab)\n\n### 関連ソフトウェア\n- [リアルタイムボイスチェンジャ VCClient](https://github.com/w-okada/voice-changer)\n- [読み上げソフトウェア TTSClient](https://github.com/w-okada/ttsclient)\n- [リアルタイム音声認識ソフトウェア ASRClient](https://github.com/w-okada/asrclient)\n\n\n\n## VC Clientの特徴\n\n## 多様なAIモデルをサポート\n\n| AIモデル                                                                                                     | v.2       | v.1                  | ライセンス                                                                                 |\n| ------------------------------------------------------------------------------------------------------------ | --------- | -------------------- | ------------------------------------------------------------------------------------------ |\n| [RVC ](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/docs/jp/README.ja.md) | supported | supported            | リポジトリを参照してください。                                                             |\n| [Beatrice v1](https://prj-beatrice.com/)                                                                     | n/a       | supported (only win) | [独自](https://github.com/w-okada/voice-changer/tree/master/server/voice_changer/Beatrice) |\n| [Beatrice v2](https://prj-beatrice.com/)                                                                     | supported | n/a                  | [独自](https://huggingface.co/wok000/vcclient_model/blob/main/beatrice_v2_beta/readme.md)  |\n| [MMVC](https://github.com/isletennos/MMVC_Trainer)                                                           | n/a       | supported            | リポジトリを参照してください。                                                             |\n| [so-vits-svc](https://github.com/svc-develop-team/so-vits-svc)                                               | n/a       | supported            | リポジトリを参照してください。                                                             |\n| [DDSP-SVC](https://github.com/yxlllc/DDSP-SVC)                                                               | n/a       | supported            | リポジトリを参照してください。                                                             |\n\n## スタンドアロン、ネットワーク経由の両構成をサポート\nローカルPCで完結した音声変換も、ネットワークを介した音声変換もサポートしています。\nネットワークを介した利用を行うことで、ゲームなどの高負荷なアプリケーションと同時に使用する場合に音声変換の負荷を外部にオフロードすることができます。\n\n![image](https://user-images.githubusercontent.com/48346627/206640768-53f6052d-0a96-403b-a06c-6714a0b7471d.png)\n\n## 複数プラットフォームに対応\n\nWindows, Mac(M1), Linux, Google Colab\n\n*1 Linuxはリポジトリをcloneしてお使いください。\n\n## REST APIを提供\n各種プログラミング言語でクライアントを作成することができます。\n\nまた、curlなどのOSに組み込まれているHTTPクライアントを使って操作ができます。\n\n## トラブルシュート\n\n[通信編](tutorials/trouble_shoot_communication_ja.md)\n\n\n## 開発者の署名について\n\n本ソフトウェアは開発元の署名しておりません。下記のように警告が出ますが、コントロールキーを押しながらアイコンをクリックすると実行できるようになります。これは Apple のセキュリティポリシーによるものです。実行は自己責任となります。\n\n![image](https://user-images.githubusercontent.com/48346627/212567711-c4a8d599-e24c-4fa3-8145-a5df7211f023.png)\n\n## Acknowledgments\n\n- [立ちずんだもん素材](https://seiga.nicovideo.jp/seiga/im10792934)\n- [いらすとや](https://www.irasutoya.com/)\n- [つくよみちゃん](https://tyc.rei-yumesaki.net/)\n\n```\n  本ソフトウェアの音声合成には、フリー素材キャラクター「つくよみちゃん」が無料公開している音声データを使用しています。\n  ■つくよみちゃんコーパス（CV.夢前黎）\n  https://tyc.rei-yumesaki.net/material/corpus/\n  © Rei Yumesaki\n```\n\n- [あみたろの声素材工房](https://amitaro.net/)\n- [れぷりかどーる](https://kikyohiroto1227.wixsite.com/kikoto-utau)\n\n## 利用規約\n\n- リアルタイムボイスチェンジャーつくよみちゃんについては、つくよみちゃんコーパスの利用規約に準じ、次の目的で変換後の音声を使用することを禁止します。\n\n```\n\n■人を批判・攻撃すること。（「批判・攻撃」の定義は、つくよみちゃんキャラクターライセンスに準じます）\n\n■特定の政治的立場・宗教・思想への賛同または反対を呼びかけること。\n\n■刺激の強い表現をゾーニングなしで公開すること。\n\n■他者に対して二次利用（素材としての利用）を許可する形で公開すること。\n※鑑賞用の作品として配布・販売していただくことは問題ございません。\n```\n\n- リアルタイムボイスチェンジャーあみたろについては、あみたろの声素材工房様の次の利用規約に準じます。詳細は[こちら](https://amitaro.net/voice/faq/#index_id6)\n\n```\nあみたろの声素材やコーパス読み上げ音声を使って音声モデルを作ったり、ボイスチェンジャーや声質変換などを使用して、自分の声をあみたろの声に変換して使うのもOKです。\n\nただしその場合は絶対に、あみたろ（もしくは小春音アミ）の声に声質変換していることを明記し、あみたろ（および小春音アミ）が話しているわけではないことが誰でもわかるようにしてください。\nまた、あみたろの声で話す内容は声素材の利用規約の範囲内のみとし、センシティブな発言などはしないでください。\n```\n\n- リアルタイムボイスチェンジャー黄琴まひろについては、れぷりかどーるの利用規約に準じます。詳細は[こちら](https://kikyohiroto1227.wixsite.com/kikoto-utau/ter%EF%BD%8Ds-of-service)\n\n## 免責事項\n\n本ソフトウェアの使用または使用不能により生じたいかなる直接損害・間接損害・波及的損害・結果的損害 または特別損害についても、一切責任を負いません。\n"
        },
        {
          "name": "README_dev_en.md",
          "type": "blob",
          "size": 2.787109375,
          "content": "## For Developper\n\n[Japanese](/README_dev_ja.md) [Russian](/README_dev_ru.md)\n\n## Prerequisit\n\n- Linux(ubuntu, debian) or WSL2, (not tested for other linux distributions and Mac)\n- Anaconda\n\n## Preparation\n\n1. Create anaconda virtual environment\n\n```\n$ conda create -n vcclient-dev python=3.10\n$ conda activate vcclient-dev\n```\n\n2. clone repository\n\n```\n$ git clone https://github.com/w-okada/voice-changer.git\n```\n\n## For Server Developer\n\n1. Install requirements\n\n```\n$ cd voice-changer/server\n$ pip install -r requirements.txt\n```\n\n2. Run server\n\nRun server with the below command. You can replace the path to each weight.\n\n```\n$ python3 MMVCServerSIO.py -p 18888 --https true \\\n    --content_vec_500 pretrain/checkpoint_best_legacy_500.pt  \\\n    --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n    --content_vec_500_onnx_on true \\\n    --hubert_base pretrain/hubert_base.pt \\\n    --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n    --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n    --nsf_hifigan pretrain/nsf_hifigan/model \\\n    --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n    --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n    --rmvpe pretrain/rmvpe.pt \\\n    --model_dir model_dir \\\n    --samples samples.json\n\n```\n\nAccess with Browser (currently only chrome is supported), then you can see gui.\n\n2-1. Trouble shoot\n\n(1) OSError: PortAudio library not found\nIf you get the message below, you shold install additional library.\n\n```\nOSError: PortAudio library not found\n```\n\nYou can install the library this command.\n\n```\n$ sudo apt-get install libportaudio2\n$ sudo apt-get install libasound-dev\n```\n\n(2) It's not starting up! Damn software!\n\nThe client will not start automatically. Please launch your browser and access the URL displayed on the console. And watch your words.\n\n(3) Could not load library libcudnn_cnn_infer.so.8\n\nWhen using WSL, you might encounter a message saying `Could not load library libcudnn_cnn_infer.so.8. Error: libcuda.so: cannot open shared object file: No such file or directory`. This often happens because the path hasn't been properly set. Please set the path as shown below. It might be handy to add this to your launch script, such as .bashrc.\n\n```\nexport LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH\n```\n\n- reference\n  - https://qiita.com/cacaoMath/items/811146342946cdde5b83\n  - https://github.com/microsoft/WSL/issues/8587\n\n3. Enjoy developing.\n\n### Appendix\n\n1. Win + Anaconda (not supported)\n\nuse conda to install pytorch\n\n```\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n```\n\nAlso run these command.\n\n```\npip install chardet\npip install numpy==1.24.0\n```\n\n## For Client Developer\n\n1. Import modules and initial build\n\n```\ncd client\ncd lib\nnpm install\nnpm run build:dev\ncd ../demo\nnpm install\nnpm run build:dev\n```\n\n2. Enjoy developing.\n"
        },
        {
          "name": "README_dev_ja.md",
          "type": "blob",
          "size": 3.185546875,
          "content": "## 開発者向け\n\n[English](/README_dev_en.md)\n\n## 前提\n\n- Linux(ubuntu, debian) or WSL2, (not tested for other linux distributions and Mac)\n- Anaconda\n\n## 準備\n\n1. Anaconda の仮想環境を作成する\n\n```\n$ conda create -n vcclient-dev python=3.10\n$ conda activate vcclient-dev\n```\n\n2. リポジトリをクローンする\n\n```\n$ git clone https://github.com/w-okada/voice-changer.git\n```\n\n## サーバ開発者向け\n\n1. モジュールをインストールする\n\n```\n$ cd voice-changer/server\n$ pip install -r requirements.txt\n```\n\n2. サーバを起動する\n\n次のコマンドで起動します。各種重みについてのパスは環境に合わせて変えてください。\n\n```\n$ python3 MMVCServerSIO.py -p 18888 --https true \\\n    --content_vec_500 pretrain/checkpoint_best_legacy_500.pt  \\\n    --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n    --content_vec_500_onnx_on true \\\n    --hubert_base pretrain/hubert_base.pt \\\n    --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n    --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n    --nsf_hifigan pretrain/nsf_hifigan/model \\\n    --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n    --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n    --rmvpe pretrain/rmvpe.pt \\\n    --model_dir model_dir \\\n    --samples samples.json\n```\n\nブラウザ(Chrome のみサポート)でアクセスすると画面が表示されます。\n\n2-1. トラブルシュート\n\n(1) OSError: PortAudio library not found\n次のようなメッセージが表示される場合、追加でライブラリを追加する必要があります。\n\n```\nOSError: PortAudio library not found\n```\n\nubuntu(wsl2)の場合下記のコマンドでインストールできます。\n\n```\n$ sudo apt-get install libportaudio2\n$ sudo apt-get install libasound-dev\n```\n\n(2) 起動しないんだけど！？\n\n自動でクライアントは起動しません。ブラウザを立ち上げてコンソールに表示された URL にアクセスしてください。\n\n(3) Could not load library libcudnn_cnn_infer.so.8\nWSL を使っていると`Could not load library libcudnn_cnn_infer.so.8. Error: libcuda.so: cannot open shared object file: No such file or directory`と表示される場合があります。\nパスが通っていないことが原因のことが多いです。下記のようにパスを通して実行してください。\n.bashrc など起動スクリプトに追加しておくと便利だと思います。\n\n```\nexport LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH\n```\n\n- 参考\n  - https://qiita.com/cacaoMath/items/811146342946cdde5b83\n  - https://github.com/microsoft/WSL/issues/8587\n\n3. 開発しましょう\n\n### Appendix\n\n1. Win + Anaconda のとき (not supported)\n\npytorch を conda で入れないと gpu を認識しないかもしれない。\n\n```\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n```\n\nまた、追加で下記も必要のようだ。\n\n```\npip install chardet\npip install numpy==1.24.0\n```\n\n## クライアント開発者向け\n\n1. モジュールをインストールして、一度ビルドします\n\n```\ncd client\ncd lib\nnpm install\nnpm run build:dev\ncd ../demo\nnpm install\nnpm run build:dev\n```\n\n2. 開発しましょう\n"
        },
        {
          "name": "README_dev_ko.md",
          "type": "blob",
          "size": 3.0537109375,
          "content": "## 개발자용\n\n[English](/README_dev_en.md) [Korean](/README_dev_ko.md)\n\n## 전제\n\n- Linux(ubuntu, debian) or WSL2, (다른 리눅스 배포판과 Mac에서는 테스트하지 않았습니다)\n- Anaconda\n\n## 준비\n\n1. Anaconda 가상 환경을 작성한다\n\n```\n$ conda create -n vcclient-dev python=3.10\n$ conda activate vcclient-dev\n```\n \n2. 리포지토리를 클론한다\n\n```\n$ git clone https://github.com/w-okada/voice-changer.git\n```\n\n## 서버 개발자용\n\n1. 모듈을 설치한다\n\n```\n$ cd voice-changer/server\n$ pip install -r requirements.txt\n```\n\n2. 서버를 구동한다\n\n다음 명령어로 구동합니다. 여러 가중치에 대한 경로는 환경에 맞게 변경하세요.\n\n```\n$ python3 MMVCServerSIO.py -p 18888 --https true \\\n    --content_vec_500 pretrain/checkpoint_best_legacy_500.pt  \\\n    --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n    --content_vec_500_onnx_on true \\\n    --hubert_base pretrain/hubert_base.pt \\\n    --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n    --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n    --nsf_hifigan pretrain/nsf_hifigan/model \\\n    --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n    --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n    --rmvpe pretrain/rmvpe.pt \\\n    --model_dir model_dir \\\n    --samples samples.json\n```\n\n브라우저(Chrome에서만 지원)에서 접속하면 화면이 나옵니다.\n\n2-1. 문제 해결법\n\n(1) OSError: PortAudio library not found\n다음과 같은 메시지가 나올 경우에는 추가 라이브러리를 설치해야 합니다.\n\n```\nOSError: PortAudio library not found\n```\n\nubuntu(wsl2)인 경우에는 아래 명령어로 설치할 수 있습니다.\n\n```\n$ sudo apt-get install libportaudio2\n$ sudo apt-get install libasound-dev\n```\n\n(2) 서버 구동이 안 되는데요?!\n\n클라이언트는 자동으로 구동되지 않습니다. 브라우저를 실행하고 콘솔에 표시된 URL로 접속하세요.\n\n(3) Could not load library libcudnn_cnn_infer.so.8\nWSL를 사용 중이라면 `Could not load library libcudnn_cnn_infer.so.8. Error: libcuda.so: cannot open shared object file: No such file or directory`라는 메시지가 나오는 경우가 있습니다.\n잘못된 경로가 원인인 경우가 많습니다. 아래와 같이 경로를 바꾸고 실행해 보세요.\n.bashrc 등 구동 스크립트에 추가해 두면 편리합니다.\n\n```\nexport LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH\n```\n\n- 참고\n  - https://qiita.com/cacaoMath/items/811146342946cdde5b83\n  - https://github.com/microsoft/WSL/issues/8587\n\n3. 개발하세요\n\n### Appendix\n\n1. Win + Anaconda일 때 (not supported)\n\npytorch를 conda가 없으면 gpu를 인식하지 않을 수 있습니다.\n\n```\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n```\n\n또한 추가로 아래 내용도 필요합니다.\n\n```\npip install chardet\npip install numpy==1.24.0\n```\n\n## 클라이언트 개발자용\n\n1. 모듈을 설치하고 한번 빌드합니다\n\n```\ncd client\ncd lib\nnpm install\nnpm run build:dev\ncd ../demo\nnpm install\nnpm run build:dev\n```\n\n2. 개발하세요\n"
        },
        {
          "name": "README_dev_ru.md",
          "type": "blob",
          "size": 4.0185546875,
          "content": "Вот перевод файла `README_dev_en.md` на русский язык:\n\n## Для разработчиков\n\n[Японский](/README_dev_ja.md) [Английский](/README_dev_en.md)\n\n## Требования\n\n- Linux (Ubuntu, Debian) или WSL2 (другие дистрибуции Linux и Mac не тестировались)\n- Anaconda\n\n## Подготовка\n\n1. Создайте виртуальную среду Anaconda:\n\n```\n$ conda create -n vcclient-dev python=3.10\n$ conda activate vcclient-dev\n```\n\n2. Клонируйте репозиторий:\n\n```\n$ git clone https://github.com/w-okada/voice-changer.git\n```\n\n## Для серверных разработчиков\n\n1. Установите необходимые зависимости:\n\n```\n$ cd voice-changer/server\n$ pip install -r requirements.txt\n```\n\n2. Запустите сервер\n\nЗапустите сервер с помощью следующей команды. Вы можете указать свои пути к весам моделей.\n\n```\n$ python3 MMVCServerSIO.py -p 18888 --https true \\\n    --content_vec_500 pretrain/checkpoint_best_legacy_500.pt  \\\n    --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n    --content_vec_500_onnx_on true \\\n    --hubert_base pretrain/hubert_base.pt \\\n    --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n    --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n    --nsf_hifigan pretrain/nsf_hifigan/model \\\n    --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n    --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n    --rmvpe pretrain/rmvpe.pt \\\n    --model_dir model_dir \\\n    --samples samples.json\n```\n\nОткройте браузер (на данный момент поддерживается только Chrome), и вы увидите графический интерфейс.\n\n2-1. Устранение неполадок\n\n(1) OSError: не найдена библиотека PortAudio\n\nЕсли вы получите сообщение ниже, необходимо установить дополнительную библиотеку:\n\n```\nOSError: PortAudio library not found\n```\n\nВы можете установить библиотеку командой:\n\n```\n$ sudo apt-get install libportaudio2\n$ sudo apt-get install libasound-dev\n```\n\n(2) Не запускается! Чертова программа!\n\nКлиент не запускается автоматически. Пожалуйста, откройте браузер и перейдите по URL, отображаемому в консоли. И будьте осторожны со словами.\n\n(3) Не удалось загрузить библиотеку libcudnn_cnn_infer.so.8\n\nПри использовании WSL может возникнуть ошибка `Could not load library libcudnn_cnn_infer.so.8. Error: libcuda.so: cannot open shared object file: No such file or directory`. Это часто связано с тем, что путь к библиотеке не установлен. Установите путь с помощью команды ниже. Вы можете добавить эту команду в ваш скрипт запуска, например, в .bashrc.\n\n```\nexport LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH\n```\n\n- ссылки:\n  - https://qiita.com/cacaoMath/items/811146342946cdde5b83\n  - https://github.com/microsoft/WSL/issues/8587\n\n3. Наслаждайтесь разработкой.\n\n### Приложение\n\n1. Windows + Anaconda (не поддерживается)\n\nИспользуйте conda для установки PyTorch:\n\n```\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n```\n\nТакже выполните эти команды:\n\n```\npip install chardet\npip install numpy==1.24.0\n```\n\n## Для клиентских разработчиков\n\n1. Импорт модулей и начальная сборка:\n\n```\ncd client\ncd lib\nnpm install\nnpm run build:dev\ncd ../demo\nnpm install\nnpm run build:dev\n```\n\n2. Наслаждайтесь.\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 8.5849609375,
          "content": "## VC Client\n\n[Japanese](/README_ja.md) [Korean](/README_ko.md) [Russian](/README_ru.md)\n\n## What's New!\n- We have released a sister product, the Text To Speech client.\n  - You can enjoy voice generation with a simple interface.\n  - For more details, click [here](https://github.com/w-okada/ttsclient).\n- Beatrice V2 Training Code Released!!!\n  - [Training Code Repository](https://huggingface.co/fierce-cats/beatrice-trainer)\n  - [Colab Version](https://github.com/w-okada/beatrice-trainer-colab)\n- v.2.0.70-beta (only for m1 mac)\n  - [HERE](https://github.com/w-okada/voice-changer/tree/v.2)\n  - new feature:\n    - The M1 Mac version of VCClient now supports Beatrice v2 beta.1.\n- v.2.0.69-beta (only for win)\n  - [HERE](https://github.com/w-okada/voice-changer/tree/v.2)\n  - bugfix:\n    - Fixed a bug where the start button would not be displayed in case of some exceptions\n    - Adjusted the output buffer for server device mode\n    - Fixed a bug where the sampling rate would change when settings were modified while using server device mode\n    - Fixed a bug when using Japanese hubert\n  - misc:\n    - Added host API filter (highlighted) for server device mode\n- v.2.0.65-beta\n  - [HERE](https://github.com/w-okada/voice-changer/tree/v.2)\n  - new feature: We have supported Beatrice v2 beta.1, enabling even higher quality voice conversion.\n\n# What is VC Client\n\n1. This is a client software for performing real-time voice conversion using various Voice Conversion (VC) AI. The supported AI for voice conversion are as follows.\n\n- [MMVC](https://github.com/isletennos/MMVC_Trainer) (only v1)\n- [so-vits-svc](https://github.com/svc-develop-team/so-vits-svc) (only v1)\n- [RVC(Retrieval-based-Voice-Conversion)](https://github.com/liujing04/Retrieval-based-Voice-Conversion-WebUI)\n- [DDSP-SVC](https://github.com/yxlllc/DDSP-SVC) (only v1)\n- [Beatrice JVS Corpus Edition](https://prj-beatrice.com/) * experimental,  (***NOT MIT License*** see [readme](https://github.com/w-okada/voice-changer/blob/master/server/voice_changer/Beatrice/)) *  Only for Windows, CPU dependent (only v1)\n  - [Beatrice v2](https://prj-beatrice.com/) (only for v2)\n\n1. Distribute the load by running Voice Changer on a different PC\n   The real-time voice changer of this application works on a server-client configuration. By running the MMVC server on a separate PC, you can run it while minimizing the impact on other resource-intensive processes such as gaming commentary.\n\n![image](https://user-images.githubusercontent.com/48346627/206640768-53f6052d-0a96-403b-a06c-6714a0b7471d.png)\n\n3. Cross-platform compatibility\n   Supports Windows, Mac (including Apple Silicon M1), Linux, and Google Colaboratory.\n## Related Software\n- [Real-time Voice Changer VCClient](https://github.com/w-okada/voice-changer)\n- [Text-to-Speech Software TTSClient](https://github.com/w-okada/ttsclient)\n- [Real-Time Speech Recognition Software ASRClient](https://github.com/w-okada/asrclient)\n# usage\n\nThis is an app for performing voice changes with MMVC and so-vits-svc.\n\nIt can be used in two main ways, in order of difficulty:\n\n- Using a pre-built binary\n- Setting up an environment with Docker or Anaconda and using it\n\n## (1) Usage with pre-built binaries\n\n- You can download and run executable binaries.\n\n- Please see [here](tutorials/tutorial_rvc_en_latest.md) for the tutorial. ([trouble shoot](https://github.com/w-okada/voice-changer/blob/master/tutorials/trouble_shoot_communication_ja.md))\n\n- It's now easy to try it out on [Google Colaboratory](https://github.com/w-okada/voice-changer/tree/v.2/w_okada's_Voice_Changer_version_2_x.ipynb) (requires a ngrok account). You can launch it from the 'Open in Colab' button in the top left corner.\n\n<img src=\"https://github.com/w-okada/voice-changer/assets/48346627/3f092e2d-6834-42f6-bbfd-7d389111604e\" width=\"400\" height=\"150\">\n\n- We offer Windows and Mac versions on [hugging face](https://huggingface.co/wok000/vcclient000/tree/main)\n- v2 for Windows\n  - Please download and use `vcclient_win_std_xxx.zip`. You can perform voice conversion using a reasonably high-performance CPU without a GPU, or by utilizing DirectML to leverage GPUs (AMD, Nvidia). v2 supports both torch and onnx.\n  - If you have an Nvidia GPU, you can achieve faster voice conversion by using `vcclient_win_cuda_xxx.zip`.\n- v2 for Mac (Apple Silicon)\n  - Please download and use `vcclient_mac_xxx.zip`.\n- v1\n  - If you are using a Windows and Nvidia GPU, please download ONNX (cpu, cuda), PyTorch (cpu, cuda).\n  - If you are using a Windows and AMD/Intel GPU, please download ONNX (cpu, DirectML) and PyTorch (cpu, cuda). AMD/Intel GPUs are only enabled for ONNX models.\n  - In either case, for GPU support, PyTorch and Onnxruntime are only enabled if supported.\n  - If you are not using a GPU on Windows, please download ONNX (cpu, cuda) and PyTorch (cpu, cuda).\n\n- For Windows user, after unzipping the downloaded zip file, please run the `start_http.bat` file corresponding to your VC.\n\n- For Mac version, after unzipping the downloaded file, double-click the `startHttp.command` file corresponding to your VC. If a message indicating that the developer cannot be verified is displayed, please press the control key and click to run it again (or right-click to run it).\n\n- If you are connecting remotely, please use the `.command` file (Mac) or `.bat` file (Windows) with https instead of http.\n\n- The encoder of DDPS-SVC only supports hubert-soft.\n\n- [Download from hugging face](https://huggingface.co/wok000/vcclient000/tree/main)\n\n## (2) Usage after setting up the environment such as Docker or Anaconda\n\nClone this repository and use it. Setting up WSL2 is essential for Windows. Additionally, setting up virtual environments such as Docker or Anaconda on WSL2 is also required. On Mac, setting up Python virtual environments such as Anaconda is necessary. Although preparation is required, this method works the fastest in many environments. **<font color=\"red\"> Even without a GPU, it may work well enough with a reasonably new CPU </font>(refer to the section on real-time performance below)**.\n\n[Explanation video on installing WSL2 and Docker](https://youtu.be/POo_Cg0eFMU)\n\n[Explanation video on installing WSL2 and Anaconda](https://youtu.be/fba9Zhsukqw)\n\nTo run docker, see [start docker](docker_vcclient/README_en.md).\n\nTo run on Anaconda venv, see [server developer's guide](README_dev_en.md)\n\nTo run on Linux using an AMD GPU, see [setup guide linux](tutorials/tutorial_anaconda_amd_rocm.md)\n\n\n# Software Signing\n\nThis software is not signed by the developer. A warning message will appear, but you can run the software by clicking the icon while holding down the control key. This is due to Apple's security policy. Running the software is at your own risk.\n\n![image](https://user-images.githubusercontent.com/48346627/212567711-c4a8d599-e24c-4fa3-8145-a5df7211f023.png)\n\nhttps://user-images.githubusercontent.com/48346627/212569645-e30b7f4e-079d-4504-8cf8-7816c5f40b00.mp4\n\n# Acknowledgments\n\n- [Tachizunda-mon materials](https://seiga.nicovideo.jp/seiga/im10792934)\n- [Irasutoya](https://www.irasutoya.com/)\n- [Tsukuyomi-chan](https://tyc.rei-yumesaki.net)\n\n> This software uses the voice data of the free material character \"Tsukuyomi-chan,\" which is provided for free by CV. Yumesaki Rei.\n>\n> - Tsukuyomi-chan Corpus (CV. Yumesaki Rei)\n>\n> https://tyc.rei-yumesaki.net/material/corpus/\n>\n> Copyright. Rei Yumesaki\n\n- [Amitaro's Onsozai kobo](https://amitaro.net/)\n- [Replica doll](https://kikyohiroto1227.wixsite.com/kikoto-utau)\n\n# Terms of Use\n\nIn accordance with the Tsukuyomi-chan Corpus Terms of Use for the Tsukuyomi-chan Real-time Voice Changer, the use of the converted voice for the following purposes is prohibited.\n\n- Criticizing or attacking individuals (the definition of \"criticizing or attacking\" is based on the Tsukuyomi-chan character license).\n\n- Advocating for or opposing specific political positions, religions, or ideologies.\n\n- Publicly displaying strongly stimulating expressions without proper zoning.\n\n- Publicly disclosing secondary use (use as materials) for others.\n  (Distributing or selling as a work for viewing is not a problem.)\n\nRegarding the Real-time Voice Changer Amitaro, we prohibit the following uses in accordance with the terms of use of the Amitaro's koe-sozai kobo.[detail](https://amitaro.net/voice/faq/#index_id6)\n\nRegarding the Real-time Voice Changer Kikoto Mahiro, we prohibit the following uses in accordance with the terms of use of Replica doll.[detail](https://kikyohiroto1227.wixsite.com/kikoto-utau/ter%EF%BD%8Ds-of-service)\n\n# Disclaimer\n\nWe are not liable for any direct, indirect, consequential, incidental, or special damages arising out of or in any way connected with the use or inability to use this software.\n"
        },
        {
          "name": "README_ko.md",
          "type": "blob",
          "size": 12.416015625,
          "content": "## VC Client\n\n[English](/README_en.md) [Korean](/README_ko.md)\n\n## 새로운 기능!\n- 자매품으로 텍스트 음성 변환 클라이언트를 출시하였습니다.\n  - 간단한 인터페이스로 음성 생성을 즐길 수 있습니다.\n  - 자세한 내용은 [여기](https://github.com/w-okada/ttsclient)를 참조하세요.\n- Beatrice V2 훈련 코드 공개!!!\n  - [훈련 코드 리포지토리](https://huggingface.co/fierce-cats/beatrice-trainer)\n  - [Colab 버전](https://github.com/w-okada/beatrice-trainer-colab)\n- v.2.0.70-beta (only for m1 mac)\n  - [여기를 참조하십시오](https://github.com/w-okada/voice-changer/tree/v.2)\n  - new feature:\n    - M1 Mac 버전 VCClient에서도 Beatrice v2 beta.1을 지원합니다.\n- v.2.0.69-beta (only for win)\n  - [여기를 참조하십시오](https://github.com/w-okada/voice-changer/tree/v.2)\n  - 버그 수정:\n    - 일부 예외 발생 시 시작 버튼이 표시되지 않는 버그를 수정\n    - 서버 디바이스 모드의 출력 버퍼 조정\n    - 서버 디바이스 모드 사용 중 설정 변경 시 샘플링 레이트가 변하는 버그 수정\n    - 일본어 hubert 사용 시 버그 수정\n  - 기타:\n    - 서버 디바이스 모드에 호스트 API 필터 추가 (강조 표시)\n- v.2.0.65-beta\n  - [여기를 참조하십시오](https://github.com/w-okada/voice-changer/tree/v.2)\n  - new feature: Beatrice v2 beta.1를 지원하여 더 높은 품질의 음성 변환이 가능해졌습니다\n\n# VC Client란\n                                                                                                                                                     \n1. 각종 음성 변환 AI(VC, Voice Conversion)를 활용해 실시간 음성 변환을 하기 위한 클라이언트 소프트웨어입니다. 지원하는 음성 변환 AI는 다음과 같습니다.\n\n- 지원하는 음성 변환 AI (지원 VC)\n  - [MMVC](https://github.com/isletennos/MMVC_Trainer) (only v1)\n  - [so-vits-svc](https://github.com/svc-develop-team/so-vits-svc) (only v1)\n  - [RVC(Retrieval-based-Voice-Conversion)](https://github.com/liujing04/Retrieval-based-Voice-Conversion-WebUI)\n  - [DDSP-SVC](https://github.com/yxlllc/DDSP-SVC) (only v1)\n  - [Beatrice JVS Corpus Edition](https://prj-beatrice.com/) * experimental,  (***NOT MIT License*** see [readme](https://github.com/w-okada/voice-changer/blob/master/server/voice_changer/Beatrice/)) *  Only for Windows, CPU dependent (only v1)\n  - [Beatrice v2](https://prj-beatrice.com/) (only for v2)\n  - \n1. 이 소프트웨어는 네트워크를 통한 사용도 가능하며, 게임 등 부하가 큰 애플리케이션과 동시에 사용할 경우 음성 변화 처리의 부하를 외부로 돌릴 수도 있습니다.\n\n![image](https://user-images.githubusercontent.com/48346627/206640768-53f6052d-0a96-403b-a06c-6714a0b7471d.png)\n\n3. 여러 플랫폼을 지원합니다.\n\n- Windows, Mac(M1), Linux, Google Colab (MMVC만 지원)\n## 관련 소프트웨어\n- [실시간 음성 변조기 VCClient](https://github.com/w-okada/voice-changer)\n- [텍스트 읽기 소프트웨어 TTSClient](https://github.com/w-okada/ttsclient)\n- [실시간 음성 인식 소프트웨어 ASRClient](https://github.com/w-okada/asrclient)\n# 사용 방법\n\n크게 두 가지 방법으로 사용할 수 있습니다. 난이도 순서는 다음과 같습니다.\n\n- 사전 빌드된 Binary 사용\n- Docker, Anaconda 등으로 구축된 개발 환경에서 사용\n\n이 소프트웨어나 MMVC에 익숙하지 않은 분들은 위에서부터 차근차근 익숙해지길 추천합니다.\n\n## (1) 사전 빌드된 Binary(파일) 사용\n\n- 실행 형식 바이너리를 다운로드하여 실행할 수 있습니다.\n\n- 튜토리얼은 [이곳](tutorials/tutorial_rvc_ko_latest.md)을 확인하세요。([네트워크 문제 해결법](https://github.com/w-okada/voice-changer/blob/master/tutorials/trouble_shoot_communication_ko.md))\n\n- [Google Colaboratory](https://github.com/w-okada/voice-changer/tree/v.2/w_okada's_Voice_Changer_version_2_x.ipynb) で簡単にお試しいただけるようになりました。左上の Open in Colab のボタンから起動できます。\n\n<img src=\"https://github.com/w-okada/voice-changer/assets/48346627/3f092e2d-6834-42f6-bbfd-7d389111604e\" width=\"400\" height=\"150\">\n\n- Windows 버전과 Mac 버전을 제공하고 있습니다. [Hugging Face](https://huggingface.co/wok000/vcclient000/tree/main)에서 다운로드할 수 있습니다.\n- Windows용 v2\n  - `vcclient_win_std_xxx.zip`를 다운로드하여 사용하세요. GPU를 사용하지 않고도 (어느 정도 고성능의) CPU를 사용한 음성 변환이나, DirectML을 사용해 GPU(AMD, Nvidia)를 활용한 음성 변환이 가능합니다. v2에서는 torch와 onnx 모두를 지원합니다.\n  - Nvidia GPU를 가지고 계신 분들은 `vcclient_win_cuda_xxx.zip`를 사용하시면 더 빠른 음성 변환이 가능합니다.\n- Mac (Apple Silicon)용 v2\n  - `vcclient_mac_xxx.zip`를 다운로드하여 사용하세요.\n- v1\n  - Windows와 NVIDIA GPU를 사용하는 분은 ONNX(cpu, cuda), PyTorch(cpu, cuda)를 다운로드하세요.\n  - Windows와 AMD/Intel GPU를 사용하는 분은 ONNX(cpu, DirectML), PyTorch(cpu, cuda)를 다운로드하세요 AMD/Intel GPU는 ONNX 모델을 사용할 때만 적용됩니다.\n  - 그 외 GPU도 PyTorch, Onnxruntime가 지원할 경우에만 적용됩니다.\n  - Windows에서 GPU를 사용하지 않는 분은 ONNX(cpu, cuda), PyTorch(cpu, cuda)를 다운로드하세요.\n\n- Windows 버전은 다운로드한 zip 파일의 압축을 풀고 `start_http.bat`를 실행하세요.\n\n- Mac 버전은 다운로드한 파일을 풀고 `startHttp.command`를 실행하세요. 확인되지 않은 개발자 메시지가 나오면 다시 control 키를 누르고 클릭해 실행하세요(or 오른쪽 클릭으로 실행하세요).\n\n- 처음 실행할 때는 인터넷으로 여러 데이터를 다운로드합니다. 다운로드할 때 시간이 좀 걸릴 수 있습니다. 다운로드가 완료되면 브라우저가 실행됩니다.\n\n- 원격으로 접속할 때는 http 대신 https `.bat` 파일(win)、`.command` 파일(mac)을 실행하세요.\n\n- DDPS-SVC의 encoder는 hubert-soft만 지원합니다.\n\n\n## (2) Docker나 Anaconda 등으로 구축된 개발 환경에서 사용\n\n이 리포지토리를 클론해 사용할 수 있습니다. Windows에서는 WSL2 환경 구축이 필수입니다. 또한, WSL2 상에 Docker나 Anaconda 등의 가상환경 구축이 필요합니다. Mac에서는 Anaconda 등의 Python 가상환경 구축이 필요합니다. 사전 준비가 필요하지만, 많은 환경에서 이 방법이 가장 빠르게 작동합니다. **<font color=\"red\"> GPU가 없어도 나름 최근 출시된 CPU가 있다면 충분히 작동할 가능성이 있습니다</font>(아래 실시간성 항목 참조)**.\n\n[WSL2와 Docker 설치 설명 영상](https://youtu.be/POo_Cg0eFMU)\n\n[WSL2와 Anaconda 설치 설명 영상](https://youtu.be/fba9Zhsukqw)\n\nDocker에서 실행은 [Docker를 사용](docker_vcclient/README_ko.md)을 참고해 서버를 구동하세요.\n\nAnaconda 가상 환경에서 실행은 [서버 개발자용 문서](README_dev_ko.md)를 참고해 서버를 구동하세요.\n\n# 문제 해결법\n\n- [통신편](tutorials/trouble_shoot_communication_ko.md)\n\n\n# 개발자 서명에 대하여\n\n이 소프트웨어는 개발자 서명이 없습니다. 本ソフトウェアは開発元の署名しておりません。下記のように警告が出ますが、コントロールキーを押しながらアイコンをクリックすると実行できるようになります。これは Apple のセキュリティポリシーによるものです。実行は自己責任となります。\n\n![image](https://user-images.githubusercontent.com/48346627/212567711-c4a8d599-e24c-4fa3-8145-a5df7211f023.png)\n(이미지 번역: ctrl을 누른 채로 클릭)\n\n# 감사의 말\n\n- [立ちずんだもん素材](https://seiga.nicovideo.jp/seiga/im10792934)\n- [いらすとや](https://www.irasutoya.com/)\n- [つくよみちゃん](https://tyc.rei-yumesaki.net/)\n\n```\n  이 소프트웨어의 음성 합성에는 무료 소재 캐릭터 「つくよみちゃん(츠쿠요미 짱)」이 무료 공개하고 있는 음성 데이터를 사용했습니다.■츠쿠요미 짱 말뭉치(CV.夢前黎)\n  https://tyc.rei-yumesaki.net/material/corpus/\n  © Rei Yumesaki\n```\n\n- [あみたろの声素材工房](https://amitaro.net/)\n- [れぷりかどーる](https://kikyohiroto1227.wixsite.com/kikoto-utau)\n\n# 이용약관\n\n- 실시간 음성 변환기 츠쿠요미 짱은 츠쿠요미 짱 말뭉치 이용약관에 따라 다음과 같은 목적으로 변환 후 음성을 사용하는 것을 금지합니다.\n\n```\n\n■사람을 비판·공격하는 행위. (\"비판·공격\"의 정의는 츠쿠요미 짱 캐릭터 라이센스에 준합니다)\n\n■특정 정치적 입장·종교·사상에 대한 찬반을 논하는 행위.\n\n■자극적인 표현물을 무분별하게 공개하는 행위.\n\n■타인에게 2차 창작(소재로서의 활용)을 허가하는 형태로 공개하는 행위.\n※감상용 작품으로서 배포·판매하는 건 문제없습니다.\n```\n\n- 실시간 음성 변환기 아미타로는 あみたろの声素材工房(아미타로의 음성 소재 공방)의 다음 이용약관에 따릅니다. 자세한 내용은 [이곳](https://amitaro.net/voice/faq/#index_id6)에 있습니다.\n\n```\n아미타로의 음성 소재나 말뭉치 음성으로 음성 모델을 만들거나, 음성 변환기나 말투 변환기 등을 사용해 본인 목소리를 아미타로의 목소리로 변환해 사용하는 것도 괜찮습니다.\n\n단, 그 경우에는 반드시 아미타로(혹은 코하루네 아미)의 음성으로 변환한 것을 명시하고, 아미타로(및 코하루네 아미)가 말하는 것이 아님을 누구나 알 수 있도록 하십시오.\n또한 아미타로의 음성으로 말하는 내용은 음성 소재 이용약관의 범위 내에서만 사용해야 하며, 민감한 발언은 삼가십시오.\n```\n\n- 실시간 음성 변환기 키코토 마히로는 れぷりかどーる(레플리카 돌)의 이용약관에 따릅니다. 자세한 내용은 [이곳](https://kikyohiroto1227.wixsite.com/kikoto-utau/ter%EF%BD%8Ds-of-service)에 있습니다.\n\n# 면책 사항\n\n이 소프트웨어의 사용 또는 사용 불능으로 인해 발생한 직접 손해·간접 손해·파생적 손해·결과적 손해 또는 특별 손해에 대해 모든 책임을 지지 않습니다.\n\n# (1) 레코더(트레이닝용 음성 녹음 앱)\n\nMMVC 트레이닝용 음성을 간단하게 녹음할 수 있는 앱입니다.\nGithub Pages에서 실행할 수 있어서 브라우저만 있으면 다양한 플랫폼에서 사용할 수 있습니다.\n녹음한 데이터는 브라우저에 저장됩니다. 외부로 유출되지 않습니다.\n\n[녹음 앱 on Github Pages](https://w-okada.github.io/voice-changer/)\n\n[설명 영상](https://youtu.be/s_GirFEGvaA)\n\n# 이전 버전\n\n| Version    | OS  | 프레임워크                        | link                                                                                           | 지원 VC                                                                       | 파일 크기 |\n| ---------- | --- | --------------------------------- | ---------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | --------- |\n| v.1.5.2.9e | mac | ONNX(cpu), PyTorch(cpu,mps)       | [normal](https://drive.google.com/uc?id=1W0d7I7619PcO7kjb1SPXp6MmH5Unvd78&export=download) \\*1 | MMVC v.1.5.x, MMVC v.1.3.x, so-vits-svc 4.0, RVC                              | 796MB     |\n|            | win | ONNX(cpu,cuda), PyTorch(cpu,cuda) | [normal](https://drive.google.com/uc?id=1tmTMJRRggS2Sb4goU-eHlRvUBR88RZDl&export=download) \\*1 | MMVC v.1.5.x, MMVC v.1.3.x, so-vits-svc 4.0, so-vits-svc 4.0v2, RVC, DDSP-SVC | 2872MB    |\n| v.1.5.3.1  | mac | ONNX(cpu), PyTorch(cpu,mps)       | [normal](https://drive.google.com/uc?id=1oswF72q_cQQeXhIn6W275qLnoBAmcrR_&export=download) \\*1 | MMVC v.1.5.x, MMVC v.1.3.x, so-vits-svc 4.0, RVC                              | 796MB     |\n|            | win | ONNX(cpu,cuda), PyTorch(cpu,cuda) | [normal](https://drive.google.com/uc?id=1AWjDhW4w2Uljp1-9P8YUJBZsIlnhkJX2&export=download) \\*1 | MMVC v.1.5.x, MMVC v.1.3.x, so-vits-svc 4.0, so-vits-svc 4.0v2, RVC, DDSP-SVC | 2872MB    |\n\n# For Contributor\n\n이 리포지토리는 [CLA](https://raw.githubusercontent.com/w-okada/voice-changer/master/LICENSE-CLA)를 설정했습니다.\n"
        },
        {
          "name": "README_ru.md",
          "type": "blob",
          "size": 9.3837890625,
          "content": "[Японский](/README_ja.md) [Корейский](/README_ko.md) [Английский](/README_en.md)\n\n## Что нового!\n- Мы выпустили продукт-сестру - клиент Text To Speech.\n  - Вы можете насладиться генерацией голоса через простой интерфейс.\n  - Подробнее [здесь](https://github.com/w-okada/ttsclient).\n- Код тренировки Beatrice V2 теперь доступен!\n  - [Репозиторий кода тренировки](https://huggingface.co/fierce-cats/beatrice-trainer)\n  - [Версия для Colab](https://github.com/w-okada/beatrice-trainer-colab)\n- v.2.0.70-beta (only for m1 mac)\n  - [HERE](https://github.com/w-okada/voice-changer/tree/v.2)\n  - new feature:\n    -  В версии VCClient для Mac на базе M1 теперь поддерживается Beatrice v2 beta.1.\n- v.2.0.69-beta (only for win)\n  - [HERE](https://github.com/w-okada/voice-changer/tree/v.2)\n  - Исправления ошибок:\n    - Исправлена ошибка, из-за которой кнопка запуска не отображалась в случае некоторых исключений\n    - Настроен выходной буфер для режима серверного устройства\n    - Исправлена ошибка, при которой изменялась частота дискретизации при изменении настроек в режиме серверного устройства\n    - Исправлена ошибка при использовании японского hubert\n  - Прочее:\n    - Добавлен фильтр API хоста (выделено) для режима серверного устройства\n- v.2.0.65-beta\n  - [HERE](https://github.com/w-okada/voice-changer/tree/v.2)\n  - new feature: We have supported Beatrice v2 beta.1, enabling even higher quality voice conversion.\n\n# Что такое VC Клиент\n\n1. Это клиентское ПО для выполнения преобразования голоса в реальном времени с использованием различных AI для преобразования голоса. Поддерживаемые AI:\n   - [MMVC](https://github.com/isletennos/MMVC_Trainer) (только v1)\n   - [so-vits-svc](https://github.com/svc-develop-team/so-vits-svc) (только v1)\n   - [RVC (Retrieval-based Voice Conversion)](https://github.com/liujing04/Retrieval-based-Voice-Conversion-WebUI)\n   - [DDSP-SVC](https://github.com/yxlllc/DDSP-SVC) (только v1)\n   - [Beatrice JVS Corpus Edition](https://prj-beatrice.com/) * экспериментальный * (не по лицензии MIT, см. [readme](https://github.com/w-okada/voice-changer/blob/master/server/voice_changer/Beatrice/)), только для Windows, зависит от процессора (только v1)\n   - [Beatrice v2](https://prj-beatrice.com/) (только v2)\n\n2. Распределение нагрузки между разными ПК\n   Реализация преобразования голоса работает по схеме \"сервер-клиент\". Вы можете запустить сервер MMVC на отдельном ПК для минимизации влияния на другие ресурсоёмкие процессы, такие как стриминг.\n\n![image](https://user-images.githubusercontent.com/48346627/206640768-53f6052d-0a96-403b-a06c-6714a0b7471d.png)\n\n3. Кроссплатформенная совместимость\n   Поддержка Windows, Mac (включая Apple Silicon M1), Linux и Google Colaboratory.\n\n# Как использовать\n\nЭто приложение для изменения голоса с использованием MMVC и so-vits-svc.\n\nЕсть два основных способа использования, в порядке сложности:\n\n- Использование готового исполняемого файла\n- Настройка окружения с Docker или Anaconda\n\n## (1) Использование готовых исполняемых файлов\n\n- Вы можете скачать и запустить исполняемые файлы.\n\n- Смотрите [здесь](tutorials/tutorial_rvc_en_latest.md) для получения руководства. ([устранение неполадок](https://github.com/w-okada/voice-changer/blob/master/tutorials/trouble_shoot_communication_ja.md))\n\n- Теперь попробовать можно на [Google Colaboratory](https://github.com/w-okada/voice-changer/tree/v.2/w_okada's_Voice_Changer_version_2_x.ipynb) (требуется аккаунт ngrok). Вы можете запустить его через кнопку \"Открыть в Colab\" в верхнем левом углу.\n\n<img src=\"https://github.com/w-okada/voice-changer/assets/48346627/3f092e2d-6834-42f6-bbfd-7d389111604e\" width=\"400\" height=\"150\">\n\n- Мы предлагаем версии для Windows и Mac на [hugging face](https://huggingface.co/wok000/vcclient000/tree/main)\n- v2 для Windows\n  - Пожалуйста, скачайте и используйте `vcclient_win_std_xxx.zip`. Преобразование голоса можно выполнять с использованием мощного процессора без GPU или с использованием DirectML для GPU (AMD, Nvidia). v2 поддерживает как torch, так и onnx.\n  - Если у вас Nvidia GPU, скачайте `vcclient_win_cuda_xxx.zip` для более быстрого преобразования.\n- v2 для Mac (Apple Silicon)\n  - Пожалуйста, скачайте и используйте `vcclient_mac_xxx.zip`.\n- v1\n  - Для Windows с Nvidia GPU скачайте ONNX (cpu, cuda), PyTorch (cpu, cuda).\n  - Для Windows с AMD/Intel GPU скачайте ONNX (cpu, DirectML) и PyTorch (cpu, cuda). AMD/Intel GPU поддерживаются только для ONNX моделей.\n\n- Для пользователей Windows: после распаковки zip-файла запустите соответствующий `start_http.bat` файл.\n\n- Для Mac: после распаковки zip-файла дважды щёлкните на `startHttp.command`. Если появится сообщение о невозможности проверки разработчика, нажмите Ctrl и повторно запустите.\n\n- Если подключаетесь удалённо, используйте `.command` (Mac) или `.bat` (Windows) файл с https вместо http.\n\n- Энкодер DDPS-SVC поддерживает только hubert-soft.\n\n- [Скачать с hugging face](https://huggingface.co/wok000/vcclient000/tree/main)\n\n## (2) Использование после настройки окружения с Docker или Anaconda\n\nКлонируйте этот репозиторий и используйте его. Для Windows требуется настройка WSL2. Для Mac нужно настроить виртуальные среды Python, например Anaconda. Этот метод обеспечивает наивысшую скорость в большинстве случаев. **<font color=\"red\"> Даже без GPU можно получить достаточную производительность на современном процессоре </font>(смотрите раздел о производительности в реальном времени ниже)**.\n\n[Видео-инструкция по установке WSL2 и Docker](https://youtu.be/POo_Cg0eFMU)\n\n[Видео-инструкция по установке WSL2 и Anaconda](https://youtu.be/fba9Zhsukqw)\n\nДля запуска Docker смотрите [start docker](docker_vcclient/README_en.md).\n\nДля запуска на Anaconda venv смотрите [руководство разработчика](README_dev_ru.md).\n\nДля запуска на Linux с AMD GPU смотрите [руководство](tutorials/tutorial_anaconda_amd_rocm.md).\n\n# Подпись программного обеспечения\n\nЭто ПО не подписано разработчиком. Появится предупреждение, но его можно запустить, нажав на иконку с удержанием клавиши Ctrl. Это связано с политикой безопасности Apple. Использование ПО на ваш риск.\n\n![image](https://user-images.githubusercontent.com/48346627/212567711-c4a8d599-e24c-4fa3-8145-a5df7211f023.png)\n\nhttps://user-images.githubusercontent.com/48346627/212569645-e30b7f4e-079d-4504-8cf8-7816c5f40b00.mp4\n\n# Благодарности\n\n- [Материалы Tachizunda-mon](https://seiga.nicovideo.jp/seiga/im10792934)\n- [Irasutoya](https://www.irasutoya.com/)\n- [Tsukuyomi-chan](https://tyc.rei-yumesaki.net)\n\n> Это ПО использует голосовые данные бесплатного материала персонажа \"Цукуёми-тян\", предоставленного CV. Юмесаки Рэй.\n>\n> - Корпус Цукуёми-тян (CV. Юмесаки Рэй)\n>\n> https://tyc.rei-yumesaki.net/material/corpus/\n>\n> Авторское право. Юмесаки Рэй, Все права защищены.\n\n"
        },
        {
          "name": "Realtime_Voice_Changer_on_Colab.ipynb",
          "type": "blob",
          "size": 8.759765625,
          "content": "{\n  \"cells\": [\n\t{\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"view-in-github\",\n        \"colab_type\": \"text\"\n      },\n      \"source\": [\n        \"<a href=\\\"https://colab.research.google.com/github/w-okada/voice-changer/blob/master/Realtime_Voice_Changer_on_Colab.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"### [w-okada's Voice Changer](https://github.com/w-okada/voice-changer) | **Colab**\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"## **⬇ VERY IMPORTANT ⬇**\\n\",\n        \"\\n\",\n        \"You can use the following settings for better results:\\n\",\n        \"\\n\",\n        \"If you're using a index: `f0: RMVPE_ONNX | Chunk: 112 or higher | Extra: 8192`<br>\\n\",\n        \"If you're not using a index: `f0: RMVPE_ONNX | Chunk: 96 or higher | Extra: 16384`<br>\\n\",\n        \"**Don't forget to select a T4 GPU in the GPU field, <b>NEVER</b> use CPU!\\n\",\n        \"> Seems that PTH models performance better than ONNX for now, you can still try ONNX models and see if it satisfies you\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"*You can always [click here](https://github.com/YunaOneeChan/Voice-Changer-Settings) to check if these settings are up-to-date*\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"### <font color=red>⬇ Always use Colab GPU! (**IMPORTANT!**) ⬇</font>\\n\",\n        \"You need to use a Colab GPU so the Voice Changer can work faster and better\\\\\\n\",\n        \"Use the menu above and click on **Runtime** » **Change runtime** » **Hardware acceleration** to select a GPU (**T4 is the free one**)\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"**Credits**<br>\\n\",\n        \"Realtime Voice Changer by [w-okada](https://github.com/w-okada)<br>\\n\",\n        \"Notebook files updated by [rafacasari](https://github.com/Rafacasari)<br>\\n\",\n        \"Recommended settings by [YunaOneeChan](https://github.com/YunaOneeChan)\\n\",\n        \"\\n\",\n        \"**Need help?** [AI Hub Discord](https://discord.gg/aihub) » ***#help-realtime-vc***\\n\",\n        \"\\n\",\n        \"---\"\n      ],\n      \"metadata\": {\n        \"id\": \"Lbbmx_Vjl0zo\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title Clone repository and install dependencies\\n\",\n        \"# @markdown This first step will download the latest version of Voice Changer and install the dependencies. **It can take some time to complete.**\\n\",\n        \"%cd /content/\\n\",\n        \"\\n\",\n        \"!pip install colorama --quiet\\n\",\n        \"from colorama import Fore, Style\\n\",\n        \"import os\\n\",\n        \"\\n\",\n        \"print(f\\\"{Fore.CYAN}> Cloning the repository...{Style.RESET_ALL}\\\")\\n\",\n        \"!git clone https://github.com/w-okada/voice-changer.git --quiet\\n\",\n        \"print(f\\\"{Fore.GREEN}> Successfully cloned the repository!{Style.RESET_ALL}\\\")\\n\",\n        \"%cd voice-changer/server/\\n\",\n        \"\\n\",\n        \"print(f\\\"{Fore.CYAN}> Installing libportaudio2...{Style.RESET_ALL}\\\")\\n\",\n        \"!apt-get -y install libportaudio2 -qq\\n\",\n        \"\\n\",\n        \"print(f\\\"{Fore.CYAN}> Installing pre-dependencies...{Style.RESET_ALL}\\\")\\n\",\n        \"# Install dependencies that are missing from requirements.txt and pyngrok\\n\",\n        \"!pip install faiss-gpu fairseq pyngrok --quiet\\n\",\n        \"!pip install pyworld --no-build-isolation --quiet\\n\",\n        \"print(f\\\"{Fore.CYAN}> Installing dependencies from requirements.txt...{Style.RESET_ALL}\\\")\\n\",\n        \"!pip install -r requirements.txt --quiet\\n\",\n        \"\\n\",\n        \"print(f\\\"{Fore.GREEN}> Successfully installed all packages!{Style.RESET_ALL}\\\")\"\n      ],\n      \"metadata\": {\n        \"id\": \"86wTFmqsNMnD\",\n        \"cellView\": \"form\",\n        \"_kg_hide-output\": false,\n        \"execution\": {\n          \"iopub.status.busy\": \"2023-09-14T04:01:17.308284Z\",\n          \"iopub.execute_input\": \"2023-09-14T04:01:17.308682Z\",\n          \"iopub.status.idle\": \"2023-09-14T04:08:08.475375Z\",\n          \"shell.execute_reply.started\": \"2023-09-14T04:01:17.308652Z\",\n          \"shell.execute_reply\": \"2023-09-14T04:08:08.473827Z\"\n        },\n        \"trusted\": true\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title Start Server **using ngrok**\\n\",\n        \"# @markdown This cell will start the server, the first time that you run it will download the models, so it can take a while (~1-2 minutes)\\n\",\n        \"\\n\",\n        \"# @markdown ---\\n\",\n        \"# @markdown You'll need a ngrok account, but <font color=green>**it's free**</font> and easy to create!\\n\",\n        \"# @markdown ---\\n\",\n        \"# @markdown **1** - Create a <font color=green>**free**</font> account at [ngrok](https://dashboard.ngrok.com/signup) or **login with Google/Github account**\\\\\\n\",\n        \"# @markdown **2** - If you didn't logged in with Google/Github, you will need to **verify your e-mail**!\\\\\\n\",\n        \"# @markdown **3** - Click [this link](https://dashboard.ngrok.com/get-started/your-authtoken) to get your auth token, and place it here:\\n\",\n        \"Token = '' # @param {type:\\\"string\\\"}\\n\",\n        \"# @markdown **4** - *(optional)* Change to a region near to you or keep at United States if increase latency\\\\\\n\",\n        \"# @markdown `Default Region: us - United States (Ohio)`\\n\",\n        \"Region = \\\"us - United States (Ohio)\\\" # @param [\\\"ap - Asia/Pacific (Singapore)\\\", \\\"au - Australia (Sydney)\\\",\\\"eu - Europe (Frankfurt)\\\", \\\"in - India (Mumbai)\\\",\\\"jp - Japan (Tokyo)\\\",\\\"sa - South America (Sao Paulo)\\\", \\\"us - United States (Ohio)\\\"]\\n\",\n        \"\\n\",\n        \"#@markdown **5** - *(optional)* Other options:\\n\",\n        \"ClearConsole = True  # @param {type:\\\"boolean\\\"}\\n\",\n        \"\\n\",\n        \"# ---------------------------------\\n\",\n        \"# DO NOT TOUCH ANYTHING DOWN BELOW!\\n\",\n        \"# ---------------------------------\\n\",\n        \"\\n\",\n        \"%cd /content/voice-changer/server\\n\",\n        \"\\n\",\n        \"from pyngrok import conf, ngrok\\n\",\n        \"MyConfig = conf.PyngrokConfig()\\n\",\n        \"MyConfig.auth_token = Token\\n\",\n        \"MyConfig.region = Region[0:2]\\n\",\n        \"#conf.get_default().authtoken = Token\\n\",\n        \"#conf.get_default().region = Region\\n\",\n        \"conf.set_default(MyConfig);\\n\",\n        \"\\n\",\n        \"import subprocess, threading, time, socket, urllib.request\\n\",\n        \"PORT = 8000\\n\",\n        \"\\n\",\n        \"from pyngrok import ngrok\\n\",\n        \"ngrokConnection = ngrok.connect(PORT)\\n\",\n        \"public_url = ngrokConnection.public_url\\n\",\n        \"\\n\",\n        \"from IPython.display import clear_output\\n\",\n        \"\\n\",\n        \"def wait_for_server():\\n\",\n        \"    while True:\\n\",\n        \"        time.sleep(0.5)\\n\",\n        \"        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n\",\n        \"        result = sock.connect_ex(('127.0.0.1', PORT))\\n\",\n        \"        if result == 0:\\n\",\n        \"            break\\n\",\n        \"        sock.close()\\n\",\n        \"    if ClearConsole:\\n\",\n        \"        clear_output()\\n\",\n        \"    print(\\\"--------- SERVER READY! ---------\\\")\\n\",\n        \"    print(\\\"Your server is available at:\\\")\\n\",\n        \"    print(public_url)\\n\",\n        \"    print(\\\"---------------------------------\\\")\\n\",\n        \"\\n\",\n        \"threading.Thread(target=wait_for_server, daemon=True).start()\\n\",\n        \"\\n\",\n        \"!python3 MMVCServerSIO.py \\\\\\n\",\n        \"  -p {PORT} \\\\\\n\",\n        \"  --https False \\\\\\n\",\n        \"  --content_vec_500 pretrain/checkpoint_best_legacy_500.pt \\\\\\n\",\n        \"  --content_vec_500_onnx pretrain/content_vec_500.onnx \\\\\\n\",\n        \"  --content_vec_500_onnx_on true \\\\\\n\",\n        \"  --hubert_base pretrain/hubert_base.pt \\\\\\n\",\n        \"  --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\\\\n\",\n        \"  --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\\\\n\",\n        \"  --nsf_hifigan pretrain/nsf_hifigan/model \\\\\\n\",\n        \"  --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\\\\n\",\n        \"  --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\\\\n\",\n        \"  --rmvpe pretrain/rmvpe.pt \\\\\\n\",\n        \"  --model_dir model_dir \\\\\\n\",\n        \"  --samples samples.json\\n\",\n        \"\\n\",\n        \"ngrok.disconnect(ngrokConnection.public_url)\"\n      ],\n      \"metadata\": {\n        \"id\": \"lLWQuUd7WW9U\",\n        \"cellView\": \"form\",\n        \"_kg_hide-input\": false,\n        \"scrolled\": true,\n        \"trusted\": true\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    }\n  ],\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": [],\n      \"private_outputs\": true,\n\t  \"include_colab_link\": true,\n      \"gpuType\": \"T4\",\n      \"collapsed_sections\": [\n        \"iuf9pBHYpTn-\"\n      ]\n    },\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"name\": \"python\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0\n}\n"
        },
        {
          "name": "client",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker_folder",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker_trainer",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker_vcclient",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs_i18n",
          "type": "tree",
          "content": null
        },
        {
          "name": "package.json",
          "type": "blob",
          "size": 1.3154296875,
          "content": "{\n  \"name\": \"voice-changer\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"build:docker\": \"date +%Y%m%d%H%M%S > docker/dummy && DOCKER_BUILDKIT=1 docker build -f docker/Dockerfile docker/ -t voice-changer\",\n    \"build:docker:onnx\": \"DOCKER_BUILDKIT=1 docker build -f docker_onnx/Dockerfile docker/ -t onnx-converter\",\n    \"build:docker:trainer\": \"date +%Y%m%d%H%M%S > docker_trainer/dummy && DOCKER_BUILDKIT=1 docker build -f docker_trainer/Dockerfile docker_trainer/ -t trainer\",\n    \"build:docker:vcclient\": \"date +%Y%m%d%H%M%S > docker_vcclient/dummy && DOCKER_BUILDKIT=1 docker build -f docker_vcclient/Dockerfile docker_vcclient/ -t vcclient\",\n    \"push:docker\": \"bash script/001_pushDocker.sh\",\n    \"push:docker:trainer\": \"bash script/002_pushDockerTrainer.sh\",\n    \"push:docker:vcclient\": \"bash script/003_pushDockerVCClient.sh\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/w-okada/voice-changer.git\"\n  },\n  \"keywords\": [\n    \"voice conversion\"\n  ],\n  \"author\": \"wataru.okada@flect.co.jp\",\n  \"license\": \"ISC\",\n  \"bugs\": {\n    \"url\": \"https://github.com/w-okada/voice-changer/issues\"\n  },\n  \"homepage\": \"https://github.com/w-okada/voice-changer#readme\",\n  \"devDependencies\": {\n    \"npm-run-all\": \"^4.1.5\"\n  }\n}\n"
        },
        {
          "name": "recorder",
          "type": "tree",
          "content": null
        },
        {
          "name": "script",
          "type": "tree",
          "content": null
        },
        {
          "name": "server",
          "type": "tree",
          "content": null
        },
        {
          "name": "signatures",
          "type": "tree",
          "content": null
        },
        {
          "name": "start2.sh",
          "type": "blob",
          "size": 2.099609375,
          "content": "#!/bin/bash\nset -eu\n\nDOCKER_IMAGE=dannadori/voice-changer:20230129_152955\n#DOCKER_IMAGE=voice-changer\n\nif [ $# = 0 ]; then\n    echo \"\n    usage:\n        $0 <MODE> <params...>\n        MODE: select one of ['TRAIN', 'MMVC']\n    \" >&2\n    exit 1\nfi\n\nMODE=$1\n\n### DEFAULT VAR ###\nDEFAULT_EX_PORT=18888\nDEFAULT_EX_TB_PORT=16006\nDEFAULT_USE_GPU=on # on|off\n# DEFAULT_VERBOSE=off # on|off\n\n### ENV VAR ###\nEX_PORT=${EX_PORT:-${DEFAULT_EX_PORT}}\nEX_TB_PORT=${EX_TB_PORT:-${DEFAULT_EX_TB_PORT}}\nUSE_GPU=${USE_GPU:-${DEFAULT_USE_GPU}}\n# VERBOSE=${VERBOSE:-${DEFAULT_VERBOSE}}\n\n#echo $EX_PORT $USE_GPU $VERBOSE\n\n\n### \nif [ \"${MODE}\" = \"TRAIN\" ]; then\n    echo \"トレーニングを開始します\"\n\n    docker run -it --rm --gpus all --shm-size=128M \\\n        -v `pwd`/work_dir/logs:/voice-changer-internal/voice-change-service/MMVC_Trainer/logs \\\n        -v `pwd`/work_dir/dataset:/voice-changer-internal/voice-change-service/MMVC_Trainer/dataset \\\n        -v `pwd`/work_dir/info:/voice-changer-internal/voice-change-service/info \\\n        -e LOCAL_UID=$(id -u $USER) \\\n        -e LOCAL_GID=$(id -g $USER) \\\n        -e EX_PORT=${EX_PORT} -e EX_TB_PORT=${EX_TB_PORT} \\\n        -e EX_IP=\"`hostname -I`\" \\\n        -p ${EX_PORT}:18888 -p ${EX_TB_PORT}:6006 \\\n        $DOCKER_IMAGE \"$@\"\n\n\nelif [ \"${MODE}\" = \"MMVC\" ]; then\n    if [ \"${USE_GPU}\" = \"on\" ]; then\n        echo \"MMVCを起動します(with gpu)\"\n\n        docker run -it --rm --gpus all --shm-size=128M \\\n        -v `pwd`/vc_resources:/resources \\\n        -e LOCAL_UID=$(id -u $USER) \\\n        -e LOCAL_GID=$(id -g $USER) \\\n        -e EX_IP=\"`hostname -I`\" \\\n        -e EX_PORT=${EX_PORT} \\\n        -p ${EX_PORT}:18888 \\\n        $DOCKER_IMAGE \"$@\"\n    else\n        echo \"MMVCを起動します(only cpu)\"\n        docker run -it --rm --shm-size=128M \\\n        -v `pwd`/vc_resources:/resources \\\n        -e LOCAL_UID=$(id -u $USER) \\\n        -e LOCAL_GID=$(id -g $USER) \\\n        -e EX_IP=\"`hostname -I`\" \\\n        -e EX_PORT=${EX_PORT} \\\n        -p ${EX_PORT}:18888 \\\n        $DOCKER_IMAGE \"$@\"\n    fi\nelse\n    echo \"\nusage: \n    $0 <MODE> <params...>\n    MODE: select one of ['TRAIN', 'MMVC']\n\" >&2\nfi\n\n\n"
        },
        {
          "name": "start_docker.sh",
          "type": "blob",
          "size": 2.4453125,
          "content": "#!/bin/bash\nset -eu\n\nDOCKER_IMAGE=dannadori/vcclient:20230826_211406\n#DOCKER_IMAGE=vcclient\n\n### DEFAULT VAR ###\nDEFAULT_EX_PORT=18888\nDEFAULT_USE_GPU=on # on|off\nDEFAULT_USE_LOCAL=off # on|off\n### ENV VAR ###\nEX_PORT=${EX_PORT:-${DEFAULT_EX_PORT}}\nUSE_GPU=${USE_GPU:-${DEFAULT_USE_GPU}}\nUSE_LOCAL=${USE_LOCAL:-${DEFAULT_USE_LOCAL}}\n\nif [ \"${USE_LOCAL}\" = \"on\" ]; then\n    DOCKER_IMAGE=vcclient\nfi\n\nif [ \"${USE_GPU}\" = \"on\" ]; then\n    echo \"VC Client start...(with gpu)\"\n    docker run -it --rm --gpus all --shm-size=1024M \\\n    -e EX_IP=\"`hostname -I`\" \\\n    -e EX_PORT=${EX_PORT} \\\n    -e LOCAL_UID=$(id -u $USER) \\\n    -e LOCAL_GID=$(id -g $USER) \\\n    -v `pwd`/docker_folder/model_dir:/voice-changer/server/model_dir \\\n    -v `pwd`/docker_folder/pretrain:/voice-changer/server/pretrain \\\n    -p ${EX_PORT}:18888 \\\n    $DOCKER_IMAGE -p 18888 --https true \\\n        --content_vec_500 pretrain/checkpoint_best_legacy_500.pt  \\\n        --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n        --content_vec_500_onnx_on true \\\n        --hubert_base pretrain/hubert_base.pt \\\n        --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n        --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n        --nsf_hifigan pretrain/nsf_hifigan/model \\\n        --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n        --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n        --rmvpe pretrain/rmvpe.pt \\\n        --model_dir model_dir \\\n        --samples samples.json\nelse\n    echo \"VC Client start...(cpu)\"\n    docker run -it --rm --shm-size=1024M \\\n    -e EX_IP=\"`hostname -I`\" \\\n    -e EX_PORT=${EX_PORT} \\\n    -e LOCAL_UID=$(id -u $USER) \\\n    -e LOCAL_GID=$(id -g $USER) \\\n    -v `pwd`/docker_folder/model_dir:/voice-changer/server/model_dir \\\n    -v `pwd`/docker_folder/pretrain:/voice-changer/server/pretrain \\\n    -p ${EX_PORT}:18888 \\\n    $DOCKER_IMAGE -p 18888 --https true \\\n        --content_vec_500 pretrain/checkpoint_best_legacy_500.pt  \\\n        --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n        --content_vec_500_onnx_on true \\\n        --hubert_base pretrain/hubert_base.pt \\\n        --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n        --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n        --nsf_hifigan pretrain/nsf_hifigan/model \\\n        --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n        --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n        --rmvpe pretrain/rmvpe.pt \\\n        --model_dir model_dir \\\n        --samples samples.json\nfi\n\n\n"
        },
        {
          "name": "start_v0.1.sh",
          "type": "blob",
          "size": 9.6748046875,
          "content": "#!/bin/bash\nset -eu\n\nDOCKER_IMAGE=dannadori/voice-changer:20221028_220714\n#DOCKER_IMAGE=voice-changer\n\n\nMODE=$1\nPARAMS=${@:2:($#-1)}\n\n### DEFAULT VAR ###\nDEFAULT_EX_PORT=18888\nDEFAULT_USE_GPU=on # on|off\nDEFAULT_VERBOSE=off # on|off\n\n### ENV VAR ###\nEX_PORT=${EX_PORT:-${DEFAULT_EX_PORT}}\nUSE_GPU=${USE_GPU:-${DEFAULT_USE_GPU}}\nVERBOSE=${VERBOSE:-${DEFAULT_VERBOSE}}\n\n#echo $EX_PORT $USE_GPU $VERBOSE\n\n### INTERNAL SETTING ###\nTENSORBOARD_PORT=6006\nSIO_PORT=8080\n\n\n### \nif [ \"${MODE}\" = \"MMVC_TRAIN\" ]; then\n    echo \"トレーニングを開始します\"\n\n    docker run -it --gpus all --shm-size=128M \\\n        -v `pwd`/exp/${name}/dataset:/MMVC_Trainer/dataset \\\n        -v `pwd`/exp/${name}/logs:/MMVC_Trainer/logs \\\n        -v `pwd`/exp/${name}/filelists:/MMVC_Trainer/filelists \\\n        -v `pwd`/vc_resources:/resources \\\n        -e LOCAL_UID=$(id -u $USER) \\\n        -e LOCAL_GID=$(id -g $USER) \\\n        -e EX_IP=\"`hostname -I`\" \\\n        -e EX_PORT=${EX_PORT} \\\n        -e VERBOSE=${VERBOSE} \\        \n        -p ${EX_PORT}:6006 $DOCKER_IMAGE \"$@\"\n\nelif [ \"${MODE}\" = \"MMVC\" ]; then\n    if [ \"${USE_GPU}\" = \"on\" ]; then\n        echo \"MMVCを起動します(with gpu)\"\n\n        docker run -it --gpus all --shm-size=128M \\\n        -v `pwd`/vc_resources:/resources \\\n        -e LOCAL_UID=$(id -u $USER) \\\n        -e LOCAL_GID=$(id -g $USER) \\\n        -e EX_IP=\"`hostname -I`\" \\\n        -e EX_PORT=${EX_PORT} \\\n        -e VERBOSE=${VERBOSE} \\\n        -p ${EX_PORT}:8080 $DOCKER_IMAGE \"$@\"\n    else\n        echo \"MMVCを起動します(only cpu)\"\n        docker run -it --shm-size=128M \\\n        -v `pwd`/vc_resources:/resources \\\n        -e LOCAL_UID=$(id -u $USER) \\\n        -e LOCAL_GID=$(id -g $USER) \\\n        -e EX_IP=\"`hostname -I`\" \\\n        -e EX_PORT=${EX_PORT} \\\n        -e VERBOSE=${VERBOSE} \\\n        -p ${EX_PORT}:8080 $DOCKER_IMAGE \"$@\"\n\n        # docker run -it --shm-size=128M \\\n        # -v `pwd`/vc_resources:/resources \\\n        # -e LOCAL_UID=$(id -u $USER) \\\n        # -e LOCAL_GID=$(id -g $USER) \\\n        # -e EX_IP=\"`hostname -I`\" \\\n        # -e EX_PORT=${EX_PORT} \\\n        # -e VERBOSE=${VERBOSE} \\\n        # --entrypoint=\"\" \\\n        # -p ${EX_PORT}:8080 $DOCKER_IMAGE /bin/bash\n\n    fi\n\nelif [ \"${MODE}\" = \"SOFT_VC\" ]; then\n    if [ \"${USE_GPU}\" = \"on\" ]; then\n        echo \"Start Soft-vc\"\n\n        docker run -it --gpus all --shm-size=128M \\\n        -v `pwd`/vc_resources:/resources \\\n        -e LOCAL_UID=$(id -u $USER) \\\n        -e LOCAL_GID=$(id -g $USER) \\\n        -e EX_IP=\"`hostname -I`\" \\\n        -e EX_PORT=${EX_PORT} \\\n        -e VERBOSE=${VERBOSE} \\\n        -p ${EX_PORT}:8080 $DOCKER_IMAGE \"$@\"\n    else\n        echo \"Start Soft-vc withou GPU is not supported\"\n    fi\n\nelse\n    echo \"\nusage: \n    $0 <MODE> <params...>\n    MODE: select one of ['MMVC_TRAIN', 'MMVC', 'SOFT_VC']\n\" >&2\nfi\n\n\n\n\n# echo $EX_PORT\n\n\n# echo \"------\"\n# echo \"$@\"\n# echo \"------\"\n\n# # usage() {\n# #     echo \"\n# # usage: \n# #     For training\n# #         $0 [-t] -n <exp_name> [-b batch_size] [-r] \n# #             -t: トレーニングモードで実行する場合に指定してください。(train)\n# #             -n: トレーニングの名前です。(name)\n# #             -b: バッチサイズです。(batchsize)\n# #             -r: トレーニング再開の場合に指定してください。(resume)\n# #     For changing voice\n# #         $0 [-v] [-c config] [-m model] [-g on/off]\n# #             -v: ボイスチェンジャーモードで実行する場合に指定してください。(voice changer)\n# #             -c: トレーニングで使用したConfigのファイル名です。(config)\n# #             -m: トレーニング済みのモデルのファイル名です。(model)\n# #             -g: GPU使用/不使用。デフォルトはonなのでGPUを使う場合は指定不要。(gpu)\n# #             -p: port番号\n# #     For help\n# #         $0 [-h]\n# #             -h: show this help\n# # \" >&2\n# # }\n# # warn () {\n# #     echo \"! ! ! $1 ! ! !\"\n# #     exit 1\n# # }\n\n\n# # training_flag=false\n# # name=999_exp\n# # batch_size=10\n# # resume_flag=false\n\n# # voice_change_flag=false\n# # config=\n# # model=\n# # gpu=on\n# # port=8080\n# # escape_flag=false\n\n# # # オプション解析\n# # while getopts tn:b:rvc:m:g:p:hx OPT; do\n# #     case $OPT in\n# #     t) \n# #         training_flag=true\n# #         ;;\n# #     n) \n# #         name=\"$OPTARG\"\n# #         ;;\n# #     b) \n# #         batch_size=\"$OPTARG\"\n# #         ;;\n# #     r) \n# #         resume_flag=true\n# #         ;;\n# #     v) \n# #         voice_change_flag=true\n# #         ;;\n# #     c) \n# #         config=\"$OPTARG\"\n# #         ;;\n# #     m) \n# #         model=\"$OPTARG\"\n# #         ;;\n# #     g) \n# #         gpu=\"$OPTARG\"\n# #         ;;\n# #     p) \n# #         port=\"$OPTARG\"\n# #         ;;\n# #     h | \\?) \n# #         usage && exit 1\n# #         ;;\n# #     x)\n# #         escape_flag=true\n# #     esac\n# # done\n\n\n# # # モード解析\n# # if $training_flag && $voice_change_flag; then\n# #     warn \"-t（トレーニングモード） と -v（ボイチェンモード）は同時に指定できません。\"\n# # elif $training_flag; then\n# #     echo \"■■■  ト レ ー ニ ン グ モ ー ド   ■■■\"\n# # elif $voice_change_flag; then\n# #     echo \"■■■  ボ イ チ ェ ン モ ー ド  ■■■\"\n# # elif $escape_flag; then\n# #     /bin/bash\n# # else\n# #     warn \"-t（トレーニングモード） と -v（ボイチェンモード）のいずれかを指定してください。\"\n# # fi\n\n# if [ \"${MODE}\" = \"MMVC_TRAIN_INITIAL\" ]; then\n#     echo \"トレーニングを開始します\"\n# elif [ \"${MODE}\" = \"MMVC\" ]; then\n#     echo \"MMVCを起動します\"\n\n#     docker run -it --gpus all --shm-size=128M \\\n#     -v `pwd`/vc_resources:/resources \\\n#     -e LOCAL_UID=$(id -u $USER) \\\n#     -e LOCAL_GID=$(id -g $USER) \\\n#     -e EX_IP=\"`hostname -I`\" \\\n#     -e EX_PORT=${port} \\\n#     -p ${port}:8080 $DOCKER_IMAGE -v -c ${config} -m ${model}\n\n# elif [ \"${MODE}\" = \"MMVC_VERBOSE\" ]; then\n#     echo \"MMVCを起動します(verbose)\"\n# elif [ \"${MODE}\" = \"MMVC_CPU\" ]; then\n#     echo \"MMVCを起動します(CPU)\"\n# elif [ \"${MODE}\" = \"MMVC_CPU_VERBOSE\" ]; then\n#     echo \"MMVCを起動します(CPU)(verbose)\"\n# elif [ \"${MODE}\" = \"SOFT_VC\" ]; then\n#     echo \"Start Soft-vc\"\n# elif [ \"${MODE}\" = \"SOFT_VC_VERBOSE\" ]; then\n#     echo \"Start Soft-vc(verbose)\"\n# else\n#     echo \"\n# usage: \n#     $0 <MODE> <params...>\n#     EX_PORT: \n#     MODE: one of ['MMVC_TRAIN', 'MMVC', 'SOFT_VC']\n\n#     For 'MMVC_TRAIN':\n#         $0 MMVC_TRAIN_INITIAL -n <exp_name> [-b batch_size] [-r] \n#             -n: トレーニングの名前です。(name)\n#             -b: バッチサイズです。(batchsize)\n#             -r: トレーニング再開の場合に指定してください。(resume)\n#     For 'MMVC'\n#         $0 MMVC [-c config] [-m model] [-g on/off] [-p port] [-v]\n#             -c: トレーニングで使用したConfigのファイル名です。(config)\n#             -m: トレーニング済みのモデルのファイル名です。(model)\n#             -g: GPU使用/不使用。デフォルトはonなのでGPUを使う場合は指定不要。(gpu)\n#             -p: Docker からExposeするport番号\n#             -v: verbose\n#     For 'SOFT_VC'\n#         $0 SOFT_VC [-c config] [-m model] [-g on/off]\n#             -p: port exposed from docker container.\n#             -v: verbose\n# \" >&2\n# fi\n\n\n\n# # if $training_flag; then\n# #     if $resume_flag; then\n# #         echo \"トレーニングを再開します\"\n# #         docker run -it --gpus all --shm-size=128M \\\n# #             -v `pwd`/exp/${name}/dataset:/MMVC_Trainer/dataset \\\n# #             -v `pwd`/exp/${name}/logs:/MMVC_Trainer/logs \\\n# #             -v `pwd`/exp/${name}/filelists:/MMVC_Trainer/filelists \\\n# #             -v `pwd`/vc_resources:/resources \\\n# #             -e LOCAL_UID=$(id -u $USER) \\\n# #             -e LOCAL_GID=$(id -g $USER) \\\n# #             -p ${TENSORBOARD_PORT}:6006 $DOCKER_IMAGE -t -b ${batch_size} -r\n# #     else\n# #         echo \"トレーニングを開始します\"\n# #         docker run -it --gpus all --shm-size=128M \\\n# #             -v `pwd`/exp/${name}/dataset:/MMVC_Trainer/dataset \\\n# #             -v `pwd`/exp/${name}/logs:/MMVC_Trainer/logs \\\n# #             -v `pwd`/exp/${name}/filelists:/MMVC_Trainer/filelists \\\n# #             -v `pwd`/vc_resources:/resources \\\n# #             -e LOCAL_UID=$(id -u $USER) \\\n# #             -e LOCAL_GID=$(id -g $USER) \\\n# #             -p ${TENSORBOARD_PORT}:6006 $DOCKER_IMAGE -t -b ${batch_size}\n# #     fi\n# # fi\n\n# # if $voice_change_flag; then\n# #     if [[ -z \"$config\" ]]; then\n# #         warn \"コンフィグファイル(-c)を指定してください\"\n# #     fi\n# #     if [[ -z \"$model\" ]]; then\n# #         warn \"モデルファイル(-m)を指定してください\"\n# #     fi\n# #     if [ \"${gpu}\" = \"on\" ]; then\n# #         echo \"GPUをマウントして起動します。\"\n\n# #         docker run -it --gpus all --shm-size=128M \\\n# #         -v `pwd`/vc_resources:/resources \\\n# #         -e LOCAL_UID=$(id -u $USER) \\\n# #         -e LOCAL_GID=$(id -g $USER) \\\n# #         -e EX_IP=\"`hostname -I`\" \\\n# #         -e EX_PORT=${port} \\\n# #         -p ${port}:8080 $DOCKER_IMAGE -v -c ${config} -m ${model}\n# #     elif [ \"${gpu}\" = \"off\" ]; then\n# #         echo \"CPUのみで稼働します。GPUは使用できません。\"\n# #         docker run -it --shm-size=128M \\\n# #         -v `pwd`/vc_resources:/resources \\\n# #         -e LOCAL_UID=$(id -u $USER) \\\n# #         -e LOCAL_GID=$(id -g $USER) \\\n# #         -e EX_IP=\"`hostname -I`\" \\\n# #         -e EX_PORT=${port} \\\n# #         -p ${port}:8080 $DOCKER_IMAGE -v -c ${config} -m ${model}\n# #     else\n# #         echo ${gpu}\n# #         warn \"-g は onかoffで指定して下さい。\"\n        \n# #     fi\n\n\n# # fi\n\n\n"
        },
        {
          "name": "trainer",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorials",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}