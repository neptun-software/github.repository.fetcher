{
  "metadata": {
    "timestamp": 1736561090763,
    "page": 21,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "junyanz/pytorch-CycleGAN-and-pix2pix",
      "stars": 23394,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.75,
          "content": ".DS_Store\ndebug*\ndatasets/\ncheckpoints/\nresults/\nbuild/\ndist/\n*.png\ntorch.egg-info/\n*/**/__pycache__\ntorch/version.py\ntorch/csrc/generic/TensorMethods.cpp\ntorch/lib/*.so*\ntorch/lib/*.dylib*\ntorch/lib/*.h\ntorch/lib/build\ntorch/lib/tmp_install\ntorch/lib/include\ntorch/lib/torch_shm_manager\ntorch/csrc/cudnn/cuDNN.cpp\ntorch/csrc/nn/THNN.cwrap\ntorch/csrc/nn/THNN.cpp\ntorch/csrc/nn/THCUNN.cwrap\ntorch/csrc/nn/THCUNN.cpp\ntorch/csrc/nn/THNN_generic.cwrap\ntorch/csrc/nn/THNN_generic.cpp\ntorch/csrc/nn/THNN_generic.h\ndocs/src/**/*\ntest/data/legacy_modules.t7\ntest/data/gpu_tensors.pt\ntest/htmlcov\ntest/.coverage\n*/*.pyc\n*/**/*.pyc\n*/**/**/*.pyc\n*/**/**/**/*.pyc\n*/**/**/**/**/*.pyc\n*/*.so*\n*/**/*.so*\n*/**/*.dylib*\ntest/data/legacy_serialized.pt\n*~\n.idea\n\n#Ignore Wandb\nwandb/\n"
        },
        {
          "name": ".replit",
          "type": "blob",
          "size": 0.7763671875,
          "content": "language = \"python3\"\nrun = \"<p><a href=\\\"https://github.com/affinelayer/pix2pix-tensorflow\\\"> [Tensorflow]</a> (by Christopher Hesse), <a href=\\\"https://github.com/Eyyub/tensorflow-pix2pix\\\">[Tensorflow]</a> (by Eyy√ºb Sariu), <a href=\\\"https://github.com/datitran/face2face-demo\\\"> [Tensorflow (face2face)]</a> (by Dat Tran), <a href=\\\"https://github.com/awjuliani/Pix2Pix-Film\\\"> [Tensorflow (film)]</a> (by Arthur Juliani), <a href=\\\"https://github.com/kaonashi-tyc/zi2zi\\\">[Tensorflow (zi2zi)]</a> (by Yuchen Tian), <a href=\\\"https://github.com/pfnet-research/chainer-pix2pix\\\">[Chainer]</a> (by mattya), <a href=\\\"https://github.com/tjwei/GANotebooks\\\">[tf/torch/keras/lasagne]</a> (by tjwei), <a href=\\\"https://github.com/taey16/pix2pixBEGAN.pytorch\\\">[Pytorch]</a> (by taey16) </p> </ul>\""
        },
        {
          "name": "CycleGAN.ipynb",
          "type": "blob",
          "size": 7.8134765625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"view-in-github\"\n   },\n   \"source\": [\n    \"<a href=\\\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"5VIGyIus8Vr7\"\n   },\n   \"source\": [\n    \"Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"7wNjDKdQy35h\"\n   },\n   \"source\": [\n    \"# Install\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"TRm-USlsHgEV\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"Pt3igws3eiVp\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"os.chdir('pytorch-CycleGAN-and-pix2pix/')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"z1EySlOXwwoa\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -r requirements.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"8daqlgVhw29P\"\n   },\n   \"source\": [\n    \"# Datasets\\n\",\n    \"\\n\",\n    \"Download one of the official datasets with:\\n\",\n    \"\\n\",\n    \"-   `bash ./datasets/download_cyclegan_dataset.sh [apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos]`\\n\",\n    \"\\n\",\n    \"Or use your own dataset by creating the appropriate folders and adding in the images.\\n\",\n    \"\\n\",\n    \"-   Create a dataset folder under `/dataset` for your dataset.\\n\",\n    \"-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"vrdOettJxaCc\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!bash ./datasets/download_cyclegan_dataset.sh horse2zebra\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"gdUz4116xhpm\"\n   },\n   \"source\": [\n    \"# Pretrained models\\n\",\n    \"\\n\",\n    \"Download one of the official pretrained models with:\\n\",\n    \"\\n\",\n    \"-   `bash ./scripts/download_cyclegan_model.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\\n\",\n    \"\\n\",\n    \"Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"B75UqtKhxznS\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!bash ./scripts/download_cyclegan_model.sh horse2zebra\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"yFw1kDQBx3LN\"\n   },\n   \"source\": [\n    \"# Training\\n\",\n    \"\\n\",\n    \"-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan`\\n\",\n    \"\\n\",\n    \"Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\\n\",\n    \"\\n\",\n    \"Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\\n\",\n    \"\\n\",\n    \"Use `cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class A to class B and `cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class B to class A.\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"0sp7TCT2x9dB\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan --display_id -1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"9UkcaFZiyASl\"\n   },\n   \"source\": [\n    \"# Testing\\n\",\n    \"\\n\",\n    \"-   `python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`\\n\",\n    \"\\n\",\n    \"Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\\n\",\n    \"\\n\",\n    \"> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\\n\",\n    \"> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\\n\",\n    \"\\n\",\n    \"> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"uCsKkEq0yGh0\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"OzSKIPUByfiN\"\n   },\n   \"source\": [\n    \"# Visualize\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"9Mgg8raPyizq\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import matplotlib.pyplot as plt\\n\",\n    \"\\n\",\n    \"img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png')\\n\",\n    \"plt.imshow(img)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"0G3oVH9DyqLQ\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import matplotlib.pyplot as plt\\n\",\n    \"\\n\",\n    \"img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_real.png')\\n\",\n    \"plt.imshow(img)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"accelerator\": \"GPU\",\n  \"colab\": {\n   \"collapsed_sections\": [],\n   \"include_colab_link\": true,\n   \"name\": \"CycleGAN\",\n   \"provenance\": []\n  },\n  \"environment\": {\n   \"name\": \"tf2-gpu.2-3.m74\",\n   \"type\": \"gcloud\",\n   \"uri\": \"gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74\"\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.10\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 3.4814453125,
          "content": "Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n--------------------------- LICENSE FOR pix2pix --------------------------------\nBSD License\n\nFor pix2pix software\nCopyright (c) 2016, Phillip Isola and Jun-Yan Zhu\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n----------------------------- LICENSE FOR DCGAN --------------------------------\nBSD License\n\nFor dcgan.torch software\n\nCopyright (c) 2015, Facebook, Inc. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\nNeither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.07421875,
          "content": "\n<img src='imgs/horse2zebra.gif' align=\"right\" width=384>\n\n<br><br><br>\n\n# CycleGAN and pix2pix in PyTorch\n\n**New**:  Please check out [img2img-turbo](https://github.com/GaParmar/img2img-turbo) repo that includes both pix2pix-turbo and CycleGAN-Turbo. Our new one-step image-to-image translation methods can support both paired and unpaired training and produce better results by leveraging the pre-trained StableDiffusion-Turbo model. The inference time for 512x512 image is 0.29 sec on A6000 and 0.11 sec on A100.\n\nPlease check out [contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.\n\nWe provide PyTorch implementations for both unpaired and paired image-to-image translation.\n\nThe code was written by [Jun-Yan Zhu](https://github.com/junyanz) and [Taesung Park](https://github.com/taesungp), and supported by [Tongzhou Wang](https://github.com/SsnL).\n\nThis PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original [CycleGAN Torch](https://github.com/junyanz/CycleGAN) and [pix2pix Torch](https://github.com/phillipi/pix2pix) code in Lua/Torch.\n\n**Note**: The current software works well with PyTorch 1.4. Check out the older [branch](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/pytorch0.3.1) that supports PyTorch 0.1-0.3.\n\nYou may find useful information in [training/test tips](docs/tips.md) and [frequently asked questions](docs/qa.md). To implement custom models and datasets, check out our [templates](#custom-model-and-dataset). To help users better understand and adapt our codebase, we provide an [overview](docs/overview.md) of the code structure of this repository.\n\n**CycleGAN: [Project](https://junyanz.github.io/CycleGAN/) |  [Paper](https://arxiv.org/pdf/1703.10593.pdf) |  [Torch](https://github.com/junyanz/CycleGAN) |\n[Tensorflow Core Tutorial](https://www.tensorflow.org/tutorials/generative/cyclegan) | [PyTorch Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb)**\n\n<img src=\"https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg\" width=\"800\"/>\n\n**Pix2pix:  [Project](https://phillipi.github.io/pix2pix/) |  [Paper](https://arxiv.org/pdf/1611.07004.pdf) |  [Torch](https://github.com/phillipi/pix2pix) |\n[Tensorflow Core Tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix) | [PyTorch Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb)**\n\n<img src=\"https://phillipi.github.io/pix2pix/images/teaser_v3.png\" width=\"800px\"/>\n\n\n**[EdgesCats Demo](https://affinelayer.com/pixsrv/) | [pix2pix-tensorflow](https://github.com/affinelayer/pix2pix-tensorflow) | by [Christopher Hesse](https://twitter.com/christophrhesse)**\n\n<img src='imgs/edges2cats.jpg' width=\"400px\"/>\n\nIf you use this code for your research, please cite:\n\nUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.<br>\n[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)\\*,  [Taesung Park](https://taesung.me/)\\*, [Phillip Isola](https://people.eecs.berkeley.edu/~isola/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros). In ICCV 2017. (* equal contributions) [[Bibtex]](https://junyanz.github.io/CycleGAN/CycleGAN.txt)\n\n\nImage-to-Image Translation with Conditional Adversarial Networks.<br>\n[Phillip Isola](https://people.eecs.berkeley.edu/~isola), [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/), [Tinghui Zhou](https://people.eecs.berkeley.edu/~tinghuiz), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros). In CVPR 2017. [[Bibtex]](https://www.cs.cmu.edu/~junyanz/projects/pix2pix/pix2pix.bib)\n\n## Talks and Course\npix2pix slides: [keynote](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.key) | [pdf](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.pdf),\nCycleGAN slides: [pptx](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pptx) | [pdf](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pdf)\n\nCycleGAN course assignment [code](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip) and [handout](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-handout.pdf) designed by Prof. [Roger Grosse](http://www.cs.toronto.edu/~rgrosse/) for [CSC321](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/) \"Intro to Neural Networks and Machine Learning\" at University of Toronto. Please contact the instructor if you would like to adopt it in your course.\n\n## Colab Notebook\nTensorFlow Core CycleGAN Tutorial: [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb) | [Code](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb)\n\nTensorFlow Core pix2pix Tutorial: [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb) | [Code](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb)\n\nPyTorch Colab notebook: [CycleGAN](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb) and [pix2pix](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb)\n\nZeroCostDL4Mic Colab notebook: [CycleGAN](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/CycleGAN_ZeroCostDL4Mic.ipynb) and [pix2pix](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/pix2pix_ZeroCostDL4Mic.ipynb)\n\n## Other implementations\n### CycleGAN\n<p><a href=\"https://github.com/leehomyc/cyclegan-1\"> [Tensorflow]</a> (by Harry Yang),\n<a href=\"https://github.com/architrathore/CycleGAN/\">[Tensorflow]</a> (by Archit Rathore),\n<a href=\"https://github.com/vanhuyz/CycleGAN-TensorFlow\">[Tensorflow]</a> (by Van Huy),\n<a href=\"https://github.com/XHUJOY/CycleGAN-tensorflow\">[Tensorflow]</a> (by Xiaowei Hu),\n<a href=\"https://github.com/LynnHo/CycleGAN-Tensorflow-2\"> [Tensorflow2]</a> (by Zhenliang He),\n<a href=\"https://github.com/luoxier/CycleGAN_Tensorlayer\"> [TensorLayer1.0]</a> (by luoxier),\n<a href=\"https://github.com/tensorlayer/cyclegan\"> [TensorLayer2.0]</a> (by zsdonghao),\n<a href=\"https://github.com/Aixile/chainer-cyclegan\">[Chainer]</a> (by Yanghua Jin),\n<a href=\"https://github.com/yunjey/mnist-svhn-transfer\">[Minimal PyTorch]</a> (by yunjey),\n<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN\">[Mxnet]</a> (by Ldpe2G),\n<a href=\"https://github.com/tjwei/GANotebooks\">[lasagne/Keras]</a> (by tjwei),\n<a href=\"https://github.com/simontomaskarlsson/CycleGAN-Keras\">[Keras]</a> (by Simon Karlsson),\n<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Oneflow-Python/CycleGAN\">[OneFlow]</a> (by Ldpe2G)\n</p>\n</ul>\n\n### pix2pix\n<p><a href=\"https://github.com/affinelayer/pix2pix-tensorflow\"> [Tensorflow]</a> (by Christopher Hesse),\n<a href=\"https://github.com/Eyyub/tensorflow-pix2pix\">[Tensorflow]</a> (by Eyy√ºb Sariu),\n<a href=\"https://github.com/datitran/face2face-demo\"> [Tensorflow (face2face)]</a> (by Dat Tran),\n<a href=\"https://github.com/awjuliani/Pix2Pix-Film\"> [Tensorflow (film)]</a> (by Arthur Juliani),\n<a href=\"https://github.com/kaonashi-tyc/zi2zi\">[Tensorflow (zi2zi)]</a> (by Yuchen Tian),\n<a href=\"https://github.com/pfnet-research/chainer-pix2pix\">[Chainer]</a> (by mattya),\n<a href=\"https://github.com/tjwei/GANotebooks\">[tf/torch/keras/lasagne]</a> (by tjwei),\n<a href=\"https://github.com/taey16/pix2pixBEGAN.pytorch\">[Pytorch]</a> (by taey16)\n</p>\n</ul>\n\n## Prerequisites\n- Linux or macOS\n- Python 3\n- CPU or NVIDIA GPU + CUDA CuDNN\n\n## Getting Started\n### Installation\n\n- Clone this repo:\n```bash\ngit clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\ncd pytorch-CycleGAN-and-pix2pix\n```\n\n- Install [PyTorch](http://pytorch.org) and 0.4+ and other dependencies (e.g., torchvision, [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).\n  - For pip users, please type the command `pip install -r requirements.txt`.\n  - For Conda users, you can create a new Conda environment using `conda env create -f environment.yml`.\n  - For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our [Docker](docs/docker.md) page.\n  - For Repl users, please click [![Run on Repl.it](https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix)](https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix).\n\n### CycleGAN train/test\n- Download a CycleGAN dataset (e.g. maps):\n```bash\nbash ./datasets/download_cyclegan_dataset.sh maps\n```\n- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097.\n- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with train and test script\n- Train a model:\n```bash\n#!./scripts/train_cyclegan.sh\npython train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n```\nTo see more intermediate results, check out `./checkpoints/maps_cyclegan/web/index.html`.\n- Test the model:\n```bash\n#!./scripts/test_cyclegan.sh\npython test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n```\n- The test results will be saved to a html file here: `./results/maps_cyclegan/latest_test/index.html`.\n\n### pix2pix train/test\n- Download a pix2pix dataset (e.g.[facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)):\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097.\n- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with train and test script\n- Train a model:\n```bash\n#!./scripts/train_pix2pix.sh\npython train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n```\nTo see more intermediate results, check out  `./checkpoints/facades_pix2pix/web/index.html`.\n\n- Test the model (`bash ./scripts/test_pix2pix.sh`):\n```bash\n#!./scripts/test_pix2pix.sh\npython test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n```\n- The test results will be saved to a html file here: `./results/facades_pix2pix/test_latest/index.html`. You can find more scripts at `scripts` directory.\n- To train and test pix2pix-based colorization models, please add `--model colorization` and `--dataset_mode colorization`. See our training [tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#notes-on-colorization) for more details.\n\n### Apply a pre-trained model (CycleGAN)\n- You can download a pretrained model (e.g. horse2zebra) with the following script:\n```bash\nbash ./scripts/download_cyclegan_model.sh horse2zebra\n```\n- The pretrained model is saved at `./checkpoints/{name}_pretrained/latest_net_G.pth`. Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_cyclegan_model.sh#L3) for all the available CycleGAN models.\n- To test the model, you also need to download the  horse2zebra dataset:\n```bash\nbash ./datasets/download_cyclegan_dataset.sh horse2zebra\n```\n\n- Then generate the results using\n```bash\npython test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout\n```\n- The option `--model test` is used for generating results of CycleGAN only for one side. This option will automatically set `--dataset_mode single`, which only loads the images from one set. On the contrary, using `--model cycle_gan` requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at `./results/`. Use `--results_dir {directory_path_to_save_result}` to specify the results directory.\n\n- For pix2pix and your own models, you need to explicitly specify `--netG`, `--norm`, `--no_dropout` to match the generator architecture of the trained model. See this [FAQ](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#runtimeerror-errors-in-loading-state_dict-812-671461-296) for more details.\n\n### Apply a pre-trained model (pix2pix)\nDownload a pre-trained model with `./scripts/download_pix2pix_model.sh`.\n\n- Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_pix2pix_model.sh#L3) for all the available pix2pix models. For example, if you would like to download label2photo model on the Facades dataset,\n```bash\nbash ./scripts/download_pix2pix_model.sh facades_label2photo\n```\n- Download the pix2pix facades datasets:\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n- Then generate the results using\n```bash\npython test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained\n```\n- Note that we specified `--direction BtoA` as Facades dataset's A to B direction is photos to labels.\n\n- If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use `--model test` option. See `./scripts/test_single.sh` for how to apply a model to Facade label maps (stored in the directory `facades/testB`).\n\n- See a list of currently available models at `./scripts/download_pix2pix_model.sh`\n\n## [Docker](docs/docker.md)\nWe provide the pre-built Docker image and Dockerfile that can run this code repo. See [docker](docs/docker.md).\n\n## [Datasets](docs/datasets.md)\nDownload pix2pix/CycleGAN datasets and create your own datasets.\n\n## [Training/Test Tips](docs/tips.md)\nBest practice for training and testing your models.\n\n## [Frequently Asked Questions](docs/qa.md)\nBefore you post a new question, please first look at the above Q & A and existing GitHub issues.\n\n## Custom Model and Dataset\nIf you plan to implement custom models and dataset for your new applications, we provide a dataset [template](data/template_dataset.py) and a model [template](models/template_model.py) as a starting point.\n\n## [Code structure](docs/overview.md)\nTo help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module.\n\n## Pull Request\nYou are always welcome to contribute to this repository by sending a [pull request](https://help.github.com/articles/about-pull-requests/).\nPlease run `flake8 --ignore E501 .` and `python ./scripts/test_before_push.py` before you commit the code. Please also update the code structure [overview](docs/overview.md) accordingly if you add or remove files.\n\n## Citation\nIf you use this code for your research, please cite our papers.\n```\n@inproceedings{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},\n  year={2017}\n}\n\n\n@inproceedings{isola2017image,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},\n  year={2017}\n}\n```\n\n## Other Languages\n[Spanish](docs/README_es.md)\n\n## Related Projects\n**[contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT)**<br>\n**[CycleGAN-Torch](https://github.com/junyanz/CycleGAN) |\n[pix2pix-Torch](https://github.com/phillipi/pix2pix) | [pix2pixHD](https://github.com/NVIDIA/pix2pixHD)|\n[BicycleGAN](https://github.com/junyanz/BicycleGAN) | [vid2vid](https://tcwang0509.github.io/vid2vid/) | [SPADE/GauGAN](https://github.com/NVlabs/SPADE)**<br>\n**[iGAN](https://github.com/junyanz/iGAN) | [GAN Dissection](https://github.com/CSAILVision/GANDissect) | [GAN Paint](http://ganpaint.io/)**\n\n## Cat Paper Collection\nIf you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper [Collection](https://github.com/junyanz/CatPapers).\n\n## Acknowledgments\nOur code is inspired by [pytorch-DCGAN](https://github.com/pytorch/examples/tree/master/dcgan).\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.2412109375,
          "content": "name: pytorch-CycleGAN-and-pix2pix\nchannels:\n- pytorch\n- defaults\ndependencies:\n- python=3.8\n- pytorch=1.8.1\n- scipy\n- pip\n- pip:\n  - dominate==2.6.0\n  - torchvision==0.9.1\n  - Pillow==8.0.1\n  - numpy==1.19.2\n  - visdom==0.1.8\n  - wandb==0.12.18\n\n"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "options",
          "type": "tree",
          "content": null
        },
        {
          "name": "pix2pix.ipynb",
          "type": "blob",
          "size": 6.955078125,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"view-in-github\"\n   },\n   \"source\": [\n    \"<a href=\\\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"7wNjDKdQy35h\"\n   },\n   \"source\": [\n    \"# Install\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"TRm-USlsHgEV\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"Pt3igws3eiVp\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"os.chdir('pytorch-CycleGAN-and-pix2pix/')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"z1EySlOXwwoa\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -r requirements.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"8daqlgVhw29P\"\n   },\n   \"source\": [\n    \"# Datasets\\n\",\n    \"\\n\",\n    \"Download one of the official datasets with:\\n\",\n    \"\\n\",\n    \"-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`\\n\",\n    \"\\n\",\n    \"Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"vrdOettJxaCc\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!bash ./datasets/download_pix2pix_dataset.sh facades\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"gdUz4116xhpm\"\n   },\n   \"source\": [\n    \"# Pretrained models\\n\",\n    \"\\n\",\n    \"Download one of the official pretrained models with:\\n\",\n    \"\\n\",\n    \"-   `bash ./scripts/download_pix2pix_model.sh [edges2shoes, sat2map, map2sat, facades_label2photo, and day2night]`\\n\",\n    \"\\n\",\n    \"Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"GC2DEP4M0OsS\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!bash ./scripts/download_pix2pix_model.sh facades_label2photo\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"yFw1kDQBx3LN\"\n   },\n   \"source\": [\n    \"# Training\\n\",\n    \"\\n\",\n    \"-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`\\n\",\n    \"\\n\",\n    \"Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"0sp7TCT2x9dB\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA --display_id -1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"9UkcaFZiyASl\"\n   },\n   \"source\": [\n    \"# Testing\\n\",\n    \"\\n\",\n    \"-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`\\n\",\n    \"\\n\",\n    \"Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.\\n\",\n    \"\\n\",\n    \"> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\\n\",\n    \"> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.\\n\",\n    \"\\n\",\n    \"> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).\\n\",\n    \"\\n\",\n    \"> See a list of currently available models at ./scripts/download_pix2pix_model.sh\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"mey7o6j-0368\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!ls checkpoints/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"uCsKkEq0yGh0\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_label2photo_pretrained --use_wandb\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"OzSKIPUByfiN\"\n   },\n   \"source\": [\n    \"# Visualize\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"9Mgg8raPyizq\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import matplotlib.pyplot as plt\\n\",\n    \"\\n\",\n    \"img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_fake_B.png')\\n\",\n    \"plt.imshow(img)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"0G3oVH9DyqLQ\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_A.png')\\n\",\n    \"plt.imshow(img)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"ErK5OC1j1LH4\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_B.png')\\n\",\n    \"plt.imshow(img)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"accelerator\": \"GPU\",\n  \"colab\": {\n   \"collapsed_sections\": [],\n   \"include_colab_link\": true,\n   \"name\": \"pix2pix\",\n   \"provenance\": []\n  },\n  \"environment\": {\n   \"name\": \"tf2-gpu.2-3.m74\",\n   \"type\": \"gcloud\",\n   \"uri\": \"gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74\"\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.10\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.068359375,
          "content": "torch>=1.4.0\ntorchvision>=0.5.0\ndominate>=2.4.0\nvisdom>=0.1.8.8\nwandb\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 4.4384765625,
          "content": "\"\"\"General-purpose test script for image-to-image translation.\n\nOnce you have trained your model with train.py, you can use this script to test the model.\nIt will load a saved model from '--checkpoints_dir' and save the results to '--results_dir'.\n\nIt first creates model and dataset given the option. It will hard-code some parameters.\nIt then runs inference for '--num_test' images and save results to an HTML file.\n\nExample (You need to train models first or download pre-trained models from our website):\n    Test a CycleGAN model (both sides):\n        python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n\n    Test a CycleGAN model (one side only):\n        python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout\n\n    The option '--model test' is used for generating CycleGAN results only for one side.\n    This option will automatically set '--dataset_mode single', which only loads the images from one set.\n    On the contrary, using '--model cycle_gan' requires loading and generating results in both directions,\n    which is sometimes unnecessary. The results will be saved at ./results/.\n    Use '--results_dir <directory_path_to_save_result>' to specify the results directory.\n\n    Test a pix2pix model:\n        python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n\nSee options/base_options.py and options/test_options.py for more test options.\nSee training and test tips at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md\nSee frequently asked questions at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md\n\"\"\"\nimport os\nfrom options.test_options import TestOptions\nfrom data import create_dataset\nfrom models import create_model\nfrom util.visualizer import save_images\nfrom util import html\n\ntry:\n    import wandb\nexcept ImportError:\n    print('Warning: wandb package cannot be found. The option \"--use_wandb\" will result in error.')\n\n\nif __name__ == '__main__':\n    opt = TestOptions().parse()  # get test options\n    # hard-code some parameters for test\n    opt.num_threads = 0   # test code only supports num_threads = 0\n    opt.batch_size = 1    # test code only supports batch_size = 1\n    opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n    opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n    opt.display_id = -1   # no visdom display; the test code saves the results to a HTML file.\n    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n    model = create_model(opt)      # create a model given opt.model and other options\n    model.setup(opt)               # regular setup: load and print networks; create schedulers\n\n    # initialize logger\n    if opt.use_wandb:\n        wandb_run = wandb.init(project=opt.wandb_project_name, name=opt.name, config=opt) if not wandb.run else wandb.run\n        wandb_run._label(repo='CycleGAN-and-pix2pix')\n\n    # create a website\n    web_dir = os.path.join(opt.results_dir, opt.name, '{}_{}'.format(opt.phase, opt.epoch))  # define the website directory\n    if opt.load_iter > 0:  # load_iter is 0 by default\n        web_dir = '{:s}_iter{:d}'.format(web_dir, opt.load_iter)\n    print('creating web directory', web_dir)\n    webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.epoch))\n    # test with eval mode. This only affects layers like batchnorm and dropout.\n    # For [pix2pix]: we use batchnorm and dropout in the original pix2pix. You can experiment it with and without eval() mode.\n    # For [CycleGAN]: It should not affect CycleGAN as CycleGAN uses instancenorm without dropout.\n    if opt.eval:\n        model.eval()\n    for i, data in enumerate(dataset):\n        if i >= opt.num_test:  # only apply our model to opt.num_test images.\n            break\n        model.set_input(data)  # unpack data from data loader\n        model.test()           # run inference\n        visuals = model.get_current_visuals()  # get image results\n        img_path = model.get_image_paths()     # get image paths\n        if i % 5 == 0:  # save images to an HTML file\n            print('processing (%04d)-th image... %s' % (i, img_path))\n        save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize, use_wandb=opt.use_wandb)\n    webpage.save()  # save the HTML\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 4.8173828125,
          "content": "\"\"\"General-purpose training script for image-to-image translation.\n\nThis script works for various models (with option '--model': e.g., pix2pix, cyclegan, colorization) and\ndifferent datasets (with option '--dataset_mode': e.g., aligned, unaligned, single, colorization).\nYou need to specify the dataset ('--dataroot'), experiment name ('--name'), and model ('--model').\n\nIt first creates model, dataset, and visualizer given the option.\nIt then does standard network training. During the training, it also visualize/save the images, print/save the loss plot, and save models.\nThe script supports continue/resume training. Use '--continue_train' to resume your previous training.\n\nExample:\n    Train a CycleGAN model:\n        python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n    Train a pix2pix model:\n        python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n\nSee options/base_options.py and options/train_options.py for more training options.\nSee training and test tips at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md\nSee frequently asked questions at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md\n\"\"\"\nimport time\nfrom options.train_options import TrainOptions\nfrom data import create_dataset\nfrom models import create_model\nfrom util.visualizer import Visualizer\n\nif __name__ == '__main__':\n    opt = TrainOptions().parse()   # get training options\n    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n    dataset_size = len(dataset)    # get the number of images in the dataset.\n    print('The number of training images = %d' % dataset_size)\n\n    model = create_model(opt)      # create a model given opt.model and other options\n    model.setup(opt)               # regular setup: load and print networks; create schedulers\n    visualizer = Visualizer(opt)   # create a visualizer that display/save images and plots\n    total_iters = 0                # the total number of training iterations\n\n    for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>\n        epoch_start_time = time.time()  # timer for entire epoch\n        iter_data_time = time.time()    # timer for data loading per iteration\n        epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n        visualizer.reset()              # reset the visualizer: make sure it saves the results to HTML at least once every epoch\n        model.update_learning_rate()    # update learning rates in the beginning of every epoch.\n        for i, data in enumerate(dataset):  # inner loop within one epoch\n            iter_start_time = time.time()  # timer for computation per iteration\n            if total_iters % opt.print_freq == 0:\n                t_data = iter_start_time - iter_data_time\n\n            total_iters += opt.batch_size\n            epoch_iter += opt.batch_size\n            model.set_input(data)         # unpack data from dataset and apply preprocessing\n            model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n\n            if total_iters % opt.display_freq == 0:   # display images on visdom and save images to a HTML file\n                save_result = total_iters % opt.update_html_freq == 0\n                model.compute_visuals()\n                visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n\n            if total_iters % opt.print_freq == 0:    # print training losses and save logging information to the disk\n                losses = model.get_current_losses()\n                t_comp = (time.time() - iter_start_time) / opt.batch_size\n                visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)\n                if opt.display_id > 0:\n                    visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, losses)\n\n            if total_iters % opt.save_latest_freq == 0:   # cache our latest model every <save_latest_freq> iterations\n                print('saving the latest model (epoch %d, total_iters %d)' % (epoch, total_iters))\n                save_suffix = 'iter_%d' % total_iters if opt.save_by_iter else 'latest'\n                model.save_networks(save_suffix)\n\n            iter_data_time = time.time()\n        if epoch % opt.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs\n            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))\n            model.save_networks('latest')\n            model.save_networks(epoch)\n\n        print('End of epoch %d / %d \\t Time Taken: %d sec' % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time))\n"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}