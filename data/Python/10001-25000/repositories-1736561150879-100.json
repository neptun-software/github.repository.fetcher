{
  "metadata": {
    "timestamp": 1736561150879,
    "page": 100,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jantic/DeOldify",
      "stars": 18125,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.5009765625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# DeOldify\ndata\n*SymbolicLinks.sh\n*.ipynb_checkpoints*\nColorizeTraining*[0-9]*.ipynb\n*Colorizer[0-9]*.ipynb\nlesson7-superres*.ipynb\ntest.py\nresult_images\n*.prof\n*.pth\nvideo\ntest_images/*.jpg\ntest_images/*.JPG\ntest_images/*.PNG\ntest_images/*.png\ntest_images/*.jpeg\ntest_images/*.JPEG \ndeoldify/.ipynb_checkpoints/*-checkpoint.py\ntmp*\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.1376953125,
          "content": "repos:\n-   repo: https://github.com/ambv/black\n    rev: stable\n    hooks:\n    - id: black\n      args: [-S]\n      language_version: python3.6\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 17.5859375,
          "content": "[MASTER]\n\n# A comma-separated list of package or module names from where C extensions may\n# be loaded. Extensions are loading into the active Python interpreter and may\n# run arbitrary code.\nextension-pkg-whitelist=\n\n# Add files or directories to the blacklist. They should be base names, not\n# paths.\nignore=CVS\n\n# Add files or directories matching the regex patterns to the blacklist. The\n# regex matches against base names, not paths.\nignore-patterns=\n\n# Python code to execute, usually for sys.path manipulation such as\n# pygtk.require().\n#init-hook='import sys; sys.path.append(\"./venv/lib/python3.7/site-packages\")'\n\n# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the\n# number of processors available to use.\njobs=1\n\n# Control the amount of potential inferred values when inferring a single\n# object. This can help the performance when dealing with large functions or\n# complex, nested conditions.\nlimit-inference-results=100\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Pickle collected data for later comparisons.\npersistent=yes\n\n# Specify a configuration file.\n#rcfile=\n\n# When enabled, pylint would attempt to guess common misconfiguration and emit\n# user-friendly hints instead of false-positive error messages.\nsuggestion-mode=yes\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED.\nconfidence=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once). You can also use \"--disable=all\" to\n# disable everything first and then reenable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use \"--disable=all --enable=classes\n# --disable=W\".\ndisable=print-statement,\n        parameter-unpacking,\n        unpacking-in-except,\n        old-raise-syntax,\n        backtick,\n        long-suffix,\n        old-ne-operator,\n        old-octal-literal,\n        import-star-module-level,\n        non-ascii-bytes-literal,\n        raw-checker-failed,\n        bad-inline-option,\n        locally-disabled,\n        locally-enabled,\n        file-ignored,\n        suppressed-message,\n        useless-suppression,\n        deprecated-pragma,\n        use-symbolic-message-instead,\n        apply-builtin,\n        basestring-builtin,\n        buffer-builtin,\n        cmp-builtin,\n        coerce-builtin,\n        execfile-builtin,\n        file-builtin,\n        long-builtin,\n        raw_input-builtin,\n        reduce-builtin,\n        standarderror-builtin,\n        unicode-builtin,\n        xrange-builtin,\n        coerce-method,\n        delslice-method,\n        getslice-method,\n        setslice-method,\n        no-absolute-import,\n        old-division,\n        dict-iter-method,\n        dict-view-method,\n        next-method-called,\n        metaclass-assignment,\n        indexing-exception,\n        raising-string,\n        reload-builtin,\n        oct-method,\n        hex-method,\n        nonzero-method,\n        cmp-method,\n        input-builtin,\n        round-builtin,\n        intern-builtin,\n        unichr-builtin,\n        map-builtin-not-iterating,\n        zip-builtin-not-iterating,\n        range-builtin-not-iterating,\n        filter-builtin-not-iterating,\n        using-cmp-argument,\n        eq-without-hash,\n        div-method,\n        idiv-method,\n        rdiv-method,\n        exception-message-attribute,\n        invalid-str-codec,\n        sys-max-int,\n        bad-python3-import,\n        deprecated-string-function,\n        deprecated-str-translate-call,\n        deprecated-itertools-function,\n        deprecated-types-field,\n        next-method-defined,\n        dict-items-not-iterating,\n        dict-keys-not-iterating,\n        dict-values-not-iterating,\n        deprecated-operator-function,\n        deprecated-urllib-function,\n        xreadlines-attribute,\n        deprecated-sys-function,\n        exception-escape,\n        comprehension-escape,\n        # Disabled due Black\n        bad-continuation,\n        bad-whitespace,\n        # We don't care about these\n        redundant-keyword-arg,\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\nenable=c-extension-no-member\n\n\n[REPORTS]\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details.\n#msg-template=\n\n# Set the output format. Available formats are text, parseable, colorized, json\n# and msvs (visual studio). You can also give a reporter class, e.g.\n# mypackage.mymodule.MyReporterClass.\noutput-format=text\n\n# Tells whether to display a full report or only the messages.\nreports=no\n\n# Activate the evaluation score.\nscore=yes\n\n\n[REFACTORING]\n\n# Maximum number of nested blocks for function / method body\nmax-nested-blocks=5\n\n# Complete name of functions that never returns. When checking for\n# inconsistent-return-statements if a never returning function is called then\n# it will be considered as an explicit return statement and no message will be\n# printed.\nnever-returning-functions=sys.exit\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format.\nlogging-modules=logging\n\n\n[SIMILARITIES]\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=FIXME,\n      XXX,\n      TODO\n\n\n[FORMAT]\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=^\\s*(# )?<?https?://\\S+>?$\n\n# Number of spaces of indent required inside a hanging  or continued line.\nindent-after-paren=4\n\n# String used as indentation unit. This is usually \"    \" (4 spaces) or \"\\t\" (1\n# tab).\nindent-string='    '\n\n# Maximum number of characters on a single line.\nmax-line-length=100\n\n# Maximum number of lines in a module.\nmax-module-lines=1000\n\n# List of optional constructs for which whitespace checking is disabled. `dict-\n# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\\n222: 2}.\n# `trailing-comma` allows a space between comma and closing bracket: (a, ).\n# `empty-line` allows space-only lines.\nno-space-check=trailing-comma,\n               dict-separator\n\n# Allow the body of a class to be on the same line as the declaration if body\n# contains single statement.\nsingle-line-class-stmt=no\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=no\n\n\n[BASIC]\n\n# Naming style matching correct argument names.\nargument-naming-style=snake_case\n\n# Regular expression matching correct argument names. Overrides argument-\n# naming-style.\n#argument-rgx=\n\n# Naming style matching correct attribute names.\nattr-naming-style=snake_case\n\n# Regular expression matching correct attribute names. Overrides attr-naming-\n# style.\n#attr-rgx=\n\n# Bad variable names which should always be refused, separated by a comma.\nbad-names=foo,\n          bar,\n          baz,\n          toto,\n          tutu,\n          tata\n\n# Naming style matching correct class attribute names.\nclass-attribute-naming-style=any\n\n# Regular expression matching correct class attribute names. Overrides class-\n# attribute-naming-style.\n#class-attribute-rgx=\n\n# Naming style matching correct class names.\nclass-naming-style=PascalCase\n\n# Regular expression matching correct class names. Overrides class-naming-\n# style.\n#class-rgx=\n\n# Naming style matching correct constant names.\nconst-naming-style=UPPER_CASE\n\n# Regular expression matching correct constant names. Overrides const-naming-\n# style.\n#const-rgx=\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=-1\n\n# Naming style matching correct function names.\nfunction-naming-style=snake_case\n\n# Regular expression matching correct function names. Overrides function-\n# naming-style.\n#function-rgx=\n\n# Good variable names which should always be accepted, separated by a comma.\ngood-names=f,\n           i,\n           j,\n           k,\n           s,\n           t,\n           ex,\n           Run,\n           _\n\n# Include a hint for the correct naming format with invalid-name.\ninclude-naming-hint=no\n\n# Naming style matching correct inline iteration names.\ninlinevar-naming-style=any\n\n# Regular expression matching correct inline iteration names. Overrides\n# inlinevar-naming-style.\n#inlinevar-rgx=\n\n# Naming style matching correct method names.\nmethod-naming-style=snake_case\n\n# Regular expression matching correct method names. Overrides method-naming-\n# style.\n#method-rgx=\n\n# Naming style matching correct module names.\nmodule-naming-style=snake_case\n\n# Regular expression matching correct module names. Overrides module-naming-\n# style.\n#module-rgx=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=^_\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\n# These decorators are taken in consideration only for invalid-name.\nproperty-classes=abc.abstractproperty\n\n# Naming style matching correct variable names.\nvariable-naming-style=snake_case\n\n# Regular expression matching correct variable names. Overrides variable-\n# naming-style.\nvariable-rgx=_?[a-z][A-Za-z0-9_]{0,30}$\nargument-rgx=_?[a-z][A-Za-z0-9_]{0,30}$\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=torch.mm,\n                  torch.diag,\n                  torch.symeig,\n                  torch.sqrt,\n                  torch.cat,\n                  cv2.cvtColor,\n                  cv2.COLOR_BGR2YUV,\n                  cv2.COLOR_YUV2BGR,\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# Tells whether to warn about missing members when the owner of the attribute\n# is inferred to be None.\nignore-none=yes\n\n# This flag controls whether pylint should warn about no-member and similar\n# checks whenever an opaque object is returned when inferring. The inference\n# can return multiple potential results while evaluating a Python object, but\n# some branches might not be evaluated, which results in partial inference. In\n# that case, it might be useful to still emit no-member and other checks for\n# the rest of the inferred objects.\nignore-on-opaque-inference=yes\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis. It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# Show a hint with possible names when a member name was not found. The aspect\n# of finding the hint is based on edit distance.\nmissing-member-hint=yes\n\n# The minimum edit distance a name should have in order to be considered a\n# similar match for a missing member name.\nmissing-member-hint-distance=1\n\n# The total number of similar names that should be taken in consideration when\n# showing a hint for a missing member.\nmissing-member-max-choices=1\n\n\n[VARIABLES]\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# Tells whether unused global variables should be treated as a violation.\nallow-global-unused-variables=yes\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,\n          _cb\n\n# A regular expression matching the name of dummy variables (i.e. expected to\n# not be used).\ndummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_\n\n# Argument names that match this expression will be ignored. Default to name\n# with leading underscore.\nignored-argument-names=_.*|^ignored_|^unused_\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io\n\n\n[SPELLING]\n\n# Limits count of emitted suggestions for spelling mistakes.\nmax-spelling-suggestions=4\n\n# Spelling dictionary name. Available dictionaries: en_IE (myspell), en_ZM\n# (myspell), en_GB (myspell), en_HK (myspell), en_BZ (myspell), en_PH\n# (myspell), en_ZA (myspell), en_MW (myspell), en_AU (myspell), en_CA\n# (myspell), en_JM (myspell), en_GH (myspell), en_TT (myspell), en_SG\n# (myspell), en_BW (myspell), en_US (myspell), en_NZ (myspell), en_AG\n# (myspell), en_ZW (myspell), en_NA (myspell), en_IN (myspell), en_BS\n# (myspell), en_DK (myspell), en_NG (myspell)..\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[IMPORTS]\n\n# Allow wildcard imports from modules that define __all__.\nallow-wildcard-with-all=no\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n# Deprecated modules which should not be used, separated by a comma.\ndeprecated-modules=optparse,tkinter.tix\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled).\next-import-graph=\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled).\nimport-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled).\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,\n                  _fields,\n                  _replace,\n                  _source,\n                  _make\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=cls\n\n\n[DESIGN]\n\n# Maximum number of arguments for function / method.\nmax-args=5\n\n# Maximum number of attributes for a class (see R0902).\nmax-attributes=7\n\n# Maximum number of boolean expressions in an if statement.\nmax-bool-expr=5\n\n# Maximum number of branch for function / method body.\nmax-branches=12\n\n# Maximum number of locals for function / method body.\nmax-locals=15\n\n# Maximum number of parents for a class (see R0901).\nmax-parents=7\n\n# Maximum number of public methods for a class (see R0904).\nmax-public-methods=20\n\n# Maximum number of return / yield for function / method body.\nmax-returns=6\n\n# Maximum number of statements in function / method body.\nmax-statements=50\n\n# Minimum number of public methods for a class (see R0903).\nmin-public-methods=2\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\".\novergeneral-exceptions=Exception\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.1767578125,
          "content": "sudo: false\nlanguage: python\ninstall: pip install tox\nmatrix:\n  include:\n  - python: \"3.6\"\n    env: TOX_ENV=static\n  - python: \"3.6\"\n    env: TOX_ENV=format\nscript: tox -e $TOX_ENV\n"
        },
        {
          "name": "ColorFIDBenchmarkArtistic.ipynb",
          "type": "blob",
          "size": 7.2060546875,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Color FID Benchmark (HQ)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"os.environ['CUDA_VISIBLE_DEVICES']='1'\\n\",\n    \"os.environ['OMP_NUM_THREADS']='1'\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import statistics\\n\",\n    \"from fastai import *\\n\",\n    \"from deoldify.visualize import *\\n\",\n    \"import cv2\\n\",\n    \"from fid.fid_score import *\\n\",\n    \"from fid.inception import *\\n\",\n    \"import imageio\\n\",\n    \"plt.style.use('dark_background')\\n\",\n    \"torch.backends.cudnn.benchmark=True\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, module=\\\"torch.nn.functional\\\")\\n\",\n    \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, message='.*?retrieve source code for container of type.*?')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  Data should come from here:  'https://datasets.figure-eight.com/figure_eight_datasets/open-images/test_challenge.zip'\\n\",\n    \"#NOTE:  Minimum recommmended number of samples is 10K.  Source:  https://github.com/bioinf-jku/TTUR\\n\",\n    \"\\n\",\n    \"path = Path('data/ColorBenchmark')\\n\",\n    \"path_hr = path/'source'\\n\",\n    \"path_lr = path/'bandw'\\n\",\n    \"path_results = Path('./result_images/ColorBenchmarkFID/artistic')\\n\",\n    \"path_rendered = path_results/'rendered'\\n\",\n    \"\\n\",\n    \"#path = Path('data/DeOldifyColor')\\n\",\n    \"#path_hr = path\\n\",\n    \"#path_lr = path/'bandw'\\n\",\n    \"#path_results = Path('./result_images/ColorBenchmark/edge')\\n\",\n    \"#path_rendered = path_results/'rendered'\\n\",\n    \"\\n\",\n    \"#num_images = 2048\\n\",\n    \"#num_images = 15000\\n\",\n    \"num_images = 50000\\n\",\n    \"render_factor=35\\n\",\n    \"fid_batch_size = 4\\n\",\n    \"eval_size=299\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def inception_model(dims:int):\\n\",\n    \"    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\\n\",\n    \"    model = InceptionV3([block_idx])\\n\",\n    \"    model.cuda()\\n\",\n    \"    return model\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def create_before_images(fn,i):\\n\",\n    \"    dest = path_lr/fn.relative_to(path_hr)\\n\",\n    \"    dest.parent.mkdir(parents=True, exist_ok=True)\\n\",\n    \"    img = PIL.Image.open(fn).convert('LA').convert('RGB')\\n\",\n    \"    img.save(dest)  \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def render_images(colorizer, source_dir:Path, filtered_dir:Path, target_dir:Path, render_factor:int, num_images:int)->[(Path, Path, Path)]:\\n\",\n    \"    results = []\\n\",\n    \"    bandw_list = ImageList.from_folder(path_lr)\\n\",\n    \"    bandw_list = bandw_list[:num_images]\\n\",\n    \"\\n\",\n    \"    if len(bandw_list.items) == 0: return results\\n\",\n    \"\\n\",\n    \"    results = []\\n\",\n    \"    img_iterator = progress_bar(bandw_list.items)\\n\",\n    \"\\n\",\n    \"    for bandw_path in img_iterator:\\n\",\n    \"        target_path = target_dir/bandw_path.relative_to(source_dir)\\n\",\n    \"\\n\",\n    \"        try:\\n\",\n    \"            result_image = colorizer.get_transformed_image(path=bandw_path, render_factor=render_factor)\\n\",\n    \"            result_path = Path(str(path_results) + '/' + bandw_path.parent.name + '/' + bandw_path.name)\\n\",\n    \"            if not result_path.parent.exists():\\n\",\n    \"                result_path.parent.mkdir(parents=True, exist_ok=True)\\n\",\n    \"            result_image.save(result_path)\\n\",\n    \"            results.append((result_path, bandw_path, target_path))\\n\",\n    \"        except Exception as err:\\n\",\n    \"            print('Failed to render image.  Skipping.  Details: {0}'.format(err))\\n\",\n    \"    \\n\",\n    \"    return results \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def calculate_fid_score(render_results, bs:int, eval_size:int):\\n\",\n    \"    dims = 2048\\n\",\n    \"    cuda = True\\n\",\n    \"    model = inception_model(dims=dims)\\n\",\n    \"    rendered_paths = []\\n\",\n    \"    target_paths = []\\n\",\n    \"    \\n\",\n    \"    for render_result in render_results:\\n\",\n    \"        rendered_path, _, target_path = render_result\\n\",\n    \"        rendered_paths.append(str(rendered_path))\\n\",\n    \"        target_paths.append(str(target_path))\\n\",\n    \"        \\n\",\n    \"    rendered_m, rendered_s = calculate_activation_statistics(files=rendered_paths, model=model, batch_size=bs, dims=dims, cuda=cuda)\\n\",\n    \"    target_m, target_s = calculate_activation_statistics(files=target_paths, model=model, batch_size=bs, dims=dims, cuda=cuda)\\n\",\n    \"    fid_score = calculate_frechet_distance(rendered_m, rendered_s, target_m, target_s)\\n\",\n    \"    del model\\n\",\n    \"    return fid_score\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Create black and whites source images\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Only runs if the directory isn't already created.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if not path_lr.exists():\\n\",\n    \"    il = ImageList.from_folder(path_hr)\\n\",\n    \"    parallel(create_before_images, il.items)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"path_results.parent.mkdir(parents=True, exist_ok=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Rendering\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"colorizer = get_image_colorizer(artistic=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"render_results = render_images(colorizer=colorizer, source_dir=path_lr, target_dir=path_hr, filtered_dir=path_results, render_factor=render_factor, num_images=num_images)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Colorizaton Scoring\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"fid_score = calculate_fid_score(render_results, bs=fid_batch_size, eval_size=eval_size)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print('FID Score: ' + str(fid_score))\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        },
        {
          "name": "ColorizeTrainingArtistic.ipynb",
          "type": "blob",
          "size": 14.6728515625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Artistic Model Training\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTES:  \\n\",\n    \"* This is \\\"NoGAN\\\" based training, described in the DeOldify readme.\\n\",\n    \"* This model prioritizes colorful renderings.  It has higher variation in renderings at different resolutions compared to the \\\"stable\\\" model\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import fastai\\n\",\n    \"from fastai import *\\n\",\n    \"from fastai.vision import *\\n\",\n    \"from fastai.callbacks.tensorboard import *\\n\",\n    \"from fastai.vision.gan import *\\n\",\n    \"from deoldify.generators import *\\n\",\n    \"from deoldify.critics import *\\n\",\n    \"from deoldify.dataset import *\\n\",\n    \"from deoldify.loss import *\\n\",\n    \"from deoldify.save import *\\n\",\n    \"from PIL import Image, ImageDraw, ImageFont\\n\",\n    \"from PIL import ImageFile\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"path = Path('data/imagenet/ILSVRC/Data/CLS-LOC')\\n\",\n    \"path_hr = path\\n\",\n    \"path_lr = path/'bandw'\\n\",\n    \"\\n\",\n    \"proj_id = 'ArtisticModel'\\n\",\n    \"\\n\",\n    \"gen_name = proj_id + '_gen'\\n\",\n    \"pre_gen_name = gen_name + '_0'\\n\",\n    \"crit_name = proj_id + '_crit'\\n\",\n    \"\\n\",\n    \"name_gen = proj_id + '_image_gen'\\n\",\n    \"path_gen = path/name_gen\\n\",\n    \"\\n\",\n    \"TENSORBOARD_PATH = Path('data/tensorboard/' + proj_id)\\n\",\n    \"\\n\",\n    \"nf_factor = 1.5\\n\",\n    \"pct_start = 1e-8\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def get_data(bs:int, sz:int, keep_pct:float):\\n\",\n    \"    return get_colorize_data(sz=sz, bs=bs, crappy_path=path_lr, good_path=path_hr, \\n\",\n    \"                             random_seed=None, keep_pct=keep_pct)\\n\",\n    \"\\n\",\n    \"def get_crit_data(classes, bs, sz):\\n\",\n    \"    src = ImageList.from_folder(path, include=classes, recurse=True).split_by_rand_pct(0.1, seed=42)\\n\",\n    \"    ll = src.label_from_folder(classes=classes)\\n\",\n    \"    data = (ll.transform(get_transforms(max_zoom=2.), size=sz)\\n\",\n    \"           .databunch(bs=bs).normalize(imagenet_stats))\\n\",\n    \"    return data\\n\",\n    \"\\n\",\n    \"def create_training_images(fn,i):\\n\",\n    \"    dest = path_lr/fn.relative_to(path_hr)\\n\",\n    \"    dest.parent.mkdir(parents=True, exist_ok=True)\\n\",\n    \"    img = PIL.Image.open(fn).convert('LA').convert('RGB')\\n\",\n    \"    img.save(dest)  \\n\",\n    \"    \\n\",\n    \"def save_preds(dl):\\n\",\n    \"    i=0\\n\",\n    \"    names = dl.dataset.items\\n\",\n    \"    \\n\",\n    \"    for b in dl:\\n\",\n    \"        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\\n\",\n    \"        for o in preds:\\n\",\n    \"            o.save(path_gen/names[i].name)\\n\",\n    \"            i += 1\\n\",\n    \"    \\n\",\n    \"def save_gen_images():\\n\",\n    \"    if path_gen.exists(): shutil.rmtree(path_gen)\\n\",\n    \"    path_gen.mkdir(exist_ok=True)\\n\",\n    \"    data_gen = get_data(bs=bs, sz=sz, keep_pct=0.085)\\n\",\n    \"    save_preds(data_gen.fix_dl)\\n\",\n    \"    PIL.Image.open(path_gen.ls()[0])\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Create black and white training images\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Only runs if the directory isn't already created.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if not path_lr.exists():\\n\",\n    \"    il = ImageList.from_folder(path_hr)\\n\",\n    \"    parallel(create_training_images, il.items)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Pre-train generator\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Most of the training takes place here in pretraining for NoGAN.  The goal here is to take the generator as far as possible with conventional training, as that is much easier to control and obtain glitch-free results compared to GAN training.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 64px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=88\\n\",\n    \"sz=64\\n\",\n    \"keep_pct=1.0\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_deep(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.callback_fns.append(partial(ImageGenTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GenPre'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=0.8, max_lr=slice(1e-3))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start,  max_lr=slice(3e-7, 3e-4))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 128px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=22\\n\",\n    \"sz=128\\n\",\n    \"keep_pct=1.0\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(1e-7,1e-4))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 192px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=11\\n\",\n    \"sz=192\\n\",\n    \"keep_pct=0.50\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(5e-8,5e-5))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Repeatable GAN Cycle\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Best results so far have been based on repeating the cycle below a few times (about 5-8?), until diminishing returns are hit (no improvement in image quality).  Each time you repeat the cycle, you want to increment that old_checkpoint_num by 1 so that new check points don't overwrite the old.  \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"old_checkpoint_num = 0\\n\",\n    \"checkpoint_num = old_checkpoint_num + 1\\n\",\n    \"gen_old_checkpoint_name = gen_name + '_' + str(old_checkpoint_num)\\n\",\n    \"gen_new_checkpoint_name = gen_name + '_' + str(checkpoint_num)\\n\",\n    \"crit_old_checkpoint_name = crit_name + '_' + str(old_checkpoint_num)\\n\",\n    \"crit_new_checkpoint_name= crit_name + '_' + str(checkpoint_num)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Save Generated Images\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=192\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_deep(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"save_gen_images()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Pretrain Critic\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"##### Only need full pretraining of critic when starting from scratch.  Otherwise, just finetune!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if old_checkpoint_num == 0:\\n\",\n    \"    bs=64\\n\",\n    \"    sz=128\\n\",\n    \"    learn_gen=None\\n\",\n    \"    gc.collect()\\n\",\n    \"    data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\\n\",\n    \"    data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\\n\",\n    \"    learn_critic = colorize_crit_learner(data=data_crit, nf=256)\\n\",\n    \"    learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\\n\",\n    \"    learn_critic.fit_one_cycle(6, 1e-3)\\n\",\n    \"    learn_critic.save(crit_old_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=16\\n\",\n    \"sz=192\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic = colorize_crit_learner(data=data_crit, nf=256).load(crit_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.fit_one_cycle(4, 1e-4)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.save(crit_new_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### GAN\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit=None\\n\",\n    \"learn_gen=None\\n\",\n    \"gc.collect()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"lr=1e-5\\n\",\n    \"sz=192\\n\",\n    \"bs=9\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit = colorize_crit_learner(data=data_crit, nf=256).load(crit_new_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_deep(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\\n\",\n    \"learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.0,2.0), show_img=False, switcher=switcher,\\n\",\n    \"                                 opt_func=partial(optim.Adam, betas=(0.,0.9)), wd=1e-3)\\n\",\n    \"learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))\\n\",\n    \"learn.callback_fns.append(partial(GANTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GanLearner', visual_iters=100))\\n\",\n    \"learn.callback_fns.append(partial(GANSaveCallback, learn_gen=learn_gen, filename=gen_new_checkpoint_name, save_iters=100))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Instructions:  \\n\",\n    \"Find the checkpoint just before where glitches start to be introduced.  This is all very new so you may need to play around with just how far you go here with keep_pct.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn.data = get_data(sz=sz, bs=bs, keep_pct=0.03)\\n\",\n    \"learn_gen.freeze_to(-1)\\n\",\n    \"learn.fit(1,lr)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "ColorizeTrainingStable.ipynb",
          "type": "blob",
          "size": 14.6806640625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Stable Model Training\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTES:  \\n\",\n    \"* This is \\\"NoGAN\\\" based training, described in the DeOldify readme.\\n\",\n    \"* This model prioritizes stable and reliable renderings.  It does particularly well on portraits and landscapes.  It's not as colorful as the artistic model.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import fastai\\n\",\n    \"from fastai import *\\n\",\n    \"from fastai.vision import *\\n\",\n    \"from fastai.callbacks.tensorboard import *\\n\",\n    \"from fastai.vision.gan import *\\n\",\n    \"from deoldify.generators import *\\n\",\n    \"from deoldify.critics import *\\n\",\n    \"from deoldify.dataset import *\\n\",\n    \"from deoldify.loss import *\\n\",\n    \"from deoldify.save import *\\n\",\n    \"from PIL import Image, ImageDraw, ImageFont\\n\",\n    \"from PIL import ImageFile\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"path = Path('data/imagenet/ILSVRC/Data/CLS-LOC')\\n\",\n    \"path_hr = path\\n\",\n    \"path_lr = path/'bandw'\\n\",\n    \"\\n\",\n    \"proj_id = 'StableModel'\\n\",\n    \"\\n\",\n    \"gen_name = proj_id + '_gen'\\n\",\n    \"pre_gen_name = gen_name + '_0'\\n\",\n    \"crit_name = proj_id + '_crit'\\n\",\n    \"\\n\",\n    \"name_gen = proj_id + '_image_gen'\\n\",\n    \"path_gen = path/name_gen\\n\",\n    \"\\n\",\n    \"TENSORBOARD_PATH = Path('data/tensorboard/' + proj_id)\\n\",\n    \"\\n\",\n    \"nf_factor = 2\\n\",\n    \"pct_start = 1e-8\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def get_data(bs:int, sz:int, keep_pct:float):\\n\",\n    \"    return get_colorize_data(sz=sz, bs=bs, crappy_path=path_lr, good_path=path_hr, \\n\",\n    \"                             random_seed=None, keep_pct=keep_pct)\\n\",\n    \"\\n\",\n    \"def get_crit_data(classes, bs, sz):\\n\",\n    \"    src = ImageList.from_folder(path, include=classes, recurse=True).split_by_rand_pct(0.1, seed=42)\\n\",\n    \"    ll = src.label_from_folder(classes=classes)\\n\",\n    \"    data = (ll.transform(get_transforms(max_zoom=2.), size=sz)\\n\",\n    \"           .databunch(bs=bs).normalize(imagenet_stats))\\n\",\n    \"    return data\\n\",\n    \"\\n\",\n    \"def create_training_images(fn,i):\\n\",\n    \"    dest = path_lr/fn.relative_to(path_hr)\\n\",\n    \"    dest.parent.mkdir(parents=True, exist_ok=True)\\n\",\n    \"    img = PIL.Image.open(fn).convert('LA').convert('RGB')\\n\",\n    \"    img.save(dest)  \\n\",\n    \"    \\n\",\n    \"def save_preds(dl):\\n\",\n    \"    i=0\\n\",\n    \"    names = dl.dataset.items\\n\",\n    \"    \\n\",\n    \"    for b in dl:\\n\",\n    \"        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\\n\",\n    \"        for o in preds:\\n\",\n    \"            o.save(path_gen/names[i].name)\\n\",\n    \"            i += 1\\n\",\n    \"    \\n\",\n    \"def save_gen_images():\\n\",\n    \"    if path_gen.exists(): shutil.rmtree(path_gen)\\n\",\n    \"    path_gen.mkdir(exist_ok=True)\\n\",\n    \"    data_gen = get_data(bs=bs, sz=sz, keep_pct=0.085)\\n\",\n    \"    save_preds(data_gen.fix_dl)\\n\",\n    \"    PIL.Image.open(path_gen.ls()[0])\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Create black and white training images\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Only runs if the directory isn't already created.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if not path_lr.exists():\\n\",\n    \"    il = ImageList.from_folder(path_hr)\\n\",\n    \"    parallel(create_training_images, il.items)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Pre-train generator\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Most of the training takes place here in pretraining for NoGAN.  The goal here is to take the generator as far as possible with conventional training, as that is much easier to control and obtain glitch-free results compared to GAN training.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 64px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=88\\n\",\n    \"sz=64\\n\",\n    \"keep_pct=1.0\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.callback_fns.append(partial(ImageGenTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GenPre'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=0.8, max_lr=slice(1e-3))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start,  max_lr=slice(3e-7, 3e-4))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 128px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=20\\n\",\n    \"sz=128\\n\",\n    \"keep_pct=1.0\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(1e-7,1e-4))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 192px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=192\\n\",\n    \"keep_pct=0.50\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(5e-8,5e-5))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Repeatable GAN Cycle\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Best results so far have been based on repeating the cycle below a few times (about 5-8?), until diminishing returns are hit (no improvement in image quality).  Each time you repeat the cycle, you want to increment that old_checkpoint_num by 1 so that new check points don't overwrite the old.  \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"old_checkpoint_num = 0\\n\",\n    \"checkpoint_num = old_checkpoint_num + 1\\n\",\n    \"gen_old_checkpoint_name = gen_name + '_' + str(old_checkpoint_num)\\n\",\n    \"gen_new_checkpoint_name = gen_name + '_' + str(checkpoint_num)\\n\",\n    \"crit_old_checkpoint_name = crit_name + '_' + str(old_checkpoint_num)\\n\",\n    \"crit_new_checkpoint_name= crit_name + '_' + str(checkpoint_num)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Save Generated Images\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=192\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"save_gen_images()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Pretrain Critic\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"##### Only need full pretraining of critic when starting from scratch.  Otherwise, just finetune!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if old_checkpoint_num == 0:\\n\",\n    \"    bs=64\\n\",\n    \"    sz=128\\n\",\n    \"    learn_gen=None\\n\",\n    \"    gc.collect()\\n\",\n    \"    data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\\n\",\n    \"    data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\\n\",\n    \"    learn_critic = colorize_crit_learner(data=data_crit, nf=256)\\n\",\n    \"    learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\\n\",\n    \"    learn_critic.fit_one_cycle(6, 1e-3)\\n\",\n    \"    learn_critic.save(crit_old_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=16\\n\",\n    \"sz=192\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic = colorize_crit_learner(data=data_crit, nf=256).load(crit_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.fit_one_cycle(4, 1e-4)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.save(crit_new_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### GAN\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit=None\\n\",\n    \"learn_gen=None\\n\",\n    \"gc.collect()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"lr=2e-5\\n\",\n    \"sz=192\\n\",\n    \"bs=5\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit = colorize_crit_learner(data=data_crit, nf=256).load(crit_new_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\\n\",\n    \"learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.0,1.5), show_img=False, switcher=switcher,\\n\",\n    \"                                 opt_func=partial(optim.Adam, betas=(0.,0.9)), wd=1e-3)\\n\",\n    \"learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))\\n\",\n    \"learn.callback_fns.append(partial(GANTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GanLearner', visual_iters=100))\\n\",\n    \"learn.callback_fns.append(partial(GANSaveCallback, learn_gen=learn_gen, filename=gen_new_checkpoint_name, save_iters=100))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Instructions:  \\n\",\n    \"Find the checkpoint just before where glitches start to be introduced.  This is all very new so you may need to play around with just how far you go here with keep_pct.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn.data = get_data(sz=sz, bs=bs, keep_pct=0.03)\\n\",\n    \"learn_gen.freeze_to(-1)\\n\",\n    \"learn.fit(1,lr)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "ColorizeTrainingStableLargeBatch.ipynb",
          "type": "blob",
          "size": 18.44921875,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Stable Model Training (Large Batch/Limited GPU Memory Support)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## IMPORTANT: Training has -not- been verified by myself for this notebook ~jantic\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTES:  \\n\",\n    \"* This is \\\"NoGAN\\\" based training, described in the DeOldify readme.\\n\",\n    \"* This model prioritizes stable and reliable renderings.  It does particularly well on portraits and landscapes.  It's not as colorful as the artistic model.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"os.environ['CUDA_VISIBLE_DEVICES']='0' \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import fastai\\n\",\n    \"from fastai import *\\n\",\n    \"from fastai.vision import *\\n\",\n    \"from fastai.callbacks.tensorboard import *\\n\",\n    \"from fastai.vision.gan import *\\n\",\n    \"from deoldify.generators import *\\n\",\n    \"from deoldify.critics import *\\n\",\n    \"from deoldify.dataset import *\\n\",\n    \"from deoldify.loss import *\\n\",\n    \"from deoldify.save import *\\n\",\n    \"from PIL import Image, ImageDraw, ImageFont\\n\",\n    \"from PIL import ImageFile\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Activate Large Model Support for PyTorch\\n\",\n    \"This will allow us to fit the model within a GPU with smaller memory capacity (e.g. GTX 1070 8Gb).\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Large Model Support (LMS) is a feature provided in IBM Watson Machine Learning Community Edition (WML-CE) PyTorch V1.1.0 that allows the successful training of deep learning models that would otherwise exhaust GPU memory and abort with out-of-memory errors. LMS manages this oversubscription of GPU memory by temporarily swapping tensors to host memory when they are not needed. One or more elements of a deep learning model can lead to GPU memory exhaustion.\\n\",\n    \"\\n\",\n    \"Requires the use of IBM WML-CE (Available here: https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.6.1/welcome/welcome.html)\\n\",\n    \"\\n\",\n    \"Further Reading on PyTorch with Large Model Support: https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.6.1/navigation/wmlce_getstarted_pytorch.html\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import shutil\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Set limit of GPU used before swapping to tensors to host memory\\n\",\n    \"max_gpu_mem = 7\\n\",\n    \"\\n\",\n    \"def gb_to_bytes(gb):\\n\",\n    \"    return gb*1024*1024*1024\\n\",\n    \"\\n\",\n    \"# Enable PyTorch LMS\\n\",\n    \"torch.cuda.set.enabled_lms(True)\\n\",\n    \"# Set LMS limit\\n\",\n    \"torch.cuda.set_limit_lms(gb_to_bytes(max_gpu_mem))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Check LMS is enabled\\n\",\n    \"torch.cuda.get_enabled_lms()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Check LMS Limit has been set\\n\",\n    \"torch.cuda.get_limit_lms()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \" \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Path to Training Data\\n\",\n    \"path = Path('data/imagenet/ILSVRC/Data/CLS-LOC')\\n\",\n    \"path_hr = path\\n\",\n    \"\\n\",\n    \"# Path to Black and White images\\n\",\n    \"path_bandw = Path('/training/DeOldify')\\n\",\n    \"path_lr = path_bandw/'bandw'\\n\",\n    \"\\n\",\n    \"# Name of Model\\n\",\n    \"proj_id = 'StableModel'\\n\",\n    \"\\n\",\n    \"# Name of Generator\\n\",\n    \"gen_name = proj_id + '_gen'\\n\",\n    \"pre_gen_name = gen_name + '_0'\\n\",\n    \"\\n\",\n    \"# Name of Critic\\n\",\n    \"crit_name = proj_id + '_crit'\\n\",\n    \"\\n\",\n    \"# Name of Generated Images folder, located within the Black and White folder\\n\",\n    \"name_gen = proj_id + '_image_gen'\\n\",\n    \"path_gen = path/name_gen\\n\",\n    \"\\n\",\n    \"# Path to tensorboard data\\n\",\n    \"TENSORBOARD_PATH = Path('data/tensorboard/' + proj_id)\\n\",\n    \"\\n\",\n    \"nf_factor = 2\\n\",\n    \"pct_start = 1e-8\\n\",\n    \"\\n\",\n    \"# Number of workers for DataLoader\\n\",\n    \"num_works = 2\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def get_data(bs:int, sz:int, keep_pct:float):\\n\",\n    \"    return get_colorize_data(sz=sz, bs=bs, crappy_path=path_lr, good_path=path_hr, \\n\",\n    \"                             random_seed=None, keep_pct=keep_pct, num_workers=num_works)\\n\",\n    \"\\n\",\n    \"def get_crit_data(classes, bs, sz):\\n\",\n    \"    src = ImageList.from_folder(path, include=classes, recurse=True).split_by_rand_pct(0.1, seed=42)\\n\",\n    \"    ll = src.label_from_folder(classes=classes)\\n\",\n    \"    data = (ll.transform(get_transforms(max_zoom=2.), size=sz)\\n\",\n    \"           .databunch(bs=bs).normalize(imagenet_stats))\\n\",\n    \"    return data\\n\",\n    \"\\n\",\n    \"def create_training_images(fn,i):\\n\",\n    \"    dest = path_lr/fn.relative_to(path_hr)\\n\",\n    \"    dest.parent.mkdir(parents=True, exist_ok=True)\\n\",\n    \"    img = PIL.Image.open(fn).convert('LA').convert('RGB')\\n\",\n    \"    img.save(dest)  \\n\",\n    \"    \\n\",\n    \"def save_preds(dl):\\n\",\n    \"    i=0\\n\",\n    \"    names = dl.dataset.items\\n\",\n    \"    \\n\",\n    \"    for b in dl:\\n\",\n    \"        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\\n\",\n    \"        for o in preds:\\n\",\n    \"            o.save(path_gen/names[i].name)\\n\",\n    \"            i += 1\\n\",\n    \"    \\n\",\n    \"def save_gen_images():\\n\",\n    \"    if path_gen.exists(): shutil.rmtree(path_gen)\\n\",\n    \"    path_gen.mkdir(exist_ok=True)\\n\",\n    \"    data_gen = get_data(bs=bs, sz=sz, keep_pct=0.085)\\n\",\n    \"    save_preds(data_gen.fix_dl)\\n\",\n    \"    PIL.Image.open(path_gen.ls()[0])\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Create black and white training images\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Only runs if the directory isn't already created.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if not path_lr.exists():\\n\",\n    \"    il = ImageList.from_folder(path_hr)\\n\",\n    \"    parallel(create_training_images, il.items)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Pre-train generator\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Most of the training takes place here in pretraining for NoGAN.  The goal here is to take the generator as far as possible with conventional training, as that is much easier to control and obtain glitch-free results compared to GAN training.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 64px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=88 # This can be increased if using PyTorch LMS, training could be slower.\\n\",\n    \"sz=64\\n\",\n    \"keep_pct=1.0\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.callback_fns.append(partial(ImageGenTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GenPre'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=0.8, max_lr=slice(1e-3))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start,  max_lr=slice(3e-7, 3e-4))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 128px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=40 # This can be increased if using PyTorch LMS, training could be slower.\\n\",\n    \"sz=128\\n\",\n    \"keep_pct=1.0\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(1e-7,1e-4))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 192px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=16 # This can be increased if using PyTorch LMS, training could be slower.\\n\",\n    \"sz=192\\n\",\n    \"keep_pct=0.50\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(5e-8,5e-5))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 256px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8 # This can be increased if using PyTorch LMS, training could be slower.\\n\",\n    \"sz=256\\n\",\n    \"keep_pct=0.50\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(5e-8,5e-5))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Repeatable GAN Cycle\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Best results so far have been based on repeating the cycle below a few times (about 5-8?), until diminishing returns are hit (no improvement in image quality).  Each time you repeat the cycle, you want to increment that old_checkpoint_num by 1 so that new check points don't overwrite the old.  \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"old_checkpoint_num = 0\\n\",\n    \"checkpoint_num = old_checkpoint_num + 1\\n\",\n    \"gen_old_checkpoint_name = gen_name + '_' + str(old_checkpoint_num)\\n\",\n    \"gen_new_checkpoint_name = gen_name + '_' + str(checkpoint_num)\\n\",\n    \"crit_old_checkpoint_name = crit_name + '_' + str(old_checkpoint_num)\\n\",\n    \"crit_new_checkpoint_name= crit_name + '_' + str(checkpoint_num)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Save Generated Images\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=256\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"save_gen_images()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Pretrain Critic\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"##### Only need full pretraining of critic when starting from scratch.  Otherwise, just finetune!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if old_checkpoint_num == 0:\\n\",\n    \"    bs=64\\n\",\n    \"    sz=128\\n\",\n    \"    learn_gen=None\\n\",\n    \"    gc.collect()\\n\",\n    \"    data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\\n\",\n    \"    data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\\n\",\n    \"    learn_critic = colorize_crit_learner(data=data_crit, nf=256)\\n\",\n    \"    learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\\n\",\n    \"    learn_critic.fit_one_cycle(6, 1e-3)\\n\",\n    \"    learn_critic.save(crit_old_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=256\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic = colorize_crit_learner(data=data_crit, nf=256).load(crit_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.fit_one_cycle(4, 1e-4)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.save(crit_new_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### GAN\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit=None\\n\",\n    \"learn_gen=None\\n\",\n    \"gc.collect()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"lr=2e-5\\n\",\n    \"sz=256\\n\",\n    \"bs=5\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit = colorize_crit_learner(data=data_crit, nf=256).load(crit_new_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\\n\",\n    \"learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.0,1.5), show_img=False, switcher=switcher,\\n\",\n    \"                                 opt_func=partial(optim.Adam, betas=(0.,0.9)), wd=1e-3)\\n\",\n    \"learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))\\n\",\n    \"learn.callback_fns.append(partial(GANTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GanLearner', visual_iters=100))\\n\",\n    \"learn.callback_fns.append(partial(GANSaveCallback, learn_gen=learn_gen, filename=gen_new_checkpoint_name, save_iters=100))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Instructions:  \\n\",\n    \"Find the checkpoint just before where glitches start to be introduced.  This is all very new so you may need to play around with just how far you go here with keep_pct.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn.data = get_data(sz=sz, bs=bs, keep_pct=0.03)\\n\",\n    \"learn_gen.freeze_to(-1)\\n\",\n    \"learn.fit(1,lr)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        },
        {
          "name": "ColorizeTrainingVideo.ipynb",
          "type": "blob",
          "size": 11.7529296875,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Video Model Training\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTES:  \\n\",\n    \"* It's assumed that there's a pretrained generator from the ColorizeTrainingStable notebook available at the specified path.\\n\",\n    \"* This is \\\"NoGAN\\\" based training, described in the DeOldify readme.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import fastai\\n\",\n    \"from fastai import *\\n\",\n    \"from fastai.vision import *\\n\",\n    \"from fastai.callbacks.tensorboard import *\\n\",\n    \"from fastai.vision.gan import *\\n\",\n    \"from deoldify.generators import *\\n\",\n    \"from deoldify.critics import *\\n\",\n    \"from deoldify.dataset import *\\n\",\n    \"from deoldify.loss import *\\n\",\n    \"from deoldify.save import *\\n\",\n    \"from deoldify.augs import noisify \\n\",\n    \"from PIL import Image, ImageDraw, ImageFont\\n\",\n    \"from PIL import ImageFile\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"path = Path('data/imagenet/ILSVRC/Data/CLS-LOC')\\n\",\n    \"path_hr = path\\n\",\n    \"path_lr = path/'bandw'\\n\",\n    \"\\n\",\n    \"proj_id = 'VideoModel'\\n\",\n    \"gen_name = proj_id + '_gen'\\n\",\n    \"pre_gen_name = gen_name + '_0'\\n\",\n    \"crit_name = proj_id + '_crit'\\n\",\n    \"\\n\",\n    \"name_gen = proj_id + '_image_gen'\\n\",\n    \"path_gen = path/name_gen\\n\",\n    \"\\n\",\n    \"TENSORBOARD_PATH = Path('data/tensorboard/' + proj_id)\\n\",\n    \"\\n\",\n    \"nf_factor = 2\\n\",\n    \"xtra_tfms=[noisify(p=0.8)]\\n\",\n    \"pct_start = 1e-8\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def get_data(bs:int, sz:int, keep_pct:float):\\n\",\n    \"    return get_colorize_data(sz=sz, bs=bs, crappy_path=path_lr, good_path=path_hr, \\n\",\n    \"                             random_seed=None, keep_pct=keep_pct, xtra_tfms=xtra_tfms)\\n\",\n    \"\\n\",\n    \"def get_crit_data(classes, bs, sz):\\n\",\n    \"    src = ImageList.from_folder(path, include=classes, recurse=True).split_by_rand_pct(0.1, seed=42)\\n\",\n    \"    ll = src.label_from_folder(classes=classes)\\n\",\n    \"    data = (ll.transform(get_transforms(max_zoom=2.), size=sz)\\n\",\n    \"           .databunch(bs=bs).normalize(imagenet_stats))\\n\",\n    \"    return data\\n\",\n    \"\\n\",\n    \"def create_training_images(fn,i):\\n\",\n    \"    dest = path_lr/fn.relative_to(path_hr)\\n\",\n    \"    dest.parent.mkdir(parents=True, exist_ok=True)\\n\",\n    \"    img = PIL.Image.open(fn).convert('LA').convert('RGB')\\n\",\n    \"    img.save(dest)  \\n\",\n    \"    \\n\",\n    \"def save_preds(dl):\\n\",\n    \"    i=0\\n\",\n    \"    names = dl.dataset.items\\n\",\n    \"    \\n\",\n    \"    for b in dl:\\n\",\n    \"        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\\n\",\n    \"        for o in preds:\\n\",\n    \"            o.save(path_gen/names[i].name)\\n\",\n    \"            i += 1\\n\",\n    \"            \\n\",\n    \"def save_gen_images():\\n\",\n    \"    if path_gen.exists(): shutil.rmtree(path_gen)\\n\",\n    \"    path_gen.mkdir(exist_ok=True)\\n\",\n    \"    data_gen = get_data(bs=bs, sz=sz, keep_pct=0.085)\\n\",\n    \"    save_preds(data_gen.fix_dl)\\n\",\n    \"    PIL.Image.open(path_gen.ls()[0])\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Create black and white training images\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Only runs if the directory isn't already created.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if not path_lr.exists():\\n\",\n    \"    il = ImageList.from_folder(path_hr)\\n\",\n    \"    parallel(create_training_images, il.items)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Finetune Generator With Noise Augmented Images.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"##### This helps the generator better deal with noisy/grainy video (which is pretty normal).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=192\\n\",\n    \"keep_pct=0.25\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.callback_fns.append(partial(ImageGenTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GenPre'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = learn_gen.load(pre_gen_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(5e-8,5e-5))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Repeatable GAN Cycle\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Best results so far have been based only doing a single run of the cells below (otherwise glitches are introduced that are visible in video).  \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"old_checkpoint_num = 0\\n\",\n    \"checkpoint_num = old_checkpoint_num + 1\\n\",\n    \"gen_old_checkpoint_name = gen_name + '_' + str(old_checkpoint_num)\\n\",\n    \"gen_new_checkpoint_name = gen_name + '_' + str(checkpoint_num)\\n\",\n    \"crit_old_checkpoint_name = crit_name + '_' + str(old_checkpoint_num)\\n\",\n    \"crit_new_checkpoint_name= crit_name + '_' + str(checkpoint_num)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Save Generated Images\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=192\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"save_gen_images()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Pretrain Critic\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=16\\n\",\n    \"sz=192\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen=None\\n\",\n    \"gc.collect()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic = colorize_crit_learner(data=data_crit, nf=256).load(crit_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.callback_fns.append(partial(LearnerTensorboardWriter, base_dir=TENSORBOARD_PATH, name='CriticPre'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.fit_one_cycle(4, 1e-4)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_critic.save(crit_new_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### GAN\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit=None\\n\",\n    \"learn_gen=None\\n\",\n    \"gc.collect()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"lr=5e-6\\n\",\n    \"sz=192\\n\",\n    \"bs=5\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit = colorize_crit_learner(data=data_crit, nf=256).load(crit_new_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\\n\",\n    \"learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.0,1.5), show_img=False, switcher=switcher,\\n\",\n    \"                                 opt_func=partial(optim.Adam, betas=(0.,0.9)), wd=1e-3)\\n\",\n    \"learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))\\n\",\n    \"learn.callback_fns.append(partial(GANTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GanLearner', visual_iters=100, stats_iters=10, loss_iters=1))\\n\",\n    \"learn.callback_fns.append(partial(GANSaveCallback, learn_gen=learn_gen, filename=gen_new_checkpoint_name, save_iters=100))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Instructions:  \\n\",\n    \"Find the checkpoint just before where glitches start to be introduced.  So far this has been found at the point of iterating through 1.4% of the data when using learning rate of 1e-5, and at 2.2% of the data for 5e-6.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn.data = get_data(sz=sz, bs=bs, keep_pct=0.03)\\n\",\n    \"learn_gen.freeze_to(-1)\\n\",\n    \"learn.fit(1,lr)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "ColorizeTrainingWandb.ipynb",
          "type": "blob",
          "size": 26.4189453125,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Stable Model Training with monitoring through Weights & Biases\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTES:  \\n\",\n    \"* This is \\\"NoGAN\\\" based training, described in the DeOldify readme.\\n\",\n    \"* This model prioritizes stable and reliable renderings.  It does particularly well on portraits and landscapes.  It's not as colorful as the artistic model.\\n\",\n    \"* Training with this notebook has been logged and monitored through [Weights & Biases](https://www.wandb.com/). Refer to [W&B Report](https://app.wandb.ai/borisd13/DeOldify/reports?view=borisd13%2FDeOldify).\\n\",\n    \"* It is **highly** recommended to use a 11 Go GPU to run this notebook. Anything lower will require to reduce the batch size (leading to moro instability) or use of \\\"Large Model Support\\\" from IBM WML-CE (not so easy to setup). An alternative is to rent ressources online.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Install W&B Callback\\n\",\n    \"#!pip install wandb\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import fastai\\n\",\n    \"from fastai import *\\n\",\n    \"from fastai.vision import *\\n\",\n    \"from fastai.vision.gan import *\\n\",\n    \"from deoldify.generators import *\\n\",\n    \"from deoldify.critics import *\\n\",\n    \"from deoldify.dataset import *\\n\",\n    \"from deoldify.loss import *\\n\",\n    \"from deoldify.save import *\\n\",\n    \"from PIL import Image, ImageDraw, ImageFont\\n\",\n    \"from PIL import ImageFile\\n\",\n    \"from torch.utils.data.sampler import RandomSampler, SequentialSampler\\n\",\n    \"from tqdm import tqdm\\n\",\n    \"import wandb\\n\",\n    \"from wandb.fastai import WandbCallback\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Set up W&B: checks user can connect to W&B servers\\n\",\n    \"# Note: set up API key the first time\\n\",\n    \"wandb.login()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Dataset can be downloaded from https://www.kaggle.com/c/imagenet-object-localization-challenge/data\\n\",\n    \"path = Path('data/imagenet/ILSVRC/Data/CLS-LOC')\\n\",\n    \"path_hr = path\\n\",\n    \"path_lr = path/'bandw'\\n\",\n    \"\\n\",\n    \"proj_id = 'StableModel'\\n\",\n    \"\\n\",\n    \"gen_name = proj_id + '_gen'\\n\",\n    \"pre_gen_name = gen_name + '_0'\\n\",\n    \"crit_name = proj_id + '_crit'\\n\",\n    \"\\n\",\n    \"name_gen = proj_id + '_image_gen'\\n\",\n    \"path_gen = path/name_gen\\n\",\n    \"\\n\",\n    \"nf_factor = 2\\n\",\n    \"pct_start = 1e-8\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Iterating through the dataset\\n\",\n    \"\\n\",\n    \"The dataset is very large and it would take a long time to iterate through all the samples at each epoch.\\n\",\n    \"\\n\",\n    \"We use custom samplers in order to limit epochs to subsets of data while still iterating slowly through the entire dataset (epoch after epoch). This let us run the validation loop more often where we log metrics as well as prediction samples on validation data.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Reduce quantity of samples per training epoch\\n\",\n    \"# Adapted from https://forums.fast.ai/t/epochs-of-arbitrary-length/27777/10\\n\",\n    \"\\n\",\n    \"@classmethod\\n\",\n    \"def create(cls, train_ds:Dataset, valid_ds:Dataset, test_ds:Optional[Dataset]=None, path:PathOrStr='.', bs:int=64,\\n\",\n    \"            val_bs:int=None, num_workers:int=defaults.cpus, dl_tfms:Optional[Collection[Callable]]=None,\\n\",\n    \"            device:torch.device=None, collate_fn:Callable=data_collate, no_check:bool=False, sampler=None, **dl_kwargs)->'DataBunch':\\n\",\n    \"    \\\"Create a `DataBunch` from `train_ds`, `valid_ds` and maybe `test_ds` with a batch size of `bs`. Passes `**dl_kwargs` to `DataLoader()`\\\"\\n\",\n    \"    datasets = cls._init_ds(train_ds, valid_ds, test_ds)\\n\",\n    \"    val_bs = ifnone(val_bs, bs)\\n\",\n    \"    if sampler is None: sampler = [RandomSampler] + 3*[SequentialSampler]\\n\",\n    \"    dls = [DataLoader(d, b, sampler=sa(d), drop_last=sh, num_workers=num_workers, **dl_kwargs) for d,b,sh,sa in\\n\",\n    \"            zip(datasets, (bs,val_bs,val_bs,val_bs), (True,False,False,False), sampler) if d is not None]\\n\",\n    \"    return cls(*dls, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)\\n\",\n    \"\\n\",\n    \"ImageDataBunch.create = create\\n\",\n    \"ImageImageList._bunch = ImageDataBunch\\n\",\n    \"\\n\",\n    \"class FixedLenRandomSampler(RandomSampler):\\n\",\n    \"    def __init__(self, data_source, epoch_size):\\n\",\n    \"        super().__init__(data_source)\\n\",\n    \"        self.epoch_size = epoch_size\\n\",\n    \"        self.not_sampled = np.array([True]*len(data_source))\\n\",\n    \"    \\n\",\n    \"    @property\\n\",\n    \"    def reset_state(self): self.not_sampled[:] = True\\n\",\n    \"        \\n\",\n    \"    def __iter__(self):\\n\",\n    \"        ns = sum(self.not_sampled)\\n\",\n    \"        idx_last = []\\n\",\n    \"        if ns >= len(self):\\n\",\n    \"            idx = np.random.choice(np.where(self.not_sampled)[0], size=len(self), replace=False).tolist()\\n\",\n    \"            if ns == len(self): self.reset_state\\n\",\n    \"        else:\\n\",\n    \"            idx_last = np.where(self.not_sampled)[0].tolist()\\n\",\n    \"            self.reset_state\\n\",\n    \"            idx = np.random.choice(np.where(self.not_sampled)[0], size=len(self)-len(idx_last), replace=False).tolist()\\n\",\n    \"        self.not_sampled[idx] = False\\n\",\n    \"        idx = [*idx_last, *idx]\\n\",\n    \"        return iter(idx)\\n\",\n    \"    \\n\",\n    \"    def __len__(self):\\n\",\n    \"        return self.epoch_size\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def get_data(bs:int, sz:int, keep_pct=1.0, random_seed=None, valid_pct=0.2, epoch_size=1000):\\n\",\n    \"    \\n\",\n    \"    # Create samplers\\n\",\n    \"    train_sampler = partial(FixedLenRandomSampler, epoch_size=epoch_size)\\n\",\n    \"    samplers = [train_sampler, SequentialSampler, SequentialSampler, SequentialSampler]\\n\",\n    \"\\n\",\n    \"    return get_colorize_data(sz=sz, bs=bs, crappy_path=path_lr, good_path=path_hr, random_seed=random_seed,\\n\",\n    \"                             keep_pct=keep_pct, samplers=samplers, valid_pct=valid_pct)\\n\",\n    \"\\n\",\n    \"# Function modified to allow use of custom samplers\\n\",\n    \"def get_colorize_data(sz:int, bs:int, crappy_path:Path, good_path:Path, random_seed:int=None,\\n\",\n    \"        keep_pct:float=1.0, num_workers:int=8, samplers=None, valid_pct=0.2, xtra_tfms=[])->ImageDataBunch:\\n\",\n    \"    src = (ImageImageList.from_folder(crappy_path, convert_mode='RGB')\\n\",\n    \"        .use_partial_data(sample_pct=keep_pct, seed=random_seed)\\n\",\n    \"        .split_by_rand_pct(valid_pct, seed=random_seed))\\n\",\n    \"    data = (src.label_from_func(lambda x: good_path/x.relative_to(crappy_path))\\n\",\n    \"        .transform(get_transforms(max_zoom=1.2, max_lighting=0.5, max_warp=0.25, xtra_tfms=xtra_tfms), size=sz, tfm_y=True)\\n\",\n    \"        .databunch(bs=bs, num_workers=num_workers, sampler=samplers, no_check=True)\\n\",\n    \"        .normalize(imagenet_stats, do_y=True))\\n\",\n    \"    data.c = 3\\n\",\n    \"    return data\\n\",\n    \"\\n\",\n    \"# Function to limit amount of data in critic\\n\",\n    \"def filter_data(pct=1.0):\\n\",\n    \"    def _f(fname):\\n\",\n    \"        if 'test' in str(fname):\\n\",\n    \"            if np.random.random_sample() > pct:\\n\",\n    \"                return False\\n\",\n    \"        return True\\n\",\n    \"    return _f\\n\",\n    \"\\n\",\n    \"def get_crit_data(classes, bs, sz, pct=1.0):\\n\",\n    \"    src = ImageList.from_folder(path, include=classes, recurse=True).filter_by_func(filter_data(pct)).split_by_rand_pct(0.1)\\n\",\n    \"    ll = src.label_from_folder(classes=classes)\\n\",\n    \"    data = (ll.transform(get_transforms(max_zoom=2.), size=sz)\\n\",\n    \"           .databunch(bs=bs).normalize(imagenet_stats))\\n\",\n    \"    return data\\n\",\n    \"\\n\",\n    \"def create_training_images(fn,i):\\n\",\n    \"    dest = path_lr/fn.relative_to(path_hr)\\n\",\n    \"    dest.parent.mkdir(parents=True, exist_ok=True)\\n\",\n    \"    img = PIL.Image.open(fn).convert('LA').convert('RGB')\\n\",\n    \"    img.save(dest)  \\n\",\n    \"    \\n\",\n    \"def save_preds(dl):\\n\",\n    \"    i=0\\n\",\n    \"    names = dl.dataset.items    \\n\",\n    \"    for b in tqdm(dl):\\n\",\n    \"        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\\n\",\n    \"        for o in preds:\\n\",\n    \"            o.save(path_gen/names[i].name)\\n\",\n    \"            i += 1\\n\",\n    \"    \\n\",\n    \"def save_gen_images(keep_pct):\\n\",\n    \"    if path_gen.exists(): shutil.rmtree(path_gen)\\n\",\n    \"    path_gen.mkdir(exist_ok=True)\\n\",\n    \"    data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)\\n\",\n    \"    save_preds(data_gen.fix_dl)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Create black and white training images\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Only runs if the directory isn't already created.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if not path_lr.exists():\\n\",\n    \"    il = ImageList.from_folder(path_hr)\\n\",\n    \"    parallel(create_training_images, il.items)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Number of black & white images\\n\",\n    \"data_size = len(list(path_lr.rglob('*.*')))\\n\",\n    \"print('Number of black & white images:', data_size)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Pre-train generator\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Most of the training takes place here in pretraining for NoGAN.  The goal here is to take the generator as far as possible with conventional training, as that is much easier to control and obtain glitch-free results compared to GAN training.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 64px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Init logging of a new run\\n\",\n    \"wandb.init(tags=['Pre-train Gen'])  # tags are optional\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=88\\n\",\n    \"sz=64\\n\",\n    \"\\n\",\n    \"# Define target number of training/validation samples as well as number of epochs\\n\",\n    \"epoch_train_size = 100 * bs\\n\",\n    \"epoch_valid_size = 10 * bs\\n\",\n    \"valid_pct = epoch_valid_size / data_size\\n\",\n    \"number_epochs = (data_size - epoch_valid_size) // epoch_train_size\\n\",\n    \"\\n\",\n    \"# Log hyper parameters\\n\",\n    \"wandb.config.update({\\\"Step 1 - batch size\\\": bs, \\\"Step 1 - image size\\\": sz,\\n\",\n    \"                     \\\"Step 1 - epoch size\\\": epoch_train_size, \\\"Step 1 - number epochs\\\": number_epochs})\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_gen = get_data(bs=bs, sz=sz, random_seed=123, valid_pct=valid_pct, epoch_size=100*bs)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.callback_fns.append(partial(WandbCallback,\\n\",\n    \"                                      input_type='images'))  # log prediction samples\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(number_epochs, pct_start=0.8, max_lr=slice(1e-3))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(number_epochs, pct_start=pct_start,  max_lr=slice(3e-7, 3e-4))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 128px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=20\\n\",\n    \"sz=128\\n\",\n    \"\\n\",\n    \"# Define target number of training/validation samples as well as number of epochs\\n\",\n    \"epoch_train_size = 100 * bs\\n\",\n    \"epoch_valid_size = 10 * bs\\n\",\n    \"valid_pct = epoch_valid_size / data_size\\n\",\n    \"number_epochs = (data_size - epoch_valid_size) // epoch_train_size\\n\",\n    \"\\n\",\n    \"# Log hyper parameters\\n\",\n    \"wandb.config.update({\\\"Step 2 - batch size\\\": bs, \\\"Step 2 - image size\\\": sz,\\n\",\n    \"                     \\\"Step 2 - epoch size\\\": epoch_train_size, \\\"Step 2 - number epochs\\\": number_epochs})\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(bs=bs, sz=sz, random_seed=123, valid_pct=valid_pct, epoch_size=100*bs)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(number_epochs, pct_start=pct_start, max_lr=slice(1e-7,1e-4))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### 192px\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=192\\n\",\n    \"\\n\",\n    \"# Define target number of training/validation samples as well as number of epochs\\n\",\n    \"epoch_train_size = 100 * bs\\n\",\n    \"epoch_valid_size = 10 * bs\\n\",\n    \"valid_pct = epoch_valid_size / data_size\\n\",\n    \"number_epochs = (data_size - epoch_valid_size) // epoch_train_size // 2  # Training is long - we use half of data\\n\",\n    \"\\n\",\n    \"# Log hyper parameters\\n\",\n    \"wandb.config.update({\\\"Step 3 - batch size\\\": bs, \\\"Step 3 - image size\\\": sz,\\n\",\n    \"                     \\\"Step 3 - epoch size\\\": epoch_train_size, \\\"Step 3 - number epochs\\\": number_epochs})\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.data = get_data(bs=bs, sz=sz, random_seed=123, valid_pct=valid_pct, epoch_size=100*bs)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.unfreeze()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.fit_one_cycle(number_epochs, pct_start=pct_start, max_lr=slice(5e-8,5e-5))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen.save(pre_gen_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# End logging of current session run\\n\",\n    \"# Note: this is optional and would be automatically triggered when stopping the kernel\\n\",\n    \"wandb.join()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Repeatable GAN Cycle\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### NOTE\\n\",\n    \"Best results so far have been based on repeating the cycle below a few times (about 5-8?), until diminishing returns are hit (no improvement in image quality).  Each time you repeat the cycle, you want to increment that old_checkpoint_num by 1 so that new check points don't overwrite the old.  \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"old_checkpoint_num = 0\\n\",\n    \"checkpoint_num = old_checkpoint_num + 1\\n\",\n    \"gen_old_checkpoint_name = gen_name + '_' + str(old_checkpoint_num)\\n\",\n    \"gen_new_checkpoint_name = gen_name + '_' + str(checkpoint_num)\\n\",\n    \"crit_old_checkpoint_name = crit_name + '_' + str(old_checkpoint_num)\\n\",\n    \"crit_new_checkpoint_name= crit_name + '_' + str(checkpoint_num)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Save Generated Images\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=8\\n\",\n    \"sz=192\\n\",\n    \"\\n\",\n    \"# Define target number of training/validation samples as well as number of epochs\\n\",\n    \"epoch_train_size = 100 * bs\\n\",\n    \"epoch_valid_size = 10 * bs\\n\",\n    \"valid_pct = epoch_valid_size / data_size\\n\",\n    \"number_epochs = (data_size - epoch_valid_size) // epoch_train_size\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_gen = get_data(bs=bs, sz=sz, random_seed=123, valid_pct=valid_pct, epoch_size=100*bs)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"save_gen_images(0.1)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Pretrain Critic\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"##### Only need full pretraining of critic when starting from scratch.  Otherwise, just finetune!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if old_checkpoint_num == 0:\\n\",\n    \"    \\n\",\n    \"    # Init logging of a new run\\n\",\n    \"    wandb.init(tags=['Pre-train Crit'])  # tags are optional\\n\",\n    \"    \\n\",\n    \"    bs=64\\n\",\n    \"    sz=128\\n\",\n    \"    learn_gen=None\\n\",\n    \"    \\n\",\n    \"    # Log hyper parameters\\n\",\n    \"    wandb.config.update({\\\"Step 1 - batch size\\\": bs, \\\"Step 1 - image size\\\": sz})\\n\",\n    \"\\n\",\n    \"    gc.collect()    \\n\",\n    \"    data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\\n\",\n    \"    data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\\n\",\n    \"    learn_crit = colorize_crit_learner(data=data_crit, nf=256)\\n\",\n    \"    learn_crit.callback_fns.append(partial(WandbCallback))  # log prediction samples\\n\",\n    \"    learn_crit.fit_one_cycle(6, 1e-3)\\n\",\n    \"    learn_crit.save(crit_old_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"bs=16\\n\",\n    \"sz=192\\n\",\n    \"\\n\",\n    \"# Log hyper parameters\\n\",\n    \"wandb.config.update({\\\"Step 2 - batch size\\\": bs, \\\"Step 2 - image size\\\": sz})\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit = colorize_crit_learner(data=data_crit, nf=256).load(crit_old_checkpoint_name, with_opt=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit.fit_one_cycle(4, 1e-4)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"learn_crit.save(crit_new_checkpoint_name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### GAN\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# free up memory\\n\",\n    \"learn_crit=None\\n\",\n    \"learn_gen=None\\n\",\n    \"learn=None\\n\",\n    \"gc.collect()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Set old_checkpoint_num to last iteration\\n\",\n    \"old_checkpoint_num = 0\\n\",\n    \"save_checkpoints = False\\n\",\n    \"batch_per_epoch = 200\\n\",\n    \"\\n\",\n    \"checkpoint_num = old_checkpoint_num + 1\\n\",\n    \"gen_old_checkpoint_name = gen_name + '_' + str(old_checkpoint_num)\\n\",\n    \"gen_new_checkpoint_name = gen_name + '_' + str(checkpoint_num)\\n\",\n    \"crit_old_checkpoint_name = crit_name + '_' + str(old_checkpoint_num)\\n\",\n    \"crit_new_checkpoint_name= crit_name + '_' + str(checkpoint_num)   \\n\",\n    \"\\n\",\n    \"if False:   # need only to do it once\\n\",\n    \"        \\n\",\n    \"    # Generate data\\n\",\n    \"    print('Generating data')\\n\",\n    \"    bs=8\\n\",\n    \"    sz=192\\n\",\n    \"    epoch_train_size = batch_per_epoch * bs\\n\",\n    \"    epoch_valid_size = batch_per_epoch * bs // 10\\n\",\n    \"    valid_pct = epoch_valid_size / data_size\\n\",\n    \"    data_gen = get_data(bs=bs, sz=sz, epoch_size=epoch_train_size, valid_pct=valid_pct)\\n\",\n    \"    learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\\n\",\n    \"    save_gen_images(0.02)\\n\",\n    \"\\n\",\n    \"    # Pre-train critic\\n\",\n    \"    print('Pre-training critic')\\n\",\n    \"    bs=16\\n\",\n    \"    sz=192\\n\",\n    \"\\n\",\n    \"    len_test = len(list((path / 'test').rglob('*.*')))\\n\",\n    \"    len_gen = len(list((path / name_gen).rglob('*.*')))\\n\",\n    \"    keep_test_pct = len_gen / len_test * 2\\n\",\n    \"\\n\",\n    \"    data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz, pct=keep_test_pct)\\n\",\n    \"    learn_crit = colorize_crit_learner(data=data_crit, nf=256).load(crit_old_checkpoint_name, with_opt=False)\\n\",\n    \"    learn_crit.fit_one_cycle(1, 1e-4)\\n\",\n    \"    learn_crit.save(crit_new_checkpoint_name)\\n\",\n    \"\\n\",\n    \"# Creating GAN\\n\",\n    \"print('Creating GAN')\\n\",\n    \"sz=192\\n\",\n    \"bs=8\\n\",\n    \"lr_GAN=2e-5\\n\",\n    \"epoch_train_size = batch_per_epoch * bs\\n\",\n    \"epoch_valid_size = batch_per_epoch * bs // 10\\n\",\n    \"valid_pct = epoch_valid_size / data_size\\n\",\n    \"len_test = len(list((path / 'test').rglob('*.*')))\\n\",\n    \"len_gen = len(list((path / name_gen).rglob('*.*')))\\n\",\n    \"keep_test_pct = len_gen / len_test * 2\\n\",\n    \"\\n\",\n    \"data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz, pct=keep_test_pct)\\n\",\n    \"learn_crit = colorize_crit_learner(data=data_crit, nf=256).load(crit_new_checkpoint_name, with_opt=False)\\n\",\n    \"data_gen = get_data(bs=bs, sz=sz, epoch_size=epoch_train_size, valid_pct=valid_pct)\\n\",\n    \"learn_gen = gen_learner_wide(data=data_gen, gen_loss=FeatureLoss(), nf_factor=nf_factor).load(gen_old_checkpoint_name, with_opt=False)\\n\",\n    \"switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\\n\",\n    \"learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.0,1.5), show_img=False, switcher=switcher,\\n\",\n    \"                                 opt_func=partial(optim.Adam, betas=(0.,0.9)), wd=1e-3)\\n\",\n    \"learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))\\n\",\n    \"learn.callback_fns.append(partial(WandbCallback, input_type='images', seed=None, save_model=False))\\n\",\n    \"learn.data = get_data(bs=bs, sz=sz, epoch_size=epoch_train_size, valid_pct=valid_pct)\\n\",\n    \"\\n\",\n    \"# Start logging to W&B\\n\",\n    \"wandb.init(tags=['GAN'])\\n\",\n    \"wandb.config.update({\\\"learning rate\\\": lr_GAN})  \\n\",\n    \"\\n\",\n    \"# Run the loop until satisfied with the results\\n\",\n    \"while True:\\n\",\n    \"\\n\",\n    \"    # Current loop\\n\",\n    \"    checkpoint_num = old_checkpoint_num + 1\\n\",\n    \"    gen_old_checkpoint_name = gen_name + '_' + str(old_checkpoint_num)\\n\",\n    \"    gen_new_checkpoint_name = gen_name + '_' + str(checkpoint_num)\\n\",\n    \"    crit_old_checkpoint_name = crit_name + '_' + str(old_checkpoint_num)\\n\",\n    \"    crit_new_checkpoint_name= crit_name + '_' + str(checkpoint_num)      \\n\",\n    \"    \\n\",\n    \"    \\n\",\n    \"    # GAN for 10 epochs between each checkpoint\\n\",\n    \"    try:\\n\",\n    \"        learn.fit(1, lr_GAN)\\n\",\n    \"    except:\\n\",\n    \"        # Sometimes we get an error for some unknown reason during callbacks\\n\",\n    \"        learn.callback_fns[-1](learn).on_epoch_end(old_checkpoint_num, None, [])\\n\",\n    \"        \\n\",\n    \"    if save_checkpoints:\\n\",\n    \"        learn_crit.save(crit_new_checkpoint_name)\\n\",\n    \"        learn_gen.save(gen_new_checkpoint_name)\\n\",\n    \"    \\n\",\n    \"    old_checkpoint_num += 1\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# End logging of current session run\\n\",\n    \"# Note: this is optional and would be automatically triggered when stopping the kernel\\n\",\n    \"wandb.join()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "ImageColorizer.ipynb",
          "type": "blob",
          "size": 5.537109375,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from deoldify.visualize import *\\n\",\n    \"plt.style.use('dark_background')\\n\",\n    \"torch.backends.cudnn.benchmark=True\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, message=\\\".*?Your .*? set is empty.*?\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"NOTE:  Set artistic to False if you're having trouble getting a good render.  Chances are it will work with the Stable model. \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"colorizer = get_image_colorizer(artistic=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Instructions\\n\",\n    \"\\n\",\n    \"### source_url\\n\",\n    \"Type in a url to a direct link of an image.  Usually that means they'll end in .png, .jpg, etc.  NOTE: If you want to use your own image, you can set source_url to None and just upload the image to /test_images/ in Jupyter.  Just make sure that the source_path parameter matches the file you uploaded.\\n\",\n    \"\\n\",\n    \"### source_path\\n\",\n    \"Name this whatever sensible image path (plus extension of jpg/png/ext) you want!  Sensible means the path exists and the file exists if source_url=None.\\n\",\n    \"\\n\",\n    \"### render_factor\\n\",\n    \"The default value of 35 has been carefully chosen and should work -ok- for most scenarios (but probably won't be the -best-). This determines resolution at which the color portion of the image is rendered. Lower resolution will render faster, and colors also tend to look more vibrant. Older and lower quality images in particular will generally benefit by lowering the render factor. Higher render factors are often better for higher quality images, but the colors may get slightly washed out. \\n\",\n    \"\\n\",\n    \"### result_path\\n\",\n    \"Ditto- don't change.\\n\",\n    \"\\n\",\n    \"### How to Download a Copy\\n\",\n    \"Simply shift+right click on the displayed image and click \\\"Save Image As...\\\"!\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"## Pro Tips\\n\",\n    \"1. You can evaluate how well the image is rendered at each render_factor by using the code at the bottom (that cell under \\\"See how well render_factor values perform on a frame here\\\"). \\n\",\n    \"2. Keep in mind again that you can go up top and set artistic to False for the colorizer to use the 'Stable' model instead.  This will often tend to do better on portraits, and natural landscapes.  \\n\",\n    \"\\n\",\n    \"\\n\",\n    \"## Troubleshooting\\n\",\n    \"If you get a 'CUDA out of memory' error, you probably have the render_factor too high.  The max is 45 on 11GB video cards.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Colorize!!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  Max is 45 with 11GB video cards. 35 is a good default\\n\",\n    \"render_factor=35\\n\",\n    \"#NOTE:  Make source_url None to just read from file at ./video/source/[file_name] directly without modification\\n\",\n    \"source_url='https://upload.wikimedia.org/wikipedia/commons/e/e4/Raceland_Louisiana_Beer_Drinkers_Russell_Lee.jpg'\\n\",\n    \"source_path = 'test_images/image.png'\\n\",\n    \"result_path = None\\n\",\n    \"\\n\",\n    \"if source_url is not None:\\n\",\n    \"    result_path = colorizer.plot_transformed_image_from_url(url=source_url, path=source_path, render_factor=render_factor, compare=True)\\n\",\n    \"else:\\n\",\n    \"    result_path = colorizer.plot_transformed_image(path=source_path, render_factor=render_factor, compare=True)\\n\",\n    \"\\n\",\n    \"show_image_in_notebook(result_path)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## See how well render_factor values perform on the image here\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#for i in range(10,46):\\n\",\n    \"    #colorizer.plot_transformed_image(source_path, render_factor=i, display_render_factor=True, figsize=(10,10))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  },\n  \"toc\": {\n   \"colors\": {\n    \"hover_highlight\": \"#DAA520\",\n    \"navigate_num\": \"#000000\",\n    \"navigate_text\": \"#333333\",\n    \"running_highlight\": \"#FF0000\",\n    \"selected_highlight\": \"#FFD700\",\n    \"sidebar_border\": \"#EEEEEE\",\n    \"wrapper_background\": \"#FFFFFF\"\n   },\n   \"moveMenuLeft\": true,\n   \"nav_menu\": {\n    \"height\": \"67px\",\n    \"width\": \"252px\"\n   },\n   \"navigate_menu\": true,\n   \"number_sections\": true,\n   \"sideBar\": true,\n   \"threshold\": 4,\n   \"toc_cell\": false,\n   \"toc_section_display\": \"block\",\n   \"toc_window_display\": false,\n   \"widenNotebook\": false\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "ImageColorizerArtisticTests.ipynb",
          "type": "blob",
          "size": 77.81640625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from deoldify.visualize import *\\n\",\n    \"plt.style.use('dark_background')\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, message=\\\".*?Your .*? set is empty.*?\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#Adjust render_factor (int) if image doesn't look quite right (max 64 on 11GB GPU).  The default here works for most photos.  \\n\",\n    \"#It literally just is a number multiplied by 16 to get the square render resolution.  \\n\",\n    \"#Note that this doesn't affect the resolution of the final output- the output is the same resolution as the input.\\n\",\n    \"#Example:  render_factor=21 => color is rendered at 16x21 = 336x336 px.  \\n\",\n    \"render_factor=35\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis = get_image_colorizer(render_factor=render_factor, artistic=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/poolparty.jpg\\\", render_factor=38, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1852GatekeepersWindsor.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Chief.jpg\\\", render_factor=14, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1850SchoolForGirls.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AtlanticCityBeach1905.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CottonMillWorkers1913.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BrooklynNavyYardHospital.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FinnishPeasant1867.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AtlanticCity1905.png\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PushingCart.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Drive1905.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/IronLung.png\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FamilyWithDog.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/DayAtSeaBelgium.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/marilyn_woods.jpg\\\", render_factor=29, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/OldWomanSweden1904.jpg\\\", render_factor=36, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WomenTapingPlanes.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/overmiller.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BritishDispatchRider.jpg\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MuseauNacionalDosCoches.jpg\\\", render_factor=17, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/abe.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/RossCorbettHouseCork.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HPLabelleOfficeMontreal.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/einstein_beach.jpg\\\", render_factor=29, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/airmen1943.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/20sWoman.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/egypt-1.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Rutherford_Hayes.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/einstein_portrait.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/pinkerton.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WaltWhitman.jpg\\\", render_factor=12, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dorothea-lange.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Hemmingway2.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/hemmingway.jpg\\\", render_factor=9, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/smoking_kid.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/teddy_rubble.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_2.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/camera_man.jpg\\\", render_factor=23, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/migrant_mother.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/marktwain.jpg\\\", render_factor=10, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HelenKeller.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Evelyn_Nesbit.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Eddie-Adams.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/soldier_kids.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsYosemite.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/unnamed.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/workers_canyon.jpg\\\", render_factor=48, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CottonMill.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/JudyGarland.jpeg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/kids_pit.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/last_samurai.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsWhiteChurch.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/opium.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dorothea_lange_2.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/rgs.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/wh-auden.jpg\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/w-b-yeats.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/marilyn_portrait.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/wilson-slaverevivalmeeting.jpg\\\", render_factor=38, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ww1_trench.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/women-bikers.png\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Unidentified1855.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/skycrapper_lunch.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/sioux.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/school_kids.jpg\\\", render_factor=26, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/royal_family.jpg\\\", render_factor=33, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/redwood_lumberjacks.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/poverty.jpg\\\", render_factor=26, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/paperboy.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NativeAmericans.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/helmut_newton-.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Greece1911.jpg\\\", render_factor=26, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FatMenClub.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EgyptColosus.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/egypt-2.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_sd.jpg\\\", render_factor=12, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_people.jpg\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_5.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_1.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/DriveThroughGiantTree.jpg\\\", render_factor=39, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/covered-wagons-traveling.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/civil-war_2.jpg\\\", render_factor=12, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/civil_war_4.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/civil_war_3.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/civil_war.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BritishSlum.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/bicycles.jpg\\\", render_factor=33, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/brooklyn_girls_1940s.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/40sCouple.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1946Wedding.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Dolores1920s.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TitanicGym.jpg\\\", render_factor=31, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FrenchVillage1950s.jpg\\\", render_factor=38, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ClassDivide1930sBrittain.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1870sSphinx.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890Surfer.png\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TV1930s.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1864UnionSoldier.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sMedStudents.jpg\\\", render_factor=23, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BellyLaughWWI.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PiggyBackRide.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HealingTree.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ManPile.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1910Bike.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FreeportIL.jpg\\\", render_factor=36, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/DutchBabyCoupleEllis.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/InuitWoman1903.png\\\", render_factor=33, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920sDancing.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AirmanDad.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1910Racket.png\\\", render_factor=34, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1880Paris.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Deadwood1860s.jpg\\\", render_factor=38, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1860sSamauris.jpg\\\", render_factor=34, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonUnderground1860.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Mid1800sSisters.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1860Girls.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SanFran1851.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Kabuki1870s.png\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Mormons1870s.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EgyptianWomenLate1800s.jpg\\\", render_factor=7, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PicadillyLate1800s.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SutroBaths1880s.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1880sBrooklynBridge.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ChinaOpiumc1880.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Locomotive1880s.jpg\\\", render_factor=10, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ViennaBoys1880s.png\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/VictorianDragQueen1880s.png\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Sami1880s.jpg\\\", render_factor=39, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Ballet1890Russia.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Rottindean1890s.png\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sPingPong.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1937.png\\\", render_factor=36, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Harlem1932.jpg\\\", render_factor=27, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/OregonTrail1870s.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EasterNyc1911.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1899NycBlizzard.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Edinburgh1920s.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sShoeShopOhio.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sTouristsEgypt.png\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1938Reading.jpg\\\", render_factor=27, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1850Geography.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1901Electrophone.jpg\\\", render_factor=7, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Texas1938Woman.png\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MaioreWoman1895NZ.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WestVirginiaHouse.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920sGuadalope.jpg\\\", render_factor=33, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1909Chicago.jpg\\\", render_factor=14, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920sFarmKid.jpg\\\", render_factor=12, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ParisLate1800s.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900sDaytonaBeach.png\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1930sGeorgia.jpg\\\", render_factor=17, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NorwegianBride1920s.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Depression.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1888Slum.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LivingRoom1920Sweden.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1896NewsBoyGirl.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PetDucks1927.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1899SodaFountain.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TimesSquare1955.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PuppyGify.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890CliffHouseSF.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1908FamilyPhoto.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900sSaloon.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890BostonHospital.jpg\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1870Girl.jpg\\\", render_factor=9, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AustriaHungaryWomen1890s.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Shack.jpg\\\",render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Apsaroke1908.png\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1948CarsGrandma.jpg\\\", render_factor=14, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PlanesManhattan1931.jpg\\\", render_factor=11, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WorriedKid1940sNyc.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920sFamilyPhoto.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CatWash1931.jpg\\\", render_factor=34, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1940sBeerRiver.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/VictorianLivingRoom.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1897BlindmansBluff.jpg\\\", render_factor=23, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1874Mexico.png\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MadisonSquare1900.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1867MusicianConstantinople.jpg\\\", render_factor=11, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1925Girl.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1907Cowboys.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WWIIPeeps.jpg\\\", render_factor=26, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BabyBigBoots.jpg\\\", render_factor=17, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1895BikeMaidens.jpg\\\", render_factor=8, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/IrishLate1800s.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LibraryOfCongress1910.jpg\\\", render_factor=33, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1875Olds.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SenecaNative1908.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WWIHospital.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1892WaterLillies.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GreekImmigrants1905.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FatMensShop.jpg\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/KidCage1930s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FarmWomen1895.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NewZealand1860s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/JerseyShore1905.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonKidsEarly1900s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NYStreetClean1906.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Boston1937.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Cork1905.jpg\\\", render_factor=37, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BoxedBedEarly1900s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ZoologischerGarten1898.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EmpireState1930.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Agamemnon1919.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AppalachianLoggers1901.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WWISikhs.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MementoMori1865.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/RepBrennanRadio1922.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Late1800sNative.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GasPrices1939.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1933RockefellerCenter.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Scotland1919.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920CobblersShopLondon.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1909ParisFirstFemaleTaxisDriver.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HoovervilleSeattle1932.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ElephantLondon1934.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Jane_Addams.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsAdobe.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CricketLondon1930.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Donegal1907Yarn.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsChurch.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BreadDelivery1920sIreland.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BritishTeaBombay1890s.png\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CafeParis1928.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BigManTavern1908NYC.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Cars1890sIreland.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GalwayIreland1902.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HomeIreland1924.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HydeParkLondon1920s.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1929LondonOverFleetSt.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AccordianKid1900Paris.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsBuildings.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AthleticClubParis1913.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BombedLibraryLondon1940.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Boston1937.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BoulevardDuTemple1838.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BumperCarsParis1930.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CafeTerrace1925Paris.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CoalDeliveryParis1915.jpg\\\", render_factor=37, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CorkKids1910.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/DeepSeaDiver1915.png\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EastEndLondonStreetKids1901.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FreightTrainTeens1934.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HarrodsLondon1920.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HerbSeller1899Paris.jpg\\\", render_factor=17, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CalcuttaPoliceman1920.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ElectricScooter1915.jpeg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GreatGrandparentsIrelandEarly1900s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HalloweenEarly1900s.jpg\\\", render_factor=11, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/IceManLondon1919.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LeBonMarcheParis1875.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LittleAirplane1934.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/RoyalUniversityMedStudent1900Ireland.jpg\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LewisTomalinLondon1895.png\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SunHelmetsLondon1933.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Killarney1910.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonSheep1920s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PostOfficeVermont1914.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ServantsBessboroughHouse1908Ireland.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WaterfordIreland1909.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Lisbon1919.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1918WartimeClothesManufacture.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonHeatWave1935.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonsSmallestShop1900.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MetropolitanDistrictRailway1869London.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NativeWoman1926.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PaddysMarketCork1900s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PaddysMarketCork1900s.jpg\\\", render_factor=i, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Paris1920Cart.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ParisLadies1910.jpg\\\", render_factor=38, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ParisLadies1930s.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Sphinx.jpeg\\\") \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TheatreGroupBombay1875.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WorldsFair1900Paris.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1850Coach.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1900EastEndBlacksmith.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1930sCheetah.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonFireBrigadeMember1926.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonGarbageTruck1910.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonRailwayWork1931.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonStreets1900.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MuffinManlLondon1910.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NativeCouple1912.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NewspaperCivilWar1863.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PaddingtonStationLondon1907.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Paris1899StreetDig.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Paris1926.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ParisWomenFurs1920s.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PeddlerParis1899.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SchoolKidsConnemaraIreland1901.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SecondHandClothesLondonLate1800s.jpg\\\", render_factor=44, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SoapBoxRacerParis1920s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SoccerMotorcycles1923London.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WalkingLibraryLondon1930.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonStreetDoctor1877.png\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/jacksonville.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ZebraCarriageLondon1900.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/StreetGramaphonePlayerLondon1920s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/YaleBranchBarnardsExpress.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SynagogueInterior.PNG\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ArmisticeDay1918.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FlyingMachinesParis1909.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GreatAunt1920.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NewBrunswick1915.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ShoeMakerLate1800s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SpottedBull1908.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TouristsGermany1904.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TunisianStudents1914.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Yorktown1862.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonFashion1911.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1939GypsyKids.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1936OpiumShanghai.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1923HollandTunnel.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1939YakimaWAGirl.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GoldenGateConstruction.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PostCivilWarAncestors.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1939SewingBike.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1930MaineSchoolBus.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1913NewYorkConstruction.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1945HiroshimaChild.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1941GeorgiaFarmhouse.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1934UmbriaItaly.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900sLadiesTeaParty.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1919WWIAviationOxygenMask.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900NJThanksgiving.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1940Connecticut.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1911ThanksgivingMaskers.jpg\\\", render_factor=36, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1910ThanksgivingMaskersII.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1936PetToad.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1908RookeriesLondon.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sChineseImmigrants.jpg\\\", render_factor=36, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1897VancouverAmberlamps.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1929VictorianCosplayLondon.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1959ParisFriends.png\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1925GypsyCampMaryland.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1941PoolTableGeorgia.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900ParkDog.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1886Hoop.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1950sLondonPoliceChild.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1886ProspectPark.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1930sRooftopPoland.jpg\\\", render_factor=37, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1919RevereBeach.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1936ParisCafe.jpg\\\", render_factor=47, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1902FrenchYellowBellies.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1940PAFamily.jpg\\\", render_factor=34, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1910Finland.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ZebraCarriageLondon1900.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1904ChineseMan.jpg\\\", render_factor=14, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CrystalPalaceLondon1854.PNG\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James1.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James2.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James3.jpg\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James4.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James5.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James6.jpg\\\", render_factor=28, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  },\n  \"toc\": {\n   \"colors\": {\n    \"hover_highlight\": \"#DAA520\",\n    \"navigate_num\": \"#000000\",\n    \"navigate_text\": \"#333333\",\n    \"running_highlight\": \"#FF0000\",\n    \"selected_highlight\": \"#FFD700\",\n    \"sidebar_border\": \"#EEEEEE\",\n    \"wrapper_background\": \"#FFFFFF\"\n   },\n   \"moveMenuLeft\": true,\n   \"nav_menu\": {\n    \"height\": \"67px\",\n    \"width\": \"252px\"\n   },\n   \"navigate_menu\": true,\n   \"number_sections\": true,\n   \"sideBar\": true,\n   \"threshold\": 4,\n   \"toc_cell\": false,\n   \"toc_section_display\": \"block\",\n   \"toc_window_display\": false,\n   \"widenNotebook\": false\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "ImageColorizerColab.ipynb",
          "type": "blob",
          "size": 7.6806640625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"view-in-github\"\n   },\n   \"source\": [\n    \"<a href=\\\"https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### **<font color='blue'> Artistic Colorizer </font>**\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"663IVxfrpIAb\"\n   },\n   \"source\": [\n    \"# DeOldify - Colorize your own photos!\\n\",\n    \"\\n\",\n    \"####**Credits:**\\n\",\n    \"\\n\",\n    \"Special thanks to:\\n\",\n    \"\\n\",\n    \"Matt Robinson and Mara Benavente for pioneering the DeOldify image colab notebook.  \\n\",\n    \"\\n\",\n    \"Dana Kelley for doing things, breaking stuff & having an opinion on everything.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"ZjPqTBNoohK9\"\n   },\n   \"source\": [\n    \"\\n\",\n    \"\\n\",\n    \"---\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Verify Correct Runtime Settings\\n\",\n    \"\\n\",\n    \"**<font color='#FF000'> IMPORTANT </font>**\\n\",\n    \"\\n\",\n    \"In the \\\"Runtime\\\" menu for the notebook window, select \\\"Change runtime type.\\\" Ensure that the following are selected:\\n\",\n    \"* Runtime Type = Python 3\\n\",\n    \"* Hardware Accelerator = GPU \\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"gaEJBGDlptEo\"\n   },\n   \"source\": [\n    \"# Git clone and install DeOldify\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"-T-svuHytJ-8\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone https://github.com/jantic/DeOldify.git DeOldify \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"cd DeOldify\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"BDFjbNxaadNK\"\n   },\n   \"source\": [\n    \"# Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"00_GcC_trpdE\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\\n\",\n    \"\\n\",\n    \"import torch\\n\",\n    \"\\n\",\n    \"if not torch.cuda.is_available():\\n\",\n    \"    print('GPU not available.')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"Lsx7xCXNSVt6\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -r requirements-colab.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"MsJa69CMwj3l\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import fastai\\n\",\n    \"from deoldify.visualize import *\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, message=\\\".*?Your .*? set is empty.*?\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"!mkdir 'models'\\n\",\n    \"!wget https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth -O ./models/ColorizeArtistic_gen.pth\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"tzHVnegp21hC\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"colorizer = get_image_colorizer(artistic=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"BDFjbNxaadNJ\"\n   },\n   \"source\": [\n    \"# Instructions\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### source_url\\n\",\n    \"Type in a url to a direct link of an image.  Usually that means they'll end in .png, .jpg, etc. NOTE: If you want to use your own image, upload it first to a site like Imgur. \\n\",\n    \"\\n\",\n    \"### render_factor\\n\",\n    \"The default value of 35 has been carefully chosen and should work -ok- for most scenarios (but probably won't be the -best-). This determines resolution at which the color portion of the image is rendered. Lower resolution will render faster, and colors also tend to look more vibrant. Older and lower quality images in particular will generally benefit by lowering the render factor. Higher render factors are often better for higher quality images, but the colors may get slightly washed out. \\n\",\n    \"\\n\",\n    \"### watermarked\\n\",\n    \"Selected by default, this places a watermark icon of a palette at the bottom left corner of the image.  This is intended to be a standard way to convey to others viewing the image that it is colorized by AI. We want to help promote this as a standard, especially as the technology continues to improve and the distinction between real and fake becomes harder to discern. This palette watermark practice was initiated and lead by the company MyHeritage in the MyHeritage In Color feature (which uses a newer version of DeOldify than what you're using here).\\n\",\n    \"\\n\",\n    \"#### How to Download a Copy\\n\",\n    \"Simply right click on the displayed image and click \\\"Save image as...\\\"!\\n\",\n    \"\\n\",\n    \"## Pro Tips\\n\",\n    \"\\n\",\n    \"You can evaluate how well the image is rendered at each render_factor by using the code at the bottom (that cell under \\\"See how well render_factor values perform on a frame here\\\"). \"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"sUQrbSYipiJn\"\n   },\n   \"source\": [\n    \"# Colorize!!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"source_url = '' #@param {type:\\\"string\\\"}\\n\",\n    \"render_factor = 35  #@param {type: \\\"slider\\\", min: 7, max: 40}\\n\",\n    \"watermarked = True #@param {type:\\\"boolean\\\"}\\n\",\n    \"\\n\",\n    \"if source_url is not None and source_url !='':\\n\",\n    \"    image_path = colorizer.plot_transformed_image_from_url(url=source_url, render_factor=render_factor, compare=True, watermarked=watermarked)\\n\",\n    \"    show_image_in_notebook(image_path)\\n\",\n    \"else:\\n\",\n    \"    print('Provide an image url and try again.')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## See how well render_factor values perform on the image here\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"for i in range(10,40,2):\\n\",\n    \"    colorizer.plot_transformed_image('test_images/image.png', render_factor=i, display_render_factor=True, figsize=(8,8))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"X7Ycv_Y9xAHp\"\n   },\n   \"source\": [\n    \"---\\n\",\n    \"# Recommended image sources \\n\",\n    \"* [/r/TheWayWeWere](https://www.reddit.com/r/TheWayWeWere/)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"accelerator\": \"GPU\",\n  \"colab\": {\n   \"collapsed_sections\": [],\n   \"name\": \"ImageColorizerColab.ipynb\",\n   \"provenance\": [],\n   \"toc_visible\": true,\n   \"version\": \"0.3.2\"\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "ImageColorizerColabStable.ipynb",
          "type": "blob",
          "size": 7.626953125,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"view-in-github\"\n   },\n   \"source\": [\n    \"<a href=\\\"https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColabStable.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### **<font color='blue'> Stable Colorizer </font>**\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"663IVxfrpIAb\"\n   },\n   \"source\": [\n    \"# DeOldify - Colorize your own photos!\\n\",\n    \"\\n\",\n    \"####**Credits:**\\n\",\n    \"\\n\",\n    \"Special thanks to:\\n\",\n    \"\\n\",\n    \"Matt Robinson and Mara Benavente for pioneering the DeOldify image colab notebook.  \\n\",\n    \"\\n\",\n    \"Dana Kelley for doing things, breaking stuff & having an opinion on everything.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"ZjPqTBNoohK9\"\n   },\n   \"source\": [\n    \"\\n\",\n    \"\\n\",\n    \"---\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Verify Correct Runtime Settings\\n\",\n    \"\\n\",\n    \"**<font color='#FF000'> IMPORTANT </font>**\\n\",\n    \"\\n\",\n    \"In the \\\"Runtime\\\" menu for the notebook window, select \\\"Change runtime type.\\\" Ensure that the following are selected:\\n\",\n    \"* Runtime Type = Python 3\\n\",\n    \"* Hardware Accelerator = GPU \\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"gaEJBGDlptEo\"\n   },\n   \"source\": [\n    \"# Git clone and install DeOldify\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"-T-svuHytJ-8\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone https://github.com/jantic/DeOldify.git DeOldify \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"cd DeOldify\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"BDFjbNxaadNK\"\n   },\n   \"source\": [\n    \"# Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"00_GcC_trpdE\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\\n\",\n    \"\\n\",\n    \"import torch\\n\",\n    \"\\n\",\n    \"if not torch.cuda.is_available():\\n\",\n    \"    print('GPU not available.')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"Lsx7xCXNSVt6\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -r requirements-colab.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"MsJa69CMwj3l\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import fastai\\n\",\n    \"from deoldify.visualize import *\\n\",\n    \"\\n\",\n    \"torch.backends.cudnn.benchmark = True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"!mkdir 'models'\\n\",\n    \"!wget https://www.dropbox.com/s/axsd2g85uyixaho/ColorizeStable_gen.pth?dl=0 -O ./models/ColorizeStable_gen.pth\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"tzHVnegp21hC\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"colorizer = get_image_colorizer(artistic=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"BDFjbNxaadNJ\"\n   },\n   \"source\": [\n    \"# Instructions\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### source_url\\n\",\n    \"Type in a url to a direct link of an image.  Usually that means they'll end in .png, .jpg, etc. NOTE: If you want to use your own image, upload it first to a site like Imgur. \\n\",\n    \"\\n\",\n    \"### render_factor\\n\",\n    \"The default value of 35 has been carefully chosen and should work -ok- for most scenarios (but probably won't be the -best-). This determines resolution at which the color portion of the image is rendered. Lower resolution will render faster, and colors also tend to look more vibrant. Older and lower quality images in particular will generally benefit by lowering the render factor. Higher render factors are often better for higher quality images, but the colors may get slightly washed out. \\n\",\n    \"\\n\",\n    \"### watermarked\\n\",\n    \"Selected by default, this places a watermark icon of a palette at the bottom left corner of the image.  This is intended to be a standard way to convey to others viewing the image that it is colorized by AI. We want to help promote this as a standard, especially as the technology continues to improve and the distinction between real and fake becomes harder to discern. This palette watermark practice was initiated and lead by the company MyHeritage in the MyHeritage In Color feature (which uses a newer version of DeOldify than what you're using here).\\n\",\n    \"\\n\",\n    \"#### How to Download a Copy\\n\",\n    \"Simply right click on the displayed image and click \\\"Save image as...\\\"!\\n\",\n    \"\\n\",\n    \"## Pro Tips\\n\",\n    \"\\n\",\n    \"You can evaluate how well the image is rendered at each render_factor by using the code at the bottom (that cell under \\\"See how well render_factor values perform on a frame here\\\"). \"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"sUQrbSYipiJn\"\n   },\n   \"source\": [\n    \"# Colorize!!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"source_url = '' #@param {type:\\\"string\\\"}\\n\",\n    \"render_factor = 35  #@param {type: \\\"slider\\\", min: 7, max: 40}\\n\",\n    \"watermarked = True #@param {type:\\\"boolean\\\"}\\n\",\n    \"\\n\",\n    \"if source_url is not None and source_url !='':\\n\",\n    \"    image_path = colorizer.plot_transformed_image_from_url(url=source_url, render_factor=render_factor, compare=True, watermarked=watermarked)\\n\",\n    \"    show_image_in_notebook(image_path)\\n\",\n    \"else:\\n\",\n    \"    print('Provide an image url and try again.')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## See how well render_factor values perform on the image here\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"for i in range(10,40,2):\\n\",\n    \"    colorizer.plot_transformed_image('test_images/image.png', render_factor=i, display_render_factor=True, figsize=(8,8))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"X7Ycv_Y9xAHp\"\n   },\n   \"source\": [\n    \"---\\n\",\n    \"# Recommended image sources \\n\",\n    \"* [/r/TheWayWeWere](https://www.reddit.com/r/TheWayWeWere/)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"accelerator\": \"GPU\",\n  \"colab\": {\n   \"collapsed_sections\": [],\n   \"name\": \"ImageColorizerColabStable.ipynb\",\n   \"provenance\": [],\n   \"toc_visible\": true,\n   \"version\": \"0.3.2\"\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "ImageColorizerStableTests.ipynb",
          "type": "blob",
          "size": 77.0791015625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from deoldify.visualize import *\\n\",\n    \"plt.style.use('dark_background')\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, message=\\\".*?Your .*? set is empty.*?\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#Adjust render_factor (int) if image doesn't look quite right (max 64 on 11GB GPU).  The default here works for most photos.  \\n\",\n    \"#It literally just is a number multiplied by 16 to get the square render resolution.  \\n\",\n    \"#Note that this doesn't affect the resolution of the final output- the output is the same resolution as the input.\\n\",\n    \"#Example:  render_factor=21 => color is rendered at 16x21 = 336x336 px.  \\n\",\n    \"render_factor=35\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis = get_image_colorizer(render_factor=render_factor, artistic=False)\\n\",\n    \"#vis = get_video_colorizer(render_factor=render_factor).vis\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/poolparty.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1852GatekeepersWindsor.jpg\\\", render_factor=44, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Chief.jpg\\\", render_factor=10, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1850SchoolForGirls.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AtlanticCityBeach1905.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CottonMillWorkers1913.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BrooklynNavyYardHospital.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FinnishPeasant1867.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AtlanticCity1905.png\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PushingCart.jpg\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Drive1905.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/IronLung.png\\\", render_factor=26, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FamilyWithDog.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/DayAtSeaBelgium.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/marilyn_woods.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/OldWomanSweden1904.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WomenTapingPlanes.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/overmiller.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BritishDispatchRider.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MuseauNacionalDosCoches.jpg\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/abe.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/RossCorbettHouseCork.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HPLabelleOfficeMontreal.jpg\\\", render_factor=44, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/einstein_beach.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/airmen1943.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/20sWoman.jpg\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/egypt-1.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Rutherford_Hayes.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/einstein_portrait.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/pinkerton.jpg\\\", render_factor=7, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WaltWhitman.jpg\\\", render_factor=9, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dorothea-lange.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Hemmingway2.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/hemmingway.jpg\\\", render_factor=14, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/smoking_kid.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/teddy_rubble.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_2.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/camera_man.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/migrant_mother.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/marktwain.jpg\\\", render_factor=14, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HelenKeller.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Evelyn_Nesbit.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Eddie-Adams.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/soldier_kids.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsYosemite.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/unnamed.jpg\\\", render_factor=28, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/workers_canyon.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CottonMill.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/JudyGarland.jpeg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/kids_pit.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/last_samurai.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsWhiteChurch.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/opium.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dorothea_lange_2.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/rgs.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/wh-auden.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/w-b-yeats.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/marilyn_portrait.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/wilson-slaverevivalmeeting.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ww1_trench.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/women-bikers.png\\\", render_factor=23, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Unidentified1855.jpg\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/skycrapper_lunch.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/sioux.jpg\\\", render_factor=28, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/school_kids.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/royal_family.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/redwood_lumberjacks.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/poverty.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/paperboy.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NativeAmericans.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/helmut_newton-.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Greece1911.jpg\\\", render_factor=44, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FatMenClub.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EgyptColosus.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/egypt-2.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_sd.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_people.jpg\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_5.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/dustbowl_1.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/DriveThroughGiantTree.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/covered-wagons-traveling.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/civil-war_2.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/civil_war_4.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/civil_war_3.jpg\\\", render_factor=28, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/civil_war.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BritishSlum.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/bicycles.jpg\\\", render_factor=27, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/brooklyn_girls_1940s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/40sCouple.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1946Wedding.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Dolores1920s.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TitanicGym.jpg\\\", render_factor=26, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FrenchVillage1950s.jpg\\\", render_factor=41, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FrenchVillage1950s.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ClassDivide1930sBrittain.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1870sSphinx.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890Surfer.png\\\", render_factor=37, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TV1930s.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1864UnionSoldier.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sMedStudents.jpg\\\", render_factor=18, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BellyLaughWWI.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PiggyBackRide.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HealingTree.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ManPile.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1910Bike.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FreeportIL.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/DutchBabyCoupleEllis.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/InuitWoman1903.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920sDancing.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AirmanDad.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1910Racket.png\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1880Paris.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Deadwood1860s.jpg\\\", render_factor=13, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1860sSamauris.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonUnderground1860.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Mid1800sSisters.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1860Girls.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SanFran1851.jpg\\\", render_factor=44, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Kabuki1870s.png\\\", render_factor=8, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Mormons1870s.jpg\\\", render_factor=44, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EgyptianWomenLate1800s.jpg\\\", render_factor=44, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PicadillyLate1800s.jpg\\\", render_factor=26, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SutroBaths1880s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1880sBrooklynBridge.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ChinaOpiumc1880.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Locomotive1880s.jpg\\\", render_factor=9, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ViennaBoys1880s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/VictorianDragQueen1880s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Sami1880s.jpg\\\", render_factor=44, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ArkansasCowboys1880s.jpg\\\", render_factor=22, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Ballet1890Russia.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Rottindean1890s.png\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sPingPong.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1937.png\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Harlem1932.jpg\\\", render_factor=37, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/OregonTrail1870s.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EasterNyc1911.jpg\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1899NycBlizzard.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Edinburgh1920s.jpg\\\", render_factor=17, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sShoeShopOhio.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sTouristsEgypt.png\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1938Reading.jpg\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1850Geography.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1901Electrophone.jpg\\\", render_factor=10, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"for i in range(8, 47):\\n\",\n    \"    vis.plot_transformed_image(\\\"test_images/1901Electrophone.jpg\\\", render_factor=i, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Texas1938Woman.png\\\", render_factor=38, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MaioreWoman1895NZ.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WestVirginiaHouse.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920sGuadalope.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1909Chicago.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920sFarmKid.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ParisLate1800s.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900sDaytonaBeach.png\\\", render_factor=23, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1930sGeorgia.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NorwegianBride1920s.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Depression.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1888Slum.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LivingRoom1920Sweden.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1896NewsBoyGirl.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PetDucks1927.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1899SodaFountain.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TimesSquare1955.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PuppyGify.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890CliffHouseSF.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1908FamilyPhoto.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900sSaloon.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890BostonHospital.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1870Girl.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AustriaHungaryWomen1890s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Shack.jpg\\\",render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Apsaroke1908.png\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1948CarsGrandma.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PlanesManhattan1931.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WorriedKid1940sNyc.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920sFamilyPhoto.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CatWash1931.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1940sBeerRiver.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/VictorianLivingRoom.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1897BlindmansBluff.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1874Mexico.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MadisonSquare1900.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1867MusicianConstantinople.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1925Girl.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1907Cowboys.jpg\\\", render_factor=28, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WWIIPeeps.jpg\\\", render_factor=37, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BabyBigBoots.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1895BikeMaidens.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/IrishLate1800s.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LibraryOfCongress1910.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1875Olds.jpg\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SenecaNative1908.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WWIHospital.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1892WaterLillies.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GreekImmigrants1905.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FatMensShop.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/KidCage1930s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FarmWomen1895.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NewZealand1860s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/JerseyShore1905.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonKidsEarly1900s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NYStreetClean1906.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Boston1937.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Cork1905.jpg\\\", render_factor=28, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BoxedBedEarly1900s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ZoologischerGarten1898.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EmpireState1930.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Agamemnon1919.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AppalachianLoggers1901.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WWISikhs.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MementoMori1865.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/RepBrennanRadio1922.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Late1800sNative.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GasPrices1939.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1933RockefellerCenter.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Scotland1919.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1920CobblersShopLondon.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1909ParisFirstFemaleTaxisDriver.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HoovervilleSeattle1932.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ElephantLondon1934.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Jane_Addams.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsAdobe.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CricketLondon1930.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Donegal1907Yarn.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsChurch.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BreadDelivery1920sIreland.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BritishTeaBombay1890s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CafeParis1928.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BigManTavern1908NYC.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Cars1890sIreland.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GalwayIreland1902.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HomeIreland1924.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HydeParkLondon1920s.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1929LondonOverFleetSt.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AccordianKid1900Paris.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AnselAdamsBuildings.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/AthleticClubParis1913.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BombedLibraryLondon1940.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Boston1937.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BoulevardDuTemple1838.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/BumperCarsParis1930.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CafeTerrace1925Paris.jpg\\\", render_factor=24, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CoalDeliveryParis1915.jpg\\\", render_factor=37, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CorkKids1910.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/DeepSeaDiver1915.png\\\", render_factor=16, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/EastEndLondonStreetKids1901.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FreightTrainTeens1934.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HarrodsLondon1920.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HerbSeller1899Paris.jpg\\\", render_factor=17, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CalcuttaPoliceman1920.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ElectricScooter1915.jpeg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GreatGrandparentsIrelandEarly1900s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/HalloweenEarly1900s.jpg\\\", render_factor=11, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/IceManLondon1919.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LeBonMarcheParis1875.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LittleAirplane1934.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/RoyalUniversityMedStudent1900Ireland.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LewisTomalinLondon1895.png\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SunHelmetsLondon1933.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Killarney1910.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonSheep1920s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PostOfficeVermont1914.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ServantsBessboroughHouse1908Ireland.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WaterfordIreland1909.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Lisbon1919.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1918WartimeClothesManufacture.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonHeatWave1935.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonsSmallestShop1900.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MetropolitanDistrictRailway1869London.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NativeWoman1926.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PaddysMarketCork1900s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Paris1920Cart.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ParisLadies1910.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ParisLadies1930s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Sphinx.jpeg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TheatreGroupBombay1875.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WorldsFair1900Paris.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1850Coach.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1900EastEndBlacksmith.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/London1930sCheetah.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonFireBrigadeMember1926.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonGarbageTruck1910.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonRailwayWork1931.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonStreets1900.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/MuffinManlLondon1910.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NativeCouple1912.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NewspaperCivilWar1863.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PaddingtonStationLondon1907.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Paris1899StreetDig.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Paris1926.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ParisWomenFurs1920s.jpg\\\", render_factor=21, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PeddlerParis1899.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SchoolKidsConnemaraIreland1901.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SecondHandClothesLondonLate1800s.jpg\\\", render_factor=33, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SoapBoxRacerParis1920s.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SoccerMotorcycles1923London.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/WalkingLibraryLondon1930.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonStreetDoctor1877.png\\\", render_factor=38, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/jacksonville.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ZebraCarriageLondon1900.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/StreetGramaphonePlayerLondon1920s.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/YaleBranchBarnardsExpress.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SynagogueInterior.PNG\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ArmisticeDay1918.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/FlyingMachinesParis1909.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GreatAunt1920.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/NewBrunswick1915.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ShoeMakerLate1800s.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/SpottedBull1908.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TouristsGermany1904.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/TunisianStudents1914.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/Yorktown1862.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/LondonFashion1911.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1939GypsyKids.jpg\\\", render_factor=37, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1936OpiumShanghai.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1923HollandTunnel.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1939YakimaWAGirl.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/GoldenGateConstruction.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/PostCivilWarAncestors.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1939SewingBike.png\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1930MaineSchoolBus.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1913NewYorkConstruction.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1945HiroshimaChild.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1941GeorgiaFarmhouse.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1934UmbriaItaly.jpg\\\", render_factor=21) \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900sLadiesTeaParty.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1919WWIAviationOxygenMask.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900NJThanksgiving.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1940Connecticut.jpg\\\", render_factor=43, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1940Connecticut.jpg\\\", render_factor=i, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1911ThanksgivingMaskers.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1910ThanksgivingMaskersII.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1936PetToad.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1908RookeriesLondon.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1890sChineseImmigrants.jpg\\\", render_factor=25, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1897VancouverAmberlamps.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1929VictorianCosplayLondon.jpg\\\", render_factor=35, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1959ParisFriends.png\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1925GypsyCampMaryland.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1941PoolTableGeorgia.jpg\\\", render_factor=45, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1900ParkDog.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1886Hoop.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1950sLondonPoliceChild.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1886ProspectPark.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1930sRooftopPoland.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1919RevereBeach.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1936ParisCafe.jpg\\\", render_factor=46, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1902FrenchYellowBellies.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1940PAFamily.jpg\\\", render_factor=42, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1910Finland.jpg\\\", render_factor=40, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/ZebraCarriageLondon1900.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/1904ChineseMan.jpg\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/CrystalPalaceLondon1854.PNG\\\", compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James1.jpg\\\", render_factor=15, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James2.jpg\\\", render_factor=20, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James3.jpg\\\", render_factor=19, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James4.jpg\\\", render_factor=30, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James5.jpg\\\", render_factor=32, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"vis.plot_transformed_image(\\\"test_images/James6.jpg\\\", render_factor=28, compare=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  },\n  \"toc\": {\n   \"colors\": {\n    \"hover_highlight\": \"#DAA520\",\n    \"navigate_num\": \"#000000\",\n    \"navigate_text\": \"#333333\",\n    \"running_highlight\": \"#FF0000\",\n    \"selected_highlight\": \"#FFD700\",\n    \"sidebar_border\": \"#EEEEEE\",\n    \"wrapper_background\": \"#FFFFFF\"\n   },\n   \"moveMenuLeft\": true,\n   \"nav_menu\": {\n    \"height\": \"67px\",\n    \"width\": \"252px\"\n   },\n   \"navigate_menu\": true,\n   \"number_sections\": true,\n   \"sideBar\": true,\n   \"threshold\": 4,\n   \"toc_cell\": false,\n   \"toc_section_display\": \"block\",\n   \"toc_window_display\": false,\n   \"widenNotebook\": false\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2018 Jason Antic\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0576171875,
          "content": "include README.md\ninclude LICENSE\ninclude requirements.txt\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 26.6728515625,
          "content": "\n# DeOldify\n\n**This Reposisitory is Archived**  This project was a wild ride since I started it back in 2018.  6 years ago as of this writing (October 19, 2024)!.  It's time for me to move on and put this repo in the archives as I simply don't have the time to attend to it anymore, and frankly it's ancient as far as deep-learning projects go at this point! ~Jason\n\n**Quick Start**: The easiest way to colorize images using open source DeOldify\n(for free!) is here: [DeOldify Image Colorization on DeepAI](https://deepai.org/machine-learning-model/colorizer)\n\n**Desktop**: Want to run open source DeOldify for photos and videos on the desktop?\n* Stable Diffusion Web UI Plugin- Photos and video, cross-platform (NEW!). <https://github.com/SpenserCai/sd-webui-deoldify>\n* ColorfulSoft Windows GUI- No GPU required! Photos/Windows only. <https://github.com/ColorfulSoft/DeOldify.NET>.\nNo GPU required!\n\n**In Browser (new!)**  Check out this Onnx-based in browser implementation:  https://github.com/akbartus/DeOldify-on-Browser\n\nThe **most advanced** version of DeOldify image colorization is available here,\nexclusively.  Try a few images for free! [MyHeritage In Color](https://www.myheritage.com/incolor)\n\n**Replicate:** Image: <a href=\"https://replicate.com/arielreplicate/deoldify_image\"><img src=\"https://replicate.com/arielreplicate/deoldify_image/badge\"></a> | Video: <a href=\"https://replicate.com/arielreplicate/deoldify_video\"><img src=\"https://replicate.com/arielreplicate/deoldify_video/badge\"></a>\n\n----------------------------\n\nImage (artistic) [![Colab for images](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)\n| Video [![Colab for video](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)\n\nHaving trouble with the default image colorizer, aka \"artistic\"?  Try the\n\"stable\" one below.  It generally won't produce colors that are as interesting as\n\"artistic\", but the glitches are noticeably reduced.\n\nImage (stable) [![Colab for stable model](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColabStable.ipynb)\n\nInstructions on how to use the Colabs above have been kindly provided in video\ntutorial form by Old Ireland in Colour's John Breslin.  It's great! Click video\nimage below to watch.\n\n[![DeOldify Tutorial](http://img.youtube.com/vi/VaEl0faDw38/0.jpg)](http://www.youtube.com/watch?v=VaEl0faDw38)\n\nGet more updates on [Twitter\n![Twitter logo](resource_images/twitter.svg)](https://twitter.com/DeOldify).\n\n## Table of Contents\n\n- [About DeOldify](#about-deoldify)\n- [Example Videos](#example-videos)\n- [Example Images](#example-images)\n- [Stuff That Should Probably Be In A Paper](#stuff-that-should-probably-be-in-a-paper)\n  - [How to Achieve Stable Video](#how-to-achieve-stable-video)\n  - [What is NoGAN?](#what-is-nogan)\n- [Why Three Models?](#why-three-models)\n- [Technical Details](#the-technical-details)\n- [Going Forward](#this-project-going-forward)\n- [Getting Started Yourself](#getting-started-yourself)\n  - [Easiest Approach](#easiest-approach)\n  - [Your Own Machine](#your-own-machine-not-as-easy)\n- [Pretrained Weights](#pretrained-weights)\n\n## About DeOldify\n\nSimply put, the mission of this project is to colorize and restore old images and\nfilm footage. We'll get into the details in a bit, but first let's see some\npretty pictures and videos!\n\n### New and Exciting Stuff in DeOldify\n\n- Glitches and artifacts are almost entirely eliminated\n- Better skin (less zombies)\n- More highly detailed and photorealistic renders\n- Much less \"blue bias\"\n- **Video** - it actually looks good!  \n- **NoGAN** - a new and weird but highly effective way to do GAN training for\n  image to image.\n\n## Example Videos\n\n**Note:**  Click images to watch\n\n### Facebook F8 Demo\n\n[![DeOldify Facebook F8 Movie Colorization Demo](http://img.youtube.com/vi/l3UXXid04Ys/0.jpg)](http://www.youtube.com/watch?v=l3UXXid04Ys)\n\n### Silent Movie Examples\n\n[![DeOldify Silent Movie Examples](http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg)](http://www.youtube.com/watch?v=EXn-n2iqEjI)\n\n## Example Images\n\n\"Migrant Mother\" by Dorothea Lange (1936)\n\n![Migrant Mother](https://i.imgur.com/Bt0vnke.jpg)\n\nWoman relaxing in her livingroom in Sweden (1920)\n\n![Sweden Living Room](https://i.imgur.com/158d0oU.jpg)\n\n\"Toffs and Toughs\" by Jimmy Sime (1937)\n\n![Class Divide](https://i.imgur.com/VYuav4I.jpg)\n\nThanksgiving Maskers (1911)\n\n![Thanksgiving Maskers](https://i.imgur.com/n8qVJ5c.jpg)\n\nGlen Echo Madame Careta Gypsy Camp in Maryland (1925)\n\n![Gypsy Camp](https://i.imgur.com/1oYrJRI.jpg)\n\n\"Mr. and Mrs. Lemuel Smith and their younger children in their farm house,\nCarroll County, Georgia.\" (1941)\n\n![Georgia Farmhouse](https://i.imgur.com/I2j8ynm.jpg)\n\n\"Building the Golden Gate Bridge\" (est 1937)\n\n![Golden Gate Bridge](https://i.imgur.com/6SbFjfq.jpg)\n\n> **Note:**  What you might be wondering is while this render looks cool, are the\n> colors accurate? The original photo certainly makes it look like the towers of\n> the bridge could be white. We looked into this and it turns out the answer is\n> no - the towers were already covered in red primer by this time. So that's\n> something to keep in mind- historical accuracy remains a huge challenge!\n\n\"Terrasse de caf, Paris\" (1925)\n\n![Cafe Paris](https://i.imgur.com/WprQwP5.jpg)\n\nNorwegian Bride (est late 1890s)\n\n![Norwegian Bride](https://i.imgur.com/MmtvrZm.jpg)\n\nZitkla- (Lakota: Red Bird), also known as Gertrude Simmons Bonnin (1898)\n\n![Native Woman](https://i.imgur.com/zIGM043.jpg)\n\nChinese Opium Smokers (1880)\n\n![Opium Real](https://i.imgur.com/lVGq8Vq.jpg)\n\n## Stuff That Should Probably Be In A Paper\n\n### How to Achieve Stable Video\n\nNoGAN training is crucial to getting the kind of stable and colorful images seen\nin this iteration of DeOldify. NoGAN training combines the benefits of GAN\ntraining (wonderful colorization) while eliminating the nasty side effects\n(like flickering objects in video). Believe it or not, video is rendered using\nisolated image generation without any sort of temporal modeling tacked on. The\nprocess performs 30-60 minutes of the GAN portion of \"NoGAN\" training, using 1%\nto 3% of imagenet data once.  Then, as with still image colorization, we\n\"DeOldify\" individual frames before rebuilding the video.\n\nIn addition to improved video stability, there is an interesting thing going on\nhere worth mentioning. It turns out the models I run, even different ones and\nwith different training structures, keep arriving at more or less the same\nsolution.  That's even the case for the colorization of things you may think\nwould be arbitrary and unknowable, like the color of clothing, cars, and even\nspecial effects (as seen in \"Metropolis\").\n\n![Metropolis Special FX](https://thumbs.gfycat.com/HeavyLoneBlowfish-size_restricted.gif)\n\nMy best guess is that the models are learning some interesting rules about how to\ncolorize based on subtle cues present in the black and white images that I\ncertainly wouldn't expect to exist.  This result leads to nicely deterministic and\nconsistent results, and that means you don't have track model colorization\ndecisions because they're not arbitrary.  Additionally, they seem remarkably\nrobust so that even in moving scenes the renders are very consistent.\n\n![Moving Scene Example](https://thumbs.gfycat.com/FamiliarJubilantAsp-size_restricted.gif)\n\nOther ways to stabilize video add up as well. First, generally speaking rendering\nat a higher resolution (higher render_factor) will increase stability of\ncolorization decisions.  This stands to reason because the model has higher\nfidelity image information to work with and will have a greater chance of making\nthe \"right\" decision consistently.  Closely related to this is the use of\nresnet101 instead of resnet34 as the backbone of the generator- objects are\ndetected more consistently and correctly with this. This is especially important\nfor getting good, consistent skin rendering.  It can be particularly visually\njarring if you wind up with \"zombie hands\", for example.\n\n![Zombie Hand Example](https://thumbs.gfycat.com/ThriftyInferiorIsabellinewheatear-size_restricted.gif)\n\nAdditionally, gaussian noise augmentation during training appears to help but at\nthis point the conclusions as to just how much are bit more tenuous (I just\nhaven't formally measured this yet).  This is loosely based on work done in style\ntransfer video, described here:\n <https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42>.\n\nSpecial thanks go to Rani Horev for his contributions in implementing this noise\naugmentation.\n\n### What is NoGAN?\n\nThis is a new type of GAN training that I've developed to solve some key problems\nin the previous DeOldify model. It provides the benefits of GAN training while\nspending minimal time doing direct GAN training.  Instead, most of the training\ntime is spent pretraining the generator and critic separately with more\nstraight-forward, fast and reliable conventional methods.  A key insight here is\nthat those more \"conventional\" methods generally get you most of the results you\nneed, and that GANs can be used to close the gap on realism. During the very\nshort amount of actual GAN training the generator not only gets the full\nrealistic colorization capabilities that used to take days of progressively\nresized GAN training, but it also doesn't accrue nearly as much of the artifacts\nand other ugly baggage of GANs. In fact, you can pretty much eliminate glitches\nand artifacts almost entirely depending on your approach. As far as I know this\nis a new technique. And it's incredibly effective.\n\n#### Original DeOldify Model\n\n![Before Flicker](https://thumbs.gfycat.com/CoordinatedVeneratedHogget-size_restricted.gif)\n\n#### NoGAN-Based DeOldify Model\n\n![After Flicker](https://thumbs.gfycat.com/OilyBlackArctichare-size_restricted.gif)\n\nThe steps are as follows: First train the generator in a conventional way by\nitself with just the feature loss. Next, generate images from that, and train\nthe critic on distinguishing between those outputs and real images as a basic\nbinary classifier. Finally, train the generator and critic together in a GAN\nsetting (starting right at the target size of 192px in this case).  Now for\nthe weird part:  All the useful GAN training here only takes place within a very\nsmall window of time.  There's an inflection point where it appears the critic\nhas transferred everything it can that is useful to the generator. Past this\npoint, image quality oscillates between the best that you can get at the\ninflection point, or bad in a predictable way (orangish skin, overly red lips,\netc).  There appears to be no productive training after the inflection point.\nAnd this point lies within training on just 1% to 3% of the Imagenet Data!\nThat amounts to about 30-60 minutes of training at 192px.\n\nThe hard part is finding this inflection point.  So far, I've accomplished this\nby making a whole bunch of model save checkpoints (every 0.1% of data iterated\non) and then just looking for the point where images look great before they go\ntotally bonkers with orange skin (always the first thing to go). Additionally,\ngenerator rendering starts immediately getting glitchy and inconsistent at this\npoint, which is no good particularly for video. What I'd really like to figure\nout is what the tell-tale sign of the inflection point is that can be easily\nautomated as an early stopping point.  Unfortunately, nothing definitive is\njumping out at me yet.  For one, it's happening in the middle of training loss\ndecreasing- not when it flattens out, which would seem more reasonable on the surface.\n\nAnother key thing about NoGAN training is you can repeat pretraining the critic\non generated images after the initial GAN training, then repeat the GAN training\nitself in the same fashion.  This is how I was able to get extra colorful results\nwith the \"artistic\" model.  But this does come at a cost currently- the output of\nthe generator becomes increasingly inconsistent and you have to experiment with\nrender resolution (render_factor) to get the best result.  But the renders are\nstill glitch free and way more consistent than I was ever able to achieve with\nthe original DeOldify model. You can do about five of these repeat cycles, give\nor take, before you get diminishing returns, as far as I can tell.\n\nKeep in mind- I haven't been entirely rigorous in figuring out what all is going\non in NoGAN- I'll save that for a paper. That means there's a good chance I'm\nwrong about something.  But I think it's definitely worth putting out there now\nbecause I'm finding it very useful- it's solving basically much of my remaining\nproblems I had in DeOldify.\n\nThis builds upon a technique developed in collaboration with Jeremy Howard and\nSylvain Gugger for Fast.AI's Lesson 7 in version 3 of Practical Deep Learning\nfor Coders Part I. The particular lesson notebook can be found here:\n  <https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb>\n\n## Why Three Models?\n\nThere are now three models to choose from in DeOldify. Each of these has key\nstrengths and weaknesses, and so have different use cases.  Video is for video\nof course.  But stable and artistic are both for images, and sometimes one will\ndo images better than the other.\n\nMore details:\n\n- **Artistic** - This model achieves the highest quality results in image\ncoloration, in terms of interesting details and vibrance. The most notable\ndrawback however is that it's a bit of a pain to fiddle around with to get the\nbest results (you have to adjust the rendering resolution or render_factor to\nachieve this).  Additionally, the model does not do as well as stable in a few\nkey common scenarios- nature scenes and portraits.  The model uses a resnet34\nbackbone on a UNet with an emphasis on depth of layers on the decoder side.\nThis model was trained with 5 critic pretrain/GAN cycle repeats via NoGAN, in\naddition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.\nThis adds up to a total of 32% of Imagenet data trained once (12.5 hours of\ndirect GAN training).\n\n- **Stable** - This model achieves the best results with landscapes and\nportraits.  Notably, it produces less \"zombies\"- where faces or limbs stay gray\nrather than being colored in properly.  It generally has less weird\nmiscolorations than artistic, but it's also less colorful in general.  This\nmodel uses a resnet101 backbone on a UNet with an emphasis on width of layers on\nthe decoder side.  This model was trained with 3 critic pretrain/GAN cycle\nrepeats via NoGAN, in addition to the initial generator/critic pretrain/GAN\nNoGAN training, at 192px.  This adds up to a total of 7% of Imagenet data\ntrained once (3 hours of direct GAN training).\n\n- **Video** - This model is optimized for smooth, consistent and flicker-free\nvideo.  This would definitely be the least colorful of the three models, but\nit's honestly not too far off from \"stable\". The model is the same as \"stable\"\nin terms of architecture, but differs in training.  It's trained for a mere 2.2% \nof Imagenet data once at 192px, using only the initial generator/critic \npretrain/GAN NoGAN training (1 hour of direct GAN training).\n\nBecause the training of the artistic and stable models was done before the\n\"inflection point\" of NoGAN training described in \"What is NoGAN???\" was\ndiscovered, I believe this amount of training on them can be knocked down\nconsiderably. As far as I can tell, the models were stopped at \"good points\"\nthat were well beyond where productive training was taking place.  I'll be\nlooking into this in the future.\n\nIdeally, eventually these three models will be consolidated into one that has all\nthese good desirable unified.  I think there's a path there, but it's going to\nrequire more work!  So for now, the most practical solution appears to be to\nmaintain multiple models.\n\n## The Technical Details\n\nThis is a deep learning based model.  More specifically, what I've done is\ncombined the following approaches:\n\n### [Self-Attention Generative Adversarial Network](https://arxiv.org/abs/1805.08318)\n\nExcept the generator is a **pretrained U-Net**, and I've just modified it to\nhave the spectral normalization and self-attention.  It's a pretty\nstraightforward translation.\n\n### [Two Time-Scale Update Rule](https://arxiv.org/abs/1706.08500)\n\nThis is also very straightforward  it's just one to one generator/critic\niterations and higher critic learning rate.\nThis is modified to incorporate a \"threshold\" critic loss that makes sure that\nthe critic is \"caught up\" before moving on to generator training.\nThis is particularly useful for the \"NoGAN\" method described below.\n\n### NoGAN\n\nThere's no paper here! This is a new type of GAN training that I've developed to\nsolve some key problems in the previous DeOldify model.\nThe gist is that you get the benefits of GAN training while spending minimal time\ndoing direct GAN training.\nMore details are in the [What is NoGAN?](#what-is-nogan) section (it's a doozy).\n\n### Generator Loss\n\nLoss during NoGAN learning is two parts:  One is a basic Perceptual Loss (or\nFeature Loss) based on VGG16  this just biases the generator model to replicate\nthe input image.\nThe second is the loss score from the critic.  For the curious  Perceptual Loss\nisn't sufficient by itself to produce good results.\nIt tends to just encourage a bunch of brown/green/blue  you know, cheating to\nthe test, basically, which neural networks are really good at doing!\nKey thing to realize here is that GANs essentially are learning the loss function\nfor you  which is really one big step closer to toward the ideal that we're\nshooting for in machine learning.\nAnd of course you generally get much better results when you get the machine to\nlearn something you were previously hand coding.\nThat's certainly the case here.\n\n**Of note:**  There's no longer any \"Progressive Growing of GANs\" type training\ngoing on here.  It's just not needed in lieu of the superior results obtained\nby the \"NoGAN\" technique described above.\n\nThe beauty of this model is that it should be generally useful for all sorts of\nimage modification, and it should do it quite well.\nWhat you're seeing above are the results of the colorization model, but that's\njust one component in a pipeline that I'm developing with the exact same approach.\n\n## This Project, Going Forward\n\nSo that's the gist of this project  I'm looking to make old photos and film\nlook reeeeaaally good with GANs, and more importantly, make the project *useful*.\nIn the meantime though this is going to be my baby and I'll be actively updating\nand improving the code over the foreseeable future.\nI'll try to make this as user-friendly as possible, but I'm sure there's going\nto be hiccups along the way.\n\nOh and I swear I'll document the code properly...eventually.  Admittedly I'm\n*one of those* people who believes in \"self documenting code\" (LOL).\n\n## Getting Started Yourself\n\n### Easiest Approach\n\nThe easiest way to get started is to go straight to the Colab notebooks:\n\nImage [![Colab for images](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)\n| Video [![Colab for video](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)\n\nSpecial thanks to Matt Robinson and Mara Benavente for their image Colab notebook\ncontributions, and Robert Bell for the video Colab notebook work!\n\n### Your Own Machine (not as easy)\n\n#### Hardware and Operating System Requirements\n\n- **(Training Only) BEEFY Graphics card**.  I'd really like to have more memory\n  than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.\n  The Generators and Critic are ridiculously large.  \n- **(Colorization Alone) A decent graphics card**. Approximately 4GB+ memory\n  video cards should be sufficient.\n- **Linux**.  I'm using Ubuntu 18.04, and I know 16.04 works fine too.  **Windows\n  is not supported and any issues brought up related to this will not be investigated.**\n\n#### Easy Install\n\nYou should now be able to do a simple install with Anaconda. Here are the steps:\n\nOpen the command line and navigate to the root folder you wish to install.  Then\ntype the following commands\n\n```console\ngit clone https://github.com/jantic/DeOldify.git DeOldify\ncd DeOldify\nconda env create -f environment.yml\n```\n\nThen start running with these commands:\n\n```console\nsource activate deoldify\njupyter lab\n```\n\nFrom there you can start running the notebooks in Jupyter Lab, via the url they\nprovide you in the console.\n\n> **Note:** You can also now do \"conda activate deoldify\" if you have the latest\nversion of conda and in fact that's now recommended. But a lot of people don't\nhave that yet so I'm not going to make it the default instruction here yet.\n\n**Alternative Install:** User daddyparodz has kindly created an installer script\nfor Ubuntu, and in particular Ubuntu on WSL, that may make things easier:\n  <https://github.com/daddyparodz/AutoDeOldifyLocal>\n\n#### Note on test_images Folder\n\nThe images in the `test_images` folder have been removed because they were using\nGit LFS and that costs a lot of money when GitHub actually charges for bandwidth\non a popular open source project (they had a billing bug for while that was\nrecently fixed).  The notebooks that use them (the image test ones) still point\nto images in that directory that I (Jason) have personally and I'd like to keep\nit that way because, after all, I'm by far the primary and most active developer.\nBut they won't work for you.  Still, those notebooks are a convenient template\nfor making your own tests if you're so inclined.\n\n#### Typical training\n\nThe notebook `ColorizeTrainingWandb` has been created to log and monitor results\nthrough [Weights & Biases](https://www.wandb.com/). You can find a description of\ntypical training by consulting [W&B Report](https://app.wandb.ai/borisd13/DeOldify/reports?view=borisd13%2FDeOldify).\n\n## Pretrained Weights\n\nTo start right away on your own machine with your own images or videos without\ntraining the models yourself, you'll need to download the \"Completed Generator\nWeights\" listed below and drop them in the /models/ folder.\n\nThe colorization inference notebooks should be able to guide you from here. The\nnotebooks to use are named ImageColorizerArtistic.ipynb,\nImageColorizerStable.ipynb, and VideoColorizer.ipynb.\n\n### Completed Generator Weights\n\n- [Artistic](https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth)\n- [Stable](https://www.dropbox.com/s/axsd2g85uyixaho/ColorizeStable_gen.pth?dl=0)\n- [Video](https://data.deepai.org/deoldify/ColorizeVideo_gen.pth)\n\n### Completed Critic Weights\n\n- [Artistic](https://www.dropbox.com/s/xpq2ip9occuzgen/ColorizeArtistic_crit.pth?dl=0)\n- [Stable](https://www.dropbox.com/s/s53699e9n84q6sp/ColorizeStable_crit.pth?dl=0)\n- [Video](https://www.dropbox.com/s/xnq1z1oppvgpgtn/ColorizeVideo_crit.pth?dl=0)\n\n### Pretrain Only Generator Weights\n\n- [Artistic](https://www.dropbox.com/s/h782d1zar3vdblw/ColorizeArtistic_PretrainOnly_gen.pth?dl=0)\n- [Stable](https://www.dropbox.com/s/mz5n9hiq6hmwjq7/ColorizeStable_PretrainOnly_gen.pth?dl=0)\n- [Video](https://www.dropbox.com/s/ix993ci6ve7crlk/ColorizeVideo_PretrainOnly_gen.pth?dl=0)\n\n### Pretrain Only Critic Weights\n\n- [Artistic](https://www.dropbox.com/s/gr81b3pkidwlrc7/ColorizeArtistic_PretrainOnly_crit.pth?dl=0)\n- [Stable](https://www.dropbox.com/s/007qj0kkkxt5gb4/ColorizeStable_PretrainOnly_crit.pth?dl=0)\n- [Video](https://www.dropbox.com/s/wafc1uogyjuy4zq/ColorizeVideo_PretrainOnly_crit.pth?dl=0)\n\n## Want the Old DeOldify?\n\nWe suspect some of you are going to want access to the original DeOldify model\nfor various reasons.  We have that archived here:  <https://github.com/dana-kelley/DeOldify>\n\n## Want More?\n\nFollow [#DeOldify](https://twitter.com/search?q=%23Deoldify) on Twitter.\n\n## License\n\nAll code in this repository is under the MIT license as specified by the LICENSE\nfile.\n\nThe model weights listed in this readme under the \"Pretrained Weights\" section\nare trained by ourselves and are released under the MIT license.\n\n## A Statement on Open Source Support\n\nWe believe that open source has done a lot of good for the world. After all,\nDeOldify simply wouldn't exist without it. But we also believe that there needs\nto be boundaries on just how much is reasonable to be expected from an open\nsource project maintained by just two developers.\n\nOur stance is that we're providing the code and documentation on research that\nwe believe is beneficial to the world. What we have provided are novel takes\non colorization, GANs, and video that are hopefully somewhat friendly for\ndevelopers and researchers to learn from and adopt. This is the culmination of\nwell over a year of continuous work, free for you. What wasn't free was\nshouldered by us, the developers. We left our jobs, bought expensive GPUs, and\nhad huge electric bills as a result of dedicating ourselves to this.\n\nWhat we haven't provided here is a ready to use free \"product\" or \"app\", and we\ndon't ever intend on providing that. It's going to remain a Linux based project\nwithout Windows support, coded in Python, and requiring people to have some extra\ntechnical background to be comfortable using it. Others have stepped in with\ntheir own apps made with DeOldify, some paid and some free, which is what we want!\nWe're instead focusing on what we believe we can do best- making better\ncommercial models that people will pay for.\nDoes that mean you're not getting the very best for free? Of course. We simply\ndon't believe that we're obligated to provide that, nor is it feasible! We\ncompete on research and sell that. Not a GUI or web service that wraps said\nresearch- that part isn't something we're going to be great at anyways. We're not\nabout to shoot ourselves in the foot by giving away our actual competitive\nadvantage for free, quite frankly.\n\nWe're also not willing to go down the rabbit hole of providing endless, open\nended and personalized support on this open source project. Our position is\nthis: If you have the proper background and resources, the project provides\nmore than enough to get you started. We know this because we've seen plenty of\npeople using it and making money off of their own projects with it.\n\nThus, if you have an issue come up and it happens to be an actual bug that\nhaving it be fixed will benefit users generally, then great- that's something\nwe'll be happy to look into.\n\nIn contrast, if you're asking about something that really amounts to asking for\npersonalized and time consuming support that won't benefit anybody else, we're\nnot going to help. It's simply not in our interest to do that. We have bills to\npay, after all. And if you're asking for help on something that can already be\nderived from the documentation or code? That's simply annoying, and we're not\ngoing to pretend to be ok with that.\n"
        },
        {
          "name": "VideoColorizer.ipynb",
          "type": "blob",
          "size": 6.1904296875,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from deoldify.visualize import *\\n\",\n    \"plt.style.use('dark_background')\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, message=\\\".*?Your .*? set is empty.*?\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"colorizer = get_video_colorizer()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Instructions\\n\",\n    \"\\n\",\n    \"### source_url\\n\",\n    \"Type in a url hosting a video from YouTube, Imgur, Twitter, Reddit, Vimeo, etc.  Many sources work!  GIFs also work.  Full list here: https://ytdl-org.github.io/youtube-dl/supportedsites.html NOTE: If you want to use your own video, you can set source_url to None and just upload the file to video/source/ in Jupyter.  Just make sure that the file_name parameter matches the file you uploaded.\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"### file_name\\n\",\n    \"Name this whatever sensible file name you want (minus extension)! It should actually exist in video/source if source_url=None\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"### render_factor\\n\",\n    \"The default value of 21 has been carefully chosen and should work -ok- for most scenarios (but probably won't be the -best-). This determines resolution at which the color portion of the video is rendered. Lower resolution will render faster, and colors also tend to look more vibrant. Older and lower quality film in particular will generally benefit by lowering the render factor. Higher render factors are often better for higher quality videos and inconsistencies (flashy render) will generally be reduced, but the colors may get slightly washed out. \\n\",\n    \"\\n\",\n    \"\\n\",\n    \"### file_name_ext\\n\",\n    \"There's no reason to changes this.\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"### result_path\\n\",\n    \"Ditto- don't change.\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"### How to Download a Copy\\n\",\n    \"Simply shift+right click on the displayed video and click \\\"Save video as...\\\"!\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"## Pro Tips\\n\",\n    \"1. If a video takes a long time to render and you're wondering how well the frames will actually be colorized, you can preview how well the frames will be rendered at each render_factor by using the code at the bottom. Just stop the video rendering by hitting the stop button on the cell, then run that bottom cell under \\\"See how well render_factor values perform on a frame here\\\". It's not perfect and you may still need to experiment a bit especially when it comes to figuring out how to reduce frame inconsistency.  But it'll go a long way in narrowing down what actually works.\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"## Troubleshooting\\n\",\n    \"The video player may wind up not showing up, in which case- make sure to wait for the Jupyter cell to complete processing first (the play button will stop spinning).  Then follow these alternative download instructions\\n\",\n    \"\\n\",\n    \"1. In the menu to the left, click Home icon.\\n\",\n    \"2. By default, rendered video will be in /video/result/\\n\",\n    \"\\n\",\n    \"If a video you downloaded doesn't play, it's probably because the cell didn't complete processing and the video is in a half-finished state.\\n\",\n    \"If you get a 'CUDA out of memory' error, you probably have the render_factor too high.  The max is 44 on 11GB video cards.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Colorize!!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  Max is 44 with 11GB video cards.  21 is a good default\\n\",\n    \"render_factor=21\\n\",\n    \"#NOTE:  Make source_url None to just read from file at ./video/source/[file_name] directly without modification\\n\",\n    \"source_url='https://twitter.com/silentmoviegifs/status/1116751583386034176'\\n\",\n    \"file_name = 'DogShy1926'\\n\",\n    \"file_name_ext = file_name + '.mp4'\\n\",\n    \"result_path = None\\n\",\n    \"\\n\",\n    \"if source_url is not None:\\n\",\n    \"    result_path = colorizer.colorize_from_url(source_url, file_name_ext, render_factor=render_factor)\\n\",\n    \"else:\\n\",\n    \"    result_path = colorizer.colorize_from_file_name(file_name_ext, render_factor=render_factor)\\n\",\n    \"\\n\",\n    \"show_video_in_notebook(result_path)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## See how well render_factor values perform on a frame here\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"for i in range(10,45,2):\\n\",\n    \"    colorizer.vis.plot_transformed_image('video/bwframes/' + file_name + '/00001.jpg', render_factor=i, display_render_factor=True, figsize=(8,8))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  },\n  \"toc\": {\n   \"colors\": {\n    \"hover_highlight\": \"#DAA520\",\n    \"navigate_num\": \"#000000\",\n    \"navigate_text\": \"#333333\",\n    \"running_highlight\": \"#FF0000\",\n    \"selected_highlight\": \"#FFD700\",\n    \"sidebar_border\": \"#EEEEEE\",\n    \"wrapper_background\": \"#FFFFFF\"\n   },\n   \"moveMenuLeft\": true,\n   \"nav_menu\": {\n    \"height\": \"67px\",\n    \"width\": \"252px\"\n   },\n   \"navigate_menu\": true,\n   \"number_sections\": true,\n   \"sideBar\": true,\n   \"threshold\": 4,\n   \"toc_cell\": false,\n   \"toc_section_display\": \"block\",\n   \"toc_window_display\": false,\n   \"widenNotebook\": false\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "VideoColorizerColab.ipynb",
          "type": "blob",
          "size": 9.681640625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"view-in-github\"\n   },\n   \"source\": [\n    \"<a href=\\\"https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### **<font color='blue'> Video Colorizer </font>**\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"663IVxfrpIAb\"\n   },\n   \"source\": [\n    \"# DeOldify - Colorize your own videos!\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"_FYI: This notebook is intended as a tool to colorize gifs and short videos, if you are trying to convert longer video you may hit the limit on processing space. Running the Jupyter notebook on your own machine is recommended (and faster) for larger video sizes._\\n\",\n    \"\\n\",\n    \"####**Credits:**\\n\",\n    \"\\n\",\n    \"Big special thanks to:\\n\",\n    \"\\n\",\n    \"Robert Bell for all his work on the video Colab notebook, and paving the way to video in DeOldify!\\n\",\n    \"\\n\",\n    \"Dana Kelley for doing things, breaking stuff & having an opinion on everything.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"ZjPqTBNoohK9\"\n   },\n   \"source\": [\n    \"\\n\",\n    \"\\n\",\n    \"---\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Verify Correct Runtime Settings\\n\",\n    \"\\n\",\n    \"**<font color='#FF000'> IMPORTANT </font>**\\n\",\n    \"\\n\",\n    \"In the \\\"Runtime\\\" menu for the notebook window, select \\\"Change runtime type.\\\" Ensure that the following are selected:\\n\",\n    \"* Runtime Type = Python 3\\n\",\n    \"* Hardware Accelerator = GPU \\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"gaEJBGDlptEo\"\n   },\n   \"source\": [\n    \"# Git clone and install DeOldify\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"-T-svuHytJ-8\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone https://github.com/jantic/DeOldify.git DeOldify\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"cd DeOldify\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"BDFjbNxaadNJ\"\n   },\n   \"source\": [\n    \"# Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"00_GcC_trpdE\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#NOTE:  This must be the first call in order to work properly!\\n\",\n    \"from deoldify import device\\n\",\n    \"from deoldify.device_id import DeviceId\\n\",\n    \"#choices:  CPU, GPU0...GPU7\\n\",\n    \"device.set(device=DeviceId.GPU0)\\n\",\n    \"\\n\",\n    \"import torch\\n\",\n    \"\\n\",\n    \"if not torch.cuda.is_available():\\n\",\n    \"    print('GPU not available.')\\n\",\n    \"\\n\",\n    \"from os import path\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"Lsx7xCXNSVt6\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -r requirements-colab.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"MsJa69CMwj3l\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import fastai\\n\",\n    \"from deoldify.visualize import *\\n\",\n    \"from pathlib import Path\\n\",\n    \"torch.backends.cudnn.benchmark=True\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, message=\\\".*?Your .*? set is empty.*?\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"!mkdir 'models'\\n\",\n    \"!wget https://data.deepai.org/deoldify/ColorizeVideo_gen.pth -O ./models/ColorizeVideo_gen.pth\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {},\n    \"colab_type\": \"code\",\n    \"id\": \"tzHVnegp21hC\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"colorizer = get_video_colorizer()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Instructions\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### source_url\\n\",\n    \"Type in a url hosting a video from YouTube, Imgur, Twitter, Reddit, Vimeo, etc.  Many sources work!  GIFs also work.  Full list here: https://ytdl-org.github.io/youtube-dl/supportedsites.html NOTE: If you want to use your own video, upload it first to a site like YouTube. \\n\",\n    \"\\n\",\n    \"### render_factor\\n\",\n    \"The default value of 21 has been carefully chosen and should work -ok- for most scenarios (but probably won't be the -best-). This determines resolution at which the color portion of the video is rendered. Lower resolution will render faster, and colors also tend to look more vibrant. Older and lower quality film in particular will generally benefit by lowering the render factor. Higher render factors are often better for higher quality videos and inconsistencies (flashy render) will generally be reduced, but the colors may get slightly washed out.\\n\",\n    \"\\n\",\n    \"### watermarked\\n\",\n    \"Selected by default, this places a watermark icon of a palette at the bottom left corner of the image.  This is intended to be a standard way to convey to others viewing the image that it is colorized by AI. We want to help promote this as a standard, especially as the technology continues to improve and the distinction between real and fake becomes harder to discern. This palette watermark practice was initiated and lead by the company MyHeritage in the MyHeritage In Color feature (which uses a newer version of DeOldify than what you're using here).\\n\",\n    \"\\n\",\n    \"### How to Download a Copy\\n\",\n    \"Simply right click on the displayed video and click \\\"Save video as...\\\"!\\n\",\n    \"\\n\",\n    \"## Pro Tips\\n\",\n    \"1. If a video takes a long time to render and you're wondering how well the frames will actually be colorized, you can preview how well the frames will be rendered at each render_factor by using the code at the bottom. Just stop the video rendering by hitting the stop button on the cell, then run that bottom cell under \\\"See how well render_factor values perform on a frame here\\\". It's not perfect and you may still need to experiment a bit especially when it comes to figuring out how to reduce frame inconsistency.  But it'll go a long way in narrowing down what actually works.\\n\",\n    \"2. If videos are taking way too much time for your liking, running the Jupyter notebook VideoColorizer.ipynb on your own machine (with DeOldify installed) will generally be much faster (as long as you have the hardware for it).   \\n\",\n    \"3. Longer videos (running multiple minutes) are going to have a rough time on Colabs. You'll be much better off using a local install of DeOldify instead in this case.\\n\",\n    \"\\n\",\n    \"## Troubleshooting\\n\",\n    \"The video player may wind up not showing up, in which case- make sure to wait for the Jupyter cell to complete processing first (the play button will stop spinning).  Then follow these alternative download instructions\\n\",\n    \"\\n\",\n    \"1. In the menu to the left, click Files\\n\",\n    \"2. If you don't see the 'DeOldify' folder, click \\\"Refresh\\\"\\n\",\n    \"3. By default, rendered video will be in /DeOldify/video/result/\\n\",\n    \"\\n\",\n    \"If a video you downloaded doesn't play, it's probably because the cell didn't complete processing and the video is in a half-finished state.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"sUQrbSYipiJn\"\n   },\n   \"source\": [\n    \"# Colorize!!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"source_url = '' #@param {type:\\\"string\\\"}\\n\",\n    \"render_factor = 21  #@param {type: \\\"slider\\\", min: 5, max: 40}\\n\",\n    \"watermarked = True #@param {type:\\\"boolean\\\"}\\n\",\n    \"\\n\",\n    \"if source_url is not None and source_url !='':\\n\",\n    \"    video_path = colorizer.colorize_from_url(source_url, 'video.mp4', render_factor, watermarked=watermarked)\\n\",\n    \"    show_video_in_notebook(video_path)\\n\",\n    \"else:\\n\",\n    \"    print('Provide a video url and try again.')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## See how well render_factor values perform on a frame here\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"for i in range(10,40,2):\\n\",\n    \"    colorizer.vis.plot_transformed_image('video/bwframes/video/00001.jpg', render_factor=i, display_render_factor=True, figsize=(8,8))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"colab_type\": \"text\",\n    \"id\": \"X7Ycv_Y9xAHp\"\n   },\n   \"source\": [\n    \"---\\n\",\n    \"# Recommended video and gif sources \\n\",\n    \"* [/r/Nickelodeons/](https://www.reddit.com/r/Nickelodeons/)\\n\",\n    \"* [r/silentmoviegifs](https://www.reddit.com/r/silentmoviegifs/)\\n\",\n    \"* https://twitter.com/silentmoviegifs \"\n   ]\n  }\n ],\n \"metadata\": {\n  \"accelerator\": \"GPU\",\n  \"colab\": {\n   \"collapsed_sections\": [],\n   \"name\": \"VideoColorizerColab.ipynb\",\n   \"provenance\": [],\n   \"toc_visible\": true,\n   \"version\": \"0.3.2\"\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "deoldify",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.3251953125,
          "content": "name: deoldify\nchannels:\n- fastai\n- conda-forge\n- defaults\ndependencies:\n- pip\n- fastai=1.0.60\n- mkl=2024.0\n- python=3.10\n- pytorch::pytorch=1.11.0\n- pytorch::torchvision\n- pytorch::torchaudio\n- tensorboardX\n- jupyterlab\n- pillow>=9.0.0\n- ipywidgets\n- ffmpeg\n- pip:\n  - ffmpeg-python\n  - opencv-python>=4.2.0.32\n  - wandb\n  - yt-dlp\n"
        },
        {
          "name": "fastai",
          "type": "tree",
          "content": null
        },
        {
          "name": "fid",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements-colab.txt",
          "type": "blob",
          "size": 0.1044921875,
          "content": "fastai==1.0.60\ntensorboardX>=1.6\nffmpeg-python\nyt-dlp\nopencv-python>=4.2.0.32\nPillow\ntornado\nimgaug==0.2.6\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 0.0166015625,
          "content": "black\npre-commit\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.212890625,
          "content": "wandb\nfastai==1.0.60\ntensorboardX>=1.6\nffmpeg\nffmpeg-python\nyt-dlp\njupyterlab\nopencv-python>=4.2.0.32\nPillow==9.3.0\n--extra-index-url https://download.pytorch.org/whl/cu113\ntorch==1.11.0\ntorchvision==0.12.0\nipywidgets\n"
        },
        {
          "name": "resource_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.1806640625,
          "content": "from setuptools import setup, find_packages\n\n\ndef get_description():\n    return \"Deep Learning library for colorizing and restoring old images and video\"\n\n\n# def get_long_description():\n#     with open(\"README.md\") as f:\n#         return f.read()\n\n\ndef get_requirements():\n    with open(\"requirements.txt\") as f:\n        return f.read().splitlines()\n\n\nsetup(\n    name=\"DeOldify\",\n    version=\"0.0.1\",\n    packages=find_packages(exclude=[\"tests\"]),\n    url=\"https://github.com/jantic/DeOldify\",\n    license=\"MIT License\",\n    description=get_description(),\n    # long_description=get_long_description(),\n    # long_description_content_type=\"text/markdown\",\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Framework :: Jupyter\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n    install_requires=get_requirements(),\n    python_requires=\">=3.6\",\n)\n"
        },
        {
          "name": "test_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.2958984375,
          "content": "[tox]\nenvlist=static,format\nskipsdist=True\n\n[testenv]\nwhitelist_externals=\n\t/usr/bin/sh\n\t/usr/bin/test\n\n[testenv:format]\ndeps=\n\tblack\ncommands=\n\tblack -S --check deoldify\n\n[testenv:static]\ndeps=\n\t-rrequirements.txt\n\tpylint\ncommands=\n\tsh -c 'pylint --disable=W deoldify; test $(( $? & (1|2|4|32) )) = 0'\n"
        }
      ]
    }
  ]
}