{
  "metadata": {
    "timestamp": 1736561263556,
    "page": 256,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OpenBMB/MiniCPM-V",
      "stars": 13041,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0263671875,
          "content": "*.bk\n__pycache__\n.DS_Store\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0712890625,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2024 OpenBMB\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 64.1015625,
          "content": "<div align=\"center\">\n\n<img src=\"./assets/minicpmv.png\" width=\"300em\" ></img> \n\n**A GPT-4V Level MLLM for Single Image, Multi Image and Video on Your Phone**\n\n  <strong>[‰∏≠Êñá](./README_zh.md) |\n  English</strong>\n\nJoin our <a href=\"docs/wechat.md\" target=\"_blank\"> üí¨ WeChat</a> | View  MiniCPM-V <a href=\"docs/best_practice_summary.md\" target=\"_blank\"> üìñ best practices</a>\n\n\n<p align=\"center\">\n  MiniCPM-V 2.6 <a href=\"https://huggingface.co/openbmb/MiniCPM-V-2_6\">ü§ó</a> <a href=\"http://120.92.209.146:8887/\">ü§ñ</a> | MiniCPM-Llama3-V 2.5  <a href=\"https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/\">ü§ó</a> <a href=\"https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5\">ü§ñ</a> |\n  <a href=https://arxiv.org/abs/2408.01800>MiniCPM-Llama3-V 2.5 Technical Report</a> \n</p>\n\n</div>\n\n\n**MiniCPM-V** is a series of end-side multimodal LLMs (MLLMs) designed for vision-language understanding. The models take image, video and text as inputs and provide high-quality text outputs. Since February 2024, we have released 5 versions of the model, aiming to achieve **strong performance and efficient deployment**. The most notable models in this series currently include:\n\n- **MiniCPM-V 2.6**: üî•üî•üî• The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, the model **surpasses GPT-4V in single image, multi-image and video understanding**. It outperforms **GPT-4o mini, Gemini 1.5 Pro and Claude 3.5 Sonnet** in single image understanding, and advances MiniCPM-Llama3-V 2.5's features such as strong OCR capability, trustworthy behavior, multilingual support, and end-side deployment. Due to its superior token density, MiniCPM-V 2.6 can for the first time support real-time video understanding on end-side devices such as iPad.\n\n- **MiniCPM-V 2.0**: The lightest model in the MiniCPM-V series. With 2B parameters, it surpasses larger models such as Yi-VL 34B, CogVLM-Chat 17B, and Qwen-VL-Chat 10B in overall performance. It can accept image inputs of any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), achieving comparable performance with Gemini Pro in understanding scene-text and matches GPT-4V in low hallucination rates.\n\n\n## News <!-- omit in toc -->\n\n#### üìå Pinned\n\n* [2024.08.17] üöÄüöÄüöÄ MiniCPM-V 2.6 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf).\n* [2024.08.15] We now also support multi-image SFT. For more details, please refer to the [document](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune).\n* [2024.08.14] MiniCPM-V 2.6 now also supports [fine-tuning](https://github.com/modelscope/ms-swift/issues/1613) with the SWIFT framework!\n* [2024.08.10] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf).\n* [2024.08.06] üî•üî•üî• We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!\n* [2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See [here](https://arxiv.org/abs/2408.01800).\n* [2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See [here](#inference-with-vllm).\n* [2024.05.28] üí´ We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics [here](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics).\n* [2024.05.23] üîç We've released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmarks evaluations, multilingual capabilities, and inference efficiency üåüüìäüåçüöÄ. Click [here](./docs/compare_with_phi-3_vision.md) to view more details.\n* [2024.05.23] üî•üî•üî• MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio‚Äôs official account, is available [here](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5). Come and try it out!\n\n<br>\n\n<details> \n<summary>Click to view more news.</summary>\n\n* [2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model's layers across multiple GPUs. For more details, Check this [link](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md).\n* [2024.05.28] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and ollama! Please pull the latest code **of our provided forks** ([llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md), [ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5)). GGUF models in various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main). MiniCPM-Llama3-V 2.5 series is **not supported by the official repositories yet**, and we are working hard to merge PRs. Please stay tuned!\n* [2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage)!\n* [2024.05.24] We release the MiniCPM-Llama3-V 2.5 [gguf](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf), which supports [llama.cpp](#inference-with-llamacpp) inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!\n* [2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide [efficient inference](#deployment-on-mobile-phone) and [simple fine-tuning](./finetune/readme.md). Try it now!\n* [2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click [here](#inference-with-vllm) to view more details.\n* [2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at [here](https://huggingface.co/spaces/openbmb/MiniCPM-V-2)!\n* [2024.04.17] MiniCPM-V-2.0 supports deploying [WebUI Demo](#webui-demo) now!\n* [2024.04.15] MiniCPM-V-2.0 now also supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2ÊúÄ‰Ω≥ÂÆûË∑µ.md) with the SWIFT framework!\n* [2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on <a href=\"https://rank.opencompass.org.cn/leaderboard-multimodal\">OpenCompass</a>, a comprehensive evaluation over 11 popular benchmarks. Click <a href=\"https://openbmb.vercel.app/minicpm-v-2\">here</a> to view the MiniCPM-V 2.0 technical blog.\n* [2024.03.14] MiniCPM-V now supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-vÊúÄ‰Ω≥ÂÆûË∑µ.md) with the SWIFT framework. Thanks to [Jintao](https://github.com/Jintao-Huang) for the contributionÔºÅ\n* [2024.03.01] MiniCPM-V now can be deployed on Mac!\n* [2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.\n</details> \n\n\n## Contents <!-- omit in toc -->\n\n\n- [MiniCPM-V 2.6](#minicpm-v-26)\n- [MiniCPM-Llama3-V 2.5](#minicpm-llama3-v-25)\n- [MiniCPM-V 2.0](#minicpm-v-20)\n- [Chat with Our Demo on Gradio ü§ó](#chat-with-our-demo-on-gradio-)\n- [Install](#install)\n- [Inference](#inference)\n  - [Model Zoo](#model-zoo)\n  - [Multi-turn Conversation](#multi-turn-conversation)\n    - [Chat with multiple images](#chat-with-multiple-images)\n    - [In-context few-shot learning](#in-context-few-shot-learning)\n    - [Chat with video](#chat-with-video)\n  - [Inference on Multiple GPUs](#inference-on-multiple-gpus)\n  - [Inference on Mac](#inference-on-mac)\n  - [Deployment on Mobile Phone](#deployment-on-mobile-phone)\n  - [Inference with llama.cpp](#inference-with-llamacpp)\n  - [Inference with ollama](#inference-with-ollama)\n  - [Inference with vLLM](#inference-with-vllm)\n- [Fine-tuning](#fine-tuning)\n- [FAQs](#faqs)\n\n\n## MiniCPM-V 2.6\n\n**MiniCPM-V 2.6** is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:\n\n- üî• **Leading Performance.**\n  MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet** for single image understanding.\n\n- üñºÔ∏è **Multi Image Understanding and In-context Learning.** MiniCPM-V 2.6 can also perform **conversation and reasoning over multiple images**. It achieves **state-of-the-art performance** on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.\n\n- üé¨ **Video Understanding.** MiniCPM-V 2.6 can also **accept video inputs**, performing conversation and providing dense captions for spatial-temporal information. It outperforms **GPT-4V, Claude 3.5 Sonnet and LLaVA-NeXT-Video-34B** on Video-MME with/without subtitles.\n\n- üí™ **Strong OCR Capability and Others.**\n  MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves **state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro**.\n  Based on the the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) and [VisCPM](https://github.com/OpenBMB/VisCPM) techniques, it features **trustworthy behaviors**, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports **multilingual capabilities** on English, Chinese, German, French, Italian, Korean, etc.\n\n\n- üöÄ **Superior Efficiency.**\n  In addition to its friendly size, MiniCPM-V 2.6 also shows **state-of-the-art token density** (i.e., number of pixels encoded into each visual token). **It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models**. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-V 2.6 can efficiently support **real-time video understanding** on end-side devices such as iPad.\n\n-  üí´  **Easy Usage.**\nMiniCPM-V 2.6 can be easily used in various ways: (1) [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpmv-main/examples/llava/README-minicpmv2.6.md) and [ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md) support for efficient CPU inference on local devices, (2) [int4](https://huggingface.co/openbmb/MiniCPM-V-2_6-int4) and [GGUF](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf) format quantized models in 16 sizes, (3) [vLLM](#inference-with-vllm) support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks, (5) quick local WebUI demo setup with [Gradio](#chat-with-our-demo-on-gradio), and (6) online web [demo](http://120.92.209.146:8887/).\n\n### Evaluation  <!-- omit in toc -->\n<div align=\"center\">\n    <img src=assets/radar_final.png width=66% />\n</div>\n\n<details>\n<summary>Click to view single image results on OpenCompass, MME, MMVet, OCRBench, MMMU, MathVista, MMB, AI2D, TextVQA, DocVQA, HallusionBench, Object HalBench. </summary>\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Token Density<sup>+</sup></th>\n            <th>OpenCompass</th>\n            <th>MME</th>\n            <th>MMVet</th>\n            <th>OCRBench</th>\n            <th>MMMU val</th>\n            <th>MathVista mini</th>\n            <th>MMB1.1 test</th>\n            <th>AI2D</th>\n            <th>TextVQA val</th>\n            <th>DocVQA test</th>\n            <th>HallusionBench</th>\n            <th>Object HalBench</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"15\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>69.9</td>\n            <td>2328.7</td>\n            <td>69.1</td>\n            <td>736</td>\n            <td>69.2</td>\n            <td>61.3</td>\n            <td>82.2</td>\n            <td>84.6</td>\n            <td>-</td>\n            <td>92.8</td>\n            <td>55.0</td>\n            <td>17.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Claude 3.5 Sonnet</td>\n            <td>-</td>\n            <td>750</td>\n            <td>67.9</td>\n            <td>1920.0</td>\n            <td>66.0</td>\n            <td>788</td>\n            <td>65.9</td>\n            <td>61.6</td>\n            <td>78.5</td>\n            <td>80.2</td>\n            <td>-</td>\n            <td>95.2</td>\n            <td>49.9</td>\n            <td>13.8</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini 1.5 Pro</td>\n            <td>-</td>\n            <td>-</td>\n            <td>64.4</td>\n            <td>2110.6</td>\n            <td>64.0</td>\n            <td>754</td>\n            <td>60.6</td>\n            <td>57.7</td>\n            <td>73.9</td>\n            <td>79.1</td>\n            <td>73.5</td>\n            <td>86.5</td>\n            <td>45.6</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o mini</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>64.1</td>\n            <td>2003.4</td>\n            <td>66.9</td>\n            <td>785</td>\n            <td>60.0</td>\n            <td>52.4</td>\n            <td>76.0</td>\n            <td>77.8</td>\n            <td>-</td>\n            <td>-</td>\n            <td>46.1</td>\n            <td>12.4</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>63.5</td>\n            <td>2070.2</td>\n            <td>67.5</td>\n            <td>656</td>\n            <td>61.7</td>\n            <td>54.7</td>\n            <td>79.8</td>\n            <td>78.6</td>\n            <td>78.0</td>\n            <td>87.2</td>\n            <td>43.9</td>\n            <td>14.2</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Step-1V</td>\n            <td>-</td>\n            <td>-</td>\n            <td>59.5</td>\n            <td>2206.4</td>\n            <td>63.3</td>\n            <td>625</td>\n            <td>49.9</td>\n            <td>44.8</td>\n            <td>78.0</td>\n            <td>79.2</td>\n            <td>71.6</td>\n            <td>-</td>\n            <td>48.4</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen-VL-Max</td>\n            <td>-</td>\n            <td>784</td>\n            <td>58.3</td>\n            <td>2281.7</td>\n            <td>61.8</td>\n            <td>684</td>\n            <td>52.0</td>\n            <td>43.4</td>\n            <td>74.6</td>\n            <td>75.7</td>\n            <td>79.5</td>\n            <td>93.1</td>\n            <td>41.2</td>\n            <td>13.4</td>\n        </tr>\n        <tr>\n            <td colspan=\"15\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Yi-34B</td>\n            <td>34B</td>\n            <td>157</td>\n            <td>55.0</td>\n            <td>2006.5</td>\n            <td>50.7</td>\n            <td>574</td>\n            <td>48.8</td>\n            <td>40.4</td>\n            <td>77.8</td>\n            <td>78.9</td>\n            <td>69.3</td>\n            <td>-</td>\n            <td>34.8</td>\n            <td>12.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Mini-Gemini-HD-34B</td>\n            <td>34B</td>\n            <td>157</td>\n            <td>-</td>\n            <td>2141.0</td>\n            <td>59.3</td>\n            <td>518</td>\n            <td>48.0</td>\n            <td>43.3</td>\n            <td>-</td>\n            <td>80.5</td>\n            <td>74.1</td>\n            <td>78.9</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Cambrian-34B</td>\n            <td>34B</td>\n            <td>1820</td>\n            <td>58.3</td>\n            <td>2049.9</td>\n            <td>53.2</td>\n            <td>591</td>\n            <td>50.4</td>\n            <td>50.3</td>\n            <td>77.8</td>\n            <td>79.5</td>\n            <td>76.7</td>\n            <td>75.5</td>\n            <td>41.6</td>\n            <td>14.7</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GLM-4V-9B</td>\n            <td>13B</td>\n            <td>784</td>\n            <td>59.1</td>\n            <td>2018.8</td>\n            <td>58.0</td>\n            <td>776</td>\n            <td>46.9</td>\n            <td>51.1</td>\n            <td>67.9</td>\n            <td>71.2</td>\n            <td>-</td>\n            <td>-</td>\n            <td>45.0</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>706</td>\n            <td>64.1</td>\n            <td>2215.1</td>\n            <td>54.3</td>\n            <td>794</td>\n            <td><strong>51.2</strong></td>\n            <td>58.3</td>\n            <td><strong>79.4</strong></td>\n            <td><strong>83.6</strong></td>\n            <td>77.4</td>\n            <td><strong>91.6</strong></td>\n            <td>45.0</td>\n            <td>21.3</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-Llama-V 2.5</td>\n            <td>8B</td>\n            <td>1882</td>\n            <td>58.8</td>\n            <td>2024.6</td>\n            <td>52.8</td>\n            <td>725</td>\n            <td>45.8</td>\n            <td>54.3</td>\n            <td>72.0</td>\n            <td>78.4</td>\n            <td>76.6</td>\n            <td>84.8</td>\n            <td>42.4</td>\n            <td>10.3</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>2822</strong></td>\n            <td><strong>65.2</strong></td>\n            <td><strong>2348.4</strong>*</td>\n            <td><strong>60.0</strong></td>\n            <td><strong>852</strong>*</td>\n            <td>49.8*</td>\n            <td><strong>60.6</strong></td>\n            <td>78.0</td>\n            <td>82.1</td>\n            <td><strong>80.1<strong></td>\n            <td>90.8</td>\n            <td><strong>48.1</strong>*</td>\n            <td><strong>8.2</strong></td>\n        </tr>\n    </tbody>\n</table>\n\n</div>\n* We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set.\n\n<sup>+</sup> Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.\n\nNote: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.\n\n</details>\n\n\n<details>\n<summary>Click to view multi-image results on Mantis Eval, BLINK, Mathverse mv, Sciverse mv, MIRB.</summary>\n<div align=\"center\">\n \n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Mantis Eval</th>\n            <th>BLINK val</th>\n            <th>Mathverse mv</th>\n            <th>Sciverse mv</th>\n            <th>MIRB</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"7\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>62.7</td>\n            <td>54.6</td>\n            <td>60.3</td>\n            <td>66.9</td>\n            <td>53.1</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Interleave-14B</td>\n            <td>14B</td>\n            <td>66.4</td>\n            <td>52.6</td>\n            <td>32.7</td>\n            <td>30.2</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td colspan=\"7\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Emu2-Chat</td>\n            <td>37B</td>\n            <td>37.8</td>\n            <td>36.2</td>\n            <td>-</td>\n            <td>27.2</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM</td>\n            <td>17B</td>\n            <td>45.2</td>\n            <td>41.1</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">VPG-C</td>\n            <td>7B</td>\n            <td>52.4</td>\n            <td>43.1</td>\n            <td>24.3</td>\n            <td>23.1</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">VILA 8B</td>\n            <td>8B</td>\n            <td>51.2</td>\n            <td>39.3</td>\n            <td>-</td>\n            <td>36.5</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternLM-XComposer-2.5</td>\n            <td>8B</td>\n            <td>53.1*</td>\n            <td>48.9</td>\n            <td>32.1*</td>\n            <td>-</td>\n            <td>42.5</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>59.0*</td>\n            <td>50.9</td>\n            <td>30.5*</td>\n            <td>34.4*</td>\n            <td><strong>56.9*</strong></td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>69.1</strong></td>\n            <td><strong>53.0</strong></td>\n            <td><strong>84.9</strong></td>\n            <td><strong>74.9</strong></td>\n            <td>53.8</td>\n        </tr>\n    </tbody>\n</table>\n\n</div>\n* We evaluate the officially released checkpoint by ourselves.\n</details>\n\n<details>\n<summary>Click to view video results on Video-MME and Video-ChatGPT.</summary>\n<div align=\"center\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th colspan=\"2\">Video-MME</th>\n            <th colspan=\"5\">Video-ChatGPT</th>\n        </tr>\n        <tr>\n            <th align=\"left\"></th>\n            <th></th>\n            <th>w/o subs</th>\n            <th>w subs</th>\n            <th>Correctness</th>\n            <th>Detail</th>\n            <th>Context</th>\n            <th>Temporal</th>\n            <th>Consistency</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"9\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Claude 3.5 Sonnet</td>\n            <td>-</td>\n            <td>60.0</td>\n            <td>62.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>59.9</td>\n            <td>63.3</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td colspan=\"9\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-7B</td>\n            <td>7B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.39</td>\n            <td>3.29</td>\n            <td>3.92</td>\n            <td>2.60</td>\n            <td>3.12</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-34B</td>\n            <td>34B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.29</td>\n            <td>3.23</td>\n            <td>3.83</td>\n            <td>2.51</td>\n            <td>3.47</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM2-Video</td>\n            <td>12B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.49</td>\n            <td><strong>3.46</strong></td>\n            <td>3.23</td>\n            <td><strong>2.98</strong></td>\n            <td><strong>3.64</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LongVA</td>\n            <td>7B</td>\n            <td>52.4</td>\n            <td>54.3</td>\n            <td>3.05</td>\n            <td>3.09</td>\n            <td>3.77</td>\n            <td>2.44</td>\n            <td><strong>3.64</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>54.0</td>\n            <td>56.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternLM-XComposer-2.5</td>\n            <td>8B</td>\n            <td>55.8</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Video</td>\n            <td>32B</td>\n            <td>60.2</td>\n            <td>63.0</td>\n            <td>3.48</td>\n            <td>3.37</td>\n            <td><strong>3.95</strong></td>\n            <td>2.64</td>\n            <td>3.28</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>60.9</strong></td>\n            <td><strong>63.6</strong></td>\n            <td><strong>3.59</strong></td>\n            <td>3.28</td>\n            <td>3.93</td>\n            <td>2.73</td>\n            <td>3.62</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n</details>\n\n\n<details>\n<summary>Click to view few-shot results on TextVQA, VizWiz, VQAv2, OK-VQA.</summary>\n<div align=\"center\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Shot</th>\n            <th>TextVQA val</th>\n            <th>VizWiz test-dev</th>\n            <th>VQAv2 test-dev</th>\n            <th>OK-VQA val</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">Flamingo</td>\n            <td rowspan=\"3\">80B</td>\n            <td>0*</td>\n            <td>35.0</td>\n            <td>31.6</td>\n            <td>56.3</td>\n            <td>40.6</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>36.5</td>\n            <td>39.6</td>\n            <td>63.1</td>\n            <td><strong>57.4</strong></td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>37.3</td>\n            <td>44.8</td>\n            <td>65.6</td>\n            <td>57.5</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">IDEFICS</td>\n            <td rowspan=\"3\">80B</td>\n            <td>0*</td>\n            <td>30.9</td>\n            <td>36.0</td>\n            <td>60.0</td>\n            <td>45.2</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>34.3</td>\n            <td>40.4</td>\n            <td>63.6</td>\n            <td>52.4</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>35.7</td>\n            <td>46.1</td>\n            <td>64.8</td>\n            <td>55.1</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">OmniCorpus</td>\n            <td rowspan=\"3\">7B</td>\n            <td>0*</td>\n            <td>43.0</td>\n            <td>49.8</td>\n            <td>63.2</td>\n            <td>45.5</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>45.4</td>\n            <td>51.3</td>\n            <td>64.5</td>\n            <td>46.5</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>45.6</td>\n            <td>52.2</td>\n            <td>64.7</td>\n            <td>46.6</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">Emu2</td>\n            <td rowspan=\"3\">37B</td>\n            <td>0</td>\n            <td>26.4</td>\n            <td>40.4</td>\n            <td>33.5</td>\n            <td>26.7</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>48.2</td>\n            <td>54.6</td>\n            <td>67.0</td>\n            <td>53.2</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>49.3</td>\n            <td>54.7</td>\n            <td>67.8</td>\n            <td>54.1</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"2\">MM1</td>\n            <td rowspan=\"2\">30B</td>\n            <td>0</td>\n            <td>26.2</td>\n            <td>40.4</td>\n            <td>48.9</td>\n            <td>26.7</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>49.3</td>\n            <td>54.7</td>\n            <td><strong>70.9</strong></td>\n            <td>54.1</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">MiniCPM-V 2.6<sup>+</sup></td>\n            <td rowspan=\"3\">8B</td>\n            <td>0</td>\n            <td>43.9</td>\n            <td>33.8</td>\n            <td>45.4</td>\n            <td>23.9</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td>4</td>\n            <td>63.6</td>\n            <td>60.5</td>\n            <td>65.5</td>\n            <td>50.1</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td>8</td>\n            <td><strong>64.6</strong></td>\n            <td><strong>63.4</strong></td>\n            <td>68.2</td>\n            <td>51.4</td>\n        </tr>\n    </tbody>\n</table>\n\n\n</div>\n* denotes zero image shot and two additional text shots following Flamingo.\n\n<sup>+</sup> We evaluate the pretraining ckpt without SFT.\n</details>\n\n### Examples <!-- omit in toc -->\n\n<div style=\"display: flex; flex-direction: column; align-items: center;\">\n  <img src=\"assets/minicpmv2_6/multi_img-bike.png\" alt=\"Bike\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multi_img-menu.png\" alt=\"Menu\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multi_img-code.png\" alt=\"Code\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/ICL-Mem.png\" alt=\"Mem\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multiling-medal.png\" alt=\"medal\" style=\"margin-bottom: 10px;\">\n</div>\n<details>\n  <summary>Click to view more cases.</summary>\n  <div style=\"display: flex; flex-direction: column; align-items: center;\">\n    <img src=\"assets/minicpmv2_6/ICL-elec.png\" alt=\"elec\" style=\"margin-bottom: 5px;\">\n    <img src=\"assets/minicpmv2_6/multiling-olympic.png\" alt=\"Menu\" style=\"margin-bottom: 10px;\">\n  </div>\n</details>\n\nWe deploy MiniCPM-V 2.6 on end devices. The demo video is the raw screen recording on a iPad Pro without edition.\n\n<table align=\"center\"> \n    <p align=\"center\">\n      <img src=\"assets/gif_cases/ai.gif\" width=32%/>\n      &nbsp;&nbsp;&nbsp;&nbsp;\n      <img src=\"assets/gif_cases/beer.gif\" width=32%/>\n    </p>\n</table> \n\n<table align=\"center\"> \n    <p align=\"center\">\n      <img src=\"assets/gif_cases/ticket.gif\" width=32%/>\n      &nbsp;&nbsp;&nbsp;&nbsp;\n      <img src=\"assets/gif_cases/wfh.gif\" width=32%/>\n    </p>\n</table> \n\n<table align=\"center\">\n    <p align=\"center\">\n      <video src=\"https://github.com/user-attachments/assets/21f4b818-ede1-4822-920e-91281725c830\" width=\"360\" /> </video>\n      <!-- <video src=\"https://github.com/user-attachments/assets/c835f757-206b-4d9c-8e36-70d67b453628\" width=\"360\" /> </video> -->\n    </p>\n</table>\n\n## MiniCPM-Llama3-V 2.5\n\n<details>\n<summary>Click to view more details of MiniCPM-Llama3-V 2.5</summary>\n\n**MiniCPM-Llama3-V 2.5** is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Llama3-8B-Instruct with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.0. Notable features of MiniCPM-Llama3-V 2.5 include:\n\n- üî• **Leading Performance.**\n  MiniCPM-Llama3-V 2.5 has achieved an average score of 65.1 on OpenCompass, a comprehensive evaluation over 11 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4V-1106, Gemini Pro, Claude 3 and Qwen-VL-Max** and greatly outperforms other Llama 3-based MLLMs.\n\n- üí™ **Strong OCR Capabilities.**\n  MiniCPM-Llama3-V 2.5 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), achieving a **700+ score on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V-0409, Qwen-VL-Max and Gemini Pro**. Based on recent user feedback, MiniCPM-Llama3-V 2.5 has now enhanced full-text OCR extraction, table-to-markdown conversion, and other high-utility capabilities, and has further strengthened its instruction-following and complex reasoning abilities, enhancing multimodal interaction experiences.\n\n- üèÜ **Trustworthy Behavior.**\n  Leveraging the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) method (the newest technique in the [RLHF-V](https://github.com/RLHF-V) [CVPR'24] series), MiniCPM-Llama3-V 2.5 exhibits more trustworthy behavior. It achieves a **10.3%** hallucination rate on Object HalBench, lower than GPT-4V-1106 (13.6%), achieving the best-level performance within the open-source community. [Data released](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset).\n\n- üåè **Multilingual Support.**\n  Thanks to the strong multilingual capabilities of Llama 3 and the cross-lingual generalization technique from [VisCPM](https://github.com/OpenBMB/VisCPM), MiniCPM-Llama3-V 2.5 extends its bilingual (Chinese-English) multimodal capabilities to **over 30 languages including German, French, Spanish, Italian, Korean etc.** [All Supported Languages](./assets/minicpm-llama-v-2-5_languages.md).\n\n- üöÄ **Efficient Deployment.**\n  MiniCPM-Llama3-V 2.5 systematically employs **model quantization, CPU optimizations, NPU optimizations and compilation optimizations**, achieving high-efficiency deployment on end-side devices. For mobile phones with Qualcomm chips, we have integrated the NPU acceleration framework QNN into llama.cpp for the first time. After systematic optimization, MiniCPM-Llama3-V 2.5 has realized a **150x acceleration in end-side MLLM image encoding** and a **3x speedup in language decoding**.\n\n-  üí´  **Easy Usage.**\nMiniCPM-Llama3-V 2.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md) and [ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5) support for efficient CPU inference on local devices, (2) [GGUF](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf) format quantized models in 16 sizes, (3) efficient [LoRA](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#lora-finetuning) fine-tuning with only 2 V100 GPUs, (4) [streaming output](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage), (5) quick local WebUI demo setup with [Gradio](https://github.com/OpenBMB/MiniCPM-V/blob/main/web_demo_2.5.py) and [Streamlit](https://github.com/OpenBMB/MiniCPM-V/blob/main/web_demo_streamlit-2_5.py), and (6) interactive demos on [HuggingFace Spaces](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5).\n\n### Evaluation  <!-- omit in toc -->\n\n<div align=\"center\">\n    <img src=assets/MiniCPM-Llama3-V-2.5-peformance.png width=66% />\n</div>\n<details>\n<summary>Click to view results on TextVQA, DocVQA, OCRBench, OpenCompass, MME, MMBench, MMMU, MathVista, LLaVA Bench, RealWorld QA, Object HalBench. </summary>\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>OCRBench</th>\n            <th>TextVQA val</th>\n            <th>DocVQA test</th>\n            <th>Open-Compass</th>\n            <th>MME</th>\n            <th>MMB test (en)</th>\n            <th>MMB test (cn)</th>\n            <th>MMMU val</th>\n            <th>Math-Vista</th>\n            <th>LLaVA Bench</th>\n            <th>RealWorld QA</th>\n            <th>Object HalBench</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"14\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini Pro</td>\n            <td>-</td>\n            <td>680</td>\n            <td>74.6</td>\n            <td>88.1</td>\n            <td>62.9</td>\n            <td>2148.9</td>\n            <td>73.6</td>\n            <td>74.3</td>\n            <td>48.9</td>\n            <td>45.8</td>\n            <td>79.9</td>\n            <td>60.4</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V (2023.11.06)</td>\n            <td>-</td>\n            <td>645</td>\n            <td>78.0</td>\n            <td>88.4</td>\n            <td>63.5</td>\n            <td>1771.5</td>\n            <td>77.0</td>\n            <td>74.4</td>\n            <td>53.8</td>\n            <td>47.8</td>\n            <td>93.1</td>\n            <td>63.0</td>\n            <td>86.4</td>\n        </tr>\n        <tr>\n            <td colspan=\"14\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Mini-Gemini</td>\n            <td>2.2B</td>\n            <td>-</td>\n            <td>56.2</td>\n            <td>34.2*</td>\n            <td>-</td>\n            <td>1653.0</td>\n            <td>-</td>\n            <td>-</td>\n            <td>31.7</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen-VL-Chat</td>\n            <td>9.6B</td>\n            <td>488</td>\n            <td>61.5</td>\n            <td>62.6</td>\n            <td>51.6</td>\n            <td>1860.0</td>\n            <td>61.8</td>\n            <td>56.3</td>\n            <td>37.0</td>\n            <td>33.8</td>\n            <td>67.7</td>\n            <td>49.3</td>\n            <td>56.2</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">DeepSeek-VL-7B</td>\n            <td>7.3B</td>\n            <td>435</td>\n            <td>64.7*</td>\n            <td>47.0*</td>\n            <td>54.6</td>\n            <td>1765.4</td>\n            <td>73.8</td>\n            <td>71.4</td>\n            <td>38.3</td>\n            <td>36.8</td>\n            <td>77.8</td>\n            <td>54.2</td>\n            <td>-</td>\n        </tr>        \n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Yi-VL-34B</td>\n            <td>34B</td>\n            <td>290</td>\n            <td>43.4*</td>\n            <td>16.9*</td>\n            <td>52.2</td>\n            <td><strong>2050.2</strong></td>\n            <td>72.4</td>\n            <td>70.7</td>\n            <td>45.1</td>\n            <td>30.7</td>\n            <td>62.3</td>\n            <td>54.8</td>\n            <td>79.3</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM-Chat</td>\n            <td>17.4B</td>\n            <td>590</td>\n            <td>70.4</td>\n            <td>33.3*</td>\n            <td>54.2</td>\n            <td>1736.6</td>\n            <td>65.8</td>\n            <td>55.9</td>\n            <td>37.3</td>\n            <td>34.7</td>\n            <td>73.9</td>\n            <td>60.3</td>\n            <td>73.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">TextMonkey</td>\n            <td>9.7B</td>\n            <td>558</td>\n            <td>64.3</td>\n            <td>66.7</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n          <td nowrap=\"nowrap\" align=\"left\">Idefics2</td>\n          <td>8.0B</td>\n          <td>-</td>\n          <td>73.0</td>\n          <td>74.0</td>\n          <td>57.2</td>\n          <td>1847.6</td>\n          <td>75.7</td>\n          <td>68.6</td>\n          <td>45.2</td>\n          <td>52.2</td>\n          <td>49.1</td>\n          <td>60.7</td>\n          <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Bunny-LLama-3-8B</td>\n            <td>8.4B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>54.3</td>\n            <td>1920.3</td>\n            <td>77.0</td>\n            <td>73.9</td>\n            <td>41.3</td>\n            <td>31.5</td>\n            <td>61.2</td>\n            <td>58.8</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT Llama-3-8B</td>\n            <td>8.4B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>78.2</td>\n            <td>-</td>\n            <td>1971.5</td>\n            <td>-</td>\n            <td>-</td>\n            <td>41.7</td>\n            <td>37.5</td>\n            <td>80.1</td>\n            <td>60.0</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Phi-3-vision-128k-instruct</td>\n            <td>4.2B</td>\n            <td>639*</td>\n            <td>70.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>1537.5*</td>\n            <td>-</td>\n            <td>-</td>\n            <td>40.4</td>\n            <td>44.5</td>\n            <td>64.2*</td>\n            <td>58.8*</td>\n            <td>-</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 1.0</td>\n            <td>2.8B</td>\n            <td>366</td>\n            <td>60.6</td>\n            <td>38.2</td>\n            <td>47.5</td>\n            <td>1650.2</td>\n            <td>64.1</td>\n            <td>62.6</td>\n            <td>38.3</td>\n            <td>28.9</td>\n            <td>51.3</td>\n            <td>51.2</td>\n            <td>78.4</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.0</td>\n            <td>2.8B</td>\n            <td>605</td>\n            <td>74.1</td>\n            <td>71.9</td>\n            <td>54.5</td>\n            <td>1808.6</td>\n            <td>69.1</td>\n            <td>66.5</td>\n            <td>38.2</td>\n            <td>38.7</td>\n            <td>69.2</td>\n            <td>55.8</td>\n            <td>85.5</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-Llama3-V 2.5</td>\n            <td>8.5B</td>\n            <td><strong>725</strong></td>\n            <td><strong>76.6</strong></td>\n            <td><strong>84.8</strong></td>\n            <td><strong>65.1</strong></td>\n            <td>2024.6</td>\n            <td><strong>77.2</strong></td>\n            <td><strong>74.2</strong></td>\n            <td><strong>45.8</strong></td>\n            <td><strong>54.3</strong></td>\n            <td><strong>86.7</strong></td>\n            <td><strong>63.5</strong></td>\n            <td><strong>89.7</strong></td>\n        </tr>\n    </tbody>\n</table>\n\n\n</div>\n* We evaluate the officially released checkpoint by ourselves.\n\n</details>\n\n<div align=\"center\">\n    <img src=\"assets/llavabench_compare_3.png\" width=\"100%\" />\n    <br>\n    Evaluation results of multilingual LLaVA Bench\n</div>\n\n### Examples <!-- omit in toc -->\n\n<table align=\"center\" >\n  <p align=\"center\" > \n  <img src=\"assets/minicpmv-llama3-v2.5/cases_all.png\" />\n  </p>\n</table>\n\n</details>\n\n\n## MiniCPM-V 2.0\n\n<details>\n<summary>Click to view more details of MiniCPM-V 2.0</summary>\n\n\n**MiniCPM-V 2.0** is an efficient version with promising performance for deployment. The model is built based on SigLip-400M and [MiniCPM-2.4B](https://github.com/OpenBMB/MiniCPM/), connected by a perceiver resampler. Our latest version, MiniCPM-V 2.0 has several notable features. \n\n- üî• **State-of-the-art Performance.** \n\n  MiniCPM-V 2.0 achieves **state-of-the-art performance** on multiple benchmarks (including OCRBench, TextVQA, MME, MMB, MathVista, etc) among models under 7B parameters. It even **outperforms strong Qwen-VL-Chat 9.6B, CogVLM-Chat 17.4B, and Yi-VL 34B on OpenCompass, a comprehensive evaluation over 11 popular benchmarks**. Notably, MiniCPM-V 2.0 shows **strong OCR capability**, achieving **comparable performance to Gemini Pro in scene-text understanding**, and **state-of-the-art performance on OCRBench** among open-source models.\n\n- üèÜ **Trustworthy Behavior.** \n\n  LMMs are known for suffering from hallucination, often generating text not factually grounded in images. MiniCPM-V 2.0 is **the first end-side LMM aligned via multimodal RLHF for trustworthy behavior** (using the recent [RLHF-V](https://rlhf-v.github.io/) [CVPR'24] series technique). This allows the model to **match GPT-4V in preventing hallucinations** on Object HalBench.\n\n- üåü **High-Resolution Images at Any Aspect Raito.**\n\n  MiniCPM-V 2.0 can accept **1.8 million pixels (e.g., 1344x1344) images at any aspect ratio**. This enables better perception of fine-grained visual information such as small objects and optical characters, which is achieved via a recent technique from [LLaVA-UHD](https://arxiv.org/pdf/2403.11703.pdf).\n\n- ‚ö°Ô∏è **High Efficiency.** \n\n  MiniCPM-V 2.0 can be **efficiently deployed on most GPU cards and personal computers**, and **even on end devices such as mobile phones**. For visual encoding, we compress the image representations into much fewer tokens via a perceiver resampler. This allows MiniCPM-V 2.0 to operate with **favorable memory cost and speed during inference even when dealing with high-resolution images**.\n\n- üôå **Bilingual Support.** \n\n  MiniCPM-V 2.0 **supports strong bilingual multimodal capabilities in both English and Chinese**. This is enabled by generalizing multimodal capabilities across languages, a technique from [VisCPM](https://arxiv.org/abs/2308.12038) [ICLR'24].\n\n### Examples <!-- omit in toc -->\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/minicpmv2-cases_2.png\" width=95%/>\n    </p>\n</table>\n\nWe deploy MiniCPM-V 2.0 on end devices. The demo video is the raw screen recording on a Xiaomi 14 Pro without edition.\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/gif_cases/station.gif\" width=36%/>\n      <img src=\"assets/gif_cases/london_car.gif\" width=36%/>\n    </p>\n</table>\n\n</details>\n\n## Legacy Models <!-- omit in toc --> \n\n| Model                | Introduction and Guidance       |\n|:----------------------|:-------------------:|\n| MiniCPM-V 1.0  | [Document](./minicpm_v1.md)   | \n| OmniLMM-12B  | [Document](./omnilmm_en.md)   |  \n\n\n## Chat with Our Demo on Gradio ü§ó\n\nWe provide online and local demos powered by Hugging Face Gradio <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts,  and other useful features.\n\n\n### Online Demo <!-- omit in toc --> \n\nClick here to try out the online demo of [MiniCPM-V 2.6](http://120.92.209.146:8887/) | [MiniCPM-Llama3-V 2.5](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5) | [MiniCPM-V 2.0](https://huggingface.co/spaces/openbmb/MiniCPM-V-2).\n\n### Local WebUI Demo <!-- omit in toc --> \n  \nYou can easily build your own local WebUI demo with Gradio using the following commands.\n  \n```shell\npip install -r requirements.txt\n```\n  \n```shell\n# For NVIDIA GPUs, run:\npython web_demo_2.6.py --device cuda\n\n```\n\n\n## Install\n\n1. Clone this repository and navigate to the source folder\n\n```bash\ngit clone https://github.com/OpenBMB/MiniCPM-V.git\ncd MiniCPM-V\n```\n\n2. Create conda environment\n\n```Shell\nconda create -n MiniCPM-V python=3.10 -y\nconda activate MiniCPM-V\n```\n\n3. Install dependencies\n\n```shell\npip install -r requirements.txt\n```\n\n## Inference\n\n\n### Model Zoo\n\n| Model           | Device | Memory    | &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Description       | Download |\n|:-----------|:--:|:-----------:|:-------------------|:---------------:|\n| MiniCPM-V 2.6| GPU | 17 GB  | The latest version, achieving state-of-the-art end-side performance for single image, multi-image and video understanding.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6) |\n| MiniCPM-V 2.6 gguf | CPU | 6 GB  | The gguf version, lower memory usage and faster inference.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6-gguf) |\n| MiniCPM-V 2.6 int4 | GPU | 7 GB  | The int4 quantized version, lower GPU memory usage.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6-int4) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6-int4) |\n| MiniCPM-Llama3-V 2.5 | GPU | 19 GB | Strong end-side multimodal performance.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5) |\n| MiniCPM-Llama3-V 2.5 gguf | CPU  | 6 GB | The gguf version, lower memory usage and faster inference.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf) &nbsp;&nbsp;[<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5-gguf) |\n| MiniCPM-Llama3-V 2.5 int4 | GPU | 8 GB | The int4 quantized version, lower GPU memory usage. |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4/) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5-int4) |\n| MiniCPM-V 2.0 | GPU | 8 GB | Light version, balance the performance the computation cost.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2) |\n| MiniCPM-V 1.0 | GPU | 7 GB | Lightest version, achieving the fastest inference. |   [ü§ó](https://huggingface.co/openbmb/MiniCPM-V) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V) |\n\n### Multi-turn Conversation\n\nPlease refer to the following codes to run.\n\n<div align=\"center\">\n<img src=\"assets/airplane.jpeg\" width=\"500px\">\n</div>\n\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\ntorch.manual_seed(0)\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nimage = Image.open('./assets/airplane.jpeg').convert('RGB')\n\n# First round chat \nquestion = \"Tell me the model of this aircraft.\"\nmsgs = [{'role': 'user', 'content': [image, question]}]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n\n# Second round chat \n# pass history context of multi-turn conversation\nmsgs.append({\"role\": \"assistant\", \"content\": [answer]})\nmsgs.append({\"role\": \"user\", \"content\": [\"Introduce something about Airbus A380.\"]})\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n\nYou will get the following output:\n\n```\n\"The aircraft in the image is an Airbus A380, which can be identified by its large size, double-deck structure, and the distinctive shape of its wings and engines. The A380 is a wide-body aircraft known for being the world's largest passenger airliner, designed for long-haul flights. It has four engines, which are characteristic of large commercial aircraft. The registration number on the aircraft can also provide specific information about the model if looked up in an aviation database.\"\n\n\"The Airbus A380 is a double-deck, wide-body, four-engine jet airliner made by Airbus. It is the world's largest passenger airliner and is known for its long-haul capabilities. The aircraft was developed to improve efficiency and comfort for passengers traveling over long distances. It has two full-length passenger decks, which can accommodate more passengers than a typical single-aisle airplane. The A380 has been operated by airlines such as Lufthansa, Singapore Airlines, and Emirates, among others. It is widely recognized for its unique design and significant impact on the aviation industry.\"\n```\n\n#### Chat with multiple images\n<details>\n<summary> Click to view Python code running MiniCPM-V 2.6 with multiple images input. </summary>\n  \n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nimage1 = Image.open('image1.jpg').convert('RGB')\nimage2 = Image.open('image2.jpg').convert('RGB')\nquestion = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'\n\nmsgs = [{'role': 'user', 'content': [image1, image2, question]}]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n#### In-context few-shot learning\n<details>\n<summary> Click to view Python code running MiniCPM-V 2.6 with few-shot input. </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nquestion = \"production date\" \nimage1 = Image.open('example1.jpg').convert('RGB')\nanswer1 = \"2023.08.04\"\nimage2 = Image.open('example2.jpg').convert('RGB')\nanswer2 = \"2007.04.24\"\nimage_test = Image.open('test.jpg').convert('RGB')\n\nmsgs = [\n    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n    {'role': 'user', 'content': [image_test, question]}\n]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n#### Chat with video\n<details>\n<summary> Click to view Python code running MiniCPM-V 2.6 with video input. </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom decord import VideoReader, cpu    # pip install decord\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nMAX_NUM_FRAMES=64 # if cuda OOM set a smaller number\n\ndef encode_video(video_path):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n\n    vr = VideoReader(video_path, ctx=cpu(0))\n    sample_fps = round(vr.get_avg_fps() / 1)  # FPS\n    frame_idx = [i for i in range(0, len(vr), sample_fps)]\n    if len(frame_idx) > MAX_NUM_FRAMES:\n        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\n    frames = vr.get_batch(frame_idx).asnumpy()\n    frames = [Image.fromarray(v.astype('uint8')) for v in frames]\n    print('num frames:', len(frames))\n    return frames\n\nvideo_path=\"video_test.mp4\"\nframes = encode_video(video_path)\nquestion = \"Describe the video\"\nmsgs = [\n    {'role': 'user', 'content': frames + [question]}, \n]\n\n# Set decode params for video\nparams = {}\nparams[\"use_image_id\"] = False\nparams[\"max_slice_nums\"] = 2 # use 1 if cuda OOM and video resolution > 448*448\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer,\n    **params\n)\nprint(answer)\n```\n</details>\n\n\n### Inference on Multiple GPUs\nYou can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs (12 GB or 16 GB) by distributing the model's layers across multiple GPUs. Please refer to this [tutorial](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md) for detailed instructions on how to load the model and inference using multiple low VRAM GPUs.\n\n\n### Inference on Mac\n<details>\n<summary>Click to view an example, to run MiniCPM-Llama3-V 2.5 on üíª Mac with MPS (Apple silicon or AMD GPUs). </summary>\n\n```python\n# test.py  Need more than 16GB memory.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, low_cpu_mem_usage=True)\nmodel = model.to(device='mps')\n\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\nmodel.eval()\n\nimage = Image.open('./assets/hk_OCR.jpg').convert('RGB')\nquestion = 'Where is this photo taken?'\nmsgs = [{'role': 'user', 'content': question}]\n\nanswer, context, _ = model.chat(\n    image=image,\n    msgs=msgs,\n    context=None,\n    tokenizer=tokenizer,\n    sampling=True\n)\nprint(answer)\n```\nRun with command:\n```shell\nPYTORCH_ENABLE_MPS_FALLBACK=1 python test.py\n```\n</details>\n\n### Deployment on Mobile Phone\nMiniCPM-V 2.0 can be deployed on mobile phones with Android operating systems. üöÄ Click [MiniCPM-V 2.0](https://github.com/OpenBMB/mlc-MiniCPM) to install apk.\n\n### Inference with llama.cpp\nMiniCPM-V 2.6 can run with llama.cpp now! See [our fork of llama.cpp](https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md) for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).\n\n### Inference with ollama\nMiniCPM-V 2.6 can run with ollama now! See [our fork of ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md) for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).\n\n### Inference with vLLM\n\n<details>\n<summary> vLLM now officially supports MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0, Click to see. </summary>\n\n1. Install vLLM(>=0.5.4):\n```shell\npip install vllm\n```\n2. Install timm: (optional, MiniCPM-V 2.0 need timm)\n```shell\npip install timm==0.9.10\n```\n3. Run the example(for image):\n```python\nfrom transformers import AutoTokenizer\nfrom PIL import Image\nfrom vllm import LLM, SamplingParams\n\nMODEL_NAME = \"openbmb/MiniCPM-V-2_6\"\n# Also available for previous models\n# MODEL_NAME = \"openbmb/MiniCPM-Llama3-V-2_5\"\n# MODEL_NAME = \"HwwwH/MiniCPM-V-2\"\n\nimage = Image.open(\"xxx.png\").convert(\"RGB\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\nllm = LLM(\n    model=MODEL_NAME,\n    trust_remote_code=True,\n    gpu_memory_utilization=1,\n    max_model_len=2048\n)\n\nmessages = [{\n    \"role\":\n    \"user\",\n    \"content\":\n    # Number of images\n    \"(<image>./</image>)\" + \\\n    \"\\nWhat is the content of this image?\" \n}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Single Inference\ninputs = {\n    \"prompt\": prompt,\n    \"multi_modal_data\": {\n        \"image\": image\n        # Multi images, the number of images should be equal to that of `(<image>./</image>)`\n        # \"image\": [image, image] \n    },\n}\n# Batch Inference\n# inputs = [{\n#     \"prompt\": prompt,\n#     \"multi_modal_data\": {\n#         \"image\": image\n#     },\n# } for _ in 2]\n\n\n# 2.6\nstop_tokens = ['<|im_end|>', '<|endoftext|>']\nstop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n# 2.0\n# stop_token_ids = [tokenizer.eos_id]\n# 2.5\n# stop_token_ids = [tokenizer.eos_id, tokenizer.eot_id]\n\nsampling_params = SamplingParams(\n    stop_token_ids=stop_token_ids, \n    use_beam_search=True,\n    temperature=0, \n    best_of=3,\n    max_tokens=1024\n)\n\noutputs = llm.generate(inputs, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n```\n4. click [here](https://modelbest.feishu.cn/wiki/C2BWw4ZP0iCDy7kkCPCcX2BHnOf?from=from_copylink) if you want to use it with *video*, or get more details about `vLLM`.\n</details>\n\n## Fine-tuning\n\n### Simple Fine-tuning <!-- omit in toc -->\n\nWe support simple fine-tuning with Hugging Face for MiniCPM-V 2.0 and MiniCPM-Llama3-V 2.5.\n\n[Reference Document](./finetune/readme.md)\n\n### With the SWIFT Framework <!-- omit in toc -->\n\nWe now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.\n\nBest PracticesÔºö[MiniCPM-V 1.0](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-vÊúÄ‰Ω≥ÂÆûË∑µ.md), [MiniCPM-V 2.0](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2ÊúÄ‰Ω≥ÂÆûË∑µ.md), [MiniCPM-V 2.6](https://github.com/modelscope/ms-swift/issues/1613).\n\n## FAQs\nClick here to view the [FAQs](./docs/faqs.md)\n\n## Model License <!-- omit in toc -->\n\n* This repository is released under the [Apache-2.0](https://github.com/OpenBMB/MiniCPM/blob/main/LICENSE) License. \n\n* The usage of MiniCPM-V model weights must strictly follow [MiniCPM Model License.md](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md).\n\n* The models and weights of MiniCPM are completely free for academic research. after filling out a [\"questionnaire\"](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g) for registration, are also available for free commercial use.\n  \n\n## Statement <!-- omit in toc -->\n\nAs LMMs, MiniCPM-V models (including OmniLMM) generate contents by learning a large amount of multimodal corpora, but they cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V models does not represent the views and positions of the model developers\n\nWe will not be liable for any problems arising from the use of MiniCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\n\n\n## Institutions  <!-- omit in toc -->\n\nThis project is developed by the following institutions:\n\n- <img src=\"assets/thunlp.png\" width=\"28px\"> [THUNLP](https://nlp.csai.tsinghua.edu.cn/)\n- <img src=\"assets/modelbest.png\" width=\"28px\"> [ModelBest](https://modelbest.cn/)\n- <img src=\"assets/zhihu.webp\" width=\"28px\"> [Zhihu](https://www.zhihu.com/ )\n\n## üåü Star History <!-- omit in toc -->\n\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/star_history.svg\"/>\n    </p>\n</table>\n\n<!-- <picture>\n  <source\n    media=\"(prefers-color-scheme: dark)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date&theme=dark\n    \"\n  />\n  <source\n    media=\"(prefers-color-scheme: light)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date\n    \"\n  />\n  <img\n    alt=\"Star History Chart\"\n    src=\"https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date\"\n  />\n</picture> -->\n\n## Key Techniques and Other Multimodal Projects <!-- omit in toc -->\n\nüëè Welcome to explore key techniques of MiniCPM-V and other multimodal projects of our team:\n\n[VisCPM](https://github.com/OpenBMB/VisCPM/tree/main) | [RLHF-V](https://github.com/RLHF-V/RLHF-V) | [LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD) | [RLAIF-V](https://github.com/RLHF-V/RLAIF-V)\n\n\n## Citation <!-- omit in toc -->\n\nIf you find our model/code/paper helpful, please consider cite our papers üìù and star us ‚≠êÔ∏èÔºÅ\n\n```bib\n@article{yao2024minicpm,\n  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\n  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\n  journal={arXiv preprint arXiv:2408.01800},\n  year={2024}\n}\n```\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 64.1015625,
          "content": "<div align=\"center\">\n\n<img src=\"./assets/minicpmv.png\" width=\"300em\" ></img> \n\n**A GPT-4V Level MLLM for Single Image, Multi Image and Video on Your Phone**\n\n  <strong>[‰∏≠Êñá](./README_zh.md) |\n  English</strong>\n\nJoin our <a href=\"docs/wechat.md\" target=\"_blank\"> üí¨ WeChat</a> | View  MiniCPM-V <a href=\"docs/best_practice_summary.md\" target=\"_blank\"> üìñ best practices</a>\n\n\n<p align=\"center\">\n  MiniCPM-V 2.6 <a href=\"https://huggingface.co/openbmb/MiniCPM-V-2_6\">ü§ó</a> <a href=\"http://120.92.209.146:8887/\">ü§ñ</a> | MiniCPM-Llama3-V 2.5  <a href=\"https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/\">ü§ó</a> <a href=\"https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5\">ü§ñ</a> |\n  <a href=https://arxiv.org/abs/2408.01800>MiniCPM-Llama3-V 2.5 Technical Report</a> \n</p>\n\n</div>\n\n\n**MiniCPM-V** is a series of end-side multimodal LLMs (MLLMs) designed for vision-language understanding. The models take image, video and text as inputs and provide high-quality text outputs. Since February 2024, we have released 5 versions of the model, aiming to achieve **strong performance and efficient deployment**. The most notable models in this series currently include:\n\n- **MiniCPM-V 2.6**: üî•üî•üî• The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, the model **surpasses GPT-4V in single image, multi-image and video understanding**. It outperforms **GPT-4o mini, Gemini 1.5 Pro and Claude 3.5 Sonnet** in single image understanding, and advances MiniCPM-Llama3-V 2.5's features such as strong OCR capability, trustworthy behavior, multilingual support, and end-side deployment. Due to its superior token density, MiniCPM-V 2.6 can for the first time support real-time video understanding on end-side devices such as iPad.\n\n- **MiniCPM-V 2.0**: The lightest model in the MiniCPM-V series. With 2B parameters, it surpasses larger models such as Yi-VL 34B, CogVLM-Chat 17B, and Qwen-VL-Chat 10B in overall performance. It can accept image inputs of any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), achieving comparable performance with Gemini Pro in understanding scene-text and matches GPT-4V in low hallucination rates.\n\n\n## News <!-- omit in toc -->\n\n#### üìå Pinned\n\n* [2024.08.17] üöÄüöÄüöÄ MiniCPM-V 2.6 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf).\n* [2024.08.15] We now also support multi-image SFT. For more details, please refer to the [document](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune).\n* [2024.08.14] MiniCPM-V 2.6 now also supports [fine-tuning](https://github.com/modelscope/ms-swift/issues/1613) with the SWIFT framework!\n* [2024.08.10] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf).\n* [2024.08.06] üî•üî•üî• We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!\n* [2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See [here](https://arxiv.org/abs/2408.01800).\n* [2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See [here](#inference-with-vllm).\n* [2024.05.28] üí´ We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics [here](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics).\n* [2024.05.23] üîç We've released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmarks evaluations, multilingual capabilities, and inference efficiency üåüüìäüåçüöÄ. Click [here](./docs/compare_with_phi-3_vision.md) to view more details.\n* [2024.05.23] üî•üî•üî• MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio‚Äôs official account, is available [here](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5). Come and try it out!\n\n<br>\n\n<details> \n<summary>Click to view more news.</summary>\n\n* [2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model's layers across multiple GPUs. For more details, Check this [link](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md).\n* [2024.05.28] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and ollama! Please pull the latest code **of our provided forks** ([llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md), [ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5)). GGUF models in various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main). MiniCPM-Llama3-V 2.5 series is **not supported by the official repositories yet**, and we are working hard to merge PRs. Please stay tuned!\n* [2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage)!\n* [2024.05.24] We release the MiniCPM-Llama3-V 2.5 [gguf](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf), which supports [llama.cpp](#inference-with-llamacpp) inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!\n* [2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide [efficient inference](#deployment-on-mobile-phone) and [simple fine-tuning](./finetune/readme.md). Try it now!\n* [2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click [here](#inference-with-vllm) to view more details.\n* [2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at [here](https://huggingface.co/spaces/openbmb/MiniCPM-V-2)!\n* [2024.04.17] MiniCPM-V-2.0 supports deploying [WebUI Demo](#webui-demo) now!\n* [2024.04.15] MiniCPM-V-2.0 now also supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2ÊúÄ‰Ω≥ÂÆûË∑µ.md) with the SWIFT framework!\n* [2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on <a href=\"https://rank.opencompass.org.cn/leaderboard-multimodal\">OpenCompass</a>, a comprehensive evaluation over 11 popular benchmarks. Click <a href=\"https://openbmb.vercel.app/minicpm-v-2\">here</a> to view the MiniCPM-V 2.0 technical blog.\n* [2024.03.14] MiniCPM-V now supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-vÊúÄ‰Ω≥ÂÆûË∑µ.md) with the SWIFT framework. Thanks to [Jintao](https://github.com/Jintao-Huang) for the contributionÔºÅ\n* [2024.03.01] MiniCPM-V now can be deployed on Mac!\n* [2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.\n</details> \n\n\n## Contents <!-- omit in toc -->\n\n\n- [MiniCPM-V 2.6](#minicpm-v-26)\n- [MiniCPM-Llama3-V 2.5](#minicpm-llama3-v-25)\n- [MiniCPM-V 2.0](#minicpm-v-20)\n- [Chat with Our Demo on Gradio ü§ó](#chat-with-our-demo-on-gradio-)\n- [Install](#install)\n- [Inference](#inference)\n  - [Model Zoo](#model-zoo)\n  - [Multi-turn Conversation](#multi-turn-conversation)\n    - [Chat with multiple images](#chat-with-multiple-images)\n    - [In-context few-shot learning](#in-context-few-shot-learning)\n    - [Chat with video](#chat-with-video)\n  - [Inference on Multiple GPUs](#inference-on-multiple-gpus)\n  - [Inference on Mac](#inference-on-mac)\n  - [Deployment on Mobile Phone](#deployment-on-mobile-phone)\n  - [Inference with llama.cpp](#inference-with-llamacpp)\n  - [Inference with ollama](#inference-with-ollama)\n  - [Inference with vLLM](#inference-with-vllm)\n- [Fine-tuning](#fine-tuning)\n- [FAQs](#faqs)\n\n\n## MiniCPM-V 2.6\n\n**MiniCPM-V 2.6** is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:\n\n- üî• **Leading Performance.**\n  MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet** for single image understanding.\n\n- üñºÔ∏è **Multi Image Understanding and In-context Learning.** MiniCPM-V 2.6 can also perform **conversation and reasoning over multiple images**. It achieves **state-of-the-art performance** on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.\n\n- üé¨ **Video Understanding.** MiniCPM-V 2.6 can also **accept video inputs**, performing conversation and providing dense captions for spatial-temporal information. It outperforms **GPT-4V, Claude 3.5 Sonnet and LLaVA-NeXT-Video-34B** on Video-MME with/without subtitles.\n\n- üí™ **Strong OCR Capability and Others.**\n  MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves **state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro**.\n  Based on the the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) and [VisCPM](https://github.com/OpenBMB/VisCPM) techniques, it features **trustworthy behaviors**, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports **multilingual capabilities** on English, Chinese, German, French, Italian, Korean, etc.\n\n\n- üöÄ **Superior Efficiency.**\n  In addition to its friendly size, MiniCPM-V 2.6 also shows **state-of-the-art token density** (i.e., number of pixels encoded into each visual token). **It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models**. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-V 2.6 can efficiently support **real-time video understanding** on end-side devices such as iPad.\n\n-  üí´  **Easy Usage.**\nMiniCPM-V 2.6 can be easily used in various ways: (1) [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpmv-main/examples/llava/README-minicpmv2.6.md) and [ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md) support for efficient CPU inference on local devices, (2) [int4](https://huggingface.co/openbmb/MiniCPM-V-2_6-int4) and [GGUF](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf) format quantized models in 16 sizes, (3) [vLLM](#inference-with-vllm) support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks, (5) quick local WebUI demo setup with [Gradio](#chat-with-our-demo-on-gradio), and (6) online web [demo](http://120.92.209.146:8887/).\n\n### Evaluation  <!-- omit in toc -->\n<div align=\"center\">\n    <img src=assets/radar_final.png width=66% />\n</div>\n\n<details>\n<summary>Click to view single image results on OpenCompass, MME, MMVet, OCRBench, MMMU, MathVista, MMB, AI2D, TextVQA, DocVQA, HallusionBench, Object HalBench. </summary>\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Token Density<sup>+</sup></th>\n            <th>OpenCompass</th>\n            <th>MME</th>\n            <th>MMVet</th>\n            <th>OCRBench</th>\n            <th>MMMU val</th>\n            <th>MathVista mini</th>\n            <th>MMB1.1 test</th>\n            <th>AI2D</th>\n            <th>TextVQA val</th>\n            <th>DocVQA test</th>\n            <th>HallusionBench</th>\n            <th>Object HalBench</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"15\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>69.9</td>\n            <td>2328.7</td>\n            <td>69.1</td>\n            <td>736</td>\n            <td>69.2</td>\n            <td>61.3</td>\n            <td>82.2</td>\n            <td>84.6</td>\n            <td>-</td>\n            <td>92.8</td>\n            <td>55.0</td>\n            <td>17.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Claude 3.5 Sonnet</td>\n            <td>-</td>\n            <td>750</td>\n            <td>67.9</td>\n            <td>1920.0</td>\n            <td>66.0</td>\n            <td>788</td>\n            <td>65.9</td>\n            <td>61.6</td>\n            <td>78.5</td>\n            <td>80.2</td>\n            <td>-</td>\n            <td>95.2</td>\n            <td>49.9</td>\n            <td>13.8</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini 1.5 Pro</td>\n            <td>-</td>\n            <td>-</td>\n            <td>64.4</td>\n            <td>2110.6</td>\n            <td>64.0</td>\n            <td>754</td>\n            <td>60.6</td>\n            <td>57.7</td>\n            <td>73.9</td>\n            <td>79.1</td>\n            <td>73.5</td>\n            <td>86.5</td>\n            <td>45.6</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o mini</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>64.1</td>\n            <td>2003.4</td>\n            <td>66.9</td>\n            <td>785</td>\n            <td>60.0</td>\n            <td>52.4</td>\n            <td>76.0</td>\n            <td>77.8</td>\n            <td>-</td>\n            <td>-</td>\n            <td>46.1</td>\n            <td>12.4</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>63.5</td>\n            <td>2070.2</td>\n            <td>67.5</td>\n            <td>656</td>\n            <td>61.7</td>\n            <td>54.7</td>\n            <td>79.8</td>\n            <td>78.6</td>\n            <td>78.0</td>\n            <td>87.2</td>\n            <td>43.9</td>\n            <td>14.2</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Step-1V</td>\n            <td>-</td>\n            <td>-</td>\n            <td>59.5</td>\n            <td>2206.4</td>\n            <td>63.3</td>\n            <td>625</td>\n            <td>49.9</td>\n            <td>44.8</td>\n            <td>78.0</td>\n            <td>79.2</td>\n            <td>71.6</td>\n            <td>-</td>\n            <td>48.4</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen-VL-Max</td>\n            <td>-</td>\n            <td>784</td>\n            <td>58.3</td>\n            <td>2281.7</td>\n            <td>61.8</td>\n            <td>684</td>\n            <td>52.0</td>\n            <td>43.4</td>\n            <td>74.6</td>\n            <td>75.7</td>\n            <td>79.5</td>\n            <td>93.1</td>\n            <td>41.2</td>\n            <td>13.4</td>\n        </tr>\n        <tr>\n            <td colspan=\"15\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Yi-34B</td>\n            <td>34B</td>\n            <td>157</td>\n            <td>55.0</td>\n            <td>2006.5</td>\n            <td>50.7</td>\n            <td>574</td>\n            <td>48.8</td>\n            <td>40.4</td>\n            <td>77.8</td>\n            <td>78.9</td>\n            <td>69.3</td>\n            <td>-</td>\n            <td>34.8</td>\n            <td>12.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Mini-Gemini-HD-34B</td>\n            <td>34B</td>\n            <td>157</td>\n            <td>-</td>\n            <td>2141.0</td>\n            <td>59.3</td>\n            <td>518</td>\n            <td>48.0</td>\n            <td>43.3</td>\n            <td>-</td>\n            <td>80.5</td>\n            <td>74.1</td>\n            <td>78.9</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Cambrian-34B</td>\n            <td>34B</td>\n            <td>1820</td>\n            <td>58.3</td>\n            <td>2049.9</td>\n            <td>53.2</td>\n            <td>591</td>\n            <td>50.4</td>\n            <td>50.3</td>\n            <td>77.8</td>\n            <td>79.5</td>\n            <td>76.7</td>\n            <td>75.5</td>\n            <td>41.6</td>\n            <td>14.7</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GLM-4V-9B</td>\n            <td>13B</td>\n            <td>784</td>\n            <td>59.1</td>\n            <td>2018.8</td>\n            <td>58.0</td>\n            <td>776</td>\n            <td>46.9</td>\n            <td>51.1</td>\n            <td>67.9</td>\n            <td>71.2</td>\n            <td>-</td>\n            <td>-</td>\n            <td>45.0</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>706</td>\n            <td>64.1</td>\n            <td>2215.1</td>\n            <td>54.3</td>\n            <td>794</td>\n            <td><strong>51.2</strong></td>\n            <td>58.3</td>\n            <td><strong>79.4</strong></td>\n            <td><strong>83.6</strong></td>\n            <td>77.4</td>\n            <td><strong>91.6</strong></td>\n            <td>45.0</td>\n            <td>21.3</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-Llama-V 2.5</td>\n            <td>8B</td>\n            <td>1882</td>\n            <td>58.8</td>\n            <td>2024.6</td>\n            <td>52.8</td>\n            <td>725</td>\n            <td>45.8</td>\n            <td>54.3</td>\n            <td>72.0</td>\n            <td>78.4</td>\n            <td>76.6</td>\n            <td>84.8</td>\n            <td>42.4</td>\n            <td>10.3</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>2822</strong></td>\n            <td><strong>65.2</strong></td>\n            <td><strong>2348.4</strong>*</td>\n            <td><strong>60.0</strong></td>\n            <td><strong>852</strong>*</td>\n            <td>49.8*</td>\n            <td><strong>60.6</strong></td>\n            <td>78.0</td>\n            <td>82.1</td>\n            <td><strong>80.1<strong></td>\n            <td>90.8</td>\n            <td><strong>48.1</strong>*</td>\n            <td><strong>8.2</strong></td>\n        </tr>\n    </tbody>\n</table>\n\n</div>\n* We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set.\n\n<sup>+</sup> Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.\n\nNote: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.\n\n</details>\n\n\n<details>\n<summary>Click to view multi-image results on Mantis Eval, BLINK, Mathverse mv, Sciverse mv, MIRB.</summary>\n<div align=\"center\">\n \n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Mantis Eval</th>\n            <th>BLINK val</th>\n            <th>Mathverse mv</th>\n            <th>Sciverse mv</th>\n            <th>MIRB</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"7\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>62.7</td>\n            <td>54.6</td>\n            <td>60.3</td>\n            <td>66.9</td>\n            <td>53.1</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Interleave-14B</td>\n            <td>14B</td>\n            <td>66.4</td>\n            <td>52.6</td>\n            <td>32.7</td>\n            <td>30.2</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td colspan=\"7\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Emu2-Chat</td>\n            <td>37B</td>\n            <td>37.8</td>\n            <td>36.2</td>\n            <td>-</td>\n            <td>27.2</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM</td>\n            <td>17B</td>\n            <td>45.2</td>\n            <td>41.1</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">VPG-C</td>\n            <td>7B</td>\n            <td>52.4</td>\n            <td>43.1</td>\n            <td>24.3</td>\n            <td>23.1</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">VILA 8B</td>\n            <td>8B</td>\n            <td>51.2</td>\n            <td>39.3</td>\n            <td>-</td>\n            <td>36.5</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternLM-XComposer-2.5</td>\n            <td>8B</td>\n            <td>53.1*</td>\n            <td>48.9</td>\n            <td>32.1*</td>\n            <td>-</td>\n            <td>42.5</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>59.0*</td>\n            <td>50.9</td>\n            <td>30.5*</td>\n            <td>34.4*</td>\n            <td><strong>56.9*</strong></td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>69.1</strong></td>\n            <td><strong>53.0</strong></td>\n            <td><strong>84.9</strong></td>\n            <td><strong>74.9</strong></td>\n            <td>53.8</td>\n        </tr>\n    </tbody>\n</table>\n\n</div>\n* We evaluate the officially released checkpoint by ourselves.\n</details>\n\n<details>\n<summary>Click to view video results on Video-MME and Video-ChatGPT.</summary>\n<div align=\"center\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th colspan=\"2\">Video-MME</th>\n            <th colspan=\"5\">Video-ChatGPT</th>\n        </tr>\n        <tr>\n            <th align=\"left\"></th>\n            <th></th>\n            <th>w/o subs</th>\n            <th>w subs</th>\n            <th>Correctness</th>\n            <th>Detail</th>\n            <th>Context</th>\n            <th>Temporal</th>\n            <th>Consistency</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"9\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Claude 3.5 Sonnet</td>\n            <td>-</td>\n            <td>60.0</td>\n            <td>62.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>59.9</td>\n            <td>63.3</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td colspan=\"9\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-7B</td>\n            <td>7B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.39</td>\n            <td>3.29</td>\n            <td>3.92</td>\n            <td>2.60</td>\n            <td>3.12</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-34B</td>\n            <td>34B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.29</td>\n            <td>3.23</td>\n            <td>3.83</td>\n            <td>2.51</td>\n            <td>3.47</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM2-Video</td>\n            <td>12B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.49</td>\n            <td><strong>3.46</strong></td>\n            <td>3.23</td>\n            <td><strong>2.98</strong></td>\n            <td><strong>3.64</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LongVA</td>\n            <td>7B</td>\n            <td>52.4</td>\n            <td>54.3</td>\n            <td>3.05</td>\n            <td>3.09</td>\n            <td>3.77</td>\n            <td>2.44</td>\n            <td><strong>3.64</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>54.0</td>\n            <td>56.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternLM-XComposer-2.5</td>\n            <td>8B</td>\n            <td>55.8</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Video</td>\n            <td>32B</td>\n            <td>60.2</td>\n            <td>63.0</td>\n            <td>3.48</td>\n            <td>3.37</td>\n            <td><strong>3.95</strong></td>\n            <td>2.64</td>\n            <td>3.28</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>60.9</strong></td>\n            <td><strong>63.6</strong></td>\n            <td><strong>3.59</strong></td>\n            <td>3.28</td>\n            <td>3.93</td>\n            <td>2.73</td>\n            <td>3.62</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n</details>\n\n\n<details>\n<summary>Click to view few-shot results on TextVQA, VizWiz, VQAv2, OK-VQA.</summary>\n<div align=\"center\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Shot</th>\n            <th>TextVQA val</th>\n            <th>VizWiz test-dev</th>\n            <th>VQAv2 test-dev</th>\n            <th>OK-VQA val</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">Flamingo</td>\n            <td rowspan=\"3\">80B</td>\n            <td>0*</td>\n            <td>35.0</td>\n            <td>31.6</td>\n            <td>56.3</td>\n            <td>40.6</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>36.5</td>\n            <td>39.6</td>\n            <td>63.1</td>\n            <td><strong>57.4</strong></td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>37.3</td>\n            <td>44.8</td>\n            <td>65.6</td>\n            <td>57.5</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">IDEFICS</td>\n            <td rowspan=\"3\">80B</td>\n            <td>0*</td>\n            <td>30.9</td>\n            <td>36.0</td>\n            <td>60.0</td>\n            <td>45.2</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>34.3</td>\n            <td>40.4</td>\n            <td>63.6</td>\n            <td>52.4</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>35.7</td>\n            <td>46.1</td>\n            <td>64.8</td>\n            <td>55.1</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">OmniCorpus</td>\n            <td rowspan=\"3\">7B</td>\n            <td>0*</td>\n            <td>43.0</td>\n            <td>49.8</td>\n            <td>63.2</td>\n            <td>45.5</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>45.4</td>\n            <td>51.3</td>\n            <td>64.5</td>\n            <td>46.5</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>45.6</td>\n            <td>52.2</td>\n            <td>64.7</td>\n            <td>46.6</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">Emu2</td>\n            <td rowspan=\"3\">37B</td>\n            <td>0</td>\n            <td>26.4</td>\n            <td>40.4</td>\n            <td>33.5</td>\n            <td>26.7</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>48.2</td>\n            <td>54.6</td>\n            <td>67.0</td>\n            <td>53.2</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>49.3</td>\n            <td>54.7</td>\n            <td>67.8</td>\n            <td>54.1</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"2\">MM1</td>\n            <td rowspan=\"2\">30B</td>\n            <td>0</td>\n            <td>26.2</td>\n            <td>40.4</td>\n            <td>48.9</td>\n            <td>26.7</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>49.3</td>\n            <td>54.7</td>\n            <td><strong>70.9</strong></td>\n            <td>54.1</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">MiniCPM-V 2.6<sup>+</sup></td>\n            <td rowspan=\"3\">8B</td>\n            <td>0</td>\n            <td>43.9</td>\n            <td>33.8</td>\n            <td>45.4</td>\n            <td>23.9</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td>4</td>\n            <td>63.6</td>\n            <td>60.5</td>\n            <td>65.5</td>\n            <td>50.1</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td>8</td>\n            <td><strong>64.6</strong></td>\n            <td><strong>63.4</strong></td>\n            <td>68.2</td>\n            <td>51.4</td>\n        </tr>\n    </tbody>\n</table>\n\n\n</div>\n* denotes zero image shot and two additional text shots following Flamingo.\n\n<sup>+</sup> We evaluate the pretraining ckpt without SFT.\n</details>\n\n### Examples <!-- omit in toc -->\n\n<div style=\"display: flex; flex-direction: column; align-items: center;\">\n  <img src=\"assets/minicpmv2_6/multi_img-bike.png\" alt=\"Bike\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multi_img-menu.png\" alt=\"Menu\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multi_img-code.png\" alt=\"Code\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/ICL-Mem.png\" alt=\"Mem\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multiling-medal.png\" alt=\"medal\" style=\"margin-bottom: 10px;\">\n</div>\n<details>\n  <summary>Click to view more cases.</summary>\n  <div style=\"display: flex; flex-direction: column; align-items: center;\">\n    <img src=\"assets/minicpmv2_6/ICL-elec.png\" alt=\"elec\" style=\"margin-bottom: 5px;\">\n    <img src=\"assets/minicpmv2_6/multiling-olympic.png\" alt=\"Menu\" style=\"margin-bottom: 10px;\">\n  </div>\n</details>\n\nWe deploy MiniCPM-V 2.6 on end devices. The demo video is the raw screen recording on a iPad Pro without edition.\n\n<table align=\"center\"> \n    <p align=\"center\">\n      <img src=\"assets/gif_cases/ai.gif\" width=32%/>\n      &nbsp;&nbsp;&nbsp;&nbsp;\n      <img src=\"assets/gif_cases/beer.gif\" width=32%/>\n    </p>\n</table> \n\n<table align=\"center\"> \n    <p align=\"center\">\n      <img src=\"assets/gif_cases/ticket.gif\" width=32%/>\n      &nbsp;&nbsp;&nbsp;&nbsp;\n      <img src=\"assets/gif_cases/wfh.gif\" width=32%/>\n    </p>\n</table> \n\n<table align=\"center\">\n    <p align=\"center\">\n      <video src=\"https://github.com/user-attachments/assets/21f4b818-ede1-4822-920e-91281725c830\" width=\"360\" /> </video>\n      <!-- <video src=\"https://github.com/user-attachments/assets/c835f757-206b-4d9c-8e36-70d67b453628\" width=\"360\" /> </video> -->\n    </p>\n</table>\n\n## MiniCPM-Llama3-V 2.5\n\n<details>\n<summary>Click to view more details of MiniCPM-Llama3-V 2.5</summary>\n\n**MiniCPM-Llama3-V 2.5** is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Llama3-8B-Instruct with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.0. Notable features of MiniCPM-Llama3-V 2.5 include:\n\n- üî• **Leading Performance.**\n  MiniCPM-Llama3-V 2.5 has achieved an average score of 65.1 on OpenCompass, a comprehensive evaluation over 11 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4V-1106, Gemini Pro, Claude 3 and Qwen-VL-Max** and greatly outperforms other Llama 3-based MLLMs.\n\n- üí™ **Strong OCR Capabilities.**\n  MiniCPM-Llama3-V 2.5 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), achieving a **700+ score on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V-0409, Qwen-VL-Max and Gemini Pro**. Based on recent user feedback, MiniCPM-Llama3-V 2.5 has now enhanced full-text OCR extraction, table-to-markdown conversion, and other high-utility capabilities, and has further strengthened its instruction-following and complex reasoning abilities, enhancing multimodal interaction experiences.\n\n- üèÜ **Trustworthy Behavior.**\n  Leveraging the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) method (the newest technique in the [RLHF-V](https://github.com/RLHF-V) [CVPR'24] series), MiniCPM-Llama3-V 2.5 exhibits more trustworthy behavior. It achieves a **10.3%** hallucination rate on Object HalBench, lower than GPT-4V-1106 (13.6%), achieving the best-level performance within the open-source community. [Data released](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset).\n\n- üåè **Multilingual Support.**\n  Thanks to the strong multilingual capabilities of Llama 3 and the cross-lingual generalization technique from [VisCPM](https://github.com/OpenBMB/VisCPM), MiniCPM-Llama3-V 2.5 extends its bilingual (Chinese-English) multimodal capabilities to **over 30 languages including German, French, Spanish, Italian, Korean etc.** [All Supported Languages](./assets/minicpm-llama-v-2-5_languages.md).\n\n- üöÄ **Efficient Deployment.**\n  MiniCPM-Llama3-V 2.5 systematically employs **model quantization, CPU optimizations, NPU optimizations and compilation optimizations**, achieving high-efficiency deployment on end-side devices. For mobile phones with Qualcomm chips, we have integrated the NPU acceleration framework QNN into llama.cpp for the first time. After systematic optimization, MiniCPM-Llama3-V 2.5 has realized a **150x acceleration in end-side MLLM image encoding** and a **3x speedup in language decoding**.\n\n-  üí´  **Easy Usage.**\nMiniCPM-Llama3-V 2.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md) and [ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5) support for efficient CPU inference on local devices, (2) [GGUF](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf) format quantized models in 16 sizes, (3) efficient [LoRA](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#lora-finetuning) fine-tuning with only 2 V100 GPUs, (4) [streaming output](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage), (5) quick local WebUI demo setup with [Gradio](https://github.com/OpenBMB/MiniCPM-V/blob/main/web_demo_2.5.py) and [Streamlit](https://github.com/OpenBMB/MiniCPM-V/blob/main/web_demo_streamlit-2_5.py), and (6) interactive demos on [HuggingFace Spaces](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5).\n\n### Evaluation  <!-- omit in toc -->\n\n<div align=\"center\">\n    <img src=assets/MiniCPM-Llama3-V-2.5-peformance.png width=66% />\n</div>\n<details>\n<summary>Click to view results on TextVQA, DocVQA, OCRBench, OpenCompass, MME, MMBench, MMMU, MathVista, LLaVA Bench, RealWorld QA, Object HalBench. </summary>\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>OCRBench</th>\n            <th>TextVQA val</th>\n            <th>DocVQA test</th>\n            <th>Open-Compass</th>\n            <th>MME</th>\n            <th>MMB test (en)</th>\n            <th>MMB test (cn)</th>\n            <th>MMMU val</th>\n            <th>Math-Vista</th>\n            <th>LLaVA Bench</th>\n            <th>RealWorld QA</th>\n            <th>Object HalBench</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"14\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini Pro</td>\n            <td>-</td>\n            <td>680</td>\n            <td>74.6</td>\n            <td>88.1</td>\n            <td>62.9</td>\n            <td>2148.9</td>\n            <td>73.6</td>\n            <td>74.3</td>\n            <td>48.9</td>\n            <td>45.8</td>\n            <td>79.9</td>\n            <td>60.4</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V (2023.11.06)</td>\n            <td>-</td>\n            <td>645</td>\n            <td>78.0</td>\n            <td>88.4</td>\n            <td>63.5</td>\n            <td>1771.5</td>\n            <td>77.0</td>\n            <td>74.4</td>\n            <td>53.8</td>\n            <td>47.8</td>\n            <td>93.1</td>\n            <td>63.0</td>\n            <td>86.4</td>\n        </tr>\n        <tr>\n            <td colspan=\"14\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Mini-Gemini</td>\n            <td>2.2B</td>\n            <td>-</td>\n            <td>56.2</td>\n            <td>34.2*</td>\n            <td>-</td>\n            <td>1653.0</td>\n            <td>-</td>\n            <td>-</td>\n            <td>31.7</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen-VL-Chat</td>\n            <td>9.6B</td>\n            <td>488</td>\n            <td>61.5</td>\n            <td>62.6</td>\n            <td>51.6</td>\n            <td>1860.0</td>\n            <td>61.8</td>\n            <td>56.3</td>\n            <td>37.0</td>\n            <td>33.8</td>\n            <td>67.7</td>\n            <td>49.3</td>\n            <td>56.2</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">DeepSeek-VL-7B</td>\n            <td>7.3B</td>\n            <td>435</td>\n            <td>64.7*</td>\n            <td>47.0*</td>\n            <td>54.6</td>\n            <td>1765.4</td>\n            <td>73.8</td>\n            <td>71.4</td>\n            <td>38.3</td>\n            <td>36.8</td>\n            <td>77.8</td>\n            <td>54.2</td>\n            <td>-</td>\n        </tr>        \n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Yi-VL-34B</td>\n            <td>34B</td>\n            <td>290</td>\n            <td>43.4*</td>\n            <td>16.9*</td>\n            <td>52.2</td>\n            <td><strong>2050.2</strong></td>\n            <td>72.4</td>\n            <td>70.7</td>\n            <td>45.1</td>\n            <td>30.7</td>\n            <td>62.3</td>\n            <td>54.8</td>\n            <td>79.3</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM-Chat</td>\n            <td>17.4B</td>\n            <td>590</td>\n            <td>70.4</td>\n            <td>33.3*</td>\n            <td>54.2</td>\n            <td>1736.6</td>\n            <td>65.8</td>\n            <td>55.9</td>\n            <td>37.3</td>\n            <td>34.7</td>\n            <td>73.9</td>\n            <td>60.3</td>\n            <td>73.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">TextMonkey</td>\n            <td>9.7B</td>\n            <td>558</td>\n            <td>64.3</td>\n            <td>66.7</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n          <td nowrap=\"nowrap\" align=\"left\">Idefics2</td>\n          <td>8.0B</td>\n          <td>-</td>\n          <td>73.0</td>\n          <td>74.0</td>\n          <td>57.2</td>\n          <td>1847.6</td>\n          <td>75.7</td>\n          <td>68.6</td>\n          <td>45.2</td>\n          <td>52.2</td>\n          <td>49.1</td>\n          <td>60.7</td>\n          <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Bunny-LLama-3-8B</td>\n            <td>8.4B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>54.3</td>\n            <td>1920.3</td>\n            <td>77.0</td>\n            <td>73.9</td>\n            <td>41.3</td>\n            <td>31.5</td>\n            <td>61.2</td>\n            <td>58.8</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT Llama-3-8B</td>\n            <td>8.4B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>78.2</td>\n            <td>-</td>\n            <td>1971.5</td>\n            <td>-</td>\n            <td>-</td>\n            <td>41.7</td>\n            <td>37.5</td>\n            <td>80.1</td>\n            <td>60.0</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Phi-3-vision-128k-instruct</td>\n            <td>4.2B</td>\n            <td>639*</td>\n            <td>70.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>1537.5*</td>\n            <td>-</td>\n            <td>-</td>\n            <td>40.4</td>\n            <td>44.5</td>\n            <td>64.2*</td>\n            <td>58.8*</td>\n            <td>-</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 1.0</td>\n            <td>2.8B</td>\n            <td>366</td>\n            <td>60.6</td>\n            <td>38.2</td>\n            <td>47.5</td>\n            <td>1650.2</td>\n            <td>64.1</td>\n            <td>62.6</td>\n            <td>38.3</td>\n            <td>28.9</td>\n            <td>51.3</td>\n            <td>51.2</td>\n            <td>78.4</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.0</td>\n            <td>2.8B</td>\n            <td>605</td>\n            <td>74.1</td>\n            <td>71.9</td>\n            <td>54.5</td>\n            <td>1808.6</td>\n            <td>69.1</td>\n            <td>66.5</td>\n            <td>38.2</td>\n            <td>38.7</td>\n            <td>69.2</td>\n            <td>55.8</td>\n            <td>85.5</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-Llama3-V 2.5</td>\n            <td>8.5B</td>\n            <td><strong>725</strong></td>\n            <td><strong>76.6</strong></td>\n            <td><strong>84.8</strong></td>\n            <td><strong>65.1</strong></td>\n            <td>2024.6</td>\n            <td><strong>77.2</strong></td>\n            <td><strong>74.2</strong></td>\n            <td><strong>45.8</strong></td>\n            <td><strong>54.3</strong></td>\n            <td><strong>86.7</strong></td>\n            <td><strong>63.5</strong></td>\n            <td><strong>89.7</strong></td>\n        </tr>\n    </tbody>\n</table>\n\n\n</div>\n* We evaluate the officially released checkpoint by ourselves.\n\n</details>\n\n<div align=\"center\">\n    <img src=\"assets/llavabench_compare_3.png\" width=\"100%\" />\n    <br>\n    Evaluation results of multilingual LLaVA Bench\n</div>\n\n### Examples <!-- omit in toc -->\n\n<table align=\"center\" >\n  <p align=\"center\" > \n  <img src=\"assets/minicpmv-llama3-v2.5/cases_all.png\" />\n  </p>\n</table>\n\n</details>\n\n\n## MiniCPM-V 2.0\n\n<details>\n<summary>Click to view more details of MiniCPM-V 2.0</summary>\n\n\n**MiniCPM-V 2.0** is an efficient version with promising performance for deployment. The model is built based on SigLip-400M and [MiniCPM-2.4B](https://github.com/OpenBMB/MiniCPM/), connected by a perceiver resampler. Our latest version, MiniCPM-V 2.0 has several notable features. \n\n- üî• **State-of-the-art Performance.** \n\n  MiniCPM-V 2.0 achieves **state-of-the-art performance** on multiple benchmarks (including OCRBench, TextVQA, MME, MMB, MathVista, etc) among models under 7B parameters. It even **outperforms strong Qwen-VL-Chat 9.6B, CogVLM-Chat 17.4B, and Yi-VL 34B on OpenCompass, a comprehensive evaluation over 11 popular benchmarks**. Notably, MiniCPM-V 2.0 shows **strong OCR capability**, achieving **comparable performance to Gemini Pro in scene-text understanding**, and **state-of-the-art performance on OCRBench** among open-source models.\n\n- üèÜ **Trustworthy Behavior.** \n\n  LMMs are known for suffering from hallucination, often generating text not factually grounded in images. MiniCPM-V 2.0 is **the first end-side LMM aligned via multimodal RLHF for trustworthy behavior** (using the recent [RLHF-V](https://rlhf-v.github.io/) [CVPR'24] series technique). This allows the model to **match GPT-4V in preventing hallucinations** on Object HalBench.\n\n- üåü **High-Resolution Images at Any Aspect Raito.**\n\n  MiniCPM-V 2.0 can accept **1.8 million pixels (e.g., 1344x1344) images at any aspect ratio**. This enables better perception of fine-grained visual information such as small objects and optical characters, which is achieved via a recent technique from [LLaVA-UHD](https://arxiv.org/pdf/2403.11703.pdf).\n\n- ‚ö°Ô∏è **High Efficiency.** \n\n  MiniCPM-V 2.0 can be **efficiently deployed on most GPU cards and personal computers**, and **even on end devices such as mobile phones**. For visual encoding, we compress the image representations into much fewer tokens via a perceiver resampler. This allows MiniCPM-V 2.0 to operate with **favorable memory cost and speed during inference even when dealing with high-resolution images**.\n\n- üôå **Bilingual Support.** \n\n  MiniCPM-V 2.0 **supports strong bilingual multimodal capabilities in both English and Chinese**. This is enabled by generalizing multimodal capabilities across languages, a technique from [VisCPM](https://arxiv.org/abs/2308.12038) [ICLR'24].\n\n### Examples <!-- omit in toc -->\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/minicpmv2-cases_2.png\" width=95%/>\n    </p>\n</table>\n\nWe deploy MiniCPM-V 2.0 on end devices. The demo video is the raw screen recording on a Xiaomi 14 Pro without edition.\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/gif_cases/station.gif\" width=36%/>\n      <img src=\"assets/gif_cases/london_car.gif\" width=36%/>\n    </p>\n</table>\n\n</details>\n\n## Legacy Models <!-- omit in toc --> \n\n| Model                | Introduction and Guidance       |\n|:----------------------|:-------------------:|\n| MiniCPM-V 1.0  | [Document](./minicpm_v1.md)   | \n| OmniLMM-12B  | [Document](./omnilmm_en.md)   |  \n\n\n## Chat with Our Demo on Gradio ü§ó\n\nWe provide online and local demos powered by Hugging Face Gradio <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts,  and other useful features.\n\n\n### Online Demo <!-- omit in toc --> \n\nClick here to try out the online demo of [MiniCPM-V 2.6](http://120.92.209.146:8887/) | [MiniCPM-Llama3-V 2.5](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5) | [MiniCPM-V 2.0](https://huggingface.co/spaces/openbmb/MiniCPM-V-2).\n\n### Local WebUI Demo <!-- omit in toc --> \n  \nYou can easily build your own local WebUI demo with Gradio using the following commands.\n  \n```shell\npip install -r requirements.txt\n```\n  \n```shell\n# For NVIDIA GPUs, run:\npython web_demo_2.6.py --device cuda\n\n```\n\n\n## Install\n\n1. Clone this repository and navigate to the source folder\n\n```bash\ngit clone https://github.com/OpenBMB/MiniCPM-V.git\ncd MiniCPM-V\n```\n\n2. Create conda environment\n\n```Shell\nconda create -n MiniCPM-V python=3.10 -y\nconda activate MiniCPM-V\n```\n\n3. Install dependencies\n\n```shell\npip install -r requirements.txt\n```\n\n## Inference\n\n\n### Model Zoo\n\n| Model           | Device | Memory    | &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Description       | Download |\n|:-----------|:--:|:-----------:|:-------------------|:---------------:|\n| MiniCPM-V 2.6| GPU | 17 GB  | The latest version, achieving state-of-the-art end-side performance for single image, multi-image and video understanding.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6) |\n| MiniCPM-V 2.6 gguf | CPU | 6 GB  | The gguf version, lower memory usage and faster inference.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6-gguf) |\n| MiniCPM-V 2.6 int4 | GPU | 7 GB  | The int4 quantized version, lower GPU memory usage.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6-int4) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6-int4) |\n| MiniCPM-Llama3-V 2.5 | GPU | 19 GB | Strong end-side multimodal performance.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5) |\n| MiniCPM-Llama3-V 2.5 gguf | CPU  | 6 GB | The gguf version, lower memory usage and faster inference.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf) &nbsp;&nbsp;[<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5-gguf) |\n| MiniCPM-Llama3-V 2.5 int4 | GPU | 8 GB | The int4 quantized version, lower GPU memory usage. |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4/) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5-int4) |\n| MiniCPM-V 2.0 | GPU | 8 GB | Light version, balance the performance the computation cost.   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2) |\n| MiniCPM-V 1.0 | GPU | 7 GB | Lightest version, achieving the fastest inference. |   [ü§ó](https://huggingface.co/openbmb/MiniCPM-V) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V) |\n\n### Multi-turn Conversation\n\nPlease refer to the following codes to run.\n\n<div align=\"center\">\n<img src=\"assets/airplane.jpeg\" width=\"500px\">\n</div>\n\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\ntorch.manual_seed(0)\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nimage = Image.open('./assets/airplane.jpeg').convert('RGB')\n\n# First round chat \nquestion = \"Tell me the model of this aircraft.\"\nmsgs = [{'role': 'user', 'content': [image, question]}]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n\n# Second round chat \n# pass history context of multi-turn conversation\nmsgs.append({\"role\": \"assistant\", \"content\": [answer]})\nmsgs.append({\"role\": \"user\", \"content\": [\"Introduce something about Airbus A380.\"]})\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n\nYou will get the following output:\n\n```\n\"The aircraft in the image is an Airbus A380, which can be identified by its large size, double-deck structure, and the distinctive shape of its wings and engines. The A380 is a wide-body aircraft known for being the world's largest passenger airliner, designed for long-haul flights. It has four engines, which are characteristic of large commercial aircraft. The registration number on the aircraft can also provide specific information about the model if looked up in an aviation database.\"\n\n\"The Airbus A380 is a double-deck, wide-body, four-engine jet airliner made by Airbus. It is the world's largest passenger airliner and is known for its long-haul capabilities. The aircraft was developed to improve efficiency and comfort for passengers traveling over long distances. It has two full-length passenger decks, which can accommodate more passengers than a typical single-aisle airplane. The A380 has been operated by airlines such as Lufthansa, Singapore Airlines, and Emirates, among others. It is widely recognized for its unique design and significant impact on the aviation industry.\"\n```\n\n#### Chat with multiple images\n<details>\n<summary> Click to view Python code running MiniCPM-V 2.6 with multiple images input. </summary>\n  \n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nimage1 = Image.open('image1.jpg').convert('RGB')\nimage2 = Image.open('image2.jpg').convert('RGB')\nquestion = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'\n\nmsgs = [{'role': 'user', 'content': [image1, image2, question]}]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n#### In-context few-shot learning\n<details>\n<summary> Click to view Python code running MiniCPM-V 2.6 with few-shot input. </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nquestion = \"production date\" \nimage1 = Image.open('example1.jpg').convert('RGB')\nanswer1 = \"2023.08.04\"\nimage2 = Image.open('example2.jpg').convert('RGB')\nanswer2 = \"2007.04.24\"\nimage_test = Image.open('test.jpg').convert('RGB')\n\nmsgs = [\n    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n    {'role': 'user', 'content': [image_test, question]}\n]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n#### Chat with video\n<details>\n<summary> Click to view Python code running MiniCPM-V 2.6 with video input. </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom decord import VideoReader, cpu    # pip install decord\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nMAX_NUM_FRAMES=64 # if cuda OOM set a smaller number\n\ndef encode_video(video_path):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n\n    vr = VideoReader(video_path, ctx=cpu(0))\n    sample_fps = round(vr.get_avg_fps() / 1)  # FPS\n    frame_idx = [i for i in range(0, len(vr), sample_fps)]\n    if len(frame_idx) > MAX_NUM_FRAMES:\n        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\n    frames = vr.get_batch(frame_idx).asnumpy()\n    frames = [Image.fromarray(v.astype('uint8')) for v in frames]\n    print('num frames:', len(frames))\n    return frames\n\nvideo_path=\"video_test.mp4\"\nframes = encode_video(video_path)\nquestion = \"Describe the video\"\nmsgs = [\n    {'role': 'user', 'content': frames + [question]}, \n]\n\n# Set decode params for video\nparams = {}\nparams[\"use_image_id\"] = False\nparams[\"max_slice_nums\"] = 2 # use 1 if cuda OOM and video resolution > 448*448\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer,\n    **params\n)\nprint(answer)\n```\n</details>\n\n\n### Inference on Multiple GPUs\nYou can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs (12 GB or 16 GB) by distributing the model's layers across multiple GPUs. Please refer to this [tutorial](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md) for detailed instructions on how to load the model and inference using multiple low VRAM GPUs.\n\n\n### Inference on Mac\n<details>\n<summary>Click to view an example, to run MiniCPM-Llama3-V 2.5 on üíª Mac with MPS (Apple silicon or AMD GPUs). </summary>\n\n```python\n# test.py  Need more than 16GB memory.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, low_cpu_mem_usage=True)\nmodel = model.to(device='mps')\n\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\nmodel.eval()\n\nimage = Image.open('./assets/hk_OCR.jpg').convert('RGB')\nquestion = 'Where is this photo taken?'\nmsgs = [{'role': 'user', 'content': question}]\n\nanswer, context, _ = model.chat(\n    image=image,\n    msgs=msgs,\n    context=None,\n    tokenizer=tokenizer,\n    sampling=True\n)\nprint(answer)\n```\nRun with command:\n```shell\nPYTORCH_ENABLE_MPS_FALLBACK=1 python test.py\n```\n</details>\n\n### Deployment on Mobile Phone\nMiniCPM-V 2.0 can be deployed on mobile phones with Android operating systems. üöÄ Click [MiniCPM-V 2.0](https://github.com/OpenBMB/mlc-MiniCPM) to install apk.\n\n### Inference with llama.cpp\nMiniCPM-V 2.6 can run with llama.cpp now! See [our fork of llama.cpp](https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md) for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).\n\n### Inference with ollama\nMiniCPM-V 2.6 can run with ollama now! See [our fork of ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md) for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).\n\n### Inference with vLLM\n\n<details>\n<summary> vLLM now officially supports MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0, Click to see. </summary>\n\n1. Install vLLM(>=0.5.4):\n```shell\npip install vllm\n```\n2. Install timm: (optional, MiniCPM-V 2.0 need timm)\n```shell\npip install timm==0.9.10\n```\n3. Run the example(for image):\n```python\nfrom transformers import AutoTokenizer\nfrom PIL import Image\nfrom vllm import LLM, SamplingParams\n\nMODEL_NAME = \"openbmb/MiniCPM-V-2_6\"\n# Also available for previous models\n# MODEL_NAME = \"openbmb/MiniCPM-Llama3-V-2_5\"\n# MODEL_NAME = \"HwwwH/MiniCPM-V-2\"\n\nimage = Image.open(\"xxx.png\").convert(\"RGB\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\nllm = LLM(\n    model=MODEL_NAME,\n    trust_remote_code=True,\n    gpu_memory_utilization=1,\n    max_model_len=2048\n)\n\nmessages = [{\n    \"role\":\n    \"user\",\n    \"content\":\n    # Number of images\n    \"(<image>./</image>)\" + \\\n    \"\\nWhat is the content of this image?\" \n}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Single Inference\ninputs = {\n    \"prompt\": prompt,\n    \"multi_modal_data\": {\n        \"image\": image\n        # Multi images, the number of images should be equal to that of `(<image>./</image>)`\n        # \"image\": [image, image] \n    },\n}\n# Batch Inference\n# inputs = [{\n#     \"prompt\": prompt,\n#     \"multi_modal_data\": {\n#         \"image\": image\n#     },\n# } for _ in 2]\n\n\n# 2.6\nstop_tokens = ['<|im_end|>', '<|endoftext|>']\nstop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n# 2.0\n# stop_token_ids = [tokenizer.eos_id]\n# 2.5\n# stop_token_ids = [tokenizer.eos_id, tokenizer.eot_id]\n\nsampling_params = SamplingParams(\n    stop_token_ids=stop_token_ids, \n    use_beam_search=True,\n    temperature=0, \n    best_of=3,\n    max_tokens=1024\n)\n\noutputs = llm.generate(inputs, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n```\n4. click [here](https://modelbest.feishu.cn/wiki/C2BWw4ZP0iCDy7kkCPCcX2BHnOf?from=from_copylink) if you want to use it with *video*, or get more details about `vLLM`.\n</details>\n\n## Fine-tuning\n\n### Simple Fine-tuning <!-- omit in toc -->\n\nWe support simple fine-tuning with Hugging Face for MiniCPM-V 2.0 and MiniCPM-Llama3-V 2.5.\n\n[Reference Document](./finetune/readme.md)\n\n### With the SWIFT Framework <!-- omit in toc -->\n\nWe now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.\n\nBest PracticesÔºö[MiniCPM-V 1.0](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-vÊúÄ‰Ω≥ÂÆûË∑µ.md), [MiniCPM-V 2.0](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2ÊúÄ‰Ω≥ÂÆûË∑µ.md), [MiniCPM-V 2.6](https://github.com/modelscope/ms-swift/issues/1613).\n\n## FAQs\nClick here to view the [FAQs](./docs/faqs.md)\n\n## Model License <!-- omit in toc -->\n\n* This repository is released under the [Apache-2.0](https://github.com/OpenBMB/MiniCPM/blob/main/LICENSE) License. \n\n* The usage of MiniCPM-V model weights must strictly follow [MiniCPM Model License.md](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md).\n\n* The models and weights of MiniCPM are completely free for academic research. after filling out a [\"questionnaire\"](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g) for registration, are also available for free commercial use.\n  \n\n## Statement <!-- omit in toc -->\n\nAs LMMs, MiniCPM-V models (including OmniLMM) generate contents by learning a large amount of multimodal corpora, but they cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V models does not represent the views and positions of the model developers\n\nWe will not be liable for any problems arising from the use of MiniCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\n\n\n## Institutions  <!-- omit in toc -->\n\nThis project is developed by the following institutions:\n\n- <img src=\"assets/thunlp.png\" width=\"28px\"> [THUNLP](https://nlp.csai.tsinghua.edu.cn/)\n- <img src=\"assets/modelbest.png\" width=\"28px\"> [ModelBest](https://modelbest.cn/)\n- <img src=\"assets/zhihu.webp\" width=\"28px\"> [Zhihu](https://www.zhihu.com/ )\n\n## üåü Star History <!-- omit in toc -->\n\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/star_history.svg\"/>\n    </p>\n</table>\n\n<!-- <picture>\n  <source\n    media=\"(prefers-color-scheme: dark)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date&theme=dark\n    \"\n  />\n  <source\n    media=\"(prefers-color-scheme: light)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date\n    \"\n  />\n  <img\n    alt=\"Star History Chart\"\n    src=\"https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date\"\n  />\n</picture> -->\n\n## Key Techniques and Other Multimodal Projects <!-- omit in toc -->\n\nüëè Welcome to explore key techniques of MiniCPM-V and other multimodal projects of our team:\n\n[VisCPM](https://github.com/OpenBMB/VisCPM/tree/main) | [RLHF-V](https://github.com/RLHF-V/RLHF-V) | [LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD) | [RLAIF-V](https://github.com/RLHF-V/RLAIF-V)\n\n\n## Citation <!-- omit in toc -->\n\nIf you find our model/code/paper helpful, please consider cite our papers üìù and star us ‚≠êÔ∏èÔºÅ\n\n```bib\n@article{yao2024minicpm,\n  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\n  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\n  journal={arXiv preprint arXiv:2408.01800},\n  year={2024}\n}\n```\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 62.93359375,
          "content": "<div align=\"center\">\n\n<!-- <!-- <h1 style=\"color: #33A6B8; font-family: Helvetica\"> OmniLMM </h1> -->\n\n<img src=\"./assets/minicpmv.png\" width=\"300em\" ></img> \n\n**Á´Ø‰æßÂèØÁî®ÁöÑ GPT-4V Á∫ßÂçïÂõæ„ÄÅÂ§öÂõæ„ÄÅËßÜÈ¢ëÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã**\n\n  <strong>‰∏≠Êñá |\n  [English](./README_en.md)</strong>\n\n Âä†ÂÖ•Êàë‰ª¨ÁöÑ <a href=\"docs/wechat.md\" target=\"_blank\"> üí¨ ÂæÆ‰ø°Á§æÂå∫</a>\nÔΩú ‰∫ÜËß£ MiniCPM-V <a href=\"docs/best_practice_summary_zh.md\" target=\"_blank\"> üìñ ÊúÄ‰Ω≥ÂÆûË∑µ</a>\n  \n\n<p align=\"center\">\n  MiniCPM-V 2.6 <a href=\"https://huggingface.co/openbmb/MiniCPM-V-2_6\">ü§ó</a> <a href=\"http://120.92.209.146:8887/\">ü§ñ</a> | MiniCPM-Llama3-V 2.5  <a href=\"https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/\">ü§ó</a> <a href=\"https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5\">ü§ñ</a> |\n  <a href=https://arxiv.org/abs/2408.01800>MiniCPM-Llama3-V 2.5 ÊäÄÊúØÊä•Âëä</a> \n</p>\n\n\n</div>\n\n\n**MiniCPM-V**ÊòØÈù¢ÂêëÂõæÊñáÁêÜËß£ÁöÑÁ´Ø‰æßÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁ≥ªÂàó„ÄÇËØ•Á≥ªÂàóÊ®°ÂûãÊé•ÂèóÂõæÂÉèÂíåÊñáÊú¨ËæìÂÖ•ÔºåÂπ∂Êèê‰æõÈ´òË¥®ÈáèÁöÑÊñáÊú¨ËæìÂá∫„ÄÇËá™2024Âπ¥2Êúà‰ª•Êù•ÔºåÊàë‰ª¨ÂÖ±ÂèëÂ∏É‰∫Ü5‰∏™ÁâàÊú¨Ê®°ÂûãÔºåÊó®Âú®ÂÆûÁé∞**È¢ÜÂÖàÁöÑÊÄßËÉΩÂíåÈ´òÊïàÁöÑÈÉ®ÁΩ≤**ÔºåÁõÆÂâçËØ•Á≥ªÂàóÊúÄÂÄºÂæóÂÖ≥Ê≥®ÁöÑÊ®°ÂûãÂåÖÊã¨Ôºö\n\n\n- **MiniCPM-V 2.6**: üî•üî•üî• MiniCPM-VÁ≥ªÂàóÁöÑÊúÄÊñ∞„ÄÅÊÄßËÉΩÊúÄ‰Ω≥Ê®°Âûã„ÄÇÊÄªÂèÇÊï∞Èáè 8BÔºåÂçïÂõæ„ÄÅÂ§öÂõæÂíåËßÜÈ¢ëÁêÜËß£ÊÄßËÉΩ**Ë∂ÖË∂ä‰∫Ü GPT-4V**„ÄÇÂú®ÂçïÂõæÁêÜËß£‰∏äÔºåÂÆÉÂèñÂæó‰∫Ü‰ºò‰∫é **GPT-4o mini„ÄÅGemini 1.5 Pro Âíå Claude 3.5 Sonnet**Á≠âÂïÜÁî®Èó≠Ê∫êÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂπ∂Ëøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫Ü MiniCPM-Llama3-V 2.5 ÁöÑ OCR„ÄÅÂèØ‰ø°Ë°å‰∏∫„ÄÅÂ§öËØ≠Ë®ÄÊîØÊåÅ‰ª•ÂèäÁ´Ø‰æßÈÉ®ÁΩ≤Á≠âËØ∏Â§öÁâπÊÄß„ÄÇÂü∫‰∫éÂÖ∂È¢ÜÂÖàÁöÑËßÜËßâ token ÂØÜÂ∫¶ÔºåMiniCPM-V 2.6 Êàê‰∏∫‰∫ÜÈ¶ñ‰∏™ÊîØÊåÅÂú® iPad Á≠âÁ´Ø‰æßËÆæÂ§á‰∏äËøõË°åÂÆûÊó∂ËßÜÈ¢ëÁêÜËß£ÁöÑÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã„ÄÇ\n\n- **MiniCPM-V 2.0**ÔºöMiniCPM-VÁ≥ªÂàóÁöÑÊúÄËΩªÈáèÁ∫ßÊ®°Âûã„ÄÇÊÄªÂèÇÊï∞Èáè2BÔºåÂ§öÊ®°ÊÄÅÁªºÂêàÊÄßËÉΩË∂ÖË∂ä Yi-VL 34B„ÄÅCogVLM-Chat 17B„ÄÅQwen-VL-Chat 10B Á≠âÊõ¥Â§ßÂèÇÊï∞ËßÑÊ®°ÁöÑÊ®°ÂûãÔºåÂèØÊé•Âèó 180 ‰∏áÂÉèÁ¥†ÁöÑ‰ªªÊÑèÈïøÂÆΩÊØîÂõæÂÉèËæìÂÖ•ÔºåÂÆûÁé∞‰∫ÜÂíå Gemini Pro Áõ∏ËøëÁöÑÂú∫ÊôØÊñáÂ≠óËØÜÂà´ËÉΩÂäõ‰ª•ÂèäÂíå GPT-4V Áõ∏ÂåπÁöÑ‰ΩéÂπªËßâÁéá„ÄÇ\n\n\n\n## Êõ¥Êñ∞Êó•Âøó <!-- omit in toc -->\n\n#### üìå ÁΩÆÈ°∂\n\n\n* [2024.08.17] üöÄüöÄüöÄ llama.cpp [ÂÆòÊñπ‰ªìÂ∫ì](https://github.com/ggerganov/llama.cpp)Ê≠£ÂºèÊîØÊåÅ MiniCPM-V 2.6 Âï¶ÔºÅÁÇπÂáª[ËøôÈáå](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf)Êü•ÁúãÂêÑÁßçÂ§ßÂ∞èÁöÑ GGUF ÁâàÊú¨„ÄÇ\n* [2024.08.15] MiniCPM-V 2.6 Áé∞Âú®ÊîØÊåÅÂ§öÂõæÂÉè SFT„ÄÇÊúâÂÖ≥Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖ[ÂæÆË∞ÉÊñáÊ°£](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune)\n* [2024.08.14] MiniCPM-V 2.6 Áé∞Âú®ÂèØ‰ª•ÈÄöËøá SWIFT Ê°ÜÊû∂ [ÂæÆË∞É](https://github.com/modelscope/ms-swift/issues/1613) ‰∫ÜÔºÅ\n* [2024.08.10] üöÄüöÄüöÄ llama.cpp [ÂÆòÊñπ‰ªìÂ∫ì](https://github.com/ggerganov/llama.cpp)Ê≠£ÂºèÊîØÊåÅ MiniCPM-Llama3-V 2.5 Âï¶ÔºÅÁÇπÂáª[ËøôÈáå](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main)Êü•ÁúãÂêÑÁßçÂ§ßÂ∞èÁöÑ GGUF ÁâàÊú¨„ÄÇ\n* [2024.08.06] üî•üî•üî• Êàë‰ª¨ÂºÄÊ∫ê‰∫Ü MiniCPM-V 2.6ÔºåËØ•Ê®°ÂûãÂú®ÂçïÂõæ„ÄÅÂ§öÂõæÂíåËßÜÈ¢ëÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫Ü‰ºò‰∫é GPT-4V ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ËøòËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫Ü MiniCPM-Llama3-V 2.5 ÁöÑÂ§öÈ°π‰∫ÆÁÇπËÉΩÂäõÔºåÂπ∂È¶ñÊ¨°ÊîØÊåÅ‰∫Ü iPad ‰∏äÁöÑÂÆûÊó∂ËßÜÈ¢ëÁêÜËß£„ÄÇÊ¨¢ËøéËØïÁî®ÔºÅ\n* [2024.08.03] MiniCPM-Llama3-V 2.5 ÊäÄÊúØÊä•ÂëäÂ∑≤ÂèëÂ∏ÉÔºÅÊ¨¢ËøéÁÇπÂáª[ËøôÈáå](https://arxiv.org/abs/2408.01800)Êü•Áúã„ÄÇ\n* [2024.07.19] MiniCPM-Llama3-V 2.5 Áé∞Â∑≤ÊîØÊåÅ[vLLM](#vllm-ÈÉ®ÁΩ≤-) ÔºÅ\n* [2024.05.28] üí´ Êàë‰ª¨Áé∞Âú®ÊîØÊåÅ MiniCPM-Llama3-V 2.5 ÁöÑ LoRA ÂæÆË∞ÉÔºåÊõ¥Â§öÂÜÖÂ≠ò‰ΩøÁî®ÁªüËÆ°‰ø°ÊÅØÂèØ‰ª•Âú®[ËøôÈáå](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics)ÊâæÂà∞„ÄÇ\n* [2024.05.23] üîç Êàë‰ª¨Ê∑ªÂä†‰∫ÜPhi-3-vision-128k-instruct ‰∏é MiniCPM-Llama3-V 2.5ÁöÑÂÖ®Èù¢ÂØπÊØîÔºåÂåÖÊã¨Âü∫ÂáÜÊµãËØïËØÑ‰º∞„ÄÅÂ§öËØ≠Ë®ÄËÉΩÂäõÂíåÊé®ÁêÜÊïàÁéá üåüüìäüåçüöÄ„ÄÇÁÇπÂáª[ËøôÈáå](./docs/compare_with_phi-3_vision.md)Êü•ÁúãËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ\n* [2024.05.23] üî•üî•üî• MiniCPM-V Âú® GitHub Trending Âíå Hugging Face Trending ‰∏äÁôªÈ°∂ÔºÅMiniCPM-Llama3-V 2.5 Demo Ë¢´ Hugging Face ÁöÑ Gradio ÂÆòÊñπË¥¶Êà∑Êé®ËçêÔºåÊ¨¢ËøéÁÇπÂáª[ËøôÈáå](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5)‰ΩìÈ™åÔºÅ\n\n\n<br>\n\n<details> \n<summary>ÁÇπÂáªÊü•ÁúãÂÆåÊï¥Êõ¥Êñ∞Êó•Âøó„ÄÇ</summary>\n\n* [2024.06.03] Áé∞Âú®Ôºå‰Ω†ÂèØ‰ª•Âà©Áî®Â§öÂº†‰ΩéÊòæÂ≠òÊòæÂç°Ôºà12G/16GÔºâËøõË°åGPU‰∏≤Ë°åÊé®ÁêÜ„ÄÇËØ¶ÊÉÖËØ∑ÂèÇËßÅËØ•[ÊñáÊ°£](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md)ÈÖçÁΩÆ„ÄÇ\n* [2024.05.28] üí• MiniCPM-Llama3-V 2.5 Áé∞Âú®Âú® llama.cpp Âíå ollama ‰∏≠ÂÆåÂÖ®ÊîØÊåÅÂÖ∂ÂäüËÉΩÔºÅ**ËØ∑ÊãâÂèñÊàë‰ª¨ÊúÄÊñ∞ÁöÑ fork Êù•‰ΩøÁî®**Ôºö[llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md) & [ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5)„ÄÇÊàë‰ª¨ËøòÂèëÂ∏É‰∫ÜÂêÑÁßçÂ§ßÂ∞èÁöÑ GGUF ÁâàÊú¨ÔºåËØ∑ÁÇπÂáª[ËøôÈáå](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main)Êü•Áúã„ÄÇËØ∑Ê≥®ÊÑèÔºå**ÁõÆÂâçÂÆòÊñπ‰ªìÂ∫ìÂ∞öÊú™ÊîØÊåÅ MiniCPM-Llama3-V 2.5**ÔºåÊàë‰ª¨‰πüÊ≠£ÁßØÊûÅÊé®ËøõÂ∞ÜËøô‰∫õÂäüËÉΩÂêàÂπ∂Âà∞ llama.cpp & ollama ÂÆòÊñπ‰ªìÂ∫ìÔºåÊï¨ËØ∑ÂÖ≥Ê≥®ÔºÅ\n* [2024.05.25] MiniCPM-Llama3-V 2.5 [ÊîØÊåÅÊµÅÂºèËæìÂá∫ÂíåËá™ÂÆö‰πâÁ≥ªÁªüÊèêÁ§∫ËØç](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage)‰∫ÜÔºåÊ¨¢ËøéËØïÁî®!\n* [2024.05.24] Êàë‰ª¨ÂºÄÊ∫ê‰∫Ü MiniCPM-Llama3-V 2.5 [gguf](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf)ÔºåÊîØÊåÅ [llama.cpp](#llamacpp-ÈÉ®ÁΩ≤) Êé®ÁêÜÔºÅÂÆûÁé∞Á´Ø‰æß 6-8 tokens/s ÁöÑÊµÅÁïÖËß£Á†ÅÔºåÊ¨¢ËøéËØïÁî®ÔºÅ\n* [2024.05.20] Êàë‰ª¨ÂºÄÊ∫ê‰∫Ü MiniCPM-Llama3-V 2.5ÔºåÂ¢ûÂº∫‰∫Ü OCR ËÉΩÂäõÔºåÊîØÊåÅ 30 Â§öÁßçËØ≠Ë®ÄÔºåÂπ∂È¶ñÊ¨°Âú®Á´Ø‰æßÂÆûÁé∞‰∫Ü GPT-4V Á∫ßÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõÔºÅÊàë‰ª¨Êèê‰æõ‰∫Ü[È´òÊïàÊé®ÁêÜ](#ÊâãÊú∫Á´ØÈÉ®ÁΩ≤)Âíå[ÁÆÄÊòìÂæÆË∞É](./finetune/readme.md)ÁöÑÊîØÊåÅÔºåÊ¨¢ËøéËØïÁî®ÔºÅ\n* [2024.04.23] Êàë‰ª¨Â¢ûÂä†‰∫ÜMiniCPM-V 2.0ÂØπ [vLLM](#vllm-ÈÉ®ÁΩ≤-) ÁöÑÊîØÊåÅÔºåÊ¨¢Ëøé‰ΩìÈ™åÔºÅ\n* [2024.04.18] Êàë‰ª¨Âú® HuggingFace Space Êñ∞Â¢û‰∫Ü MiniCPM-V 2.0 ÁöÑ [demo](https://huggingface.co/spaces/openbmb/MiniCPM-V-2)ÔºåÊ¨¢Ëøé‰ΩìÈ™åÔºÅ\n* [2024.04.17] MiniCPM-V 2.0 Áé∞Âú®ÊîØÊåÅÁî®Êà∑ÈÉ®ÁΩ≤Êú¨Âú∞ [WebUI Demo](#Êú¨Âú∞webui-demoÈÉ®ÁΩ≤) ‰∫ÜÔºåÊ¨¢ËøéËØïÁî®!\n* [2024.04.15] MiniCPM-V 2.0 Áé∞Âú®ÂèØ‰ª•ÈÄöËøá SWIFT Ê°ÜÊû∂ [ÂæÆË∞É](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2ÊúÄ‰Ω≥ÂÆûË∑µ.md) ‰∫ÜÔºåÊîØÊåÅÊµÅÂºèËæìÂá∫!\n* [2024.04.12] Êàë‰ª¨ÂºÄÊ∫ê‰∫Ü MiniCPM-V 2.0ÔºåËØ•Ê®°ÂûãÂà∑Êñ∞‰∫Ü OCRBench ÂºÄÊ∫êÊ®°ÂûãÊúÄ‰Ω≥ÊàêÁª©ÔºåÂú®Âú∫ÊôØÊñáÂ≠óËØÜÂà´ËÉΩÂäõ‰∏äÊØîËÇ© Gemini ProÔºåÂêåÊó∂ËøòÂú®ÁªºÂêà‰∫Ü 11 ‰∏™‰∏ªÊµÅÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãËØÑÊµãÂü∫ÂáÜÁöÑ <a href=\"https://rank.opencompass.org.cn/leaderboard-multimodal\">OpenCompass</a> Ê¶úÂçï‰∏äË∂ÖËøá‰∫Ü Qwen-VL-Chat 10B„ÄÅCogVLM-Chat 17B Âíå Yi-VL 34B Á≠âÊõ¥Â§ßÂèÇÊï∞ËßÑÊ®°ÁöÑÊ®°ÂûãÔºÅÁÇπÂáª<a href=\"https://openbmb.vercel.app/minicpm-v-2\">ËøôÈáå</a>Êü•Áúã MiniCPM-V 2.0 ÊäÄÊúØÂçöÂÆ¢„ÄÇ\n* [2024.03.14] MiniCPM-V Áé∞Âú®ÊîØÊåÅ SWIFT Ê°ÜÊû∂‰∏ãÁöÑ[ÂæÆË∞É](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-vÊúÄ‰Ω≥ÂÆûË∑µ.md)‰∫ÜÔºåÊÑüË∞¢ [Jintao](https://github.com/Jintao-Huang) ÁöÑË¥°ÁåÆÔºÅ\n* [2024.03.01] MiniCPM-V Áé∞Âú®ÊîØÊåÅÂú® Mac ÁîµËÑë‰∏äËøõË°åÈÉ®ÁΩ≤ÔºÅ\n* [2024.02.01] Êàë‰ª¨ÂºÄÊ∫ê‰∫Ü MiniCPM-V Âíå OmniLMM-12BÔºåÂàÜÂà´ÂèØ‰ª•ÊîØÊåÅÈ´òÊïàÁöÑÁ´Ø‰æßÈÉ®ÁΩ≤ÂíåÂêåËßÑÊ®°È¢ÜÂÖàÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõÔºÅ\n</details>\n\n\n## ÁõÆÂΩï <!-- omit in toc -->\n\n- [MiniCPM-V 2.6](#minicpm-v-26)\n- [MiniCPM-Llama3-V 2.5](#minicpm-llama3-v-25)\n- [MiniCPM-V 2.0](#minicpm-v-20)\n- [Gradio Demo ü§ó](#gradio-demo-)\n- [ÂÆâË£Ö](#ÂÆâË£Ö)\n- [Êé®ÁêÜ](#Êé®ÁêÜ)\n  - [Ê®°ÂûãÂ∫ì](#Ê®°ÂûãÂ∫ì)\n  - [Â§öËΩÆÂØπËØù](#Â§öËΩÆÂØπËØù)\n    - [Â§öÂõæÁêÜËß£](#Â§öÂõæÁêÜËß£)\n    - [Â∞ëÊ†∑Êú¨‰∏ä‰∏ãÊñáÂ≠¶‰π†](#Â∞ëÊ†∑Êú¨‰∏ä‰∏ãÊñáÂ≠¶‰π†)\n    - [ËßÜÈ¢ëÁêÜËß£](#ËßÜÈ¢ëÁêÜËß£)\n  - [Â§öÂç°Êé®ÁêÜ](#Â§öÂç°Êé®ÁêÜ)\n  - [Mac Êé®ÁêÜ](#mac-Êé®ÁêÜ)\n  - [ÊâãÊú∫Á´ØÈÉ®ÁΩ≤](#ÊâãÊú∫Á´ØÈÉ®ÁΩ≤)\n  - [Êú¨Âú∞WebUI DemoÈÉ®ÁΩ≤](#Êú¨Âú∞webui-demoÈÉ®ÁΩ≤)\n  - [llama.cpp ÈÉ®ÁΩ≤](#llamacpp-ÈÉ®ÁΩ≤)\n  - [ollama ÈÉ®ÁΩ≤](#ollama-ÈÉ®ÁΩ≤)\n  - [vLLM ÈÉ®ÁΩ≤ ](#vllm-ÈÉ®ÁΩ≤-)\n- [ÂæÆË∞É](#ÂæÆË∞É)\n- [FAQs](#faqs)\n\n## MiniCPM-V 2.6\n\n**MiniCPM-V 2.6** ÊòØ MiniCPM-V Á≥ªÂàó‰∏≠ÊúÄÊñ∞„ÄÅÊÄßËÉΩÊúÄ‰Ω≥ÁöÑÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÂü∫‰∫é SigLip-400M Âíå Qwen2-7B ÊûÑÂª∫ÔºåÂÖ± 8B ÂèÇÊï∞„ÄÇ‰∏é MiniCPM-Llama3-V 2.5 Áõ∏ÊØîÔºåMiniCPM-V 2.6 ÊÄßËÉΩÊèêÂçáÊòæËëóÔºåÂπ∂ÂºïÂÖ•‰∫ÜÂ§öÂõæÂíåËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞ÂäüËÉΩ„ÄÇMiniCPM-V 2.6 ÁöÑ‰∏ªË¶ÅÁâπÁÇπÂåÖÊã¨Ôºö\n\n\n- üî• **È¢ÜÂÖàÁöÑÊÄßËÉΩ„ÄÇ**\n  MiniCPM-V 2.6 Âú®ÊúÄÊñ∞ÁâàÊú¨ OpenCompass Ê¶úÂçï‰∏äÔºàÁªºÂêà 8 ‰∏™‰∏ªÊµÅÂ§öÊ®°ÊÄÅËØÑÊµãÂü∫ÂáÜÔºâÂπ≥ÂùáÂæóÂàÜ 65.2Ôºå**‰ª•8BÈáèÁ∫ßÁöÑÂ§ßÂ∞èÂú®ÂçïÂõæÁêÜËß£ÊñπÈù¢Ë∂ÖË∂ä‰∫Ü GPT-4o mini„ÄÅGPT-4V„ÄÅGemini 1.5 Pro Âíå Claude 3.5 Sonnet Á≠â‰∏ªÊµÅÂïÜÁî®Èó≠Ê∫êÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã**„ÄÇ\n\n- üñºÔ∏è **Â§öÂõæÁêÜËß£Âíå‰∏ä‰∏ãÊñáÂ≠¶‰π†„ÄÇ**\n  MiniCPM-V 2.6 ËøòÊîØÊåÅ**Â§öÂõæÂØπËØùÂíåÊé®ÁêÜ**„ÄÇÂÆÉÂú® Mantis-Eval„ÄÅBLINK„ÄÅMathverse mv Âíå Sciverse mv Á≠â‰∏ªÊµÅÂ§öÂõæËØÑÊµãÂü∫ÂáÜ‰∏≠ÂèñÂæó‰∫Ü**ÊúÄ‰Ω≥Ê∞¥Âπ≥**ÔºåÂπ∂Â±ïÁé∞Âá∫‰∫Ü‰ºòÁßÄÁöÑ‰∏ä‰∏ãÊñáÂ≠¶‰π†ËÉΩÂäõ„ÄÇ\n\n- üé¨ **ËßÜÈ¢ëÁêÜËß£„ÄÇ**\n  MiniCPM-V 2.6 ËøòÂèØ‰ª•**Êé•ÂèóËßÜÈ¢ëËæìÂÖ•**ÔºåËøõË°åÂØπËØùÂíåÊèê‰æõÊ∂µÁõñÊó∂Â∫èÂíåÁ©∫Èó¥‰ø°ÊÅØÁöÑËØ¶ÁªÜËßÜÈ¢ëÊèèËø∞„ÄÇÊ®°ÂûãÂú® Êúâ/Êó†Â≠óÂπï ËØÑÊµãÂú∫ÊôØ‰∏ãÁöÑ Video-MME Ë°®Áé∞ÂùáË∂ÖËøá‰∫Ü **GPT-4V„ÄÅClaude 3.5 Sonnet Âíå LLaVA-NeXT-Video-34B**Á≠âÂïÜÁî®Èó≠Ê∫êÊ®°Âûã„ÄÇ\n\n- üí™ **Âº∫Â§ßÁöÑ OCR ËÉΩÂäõÂèäÂÖ∂‰ªñÂäüËÉΩ„ÄÇ**\n  MiniCPM-V 2.6 ÂèØ‰ª•Â§ÑÁêÜ‰ªªÊÑèÈïøÂÆΩÊØîÁöÑÂõæÂÉèÔºåÂÉèÁ¥†Êï∞ÂèØËææ 180 ‰∏áÔºàÂ¶Ç 1344x1344Ôºâ„ÄÇÂú® OCRBench ‰∏äÂèñÂæó**ÊúÄ‰Ω≥Ê∞¥Âπ≥ÔºåË∂ÖËøá GPT-4o„ÄÅGPT-4V Âíå Gemini 1.5 Pro Á≠âÂïÜÁî®Èó≠Ê∫êÊ®°Âûã**„ÄÇÂü∫‰∫éÊúÄÊñ∞ÁöÑ [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) Âíå [VisCPM](https://github.com/OpenBMB/VisCPM) ÊäÄÊúØÔºåÂÖ∂ÂÖ∑Â§á‰∫Ü**ÂèØ‰ø°ÁöÑÂ§öÊ®°ÊÄÅË°å‰∏∫**ÔºåÂú® Object HalBench ‰∏äÁöÑÂπªËßâÁéáÊòæËëó‰Ωé‰∫é GPT-4o Âíå GPT-4VÔºåÂπ∂ÊîØÊåÅËã±ËØ≠„ÄÅ‰∏≠Êñá„ÄÅÂæ∑ËØ≠„ÄÅÊ≥ïËØ≠„ÄÅÊÑèÂ§ßÂà©ËØ≠„ÄÅÈü©ËØ≠Á≠â**Â§öÁßçËØ≠Ë®Ä**„ÄÇ\n\n- üöÄ **ÂçìË∂äÁöÑÊïàÁéá„ÄÇ**\n  Èô§‰∫ÜÂØπ‰∏™‰∫∫Áî®Êà∑ÂèãÂ•ΩÁöÑÊ®°ÂûãÂ§ßÂ∞èÔºåMiniCPM-V 2.6 ËøòË°®Áé∞Âá∫**ÊúÄÂÖàËøõÁöÑËßÜËßâ token ÂØÜÂ∫¶**ÔºàÂç≥ÊØè‰∏™ËßÜËßâ token ÁºñÁ†ÅÁöÑÂÉèÁ¥†Êï∞ÈáèÔºâ„ÄÇÂÆÉ**‰ªÖÈúÄ 640 ‰∏™ token Âç≥ÂèØÂ§ÑÁêÜ 180 ‰∏áÂÉèÁ¥†ÂõæÂÉèÔºåÊØîÂ§ßÂ§öÊï∞Ê®°ÂûãÂ∞ë 75%**„ÄÇËøô‰∏ÄÁâπÊÄß‰ºòÂåñ‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÅÈ¶ñ token Âª∂Ëøü„ÄÅÂÜÖÂ≠òÂç†Áî®ÂíåÂäüËÄó„ÄÇÂõ†Ê≠§ÔºåMiniCPM-V 2.6 ÂèØ‰ª•ÊîØÊåÅ iPad Á≠âÁªàÁ´ØËÆæÂ§á‰∏äÁöÑÈ´òÊïà**ÂÆûÊó∂ËßÜÈ¢ëÁêÜËß£**„ÄÇ\n\n- üí´ **Êòì‰∫é‰ΩøÁî®„ÄÇ**\n  MiniCPM-V 2.6 ÂèØ‰ª•ÈÄöËøáÂ§öÁßçÊñπÂºèËΩªÊùæ‰ΩøÁî®Ôºö(1) [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpmv-main/examples/llava/README-minicpmv2.6.md) Âíå [ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md) ÊîØÊåÅÂú®Êú¨Âú∞ËÆæÂ§á‰∏äËøõË°åÈ´òÊïàÁöÑ CPU Êé®ÁêÜÔºå(2) [int4](https://huggingface.co/openbmb/MiniCPM-V-2_6-int4) Âíå [GGUF](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf) Ê†ºÂºèÁöÑÈáèÂåñÊ®°ÂûãÔºåÊúâ 16 ÁßçÂ∞∫ÂØ∏Ôºå(3) [vLLM](#vllm-ÈÉ®ÁΩ≤-) ÊîØÊåÅÈ´òÂêûÂêêÈáèÂíåÂÜÖÂ≠òÈ´òÊïàÁöÑÊé®ÁêÜÔºå(4) ÈíàÂØπÊñ∞È¢ÜÂüüÂíå‰ªªÂä°ËøõË°åÂæÆË∞ÉÔºå(5) ‰ΩøÁî® [Gradio](#Êú¨Âú∞-webui-demo-) Âø´ÈÄüËÆæÁΩÆÊú¨Âú∞ WebUI ÊºîÁ§∫Ôºå(6) Âú®Á∫ø[demo](http://120.92.209.146:8887/)Âç≥ÂèØ‰ΩìÈ™å„ÄÇ\n\n### ÊÄßËÉΩËØÑ‰º∞  <!-- omit in toc -->\n<div align=\"center\">\n    <img src=assets/radar_final.png width=66% />\n</div>\n\n<details>\n<summary>ÁÇπÂáªÊü•Áúã OpenCompass, MME, MMVet, OCRBench, MMMU, MathVista, MMB, AI2D, TextVQA, DocVQA, HallusionBench, Object HalBench ‰∏äÁöÑÂçïÂõæËØÑÊµãÁªìÊûúËØ¶ÊÉÖ„ÄÇ </summary>\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Token Density<sup>+</sup></th>\n            <th>OpenCompass</th>\n            <th>MME</th>\n            <th>MMVet</th>\n            <th>OCRBench</th>\n            <th>MMMU val</th>\n            <th>MathVista mini</th>\n            <th>MMB1.1 test</th>\n            <th>AI2D</th>\n            <th>TextVQA val</th>\n            <th>DocVQA test</th>\n            <th>HallusionBench</th>\n            <th>Object HalBench</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"15\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>69.9</td>\n            <td>2328.7</td>\n            <td>69.1</td>\n            <td>736</td>\n            <td>69.2</td>\n            <td>61.3</td>\n            <td>82.2</td>\n            <td>84.6</td>\n            <td>-</td>\n            <td>92.8</td>\n            <td>55.0</td>\n            <td>17.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Claude 3.5 Sonnet</td>\n            <td>-</td>\n            <td>750</td>\n            <td>67.9</td>\n            <td>1920.0</td>\n            <td>66.0</td>\n            <td>788</td>\n            <td>65.9</td>\n            <td>61.6</td>\n            <td>78.5</td>\n            <td>80.2</td>\n            <td>-</td>\n            <td>95.2</td>\n            <td>49.9</td>\n            <td>13.8</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini 1.5 Pro</td>\n            <td>-</td>\n            <td>-</td>\n            <td>64.4</td>\n            <td>2110.6</td>\n            <td>64.0</td>\n            <td>754</td>\n            <td>60.6</td>\n            <td>57.7</td>\n            <td>73.9</td>\n            <td>79.1</td>\n            <td>73.5</td>\n            <td>86.5</td>\n            <td>45.6</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o mini</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>64.1</td>\n            <td>2003.4</td>\n            <td>66.9</td>\n            <td>785</td>\n            <td>60.0</td>\n            <td>52.4</td>\n            <td>76.0</td>\n            <td>77.8</td>\n            <td>-</td>\n            <td>-</td>\n            <td>46.1</td>\n            <td>12.4</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>63.5</td>\n            <td>2070.2</td>\n            <td>67.5</td>\n            <td>656</td>\n            <td>61.7</td>\n            <td>54.7</td>\n            <td>79.8</td>\n            <td>78.6</td>\n            <td>78.0</td>\n            <td>87.2</td>\n            <td>43.9</td>\n            <td>14.2</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Step-1V</td>\n            <td>-</td>\n            <td>-</td>\n            <td>59.5</td>\n            <td>2206.4</td>\n            <td>63.3</td>\n            <td>625</td>\n            <td>49.9</td>\n            <td>44.8</td>\n            <td>78.0</td>\n            <td>79.2</td>\n            <td>71.6</td>\n            <td>-</td>\n            <td>48.4</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen-VL-Max</td>\n            <td>-</td>\n            <td>784</td>\n            <td>58.3</td>\n            <td>2281.7</td>\n            <td>61.8</td>\n            <td>684</td>\n            <td>52.0</td>\n            <td>43.4</td>\n            <td>74.6</td>\n            <td>75.7</td>\n            <td>79.5</td>\n            <td>93.1</td>\n            <td>41.2</td>\n            <td>13.4</td>\n        </tr>\n        <tr>\n            <td colspan=\"15\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Yi-34B</td>\n            <td>34B</td>\n            <td>157</td>\n            <td>55.0</td>\n            <td>2006.5</td>\n            <td>50.7</td>\n            <td>574</td>\n            <td>48.8</td>\n            <td>40.4</td>\n            <td>77.8</td>\n            <td>78.9</td>\n            <td>69.3</td>\n            <td>-</td>\n            <td>34.8</td>\n            <td>12.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Mini-Gemini-HD-34B</td>\n            <td>34B</td>\n            <td>157</td>\n            <td>-</td>\n            <td>2141</td>\n            <td>59.3</td>\n            <td>518</td>\n            <td>48.0</td>\n            <td>43.3</td>\n            <td>-</td>\n            <td>80.5</td>\n            <td>74.1</td>\n            <td>78.9</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Cambrian-34B</td>\n            <td>34B</td>\n            <td>1820</td>\n            <td>58.3</td>\n            <td>2049.9</td>\n            <td>53.2</td>\n            <td>591</td>\n            <td>50.4</td>\n            <td>50.3</td>\n            <td>77.8</td>\n            <td>79.5</td>\n            <td>76.7</td>\n            <td>75.5</td>\n            <td>41.6</td>\n            <td>14.7</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GLM-4V-9B</td>\n            <td>13B</td>\n            <td>784</td>\n            <td>59.1</td>\n            <td>2018.8</td>\n            <td>58.0</td>\n            <td>776</td>\n            <td>46.9</td>\n            <td>51.1</td>\n            <td>67.9</td>\n            <td>71.2</td>\n            <td>-</td>\n            <td>-</td>\n            <td>45.0</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>706</td>\n            <td>64.1</td>\n            <td>2215.1</td>\n            <td>54.3</td>\n            <td>794</td>\n            <td><strong>51.2</strong></td>\n            <td>58.3</td>\n            <td><strong>79.4</strong></td>\n            <td><strong>83.6</strong></td>\n            <td>77.4</td>\n            <td><strong>91.6</strong></td>\n            <td>45.0</td>\n            <td>21.3</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-Llama-V 2.5</td>\n            <td>8B</td>\n            <td>1882</td>\n            <td>58.8</td>\n            <td>2024.6</td>\n            <td>52.8</td>\n            <td>725</td>\n            <td>45.8</td>\n            <td>54.3</td>\n            <td>72.0</td>\n            <td>78.4</td>\n            <td>76.6</td>\n            <td>84.8</td>\n            <td>42.4</td>\n            <td>10.3</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>2822</strong></td>\n            <td><strong>65.2</strong></td>\n            <td><strong>2348.4</strong>*</td>\n            <td><strong>60.0</strong></td>\n            <td><strong>852</strong>*</td>\n            <td>49.8*</td>\n            <td><strong>60.6</strong></td>\n            <td>78.0</td>\n            <td>82.1</td>\n            <td><strong>80.1<strong></td>\n            <td>90.8</td>\n            <td><strong>48.1</strong>*</td>\n            <td><strong>8.2</strong></td>\n        </tr>\n    </tbody>\n</table>\n\n</div>\n* Êàë‰ª¨‰ΩøÁî®ÊÄùÁª¥ÈìæÊèêÁ§∫ËØçÊù•ËØÑ‰º∞Ëøô‰∫õÂü∫ÂáÜ„ÄÇ\n\n<sup>+</sup> Token DensityÔºöÊØè‰∏™ËßÜËßâ token Âú®ÊúÄÂ§ßÂàÜËæ®Áéá‰∏ãÁºñÁ†ÅÁöÑÂÉèÁ¥†Êï∞ÔºåÂç≥ÊúÄÂ§ßÂàÜËæ®Áéá‰∏ãÁöÑÂÉèÁ¥†Êï∞ / ËßÜËßâ token Êï∞„ÄÇ\n\nÊ≥®ÊÑèÔºöÈó≠Ê∫êÊ®°ÂûãÁöÑ Token Density Áî± API Êî∂Ë¥πÊñπÂºè‰º∞ÁÆóÂæóÂà∞„ÄÇ\n</details>\n\n\n<details>\n<summary>ÁÇπÂáªÊü•Áúã Mantis Eval, BLINK, Mathverse mv, Sciverse mv, MIRB ‰∏äÁöÑÂ§öÂõæËØÑÊµãÁªìÊûúËØ¶ÊÉÖ„ÄÇ</summary>\n<div align=\"center\">\n \n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Mantis Eval</th>\n            <th>BLINK val</th>\n            <th>Mathverse mv</th>\n            <th>Sciverse mv</th>\n            <th>MIRB</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"7\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>62.7</td>\n            <td>54.6</td>\n            <td>60.3</td>\n            <td>66.9</td>\n            <td>53.1</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Interleave-14B</td>\n            <td>14B</td>\n            <td>66.4</td>\n            <td>52.6</td>\n            <td>32.7</td>\n            <td>30.2</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td colspan=\"7\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Emu2-Chat</td>\n            <td>37B</td>\n            <td>37.8</td>\n            <td>36.2</td>\n            <td>-</td>\n            <td>27.2</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM</td>\n            <td>17B</td>\n            <td>45.2</td>\n            <td>41.1</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">VPG-C</td>\n            <td>7B</td>\n            <td>52.4</td>\n            <td>43.1</td>\n            <td>24.3</td>\n            <td>23.1</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">VILA 8B</td>\n            <td>8B</td>\n            <td>51.2</td>\n            <td>39.3</td>\n            <td>-</td>\n            <td>36.5</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternLM-XComposer-2.5</td>\n            <td>8B</td>\n            <td>53.1*</td>\n            <td>48.9</td>\n            <td>32.1*</td>\n            <td>-</td>\n            <td>42.5</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>59.0*</td>\n            <td>50.9</td>\n            <td>30.5*</td>\n            <td>34.4*</td>\n            <td><strong>56.9*</strong></td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>69.1</strong></td>\n            <td><strong>53.0</strong></td>\n            <td><strong>84.9</strong></td>\n            <td><strong>74.9</strong></td>\n            <td>53.8</td>\n        </tr>\n    </tbody>\n</table>\n\n\n</div>\n* Ê≠£ÂºèÂºÄÊ∫êÊ®°ÂûãÊùÉÈáçÁöÑËØÑÊµãÁªìÊûú„ÄÇ\n</details>\n\n<details>\n<summary>ÁÇπÂáªÊü•Áúã Video-MME Âíå Video-ChatGPT ‰∏äÁöÑËßÜÈ¢ëËØÑÊµãÁªìÊûúËØ¶ÊÉÖ„ÄÇ</summary>\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th colspan=\"2\">Video-MME</th>\n            <th colspan=\"5\">Video-ChatGPT</th>\n        </tr>\n        <tr>\n            <th align=\"left\"></th>\n            <th></th>\n            <th>w/o subs</th>\n            <th>w subs</th>\n            <th>Correctness</th>\n            <th>Detail</th>\n            <th>Context</th>\n            <th>Temporal</th>\n            <th>Consistency</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"9\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Claude 3.5 Sonnet</td>\n            <td>-</td>\n            <td>60.0</td>\n            <td>62.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V</td>\n            <td>-</td>\n            <td>59.9</td>\n            <td>63.3</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td colspan=\"9\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-7B</td>\n            <td>7B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.39</td>\n            <td>3.29</td>\n            <td>3.92</td>\n            <td>2.60</td>\n            <td>3.12</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-34B</td>\n            <td>34B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.29</td>\n            <td>3.23</td>\n            <td>3.83</td>\n            <td>2.51</td>\n            <td>3.47</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM2-Video</td>\n            <td>12B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.49</td>\n            <td><strong>3.46</strong></td>\n            <td>3.23</td>\n            <td><strong>2.98</strong></td>\n            <td><strong>3.64</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LongVA</td>\n            <td>7B</td>\n            <td>52.4</td>\n            <td>54.3</td>\n            <td>3.05</td>\n            <td>3.09</td>\n            <td>3.77</td>\n            <td>2.44</td>\n            <td><strong>3.64</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>54.0</td>\n            <td>56.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternLM-XComposer-2.5</td>\n            <td>8B</td>\n            <td>55.8</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Video</td>\n            <td>32B</td>\n            <td>60.2</td>\n            <td>63.0</td>\n            <td>3.48</td>\n            <td>3.37</td>\n            <td><strong>3.95</strong></td>\n            <td>2.64</td>\n            <td>3.28</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>60.9</strong></td>\n            <td><strong>63.6</strong></td>\n            <td><strong>3.59</strong></td>\n            <td>3.28</td>\n            <td>3.93</td>\n            <td>2.73</td>\n            <td>3.62</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n</details>\n\n\n<details>\n<summary>ÁÇπÂáªÊü•Áúã TextVQA, VizWiz, VQAv2, OK-VQA‰∏äÁöÑÂ∞ëÊ†∑Êú¨ËØÑÊµãÁªìÊûúËØ¶ÊÉÖ„ÄÇ</summary>\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Shot</th>\n            <th>TextVQA val</th>\n            <th>VizWiz test-dev</th>\n            <th>VQAv2 test-dev</th>\n            <th>OK-VQA val</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">Flamingo</td>\n            <td rowspan=\"3\">80B</td>\n            <td>0*</td>\n            <td>35.0</td>\n            <td>31.6</td>\n            <td>56.3</td>\n            <td>40.6</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>36.5</td>\n            <td>39.6</td>\n            <td>63.1</td>\n            <td><strong>57.4</strong></td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>37.3</td>\n            <td>44.8</td>\n            <td>65.6</td>\n            <td>57.5</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">IDEFICS</td>\n            <td rowspan=\"3\">80B</td>\n            <td>0*</td>\n            <td>30.9</td>\n            <td>36.0</td>\n            <td>60.0</td>\n            <td>45.2</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>34.3</td>\n            <td>40.4</td>\n            <td>63.6</td>\n            <td>52.4</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>35.7</td>\n            <td>46.1</td>\n            <td>64.8</td>\n            <td>55.1</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">OmniCorpus</td>\n            <td rowspan=\"3\">7B</td>\n            <td>0*</td>\n            <td>43.0</td>\n            <td>49.8</td>\n            <td>63.2</td>\n            <td>45.5</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>45.4</td>\n            <td>51.3</td>\n            <td>64.5</td>\n            <td>46.5</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>45.6</td>\n            <td>52.2</td>\n            <td>64.7</td>\n            <td>46.6</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">Emu2</td>\n            <td rowspan=\"3\">37B</td>\n            <td>0</td>\n            <td>26.4</td>\n            <td>40.4</td>\n            <td>33.5</td>\n            <td>26.7</td>\n        </tr>\n        <tr>\n            <td>4</td>\n            <td>48.2</td>\n            <td>54.6</td>\n            <td>67.0</td>\n            <td>53.2</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>49.3</td>\n            <td>54.7</td>\n            <td>67.8</td>\n            <td>54.1</td>\n        </tr>\n        <tr>\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"2\">MM1</td>\n            <td rowspan=\"2\">30B</td>\n            <td>0</td>\n            <td>26.2</td>\n            <td>40.4</td>\n            <td>48.9</td>\n            <td>26.7</td>\n        </tr>\n        <tr>\n            <td>8</td>\n            <td>49.3</td>\n            <td>54.7</td>\n            <td><strong>70.9</strong></td>\n            <td>54.1</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td align=\"left\" nowrap=\"nowrap\" rowspan=\"3\">MiniCPM-V 2.6<sup>+</sup></td>\n            <td rowspan=\"3\">8B</td>\n            <td>0</td>\n            <td>43.9</td>\n            <td>33.8</td>\n            <td>45.4</td>\n            <td>23.9</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td>4</td>\n            <td>63.6</td>\n            <td>60.5</td>\n            <td>65.5</td>\n            <td>50.1</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td>8</td>\n            <td><strong>64.6</strong></td>\n            <td><strong>63.4</strong></td>\n            <td>68.2</td>\n            <td>51.4</td>\n        </tr>\n    </tbody>\n</table>\n\n\n</div>\n* ‰ΩøÁî® Flamingo ÊñπÂºè zero image shot Âíå two additional text shots ËØÑ‰º∞Èõ∂Ê†∑Êú¨ÊÄßËÉΩ„ÄÇ\n\n<sup>+</sup> Êàë‰ª¨Âú®Ê≤°ÊúâËøõË°åÁõëÁù£ÂæÆË∞É (SFT) ÁöÑÊÉÖÂÜµ‰∏ãËØÑ‰º∞È¢ÑËÆ≠ÁªÉÁöÑÊ®°ÂûãÊùÉÈáç (ckpt)„ÄÇ\n</details>\n\n### ÂÖ∏ÂûãÁ§∫‰æã <!-- omit in toc -->\n\n<div style=\"display: flex; flex-direction: column; align-items: center;\">\n  <img src=\"assets/minicpmv2_6/multi_img-bike.png\" alt=\"Bike\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multi_img-menu.png\" alt=\"Menu\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multi_img-code.png\" alt=\"Code\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/ICL-Mem.png\" alt=\"Mem\" style=\"margin-bottom: 5px;\">\n  <img src=\"assets/minicpmv2_6/multiling-medal.png\" alt=\"medal\" style=\"margin-bottom: 10px;\">\n</div>\n<details>\n  <summary>ÁÇπÂáªÊü•ÁúãÊõ¥Â§öÁ§∫‰æã„ÄÇ</summary>\n  <div style=\"display: flex; flex-direction: column; align-items: center;\">\n    <img src=\"assets/minicpmv2_6/ICL-elec.png\" alt=\"elec\" style=\"margin-bottom: 5px;\">\n    <img src=\"assets/minicpmv2_6/multiling-olympic.png\" alt=\"Menu\" style=\"margin-bottom: 10px;\">\n  </div>\n</details>\n\nÊàë‰ª¨Â∞Ü MiniCPM-V 2.6 ÈÉ®ÁΩ≤Âú®iPad Pro‰∏äÔºåÂπ∂ÂΩïÂà∂‰∫Ü‰ª•‰∏ãÊºîÁ§∫ËßÜÈ¢ë„ÄÇ\n\n<table align=\"center\"> \n    <p align=\"center\">\n      <img src=\"assets/gif_cases/ai.gif\" width=32%/>\n      &nbsp;&nbsp;&nbsp;&nbsp;\n      <img src=\"assets/gif_cases/beer.gif\" width=32%/>\n    </p>\n</table>\n\n<table align=\"center\">\n    <p align=\"center\">\n      <video src=\"https://github.com/user-attachments/assets/21f4b818-ede1-4822-920e-91281725c830\" width=\"360\" /> </video>\n      <!-- <video src=\"https://github.com/user-attachments/assets/c835f757-206b-4d9c-8e36-70d67b453628\" width=\"360\" /> </video> -->\n    </p>\n</table>\n\n## MiniCPM-Llama3-V 2.5\n\n<details>\n<summary>Êü•Áúã MiniCPM-Llama3-V 2.5 ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ</summary>\n\n**MiniCPM-Llama3-V 2.5** ÊòØ MiniCPM-V Á≥ªÂàóÁöÑÊúÄÊñ∞ÁâàÊú¨Ê®°ÂûãÔºåÂü∫‰∫é SigLip-400M Âíå¬†Llama3-8B-Instruct ÊûÑÂª∫ÔºåÂÖ± 8B ÂèÇÊï∞ÈáèÔºåÁõ∏ËæÉ‰∫é MiniCPM-V 2.0 ÊÄßËÉΩÂèñÂæóËæÉÂ§ßÂπÖÂ∫¶ÊèêÂçá„ÄÇMiniCPM-Llama3-V 2.5 ÂÄºÂæóÂÖ≥Ê≥®ÁöÑÁâπÁÇπÂåÖÊã¨Ôºö\n\n- üî• **È¢ÜÂÖàÁöÑÊÄßËÉΩ„ÄÇ**\n  MiniCPM-Llama3-V 2.5 Âú®ÁªºÂêà‰∫Ü 11 ‰∏™‰∏ªÊµÅÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãËØÑÊµãÂü∫ÂáÜÁöÑ OpenCompass Ê¶úÂçï‰∏äÂπ≥ÂùáÂæóÂàÜ 65.1Ôºå**‰ª• 8B ÈáèÁ∫ßÁöÑÂ§ßÂ∞èË∂ÖËøá‰∫Ü GPT-4V-1106„ÄÅGemini Pro„ÄÅClaude 3„ÄÅQwen-VL-Max Á≠â‰∏ªÊµÅÂïÜÁî®Èó≠Ê∫êÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã**ÔºåÂ§ßÂπÖË∂ÖË∂äÂü∫‰∫éLlama 3ÊûÑÂª∫ÁöÑÂÖ∂‰ªñÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã„ÄÇ\n\n- üí™ **‰ºòÁßÄÁöÑ OCR ËÉΩÂäõ„ÄÇ**\n  MiniCPM-Llama3-V 2.5 ÂèØÊé•Âèó 180 ‰∏áÂÉèÁ¥†ÁöÑ‰ªªÊÑèÂÆΩÈ´òÊØîÂõæÂÉèËæìÂÖ•Ôºå**OCRBench ÂæóÂàÜËææÂà∞ 725ÔºåË∂ÖË∂ä GPT-4o„ÄÅGPT-4V„ÄÅGemini Pro„ÄÅQwen-VL-Max Á≠âÂïÜÁî®Èó≠Ê∫êÊ®°Âûã**ÔºåËææÂà∞ÊúÄ‰Ω≥Ê∞¥Âπ≥„ÄÇÂü∫‰∫éËøëÊúüÁî®Êà∑ÂèçÈ¶àÂª∫ËÆÆÔºåMiniCPM-Llama3-V 2.5 Â¢ûÂº∫‰∫ÜÂÖ®Êñá OCR ‰ø°ÊÅØÊèêÂèñ„ÄÅË°®Ê†ºÂõæÂÉèËΩ¨ markdown Á≠âÈ´òÈ¢ëÂÆûÁî®ËÉΩÂäõÔºåÂπ∂‰∏îËøõ‰∏ÄÊ≠•Âä†Âº∫‰∫ÜÊåá‰ª§Ë∑üÈöè„ÄÅÂ§çÊùÇÊé®ÁêÜËÉΩÂäõÔºåÂ∏¶Êù•Êõ¥Â•ΩÁöÑÂ§öÊ®°ÊÄÅ‰∫§‰∫í‰ΩìÊÑü„ÄÇ\n\n- üèÜ **ÂèØ‰ø°Ë°å‰∏∫„ÄÇ** \n  ÂÄüÂä©ÊúÄÊñ∞ÁöÑ [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) ÂØπÈΩêÊäÄÊúØÔºà[RLHF-V](https://github.com/RLHF-V/) [CVPR'24]Á≥ªÂàóÁöÑÊúÄÊñ∞ÊäÄÊúØÔºâÔºåMiniCPM-Llama3-V 2.5 ÂÖ∑ÊúâÊõ¥Âä†ÂèØ‰ø°ÁöÑÂ§öÊ®°ÊÄÅË°å‰∏∫ÔºåÂú®¬†Object HalBench ÁöÑÂπªËßâÁéáÈôç‰ΩéÂà∞‰∫Ü **10.3%**ÔºåÊòæËëó‰Ωé‰∫é GPT-4V-1106 (13.6%)ÔºåËææÂà∞ÂºÄÊ∫êÁ§æÂå∫ÊúÄ‰Ω≥Ê∞¥Âπ≥„ÄÇ[Êï∞ÊçÆÈõÜÂ∑≤ÂèëÂ∏É](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)„ÄÇ\n\n- üåè **Â§öËØ≠Ë®ÄÊîØÊåÅ„ÄÇ**\n  ÂæóÁõä‰∫é Llama 3 Âº∫Â§ßÁöÑÂ§öËØ≠Ë®ÄËÉΩÂäõÂíå VisCPM ÁöÑË∑®ËØ≠Ë®ÄÊ≥õÂåñÊäÄÊúØÔºåMiniCPM-Llama3-V 2.5¬†Âú®‰∏≠Ëã±ÂèåËØ≠Â§öÊ®°ÊÄÅËÉΩÂäõÁöÑÂü∫Á°Ä‰∏äÔºå‰ªÖÈÄöËøáÂ∞ëÈáèÁøªËØëÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÁöÑÊåá‰ª§ÂæÆË∞ÉÔºåÈ´òÊïàÊ≥õÂåñÊîØÊåÅ‰∫Ü**Âæ∑ËØ≠„ÄÅÊ≥ïËØ≠„ÄÅË•øÁè≠ÁâôËØ≠„ÄÅÊÑèÂ§ßÂà©ËØ≠„ÄÅÈü©ËØ≠Á≠â 30+ ÁßçËØ≠Ë®Ä**ÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõÔºåÂπ∂Ë°®Áé∞Âá∫‰∫ÜËâØÂ•ΩÁöÑÂ§öËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÂØπËØùÊÄßËÉΩ„ÄÇ[Êü•ÁúãÊâÄÊúâÊîØÊåÅËØ≠Ë®Ä](./assets/minicpm-llama-v-2-5_languages.md)\n\n- üöÄ **È´òÊïàÈÉ®ÁΩ≤„ÄÇ**\n  MiniCPM-Llama3-V 2.5 ËæÉ‰∏∫Á≥ªÁªüÂú∞ÈÄöËøá**Ê®°ÂûãÈáèÂåñ„ÄÅCPU„ÄÅNPU„ÄÅÁºñËØë‰ºòÂåñ**Á≠âÈ´òÊïàÂä†ÈÄüÊäÄÊúØÔºåÂÆûÁé∞È´òÊïàÁöÑÁªàÁ´ØËÆæÂ§áÈÉ®ÁΩ≤„ÄÇÂØπ‰∫éÈ´òÈÄöËäØÁâáÁöÑÁßªÂä®ÊâãÊú∫ÔºåÊàë‰ª¨È¶ñÊ¨°Â∞Ü NPU Âä†ÈÄüÊ°ÜÊû∂ QNN Êï¥ÂêàËøõ‰∫Ü llama.cpp„ÄÇÁªèËøáÁ≥ªÁªü‰ºòÂåñÂêéÔºåMiniCPM-Llama3-V 2.5 ÂÆûÁé∞‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁ´Ø‰æß**ËØ≠Ë®ÄËß£Á†ÅÈÄüÂ∫¶ 3 ÂÄçÂä†ÈÄü**„ÄÅ**ÂõæÂÉèÁºñÁ†Å 150 ÂÄçÂä†ÈÄü**ÁöÑÂ∑®Â§ßÊèêÂçá„ÄÇ\n\n- üí´ **Êòì‰∫é‰ΩøÁî®„ÄÇ**\n  MiniCPM-Llama3-V 2.5 ÂèØ‰ª•ÈÄöËøáÂ§öÁßçÊñπÂºèËΩªÊùæ‰ΩøÁî®ÔºöÔºà1Ôºâ[llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md) Âíå [ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5) ÊîØÊåÅÂú®Êú¨Âú∞ËÆæÂ§á‰∏äËøõË°åÈ´òÊïàÁöÑ CPU Êé®ÁêÜÔºõÔºà2ÔºâÊèê‰æõ 16 ÁßçÂ∞∫ÂØ∏ÁöÑ [GGUF](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf) Ê†ºÂºèÈáèÂåñÊ®°ÂûãÔºõÔºà3Ôºâ‰ªÖÈúÄ 2 Âº† V100 GPU Âç≥ÂèØËøõË°åÈ´òÊïàÁöÑ [LoRA](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#lora-finetuning) ÂæÆË∞ÉÔºõÔºà\t4ÔºâÊîØÊåÅ[ÊµÅÂºèËæìÂá∫](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage)ÔºõÔºà5ÔºâÂø´ÈÄüÊê≠Âª∫ [Gradio](https://github.com/OpenBMB/MiniCPM-V/blob/main/web_demo_2.5.py) Âíå [Streamlit](https://github.com/OpenBMB/MiniCPM-V/blob/main/web_demo_streamlit-2_5.py) Êú¨Âú∞ WebUI demoÔºõÔºà\t6.Ôºâ[HuggingFace Spaces](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5) ‰∫§‰∫íÂºè demo„ÄÇ\n\n### ÊÄßËÉΩËØÑ‰º∞ <!-- omit in toc -->\n\n<div align=\"center\">\n    <img src=\"assets/MiniCPM-Llama3-V-2.5-peformance.png\" width=\"66%\" />\n</div>\n<details>\n<summary>TextVQA, DocVQA, OCRBench, OpenCompass MultiModal Avg Score, MME, MMBench, MMMU, MathVista, LLaVA Bench, RealWorld QA, Object HalBench‰∏äÁöÑËØ¶ÁªÜËØÑÊµãÁªìÊûú„ÄÇ </summary>\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>OCRBench</th>\n            <th>TextVQA val</th>\n            <th>DocVQA test</th>\n            <th>Open-Compass</th>\n            <th>MME</th>\n            <th>MMB test (en)</th>\n            <th>MMB test (cn)</th>\n            <th>MMMU val</th>\n            <th>Math-Vista</th>\n            <th>LLaVA Bench</th>\n            <th>RealWorld QA</th>\n            <th>Object HalBench</th>\n        </tr>\n    </thead>\n            <tbody align=\"center\">\n        <tr>\n            <td colspan=\"14\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini Pro</td>\n            <td>-</td>\n            <td>680</td>\n            <td>74.6</td>\n            <td>88.1</td>\n            <td>62.9</td>\n            <td>2148.9</td>\n            <td>73.6</td>\n            <td>74.3</td>\n            <td>48.9</td>\n            <td>45.8</td>\n            <td>79.9</td>\n            <td>60.4</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4V (2023.11.06)</td>\n            <td>-</td>\n            <td>645</td>\n            <td>78.0</td>\n            <td>88.4</td>\n            <td>63.5</td>\n            <td>1771.5</td>\n            <td>77.0</td>\n            <td>74.4</td>\n            <td>53.8</td>\n            <td>47.8</td>\n            <td>93.1</td>\n            <td>63.0</td>\n            <td>86.4</td>\n        </tr>\n        <tr>\n            <td colspan=\"14\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Mini-Gemini</td>\n            <td>2.2B</td>\n            <td>-</td>\n            <td>56.2</td>\n            <td>34.2*</td>\n            <td>-</td>\n            <td>1653.0</td>\n            <td>-</td>\n            <td>-</td>\n            <td>31.7</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen-VL-Chat</td>\n            <td>9.6B</td>\n            <td>488</td>\n            <td>61.5</td>\n            <td>62.6</td>\n            <td>51.6</td>\n            <td>1860.0</td>\n            <td>61.8</td>\n            <td>56.3</td>\n            <td>37.0</td>\n            <td>33.8</td>\n            <td>67.7</td>\n            <td>49.3</td>\n            <td>56.2</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">DeepSeek-VL-7B</td>\n            <td>7.3B</td>\n            <td>435</td>\n            <td>64.7*</td>\n            <td>47.0*</td>\n            <td>54.6</td>\n            <td>1765.4</td>\n            <td>73.8</td>\n            <td>71.4</td>\n            <td>38.3</td>\n            <td>36.8</td>\n            <td>77.8</td>\n            <td>54.2</td>\n            <td>-</td>\n        </tr>        \n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Yi-VL-34B</td>\n            <td>34B</td>\n            <td>290</td>\n            <td>43.4*</td>\n            <td>16.9*</td>\n            <td>52.2</td>\n            <td><strong>2050.2</strong></td>\n            <td>72.4</td>\n            <td>70.7</td>\n            <td>45.1</td>\n            <td>30.7</td>\n            <td>62.3</td>\n            <td>54.8</td>\n            <td>79.3</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CogVLM-Chat</td>\n            <td>17.4B</td>\n            <td>590</td>\n            <td>70.4</td>\n            <td>33.3*</td>\n            <td>54.2</td>\n            <td>1736.6</td>\n            <td>65.8</td>\n            <td>55.9</td>\n            <td>37.3</td>\n            <td>34.7</td>\n            <td>73.9</td>\n            <td>60.3</td>\n            <td>73.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">TextMonkey</td>\n            <td>9.7B</td>\n            <td>558</td>\n            <td>64.3</td>\n            <td>66.7</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n          <td nowrap=\"nowrap\" align=\"left\">Idefics2</td>\n          <td>8.0B</td>\n          <td>-</td>\n          <td>73.0</td>\n          <td>74.0</td>\n          <td>57.2</td>\n          <td>1847.6</td>\n          <td>75.7</td>\n          <td>68.6</td>\n          <td>45.2</td>\n          <td>52.2</td>\n          <td>49.1</td>\n          <td>60.7</td>\n          <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Bunny-LLama-3-8B</td>\n            <td>8.4B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>54.3</td>\n            <td>1920.3</td>\n            <td>77.0</td>\n            <td>73.9</td>\n            <td>41.3</td>\n            <td>31.5</td>\n            <td>61.2</td>\n            <td>58.8</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT Llama-3-8B</td>\n            <td>8.4B</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>1971.5</td>\n            <td>-</td>\n            <td>-</td>\n            <td>41.7</td>\n            <td>-</td>\n            <td>80.1</td>\n            <td>60.0</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Phi-3-vision-128k-instruct</td>\n            <td>4.2B</td>\n            <td>639*</td>\n            <td>70.9</td>\n            <td>-</td>\n            <td>-</td>\n            <td>1537.5*</td>\n            <td>-</td>\n            <td>-</td>\n            <td>40.4</td>\n            <td>44.5</td>\n            <td>64.2*</td>\n            <td>58.8*</td>\n            <td>-</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 1.0</td>\n            <td>2.8B</td>\n            <td>366</td>\n            <td>60.6</td>\n            <td>38.2</td>\n            <td>47.5</td>\n            <td>1650.2</td>\n            <td>64.1</td>\n            <td>62.6</td>\n            <td>38.3</td>\n            <td>28.9</td>\n            <td>51.3</td>\n            <td>51.2</td>\n            <td>78.4</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.0</td>\n            <td>2.8B</td>\n            <td>605</td>\n            <td>74.1</td>\n            <td>71.9</td>\n            <td>54.5</td>\n            <td>1808.6</td>\n            <td>69.1</td>\n            <td>66.5</td>\n            <td>38.2</td>\n            <td>38.7</td>\n            <td>69.2</td>\n            <td>55.8</td>\n            <td>85.5</td>\n        </tr>\n        <tr style=\"background-color: #e6f2ff;\">\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-Llama3-V 2.5</td>\n            <td>8.5B</td>\n            <td><strong>725</strong></td>\n            <td><strong>76.6</strong></td>\n            <td><strong>84.8</strong></td>\n            <td><strong>65.1</strong></td>\n            <td>2024.6</td>\n            <td><strong>77.2</strong></td>\n            <td><strong>74.2</strong></td>\n            <td><strong>45.8</strong></td>\n            <td><strong>54.3</strong></td>\n            <td><strong>86.7</strong></td>\n            <td><strong>63.5</strong></td>\n            <td><strong>89.7</strong></td>\n        </tr>\n    </tbody>\n</table>\n\n</div>\n* Ê≠£ÂºèÂºÄÊ∫êÊ®°ÂûãÊùÉÈáçÁöÑËØÑÊµãÁªìÊûú„ÄÇ\n</details>\n\n<div align=\"center\">\n    <img src=\"assets/llavabench_compare_3.png\" width=\"80%\" />\n    <br>\n    Â§öËØ≠Ë®ÄLLaVA BenchËØÑÊµãÁªìÊûú\n</div>\n\n\n### ÂÖ∏ÂûãÁ§∫‰æã <!-- omit in toc -->\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/minicpmv-llama3-v2.5/cases_all.png\" width=95%/>\n    </p>\n</table>\n\n\n</details>\n\n## MiniCPM-V 2.0\n\n<details>\n<summary>Êü•Áúã MiniCPM-V 2.0 ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ</summary>\n\n**MiniCPM-V 2.0**ÂèØ‰ª•È´òÊïàÈÉ®ÁΩ≤Âà∞ÁªàÁ´ØËÆæÂ§á„ÄÇËØ•Ê®°ÂûãÂü∫‰∫é SigLip-400M Âíå [MiniCPM-2.4B](https://github.com/OpenBMB/MiniCPM/)ÊûÑÂª∫ÔºåÈÄöËøáperceiver resamplerËøûÊé•„ÄÇÂÖ∂ÁâπÁÇπÂåÖÊã¨Ôºö\n\n- üî• **‰ºòÁßÄÁöÑÊÄßËÉΩ„ÄÇ**\n\n  MiniCPM-V 2.0 Âú®Â§ö‰∏™ÊµãËØïÂü∫ÂáÜÔºàÂ¶Ç OCRBench, TextVQA, MME, MMB, MathVista Á≠âÔºâ‰∏≠ÂÆûÁé∞‰∫Ü 7B ‰ª•‰∏ãÊ®°ÂûãÁöÑ**ÊúÄ‰Ω≥ÊÄßËÉΩ**„ÄÇ**Âú®ÁªºÂêà‰∫Ü 11 ‰∏™‰∏ªÊµÅÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãËØÑÊµãÂü∫ÂáÜÁöÑ OpenCompass Ê¶úÂçï‰∏äË∂ÖËøá‰∫Ü Qwen-VL-Chat 9.6B„ÄÅCogVLM-Chat 17.4B Âíå Yi-VL 34B Á≠âÊõ¥Â§ßÂèÇÊï∞ËßÑÊ®°ÁöÑÊ®°Âûã**„ÄÇMiniCPM-V 2.0 ËøòÂ±ïÁé∞Âá∫**È¢ÜÂÖàÁöÑ OCR ËÉΩÂäõ**ÔºåÂú®Âú∫ÊôØÊñáÂ≠óËØÜÂà´ËÉΩÂäõ‰∏ä**Êé•Ëøë Gemini Pro**ÔºåOCRBench ÂæóÂàÜËææÂà∞**ÂºÄÊ∫êÊ®°ÂûãÁ¨¨‰∏Ä**„ÄÇ\n  \n\n- üèÜ **ÂèØ‰ø°Ë°å‰∏∫„ÄÇ** \n\n  Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÊ∑±ÂèóÂπªËßâÈóÆÈ¢òÂõ∞Êâ∞ÔºåÊ®°ÂûãÁªèÂ∏∏ÁîüÊàêÂíåÂõæÂÉè‰∏≠ÁöÑ‰∫ãÂÆû‰∏çÁ¨¶ÁöÑÊñáÊú¨„ÄÇMiniCPM-V 2.0 ÊòØ **Á¨¨‰∏Ä‰∏™ÈÄöËøáÂ§öÊ®°ÊÄÅ RLHF ÂØπÈΩêÁöÑÁ´Ø‰æßÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã**ÔºàÂÄüÂä© [RLHF-V](https://rlhf-v.github.io/) [CVPR'24] Á≥ªÂàóÊäÄÊúØÔºâ„ÄÇËØ•Ê®°ÂûãÂú® [Object HalBench](https://arxiv.org/abs/2312.00849) ËææÂà∞**Âíå GPT-4V Áõ∏‰ªø**ÁöÑÊÄßËÉΩ„ÄÇ\n\n\n- üåü **È´òÊ∏ÖÂõæÂÉèÈ´òÊïàÁºñÁ†Å„ÄÇ**\n\n  MiniCPM-V 2.0 ÂèØ‰ª•Êé•Âèó **180 ‰∏áÂÉèÁ¥†ÁöÑ‰ªªÊÑèÈïøÂÆΩÊØîÂõæÂÉèËæìÂÖ•**ÔºàÂü∫‰∫éÊúÄÊñ∞ÁöÑ[LLaVA-UHD](https://arxiv.org/pdf/2403.11703.pdf) ÊäÄÊúØÔºâÔºåËøô‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•ÊÑüÁü•Âà∞Â∞èÁâ©‰Ωì„ÄÅÂØÜÈõÜÊñáÂ≠óÁ≠âÊõ¥Âä†ÁªÜÁ≤íÂ∫¶ÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÇ \n\n\n- ‚ö°Ô∏è **È´òÊïàÈÉ®ÁΩ≤„ÄÇ**\n\n  MiniCPM-V 2.0 ÂèØ‰ª•**È´òÊïàÈÉ®ÁΩ≤Âú®Â§ßÂ§öÊï∞Ê∂àË¥πÁ∫ßÊòæÂç°Âíå‰∏™‰∫∫ÁîµËÑë‰∏ä**ÔºåÂåÖÊã¨**ÁßªÂä®ÊâãÊú∫Á≠âÁªàÁ´ØËÆæÂ§á**„ÄÇÂú®ËßÜËßâÁºñÁ†ÅÊñπÈù¢ÔºåÊàë‰ª¨ÈÄöËøáperceiver resamplerÂ∞ÜÂõæÂÉèË°®Á§∫ÂéãÁº©‰∏∫Êõ¥Â∞ëÁöÑ token„ÄÇËøô‰ΩøÂæó MiniCPM-V 2.0 Âç≥‰æøÊòØ**Èù¢ÂØπÈ´òÂàÜËæ®ÁéáÂõæÂÉèÔºå‰πüËÉΩÂç†Áî®ËæÉ‰ΩéÁöÑÂ≠òÂÇ®Âπ∂Â±ïÁé∞‰ºòÁßÄÁöÑÊé®ÁêÜÈÄüÂ∫¶**„ÄÇ\n\n- üôå **ÂèåËØ≠ÊîØÊåÅ„ÄÇ**\n\n  MiniCPM-V 2.0 **Êèê‰æõÈ¢ÜÂÖàÁöÑ‰∏≠Ëã±ÂèåËØ≠Â§öÊ®°ÊÄÅËÉΩÂäõÊîØÊåÅ**„ÄÇ\n  ËØ•ËÉΩÂäõÈÄöËøá [VisCPM](https://arxiv.org/abs/2308.12038) [ICLR'24] ËÆ∫Êñá‰∏≠ÊèêÂá∫ÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõÁöÑË∑®ËØ≠Ë®ÄÊ≥õÂåñÊäÄÊúØÂÆûÁé∞„ÄÇ\n\n### ÂÖ∏ÂûãÁ§∫‰æã <!-- omit in toc -->\n\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/minicpmv2-cases_2.png\" width=95%/>\n    </p>\n</table>\n\nÊàë‰ª¨Â∞Ü MiniCPM-V 2.0 ÈÉ®ÁΩ≤Âú®Â∞èÁ±≥ 14 Pro ‰∏äÔºåÂπ∂ÂΩïÂà∂‰∫Ü‰ª•‰∏ãÊºîÁ§∫ËßÜÈ¢ëÔºåÊú™Áªè‰ªª‰ΩïËßÜÈ¢ëÂâ™Ëæë„ÄÇ\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/gif_cases/station.gif\" width=36%/>\n      <img src=\"assets/gif_cases/london_car.gif\" width=36%/>\n    </p>\n</table>\n\n</details>\n\n\n<a id='legacy-models'></a>\n\n## ÂéÜÂè≤ÁâàÊú¨Ê®°Âûã  <!-- omit in toc -->\n\n\n| Ê®°Âûã                | ‰ªãÁªç‰ø°ÊÅØÂíå‰ΩøÁî®ÊïôÁ®ã       |\n|:----------------------|:-------------------:|\n| MiniCPM-V 1.0  | [ÊñáÊ°£](./minicpm_v1.md)   | \n| OmniLMM-12B  | [ÊñáÊ°£](./omnilmm.md)   |  \n\n\n## Gradio Demo ü§ó\n\nÊàë‰ª¨Êèê‰æõÁî± Hugging Face Gradio <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a> ÊîØÊåÅÁöÑÂú®Á∫øÂíåÊú¨Âú∞ Demo„ÄÇGradio ÊòØÁõÆÂâçÊúÄÊµÅË°åÁöÑÊ®°ÂûãÈÉ®ÁΩ≤Ê°ÜÊû∂ÔºåÊîØÊåÅÊµÅÂºèËæìÂá∫„ÄÅËøõÂ∫¶Êù°„ÄÅprocess bars ÂíåÂÖ∂‰ªñÂ∏∏Áî®ÂäüËÉΩ„ÄÇ\n\n### Online Demo <!-- omit in toc --> \n\nÊ¨¢ËøéËØïÁî® Online Demo: [MiniCPM-V 2.6](http://120.92.209.146:8887/) | [MiniCPM-Llama3-V 2.5](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5) | [MiniCPM-V 2.0](https://huggingface.co/spaces/openbmb/MiniCPM-V-2) „ÄÇ\n\n### Êú¨Âú∞ WebUI Demo <!-- omit in toc --> \n\nÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ËΩªÊùæÊûÑÂª∫Ëá™Â∑±ÁöÑÊú¨Âú∞ WebUI Demo„ÄÇ\n\n```shell\npip install -r requirements.txt\n```\n\n```shell\n# ÂØπ‰∫é NVIDIA GPUÔºåËØ∑ËøêË°åÔºö\npython web_demo_2.6.py --device cuda\n\n```\n\n\n\n## ÂÆâË£Ö\n\n1. ÂÖãÈöÜÊàë‰ª¨ÁöÑ‰ªìÂ∫ìÂπ∂Ë∑≥ËΩ¨Âà∞Áõ∏Â∫îÁõÆÂΩï\n\n```bash\ngit clone https://github.com/OpenBMB/MiniCPM-V.git\ncd MiniCPM-V\n```\n\n1. ÂàõÂª∫ conda ÁéØÂ¢É\n\n```Shell\nconda create -n MiniCPMV python=3.10 -y\nconda activate MiniCPMV\n```\n\n3. ÂÆâË£Ö‰æùËµñ\n\n```shell\npip install -r requirements.txt\n```\n\n## Êé®ÁêÜ\n\n### Ê®°ÂûãÂ∫ì\n\n| Ê®°Âûã           | ËÆæÂ§á | ËµÑÊ∫ê     | &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ÁÆÄ‰ªã       | ‰∏ãËΩΩÈìæÊé• |\n|:--------------|:-:|:----------:|:-------------------|:---------------:|\n| MiniCPM-V 2.6| GPU | 17 GB  | ÊúÄÊñ∞ÁâàÊú¨ÔºåÊèê‰æõÊúÄ‰Ω≥ÁöÑÁ´Ø‰æßÂçïÂõæ„ÄÅÂ§öÂõæ„ÄÅËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6) |\n| MiniCPM-V 2.6 gguf | CPU | 6 GB  | gguf ÁâàÊú¨ÔºåÊõ¥‰ΩéÁöÑÂÜÖÂ≠òÂç†Áî®ÂíåÊõ¥È´òÁöÑÊé®ÁêÜÊïàÁéá„ÄÇ   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6-gguf) |\n| MiniCPM-V 2.6 int4 | GPU | 7 GB  | int4ÈáèÂåñÁâàÔºåÊõ¥‰ΩéÊòæÂ≠òÂç†Áî®„ÄÇ   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2_6-int4) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6-int4) |\n| MiniCPM-Llama3-V 2.5| GPU | 19 GB  | Êèê‰æõÂá∫Ëâ≤ÁöÑÁ´Ø‰æßÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõ„ÄÇ   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5) |\n| MiniCPM-Llama3-V 2.5 gguf | CPU  | 6 GB | gguf ÁâàÊú¨ÔºåÊõ¥‰ΩéÁöÑÂÜÖÂ≠òÂç†Áî®ÂíåÊõ¥È´òÁöÑÊé®ÁêÜÊïàÁéá„ÄÇ  |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5-gguf) |\n| MiniCPM-Llama3-V 2.5 int4 | GPU | 8 GB | int4ÈáèÂåñÁâàÔºåÊõ¥‰ΩéÊòæÂ≠òÂç†Áî®„ÄÇ   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4/) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5-int4) |\n| MiniCPM-V 2.0 | GPU | 8 GB  | ËΩªÈáèÁ∫ßÁâàÊú¨ÔºåÂπ≥Ë°°ËÆ°ÁÆóÂºÄÈîÄÂíåÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõ„ÄÇ   |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-2) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2) |\n| MiniCPM-V 1.0 | GPU | 7 GB | ÊúÄËΩªÈáèÁâàÊú¨Ôºå Êèê‰æõÊúÄÂø´ÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇ    |   [ü§ó](https://huggingface.co/openbmb/MiniCPM-V) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V) |\n\nÊõ¥Â§ö[ÂéÜÂè≤ÁâàÊú¨Ê®°Âûã](#legacy-models)\n\n### Â§öËΩÆÂØπËØù\n\nËØ∑ÂèÇËÄÉ‰ª•‰∏ã‰ª£Á†ÅËøõË°åÊé®ÁêÜ„ÄÇ\n\n<div align=\"center\">\n<img src=\"assets/airplane.jpeg\" width=\"500px\">\n</div>\n\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\ntorch.manual_seed(0)\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nimage = Image.open('./assets/airplane.jpeg').convert('RGB')\n\n# First round chat \nquestion = \"Tell me the model of this aircraft.\"\nmsgs = [{'role': 'user', 'content': [image, question]}]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n\n# Second round chat \n# pass history context of multi-turn conversation\nmsgs.append({\"role\": \"assistant\", \"content\": [answer]})\nmsgs.append({\"role\": \"user\", \"content\": [\"Introduce something about Airbus A380.\"]})\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n\nÂèØ‰ª•ÂæóÂà∞‰ª•‰∏ãËæìÂá∫:\n\n```\n\"The aircraft in the image is an Airbus A380, which can be identified by its large size, double-deck structure, and the distinctive shape of its wings and engines. The A380 is a wide-body aircraft known for being the world's largest passenger airliner, designed for long-haul flights. It has four engines, which are characteristic of large commercial aircraft. The registration number on the aircraft can also provide specific information about the model if looked up in an aviation database.\"\n\n\"The Airbus A380 is a double-deck, wide-body, four-engine jet airliner made by Airbus. It is the world's largest passenger airliner and is known for its long-haul capabilities. The aircraft was developed to improve efficiency and comfort for passengers traveling over long distances. It has two full-length passenger decks, which can accommodate more passengers than a typical single-aisle airplane. The A380 has been operated by airlines such as Lufthansa, Singapore Airlines, and Emirates, among others. It is widely recognized for its unique design and significant impact on the aviation industry.\"\n```\n\n#### Â§öÂõæÁêÜËß£\n<details>\n<summary> ÁÇπÂáªÊü•Áúã‰ΩøÁî® MiniCPM-V 2.6 ËøõË°åÂ§öÂõæÁêÜËß£ÁöÑPythonÁ§∫‰æã </summary>\n  \n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nimage1 = Image.open('image1.jpg').convert('RGB')\nimage2 = Image.open('image2.jpg').convert('RGB')\nquestion = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'\n\nmsgs = [{'role': 'user', 'content': [image1, image2, question]}]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n#### Â∞ëÊ†∑Êú¨‰∏ä‰∏ãÊñáÂ≠¶‰π†\n\n<details>\n<summary> ÁÇπÂáªÊü•Áúã‰ΩøÁî® MiniCPM-V 2.6 ËøõË°åfew-shotÊé®ÁêÜÁöÑPythonÁ§∫‰æã </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nquestion = \"production date\" \nimage1 = Image.open('example1.jpg').convert('RGB')\nanswer1 = \"2023.08.04\"\nimage2 = Image.open('example2.jpg').convert('RGB')\nanswer2 = \"2007.04.24\"\nimage_test = Image.open('test.jpg').convert('RGB')\n\nmsgs = [\n    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n    {'role': 'user', 'content': [image_test, question]}\n]\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n#### ËßÜÈ¢ëÁêÜËß£\n<details>\n<summary> ÁÇπÂáªÊü•Áúã‰ΩøÁî® MiniCPM-V 2.6 ËøõË°åËßÜÈ¢ëÁêÜËß£ÁöÑPythonÁ§∫‰æã </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom decord import VideoReader, cpu    # pip install decord\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\nMAX_NUM_FRAMES=64 # if cuda OOM set a smaller number\n\ndef encode_video(video_path):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n\n    vr = VideoReader(video_path, ctx=cpu(0))\n    sample_fps = round(vr.get_avg_fps() / 1)  # FPS\n    frame_idx = [i for i in range(0, len(vr), sample_fps)]\n    if len(frame_idx) > MAX_NUM_FRAMES:\n        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\n    frames = vr.get_batch(frame_idx).asnumpy()\n    frames = [Image.fromarray(v.astype('uint8')) for v in frames]\n    print('num frames:', len(frames))\n    return frames\n\nvideo_path=\"video_test.mp4\"\nframes = encode_video(video_path)\nquestion = \"Describe the video\"\nmsgs = [\n    {'role': 'user', 'content': frames + [question]}, \n]\n\n# Set decode params for video\nparams = {}\nparams[\"use_image_id\"] = False\nparams[\"max_slice_nums\"] = 2 # Â¶ÇÊûúcuda OOM‰∏îËßÜÈ¢ëÂàÜËæ®ÁéáÂ§ß‰∫é448*448ÂèØËÆæ‰∏∫1\n\nanswer = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer,\n    **params\n)\nprint(answer)\n```\n</details>\n\n\n### Â§öÂç°Êé®ÁêÜ\nÊÇ®ÂèØ‰ª•ÈÄöËøáÂ∞ÜÊ®°ÂûãÁöÑÂ±ÇÂàÜÂ∏ÉÂú®Â§ö‰∏™‰ΩéÊòæÂ≠òÊòæÂç°Ôºà12 GB Êàñ 16 GBÔºâ‰∏äÔºåËøêË°å MiniCPM-Llama3-V 2.5„ÄÇËØ∑Êü•ÁúãËØ•[ÊïôÁ®ã](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md)ÔºåËØ¶ÁªÜ‰∫ÜËß£Â¶Ç‰Ωï‰ΩøÁî®Â§öÂº†‰ΩéÊòæÂ≠òÊòæÂç°ËΩΩÂÖ•Ê®°ÂûãÂπ∂ËøõË°åÊé®ÁêÜ„ÄÇ\n\n\n### Mac Êé®ÁêÜ\n<details>\n<summary>ÁÇπÂáªÊü•Áúã MiniCPM-Llama3-V 2.5 / MiniCPM-V 2.0 Âü∫‰∫éMac MPSËøêË°å (Apple silicon Êàñ AMD GPUs)ÁöÑÁ§∫‰æã„ÄÇ </summary>\n\n```python\n# test.py    Need more than 16GB memory to run.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, low_cpu_mem_usage=True)\nmodel = model.to(device='mps')\n\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\nmodel.eval()\n\nimage = Image.open('./assets/hk_OCR.jpg').convert('RGB')\nquestion = 'Where is this photo taken?'\nmsgs = [{'role': 'user', 'content': question}]\n\nanswer, context, _ = model.chat(\n    image=image,\n    msgs=msgs,\n    context=None,\n    tokenizer=tokenizer,\n    sampling=True\n)\nprint(answer)\n```\nËøêË°å:\n```shell\nPYTORCH_ENABLE_MPS_FALLBACK=1 python test.py\n```\n</details>\n\n\n### ÊâãÊú∫Á´ØÈÉ®ÁΩ≤\nMiniCPM-V 2.0 ÂèØËøêË°åÂú®AndroidÊâãÊú∫‰∏äÔºåÁÇπÂáª[MiniCPM-V 2.0](https://github.com/OpenBMB/mlc-MiniCPM)ÂÆâË£Öapk‰ΩøÁî®; \n\n### Êú¨Âú∞WebUI DemoÈÉ®ÁΩ≤\n<details>\n<summary>ÁÇπÂáªÊü•ÁúãÊú¨Âú∞WebUI demo Âú® NVIDIA GPU„ÄÅMacÁ≠â‰∏çÂêåËÆæÂ§áÈÉ®ÁΩ≤ÊñπÊ≥ï </summary>\n  \n```shell\npip install -r requirements.txt\n```\n  \n```shell\n# For NVIDIA GPUs, run:\npython web_demo_2.6.py --device cuda\n```\n</details>\n\n### llama.cpp ÈÉ®ÁΩ≤<a id=\"llamacpp-ÈÉ®ÁΩ≤\"></a>\nMiniCPM-V 2.6 Áé∞Âú®ÊîØÊåÅllama.cppÂï¶! Áî®Ê≥ïËØ∑ÂèÇËÄÉ[Êàë‰ª¨ÁöÑfork llama.cpp](https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md)Ôºå Âú®iPad‰∏äÂèØ‰ª•ÊîØÊåÅ 16~18 token/s ÁöÑÊµÅÁïÖÊé®ÁêÜÔºàÊµãËØïÁéØÂ¢ÉÔºöiPad Pro + M4Ôºâ„ÄÇ\n\n### ollama ÈÉ®ÁΩ≤<a id=\"ollama-ÈÉ®ÁΩ≤\"></a>\nMiniCPM-V 2.6 Áé∞Âú®ÊîØÊåÅollamaÂï¶! Áî®Ê≥ïËØ∑ÂèÇËÄÉ[Êàë‰ª¨ÁöÑfork ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md)Ôºå Âú®iPad‰∏äÂèØ‰ª•ÊîØÊåÅ 16~18 token/s ÁöÑÊµÅÁïÖÊé®ÁêÜÔºàÊµãËØïÁéØÂ¢ÉÔºöiPad Pro + M4Ôºâ„ÄÇ\n\n### vLLM ÈÉ®ÁΩ≤ <a id='vllm'></a>\n<details>\n<summary>ÁÇπÂáªÊü•Áúã, vLLM Áé∞Â∑≤ÂÆòÊñπÊîØÊåÅMiniCPM-V 2.6„ÄÅMiniCPM-Llama3-V 2.5 Âíå MiniCPM-V 2.0  </summary>\n\n1. ÂÆâË£Ö vLLM(>=0.5.4):\n```shell\npip install vllm\n```\n3. ÂÆâË£Ö timm Â∫ì: ÔºàÂèØÈÄâÔºåMiniCPM-V 2.0ÈúÄÂÆâË£ÖÔºâ\n```shell\npip install timm=0.9.10\n```\n4. ËøêË°åÁ§∫‰æã‰ª£Á†Å:ÔºàÊ≥®ÊÑèÔºöÂ¶ÇÊûú‰ΩøÁî®Êú¨Âú∞Ë∑ØÂæÑÁöÑÊ®°ÂûãÔºåËØ∑Á°Æ‰øùÊ®°Âûã‰ª£Á†ÅÂ∑≤Êõ¥Êñ∞Âà∞Hugging Face‰∏äÁöÑÊúÄÊñ∞Áâà)\n```python\nfrom transformers import AutoTokenizer\nfrom PIL import Image\nfrom vllm import LLM, SamplingParams\n\nMODEL_NAME = \"openbmb/MiniCPM-V-2_6\"\n# Also available for previous models\n# MODEL_NAME = \"openbmb/MiniCPM-Llama3-V-2_5\"\n# MODEL_NAME = \"HwwwH/MiniCPM-V-2\"\n\nimage = Image.open(\"xxx.png\").convert(\"RGB\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\nllm = LLM(\n    model=MODEL_NAME,\n    trust_remote_code=True,\n    gpu_memory_utilization=1,\n    max_model_len=2048\n)\n\nmessages = [{\n    \"role\":\n    \"user\",\n    \"content\":\n    # Number of images\n    \"(<image>./</image>)\" + \\\n    \"\\nWhat is the content of this image?\" \n}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Single Inference\ninputs = {\n    \"prompt\": prompt,\n    \"multi_modal_data\": {\n        \"image\": image\n        # Multi images, the number of images should be equal to that of `(<image>./</image>)`\n        # \"image\": [image, image] \n    },\n}\n# Batch Inference\n# inputs = [{\n#     \"prompt\": prompt,\n#     \"multi_modal_data\": {\n#         \"image\": image\n#     },\n# } for _ in 2]\n\n\n# 2.6\nstop_tokens = ['<|im_end|>', '<|endoftext|>']\nstop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n# 2.0\n# stop_token_ids = [tokenizer.eos_id]\n# 2.5\n# stop_token_ids = [tokenizer.eos_id, tokenizer.eot_id]\n\nsampling_params = SamplingParams(\n    stop_token_ids=stop_token_ids, \n    use_beam_search=True,\n    temperature=0, \n    best_of=3,\n    max_tokens=1024\n)\n\noutputs = llm.generate(inputs, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n```\n4. [ÁÇπÂáªÊ≠§Â§Ñ](https://modelbest.feishu.cn/wiki/C2BWw4ZP0iCDy7kkCPCcX2BHnOf?from=from_copylink)Êü•ÁúãÂ∏¶ËßÜÈ¢ëÊé®ÁêÜÂíåÂÖ∂‰ªñÊúâÂÖ≥ `vLLM` ÁöÑ‰ø°ÊÅØ„ÄÇ\n\n</details>\n\n\n## ÂæÆË∞É\n\n### ÁÆÄÊòìÂæÆË∞É <!-- omit in toc -->\n\nÊàë‰ª¨ÊîØÊåÅ‰ΩøÁî® Huggingface Transformers Â∫ìÁÆÄÊòìÂú∞ÂæÆË∞É MiniCPM-V 2.0 Âíå MiniCPM-Llama3-V 2.5 Ê®°Âûã„ÄÇ\n\n[ÂèÇËÄÉÊñáÊ°£](./finetune/readme.md)\n\n### ‰ΩøÁî® SWIFT Ê°ÜÊû∂ <!-- omit in toc -->\n\nÊàë‰ª¨ÊîØÊåÅ‰ΩøÁî® SWIFT Ê°ÜÊû∂ÂæÆË∞É MiniCPM-V Á≥ªÂàóÊ®°Âûã„ÄÇSWIFT ÊîØÊåÅËøë 200 ÁßçÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÅÊé®ÁêÜ„ÄÅËØÑÊµãÂíåÈÉ®ÁΩ≤„ÄÇÊîØÊåÅ PEFT Êèê‰æõÁöÑËΩªÈáèËÆ≠ÁªÉÊñπÊ°àÂíåÂÆåÊï¥ÁöÑ Adapters Â∫ìÊîØÊåÅÁöÑÊúÄÊñ∞ËÆ≠ÁªÉÊäÄÊúØÂ¶Ç NEFTune„ÄÅLoRA+„ÄÅLLaMA-PRO Á≠â„ÄÇ \n\nÂèÇËÄÉÊñáÊ°£Ôºö[MiniCPM-V 1.0](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-vÊúÄ‰Ω≥ÂÆûË∑µ.md)Ôºå[MiniCPM-V 2.0](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2ÊúÄ‰Ω≥ÂÆûË∑µ.md) [MiniCPM-V 2.6](https://github.com/modelscope/ms-swift/issues/1613).\n\n## FAQs\nÁÇπÂáªÊü•Áúã [FAQs](./docs/faqs.md)\n\n\n## Ê®°ÂûãÂçèËÆÆ <!-- omit in toc -->\n\n* Êú¨‰ªìÂ∫ì‰∏≠‰ª£Á†Å‰æùÁÖß [Apache-2.0](https://github.com/OpenBMB/MiniCPM/blob/main/LICENSE) ÂçèËÆÆÂºÄÊ∫ê\n* MiniCPM-V Ê®°ÂûãÊùÉÈáçÁöÑ‰ΩøÁî®ÂàôÈúÄË¶ÅÈÅµÂæ™ [‚ÄúMiniCPMÊ®°ÂûãÂïÜÁî®ËÆ∏ÂèØÂçèËÆÆ.md‚Äù](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%E6%A8%A1%E5%9E%8B%E5%95%86%E7%94%A8%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.md)„ÄÇ\n* MiniCPM Ê®°ÂûãÊùÉÈáçÂØπÂ≠¶ÊúØÁ†îÁ©∂ÂÆåÂÖ®ÂºÄÊîæÔºåÂú®Â°´ÂÜô[‚ÄúÈóÆÂç∑‚Äù](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g)ËøõË°åÁôªËÆ∞Âêé‰∫¶ÂÖÅËÆ∏ÂÖçË¥πÂïÜ‰∏ö‰ΩøÁî®„ÄÇ\n\n## Â£∞Êòé <!-- omit in toc -->\n\n‰Ωú‰∏∫Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºåMiniCPM-V Á≥ªÂàóÊ®°ÂûãÔºàÂåÖÊã¨ OmniLMMÔºâÈÄöËøáÂ≠¶‰π†Â§ßÈáèÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÊù•ÁîüÊàêÂÜÖÂÆπÔºå‰ΩÜÂÆÉÊó†Ê≥ïÁêÜËß£„ÄÅË°®Ëææ‰∏™‰∫∫ËßÇÁÇπÊàñ‰ª∑ÂÄºÂà§Êñ≠ÔºåÂÆÉÊâÄËæìÂá∫ÁöÑ‰ªª‰ΩïÂÜÖÂÆπÈÉΩ‰∏ç‰ª£Ë°®Ê®°ÂûãÂºÄÂèëËÄÖÁöÑËßÇÁÇπÂíåÁ´ãÂú∫„ÄÇ\n\nÂõ†Ê≠§Áî®Êà∑Âú®‰ΩøÁî®Êú¨È°πÁõÆÁöÑÁ≥ªÂàóÊ®°ÂûãÁîüÊàêÁöÑÂÜÖÂÆπÊó∂ÔºåÂ∫îËá™Ë°åË¥üË¥£ÂØπÂÖ∂ËøõË°åËØÑ‰º∞ÂíåÈ™åËØÅ„ÄÇÂ¶ÇÊûúÁî±‰∫é‰ΩøÁî®Êú¨È°πÁõÆÁöÑÁ≥ªÂàóÂºÄÊ∫êÊ®°ÂûãËÄåÂØºËá¥ÁöÑ‰ªª‰ΩïÈóÆÈ¢òÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÊï∞ÊçÆÂÆâÂÖ®ÈóÆÈ¢ò„ÄÅÂÖ¨ÂÖ±ËàÜËÆ∫È£éÈô©ÔºåÊàñÊ®°ÂûãË¢´ËØØÂØº„ÄÅÊª•Áî®„ÄÅ‰º†Êí≠Êàñ‰∏çÂΩìÂà©Áî®ÊâÄÂ∏¶Êù•ÁöÑ‰ªª‰ΩïÈ£éÈô©ÂíåÈóÆÈ¢òÔºåÊàë‰ª¨Â∞Ü‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ\n\n\n## Êú∫ÊûÑ <!-- omit in toc -->\n\nÊú¨È°πÁõÆÁî±‰ª•‰∏ãÊú∫ÊûÑÂÖ±ÂêåÂºÄÂèëÔºö\n\n- <img src=\"assets/thunlp.png\" width=\"28px\"> [Ê∏ÖÂçéÂ§ßÂ≠¶Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÆûÈ™åÂÆ§](https://nlp.csai.tsinghua.edu.cn/)\n- <img src=\"assets/modelbest.png\" width=\"28px\"> [Èù¢Â£ÅÊô∫ËÉΩ](https://modelbest.cn/)\n- <img src=\"assets/zhihu.webp\" width=\"28px\"> [Áü•‰πé](https://www.zhihu.com/ )\n\n## üåü Star History <!-- omit in toc -->\n\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/star_history.svg\"/>\n    </p>\n</table>\n\n<!-- <picture>\n  <source\n    media=\"(prefers-color-scheme: dark)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date&theme=dark\n    \"\n  />\n  <source\n    media=\"(prefers-color-scheme: light)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date\n    \"\n  />\n  <img\n    alt=\"Star History Chart\"\n    src=\"https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&type=Date\"\n  />\n</picture> -->\n\n## ÊîØÊåÅÊäÄÊúØÂíåÂÖ∂‰ªñÂ§öÊ®°ÊÄÅÈ°πÁõÆ <!-- omit in toc -->\n\nüëè Ê¨¢Ëøé‰∫ÜËß£ MiniCPM-V ËÉåÂêéÁöÑÊîØÊåÅÊäÄÊúØÂíåÊõ¥Â§öÊàë‰ª¨ÁöÑÂ§öÊ®°ÊÄÅÈ°πÁõÆÔºÅ\n\n[VisCPM](https://github.com/OpenBMB/VisCPM/tree/main) | [RLHF-V](https://github.com/RLHF-V/RLHF-V) | [LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD) | [RLAIF-V](https://github.com/RLHF-V/RLAIF-V)\n\n\n\n## ÂºïÁî® <!-- omit in toc -->\n\nÂ¶ÇÊûúÊÇ®ËßâÂæóÊàë‰ª¨Ê®°Âûã/‰ª£Á†Å/ËÆ∫ÊñáÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨ ‚≠ê Âíå ÂºïÁî® üìùÔºåÊÑüË∞¢ÔºÅ\n\n```bib\n@article{yao2024minicpm,\n  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\n  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\n  journal={arXiv preprint arXiv:2408.01800},\n  year={2024}\n}\n```\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "chat.py",
          "type": "blob",
          "size": 11,
          "content": "import os\nimport torch\nimport json\nfrom PIL import Image\nimport base64\nimport io\nfrom accelerate import load_checkpoint_and_dispatch, init_empty_weights\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom omnilmm.utils import disable_torch_init\nfrom omnilmm.model.omnilmm import OmniLMMForCausalLM\nfrom omnilmm.model.utils import build_transform\nfrom omnilmm.train.train_utils import omni_preprocess\n\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n\n    \n\ndef init_omni_lmm(model_path):\n    torch.backends.cuda.matmul.allow_tf32 = True\n    disable_torch_init()\n    model_name = os.path.expanduser(model_path)\n    print(f'Load omni_lmm model and tokenizer from {model_name}')\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name, model_max_length=2048)\n\n    if False:\n        # model on multiple devices for small size gpu memory (Nvidia 3090 24G x2) \n        with init_empty_weights():\n            model = OmniLMMForCausalLM.from_pretrained(model_name, tune_clip=True, torch_dtype=torch.bfloat16)\n        model = load_checkpoint_and_dispatch(model, model_name, dtype=torch.bfloat16, \n                    device_map=\"auto\",  no_split_module_classes=['Eva','MistralDecoderLayer', 'ModuleList', 'Resampler']\n        )\n    else:\n        model = OmniLMMForCausalLM.from_pretrained(\n            model_name, tune_clip=True, torch_dtype=torch.bfloat16\n        ).to(device='cuda', dtype=torch.bfloat16)\n\n    image_processor = build_transform(\n        is_train=False, input_size=model.model.config.image_size, std_mode='OPENAI_CLIP')\n\n    mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n    assert mm_use_im_start_end\n\n    tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN,\n                         DEFAULT_IM_END_TOKEN], special_tokens=True)\n\n\n    vision_config = model.model.vision_config\n    vision_config.im_patch_token = tokenizer.convert_tokens_to_ids(\n        [DEFAULT_IMAGE_PATCH_TOKEN])[0]\n    vision_config.use_im_start_end = mm_use_im_start_end\n    vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids(\n        [DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n    image_token_len = model.model.config.num_query\n\n    return model, image_processor, image_token_len, tokenizer\n\ndef expand_question_into_multimodal(question_text, image_token_len, im_st_token, im_ed_token, im_patch_token):\n    if '<image>' in question_text[0]['content']:\n        question_text[0]['content'] = question_text[0]['content'].replace(\n            '<image>', im_st_token + im_patch_token * image_token_len + im_ed_token)\n    else:\n        question_text[0]['content'] = im_st_token + im_patch_token * \\\n            image_token_len + im_ed_token + '\\n' + question_text[0]['content']\n    return question_text\n\ndef wrap_question_for_omni_lmm(question, image_token_len, tokenizer):\n    question = expand_question_into_multimodal(\n        question, image_token_len, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, DEFAULT_IMAGE_PATCH_TOKEN)\n\n    conversation = question\n    data_dict = omni_preprocess(sources=[conversation],\n                                  tokenizer=tokenizer,\n                                  generation=True)\n\n    data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n                     labels=data_dict[\"labels\"][0])\n    return data_dict\n\n\n\nclass OmniLMM12B:\n    def __init__(self, model_path) -> None:\n        model, img_processor, image_token_len, tokenizer = init_omni_lmm(model_path)\n        self.model = model\n        self.image_token_len = image_token_len\n        self.image_transform = img_processor\n        self.tokenizer = tokenizer\n        self.model.eval()\n\n    def decode(self, image, input_ids):\n        with torch.inference_mode():\n            output = self.model.generate_vllm(\n                input_ids=input_ids.unsqueeze(0).cuda(),\n                images=image.unsqueeze(0).half().cuda(),\n                temperature=0.6,\n                max_new_tokens=1024,\n                # num_beams=num_beams,\n                do_sample=True,\n                output_scores=True,\n                return_dict_in_generate=True,\n                repetition_penalty=1.1,\n                top_k=30,\n                top_p=0.9,\n            )\n\n            response = self.tokenizer.decode(\n                output.sequences[0], skip_special_tokens=True)\n            response = response.strip()\n            return response\n\n    def chat(self, input):\n        try:\n            image = Image.open(io.BytesIO(base64.b64decode(input['image']))).convert('RGB')\n        except Exception as e:\n            return \"Image decode error\"\n\n        msgs = json.loads(input['question'])\n        input_ids = wrap_question_for_omni_lmm(\n            msgs, self.image_token_len, self.tokenizer)['input_ids']\n        input_ids = torch.as_tensor(input_ids)\n        #print('input_ids', input_ids)\n        image = self.image_transform(image)\n\n        out = self.decode(image, input_ids)\n\n        return out\n        \n\ndef img2base64(file_name):\n    with open(file_name, 'rb') as f:\n        encoded_string = base64.b64encode(f.read())\n        return encoded_string\n\nclass MiniCPMV:\n    def __init__(self, model_path) -> None:\n        self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(dtype=torch.bfloat16)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n        self.model.eval().cuda()\n\n    def chat(self, input):\n        try:\n            image = Image.open(io.BytesIO(base64.b64decode(input['image']))).convert('RGB')\n        except Exception as e:\n            return \"Image decode error\"\n\n        msgs = json.loads(input['question'])\n        \n        answer, context, _ = self.model.chat(\n            image=image,\n            msgs=msgs,\n            context=None,\n            tokenizer=self.tokenizer,\n            sampling=True,\n            temperature=0.7\n    \t)\n        return answer\n\nclass MiniCPMV2_5:\n    def __init__(self, model_path) -> None:\n        self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(dtype=torch.float16)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n        self.model.eval().cuda()\n\n    def chat(self, input):\n        try:\n            image = Image.open(io.BytesIO(base64.b64decode(input['image']))).convert('RGB')\n        except Exception as e:\n            return \"Image decode error\"\n\n        msgs = json.loads(input['question'])\n        \n        answer = self.model.chat(\n            image=image,\n            msgs=msgs,\n            tokenizer=self.tokenizer,\n            sampling=True,\n            temperature=0.7\n    \t)\n        return answer\n\nclass MiniCPMV2_6:\n    def __init__(self, model_path, multi_gpus=False) -> None:\n\n        print('torch_version:', torch.__version__)\n        if multi_gpus: # inference on multi-gpus\n            from accelerate import load_checkpoint_and_dispatch, init_empty_weights, infer_auto_device_map\n            with init_empty_weights():\n                model = AutoModel.from_pretrained(model_path, trust_remote_code=True, \n                    attn_implementation='sdpa', torch_dtype=torch.bfloat16)\n\n            device_map = infer_auto_device_map(model, max_memory={0: \"10GB\", 1: \"10GB\"},\n                no_split_module_classes=['SiglipVisionTransformer', 'Qwen2DecoderLayer'])\n            device_id = device_map[\"llm.model.embed_tokens\"]\n            device_map[\"llm.lm_head\"] = device_id # first and last layer of llm should be in the same device\n            device_map[\"vpm\"] = device_id\n            device_map[\"resampler\"] = device_id\n            device_id2 = device_map[\"llm.model.layers.26\"]\n            device_map[\"llm.model.layers.8\"] = device_id2\n            device_map[\"llm.model.layers.9\"] = device_id2\n            device_map[\"llm.model.layers.10\"] = device_id2\n            device_map[\"llm.model.layers.11\"] = device_id2\n            device_map[\"llm.model.layers.12\"] = device_id2\n            device_map[\"llm.model.layers.13\"] = device_id2\n            device_map[\"llm.model.layers.14\"] = device_id2\n            device_map[\"llm.model.layers.15\"] = device_id2\n            device_map[\"llm.model.layers.16\"] = device_id2\n            print(device_map)\n\n            self.model = load_checkpoint_and_dispatch(model, model_path, dtype=torch.bfloat16, device_map=device_map)\n            self.model.eval()\n        else:\n            self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True,\n                attn_implementation='sdpa', torch_dtype=torch.bfloat16)\n            self.model.eval().cuda()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n    def chat(self, input):\n        image = None\n        if \"image\" in input and len(input[\"image\"]) > 10: # legacy API\n            try:\n                image = Image.open(io.BytesIO(base64.b64decode(input['image']))).convert('RGB')\n            except Exception as e:\n                return \"Image decode error\"\n\n        msgs = json.loads(input[\"question\"])\n\n        for msg in msgs:\n            contents = msg.pop('content') # support str or List[Dict]\n            if isinstance(contents, str):\n                contents = [contents]\n            \n            new_cnts = []\n            for c in contents:\n                if isinstance(c, dict):\n                    if c['type'] == 'text':\n                        c = c['pairs']\n                    elif c['type'] == 'image':\n                        c = Image.open(io.BytesIO(base64.b64decode(c[\"pairs\"]))).convert('RGB')\n                    else:\n                        raise ValueError(\"content type only support text and image.\")\n                new_cnts.append(c)\n            msg['content'] = new_cnts \n        print(f'msgs: {str(msgs)}')\n\n        answer = self.model.chat(\n            image=image,\n            msgs=msgs,\n            tokenizer=self.tokenizer,\n        )\n        return answer\n\n\nclass MiniCPMVChat:\n    def __init__(self, model_path, multi_gpus=False) -> None:\n        if '12B' in model_path:\n            self.model = OmniLMM12B(model_path)\n        elif 'MiniCPM-Llama3-V' in model_path:\n            self.model = MiniCPMV2_5(model_path)\n        elif 'MiniCPM-V-2_6' in model_path:\n            self.model = MiniCPMV2_6(model_path, multi_gpus)\n        else:\n            self.model = MiniCPMV(model_path)\n\n    def chat(self, input):\n        return self.model.chat(input)\n\n\nif __name__ == '__main__':\n    \n    model_path = 'openbmb/OmniLMM-12B'\n    chat_model = MiniCPMVChat(model_path)\n\n    im_64 = img2base64('./assets/worldmap_ck.jpg')\n\n    # first round chat \n    msgs = [{\"role\": \"user\", \"content\": \"What is interesting about this image?\"}]\n    input = {\"image\": im_64, \"question\": json.dumps(msgs, ensure_ascii=True)}\n    answer = chat_model.chat(input)\n    print(msgs[-1][\"content\"]+'\\n', answer)\n\n    # second round chat \n    msgs.append({\"role\": \"assistant\", \"content\": answer})\n    msgs.append({\"role\": \"user\", \"content\": \"Where is China in the image\"})\n    input = {\"image\": im_64,\"question\": json.dumps(msgs, ensure_ascii=True)}\n    answer = chat_model.chat(input)\n    print(msgs[-1][\"content\"]+'\\n', answer)\n\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval_mm",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "minicpm_v1.md",
          "type": "blob",
          "size": 5.8974609375,
          "content": "## MiniCPM-V 1.0\n\n\n> Archive atÔºö2024-05-19\n\nMiniCPM-V 1.0 is an efficient version with promising performance for deployment. The model is built based on SigLip-400M and [MiniCPM-2.4B](https://github.com/OpenBMB/MiniCPM/), connected by a perceiver resampler. Notable features of MiniCPM-V 1.0 include:\n\n- ‚ö°Ô∏è **High Efficiency.** \n\n  MiniCPM-V 1.0 can be **efficiently deployed on most GPU cards and personal computers**, and **even on end devices such as mobile phones**. In terms of visual encoding, we compress the image representations into 64 tokens via a perceiver resampler, which is significantly fewer than other LMMs based on MLP architecture (typically > 512 tokens). This allows MiniCPM-V 1.0 to operate with **much less memory cost and higher speed during inference**.\n\n- üî• **Promising Performance.** \n\n  MiniCPM-V 1.0 achieves **state-of-the-art performance** on multiple benchmarks (including MMMU, MME, and MMbech, etc) among models with comparable sizes, surpassing existing LMMs built on Phi-2. It even **achieves comparable or better performance than the 9.6B Qwen-VL-Chat**.\n\n- üôå **Bilingual Support.** \n\n  MiniCPM-V 1.0 is **the first end-deployable LMM supporting bilingual multimodal interaction in English and Chinese**. This is achieved by generalizing multimodal capabilities across languages, a technique from the ICLR 2024 spotlight [paper](https://arxiv.org/abs/2308.12038).\n\n### Evaluation\n\n<div align=\"center\">\n\n<table style=\"margin: 0px auto;\">\n<thead>\n  <tr>\n    <th align=\"left\">Model</th>\n    <th>Size</th>\n    <th nowrap=\"nowrap\" >Visual Tokens</th>\n    <th>MME</th>\n    <th nowrap=\"nowrap\" >MMB dev (en)</th>\n    <th nowrap=\"nowrap\" >MMB dev (zh)</th>\n    <th nowrap=\"nowrap\" >MMMU val</th>\n    <th nowrap=\"nowrap\" >CMMMU val</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td align=\"left\">LLaVA-Phi</td>\n    <td align=\"right\">3B</td>\n    <td>576</td>\n    <td>1335</td>\n    <td>59.8</td>\n    <td>- </td>\n    <td>- </td>\n    <td>- </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\">MobileVLM</td>\n    <td align=\"right\">3B</td>\n    <td>144</td>\n    <td>1289</td>\n    <td>59.6</td>\n    <td>- </td>\n    <td>- </td>\n    <td>- </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\" >Imp-v1</td>\n    <td align=\"right\">3B</td>\n    <td>576</td>\n    <td>1434</td>\n    <td>66.5</td>\n    <td>- </td>\n    <td>- </td>\n    <td>- </td>\n  </tr>\n  <tr>\n    <td  nowrap=\"nowrap\" align=\"left\" >Qwen-VL-Chat</td>\n    <td align=\"right\" >9.6B</td>\n    <td>256</td>\n    <td>1487</td>\n    <td>60.6 </td>\n    <td>56.7 </td>\n    <td>35.9 </td>\n    <td>30.7 </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\" >CogVLM</td>\n    <td align=\"right\">17.4B </td>\n    <td>1225</td>\n    <td>1438 </td>\n    <td>63.7 </td>\n    <td>53.8 </td>\n    <td>32.1 </td>\n    <td>- </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\" ><b>MiniCPM-V 1.0</b></td>\n    <td align=\"right\">3B </td>\n    <td>64</td>\n    <td>1452 </td>\n    <td>67.9 </td>\n    <td>65.3 </td>\n    <td>37.2 </td>\n    <td>32.1 </td>\n  </tr>\n</tbody>\n</table>\n\n</div>\n\n### Examples\n\nWe deploy MiniCPM-V 1.0 on end devices. The demo video is the raw screen recording on a OnePlus 9R without edition.\n\n<table align=\"center\">\n    <p align=\"center\">\n      <img src=\"assets/gif_cases/Ëõá_cn.gif\" width=36%/>\n      <img src=\"assets/gif_cases/Mushroom_en.gif\" width=36%/>\n    </p>\n</table>\n\n## Install\n\n1. Clone this repository and navigate to the source folder\n\n```bash\ngit clone https://github.com/OpenBMB/OmniLMM.git\ncd OmniLMM\n```\n\n2. Create conda environment\n\n```Shell\nconda create -n OmniLMM python=3.10 -y\nconda activate OmniLMM\n```\n\n3. Install dependencies\n\n```shell\npip install -r requirements.txt\n```\n\n## Inference\n\n### Model Zoo\n| Model                | Description       | Download Link |\n|:----------------------|:-------------------|:---------------:|\n| MiniCPM-V 1.0  | The efficient version for end device deployment.    |  [ü§ó](https://huggingface.co/openbmb/MiniCPM-V) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/MiniCPM-V/files) |\n\n\n### Multi-turn Conversation\nPlease refer to the following codes to run `MiniCPM-V 1.0`.\n\n<div align=\"center\">\n<img src=\"assets/worldmap_ck.jpg\" width=\"500px\">\n</div>\n\n\n```python\nfrom chat import OmniLMMChat, img2base64\n\nchat_model = OmniLMMChat('openbmb/MiniCPM-V')\n\nim_64 = img2base64('./assets/worldmap_ck.jpg')\n\n# First round chat \nmsgs = [{\"role\": \"user\", \"content\": \"What is interesting about this image?\"}]\n\ninputs = {\"image\": im_64, \"question\": json.dumps(msgs)}\nanswer = chat_model.chat(inputs)\nprint(answer)\n\n# Second round chat \n# pass history context of multi-turn conversation\nmsgs.append({\"role\": \"assistant\", \"content\": answer})\nmsgs.append({\"role\": \"user\", \"content\": \"Where is China in the image\"})\n\ninputs = {\"image\": im_64, \"question\": json.dumps(msgs)}\nanswer = chat_model.chat(inputs)\nprint(answer)\n```\n\n\n### Inference on Mac\n<details>\n<summary>Click to view example, MiniCPM-V 1.0 can run on Mac with MPS (Apple silicon or AMD GPUs). </summary>\n\n```python\n# test.py\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to(device='mps', dtype=torch.float16)\n\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True)\nmodel.eval()\n\nimage = Image.open('./assets/worldmap_ck.jpg').convert('RGB')\nquestion = 'What is interesting about this image?'\nmsgs = [{'role': 'user', 'content': question}]\n\nanswer, context, _ = model.chat(\n    image=image,\n    msgs=msgs,\n    context=None,\n    tokenizer=tokenizer,\n    sampling=True\n)\nprint(answer)\n```\nRun with command:\n```shell\nPYTORCH_ENABLE_MPS_FALLBACK=1 python test.py\n```\n</details>\n\n### Deployment on Mobile Phone\n\nCurrently MiniCPM-V 1.0 can be deployed on mobile phones with Android and Harmony operating systems. üöÄ Try it out [here](https://github.com/OpenBMB/mlc-MiniCPM).\n"
        },
        {
          "name": "omnilmm.md",
          "type": "blob",
          "size": 5.4306640625,
          "content": "## OmniLMM-12B\n\n> OmniLMM-12B ÂèëÂ∏É‰∫éÊú¨È°πÁõÆÊó©Êúü„ÄÇÊé®ËçêÊÇ®‰ΩøÁî®Êàë‰ª¨[ÊúÄÊñ∞ÂèëÂ∏ÉÁöÑÊ®°Âûã](./README_zh.md)Ôºå‰ª•Ëé∑ÂæóÊõ¥È´òÊïàÁöÑÊé®ÁêÜÂíåÊõ¥Âº∫Â§ßÁöÑÊÄßËÉΩ‰ΩìÈ™å„ÄÇ\n\n> ÂΩíÊ°£Êó∂Èó¥Ôºö2024-05-19\n\n**OmniLMM-12B** ÊòØÂΩìÂâçÁ≥ªÂàó‰∏≠ÊÄßËÉΩÊúÄ‰Ω≥ÁöÑÁâàÊú¨„ÄÇËØ•Ê®°ÂûãÂü∫‰∫éEVA02-5BÂíåZephyr-7B-Œ≤ÂàùÂßãÂåñÊûÑÂª∫ÔºåÂπ∂‰ΩøÁî®perceiver resamplerËøûÊé•ÔºåÈááÁî®‰∫ÜËØæÁ®ãÂ≠¶‰π†ÁöÑÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇËØ•Ê®°ÂûãÂÖ∑Êúâ‰∏â‰∏™ÁâπÁÇπÔºö\n\n- üî• **ÊÄßËÉΩÈ¢ÜÂÖà„ÄÇ**\n\n  OmniLMM-12B Áõ∏ÊØîÂÖ∂‰ªñÂêåËßÑÊ®°Ê®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó**È¢ÜÂÖàÁöÑÊÄßËÉΩ**ÔºàÂåÖÊã¨ MME„ÄÅMMBench„ÄÅSEED-Bench Á≠âÔºâÔºåÊ®°ÂûãÊéåÊè°‰∫ÜËæÉ‰∏∫‰∏∞ÂØåÁöÑÂ§öÊ®°ÊÄÅ‰∏ñÁïåÁü•ËØÜ„ÄÇ\n\n- üèÜ **Ë°å‰∏∫ÂèØ‰ø°„ÄÇ**\n\n  Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁöÑÂπªËßâÈóÆÈ¢òÂ§áÂèóÂÖ≥Ê≥®ÔºåÊ®°ÂûãÁªèÂ∏∏ÁîüÊàêÂíåÂõæÂÉè‰∏≠ÁöÑ‰∫ãÂÆû‰∏çÁ¨¶ÁöÑÊñáÊú¨Ôºà‰æãÂ¶ÇÔºåÁ°Æ‰ø°Âú∞ÊèèËø∞ÂõæÁâá‰∏≠Âπ∂‰∏çÂ≠òÂú®ÁöÑÁâ©‰ΩìÔºâ„ÄÇOmniLMM-12BÊòØ **Á¨¨‰∏Ä‰∏™ÈÄöËøáÂ§öÊ®°ÊÄÅ RLHF ÂØπÈΩêÁöÑÁªºÂêàËÉΩÂäõ‰ºòÁßÄÁöÑÂºÄÊ∫êÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã**ÔºàÂÄüÂä© [RLHF-V](https://rlhf-v.github.io/) [CVPR'24] Á≥ªÂàóÊäÄÊúØÔºâ„ÄÇËØ•Ê®°ÂûãÂú® [MMHal-Bench](https://huggingface.co/datasets/Shengcao1006/MMHal-Bench) ÂπªËßâËØÑÊµãÂü∫ÂáÜ‰∏äËææÂà∞**ÂºÄÊ∫êÊ®°ÂûãÊúÄ‰Ω≥Ê∞¥Âπ≥**ÔºåÂπ∂Âú® [Object HalBench](https://arxiv.org/abs/2312.00849) ‰∏≠**‰ºò‰∫éGPT-4V**„ÄÇ\n\n- üïπ **ÂÆûÊó∂Â§öÊ®°ÊÄÅ‰∫§‰∫í„ÄÇ**\n\n  Êàë‰ª¨Â∞ùËØïÁªìÂêàOmniLMM-12BÂíåGPT-3.5 (Á∫ØÊñáÊú¨Ê®°Âûã) ÔºåÂÆûÁé∞**ÂÆûÊó∂Â§öÊ®°ÊÄÅ‰∫§‰∫íÂä©Êâã**„ÄÇËØ•Ê®°ÂûãÊé•ÂèóÊù•Ëá™ÊëÑÂÉèÂ§¥ÁöÑËßÜÈ¢ëÊµÅÔºåÂπ∂ÂÄüÂä©Â∑•ÂÖ∑Â§ÑÁêÜËØ≠Èü≥ËæìÂÖ•ËæìÂá∫„ÄÇËôΩÁÑ∂ËøòÂæàÂàùÊ≠•ÔºåÊàë‰ª¨ÂèëÁé∞ËØ•Ê®°ÂûãÊó†ÈúÄËßÜÈ¢ëÁºñËæëÂèØ‰ª•**Â§çÁé∞GeminiÊºîÁ§∫ËßÜÈ¢ë‰∏≠ÁöÑ‰∏Ä‰∫õÊúâË∂£‰æãÂ≠ê**„ÄÇ\n\n### ËØÑÊµãÁªìÊûú <!-- omit in toc -->\n\n<div align=\"center\">\n    <img src=assets/radar_omnilmm12b.png width=66% />\n</div>\n<details>\n<summary> MME, MMBench, MMMU, MMBench, MMHal-Bench, Object HalBench, SeedBench, LLaVA Bench W, MathVista ‰∏äÁöÑËØ¶ÁªÜËØÑÊµãÁªìÊûú„ÄÇ </summary>\n\n<table>\n<thead>\n  <tr>\n    <th align=\"left\">Model</th>\n    <th>Size</th>\n    <th>MME</th>\n    <th nowrap=\"nowrap\">MMB dev (en)</th>\n    <th nowrap=\"nowrap\" >MMMU val</th>\n    <th nowrap=\"nowrap\" >MMHal-Bench</th>\n    <th nowrap=\"nowrap\" >Object HalBench</th>\n    <th nowrap=\"nowrap\" >SeedBench-I</th>\n    <th>MathVista</th>\n    <th nowrap=\"nowrap\" >LLaVA Bench</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td align=\"left\">GPT-4V‚Ä†</td>\n    <td>-</td>\n    <td>1771.5</td>\n    <td>75.1 </td>\n    <td>56.8</td>\n    <td>3.53 / 70.8</td>\n    <td>86.4 / 92.7</td>\n    <td>71.6 </td>\n    <td>47.8 </td>\n    <td>93.1 </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\">Qwen-VL-Plus‚Ä†</td>\n    <td>-</td>\n    <td>2183.4</td>\n    <td>66.2 </td>\n    <td>45.2</td>\n    <td>- </td>\n    <td>- </td>\n    <td>65.7 </td>\n    <td>36.0 </td>\n    <td>73.7 </td>\n  </tr>\n  <tr>\n    <td align=\"left\">Yi-VL 6B</td>\n    <td align=\"right\">6.7B </td>\n    <td>1915.1 </td>\n    <td>68.6 </td>\n    <td>40.3 </td>\n    <td>- </td>\n    <td>- </td>\n    <td>67.5 </td>\n    <td>28.8 </td>\n    <td>51.9 </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\" >Qwen-VL-Chat</td>\n    <td align=\"right\">9.6B</td>\n    <td>1860.0</td>\n    <td>60.6 </td>\n    <td>35.9</td>\n    <td>2.93 / 59.4</td>\n    <td>56.2 / 80.0</td>\n    <td>64.8 </td>\n    <td>33.8 </td>\n    <td>67.7 </td>\n  </tr>\n  <tr>\n    <td align=\"left\" >CogVLM-Chat</td>\n    <td align=\"right\">17.4B</td>\n    <td>1736.6</td>\n    <td>63.7 </td>\n    <td>32.1 </td>\n    <td>2.68 / 52.1 </td>\n    <td>73.6 / 87.4 </td>\n    <td>68.8 </td>\n    <td>34.7 </td>\n    <td>73.9 </td>\n  </tr>\n  <tr>\n    <td align=\"left\" >LLaVA 1.5</td>\n    <td align=\"right\">13.6B </td>\n    <td>1808.4 </td>\n    <td>68.2 </td>\n    <td>36.4 </td>\n    <td>2.71 / 51.0 </td>\n    <td>53.7 / 77.4 </td>\n    <td>68.1 </td>\n    <td>26.4 </td>\n    <td>64.6 </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\" ><b>OmniLMM-12B</b></td>\n    <td align=\"right\">11.6B </td>\n    <td>1935.8 </td>\n    <td>71.6 </td>\n    <td>40.7 </td>\n    <td>3.45 / 68.8 </td>\n    <td>90.3 / 95.5 </td>\n    <td>71.1 </td>\n    <td>34.9 </td>\n    <td>72.0 </td>\n  </tr>\n</tbody>\n</table>\n<small>‚Ä†: Èó≠Ê∫êÊ®°Âûã</small>\n<br>\n</details>\n\n### ÂÖ∏ÂûãÁ§∫‰æã <!-- omit in toc -->\n\n<table align=\"center\" >\n  <p align=\"center\" > \n    <img src=\"assets/omnilmm-12b-examples_2.png\" />\n  </p>\n</table>\n\n\nÊàë‰ª¨ÁªìÂêà OmniLMM-12B Âíå ChatGPT-3.5 (Á∫ØÊñáÊú¨Ê®°Âûã) Â∞ùËØïÊûÑÂª∫ **ÂÆûÊó∂Â§öÊ®°ÊÄÅ‰∫§‰∫íÂä©Êâã**. OmniLMM-12B Â∞ÜËßÜÈ¢ëÂ∏ßËΩ¨‰∏∫ÂØπÂ∫îÁöÑÂõæÂÉèÊèèËø∞Âπ∂ËæìÂÖ•ÁªôChatGPT-3.5Êù•ÁîüÊàêÂØπÁî®Êà∑Êåá‰ª§ÁöÑÂìçÂ∫î„ÄÇÊºîÁ§∫ËßÜÈ¢ëÊú™ÁªèÁºñËæë„ÄÇ\n\n<div align=\"center\" >\n  <video controls src=\"https://github.com/OpenBMB/OmniLMM/assets/157115220/8fec13bf-bb47-4bf8-8f8c-d0b716a964ec\" type=\"video/mp4\" width=80%/>\n</div>\n\n## Online Demo\n\nÊ¨¢ËøéÈÄöËøá‰ª•‰∏ãÈìæÊé•‰ΩøÁî®Êàë‰ª¨ÁöÑÁΩëÈ°µÁ´ØÊé®ÁêÜÊúçÂä°Ôºö [OmniLMM-12B](http://120.92.209.146:8081) ÔΩú [MiniCPM-V 2.0](http://120.92.209.146:80).\n\n## ÂÆâË£Ö\n\n1. ÂÖãÈöÜÊàë‰ª¨ÁöÑ‰ªìÂ∫ìÂπ∂Ë∑≥ËΩ¨Âà∞Áõ∏Â∫îÁõÆÂΩï\n\n```bash\ngit clone https://github.com/OpenBMB/MiniCPM-V.git\ncd MiniCPM-V\n```\n\n1. ÂàõÂª∫ conda ÁéØÂ¢É\n\n```Shell\nconda create -n MiniCPMV python=3.10 -y\nconda activate MiniCPMV\n```\n\n3. ÂÆâË£Ö‰æùËµñ\n\n```shell\npip install -r requirements.txt\n```\n\n## Êé®ÁêÜ\n\n### Ê®°ÂûãÂ∫ì\n\n| Ê®°Âûã                | ÁÆÄ‰ªã       | ‰∏ãËΩΩÈìæÊé• |\n|:----------------------|:-------------------|:---------------:|\n| OmniLMM-12B | ÊÄßËÉΩÊúÄÂº∫ÁöÑÁâàÊú¨                   |  [ü§ó](https://huggingface.co/openbmb/OmniLMM-12B) &nbsp;&nbsp;  [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/OmniLMM-12B/files) |\n\n"
        },
        {
          "name": "omnilmm",
          "type": "tree",
          "content": null
        },
        {
          "name": "omnilmm_en.md",
          "type": "blob",
          "size": 5.1396484375,
          "content": "## OmniLMM-12B\n\n> OmniLMM-12B is released at early time of this project. We recommond you to use our [recently released models](./README_en.md), for better performance and efficiency.\n\n> Archieve at: 2024-05-19\n\n\n**OmniLMM-12B** is the most capable version. The model is built based on EVA02-5B and Zephyr-7B-Œ≤, connected with a perceiver resampler layer, and trained on multimodal data in a curriculum fashion. The model has three notable features:\n\n- üî• **Strong Performance.** \n\n  OmniLMM-12B achieves **leading performance** among models with comparable sizes, surpassing established LMMs on multiple benchmarks (including MME, MMBench, SEED-Bench, etc). The model also endows rich multi-modal world knowledge.\n\n- üèÜ **Trustworthy Behavior.** \n\n  LMMs are known for suffering from hallucination, often generating text that is not factually grounded in images (e.g., faithfully describing non-existing objects in images). OmniLMM-12B is **the first state-of-the-art open-source LMM aligned via multimodal RLHF for trustworthy behavior** (using the recent [RLHF-V](https://rlhf-v.github.io/) technique). It **ranks #1** among open-source models on [MMHal-Bench](https://huggingface.co/datasets/Shengcao1006/MMHal-Bench), and **outperforms GPT-4V** on [Object HalBench](https://arxiv.org/abs/2312.00849).\n\n- üïπ **Real-time Multimodal Interaction.** \n\n  We combine the OmniLMM-12B and GPT-3.5 (text-only) into a **real-time multimodal interactive assistant**. The assistant accepts video streams from the camera and speech streams from the microphone and emits speech output. While still primary, we find the model can **replicate some of the fun cases shown in the Gemini Demo video, without any video edition**.\n\n\n### Evaluation <!-- omit in toc -->\n<div align=\"center\">\n    <img src=assets/radar_omnilmm12b.png width=66% />\n</div>\n<details>\n<summary>Click to view results on MME, MMBench, MMMU, MMBench, MMHal-Bench, Object HalBench, SeedBench, LLaVA Bench, MathVista. </summary>\n\n<table>\n<thead>\n  <tr>\n    <th align=\"left\">Model</th>\n    <th>Size</th>\n    <th>MME</th>\n    <th nowrap=\"nowrap\">MMB dev (en)</th>\n    <th nowrap=\"nowrap\" >MMMU val</th>\n    <th nowrap=\"nowrap\" >MMHal-Bench</th>\n    <th nowrap=\"nowrap\" >Object HalBench</th>\n    <th nowrap=\"nowrap\" >SeedBench-I</th>\n    <th>MathVista</th>\n    <th nowrap=\"nowrap\" >LLaVA Bench</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td align=\"left\">GPT-4V‚Ä†</td>\n    <td>-</td>\n    <td>1771.5</td>\n    <td>75.1 </td>\n    <td>56.8</td>\n    <td>3.53 / 70.8</td>\n    <td>86.4 / 92.7</td>\n    <td>71.6 </td>\n    <td>47.8 </td>\n    <td>93.1 </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\">Qwen-VL-Plus‚Ä†</td>\n    <td>-</td>\n    <td>2183.4</td>\n    <td>66.2 </td>\n    <td>45.2</td>\n    <td>- </td>\n    <td>- </td>\n    <td>65.7 </td>\n    <td>36.0 </td>\n    <td>73.7 </td>\n  </tr>\n  <tr>\n    <td align=\"left\">Yi-VL 6B</td>\n    <td align=\"right\">6.7B </td>\n    <td>1915.1 </td>\n    <td>68.6 </td>\n    <td>40.3 </td>\n    <td>- </td>\n    <td>- </td>\n    <td>67.5 </td>\n    <td>28.8 </td>\n    <td>51.9 </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\" >Qwen-VL-Chat</td>\n    <td align=\"right\">9.6B</td>\n    <td>1860.0</td>\n    <td>60.6 </td>\n    <td>35.9</td>\n    <td>2.93 / 59.4</td>\n    <td>56.2 / 80.0</td>\n    <td>64.8 </td>\n    <td>33.8 </td>\n    <td>67.7 </td>\n  </tr>\n  <tr>\n    <td align=\"left\" >CogVLM-Chat</td>\n    <td align=\"right\">17.4B</td>\n    <td>1736.6</td>\n    <td>63.7 </td>\n    <td>32.1 </td>\n    <td>2.68 / 52.1 </td>\n    <td>73.6 / 87.4 </td>\n    <td>68.8 </td>\n    <td>34.7 </td>\n    <td>73.9 </td>\n  </tr>\n  <tr>\n    <td align=\"left\" >LLaVA 1.5</td>\n    <td align=\"right\">13.6B </td>\n    <td>1808.4 </td>\n    <td>68.2 </td>\n    <td>36.4 </td>\n    <td>2.71 / 51.0 </td>\n    <td>53.7 / 77.4 </td>\n    <td>68.1 </td>\n    <td>26.4 </td>\n    <td>64.6 </td>\n  </tr>\n  <tr>\n    <td nowrap=\"nowrap\" align=\"left\" ><b>OmniLMM-12B</b></td>\n    <td align=\"right\">11.6B </td>\n    <td>1935.8 </td>\n    <td>71.6 </td>\n    <td>40.7 </td>\n    <td>3.45 / 68.8 </td>\n    <td>90.3 / 95.5 </td>\n    <td>71.1 </td>\n    <td>34.9 </td>\n    <td>72.0 </td>\n  </tr>\n</tbody>\n</table>\n<small>‚Ä†: Proprietary models</small>\n<br>\n</details>\n\n### Examples <!-- omit in toc -->\n\n<table align=\"center\" >\n  <p align=\"center\" > \n    <img src=\"assets/omnilmm-12b-examples_2.png\" />\n  </p>\n</table>\n\n\nWe combine the OmniLMM-12B and GPT-3.5 (text-only) into a **real-time multimodal interactive assistant**. Video frames are described in text using OmniLMM-12B, and ChatGPT 3.5 (text-only) is employed to generate response according to the descriptions and user prompts. The demo video is a raw recording without edition. \n\n<div align=\"center\" >\n  <video controls src=\"https://github.com/OpenBMB/OmniLMM/assets/157115220/485a8f52-fb4d-4eca-8fee-506347efcfc6\" type=\"video/mp4\" width=80%/>\n</div>\n\n### Model Zoo\n\n| Model                | Description       | Download Link |\n|:----------------------|:-------------------|:---------------:|\n| OmniLMM-12B | The most capable version with leading performance.   |  [ü§ó](https://huggingface.co/openbmb/OmniLMM-12B) &nbsp;&nbsp; [<img src=\"./assets/modelscope_logo.png\" width=\"20px\"></img>](https://modelscope.cn/models/OpenBMB/OmniLMM-12B/files) |\n"
        },
        {
          "name": "quantize",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.6845703125,
          "content": "packaging==23.2\naddict==2.4.0\neditdistance==0.6.2\neinops==0.7.0\nfairscale==0.4.0\njsonlines==4.0.0\nmarkdown2==2.4.10\nmatplotlib==3.7.4\nmore_itertools==10.1.0\nnltk==3.8.1\nnumpy==1.24.4\nopencv_python_headless==4.5.5.64\nopenpyxl==3.1.2\nPillow==10.1.0\nsacrebleu==2.3.2\nseaborn==0.13.0\nshortuuid==1.0.11\n#spacy==3.7.2\ntimm==0.9.10\ntorch==2.1.2\ntorchvision==0.16.2\ntqdm==4.66.1\nprotobuf==4.25.0\ntransformers==4.40.0\ntyping_extensions==4.8.0\nuvicorn==0.24.0.post1\n#xformers==0.0.22.post7\n#flash_attn==2.3.4\nsentencepiece==0.1.99\naccelerate==0.30.1\nsocksio==1.0.0\ngradio==4.41.0\ngradio_client\nhttp://thunlp.oss-cn-qingdao.aliyuncs.com/multi_modal/never_delete/modelscope_studio-0.4.0.9-py3-none-any.whl\ndecord\n"
        },
        {
          "name": "web_demo.py",
          "type": "blob",
          "size": 8.275390625,
          "content": "#!/usr/bin/env python\n# encoding: utf-8\nimport gradio as gr\nfrom PIL import Image\nimport traceback\nimport re\nimport torch\nimport argparse\nfrom transformers import AutoModel, AutoTokenizer\n\n# README, How to run demo on different devices\n# For Nvidia GPUs support BF16 (like A100, H100, RTX3090)\n# python web_demo.py --device cuda --dtype bf16\n\n# For Nvidia GPUs do NOT support BF16 (like V100, T4, RTX2080)\n# python web_demo.py --device cuda --dtype fp16\n\n# For Mac with MPS (Apple silicon or AMD GPUs).\n# PYTORCH_ENABLE_MPS_FALLBACK=1 python web_demo.py --device mps --dtype fp16\n\n# Argparser\nparser = argparse.ArgumentParser(description='demo')\nparser.add_argument('--device', type=str, default='cuda', help='cuda or mps')\nparser.add_argument('--dtype', type=str, default='bf16', help='bf16 or fp16')\nargs = parser.parse_args()\ndevice = args.device\nassert device in ['cuda', 'mps']\nif args.dtype == 'bf16':\n    if device == 'mps':\n        print('Warning: MPS does not support bf16, will use fp16 instead')\n        dtype = torch.float16\n    else:\n        dtype = torch.bfloat16\nelse:\n    dtype = torch.float16\n\n# Load model\nmodel_path = 'openbmb/MiniCPM-V-2'\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\nmodel = model.to(device=device, dtype=dtype)\nmodel.eval()\n\n\n\nERROR_MSG = \"Error, please retry\"\nmodel_name = 'MiniCPM-V 2.0'\n\nform_radio = {\n    'choices': ['Beam Search', 'Sampling'],\n    #'value': 'Beam Search',\n    'value': 'Sampling',\n    'interactive': True,\n    'label': 'Decode Type'\n}\n# Beam Form\nnum_beams_slider = {\n    'minimum': 0,\n    'maximum': 5,\n    'value': 3,\n    'step': 1,\n    'interactive': True,\n    'label': 'Num Beams'\n}\nrepetition_penalty_slider = {\n    'minimum': 0,\n    'maximum': 3,\n    'value': 1.2,\n    'step': 0.01,\n    'interactive': True,\n    'label': 'Repetition Penalty'\n}\nrepetition_penalty_slider2 = {\n    'minimum': 0,\n    'maximum': 3,\n    'value': 1.05,\n    'step': 0.01,\n    'interactive': True,\n    'label': 'Repetition Penalty'\n}\nmax_new_tokens_slider = {\n    'minimum': 1,\n    'maximum': 4096,\n    'value': 1024,\n    'step': 1,\n    'interactive': True,\n    'label': 'Max New Tokens'    \n}\n\ntop_p_slider = {\n    'minimum': 0,\n    'maximum': 1,\n    'value': 0.8,\n    'step': 0.05,\n    'interactive': True,\n    'label': 'Top P'    \n}\ntop_k_slider = {\n    'minimum': 0,\n    'maximum': 200,\n    'value': 100,\n    'step': 1,\n    'interactive': True,\n    'label': 'Top K'    \n}\ntemperature_slider = {\n    'minimum': 0,\n    'maximum': 2,\n    'value': 0.7,\n    'step': 0.05,\n    'interactive': True,\n    'label': 'Temperature'    \n}\n\n\ndef create_component(params, comp='Slider'):\n    if comp == 'Slider':\n        return gr.Slider(\n            minimum=params['minimum'],\n            maximum=params['maximum'],\n            value=params['value'],\n            step=params['step'],\n            interactive=params['interactive'],\n            label=params['label']\n        )\n    elif comp == 'Radio':\n        return gr.Radio(\n            choices=params['choices'],\n            value=params['value'],\n            interactive=params['interactive'],\n            label=params['label']\n        )\n    elif comp == 'Button':\n        return gr.Button(\n            value=params['value'],\n            interactive=True\n        )\n\n\ndef chat(img, msgs, ctx, params=None, vision_hidden_states=None):\n    default_params = {\"num_beams\":3, \"repetition_penalty\": 1.2, \"max_new_tokens\": 1024}\n    if params is None:\n        params = default_params\n    if img is None:\n        return -1, \"Error, invalid image, please upload a new image\", None, None\n    try:\n        image = img.convert('RGB')\n        answer, context, _ = model.chat(\n            image=image,\n            msgs=msgs,\n            context=None,\n            tokenizer=tokenizer,\n            **params\n        )\n        res = re.sub(r'(<box>.*</box>)', '', answer)\n        res = res.replace('<ref>', '')\n        res = res.replace('</ref>', '')\n        res = res.replace('<box>', '')\n        answer = res.replace('</box>', '')\n        return 0, answer, None, None\n    except Exception as err:\n        print(err)\n        traceback.print_exc()\n        return -1, ERROR_MSG, None, None\n\n\ndef upload_img(image, _chatbot, _app_session):\n    image = Image.fromarray(image)\n\n    _app_session['sts']=None\n    _app_session['ctx']=[]\n    _app_session['img']=image \n    _chatbot.append(('', 'Image uploaded successfully, you can talk to me now'))\n    return _chatbot, _app_session\n\n\ndef respond(_question, _chat_bot, _app_cfg, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature):\n    if _app_cfg.get('ctx', None) is None:\n        _chat_bot.append((_question, 'Please upload an image to start'))\n        return '', _chat_bot, _app_cfg\n\n    _context = _app_cfg['ctx'].copy()\n    if _context:\n        _context.append({\"role\": \"user\", \"content\": _question})\n    else:\n        _context = [{\"role\": \"user\", \"content\": _question}] \n    print('<User>:', _question)\n\n    if params_form == 'Beam Search':\n        params = {\n            'sampling': False,\n            'num_beams': num_beams,\n            'repetition_penalty': repetition_penalty,\n            \"max_new_tokens\": 896 \n        }\n    else:\n        params = {\n            'sampling': True,\n            'top_p': top_p,\n            'top_k': top_k,\n            'temperature': temperature,\n            'repetition_penalty': repetition_penalty_2,\n            \"max_new_tokens\": 896 \n        }\n    code, _answer, _, sts = chat(_app_cfg['img'], _context, None, params)\n    print('<Assistant>:', _answer)\n\n    _context.append({\"role\": \"assistant\", \"content\": _answer}) \n    _chat_bot.append((_question, _answer))\n    if code == 0:\n        _app_cfg['ctx']=_context\n        _app_cfg['sts']=sts\n    return '', _chat_bot, _app_cfg\n\n\ndef regenerate_button_clicked(_question, _chat_bot, _app_cfg, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature):\n    if len(_chat_bot) <= 1:\n        _chat_bot.append(('Regenerate', 'No question for regeneration.'))\n        return '', _chat_bot, _app_cfg\n    elif _chat_bot[-1][0] == 'Regenerate':\n        return '', _chat_bot, _app_cfg\n    else:\n        _question = _chat_bot[-1][0]\n        _chat_bot = _chat_bot[:-1]\n        _app_cfg['ctx'] = _app_cfg['ctx'][:-2]\n    return respond(_question, _chat_bot, _app_cfg, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature)\n\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column(scale=1, min_width=300):\n            params_form = create_component(form_radio, comp='Radio')\n            with gr.Accordion(\"Beam Search\") as beams_according:\n                num_beams = create_component(num_beams_slider)\n                repetition_penalty = create_component(repetition_penalty_slider)\n            with gr.Accordion(\"Sampling\") as sampling_according:\n                top_p = create_component(top_p_slider)\n                top_k = create_component(top_k_slider)\n                temperature = create_component(temperature_slider)\n                repetition_penalty_2 = create_component(repetition_penalty_slider2)\n            regenerate = create_component({'value': 'Regenerate'}, comp='Button')\n        with gr.Column(scale=3, min_width=500):\n            app_session = gr.State({'sts':None,'ctx':None,'img':None})\n            bt_pic = gr.Image(label=\"Upload an image to start\")\n            chat_bot = gr.Chatbot(label=f\"Chat with {model_name}\")\n            txt_message = gr.Textbox(label=\"Input text\")\n            \n            regenerate.click(\n                regenerate_button_clicked,\n                [txt_message, chat_bot, app_session, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature],\n                [txt_message, chat_bot, app_session]\n            )\n            txt_message.submit(\n                respond, \n                [txt_message, chat_bot, app_session, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature], \n                [txt_message, chat_bot, app_session]\n            )\n            bt_pic.upload(lambda: None, None, chat_bot, queue=False).then(upload_img, inputs=[bt_pic,chat_bot,app_session], outputs=[chat_bot,app_session])\n\n# launch\ndemo.launch(share=False, debug=True, show_api=False, server_port=8080, server_name=\"0.0.0.0\")\n\n"
        },
        {
          "name": "web_demo_2.5.py",
          "type": "blob",
          "size": 7.9990234375,
          "content": "#!/usr/bin/env python\n# encoding: utf-8\nimport gradio as gr\nfrom PIL import Image\nimport traceback\nimport re\nimport torch\nimport argparse\nfrom transformers import AutoModel, AutoTokenizer\n\n# README, How to run demo on different devices\n\n# For Nvidia GPUs.\n# python web_demo_2.5.py --device cuda\n\n# For Mac with MPS (Apple silicon or AMD GPUs).\n# PYTORCH_ENABLE_MPS_FALLBACK=1 python web_demo_2.5.py --device mps\n\n# Argparser\nparser = argparse.ArgumentParser(description='demo')\nparser.add_argument('--device', type=str, default='cuda', help='cuda or mps')\nargs = parser.parse_args()\ndevice = args.device\nassert device in ['cuda', 'mps']\n\n# Load model\nmodel_path = 'openbmb/MiniCPM-Llama3-V-2_5'\nif 'int4' in model_path:\n    if device == 'mps':\n        print('Error: running int4 model with bitsandbytes on Mac is not supported right now.')\n        exit()\n    model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\nelse:\n    model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16, device_map=device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nmodel.eval()\n\n\n\nERROR_MSG = \"Error, please retry\"\nmodel_name = 'MiniCPM-V 2.5'\n\nform_radio = {\n    'choices': ['Beam Search', 'Sampling'],\n    #'value': 'Beam Search',\n    'value': 'Sampling',\n    'interactive': True,\n    'label': 'Decode Type'\n}\n# Beam Form\nnum_beams_slider = {\n    'minimum': 0,\n    'maximum': 5,\n    'value': 3,\n    'step': 1,\n    'interactive': True,\n    'label': 'Num Beams'\n}\nrepetition_penalty_slider = {\n    'minimum': 0,\n    'maximum': 3,\n    'value': 1.2,\n    'step': 0.01,\n    'interactive': True,\n    'label': 'Repetition Penalty'\n}\nrepetition_penalty_slider2 = {\n    'minimum': 0,\n    'maximum': 3,\n    'value': 1.05,\n    'step': 0.01,\n    'interactive': True,\n    'label': 'Repetition Penalty'\n}\nmax_new_tokens_slider = {\n    'minimum': 1,\n    'maximum': 4096,\n    'value': 1024,\n    'step': 1,\n    'interactive': True,\n    'label': 'Max New Tokens'    \n}\n\ntop_p_slider = {\n    'minimum': 0,\n    'maximum': 1,\n    'value': 0.8,\n    'step': 0.05,\n    'interactive': True,\n    'label': 'Top P'    \n}\ntop_k_slider = {\n    'minimum': 0,\n    'maximum': 200,\n    'value': 100,\n    'step': 1,\n    'interactive': True,\n    'label': 'Top K'    \n}\ntemperature_slider = {\n    'minimum': 0,\n    'maximum': 2,\n    'value': 0.7,\n    'step': 0.05,\n    'interactive': True,\n    'label': 'Temperature'    \n}\n\n\ndef create_component(params, comp='Slider'):\n    if comp == 'Slider':\n        return gr.Slider(\n            minimum=params['minimum'],\n            maximum=params['maximum'],\n            value=params['value'],\n            step=params['step'],\n            interactive=params['interactive'],\n            label=params['label']\n        )\n    elif comp == 'Radio':\n        return gr.Radio(\n            choices=params['choices'],\n            value=params['value'],\n            interactive=params['interactive'],\n            label=params['label']\n        )\n    elif comp == 'Button':\n        return gr.Button(\n            value=params['value'],\n            interactive=True\n        )\n\n\ndef chat(img, msgs, ctx, params=None, vision_hidden_states=None):\n    default_params = {\"num_beams\":3, \"repetition_penalty\": 1.2, \"max_new_tokens\": 1024}\n    if params is None:\n        params = default_params\n    if img is None:\n        return -1, \"Error, invalid image, please upload a new image\", None, None\n    try:\n        image = img.convert('RGB')\n        answer = model.chat(\n            image=image,\n            msgs=msgs,\n            tokenizer=tokenizer,\n            **params\n        )\n        res = re.sub(r'(<box>.*</box>)', '', answer)\n        res = res.replace('<ref>', '')\n        res = res.replace('</ref>', '')\n        res = res.replace('<box>', '')\n        answer = res.replace('</box>', '')\n        return 0, answer, None, None\n    except Exception as err:\n        print(err)\n        traceback.print_exc()\n        return -1, ERROR_MSG, None, None\n\n\ndef upload_img(image, _chatbot, _app_session):\n    image = Image.fromarray(image)\n\n    _app_session['sts']=None\n    _app_session['ctx']=[]\n    _app_session['img']=image \n    _chatbot.append(('', 'Image uploaded successfully, you can talk to me now'))\n    return _chatbot, _app_session\n\n\ndef respond(_question, _chat_bot, _app_cfg, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature):\n    if _app_cfg.get('ctx', None) is None:\n        _chat_bot.append((_question, 'Please upload an image to start'))\n        return '', _chat_bot, _app_cfg\n\n    _context = _app_cfg['ctx'].copy()\n    if _context:\n        _context.append({\"role\": \"user\", \"content\": _question})\n    else:\n        _context = [{\"role\": \"user\", \"content\": _question}] \n    print('<User>:', _question)\n\n    if params_form == 'Beam Search':\n        params = {\n            'sampling': False,\n            'num_beams': num_beams,\n            'repetition_penalty': repetition_penalty,\n            \"max_new_tokens\": 896 \n        }\n    else:\n        params = {\n            'sampling': True,\n            'top_p': top_p,\n            'top_k': top_k,\n            'temperature': temperature,\n            'repetition_penalty': repetition_penalty_2,\n            \"max_new_tokens\": 896 \n        }\n    code, _answer, _, sts = chat(_app_cfg['img'], _context, None, params)\n    print('<Assistant>:', _answer)\n\n    _context.append({\"role\": \"assistant\", \"content\": _answer}) \n    _chat_bot.append((_question, _answer))\n    if code == 0:\n        _app_cfg['ctx']=_context\n        _app_cfg['sts']=sts\n    return '', _chat_bot, _app_cfg\n\n\ndef regenerate_button_clicked(_question, _chat_bot, _app_cfg, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature):\n    if len(_chat_bot) <= 1:\n        _chat_bot.append(('Regenerate', 'No question for regeneration.'))\n        return '', _chat_bot, _app_cfg\n    elif _chat_bot[-1][0] == 'Regenerate':\n        return '', _chat_bot, _app_cfg\n    else:\n        _question = _chat_bot[-1][0]\n        _chat_bot = _chat_bot[:-1]\n        _app_cfg['ctx'] = _app_cfg['ctx'][:-2]\n    return respond(_question, _chat_bot, _app_cfg, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature)\n\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column(scale=1, min_width=300):\n            params_form = create_component(form_radio, comp='Radio')\n            with gr.Accordion(\"Beam Search\") as beams_according:\n                num_beams = create_component(num_beams_slider)\n                repetition_penalty = create_component(repetition_penalty_slider)\n            with gr.Accordion(\"Sampling\") as sampling_according:\n                top_p = create_component(top_p_slider)\n                top_k = create_component(top_k_slider)\n                temperature = create_component(temperature_slider)\n                repetition_penalty_2 = create_component(repetition_penalty_slider2)\n            regenerate = create_component({'value': 'Regenerate'}, comp='Button')\n        with gr.Column(scale=3, min_width=500):\n            app_session = gr.State({'sts':None,'ctx':None,'img':None})\n            bt_pic = gr.Image(label=\"Upload an image to start\")\n            chat_bot = gr.Chatbot(label=f\"Chat with {model_name}\")\n            txt_message = gr.Textbox(label=\"Input text\")\n            \n            regenerate.click(\n                regenerate_button_clicked,\n                [txt_message, chat_bot, app_session, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature],\n                [txt_message, chat_bot, app_session]\n            )\n            txt_message.submit(\n                respond, \n                [txt_message, chat_bot, app_session, params_form, num_beams, repetition_penalty, repetition_penalty_2, top_p, top_k, temperature], \n                [txt_message, chat_bot, app_session]\n            )\n            bt_pic.upload(lambda: None, None, chat_bot, queue=False).then(upload_img, inputs=[bt_pic,chat_bot,app_session], outputs=[chat_bot,app_session])\n\n# launch\ndemo.launch(share=False, debug=True, show_api=False, server_port=8080, server_name=\"0.0.0.0\")\n\n"
        },
        {
          "name": "web_demo_2.6.py",
          "type": "blob",
          "size": 19.9443359375,
          "content": "#!/usr/bin/env python\n# encoding: utf-8\nimport torch\nimport argparse\nfrom transformers import AutoModel, AutoTokenizer\nimport gradio as gr\nfrom PIL import Image\nfrom decord import VideoReader, cpu\nimport io\nimport os\nimport copy\nimport requests\nimport base64\nimport json\nimport traceback\nimport re\nimport modelscope_studio as mgr\n\n\n# README, How to run demo on different devices\n\n# For Nvidia GPUs.\n# python web_demo_2.6.py --device cuda\n\n# For Mac with MPS (Apple silicon or AMD GPUs).\n# PYTORCH_ENABLE_MPS_FALLBACK=1 python web_demo_2.6.py --device mps\n\n# Argparser\nparser = argparse.ArgumentParser(description='demo')\nparser.add_argument('--device', type=str, default='cuda', help='cuda or mps')\nparser.add_argument('--multi-gpus', action='store_true', default=False, help='use multi-gpus')\nargs = parser.parse_args()\ndevice = args.device\nassert device in ['cuda', 'mps']\n\n# Load model\nmodel_path = 'openbmb/MiniCPM-V-2_6'\nif 'int4' in model_path:\n    if device == 'mps':\n        print('Error: running int4 model with bitsandbytes on Mac is not supported right now.')\n        exit()\n    model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\nelse:\n    if args.multi_gpus:\n        from accelerate import load_checkpoint_and_dispatch, init_empty_weights, infer_auto_device_map\n        with init_empty_weights():\n            model = AutoModel.from_pretrained(model_path, trust_remote_code=True, attn_implementation='sdpa', torch_dtype=torch.bfloat16)\n        device_map = infer_auto_device_map(model, max_memory={0: \"10GB\", 1: \"10GB\"},\n            no_split_module_classes=['SiglipVisionTransformer', 'Qwen2DecoderLayer'])\n        device_id = device_map[\"llm.model.embed_tokens\"]\n        device_map[\"llm.lm_head\"] = device_id # firtt and last layer should be in same device\n        device_map[\"vpm\"] = device_id\n        device_map[\"resampler\"] = device_id\n        device_id2 = device_map[\"llm.model.layers.26\"]\n        device_map[\"llm.model.layers.8\"] = device_id2\n        device_map[\"llm.model.layers.9\"] = device_id2\n        device_map[\"llm.model.layers.10\"] = device_id2\n        device_map[\"llm.model.layers.11\"] = device_id2\n        device_map[\"llm.model.layers.12\"] = device_id2\n        device_map[\"llm.model.layers.13\"] = device_id2\n        device_map[\"llm.model.layers.14\"] = device_id2\n        device_map[\"llm.model.layers.15\"] = device_id2\n        device_map[\"llm.model.layers.16\"] = device_id2\n        #print(device_map)\n\n        model = load_checkpoint_and_dispatch(model, model_path, dtype=torch.bfloat16, device_map=device_map)\n    else:\n        model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16)\n        model = model.to(device=device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nmodel.eval()\n\n\n\n\nERROR_MSG = \"Error, please retry\"\nmodel_name = 'MiniCPM-V 2.6'\nMAX_NUM_FRAMES = 64\nIMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\nVIDEO_EXTENSIONS = {'.mp4', '.mkv', '.mov', '.avi', '.flv', '.wmv', '.webm', '.m4v'}\n\ndef get_file_extension(filename):\n    return os.path.splitext(filename)[1].lower()\n\ndef is_image(filename):\n    return get_file_extension(filename) in IMAGE_EXTENSIONS\n\ndef is_video(filename):\n    return get_file_extension(filename) in VIDEO_EXTENSIONS\n\n\nform_radio = {\n    'choices': ['Beam Search', 'Sampling'],\n    #'value': 'Beam Search',\n    'value': 'Sampling',\n    'interactive': True,\n    'label': 'Decode Type'\n}\n\n\ndef create_component(params, comp='Slider'):\n    if comp == 'Slider':\n        return gr.Slider(\n            minimum=params['minimum'],\n            maximum=params['maximum'],\n            value=params['value'],\n            step=params['step'],\n            interactive=params['interactive'],\n            label=params['label']\n        )\n    elif comp == 'Radio':\n        return gr.Radio(\n            choices=params['choices'],\n            value=params['value'],\n            interactive=params['interactive'],\n            label=params['label']\n        )\n    elif comp == 'Button':\n        return gr.Button(\n            value=params['value'],\n            interactive=True\n        )\n\n\ndef create_multimodal_input(upload_image_disabled=False, upload_video_disabled=False):\n    return mgr.MultimodalInput(upload_image_button_props={'label': 'Upload Image', 'disabled': upload_image_disabled, 'file_count': 'multiple'}, \n                                        upload_video_button_props={'label': 'Upload Video', 'disabled': upload_video_disabled, 'file_count': 'single'},\n                                        submit_button_props={'label': 'Submit'})\n\n\ndef chat(img, msgs, ctx, params=None, vision_hidden_states=None):\n    try:\n        print('msgs:', msgs)\n        answer = model.chat(\n            image=None,\n            msgs=msgs,\n            tokenizer=tokenizer,\n            **params\n        )\n        res = re.sub(r'(<box>.*</box>)', '', answer)\n        res = res.replace('<ref>', '')\n        res = res.replace('</ref>', '')\n        res = res.replace('<box>', '')\n        answer = res.replace('</box>', '')\n        print('answer:', answer)\n        return 0, answer, None, None\n    except Exception as e:\n        print(e)\n        traceback.print_exc()\n        return -1, ERROR_MSG, None, None\n\n\ndef encode_image(image):\n    if not isinstance(image, Image.Image):\n        if hasattr(image, 'path'):\n            image = Image.open(image.path).convert(\"RGB\")\n        else:\n            image = Image.open(image.file.path).convert(\"RGB\")\n    # resize to max_size\n    max_size = 448*16 \n    if max(image.size) > max_size:\n        w,h = image.size\n        if w > h:\n            new_w = max_size\n            new_h = int(h * max_size / w)\n        else:\n            new_h = max_size\n            new_w = int(w * max_size / h)\n        image = image.resize((new_w, new_h), resample=Image.BICUBIC)\n    return image\n    ## save by BytesIO and convert to base64\n    #buffered = io.BytesIO()\n    #image.save(buffered, format=\"png\")\n    #im_b64 = base64.b64encode(buffered.getvalue()).decode()\n    #return {\"type\": \"image\", \"pairs\": im_b64}\n\n\ndef encode_video(video):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n\n    if hasattr(video, 'path'):\n        vr = VideoReader(video.path, ctx=cpu(0))\n    else:\n        vr = VideoReader(video.file.path, ctx=cpu(0))\n    sample_fps = round(vr.get_avg_fps() / 1)  # FPS\n    frame_idx = [i for i in range(0, len(vr), sample_fps)]\n    if len(frame_idx)>MAX_NUM_FRAMES:\n        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\n    video = vr.get_batch(frame_idx).asnumpy()\n    video = [Image.fromarray(v.astype('uint8')) for v in video]\n    video = [encode_image(v) for v in video]\n    print('video frames:', len(video))\n    return video\n\n\ndef check_mm_type(mm_file):\n    if hasattr(mm_file, 'path'):\n        path = mm_file.path\n    else:\n        path = mm_file.file.path\n    if is_image(path):\n        return \"image\"\n    if is_video(path):\n        return \"video\"\n    return None\n\n\ndef encode_mm_file(mm_file):\n    if check_mm_type(mm_file) == 'image':\n        return [encode_image(mm_file)]\n    if check_mm_type(mm_file) == 'video':\n        return encode_video(mm_file)\n    return None\n\ndef make_text(text):\n    #return {\"type\": \"text\", \"pairs\": text} # # For remote call\n    return text\n\ndef encode_message(_question):\n    files = _question.files\n    question = _question.text\n    pattern = r\"\\[mm_media\\]\\d+\\[/mm_media\\]\"\n    matches = re.split(pattern, question)\n    message = []\n    if len(matches) != len(files) + 1:\n        gr.Warning(\"Number of Images not match the placeholder in text, please refresh the page to restart!\")\n    assert len(matches) == len(files) + 1\n\n    text = matches[0].strip()\n    if text:\n        message.append(make_text(text))\n    for i in range(len(files)):\n        message += encode_mm_file(files[i])\n        text = matches[i + 1].strip()\n        if text:\n            message.append(make_text(text))\n    return message\n\n\ndef check_has_videos(_question):\n    images_cnt = 0\n    videos_cnt = 0\n    for file in _question.files:\n        if check_mm_type(file) == \"image\":\n            images_cnt += 1 \n        else:\n            videos_cnt += 1\n    return images_cnt, videos_cnt \n\n\ndef count_video_frames(_context):\n    num_frames = 0\n    for message in _context:\n        for item in message[\"content\"]:\n            #if item[\"type\"] == \"image\": # For remote call\n            if isinstance(item, Image.Image):\n                num_frames += 1\n    return num_frames\n\n\ndef respond(_question, _chat_bot, _app_cfg, params_form):\n    _context = _app_cfg['ctx'].copy()\n    _context.append({'role': 'user', 'content': encode_message(_question)})\n\n    images_cnt = _app_cfg['images_cnt']\n    videos_cnt = _app_cfg['videos_cnt']\n    files_cnts = check_has_videos(_question)\n    if files_cnts[1] + videos_cnt > 1 or (files_cnts[1] + videos_cnt == 1 and files_cnts[0] + images_cnt > 0):\n        gr.Warning(\"Only supports single video file input right now!\")\n        return _question, _chat_bot, _app_cfg\n\n    if params_form == 'Beam Search':\n        params = {\n            'sampling': False,\n            'num_beams': 3,\n            'repetition_penalty': 1.2,\n            \"max_new_tokens\": 2048\n        }\n    else:\n        params = {\n            'sampling': True,\n            'top_p': 0.8,\n            'top_k': 100,\n            'temperature': 0.7,\n            'repetition_penalty': 1.05,\n            \"max_new_tokens\": 2048\n        }\n    \n    if files_cnts[1] + videos_cnt > 0:\n        params[\"max_inp_length\"] = 4352 # 4096+256\n        params[\"use_image_id\"] = False\n        params[\"max_slice_nums\"] = 1 if count_video_frames(_context) > 16 else 2\n\n    code, _answer, _, sts = chat(\"\", _context, None, params)\n\n    images_cnt += files_cnts[0]\n    videos_cnt += files_cnts[1]\n    _context.append({\"role\": \"assistant\", \"content\": [make_text(_answer)]}) \n    _chat_bot.append((_question, _answer))\n    if code == 0:\n        _app_cfg['ctx']=_context\n        _app_cfg['sts']=sts\n    _app_cfg['images_cnt'] = images_cnt\n    _app_cfg['videos_cnt'] = videos_cnt\n\n    upload_image_disabled = videos_cnt > 0\n    upload_video_disabled = videos_cnt > 0 or images_cnt > 0\n    return create_multimodal_input(upload_image_disabled, upload_video_disabled), _chat_bot, _app_cfg\n\n\ndef fewshot_add_demonstration(_image, _user_message, _assistant_message, _chat_bot, _app_cfg):\n    ctx = _app_cfg[\"ctx\"]\n    message_item = []\n    if _image is not None:\n        image = Image.open(_image).convert(\"RGB\")\n        ctx.append({\"role\": \"user\", \"content\": [encode_image(image), make_text(_user_message)]})\n        message_item.append({\"text\": \"[mm_media]1[/mm_media]\" + _user_message, \"files\": [_image]})\n    else:\n        if _user_message:\n            ctx.append({\"role\": \"user\", \"content\": [make_text(_user_message)]})\n            message_item.append({\"text\": _user_message, \"files\": []})\n        else:\n            message_item.append(None)\n    if _assistant_message:\n        ctx.append({\"role\": \"assistant\", \"content\": [make_text(_assistant_message)]})\n        message_item.append({\"text\": _assistant_message, \"files\": []})\n    else:\n        message_item.append(None)\n\n    _chat_bot.append(message_item)\n    return None, \"\", \"\", _chat_bot, _app_cfg\n\n\ndef fewshot_respond(_image, _user_message, _chat_bot, _app_cfg, params_form):\n    user_message_contents = []\n    _context = _app_cfg[\"ctx\"].copy()\n    if _image:\n        image = Image.open(_image).convert(\"RGB\")\n        user_message_contents += [encode_image(image)]\n    if _user_message:\n        user_message_contents += [make_text(_user_message)]\n    if user_message_contents:\n        _context.append({\"role\": \"user\", \"content\": user_message_contents})\n\n    if params_form == 'Beam Search':\n        params = {\n            'sampling': False,\n            'num_beams': 3,\n            'repetition_penalty': 1.2,\n            \"max_new_tokens\": 2048\n        }\n    else:\n        params = {\n            'sampling': True,\n            'top_p': 0.8,\n            'top_k': 100,\n            'temperature': 0.7,\n            'repetition_penalty': 1.05,\n            \"max_new_tokens\": 2048\n        }\n    \n    code, _answer, _, sts = chat(\"\", _context, None, params)\n\n    _context.append({\"role\": \"assistant\", \"content\": [make_text(_answer)]})\n\n    if _image:\n        _chat_bot.append([\n            {\"text\": \"[mm_media]1[/mm_media]\" + _user_message, \"files\": [_image]},\n            {\"text\": _answer, \"files\": []}        \n        ])\n    else:\n        _chat_bot.append([\n            {\"text\": _user_message, \"files\": [_image]},\n            {\"text\": _answer, \"files\": []}        \n        ])\n    if code == 0:\n        _app_cfg['ctx']=_context\n        _app_cfg['sts']=sts\n    return None, '', '', _chat_bot, _app_cfg\n\n\ndef regenerate_button_clicked(_question, _image, _user_message, _assistant_message, _chat_bot, _app_cfg, params_form):\n    if len(_chat_bot) <= 1 or not _chat_bot[-1][1]:\n        gr.Warning('No question for regeneration.')\n        return '', _image, _user_message, _assistant_message, _chat_bot, _app_cfg\n    if _app_cfg[\"chat_type\"] == \"Chat\":\n        images_cnt = _app_cfg['images_cnt']\n        videos_cnt = _app_cfg['videos_cnt']\n        _question = _chat_bot[-1][0]\n        _chat_bot = _chat_bot[:-1]\n        _app_cfg['ctx'] = _app_cfg['ctx'][:-2]\n        files_cnts = check_has_videos(_question)\n        images_cnt -= files_cnts[0]\n        videos_cnt -= files_cnts[1]\n        _app_cfg['images_cnt'] = images_cnt\n        _app_cfg['videos_cnt'] = videos_cnt\n        upload_image_disabled = videos_cnt > 0\n        upload_video_disabled = videos_cnt > 0 or images_cnt > 0\n        _question, _chat_bot, _app_cfg = respond(_question, _chat_bot, _app_cfg, params_form)\n        return _question, _image, _user_message, _assistant_message, _chat_bot, _app_cfg\n    else: \n        last_message = _chat_bot[-1][0]\n        last_image = None\n        last_user_message = ''\n        if last_message.text:\n            last_user_message = last_message.text\n        if last_message.files:\n            last_image = last_message.files[0].file.path\n        _chat_bot = _chat_bot[:-1]\n        _app_cfg['ctx'] = _app_cfg['ctx'][:-2]\n        _image, _user_message, _assistant_message, _chat_bot, _app_cfg = fewshot_respond(last_image, last_user_message, _chat_bot, _app_cfg, params_form)\n        return _question, _image, _user_message, _assistant_message, _chat_bot, _app_cfg\n\n\ndef flushed():\n    return gr.update(interactive=True)\n\n\ndef clear(txt_message, chat_bot, app_session):\n    txt_message.files.clear()\n    txt_message.text = ''\n    chat_bot = copy.deepcopy(init_conversation)\n    app_session['sts'] = None\n    app_session['ctx'] = []\n    app_session['images_cnt'] = 0\n    app_session['videos_cnt'] = 0\n    return create_multimodal_input(), chat_bot, app_session, None, '', ''\n    \n\ndef select_chat_type(_tab, _app_cfg):\n    _app_cfg[\"chat_type\"] = _tab\n    return _app_cfg\n\n\ninit_conversation = [\n    [\n        None,\n        {\n            # The first message of bot closes the typewriter.\n            \"text\": \"You can talk to me now\",\n            \"flushing\": False\n        }\n    ],\n]\n\n\ncss = \"\"\"\nvideo { height: auto !important; }\n.example label { font-size: 16px;}\n\"\"\"\n\nintroduction = \"\"\"\n\n## Features:\n1. Chat with single image\n2. Chat with multiple images\n3. Chat with video\n4. In-context few-shot learning\n\nClick `How to use` tab to see examples.\n\"\"\"\n\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Tab(model_name):\n        with gr.Row():\n            with gr.Column(scale=1, min_width=300):\n                gr.Markdown(value=introduction)\n                params_form = create_component(form_radio, comp='Radio')\n                regenerate = create_component({'value': 'Regenerate'}, comp='Button')\n                clear_button = create_component({'value': 'Clear History'}, comp='Button')\n\n            with gr.Column(scale=3, min_width=500):\n                app_session = gr.State({'sts':None,'ctx':[], 'images_cnt': 0, 'videos_cnt': 0, 'chat_type': 'Chat'})\n                chat_bot = mgr.Chatbot(label=f\"Chat with {model_name}\", value=copy.deepcopy(init_conversation), height=600, flushing=False, bubble_full_width=False)\n                \n                with gr.Tab(\"Chat\") as chat_tab:\n                    txt_message = create_multimodal_input()\n                    chat_tab_label = gr.Textbox(value=\"Chat\", interactive=False, visible=False)\n\n                    txt_message.submit(\n                        respond,\n                        [txt_message, chat_bot, app_session, params_form], \n                        [txt_message, chat_bot, app_session]\n                    )\n\n                with gr.Tab(\"Few Shot\") as fewshot_tab:\n                    fewshot_tab_label = gr.Textbox(value=\"Few Shot\", interactive=False, visible=False)\n                    with gr.Row():\n                        with gr.Column(scale=1):\n                            image_input = gr.Image(type=\"filepath\", sources=[\"upload\"])\n                        with gr.Column(scale=3):\n                            user_message = gr.Textbox(label=\"User\")\n                            assistant_message = gr.Textbox(label=\"Assistant\")\n                            with gr.Row():\n                                add_demonstration_button = gr.Button(\"Add Example\")\n                                generate_button = gr.Button(value=\"Generate\", variant=\"primary\")\n                    add_demonstration_button.click(\n                        fewshot_add_demonstration,\n                        [image_input, user_message, assistant_message, chat_bot, app_session],\n                        [image_input, user_message, assistant_message, chat_bot, app_session]\n                    )\n                    generate_button.click(\n                        fewshot_respond,\n                        [image_input, user_message, chat_bot, app_session, params_form],\n                        [image_input, user_message, assistant_message, chat_bot, app_session]\n                    )\n\n                chat_tab.select(\n                    select_chat_type,\n                    [chat_tab_label, app_session],\n                    [app_session]\n                )\n                chat_tab.select( # do clear\n                    clear,\n                    [txt_message, chat_bot, app_session],\n                    [txt_message, chat_bot, app_session, image_input, user_message, assistant_message]\n                )\n                fewshot_tab.select(\n                    select_chat_type,\n                    [fewshot_tab_label, app_session],\n                    [app_session]\n                )\n                fewshot_tab.select( # do clear\n                    clear,\n                    [txt_message, chat_bot, app_session],\n                    [txt_message, chat_bot, app_session, image_input, user_message, assistant_message]\n                )\n                chat_bot.flushed(\n                    flushed,\n                    outputs=[txt_message]\n                )\n                regenerate.click(\n                    regenerate_button_clicked,\n                    [txt_message, image_input, user_message, assistant_message, chat_bot, app_session, params_form],\n                    [txt_message, image_input, user_message, assistant_message, chat_bot, app_session]\n                )\n                clear_button.click(\n                    clear,\n                    [txt_message, chat_bot, app_session],\n                    [txt_message, chat_bot, app_session, image_input, user_message, assistant_message]\n                )\n\n    with gr.Tab(\"How to use\"):\n        with gr.Column():\n            with gr.Row():\n                image_example = gr.Image(value=\"http://thunlp.oss-cn-qingdao.aliyuncs.com/multi_modal/never_delete/m_bear2.gif\", label='1. Chat with single or multiple images', interactive=False, width=400, elem_classes=\"example\")\n                example2 = gr.Image(value=\"http://thunlp.oss-cn-qingdao.aliyuncs.com/multi_modal/never_delete/video2.gif\", label='2. Chat with video', interactive=False, width=400, elem_classes=\"example\")\n                example3 = gr.Image(value=\"http://thunlp.oss-cn-qingdao.aliyuncs.com/multi_modal/never_delete/fshot.gif\", label='3. Few shot', interactive=False, width=400, elem_classes=\"example\")\n\n\n# launch\ndemo.launch(share=False, debug=True, show_api=False, server_port=8885, server_name=\"0.0.0.0\")\n\n"
        },
        {
          "name": "web_demo_streamlit-2_5.py",
          "type": "blob",
          "size": 4.2734375,
          "content": "import streamlit as st\r\nfrom PIL import Image\r\nimport torch\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\n# Model path\r\nmodel_path = \"openbmb/MiniCPM-Llama3-V-2_5\"\r\n\r\n# User and assistant names\r\nU_NAME = \"User\"\r\nA_NAME = \"Assistant\"\r\n\r\n# Set page configuration\r\nst.set_page_config(\r\n    page_title=\"MiniCPM-Llama3-V-2_5 Streamlit\",\r\n    page_icon=\":robot:\",\r\n    layout=\"wide\"\r\n)\r\n\r\n\r\n# Load model and tokenizer\r\n@st.cache_resource\r\ndef load_model_and_tokenizer():\r\n    print(f\"load_model_and_tokenizer from {model_path}\")\r\n    model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to(device=\"cuda\")\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n    return model, tokenizer\r\n\r\n\r\n# Initialize session state\r\nif 'model' not in st.session_state:\r\n    st.session_state.model, st.session_state.tokenizer = load_model_and_tokenizer()\r\n    st.session_state.model.eval()\r\n    print(\"model and tokenizer had loaded completed!\")\r\n\r\n# Initialize session state\r\nif 'chat_history' not in st.session_state:\r\n    st.session_state.chat_history = []\r\n\r\n# Sidebar settings\r\nsidebar_name = st.sidebar.title(\"MiniCPM-Llama3-V-2_5 Streamlit\")\r\nmax_length = st.sidebar.slider(\"max_length\", 0, 4096, 2048, step=2)\r\nrepetition_penalty = st.sidebar.slider(\"repetition_penalty\", 0.0, 2.0, 1.05, step=0.01)\r\ntop_p = st.sidebar.slider(\"top_p\", 0.0, 1.0, 0.8, step=0.01)\r\ntop_k = st.sidebar.slider(\"top_k\", 0, 100, 100, step=1)\r\ntemperature = st.sidebar.slider(\"temperature\", 0.0, 1.0, 0.7, step=0.01)\r\n\r\n# Clear chat history button\r\nbuttonClean = st.sidebar.button(\"Clear chat history\", key=\"clean\")\r\nif buttonClean:\r\n    st.session_state.chat_history = []\r\n    st.session_state.response = \"\"\r\n    if torch.cuda.is_available():\r\n        torch.cuda.empty_cache()\r\n    st.rerun()\r\n\r\n# Display chat history\r\nfor i, message in enumerate(st.session_state.chat_history):\r\n    if message[\"role\"] == \"user\":\r\n        with st.chat_message(name=\"user\", avatar=\"user\"):\r\n            if message[\"image\"] is not None:\r\n                st.image(message[\"image\"], caption='User uploaded image', width=448, use_column_width=False)\r\n                continue\r\n            elif message[\"content\"] is not None:\r\n                st.markdown(message[\"content\"])\r\n    else:\r\n        with st.chat_message(name=\"model\", avatar=\"assistant\"):\r\n            st.markdown(message[\"content\"])\r\n\r\n# Select mode\r\nselected_mode = st.sidebar.selectbox(\"Select mode\", [\"Text\", \"Image\"])\r\nif selected_mode == \"Image\":\r\n    # Image mode\r\n    uploaded_image = st.sidebar.file_uploader(\"Upload image\", key=1, type=[\"jpg\", \"jpeg\", \"png\"],\r\n                                              accept_multiple_files=False)\r\n    if uploaded_image is not None:\r\n        st.image(uploaded_image, caption='User uploaded image', width=468, use_column_width=False)\r\n        # Add uploaded image to chat history\r\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": None, \"image\": uploaded_image})\r\n\r\n# User input box\r\nuser_text = st.chat_input(\"Enter your question\")\r\nif user_text:\r\n    with st.chat_message(U_NAME, avatar=\"user\"):\r\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_text, \"image\": None})\r\n        st.markdown(f\"{U_NAME}: {user_text}\")\r\n\r\n    # Generate reply using the model\r\n    model = st.session_state.model\r\n    tokenizer = st.session_state.tokenizer\r\n    imagefile = None\r\n\r\n    with st.chat_message(A_NAME, avatar=\"assistant\"):\r\n        # If the previous message contains an image, pass the image to the model\r\n        if len(st.session_state.chat_history) > 1 and st.session_state.chat_history[-2][\"image\"] is not None:\r\n            uploaded_image = st.session_state.chat_history[-2][\"image\"]\r\n            imagefile = Image.open(uploaded_image).convert('RGB')\r\n\r\n        msgs = [{\"role\": \"user\", \"content\": user_text}]\r\n        res = model.chat(image=imagefile, msgs=msgs, context=None, tokenizer=tokenizer,\r\n                         sampling=True, top_p=top_p, top_k=top_k, repetition_penalty=repetition_penalty,\r\n                         temperature=temperature, stream=True)\r\n\r\n        # Collect the generated_text str\r\n        generated_text = st.write_stream(res)\r\n\r\n        st.session_state.chat_history.append({\"role\": \"model\", \"content\": generated_text, \"image\": None})\r\n\r\n    st.divider()\r\n"
        },
        {
          "name": "web_demo_streamlit-minicpmv2_6.py",
          "type": "blob",
          "size": 11.8876953125,
          "content": "import os.path\r\n\r\nimport streamlit as st\r\nimport torch\r\nfrom PIL import Image\r\nfrom decord import VideoReader, cpu\r\nimport numpy as np\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\n# Model path\r\nmodel_path = \"openbmb/MiniCPM-V-2_6\"\r\nupload_path = \".\\\\uploads\"\r\n\r\n# User and assistant names\r\nU_NAME = \"User\"\r\nA_NAME = \"Assistant\"\r\n\r\n# Set page configuration\r\nst.set_page_config(\r\n    page_title=\"MiniCPM-V-2_6 Streamlit\",\r\n    page_icon=\":robot:\",\r\n    layout=\"wide\"\r\n)\r\n\r\n\r\n# Load model and tokenizer\r\n@st.cache_resource\r\ndef load_model_and_tokenizer():\r\n    print(f\"load_model_and_tokenizer from {model_path}\")\r\n    model = (AutoModel.from_pretrained(model_path, trust_remote_code=True, attn_implementation='sdpa').\r\n             to(dtype=torch.bfloat16))\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n    return model, tokenizer\r\n\r\n\r\n# Initialize session state\r\nif 'model' not in st.session_state:\r\n    st.session_state.model, st.session_state.tokenizer = load_model_and_tokenizer()\r\n    st.session_state.model.eval().cuda()\r\n    print(\"model and tokenizer had loaded completed!\")\r\n\r\n# Initialize session state\r\nif 'chat_history' not in st.session_state:\r\n    st.session_state.chat_history = []\r\n    st.session_state.uploaded_image_list = []\r\n    st.session_state.uploaded_image_num = 0\r\n    st.session_state.uploaded_video_list = []\r\n    st.session_state.uploaded_video_num = 0\r\n    st.session_state.response = \"\"\r\n\r\n# Sidebar settings\r\nsidebar_name = st.sidebar.title(\"MiniCPM-V-2_6 Streamlit\")\r\nmax_length = st.sidebar.slider(\"max_length\", 0, 4096, 2048, step=2)\r\nrepetition_penalty = st.sidebar.slider(\"repetition_penalty\", 0.0, 2.0, 1.05, step=0.01)\r\ntop_k = st.sidebar.slider(\"top_k\", 0, 100, 100, step=1)\r\ntop_p = st.sidebar.slider(\"top_p\", 0.0, 1.0, 0.8, step=0.01)\r\ntemperature = st.sidebar.slider(\"temperature\", 0.0, 1.0, 0.7, step=0.01)\r\n\r\n# Button to clear session history\r\nbuttonClean = st.sidebar.button(\"Clearing session history\", key=\"clean\")\r\nif buttonClean:\r\n    # Reset the session state history and uploaded file lists\r\n    st.session_state.chat_history = []\r\n    st.session_state.uploaded_image_list = []\r\n    st.session_state.uploaded_image_num = 0\r\n    st.session_state.uploaded_video_list = []\r\n    st.session_state.uploaded_video_num = 0\r\n    st.session_state.response = \"\"\r\n\r\n    # If using GPU, clear the CUDA cache to free up memory\r\n    if torch.cuda.is_available():\r\n        torch.cuda.empty_cache()\r\n\r\n    # Rerun to refresh the interface\r\n    st.rerun()\r\n\r\n# Display chat history\r\nfor i, message in enumerate(st.session_state.chat_history):\r\n    if message[\"role\"] == \"user\":\r\n        with st.chat_message(name=\"user\", avatar=\"user\"):\r\n            if message[\"image\"] is not None:\r\n                st.image(message[\"image\"], caption='User uploaded images', width=512, use_column_width=False)\r\n                continue\r\n            elif message[\"video\"] is not None:\r\n                st.video(message[\"video\"], format=\"video/mp4\", loop=False, autoplay=False, muted=True)\r\n                continue\r\n            elif message[\"content\"] is not None:\r\n                st.markdown(message[\"content\"])\r\n    else:\r\n        with st.chat_message(name=\"model\", avatar=\"assistant\"):\r\n            st.markdown(message[\"content\"])\r\n\r\n# Select mode\r\nselected_mode = st.sidebar.selectbox(\"Select Mode\", [\"Text\", \"Single Image\", \"Multiple Images\", \"Video\"])\r\n\r\n# Supported image file extensions\r\nimage_type = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\r\n\r\nif selected_mode == \"Single Image\":\r\n    # Single Image Mode\r\n    uploaded_image = st.sidebar.file_uploader(\"Upload a Single Image\", key=1, type=image_type,\r\n                                              accept_multiple_files=False)\r\n    if uploaded_image is not None:\r\n        st.image(uploaded_image, caption='User Uploaded Image', width=512, use_column_width=False)\r\n        # Add the uploaded image to the chat history\r\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": None, \"image\": uploaded_image, \"video\": None})\r\n        st.session_state.uploaded_image_list = [uploaded_image]\r\n        st.session_state.uploaded_image_num = 1\r\n\r\nif selected_mode == \"Multiple Images\":\r\n    # Multiple Images Mode\r\n    uploaded_image_list = st.sidebar.file_uploader(\"Upload Multiple Images\", key=2, type=image_type,\r\n                                                   accept_multiple_files=True)\r\n    uploaded_image_num = len(uploaded_image_list)\r\n\r\n    if uploaded_image_list is not None and uploaded_image_num > 0:\r\n        for img in uploaded_image_list:\r\n            st.image(img, caption='User Uploaded Image', width=512, use_column_width=False)\r\n            # Add the uploaded images to the chat history\r\n            st.session_state.chat_history.append({\"role\": \"user\", \"content\": None, \"image\": img, \"video\": None})\r\n        # Update the uploaded image list and count in st.session_state\r\n        st.session_state.uploaded_image_list = uploaded_image_list\r\n        st.session_state.uploaded_image_num = uploaded_image_num\r\n\r\n# Supported video format suffixes\r\nvideo_type = ['.mp4', '.mkv', '.mov', '.avi', '.flv', '.wmv', '.webm', '.m4v']\r\n\r\n# Tip: You can use the command `streamlit run ./web_demo_streamlit-minicpmv2_6.py --server.maxUploadSize 1024`\r\n# to adjust the maximum upload size to 1024MB or larger files.\r\n# The default 200MB limit of Streamlit's file_uploader component might be insufficient for video-based interactions.\r\n# Adjust the size based on your GPU memory usage.\r\n\r\nif selected_mode == \"Video\":\r\n    # Âçï‰∏™ËßÜÈ¢ëÊ®°ÊÄÅ\r\n    uploaded_video = st.sidebar.file_uploader(\"Upload a single video file\", key=3, type=video_type,\r\n                                              accept_multiple_files=False)\r\n    if uploaded_video is not None:\r\n        st.video(uploaded_video, format=\"video/mp4\", loop=False, autoplay=False, muted=True)\r\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": None, \"image\": None, \"video\": uploaded_video})\r\n\r\n        uploaded_video_path = os.path.join(upload_path, uploaded_video.name)\r\n        with open(uploaded_video_path, \"wb\") as vf:\r\n            vf.write(uploaded_video.getvalue())\r\n        st.session_state.uploaded_video_list = [uploaded_video_path]\r\n        st.session_state.uploaded_video_num = 1\r\n\r\nMAX_NUM_FRAMES = 64  # if cuda OOM set a smaller number\r\n\r\n\r\n# Encodes a video by sampling frames at a fixed rate and converting them to image arrays.\r\ndef encode_video(video_path):\r\n    def uniform_sample(frame_indices, num_samples):\r\n        # Calculate sampling interval and uniformly sample frame indices\r\n        gap = len(frame_indices) / num_samples\r\n        sampled_idxs = np.linspace(gap / 2, len(frame_indices) - gap / 2, num_samples, dtype=int)\r\n        return [frame_indices[i] for i in sampled_idxs]\r\n\r\n    # Read the video and set the decoder's context to CPU\r\n    vr = VideoReader(video_path, ctx=cpu(0))\r\n\r\n    # Calculate the sampling interval to sample video frames at 1 FPS\r\n    sample_fps = round(vr.get_avg_fps() / 1)  # Use integer FPS\r\n    frame_idx = list(range(0, len(vr), sample_fps))\r\n\r\n    # If the number of sampled frames exceeds the maximum limit, uniformly sample them\r\n    if len(frame_idx) > MAX_NUM_FRAMES:\r\n        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\r\n\r\n    # Retrieve the sampled frames and convert them to image arrays\r\n    frames = vr.get_batch(frame_idx).asnumpy()\r\n    frames = [Image.fromarray(frame.astype('uint8')) for frame in frames]\r\n\r\n    print('Number of frames:', len(frames))\r\n    return frames\r\n\r\n\r\n\r\n# User input box\r\nuser_text = st.chat_input(\"Enter your question\")\r\nif user_text is not None:\r\n    if user_text.strip() is \"\":\r\n        st.warning('Input message could not be empty!', icon=\"‚ö†Ô∏è\")\r\n    else:\r\n        # Display user input and save it to session history\r\n        with st.chat_message(U_NAME, avatar=\"user\"):\r\n            st.session_state.chat_history.append({\r\n                \"role\": \"user\",\r\n                \"content\": user_text,\r\n                \"image\": None,\r\n                \"video\": None\r\n            })\r\n            st.markdown(f\"{U_NAME}: {user_text}\")\r\n\r\n        # Generate responses using the model\r\n        model = st.session_state.model\r\n        tokenizer = st.session_state.tokenizer\r\n        content_list = []  # Store the content (text or image) that will be passed into the model\r\n        imageFile = None\r\n\r\n        with st.chat_message(A_NAME, avatar=\"assistant\"):\r\n            # Handle different inputs depending on the mode selected by the user\r\n            if selected_mode == \"Single Image\":\r\n                # Single image mode: pass in the last uploaded image\r\n                print(\"Single Images mode in use\")\r\n                if len(st.session_state.chat_history) > 1 and len(st.session_state.uploaded_image_list) >= 1:\r\n                    uploaded_image = st.session_state.uploaded_image_list[-1]\r\n                    if uploaded_image:\r\n                        imageFile = Image.open(uploaded_image).convert('RGB')\r\n                        content_list.append(imageFile)\r\n                else:\r\n                    print(\"Single Images mode: No image found\")\r\n\r\n            elif selected_mode == \"Multiple Images\":\r\n                # Multi-image mode: pass in all the images uploaded last time\r\n                print(\"Multiple Images mode in use\")\r\n                if len(st.session_state.chat_history) > 1 and st.session_state.uploaded_image_num >= 1:\r\n                    for uploaded_image in st.session_state.uploaded_image_list:\r\n                        imageFile = Image.open(uploaded_image).convert('RGB')\r\n                        content_list.append(imageFile)\r\n                else:\r\n                    print(\"Multiple Images mode: No image found\")\r\n\r\n            elif selected_mode == \"Video\":\r\n                # Video mode: pass in slice frames of uploaded video\r\n                print(\"Video mode in use\")\r\n                if len(st.session_state.chat_history) > 1 and st.session_state.uploaded_video_num == 1:\r\n                    uploaded_video_path = st.session_state.uploaded_video_list[-1]\r\n                    if uploaded_video_path:\r\n                        with st.spinner('Encoding your video, please wait...'):\r\n                            frames = encode_video(uploaded_video_path)\r\n                else:\r\n                    print(\"Video Mode: No video found\")\r\n\r\n            # Defining model parameters\r\n            params = {\r\n                'sampling': True,\r\n                'top_p': top_p,\r\n                'top_k': top_k,\r\n                'temperature': temperature,\r\n                'repetition_penalty': repetition_penalty,\r\n                \"max_new_tokens\": max_length,\r\n                \"stream\": True\r\n            }\r\n\r\n            # Set different input parameters depending on whether to upload a video\r\n            if st.session_state.uploaded_video_num == 1 and selected_mode == \"Video\":\r\n                msgs = [{\"role\": \"user\", \"content\": frames + [user_text]}]\r\n                # Set decode params for video\r\n                params[\"max_inp_length\"] = 4352  # Set the maximum input length of the video mode\r\n                params[\"use_image_id\"] = False  # Do not use image_id\r\n                params[\"max_slice_nums\"] = 1  # # use 1 if cuda OOM and video resolution >  448*448\r\n            else:\r\n                content_list.append(user_text)\r\n                msgs = [{\"role\": \"user\", \"content\": content_list}]\r\n\r\n            print(\"content_list:\", content_list)  # debug\r\n            print(\"params:\", params)  # debug\r\n\r\n            # Generate and display the model's responses\r\n            with st.spinner('AI is thinking...'):\r\n                response = model.chat(image=None, msgs=msgs, context=None, tokenizer=tokenizer, **params)\r\n            st.session_state.response = st.write_stream(response)\r\n            st.session_state.chat_history.append({\r\n                \"role\": \"model\",\r\n                \"content\": st.session_state.response,\r\n                \"image\": None,\r\n                \"video\": None\r\n            })\r\n\r\n        st.divider()  # Add separators to the interface\r\n\r\n"
        },
        {
          "name": "web_demo_streamlit.py",
          "type": "blob",
          "size": 3.904296875,
          "content": "import streamlit as st\r\nfrom PIL import Image\r\nimport torch\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\n# Model path\r\nmodel_path = \"openbmb/MiniCPM-V-2\"\r\n\r\n# User and assistant names\r\nU_NAME = \"User\"\r\nA_NAME = \"Assistant\"\r\n\r\n# Set page configuration\r\nst.set_page_config(\r\n    page_title=\"Minicpm-V-2 Streamlit\",\r\n    page_icon=\":robot:\",\r\n    layout=\"wide\"\r\n)\r\n\r\n# Load model and tokenizer\r\n@st.cache_resource\r\ndef load_model_and_tokenizer():\r\n    print(f\"load_model_and_tokenizer from {model_path}\")\r\n    model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16).to(\r\n        device=\"cuda:0\", dtype=torch.bfloat16)\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n    return model, tokenizer\r\n\r\n# Initialize session state\r\nif 'model' not in st.session_state:\r\n    st.session_state.model, st.session_state.tokenizer = load_model_and_tokenizer()\r\n    print(\"model and tokenizer had loaded completed!\")\r\n\r\n# Initialize session state\r\nif 'chat_history' not in st.session_state:\r\n    st.session_state.chat_history = []\r\n\r\n# Sidebar settings\r\nsidebar_name = st.sidebar.title(\"Minicpm-V-2 Streamlit\")\r\nmax_length = st.sidebar.slider(\"max_length\", 0, 4096, 2048, step=2)\r\ntop_p = st.sidebar.slider(\"top_p\", 0.0, 1.0, 0.8, step=0.01)\r\ntemperature = st.sidebar.slider(\"temperature\", 0.0, 1.0, 0.7, step=0.01)\r\n\r\n# Clear chat history button\r\nbuttonClean = st.sidebar.button(\"Clear chat history\", key=\"clean\")\r\nif buttonClean:\r\n    st.session_state.chat_history = []\r\n    st.session_state.response = \"\"\r\n    if torch.cuda.is_available():\r\n        torch.cuda.empty_cache()\r\n    st.rerun()\r\n\r\n# Display chat history\r\nfor i, message in enumerate(st.session_state.chat_history):\r\n    if message[\"role\"] == \"user\":\r\n        with st.chat_message(name=\"user\", avatar=\"user\"):\r\n            if message[\"image\"] is not None:\r\n                st.image(message[\"image\"], caption='User uploaded image', width=468, use_column_width=False)\r\n                continue\r\n            elif message[\"content\"] is not None:\r\n                st.markdown(message[\"content\"])\r\n    else:\r\n        with st.chat_message(name=\"model\", avatar=\"assistant\"):\r\n            st.markdown(message[\"content\"])\r\n\r\n# Select mode\r\nselected_mode = st.sidebar.selectbox(\"Select mode\", [\"Text\", \"Image\"])\r\nif selected_mode == \"Image\":\r\n    # Image mode\r\n    uploaded_image = st.sidebar.file_uploader(\"Upload image\", key=1, type=[\"jpg\", \"jpeg\", \"png\"], accept_multiple_files=False)\r\n    if uploaded_image is not None:\r\n        st.image(uploaded_image, caption='User uploaded image', width=468, use_column_width=False)\r\n        # Add uploaded image to chat history\r\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": None, \"image\": uploaded_image})\r\n\r\n# User input box\r\nuser_text = st.chat_input(\"Enter your question\")\r\nif user_text:\r\n    with st.chat_message(U_NAME, avatar=\"user\"):\r\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_text, \"image\": None})\r\n        st.markdown(f\"{U_NAME}: {user_text}\")\r\n\r\n    # Generate reply using the model\r\n    model = st.session_state.model\r\n    tokenizer = st.session_state.tokenizer\r\n\r\n    with st.chat_message(A_NAME, avatar=\"assistant\"):\r\n        # If the previous message contains an image, pass the image to the model\r\n        if len(st.session_state.chat_history) > 1 and st.session_state.chat_history[-2][\"image\"] is not None:\r\n            uploaded_image = st.session_state.chat_history[-2][\"image\"]\r\n            imagefile = Image.open(uploaded_image).convert('RGB')\r\n\r\n        msgs = [{\"role\": \"user\", \"content\": user_text}]\r\n        res, context, _ = model.chat(image=imagefile, msgs=msgs, context=None, tokenizer=tokenizer,\r\n                                     sampling=True,top_p=top_p,temperature=temperature)\r\n        st.markdown(f\"{A_NAME}: {res}\")\r\n        st.session_state.chat_history.append({\"role\": \"model\", \"content\": res, \"image\": None})\r\n\r\n    st.divider()\r\n"
        }
      ]
    }
  ]
}