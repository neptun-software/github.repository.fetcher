{
  "metadata": {
    "timestamp": 1736561323808,
    "page": 338,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "coleifer/peewee",
      "stars": 11314,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.16015625,
          "content": "*.pyc\nbuild\nprof/\ndocs/_build/\nplayhouse/*.c\nplayhouse/*.h\nplayhouse/*.so\nplayhouse/tests/peewee_test.db\n.idea/\nMANIFEST\npeewee_test.db\nclosure.so\nlsm.so\nregexp.so\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.16015625,
          "content": "version: 2\npython:\n  install:\n  - requirements: docs/requirements.txt\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n\nsphinx:\n  configuration: docs/conf.py\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.818359375,
          "content": "language: python\npython:\n  - 2.7\n  - 3.4\n  - 3.5\n  - 3.6\nenv:\n  - PEEWEE_TEST_BACKEND=sqlite\n  - PEEWEE_TEST_BACKEND=postgresql\n  - PEEWEE_TEST_BACKEND=mysql\nmatrix:\n  include:\n    - python: 3.7\n      dist: xenial\n      env: PEEWEE_TEST_BACKEND=sqlite\n    - python: 3.7\n      dist: xenial\n      env: PEEWEE_TEST_BACKEND=postgresql\n    - python: 3.7\n      dist: xenial\n      env: PEEWEE_TEST_BACKEND=mysql\n    - python: 3.8\n      dist: xenial\n    - python: 3.7\n      dist: xenial\n      env:\n      - PEEWEE_TEST_BUILD_SQLITE=1\n      - PEEWEE_CLOSURE_EXTENSION=/usr/local/lib/closure.so\n      - LSM_EXTENSION=/usr/local/lib/lsm.so\n      before_install:\n        - sudo apt-get install -y tcl-dev\n        - ./.travis_deps.sh\n        - sudo ldconfig\n      script: \"python runtests.py -v2\"\n    - python: 3.7\n      dist: xenial\n      env:\n      - PEEWEE_TEST_BACKEND=cockroachdb\n      before_install:\n        - wget -qO- https://binaries.cockroachdb.com/cockroach-v20.1.1.linux-amd64.tgz | tar xvz\n        - ./cockroach-v20.1.1.linux-amd64/cockroach start --insecure --background\n        - ./cockroach-v20.1.1.linux-amd64/cockroach sql --insecure -e 'create database peewee_test;'\n  allow_failures:\naddons:\n  postgresql: \"9.6\"\n  mariadb: \"10.3\"\nservices:\n  - postgresql\n  - mariadb\ninstall: \"pip install psycopg2-binary Cython pymysql apsw mysql-connector\"\nbefore_script:\n  - python setup.py build_ext -i\n  - psql -c 'drop database if exists peewee_test;' -U postgres\n  - psql -c 'create database peewee_test;' -U postgres\n  - psql peewee_test -c 'create extension hstore;' -U postgres\n  - mysql -e 'drop user if exists travis@localhost;'\n  - mysql -e 'create user travis@localhost;'\n  - mysql -e 'drop database if exists peewee_test;'\n  - mysql -e 'create database peewee_test;'\n  - mysql -e 'grant all on *.* to travis@localhost;' || true\nscript: \"python runtests.py\"\n"
        },
        {
          "name": ".travis_deps.sh",
          "type": "blob",
          "size": 1.0087890625,
          "content": "#!/bin/bash\n\nsetup_sqlite_deps() {\n  wget https://www.sqlite.org/src/tarball/sqlite.tar.gz\n  tar xzf sqlite.tar.gz\n  cd sqlite/\n  export CFLAGS=\"-DSQLITE_ENABLE_FTS3 \\\n    -DSQLITE_ENABLE_FTS3_PARENTHESIS \\\n    -DSQLITE_ENABLE_FTS4 \\\n    -DSQLITE_ENABLE_FTS5 \\\n    -DSQLITE_ENABLE_JSON1 \\\n    -DSQLITE_ENABLE_LOAD_EXTENSION \\\n    -DSQLITE_ENABLE_UPDATE_DELETE_LIMIT \\\n    -DSQLITE_TEMP_STORE=3 \\\n    -DSQLITE_USE_URI \\\n    -O2 \\\n    -fPIC\"\n  export PREFIX=\"/usr/local\"\n  LIBS=\"-lm\" ./configure \\\n    --disable-tcl \\\n    --enable-shared \\\n    --enable-tempstore=always \\\n    --prefix=\"$PREFIX\"\n  make && sudo make install\n\n  cd ext/misc/\n\n  # Build the transitive closure extension and copy shared library.\n  gcc -fPIC -O2 -lsqlite3 -shared closure.c -o closure.so\n  sudo cp closure.so /usr/local/lib\n\n  # Build the lsm1 extension and copy shared library.\n  cd ../lsm1\n  export CFLAGS=\"-fPIC -O2\"\n  TCCX=\"gcc -fPIC -O2\" make lsm.so\n  sudo cp lsm.so /usr/local/lib\n}\n\nif [ -n \"$PEEWEE_TEST_BUILD_SQLITE\" ]; then\n  setup_sqlite_deps\nfi\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 125.490234375,
          "content": "# Changelog\n\nTracking changes in peewee between versions.  For a complete view of all the\nreleases, visit GitHub:\n\nhttps://github.com/coleifer/peewee/releases\n\n## master\n\n[View commits](https://github.com/coleifer/peewee/compare/3.17.8...master)\n\n## 3.17.8\n\n* Fix regression in behavior of `delete_instance()` when traversing nullable\n  foreign-keys, #2952. Introduced in 3.17.6. **Recommended that you update**.\n* Fix bug where joins not cloned when going from join-less -> joined query,\n  refs #2941.\n\n## 3.17.7\n\n* Add db_url support for psycopg3 via `psycopg3://`.\n* Ensure double-quotes are escaped properly when introspecting constraints.\n* A few documentation-related fixes.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.17.6...3.17.7)\n\n## 3.17.6\n\n* Fix bug in recursive `model.delete_instance()` when a table contains\n  foreign-keys at multiple depths of the graph, #2893.\n* Fix regression in pool behavior on systems where `time.time()` returns\n  identical values for two connections. This adds a no-op comparable sentinel\n  to the heap to prevent any recurrence of this problem, #2901.\n* Ensure that subqueries inside `CASE` statements generate correct SQL.\n* Fix regression that broke server-side cursors with Postgres (introduced in\n  3.16.0).\n* Fix to ensure compatibility with psycopg3 - the libpq TransactionStatus\n  constants are no longer available on the `Connection` instance.\n* Fix quoting issue in pwiz that could generate invalid python code for\n  double-quoted string literals used as column defaults.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.17.5...3.17.6)\n\n## 3.17.5\n\nThis release fixes a build system problem in Python 3.12, #2891.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.17.4...3.17.5)\n\n## 3.17.4\n\n* Fix bug that could occur when using CASE inside a function, and one or more\n  of the CASE clauses consisted of a subquery. Refs #2873.\n  new fix in #2872 for regression in truthiness of cursor.\n* Fix bug in the conversion of TIMESTAMP type in Sqlite on Python 3.12+.\n* Fix for hybrid properties on subclasses when aliased (#2888).\n* Many fixes for SqliteQueueDatabase (#2874, #2876, #2877).\n\n[View commits](https://github.com/coleifer/peewee/compare/3.17.3...3.17.4)\n\n## 3.17.3\n\n* Better fix for #2871 (extraneous queries when coercing query to list), and\n\n[View commits](https://github.com/coleifer/peewee/compare/3.17.2...3.17.3)\n\n## 3.17.2\n\n* Full support for `psycopg3`.\n* Basic support for Sqlite `jsonb`.\n* Fix bug where calling `list(query)` resulted in extra queries, #2871\n\n[View commits](https://github.com/coleifer/peewee/compare/3.17.1...3.17.2)\n\n## 3.17.1\n\n* Add bitwise and other helper methods to `BigBitField`, #2802.\n* Add `add_column_default` and `drop_column_default` migrator methods for\n  specifying a server-side default value, #2803.\n* The new `star` attribute was causing issues for users who had a field named\n  star on their models. This attribute is now renamed to `__star__`. #2796.\n* Fix compatibility issues with 3.12 related to utcnow() deprecation.\n* Add stricter locking on connection pool to prevent race conditions.\n* Add adapters and converters to Sqlite to replace ones deprecated in 3.12.\n* Fix bug in `model_to_dict()` when only aliases are present.\n* Fix version check for Sqlite native drop column support.\n* Do not specify a `reconnect=` argument to `ping()` if using MySQL 8.x.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.17.0...3.17.1)\n\n## 3.17.0\n\n* Only roll-back in the outermost `@db.transaction` decorator/ctx manager if\n  an unhandled exception occurs. Previously, an unhandled exception that\n  occurred in a nested `transaction` context would trigger a rollback. The use\n  of nested `transaction` has long been discouraged in the documentation: the\n  recommendation is to always use `db.atomic`, which will use savepoints to\n  properly handle nested blocks. However, the new behavior should make it\n  easier to reason about transaction boundaries - see #2767 for discussion.\n* Cover transaction `BEGIN` in the reconnect-mixin. Given that no transaction\n  has been started, reconnecting when beginning a new transaction ensures that\n  a reconnect will occur if it is safe to do so.\n* Add support for setting `isolation_level` in `db.atomic()` and\n  `db.transaction()` when using Postgres and MySQL/MariaDB, which will apply to\n  the wrapped transaction. Note: Sqlite has supported a similar `lock_type`\n  parameter for some time.\n* Add support for the Sqlite `SQLITE_DETERMINISTIC` function flag. This allows\n  user-defined Sqlite functions to be used in indexes and may be used by the\n  query planner.\n* Fix unreported bug in dataset import when inferred field name differs from\n  column name.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.16.3...3.17.0)\n\n## 3.16.3\n\n* Support for Cython 3.0.\n* Add flag to `ManyToManyField` to prevent setting/getting values on unsaved\n  instances. This is worthwhile, since reading or writing a many-to-many has no\n  meaning when the instance is unsaved.\n* Adds a `star()` helper to `Source` base-class for selecting all columns.\n* Fix missing `binary` types for mysql-connector and mariadb-connector.\n* Add `extract()` method to MySQL `JSONField` for extracting a jsonpath.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.16.2...3.16.3)\n\n## 3.16.2\n\nFixes a longstanding issue with thread-safety of various decorators, including\n`atomic()`, `transaction()`, `savepoint()`. The context-managers are\nunaffected. See #2709 for details.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.16.1...3.16.2)\n\n## 3.16.1\n\n* Add changes required for building against Cython 3.0 and set Cython\n  language-level to 3.\n* Ensure indexes aren't added to unindexed fields during introspection, #2691.\n* Ensure we don't redundantly select same PK in prefetch when using\n  PREFETCH_TYPE.JOIN.\n* In Sqlite migrator, use Sqlite's builtin DROP and RENAME column facilities\n  when possible. This can be overridden by passing `legacy=True` flag.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.16.0...3.16.1)\n\n## 3.16.0\n\nThis release contains backwards-incompatible changes in the way Peewee\ninitializes connections to the underlying database driver. Previously, peewee\nimplemented autocommit semantics *on-top* of the existing DB-API transactional\nworkflow. Going forward, Peewee instead places the DB-API driver into\nautocommit mode directly.\n\nWhy this change?\n\nPreviously, Peewee emulated autocommit behavior for top-level queries issued\noutside of a transaction. This necessitated a number of checks which had to be\nperformed each time a query was executed, so as to ensure that we didn't end up\nwith uncommitted writes or, conversely, idle read transactions. By running the\nunderlying driver in autocommit mode, we can eliminate all these checks, since\nwe are already managing transactions ourselves.\n\nBehaviorally, there should be no change -- Peewee will still treat top-level\nqueries outside of transactions as being autocommitted, while queries inside of\n`atomic()` / `with db:` blocks are implicitly committed at the end of the\nblock, or rolled-back if an exception occurs.\n\n**How might this affect me?**\n\n* If you are using the underlying database connection or cursors, e.g. via\n  `Database.connection()` or `Database.cursor()`, your queries will now be\n  executed in autocommit mode.\n* The `commit=` argument is deprecated for the `cursor()`, `execute()` and\n  `execute_sql()` methods.\n* If you have a custom `Database` implementation (whether for a database that\n  is not officially supported, or for the purpose of overriding default\n  behaviors), you will want to ensure that your connections are opened in\n  autocommit mode.\n\nOther changes:\n\n* Some fixes to help with packaging in Python 3.11.\n* MySQL `get_columns()` implementation now returns columns in their declared\n  order.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.15.4...3.16.0)\n\n## 3.15.4\n\n* Raise an exception in `ReconnectMixin` if connection is lost while inside a\n  transaction (if the transaction was interrupted presumably some changes were\n  lost and explicit intervention is needed).\n* Add `db.Model` property to reduce boilerplate.\n* Add support for running `prefetch()` queries with joins instead of subqueries\n  (this helps overcome a MySQL limitation about applying LIMITs to a subquery).\n* Add SQL `AVG` to whitelist to avoid coercing by default.\n* Allow arbitrary keywords in metaclass constructor, #2627\n* Add a `pyproject.toml` to silence warnings from newer pips when `wheel`\n  package is not available.\n\nThis release has a small helper for reducing boilerplate in some cases by\nexposing a base model class as an attribute of the database instance.\n\n```python\n# old:\ndb = SqliteDatabase('...')\n\nclass BaseModel(Model):\n    class Meta:\n        database = db\n\nclass MyModel(BaseModel):\n    pass\n\n# new:\ndb = SqliteDatabase('...')\n\nclass MyModel(db.Model):\n    pass\n```\n\n[View commits](https://github.com/coleifer/peewee/compare/3.15.3...3.15.4)\n\n## 3.15.3\n\n* Add `scalars()` query method (complements `scalar()`), roughly equivalent to\n  writing `[t[0] for t in query.tuples()]`.\n* Small doc improvements\n* Fix and remove some flaky test assertions with Sqlite INSERT + RETURNING.\n* Fix innocuous failing Sqlite test on big-endian machines.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.15.2...3.15.3)\n\n## 3.15.2\n\n* Fix bug where field-specific conversions were being applied to the pattern\n  used for LIKE / ILIKE operations. Refs #2609\n* Fix possible infinite loop when accidentally invoking the `__iter__` method\n  on certain `Column` subclasses. Refs #2606\n* Add new helper for specifying which Model a particular selected column-like\n  should be bound to, in queries with joins that select from multiple sources.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.15.1...3.15.2)\n\n## 3.15.1\n\n* Fix issue introduced in Sqlite 3.39.0 regarding the propagation of column\n  subtypes in subqueries.\n* Fix bug where cockroachdb server version was not set when beginning a\n  transaction on an unopened database.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.15.0...3.15.1)\n\n## 3.15.0\n\nRollback behavior change in commit ab43376697 (GH #2026). Peewee will no longer\nautomatically return the cursor `rowcount` for certain bulk-inserts.  This\nshould mainly affect users of MySQL and Sqlite who relied on a bulk INSERT\nreturning the `rowcount` (as opposed to the cursor's `lastrowid`). The\n`rowcount` behavior is still available chaining the ``as_rowcount()`` method:\n\n```python\n# NOTE: this change only affects MySQL or Sqlite.\ndb = MySQLDatabase(...)\n\n# Previously, bulk inserts of the following forms would return the rowcount.\nquery = User.insert_many(...)  # Bulk insert.\nquery = User.insert_from(...)  # Bulk insert (INSERT INTO .. SELECT FROM).\n\n# Previous behavior (peewee 3.12 - 3.14.10):\n# rows_inserted = query.execute()\n\n# New behavior:\nlast_id = query.execute()\n\n# To get the old behavior back:\nrows_inserted = query.as_rowcount().execute()\n```\n\nAdditionally, in previous versions specifying an empty `.returning()` with\nPostgres would cause the rowcount to be returned. For Postgres users who wish\nto receive the rowcount:\n\n```python\n# NOTE: this change only affects Postgresql.\ndb = PostgresqlDatabase(...)\n\n# Previously, an empty returning() would return the rowcount.\nquery = User.insert_many(...)  # Bulk insert.\nquery = User.insert_from(...)  # Bulk insert (INSERT INTO .. SELECT FROM).\n\n# Old behavior:\n# rows_inserted = query.returning().execute()\n\n# To get the rows inserted in 3.15 and newer:\nrows_inserted = query.as_rowcount().execute()\n```\n\nThis release contains a fix for a long-standing request to allow data-modifying\nqueries to support CTEs. CTEs are now supported for use with INSERT, DELETE and\nUPDATE queries - see #2152.\n\nAdditionally, this release adds better support for using the new `RETURNING`\nsyntax with Sqlite automatically. Specify `returning_clause=True` when\ninitializing your `SqliteDatabase` and all bulk inserts will automatically\nspecify a `RETURNING` clause, returning the newly-inserted primary keys. This\nfunctionality requires Sqlite 3.35 or newer.\n\nSmaller changes:\n\n* Add `shortcuts.insert_where()` helper for generating conditional INSERT with\n  a bit less boilerplate.\n* Fix bug in `test_utils.count_queres()` which could erroneously include pool\n  events such as connect/disconnect, etc.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.10...3.15.0)\n\n## 3.14.10\n\n* Add shortcut for conditional insert using sub-select, see #2528\n* Add convenience `left_outer_join()` method to query.\n* Add `selected_columns` property to Select queries.\n* Add `name` property to Alias instances.\n* Fix regression in tests introduced by change to DataSet in 3.14.9.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.9...3.14.10)\n\n## 3.14.9\n\n* Allow calling `table_exists()` with a model-class, refs\n* Improve `is_connection_usable()` method of `MySQLDatabase` class.\n* Better support for VIEWs with `playhouse.dataset.DataSet` and sqlite-web.\n* Support INSERT / ON CONFLICT in `playhosue.kv` for newer Sqlite.\n* Add `ArrayField.contained_by()` method, a corollary to `contains()` and\n  the `contains_any()` methods.\n* Support cyclical foreign-key relationships in reflection/introspection, and\n  also for sqlite-web.\n* Add magic methods for FTS5 field to optimize, rebuild and integrity check the\n  full-text index.\n* Add fallbacks in `setup.py` in the event distutils is not available.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.8...3.14.9)\n\n## 3.14.8\n\nBack-out all changes to automatically use RETURNING for `SqliteExtDatabase`,\n`CSqliteExtDatabase` and `APSWDatabase`. The issue I found is that when a\nRETURNING cursor is not fully-consumed, any parent SAVEPOINT (and possibly\ntransaction) would not be able to be released. Since this is a\nbackwards-incompatible change, I am going to back it out for now.\n\nReturning clause can still be specified for Sqlite, however it just needs to be\ndone so manually rather than having it applied automatically.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.7...3.14.8)\n\n## 3.14.7\n\nFix bug in APSW extension with Sqlite 3.35 and newer, due to handling of last\ninsert rowid with RETURNING. Refs #2479.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.6...3.14.7)\n\n## 3.14.6\n\nFix pesky bug in new `last_insert_id()` on the `SqliteExtDatabase`.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.5...3.14.6)\n\n## 3.14.5\n\nThis release contains a number of bug-fixes and small improvements.\n\n* Only raise `DoesNotExist` when `lazy_load` is enabled on ForeignKeyField,\n  fixes issue #2377.\n* Add missing convenience method `ModelSelect.get_or_none()`\n* Allow `ForeignKeyField` to specify a custom `BackrefAccessorClass`,\n  references issue #2391.\n* Ensure foreign-key-specific conversions are applied on INSERT and UPDATE,\n  fixes #2408.\n* Add handling of MySQL error 4031 (inactivity timeout) to the `ReconnectMixin`\n  helper class. Fixes #2419.\n* Support specification of conflict target for ON CONFLICT/DO NOTHING.\n* Add `encoding` parameter to the DataSet `freeze()` and `thaw()` methods,\n  fixes #2425.\n* Fix bug which prevented `DeferredForeignKey` from being used as a model's\n  primary key, fixes #2427.\n* Ensure foreign key's related object cache is cleared when the foreign-key is\n  set to `None`. Fixes #2428.\n* Allow specification of `(schema, table)` to be used with CREATE TABLE AS...,\n  fixes #2423.\n* Allow reusing open connections with DataSet, refs #2441.\n* Add `highlight()` and `snippet()` helpers to Sqlite `SearchField`, for use\n  with full-text search extension.\n* Preserve user-provided aliases in column names. Fixes #2453.\n* Add support for Sqlite 3.37 strict tables.\n* Ensure database is inherited when using `ThreadSafeDatabaseMetadata`, and\n  also adds an implementation in `playhouse.shortcuts` along with basic unit\n  tests.\n* Better handling of Model's dirty fields when saving, fixes #2466.\n* Add basic support for MariaDB connector driver in `playhouse.mysql_ext`, refs\n  issue #2471.\n* Begin a basic implementation for a psycopg3-compatible pg database, refs\n  issue #2473.\n* Add provisional support for RETURNING when using the appropriate versions of\n  Sqlite or MariaDB.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.4...3.14.5)\n\n## 3.14.4\n\nThis release contains an important fix for a regression introduced by commit\nebe3ad5, which affected the way model instances are converted to parameters for\nuse in expressions within a query. The bug could manifest when code uses model\ninstances as parameters in expressions against fields that are not\nforeign-keys.\n\nThe issue is described in #2376.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.3...3.14.4)\n\n## 3.14.3\n\nThis release contains a single fix for ensuring NULL values are inserted when\nissuing a bulk-insert of heterogeneous dictionaries which may be missing\nexplicit NULL values. Fixes issue #2638.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.2...3.14.3)\n\n## 3.14.2\n\nThis is a small release mainly to get some fixes out.\n\n* Support for named `Check` and foreign-key constraints.\n* Better foreign-key introspection for CockroachDB (and Postgres).\n* Register UUID adapter for Postgres.\n* Add `fn.array_agg()` to blacklist for automatic value coercion.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.1...3.14.2)\n\n## 3.14.1\n\nThis release contains primarily bugfixes.\n\n* Properly delegate to a foreign-key field's `db_value()` function when\n  converting model instances. #2304.\n* Strip quote marks and parentheses from column names returned by sqlite\n  cursor when a function-call is projected without an alias. #2305.\n* Fix `DataSet.create_index()` method, #2319.\n* Fix column-to-model mapping in model-select from subquery with joins, #2320.\n* Improvements to foreign-key lazy-loading thanks @conqp, #2328.\n* Preserve and handle `CHECK()` constraints in Sqlite migrator, #2343.\n* Add `stddev` aggregate function to collection of sqlite user-defined funcs.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.14.0...3.14.1)\n\n## 3.14.0\n\nThis release has been a bit overdue and there are numerous small improvements\nand bug-fixes. The bugfix that prompted this release is #2293, which is a\nregression in the Django-inspired `.filter()` APIs that could cause some\nfilter expressions to be discarded from the generated SQL. Many thanks for the\nexcellent bug report, Jakub.\n\n* Add an experimental helper, `shortcuts.resolve_multimodel_query()`, for\n  resolving multiple models used in a compound select query.\n* Add a `lateral()` method to select query for use with lateral joins, refs\n  issue #2205.\n* Added support for nested transactions (savepoints) in cockroach-db (requires\n  20.1 or newer).\n* Automatically escape wildcards passed to string-matching methods, refs #2224.\n* Allow index-type to be specified on MySQL, refs #2242.\n* Added a new API, `converter()` to be used for specifying a function to use to\n  convert a row-value pulled off the cursor, refs #2248.\n* Add `set()` and `clear()` method to the bitfield flag descriptor, refs #2257.\n* Add support for `range` types with `IN` and other expressions.\n* Support CTEs bound to compound select queries, refs #2289.\n\n### Bug-fixes\n\n* Fix to return related object id when accessing via the object-id descriptor,\n  when the related object is not populated, refs #2162.\n* Fix to ensure we do not insert a NULL value for a primary key.\n* Fix to conditionally set the field/column on an added column in a migration,\n  refs #2171.\n* Apply field conversion logic to model-class values. Relocates the logic from\n  issue #2131 and fixes #2185.\n* Clone node before modifying it to be flat in an enclosed nodelist expr, fixes\n  issue #2200.\n* Fix an invalid item assignment in nodelist, refs #2220.\n* Fix an incorrect truthiness check used with `save()` and `only=`, refs #2269.\n* Fix regression in `filter()` where using both `*args` and `**kwargs` caused\n  the expressions passed as `args` to be discarded. See #2293.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.13.3...3.14.0)\n\n## 3.13.3\n\n* Allow arbitrary keyword arguments to be passed to `DataSet` constructor,\n  which are then passed to the instrospector.\n* Allow scalar subqueries to be compared using numeric operands.\n* Fix `bulk_create()` when model being inserted uses FK identifiers.\n* Fix `bulk_update()` so that PK values are properly coerced to the right\n  data-type (e.g. UUIDs to strings for Sqlite).\n* Allow array indices to be used as dict keys, e.g. for the purposes of\n  updating a single array index value.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.13.2...3.13.3)\n\n## 3.13.2\n\n* Allow aggregate functions to support an `ORDER BY` clause, via the addition\n  of an `order_by()` method to the function (`fn`) instance. Refs #2094.\n* Fix `prefetch()` bug, where related \"backref\" instances were marked as dirty,\n  even though they had no changes. Fixes #2091.\n* Support `LIMIT 0`. Previously a limit of 0 would be translated into\n  effectively an unlimited query on MySQL. References #2084.\n* Support indexing into arrays using expressions with Postgres array fields.\n  References #2085.\n* Ensure postgres introspection methods return the columns for multi-column\n  indexes in the correct order. Fixes #2104.\n* Add support for arrays of UUIDs to postgres introspection.\n* Fix introspection of columns w/capitalized table names in postgres (#2110).\n* Fix to ensure correct exception is raised in SqliteQueueDatabase when\n  iterating over cursor/result-set.\n* Fix bug comparing subquery against a scalar value. Fixes #2118.\n* Fix issue resolving composite primary-keys that include foreign-keys when\n  building the model-graph. Fixes #2115.\n* Allow model-classes to be passed as arguments, e.g., to a table function.\n  Refs #2131.\n* Ensure postgres `JSONField.concat()` accepts expressions as arguments.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.13.1...3.13.2)\n\n## 3.13.1\n\nFix a regression when specifying keyword arguments to the `atomic()` or\n`transaction()` helper methods. Note: this only occurs if you were using Sqlite\nand were explicitly setting the `lock_type=` parameter.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.13.0...3.13.1)\n\n## 3.13.0\n\n### CockroachDB support added\n\nThis will be a notable release as it adds support for\n[CockroachDB](https://cockroachlabs.com/), a distributed, horizontally-scalable\nSQL database.\n\n* [CockroachDB usage overview](http://docs.peewee-orm.com/en/latest/peewee/database.html#using-crdb)\n* [CockroachDB API documentation](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#crdb)\n\n### Other features and fixes\n\n* Allow `FOR UPDATE` clause to specify one or more tables (`FOR UPDATE OF...`).\n* Support for Postgres `LATERAL` join.\n* Properly wrap exceptions raised during explicit commit/rollback in the\n  appropriate peewee-specific exception class.\n* Capture original exception object and expose it as `exc.orig` on the\n  wrapped exception.\n* Properly introspect `SMALLINT` columns in Postgres schema reflection.\n* More flexible handling of passing database-specific arguments to `atomic()`\n  and `transaction()` context-manager/decorator.\n* Fix non-deterministic join ordering issue when using the `filter()` API\n  across several tables (#2063).\n\n[View commits](https://github.com/coleifer/peewee/compare/3.12.0...3.13.0)\n\n## 3.12.0\n\n* Bulk insert (`insert_many()` and `insert_from()`) will now return the row\n  count instead of the last insert ID. If you are using Postgres, peewee will\n  continue to return a cursor that provides an iterator over the newly-inserted\n  primary-key values by default. This behavior is being retained by default for\n  compatibility. Postgres users can simply specify an empty `returning()` call\n  to disable the cursor and retrieve the rowcount instead.\n* Migration extension now supports altering a column's data-type, via the new\n  `alter_column_type()` method.\n* Added `Database.is_connection_usabe()` method, which attempts to look at the\n  status of the underlying DB-API connection to determine whether the\n  connection is usable.\n* Common table expressions include a `materialized` parameter, which can be\n  used to control Postgres' optimization fencing around CTEs.\n* Added `BloomFilter.from_buffer()` method for populating a bloom-filter from\n  the output of a previous call to the `to_buffer()` method.\n* Fixed APSW extension's `commit()` and `rollback()` methods to no-op if the\n  database is in auto-commit mode.\n* Added `generate_always=` option to the `IdentityField` (defaults to False).\n\n[View commits](https://github.com/coleifer/peewee/compare/3.11.2...3.12.0)\n\n## 3.11.2\n\n* Implement `hash` interface for `Alias` instances, allowing them to be used in\n  multi-source queries.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.11.1...3.11.2)\n\n## 3.11.1\n\n* Fix bug in new `_pk` / `get_id()` implementation for models that explicitly\n  have disabled a primary-key.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.11.0...3.11.1)\n\n## 3.11.0\n\n* Fixes #1991. This particular issue involves joining 3 models together in a\n  chain, where the outer two models are empty. Previously peewee would make the\n  middle model an empty model instance (since a link might be needed from the\n  source model to the outermost model). But since both were empty, it is more\n  correct to make the intervening model a NULL value on the foreign-key field\n  rather than an empty instance.\n* An unrelated fix came out of the work on #1991 where hashing a model whose\n  primary-key happened to be a foreign-key could trigger the FK resolution\n  query. This patch fixes the `Model._pk` and `get_id()` interfaces so they\n  no longer introduce the possibility of accidentally resolving the FK.\n* Allow `Field.contains()`, `startswith()` and `endswith()` to compare against\n  another column-like object or expression.\n* Workaround for MySQL prior to 8 and MariaDB handling of union queries inside\n  of parenthesized expressions (like IN).\n* Be more permissive in letting invalid values be stored in a field whose type\n  is INTEGER or REAL, since Sqlite allows this.\n* `TimestampField` resolution cleanup. Now values 0 *and* 1 will resolve to a\n  timestamp resolution of 1 second. Values 2-6 specify the number of decimal\n  places (hundredths to microsecond), or alternatively the resolution can still\n  be provided as a power of 10, e.g. 10, 1000 (millisecond), 1e6 (microsecond).\n* When self-referential foreign-keys are inherited, the foreign-key on the\n  subclass will also be self-referential (rather than pointing to the parent\n  model).\n* Add TSV import/export option to the `dataset` extension.\n* Add item interface to the `dataset.Table` class for doing primary-key lookup,\n  assignment, or deletion.\n* Extend the mysql `ReconnectMixin` helper to work with mysql-connector.\n* Fix mapping of double-precision float in postgres schema reflection.\n  Previously it mapped to single-precision, now it correctly uses a double.\n* Fix issue where `PostgresqlExtDatabase` and `MySQLConnectorDatabase` did not\n  respect the `autoconnect` setting.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.10.0...3.11.0)\n\n## 3.10.0\n\n* Add a helper to `playhouse.mysql_ext` for creating `Match` full-text search\n  expressions.\n* Added date-part properties to `TimestampField` for accessing the year, month,\n  day, etc., within a SQL expression.\n* Added `to_timestamp()` helper for `DateField` and `DateTimeField` that\n  produces an expression returning a unix timestamp.\n* Add `autoconnect` parameter to `Database` classes. This parameter defaults to\n  `True` and is compatible with previous versions of Peewee, in which executing\n  a query on a closed database would open a connection automatically. To make\n  it easier to catch inconsistent use of the database connection, this behavior\n  can now be disabled by specifying `autoconnect=False`, making an explicit\n  call to `Database.connect()` needed before executing a query.\n* Added database-agnostic interface for obtaining a random value.\n* Allow `isolation_level` to be specified when initializing a Postgres db.\n* Allow hybrid properties to be used on model aliases. Refs #1969.\n* Support aggregates with FILTER predicates on the latest Sqlite.\n\n#### Changes\n\n* More aggressively slot row values into the appropriate field when building\n  objects from the database cursor (rather than using whatever\n  `cursor.description` tells us, which is buggy in older Sqlite).\n* Be more permissive in what we accept in the `insert_many()` and `insert()`\n  methods.\n* When implicitly joining a model with multiple foreign-keys, choose the\n  foreign-key whose name matches that of the related model. Previously, this\n  would have raised a `ValueError` stating that multiple FKs existed.\n* Improved date truncation logic for Sqlite and MySQL to make more compatible\n  with Postgres' `date_trunc()` behavior. Previously, truncating a datetime to\n  month resolution would return `'2019-08'` for example. As of 3.10.0, the\n  Sqlite and MySQL `date_trunc` implementation returns a full datetime, e.g.\n  `'2019-08-01 00:00:00'`.\n* Apply slightly different logic for casting JSON values with Postgres.\n  Previously, Peewee just wrapped the value in the psycopg2 `Json()` helper.\n  In this version, Peewee now dumps the json to a string and applies an\n  explicit cast to the underlying JSON data-type (e.g. json or jsonb).\n\n#### Bug fixes\n\n* Save hooks can now be called for models without a primary key.\n* Fixed bug in the conversion of Python values to JSON when using Postgres.\n* Fix for differentiating empty values from NULL values in `model_to_dict`.\n* Fixed a bug referencing primary-key values that required some kind of\n  conversion (e.g., a UUID). See #1979 for details.\n* Add small jitter to the pool connection timestamp to avoid issues when\n  multiple connections are checked-out at the same exact time.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.9.6...3.10.0)\n\n## 3.9.6\n\n* Support nesting the `Database` instance as a context-manager. The outermost\n  block will handle opening and closing the connection along with wrapping\n  everything in a transaction. Nested blocks will use savepoints.\n* Add new `session_start()`, `session_commit()` and `session_rollback()`\n  interfaces to the Database object to support using transactional controls in\n  situations where a context-manager or decorator is awkward.\n* Fix error that would arise when attempting to do an empty bulk-insert.\n* Set `isolation_level=None` in SQLite connection constructor rather than\n  afterwards using the setter.\n* Add `create_table()` method to `Select` query to implement `CREATE TABLE AS`.\n* Cleanup some declarations in the Sqlite C extension.\n* Add new example showing how to implement Reddit's ranking algorithm in SQL.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.9.5...3.9.6)\n\n## 3.9.5\n\n* Added small helper for setting timezone when using Postgres.\n* Improved SQL generation for `VALUES` clause.\n* Support passing resolution to `TimestampField` as a power-of-10.\n* Small improvements to `INSERT` queries when the primary-key is not an\n  auto-incrementing integer, but is generated by the database server (eg uuid).\n* Cleanups to virtual table implementation and python-to-sqlite value\n  conversions.\n* Fixed bug related to binding previously-unbound models to a database using a\n  context manager, #1913.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.9.4...3.9.5)\n\n## 3.9.4\n\n* Add `Model.bulk_update()` method for bulk-updating fields across multiple\n  model instances. [Docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#Model.bulk_update).\n* Add `lazy_load` parameter to `ForeignKeyField`. When initialized with\n  `lazy_load=False`, the foreign-key will not use an additional query to\n  resolve the related model instance. Instead, if the related model instance is\n  not available, the underlying FK column value is returned (behaving like the\n  \"_id\" descriptor).\n* Added `Model.truncate_table()` method.\n* The `reflection` and `pwiz` extensions now attempt to be smarter about\n  converting database table and column names into snake-case. To disable this,\n  you can set `snake_case=False` when calling the `Introspector.introspect()`\n  method or use the `-L` (legacy naming) option with the `pwiz` script.\n* Bulk insert via ``insert_many()`` no longer require specification of the\n  fields argument when the inserted rows are lists/tuples. In that case, the\n  fields will be inferred to be all model fields except any auto-increment id.\n* Add `DatabaseProxy`, which implements several of the `Database` class context\n  managers. This allows you to reference some of the special features of the\n  database object without directly needing to initialize the proxy first.\n* Add support for window function frame exclusion and added built-in support\n  for the GROUPS frame type.\n* Add support for chaining window functions by extending a previously-declared\n  window function.\n* Playhouse Postgresql extension `TSVectorField.match()` method supports an\n  additional argument `plain`, which can be used to control the parsing of the\n  TS query.\n* Added very minimal `JSONField` to the playhouse MySQL extension.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.9.3...3.9.4)\n\n## 3.9.3\n\n* Added cross-database support for `NULLS FIRST/LAST` when specifying the\n  ordering for a query. Previously this was only supported for Postgres. Peewee\n  will now generate an equivalent `CASE` statement for Sqlite and MySQL.\n* Added [EXCLUDED](http://docs.peewee-orm.com/en/latest/peewee/api.html#EXCLUDED)\n  helper for referring to the `EXCLUDED` namespace used with `INSERT...ON CONFLICT`\n  queries, when referencing values in the conflicting row data.\n* Added helper method to the model `Metadata` class for setting the table name\n  at run-time. Setting the `Model._meta.table_name` directly may have appeared\n  to work in some situations, but could lead to subtle bugs. The new API is\n  `Model._meta.set_table_name()`.\n* Enhanced helpers for working with Peewee interactively, [see doc](http://docs.peewee-orm.com/en/latest/peewee/interactive.html).\n* Fix cache invalidation bug in `DataSet` that was originally reported on the\n  sqlite-web project.\n* New example script implementing a [hexastore](https://github.com/coleifer/peewee/blob/master/examples/hexastore.py).\n\n[View commits](https://github.com/coleifer/peewee/compare/3.9.2...3.9.3)\n\n## 3.9.1 and 3.9.2\n\nIncludes a bugfix for an `AttributeError` that occurs when using MySQL with the\n`MySQLdb` client. The 3.9.2 release includes fixes for a test failure.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.9.0...3.9.2)\n\n## 3.9.0\n\n* Added new document describing how to [use peewee interactively](http://docs.peewee-orm.com/en/latest/peewee/interactive.html).\n* Added convenience functions for generating model classes from a pre-existing\n  database, printing model definitions and printing CREATE TABLE sql for a\n  model. See the \"use peewee interactively\" section for details.\n* Added a `__str__` implementation to all `Query` subclasses which converts the\n  query to a string and interpolates the parameters.\n* Improvements to `sqlite_ext.JSONField` regarding the serialization of data,\n  as well as the addition of options to override the JSON serialization and\n  de-serialization functions.\n* Added `index_type` parameter to `Field`\n* Added `DatabaseProxy`, which allows one to use database-specific decorators\n  with an uninitialized `Proxy` object. See #1842 for discussion. Recommend\n  that you update any usage of `Proxy` for deferring database initialization to\n  use the new `DatabaseProxy` class instead.\n* Added support for `INSERT ... ON CONFLICT` when the conflict target is a\n  partial index (e.g., contains a `WHERE` clause). The `OnConflict` and\n  `on_conflict()` APIs now take an additional `conflict_where` parameter to\n  represent the `WHERE` clause of the partial index in question. See #1860.\n* Enhanced the `playhouse.kv` extension to use efficient upsert for *all*\n  database engines. Previously upsert was only supported for sqlite and mysql.\n* Re-added the `orwhere()` query filtering method, which will append the given\n  expressions using `OR` instead of `AND`. See #391 for old discussion.\n* Added some new examples to the ``examples/`` directory\n* Added `select_from()` API for wrapping a query and selecting one or more\n  columns from the wrapped subquery. [Docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#SelectQuery.select_from).\n* Added documentation on using [row values](http://docs.peewee-orm.com/en/latest/peewee/query_operators.html#row-values).\n* Removed the (defunct) \"speedups\" C extension, which as of 3.8.2 only\n  contained a barely-faster function for quoting entities.\n\n**Bugfixes**\n\n* Fix bug in SQL generation when there was a subquery that used a common table\n  expressions.\n* Enhanced `prefetch()` and fixed bug that could occur when mixing\n  self-referential foreign-keys and model aliases.\n* MariaDB 10.3.3 introduces backwards-incompatible changes to the SQL used for\n  upsert. Peewee now introspects the MySQL server version at connection time to\n  ensure proper handling of version-specific features. See #1834 for details.\n* Fixed bug where `TimestampField` would treat zero values as `None` when\n  reading from the database.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.8.2...3.9.0)\n\n## 3.8.2\n\n**Backwards-incompatible changes**\n\n* The default row-type for `INSERT` queries executed with a non-default\n  `RETURNING` clause has changed from `tuple` to `Model` instances. This makes\n  `INSERT` behavior consistent with `UPDATE` and `DELETE` queries that specify\n  a `RETURNING` clause. To revert back to the old behavior, just append a call\n  to `.tuples()` to your `INSERT ... RETURNING` query.\n* Removing support for the `table_alias` model `Meta` option. Previously, this\n  attribute could be used to specify a \"vanity\" alias for a model class in the\n  generated SQL. As a result of some changes to support more robust UPDATE and\n  DELETE queries, supporting this feature will require some re-working. As of\n  the 3.8.0 release, it was broken and resulted in incorrect SQL for UPDATE\n  queries, so now it is removed.\n\n**New features**\n\n* Added `playhouse.shortcuts.ReconnectMixin`, which can be used to implement\n  automatic reconnect under certain error conditions (notably the MySQL error\n  2006 - server has gone away).\n\n**Bugfixes**\n\n* Fix SQL generation bug when using an inline window function in the `ORDER BY`\n  clause of a query.\n* Fix possible zero-division in user-defined implementation of BM25 ranking\n  algorithm for SQLite full-text search.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.8.1...3.8.2)\n\n## 3.8.1\n\n**New features**\n\n* Sqlite `SearchField` now supports the `match()` operator, allowing full-text\n  search to be performed on a single column (as opposed to the whole table).\n\n**Changes**\n\n* Remove minimum passphrase restrictions in SQLCipher integration.\n\n**Bugfixes**\n\n* Support inheritance of `ManyToManyField` instances.\n* Ensure operator overloads are invoked when generating filter expressions.\n* Fix incorrect scoring in Sqlite BM25, BM25f and Lucene ranking algorithms.\n* Support string field-names in data dictionary when performing an ON CONFLICT\n  ... UPDATE query, which allows field-specific conversions to be applied.\n  References #1815.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.8.0...3.8.1)\n\n## 3.8.0\n\n**New features**\n\n* Postgres `BinaryJSONField` now supports `has_key()`, `concat()` and\n  `remove()` methods (though remove may require pg10+).\n* Add `python_value()` method to the SQL-function helper `fn`, to allow\n  specifying a custom function for mapping database values to Python values.\n\n**Changes**\n\n* Better support for UPDATE ... FROM queries, and more generally, more robust\n  support for UPDATE and RETURNING clauses. This means that the\n  `QualifiedNames` helper is no longer needed for certain types of queries.\n* The `SqlCipherDatabase` no longer accepts a `kdf_iter` parameter. To\n  configure the various SQLCipher encryption settings, specify the setting\n  values as `pragmas` when initializing the database.\n* Introspection will now, by default, only strip \"_id\" from introspected column\n  names if those columns are foreign-keys. See #1799 for discussion.\n* Allow `UUIDField` and `BinaryUUIDField` to accept hexadecimal UUID strings as\n  well as raw binary UUID bytestrings (in addition to `UUID` instances, which\n  are already supported).\n* Allow `ForeignKeyField` to be created without an index.\n* Allow multiple calls to `cast()` to be chained (#1795).\n* Add logic to ensure foreign-key constraint names that exceed 64 characters\n  are truncated using the same logic as is currently in place for long indexes.\n* `ManyToManyField` supports foreign-keys to fields other than primary-keys.\n* When linked against SQLite 3.26 or newer, support `SQLITE_CONSTRAINT` to\n  designate invalid queries against virtual tables.\n* SQL-generation changes to aid in supporting using queries within expressions\n  following the SELECT statement.\n\n**Bugfixes**\n\n* Fixed bug in `order_by_extend()`, thanks @nhatHero.\n* Fixed bug where the `DataSet` CSV import/export did not support non-ASCII\n  characters in Python 3.x.\n* Fixed bug where `model_to_dict` would attempt to traverse explicitly disabled\n  foreign-key backrefs (#1785).\n* Fixed bug when attempting to migrate SQLite tables that have a field whose\n  column-name begins with \"primary_\".\n* Fixed bug with inheriting deferred foreign-keys.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.7.1...3.8.0)\n\n## 3.7.1\n\n**New features**\n\n* Added `table_settings` model `Meta` option, which should be a list of strings\n  specifying additional options for `CREATE TABLE`, which are placed *after*\n  the closing parentheses.\n* Allow specification of `on_update` and `on_delete` behavior for many-to-many\n  relationships when using `ManyToManyField`.\n\n**Bugfixes**\n\n* Fixed incorrect SQL generation for Postgresql ON CONFLICT clause when the\n  conflict_target is a named constraint (rather than an index expression). This\n  introduces a new keyword-argument to the `on_conflict()` method:\n  `conflict_constraint`, which is currently only supported by Postgresql. Refs\n  issue #1737.\n* Fixed incorrect SQL for sub-selects used on the right side of `IN`\n  expressions. Previously the query would be assigned an alias, even though an\n  alias was not needed.\n* Fixed incorrect SQL generation for Model indexes which contain SQL functions\n  as indexed columns.\n* Fixed bug in the generation of special queries used to perform operations on\n  SQLite FTS5 virtual tables.\n* Allow `frozenset` to be correctly parameterized as a list of values.\n* Allow multi-value INSERT queries to specify `columns` as a list of strings.\n* Support `CROSS JOIN` for model select queries.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.7.0...3.7.1)\n\n## 3.7.0\n\n**Backwards-incompatible changes**\n\n* Pool database `close_all()` method renamed to `close_idle()` to better\n  reflect the actual behavior.\n* Databases will now raise `InterfaceError` when `connect()` or `close()` are\n  called on an uninitialized, deferred database object.\n\n**New features**\n\n* Add methods to the migrations extension to support adding and dropping table\n  constraints.\n* Add [Model.bulk_create()](http://docs.peewee-orm.com/en/latest/peewee/api.html#Model.bulk_create)\n  method for bulk-inserting unsaved model instances.\n* Add `close_stale()` method to the connection pool to support closing stale\n  connections.\n* The `FlaskDB` class in `playhouse.flask_utils` now accepts a `model_class`\n  parameter, which can be used to specify a custom base-class for models.\n\n**Bugfixes**\n\n* Parentheses were not added to subqueries used in function calls with more\n  than one argument.\n* Fixed bug when attempting to serialize many-to-many fields which were created\n  initially with a `DeferredThroughModel`, see #1708.\n* Fixed bug when using the Postgres `ArrayField` with an array of `BlobField`.\n* Allow `Proxy` databases to be used as a context-manager.\n* Fixed bug where the APSW driver was referring to the SQLite version from the\n  standard library `sqlite3` driver, rather than from `apsw`.\n* Reflection library attempts to wrap server-side column defaults in quotation\n  marks if the column data-type is text/varchar.\n* Missing import in migrations library, which would cause errors when\n  attempting to add indexes whose name exceeded 64 chars.\n* When using the Postgres connection pool, ensure any open/pending transactions\n  are rolled-back when the connection is recycled.\n* Even *more* changes to the `setup.py` script. In this case I've added a\n  helper function which will reliably determine if the SQLite3 extensions can\n  be built. This follows the approach taken by the Python YAML package.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.6.4...3.7.0)\n\n## 3.6.4\n\nTake a whole new approach, following what `simplejson` does. Allow the\n`build_ext` command class to fail, and retry without extensions in the event we\nrun into issues building extensions. References #1676.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.6.3...3.6.4)\n\n## 3.6.3\n\nAdd check in `setup.py` to determine if a C compiler is available before\nbuilding C extensions. References #1676.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.6.2...3.6.3)\n\n## 3.6.2\n\nUse `ctypes.util.find_library` to determine if `libsqlite3` is installed.\nShould fix problems people are encountering installing when SQLite3 is not\navailable.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.6.1...3.6.2)\n\n## 3.6.1\n\nFixed issue with setup script.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.6.0...3.6.1)\n\n## 3.6.0\n\n* Support for Python 3.7, including bugfixes related to new StopIteration\n  handling inside of generators.\n* Support for specifying `ROWS` or `RANGE` window frame types. For more\n  information, see the new [frame type documentation](http://docs.peewee-orm.com/en/latest/peewee/querying.html#frame-types-range-vs-rows).\n* Add APIs for user-defined window functions if using [pysqlite3](https://github.com/coleifer/pysqlite3)\n  and sqlite 3.25.0 or newer.\n* `TimestampField` now uses 64-bit integer data-type for storage.\n* Added support to `pwiz` and `playhouse.reflection` to enable generating\n  models from VIEWs.\n* Added lower-level database API for introspecting VIEWs.\n* Revamped continuous integration setup for better coverage, including 3.7 and\n  3.8-dev.\n* Allow building C extensions even if Cython is not installed, by distributing\n  pre-generated C source files.\n* Switch to using `setuptools` for packaging.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.5.2...3.6.0)\n\n## 3.5.2\n\n* New guide to using [window functions in Peewee](http://docs.peewee-orm.com/en/latest/peewee/querying.html#window-functions).\n* New and improved table name auto-generation. This feature is not backwards\n  compatible, so it is **disabled by default**. To enable, set\n  `legacy_table_names=False` in your model's `Meta` options. For more details,\n  see [table names](http://docs.peewee-orm.com/en/latest/peewee/models.html#table_names)\n  documentation.\n* Allow passing single fields/columns to window function `order_by` and\n  `partition_by` arguments.\n* Support for `FILTER (WHERE...)` clauses with window functions and aggregates.\n* Added `IdentityField` class suitable for use with Postgres 10's new identity\n  column type. It can be used anywhere `AutoField` or `BigAutoField` was being\n  used previously.\n* Fixed bug creating indexes on tables that are in attached databases (SQLite).\n* Fixed obscure bug when using `prefetch()` and `ModelAlias` to populate a\n  back-reference related model.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.5.1...3.5.2)\n\n## 3.5.1\n\n**New features**\n\n* New documentation for working with [relationships](http://docs.peewee-orm.com/en/latest/peewee/relationships.html)\n  in Peewee.\n* Improved tests and documentation for MySQL upsert functionality.\n* Allow `database` parameter to be specified with `ModelSelect.get()` method.\n  For discussion, see #1620.\n* Add `QualifiedNames` helper to peewee module exports.\n* Add `temporary=` meta option to support temporary tables.\n* Allow a `Database` object to be passed to constructor of `DataSet` helper.\n\n**Bug fixes**\n\n* Fixed edge-case where attempting to alias a field to it's underlying\n  column-name (when different), Peewee would not respect the alias and use the\n  field name instead. See #1625 for details and discussion.\n* Raise a `ValueError` when joining and aliasing the join to a foreign-key's\n  `object_id_name` descriptor. Should prevent accidentally introducing O(n)\n  queries or silently ignoring data from a joined-instance.\n* Fixed bug for MySQL when creating a foreign-key to a model which used the\n  `BigAutoField` for it's primary-key.\n* Fixed bugs in the implementation of user-defined aggregates and extensions\n  with the APSW SQLite driver.\n* Fixed regression introduced in 3.5.0 which ignored custom Model `__repr__()`.\n* Fixed regression from 2.x in which inserting from a query using a `SQL()` was\n  no longer working. Refs #1645.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.5.0...3.5.1)\n\n## 3.5.0\n\n**Backwards-incompatible changes**\n\n* Custom Model `repr` no longer use the convention of overriding `__unicode__`,\n  and now use `__str__`.\n* Redesigned the [sqlite json1 integration](http://docs.peewee-orm.com/en/latest/peewee/sqlite_ext.html#sqlite-json1).\n  and changed some of the APIs and semantics of various `JSONField` methods.\n  The documentation has been expanded to include more examples and the API has\n  been simplified to make it easier to work with. These changes **do not** have\n  any effect on the [Postgresql JSON fields](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#pgjson).\n\n**New features**\n\n* Better default `repr` for model classes and fields.\n* `ForeignKeyField()` accepts a new initialization parameter, `deferrable`, for\n  specifying when constraints should be enforced.\n* `BitField.flag()` can be called without a value parameter for the common\n  use-case of using flags that are powers-of-2.\n* `SqliteDatabase` pragmas can be specified as a `dict` (previously required a\n  list of 2-tuples).\n* SQLite `TableFunction` ([docs](http://docs.peewee-orm.com/en/latest/peewee/sqlite_ext.html#sqlite-vtfunc))\n  will print Python exception tracebacks raised in the `initialize` and\n  `iterate` callbacks, making debugging significantly easier.\n\n**Bug fixes**\n\n* Fixed bug in `migrator.add_column()` where, if the field being added declared\n  a non-standard index type (e.g., binary json field with GIN index), this\n  index type was not being respected.\n* Fixed bug in `database.table_exists()` where the implementation did not match\n  the documentation. Implementation has been updated to match the\n  documentation.\n* Fixed bug in SQLite `TableFunction` implementation which raised errors if the\n  return value of the `iterate()` method was not a `tuple`.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.4.0...3.5.0)\n\n## 3.4.0\n\n**Backwards-incompatible changes**\n\n* The `regexp()` operation is now case-sensitive for MySQL and Postgres. To\n  perform case-insensitive regexp operations, use `iregexp()`.\n* The SQLite `BareField()` field-type now supports all column constraints\n  *except* specifying the data-type. Previously it silently ignored any column\n  constraints.\n* LIMIT and OFFSET parameters are now treated as parameterized values instead\n  of literals.\n* The `schema` parameter for SQLite database introspection methods is no longer\n  ignored by default. The schema corresponds to the name given to an attached\n  database.\n* `ArrayField` now accepts a new parameter `field_kwargs`, which is used to\n  pass information to the array field's `field_class` initializer.\n\n**New features and other changes**\n\n* SQLite backup interface supports specifying page-counts and a user-defined\n  progress handler.\n* GIL is released when doing backups or during SQLite busy timeouts (when using\n  the peewee SQLite busy-handler).\n* Add NATURAL join-type to the `JOIN` helper.\n* Improved identifier quoting to allow specifying distinct open/close-quote\n  characters. Enables adding support for MSSQL, for instance, which uses square\n  brackets, e.g. `[table].[column]`.\n* Unify timeout interfaces for SQLite databases (use seconds everywhere rather\n  than mixing seconds and milliseconds, which was confusing).\n* Added `attach()` and `detach()` methods to SQLite database, making it\n  possible to attach additional databases (e.g. an in-memory cache db).\n\n[View commits](https://github.com/coleifer/peewee/compare/3.3.4...3.4.0)\n\n## 3.3.4\n\n* Added a `BinaryUUIDField` class for efficiently storing UUIDs in 16-bytes.\n* Fix dataset's `update_cache()` logic so that when updating a single table\n  that was newly-added, we also ensure that all dependent tables are updated at\n  the same time. Refs coleifer/sqlite-web#42.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.3.3...3.3.4)\n\n## 3.3.3\n\n* More efficient implementation of model dependency-graph generation. Improves\n  performance of recursively deleting related objects by omitting unnecessary\n  subqueries.\n* Added `union()`, `union_all()`, `intersect()` and `except_()` to the\n  `Model`-specific query implementations. This was an oversight that should\n  have been patched in 3.3.2, but is fixed in 3.3.3.\n* Major cleanup to test runner and standardized test skipping logic to\n  integrate with standard-library `unittest` conventions.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.3.2...3.3.3)\n\n## 3.3.2\n\n* Add methods for `union()`, `union_all`, `intersect()` and `except_()`.\n  Previously, these methods were only available as operator overloads.\n* Removed some Python 2.6-specific support code, as 2.6 is no longer officially\n  supported.\n* Fixed model-graph resolution logic for deferred foreign-keys.\n* Better support for UPDATE...FROM queries (Postgresql).\n\n[View commits](https://github.com/coleifer/peewee/compare/3.3.1...3.3.2)\n\n## 3.3.1\n\n* Fixed long-standing bug in 3.x regarding using column aliases with queries\n  that utilize the ModelCursorWrapper (typically queries with one or more\n  joins).\n* Fix typo in model metadata code, thanks @klen.\n* Add examples of using recursive CTEs to docs.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.3.0...3.3.1)\n\n## 3.3.0\n\n* Added support for SQLite's new `ON CONFLICT` clause, which is modelled on the\n  syntax used by Postgresql and will be available in SQLite 3.24.0 and onward.\n* Added better support for using common table expressions and a cleaner way of\n  implementing recursive CTEs, both of which are also tested with integration\n  tests (as opposed to just checking the generated SQL).\n* Modernized the CI environment to utilize the latest MariaDB features, so we\n  can test window functions and CTEs with MySQL (when available).\n* Reorganized and unified the feature-flags in the test suite.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.2.5...3.3.0)\n\n## 3.2.5\n\n* Added `ValuesList` for representing values lists. [Docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#ValuesList).\n* `DateTimeField`, `DateField` and `TimeField` will parse formatted-strings\n  before sending to the database. Previously this only occurred when reading\n  values from the database.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.2.4...3.2.5)\n\n## 3.2.4\n\n* Smarter handling of model-graph when dealing with compound queries (union,\n  intersect, etc). #1579.\n* If the same column-name is selected multiple times, first value wins. #1579.\n* If `ModelSelect.switch()` is called without any arguments, default to the\n  query's model. Refs #1573.\n* Fix issue where cloning a ModelSelect query did not result in the joins being\n  cloned. #1576.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.2.3...3.2.4)\n\n## 3.2.3\n\n* `pwiz` tool will capture column defaults defined as part of the table schema.\n* Fixed a misleading error message - #1563.\n* Ensure `reuse_if_open` parameter has effect on pooled databases.\n* Added support for on update/delete when migrating foreign-key.\n* Fixed bug in SQL generation for subqueries in aliased functions #1572.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.2.2...3.2.3)\n\n## 3.2.2\n\n* Added support for passing `Model` classes to the `returning()` method when\n  you intend to return all columns for the given model.\n* Fixed a bug when using user-defined sequences, and the underlying sequence\n  already exists.\n* Added `drop_sequences` parameter to `drop_table()` method which allows you to\n  conditionally drop any user-defined sequences when dropping the table.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.2.1...3.2.2)\n\n## 3.2.1\n\n**Notice:** the default mysql driver for Peewee has changed to [pymysql](https://github.com/PyMySQL/PyMySQL)\nin version 3.2.1. In previous versions, if both *mysql-python* and *pymysql*\nwere installed, Peewee would use *mysql-python*. As of 3.2.1, if both libraries\nare installed Peewee will use *pymysql*.\n\n* Added new module `playhouse.mysql_ext` which includes\n  `MySQLConnectorDatabase`, a database implementation that works with the\n  [mysql-connector](https://dev.mysql.com/doc/connector-python/en/) driver.\n* Added new field to `ColumnMetadata` class which captures a database column's\n  default value. `ColumnMetadata` is returned by `Database.get_columns()`.\n* Added [documentation on making Peewee async](http://docs.peewee-orm.com/en/latest/peewee/database.html#async-with-gevent).\n\n[View commits](https://github.com/coleifer/peewee/compare/3.2.0...3.2.1)\n\n## 3.2.0\n\nThe 3.2.0 release introduces a potentially backwards-incompatible change. The\nonly users affected will be those that have implemented custom `Field` types\nwith a user-defined `coerce` method. tl/dr: rename the coerce attribute to\nadapt and you should be set.\n\n#### Field.coerce renamed to Field.adapt\n\nThe `Field.coerce` method has been renamed to `Field.adapt`. The purpose of\nthis method is to convert a value from the application/database into the\nappropriate Python data-type. For instance, `IntegerField.adapt` is simply the\n`int` built-in function.\n\nThe motivation for this change is to support adding metadata to any AST node\ninstructing Peewee to not coerce the associated value. As an example, consider\nthis code:\n\n```python\n\nclass Note(Model):\n    id = AutoField()  # autoincrementing integer primary key.\n    content = TextField()\n\n# Query notes table and cast the \"id\" to a string and store as \"id_text\" attr.\nquery = Note.select(Note.id.cast('TEXT').alias('id_text'), Note.content)\n\na_note = query.get()\nprint((a_note.id_text, a_note.content))\n\n# Prior to 3.2.0 the CAST is \"un-done\" because the value gets converted\n# back to an integer, since the value is associated with the Note.id field:\n(1, u'some note')  # 3.1.7, e.g. -- \"id_text\" is an integer!\n\n# As of 3.2.0, CAST will automatically prevent the conversion of field values,\n# which is an extension of a more general metadata API that can instruct Peewee\n# not to convert certain values.\n(u'1', u'some note')  # 3.2.0 -- \"id_text\" is a string as expected.\n```\n\nIf you have implemented custom `Field` classes and are using `coerce` to\nenforce a particular data-type, you can simply rename the attribute to `adapt`.\n\n#### Other changes\n\nOld versions of SQLite do not strip quotation marks from aliased column names\nin compound queries (e.g. UNION). Fixed in 3.2.0.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.1.7...3.2.0)\n\n## 3.1.7\n\nFor all the winblows lusers out there, added an option to skip compilation of\nthe SQLite C extensions during installation. Set env var `NO_SQLITE=1` and run\n`setup.py install` and you should be able to build without requiring SQLite.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.1.6...3.1.7)\n\n## 3.1.6\n\n* Added `rekey()` method to SqlCipher database for changing encryption key and\n  documentation for `set_passphrase()` method.\n* Added `convert_values` parameter to `ArrayField` constructor, which will\n  cause the array values to be processed using the underlying data-type's\n  conversion logic.\n* Fixed unreported bug using `TimestampField` with sub-second resolutions.\n* Fixed bug where options were not being processed when calling `drop_table()`.\n* Some fixes and improvements to `signals` extension.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.1.5...3.1.6)\n\n## 3.1.5\n\nFixed Python 2/3 incompatibility with `itertools.izip_longest()`.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.1.4...3.1.5)\n\n## 3.1.4\n\n* Added `BigAutoField` to support 64-bit auto-incrementing primary keys.\n* Use Peewee-compatible datetime serialization when exporting JSON from\n  a `DataSet`. Previously the JSON export used ISO-8601 by default. See #1536.\n* Added `Database.batch_commit` helper to wrap iterators in chunked\n  transactions. See #1539 for discussion.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.1.3...3.1.4)\n\n## 3.1.3\n\n* Fixed issue where scope-specific settings were being updated in-place instead\n  of copied. #1534.\n* Fixed bug where setting a `ForeignKeyField` did not add it to the model's\n  \"dirty\" fields list. #1530.\n* Use pre-fetched data when using `prefetch()` with `ManyToManyField`. Thanks\n  to @iBelieve for the patch. #1531.\n* Use `JSON` data-type for SQLite `JSONField` instances.\n* Add a `json_contains` function for use with SQLite `json1` extension.\n* Various documentation updates and additions.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.1.2...3.1.3)\n\n## 3.1.2\n\n#### New behavior for INSERT queries with RETURNING clause\n\nInvestigating #1522, it occurred to me that INSERT queries with non-default\n*RETURNING* clauses (postgres-only feature) should always return a cursor\nobject. Previously, if executing a single-row INSERT query, the last-inserted\nrow ID would be returned, regardless of what was specified by the RETURNING\nclause.\n\nThis change only affects INSERT queries with non-default RETURNING clauses and\nwill cause a cursor to be returned, as opposed to the last-inserted row ID.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.1.1...3.1.2)\n\n## 3.1.1\n\n* Fixed bug when using `Model.alias()` when the model defined a particular\n  database schema.\n* Added `SchemaManager.create_foreign_key` API to simplify adding constraints\n  when dealing with circular foreign-key relationships. Updated docs\n  accordingly.\n* Improved implementation of `Migrator.add_foreign_key_constraint` so that it\n  can be used with Postgresql (in addition to MySQL).\n* Added `PickleField` to the `playhouse.fields` module. [Docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#PickleField).\n* Fixed bug in implementation of `CompressedField` when using Python 3.\n* Added `KeyValue` API in `playhouse.kv` module. [Docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#key-value-store).\n* More test cases for joining on sub-selects or common table expressions.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.1.0...3.1.1)\n\n## 3.1.0\n\n#### Backwards-incompatible changes\n\n`Database.bind()` has been renamed to `Database.bind_ctx()`, to more closely\nmatch the semantics of the corresponding model methods, `Model.bind()` and\n`Model.bind_ctx()`. The new `Database.bind()` method is a one-time operation\nthat binds the given models to the database. See documentation:\n\n* [Database.bind()](http://docs.peewee-orm.com/en/latest/peewee/api.html#Database.bind)\n* [Database.bind_ctx()](http://docs.peewee-orm.com/en/latest/peewee/api.html#Database.bind_ctx)\n\n#### Other changes\n\n* Removed Python 2.6 support code from a few places.\n* Fixed example analytics app code to ensure hstore extension is registered.\n* Small efficiency improvement to bloom filter.\n* Removed \"attention!\" from *README*.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.20...3.1.0)\n\n## 3.0.20\n\n* Include `schema` (if specified) when checking for table-existence.\n* Correct placement of ORDER BY / LIMIT clauses in compound select queries.\n* Fix bug in back-reference lookups when using `filter()` API.\n* Fix bug in SQL generation for ON CONFLICT queries with Postgres, #1512.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.19...3.0.20)\n\n## 3.0.19\n\n* Support for more types of mappings in `insert_many()`, refs #1495.\n* Lots of documentation improvements.\n* Fix bug when calling `tuples()` on a `ModelRaw` query. This was reported\n  originally as a bug with *sqlite-web* CSV export. See coleifer/sqlite-web#38.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.18...3.0.19)\n\n## 3.0.18\n\n* Improved error messages when attempting to use a database class for which the\n  corresponding driver is not installed.\n* Added tests showing the use of custom operator (a-la the docs).\n* Fixed indentation issue in docs, #1493.\n* Fixed issue with the SQLite date_part issue, #1494.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.17...3.0.18)\n\n## 3.0.17\n\n* Fix `schema` inheritance regression, #1485.\n* Add helper method to postgres migrator for setting search_path, #1353.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.16...3.0.17)\n\n## 3.0.16\n\n* Improve model graph resolution when iterating results of a query. Refs #1482.\n* Allow Model._meta.schema to be changed at run-time. #1483.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.15...3.0.16)\n\n## 3.0.15\n\n* Use same `schema` used for reflection in generated models.\n* Preserve `pragmas` set on deferred Sqlite database if database is\n  re-initialized without re-specifying pragmas.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.14...3.0.15)\n\n## 3.0.14\n\n* Fix bug creating model instances on Postgres when model does not have a\n  primary key column.\n* Extend postgresql reflection to support array types.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.13...3.0.14)\n\n## 3.0.13\n\n* Fix bug where simple field aliases were being ignored. Fixes #1473.\n* More strict about column type inference for postgres + pwiz.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.12...3.0.13)\n\n## 3.0.12\n\n* Fix queries of the form INSERT ... VALUES (SELECT...) so that sub-select is\n  wrapped in parentheses.\n* Improve model-graph resolution when selecting from multiple tables that are\n  joined by foreign-keys, and an intermediate table is omitted from selection.\n* Docs update to reflect deletion of post_init signal.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.11...3.0.12)\n\n## 3.0.11\n\n* Add note to changelog about `cursor()` method.\n* Add hash method to postgres indexedfield subclasses.\n* Add TableFunction to sqlite_ext module namespace.\n* Fix bug regarding NOT IN queries where the right-hand-side is an empty set.\n* Fallback implementations of bm25f and lucene search ranking algorithms.\n* Fixed DecimalField issue.\n* Fixed issue with BlobField when database is a Proxy object.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.10...3.0.11)\n\n## 3.0.10\n\n* Fix `Database.drop_tables()` signature to support `cascade` argument - #1453.\n* Fix querying documentation for custom functions - #1454.\n* Added len() method to `ModelBase` for convenient counting.\n* Fix bug related to unsaved relation population (thanks @conqp) - #1459.\n* Fix count() on compound select - #1460.\n* Support `coerce` keyword argument with `fn.XXX()` - #1463.\n* Support updating existing model instance with dict_to_model-like API - #1456.\n* Fix equality tests with ArrayField - #1461.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.9...3.0.10)\n\n## 3.0.9\n\n* Add deprecation notice if passing `autocommit` as keyword argument to the\n  `Database` initializer. Refs #1452.\n* Add `JSONPath` and \"J\" helpers to sqlite extension.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.8...3.0.9)\n\n## 3.0.8\n\n* Add support for passing `cascade=True` when dropping tables. Fixes #1449.\n* Fix issues with backrefs and inherited foreign-keys. Fixes #1448.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.7...3.0.8)\n\n## 3.0.7\n\n* Add `select_extend()` method to extend existing SELECT-ion. [Doc](http://docs.peewee-orm.com/en/latest/peewee/api.html#Select.select_extend).\n* Accept `set()` as iterable value type, fixes #1445\n* Add test for model/field inheritance and fix bug relating to recursion error\n  when inheriting foreign-key field. Fixes #1448.\n* Fix regression where consecutive calls to `ModelSelect.select()` with no\n  parameters resulted in an empty selection. Fixes #1438.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.6...3.0.7)\n\n## 3.0.6\n\nAdd constraints for ON UPDATE/ON DELETE to foreign-key constraint - #1443.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.5...3.0.6)\n\n## 3.0.5\n\nAdds Model.index(), a short-hand method for declaring ModelIndex instances.\n\n* [Model.index docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#Model.index)\n* [Model.add_index docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#Model.add_index)\n* [ModelIndex docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#ModelIndex)\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.4...3.0.5)\n\n## 3.0.4\n\nRe-add a shim for `PrimaryKeyField` (renamed to `AutoField`) and log a\ndeprecation warning if you try to use it.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.3...3.0.4)\n\n## 3.0.3\n\nIncludes fix for bug where column-name to field-name translation was not being\ndone when running select queries on models whose field name differed from the\nunderlying column name (#1437).\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.2...3.0.3)\n\n## 3.0.2\n\nEnsures that the pysqlite headers are included in the source distribution so\nthat certain C extensions can be compiled.\n\n[View commits](https://github.com/coleifer/peewee/compare/3.0.0...3.0.2)\n\n## 3.0.0\n\n* Complete rewrite of SQL AST and code-generation.\n* Inclusion of new, low-level query builder APIs.\n* List of [backwards-incompatible changes](http://docs.peewee-orm.com/en/latest/peewee/changes.html).\n\n[View commits](https://github.com/coleifer/peewee/compare/2.10.2...3.0.0)\n\n## 2.10.2\n\n* Update travis-ci build scripts to use Postgres 9.6 and test against Python\n  3.6.\n* Added support for returning `namedtuple` objects when iterating over a\n  cursor.\n* Added support for specifying the \"object id\" attribute used when declaring a\n  foreign key. By default, it is `foreign-key-name_id`, but it can now be\n  customized.\n* Fixed small bug in the calculation of search scores when using the SQLite C\n  extension or the `sqlite_ext` module.\n* Support literal column names with the `dataset` module.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.10.1...2.10.2)\n\n## 2.10.1\n\nRemoved `AESEncryptedField`.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.10.0...2.10.1)\n\n## 2.10.0\n\nThe main change in this release is the removal of the `AESEncryptedField`,\nwhich was included as part of the `playhouse.fields` extension. It was brought\nto my attention that there was some serious potential for security\nvulnerabilities. Rather than give users a false sense of security, I've decided\nthe best course of action is to remove the field.\n\n* Remove the `playhouse.fields.AESEncryptedField` over security concerns\ndescribed in ticket #1264.\n* Correctly resolve explicit table dependencies when creating tables, refs\n  #1076. Thanks @maaaks.\n* Implement not equals comparison for `CompositeKey`.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.9.2...2.10.0)\n\n## 2.9.2\n\n* Fixed significant bug in the `savepoint` commit/rollback implementation. Many\n  thanks to @Syeberman for raising the issue. See #1225 for details.\n* Added support for postgresql `INTERVAL` columns. The new `IntervalField` in\n  the `postgres_ext` module is suitable for storing `datetime.timedelta`.\n* Fixed bug where missing `sqlite3` library was causing other, unrelated\n  libraries to throw errors when attempting to import.\n* Added a `case_sensitive` parameter to the SQLite `REGEXP` function\n  implementation. The default is `False`, to preserve backwards-compatibility.\n* Fixed bug that caused tables not to be created when using the `dataset`\n  extension. See #1213 for details.\n* Modified `drop_table` to raise an exception if the user attempts to drop\n  tables with `CASCADE` when the database backend does not support it.\n* Fixed Python3 issue in the `AESEncryptedField`.\n* Modified the behavior of string-typed fields to treat the addition operator\n  as concatenation. See #1241 for details.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.9.1...2.9.2)\n\n## 2.9.1\n\n* Fixed #1218, where the use of `playhouse.flask_utils` was requiring the\n  `sqlite3` module to be installed.\n* Fixed #1219 regarding the SQL generation for composite key sub-selects,\n  joins, etc.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.9.0...2.9.1)\n\n## 2.9.0\n\nIn this release there are two notable changes:\n\n* The ``Model.create_or_get()`` method was removed. See the [documentation](http://docs.peewee-orm.com/en/latest/peewee/querying.html#create-or-get)\n  for an example of the code one would write to replicate this functionality.\n* The SQLite closure table extension gained support for many-to-many\n  relationships thanks to a nice PR by @necoro. [Docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#ClosureTable).\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.8...2.9.0)\n\n## 2.8.8\n\nThis release contains a single important bugfix for a regression in specifying\nthe type of lock to use when opening a SQLite transaction.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.7...2.8.8)\n\n## 2.8.7\n\nThis release contains numerous cleanups.\n\n### Bugs fixed\n\n* #1087 - Fixed a misuse of the iteration protocol in the `sqliteq` extension.\n* Ensure that driver exceptions are wrapped when calling `commit` and\n  `rollback`.\n* #1096 - Fix representation of recursive foreign key relations when using the\n  `model_to_dict` helper.\n* #1126 - Allow `pskel` to be installed into `bin` directory.\n* #1105 - Added a `Tuple()` type to Peewee to enable expressing arbitrary\n  tuple expressions in SQL.\n* #1133 - Fixed bug in the conversion of objects to `Decimal` instances in the\n  `DecimalField`.\n* Fixed an issue renaming a unique foreign key in MySQL.\n* Remove the join predicate from CROSS JOINs.\n* #1148 - Ensure indexes are created when a column is added using a schema\n  migration.\n* #1165 - Fix bug where the primary key was being overwritten in queries using\n  the closure-table extension.\n\n### New stuff\n\n* Added properties to the `SqliteExtDatabase` to expose common `PRAGMA`\n  settings. For example, to set the cache size to 4MB, `db.cache_size = 1000`.\n* Clarified documentation on calling `commit()` or `rollback()` from within the\n  scope of an atomic block. [See docs](http://docs.peewee-orm.com/en/latest/peewee/transactions.html#transactions).\n* Allow table creation dependencies to be specified using new `depends_on` meta\n  option. Refs #1076.\n* Allow specification of the lock type used in SQLite transactions. Previously\n  this behavior was only present in `playhouse.sqlite_ext.SqliteExtDatabase`,\n  but it now exists in `peewee.SqliteDatabase`.\n* Added support for `CROSS JOIN` expressions in select queries.\n* Docs on how to implement [optimistic locking](http://docs.peewee-orm.com/en/latest/peewee/hacks.html#optimistic-locking).\n* Documented optional dependencies.\n* Generic support for specifying select queries as locking the selected rows\n  `FOR X`, e.g. `FOR UPDATE` or `FOR SHARE`.\n* Support for specifying the frame-of-reference in window queries, e.g.\n  specifying `UNBOUNDED PRECEDING`, etc. [See docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#Window).\n\n### Backwards-incompatible changes\n\n* As of 9e76c99, an `OperationalError` is raised if the user calls `connect()`\n  on an already-open Database object. Previously, the existing connection would\n  remain open and a new connection would overwrite it, making it impossible to\n  close the previous connection. If you find this is causing breakage in your\n  application, you can switch the `connect()` call to `get_conn()` which will\n  only open a connection if necessary. The error **is** indicative of a real\n  issue, though, so audit your code for places where you may be opening a\n  connection without closing it (module-scope operations, e.g.).\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.5...2.8.7)\n\n## 2.8.6\n\nThis release was later removed due to containing a bug. See notes on 2.8.7.\n\n## 2.8.5\n\nThis release contains two small bugfixes.\n\n* #1081 - fixed the use of parentheses in compound queries on MySQL.\n* Fixed some grossness in a helper function used by `prefetch` that was\n  clearing out the `GROUP BY` and `HAVING` clauses of sub-queries.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.4...2.8.5)\n\n## 2.8.4\n\nThis release contains bugfixes as well as a new playhouse extension module for\nworking with [SQLite in multi-threaded / concurrent environments](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#sqliteq).\nThe new module is called `playhouse.sqliteq` and it works by serializing\nqueries using a dedicated worker thread (or greenlet). The performance is quite\ngood, hopefully this proves useful to someone besides myself! You can learn\nmore by reading the [sqliteq documentation](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#sqliteq).\n\nAs a miscellaneous note, I did some major refactoring and cleanup in\n`ExtQueryResultsWrapper` and it's corollary in the `speedups` module. The code\nis much easier to read than before.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.3...2.8.4)\n\n### Bugs fixed\n\n* #1061 - @akrs patched a bug in `TimestampField` which affected the accuracy\n  of sub-second timestamps (for resolution > 1).\n* #1071, small python 3 fix.\n* #1072, allow `DeferredRelation` to be used multiple times if there are\n  multiple references to a given deferred model.\n* #1073, fixed regression in the speedups module that caused SQL functions to\n  always coerce return values, regardless of the `coerce` flag.\n* #1083, another Python 3 issue - this time regarding the use of `exc.message`.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.3...2.8.4)\n\n## 2.8.3\n\nThis release contains bugfixes and a small backwards-incompatible change to the\nway foreign key `ObjectIdDescriptor` is named (issue #1050).\n\n### Bugs fixed and general changes\n\n* #1028 - allow the `ensure_join` method to accept `on` and `join_type`\n  parameters. Thanks @paulbooth.\n* #1032 - fix bug related to coercing model instances to database parameters\n  when the model's primary key is a foreign key.\n* #1035 - fix bug introduced in 2.8.2, where I had added some logic to try and\n  restrict the base `Model` class from being treated as a \"real\" Model.\n* #1039 - update documentation to clarify that lists *or tuples* are acceptable\n  values when specifying SQLite `PRAGMA` statements.\n* #1041 - PyPy user was unable to install Peewee. (Who in their right mind\n  would *ever* use PyPy?!) Bug was fixed by removing the pre-generated C files\n  from the distribution.\n* #1043 - fix bug where the `speedups` C extension was not calling the correct\n  model initialization method, resulting in model instances returned as results\n  of a query having their `dirty` flag incorrectly set.\n* #1048 - similar to #1043, add logic to ensure that fields with default values\n  are considered dirty when instantiating the model.\n* #1049 - update URL to [APSW](https://rogerbinns.github.io/apsw).\n* Fixed unreported bug regarding `TimestampField` with zero values reporting\n  the incorrect datetime.\n\n### New stuff\n\n* [djpeewee](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#djpeewee) extension\n  module now works with Django 1.9.\n* [TimestampField](http://docs.peewee-orm.com/en/latest/peewee/api.html#TimestampField)\n  is now an officially documented field.\n* #1050 - use the `db_column` of a `ForeignKeyField` for the name of the\n  `ObjectIdDescriptor`, except when the `db_column` and field `name` are the\n  same, in which case the ID descriptor will be named `<field_name>_id`.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.2...2.8.3)\n\n## 2.8.2\n\nThis release contains mostly bug-fixes, clean-ups, and API enhancements.\n\n### Bugs fixed and general cleanups\n\n* #820 - fixed some bugs related to the Cython extension build process.\n* #858 - allow blanks and perform type conversion when using the `db_url`\n  extension\n* #922 - ensure that `peewee.OperationalError` is raised consistently when\n  using the `RetryOperationalError` mixin.\n* #929 - ensure that `pwiz` will import the appropriate extensions when\n  vendor-specific fields are used.\n* #930 - ensure that `pwiz`-generated models containing `UnknownField`\n  placeholders do not blow up when you instantiate them.\n* #932 - correctly limit the length of automatically-generated index names.\n* #933 - fixed bug where `BlobField` could not be used if it's parent model\n  pointed to an uninitialized database `Proxy`.\n* #935 - greater consistency with the conversion to Python data-types when\n  performing aggregations, annotations, or calling `scalar()`.\n* #939 - ensure the correct data-types are used when initializing a connection\n  pool.\n* #947 - fix bug where `Signal` subclasses were not returning rows affected on\n  save.\n* #951 - better warnings regarding C extension compilation, thanks @dhaase-de.\n* #968 - fix bug where table names starting with numbers generated invalid\n  table names when using `pwiz`.\n* #971 - fix bug where parameter was not being used. Thanks @jberkel.\n* #974 - fixed the way `SqliteExtDatabase` handles the automatic `rowid` (and\n    `docid`) columns. Thanks for alerting me to the issue and providing a\n    failing test case @jberkel.\n* #976 - fix obscure bug relating to cloning foreign key fields twice.\n* #981 - allow `set` instances to be used on the right-hand side of `IN` exprs.\n* #983 - fix behavior where the default `id` primary key was inherited\n  regardless. When users would inadvertently include it in their queries, it\n  would use the table alias of it's parent class.\n* #992 - add support for `db_column` in `djpeewee`\n* #995 - fix the behavior of `truncate_date` with Postgresql. Thanks @Zverik.\n* #1011 - correctly handle `bytes` wrapper used by `PasswordField` to `bytes`.\n* #1012 - when selecting and joining on multiple models, do not create model\n  instances when the foreign key is NULL.\n* #1017 - do not coerce the return value of function calls to `COUNT` or `SUM`,\n  since the python driver will already give us the right Python value.\n* #1018 - use global state to resolve `DeferredRelations`, allowing for a nicer\n  API. Thanks @brenguyen711.\n* #1022 - attempt to avoid creating invalid Python when using `pwiz` with MySQL\n  database columns containing spaces. Yes, fucking spaces.\n* #1024 - fix bug in SQLite migrator which had a naive approach to fixing\n  indexes.\n* #1025 - explicitly check for `None` when determining if the database has been\n  set on `ModelOptions`. Thanks @joeyespo.\n\n### New stuff\n\n* Added `TimestampField` for storing datetimes using integers. Greater than\n  second delay is possible through exponentiation.\n* Added `Database.drop_index()` method.\n* Added a `max_depth` parameter to the `model_to_dict` function in\n  the `playhouse.shortcuts` extension module.\n* `SelectQuery.first()` function accepts a parameter `n` which\n  applies a limit to the query and returns the first row. Previously the limit\n  was not applied out of consideration for subsequent iterations, but I believe\n  usage has shown that a limit is more desirable than reserving the option to\n  iterate without a second query. The old behavior is preserved in the new\n  `SelectQuery.peek()` method.\n* `group_by()`, `order_by()`, `window()` now accept a keyward argument\n  `extend`, which, when set to `True`, will append to the existing values\n  rather than overwriting them.\n* Query results support negative indexing.\n* C sources are included now as part of the package. I *think* they should be\n  able to compile for python 2 or 3, on linux or windows...but not positive.\n* #895 - added the ability to query using the `<foreign_key>_id` attribute.\n* #948 - added documentation about SQLite limits and how they affect\n* #1009 - allow `DATABASE_URL` as a recognized parameter to the Flask config.\n  `insert_many`.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.1...2.8.2)\n\n## 2.8.1\n\nThis release is long overdue so apologies if you've been waiting on it and\nrunning off master. There are numerous bugfixes contained in this release, so\nI'll list those first this time.\n\n### Bugs fixed\n\n* #821 - issue warning if Cython is old\n* #822 - better handling of MySQL connections\npoint for advanced use-cases.\n* #313 - support equality/inequality with generic foreign key queries, and\nensure `get_or_create` works with GFKs.\n* #834 - fixed Python3 incompatibilities in the `PasswordField`, thanks\n@mosquito.\n* #836 - fix handling of `last_insert_id()` when using `APSWDatabase`.\n* #845 - add connection hooks to `APSWDatabase`.\n* #852 - check SQLite library version to avoid calls to missing APIs.\n* #857 - allow database definition to be deferred when using the connection\npool.\n* #878 - formerly `.limit(0)` had no effect. Now adds `LIMIT 0`.\n* #879 - implement a `__hash__` method for `Model`\n* #886 - fix `count()` for compound select queries.\n* #895 - allow writing to the `foreign_key_id` descriptor to set the foreign\nkey value.\n* #893 - fix boolean logic bug in `model_to_dict()`.\n* #904 - fix side-effect in `clean_prefetch_query`, thanks to @p.kamayev\n* #907 - package includes `pskel` now.\n* #852 - fix sqlite version check in BerkeleyDB backend.\n* #919 - add runtime check for `sqlite3` library to match MySQL and Postgres.\nThanks @M157q\n\n### New features\n\n* Added a number of [SQLite user-defined functions and\naggregates](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#sqlite-udf).\n* Use the DB-API2 `Binary` type for `BlobField`.\n* Implemented the lucene scoring algorithm in the `sqlite_ext` Cython library.\n* #825 - allow a custom base class for `ModelOptions`, providing an extension\n* #830 - added `SmallIntegerField` type.\n* #838 - allow using a custom descriptor class with `ManyToManyField`.\n* #855 - merged change from @lez which included docs on using peewee with\nPyramid.\n* #858 - allow arguments to be passed on query-string when using the `db_url`\nmodule. Thanks @RealSalmon\n* #862 - add support for `truncate table`, thanks @dev-zero for the sample\ncode.\n* Allow the `related_name` model `Meta` option to be a callable that accepts\nthe foreign key field instance.\n\n\n[View commits](https://github.com/coleifer/peewee/compare/2.8.0...2.8.1)\n\n## 2.8.0\n\nThis release includes a couple new field types and greatly improved C extension support for both speedups and SQLite enhancements. Also includes some work, suggested by @foxx, to remove some places where `Proxy` was used in favor of more obvious APIs.\n\n### New features\n\n* [travis-ci builds](http://travis-ci.org/coleifer/peewee/builds/) now include MySQL and Python 3.5. Dropped support for Python 3.2 and 3.3. Builds also will run the C-extension code.\n* C extension speedups now enabled by default, includes faster implementations for `dict` and `tuple` `QueryResultWrapper` classes, faster date formatting, and a faster field and model sorting.\n* C implementations of SQLite functions is now enabled by default. SQLite extension is now compatible with APSW and can be used in standalone form directly from Python. See [SqliteExtDatabase](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#SqliteExtDatabase) for more details.\n* SQLite C extension now supports `murmurhash2`.\n* `UUIDField` is now supported for SQLite and MySQL, using `text` and `varchar` respectively, thanks @foxx!\n* Added `BinaryField`, thanks again, @foxx!\n* Added `PickledField` to `playhouse.fields`.\n* `ManyToManyField` now accepts a list of primary keys when adding or removing values from the through relationship.\n* Added support for SQLite [table-valued functions](http://sqlite.org/vtab.html#tabfunc2) using the [sqlite-vtfunc library](https://github.com/coleifer/sqlite-vtfunc).\n* Significantly simplified the build process for compiling the C extensions.\n\n### Backwards-incompatible changes\n\n* Instead of using a `Proxy` for defining circular foreign key relationships, you now need to use [DeferredRelation](http://docs.peewee-orm.com/en/latest/peewee/api.html#DeferredRelation).\n* Instead of using a `Proxy` for defining many-to-many through tables, you now need to use [DeferredThroughModel](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#DeferredThroughModel).\n* SQLite Virtual Models must now use `Meta.extension_module` and `Meta.extension_options` to declare extension and any options. For more details, see [VirtualModel](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#VirtualModel).\n* MySQL database will now issue `COMMIT` statements for `SELECT` queries. This was not necessary, but added due to an influx of confused users creating GitHub tickets. Hint: learn to user your damn database, it's not magic!\n\n### Bugs fixed\n\nSome of these may have been included in a previous release, but since I did not list them I'm listing them here.\n\n* #766, fixed bug with PasswordField and Python3. Fuck Python 3.\n* #768, fixed SortedFieldList and `remove_field()`. Thanks @klen!\n* #771, clarified docs for APSW.\n* #773, added docs for request hooks in Pyramid (who uses Pyramid, by the way?).\n* #774, prefetch() only loads first ForeignKeyField for a given relation.\n* #782, fixed typo in docs.\n* #791, foreign keys were not correctly handling coercing to the appropriate python value.\n* #792, cleaned up some CSV utils code.\n* #798, cleaned up iteration protocol in QueryResultWrappers.\n* #806, not really a bug, but MySQL users were clowning around and needed help.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.7.4...2.8.0)\n\n## 2.7.4\n\nThis is another small release which adds code to automatically build the SQLite C extension if `libsqlite` is available. The release also includes:\n\n* Support for `UUIDField` with SQLite.\n* Support for registering additional database classes with the `db_url` module via `register_database`.\n* `prefetch()` supports fetching multiple foreign-keys to the same model class.\n* Added method to validate FTS5 search queries.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.7.3...2.7.4)\n\n## 2.7.3\n\nSmall release which includes some changes to the BM25 sorting algorithm and the addition of a [`JSONField`](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#JSONField) for use with the new [JSON1 extension](http://sqlite.org/json1.html).\n\n## 2.7.2\n\nPeople were having trouble building the sqlite extension. I figure enough people are having trouble that I made it a separate command: `python setup.py build_sqlite_ext`.\n\n## 2.7.1\n\nJacked up the setup.py\n\n## 2.7.0\n\nNew APIs, features, and performance improvements.\n\n### Notable changes and new features\n\n* [`PasswordField`](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#PasswordField) that uses the `bcrypt` module.\n* Added new Model [`Meta.only_save_dirty`](http://docs.peewee-orm.com/en/latest/peewee/models.html#model-options-and-table-metadata) flag to, by default, only save fields that have been modified.\n* Added support for [`upsert()`](http://docs.peewee-orm.com/en/latest/peewee/api.html#InsertQuery.upsert) on MySQL (in addition to SQLite).\n* Implemented SQLite ranking functions (``rank`` and ``bm25``) in Cython, and changed both the Cython and Python APIs to accept weight values for every column in the search index. This more closely aligns with the APIs provided by FTS5. In fact, made the APIs for FTS4 and FTS5 result ranking compatible.\n* Major changes to the :ref:`sqlite_ext` module. Function callbacks implemented in Python were implemented in Cython (e.g. date manipulation and regex processing) and will be used if Cython is available when Peewee is installed.\n* Support for the experimental new [FTS5](http://sqlite.org/fts5.html) SQLite search extension.\n* Added :py:class:`SearchField` for use with the SQLite FTS extensions.\n* Added :py:class:`RowIDField` for working with the special ``rowid`` column in SQLite.\n* Added a model class validation hook to allow model subclasses to perform any validation after class construction. This is currently used to ensure that ``FTS5Model`` subclasses do not violate any rules required by the FTS5 virtual table.\n\n### Bugs fixed\n\n* **#751**, fixed some very broken behavior in the MySQL migrator code. Added more tests.\n* **#718**, added a `RetryOperationalError` mixin that will try automatically reconnecting after a failed query. There was a bug in the previous error handler implementation that made this impossible, which is also fixed.\n\n#### Small bugs\n\n* #713, fix column name regular expression in SQLite migrator.\n* #724, fixed `NULL` handling with the Postgresql `JSONField`.\n* #725, added `__module__` attribute to `DoesNotExist` classes.\n* #727, removed the `commit_select` logic for MySQL databases.\n* #730, added documentation for `Meta.order_by` API.\n* #745, added `cast()` method for casting JSON field values.\n* #748, added docs and method override to indicate that SQLite does not support adding foreign key constraints after table creation.\n* Check whether pysqlite or libsqlite were compiled with BerkeleyDB support when using the :py:class:`BerkeleyDatabase`.\n* Clean up the options passed to SQLite virtual tables on creation.\n\n### Small features\n\n* #700, use sensible default if field's declared data-type is not present in the field type map.\n* #707, allow model to be specified explicitly in `prefetch()`.\n* #734, automatic testing against python 3.5.\n* #753, added support for `upsert()` ith MySQL via the `REPLACE INTO ...` statement.\n* #757, `pwiz`, the schema intropsection tool, will now generate multi-column index declarations.\n* #756, `pwiz` will capture passwords using the `getpass()` function rather than via the command-line.\n* Removed `Database.sql_error_handler()`, replaced with the `RetryOperationalError` mixin class.\n* Documentation for `Meta.order_by` and `Meta.primary_key`.\n* Better documentation around column and table constraints.\n* Improved performance for some methods that are called frequently.\n* Added `coerce` parameter to `BareField` and added documentation.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.6.4...2.7.0)\n\n\n## 2.6.4\n\nUpdating so some of the new APIs are available on pypi.\n\n### Bugs fixed\n\n* #646, fixed a bug with the Cython speedups not being included in package.\n* #654, documented how to create models with no primary key.\n* #659, allow bare `INSERT` statements.\n* #674, regarding foreign key / one-to-one relationships.\n* #676, allow `ArrayField` to accept tuples in addition to lists.\n* #679, fix regarding unsaved relations.\n* #682, refactored QueryResultWrapper to allow multiple independent iterations over the same underlying result cache.\n* #692, fix bug with multiple joins to same table + eager loading.\n* #695, fix bug when connection fails while using an execution context.\n* #698, use correct column names with non-standard django foreign keys.\n* #706, return `datetime.time` instead of `timedelta` for MySQL time fields.\n* #712, fixed SQLite migrator regular expressions. Thanks @sroebert.\n\n### New features\n\n* #647, #649, #650, added support for `RETURNING` clauses. Update, Insert and Delete queries can now be called with `RETURNING` to retrieve the rows that were affected. [See docs](http://docs.peewee-orm.com/en/latest/peewee/querying.html#returning-clause).\n* #685, added web request hook docs.\n* #691, allowed arbitrary model attributes and methods to be serialized by `model_to_dict()`. [Docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#model_to_dict).\n* #696, allow `model_to_dict()` to introspect query for which fields to serialize.\n* Added backend-agnostic [truncate_date()](http://docs.peewee-orm.com/en/latest/peewee/api.html#Database.truncate_date) implementation.\n* Added a `FixedCharField` which uses column type `CHAR`.\n* Added support for arbitrary `PRAGMA` statements to be run on new SQLite connections. [Docs](http://docs.peewee-orm.com/en/latest/peewee/databases.html#sqlite-pragma).\n* Removed `berkeley_build.sh` script. See instructions [on my blog instead](http://charlesleifer.com/blog/building-the-python-sqlite-driver-for-use-with-berkeleydb/).\n\n[View commits](https://github.com/coleifer/peewee/compare/2.6.2...2.6.4)\n\n## 2.6.2\n\nJust a regular old release.\n\n### Bugs fixed\n\n* #641, fixed bug with exception wrapping and Python 2.6\n* #634, fixed bug where correct query result wrapper was not being used for certain composite queries.\n* #625, cleaned up some example code.\n* #614, fixed bug with `aggregate_rows()` when there are multiple joins to the same table.\n\n### New features\n\n* Added [create_or_get()](http://docs.peewee-orm.com/en/latest/peewee/querying.html#create-or-get) as a companion to `get_or_create()`.\n* Added support for `ON CONFLICT` clauses for `UPDATE` and `INSERT` queries. [Docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#UpdateQuery.on_conflict).\n* Added a [JSONKeyStore](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#JSONKeyStore) to `playhouse.kv`.\n* Added Cythonized version of `strip_parens()`, with plans to perhaps move more performance-critical code to Cython in the future.\n* Added docs on specifying [vendor-specific database parameters](http://docs.peewee-orm.com/en/latest/peewee/database.html#vendor-specific-parameters).\n* Added docs on specifying [field default values](http://docs.peewee-orm.com/en/latest/peewee/models.html#default-field-values) (both client and server-side).\n* Added docs on [foreign key field back-references](http://docs.peewee-orm.com/en/latest/peewee/models.html#foreignkeyfield).\n* Added docs for [models without a primary key](http://docs.peewee-orm.com/en/latest/peewee/models.html#models-without-a-primary-key).\n* Cleaned up docs on `prefetch()` and `aggregate_rows()`.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.6.1...2.6.2)\n\n## 2.6.1\n\nThis release contains a number of small fixes and enhancements.\n\n### Bugs fixed\n\n* #606, support self-referential joins with `prefetch` and `aggregate_rows()` methods.\n* #588, accomodate changes in SQLite's `PRAGMA index_list()` return value.\n* #607, fixed bug where `pwiz` was not passing table names to introspector.\n* #591, fixed bug with handling of named cursors in older psycopg2 version.\n* Removed some cruft from the `APSWDatabase` implementation.\n\n### New features\n\n* Added [CompressedField](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#CompressedField) and [AESEncryptedField](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#AESEncryptedField)\n* #609, #610, added Django-style foreign key ID lookup. [Docs](http://docs.peewee-orm.com/en/latest/peewee/models.html#foreignkeyfield).\n* Added support for [Hybrid Attributes](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#hybrid-attributes) (cool idea courtesy of SQLAlchemy).\n* Added ``upsert`` keyword argument to the `Model.save()` function (SQLite only).\n* #587, added support for ``ON CONFLICT`` SQLite clause for `INSERT` and `UPDATE` queries. [Docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#UpdateQuery.on_conflict)\n* #601, added hook for programmatically defining table names. [Model options docs](http://docs.peewee-orm.com/en/latest/peewee/models.html#model-options-and-table-metadata)\n* #581, #611, support connection pools with `playhouse.db_url.connect()`. [Docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#connect).\n* Added [Contributing section](http://docs.peewee-orm.com/en/latest/peewee/contributing.html) section to docs.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.6.0...2.6.1)\n\n## 2.6.0\n\nThis is a tiny update, mainly consisting of a new-and-improved implementation of ``get_or_create()`` ([docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#Model.get_or_create)).\n\n### Backwards-incompatible changes\n\n* ``get_or_create()`` now returns a 2-tuple consisting of the model instance and a boolean indicating whether the instance was created. The function now behaves just like the Django equivalent.\n\n### New features\n\n* #574, better support for setting the character encoding on Postgresql database connections. Thanks @klen!\n* Improved implementation of [get_or_create()](http://docs.peewee-orm.com/en/latest/peewee/api.html#Model.get_or_create).\n\n[View commits](https://github.com/coleifer/peewee/compare/2.5.1...2.6.0)\n\n## 2.5.1\n\nThis is a relatively small release with a few important bugfixes.\n\n### Bugs fixed\n\n* #566, fixed a bug regarding parentheses around compound `SELECT` queries (i.e. `UNION`, `INTERSECT`, etc).\n* Fixed unreported bug where table aliases were not generated correctly for compound `SELECT` queries.\n* #559, add option to preserve original column order with `pwiz`. Thanks @elgow!\n* Fixed unreported bug where selecting all columns from a `ModelAlias` does not use the appropriate `FieldAlias` objects.\n\n### New features\n\n* #561, added an option for bulk insert queries to return the list of auto-generated primary keys. See [docs for InsertQuery.return_id_list](http://docs.peewee-orm.com/en/latest/peewee/api.html#InsertQuery.return_id_list).\n* #569, added `parse` function to the `playhouse.db_url` module. Thanks @stt!\n* Added [hacks](http://docs.peewee-orm.com/en/latest/peewee/hacks.html) section to the docs. Please contribute your hacks!\n\n### Backwards-incompatible changes\n\n* Calls to `Node.in_()` and `Node.not_in()` do not take `*args` anymore and instead take a single argument.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.5.0...2.5.1)\n\n## 2.5.0\n\nThere are a couple new features so I thought I'd bump to 2.5.x. One change Postgres users may be happy to see is the use of `INSERT ... RETURNING` to perform inserts. This should definitely speed up inserts for Postgres, since an extra query is no longer needed to get the new auto-generated primary key.\n\nI also added a [new context manager/decorator](http://docs.peewee-orm.com/en/latest/peewee/database.html#using-multiple-databases) that allows you to use a different database for the duration of the wrapped block.\n\n### Bugs fixed\n\n* #534, CSV utils was erroneously stripping the primary key from CSV data.\n* #537, fix upserts when using `insert_many`.\n* #541, respect `autorollback` with `PostgresqlExtDatabase`. Thanks @davidmcclure.\n* #551, fix for QueryResultWrapper's implementation of the iterator protocol.\n* #554, allow SQLite journal_mode to be set at run-time.\n* Fixed case-sensitivity issue with `DataSet`.\n\n### New features\n\n* Added support for [CAST expressions](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#cast).\n* Added a hook for [extending Node](http://docs.peewee-orm.com/en/latest/peewee/api.html#Node.extend) with custom methods.\n* `JOIN_<type>` became `JOIN.<type>`, e.g. `.join(JOIN.LEFT_OUTER)`.\n* `OP_<code>` became `OP.<code>`.\n* #556, allowed using `+` and `-` prefixes to indicate ascending/descending ordering.\n* #550, added [Database.initialize_connection()](http://docs.peewee-orm.com/en/latest/peewee/database.html#additional-connection-initialization) hook.\n* #549, bind selected columns to a particular model. Thanks @jhorman, nice PR!\n* #531, support for swapping databases at run-time via [Using](http://docs.peewee-orm.com/en/latest/peewee/database.html#using-multiple-databases).\n* #530, support for SQLCipher and Python3.\n* New `RowIDField` for `sqlite_ext` playhouse module. This field can be used to interact with SQLite `rowid` fields.\n* Added `LateralJoin` helper to the `postgres_ext` playhouse module.\n* New [example blog app](https://github.com/coleifer/peewee/tree/master/examples/blog).\n\n[View commits](https://github.com/coleifer/peewee/compare/2.4.7...2.5.0)\n\n## 2.4.7\n\n### Bugs fixed\n\n* #504, Docs updates.\n* #506, Fixed regression in `aggregate_rows()`\n* #510, Fixes bug in pwiz overwriting columns.\n* #514, Correctly cast foreign keys in `prefetch()`.\n* #515, Simplifies queries issued when doing recursive deletes.\n* #516, Fix cloning of Field objects.\n* #519, Aggregate rows now correctly preserves ordering of joined instances.\n* Unreported, fixed bug to not leave expired connections sitting around in the pool.\n\n### New features\n\n* Added support for Postgresql's ``jsonb`` type with [BinaryJSONField](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#BinaryJSONField).\n* Add some basic [Flask helpers](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#flask-utils).\n* Add support for `UNION ALL` queries in #512\n* Add `SqlCipherExtDatabase`, which combines the sqlcipher database with the sqlite extensions.\n* Add option to print metadata when generating code with ``pwiz``.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.4.6...2.4.7)\n\n## 2.4.6\n\nThis is a relatively small release with mostly bug fixes and updates to the documentation. The one new feature I'd like to highlight is the ``ManyToManyField`` ([docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#ManyToManyField)).\n\n### Bugs fixed\n\n* #503, fixes behavior of `aggregate_rows()` when used with a `CompositeKey`.\n* #498, fixes value coercion for field aliases.\n* #492, fixes bug with pwiz and composite primary keys.\n* #486, correctly handle schemas with reflection module.\n\n### New features\n\n* Peewee has a new [ManyToManyField](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#ManyToManyField) available in the ``playhouse.shortcuts`` module.\n* Peewee now has proper support for *NOT IN* queries through the ``Node.not_in()`` method.\n* Models now support iteration. This is equivalent to ``Model.select()``.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.4.5...2.4.6)\n\n## 2.4.5\n\nI'm excited about this release, as in addition to a number of new features and bugfixes, it also is a step towards cleaner code. I refactored the tests into a number of modules, using a standard set of base test-cases and helpers. I also introduced the `mock` library into the test suite and plan to use it for cleaner tests going forward. There's a lot of work to do to continue cleaning up the tests, but I'm feeling good about the changes. Curiously, the test suite runs faster now.\n\n### Bugs fixed\n\n* #471, #482 and #484, all of which had to do with how joins were handled by the `aggregate_rows()` query result wrapper.\n* #472 removed some needless special-casing in `Model.save()`.\n* #466 fixed case-sensitive issues with the SQLite migrator.\n* #474 fixed a handful of bugs that cropped up migrating foreign keys with SQLite.\n* #475 fixed the behavior of the SQLite migrator regarding auto-generated indexes.\n* #479 fixed a bug in the code that stripped extra parentheses in the SQL generator.\n* Fixed a handful of bugs in the APSW extension.\n\n### New features\n\n* Added connection abstraction called `ExecutionContext` ([see docs](http://docs.peewee-orm.com/en/latest/peewee/database.html#advanced-connection-management)).\n* Made all context managers work as decorators (`atomic`, `transaction`, `savepoint`, `execution_context`).\n* Added explicit methods for `IS NULL` and `IS NOT NULL` queries. The latter was actually necessary since the behavior is different from `NOT IS NULL (...)`.\n* Allow disabling backref validation (#465)\n* Made quite a few improvements to the documentation, particularly sections on transactions.\n* Added caching to the [DataSet](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#dataset) extension, which should improve performance.\n* Made the SQLite migrator smarter with regards to preserving indexes when a table copy is necessary.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.4.4...2.4.5)\n\n## 2.4.4\n\nBiggest news: peewee has a new logo!\n\n![](https://media.charlesleifer.com/blog/photos/peewee-logo-bold.png)\n\n* Small documentation updates here and there.\n\n### Backwards-incompatible changes\n\n* The argument signature for the `SqliteExtDatabase.aggregate()` decorator changed so that the aggregate name is the first parameter, and the number of parameters is the second parameter. If no values are specified, peewee will choose the name of the class and an un-specified number of arguments (`-1`).\n* The logic for saving a model with a composite key changed slightly. Previously, if a model had a composite primary key and you called `save()`, only the dirty fields would be saved.\n\n### Bugs fixed\n\n* #462\n* #465, add hook for disabling backref validation.\n* #466, fix case-sensitive table names with migration module.\n* #469, save only dirty fields.\n\n### New features\n\n* Lots of enhancements and cleanup to the `playhouse.apsw_ext` module.\n* The `playhouse.reflection` module now supports introspecting indexes.\n* Added a model option for disabling backref validation.\n* Added support for the SQLite [closure table extension](http://charlesleifer.com/blog/querying-tree-structures-in-sqlite-using-python-and-the-transitive-closure-extension/).\n* Added support for *virtual fields*, which act on dynamically-created virtual table fields.\n* Added a new example: a virtual table implementation that exposes Redis as a relational database table.\n* Added a module `playhouse.sqlite_aggregates` that contains a handful of aggregates you may find useful when developing with SQLite.\n\n\n[View commits](https://github.com/coleifer/peewee/compare/2.4.3...2.4.4)\n\n## 2.4.3\n\nThis release contains numerous improvements, particularly around the built-in database introspection utilities. Peewee should now also be compatible with PyPy.\n\n### Bugs fixed\n\n* #466, table names are case sensitive in the SQLite migrations module.\n* #465, added option to disable backref validation.\n* #462, use the schema name consistently with postgres reflection.\n\n### New features\n\n* New model *Meta* option to disable backref validation. [See validate_backrefs](http://docs.peewee-orm.com/en/latest/peewee/models.html#model-options-and-table-metadata).\n* Added documentation on ordering by calculated values.\n* Added basic PyPy compatibility.\n* Added logic to close cursors after they have been exhausted.\n* Structured and consolidated database metadata introspection, including improvements for introspecting indexes.\n* Added support to [prefetch](http://docs.peewee-orm.com/en/latest/peewee/api.html?highlight=prefetch#prefetch) for traversing *up* the query tree.\n* Added introspection option to skip invalid models while introspecting.\n* Added option to limit the tables introspected.\n* Added closed connection detection to the MySQL connection pool.\n* Enhancements to passing options to creating virtual tables with SQLite.\n* Added factory method for generating Closure tables for use with the `transitive_closure` SQLite extension.\n* Added support for loading SQLite extensions.\n* Numerous test-suite enhancements and new test-cases.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.4.2...2.4.3)\n\n## 2.4.2\n\nThis release contains a number of improvements to the `reflection` and `migrate` extension modules. I also added an encrypted *diary* app to the [examples](https://github.com/coleifer/peewee/tree/master/examples) directory.\n\n### Bugs fixed\n\n* #449, typo in the db_url extension, thanks to @malea for the fix.\n* #457 and #458, fixed documentation deficiences.\n\n### New features\n\n* Added support for [importing data](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#importing-data) when using the [DataSet extension](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#dataset).\n* Added an encrypted diary app to the examples.\n* Better index reconstruction when altering columns on SQLite databases with the [migrate](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#migrate) module.\n* Support for multi-column primary keys in the [reflection](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#reflection) module.\n* Close cursors more aggressively when executing SELECT queries.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.4.1...2.4.2)\n\n## 2.4.1\n\nThis release contains a few small bugfixes.\n\n### Bugs fixed\n\n* #448, add hook to the connection pool for detecting closed connections.\n* #229, fix join attribute detection.\n* #447, fixed documentation typo.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.4.0...2.4.1)\n\n## 2.4.0\n\nThis release contains a number of enhancements to the `playhouse` collection of extensions.\n\n### Backwards-incompatible changes\n\nAs of 2.4.0, most of the introspection logic was moved out of the ``pwiz`` module and into ``playhouse.reflection``.\n\n### New features\n\n* Created a new [reflection](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#reflection) extension for introspecting databases. The *reflection* module additionally can generate actual peewee Model classes dynamically.\n* Created a [dataset](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#dataset) library (based on the [SQLAlchemy project](https://dataset.readthedocs.io/) of the same name). For more info check out the blog post [announcing playhouse.dataset](http://charlesleifer.com/blog/saturday-morning-hacks-dataset-for-peewee/).\n* Added a [db_url](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#database-url) module which creates `Database` objects from a connection string.\n* Added [csv dump](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#dumping-csv) functionality to the [CSV utils](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#csv-utils) extension.\n* Added an [atomic](http://docs.peewee-orm.com/en/latest/peewee/transactions.html#nesting-transactions) context manager to support nested transactions.\n* Added support for HStore, JSON and TSVector to the `reflection` module.\n* More documentation updates.\n\n### Bugs fixed\n\n* Fixed #440, which fixes a bug where `Model.dirty_fields` did not return an empty set for some subclasses of `QueryResultWrapper`.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.3.3...2.4.0)\n\n## 2.3.3\n\nThis release contains a lot of improvements to the documentation and a mixed bag of other new features and bugfixes.\n\n### Backwards-incompatible changes\n\nAs of 2.3.3, all peewee `Database` instances have a default of `True` for the `threadlocals` parameter. This means that a connection is opened for each thread. It seemed to me that by sharing connections across threads caused a lot of confusion to users who weren't aware of (or familiar with) the `threadlocals` parameter. For single-threaded apps the behavior will not be affected, but for multi-threaded applications, if you wish to share your connection across threads you must now specify `threadlocals=False`. For more information, see the [documentation](http://docs.peewee-orm.com/en/latest/peewee/api.html#Database).\n\nI also renamed the `Model.get_id()` and `Model.set_id()` convenience methods so as not to conflict with Flask-Login. These methods should have probably been private anyways, and the new methods are named `_get_pk_value()` and `_set_pk_value()`.\n\n### New features\n\n* Basic support for [Postgresql full-text search](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#pg-fts).\n* Helper functions for converting models to dictionaries and unpacking dictionaries into model instances. See [docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#model_to_dict).\n\n### Bugs fixed\n\n* Fixed #428, documentation formatting error.\n* Fixed #429, which fixes the way default values are initialized for bulk inserts.\n* Fixed #432, making the HStore extension optional when using `PostgresqlExtDatabase`.\n* Fixed #435, allowing peewee to be used with Flask-Login.\n* Fixed #436, allowing the SQLite date_part and date_trunc functions to correctly handle NULL values.\n* Fixed #438, in which the ordering of clauses in a Join expression were causing unpredictable behavior when selecting related instances.\n* Updated the `berkeley_build.sh` script, which was incompatible with the newest version of `bsddb3`.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.3.2...2.3.3)\n\n## 2.3.2\n\nThis release contains mostly bugfixes.\n\n### Changes in 2.3.2\n\n* Fixed #421, allowing division operations to work correctly in py3k.\n* Added support for custom json.dumps command, thanks to @alexlatchford.\n* Fixed some foreign key generation bugs with pwiz in #426.\n* Fixed a parentheses bug with UNION queries, #422.\n* Added support for returning partial JSON data-structures from postgresql.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.3.1...2.3.2)\n\n## 2.3.1\n\nThis release contains a fix for a bug introducted in 2.3.0. Table names are included, unquoted, in update queries now, which is causing some problems when the table name is a keyword.\n\n### Changes in 2.3.1\n\n* [Quote table name / alias](https://github.com/coleifer/peewee/issues/414)\n\n[View commits](https://github.com/coleifer/peewee/compare/2.3.0...2.3.1)\n\n## 2.3.0\n\nThis release contains a number of bugfixes, enhancements and a rewrite of much of the documentation.\n\n### Changes in 2.3.0\n\n* [New and improved documentation](http://docs.peewee-orm.com/)\n* Added [aggregate_rows()](http://docs.peewee-orm.com/en/latest/peewee/querying.html#list-users-and-all-their-tweets) method for mitigating N+1 queries.\n* Query compiler performance improvements and rewrite of table alias internals (51d82fcd and d8d55df04).\n* Added context-managers and decorators for [counting queries](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#count_queries) and [asserting query counts](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#assert_query_count).\n* Allow `UPDATE` queries to contain subqueries for values ([example](http://docs.peewee-orm.com/en/latest/peewee/querying.html#atomic-updates)).\n* Support for `INSERT INTO / SELECT FROM` queries ([docs](http://docs.peewee-orm.com/en/latest/peewee/api.html?highlight=insert_from#Model.insert_from)).\n* Allow `SqliteDatabase` to set the database's journal mode.\n* Added method for concatenation ([docs]()).\n* Moved ``UUIDField`` out of the playhouse and into peewee\n* Added [pskel](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#pskel) script.\n* Documentation for [BerkeleyDB](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#berkeleydb).\n\n### Bugs fixed\n\n* #340, allow inner query values to be used in outer query joins.\n* #380, fixed foreign key handling in SQLite migrations.\n* #389, mark foreign keys as dirty on assignment.\n* #391, added an ``orwhere()`` method.\n* #392, fixed ``order_by`` meta option inheritance bug.\n* #394, fixed UUID and conversion of foreign key values (thanks @alexlatchford).\n* #395, allow selecting all columns using ``SQL('*')``.\n* #396, fixed query compiler bug that was adding unnecessary parentheses around expressions.\n* #405, fixed behavior of ``count()`` when query has a limit or offset.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.2.5...2.3.0)\n\n## 2.2.5\n\nThis is a small release and contains a handful of fixes.\n\n### Changes in 2.2.5\n\n* Added a `Window` object for creating reusable window definitions.\n* Added support for `DISTINCT ON (...)`.\n* Added a BerkeleyDB-backed sqlite `Database` and build script.\n* Fixed how the `UUIDField` handles `None` values (thanks @alexlatchford).\n* Fixed various things in the example app.\n* Added 3.4 to the travis build (thanks @frewsxcv).\n\n[View commits](https://github.com/coleifer/peewee/compare/2.2.4...2.2.5)\n\n## 2.2.4\n\nThis release contains a complete rewrite of `pwiz` as well as some improvements to the SQLite extension, including support for the BM25 ranking algorithm for full-text searches. I also merged support for sqlcipher, an encrypted SQLite database with many thanks to @thedod!\n\n### Changes in 2.2.4\n\n* Rewrite of `pwiz`, schema introspection utility.\n* `Model.save()` returns a value indicating the number of modified rows.\n* Fixed bug with `PostgresqlDatabase.last_insert_id()` leaving a transaction open in autocommit mode (#353).\n* Added BM25 ranking algorithm for full-text searches with SQLite.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.2.3...2.2.4)\n\n## 2.2.3\n\nThis release contains a new migrations module in addition to a number of small features and bug fixes.\n\n### Changes in 2.2.3\n\n* New migrations module.\n* Added a return value to `Model.save()` indicating number of rows affected.\n* Added a `date_trunc()` method that works for Sqlite.\n* Added a `Model.sqlall()` class-method to return all the SQL to generate the model / indices.\n\n### Bugs fixed\n\n* #342, allow functions to not coerce parameters automatically.\n* #338, fixed unaliased columns when using Array and Json fields with postgres, thanks @mtwesley.\n* #331, corrected issue with the way unicode arrays were adapted with psycopg2.\n* #328, pwiz / mysql bug.\n* #326, fixed calculation of the alias_map when using subqueries.\n* #324, bug with `prefetch()` not selecting the correct primary key.\n\n\n[View commits](https://github.com/coleifer/peewee/compare/2.2.2...2.2.3)\n\n\n## 2.2.1\n\nI've been looking forward to this release, as it contains a couple new features\nthat I've been wanting to add for some time now. Hope you find them useful.\n\n### Changes in 2.2.1\n\n* Window queries using ``OVER`` syntax.\n* Compound query operations ``UNION``, ``INTERSECT``, ``EXCEPT`` as well as symmetric difference.\n\n### Bugs fixed\n\n* #300, pwiz was not correctly interpreting some foreign key constraints in SQLite.\n* #298, drop table with cascade API was missing.\n* #294, typo.\n\n[View commits](https://github.com/coleifer/peewee/compare/2.2.0...2.2.1)\n\n## 2.2.0\n\nThis release contains a large refactoring of the way SQL was generated for both\nthe standard query classes (`Select`, `Insert`, `Update`, `Delete`) as well as\nfor the DDL methods (`create_table`, `create_index`, etc). Instead of joining\nstrings of SQL and manually quoting things, I've created `Clause` objects\ncontaining multiple `Node` objects to represent all parts of the query.\n\nI also changed the way peewee determins the SQL to represent a field. Now a\nfield implements ``__ddl__`` and ``__ddl_column__`` methods. The former creates\nthe entire field definition, e.g.:\n\n    \"quoted_column_name\" <result of call to __ddl_column__> [NOT NULL/PRIMARY KEY/DEFAULT NEXTVAL(...)/CONSTRAINTS...]\n\nThe latter method is responsible just for the column type definition. This might\nreturn ``VARCHAR(255)`` or simply ``TEXT``. I've also added support for\narbitrary constraints on each field, so you might have:\n\n    price = DecimalField(decimal_places=2, constraints=[Check('price > 0')])\n\n### Changes in 2.2.0\n\n* Refactored query generation for both SQL queries and DDL queries.\n* Support for arbitrary column constraints.\n* `autorollback` option to the `Database` class that will roll back the\n  transaction before raising an exception.\n* Added `JSONField` type to the `postgresql_ext` module.\n* Track fields that are explicitly set, allowing faster saves (thanks @soasme).\n* Allow the `FROM` clause to be an arbitrary `Node` object (#290).\n* `schema` is a new `Model.Mketa` option and is used throughout the code.\n* Allow indexing operation on HStore fields (thanks @zdxerr, #293).\n\n### Bugs fixed\n\n* #277 (where calls not chainable with update query)\n* #278, use `wraps()`, thanks @lucasmarshall\n* #284, call `prepared()` after `create()`, thanks @soasme.\n* #286, cursor description issue with pwiz + postgres\n\n[View commits](https://github.com/coleifer/peewee/compare/2.1.7...2.2.0)\n\n\n## 2.1.7\n\n### Changes in 2.1.7\n\n* Support for savepoints (Sqlite, Postgresql and MySQL) using an API similar to that of transactions.\n* Common set of exceptions to wrap DB-API 2 driver-specific exception classes, e.g. ``peewee.IntegrityError``.\n* When pwiz cannot determine the underlying column type, display it in a comment in the generated code.\n* Support for circular foreign-keys.\n* Moved ``Proxy`` into peewee (previously in ``playhouse.proxy``).\n* Renamed ``R()`` to ``SQL()``.\n* General code cleanup, some new comments and docstrings.\n\n### Bugs fixed\n\n* Fixed a small bug in the way errors were handled in transaction context manager.\n* #257\n* #265, nest multiple calls to functions decorated with `@database.commit_on_success`.\n* #266\n* #267\n\nCommits: https://github.com/coleifer/peewee/compare/2.1.6...2.1.7\nReleased 2013-12-25\n\n## 2.1.6\n\nChanges included in 2.1.6:\n\n* [Lightweight Django integration](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#django-integration).\n* Added a [csv loader](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#csv-loader) to playhouse.\n* Register unicode converters per-connection instead of globally when using `pscyopg2`.\n* Fix for how the related object cache is invalidated (#243).\n\nCommits: https://github.com/coleifer/peewee/compare/2.1.5...2.1.6\nReleased 2013-11-19\n\n## 2.1.5\n\n### Summary of new features\n\n* Rewrote the ``playhouse.postgres_ext.ServerSideCursor`` helper to work with a single query.  [Docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#server-side-cursors).\n* Added error handler hook to the database class, allowing your code to choose how to handle errors executing SQL.  [Docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#Database.sql_error_handler).\n* Allow arbitrary attributes to be stored in ``Model.Meta`` a5e13bb26d6196dbd24ff228f99ff63d9c046f79.\n* Support for composite primary keys (!!).  [How-to](http://docs.peewee-orm.com/en/latest/peewee/cookbook.html#composite-primary-keys) and [API docs](http://docs.peewee-orm.com/en/latest/peewee/api.html#CompositeKey).\n* Added helper for generating ``CASE`` expressions.  [Docs](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#case).\n* Allow the table alias to be specified as a model ``Meta`` option.\n* Added ability to specify ``NOWAIT`` when issuing ``SELECT FOR UPDATE`` queries.\n\n### Bug fixes\n\n* #147, SQLite auto-increment behavior.\n* #222\n* #223, missing call to ``execute()`` in docs.\n* #224, python 3 compatibility fix.\n* #227, was using wrong column type for boolean with MySQL.\n\nCommits: https://github.com/coleifer/peewee/compare/2.1.4...2.1.5\nReleased 2013-10-19\n\n## 2.1.4\n\n* Small refactor of some components used to represent expressions (mostly better names).\n* Support for [Array fields](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#ArrayField) in postgresql.\n* Added notes on [Proxy](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#proxy)\n* Support for [Server side cursors](http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#server-side-cursors) with postgresql.\n* Code cleanups for more consistency.\n\nCommits: https://github.com/coleifer/peewee/compare/2.1.3...2.1.4\nReleased 2013-08-05\n\n## 2.1.3\n\n* Added the ``sqlite_ext`` module, including support for virtual tables, full-text search, user-defined functions, collations and aggregates, as well as more granular locking.\n* Manually convert data-types when doing simple aggregations - fixes issue #208\n* Profiled code and dramatically increased performance of benchmarks.\n* Added a proxy object for lazy database initialization - fixes issue #210\n\nCommits: https://github.com/coleifer/peewee/compare/2.1.2...2.1.3\nReleased 2013-06-28\n\n-------------------------------------\n\n## 2.0.0\n\nMajor rewrite, see notes here: http://docs.peewee-orm.com/en/latest/peewee/upgrading.html#upgrading\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.033203125,
          "content": "Copyright (c) 2010 Charles Leifer\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.34765625,
          "content": "include CHANGELOG.md\ninclude LICENSE\ninclude README.rst\ninclude TODO.rst\ninclude pyproject.toml\ninclude runtests.py\ninclude tests.py\ninclude playhouse/*.pyx\ninclude playhouse/*.c\ninclude playhouse/pskel\ninclude playhouse/README.md\ninclude playhouse/tests/README\nrecursive-include examples *\nrecursive-include docs *\nrecursive-include playhouse/_pysqlite *\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 5.9169921875,
          "content": ".. image:: https://media.charlesleifer.com/blog/photos/peewee3-logo.png\n\npeewee\n======\n\nPeewee is a simple and small ORM. It has few (but expressive) concepts, making it easy to learn and intuitive to use.\n\n* a small, expressive ORM\n* python 2.7+ and 3.4+\n* supports sqlite, mysql, mariadb, postgresql\n* tons of `extensions <http://docs.peewee-orm.com/en/latest/peewee/playhouse.html>`_\n\nNew to peewee? These may help:\n\n* `Quickstart <http://docs.peewee-orm.com/en/latest/peewee/quickstart.html#quickstart>`_\n* `Example twitter app <http://docs.peewee-orm.com/en/latest/peewee/example.html>`_\n* `Using peewee interactively <http://docs.peewee-orm.com/en/latest/peewee/interactive.html>`_\n* `Models and fields <http://docs.peewee-orm.com/en/latest/peewee/models.html>`_\n* `Querying <http://docs.peewee-orm.com/en/latest/peewee/querying.html>`_\n* `Relationships and joins <http://docs.peewee-orm.com/en/latest/peewee/relationships.html>`_\n\nExamples\n--------\n\nDefining models is similar to Django or SQLAlchemy:\n\n.. code-block:: python\n\n    from peewee import *\n    import datetime\n\n\n    db = SqliteDatabase('my_database.db')\n\n    class BaseModel(Model):\n        class Meta:\n            database = db\n\n    class User(BaseModel):\n        username = CharField(unique=True)\n\n    class Tweet(BaseModel):\n        user = ForeignKeyField(User, backref='tweets')\n        message = TextField()\n        created_date = DateTimeField(default=datetime.datetime.now)\n        is_published = BooleanField(default=True)\n\nConnect to the database and create tables:\n\n.. code-block:: python\n\n    db.connect()\n    db.create_tables([User, Tweet])\n\nCreate a few rows:\n\n.. code-block:: python\n\n    charlie = User.create(username='charlie')\n    huey = User(username='huey')\n    huey.save()\n\n    # No need to set `is_published` or `created_date` since they\n    # will just use the default values we specified.\n    Tweet.create(user=charlie, message='My first tweet')\n\nQueries are expressive and composable:\n\n.. code-block:: python\n\n    # A simple query selecting a user.\n    User.get(User.username == 'charlie')\n\n    # Get tweets created by one of several users.\n    usernames = ['charlie', 'huey', 'mickey']\n    users = User.select().where(User.username.in_(usernames))\n    tweets = Tweet.select().where(Tweet.user.in_(users))\n\n    # We could accomplish the same using a JOIN:\n    tweets = (Tweet\n              .select()\n              .join(User)\n              .where(User.username.in_(usernames)))\n\n    # How many tweets were published today?\n    tweets_today = (Tweet\n                    .select()\n                    .where(\n                        (Tweet.created_date >= datetime.date.today()) &\n                        (Tweet.is_published == True))\n                    .count())\n\n    # Paginate the user table and show me page 3 (users 41-60).\n    User.select().order_by(User.username).paginate(3, 20)\n\n    # Order users by the number of tweets they've created:\n    tweet_ct = fn.Count(Tweet.id)\n    users = (User\n             .select(User, tweet_ct.alias('ct'))\n             .join(Tweet, JOIN.LEFT_OUTER)\n             .group_by(User)\n             .order_by(tweet_ct.desc()))\n\n    # Do an atomic update (for illustrative purposes only, imagine a simple\n    # table for tracking a \"count\" associated with each URL). We don't want to\n    # naively get the save in two separate steps since this is prone to race\n    # conditions.\n    Counter.update(count=Counter.count + 1).where(Counter.url == request.url)\n\nCheck out the `example twitter app <http://docs.peewee-orm.com/en/latest/peewee/example.html>`_.\n\nLearning more\n-------------\n\nCheck the `documentation <http://docs.peewee-orm.com/>`_ for more examples.\n\nSpecific question? Come hang out in the #peewee channel on irc.libera.chat, or post to the mailing list, http://groups.google.com/group/peewee-orm . If you would like to report a bug, `create a new issue <https://github.com/coleifer/peewee/issues/new>`_ on GitHub.\n\nStill want more info?\n---------------------\n\n.. image:: https://media.charlesleifer.com/blog/photos/wat.jpg\n\nI've written a number of blog posts about building applications and web-services with peewee (and usually Flask). If you'd like to see some real-life applications that use peewee, the following resources may be useful:\n\n* `Building a note-taking app with Flask and Peewee <https://charlesleifer.com/blog/saturday-morning-hack-a-little-note-taking-app-with-flask/>`_ as well as `Part 2 <https://charlesleifer.com/blog/saturday-morning-hacks-revisiting-the-notes-app/>`_ and `Part 3 <https://charlesleifer.com/blog/saturday-morning-hacks-adding-full-text-search-to-the-flask-note-taking-app/>`_.\n* `Analytics web service built with Flask and Peewee <https://charlesleifer.com/blog/saturday-morning-hacks-building-an-analytics-app-with-flask/>`_.\n* `Personalized news digest (with a boolean query parser!) <https://charlesleifer.com/blog/saturday-morning-hack-personalized-news-digest-with-boolean-query-parser/>`_.\n* `Structuring Flask apps with Peewee <https://charlesleifer.com/blog/structuring-flask-apps-a-how-to-for-those-coming-from-django/>`_.\n* `Creating a lastpass clone with Flask and Peewee <https://charlesleifer.com/blog/creating-a-personal-password-manager/>`_.\n* `Creating a bookmarking web-service that takes screenshots of your bookmarks <https://charlesleifer.com/blog/building-bookmarking-service-python-and-phantomjs/>`_.\n* `Building a pastebin, wiki and a bookmarking service using Flask and Peewee <https://charlesleifer.com/blog/dont-sweat-small-stuff-use-flask-blueprints/>`_.\n* `Encrypted databases with Python and SQLCipher <https://charlesleifer.com/blog/encrypted-sqlite-databases-with-python-and-sqlcipher/>`_.\n* `Dear Diary: An Encrypted, Command-Line Diary with Peewee <https://charlesleifer.com/blog/dear-diary-an-encrypted-command-line-diary-with-python/>`_.\n* `Query Tree Structures in SQLite using Peewee and the Transitive Closure Extension <https://charlesleifer.com/blog/querying-tree-structures-in-sqlite-using-python-and-the-transitive-closure-extension/>`_.\n"
        },
        {
          "name": "TODO.rst",
          "type": "blob",
          "size": 0.009765625,
          "content": "todo\n====\n"
        },
        {
          "name": "bench.py",
          "type": "blob",
          "size": 3.6005859375,
          "content": "from peewee import *\n\n\ndb = SqliteDatabase(':memory:')\n#db = PostgresqlDatabase('peewee_test', host='127.0.0.1', port=26257, user='root')\n#db = PostgresqlDatabase('peewee_test', host='127.0.0.1', user='postgres')\n\nclass Base(Model):\n    class Meta:\n        database = db\n\nclass Register(Base):\n    value = IntegerField()\n\nclass Collection(Base):\n    name = TextField()\n\nclass Item(Base):\n    collection = ForeignKeyField(Collection, backref='items')\n    name = TextField()\n\nimport functools\nimport time\n\ndef timed(fn):\n    @functools.wraps(fn)\n    def inner(*args, **kwargs):\n        times = []\n        N = 10\n        for i in range(N):\n            start = time.perf_counter()\n            fn(i, *args, **kwargs)\n            times.append(time.perf_counter() - start)\n        print('%0.3f ... %s' % (round(sum(times) / N, 3), fn.__name__))\n    return inner\n\ndef populate_register(s, n):\n    for i in range(s, n):\n        Register.create(value=i)\n\ndef populate_collections(n, n_i):\n    for i in range(n):\n        c = Collection.create(name=str(i))\n        for j in range(n_i):\n            Item.create(collection=c, name=str(j))\n\n@timed\ndef insert(i):\n    with db.atomic():\n        populate_register((i * 1000), (i + 1) * 1000)\n\n@timed\ndef batch_insert(i):\n    it = range(i * 1000, (i + 1) * 1000)\n    for i in db.batch_commit(it, 100):\n        Register.insert(value=i).execute()\n\n@timed\ndef bulk_insert(i):\n    with db.atomic():\n        for i in range(i * 1000, (i + 1) * 1000, 100):\n            data = [(j,) for j in range(i, i + 100)]\n            Register.insert_many(data, fields=[Register.value]).execute()\n\n@timed\ndef bulk_create(i):\n    with db.atomic():\n        data = [Register(value=i) for i in range(i * 1000, (i + 1) * 1000)]\n        Register.bulk_create(data, batch_size=100)\n\n@timed\ndef select(i):\n    query = Register.select()\n    for row in query:\n        pass\n\n@timed\ndef select_related_dbapi_raw(i):\n    query = Item.select(Item, Collection).join(Collection)\n    cursor = db.execute(query)\n    for row in cursor:\n        pass\n\n@timed\ndef insert_related(i):\n    with db.atomic():\n        populate_collections(30, 60)\n\n@timed\ndef select_related(i):\n    query = Item.select(Item, Collection).join(Collection)\n    for item in query:\n        pass\n\n@timed\ndef select_related_left(i):\n    query = Collection.select(Collection, Item).join(Item, JOIN.LEFT_OUTER)\n    for collection in query:\n        pass\n\n@timed\ndef select_related_dicts(i):\n    query = Item.select(Item, Collection).join(Collection).dicts()\n    for row in query:\n        pass\n\n@timed\ndef select_related_objects(i):\n    query = Item.select(Item, Collection).join(Collection).objects()\n    for item in query:\n        pass\n\n@timed\ndef select_prefetch(i):\n    query = prefetch(Collection.select(), Item)\n    for c in query:\n        for i in c.items:\n            pass\n\n@timed\ndef select_prefetch_join(i):\n    query = prefetch(Collection.select(), Item,\n                     prefetch_type=PREFETCH_TYPE.JOIN)\n    for c in query:\n        for i in c.items:\n            pass\n\n\nif __name__ == '__main__':\n    db.create_tables([Register, Collection, Item])\n    insert()\n    insert_related()\n    Register.delete().execute()\n    batch_insert()\n    assert Register.select().count() == 10000\n    Register.delete().execute()\n    bulk_insert()\n    assert Register.select().count() == 10000\n    Register.delete().execute()\n    bulk_create()\n    assert Register.select().count() == 10000\n    select()\n    select_related()\n    select_related_left()\n    select_related_objects()\n    select_related_dicts()\n    select_related_dbapi_raw()\n    select_prefetch()\n    select_prefetch_join()\n    db.drop_tables([Register, Collection, Item])\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "peewee.py",
          "type": "blob",
          "size": 275.369140625,
          "content": "from bisect import bisect_left\nfrom bisect import bisect_right\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom functools import wraps\nfrom inspect import isclass\nimport calendar\nimport collections\nimport datetime\nimport decimal\nimport hashlib\nimport itertools\nimport logging\nimport operator\nimport re\nimport socket\nimport struct\nimport sys\nimport threading\nimport time\nimport uuid\nimport warnings\ntry:\n    from collections.abc import Mapping\nexcept ImportError:\n    from collections import Mapping\n\ntry:\n    from pysqlite3 import dbapi2 as pysq3\nexcept ImportError:\n    try:\n        from pysqlite2 import dbapi2 as pysq3\n    except ImportError:\n        pysq3 = None\ntry:\n    import sqlite3\nexcept ImportError:\n    sqlite3 = pysq3\nelse:\n    if pysq3 and pysq3.sqlite_version_info >= sqlite3.sqlite_version_info:\n        sqlite3 = pysq3\ntry:\n    from psycopg2cffi import compat\n    compat.register()\nexcept ImportError:\n    pass\ntry:\n    import psycopg2\n    from psycopg2 import extensions as pg_extensions\n    try:\n        from psycopg2 import errors as pg_errors\n    except ImportError:\n        pg_errors = None\nexcept ImportError:\n    psycopg2 = pg_errors = None\ntry:\n    from psycopg2.extras import register_uuid as pg_register_uuid\n    pg_register_uuid()\nexcept Exception:\n    pass\ntry:\n    from psycopg import errors as pg3_errors\nexcept ImportError:\n    pg3_errors = None\n\nmysql_passwd = False\ntry:\n    import pymysql as mysql\nexcept ImportError:\n    try:\n        import MySQLdb as mysql\n        mysql_passwd = True\n    except ImportError:\n        mysql = None\n\n\n__version__ = '3.17.8'\n__all__ = [\n    'AnyField',\n    'AsIs',\n    'AutoField',\n    'BareField',\n    'BigAutoField',\n    'BigBitField',\n    'BigIntegerField',\n    'BinaryUUIDField',\n    'BitField',\n    'BlobField',\n    'BooleanField',\n    'Case',\n    'Cast',\n    'CharField',\n    'Check',\n    'chunked',\n    'Column',\n    'CompositeKey',\n    'Context',\n    'Database',\n    'DatabaseError',\n    'DatabaseProxy',\n    'DataError',\n    'DateField',\n    'DateTimeField',\n    'DecimalField',\n    'DeferredForeignKey',\n    'DeferredThroughModel',\n    'DJANGO_MAP',\n    'DoesNotExist',\n    'DoubleField',\n    'DQ',\n    'EXCLUDED',\n    'Field',\n    'FixedCharField',\n    'FloatField',\n    'fn',\n    'ForeignKeyField',\n    'IdentityField',\n    'ImproperlyConfigured',\n    'Index',\n    'IntegerField',\n    'IntegrityError',\n    'InterfaceError',\n    'InternalError',\n    'IPField',\n    'JOIN',\n    'ManyToManyField',\n    'Model',\n    'ModelIndex',\n    'MySQLDatabase',\n    'NotSupportedError',\n    'OP',\n    'OperationalError',\n    'PostgresqlDatabase',\n    'PrimaryKeyField',  # XXX: Deprecated, change to AutoField.\n    'prefetch',\n    'PREFETCH_TYPE',\n    'ProgrammingError',\n    'Proxy',\n    'QualifiedNames',\n    'SchemaManager',\n    'SmallIntegerField',\n    'Select',\n    'SQL',\n    'SqliteDatabase',\n    'Table',\n    'TextField',\n    'TimeField',\n    'TimestampField',\n    'Tuple',\n    'UUIDField',\n    'Value',\n    'ValuesList',\n    'Window',\n]\n\ntry:  # Python 2.7+\n    from logging import NullHandler\nexcept ImportError:\n    class NullHandler(logging.Handler):\n        def emit(self, record):\n            pass\n\nlogger = logging.getLogger('peewee')\nlogger.addHandler(NullHandler())\n\n\nif sys.version_info[0] == 2:\n    text_type = unicode\n    bytes_type = str\n    buffer_type = buffer\n    izip_longest = itertools.izip_longest\n    callable_ = callable\n    multi_types = (list, tuple, frozenset, set)\n    exec('def reraise(tp, value, tb=None): raise tp, value, tb')\n    def print_(s):\n        sys.stdout.write(s)\n        sys.stdout.write('\\n')\nelse:\n    import builtins\n    try:\n        from collections.abc import Callable\n    except ImportError:\n        from collections import Callable\n    from functools import reduce\n    callable_ = lambda c: isinstance(c, Callable)\n    text_type = str\n    bytes_type = bytes\n    buffer_type = memoryview\n    basestring = str\n    long = int\n    multi_types = (list, tuple, frozenset, set, range)\n    print_ = getattr(builtins, 'print')\n    izip_longest = itertools.zip_longest\n    def reraise(tp, value, tb=None):\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n\n# Other compat issues.\nif sys.version_info < (3, 12):\n    utcfromtimestamp = datetime.datetime.utcfromtimestamp\n    utcnow = datetime.datetime.utcnow\nelse:\n    def utcfromtimestamp(ts):\n        return (datetime.datetime\n                .fromtimestamp(ts, tz=datetime.timezone.utc)\n                .replace(tzinfo=None))\n    def utcnow():\n        return (datetime.datetime\n                .now(datetime.timezone.utc)\n                .replace(tzinfo=None))\n\n\nif sqlite3:\n    sqlite3.register_adapter(decimal.Decimal, str)\n    sqlite3.register_adapter(datetime.date, str)\n    sqlite3.register_adapter(datetime.time, str)\n    if sys.version_info >= (3, 12):\n        # We need to register datetime adapters as these are deprecated.\n        def datetime_adapter(d): return d.isoformat(' ')\n        def convert_date(d): return datetime.date(*map(int, d.split(b'-')))\n        def convert_timestamp(t):\n            date, time = t.split(b' ')\n            y, m, d = map(int, date.split(b'-'))\n            t_full = time.split(b'.')\n            hour, minute, second = map(int, t_full[0].split(b':'))\n            if len(t_full) == 2:\n                usec = int('{:0<6.6}'.format(t_full[1].decode()))\n            else:\n                usec = 0\n            return datetime.datetime(y, m, d, hour, minute, second, usec)\n        sqlite3.register_adapter(datetime.datetime, datetime_adapter)\n        sqlite3.register_converter('date', convert_date)\n        sqlite3.register_converter('timestamp', convert_timestamp)\n\n    __sqlite_version__ = sqlite3.sqlite_version_info\nelse:\n    __sqlite_version__ = (0, 0, 0)\n\n\n__date_parts__ = set(('year', 'month', 'day', 'hour', 'minute', 'second'))\n\n# Sqlite does not support the `date_part` SQL function, so we will define an\n# implementation in python.\n__sqlite_datetime_formats__ = (\n    '%Y-%m-%d %H:%M:%S',\n    '%Y-%m-%d %H:%M:%S.%f',\n    '%Y-%m-%d',\n    '%H:%M:%S',\n    '%H:%M:%S.%f',\n    '%H:%M')\n\n__sqlite_date_trunc__ = {\n    'year': '%Y-01-01 00:00:00',\n    'month': '%Y-%m-01 00:00:00',\n    'day': '%Y-%m-%d 00:00:00',\n    'hour': '%Y-%m-%d %H:00:00',\n    'minute': '%Y-%m-%d %H:%M:00',\n    'second': '%Y-%m-%d %H:%M:%S'}\n\n__mysql_date_trunc__ = __sqlite_date_trunc__.copy()\n__mysql_date_trunc__['minute'] = '%Y-%m-%d %H:%i:00'\n__mysql_date_trunc__['second'] = '%Y-%m-%d %H:%i:%S'\n\ndef _sqlite_date_part(lookup_type, datetime_string):\n    assert lookup_type in __date_parts__\n    if not datetime_string:\n        return\n    dt = format_date_time(datetime_string, __sqlite_datetime_formats__)\n    return getattr(dt, lookup_type)\n\ndef _sqlite_date_trunc(lookup_type, datetime_string):\n    assert lookup_type in __sqlite_date_trunc__\n    if not datetime_string:\n        return\n    dt = format_date_time(datetime_string, __sqlite_datetime_formats__)\n    return dt.strftime(__sqlite_date_trunc__[lookup_type])\n\n\ndef __deprecated__(s):\n    warnings.warn(s, DeprecationWarning)\n\n\nclass attrdict(dict):\n    def __getattr__(self, attr):\n        try:\n            return self[attr]\n        except KeyError:\n            raise AttributeError(attr)\n    def __setattr__(self, attr, value): self[attr] = value\n    def __iadd__(self, rhs): self.update(rhs); return self\n    def __add__(self, rhs): d = attrdict(self); d.update(rhs); return d\n\nSENTINEL = object()\n\n#: Operations for use in SQL expressions.\nOP = attrdict(\n    AND='AND',\n    OR='OR',\n    ADD='+',\n    SUB='-',\n    MUL='*',\n    DIV='/',\n    BIN_AND='&',\n    BIN_OR='|',\n    XOR='#',\n    MOD='%',\n    EQ='=',\n    LT='<',\n    LTE='<=',\n    GT='>',\n    GTE='>=',\n    NE='!=',\n    IN='IN',\n    NOT_IN='NOT IN',\n    IS='IS',\n    IS_NOT='IS NOT',\n    LIKE='LIKE',\n    ILIKE='ILIKE',\n    BETWEEN='BETWEEN',\n    REGEXP='REGEXP',\n    IREGEXP='IREGEXP',\n    CONCAT='||',\n    BITWISE_NEGATION='~')\n\n# To support \"django-style\" double-underscore filters, create a mapping between\n# operation name and operation code, e.g. \"__eq\" == OP.EQ.\nDJANGO_MAP = attrdict({\n    'eq': operator.eq,\n    'lt': operator.lt,\n    'lte': operator.le,\n    'gt': operator.gt,\n    'gte': operator.ge,\n    'ne': operator.ne,\n    'in': operator.lshift,\n    'is': lambda l, r: Expression(l, OP.IS, r),\n    'like': lambda l, r: Expression(l, OP.LIKE, r),\n    'ilike': lambda l, r: Expression(l, OP.ILIKE, r),\n    'regexp': lambda l, r: Expression(l, OP.REGEXP, r),\n})\n\n#: Mapping of field type to the data-type supported by the database. Databases\n#: may override or add to this list.\nFIELD = attrdict(\n    AUTO='INTEGER',\n    BIGAUTO='BIGINT',\n    BIGINT='BIGINT',\n    BLOB='BLOB',\n    BOOL='SMALLINT',\n    CHAR='CHAR',\n    DATE='DATE',\n    DATETIME='DATETIME',\n    DECIMAL='DECIMAL',\n    DEFAULT='',\n    DOUBLE='REAL',\n    FLOAT='REAL',\n    INT='INTEGER',\n    SMALLINT='SMALLINT',\n    TEXT='TEXT',\n    TIME='TIME',\n    UUID='TEXT',\n    UUIDB='BLOB',\n    VARCHAR='VARCHAR')\n\n#: Join helpers (for convenience) -- all join types are supported, this object\n#: is just to help avoid introducing errors by using strings everywhere.\nJOIN = attrdict(\n    INNER='INNER JOIN',\n    LEFT_OUTER='LEFT OUTER JOIN',\n    RIGHT_OUTER='RIGHT OUTER JOIN',\n    FULL='FULL JOIN',\n    FULL_OUTER='FULL OUTER JOIN',\n    CROSS='CROSS JOIN',\n    NATURAL='NATURAL JOIN',\n    LATERAL='LATERAL',\n    LEFT_LATERAL='LEFT JOIN LATERAL')\n\n# Row representations.\nROW = attrdict(\n    TUPLE=1,\n    DICT=2,\n    NAMED_TUPLE=3,\n    CONSTRUCTOR=4,\n    MODEL=5)\n\n# Query type to use with prefetch\nPREFETCH_TYPE = attrdict(\n    WHERE=1,\n    JOIN=2)\n\nSCOPE_NORMAL = 1\nSCOPE_SOURCE = 2\nSCOPE_VALUES = 4\nSCOPE_CTE = 8\nSCOPE_COLUMN = 16\n\n# Rules for parentheses around subqueries in compound select.\nCSQ_PARENTHESES_NEVER = 0\nCSQ_PARENTHESES_ALWAYS = 1\nCSQ_PARENTHESES_UNNESTED = 2\n\n# Regular expressions used to convert class names to snake-case table names.\n# First regex handles acronym followed by word or initial lower-word followed\n# by a capitalized word. e.g. APIResponse -> API_Response / fooBar -> foo_Bar.\n# Second regex handles the normal case of two title-cased words.\nSNAKE_CASE_STEP1 = re.compile('(.)_*([A-Z][a-z]+)')\nSNAKE_CASE_STEP2 = re.compile('([a-z0-9])_*([A-Z])')\n\n# Helper functions that are used in various parts of the codebase.\nMODEL_BASE = '_metaclass_helper_'\n\ndef with_metaclass(meta, base=object):\n    return meta(MODEL_BASE, (base,), {})\n\ndef merge_dict(source, overrides):\n    merged = source.copy()\n    if overrides:\n        merged.update(overrides)\n    return merged\n\ndef quote(path, quote_chars):\n    if len(path) == 1:\n        return path[0].join(quote_chars)\n    return '.'.join([part.join(quote_chars) for part in path])\n\nis_model = lambda o: isclass(o) and issubclass(o, Model)\n\ndef ensure_tuple(value):\n    if value is not None:\n        return value if isinstance(value, (list, tuple)) else (value,)\n\ndef ensure_entity(value):\n    if value is not None:\n        return value if isinstance(value, Node) else Entity(value)\n\ndef make_snake_case(s):\n    first = SNAKE_CASE_STEP1.sub(r'\\1_\\2', s)\n    return SNAKE_CASE_STEP2.sub(r'\\1_\\2', first).lower()\n\ndef chunked(it, n):\n    marker = object()\n    for group in (list(g) for g in izip_longest(*[iter(it)] * n,\n                                                fillvalue=marker)):\n        if group[-1] is marker:\n            del group[group.index(marker):]\n        yield group\n\n\nclass _callable_context_manager(object):\n    def __call__(self, fn):\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            with self:\n                return fn(*args, **kwargs)\n        return inner\n\n\nclass Proxy(object):\n    \"\"\"\n    Create a proxy or placeholder for another object.\n    \"\"\"\n    __slots__ = ('obj', '_callbacks')\n\n    def __init__(self):\n        self._callbacks = []\n        self.initialize(None)\n\n    def initialize(self, obj):\n        self.obj = obj\n        for callback in self._callbacks:\n            callback(obj)\n\n    def attach_callback(self, callback):\n        self._callbacks.append(callback)\n        return callback\n\n    def passthrough(method):\n        def inner(self, *args, **kwargs):\n            if self.obj is None:\n                raise AttributeError('Cannot use uninitialized Proxy.')\n            return getattr(self.obj, method)(*args, **kwargs)\n        return inner\n\n    # Allow proxy to be used as a context-manager.\n    __enter__ = passthrough('__enter__')\n    __exit__ = passthrough('__exit__')\n\n    def __getattr__(self, attr):\n        if self.obj is None:\n            raise AttributeError('Cannot use uninitialized Proxy.')\n        return getattr(self.obj, attr)\n\n    def __setattr__(self, attr, value):\n        if attr not in self.__slots__:\n            raise AttributeError('Cannot set attribute on proxy.')\n        return super(Proxy, self).__setattr__(attr, value)\n\n\nclass DatabaseProxy(Proxy):\n    \"\"\"\n    Proxy implementation specifically for proxying `Database` objects.\n    \"\"\"\n    __slots__ = ('obj', '_callbacks', '_Model')\n\n    def connection_context(self):\n        return ConnectionContext(self)\n    def atomic(self, *args, **kwargs):\n        return _atomic(self, *args, **kwargs)\n    def manual_commit(self):\n        return _manual(self)\n    def transaction(self, *args, **kwargs):\n        return _transaction(self, *args, **kwargs)\n    def savepoint(self):\n        return _savepoint(self)\n    @property\n    def Model(self):\n        if not hasattr(self, '_Model'):\n            class Meta: database = self\n            self._Model = type('BaseModel', (Model,), {'Meta': Meta})\n        return self._Model\n\n\nclass ModelDescriptor(object): pass\n\n\n# SQL Generation.\n\n\nclass AliasManager(object):\n    __slots__ = ('_counter', '_current_index', '_mapping')\n\n    def __init__(self):\n        # A list of dictionaries containing mappings at various depths.\n        self._counter = 0\n        self._current_index = 0\n        self._mapping = []\n        self.push()\n\n    @property\n    def mapping(self):\n        return self._mapping[self._current_index - 1]\n\n    def add(self, source):\n        if source not in self.mapping:\n            self._counter += 1\n            self[source] = 't%d' % self._counter\n        return self.mapping[source]\n\n    def get(self, source, any_depth=False):\n        if any_depth:\n            for idx in reversed(range(self._current_index)):\n                if source in self._mapping[idx]:\n                    return self._mapping[idx][source]\n        return self.add(source)\n\n    def __getitem__(self, source):\n        return self.get(source)\n\n    def __setitem__(self, source, alias):\n        self.mapping[source] = alias\n\n    def push(self):\n        self._current_index += 1\n        if self._current_index > len(self._mapping):\n            self._mapping.append({})\n\n    def pop(self):\n        if self._current_index == 1:\n            raise ValueError('Cannot pop() from empty alias manager.')\n        self._current_index -= 1\n\n\nclass State(collections.namedtuple('_State', ('scope', 'parentheses',\n                                              'settings'))):\n    def __new__(cls, scope=SCOPE_NORMAL, parentheses=False, **kwargs):\n        return super(State, cls).__new__(cls, scope, parentheses, kwargs)\n\n    def __call__(self, scope=None, parentheses=None, **kwargs):\n        # Scope and settings are \"inherited\" (parentheses is not, however).\n        scope = self.scope if scope is None else scope\n\n        # Try to avoid unnecessary dict copying.\n        if kwargs and self.settings:\n            settings = self.settings.copy()  # Copy original settings dict.\n            settings.update(kwargs)  # Update copy with overrides.\n        elif kwargs:\n            settings = kwargs\n        else:\n            settings = self.settings\n        return State(scope, parentheses, **settings)\n\n    def __getattr__(self, attr_name):\n        return self.settings.get(attr_name)\n\n\ndef __scope_context__(scope):\n    @contextmanager\n    def inner(self, **kwargs):\n        with self(scope=scope, **kwargs):\n            yield self\n    return inner\n\n\nclass Context(object):\n    __slots__ = ('stack', '_sql', '_values', 'alias_manager', 'state')\n\n    def __init__(self, **settings):\n        self.stack = []\n        self._sql = []\n        self._values = []\n        self.alias_manager = AliasManager()\n        self.state = State(**settings)\n\n    def as_new(self):\n        return Context(**self.state.settings)\n\n    def column_sort_key(self, item):\n        return item[0].get_sort_key(self)\n\n    @property\n    def scope(self):\n        return self.state.scope\n\n    @property\n    def parentheses(self):\n        return self.state.parentheses\n\n    @property\n    def subquery(self):\n        return self.state.subquery\n\n    def __call__(self, **overrides):\n        if overrides and overrides.get('scope') == self.scope:\n            del overrides['scope']\n\n        self.stack.append(self.state)\n        self.state = self.state(**overrides)\n        return self\n\n    scope_normal = __scope_context__(SCOPE_NORMAL)\n    scope_source = __scope_context__(SCOPE_SOURCE)\n    scope_values = __scope_context__(SCOPE_VALUES)\n    scope_cte = __scope_context__(SCOPE_CTE)\n    scope_column = __scope_context__(SCOPE_COLUMN)\n\n    def __enter__(self):\n        if self.parentheses:\n            self.literal('(')\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.parentheses:\n            self.literal(')')\n        self.state = self.stack.pop()\n\n    @contextmanager\n    def push_alias(self):\n        self.alias_manager.push()\n        yield\n        self.alias_manager.pop()\n\n    def sql(self, obj):\n        if isinstance(obj, (Node, Context)):\n            return obj.__sql__(self)\n        elif is_model(obj):\n            return obj._meta.table.__sql__(self)\n        else:\n            return self.sql(Value(obj))\n\n    def literal(self, keyword):\n        self._sql.append(keyword)\n        return self\n\n    def value(self, value, converter=None, add_param=True):\n        if converter:\n            value = converter(value)\n        elif converter is None and self.state.converter:\n            # Explicitly check for None so that \"False\" can be used to signify\n            # that no conversion should be applied.\n            value = self.state.converter(value)\n\n        if isinstance(value, Node):\n            with self(converter=None):\n                return self.sql(value)\n        elif is_model(value):\n            # Under certain circumstances, we could end-up treating a model-\n            # class itself as a value. This check ensures that we drop the\n            # table alias into the query instead of trying to parameterize a\n            # model (for instance, passing a model as a function argument).\n            with self.scope_column():\n                return self.sql(value)\n\n        if self.state.value_literals:\n            return self.literal(_query_val_transform(value))\n\n        self._values.append(value)\n        return self.literal(self.state.param or '?') if add_param else self\n\n    def __sql__(self, ctx):\n        ctx._sql.extend(self._sql)\n        ctx._values.extend(self._values)\n        return ctx\n\n    def parse(self, node):\n        return self.sql(node).query()\n\n    def query(self):\n        return ''.join(self._sql), self._values\n\n\ndef query_to_string(query):\n    # NOTE: this function is not exported by default as it might be misused --\n    # and this misuse could lead to sql injection vulnerabilities. This\n    # function is intended for debugging or logging purposes ONLY.\n    db = getattr(query, '_database', None)\n    if db is not None:\n        ctx = db.get_sql_context()\n    else:\n        ctx = Context()\n\n    sql, params = ctx.sql(query).query()\n    if not params:\n        return sql\n\n    param = ctx.state.param or '?'\n    if param == '?':\n        sql = sql.replace('?', '%s')\n\n    return sql % tuple(map(_query_val_transform, params))\n\ndef _query_val_transform(v):\n    # Interpolate parameters.\n    if isinstance(v, (text_type, datetime.datetime, datetime.date,\n                      datetime.time)):\n        v = \"'%s'\" % v\n    elif isinstance(v, bytes_type):\n        try:\n            v = v.decode('utf8')\n        except UnicodeDecodeError:\n            v = v.decode('raw_unicode_escape')\n        v = \"'%s'\" % v\n    elif isinstance(v, int):\n        v = '%s' % int(v)  # Also handles booleans -> 1 or 0.\n    elif v is None:\n        v = 'NULL'\n    else:\n        v = str(v)\n    return v\n\n\n# AST.\n\n\nclass Node(object):\n    _coerce = True\n    __isabstractmethod__ = False  # Avoid issue w/abc and __getattr__, eg fn.X\n\n    def clone(self):\n        obj = self.__class__.__new__(self.__class__)\n        obj.__dict__ = self.__dict__.copy()\n        return obj\n\n    def __sql__(self, ctx):\n        raise NotImplementedError\n\n    @staticmethod\n    def copy(method):\n        def inner(self, *args, **kwargs):\n            clone = self.clone()\n            method(clone, *args, **kwargs)\n            return clone\n        return inner\n\n    def coerce(self, _coerce=True):\n        if _coerce != self._coerce:\n            clone = self.clone()\n            clone._coerce = _coerce\n            return clone\n        return self\n\n    def is_alias(self):\n        return False\n\n    def unwrap(self):\n        return self\n\n\nclass ColumnFactory(object):\n    __slots__ = ('node',)\n\n    def __init__(self, node):\n        self.node = node\n\n    def __getattr__(self, attr):\n        return Column(self.node, attr)\n    __getitem__ = __getattr__\n\n\nclass _DynamicColumn(object):\n    __slots__ = ()\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return ColumnFactory(instance)  # Implements __getattr__().\n        return self\n\n\nclass _ExplicitColumn(object):\n    __slots__ = ()\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            raise AttributeError(\n                '%s specifies columns explicitly, and does not support '\n                'dynamic column lookups.' % instance)\n        return self\n\n\nclass Star(Node):\n    def __init__(self, source):\n        self.source = source\n    def __sql__(self, ctx):\n        return ctx.sql(QualifiedNames(self.source)).literal('.*')\n\n\nclass Source(Node):\n    c = _DynamicColumn()\n\n    def __init__(self, alias=None):\n        super(Source, self).__init__()\n        self._alias = alias\n\n    @Node.copy\n    def alias(self, name):\n        self._alias = name\n\n    def select(self, *columns):\n        if not columns:\n            columns = (SQL('*'),)\n        return Select((self,), columns)\n\n    @property\n    def __star__(self):\n        return Star(self)\n\n    def join(self, dest, join_type=JOIN.INNER, on=None):\n        return Join(self, dest, join_type, on)\n\n    def left_outer_join(self, dest, on=None):\n        return Join(self, dest, JOIN.LEFT_OUTER, on)\n\n    def cte(self, name, recursive=False, columns=None, materialized=None):\n        return CTE(name, self, recursive=recursive, columns=columns,\n                   materialized=materialized)\n\n    def get_sort_key(self, ctx):\n        if self._alias:\n            return (self._alias,)\n        return (ctx.alias_manager[self],)\n\n    def apply_alias(self, ctx):\n        # If we are defining the source, include the \"AS alias\" declaration. An\n        # alias is created for the source if one is not already defined.\n        if ctx.scope == SCOPE_SOURCE:\n            if self._alias:\n                ctx.alias_manager[self] = self._alias\n            ctx.literal(' AS ').sql(Entity(ctx.alias_manager[self]))\n        return ctx\n\n    def apply_column(self, ctx):\n        if self._alias:\n            ctx.alias_manager[self] = self._alias\n        return ctx.sql(Entity(ctx.alias_manager[self]))\n\n\nclass _HashableSource(object):\n    def __init__(self, *args, **kwargs):\n        super(_HashableSource, self).__init__(*args, **kwargs)\n        self._update_hash()\n\n    @Node.copy\n    def alias(self, name):\n        self._alias = name\n        self._update_hash()\n\n    def _update_hash(self):\n        self._hash = self._get_hash()\n\n    def _get_hash(self):\n        return hash((self.__class__, self._path, self._alias))\n\n    def __hash__(self):\n        return self._hash\n\n    def __eq__(self, other):\n        if isinstance(other, _HashableSource):\n            return self._hash == other._hash\n        return Expression(self, OP.EQ, other)\n\n    def __ne__(self, other):\n        if isinstance(other, _HashableSource):\n            return self._hash != other._hash\n        return Expression(self, OP.NE, other)\n\n    def _e(op):\n        def inner(self, rhs):\n            return Expression(self, op, rhs)\n        return inner\n    __lt__ = _e(OP.LT)\n    __le__ = _e(OP.LTE)\n    __gt__ = _e(OP.GT)\n    __ge__ = _e(OP.GTE)\n\n\ndef __bind_database__(meth):\n    @wraps(meth)\n    def inner(self, *args, **kwargs):\n        result = meth(self, *args, **kwargs)\n        if self._database:\n            return result.bind(self._database)\n        return result\n    return inner\n\n\ndef __join__(join_type=JOIN.INNER, inverted=False):\n    def method(self, other):\n        if inverted:\n            self, other = other, self\n        return Join(self, other, join_type=join_type)\n    return method\n\n\nclass BaseTable(Source):\n    __and__ = __join__(JOIN.INNER)\n    __add__ = __join__(JOIN.LEFT_OUTER)\n    __sub__ = __join__(JOIN.RIGHT_OUTER)\n    __or__ = __join__(JOIN.FULL_OUTER)\n    __mul__ = __join__(JOIN.CROSS)\n    __rand__ = __join__(JOIN.INNER, inverted=True)\n    __radd__ = __join__(JOIN.LEFT_OUTER, inverted=True)\n    __rsub__ = __join__(JOIN.RIGHT_OUTER, inverted=True)\n    __ror__ = __join__(JOIN.FULL_OUTER, inverted=True)\n    __rmul__ = __join__(JOIN.CROSS, inverted=True)\n\n\nclass _BoundTableContext(object):\n    def __init__(self, table, database):\n        self.table = table\n        self.database = database\n\n    def __call__(self, fn):\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            with _BoundTableContext(self.table, self.database):\n                return fn(*args, **kwargs)\n        return inner\n\n    def __enter__(self):\n        self._orig_database = self.table._database\n        self.table.bind(self.database)\n        if self.table._model is not None:\n            self.table._model.bind(self.database)\n        return self.table\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.table.bind(self._orig_database)\n        if self.table._model is not None:\n            self.table._model.bind(self._orig_database)\n\n\nclass Table(_HashableSource, BaseTable):\n    def __init__(self, name, columns=None, primary_key=None, schema=None,\n                 alias=None, _model=None, _database=None):\n        self.__name__ = name\n        self._columns = columns\n        self._primary_key = primary_key\n        self._schema = schema\n        self._path = (schema, name) if schema else (name,)\n        self._model = _model\n        self._database = _database\n        super(Table, self).__init__(alias=alias)\n\n        # Allow tables to restrict what columns are available.\n        if columns is not None:\n            self.c = _ExplicitColumn()\n            for column in columns:\n                setattr(self, column, Column(self, column))\n\n        if primary_key:\n            col_src = self if self._columns else self.c\n            self.primary_key = getattr(col_src, primary_key)\n        else:\n            self.primary_key = None\n\n    def clone(self):\n        # Ensure a deep copy of the column instances.\n        return Table(\n            self.__name__,\n            columns=self._columns,\n            primary_key=self._primary_key,\n            schema=self._schema,\n            alias=self._alias,\n            _model=self._model,\n            _database=self._database)\n\n    def bind(self, database=None):\n        self._database = database\n        return self\n\n    def bind_ctx(self, database=None):\n        return _BoundTableContext(self, database)\n\n    def _get_hash(self):\n        return hash((self.__class__, self._path, self._alias, self._model))\n\n    @__bind_database__\n    def select(self, *columns):\n        if not columns and self._columns:\n            columns = [Column(self, column) for column in self._columns]\n        return Select((self,), columns)\n\n    @__bind_database__\n    def insert(self, insert=None, columns=None, **kwargs):\n        if kwargs:\n            insert = {} if insert is None else insert\n            src = self if self._columns else self.c\n            for key, value in kwargs.items():\n                insert[getattr(src, key)] = value\n        return Insert(self, insert=insert, columns=columns)\n\n    @__bind_database__\n    def replace(self, insert=None, columns=None, **kwargs):\n        return (self\n                .insert(insert=insert, columns=columns)\n                .on_conflict('REPLACE'))\n\n    @__bind_database__\n    def update(self, update=None, **kwargs):\n        if kwargs:\n            update = {} if update is None else update\n            for key, value in kwargs.items():\n                src = self if self._columns else self.c\n                update[getattr(src, key)] = value\n        return Update(self, update=update)\n\n    @__bind_database__\n    def delete(self):\n        return Delete(self)\n\n    def __sql__(self, ctx):\n        if ctx.scope == SCOPE_VALUES:\n            # Return the quoted table name.\n            return ctx.sql(Entity(*self._path))\n\n        if self._alias:\n            ctx.alias_manager[self] = self._alias\n\n        if ctx.scope == SCOPE_SOURCE:\n            # Define the table and its alias.\n            return self.apply_alias(ctx.sql(Entity(*self._path)))\n        else:\n            # Refer to the table using the alias.\n            return self.apply_column(ctx)\n\n\nclass Join(BaseTable):\n    def __init__(self, lhs, rhs, join_type=JOIN.INNER, on=None, alias=None):\n        super(Join, self).__init__(alias=alias)\n        self.lhs = lhs\n        self.rhs = rhs\n        self.join_type = join_type\n        self._on = on\n\n    def on(self, predicate):\n        self._on = predicate\n        return self\n\n    def __sql__(self, ctx):\n        (ctx\n         .sql(self.lhs)\n         .literal(' %s ' % self.join_type)\n         .sql(self.rhs))\n        if self._on is not None:\n            ctx.literal(' ON ').sql(self._on)\n        return ctx\n\n\nclass ValuesList(_HashableSource, BaseTable):\n    def __init__(self, values, columns=None, alias=None):\n        self._values = values\n        self._columns = columns\n        super(ValuesList, self).__init__(alias=alias)\n\n    def _get_hash(self):\n        return hash((self.__class__, id(self._values), self._alias))\n\n    @Node.copy\n    def columns(self, *names):\n        self._columns = names\n\n    def __sql__(self, ctx):\n        if self._alias:\n            ctx.alias_manager[self] = self._alias\n\n        if ctx.scope == SCOPE_SOURCE or ctx.scope == SCOPE_NORMAL:\n            with ctx(parentheses=not ctx.parentheses):\n                ctx = (ctx\n                       .literal('VALUES ')\n                       .sql(CommaNodeList([\n                           EnclosedNodeList(row) for row in self._values])))\n\n            if ctx.scope == SCOPE_SOURCE:\n                ctx.literal(' AS ').sql(Entity(ctx.alias_manager[self]))\n                if self._columns:\n                    entities = [Entity(c) for c in self._columns]\n                    ctx.sql(EnclosedNodeList(entities))\n        else:\n            ctx.sql(Entity(ctx.alias_manager[self]))\n\n        return ctx\n\n\nclass CTE(_HashableSource, Source):\n    def __init__(self, name, query, recursive=False, columns=None,\n                 materialized=None):\n        self._alias = name\n        self._query = query\n        self._recursive = recursive\n        self._materialized = materialized\n        if columns is not None:\n            columns = [Entity(c) if isinstance(c, basestring) else c\n                       for c in columns]\n        self._columns = columns\n        query._cte_list = ()\n        super(CTE, self).__init__(alias=name)\n\n    def select_from(self, *columns):\n        if not columns:\n            raise ValueError('select_from() must specify one or more columns '\n                             'from the CTE to select.')\n\n        query = (Select((self,), columns)\n                 .with_cte(self)\n                 .bind(self._query._database))\n        try:\n            query = query.objects(self._query.model)\n        except AttributeError:\n            pass\n        return query\n\n    def _get_hash(self):\n        return hash((self.__class__, self._alias, id(self._query)))\n\n    def union_all(self, rhs):\n        clone = self._query.clone()\n        return CTE(self._alias, clone + rhs, self._recursive, self._columns)\n    __add__ = union_all\n\n    def union(self, rhs):\n        clone = self._query.clone()\n        return CTE(self._alias, clone | rhs, self._recursive, self._columns)\n    __or__ = union\n\n    def __sql__(self, ctx):\n        if ctx.scope != SCOPE_CTE:\n            return ctx.sql(Entity(self._alias))\n\n        with ctx.push_alias():\n            ctx.alias_manager[self] = self._alias\n            ctx.sql(Entity(self._alias))\n\n            if self._columns:\n                ctx.literal(' ').sql(EnclosedNodeList(self._columns))\n            ctx.literal(' AS ')\n\n            if self._materialized:\n                ctx.literal('MATERIALIZED ')\n            elif self._materialized is False:\n                ctx.literal('NOT MATERIALIZED ')\n\n            with ctx.scope_normal(parentheses=True):\n                ctx.sql(self._query)\n        return ctx\n\n\nclass ColumnBase(Node):\n    _converter = None\n\n    @Node.copy\n    def converter(self, converter=None):\n        self._converter = converter\n\n    def alias(self, alias):\n        if alias:\n            return Alias(self, alias)\n        return self\n\n    def unalias(self):\n        return self\n\n    def bind_to(self, dest):\n        return BindTo(self, dest)\n\n    def cast(self, as_type):\n        return Cast(self, as_type)\n\n    def asc(self, collation=None, nulls=None):\n        return Asc(self, collation=collation, nulls=nulls)\n    __pos__ = asc\n\n    def desc(self, collation=None, nulls=None):\n        return Desc(self, collation=collation, nulls=nulls)\n    __neg__ = desc\n\n    def __invert__(self):\n        return Negated(self)\n\n    def _e(op, inv=False):\n        \"\"\"\n        Lightweight factory which returns a method that builds an Expression\n        consisting of the left-hand and right-hand operands, using `op`.\n        \"\"\"\n        def inner(self, rhs):\n            if inv:\n                return Expression(rhs, op, self)\n            return Expression(self, op, rhs)\n        return inner\n    __and__ = _e(OP.AND)\n    __or__ = _e(OP.OR)\n\n    __add__ = _e(OP.ADD)\n    __sub__ = _e(OP.SUB)\n    __mul__ = _e(OP.MUL)\n    __div__ = __truediv__ = _e(OP.DIV)\n    __xor__ = _e(OP.XOR)\n    __radd__ = _e(OP.ADD, inv=True)\n    __rsub__ = _e(OP.SUB, inv=True)\n    __rmul__ = _e(OP.MUL, inv=True)\n    __rdiv__ = __rtruediv__ = _e(OP.DIV, inv=True)\n    __rand__ = _e(OP.AND, inv=True)\n    __ror__ = _e(OP.OR, inv=True)\n    __rxor__ = _e(OP.XOR, inv=True)\n\n    def __eq__(self, rhs):\n        op = OP.IS if rhs is None else OP.EQ\n        return Expression(self, op, rhs)\n    def __ne__(self, rhs):\n        op = OP.IS_NOT if rhs is None else OP.NE\n        return Expression(self, op, rhs)\n\n    __lt__ = _e(OP.LT)\n    __le__ = _e(OP.LTE)\n    __gt__ = _e(OP.GT)\n    __ge__ = _e(OP.GTE)\n    __lshift__ = _e(OP.IN)\n    __rshift__ = _e(OP.IS)\n    __mod__ = _e(OP.LIKE)\n    __pow__ = _e(OP.ILIKE)\n\n    like = _e(OP.LIKE)\n    ilike = _e(OP.ILIKE)\n\n    bin_and = _e(OP.BIN_AND)\n    bin_or = _e(OP.BIN_OR)\n    in_ = _e(OP.IN)\n    not_in = _e(OP.NOT_IN)\n    regexp = _e(OP.REGEXP)\n    iregexp = _e(OP.IREGEXP)\n\n    # Special expressions.\n    def is_null(self, is_null=True):\n        op = OP.IS if is_null else OP.IS_NOT\n        return Expression(self, op, None)\n\n    def _escape_like_expr(self, s, template):\n        if s.find('_') >= 0 or s.find('%') >= 0 or s.find('\\\\') >= 0:\n            s = s.replace('\\\\', '\\\\\\\\').replace('_', '\\\\_').replace('%', '\\\\%')\n            # Pass the expression and escape string as unconverted values, to\n            # avoid (e.g.) a Json field converter turning the escaped LIKE\n            # pattern into a Json-quoted string.\n            return NodeList((\n                Value(template % s, converter=False),\n                SQL('ESCAPE'),\n                Value('\\\\', converter=False)))\n        return template % s\n    def contains(self, rhs):\n        if isinstance(rhs, Node):\n            rhs = Expression('%', OP.CONCAT,\n                             Expression(rhs, OP.CONCAT, '%'))\n        else:\n            rhs = self._escape_like_expr(rhs, '%%%s%%')\n        return Expression(self, OP.ILIKE, rhs)\n    def startswith(self, rhs):\n        if isinstance(rhs, Node):\n            rhs = Expression(rhs, OP.CONCAT, '%')\n        else:\n            rhs = self._escape_like_expr(rhs, '%s%%')\n        return Expression(self, OP.ILIKE, rhs)\n    def endswith(self, rhs):\n        if isinstance(rhs, Node):\n            rhs = Expression('%', OP.CONCAT, rhs)\n        else:\n            rhs = self._escape_like_expr(rhs, '%%%s')\n        return Expression(self, OP.ILIKE, rhs)\n    def between(self, lo, hi):\n        return Expression(self, OP.BETWEEN, NodeList((lo, SQL('AND'), hi)))\n    def concat(self, rhs):\n        return StringExpression(self, OP.CONCAT, rhs)\n    def __getitem__(self, item):\n        if isinstance(item, slice):\n            if item.start is None or item.stop is None:\n                raise ValueError('BETWEEN range must have both a start- and '\n                                 'end-point.')\n            return self.between(item.start, item.stop)\n        return self == item\n    __iter__ = None  # Prevent infinite loop.\n\n    def distinct(self):\n        return NodeList((SQL('DISTINCT'), self))\n\n    def collate(self, collation):\n        return NodeList((self, SQL('COLLATE %s' % collation)))\n\n    def get_sort_key(self, ctx):\n        return ()\n\n\nclass Column(ColumnBase):\n    def __init__(self, source, name):\n        self.source = source\n        self.name = name\n\n    def get_sort_key(self, ctx):\n        if ctx.scope == SCOPE_VALUES:\n            return (self.name,)\n        else:\n            return self.source.get_sort_key(ctx) + (self.name,)\n\n    def __hash__(self):\n        return hash((self.source, self.name))\n\n    def __sql__(self, ctx):\n        if ctx.scope == SCOPE_VALUES:\n            return ctx.sql(Entity(self.name))\n        else:\n            with ctx.scope_column():\n                return ctx.sql(self.source).literal('.').sql(Entity(self.name))\n\n\nclass WrappedNode(ColumnBase):\n    def __init__(self, node):\n        self.node = node\n        self._coerce = getattr(node, '_coerce', True)\n        self._converter = getattr(node, '_converter', None)\n\n    def is_alias(self):\n        return self.node.is_alias()\n\n    def unwrap(self):\n        return self.node.unwrap()\n\n\nclass EntityFactory(object):\n    __slots__ = ('node',)\n    def __init__(self, node):\n        self.node = node\n    def __getattr__(self, attr):\n        return Entity(self.node, attr)\n\n\nclass _DynamicEntity(object):\n    __slots__ = ()\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return EntityFactory(instance._alias)  # Implements __getattr__().\n        return self\n\n\nclass Alias(WrappedNode):\n    c = _DynamicEntity()\n\n    def __init__(self, node, alias):\n        super(Alias, self).__init__(node)\n        self._alias = alias\n\n    def __hash__(self):\n        return hash(self._alias)\n\n    @property\n    def name(self):\n        return self._alias\n    @name.setter\n    def name(self, value):\n        self._alias = value\n\n    def alias(self, alias=None):\n        if alias is None:\n            return self.node\n        else:\n            return Alias(self.node, alias)\n\n    def unalias(self):\n        return self.node\n\n    def is_alias(self):\n        return True\n\n    def __sql__(self, ctx):\n        if ctx.scope == SCOPE_SOURCE:\n            return (ctx\n                    .sql(self.node)\n                    .literal(' AS ')\n                    .sql(Entity(self._alias)))\n        else:\n            return ctx.sql(Entity(self._alias))\n\n\nclass BindTo(WrappedNode):\n    def __init__(self, node, dest):\n        super(BindTo, self).__init__(node)\n        self.dest = dest\n\n    def __sql__(self, ctx):\n        return ctx.sql(self.node)\n\n\nclass Negated(WrappedNode):\n    def __invert__(self):\n        return self.node\n\n    def __sql__(self, ctx):\n        return ctx.literal('NOT ').sql(self.node)\n\n\nclass BitwiseMixin(object):\n    def __and__(self, other):\n        return self.bin_and(other)\n\n    def __or__(self, other):\n        return self.bin_or(other)\n\n    def __sub__(self, other):\n        return self.bin_and(other.bin_negated())\n\n    def __invert__(self):\n        return BitwiseNegated(self)\n\n\nclass BitwiseNegated(BitwiseMixin, WrappedNode):\n    def __invert__(self):\n        return self.node\n\n    def __sql__(self, ctx):\n        if ctx.state.operations:\n            op_sql = ctx.state.operations.get(self.op, self.op)\n        else:\n            op_sql = self.op\n        return ctx.literal(op_sql).sql(self.node)\n\n\nclass Value(ColumnBase):\n    def __init__(self, value, converter=None, unpack=True):\n        self.value = value\n        self.converter = converter\n        self.multi = unpack and isinstance(self.value, multi_types)\n        if self.multi:\n            self.values = []\n            for item in self.value:\n                if isinstance(item, Node):\n                    self.values.append(item)\n                else:\n                    self.values.append(Value(item, self.converter))\n\n    def __sql__(self, ctx):\n        if self.multi:\n            # For multi-part values (e.g. lists of IDs).\n            return ctx.sql(EnclosedNodeList(self.values))\n\n        return ctx.value(self.value, self.converter)\n\n\nclass ValueLiterals(WrappedNode):\n    def __sql__(self, ctx):\n        with ctx(value_literals=True):\n            return ctx.sql(self.node)\n\n\ndef AsIs(value):\n    return Value(value, unpack=False)\n\n\nclass Cast(WrappedNode):\n    def __init__(self, node, cast):\n        super(Cast, self).__init__(node)\n        self._cast = cast\n        self._coerce = False\n\n    def __sql__(self, ctx):\n        return (ctx\n                .literal('CAST(')\n                .sql(self.node)\n                .literal(' AS %s)' % self._cast))\n\n\nclass Ordering(WrappedNode):\n    def __init__(self, node, direction, collation=None, nulls=None):\n        super(Ordering, self).__init__(node)\n        self.direction = direction\n        self.collation = collation\n        self.nulls = nulls\n        if nulls and nulls.lower() not in ('first', 'last'):\n            raise ValueError('Ordering nulls= parameter must be \"first\" or '\n                             '\"last\", got: %s' % nulls)\n\n    def collate(self, collation=None):\n        return Ordering(self.node, self.direction, collation)\n\n    def _null_ordering_case(self, nulls):\n        if nulls.lower() == 'last':\n            ifnull, notnull = 1, 0\n        elif nulls.lower() == 'first':\n            ifnull, notnull = 0, 1\n        else:\n            raise ValueError('unsupported value for nulls= ordering.')\n        return Case(None, ((self.node.is_null(), ifnull),), notnull)\n\n    def __sql__(self, ctx):\n        if self.nulls and not ctx.state.nulls_ordering:\n            ctx.sql(self._null_ordering_case(self.nulls)).literal(', ')\n\n        ctx.sql(self.node).literal(' %s' % self.direction)\n        if self.collation:\n            ctx.literal(' COLLATE %s' % self.collation)\n        if self.nulls and ctx.state.nulls_ordering:\n            ctx.literal(' NULLS %s' % self.nulls)\n        return ctx\n\n\ndef Asc(node, collation=None, nulls=None):\n    return Ordering(node, 'ASC', collation, nulls)\n\n\ndef Desc(node, collation=None, nulls=None):\n    return Ordering(node, 'DESC', collation, nulls)\n\n\nclass Expression(ColumnBase):\n    def __init__(self, lhs, op, rhs, flat=False):\n        self.lhs = lhs\n        self.op = op\n        self.rhs = rhs\n        self.flat = flat\n\n    def __sql__(self, ctx):\n        overrides = {'parentheses': not self.flat, 'in_expr': True}\n\n        # First attempt to unwrap the node on the left-hand-side, so that we\n        # can get at the underlying Field if one is present.\n        node = raw_node = self.lhs\n        if isinstance(raw_node, WrappedNode):\n            node = raw_node.unwrap()\n\n        # Set up the appropriate converter if we have a field on the left side.\n        if isinstance(node, Field) and raw_node._coerce:\n            overrides['converter'] = node.db_value\n            overrides['is_fk_expr'] = isinstance(node, ForeignKeyField)\n        else:\n            overrides['converter'] = None\n\n        if ctx.state.operations:\n            op_sql = ctx.state.operations.get(self.op, self.op)\n        else:\n            op_sql = self.op\n\n        with ctx(**overrides):\n            # Postgresql reports an error for IN/NOT IN (), so convert to\n            # the equivalent boolean expression.\n            op_in = self.op == OP.IN or self.op == OP.NOT_IN\n            if op_in and ctx.as_new().parse(self.rhs)[0] == '()':\n                return ctx.literal('0 = 1' if self.op == OP.IN else '1 = 1')\n            rhs = self.rhs\n            if rhs is None and (self.op == OP.IS or self.op == OP.IS_NOT):\n                rhs = SQL('NULL')\n\n            return (ctx\n                    .sql(self.lhs)\n                    .literal(' %s ' % op_sql)\n                    .sql(rhs))\n\n\nclass StringExpression(Expression):\n    def __add__(self, rhs):\n        return self.concat(rhs)\n    def __radd__(self, lhs):\n        return StringExpression(lhs, OP.CONCAT, self)\n\n\nclass Entity(ColumnBase):\n    def __init__(self, *path):\n        self._path = [part.replace('\"', '\"\"') for part in path if part]\n\n    def __getattr__(self, attr):\n        return Entity(*self._path + [attr])\n\n    def get_sort_key(self, ctx):\n        return tuple(self._path)\n\n    def __hash__(self):\n        return hash((self.__class__.__name__, tuple(self._path)))\n\n    def __sql__(self, ctx):\n        return ctx.literal(quote(self._path, ctx.state.quote or '\"\"'))\n\n\nclass SQL(ColumnBase):\n    def __init__(self, sql, params=None):\n        self.sql = sql\n        self.params = params\n\n    def __sql__(self, ctx):\n        ctx.literal(self.sql)\n        if self.params:\n            for param in self.params:\n                ctx.value(param, False, add_param=False)\n        return ctx\n\n\ndef Check(constraint, name=None):\n    check = SQL('CHECK (%s)' % constraint)\n    if not name:\n        return check\n    return NodeList((SQL('CONSTRAINT'), Entity(name), check))\n\n\nclass Function(ColumnBase):\n    no_coerce_functions = set(('sum', 'count', 'avg', 'cast', 'array_agg'))\n\n    def __init__(self, name, arguments, coerce=True, python_value=None):\n        self.name = name\n        self.arguments = arguments\n        self._filter = None\n        self._order_by = None\n        self._python_value = python_value\n        if name and name.lower() in self.no_coerce_functions:\n            self._coerce = False\n        else:\n            self._coerce = coerce\n\n    def __getattr__(self, attr):\n        def decorator(*args, **kwargs):\n            return Function(attr, args, **kwargs)\n        return decorator\n\n    @Node.copy\n    def filter(self, where=None):\n        self._filter = where\n\n    @Node.copy\n    def order_by(self, *ordering):\n        self._order_by = ordering\n\n    @Node.copy\n    def python_value(self, func=None):\n        self._python_value = func\n\n    def over(self, partition_by=None, order_by=None, start=None, end=None,\n             frame_type=None, window=None, exclude=None):\n        if isinstance(partition_by, Window) and window is None:\n            window = partition_by\n\n        if window is not None:\n            node = WindowAlias(window)\n        else:\n            node = Window(partition_by=partition_by, order_by=order_by,\n                          start=start, end=end, frame_type=frame_type,\n                          exclude=exclude, _inline=True)\n        return NodeList((self, SQL('OVER'), node))\n\n    def __sql__(self, ctx):\n        ctx.literal(self.name)\n        if not len(self.arguments):\n            ctx.literal('()')\n        else:\n            args = self.arguments\n\n            # If this is an ordered aggregate, then we will modify the last\n            # argument to append the ORDER BY ... clause. We do this to avoid\n            # double-wrapping any expression args in parentheses, as NodeList\n            # has a special check (hack) in place to work around this.\n            if self._order_by:\n                args = list(args)\n                args[-1] = NodeList((args[-1], SQL('ORDER BY'),\n                                     CommaNodeList(self._order_by)))\n\n            with ctx(in_function=True, function_arg_count=len(self.arguments)):\n                ctx.sql(EnclosedNodeList([\n                    (arg if isinstance(arg, Node) else Value(arg, False))\n                    for arg in args]))\n\n        if self._filter:\n            ctx.literal(' FILTER (WHERE ').sql(self._filter).literal(')')\n        return ctx\n\n\nfn = Function(None, None)\n\n\nclass Window(Node):\n    # Frame start/end and frame exclusion.\n    CURRENT_ROW = SQL('CURRENT ROW')\n    GROUP = SQL('GROUP')\n    TIES = SQL('TIES')\n    NO_OTHERS = SQL('NO OTHERS')\n\n    # Frame types.\n    GROUPS = 'GROUPS'\n    RANGE = 'RANGE'\n    ROWS = 'ROWS'\n\n    def __init__(self, partition_by=None, order_by=None, start=None, end=None,\n                 frame_type=None, extends=None, exclude=None, alias=None,\n                 _inline=False):\n        super(Window, self).__init__()\n        if start is not None and not isinstance(start, SQL):\n            start = SQL(start)\n        if end is not None and not isinstance(end, SQL):\n            end = SQL(end)\n\n        self.partition_by = ensure_tuple(partition_by)\n        self.order_by = ensure_tuple(order_by)\n        self.start = start\n        self.end = end\n        if self.start is None and self.end is not None:\n            raise ValueError('Cannot specify WINDOW end without start.')\n        self._alias = alias or 'w'\n        self._inline = _inline\n        self.frame_type = frame_type\n        self._extends = extends\n        self._exclude = exclude\n\n    def alias(self, alias=None):\n        self._alias = alias or 'w'\n        return self\n\n    @Node.copy\n    def as_range(self):\n        self.frame_type = Window.RANGE\n\n    @Node.copy\n    def as_rows(self):\n        self.frame_type = Window.ROWS\n\n    @Node.copy\n    def as_groups(self):\n        self.frame_type = Window.GROUPS\n\n    @Node.copy\n    def extends(self, window=None):\n        self._extends = window\n\n    @Node.copy\n    def exclude(self, frame_exclusion=None):\n        if isinstance(frame_exclusion, basestring):\n            frame_exclusion = SQL(frame_exclusion)\n        self._exclude = frame_exclusion\n\n    @staticmethod\n    def following(value=None):\n        if value is None:\n            return SQL('UNBOUNDED FOLLOWING')\n        return SQL('%d FOLLOWING' % value)\n\n    @staticmethod\n    def preceding(value=None):\n        if value is None:\n            return SQL('UNBOUNDED PRECEDING')\n        return SQL('%d PRECEDING' % value)\n\n    def __sql__(self, ctx):\n        if ctx.scope != SCOPE_SOURCE and not self._inline:\n            ctx.literal(self._alias)\n            ctx.literal(' AS ')\n\n        with ctx(parentheses=True):\n            parts = []\n            if self._extends is not None:\n                ext = self._extends\n                if isinstance(ext, Window):\n                    ext = SQL(ext._alias)\n                elif isinstance(ext, basestring):\n                    ext = SQL(ext)\n                parts.append(ext)\n            if self.partition_by:\n                parts.extend((\n                    SQL('PARTITION BY'),\n                    CommaNodeList(self.partition_by)))\n            if self.order_by:\n                parts.extend((\n                    SQL('ORDER BY'),\n                    CommaNodeList(self.order_by)))\n            if self.start is not None and self.end is not None:\n                frame = self.frame_type or 'ROWS'\n                parts.extend((\n                    SQL('%s BETWEEN' % frame),\n                    self.start,\n                    SQL('AND'),\n                    self.end))\n            elif self.start is not None:\n                parts.extend((SQL(self.frame_type or 'ROWS'), self.start))\n            elif self.frame_type is not None:\n                parts.append(SQL('%s UNBOUNDED PRECEDING' % self.frame_type))\n            if self._exclude is not None:\n                parts.extend((SQL('EXCLUDE'), self._exclude))\n            ctx.sql(NodeList(parts))\n        return ctx\n\n\nclass WindowAlias(Node):\n    def __init__(self, window):\n        self.window = window\n\n    def alias(self, window_alias):\n        self.window._alias = window_alias\n        return self\n\n    def __sql__(self, ctx):\n        return ctx.literal(self.window._alias or 'w')\n\n\nclass _InFunction(Node):\n    def __init__(self, node, in_function=True):\n        self.node = node\n        self.in_function = in_function\n\n    def __sql__(self, ctx):\n        with ctx(in_function=self.in_function):\n            return ctx.sql(self.node)\n\n\nclass Case(ColumnBase):\n    def __init__(self, predicate, expression_tuples, default=None):\n        self.predicate = predicate\n        self.expression_tuples = expression_tuples\n        self.default = default\n\n    def __sql__(self, ctx):\n        clauses = [SQL('CASE')]\n        if self.predicate is not None:\n            clauses.append(self.predicate)\n        for expr, value in self.expression_tuples:\n            clauses.extend((SQL('WHEN'), expr,\n                            SQL('THEN'), _InFunction(value)))\n        if self.default is not None:\n            clauses.extend((SQL('ELSE'), _InFunction(self.default)))\n        clauses.append(SQL('END'))\n        with ctx(in_function=False):\n            return ctx.sql(NodeList(clauses))\n\n\nclass ForUpdate(Node):\n    def __init__(self, expr, of=None, nowait=None):\n        expr = 'FOR UPDATE' if expr is True else expr\n        if expr.lower().endswith('nowait'):\n            expr = expr[:-7]  # Strip off the \"nowait\" bit.\n            nowait = True\n\n        self._expr = expr\n        if of is not None and not isinstance(of, (list, set, tuple)):\n            of = (of,)\n        self._of = of\n        self._nowait = nowait\n\n    def __sql__(self, ctx):\n        ctx.literal(self._expr)\n        if self._of is not None:\n            ctx.literal(' OF ').sql(CommaNodeList(self._of))\n        if self._nowait:\n            ctx.literal(' NOWAIT')\n        return ctx\n\n\nclass NodeList(ColumnBase):\n    def __init__(self, nodes, glue=' ', parens=False):\n        self.nodes = nodes\n        self.glue = glue\n        self.parens = parens\n        if parens and len(self.nodes) == 1 and \\\n           isinstance(self.nodes[0], Expression) and \\\n           not self.nodes[0].flat:\n            # Hack to avoid double-parentheses.\n            self.nodes = (self.nodes[0].clone(),)\n            self.nodes[0].flat = True\n\n    def __sql__(self, ctx):\n        n_nodes = len(self.nodes)\n        if n_nodes == 0:\n            return ctx.literal('()') if self.parens else ctx\n        with ctx(parentheses=self.parens):\n            for i in range(n_nodes - 1):\n                ctx.sql(self.nodes[i])\n                ctx.literal(self.glue)\n            ctx.sql(self.nodes[n_nodes - 1])\n        return ctx\n\n\ndef CommaNodeList(nodes):\n    return NodeList(nodes, ', ')\n\n\ndef EnclosedNodeList(nodes):\n    return NodeList(nodes, ', ', True)\n\n\nclass _Namespace(Node):\n    __slots__ = ('_name',)\n    def __init__(self, name):\n        self._name = name\n    def __getattr__(self, attr):\n        return NamespaceAttribute(self, attr)\n    __getitem__ = __getattr__\n\nclass NamespaceAttribute(ColumnBase):\n    def __init__(self, namespace, attribute):\n        self._namespace = namespace\n        self._attribute = attribute\n\n    def __sql__(self, ctx):\n        return (ctx\n                .literal(self._namespace._name + '.')\n                .sql(Entity(self._attribute)))\n\nEXCLUDED = _Namespace('EXCLUDED')\n\n\nclass DQ(ColumnBase):\n    def __init__(self, **query):\n        super(DQ, self).__init__()\n        self.query = query\n        self._negated = False\n\n    @Node.copy\n    def __invert__(self):\n        self._negated = not self._negated\n\n    def clone(self):\n        node = DQ(**self.query)\n        node._negated = self._negated\n        return node\n\n#: Represent a row tuple.\nTuple = lambda *a: EnclosedNodeList(a)\n\n\nclass QualifiedNames(WrappedNode):\n    def __sql__(self, ctx):\n        with ctx.scope_column():\n            return ctx.sql(self.node)\n\n\ndef qualify_names(node):\n    # Search a node heirarchy to ensure that any column-like objects are\n    # referenced using fully-qualified names.\n    if isinstance(node, Expression):\n        return node.__class__(qualify_names(node.lhs), node.op,\n                              qualify_names(node.rhs), node.flat)\n    elif isinstance(node, ColumnBase):\n        return QualifiedNames(node)\n    return node\n\n\nclass OnConflict(Node):\n    def __init__(self, action=None, update=None, preserve=None, where=None,\n                 conflict_target=None, conflict_where=None,\n                 conflict_constraint=None):\n        self._action = action\n        self._update = update\n        self._preserve = ensure_tuple(preserve)\n        self._where = where\n        if conflict_target is not None and conflict_constraint is not None:\n            raise ValueError('only one of \"conflict_target\" and '\n                             '\"conflict_constraint\" may be specified.')\n        self._conflict_target = ensure_tuple(conflict_target)\n        self._conflict_where = conflict_where\n        self._conflict_constraint = conflict_constraint\n\n    def get_conflict_statement(self, ctx, query):\n        return ctx.state.conflict_statement(self, query)\n\n    def get_conflict_update(self, ctx, query):\n        return ctx.state.conflict_update(self, query)\n\n    @Node.copy\n    def preserve(self, *columns):\n        self._preserve = columns\n\n    @Node.copy\n    def update(self, _data=None, **kwargs):\n        if _data and kwargs and not isinstance(_data, dict):\n            raise ValueError('Cannot mix data with keyword arguments in the '\n                             'OnConflict update method.')\n        _data = _data or {}\n        if kwargs:\n            _data.update(kwargs)\n        self._update = _data\n\n    @Node.copy\n    def where(self, *expressions):\n        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n\n    @Node.copy\n    def conflict_target(self, *constraints):\n        self._conflict_constraint = None\n        self._conflict_target = constraints\n\n    @Node.copy\n    def conflict_where(self, *expressions):\n        if self._conflict_where is not None:\n            expressions = (self._conflict_where,) + expressions\n        self._conflict_where = reduce(operator.and_, expressions)\n\n    @Node.copy\n    def conflict_constraint(self, constraint):\n        self._conflict_constraint = constraint\n        self._conflict_target = None\n\n\ndef database_required(method):\n    @wraps(method)\n    def inner(self, database=None, *args, **kwargs):\n        database = self._database if database is None else database\n        if not database:\n            raise InterfaceError('Query must be bound to a database in order '\n                                 'to call \"%s\".' % method.__name__)\n        return method(self, database, *args, **kwargs)\n    return inner\n\n# BASE QUERY INTERFACE.\n\nclass BaseQuery(Node):\n    default_row_type = ROW.DICT\n\n    def __init__(self, _database=None, **kwargs):\n        self._database = _database\n        self._cursor_wrapper = None\n        self._row_type = None\n        self._constructor = None\n        super(BaseQuery, self).__init__(**kwargs)\n\n    def bind(self, database=None):\n        self._database = database\n        return self\n\n    def clone(self):\n        query = super(BaseQuery, self).clone()\n        query._cursor_wrapper = None\n        return query\n\n    @Node.copy\n    def dicts(self, as_dict=True):\n        self._row_type = ROW.DICT if as_dict else None\n        return self\n\n    @Node.copy\n    def tuples(self, as_tuple=True):\n        self._row_type = ROW.TUPLE if as_tuple else None\n        return self\n\n    @Node.copy\n    def namedtuples(self, as_namedtuple=True):\n        self._row_type = ROW.NAMED_TUPLE if as_namedtuple else None\n        return self\n\n    @Node.copy\n    def objects(self, constructor=None):\n        self._row_type = ROW.CONSTRUCTOR if constructor else None\n        self._constructor = constructor\n        return self\n\n    def _get_cursor_wrapper(self, cursor):\n        row_type = self._row_type or self.default_row_type\n\n        if row_type == ROW.DICT:\n            return DictCursorWrapper(cursor)\n        elif row_type == ROW.TUPLE:\n            return CursorWrapper(cursor)\n        elif row_type == ROW.NAMED_TUPLE:\n            return NamedTupleCursorWrapper(cursor)\n        elif row_type == ROW.CONSTRUCTOR:\n            return ObjectCursorWrapper(cursor, self._constructor)\n        else:\n            raise ValueError('Unrecognized row type: \"%s\".' % row_type)\n\n    def __sql__(self, ctx):\n        raise NotImplementedError\n\n    def sql(self):\n        if self._database:\n            context = self._database.get_sql_context()\n        else:\n            context = Context()\n        return context.parse(self)\n\n    @database_required\n    def execute(self, database):\n        return self._execute(database)\n\n    def _execute(self, database):\n        raise NotImplementedError\n\n    def iterator(self, database=None):\n        return iter(self.execute(database).iterator())\n\n    def _ensure_execution(self):\n        if self._cursor_wrapper is None:\n            if not self._database:\n                raise ValueError('Query has not been executed.')\n            self.execute()\n\n    def __iter__(self):\n        self._ensure_execution()\n        return iter(self._cursor_wrapper)\n\n    def __getitem__(self, value):\n        self._ensure_execution()\n        if isinstance(value, slice):\n            index = value.stop\n        else:\n            index = value\n        if index is not None:\n            index = index + 1 if index >= 0 else 0\n        self._cursor_wrapper.fill_cache(index)\n        return self._cursor_wrapper.row_cache[value]\n\n    def __len__(self):\n        self._ensure_execution()\n        return len(self._cursor_wrapper)\n\n    def __str__(self):\n        return query_to_string(self)\n\n\nclass RawQuery(BaseQuery):\n    def __init__(self, sql=None, params=None, **kwargs):\n        super(RawQuery, self).__init__(**kwargs)\n        self._sql = sql\n        self._params = params\n\n    def __sql__(self, ctx):\n        ctx.literal(self._sql)\n        if self._params:\n            for param in self._params:\n                ctx.value(param, add_param=False)\n        return ctx\n\n    def _execute(self, database):\n        if self._cursor_wrapper is None:\n            cursor = database.execute(self)\n            self._cursor_wrapper = self._get_cursor_wrapper(cursor)\n        return self._cursor_wrapper\n\n\nclass Query(BaseQuery):\n    def __init__(self, where=None, order_by=None, limit=None, offset=None,\n                 **kwargs):\n        super(Query, self).__init__(**kwargs)\n        self._where = where\n        self._order_by = order_by\n        self._limit = limit\n        self._offset = offset\n\n        self._cte_list = None\n\n    @Node.copy\n    def with_cte(self, *cte_list):\n        self._cte_list = cte_list\n\n    @Node.copy\n    def where(self, *expressions):\n        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n\n    @Node.copy\n    def orwhere(self, *expressions):\n        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.or_, expressions)\n\n    @Node.copy\n    def order_by(self, *values):\n        self._order_by = values\n\n    @Node.copy\n    def order_by_extend(self, *values):\n        self._order_by = ((self._order_by or ()) + values) or None\n\n    @Node.copy\n    def limit(self, value=None):\n        self._limit = value\n\n    @Node.copy\n    def offset(self, value=None):\n        self._offset = value\n\n    @Node.copy\n    def paginate(self, page, paginate_by=20):\n        if page > 0:\n            page -= 1\n        self._limit = paginate_by\n        self._offset = page * paginate_by\n\n    def _apply_ordering(self, ctx):\n        if self._order_by:\n            (ctx\n             .literal(' ORDER BY ')\n             .sql(CommaNodeList(self._order_by)))\n        if self._limit is not None or (self._offset is not None and\n                                       ctx.state.limit_max):\n            limit = ctx.state.limit_max if self._limit is None else self._limit\n            ctx.literal(' LIMIT ').sql(limit)\n        if self._offset is not None:\n            ctx.literal(' OFFSET ').sql(self._offset)\n        return ctx\n\n    def __sql__(self, ctx):\n        if self._cte_list:\n            # The CTE scope is only used at the very beginning of the query,\n            # when we are describing the various CTEs we will be using.\n            recursive = any(cte._recursive for cte in self._cte_list)\n\n            # Explicitly disable the \"subquery\" flag here, so as to avoid\n            # unnecessary parentheses around subsequent selects.\n            with ctx.scope_cte(subquery=False):\n                (ctx\n                 .literal('WITH RECURSIVE ' if recursive else 'WITH ')\n                 .sql(CommaNodeList(self._cte_list))\n                 .literal(' '))\n        return ctx\n\n\ndef __compound_select__(operation, inverted=False):\n    @__bind_database__\n    def method(self, other):\n        if inverted:\n            self, other = other, self\n        return CompoundSelectQuery(self, operation, other)\n    return method\n\n\nclass SelectQuery(Query):\n    union_all = __add__ = __compound_select__('UNION ALL')\n    union = __or__ = __compound_select__('UNION')\n    intersect = __and__ = __compound_select__('INTERSECT')\n    except_ = __sub__ = __compound_select__('EXCEPT')\n    __radd__ = __compound_select__('UNION ALL', inverted=True)\n    __ror__ = __compound_select__('UNION', inverted=True)\n    __rand__ = __compound_select__('INTERSECT', inverted=True)\n    __rsub__ = __compound_select__('EXCEPT', inverted=True)\n\n    def select_from(self, *columns):\n        if not columns:\n            raise ValueError('select_from() must specify one or more columns.')\n\n        query = (Select((self,), columns)\n                 .bind(self._database))\n        if getattr(self, 'model', None) is not None:\n            # Bind to the sub-select's model type, if defined.\n            query = query.objects(self.model)\n        return query\n\n\nclass SelectBase(_HashableSource, Source, SelectQuery):\n    def _get_hash(self):\n        return hash((self.__class__, self._alias or id(self)))\n\n    def _execute(self, database):\n        if self._cursor_wrapper is None:\n            cursor = database.execute(self)\n            self._cursor_wrapper = self._get_cursor_wrapper(cursor)\n        return self._cursor_wrapper\n\n    @database_required\n    def peek(self, database, n=1):\n        rows = self.execute(database)[:n]\n        if rows:\n            return rows[0] if n == 1 else rows\n\n    @database_required\n    def first(self, database, n=1):\n        if self._limit != n:\n            self._limit = n\n            self._cursor_wrapper = None\n        return self.peek(database, n=n)\n\n    @database_required\n    def scalar(self, database, as_tuple=False, as_dict=False):\n        if as_dict:\n            return self.dicts().peek(database)\n        row = self.tuples().peek(database)\n        return row[0] if row and not as_tuple else row\n\n    @database_required\n    def scalars(self, database):\n        for row in self.tuples().execute(database):\n            yield row[0]\n\n    @database_required\n    def count(self, database, clear_limit=False):\n        clone = self.order_by().alias('_wrapped')\n        if clear_limit:\n            clone._limit = clone._offset = None\n        try:\n            if clone._having is None and clone._group_by is None and \\\n               clone._windows is None and clone._distinct is None and \\\n               clone._simple_distinct is not True:\n                clone = clone.select(SQL('1'))\n        except AttributeError:\n            pass\n        return Select([clone], [fn.COUNT(SQL('1'))]).scalar(database)\n\n    @database_required\n    def exists(self, database):\n        clone = self.columns(SQL('1'))\n        clone._limit = 1\n        clone._offset = None\n        return bool(clone.scalar())\n\n    @database_required\n    def get(self, database):\n        self._cursor_wrapper = None\n        try:\n            return self.execute(database)[0]\n        except IndexError:\n            pass\n\n\n# QUERY IMPLEMENTATIONS.\n\n\nclass CompoundSelectQuery(SelectBase):\n    def __init__(self, lhs, op, rhs):\n        super(CompoundSelectQuery, self).__init__()\n        self.lhs = lhs\n        self.op = op\n        self.rhs = rhs\n\n    @property\n    def _returning(self):\n        return self.lhs._returning\n\n    @database_required\n    def exists(self, database):\n        query = Select((self.limit(1),), (SQL('1'),)).bind(database)\n        return bool(query.scalar())\n\n    def _get_query_key(self):\n        return (self.lhs.get_query_key(), self.rhs.get_query_key())\n\n    def _wrap_parens(self, ctx, subq):\n        csq_setting = ctx.state.compound_select_parentheses\n\n        if not csq_setting or csq_setting == CSQ_PARENTHESES_NEVER:\n            return False\n        elif csq_setting == CSQ_PARENTHESES_ALWAYS:\n            return True\n        elif csq_setting == CSQ_PARENTHESES_UNNESTED:\n            if ctx.state.in_expr or ctx.state.in_function:\n                # If this compound select query is being used inside an\n                # expression, e.g., an IN or EXISTS().\n                return False\n\n            # If the query on the left or right is itself a compound select\n            # query, then we do not apply parentheses. However, if it is a\n            # regular SELECT query, we will apply parentheses.\n            return not isinstance(subq, CompoundSelectQuery)\n\n    def __sql__(self, ctx):\n        if ctx.scope == SCOPE_COLUMN:\n            return self.apply_column(ctx)\n\n        # Call parent method to handle any CTEs.\n        super(CompoundSelectQuery, self).__sql__(ctx)\n\n        outer_parens = ctx.subquery or (ctx.scope == SCOPE_SOURCE)\n        with ctx(parentheses=outer_parens):\n            # Should the left-hand query be wrapped in parentheses?\n            lhs_parens = self._wrap_parens(ctx, self.lhs)\n            with ctx.scope_normal(parentheses=lhs_parens, subquery=False):\n                ctx.sql(self.lhs)\n            ctx.literal(' %s ' % self.op)\n            with ctx.push_alias():\n                # Should the right-hand query be wrapped in parentheses?\n                rhs_parens = self._wrap_parens(ctx, self.rhs)\n                with ctx.scope_normal(parentheses=rhs_parens, subquery=False):\n                    ctx.sql(self.rhs)\n\n            # Apply ORDER BY, LIMIT, OFFSET. We use the \"values\" scope so that\n            # entity names are not fully-qualified. This is a bit of a hack, as\n            # we're relying on the logic in Column.__sql__() to not fully\n            # qualify column names.\n            with ctx.scope_values():\n                self._apply_ordering(ctx)\n\n        return self.apply_alias(ctx)\n\n\nclass Select(SelectBase):\n    def __init__(self, from_list=None, columns=None, group_by=None,\n                 having=None, distinct=None, windows=None, for_update=None,\n                 for_update_of=None, nowait=None, lateral=None, **kwargs):\n        super(Select, self).__init__(**kwargs)\n        self._from_list = (list(from_list) if isinstance(from_list, tuple)\n                           else from_list) or []\n        self._returning = columns\n        self._group_by = group_by\n        self._having = having\n        self._windows = None\n        self._for_update = for_update  # XXX: consider reorganizing.\n        self._for_update_of = for_update_of\n        self._for_update_nowait = nowait\n        self._lateral = lateral\n\n        self._distinct = self._simple_distinct = None\n        if distinct:\n            if isinstance(distinct, bool):\n                self._simple_distinct = distinct\n            else:\n                self._distinct = distinct\n\n        self._cursor_wrapper = None\n\n    def clone(self):\n        clone = super(Select, self).clone()\n        if clone._from_list:\n            clone._from_list = list(clone._from_list)\n        return clone\n\n    @Node.copy\n    def columns(self, *columns, **kwargs):\n        self._returning = columns\n    select = columns\n\n    @Node.copy\n    def select_extend(self, *columns):\n        self._returning = tuple(self._returning) + columns\n\n    @property\n    def selected_columns(self):\n        return self._returning\n    @selected_columns.setter\n    def selected_columns(self, value):\n        self._returning = value\n\n    @Node.copy\n    def from_(self, *sources):\n        self._from_list = list(sources)\n\n    @Node.copy\n    def join(self, dest, join_type=JOIN.INNER, on=None):\n        if not self._from_list:\n            raise ValueError('No sources to join on.')\n        item = self._from_list.pop()\n        self._from_list.append(Join(item, dest, join_type, on))\n\n    def left_outer_join(self, dest, on=None):\n        return self.join(dest, JOIN.LEFT_OUTER, on)\n\n    @Node.copy\n    def group_by(self, *columns):\n        grouping = []\n        for column in columns:\n            if isinstance(column, Table):\n                if not column._columns:\n                    raise ValueError('Cannot pass a table to group_by() that '\n                                     'does not have columns explicitly '\n                                     'declared.')\n                grouping.extend([getattr(column, col_name)\n                                 for col_name in column._columns])\n            else:\n                grouping.append(column)\n        self._group_by = grouping\n\n    def group_by_extend(self, *values):\n        \"\"\"@Node.copy used from group_by() call\"\"\"\n        group_by = tuple(self._group_by or ()) + values\n        return self.group_by(*group_by)\n\n    @Node.copy\n    def having(self, *expressions):\n        if self._having is not None:\n            expressions = (self._having,) + expressions\n        self._having = reduce(operator.and_, expressions)\n\n    @Node.copy\n    def distinct(self, *columns):\n        if len(columns) == 1 and (columns[0] is True or columns[0] is False):\n            self._simple_distinct = columns[0]\n        else:\n            self._simple_distinct = False\n            self._distinct = columns\n\n    @Node.copy\n    def window(self, *windows):\n        self._windows = windows if windows else None\n\n    @Node.copy\n    def for_update(self, for_update=True, of=None, nowait=None):\n        if not for_update and (of is not None or nowait):\n            for_update = True\n        self._for_update = for_update\n        self._for_update_of = of\n        self._for_update_nowait = nowait\n\n    @Node.copy\n    def lateral(self, lateral=True):\n        self._lateral = lateral\n\n    def _get_query_key(self):\n        return self._alias\n\n    def __sql_selection__(self, ctx, is_subquery=False):\n        return ctx.sql(CommaNodeList(self._returning))\n\n    def __sql__(self, ctx):\n        if ctx.scope == SCOPE_COLUMN:\n            return self.apply_column(ctx)\n\n        if self._lateral and ctx.scope == SCOPE_SOURCE:\n            ctx.literal('LATERAL ')\n\n        is_subquery = ctx.subquery\n        state = {\n            'converter': None,\n            'in_function': False,\n            'parentheses': is_subquery or (ctx.scope == SCOPE_SOURCE),\n            'subquery': True,\n        }\n        if ctx.state.in_function and ctx.state.function_arg_count == 1:\n            state['parentheses'] = False\n\n        with ctx.scope_normal(**state):\n            # Defer calling parent SQL until here. This ensures that any CTEs\n            # for this query will be properly nested if this query is a\n            # sub-select or is used in an expression. See GH#1809 for example.\n            super(Select, self).__sql__(ctx)\n\n            ctx.literal('SELECT ')\n            if self._simple_distinct or self._distinct is not None:\n                ctx.literal('DISTINCT ')\n                if self._distinct:\n                    (ctx\n                     .literal('ON ')\n                     .sql(EnclosedNodeList(self._distinct))\n                     .literal(' '))\n\n            with ctx.scope_source():\n                ctx = self.__sql_selection__(ctx, is_subquery)\n\n            if self._from_list:\n                with ctx.scope_source(parentheses=False):\n                    ctx.literal(' FROM ').sql(CommaNodeList(self._from_list))\n\n            if self._where is not None:\n                ctx.literal(' WHERE ').sql(self._where)\n\n            if self._group_by:\n                ctx.literal(' GROUP BY ').sql(CommaNodeList(self._group_by))\n\n            if self._having is not None:\n                ctx.literal(' HAVING ').sql(self._having)\n\n            if self._windows is not None:\n                ctx.literal(' WINDOW ')\n                ctx.sql(CommaNodeList(self._windows))\n\n            # Apply ORDER BY, LIMIT, OFFSET.\n            self._apply_ordering(ctx)\n\n            if self._for_update:\n                if not ctx.state.for_update:\n                    raise ValueError('FOR UPDATE specified but not supported '\n                                     'by database.')\n                ctx.literal(' ')\n                ctx.sql(ForUpdate(self._for_update, self._for_update_of,\n                                  self._for_update_nowait))\n\n        # If the subquery is inside a function -or- we are evaluating a\n        # subquery on either side of an expression w/o an explicit alias, do\n        # not generate an alias + AS clause.\n        if ctx.state.in_function or (ctx.state.in_expr and\n                                     self._alias is None):\n            return ctx\n\n        return self.apply_alias(ctx)\n\n\nclass _WriteQuery(Query):\n    def __init__(self, table, returning=None, **kwargs):\n        self.table = table\n        self._returning = returning\n        self._return_cursor = True if returning else False\n        super(_WriteQuery, self).__init__(**kwargs)\n\n    def cte(self, name, recursive=False, columns=None, materialized=None):\n        return CTE(name, self, recursive=recursive, columns=columns,\n                   materialized=materialized)\n\n    @Node.copy\n    def returning(self, *returning):\n        self._returning = returning\n        self._return_cursor = True if returning else False\n\n    def apply_returning(self, ctx):\n        if self._returning:\n            with ctx.scope_source():\n                ctx.literal(' RETURNING ').sql(CommaNodeList(self._returning))\n        return ctx\n\n    def _execute(self, database):\n        if self._returning:\n            cursor = self.execute_returning(database)\n        else:\n            cursor = database.execute(self)\n        return self.handle_result(database, cursor)\n\n    def execute_returning(self, database):\n        if self._cursor_wrapper is None:\n            cursor = database.execute(self)\n            self._cursor_wrapper = self._get_cursor_wrapper(cursor)\n        return self._cursor_wrapper\n\n    def handle_result(self, database, cursor):\n        if self._return_cursor:\n            return cursor\n        return database.rows_affected(cursor)\n\n    def _set_table_alias(self, ctx):\n        ctx.alias_manager[self.table] = self.table.__name__\n\n    def __sql__(self, ctx):\n        super(_WriteQuery, self).__sql__(ctx)\n        # We explicitly set the table alias to the table's name, which ensures\n        # that if a sub-select references a column on the outer table, we won't\n        # assign it a new alias (e.g. t2) but will refer to it as table.column.\n        self._set_table_alias(ctx)\n        return ctx\n\n\nclass Update(_WriteQuery):\n    def __init__(self, table, update=None, **kwargs):\n        super(Update, self).__init__(table, **kwargs)\n        self._update = update\n        self._from = None\n\n    @Node.copy\n    def from_(self, *sources):\n        self._from = sources\n\n    def __sql__(self, ctx):\n        super(Update, self).__sql__(ctx)\n\n        with ctx.scope_values(subquery=True):\n            ctx.literal('UPDATE ')\n\n            expressions = []\n            for k, v in sorted(self._update.items(), key=ctx.column_sort_key):\n                if not isinstance(v, Node):\n                    if isinstance(k, Field):\n                        v = k.to_value(v)\n                    else:\n                        v = Value(v, unpack=False)\n                elif isinstance(v, Model) and isinstance(k, ForeignKeyField):\n                    # NB: we want to ensure that when passed a model instance\n                    # in the context of a foreign-key, we apply the fk-specific\n                    # adaptation of the model.\n                    v = k.to_value(v)\n\n                if not isinstance(v, Value):\n                    v = qualify_names(v)\n\n                expressions.append(NodeList((k, SQL('='), v)))\n\n            (ctx\n             .sql(self.table)\n             .literal(' SET ')\n             .sql(CommaNodeList(expressions)))\n\n            if self._from:\n                with ctx.scope_source(parentheses=False):\n                    ctx.literal(' FROM ').sql(CommaNodeList(self._from))\n\n            if self._where:\n                with ctx.scope_normal():\n                    ctx.literal(' WHERE ').sql(self._where)\n            self._apply_ordering(ctx)\n            return self.apply_returning(ctx)\n\n\nclass Insert(_WriteQuery):\n    SIMPLE = 0\n    QUERY = 1\n    MULTI = 2\n    class DefaultValuesException(Exception): pass\n\n    def __init__(self, table, insert=None, columns=None, on_conflict=None,\n                 **kwargs):\n        super(Insert, self).__init__(table, **kwargs)\n        self._insert = insert\n        self._columns = columns\n        self._on_conflict = on_conflict\n        self._query_type = None\n        self._as_rowcount = False\n\n    def where(self, *expressions):\n        raise NotImplementedError('INSERT queries cannot have a WHERE clause.')\n\n    @Node.copy\n    def as_rowcount(self, _as_rowcount=True):\n        self._as_rowcount = _as_rowcount\n\n    @Node.copy\n    def on_conflict_ignore(self, ignore=True):\n        self._on_conflict = OnConflict('IGNORE') if ignore else None\n\n    @Node.copy\n    def on_conflict_replace(self, replace=True):\n        self._on_conflict = OnConflict('REPLACE') if replace else None\n\n    @Node.copy\n    def on_conflict(self, *args, **kwargs):\n        self._on_conflict = (OnConflict(*args, **kwargs) if (args or kwargs)\n                             else None)\n\n    def _simple_insert(self, ctx):\n        if not self._insert:\n            raise self.DefaultValuesException('Error: no data to insert.')\n        return self._generate_insert((self._insert,), ctx)\n\n    def get_default_data(self):\n        return {}\n\n    def get_default_columns(self):\n        if self.table._columns:\n            return [getattr(self.table, col) for col in self.table._columns\n                    if col != self.table._primary_key]\n\n    def _generate_insert(self, insert, ctx):\n        rows_iter = iter(insert)\n        columns = self._columns\n\n        # Load and organize column defaults (if provided).\n        defaults = self.get_default_data()\n\n        # First figure out what columns are being inserted (if they weren't\n        # specified explicitly). Resulting columns are normalized and ordered.\n        if not columns:\n            try:\n                row = next(rows_iter)\n            except StopIteration:\n                raise self.DefaultValuesException('Error: no rows to insert.')\n\n            if not isinstance(row, Mapping):\n                columns = self.get_default_columns()\n                if columns is None:\n                    raise ValueError('Bulk insert must specify columns.')\n            else:\n                # Infer column names from the dict of data being inserted.\n                accum = []\n                for column in row:\n                    if isinstance(column, basestring):\n                        column = getattr(self.table, column)\n                    accum.append(column)\n\n                # Add any columns present in the default data that are not\n                # accounted for by the dictionary of row data.\n                column_set = set(accum)\n                for col in (set(defaults) - column_set):\n                    accum.append(col)\n\n                columns = sorted(accum, key=lambda obj: obj.get_sort_key(ctx))\n            rows_iter = itertools.chain(iter((row,)), rows_iter)\n        else:\n            clean_columns = []\n            seen = set()\n            for column in columns:\n                if isinstance(column, basestring):\n                    column_obj = getattr(self.table, column)\n                else:\n                    column_obj = column\n                clean_columns.append(column_obj)\n                seen.add(column_obj)\n\n            columns = clean_columns\n            for col in sorted(defaults, key=lambda obj: obj.get_sort_key(ctx)):\n                if col not in seen:\n                    columns.append(col)\n\n        fk_fields = set()\n        nullable_columns = set()\n        value_lookups = {}\n        for column in columns:\n            lookups = [column, column.name]\n            if isinstance(column, Field):\n                if column.name != column.column_name:\n                    lookups.append(column.column_name)\n                if column.null:\n                    nullable_columns.add(column)\n                if isinstance(column, ForeignKeyField):\n                    fk_fields.add(column)\n            value_lookups[column] = lookups\n\n        ctx.sql(EnclosedNodeList(columns)).literal(' VALUES ')\n        columns_converters = [\n            (column, column.db_value if isinstance(column, Field) else None)\n            for column in columns]\n\n        all_values = []\n        for row in rows_iter:\n            values = []\n            is_dict = isinstance(row, Mapping)\n            for i, (column, converter) in enumerate(columns_converters):\n                try:\n                    if is_dict:\n                        # The logic is a bit convoluted, but in order to be\n                        # flexible in what we accept (dict keyed by\n                        # column/field, field name, or underlying column name),\n                        # we try accessing the row data dict using each\n                        # possible key. If no match is found, throw an error.\n                        for lookup in value_lookups[column]:\n                            try:\n                                val = row[lookup]\n                            except KeyError: pass\n                            else: break\n                        else:\n                            raise KeyError\n                    else:\n                        val = row[i]\n                except (KeyError, IndexError):\n                    if column in defaults:\n                        val = defaults[column]\n                        if callable_(val):\n                            val = val()\n                    elif column in nullable_columns:\n                        val = None\n                    else:\n                        raise ValueError('Missing value for %s.' % column.name)\n\n                if not isinstance(val, Node) or (isinstance(val, Model) and\n                                                 column in fk_fields):\n                    val = Value(val, converter=converter, unpack=False)\n                values.append(val)\n\n            all_values.append(EnclosedNodeList(values))\n\n        if not all_values:\n            raise self.DefaultValuesException('Error: no data to insert.')\n\n        with ctx.scope_values(subquery=True):\n            return ctx.sql(CommaNodeList(all_values))\n\n    def _query_insert(self, ctx):\n        return (ctx\n                .sql(EnclosedNodeList(self._columns))\n                .literal(' ')\n                .sql(self._insert))\n\n    def _default_values(self, ctx):\n        if not self._database:\n            return ctx.literal('DEFAULT VALUES')\n        return self._database.default_values_insert(ctx)\n\n    def __sql__(self, ctx):\n        super(Insert, self).__sql__(ctx)\n        with ctx.scope_values():\n            stmt = None\n            if self._on_conflict is not None:\n                stmt = self._on_conflict.get_conflict_statement(ctx, self)\n\n            (ctx\n             .sql(stmt or SQL('INSERT'))\n             .literal(' INTO ')\n             .sql(self.table)\n             .literal(' '))\n\n            if isinstance(self._insert, Mapping) and not self._columns:\n                try:\n                    self._simple_insert(ctx)\n                except self.DefaultValuesException:\n                    self._default_values(ctx)\n                self._query_type = Insert.SIMPLE\n            elif isinstance(self._insert, (SelectQuery, SQL)):\n                self._query_insert(ctx)\n                self._query_type = Insert.QUERY\n            else:\n                self._generate_insert(self._insert, ctx)\n                self._query_type = Insert.MULTI\n\n            if self._on_conflict is not None:\n                update = self._on_conflict.get_conflict_update(ctx, self)\n                if update is not None:\n                    ctx.literal(' ').sql(update)\n\n            return self.apply_returning(ctx)\n\n    def _execute(self, database):\n        if self._returning is None and database.returning_clause \\\n           and self.table._primary_key:\n            self._returning = (self.table._primary_key,)\n        try:\n            return super(Insert, self)._execute(database)\n        except self.DefaultValuesException:\n            pass\n\n    def handle_result(self, database, cursor):\n        if self._return_cursor:\n            return cursor\n        if self._as_rowcount:\n            return database.rows_affected(cursor)\n        return database.last_insert_id(cursor, self._query_type)\n\n\nclass Delete(_WriteQuery):\n    def __sql__(self, ctx):\n        super(Delete, self).__sql__(ctx)\n\n        with ctx.scope_values(subquery=True):\n            ctx.literal('DELETE FROM ').sql(self.table)\n            if self._where is not None:\n                with ctx.scope_normal():\n                    ctx.literal(' WHERE ').sql(self._where)\n\n            self._apply_ordering(ctx)\n            return self.apply_returning(ctx)\n\n\nclass Index(Node):\n    def __init__(self, name, table, expressions, unique=False, safe=False,\n                 where=None, using=None):\n        self._name = name\n        self._table = Entity(table) if not isinstance(table, Table) else table\n        self._expressions = expressions\n        self._where = where\n        self._unique = unique\n        self._safe = safe\n        self._using = using\n\n    @Node.copy\n    def safe(self, _safe=True):\n        self._safe = _safe\n\n    @Node.copy\n    def where(self, *expressions):\n        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n\n    @Node.copy\n    def using(self, _using=None):\n        self._using = _using\n\n    def __sql__(self, ctx):\n        statement = 'CREATE UNIQUE INDEX ' if self._unique else 'CREATE INDEX '\n        with ctx.scope_values(subquery=True):\n            ctx.literal(statement)\n            if self._safe:\n                ctx.literal('IF NOT EXISTS ')\n\n            # Sqlite uses CREATE INDEX <schema>.<name> ON <table>, whereas most\n            # others use: CREATE INDEX <name> ON <schema>.<table>.\n            if ctx.state.index_schema_prefix and \\\n               isinstance(self._table, Table) and self._table._schema:\n                index_name = Entity(self._table._schema, self._name)\n                table_name = Entity(self._table.__name__)\n            else:\n                index_name = Entity(self._name)\n                table_name = self._table\n\n            ctx.sql(index_name)\n            if self._using is not None and \\\n               ctx.state.index_using_precedes_table:\n                ctx.literal(' USING %s' % self._using)  # MySQL style.\n\n            (ctx\n             .literal(' ON ')\n             .sql(table_name)\n             .literal(' '))\n\n            if self._using is not None and not \\\n               ctx.state.index_using_precedes_table:\n                ctx.literal('USING %s ' % self._using)  # Postgres/default.\n\n            ctx.sql(EnclosedNodeList([\n                SQL(expr) if isinstance(expr, basestring) else expr\n                for expr in self._expressions]))\n            if self._where is not None:\n                ctx.literal(' WHERE ').sql(self._where)\n\n        return ctx\n\n\nclass ModelIndex(Index):\n    def __init__(self, model, fields, unique=False, safe=True, where=None,\n                 using=None, name=None):\n        self._model = model\n        if name is None:\n            name = self._generate_name_from_fields(model, fields)\n        if using is None:\n            for field in fields:\n                if isinstance(field, Field) and hasattr(field, 'index_type'):\n                    using = field.index_type\n        super(ModelIndex, self).__init__(\n            name=name,\n            table=model._meta.table,\n            expressions=fields,\n            unique=unique,\n            safe=safe,\n            where=where,\n            using=using)\n\n    def _generate_name_from_fields(self, model, fields):\n        accum = []\n        for field in fields:\n            if isinstance(field, basestring):\n                accum.append(field.split()[0])\n            else:\n                if isinstance(field, Node) and not isinstance(field, Field):\n                    field = field.unwrap()\n                if isinstance(field, Field):\n                    accum.append(field.column_name)\n\n        if not accum:\n            raise ValueError('Unable to generate a name for the index, please '\n                             'explicitly specify a name.')\n\n        clean_field_names = re.sub(r'[^\\w]+', '', '_'.join(accum))\n        meta = model._meta\n        prefix = meta.name if meta.legacy_table_names else meta.table_name\n        return _truncate_constraint_name('_'.join((prefix, clean_field_names)))\n\n\ndef _truncate_constraint_name(constraint, maxlen=64):\n    if len(constraint) > maxlen:\n        name_hash = hashlib.md5(constraint.encode('utf-8')).hexdigest()\n        constraint = '%s_%s' % (constraint[:(maxlen - 8)], name_hash[:7])\n    return constraint\n\n\n# DB-API 2.0 EXCEPTIONS.\n\n\nclass PeeweeException(Exception):\n    def __init__(self, *args):\n        if args and isinstance(args[0], Exception):\n            self.orig, args = args[0], args[1:]\n        super(PeeweeException, self).__init__(*args)\nclass ImproperlyConfigured(PeeweeException): pass\nclass DatabaseError(PeeweeException): pass\nclass DataError(DatabaseError): pass\nclass IntegrityError(DatabaseError): pass\nclass InterfaceError(PeeweeException): pass\nclass InternalError(DatabaseError): pass\nclass NotSupportedError(DatabaseError): pass\nclass OperationalError(DatabaseError): pass\nclass ProgrammingError(DatabaseError): pass\n\n\nclass ExceptionWrapper(object):\n    __slots__ = ('exceptions',)\n    def __init__(self, exceptions):\n        self.exceptions = exceptions\n    def __enter__(self): pass\n    def __exit__(self, exc_type, exc_value, traceback):\n        if exc_type is None:\n            return\n        # psycopg shits out a million cute error types. Try to catch em all.\n        if pg_errors is not None and exc_type.__name__ not in self.exceptions \\\n           and issubclass(exc_type, pg_errors.Error):\n            exc_type = exc_type.__bases__[0]\n        elif pg3_errors is not None and \\\n           exc_type.__name__ not in self.exceptions \\\n           and issubclass(exc_type, pg3_errors.Error):\n            exc_type = exc_type.__bases__[0]\n        if exc_type.__name__ in self.exceptions:\n            new_type = self.exceptions[exc_type.__name__]\n            exc_args = exc_value.args\n            reraise(new_type, new_type(exc_value, *exc_args), traceback)\n\n\nEXCEPTIONS = {\n    'ConstraintError': IntegrityError,\n    'DatabaseError': DatabaseError,\n    'DataError': DataError,\n    'IntegrityError': IntegrityError,\n    'InterfaceError': InterfaceError,\n    'InternalError': InternalError,\n    'NotSupportedError': NotSupportedError,\n    'OperationalError': OperationalError,\n    'ProgrammingError': ProgrammingError,\n    'TransactionRollbackError': OperationalError,\n    'UndefinedFunction': ProgrammingError,\n    'UniqueViolation': IntegrityError}\n\n__exception_wrapper__ = ExceptionWrapper(EXCEPTIONS)\n\n\n# DATABASE INTERFACE AND CONNECTION MANAGEMENT.\n\n\nIndexMetadata = collections.namedtuple(\n    'IndexMetadata',\n    ('name', 'sql', 'columns', 'unique', 'table'))\nColumnMetadata = collections.namedtuple(\n    'ColumnMetadata',\n    ('name', 'data_type', 'null', 'primary_key', 'table', 'default'))\nForeignKeyMetadata = collections.namedtuple(\n    'ForeignKeyMetadata',\n    ('column', 'dest_table', 'dest_column', 'table'))\nViewMetadata = collections.namedtuple('ViewMetadata', ('name', 'sql'))\n\n\nclass _ConnectionState(object):\n    def __init__(self, **kwargs):\n        super(_ConnectionState, self).__init__(**kwargs)\n        self.reset()\n\n    def reset(self):\n        self.closed = True\n        self.conn = None\n        self.ctx = []\n        self.transactions = []\n\n    def set_connection(self, conn):\n        self.conn = conn\n        self.closed = False\n        self.ctx = []\n        self.transactions = []\n\n\nclass _ConnectionLocal(_ConnectionState, threading.local): pass\nclass _NoopLock(object):\n    __slots__ = ()\n    def __enter__(self): return self\n    def __exit__(self, exc_type, exc_val, exc_tb): pass\n\n\nclass ConnectionContext(object):\n    __slots__ = ('db',)\n    def __init__(self, db): self.db = db\n    def __enter__(self):\n        if self.db.is_closed():\n            self.db.connect()\n    def __exit__(self, exc_type, exc_val, exc_tb): self.db.close()\n    def __call__(self, fn):\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            with ConnectionContext(self.db):\n                return fn(*args, **kwargs)\n        return inner\n\n\nclass Database(_callable_context_manager):\n    context_class = Context\n    field_types = {}\n    operations = {}\n    param = '?'\n    quote = '\"\"'\n    server_version = None\n\n    # Feature toggles.\n    compound_select_parentheses = CSQ_PARENTHESES_NEVER\n    for_update = False\n    index_schema_prefix = False\n    index_using_precedes_table = False\n    limit_max = None\n    nulls_ordering = False\n    returning_clause = False\n    safe_create_index = True\n    safe_drop_index = True\n    sequences = False\n    truncate_table = True\n\n    def __init__(self, database, thread_safe=True, autorollback=False,\n                 field_types=None, operations=None, autocommit=None,\n                 autoconnect=True, **kwargs):\n        self._field_types = merge_dict(FIELD, self.field_types)\n        self._operations = merge_dict(OP, self.operations)\n        if field_types:\n            self._field_types.update(field_types)\n        if operations:\n            self._operations.update(operations)\n\n        self.autoconnect = autoconnect\n        self.thread_safe = thread_safe\n        if thread_safe:\n            self._state = _ConnectionLocal()\n            self._lock = threading.Lock()\n        else:\n            self._state = _ConnectionState()\n            self._lock = _NoopLock()\n\n        if autorollback:\n            __deprecated__('Peewee no longer uses the \"autorollback\" option, '\n                           'as we always run in autocommit-mode now. This '\n                           'changes psycopg2\\'s semantics so that the conn '\n                           'is not left in a transaction-aborted state.')\n\n        if autocommit is not None:\n            __deprecated__('Peewee no longer uses the \"autocommit\" option, as '\n                           'the semantics now require it to always be True. '\n                           'Because some database-drivers also use the '\n                           '\"autocommit\" parameter, you are receiving a '\n                           'warning so you may update your code and remove '\n                           'the parameter, as in the future, specifying '\n                           'autocommit could impact the behavior of the '\n                           'database driver you are using.')\n\n        self.connect_params = {}\n        self.init(database, **kwargs)\n\n    def init(self, database, **kwargs):\n        if not self.is_closed():\n            self.close()\n        self.database = database\n        self.connect_params.update(kwargs)\n        self.deferred = not bool(database)\n\n    def __enter__(self):\n        if self.is_closed():\n            self.connect()\n        ctx = self.atomic()\n        self._state.ctx.append(ctx)\n        ctx.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        ctx = self._state.ctx.pop()\n        try:\n            ctx.__exit__(exc_type, exc_val, exc_tb)\n        finally:\n            if not self._state.ctx:\n                self.close()\n\n    def connection_context(self):\n        return ConnectionContext(self)\n\n    def _connect(self):\n        raise NotImplementedError\n\n    def connect(self, reuse_if_open=False):\n        with self._lock:\n            if self.deferred:\n                raise InterfaceError('Error, database must be initialized '\n                                     'before opening a connection.')\n            if not self._state.closed:\n                if reuse_if_open:\n                    return False\n                raise OperationalError('Connection already opened.')\n\n            self._state.reset()\n            with __exception_wrapper__:\n                self._state.set_connection(self._connect())\n                if self.server_version is None:\n                    self._set_server_version(self._state.conn)\n                self._initialize_connection(self._state.conn)\n        return True\n\n    def _initialize_connection(self, conn):\n        pass\n\n    def _set_server_version(self, conn):\n        self.server_version = 0\n\n    def close(self):\n        with self._lock:\n            if self.deferred:\n                raise InterfaceError('Error, database must be initialized '\n                                     'before opening a connection.')\n            if self.in_transaction():\n                raise OperationalError('Attempting to close database while '\n                                       'transaction is open.')\n            is_open = not self._state.closed\n            try:\n                if is_open:\n                    with __exception_wrapper__:\n                        self._close(self._state.conn)\n            finally:\n                self._state.reset()\n            return is_open\n\n    def _close(self, conn):\n        conn.close()\n\n    def is_closed(self):\n        return self._state.closed\n\n    def is_connection_usable(self):\n        return not self._state.closed\n\n    def connection(self):\n        if self.is_closed():\n            self.connect()\n        return self._state.conn\n\n    def cursor(self, commit=None, named_cursor=None):\n        if commit is not None:\n            __deprecated__('\"commit\" has been deprecated and is a no-op.')\n        if self.is_closed():\n            if self.autoconnect:\n                self.connect()\n            else:\n                raise InterfaceError('Error, database connection not opened.')\n        return self._state.conn.cursor()\n\n    def execute_sql(self, sql, params=None, commit=None):\n        if commit is not None:\n            __deprecated__('\"commit\" has been deprecated and is a no-op.')\n        logger.debug((sql, params))\n        with __exception_wrapper__:\n            cursor = self.cursor()\n            cursor.execute(sql, params or ())\n        return cursor\n\n    def execute(self, query, commit=None, **context_options):\n        if commit is not None:\n            __deprecated__('\"commit\" has been deprecated and is a no-op.')\n        ctx = self.get_sql_context(**context_options)\n        sql, params = ctx.sql(query).query()\n        return self.execute_sql(sql, params)\n\n    def get_context_options(self):\n        return {\n            'field_types': self._field_types,\n            'operations': self._operations,\n            'param': self.param,\n            'quote': self.quote,\n            'compound_select_parentheses': self.compound_select_parentheses,\n            'conflict_statement': self.conflict_statement,\n            'conflict_update': self.conflict_update,\n            'for_update': self.for_update,\n            'index_schema_prefix': self.index_schema_prefix,\n            'index_using_precedes_table': self.index_using_precedes_table,\n            'limit_max': self.limit_max,\n            'nulls_ordering': self.nulls_ordering,\n        }\n\n    def get_sql_context(self, **context_options):\n        context = self.get_context_options()\n        if context_options:\n            context.update(context_options)\n        return self.context_class(**context)\n\n    def conflict_statement(self, on_conflict, query):\n        raise NotImplementedError\n\n    def conflict_update(self, on_conflict, query):\n        raise NotImplementedError\n\n    def _build_on_conflict_update(self, on_conflict, query):\n        if on_conflict._conflict_target:\n            stmt = SQL('ON CONFLICT')\n            target = EnclosedNodeList([\n                Entity(col) if isinstance(col, basestring) else col\n                for col in on_conflict._conflict_target])\n            if on_conflict._conflict_where is not None:\n                target = NodeList([target, SQL('WHERE'),\n                                   on_conflict._conflict_where])\n        else:\n            stmt = SQL('ON CONFLICT ON CONSTRAINT')\n            target = on_conflict._conflict_constraint\n            if isinstance(target, basestring):\n                target = Entity(target)\n\n        updates = []\n        if on_conflict._preserve:\n            for column in on_conflict._preserve:\n                excluded = NodeList((SQL('EXCLUDED'), ensure_entity(column)),\n                                    glue='.')\n                expression = NodeList((ensure_entity(column), SQL('='),\n                                       excluded))\n                updates.append(expression)\n\n        if on_conflict._update:\n            for k, v in on_conflict._update.items():\n                if not isinstance(v, Node):\n                    # Attempt to resolve string field-names to their respective\n                    # field object, to apply data-type conversions.\n                    if isinstance(k, basestring):\n                        k = getattr(query.table, k)\n                    if isinstance(k, Field):\n                        v = k.to_value(v)\n                    else:\n                        v = Value(v, unpack=False)\n                else:\n                    v = QualifiedNames(v)\n                updates.append(NodeList((ensure_entity(k), SQL('='), v)))\n\n        parts = [stmt, target, SQL('DO UPDATE SET'), CommaNodeList(updates)]\n        if on_conflict._where:\n            parts.extend((SQL('WHERE'), QualifiedNames(on_conflict._where)))\n\n        return NodeList(parts)\n\n    def last_insert_id(self, cursor, query_type=None):\n        return cursor.lastrowid\n\n    def rows_affected(self, cursor):\n        return cursor.rowcount\n\n    def default_values_insert(self, ctx):\n        return ctx.literal('DEFAULT VALUES')\n\n    def session_start(self):\n        return self.transaction().__enter__()\n\n    def session_commit(self):\n        try:\n            txn = self.pop_transaction()\n        except IndexError:\n            return False\n        txn.commit(begin=self.in_transaction())\n        return True\n\n    def session_rollback(self):\n        try:\n            txn = self.pop_transaction()\n        except IndexError:\n            return False\n        txn.rollback(begin=self.in_transaction())\n        return True\n\n    def in_transaction(self):\n        return bool(self._state.transactions)\n\n    def push_transaction(self, transaction):\n        self._state.transactions.append(transaction)\n\n    def pop_transaction(self):\n        return self._state.transactions.pop()\n\n    def transaction_depth(self):\n        return len(self._state.transactions)\n\n    def top_transaction(self):\n        if self._state.transactions:\n            return self._state.transactions[-1]\n\n    def atomic(self, *args, **kwargs):\n        return _atomic(self, *args, **kwargs)\n\n    def manual_commit(self):\n        return _manual(self)\n\n    def transaction(self, *args, **kwargs):\n        return _transaction(self, *args, **kwargs)\n\n    def savepoint(self):\n        return _savepoint(self)\n\n    def begin(self):\n        if self.is_closed():\n            self.connect()\n        with __exception_wrapper__:\n            self.cursor().execute('BEGIN')\n\n    def rollback(self):\n        with __exception_wrapper__:\n            self.cursor().execute('ROLLBACK')\n\n    def commit(self):\n        with __exception_wrapper__:\n            self.cursor().execute('COMMIT')\n\n    def batch_commit(self, it, n):\n        for group in chunked(it, n):\n            with self.atomic():\n                for obj in group:\n                    yield obj\n\n    def table_exists(self, table_name, schema=None):\n        if is_model(table_name):\n            model = table_name\n            table_name = model._meta.table_name\n            schema = model._meta.schema\n        return table_name in self.get_tables(schema=schema)\n\n    def get_tables(self, schema=None):\n        raise NotImplementedError\n\n    def get_indexes(self, table, schema=None):\n        raise NotImplementedError\n\n    def get_columns(self, table, schema=None):\n        raise NotImplementedError\n\n    def get_primary_keys(self, table, schema=None):\n        raise NotImplementedError\n\n    def get_foreign_keys(self, table, schema=None):\n        raise NotImplementedError\n\n    def sequence_exists(self, seq):\n        raise NotImplementedError\n\n    def create_tables(self, models, **options):\n        for model in sort_models(models):\n            model.create_table(**options)\n\n    def drop_tables(self, models, **kwargs):\n        for model in reversed(sort_models(models)):\n            model.drop_table(**kwargs)\n\n    def extract_date(self, date_part, date_field):\n        raise NotImplementedError\n\n    def truncate_date(self, date_part, date_field):\n        raise NotImplementedError\n\n    def to_timestamp(self, date_field):\n        raise NotImplementedError\n\n    def from_timestamp(self, date_field):\n        raise NotImplementedError\n\n    def random(self):\n        return fn.random()\n\n    def bind(self, models, bind_refs=True, bind_backrefs=True):\n        for model in models:\n            model.bind(self, bind_refs=bind_refs, bind_backrefs=bind_backrefs)\n\n    def bind_ctx(self, models, bind_refs=True, bind_backrefs=True):\n        return _BoundModelsContext(models, self, bind_refs, bind_backrefs)\n\n    def get_noop_select(self, ctx):\n        return ctx.sql(Select().columns(SQL('0')).where(SQL('0')))\n\n    @property\n    def Model(self):\n        if not hasattr(self, '_Model'):\n            class Meta: database = self\n            self._Model = type('BaseModel', (Model,), {'Meta': Meta})\n        return self._Model\n\n\ndef __pragma__(name):\n    def __get__(self):\n        return self.pragma(name)\n    def __set__(self, value):\n        return self.pragma(name, value)\n    return property(__get__, __set__)\n\n\nclass SqliteDatabase(Database):\n    field_types = {\n        'BIGAUTO': FIELD.AUTO,\n        'BIGINT': FIELD.INT,\n        'BOOL': FIELD.INT,\n        'DOUBLE': FIELD.FLOAT,\n        'SMALLINT': FIELD.INT,\n        'UUID': FIELD.TEXT}\n    operations = {\n        'LIKE': 'GLOB',\n        'ILIKE': 'LIKE'}\n    index_schema_prefix = True\n    limit_max = -1\n    server_version = __sqlite_version__\n    truncate_table = False\n\n    def __init__(self, database, *args, **kwargs):\n        self._pragmas = kwargs.pop('pragmas', ())\n        super(SqliteDatabase, self).__init__(database, *args, **kwargs)\n        self._aggregates = {}\n        self._collations = {}\n        self._functions = {}\n        self._window_functions = {}\n        self._table_functions = []\n        self._extensions = set()\n        self._attached = {}\n        self.register_function(_sqlite_date_part, 'date_part', 2)\n        self.register_function(_sqlite_date_trunc, 'date_trunc', 2)\n        self.nulls_ordering = self.server_version >= (3, 30, 0)\n\n    def init(self, database, pragmas=None, timeout=5, returning_clause=None,\n             **kwargs):\n        if pragmas is not None:\n            self._pragmas = pragmas\n        if isinstance(self._pragmas, dict):\n            self._pragmas = list(self._pragmas.items())\n        if returning_clause is not None:\n            if __sqlite_version__ < (3, 35, 0):\n                warnings.warn('RETURNING clause requires Sqlite 3.35 or newer')\n            self.returning_clause = returning_clause\n        self._timeout = timeout\n        super(SqliteDatabase, self).init(database, **kwargs)\n\n    def _set_server_version(self, conn):\n        pass\n\n    def _connect(self):\n        if sqlite3 is None:\n            raise ImproperlyConfigured('SQLite driver not installed!')\n        conn = sqlite3.connect(self.database, timeout=self._timeout,\n                               isolation_level=None, **self.connect_params)\n        try:\n            self._add_conn_hooks(conn)\n        except:\n            conn.close()\n            raise\n        return conn\n\n    def _add_conn_hooks(self, conn):\n        if self._attached:\n            self._attach_databases(conn)\n        if self._pragmas:\n            self._set_pragmas(conn)\n        self._load_aggregates(conn)\n        self._load_collations(conn)\n        self._load_functions(conn)\n        if self.server_version >= (3, 25, 0):\n            self._load_window_functions(conn)\n        if self._table_functions:\n            for table_function in self._table_functions:\n                table_function.register(conn)\n        if self._extensions:\n            self._load_extensions(conn)\n\n    def _set_pragmas(self, conn):\n        cursor = conn.cursor()\n        for pragma, value in self._pragmas:\n            cursor.execute('PRAGMA %s = %s;' % (pragma, value))\n        cursor.close()\n\n    def _attach_databases(self, conn):\n        cursor = conn.cursor()\n        for name, db in self._attached.items():\n            cursor.execute('ATTACH DATABASE \"%s\" AS \"%s\"' % (db, name))\n        cursor.close()\n\n    def pragma(self, key, value=SENTINEL, permanent=False, schema=None):\n        if schema is not None:\n            key = '\"%s\".%s' % (schema, key)\n        sql = 'PRAGMA %s' % key\n        if value is not SENTINEL:\n            sql += ' = %s' % (value or 0)\n            if permanent:\n                pragmas = dict(self._pragmas or ())\n                pragmas[key] = value\n                self._pragmas = list(pragmas.items())\n        elif permanent:\n            raise ValueError('Cannot specify a permanent pragma without value')\n        row = self.execute_sql(sql).fetchone()\n        if row:\n            return row[0]\n\n    cache_size = __pragma__('cache_size')\n    foreign_keys = __pragma__('foreign_keys')\n    journal_mode = __pragma__('journal_mode')\n    journal_size_limit = __pragma__('journal_size_limit')\n    mmap_size = __pragma__('mmap_size')\n    page_size = __pragma__('page_size')\n    read_uncommitted = __pragma__('read_uncommitted')\n    synchronous = __pragma__('synchronous')\n    wal_autocheckpoint = __pragma__('wal_autocheckpoint')\n    application_id = __pragma__('application_id')\n    user_version = __pragma__('user_version')\n    data_version = __pragma__('data_version')\n\n    @property\n    def timeout(self):\n        return self._timeout\n\n    @timeout.setter\n    def timeout(self, seconds):\n        if self._timeout == seconds:\n            return\n\n        self._timeout = seconds\n        if not self.is_closed():\n            # PySQLite multiplies user timeout by 1000, but the unit of the\n            # timeout PRAGMA is actually milliseconds.\n            self.execute_sql('PRAGMA busy_timeout=%d;' % (seconds * 1000))\n\n    def _load_aggregates(self, conn):\n        for name, (klass, num_params) in self._aggregates.items():\n            conn.create_aggregate(name, num_params, klass)\n\n    def _load_collations(self, conn):\n        for name, fn in self._collations.items():\n            conn.create_collation(name, fn)\n\n    def _load_functions(self, conn):\n        for name, (fn, n_params, deterministic) in self._functions.items():\n            kwargs = {'deterministic': deterministic} if deterministic else {}\n            conn.create_function(name, n_params, fn, **kwargs)\n\n    def _load_window_functions(self, conn):\n        for name, (klass, num_params) in self._window_functions.items():\n            conn.create_window_function(name, num_params, klass)\n\n    def register_aggregate(self, klass, name=None, num_params=-1):\n        self._aggregates[name or klass.__name__.lower()] = (klass, num_params)\n        if not self.is_closed():\n            self._load_aggregates(self.connection())\n\n    def aggregate(self, name=None, num_params=-1):\n        def decorator(klass):\n            self.register_aggregate(klass, name, num_params)\n            return klass\n        return decorator\n\n    def register_collation(self, fn, name=None):\n        name = name or fn.__name__\n        def _collation(*args):\n            expressions = args + (SQL('collate %s' % name),)\n            return NodeList(expressions)\n        fn.collation = _collation\n        self._collations[name] = fn\n        if not self.is_closed():\n            self._load_collations(self.connection())\n\n    def collation(self, name=None):\n        def decorator(fn):\n            self.register_collation(fn, name)\n            return fn\n        return decorator\n\n    def register_function(self, fn, name=None, num_params=-1,\n                          deterministic=None):\n        self._functions[name or fn.__name__] = (fn, num_params, deterministic)\n        if not self.is_closed():\n            self._load_functions(self.connection())\n\n    def func(self, name=None, num_params=-1, deterministic=None):\n        def decorator(fn):\n            self.register_function(fn, name, num_params, deterministic)\n            return fn\n        return decorator\n\n    def register_window_function(self, klass, name=None, num_params=-1):\n        name = name or klass.__name__.lower()\n        self._window_functions[name] = (klass, num_params)\n        if not self.is_closed():\n            self._load_window_functions(self.connection())\n\n    def window_function(self, name=None, num_params=-1):\n        def decorator(klass):\n            self.register_window_function(klass, name, num_params)\n            return klass\n        return decorator\n\n    def register_table_function(self, klass, name=None):\n        if name is not None:\n            klass.name = name\n        self._table_functions.append(klass)\n        if not self.is_closed():\n            klass.register(self.connection())\n\n    def table_function(self, name=None):\n        def decorator(klass):\n            self.register_table_function(klass, name)\n            return klass\n        return decorator\n\n    def unregister_aggregate(self, name):\n        del(self._aggregates[name])\n\n    def unregister_collation(self, name):\n        del(self._collations[name])\n\n    def unregister_function(self, name):\n        del(self._functions[name])\n\n    def unregister_window_function(self, name):\n        del(self._window_functions[name])\n\n    def unregister_table_function(self, name):\n        for idx, klass in enumerate(self._table_functions):\n            if klass.name == name:\n                break\n        else:\n            return False\n        self._table_functions.pop(idx)\n        return True\n\n    def _load_extensions(self, conn):\n        conn.enable_load_extension(True)\n        for extension in self._extensions:\n            conn.load_extension(extension)\n\n    def load_extension(self, extension):\n        self._extensions.add(extension)\n        if not self.is_closed():\n            conn = self.connection()\n            conn.enable_load_extension(True)\n            conn.load_extension(extension)\n\n    def unload_extension(self, extension):\n        self._extensions.remove(extension)\n\n    def attach(self, filename, name):\n        if name in self._attached:\n            if self._attached[name] == filename:\n                return False\n            raise OperationalError('schema \"%s\" already attached.' % name)\n\n        self._attached[name] = filename\n        if not self.is_closed():\n            self.execute_sql('ATTACH DATABASE \"%s\" AS \"%s\"' % (filename, name))\n        return True\n\n    def detach(self, name):\n        if name not in self._attached:\n            return False\n\n        del self._attached[name]\n        if not self.is_closed():\n            self.execute_sql('DETACH DATABASE \"%s\"' % name)\n        return True\n\n    def last_insert_id(self, cursor, query_type=None):\n        if not self.returning_clause:\n            return cursor.lastrowid\n        elif query_type == Insert.SIMPLE:\n            try:\n                return cursor[0][0]\n            except (IndexError, KeyError, TypeError):\n                pass\n        return cursor\n\n    def rows_affected(self, cursor):\n        try:\n            return cursor.rowcount\n        except AttributeError:\n            return cursor.cursor.rowcount  # This was a RETURNING query.\n\n    def begin(self, lock_type=None):\n        statement = 'BEGIN %s' % lock_type if lock_type else 'BEGIN'\n        self.execute_sql(statement)\n\n    def commit(self):\n        with __exception_wrapper__:\n            return self._state.conn.commit()\n\n    def rollback(self):\n        with __exception_wrapper__:\n            return self._state.conn.rollback()\n\n    def get_tables(self, schema=None):\n        schema = schema or 'main'\n        cursor = self.execute_sql('SELECT name FROM \"%s\".sqlite_master WHERE '\n                                  'type=? ORDER BY name' % schema, ('table',))\n        return [row for row, in cursor.fetchall()]\n\n    def get_views(self, schema=None):\n        sql = ('SELECT name, sql FROM \"%s\".sqlite_master WHERE type=? '\n               'ORDER BY name') % (schema or 'main')\n        return [ViewMetadata(*row) for row in self.execute_sql(sql, ('view',))]\n\n    def get_indexes(self, table, schema=None):\n        schema = schema or 'main'\n        query = ('SELECT name, sql FROM \"%s\".sqlite_master '\n                 'WHERE tbl_name = ? AND type = ? ORDER BY name') % schema\n        cursor = self.execute_sql(query, (table, 'index'))\n        index_to_sql = dict(cursor.fetchall())\n\n        # Determine which indexes have a unique constraint.\n        unique_indexes = set()\n        cursor = self.execute_sql('PRAGMA \"%s\".index_list(\"%s\")' %\n                                  (schema, table))\n        for row in cursor.fetchall():\n            name = row[1]\n            is_unique = int(row[2]) == 1\n            if is_unique:\n                unique_indexes.add(name)\n\n        # Retrieve the indexed columns.\n        index_columns = {}\n        for index_name in sorted(index_to_sql):\n            cursor = self.execute_sql('PRAGMA \"%s\".index_info(\"%s\")' %\n                                      (schema, index_name))\n            index_columns[index_name] = [row[2] for row in cursor.fetchall()]\n\n        return [\n            IndexMetadata(\n                name,\n                index_to_sql[name],\n                index_columns[name],\n                name in unique_indexes,\n                table)\n            for name in sorted(index_to_sql)]\n\n    def get_columns(self, table, schema=None):\n        cursor = self.execute_sql('PRAGMA \"%s\".table_info(\"%s\")' %\n                                  (schema or 'main', table))\n        return [ColumnMetadata(r[1], r[2], not r[3], bool(r[5]), table, r[4])\n                for r in cursor.fetchall()]\n\n    def get_primary_keys(self, table, schema=None):\n        cursor = self.execute_sql('PRAGMA \"%s\".table_info(\"%s\")' %\n                                  (schema or 'main', table))\n        return [row[1] for row in filter(lambda r: r[-1], cursor.fetchall())]\n\n    def get_foreign_keys(self, table, schema=None):\n        cursor = self.execute_sql('PRAGMA \"%s\".foreign_key_list(\"%s\")' %\n                                  (schema or 'main', table))\n        return [ForeignKeyMetadata(row[3], row[2], row[4], table)\n                for row in cursor.fetchall()]\n\n    def get_binary_type(self):\n        return sqlite3.Binary\n\n    def conflict_statement(self, on_conflict, query):\n        action = on_conflict._action.lower() if on_conflict._action else ''\n        if action and action not in ('nothing', 'update'):\n            return SQL('INSERT OR %s' % on_conflict._action.upper())\n\n    def conflict_update(self, oc, query):\n        # Sqlite prior to 3.24.0 does not support Postgres-style upsert.\n        if self.server_version < (3, 24, 0) and \\\n           any((oc._preserve, oc._update, oc._where, oc._conflict_target,\n                oc._conflict_constraint)):\n            raise ValueError('SQLite does not support specifying which values '\n                             'to preserve or update.')\n\n        action = oc._action.lower() if oc._action else ''\n        if action and action not in ('nothing', 'update', ''):\n            return\n\n        if action == 'nothing':\n            return SQL('ON CONFLICT DO NOTHING')\n        elif not oc._update and not oc._preserve:\n            raise ValueError('If you are not performing any updates (or '\n                             'preserving any INSERTed values), then the '\n                             'conflict resolution action should be set to '\n                             '\"NOTHING\".')\n        elif oc._conflict_constraint:\n            raise ValueError('SQLite does not support specifying named '\n                             'constraints for conflict resolution.')\n        elif not oc._conflict_target:\n            raise ValueError('SQLite requires that a conflict target be '\n                             'specified when doing an upsert.')\n\n        return self._build_on_conflict_update(oc, query)\n\n    def extract_date(self, date_part, date_field):\n        return fn.date_part(date_part, date_field, python_value=int)\n\n    def truncate_date(self, date_part, date_field):\n        return fn.date_trunc(date_part, date_field,\n                             python_value=simple_date_time)\n\n    def to_timestamp(self, date_field):\n        return fn.strftime('%s', date_field).cast('integer')\n\n    def from_timestamp(self, date_field):\n        return fn.datetime(date_field, 'unixepoch')\n\n\nclass PostgresqlDatabase(Database):\n    field_types = {\n        'AUTO': 'SERIAL',\n        'BIGAUTO': 'BIGSERIAL',\n        'BLOB': 'BYTEA',\n        'BOOL': 'BOOLEAN',\n        'DATETIME': 'TIMESTAMP',\n        'DECIMAL': 'NUMERIC',\n        'DOUBLE': 'DOUBLE PRECISION',\n        'UUID': 'UUID',\n        'UUIDB': 'BYTEA'}\n    operations = {'REGEXP': '~', 'IREGEXP': '~*'}\n    param = '%s'\n\n    compound_select_parentheses = CSQ_PARENTHESES_ALWAYS\n    for_update = True\n    nulls_ordering = True\n    returning_clause = True\n    safe_create_index = False\n    sequences = True\n\n    def init(self, database, register_unicode=True, encoding=None,\n             isolation_level=None, **kwargs):\n        self._register_unicode = register_unicode\n        self._encoding = encoding\n        self._isolation_level = isolation_level\n        super(PostgresqlDatabase, self).init(database, **kwargs)\n\n    def _connect(self):\n        if psycopg2 is None:\n            raise ImproperlyConfigured('Postgres driver not installed!')\n\n        # Handle connection-strings nicely, since psycopg2 will accept them,\n        # and they may be easier when lots of parameters are specified.\n        params = self.connect_params.copy()\n        if self.database.startswith('postgresql://'):\n            params.setdefault('dsn', self.database)\n        else:\n            params.setdefault('dbname', self.database)\n\n        conn = psycopg2.connect(**params)\n        if self._register_unicode:\n            pg_extensions.register_type(pg_extensions.UNICODE, conn)\n            pg_extensions.register_type(pg_extensions.UNICODEARRAY, conn)\n        if self._encoding:\n            conn.set_client_encoding(self._encoding)\n        if self._isolation_level:\n            conn.set_isolation_level(self._isolation_level)\n        conn.autocommit = True\n        return conn\n\n    def _set_server_version(self, conn):\n        self.server_version = conn.server_version\n        if self.server_version >= 90600:\n            self.safe_create_index = True\n\n    def is_connection_usable(self):\n        if self._state.closed:\n            return False\n\n        # Returns True if we are idle, running a command, or in an active\n        # connection. If the connection is in an error state or the connection\n        # is otherwise unusable, return False.\n        txn_status = self._state.conn.get_transaction_status()\n        return txn_status < pg_extensions.TRANSACTION_STATUS_INERROR\n\n    def last_insert_id(self, cursor, query_type=None):\n        try:\n            return cursor if query_type != Insert.SIMPLE else cursor[0][0]\n        except (IndexError, KeyError, TypeError):\n            pass\n\n    def rows_affected(self, cursor):\n        try:\n            return cursor.rowcount\n        except AttributeError:\n            return cursor.cursor.rowcount\n\n    def begin(self, isolation_level=None):\n        if self.is_closed():\n            self.connect()\n        if isolation_level:\n            stmt = 'BEGIN TRANSACTION ISOLATION LEVEL %s' % isolation_level\n        else:\n            stmt = 'BEGIN'\n        with __exception_wrapper__:\n            self.cursor().execute(stmt)\n\n    def get_tables(self, schema=None):\n        query = ('SELECT tablename FROM pg_catalog.pg_tables '\n                 'WHERE schemaname = %s ORDER BY tablename')\n        cursor = self.execute_sql(query, (schema or 'public',))\n        return [table for table, in cursor.fetchall()]\n\n    def get_views(self, schema=None):\n        query = ('SELECT viewname, definition FROM pg_catalog.pg_views '\n                 'WHERE schemaname = %s ORDER BY viewname')\n        cursor = self.execute_sql(query, (schema or 'public',))\n        return [ViewMetadata(view_name, sql.strip(' \\t;'))\n                for (view_name, sql) in cursor.fetchall()]\n\n    def get_indexes(self, table, schema=None):\n        query = \"\"\"\n            SELECT\n                i.relname, idxs.indexdef, idx.indisunique,\n                array_to_string(ARRAY(\n                    SELECT pg_get_indexdef(idx.indexrelid, k + 1, TRUE)\n                    FROM generate_subscripts(idx.indkey, 1) AS k\n                    ORDER BY k), ',')\n            FROM pg_catalog.pg_class AS t\n            INNER JOIN pg_catalog.pg_index AS idx ON t.oid = idx.indrelid\n            INNER JOIN pg_catalog.pg_class AS i ON idx.indexrelid = i.oid\n            INNER JOIN pg_catalog.pg_indexes AS idxs ON\n                (idxs.tablename = t.relname AND idxs.indexname = i.relname)\n            WHERE t.relname = %s AND t.relkind = %s AND idxs.schemaname = %s\n            ORDER BY idx.indisunique DESC, i.relname;\"\"\"\n        cursor = self.execute_sql(query, (table, 'r', schema or 'public'))\n        return [IndexMetadata(name, sql.rstrip(' ;'), columns.split(','),\n                              is_unique, table)\n                for name, sql, is_unique, columns in cursor.fetchall()]\n\n    def get_columns(self, table, schema=None):\n        query = \"\"\"\n            SELECT column_name, is_nullable, data_type, column_default\n            FROM information_schema.columns\n            WHERE table_name = %s AND table_schema = %s\n            ORDER BY ordinal_position\"\"\"\n        cursor = self.execute_sql(query, (table, schema or 'public'))\n        pks = set(self.get_primary_keys(table, schema))\n        return [ColumnMetadata(name, dt, null == 'YES', name in pks, table, df)\n                for name, null, dt, df in cursor.fetchall()]\n\n    def get_primary_keys(self, table, schema=None):\n        query = \"\"\"\n            SELECT kc.column_name\n            FROM information_schema.table_constraints AS tc\n            INNER JOIN information_schema.key_column_usage AS kc ON (\n                tc.table_name = kc.table_name AND\n                tc.table_schema = kc.table_schema AND\n                tc.constraint_name = kc.constraint_name)\n            WHERE\n                tc.constraint_type = %s AND\n                tc.table_name = %s AND\n                tc.table_schema = %s\"\"\"\n        ctype = 'PRIMARY KEY'\n        cursor = self.execute_sql(query, (ctype, table, schema or 'public'))\n        return [pk for pk, in cursor.fetchall()]\n\n    def get_foreign_keys(self, table, schema=None):\n        sql = \"\"\"\n            SELECT DISTINCT\n                kcu.column_name, ccu.table_name, ccu.column_name\n            FROM information_schema.table_constraints AS tc\n            JOIN information_schema.key_column_usage AS kcu\n                ON (tc.constraint_name = kcu.constraint_name AND\n                    tc.constraint_schema = kcu.constraint_schema AND\n                    tc.table_name = kcu.table_name AND\n                    tc.table_schema = kcu.table_schema)\n            JOIN information_schema.constraint_column_usage AS ccu\n                ON (ccu.constraint_name = tc.constraint_name AND\n                    ccu.constraint_schema = tc.constraint_schema)\n            WHERE\n                tc.constraint_type = 'FOREIGN KEY' AND\n                tc.table_name = %s AND\n                tc.table_schema = %s\"\"\"\n        cursor = self.execute_sql(sql, (table, schema or 'public'))\n        return [ForeignKeyMetadata(row[0], row[1], row[2], table)\n                for row in cursor.fetchall()]\n\n    def sequence_exists(self, sequence):\n        res = self.execute_sql(\"\"\"\n            SELECT COUNT(*) FROM pg_class, pg_namespace\n            WHERE relkind='S'\n                AND pg_class.relnamespace = pg_namespace.oid\n                AND relname=%s\"\"\", (sequence,))\n        return bool(res.fetchone()[0])\n\n    def get_binary_type(self):\n        return psycopg2.Binary\n\n    def conflict_statement(self, on_conflict, query):\n        return\n\n    def conflict_update(self, oc, query):\n        action = oc._action.lower() if oc._action else ''\n        if action in ('ignore', 'nothing'):\n            parts = [SQL('ON CONFLICT')]\n            if oc._conflict_target:\n                parts.append(EnclosedNodeList([\n                    Entity(col) if isinstance(col, basestring) else col\n                    for col in oc._conflict_target]))\n            parts.append(SQL('DO NOTHING'))\n            return NodeList(parts)\n        elif action and action != 'update':\n            raise ValueError('The only supported actions for conflict '\n                             'resolution with Postgresql are \"ignore\" or '\n                             '\"update\".')\n        elif not oc._update and not oc._preserve:\n            raise ValueError('If you are not performing any updates (or '\n                             'preserving any INSERTed values), then the '\n                             'conflict resolution action should be set to '\n                             '\"IGNORE\".')\n        elif not (oc._conflict_target or oc._conflict_constraint):\n            raise ValueError('Postgres requires that a conflict target be '\n                             'specified when doing an upsert.')\n\n        return self._build_on_conflict_update(oc, query)\n\n    def extract_date(self, date_part, date_field):\n        return fn.EXTRACT(NodeList((date_part, SQL('FROM'), date_field)))\n\n    def truncate_date(self, date_part, date_field):\n        return fn.DATE_TRUNC(date_part, date_field)\n\n    def to_timestamp(self, date_field):\n        return self.extract_date('EPOCH', date_field)\n\n    def from_timestamp(self, date_field):\n        # Ironically, here, Postgres means \"to the Postgresql timestamp type\".\n        return fn.to_timestamp(date_field)\n\n    def get_noop_select(self, ctx):\n        return ctx.sql(Select().columns(SQL('0')).where(SQL('false')))\n\n    def set_time_zone(self, timezone):\n        self.execute_sql('set time zone \"%s\";' % timezone)\n\n\nclass MySQLDatabase(Database):\n    field_types = {\n        'AUTO': 'INTEGER AUTO_INCREMENT',\n        'BIGAUTO': 'BIGINT AUTO_INCREMENT',\n        'BOOL': 'BOOL',\n        'DECIMAL': 'NUMERIC',\n        'DOUBLE': 'DOUBLE PRECISION',\n        'FLOAT': 'FLOAT',\n        'UUID': 'VARCHAR(40)',\n        'UUIDB': 'VARBINARY(16)'}\n    operations = {\n        'LIKE': 'LIKE BINARY',\n        'ILIKE': 'LIKE',\n        'REGEXP': 'REGEXP BINARY',\n        'IREGEXP': 'REGEXP',\n        'XOR': 'XOR'}\n    param = '%s'\n    quote = '``'\n\n    compound_select_parentheses = CSQ_PARENTHESES_UNNESTED\n    for_update = True\n    index_using_precedes_table = True\n    limit_max = 2 ** 64 - 1\n    safe_create_index = False\n    safe_drop_index = False\n    sql_mode = 'PIPES_AS_CONCAT'\n\n    def init(self, database, **kwargs):\n        params = {\n            'charset': 'utf8',\n            'sql_mode': self.sql_mode,\n            'use_unicode': True}\n        params.update(kwargs)\n        if 'password' in params and mysql_passwd:\n            params['passwd'] = params.pop('password')\n        super(MySQLDatabase, self).init(database, **params)\n\n    def _connect(self):\n        if mysql is None:\n            raise ImproperlyConfigured('MySQL driver not installed!')\n        conn = mysql.connect(db=self.database, autocommit=True,\n                             **self.connect_params)\n        return conn\n\n    def _set_server_version(self, conn):\n        try:\n            version_raw = conn.server_version\n        except AttributeError:\n            version_raw = conn.get_server_info()\n        self.server_version = self._extract_server_version(version_raw)\n\n    def _extract_server_version(self, version):\n        version = version.lower()\n        if 'maria' in version:\n            match_obj = re.search(r'(1\\d\\.\\d+\\.\\d+)', version)\n        else:\n            match_obj = re.search(r'(\\d\\.\\d+\\.\\d+)', version)\n        if match_obj is not None:\n            return tuple(int(num) for num in match_obj.groups()[0].split('.'))\n\n        warnings.warn('Unable to determine MySQL version: \"%s\"' % version)\n        return (0, 0, 0)  # Unable to determine version!\n\n    def is_connection_usable(self):\n        if self._state.closed:\n            return False\n\n        conn = self._state.conn\n        if hasattr(conn, 'ping'):\n            if self.server_version[0] == 8:\n                args = ()\n            else:\n                args = (False,)\n            try:\n                conn.ping(*args)\n            except Exception:\n                return False\n        return True\n\n    def default_values_insert(self, ctx):\n        return ctx.literal('() VALUES ()')\n\n    def begin(self, isolation_level=None):\n        if self.is_closed():\n            self.connect()\n        with __exception_wrapper__:\n            curs = self.cursor()\n            if isolation_level:\n                curs.execute('SET TRANSACTION ISOLATION LEVEL %s' %\n                             isolation_level)\n            curs.execute('BEGIN')\n\n    def get_tables(self, schema=None):\n        query = ('SELECT table_name FROM information_schema.tables '\n                 'WHERE table_schema = DATABASE() AND table_type != %s '\n                 'ORDER BY table_name')\n        return [table for table, in self.execute_sql(query, ('VIEW',))]\n\n    def get_views(self, schema=None):\n        query = ('SELECT table_name, view_definition '\n                 'FROM information_schema.views '\n                 'WHERE table_schema = DATABASE() ORDER BY table_name')\n        cursor = self.execute_sql(query)\n        return [ViewMetadata(*row) for row in cursor.fetchall()]\n\n    def get_indexes(self, table, schema=None):\n        cursor = self.execute_sql('SHOW INDEX FROM `%s`' % table)\n        unique = set()\n        indexes = {}\n        for row in cursor.fetchall():\n            if not row[1]:\n                unique.add(row[2])\n            indexes.setdefault(row[2], [])\n            indexes[row[2]].append(row[4])\n        return [IndexMetadata(name, None, indexes[name], name in unique, table)\n                for name in indexes]\n\n    def get_columns(self, table, schema=None):\n        sql = \"\"\"\n            SELECT column_name, is_nullable, data_type, column_default\n            FROM information_schema.columns\n            WHERE table_name = %s AND table_schema = DATABASE()\n            ORDER BY ordinal_position\"\"\"\n        cursor = self.execute_sql(sql, (table,))\n        pks = set(self.get_primary_keys(table))\n        return [ColumnMetadata(name, dt, null == 'YES', name in pks, table, df)\n                for name, null, dt, df in cursor.fetchall()]\n\n    def get_primary_keys(self, table, schema=None):\n        cursor = self.execute_sql('SHOW INDEX FROM `%s`' % table)\n        return [row[4] for row in\n                filter(lambda row: row[2] == 'PRIMARY', cursor.fetchall())]\n\n    def get_foreign_keys(self, table, schema=None):\n        query = \"\"\"\n            SELECT column_name, referenced_table_name, referenced_column_name\n            FROM information_schema.key_column_usage\n            WHERE table_name = %s\n                AND table_schema = DATABASE()\n                AND referenced_table_name IS NOT NULL\n                AND referenced_column_name IS NOT NULL\"\"\"\n        cursor = self.execute_sql(query, (table,))\n        return [\n            ForeignKeyMetadata(column, dest_table, dest_column, table)\n            for column, dest_table, dest_column in cursor.fetchall()]\n\n    def get_binary_type(self):\n        return mysql.Binary\n\n    def conflict_statement(self, on_conflict, query):\n        if not on_conflict._action: return\n\n        action = on_conflict._action.lower()\n        if action == 'replace':\n            return SQL('REPLACE')\n        elif action == 'ignore':\n            return SQL('INSERT IGNORE')\n        elif action != 'update':\n            raise ValueError('Un-supported action for conflict resolution. '\n                             'MySQL supports REPLACE, IGNORE and UPDATE.')\n\n    def conflict_update(self, on_conflict, query):\n        if on_conflict._where or on_conflict._conflict_target or \\\n           on_conflict._conflict_constraint:\n            raise ValueError('MySQL does not support the specification of '\n                             'where clauses or conflict targets for conflict '\n                             'resolution.')\n\n        updates = []\n        if on_conflict._preserve:\n            # Here we need to determine which function to use, which varies\n            # depending on the MySQL server version. MySQL and MariaDB prior to\n            # 10.3.3 use \"VALUES\", while MariaDB 10.3.3+ use \"VALUE\".\n            version = self.server_version or (0,)\n            if version[0] == 10 and version >= (10, 3, 3):\n                VALUE_FN = fn.VALUE\n            else:\n                VALUE_FN = fn.VALUES\n\n            for column in on_conflict._preserve:\n                entity = ensure_entity(column)\n                expression = NodeList((\n                    ensure_entity(column),\n                    SQL('='),\n                    VALUE_FN(entity)))\n                updates.append(expression)\n\n        if on_conflict._update:\n            for k, v in on_conflict._update.items():\n                if not isinstance(v, Node):\n                    # Attempt to resolve string field-names to their respective\n                    # field object, to apply data-type conversions.\n                    if isinstance(k, basestring):\n                        k = getattr(query.table, k)\n                    if isinstance(k, Field):\n                        v = k.to_value(v)\n                    else:\n                        v = Value(v, unpack=False)\n                updates.append(NodeList((ensure_entity(k), SQL('='), v)))\n\n        if updates:\n            return NodeList((SQL('ON DUPLICATE KEY UPDATE'),\n                             CommaNodeList(updates)))\n\n    def extract_date(self, date_part, date_field):\n        return fn.EXTRACT(NodeList((SQL(date_part), SQL('FROM'), date_field)))\n\n    def truncate_date(self, date_part, date_field):\n        return fn.DATE_FORMAT(date_field, __mysql_date_trunc__[date_part],\n                              python_value=simple_date_time)\n\n    def to_timestamp(self, date_field):\n        return fn.UNIX_TIMESTAMP(date_field)\n\n    def from_timestamp(self, date_field):\n        return fn.FROM_UNIXTIME(date_field)\n\n    def random(self):\n        return fn.rand()\n\n    def get_noop_select(self, ctx):\n        return ctx.literal('DO 0')\n\n\n# TRANSACTION CONTROL.\n\n\nclass _manual(object):\n    def __init__(self, db):\n        self.db = db\n\n    def __call__(self, fn):\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            with _manual(self.db):\n                return fn(*args, **kwargs)\n        return inner\n\n    def __enter__(self):\n        top = self.db.top_transaction()\n        if top is not None and not isinstance(top, _manual):\n            raise ValueError('Cannot enter manual commit block while a '\n                             'transaction is active.')\n        self.db.push_transaction(self)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.db.pop_transaction() is not self:\n            raise ValueError('Transaction stack corrupted while exiting '\n                             'manual commit block.')\n\n\nclass _atomic(object):\n    def __init__(self, db, *args, **kwargs):\n        self.db = db\n        self._transaction_args = (args, kwargs)\n\n    def __call__(self, fn):\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            a, k = self._transaction_args\n            with _atomic(self.db, *a, **k):\n                return fn(*args, **kwargs)\n        return inner\n\n    def __enter__(self):\n        if self.db.transaction_depth() == 0:\n            args, kwargs = self._transaction_args\n            self._helper = self.db.transaction(*args, **kwargs)\n        elif isinstance(self.db.top_transaction(), _manual):\n            raise ValueError('Cannot enter atomic commit block while in '\n                             'manual commit mode.')\n        else:\n            self._helper = self.db.savepoint()\n        return self._helper.__enter__()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        return self._helper.__exit__(exc_type, exc_val, exc_tb)\n\n\nclass _transaction(object):\n    def __init__(self, db, *args, **kwargs):\n        self.db = db\n        self._begin_args = (args, kwargs)\n\n    def __call__(self, fn):\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            a, k = self._begin_args\n            with _transaction(self.db, *a, **k):\n                return fn(*args, **kwargs)\n        return inner\n\n    def _begin(self):\n        args, kwargs = self._begin_args\n        self.db.begin(*args, **kwargs)\n\n    def commit(self, begin=True):\n        self.db.commit()\n        if begin:\n            self._begin()\n\n    def rollback(self, begin=True):\n        self.db.rollback()\n        if begin:\n            self._begin()\n\n    def __enter__(self):\n        if self.db.transaction_depth() == 0:\n            self._begin()\n        self.db.push_transaction(self)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        depth = self.db.transaction_depth()\n        try:\n            if exc_type and depth == 1:\n                self.rollback(False)\n            elif depth == 1:\n                try:\n                    self.commit(False)\n                except:\n                    self.rollback(False)\n                    raise\n        finally:\n            self.db.pop_transaction()\n\n\nclass _savepoint(object):\n    def __init__(self, db, sid=None):\n        self.db = db\n        self.sid = sid or 's' + uuid.uuid4().hex\n        self.quoted_sid = self.sid.join(self.db.quote)\n\n    def __call__(self, fn):\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            with _savepoint(self.db):\n                return fn(*args, **kwargs)\n        return inner\n\n    def _begin(self):\n        self.db.execute_sql('SAVEPOINT %s;' % self.quoted_sid)\n\n    def commit(self, begin=True):\n        self.db.execute_sql('RELEASE SAVEPOINT %s;' % self.quoted_sid)\n        if begin: self._begin()\n\n    def rollback(self):\n        self.db.execute_sql('ROLLBACK TO SAVEPOINT %s;' % self.quoted_sid)\n\n    def __enter__(self):\n        self._begin()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type:\n            self.rollback()\n        else:\n            try:\n                self.commit(begin=False)\n            except:\n                self.rollback()\n                raise\n\n\n# CURSOR REPRESENTATIONS.\n\n\nclass CursorWrapper(object):\n    def __init__(self, cursor):\n        self.cursor = cursor\n        self.count = 0\n        self.index = 0\n        self.initialized = False\n        self.populated = False\n        self.row_cache = []\n\n    def __iter__(self):\n        if self.populated:\n            return iter(self.row_cache)\n        return ResultIterator(self)\n\n    def __getitem__(self, item):\n        if isinstance(item, slice):\n            stop = item.stop\n            if stop is None or stop < 0:\n                self.fill_cache()\n            else:\n                self.fill_cache(stop)\n            return self.row_cache[item]\n        elif isinstance(item, int):\n            self.fill_cache(item if item > 0 else 0)\n            return self.row_cache[item]\n        else:\n            raise ValueError('CursorWrapper only supports integer and slice '\n                             'indexes.')\n\n    def __len__(self):\n        self.fill_cache()\n        return self.count\n\n    def initialize(self):\n        pass\n\n    def iterate(self, cache=True):\n        row = self.cursor.fetchone()\n        if row is None:\n            self.populated = True\n            self.cursor.close()\n            raise StopIteration\n        elif not self.initialized:\n            self.initialize()  # Lazy initialization.\n            self.initialized = True\n        self.count += 1\n        result = self.process_row(row)\n        if cache:\n            self.row_cache.append(result)\n        return result\n\n    def process_row(self, row):\n        return row\n\n    def iterator(self):\n        \"\"\"Efficient one-pass iteration over the result set.\"\"\"\n        while True:\n            try:\n                yield self.iterate(False)\n            except StopIteration:\n                return\n\n    def fill_cache(self, n=0):\n        n = n or float('Inf')\n        if n < 0:\n            raise ValueError('Negative values are not supported.')\n\n        iterator = ResultIterator(self)\n        iterator.index = self.count\n        while not self.populated and (n > self.count):\n            try:\n                iterator.next()\n            except StopIteration:\n                break\n\n\nclass DictCursorWrapper(CursorWrapper):\n    def _initialize_columns(self):\n        description = self.cursor.description\n        self.columns = [t[0][t[0].rfind('.') + 1:].strip('()\"`')\n                        for t in description]\n        self.ncols = len(description)\n\n    initialize = _initialize_columns\n\n    def _row_to_dict(self, row):\n        result = {}\n        for i in range(self.ncols):\n            result.setdefault(self.columns[i], row[i])  # Do not overwrite.\n        return result\n\n    process_row = _row_to_dict\n\n\nclass NamedTupleCursorWrapper(CursorWrapper):\n    def initialize(self):\n        description = self.cursor.description\n        self.tuple_class = collections.namedtuple('Row', [\n            t[0][t[0].rfind('.') + 1:].strip('()\"`') for t in description])\n\n    def process_row(self, row):\n        return self.tuple_class(*row)\n\n\nclass ObjectCursorWrapper(DictCursorWrapper):\n    def __init__(self, cursor, constructor):\n        super(ObjectCursorWrapper, self).__init__(cursor)\n        self.constructor = constructor\n\n    def process_row(self, row):\n        row_dict = self._row_to_dict(row)\n        return self.constructor(**row_dict)\n\n\nclass ResultIterator(object):\n    def __init__(self, cursor_wrapper):\n        self.cursor_wrapper = cursor_wrapper\n        self.index = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.index < self.cursor_wrapper.count:\n            obj = self.cursor_wrapper.row_cache[self.index]\n        elif not self.cursor_wrapper.populated:\n            self.cursor_wrapper.iterate()\n            obj = self.cursor_wrapper.row_cache[self.index]\n        else:\n            raise StopIteration\n        self.index += 1\n        return obj\n\n    __next__ = next\n\n# FIELDS\n\nclass FieldAccessor(object):\n    def __init__(self, model, field, name):\n        self.model = model\n        self.field = field\n        self.name = name\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return instance.__data__.get(self.name)\n        return self.field\n\n    def __set__(self, instance, value):\n        instance.__data__[self.name] = value\n        instance._dirty.add(self.name)\n\n\nclass ForeignKeyAccessor(FieldAccessor):\n    def __init__(self, model, field, name):\n        super(ForeignKeyAccessor, self).__init__(model, field, name)\n        self.rel_model = field.rel_model\n\n    def get_rel_instance(self, instance):\n        value = instance.__data__.get(self.name)\n        if value is not None or self.name in instance.__rel__:\n            if self.name not in instance.__rel__ and self.field.lazy_load:\n                obj = self.rel_model.get(self.field.rel_field == value)\n                instance.__rel__[self.name] = obj\n            return instance.__rel__.get(self.name, value)\n        elif not self.field.null and self.field.lazy_load:\n            raise self.rel_model.DoesNotExist\n        return value\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return self.get_rel_instance(instance)\n        return self.field\n\n    def __set__(self, instance, obj):\n        if isinstance(obj, self.rel_model):\n            instance.__data__[self.name] = getattr(obj, self.field.rel_field.name)\n            instance.__rel__[self.name] = obj\n        else:\n            fk_value = instance.__data__.get(self.name)\n            instance.__data__[self.name] = obj\n            if (obj != fk_value or obj is None) and \\\n               self.name in instance.__rel__:\n                del instance.__rel__[self.name]\n        instance._dirty.add(self.name)\n\n\nclass BackrefAccessor(object):\n    def __init__(self, field):\n        self.field = field\n        self.model = field.rel_model\n        self.rel_model = field.model\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            dest = self.field.rel_field.name\n            return (self.rel_model\n                    .select()\n                    .where(self.field == getattr(instance, dest)))\n        return self\n\n\nclass ObjectIdAccessor(object):\n    \"\"\"Gives direct access to the underlying id\"\"\"\n    def __init__(self, field):\n        self.field = field\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            value = instance.__data__.get(self.field.name)\n            # Pull the object-id from the related object if it is not set.\n            if value is None and self.field.name in instance.__rel__:\n                rel_obj = instance.__rel__[self.field.name]\n                value = getattr(rel_obj, self.field.rel_field.name)\n            return value\n        return self.field\n\n    def __set__(self, instance, value):\n        setattr(instance, self.field.name, value)\n\n\nclass Field(ColumnBase):\n    _field_counter = 0\n    _order = 0\n    accessor_class = FieldAccessor\n    auto_increment = False\n    default_index_type = None\n    field_type = 'DEFAULT'\n    unpack = True\n\n    def __init__(self, null=False, index=False, unique=False, column_name=None,\n                 default=None, primary_key=False, constraints=None,\n                 sequence=None, collation=None, unindexed=False, choices=None,\n                 help_text=None, verbose_name=None, index_type=None,\n                 db_column=None, _hidden=False):\n        if db_column is not None:\n            __deprecated__('\"db_column\" has been deprecated in favor of '\n                           '\"column_name\" for Field objects.')\n            column_name = db_column\n\n        self.null = null\n        self.index = index\n        self.unique = unique\n        self.column_name = column_name\n        self.default = default\n        self.primary_key = primary_key\n        self.constraints = constraints  # List of column constraints.\n        self.sequence = sequence  # Name of sequence, e.g. foo_id_seq.\n        self.collation = collation\n        self.unindexed = unindexed\n        self.choices = choices\n        self.help_text = help_text\n        self.verbose_name = verbose_name\n        self.index_type = index_type or self.default_index_type\n        self._hidden = _hidden\n\n        # Used internally for recovering the order in which Fields were defined\n        # on the Model class.\n        Field._field_counter += 1\n        self._order = Field._field_counter\n        self._sort_key = (self.primary_key and 1 or 2), self._order\n\n    def __hash__(self):\n        return hash(self.name + '.' + self.model.__name__)\n\n    def __repr__(self):\n        if hasattr(self, 'model') and getattr(self, 'name', None):\n            return '<%s: %s.%s>' % (type(self).__name__,\n                                    self.model.__name__,\n                                    self.name)\n        return '<%s: (unbound)>' % type(self).__name__\n\n    def bind(self, model, name, set_attribute=True):\n        self.model = model\n        self.name = self.safe_name = name\n        self.column_name = self.column_name or name\n        if set_attribute:\n            setattr(model, name, self.accessor_class(model, self, name))\n\n    @property\n    def column(self):\n        return Column(self.model._meta.table, self.column_name)\n\n    def adapt(self, value):\n        return value\n\n    def db_value(self, value):\n        return value if value is None else self.adapt(value)\n\n    def python_value(self, value):\n        return value if value is None else self.adapt(value)\n\n    def to_value(self, value):\n        return Value(value, self.db_value, unpack=False)\n\n    def get_sort_key(self, ctx):\n        return self._sort_key\n\n    def __sql__(self, ctx):\n        return ctx.sql(self.column)\n\n    def get_modifiers(self):\n        pass\n\n    def ddl_datatype(self, ctx):\n        if ctx and ctx.state.field_types:\n            column_type = ctx.state.field_types.get(self.field_type,\n                                                    self.field_type)\n        else:\n            column_type = self.field_type\n\n        modifiers = self.get_modifiers()\n        if column_type and modifiers:\n            modifier_literal = ', '.join([str(m) for m in modifiers])\n            return SQL('%s(%s)' % (column_type, modifier_literal))\n        else:\n            return SQL(column_type)\n\n    def ddl(self, ctx):\n        accum = [Entity(self.column_name)]\n        data_type = self.ddl_datatype(ctx)\n        if data_type:\n            accum.append(data_type)\n        if self.unindexed:\n            accum.append(SQL('UNINDEXED'))\n        if not self.null:\n            accum.append(SQL('NOT NULL'))\n        if self.primary_key:\n            accum.append(SQL('PRIMARY KEY'))\n        if self.sequence:\n            accum.append(SQL(\"DEFAULT NEXTVAL('%s')\" % self.sequence))\n        if self.constraints:\n            accum.extend(self.constraints)\n        if self.collation:\n            accum.append(SQL('COLLATE %s' % self.collation))\n        return NodeList(accum)\n\n\nclass AnyField(Field):\n    field_type = 'ANY'\n\n\nclass IntegerField(Field):\n    field_type = 'INT'\n\n    def adapt(self, value):\n        try:\n            return int(value)\n        except ValueError:\n            return value\n\n\nclass BigIntegerField(IntegerField):\n    field_type = 'BIGINT'\n\n\nclass SmallIntegerField(IntegerField):\n    field_type = 'SMALLINT'\n\n\nclass AutoField(IntegerField):\n    auto_increment = True\n    field_type = 'AUTO'\n\n    def __init__(self, *args, **kwargs):\n        if kwargs.get('primary_key') is False:\n            raise ValueError('%s must always be a primary key.' % type(self))\n        kwargs['primary_key'] = True\n        super(AutoField, self).__init__(*args, **kwargs)\n\n\nclass BigAutoField(AutoField):\n    field_type = 'BIGAUTO'\n\n\nclass IdentityField(AutoField):\n    field_type = 'INT GENERATED BY DEFAULT AS IDENTITY'\n\n    def __init__(self, generate_always=False, **kwargs):\n        if generate_always:\n            self.field_type = 'INT GENERATED ALWAYS AS IDENTITY'\n        super(IdentityField, self).__init__(**kwargs)\n\n\nclass PrimaryKeyField(AutoField):\n    def __init__(self, *args, **kwargs):\n        __deprecated__('\"PrimaryKeyField\" has been renamed to \"AutoField\". '\n                       'Please update your code accordingly as this will be '\n                       'completely removed in a subsequent release.')\n        super(PrimaryKeyField, self).__init__(*args, **kwargs)\n\n\nclass FloatField(Field):\n    field_type = 'FLOAT'\n\n    def adapt(self, value):\n        try:\n            return float(value)\n        except ValueError:\n            return value\n\n\nclass DoubleField(FloatField):\n    field_type = 'DOUBLE'\n\n\nclass DecimalField(Field):\n    field_type = 'DECIMAL'\n\n    def __init__(self, max_digits=10, decimal_places=5, auto_round=False,\n                 rounding=None, *args, **kwargs):\n        self.max_digits = max_digits\n        self.decimal_places = decimal_places\n        self.auto_round = auto_round\n        self.rounding = rounding or decimal.DefaultContext.rounding\n        self._exp = decimal.Decimal(10) ** (-self.decimal_places)\n        super(DecimalField, self).__init__(*args, **kwargs)\n\n    def get_modifiers(self):\n        return [self.max_digits, self.decimal_places]\n\n    def db_value(self, value):\n        D = decimal.Decimal\n        if not value:\n            return value if value is None else D(0)\n        if self.auto_round:\n            decimal_value = D(text_type(value))\n            return decimal_value.quantize(self._exp, rounding=self.rounding)\n        return value\n\n    def python_value(self, value):\n        if value is not None:\n            if isinstance(value, decimal.Decimal):\n                return value\n            return decimal.Decimal(text_type(value))\n\n\nclass _StringField(Field):\n    def adapt(self, value):\n        if isinstance(value, text_type):\n            return value\n        elif isinstance(value, bytes_type):\n            return value.decode('utf-8')\n        return text_type(value)\n\n    def __add__(self, other): return StringExpression(self, OP.CONCAT, other)\n    def __radd__(self, other): return StringExpression(other, OP.CONCAT, self)\n\n\nclass CharField(_StringField):\n    field_type = 'VARCHAR'\n\n    def __init__(self, max_length=255, *args, **kwargs):\n        self.max_length = max_length\n        super(CharField, self).__init__(*args, **kwargs)\n\n    def get_modifiers(self):\n        return self.max_length and [self.max_length] or None\n\n\nclass FixedCharField(CharField):\n    field_type = 'CHAR'\n\n    def python_value(self, value):\n        value = super(FixedCharField, self).python_value(value)\n        if value:\n            value = value.strip()\n        return value\n\n\nclass TextField(_StringField):\n    field_type = 'TEXT'\n\n\nclass BlobField(Field):\n    field_type = 'BLOB'\n\n    def _db_hook(self, database):\n        if database is None:\n            self._constructor = bytearray\n        else:\n            self._constructor = database.get_binary_type()\n\n    def bind(self, model, name, set_attribute=True):\n        self._constructor = bytearray\n        if model._meta.database:\n            if isinstance(model._meta.database, Proxy):\n                model._meta.database.attach_callback(self._db_hook)\n            else:\n                self._db_hook(model._meta.database)\n\n        # Attach a hook to the model metadata; in the event the database is\n        # changed or set at run-time, we will be sure to apply our callback and\n        # use the proper data-type for our database driver.\n        model._meta._db_hooks.append(self._db_hook)\n        return super(BlobField, self).bind(model, name, set_attribute)\n\n    def db_value(self, value):\n        if isinstance(value, text_type):\n            value = value.encode('raw_unicode_escape')\n        if isinstance(value, bytes_type):\n            return self._constructor(value)\n        return value\n\n\nclass BitField(BitwiseMixin, BigIntegerField):\n    def __init__(self, *args, **kwargs):\n        kwargs.setdefault('default', 0)\n        super(BitField, self).__init__(*args, **kwargs)\n        self.__current_flag = 1\n\n    def flag(self, value=None):\n        if value is None:\n            value = self.__current_flag\n            self.__current_flag <<= 1\n        else:\n            self.__current_flag = value << 1\n\n        class FlagDescriptor(ColumnBase):\n            def __init__(self, field, value):\n                self._field = field\n                self._value = value\n                super(FlagDescriptor, self).__init__()\n            def clear(self):\n                return self._field.bin_and(~self._value)\n            def set(self):\n                return self._field.bin_or(self._value)\n            def __get__(self, instance, instance_type=None):\n                if instance is None:\n                    return self\n                value = getattr(instance, self._field.name) or 0\n                return (value & self._value) != 0\n            def __set__(self, instance, is_set):\n                if is_set not in (True, False):\n                    raise ValueError('Value must be either True or False')\n                value = getattr(instance, self._field.name) or 0\n                if is_set:\n                    value |= self._value\n                else:\n                    value &= ~self._value\n                setattr(instance, self._field.name, value)\n            def __sql__(self, ctx):\n                return ctx.sql(self._field.bin_and(self._value) != 0)\n        return FlagDescriptor(self, value)\n\n\nclass BigBitFieldData(object):\n    def __init__(self, instance, name):\n        self.instance = instance\n        self.name = name\n        value = self.instance.__data__.get(self.name)\n        if not value:\n            value = bytearray()\n        elif not isinstance(value, bytearray):\n            value = bytearray(value)\n        self._buffer = self.instance.__data__[self.name] = value\n\n    def clear(self):\n        self._buffer.clear()\n\n    def _ensure_length(self, idx):\n        byte_num, byte_offset = divmod(idx, 8)\n        cur_size = len(self._buffer)\n        if cur_size <= byte_num:\n            self._buffer.extend(b'\\x00' * ((byte_num + 1) - cur_size))\n        return byte_num, byte_offset\n\n    def set_bit(self, idx):\n        byte_num, byte_offset = self._ensure_length(idx)\n        self._buffer[byte_num] |= (1 << byte_offset)\n\n    def clear_bit(self, idx):\n        byte_num, byte_offset = self._ensure_length(idx)\n        self._buffer[byte_num] &= ~(1 << byte_offset)\n\n    def toggle_bit(self, idx):\n        byte_num, byte_offset = self._ensure_length(idx)\n        self._buffer[byte_num] ^= (1 << byte_offset)\n        return bool(self._buffer[byte_num] & (1 << byte_offset))\n\n    def is_set(self, idx):\n        byte_num, byte_offset = divmod(idx, 8)\n        cur_size = len(self._buffer)\n        if cur_size <= byte_num:\n            return False\n        return bool(self._buffer[byte_num] & (1 << byte_offset))\n\n    __getitem__ = is_set\n    def __setitem__(self, item, value):\n        self.set_bit(item) if value else self.clear_bit(item)\n    __delitem__ = clear_bit\n\n    def __len__(self):\n        return len(self._buffer)\n\n    def _get_compatible_data(self, other):\n        if isinstance(other, BigBitFieldData):\n            data = other._buffer\n        elif isinstance(other, (bytes, bytearray, memoryview)):\n            data = other\n        else:\n            raise ValueError('Incompatible data-type')\n        diff = len(data) - len(self)\n        if diff > 0: self._buffer.extend(b'\\x00' * diff)\n        return data\n\n    def _bitwise_op(self, other, op):\n        if isinstance(other, BigBitFieldData):\n            data = other._buffer\n        elif isinstance(other, (bytes, bytearray, memoryview)):\n            data = other\n        else:\n            raise ValueError('Incompatible data-type')\n        buf = bytearray(b'\\x00' * max(len(self), len(other)))\n        it = itertools.zip_longest(self._buffer, data, fillvalue=0)\n        for i, (a, b) in enumerate(it):\n            buf[i] = op(a, b)\n        return buf\n\n    def __and__(self, other):\n        return self._bitwise_op(other, operator.and_)\n    def __or__(self, other):\n        return self._bitwise_op(other, operator.or_)\n    def __xor__(self, other):\n        return self._bitwise_op(other, operator.xor)\n\n    def __iter__(self):\n        for b in self._buffer:\n            for j in range(8):\n                yield 1 if (b & (1 << j)) else 0\n\n    def __repr__(self):\n        return repr(self._buffer)\n    if sys.version_info[0] < 3:\n        def __str__(self):\n            return bytes_type(self._buffer)\n    else:\n        def __bytes__(self):\n            return bytes_type(self._buffer)\n\n\nclass BigBitFieldAccessor(FieldAccessor):\n    def __get__(self, instance, instance_type=None):\n        if instance is None:\n            return self.field\n        return BigBitFieldData(instance, self.name)\n    def __set__(self, instance, value):\n        if isinstance(value, memoryview):\n            value = value.tobytes()\n        elif isinstance(value, buffer_type):\n            value = bytes(value)\n        elif isinstance(value, bytearray):\n            value = bytes_type(value)\n        elif isinstance(value, BigBitFieldData):\n            value = bytes_type(value._buffer)\n        elif isinstance(value, text_type):\n            value = value.encode('utf-8')\n        elif not isinstance(value, bytes_type):\n            raise ValueError('Value must be either a bytes, memoryview or '\n                             'BigBitFieldData instance.')\n        super(BigBitFieldAccessor, self).__set__(instance, value)\n\n\nclass BigBitField(BlobField):\n    accessor_class = BigBitFieldAccessor\n\n    def __init__(self, *args, **kwargs):\n        kwargs.setdefault('default', bytes_type)\n        super(BigBitField, self).__init__(*args, **kwargs)\n\n    def db_value(self, value):\n        return bytes_type(value) if value is not None else value\n\n\nclass UUIDField(Field):\n    field_type = 'UUID'\n\n    def db_value(self, value):\n        if isinstance(value, basestring) and len(value) == 32:\n            # Hex string. No transformation is necessary.\n            return value\n        elif isinstance(value, bytes) and len(value) == 16:\n            # Allow raw binary representation.\n            value = uuid.UUID(bytes=value)\n        if isinstance(value, uuid.UUID):\n            return value.hex\n        try:\n            return uuid.UUID(value).hex\n        except:\n            return value\n\n    def python_value(self, value):\n        if isinstance(value, uuid.UUID):\n            return value\n        return uuid.UUID(value) if value is not None else None\n\n\nclass BinaryUUIDField(BlobField):\n    field_type = 'UUIDB'\n\n    def db_value(self, value):\n        if isinstance(value, bytes) and len(value) == 16:\n            # Raw binary value. No transformation is necessary.\n            return self._constructor(value)\n        elif isinstance(value, basestring) and len(value) == 32:\n            # Allow hex string representation.\n            value = uuid.UUID(hex=value)\n        if isinstance(value, uuid.UUID):\n            return self._constructor(value.bytes)\n        elif value is not None:\n            raise ValueError('value for binary UUID field must be UUID(), '\n                             'a hexadecimal string, or a bytes object.')\n\n    def python_value(self, value):\n        if isinstance(value, uuid.UUID):\n            return value\n        elif isinstance(value, memoryview):\n            value = value.tobytes()\n        elif value and not isinstance(value, bytes):\n            value = bytes(value)\n        return uuid.UUID(bytes=value) if value is not None else None\n\n\ndef _date_part(date_part):\n    def dec(self):\n        return self.model._meta.database.extract_date(date_part, self)\n    return dec\n\ndef format_date_time(value, formats, post_process=None):\n    post_process = post_process or (lambda x: x)\n    for fmt in formats:\n        try:\n            return post_process(datetime.datetime.strptime(value, fmt))\n        except ValueError:\n            pass\n    return value\n\ndef simple_date_time(value):\n    try:\n        return datetime.datetime.strptime(value, '%Y-%m-%d %H:%M:%S')\n    except (TypeError, ValueError):\n        return value\n\n\nclass _BaseFormattedField(Field):\n    formats = None\n\n    def __init__(self, formats=None, *args, **kwargs):\n        if formats is not None:\n            self.formats = formats\n        super(_BaseFormattedField, self).__init__(*args, **kwargs)\n\n\nclass DateTimeField(_BaseFormattedField):\n    field_type = 'DATETIME'\n    formats = [\n        '%Y-%m-%d %H:%M:%S.%f',\n        '%Y-%m-%d %H:%M:%S',\n        '%Y-%m-%d',\n    ]\n\n    def adapt(self, value):\n        if value and isinstance(value, basestring):\n            return format_date_time(value, self.formats)\n        return value\n\n    def to_timestamp(self):\n        return self.model._meta.database.to_timestamp(self)\n\n    def truncate(self, part):\n        return self.model._meta.database.truncate_date(part, self)\n\n    year = property(_date_part('year'))\n    month = property(_date_part('month'))\n    day = property(_date_part('day'))\n    hour = property(_date_part('hour'))\n    minute = property(_date_part('minute'))\n    second = property(_date_part('second'))\n\n\nclass DateField(_BaseFormattedField):\n    field_type = 'DATE'\n    formats = [\n        '%Y-%m-%d',\n        '%Y-%m-%d %H:%M:%S',\n        '%Y-%m-%d %H:%M:%S.%f',\n    ]\n\n    def adapt(self, value):\n        if value and isinstance(value, basestring):\n            pp = lambda x: x.date()\n            return format_date_time(value, self.formats, pp)\n        elif value and isinstance(value, datetime.datetime):\n            return value.date()\n        return value\n\n    def to_timestamp(self):\n        return self.model._meta.database.to_timestamp(self)\n\n    def truncate(self, part):\n        return self.model._meta.database.truncate_date(part, self)\n\n    year = property(_date_part('year'))\n    month = property(_date_part('month'))\n    day = property(_date_part('day'))\n\n\nclass TimeField(_BaseFormattedField):\n    field_type = 'TIME'\n    formats = [\n        '%H:%M:%S.%f',\n        '%H:%M:%S',\n        '%H:%M',\n        '%Y-%m-%d %H:%M:%S.%f',\n        '%Y-%m-%d %H:%M:%S',\n    ]\n\n    def adapt(self, value):\n        if value:\n            if isinstance(value, basestring):\n                pp = lambda x: x.time()\n                return format_date_time(value, self.formats, pp)\n            elif isinstance(value, datetime.datetime):\n                return value.time()\n        if value is not None and isinstance(value, datetime.timedelta):\n            return (datetime.datetime.min + value).time()\n        return value\n\n    hour = property(_date_part('hour'))\n    minute = property(_date_part('minute'))\n    second = property(_date_part('second'))\n\n\ndef _timestamp_date_part(date_part):\n    def dec(self):\n        db = self.model._meta.database\n        expr = ((self / Value(self.resolution, converter=False))\n                if self.resolution > 1 else self)\n        return db.extract_date(date_part, db.from_timestamp(expr))\n    return dec\n\n\nclass TimestampField(BigIntegerField):\n    # Support second -> microsecond resolution.\n    valid_resolutions = [10**i for i in range(7)]\n\n    def __init__(self, *args, **kwargs):\n        self.resolution = kwargs.pop('resolution', None)\n\n        if not self.resolution:\n            self.resolution = 1\n        elif self.resolution in range(2, 7):\n            self.resolution = 10 ** self.resolution\n        elif self.resolution not in self.valid_resolutions:\n            raise ValueError('TimestampField resolution must be one of: %s' %\n                             ', '.join(str(i) for i in self.valid_resolutions))\n        self.ticks_to_microsecond = 1000000 // self.resolution\n\n        self.utc = kwargs.pop('utc', False) or False\n        dflt = utcnow if self.utc else datetime.datetime.now\n        kwargs.setdefault('default', dflt)\n        super(TimestampField, self).__init__(*args, **kwargs)\n\n    def local_to_utc(self, dt):\n        # Convert naive local datetime into naive UTC, e.g.:\n        # 2019-03-01T12:00:00 (local=US/Central) -> 2019-03-01T18:00:00.\n        # 2019-05-01T12:00:00 (local=US/Central) -> 2019-05-01T17:00:00.\n        # 2019-03-01T12:00:00 (local=UTC)        -> 2019-03-01T12:00:00.\n        return datetime.datetime(*time.gmtime(time.mktime(dt.timetuple()))[:6])\n\n    def utc_to_local(self, dt):\n        # Convert a naive UTC datetime into local time, e.g.:\n        # 2019-03-01T18:00:00 (local=US/Central) -> 2019-03-01T12:00:00.\n        # 2019-05-01T17:00:00 (local=US/Central) -> 2019-05-01T12:00:00.\n        # 2019-03-01T12:00:00 (local=UTC)        -> 2019-03-01T12:00:00.\n        ts = calendar.timegm(dt.utctimetuple())\n        return datetime.datetime.fromtimestamp(ts)\n\n    def get_timestamp(self, value):\n        if self.utc:\n            # If utc-mode is on, then we assume all naive datetimes are in UTC.\n            return calendar.timegm(value.utctimetuple())\n        else:\n            return time.mktime(value.timetuple())\n\n    def db_value(self, value):\n        if value is None:\n            return\n\n        if isinstance(value, datetime.datetime):\n            pass\n        elif isinstance(value, datetime.date):\n            value = datetime.datetime(value.year, value.month, value.day)\n        else:\n            return int(round(value * self.resolution))\n\n        timestamp = self.get_timestamp(value)\n        if self.resolution > 1:\n            timestamp += (value.microsecond * .000001)\n            timestamp *= self.resolution\n        return int(round(timestamp))\n\n    def python_value(self, value):\n        if value is not None and isinstance(value, (int, float, long)):\n            if self.resolution > 1:\n                value, ticks = divmod(value, self.resolution)\n                microseconds = int(ticks * self.ticks_to_microsecond)\n            else:\n                microseconds = 0\n\n            if self.utc:\n                value = utcfromtimestamp(value)\n            else:\n                value = datetime.datetime.fromtimestamp(value)\n\n            if microseconds:\n                value = value.replace(microsecond=microseconds)\n\n        return value\n\n    def from_timestamp(self):\n        expr = ((self / Value(self.resolution, converter=False))\n                if self.resolution > 1 else self)\n        return self.model._meta.database.from_timestamp(expr)\n\n    year = property(_timestamp_date_part('year'))\n    month = property(_timestamp_date_part('month'))\n    day = property(_timestamp_date_part('day'))\n    hour = property(_timestamp_date_part('hour'))\n    minute = property(_timestamp_date_part('minute'))\n    second = property(_timestamp_date_part('second'))\n\n\nclass IPField(BigIntegerField):\n    def db_value(self, val):\n        if val is not None:\n            return struct.unpack('!I', socket.inet_aton(val))[0]\n\n    def python_value(self, val):\n        if val is not None:\n            return socket.inet_ntoa(struct.pack('!I', val))\n\n\nclass BooleanField(Field):\n    field_type = 'BOOL'\n    adapt = bool\n\n\nclass BareField(Field):\n    def __init__(self, adapt=None, *args, **kwargs):\n        super(BareField, self).__init__(*args, **kwargs)\n        if adapt is not None:\n            self.adapt = adapt\n\n    def ddl_datatype(self, ctx):\n        return\n\n\nclass ForeignKeyField(Field):\n    accessor_class = ForeignKeyAccessor\n    backref_accessor_class = BackrefAccessor\n\n    def __init__(self, model, field=None, backref=None, on_delete=None,\n                 on_update=None, deferrable=None, _deferred=None,\n                 rel_model=None, to_field=None, object_id_name=None,\n                 lazy_load=True, constraint_name=None, related_name=None,\n                 *args, **kwargs):\n        kwargs.setdefault('index', True)\n\n        super(ForeignKeyField, self).__init__(*args, **kwargs)\n\n        if rel_model is not None:\n            __deprecated__('\"rel_model\" has been deprecated in favor of '\n                           '\"model\" for ForeignKeyField objects.')\n            model = rel_model\n        if to_field is not None:\n            __deprecated__('\"to_field\" has been deprecated in favor of '\n                           '\"field\" for ForeignKeyField objects.')\n            field = to_field\n        if related_name is not None:\n            __deprecated__('\"related_name\" has been deprecated in favor of '\n                           '\"backref\" for Field objects.')\n            backref = related_name\n\n        self._is_self_reference = model == 'self'\n        self.rel_model = model\n        self.rel_field = field\n        self.declared_backref = backref\n        self.backref = None\n        self.on_delete = on_delete\n        self.on_update = on_update\n        self.deferrable = deferrable\n        self.deferred = _deferred\n        self.object_id_name = object_id_name\n        self.lazy_load = lazy_load\n        self.constraint_name = constraint_name\n\n    @property\n    def field_type(self):\n        if not isinstance(self.rel_field, AutoField):\n            return self.rel_field.field_type\n        elif isinstance(self.rel_field, BigAutoField):\n            return BigIntegerField.field_type\n        return IntegerField.field_type\n\n    def get_modifiers(self):\n        if not isinstance(self.rel_field, AutoField):\n            return self.rel_field.get_modifiers()\n        return super(ForeignKeyField, self).get_modifiers()\n\n    def get_constraint_name(self):\n        return self.constraint_name or 'fk_%s_%s_refs_%s' % (\n            self.model._meta.table_name,\n            self.column_name,\n            self.rel_model._meta.table_name)\n\n    def adapt(self, value):\n        return self.rel_field.adapt(value)\n\n    def db_value(self, value):\n        if isinstance(value, self.rel_model):\n            value = getattr(value, self.rel_field.name)\n        return self.rel_field.db_value(value)\n\n    def python_value(self, value):\n        if isinstance(value, self.rel_model):\n            return value\n        return self.rel_field.python_value(value)\n\n    def bind(self, model, name, set_attribute=True):\n        if not self.column_name:\n            self.column_name = name if name.endswith('_id') else name + '_id'\n        if not self.object_id_name:\n            self.object_id_name = self.column_name\n            if self.object_id_name == name:\n                self.object_id_name += '_id'\n        elif self.object_id_name == name:\n            raise ValueError('ForeignKeyField \"%s\".\"%s\" specifies an '\n                             'object_id_name that conflicts with its field '\n                             'name.' % (model._meta.name, name))\n        if self._is_self_reference:\n            self.rel_model = model\n        if isinstance(self.rel_field, basestring):\n            self.rel_field = getattr(self.rel_model, self.rel_field)\n        elif self.rel_field is None:\n            self.rel_field = self.rel_model._meta.primary_key\n\n        # Bind field before assigning backref, so field is bound when\n        # calling declared_backref() (if callable).\n        super(ForeignKeyField, self).bind(model, name, set_attribute)\n        self.safe_name = self.object_id_name\n\n        if callable_(self.declared_backref):\n            self.backref = self.declared_backref(self)\n        else:\n            self.backref, self.declared_backref = self.declared_backref, None\n        if not self.backref:\n            self.backref = '%s_set' % model._meta.name\n\n        if set_attribute:\n            setattr(model, self.object_id_name, ObjectIdAccessor(self))\n            if self.backref not in '!+':\n                setattr(self.rel_model, self.backref,\n                        self.backref_accessor_class(self))\n\n    def foreign_key_constraint(self, explicit_name=False):\n        parts = []\n        if self.constraint_name or explicit_name:\n            name = self.get_constraint_name()\n            parts.extend([\n                SQL('CONSTRAINT'),\n                Entity(_truncate_constraint_name(name))])\n\n        parts.extend([\n            SQL('FOREIGN KEY'),\n            EnclosedNodeList((self,)),\n            SQL('REFERENCES'),\n            self.rel_model,\n            EnclosedNodeList((self.rel_field,))])\n        if self.on_delete:\n            parts.append(SQL('ON DELETE %s' % self.on_delete))\n        if self.on_update:\n            parts.append(SQL('ON UPDATE %s' % self.on_update))\n        if self.deferrable:\n            parts.append(SQL('DEFERRABLE %s' % self.deferrable))\n        return NodeList(parts)\n\n    def __getattr__(self, attr):\n        if attr.startswith('__'):\n            # Prevent recursion error when deep-copying.\n            raise AttributeError('Cannot look-up non-existant \"__\" methods.')\n        if attr in self.rel_model._meta.fields:\n            return self.rel_model._meta.fields[attr]\n        raise AttributeError('Foreign-key has no attribute %s, nor is it a '\n                             'valid field on the related model.' % attr)\n\n\nclass DeferredForeignKey(Field):\n    _unresolved = set()\n\n    def __init__(self, rel_model_name, **kwargs):\n        self.field_kwargs = kwargs\n        self.rel_model_name = rel_model_name.lower()\n        DeferredForeignKey._unresolved.add(self)\n        super(DeferredForeignKey, self).__init__(\n            column_name=kwargs.get('column_name'),\n            null=kwargs.get('null'),\n            primary_key=kwargs.get('primary_key'))\n\n    __hash__ = object.__hash__\n\n    def __deepcopy__(self, memo=None):\n        return DeferredForeignKey(self.rel_model_name, **self.field_kwargs)\n\n    def set_model(self, rel_model):\n        field = ForeignKeyField(rel_model, _deferred=True, **self.field_kwargs)\n        if field.primary_key:\n            # NOTE: this calls add_field() under-the-hood.\n            self.model._meta.set_primary_key(self.name, field)\n        else:\n            self.model._meta.add_field(self.name, field)\n\n    @staticmethod\n    def resolve(model_cls):\n        unresolved = sorted(DeferredForeignKey._unresolved,\n                            key=operator.attrgetter('_order'))\n        for dr in unresolved:\n            if dr.rel_model_name == model_cls.__name__.lower():\n                dr.set_model(model_cls)\n                DeferredForeignKey._unresolved.discard(dr)\n\n\nclass DeferredThroughModel(object):\n    def __init__(self):\n        self._refs = []\n\n    def set_field(self, model, field, name):\n        self._refs.append((model, field, name))\n\n    def set_model(self, through_model):\n        for src_model, m2mfield, name in self._refs:\n            m2mfield.through_model = through_model\n            src_model._meta.add_field(name, m2mfield)\n\n\nclass MetaField(Field):\n    column_name = default = model = name = None\n    primary_key = False\n\n\nclass ManyToManyFieldAccessor(FieldAccessor):\n    def __init__(self, model, field, name):\n        super(ManyToManyFieldAccessor, self).__init__(model, field, name)\n        self.model = field.model\n        self.rel_model = field.rel_model\n        self.through_model = field.through_model\n        src_fks = self.through_model._meta.model_refs[self.model]\n        dest_fks = self.through_model._meta.model_refs[self.rel_model]\n        if not src_fks:\n            raise ValueError('Cannot find foreign-key to \"%s\" on \"%s\" model.' %\n                             (self.model, self.through_model))\n        elif not dest_fks:\n            raise ValueError('Cannot find foreign-key to \"%s\" on \"%s\" model.' %\n                             (self.rel_model, self.through_model))\n        self.src_fk = src_fks[0]\n        self.dest_fk = dest_fks[0]\n\n    def __get__(self, instance, instance_type=None, force_query=False):\n        if instance is not None:\n            if not force_query and self.src_fk.backref != '+':\n                backref = getattr(instance, self.src_fk.backref)\n                if isinstance(backref, list):\n                    return [getattr(obj, self.dest_fk.name) for obj in backref]\n\n            src_id = getattr(instance, self.src_fk.rel_field.name)\n            if src_id is None and self.field._prevent_unsaved:\n                raise ValueError('Cannot get many-to-many \"%s\" for unsaved '\n                                 'instance \"%s\".' % (self.field, instance))\n            return (ManyToManyQuery(instance, self, self.rel_model)\n                    .join(self.through_model)\n                    .join(self.model)\n                    .where(self.src_fk == src_id))\n\n        return self.field\n\n    def __set__(self, instance, value):\n        src_id = getattr(instance, self.src_fk.rel_field.name)\n        if src_id is None and self.field._prevent_unsaved:\n            raise ValueError('Cannot set many-to-many \"%s\" for unsaved '\n                             'instance \"%s\".' % (self.field, instance))\n        query = self.__get__(instance, force_query=True)\n        query.add(value, clear_existing=True)\n\n\nclass ManyToManyField(MetaField):\n    accessor_class = ManyToManyFieldAccessor\n\n    def __init__(self, model, backref=None, through_model=None, on_delete=None,\n                 on_update=None, prevent_unsaved=True, _is_backref=False):\n        if through_model is not None:\n            if not (isinstance(through_model, DeferredThroughModel) or\n                    is_model(through_model)):\n                raise TypeError('Unexpected value for through_model. Expected '\n                                'Model or DeferredThroughModel.')\n            if not _is_backref and (on_delete is not None or on_update is not None):\n                raise ValueError('Cannot specify on_delete or on_update when '\n                                 'through_model is specified.')\n        self.rel_model = model\n        self.backref = backref\n        self._through_model = through_model\n        self._on_delete = on_delete\n        self._on_update = on_update\n        self._prevent_unsaved = prevent_unsaved\n        self._is_backref = _is_backref\n\n    def _get_descriptor(self):\n        return ManyToManyFieldAccessor(self)\n\n    def bind(self, model, name, set_attribute=True):\n        if isinstance(self._through_model, DeferredThroughModel):\n            self._through_model.set_field(model, self, name)\n            return\n\n        super(ManyToManyField, self).bind(model, name, set_attribute)\n\n        if not self._is_backref:\n            many_to_many_field = ManyToManyField(\n                self.model,\n                backref=name,\n                through_model=self.through_model,\n                on_delete=self._on_delete,\n                on_update=self._on_update,\n                _is_backref=True)\n            self.backref = self.backref or model._meta.name + 's'\n            self.rel_model._meta.add_field(self.backref, many_to_many_field)\n\n    def get_models(self):\n        return [model for _, model in sorted((\n            (self._is_backref, self.model),\n            (not self._is_backref, self.rel_model)))]\n\n    @property\n    def through_model(self):\n        if self._through_model is None:\n            self._through_model = self._create_through_model()\n        return self._through_model\n\n    @through_model.setter\n    def through_model(self, value):\n        self._through_model = value\n\n    def _create_through_model(self):\n        lhs, rhs = self.get_models()\n        tables = [model._meta.table_name for model in (lhs, rhs)]\n\n        class Meta:\n            database = self.model._meta.database\n            schema = self.model._meta.schema\n            table_name = '%s_%s_through' % tuple(tables)\n            indexes = (\n                ((lhs._meta.name, rhs._meta.name),\n                 True),)\n\n        params = {'on_delete': self._on_delete, 'on_update': self._on_update}\n        attrs = {\n            lhs._meta.name: ForeignKeyField(lhs, **params),\n            rhs._meta.name: ForeignKeyField(rhs, **params),\n            'Meta': Meta}\n\n        klass_name = '%s%sThrough' % (lhs.__name__, rhs.__name__)\n        return type(klass_name, (Model,), attrs)\n\n    def get_through_model(self):\n        # XXX: Deprecated. Just use the \"through_model\" property.\n        return self.through_model\n\n\nclass VirtualField(MetaField):\n    field_class = None\n\n    def __init__(self, field_class=None, *args, **kwargs):\n        Field = field_class if field_class is not None else self.field_class\n        self.field_instance = Field() if Field is not None else None\n        super(VirtualField, self).__init__(*args, **kwargs)\n\n    def db_value(self, value):\n        if self.field_instance is not None:\n            return self.field_instance.db_value(value)\n        return value\n\n    def python_value(self, value):\n        if self.field_instance is not None:\n            return self.field_instance.python_value(value)\n        return value\n\n    def bind(self, model, name, set_attribute=True):\n        self.model = model\n        self.column_name = self.name = self.safe_name = name\n        setattr(model, name, self.accessor_class(model, self, name))\n\n\nclass CompositeKey(MetaField):\n    sequence = None\n\n    def __init__(self, *field_names):\n        self.field_names = field_names\n        self._safe_field_names = None\n\n    @property\n    def safe_field_names(self):\n        if self._safe_field_names is None:\n            if self.model is None:\n                return self.field_names\n\n            self._safe_field_names = [self.model._meta.fields[f].safe_name\n                                      for f in self.field_names]\n        return self._safe_field_names\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return tuple([getattr(instance, f) for f in self.safe_field_names])\n        return self\n\n    def __set__(self, instance, value):\n        if not isinstance(value, (list, tuple)):\n            raise TypeError('A list or tuple must be used to set the value of '\n                            'a composite primary key.')\n        if len(value) != len(self.field_names):\n            raise ValueError('The length of the value must equal the number '\n                             'of columns of the composite primary key.')\n        for idx, field_value in enumerate(value):\n            setattr(instance, self.field_names[idx], field_value)\n\n    def __eq__(self, other):\n        expressions = [(self.model._meta.fields[field] == value)\n                       for field, value in zip(self.field_names, other)]\n        return reduce(operator.and_, expressions)\n\n    def __ne__(self, other):\n        return ~(self == other)\n\n    def __hash__(self):\n        return hash((self.model.__name__, self.field_names))\n\n    def __sql__(self, ctx):\n        # If the composite PK is being selected, do not use parens. Elsewhere,\n        # such as in an expression, we want to use parentheses and treat it as\n        # a row value.\n        parens = ctx.scope != SCOPE_SOURCE\n        return ctx.sql(NodeList([self.model._meta.fields[field]\n                                 for field in self.field_names], ', ', parens))\n\n    def bind(self, model, name, set_attribute=True):\n        self.model = model\n        self.column_name = self.name = self.safe_name = name\n        setattr(model, self.name, self)\n\n\nclass _SortedFieldList(object):\n    __slots__ = ('_keys', '_items')\n\n    def __init__(self):\n        self._keys = []\n        self._items = []\n\n    def __getitem__(self, i):\n        return self._items[i]\n\n    def __iter__(self):\n        return iter(self._items)\n\n    def __contains__(self, item):\n        k = item._sort_key\n        i = bisect_left(self._keys, k)\n        j = bisect_right(self._keys, k)\n        return item in self._items[i:j]\n\n    def index(self, field):\n        return self._keys.index(field._sort_key)\n\n    def insert(self, item):\n        k = item._sort_key\n        i = bisect_left(self._keys, k)\n        self._keys.insert(i, k)\n        self._items.insert(i, item)\n\n    def remove(self, item):\n        idx = self.index(item)\n        del self._items[idx]\n        del self._keys[idx]\n\n\n# MODELS\n\n\nclass SchemaManager(object):\n    def __init__(self, model, database=None, **context_options):\n        self.model = model\n        self._database = database\n        context_options.setdefault('scope', SCOPE_VALUES)\n        self.context_options = context_options\n\n    @property\n    def database(self):\n        db = self._database or self.model._meta.database\n        if db is None:\n            raise ImproperlyConfigured('database attribute does not appear to '\n                                       'be set on the model: %s' % self.model)\n        return db\n\n    @database.setter\n    def database(self, value):\n        self._database = value\n\n    def _create_context(self):\n        return self.database.get_sql_context(**self.context_options)\n\n    def _create_table(self, safe=True, **options):\n        is_temp = options.pop('temporary', False)\n        ctx = self._create_context()\n        ctx.literal('CREATE TEMPORARY TABLE ' if is_temp else 'CREATE TABLE ')\n        if safe:\n            ctx.literal('IF NOT EXISTS ')\n        ctx.sql(self.model).literal(' ')\n\n        columns = []\n        constraints = []\n        meta = self.model._meta\n        if meta.composite_key:\n            pk_columns = [meta.fields[field_name].column\n                          for field_name in meta.primary_key.field_names]\n            constraints.append(NodeList((SQL('PRIMARY KEY'),\n                                         EnclosedNodeList(pk_columns))))\n\n        for field in meta.sorted_fields:\n            columns.append(field.ddl(ctx))\n            if isinstance(field, ForeignKeyField) and not field.deferred:\n                constraints.append(field.foreign_key_constraint())\n\n        if meta.constraints:\n            constraints.extend(meta.constraints)\n\n        constraints.extend(self._create_table_option_sql(options))\n        ctx.sql(EnclosedNodeList(columns + constraints))\n\n        if meta.table_settings is not None:\n            table_settings = ensure_tuple(meta.table_settings)\n            for setting in table_settings:\n                if not isinstance(setting, basestring):\n                    raise ValueError('table_settings must be strings')\n                ctx.literal(' ').literal(setting)\n\n        extra_opts = []\n        if meta.strict_tables: extra_opts.append('STRICT')\n        if meta.without_rowid: extra_opts.append('WITHOUT ROWID')\n        if extra_opts:\n            ctx.literal(' %s' % ', '.join(extra_opts))\n        return ctx\n\n    def _create_table_option_sql(self, options):\n        accum = []\n        options = merge_dict(self.model._meta.options or {}, options)\n        if not options:\n            return accum\n\n        for key, value in sorted(options.items()):\n            if not isinstance(value, Node):\n                if is_model(value):\n                    value = value._meta.table\n                else:\n                    value = SQL(str(value))\n            accum.append(NodeList((SQL(key), value), glue='='))\n        return accum\n\n    def create_table(self, safe=True, **options):\n        self.database.execute(self._create_table(safe=safe, **options))\n\n    def _create_table_as(self, table_name, query, safe=True, **meta):\n        ctx = (self._create_context()\n               .literal('CREATE TEMPORARY TABLE '\n                        if meta.get('temporary') else 'CREATE TABLE '))\n        if safe:\n            ctx.literal('IF NOT EXISTS ')\n        return (ctx\n                .sql(Entity(*ensure_tuple(table_name)))\n                .literal(' AS ')\n                .sql(query))\n\n    def create_table_as(self, table_name, query, safe=True, **meta):\n        ctx = self._create_table_as(table_name, query, safe=safe, **meta)\n        self.database.execute(ctx)\n\n    def _drop_table(self, safe=True, **options):\n        ctx = (self._create_context()\n               .literal('DROP TABLE IF EXISTS ' if safe else 'DROP TABLE ')\n               .sql(self.model))\n        if options.get('cascade'):\n            ctx = ctx.literal(' CASCADE')\n        elif options.get('restrict'):\n            ctx = ctx.literal(' RESTRICT')\n        return ctx\n\n    def drop_table(self, safe=True, **options):\n        self.database.execute(self._drop_table(safe=safe, **options))\n\n    def _truncate_table(self, restart_identity=False, cascade=False):\n        db = self.database\n        if not db.truncate_table:\n            return (self._create_context()\n                    .literal('DELETE FROM ').sql(self.model))\n\n        ctx = self._create_context().literal('TRUNCATE TABLE ').sql(self.model)\n        if restart_identity:\n            ctx = ctx.literal(' RESTART IDENTITY')\n        if cascade:\n            ctx = ctx.literal(' CASCADE')\n        return ctx\n\n    def truncate_table(self, restart_identity=False, cascade=False):\n        self.database.execute(self._truncate_table(restart_identity, cascade))\n\n    def _create_indexes(self, safe=True):\n        return [self._create_index(index, safe)\n                for index in self.model._meta.fields_to_index()]\n\n    def _create_index(self, index, safe=True):\n        if isinstance(index, Index):\n            if not self.database.safe_create_index:\n                index = index.safe(False)\n            elif index._safe != safe:\n                index = index.safe(safe)\n            if isinstance(self._database, SqliteDatabase):\n                # Ensure we do not use value placeholders with Sqlite, as they\n                # are not supported.\n                index = ValueLiterals(index)\n        return self._create_context().sql(index)\n\n    def create_indexes(self, safe=True):\n        for query in self._create_indexes(safe=safe):\n            self.database.execute(query)\n\n    def _drop_indexes(self, safe=True):\n        return [self._drop_index(index, safe)\n                for index in self.model._meta.fields_to_index()\n                if isinstance(index, Index)]\n\n    def _drop_index(self, index, safe):\n        statement = 'DROP INDEX '\n        if safe and self.database.safe_drop_index:\n            statement += 'IF EXISTS '\n        if isinstance(index._table, Table) and index._table._schema:\n            index_name = Entity(index._table._schema, index._name)\n        else:\n            index_name = Entity(index._name)\n        return (self\n                ._create_context()\n                .literal(statement)\n                .sql(index_name))\n\n    def drop_indexes(self, safe=True):\n        for query in self._drop_indexes(safe=safe):\n            self.database.execute(query)\n\n    def _check_sequences(self, field):\n        if not field.sequence or not self.database.sequences:\n            raise ValueError('Sequences are either not supported, or are not '\n                             'defined for \"%s\".' % field.name)\n\n    def _sequence_for_field(self, field):\n        if field.model._meta.schema:\n            return Entity(field.model._meta.schema, field.sequence)\n        else:\n            return Entity(field.sequence)\n\n    def _create_sequence(self, field):\n        self._check_sequences(field)\n        if not self.database.sequence_exists(field.sequence):\n            return (self\n                    ._create_context()\n                    .literal('CREATE SEQUENCE ')\n                    .sql(self._sequence_for_field(field)))\n\n    def create_sequence(self, field):\n        seq_ctx = self._create_sequence(field)\n        if seq_ctx is not None:\n            self.database.execute(seq_ctx)\n\n    def _drop_sequence(self, field):\n        self._check_sequences(field)\n        if self.database.sequence_exists(field.sequence):\n            return (self\n                    ._create_context()\n                    .literal('DROP SEQUENCE ')\n                    .sql(self._sequence_for_field(field)))\n\n    def drop_sequence(self, field):\n        seq_ctx = self._drop_sequence(field)\n        if seq_ctx is not None:\n            self.database.execute(seq_ctx)\n\n    def _create_foreign_key(self, field):\n        return (self\n                ._create_context()\n                .literal('ALTER TABLE ')\n                .sql(field.model)\n                .literal(' ADD ')\n                .sql(field.foreign_key_constraint(True)))\n\n    def create_foreign_key(self, field):\n        self.database.execute(self._create_foreign_key(field))\n\n    def create_sequences(self):\n        if self.database.sequences:\n            for field in self.model._meta.sorted_fields:\n                if field.sequence:\n                    self.create_sequence(field)\n\n    def create_all(self, safe=True, **table_options):\n        self.create_sequences()\n        self.create_table(safe, **table_options)\n        self.create_indexes(safe=safe)\n\n    def drop_sequences(self):\n        if self.database.sequences:\n            for field in self.model._meta.sorted_fields:\n                if field.sequence:\n                    self.drop_sequence(field)\n\n    def drop_all(self, safe=True, drop_sequences=True, **options):\n        self.drop_table(safe, **options)\n        if drop_sequences:\n            self.drop_sequences()\n\n\nclass Metadata(object):\n    def __init__(self, model, database=None, table_name=None, indexes=None,\n                 primary_key=None, constraints=None, schema=None,\n                 only_save_dirty=False, depends_on=None, options=None,\n                 db_table=None, table_function=None, table_settings=None,\n                 without_rowid=False, temporary=False, strict_tables=None,\n                 legacy_table_names=True, **kwargs):\n        if db_table is not None:\n            __deprecated__('\"db_table\" has been deprecated in favor of '\n                           '\"table_name\" for Models.')\n            table_name = db_table\n        self.model = model\n        self.database = database\n\n        self.fields = {}\n        self.columns = {}\n        self.combined = {}\n\n        self._sorted_field_list = _SortedFieldList()\n        self.sorted_fields = []\n        self.sorted_field_names = []\n\n        self.defaults = {}\n        self._default_by_name = {}\n        self._default_dict = {}\n        self._default_callables = {}\n        self._default_callable_list = []\n\n        self.name = model.__name__.lower()\n        self.table_function = table_function\n        self.legacy_table_names = legacy_table_names\n        if not table_name:\n            table_name = (self.table_function(model)\n                          if self.table_function\n                          else self.make_table_name())\n        self.table_name = table_name\n        self._table = None\n\n        self.indexes = list(indexes) if indexes else []\n        self.constraints = constraints\n        self._schema = schema\n        self.primary_key = primary_key\n        self.composite_key = self.auto_increment = None\n        self.only_save_dirty = only_save_dirty\n        self.depends_on = depends_on\n        self.table_settings = table_settings\n        self.without_rowid = without_rowid\n        self.strict_tables = strict_tables\n        self.temporary = temporary\n\n        self.refs = {}\n        self.backrefs = {}\n        self.model_refs = collections.defaultdict(list)\n        self.model_backrefs = collections.defaultdict(list)\n        self.manytomany = {}\n\n        self.options = options or {}\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n        self._additional_keys = set(kwargs.keys())\n\n        # Allow objects to register hooks that are called if the model is bound\n        # to a different database. For example, BlobField uses a different\n        # Python data-type depending on the db driver / python version. When\n        # the database changes, we need to update any BlobField so they can use\n        # the appropriate data-type.\n        self._db_hooks = []\n\n    def make_table_name(self):\n        if self.legacy_table_names:\n            return re.sub(r'[^\\w]+', '_', self.name)\n        return make_snake_case(self.model.__name__)\n\n    def model_graph(self, refs=True, backrefs=True, depth_first=True):\n        if not refs and not backrefs:\n            raise ValueError('One of `refs` or `backrefs` must be True.')\n\n        accum = [(None, self.model, None)]\n        seen = set()\n        queue = collections.deque((self,))\n        method = queue.pop if depth_first else queue.popleft\n\n        while queue:\n            curr = method()\n            if curr in seen: continue\n            seen.add(curr)\n\n            if refs:\n                for fk, model in curr.refs.items():\n                    accum.append((fk, model, False))\n                    queue.append(model._meta)\n            if backrefs:\n                for fk, model in curr.backrefs.items():\n                    accum.append((fk, model, True))\n                    queue.append(model._meta)\n\n        return accum\n\n    def add_ref(self, field):\n        rel = field.rel_model\n        self.refs[field] = rel\n        self.model_refs[rel].append(field)\n        rel._meta.backrefs[field] = self.model\n        rel._meta.model_backrefs[self.model].append(field)\n\n    def remove_ref(self, field):\n        rel = field.rel_model\n        del self.refs[field]\n        self.model_refs[rel].remove(field)\n        del rel._meta.backrefs[field]\n        rel._meta.model_backrefs[self.model].remove(field)\n\n    def add_manytomany(self, field):\n        self.manytomany[field.name] = field\n\n    def remove_manytomany(self, field):\n        del self.manytomany[field.name]\n\n    @property\n    def table(self):\n        if self._table is None:\n            self._table = Table(\n                self.table_name,\n                [field.column_name for field in self.sorted_fields],\n                schema=self.schema,\n                _model=self.model,\n                _database=self.database)\n        return self._table\n\n    @table.setter\n    def table(self, value):\n        raise AttributeError('Cannot set the \"table\".')\n\n    @table.deleter\n    def table(self):\n        self._table = None\n\n    @property\n    def schema(self):\n        return self._schema\n\n    @schema.setter\n    def schema(self, value):\n        self._schema = value\n        del self.table\n\n    @property\n    def entity(self):\n        if self._schema:\n            return Entity(self._schema, self.table_name)\n        else:\n            return Entity(self.table_name)\n\n    def _update_sorted_fields(self):\n        self.sorted_fields = list(self._sorted_field_list)\n        self.sorted_field_names = [f.name for f in self.sorted_fields]\n\n    def get_rel_for_model(self, model):\n        if isinstance(model, ModelAlias):\n            model = model.model\n        forwardrefs = self.model_refs.get(model, [])\n        backrefs = self.model_backrefs.get(model, [])\n        return (forwardrefs, backrefs)\n\n    def add_field(self, field_name, field, set_attribute=True):\n        if field_name in self.fields:\n            self.remove_field(field_name)\n        elif field_name in self.manytomany:\n            self.remove_manytomany(self.manytomany[field_name])\n\n        if not isinstance(field, MetaField):\n            del self.table\n            field.bind(self.model, field_name, set_attribute)\n            self.fields[field.name] = field\n            self.columns[field.column_name] = field\n            self.combined[field.name] = field\n            self.combined[field.column_name] = field\n\n            self._sorted_field_list.insert(field)\n            self._update_sorted_fields()\n\n            if field.default is not None:\n                # This optimization helps speed up model instance construction.\n                self.defaults[field] = field.default\n                if callable_(field.default):\n                    self._default_callables[field] = field.default\n                    self._default_callable_list.append((field.name,\n                                                        field.default))\n                else:\n                    self._default_dict[field] = field.default\n                    self._default_by_name[field.name] = field.default\n        else:\n            field.bind(self.model, field_name, set_attribute)\n\n        if isinstance(field, ForeignKeyField):\n            self.add_ref(field)\n        elif isinstance(field, ManyToManyField) and field.name:\n            self.add_manytomany(field)\n\n    def remove_field(self, field_name):\n        if field_name not in self.fields:\n            return\n\n        del self.table\n        original = self.fields.pop(field_name)\n        del self.columns[original.column_name]\n        del self.combined[field_name]\n        try:\n            del self.combined[original.column_name]\n        except KeyError:\n            pass\n        self._sorted_field_list.remove(original)\n        self._update_sorted_fields()\n\n        if original.default is not None:\n            del self.defaults[original]\n            if self._default_callables.pop(original, None):\n                for i, (name, _) in enumerate(self._default_callable_list):\n                    if name == field_name:\n                        self._default_callable_list.pop(i)\n                        break\n            else:\n                self._default_dict.pop(original, None)\n                self._default_by_name.pop(original.name, None)\n\n        if isinstance(original, ForeignKeyField):\n            self.remove_ref(original)\n\n    def set_primary_key(self, name, field):\n        self.composite_key = isinstance(field, CompositeKey)\n        self.add_field(name, field)\n        self.primary_key = field\n        self.auto_increment = (\n            field.auto_increment or\n            bool(field.sequence))\n\n    def get_primary_keys(self):\n        if self.composite_key:\n            return tuple([self.fields[field_name]\n                          for field_name in self.primary_key.field_names])\n        else:\n            return (self.primary_key,) if self.primary_key is not False else ()\n\n    def get_default_dict(self):\n        dd = self._default_by_name.copy()\n        for field_name, default in self._default_callable_list:\n            dd[field_name] = default()\n        return dd\n\n    def fields_to_index(self):\n        indexes = []\n        for f in self.sorted_fields:\n            if f.primary_key:\n                continue\n            if f.index or f.unique:\n                indexes.append(ModelIndex(self.model, (f,), unique=f.unique,\n                                          using=f.index_type))\n\n        for index_obj in self.indexes:\n            if isinstance(index_obj, Node):\n                indexes.append(index_obj)\n            elif isinstance(index_obj, (list, tuple)):\n                index_parts, unique = index_obj\n                fields = []\n                for part in index_parts:\n                    if isinstance(part, basestring):\n                        fields.append(self.combined[part])\n                    elif isinstance(part, Node):\n                        fields.append(part)\n                    else:\n                        raise ValueError('Expected either a field name or a '\n                                         'subclass of Node. Got: %s' % part)\n                indexes.append(ModelIndex(self.model, fields, unique=unique))\n\n        return indexes\n\n    def set_database(self, database):\n        self.database = database\n        self.model._schema._database = database\n        del self.table\n\n        # Apply any hooks that have been registered. If we have an\n        # uninitialized proxy object, we will treat that as `None`.\n        if isinstance(database, Proxy) and database.obj is None:\n            database = None\n\n        for hook in self._db_hooks:\n            hook(database)\n\n    def set_table_name(self, table_name):\n        self.table_name = table_name\n        del self.table\n\n\nclass SubclassAwareMetadata(Metadata):\n    models = []\n\n    def __init__(self, model, *args, **kwargs):\n        super(SubclassAwareMetadata, self).__init__(model, *args, **kwargs)\n        self.models.append(model)\n\n    def map_models(self, fn):\n        for model in self.models:\n            fn(model)\n\n\nclass DoesNotExist(Exception): pass\n\n\nclass ModelBase(type):\n    inheritable = set(['constraints', 'database', 'indexes', 'primary_key',\n                       'options', 'schema', 'table_function', 'temporary',\n                       'only_save_dirty', 'legacy_table_names',\n                       'table_settings', 'strict_tables'])\n\n    def __new__(cls, name, bases, attrs, **kwargs):\n        if name == MODEL_BASE or bases[0].__name__ == MODEL_BASE:\n            return super(ModelBase, cls).__new__(cls, name, bases, attrs,\n                                                 **kwargs)\n\n        meta_options = {}\n        meta = attrs.pop('Meta', None)\n        if meta:\n            for k, v in meta.__dict__.items():\n                if not k.startswith('_'):\n                    meta_options[k] = v\n\n        pk = getattr(meta, 'primary_key', None)\n        pk_name = parent_pk = None\n\n        # Inherit any field descriptors by deep copying the underlying field\n        # into the attrs of the new model, additionally see if the bases define\n        # inheritable model options and swipe them.\n        for b in bases:\n            if not hasattr(b, '_meta'):\n                continue\n\n            base_meta = b._meta\n            if parent_pk is None:\n                parent_pk = deepcopy(base_meta.primary_key)\n            all_inheritable = cls.inheritable | base_meta._additional_keys\n            for k in base_meta.__dict__:\n                if k in all_inheritable and k not in meta_options:\n                    meta_options[k] = base_meta.__dict__[k]\n            meta_options.setdefault('database', base_meta.database)\n            meta_options.setdefault('schema', base_meta.schema)\n\n            for (k, v) in b.__dict__.items():\n                if k in attrs: continue\n\n                if isinstance(v, FieldAccessor) and not v.field.primary_key:\n                    attrs[k] = deepcopy(v.field)\n\n        sopts = meta_options.pop('schema_options', None) or {}\n        Meta = meta_options.get('model_metadata_class', Metadata)\n        Schema = meta_options.get('schema_manager_class', SchemaManager)\n\n        # Construct the new class.\n        cls = super(ModelBase, cls).__new__(cls, name, bases, attrs, **kwargs)\n        cls.__data__ = cls.__rel__ = None\n\n        cls._meta = Meta(cls, **meta_options)\n        cls._schema = Schema(cls, **sopts)\n\n        fields = []\n        for key, value in cls.__dict__.items():\n            if isinstance(value, Field):\n                if value.primary_key and pk:\n                    raise ValueError('over-determined primary key %s.' % name)\n                elif value.primary_key:\n                    pk, pk_name = value, key\n                else:\n                    fields.append((key, value))\n\n        if pk is None:\n            if parent_pk is not False:\n                pk, pk_name = ((parent_pk, parent_pk.name)\n                               if parent_pk is not None else\n                               (AutoField(), 'id'))\n            else:\n                pk = False\n        elif isinstance(pk, CompositeKey):\n            pk_name = '__composite_key__'\n            cls._meta.composite_key = True\n\n        if pk is not False:\n            cls._meta.set_primary_key(pk_name, pk)\n\n        for name, field in fields:\n            cls._meta.add_field(name, field)\n\n        # Create a repr and error class before finalizing.\n        if hasattr(cls, '__str__') and '__repr__' not in attrs:\n            setattr(cls, '__repr__', lambda self: '<%s: %s>' % (\n                cls.__name__, self.__str__()))\n\n        exc_name = '%sDoesNotExist' % cls.__name__\n        exc_attrs = {'__module__': cls.__module__}\n        exception_class = type(exc_name, (DoesNotExist,), exc_attrs)\n        cls.DoesNotExist = exception_class\n\n        # Call validation hook, allowing additional model validation.\n        cls.validate_model()\n        DeferredForeignKey.resolve(cls)\n        return cls\n\n    def __repr__(self):\n        return '<Model: %s>' % self.__name__\n\n    def __iter__(self):\n        return iter(self.select())\n\n    def __getitem__(self, key):\n        return self.get_by_id(key)\n\n    def __setitem__(self, key, value):\n        self.set_by_id(key, value)\n\n    def __delitem__(self, key):\n        self.delete_by_id(key)\n\n    def __contains__(self, key):\n        try:\n            self.get_by_id(key)\n        except self.DoesNotExist:\n            return False\n        else:\n            return True\n\n    def __len__(self):\n        return self.select().count()\n    def __bool__(self): return True\n    __nonzero__ = __bool__  # Python 2.\n\n    def __sql__(self, ctx):\n        return ctx.sql(self._meta.table)\n\n\nclass _BoundModelsContext(object):\n    def __init__(self, models, database, bind_refs, bind_backrefs):\n        self.models = models\n        self.database = database\n        self.bind_refs = bind_refs\n        self.bind_backrefs = bind_backrefs\n\n    def __enter__(self):\n        self._orig_database = []\n        for model in self.models:\n            self._orig_database.append(model._meta.database)\n            model.bind(self.database, self.bind_refs, self.bind_backrefs,\n                       _exclude=set(self.models))\n        return self.models\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        for model, db in zip(self.models, self._orig_database):\n            model.bind(db, self.bind_refs, self.bind_backrefs,\n                       _exclude=set(self.models))\n\n\nclass Model(with_metaclass(ModelBase, Node)):\n    def __init__(self, *args, **kwargs):\n        if kwargs.pop('__no_default__', None):\n            self.__data__ = {}\n        else:\n            self.__data__ = self._meta.get_default_dict()\n        self._dirty = set(self.__data__)\n        self.__rel__ = {}\n\n        for k in kwargs:\n            setattr(self, k, kwargs[k])\n\n    def __str__(self):\n        return str(self._pk) if self._meta.primary_key is not False else 'n/a'\n\n    @classmethod\n    def validate_model(cls):\n        pass\n\n    @classmethod\n    def alias(cls, alias=None):\n        return ModelAlias(cls, alias)\n\n    @classmethod\n    def select(cls, *fields):\n        is_default = not fields\n        if not fields:\n            fields = cls._meta.sorted_fields\n        return ModelSelect(cls, fields, is_default=is_default)\n\n    @classmethod\n    def _normalize_data(cls, data, kwargs):\n        normalized = {}\n        if data:\n            if not isinstance(data, dict):\n                if kwargs:\n                    raise ValueError('Data cannot be mixed with keyword '\n                                     'arguments: %s' % data)\n                return data\n            for key in data:\n                try:\n                    field = (key if isinstance(key, Field)\n                             else cls._meta.combined[key])\n                except KeyError:\n                    if not isinstance(key, Node):\n                        raise ValueError('Unrecognized field name: \"%s\" in %s.'\n                                         % (key, data))\n                    field = key\n                normalized[field] = data[key]\n        if kwargs:\n            for key in kwargs:\n                try:\n                    normalized[cls._meta.combined[key]] = kwargs[key]\n                except KeyError:\n                    normalized[getattr(cls, key)] = kwargs[key]\n        return normalized\n\n    @classmethod\n    def update(cls, __data=None, **update):\n        return ModelUpdate(cls, cls._normalize_data(__data, update))\n\n    @classmethod\n    def insert(cls, __data=None, **insert):\n        return ModelInsert(cls, cls._normalize_data(__data, insert))\n\n    @classmethod\n    def insert_many(cls, rows, fields=None):\n        return ModelInsert(cls, insert=rows, columns=fields)\n\n    @classmethod\n    def insert_from(cls, query, fields):\n        columns = [getattr(cls, field) if isinstance(field, basestring)\n                   else field for field in fields]\n        return ModelInsert(cls, insert=query, columns=columns)\n\n    @classmethod\n    def replace(cls, __data=None, **insert):\n        return cls.insert(__data, **insert).on_conflict('REPLACE')\n\n    @classmethod\n    def replace_many(cls, rows, fields=None):\n        return (cls\n                .insert_many(rows=rows, fields=fields)\n                .on_conflict('REPLACE'))\n\n    @classmethod\n    def raw(cls, sql, *params):\n        return ModelRaw(cls, sql, params)\n\n    @classmethod\n    def delete(cls):\n        return ModelDelete(cls)\n\n    @classmethod\n    def create(cls, **query):\n        inst = cls(**query)\n        inst.save(force_insert=True)\n        return inst\n\n    @classmethod\n    def bulk_create(cls, model_list, batch_size=None):\n        if batch_size is not None:\n            batches = chunked(model_list, batch_size)\n        else:\n            batches = [model_list]\n\n        field_names = list(cls._meta.sorted_field_names)\n        if cls._meta.auto_increment:\n            pk_name = cls._meta.primary_key.name\n            field_names.remove(pk_name)\n\n        if cls._meta.database.returning_clause and \\\n           cls._meta.primary_key is not False:\n            pk_fields = cls._meta.get_primary_keys()\n        else:\n            pk_fields = None\n\n        fields = [cls._meta.fields[field_name] for field_name in field_names]\n        attrs = []\n        for field in fields:\n            if isinstance(field, ForeignKeyField):\n                attrs.append(field.object_id_name)\n            else:\n                attrs.append(field.name)\n\n        for batch in batches:\n            accum = ([getattr(model, f) for f in attrs]\n                     for model in batch)\n            res = cls.insert_many(accum, fields=fields).execute()\n            if pk_fields and res is not None:\n                for row, model in zip(res, batch):\n                    for (pk_field, obj_id) in zip(pk_fields, row):\n                        setattr(model, pk_field.name, obj_id)\n\n    @classmethod\n    def bulk_update(cls, model_list, fields, batch_size=None):\n        if isinstance(cls._meta.primary_key, CompositeKey):\n            raise ValueError('bulk_update() is not supported for models with '\n                             'a composite primary key.')\n\n        # First normalize list of fields so all are field instances.\n        fields = [cls._meta.fields[f] if isinstance(f, basestring) else f\n                  for f in fields]\n        # Now collect list of attribute names to use for values.\n        attrs = [field.object_id_name if isinstance(field, ForeignKeyField)\n                 else field.name for field in fields]\n\n        if batch_size is not None:\n            batches = chunked(model_list, batch_size)\n        else:\n            batches = [model_list]\n\n        n = 0\n        pk = cls._meta.primary_key\n\n        for batch in batches:\n            id_list = [model._pk for model in batch]\n            update = {}\n            for field, attr in zip(fields, attrs):\n                accum = []\n                for model in batch:\n                    value = getattr(model, attr)\n                    if not isinstance(value, Node):\n                        value = field.to_value(value)\n                    accum.append((pk.to_value(model._pk), value))\n                case = Case(pk, accum)\n                update[field] = case\n\n            n += (cls.update(update)\n                  .where(cls._meta.primary_key.in_(id_list))\n                  .execute())\n        return n\n\n    @classmethod\n    def noop(cls):\n        return NoopModelSelect(cls, ())\n\n    @classmethod\n    def get(cls, *query, **filters):\n        sq = cls.select()\n        if query:\n            # Handle simple lookup using just the primary key.\n            if len(query) == 1 and isinstance(query[0], int):\n                sq = sq.where(cls._meta.primary_key == query[0])\n            else:\n                sq = sq.where(*query)\n        if filters:\n            sq = sq.filter(**filters)\n        return sq.get()\n\n    @classmethod\n    def get_or_none(cls, *query, **filters):\n        try:\n            return cls.get(*query, **filters)\n        except DoesNotExist:\n            pass\n\n    @classmethod\n    def get_by_id(cls, pk):\n        return cls.get(cls._meta.primary_key == pk)\n\n    @classmethod\n    def set_by_id(cls, key, value):\n        if key is None:\n            return cls.insert(value).execute()\n        else:\n            return (cls.update(value)\n                    .where(cls._meta.primary_key == key).execute())\n\n    @classmethod\n    def delete_by_id(cls, pk):\n        return cls.delete().where(cls._meta.primary_key == pk).execute()\n\n    @classmethod\n    def get_or_create(cls, **kwargs):\n        defaults = kwargs.pop('defaults', {})\n        query = cls.select()\n        for field, value in kwargs.items():\n            query = query.where(getattr(cls, field) == value)\n\n        try:\n            return query.get(), False\n        except cls.DoesNotExist:\n            try:\n                if defaults:\n                    kwargs.update(defaults)\n                with cls._meta.database.atomic():\n                    return cls.create(**kwargs), True\n            except IntegrityError as exc:\n                try:\n                    return query.get(), False\n                except cls.DoesNotExist:\n                    raise exc\n\n    @classmethod\n    def filter(cls, *dq_nodes, **filters):\n        return cls.select().filter(*dq_nodes, **filters)\n\n    def get_id(self):\n        # Using getattr(self, pk-name) could accidentally trigger a query if\n        # the primary-key is a foreign-key. So we use the safe_name attribute,\n        # which defaults to the field-name, but will be the object_id_name for\n        # foreign-key fields.\n        if self._meta.primary_key is not False:\n            return getattr(self, self._meta.primary_key.safe_name)\n\n    _pk = property(get_id)\n\n    @_pk.setter\n    def _pk(self, value):\n        setattr(self, self._meta.primary_key.name, value)\n\n    def _pk_expr(self):\n        return self._meta.primary_key == self._pk\n\n    def _prune_fields(self, field_dict, only):\n        new_data = {}\n        for field in only:\n            if isinstance(field, basestring):\n                field = self._meta.combined[field]\n            if field.name in field_dict:\n                new_data[field.name] = field_dict[field.name]\n        return new_data\n\n    def _populate_unsaved_relations(self, field_dict):\n        for foreign_key_field in self._meta.refs:\n            foreign_key = foreign_key_field.name\n            conditions = (\n                foreign_key in field_dict and\n                field_dict[foreign_key] is None and\n                self.__rel__.get(foreign_key) is not None)\n            if conditions:\n                setattr(self, foreign_key, getattr(self, foreign_key))\n                field_dict[foreign_key] = self.__data__[foreign_key]\n\n    def save(self, force_insert=False, only=None):\n        field_dict = self.__data__.copy()\n        if self._meta.primary_key is not False:\n            pk_field = self._meta.primary_key\n            pk_value = self._pk\n        else:\n            pk_field = pk_value = None\n        if only is not None:\n            field_dict = self._prune_fields(field_dict, only)\n        elif self._meta.only_save_dirty and not force_insert:\n            field_dict = self._prune_fields(field_dict, self.dirty_fields)\n            if not field_dict:\n                self._dirty.clear()\n                return False\n\n        self._populate_unsaved_relations(field_dict)\n        rows = 1\n\n        if self._meta.auto_increment and pk_value is None:\n            field_dict.pop(pk_field.name, None)\n\n        if pk_value is not None and not force_insert:\n            if self._meta.composite_key:\n                for pk_part_name in pk_field.field_names:\n                    field_dict.pop(pk_part_name, None)\n            else:\n                field_dict.pop(pk_field.name, None)\n            if not field_dict:\n                raise ValueError('no data to save!')\n            rows = self.update(**field_dict).where(self._pk_expr()).execute()\n        elif pk_field is not None:\n            pk = self.insert(**field_dict).execute()\n            if pk is not None and (self._meta.auto_increment or\n                                   pk_value is None):\n                self._pk = pk\n                # Although we set the primary-key, do not mark it as dirty.\n                self._dirty.discard(pk_field.name)\n        else:\n            self.insert(**field_dict).execute()\n\n        self._dirty -= set(field_dict)  # Remove any fields we saved.\n        return rows\n\n    def is_dirty(self):\n        return bool(self._dirty)\n\n    @property\n    def dirty_fields(self):\n        return [f for f in self._meta.sorted_fields if f.name in self._dirty]\n\n    def dependencies(self, search_nullable=True, exclude_null_children=False):\n        model_class = type(self)\n        stack = [(type(self), None)]\n        queries = {}\n        seen = set()\n\n        while stack:\n            klass, query = stack.pop()\n            if klass in seen:\n                continue\n            seen.add(klass)\n            for fk, rel_model in klass._meta.backrefs.items():\n                if rel_model is model_class or query is None:\n                    node = (fk == self.__data__[fk.rel_field.name])\n                else:\n                    node = fk << query\n                subquery = (rel_model.select(rel_model._meta.primary_key)\n                            .where(node))\n                if not fk.null or search_nullable:\n                    queries.setdefault(rel_model, []).append((node, fk))\n                    if fk.null and exclude_null_children:\n                        # Do not process additional children of this node, but\n                        # include it in the list of dependencies.\n                        seen.add(rel_model)\n                    else:\n                        stack.append((rel_model, subquery))\n\n        for m in reversed(sort_models(seen)):\n            for sq, q in queries.get(m, ()):\n                yield sq, q\n\n    def delete_instance(self, recursive=False, delete_nullable=False):\n        if recursive:\n            for query, fk in self.dependencies(exclude_null_children=not delete_nullable):\n                model = fk.model\n                if fk.null and not delete_nullable:\n                    model.update(**{fk.name: None}).where(query).execute()\n                else:\n                    model.delete().where(query).execute()\n        return type(self).delete().where(self._pk_expr()).execute()\n\n    def __hash__(self):\n        return hash((self.__class__, self._pk))\n\n    def __eq__(self, other):\n        return (\n            other.__class__ == self.__class__ and\n            self._pk is not None and\n            self._pk == other._pk)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __sql__(self, ctx):\n        # NOTE: when comparing a foreign-key field whose related-field is not a\n        # primary-key, then doing an equality test for the foreign-key with a\n        # model instance will return the wrong value; since we would return\n        # the primary key for a given model instance.\n        #\n        # This checks to see if we have a converter in the scope, and that we\n        # are converting a foreign-key expression. If so, we hand the model\n        # instance to the converter rather than blindly grabbing the primary-\n        # key. In the event the provided converter fails to handle the model\n        # instance, then we will return the primary-key.\n        if ctx.state.converter is not None and ctx.state.is_fk_expr:\n            try:\n                return ctx.sql(Value(self, converter=ctx.state.converter))\n            except (TypeError, ValueError):\n                pass\n\n        return ctx.sql(Value(getattr(self, self._meta.primary_key.name),\n                             converter=self._meta.primary_key.db_value))\n\n    @classmethod\n    def bind(cls, database, bind_refs=True, bind_backrefs=True, _exclude=None):\n        is_different = cls._meta.database is not database\n        cls._meta.set_database(database)\n        if bind_refs or bind_backrefs:\n            if _exclude is None:\n                _exclude = set()\n            G = cls._meta.model_graph(refs=bind_refs, backrefs=bind_backrefs)\n            for _, model, is_backref in G:\n                if model not in _exclude:\n                    model._meta.set_database(database)\n                    _exclude.add(model)\n        return is_different\n\n    @classmethod\n    def bind_ctx(cls, database, bind_refs=True, bind_backrefs=True):\n        return _BoundModelsContext((cls,), database, bind_refs, bind_backrefs)\n\n    @classmethod\n    def table_exists(cls):\n        M = cls._meta\n        return cls._schema.database.table_exists(M.table.__name__, M.schema)\n\n    @classmethod\n    def create_table(cls, safe=True, **options):\n        if 'fail_silently' in options:\n            __deprecated__('\"fail_silently\" has been deprecated in favor of '\n                           '\"safe\" for the create_table() method.')\n            safe = options.pop('fail_silently')\n\n        if safe and not cls._schema.database.safe_create_index \\\n           and cls.table_exists():\n            return\n        if cls._meta.temporary:\n            options.setdefault('temporary', cls._meta.temporary)\n        cls._schema.create_all(safe, **options)\n\n    @classmethod\n    def drop_table(cls, safe=True, drop_sequences=True, **options):\n        if safe and not cls._schema.database.safe_drop_index \\\n           and not cls.table_exists():\n            return\n        if cls._meta.temporary:\n            options.setdefault('temporary', cls._meta.temporary)\n        cls._schema.drop_all(safe, drop_sequences, **options)\n\n    @classmethod\n    def truncate_table(cls, **options):\n        cls._schema.truncate_table(**options)\n\n    @classmethod\n    def index(cls, *fields, **kwargs):\n        return ModelIndex(cls, fields, **kwargs)\n\n    @classmethod\n    def add_index(cls, *fields, **kwargs):\n        if len(fields) == 1 and isinstance(fields[0], (SQL, Index)):\n            cls._meta.indexes.append(fields[0])\n        else:\n            cls._meta.indexes.append(ModelIndex(cls, fields, **kwargs))\n\n\nclass ModelAlias(Node):\n    \"\"\"Provide a separate reference to a model in a query.\"\"\"\n    def __init__(self, model, alias=None):\n        self.__dict__['model'] = model\n        self.__dict__['alias'] = alias\n\n    def __getattr__(self, attr):\n        # Hack to work-around the fact that properties or other objects\n        # implementing the descriptor protocol (on the model being aliased),\n        # will not work correctly when we use getattr(). So we explicitly pass\n        # the model alias to the descriptor's getter.\n        for b in (self.model,) + self.model.__bases__:\n            try:\n                obj = b.__dict__[attr]\n                if isinstance(obj, ModelDescriptor):\n                    return obj.__get__(None, self)\n            except KeyError:\n                continue\n\n        model_attr = getattr(self.model, attr)\n        if isinstance(model_attr, Field):\n            self.__dict__[attr] = FieldAlias.create(self, model_attr)\n            return self.__dict__[attr]\n        return model_attr\n\n    def __setattr__(self, attr, value):\n        raise AttributeError('Cannot set attributes on model aliases.')\n\n    def get_field_aliases(self):\n        return [getattr(self, n) for n in self.model._meta.sorted_field_names]\n\n    def select(self, *selection):\n        if not selection:\n            selection = self.get_field_aliases()\n        return ModelSelect(self, selection)\n\n    def __call__(self, **kwargs):\n        return self.model(**kwargs)\n\n    def __sql__(self, ctx):\n        if ctx.scope == SCOPE_VALUES:\n            # Return the quoted table name.\n            return ctx.sql(self.model)\n\n        if self.alias:\n            ctx.alias_manager[self] = self.alias\n\n        if ctx.scope == SCOPE_SOURCE:\n            # Define the table and its alias.\n            return (ctx\n                    .sql(self.model._meta.entity)\n                    .literal(' AS ')\n                    .sql(Entity(ctx.alias_manager[self])))\n        else:\n            # Refer to the table using the alias.\n            return ctx.sql(Entity(ctx.alias_manager[self]))\n\n\nclass FieldAlias(Field):\n    def __init__(self, source, field):\n        self.source = source\n        self.model = source.model\n        self.field = field\n\n    @classmethod\n    def create(cls, source, field):\n        class _FieldAlias(cls, type(field)):\n            pass\n        return _FieldAlias(source, field)\n\n    def clone(self):\n        return FieldAlias(self.source, self.field)\n\n    def adapt(self, value): return self.field.adapt(value)\n    def python_value(self, value): return self.field.python_value(value)\n    def db_value(self, value): return self.field.db_value(value)\n    def __getattr__(self, attr):\n        return self.source if attr == 'model' else getattr(self.field, attr)\n\n    def __sql__(self, ctx):\n        return ctx.sql(Column(self.source, self.field.column_name))\n\n\ndef sort_models(models):\n    models = set(models)\n    seen = set()\n    ordering = []\n    def dfs(model):\n        if model in models and model not in seen:\n            seen.add(model)\n            for foreign_key, rel_model in model._meta.refs.items():\n                # Do not depth-first search deferred foreign-keys as this can\n                # cause tables to be created in the incorrect order.\n                if not foreign_key.deferred:\n                    dfs(rel_model)\n            if model._meta.depends_on:\n                for dependency in model._meta.depends_on:\n                    dfs(dependency)\n            ordering.append(model)\n\n    names = lambda m: (m._meta.name, m._meta.table_name)\n    for m in sorted(models, key=names):\n        dfs(m)\n    return ordering\n\n\nclass _ModelQueryHelper(object):\n    default_row_type = ROW.MODEL\n\n    def __init__(self, *args, **kwargs):\n        super(_ModelQueryHelper, self).__init__(*args, **kwargs)\n        if not self._database:\n            self._database = self.model._meta.database\n\n    @Node.copy\n    def objects(self, constructor=None):\n        self._row_type = ROW.CONSTRUCTOR\n        self._constructor = self.model if constructor is None else constructor\n\n    def _get_cursor_wrapper(self, cursor):\n        row_type = self._row_type or self.default_row_type\n        if row_type == ROW.MODEL:\n            return self._get_model_cursor_wrapper(cursor)\n        elif row_type == ROW.DICT:\n            return ModelDictCursorWrapper(cursor, self.model, self._returning)\n        elif row_type == ROW.TUPLE:\n            return ModelTupleCursorWrapper(cursor, self.model, self._returning)\n        elif row_type == ROW.NAMED_TUPLE:\n            return ModelNamedTupleCursorWrapper(cursor, self.model,\n                                                self._returning)\n        elif row_type == ROW.CONSTRUCTOR:\n            return ModelObjectCursorWrapper(cursor, self.model,\n                                            self._returning, self._constructor)\n        else:\n            raise ValueError('Unrecognized row type: \"%s\".' % row_type)\n\n    def _get_model_cursor_wrapper(self, cursor):\n        return ModelObjectCursorWrapper(cursor, self.model, [], self.model)\n\n\nclass ModelRaw(_ModelQueryHelper, RawQuery):\n    def __init__(self, model, sql, params, **kwargs):\n        self.model = model\n        self._returning = ()\n        super(ModelRaw, self).__init__(sql=sql, params=params, **kwargs)\n\n    def get(self):\n        try:\n            return self.execute()[0]\n        except IndexError:\n            sql, params = self.sql()\n            raise self.model.DoesNotExist('%s instance matching query does '\n                                          'not exist:\\nSQL: %s\\nParams: %s' %\n                                          (self.model, sql, params))\n\n\nclass BaseModelSelect(_ModelQueryHelper):\n    def union_all(self, rhs):\n        return ModelCompoundSelectQuery(self.model, self, 'UNION ALL', rhs)\n    __add__ = union_all\n\n    def union(self, rhs):\n        return ModelCompoundSelectQuery(self.model, self, 'UNION', rhs)\n    __or__ = union\n\n    def intersect(self, rhs):\n        return ModelCompoundSelectQuery(self.model, self, 'INTERSECT', rhs)\n    __and__ = intersect\n\n    def except_(self, rhs):\n        return ModelCompoundSelectQuery(self.model, self, 'EXCEPT', rhs)\n    __sub__ = except_\n\n    def __iter__(self):\n        if not self._cursor_wrapper:\n            self.execute()\n        return iter(self._cursor_wrapper)\n\n    def prefetch(self, *subqueries, **kwargs):\n        return prefetch(self, *subqueries, **kwargs)\n\n    def get(self, database=None):\n        clone = self.paginate(1, 1)\n        clone._cursor_wrapper = None\n        try:\n            return clone.execute(database)[0]\n        except IndexError:\n            sql, params = clone.sql()\n            raise self.model.DoesNotExist('%s instance matching query does '\n                                          'not exist:\\nSQL: %s\\nParams: %s' %\n                                          (clone.model, sql, params))\n\n    def get_or_none(self, database=None):\n        try:\n            return self.get(database=database)\n        except self.model.DoesNotExist:\n            pass\n\n    @Node.copy\n    def group_by(self, *columns):\n        grouping = []\n        for column in columns:\n            if is_model(column):\n                grouping.extend(column._meta.sorted_fields)\n            elif isinstance(column, Table):\n                if not column._columns:\n                    raise ValueError('Cannot pass a table to group_by() that '\n                                     'does not have columns explicitly '\n                                     'declared.')\n                grouping.extend([getattr(column, col_name)\n                                 for col_name in column._columns])\n            else:\n                grouping.append(column)\n        self._group_by = grouping\n\n\nclass ModelCompoundSelectQuery(BaseModelSelect, CompoundSelectQuery):\n    def __init__(self, model, *args, **kwargs):\n        self.model = model\n        super(ModelCompoundSelectQuery, self).__init__(*args, **kwargs)\n\n    def _get_model_cursor_wrapper(self, cursor):\n        return self.lhs._get_model_cursor_wrapper(cursor)\n\n\ndef _normalize_model_select(fields_or_models):\n    fields = []\n    for fm in fields_or_models:\n        if is_model(fm):\n            fields.extend(fm._meta.sorted_fields)\n        elif isinstance(fm, ModelAlias):\n            fields.extend(fm.get_field_aliases())\n        elif isinstance(fm, Table) and fm._columns:\n            fields.extend([getattr(fm, col) for col in fm._columns])\n        else:\n            fields.append(fm)\n    return fields\n\n\nclass ModelSelect(BaseModelSelect, Select):\n    def __init__(self, model, fields_or_models, is_default=False):\n        self.model = self._join_ctx = model\n        self._joins = {}\n        self._is_default = is_default\n        fields = _normalize_model_select(fields_or_models)\n        super(ModelSelect, self).__init__([model], fields)\n\n    def clone(self):\n        clone = super(ModelSelect, self).clone()\n        clone._joins = dict(clone._joins)\n        return clone\n\n    def select(self, *fields_or_models):\n        if fields_or_models or not self._is_default:\n            self._is_default = False\n            fields = _normalize_model_select(fields_or_models)\n            return super(ModelSelect, self).select(*fields)\n        return self\n\n    def select_extend(self, *columns):\n        self._is_default = False\n        fields = _normalize_model_select(columns)\n        return super(ModelSelect, self).select_extend(*fields)\n\n    def switch(self, ctx=None):\n        self._join_ctx = self.model if ctx is None else ctx\n        return self\n\n    def _get_model(self, src):\n        if is_model(src):\n            return src, True\n        elif isinstance(src, Table) and src._model:\n            return src._model, False\n        elif isinstance(src, ModelAlias):\n            return src.model, False\n        elif isinstance(src, ModelSelect):\n            return src.model, False\n        return None, False\n\n    def _normalize_join(self, src, dest, on, attr):\n        # Allow \"on\" expression to have an alias that determines the\n        # destination attribute for the joined data.\n        on_alias = isinstance(on, Alias)\n        if on_alias:\n            attr = attr or on._alias\n            on = on.alias()\n\n        # Obtain references to the source and destination models being joined.\n        src_model, src_is_model = self._get_model(src)\n        dest_model, dest_is_model = self._get_model(dest)\n\n        if src_model and dest_model:\n            self._join_ctx = dest\n            constructor = dest_model\n\n            # In the case where the \"on\" clause is a Column or Field, we will\n            # convert that field into the appropriate predicate expression.\n            if not (src_is_model and dest_is_model) and isinstance(on, Column):\n                if on.source is src:\n                    to_field = src_model._meta.columns[on.name]\n                elif on.source is dest:\n                    to_field = dest_model._meta.columns[on.name]\n                else:\n                    raise AttributeError('\"on\" clause Column %s does not '\n                                         'belong to %s or %s.' %\n                                         (on, src_model, dest_model))\n                on = None\n            elif isinstance(on, Field):\n                to_field = on\n                on = None\n            else:\n                to_field = None\n\n            fk_field, is_backref = self._generate_on_clause(\n                src_model, dest_model, to_field, on)\n\n            if on is None:\n                src_attr = 'name' if src_is_model else 'column_name'\n                dest_attr = 'name' if dest_is_model else 'column_name'\n                if is_backref:\n                    lhs = getattr(dest, getattr(fk_field, dest_attr))\n                    rhs = getattr(src, getattr(fk_field.rel_field, src_attr))\n                else:\n                    lhs = getattr(src, getattr(fk_field, src_attr))\n                    rhs = getattr(dest, getattr(fk_field.rel_field, dest_attr))\n                on = (lhs == rhs)\n\n            if not attr:\n                if fk_field is not None and not is_backref:\n                    attr = fk_field.name\n                else:\n                    attr = dest_model._meta.name\n            elif on_alias and fk_field is not None and \\\n                    attr == fk_field.object_id_name and not is_backref:\n                raise ValueError('Cannot assign join alias to \"%s\", as this '\n                                 'attribute is the object_id_name for the '\n                                 'foreign-key field \"%s\"' % (attr, fk_field))\n\n        elif isinstance(dest, Source):\n            constructor = dict\n            attr = attr or dest._alias\n            if not attr and isinstance(dest, Table):\n                attr = attr or dest.__name__\n\n        return (on, attr, constructor)\n\n    def _generate_on_clause(self, src, dest, to_field=None, on=None):\n        meta = src._meta\n        is_backref = fk_fields = False\n\n        # Get all the foreign keys between source and dest, and determine if\n        # the join is via a back-reference.\n        if dest in meta.model_refs:\n            fk_fields = meta.model_refs[dest]\n        elif dest in meta.model_backrefs:\n            fk_fields = meta.model_backrefs[dest]\n            is_backref = True\n\n        if not fk_fields:\n            if on is not None:\n                return None, False\n            raise ValueError('Unable to find foreign key between %s and %s. '\n                             'Please specify an explicit join condition.' %\n                             (src, dest))\n        elif to_field is not None:\n            # If the foreign-key field was specified explicitly, remove all\n            # other foreign-key fields from the list.\n            target = (to_field.field if isinstance(to_field, FieldAlias)\n                      else to_field)\n            fk_fields = [f for f in fk_fields if (\n                         (f is target) or\n                         (is_backref and f.rel_field is to_field))]\n\n        if len(fk_fields) == 1:\n            return fk_fields[0], is_backref\n\n        if on is None:\n            # If multiple foreign-keys exist, try using the FK whose name\n            # matches that of the related model. If not, raise an error as this\n            # is ambiguous.\n            for fk in fk_fields:\n                if fk.name == dest._meta.name:\n                    return fk, is_backref\n\n            raise ValueError('More than one foreign key between %s and %s.'\n                             ' Please specify which you are joining on.' %\n                             (src, dest))\n\n        # If there are multiple foreign-keys to choose from and the join\n        # predicate is an expression, we'll try to figure out which\n        # foreign-key field we're joining on so that we can assign to the\n        # correct attribute when resolving the model graph.\n        to_field = None\n        if isinstance(on, Expression):\n            lhs, rhs = on.lhs, on.rhs\n            # Coerce to set() so that we force Python to compare using the\n            # object's hash rather than equality test, which returns a\n            # false-positive due to overriding __eq__.\n            fk_set = set(fk_fields)\n\n            if isinstance(lhs, Field):\n                lhs_f = lhs.field if isinstance(lhs, FieldAlias) else lhs\n                if lhs_f in fk_set:\n                    to_field = lhs_f\n            elif isinstance(rhs, Field):\n                rhs_f = rhs.field if isinstance(rhs, FieldAlias) else rhs\n                if rhs_f in fk_set:\n                    to_field = rhs_f\n\n        return to_field, False\n\n    @Node.copy\n    def join(self, dest, join_type=JOIN.INNER, on=None, src=None, attr=None):\n        src = self._join_ctx if src is None else src\n\n        if join_type == JOIN.LATERAL or join_type == JOIN.LEFT_LATERAL:\n            on = True\n        elif join_type != JOIN.CROSS:\n            on, attr, constructor = self._normalize_join(src, dest, on, attr)\n            if attr:\n                self._joins.setdefault(src, [])\n                self._joins[src].append((dest, attr, constructor, join_type))\n        elif on is not None:\n            raise ValueError('Cannot specify on clause with cross join.')\n\n        if not self._from_list:\n            raise ValueError('No sources to join on.')\n\n        item = self._from_list.pop()\n        self._from_list.append(Join(item, dest, join_type, on))\n\n    def left_outer_join(self, dest, on=None, src=None, attr=None):\n        return self.join(dest, JOIN.LEFT_OUTER, on, src, attr)\n\n    def join_from(self, src, dest, join_type=JOIN.INNER, on=None, attr=None):\n        return self.join(dest, join_type, on, src, attr)\n\n    def _get_model_cursor_wrapper(self, cursor):\n        if len(self._from_list) == 1 and not self._joins:\n            return ModelObjectCursorWrapper(cursor, self.model,\n                                            self._returning, self.model)\n        return ModelCursorWrapper(cursor, self.model, self._returning,\n                                  self._from_list, self._joins)\n\n    def ensure_join(self, lm, rm, on=None, **join_kwargs):\n        join_ctx = self._join_ctx\n        for dest, _, constructor, _ in self._joins.get(lm, []):\n            if dest == rm:\n                return self\n        return self.switch(lm).join(rm, on=on, **join_kwargs).switch(join_ctx)\n\n    def convert_dict_to_node(self, qdict):\n        accum = []\n        joins = []\n        fks = (ForeignKeyField, BackrefAccessor)\n        for key, value in sorted(qdict.items()):\n            curr = self.model\n            if '__' in key and key.rsplit('__', 1)[1] in DJANGO_MAP:\n                key, op = key.rsplit('__', 1)\n                op = DJANGO_MAP[op]\n            elif value is None:\n                op = DJANGO_MAP['is']\n            else:\n                op = DJANGO_MAP['eq']\n\n            if '__' not in key:\n                # Handle simplest case. This avoids joining over-eagerly when a\n                # direct FK lookup is all that is required.\n                model_attr = getattr(curr, key)\n            else:\n                for piece in key.split('__'):\n                    for dest, attr, _, _ in self._joins.get(curr, ()):\n                        try: model_attr = getattr(curr, piece, None)\n                        except: pass\n                        if attr == piece or (isinstance(dest, ModelAlias) and\n                                             dest.alias == piece):\n                            curr = dest\n                            break\n                    else:\n                        model_attr = getattr(curr, piece)\n                        if value is not None and isinstance(model_attr, fks):\n                            curr = model_attr.rel_model\n                            joins.append(model_attr)\n            accum.append(op(model_attr, value))\n        return accum, joins\n\n    def filter(self, *args, **kwargs):\n        # normalize args and kwargs into a new expression\n        if args and kwargs:\n            dq_node = (reduce(operator.and_, [a.clone() for a in args]) &\n                       DQ(**kwargs))\n        elif args:\n            dq_node = (reduce(operator.and_, [a.clone() for a in args]) &\n                       ColumnBase())\n        elif kwargs:\n            dq_node = DQ(**kwargs) & ColumnBase()\n        else:\n            return self.clone()\n\n        # dq_node should now be an Expression, lhs = Node(), rhs = ...\n        q = collections.deque([dq_node])\n        dq_joins = []\n        seen_joins = set()\n        while q:\n            curr = q.popleft()\n            if not isinstance(curr, Expression):\n                continue\n            for side, piece in (('lhs', curr.lhs), ('rhs', curr.rhs)):\n                if isinstance(piece, DQ):\n                    query, joins = self.convert_dict_to_node(piece.query)\n                    for join in joins:\n                        if join not in seen_joins:\n                            dq_joins.append(join)\n                            seen_joins.add(join)\n                    expression = reduce(operator.and_, query)\n                    # Apply values from the DQ object.\n                    if piece._negated:\n                        expression = Negated(expression)\n                    #expression._alias = piece._alias\n                    setattr(curr, side, expression)\n                else:\n                    q.append(piece)\n\n        if not args or not kwargs:\n            dq_node = dq_node.lhs\n\n        query = self.clone()\n        for field in dq_joins:\n            if isinstance(field, ForeignKeyField):\n                lm, rm = field.model, field.rel_model\n                field_obj = field\n            elif isinstance(field, BackrefAccessor):\n                lm, rm = field.model, field.rel_model\n                field_obj = field.field\n            query = query.ensure_join(lm, rm, field_obj)\n        return query.where(dq_node)\n\n    def create_table(self, name, safe=True, **meta):\n        return self.model._schema.create_table_as(name, self, safe, **meta)\n\n    def __sql_selection__(self, ctx, is_subquery=False):\n        if self._is_default and is_subquery and len(self._returning) > 1 and \\\n           self.model._meta.primary_key is not False:\n            return ctx.sql(self.model._meta.primary_key)\n\n        return ctx.sql(CommaNodeList(self._returning))\n\n\nclass NoopModelSelect(ModelSelect):\n    def __sql__(self, ctx):\n        return self.model._meta.database.get_noop_select(ctx)\n\n    def _get_cursor_wrapper(self, cursor):\n        return CursorWrapper(cursor)\n\n\nclass _ModelWriteQueryHelper(_ModelQueryHelper):\n    def __init__(self, model, *args, **kwargs):\n        self.model = model\n        super(_ModelWriteQueryHelper, self).__init__(model, *args, **kwargs)\n\n    def returning(self, *returning):\n        accum = []\n        for item in returning:\n            if is_model(item):\n                accum.extend(item._meta.sorted_fields)\n            else:\n                accum.append(item)\n        return super(_ModelWriteQueryHelper, self).returning(*accum)\n\n    def _set_table_alias(self, ctx):\n        table = self.model._meta.table\n        ctx.alias_manager[table] = table.__name__\n\n\nclass ModelUpdate(_ModelWriteQueryHelper, Update):\n    pass\n\n\nclass ModelInsert(_ModelWriteQueryHelper, Insert):\n    default_row_type = ROW.TUPLE\n\n    def __init__(self, *args, **kwargs):\n        super(ModelInsert, self).__init__(*args, **kwargs)\n        if self._returning is None and self.model._meta.database is not None:\n            if self.model._meta.database.returning_clause:\n                self._returning = self.model._meta.get_primary_keys()\n\n    def returning(self, *returning):\n        # By default ModelInsert will yield a `tuple` containing the\n        # primary-key of the newly inserted row. But if we are explicitly\n        # specifying a returning clause and have not set a row type, we will\n        # default to returning model instances instead.\n        if returning and self._row_type is None:\n            self._row_type = ROW.MODEL\n        return super(ModelInsert, self).returning(*returning)\n\n    def get_default_data(self):\n        return self.model._meta.defaults\n\n    def get_default_columns(self):\n        fields = self.model._meta.sorted_fields\n        return fields[1:] if self.model._meta.auto_increment else fields\n\n\nclass ModelDelete(_ModelWriteQueryHelper, Delete):\n    pass\n\n\nclass ManyToManyQuery(ModelSelect):\n    def __init__(self, instance, accessor, rel, *args, **kwargs):\n        self._instance = instance\n        self._accessor = accessor\n        self._src_attr = accessor.src_fk.rel_field.name\n        self._dest_attr = accessor.dest_fk.rel_field.name\n        super(ManyToManyQuery, self).__init__(rel, (rel,), *args, **kwargs)\n\n    def _id_list(self, model_or_id_list):\n        if isinstance(model_or_id_list[0], Model):\n            return [getattr(obj, self._dest_attr) for obj in model_or_id_list]\n        return model_or_id_list\n\n    def add(self, value, clear_existing=False):\n        if clear_existing:\n            self.clear()\n\n        accessor = self._accessor\n        src_id = getattr(self._instance, self._src_attr)\n        if isinstance(value, SelectQuery):\n            query = value.columns(\n                Value(src_id),\n                accessor.dest_fk.rel_field)\n            accessor.through_model.insert_from(\n                fields=[accessor.src_fk, accessor.dest_fk],\n                query=query).execute()\n        else:\n            value = ensure_tuple(value)\n            if not value: return\n\n            inserts = [{\n                accessor.src_fk.name: src_id,\n                accessor.dest_fk.name: rel_id}\n                for rel_id in self._id_list(value)]\n            accessor.through_model.insert_many(inserts).execute()\n\n    def remove(self, value):\n        src_id = getattr(self._instance, self._src_attr)\n        if isinstance(value, SelectQuery):\n            column = getattr(value.model, self._dest_attr)\n            subquery = value.columns(column)\n            return (self._accessor.through_model\n                    .delete()\n                    .where(\n                        (self._accessor.dest_fk << subquery) &\n                        (self._accessor.src_fk == src_id))\n                    .execute())\n        else:\n            value = ensure_tuple(value)\n            if not value:\n                return\n            return (self._accessor.through_model\n                    .delete()\n                    .where(\n                        (self._accessor.dest_fk << self._id_list(value)) &\n                        (self._accessor.src_fk == src_id))\n                    .execute())\n\n    def clear(self):\n        src_id = getattr(self._instance, self._src_attr)\n        return (self._accessor.through_model\n                .delete()\n                .where(self._accessor.src_fk == src_id)\n                .execute())\n\n\ndef safe_python_value(conv_func):\n    def validate(value):\n        try:\n            return conv_func(value)\n        except (TypeError, ValueError):\n            return value\n    return validate\n\n\nclass BaseModelCursorWrapper(DictCursorWrapper):\n    def __init__(self, cursor, model, columns):\n        super(BaseModelCursorWrapper, self).__init__(cursor)\n        self.model = model\n        self.select = columns or []\n\n    def _initialize_columns(self):\n        combined = self.model._meta.combined\n        table = self.model._meta.table\n        description = self.cursor.description\n\n        self.ncols = len(self.cursor.description)\n        self.columns = []\n        self.converters = converters = [None] * self.ncols\n        self.fields = fields = [None] * self.ncols\n\n        for idx, description_item in enumerate(description):\n            column = orig_column = description_item[0]\n\n            # Try to clean-up messy column descriptions when people do not\n            # provide an alias. The idea is that we take something like:\n            # SUM(\"t1\".\"price\") -> \"price\") -> price\n            dot_index = column.rfind('.')\n            if dot_index != -1:\n                column = column[dot_index + 1:]\n            column = column.strip('()\"`')\n            self.columns.append(column)\n\n            # Now we'll see what they selected and see if we can improve the\n            # column-name being returned - e.g. by mapping it to the selected\n            # field's name.\n            try:\n                raw_node = self.select[idx]\n            except IndexError:\n                if column in combined:\n                    raw_node = node = combined[column]\n                else:\n                    continue\n            else:\n                node = raw_node.unwrap()\n\n            # If this column was given an alias, then we will use whatever\n            # alias was returned by the cursor.\n            is_alias = raw_node.is_alias()\n            if is_alias:\n                self.columns[idx] = orig_column\n\n            # Heuristics used to attempt to get the field associated with a\n            # given SELECT column, so that we can accurately convert the value\n            # returned by the database-cursor into a Python object.\n            if isinstance(node, Field):\n                if raw_node._coerce:\n                    converters[idx] = node.python_value\n                fields[idx] = node\n                if not is_alias:\n                    self.columns[idx] = node.name\n            elif isinstance(node, ColumnBase) and raw_node._converter:\n                converters[idx] = raw_node._converter\n            elif isinstance(node, Function) and node._coerce:\n                if node._python_value is not None:\n                    converters[idx] = node._python_value\n                elif node.arguments and isinstance(node.arguments[0], Node):\n                    # If the first argument is a field or references a column\n                    # on a Model, try using that field's conversion function.\n                    # This usually works, but we use \"safe_python_value()\" so\n                    # that if a TypeError or ValueError occurs during\n                    # conversion we can just fall-back to the raw cursor value.\n                    first = node.arguments[0].unwrap()\n                    if isinstance(first, Entity):\n                        path = first._path[-1]  # Try to look-up by name.\n                        first = combined.get(path)\n                    if isinstance(first, Field):\n                        converters[idx] = safe_python_value(first.python_value)\n            elif column in combined:\n                if node._coerce:\n                    converters[idx] = combined[column].python_value\n                if isinstance(node, Column) and node.source == table:\n                    fields[idx] = combined[column]\n\n    initialize = _initialize_columns\n\n    def process_row(self, row):\n        raise NotImplementedError\n\n\nclass ModelDictCursorWrapper(BaseModelCursorWrapper):\n    def process_row(self, row):\n        result = {}\n        columns, converters = self.columns, self.converters\n        fields = self.fields\n\n        for i in range(self.ncols):\n            attr = columns[i]\n            if attr in result: continue  # Don't overwrite if we have dupes.\n            if converters[i] is not None:\n                result[attr] = converters[i](row[i])\n            else:\n                result[attr] = row[i]\n\n        return result\n\n\nclass ModelTupleCursorWrapper(ModelDictCursorWrapper):\n    constructor = tuple\n\n    def process_row(self, row):\n        columns, converters = self.columns, self.converters\n        return self.constructor([\n            (converters[i](row[i]) if converters[i] is not None else row[i])\n            for i in range(self.ncols)])\n\n\nclass ModelNamedTupleCursorWrapper(ModelTupleCursorWrapper):\n    def initialize(self):\n        self._initialize_columns()\n        attributes = []\n        for i in range(self.ncols):\n            attributes.append(self.columns[i])\n        self.tuple_class = collections.namedtuple('Row', attributes)\n        self.constructor = lambda row: self.tuple_class(*row)\n\n\nclass ModelObjectCursorWrapper(ModelDictCursorWrapper):\n    def __init__(self, cursor, model, select, constructor):\n        self.constructor = constructor\n        self.is_model = is_model(constructor)\n        super(ModelObjectCursorWrapper, self).__init__(cursor, model, select)\n\n    def process_row(self, row):\n        data = super(ModelObjectCursorWrapper, self).process_row(row)\n        if self.is_model:\n            # Clear out any dirty fields before returning to the user.\n            obj = self.constructor(__no_default__=1, **data)\n            obj._dirty.clear()\n            return obj\n        else:\n            return self.constructor(**data)\n\n\nclass ModelCursorWrapper(BaseModelCursorWrapper):\n    def __init__(self, cursor, model, select, from_list, joins):\n        super(ModelCursorWrapper, self).__init__(cursor, model, select)\n        self.from_list = from_list\n        self.joins = joins\n\n    def initialize(self):\n        self._initialize_columns()\n        selected_src = set([field.model for field in self.fields\n                            if field is not None])\n        select, columns = self.select, self.columns\n\n        self.key_to_constructor = {self.model: self.model}\n        self.src_is_dest = {}\n        self.src_to_dest = []\n        accum = collections.deque(self.from_list)\n        dests = set()\n\n        while accum:\n            curr = accum.popleft()\n            if isinstance(curr, Join):\n                accum.append(curr.lhs)\n                accum.append(curr.rhs)\n                continue\n\n            if curr not in self.joins:\n                continue\n\n            is_dict = isinstance(curr, dict)\n            for key, attr, constructor, join_type in self.joins[curr]:\n                if key not in self.key_to_constructor:\n                    self.key_to_constructor[key] = constructor\n\n                    # (src, attr, dest, is_dict, join_type).\n                    self.src_to_dest.append((curr, attr, key, is_dict,\n                                             join_type))\n                    dests.add(key)\n                    accum.append(key)\n\n        # Ensure that we accommodate everything selected.\n        for src in selected_src:\n            if src not in self.key_to_constructor:\n                if is_model(src):\n                    self.key_to_constructor[src] = src\n                elif isinstance(src, ModelAlias):\n                    self.key_to_constructor[src] = src.model\n\n        # Indicate which sources are also dests.\n        for src, _, dest, _, _ in self.src_to_dest:\n            self.src_is_dest[src] = src in dests and (dest in selected_src\n                                                      or src in selected_src)\n\n        self.column_keys = []\n        for idx, node in enumerate(select):\n            key = self.model\n            field = self.fields[idx]\n            if field is not None:\n                if isinstance(field, FieldAlias):\n                    key = field.source\n                else:\n                    key = field.model\n            elif isinstance(node, BindTo):\n                if node.dest not in self.key_to_constructor:\n                    raise ValueError('%s specifies bind-to %s, but %s is not '\n                                     'among the selected sources.' %\n                                     (node.unwrap(), node.dest, node.dest))\n                key = node.dest\n            else:\n                if isinstance(node, Node):\n                    node = node.unwrap()\n                if isinstance(node, Column):\n                    key = node.source\n\n            self.column_keys.append(key)\n\n    def process_row(self, row):\n        objects = {}\n        object_list = []\n        for key, constructor in self.key_to_constructor.items():\n            objects[key] = constructor(__no_default__=True)\n            object_list.append(objects[key])\n\n        default_instance = objects[self.model]\n\n        set_keys = set()\n        for idx, key in enumerate(self.column_keys):\n            # Get the instance corresponding to the selected column/value,\n            # falling back to the \"root\" model instance.\n            instance = objects.get(key, default_instance)\n            column = self.columns[idx]\n            value = row[idx]\n            if value is not None:\n                set_keys.add(key)\n            if self.converters[idx]:\n                value = self.converters[idx](value)\n\n            if isinstance(instance, dict):\n                instance[column] = value\n            else:\n                setattr(instance, column, value)\n\n        # Need to do some analysis on the joins before this.\n        for (src, attr, dest, is_dict, join_type) in self.src_to_dest:\n            instance = objects[src]\n            try:\n                joined_instance = objects[dest]\n            except KeyError:\n                continue\n\n            # If no fields were set on the destination instance then do not\n            # assign an \"empty\" instance.\n            if instance is None or dest is None or \\\n               (dest not in set_keys and not self.src_is_dest.get(dest)):\n                continue\n\n            # If no fields were set on either the source or the destination,\n            # then we have nothing to do here.\n            if instance not in set_keys and dest not in set_keys \\\n               and join_type.endswith('OUTER JOIN'):\n                continue\n\n            if is_dict:\n                instance[attr] = joined_instance\n            else:\n                setattr(instance, attr, joined_instance)\n\n        # When instantiating models from a cursor, we clear the dirty fields.\n        for instance in object_list:\n            if isinstance(instance, Model):\n                instance._dirty.clear()\n\n        return objects[self.model]\n\n\nclass PrefetchQuery(collections.namedtuple('_PrefetchQuery', (\n    'query', 'fields', 'is_backref', 'rel_models', 'field_to_name', 'model'))):\n    def __new__(cls, query, fields=None, is_backref=None, rel_models=None,\n                field_to_name=None, model=None):\n        if fields:\n            if is_backref:\n                if rel_models is None:\n                    rel_models = [field.model for field in fields]\n                foreign_key_attrs = [field.rel_field.name for field in fields]\n            else:\n                if rel_models is None:\n                    rel_models = [field.rel_model for field in fields]\n                foreign_key_attrs = [field.name for field in fields]\n            field_to_name = list(zip(fields, foreign_key_attrs))\n        model = query.model\n        return super(PrefetchQuery, cls).__new__(\n            cls, query, fields, is_backref, rel_models, field_to_name, model)\n\n    def populate_instance(self, instance, id_map):\n        if self.is_backref:\n            for field in self.fields:\n                identifier = instance.__data__[field.name]\n                key = (field, identifier)\n                if key in id_map:\n                    setattr(instance, field.name, id_map[key])\n        else:\n            for field, attname in self.field_to_name:\n                identifier = instance.__data__[field.rel_field.name]\n                key = (field, identifier)\n                rel_instances = id_map.get(key, [])\n                for inst in rel_instances:\n                    setattr(inst, attname, instance)\n                    inst._dirty.clear()\n                setattr(instance, field.backref, rel_instances)\n\n    def store_instance(self, instance, id_map):\n        for field, attname in self.field_to_name:\n            identity = field.rel_field.python_value(instance.__data__[attname])\n            key = (field, identity)\n            if self.is_backref:\n                id_map[key] = instance\n            else:\n                id_map.setdefault(key, [])\n                id_map[key].append(instance)\n\n\ndef prefetch_add_subquery(sq, subqueries, prefetch_type):\n    fixed_queries = [PrefetchQuery(sq)]\n    for i, subquery in enumerate(subqueries):\n        if isinstance(subquery, tuple):\n            subquery, target_model = subquery\n        else:\n            target_model = None\n        if not isinstance(subquery, Query) and is_model(subquery) or \\\n           isinstance(subquery, ModelAlias):\n            subquery = subquery.select()\n        subquery_model = subquery.model\n        for j in reversed(range(i + 1)):\n            fks = backrefs = None\n            fixed = fixed_queries[j]\n            last_query = fixed.query\n            last_model = last_obj = fixed.model\n            if isinstance(last_model, ModelAlias):\n                last_model = last_model.model\n            rels = subquery_model._meta.model_refs.get(last_model, [])\n            if rels:\n                fks = [getattr(subquery_model, fk.name) for fk in rels]\n                pks = [getattr(last_obj, fk.rel_field.name) for fk in rels]\n            else:\n                backrefs = subquery_model._meta.model_backrefs.get(last_model)\n            if (fks or backrefs) and ((target_model is last_obj) or\n                                      (target_model is None)):\n                break\n\n        else:\n            tgt_err = ' using %s' % target_model if target_model else ''\n            raise AttributeError('Error: unable to find foreign key for '\n                                 'query: %s%s' % (subquery, tgt_err))\n\n        dest = (target_model,) if target_model else None\n\n        if fks:\n            if prefetch_type == PREFETCH_TYPE.WHERE:\n                expr = reduce(operator.or_, [\n                    (fk << last_query.select(pk))\n                    for (fk, pk) in zip(fks, pks)])\n                subquery = subquery.where(expr)\n            elif prefetch_type == PREFETCH_TYPE.JOIN:\n                expr = []\n                select_pks = set()\n                for fk, pk in zip(fks, pks):\n                    expr.append(getattr(last_query.c, pk.column_name) == fk)\n                    select_pks.add(pk)\n                subquery = subquery.distinct().join(\n                    last_query.select(*select_pks),\n                    on=reduce(operator.or_, expr))\n            fixed_queries.append(PrefetchQuery(subquery, fks, False, dest))\n        elif backrefs:\n            expr = []\n            fields = []\n            for backref in backrefs:\n                rel_field = getattr(subquery_model, backref.rel_field.name)\n                fk_field = getattr(last_obj, backref.name)\n                fields.append((rel_field, fk_field))\n\n            if prefetch_type == PREFETCH_TYPE.WHERE:\n                for rel_field, fk_field in fields:\n                    expr.append(rel_field << last_query.select(fk_field))\n                subquery = subquery.where(reduce(operator.or_, expr))\n            elif prefetch_type == PREFETCH_TYPE.JOIN:\n                select_fks = []\n                for rel_field, fk_field in fields:\n                    select_fks.append(fk_field)\n                    target = getattr(last_query.c, fk_field.column_name)\n                    expr.append(rel_field == target)\n                subquery = subquery.distinct().join(\n                    last_query.select(*select_fks),\n                    on=reduce(operator.or_, expr))\n            fixed_queries.append(PrefetchQuery(subquery, backrefs, True, dest))\n\n    return fixed_queries\n\n\ndef prefetch(sq, *subqueries, **kwargs):\n    if not subqueries:\n        return sq\n    prefetch_type = kwargs.pop('prefetch_type', PREFETCH_TYPE.WHERE)\n    if kwargs:\n        raise ValueError('Unrecognized arguments: %s' % kwargs)\n\n    fixed_queries = prefetch_add_subquery(sq, subqueries, prefetch_type)\n    deps = {}\n    rel_map = {}\n    for pq in reversed(fixed_queries):\n        query_model = pq.model\n        if pq.fields:\n            for rel_model in pq.rel_models:\n                rel_map.setdefault(rel_model, [])\n                rel_map[rel_model].append(pq)\n\n        deps.setdefault(query_model, {})\n        id_map = deps[query_model]\n        has_relations = bool(rel_map.get(query_model))\n\n        for instance in pq.query:\n            if pq.fields:\n                pq.store_instance(instance, id_map)\n            if has_relations:\n                for rel in rel_map[query_model]:\n                    rel.populate_instance(instance, deps[rel.model])\n\n    return list(pq.query)\n"
        },
        {
          "name": "playhouse",
          "type": "tree",
          "content": null
        },
        {
          "name": "pwiz.py",
          "type": "blob",
          "size": 8.0009765625,
          "content": "#!/usr/bin/env python\n\nimport datetime\nimport os\nimport sys\nfrom getpass import getpass\nfrom optparse import OptionParser\n\nfrom peewee import *\nfrom peewee import print_\nfrom peewee import __version__ as peewee_version\nfrom playhouse.cockroachdb import CockroachDatabase\nfrom playhouse.reflection import *\n\n\nHEADER = \"\"\"from peewee import *%s\n\ndatabase = %s('%s'%s)\n\"\"\"\n\nBASE_MODEL = \"\"\"\\\nclass BaseModel(Model):\n    class Meta:\n        database = database\n\"\"\"\n\nUNKNOWN_FIELD = \"\"\"\\\nclass UnknownField(object):\n    def __init__(self, *_, **__): pass\n\"\"\"\n\nDATABASE_ALIASES = {\n    CockroachDatabase: ['cockroach', 'cockroachdb', 'crdb'],\n    MySQLDatabase: ['mysql', 'mysqldb'],\n    PostgresqlDatabase: ['postgres', 'postgresql'],\n    SqliteDatabase: ['sqlite', 'sqlite3'],\n}\n\nDATABASE_MAP = dict((value, key)\n                    for key in DATABASE_ALIASES\n                    for value in DATABASE_ALIASES[key])\n\ndef make_introspector(database_type, database_name, **kwargs):\n    if database_type not in DATABASE_MAP:\n        err('Unrecognized database, must be one of: %s' %\n            ', '.join(DATABASE_MAP.keys()))\n        sys.exit(1)\n\n    schema = kwargs.pop('schema', None)\n    DatabaseClass = DATABASE_MAP[database_type]\n    db = DatabaseClass(database_name, **kwargs)\n    return Introspector.from_database(db, schema=schema)\n\ndef print_models(introspector, tables=None, preserve_order=False,\n                 include_views=False, ignore_unknown=False, snake_case=True):\n    database = introspector.introspect(table_names=tables,\n                                       include_views=include_views,\n                                       snake_case=snake_case)\n\n    db_kwargs = introspector.get_database_kwargs()\n    header = HEADER % (\n        introspector.get_additional_imports(),\n        introspector.get_database_class().__name__,\n        introspector.get_database_name(),\n        ', **%s' % repr(db_kwargs) if db_kwargs else '')\n    print_(header)\n\n    if not ignore_unknown:\n        print_(UNKNOWN_FIELD)\n\n    print_(BASE_MODEL)\n\n    def _print_table(table, seen, accum=None):\n        accum = accum or []\n        foreign_keys = database.foreign_keys[table]\n        for foreign_key in foreign_keys:\n            dest = foreign_key.dest_table\n\n            # In the event the destination table has already been pushed\n            # for printing, then we have a reference cycle.\n            if dest in accum and table not in accum:\n                print_('# Possible reference cycle: %s' % dest)\n\n            # If this is not a self-referential foreign key, and we have\n            # not already processed the destination table, do so now.\n            if dest not in seen and dest not in accum:\n                seen.add(dest)\n                if dest != table:\n                    _print_table(dest, seen, accum + [table])\n\n        print_('class %s(BaseModel):' % database.model_names[table])\n        columns = database.columns[table].items()\n        if not preserve_order:\n            columns = sorted(columns)\n        primary_keys = database.primary_keys[table]\n        for name, column in columns:\n            skip = all([\n                name in primary_keys,\n                name == 'id',\n                len(primary_keys) == 1,\n                column.field_class in introspector.pk_classes])\n            if skip:\n                continue\n            if column.primary_key and len(primary_keys) > 1:\n                # If we have a CompositeKey, then we do not want to explicitly\n                # mark the columns as being primary keys.\n                column.primary_key = False\n\n            is_unknown = column.field_class is UnknownField\n            if is_unknown and ignore_unknown:\n                disp = '%s - %s' % (column.name, column.raw_column_type or '?')\n                print_('    # %s' % disp)\n            else:\n                print_('    %s' % column.get_field())\n\n        print_('')\n        print_('    class Meta:')\n        print_('        table_name = \\'%s\\'' % table)\n        multi_column_indexes = database.multi_column_indexes(table)\n        if multi_column_indexes:\n            print_('        indexes = (')\n            for fields, unique in sorted(multi_column_indexes):\n                print_('            ((%s), %s),' % (\n                    ', '.join(\"'%s'\" % field for field in fields),\n                    unique,\n                ))\n            print_('        )')\n\n        if introspector.schema:\n            print_('        schema = \\'%s\\'' % introspector.schema)\n        if len(primary_keys) > 1:\n            pk_field_names = sorted([\n                field.name for col, field in columns\n                if col in primary_keys])\n            pk_list = ', '.join(\"'%s'\" % pk for pk in pk_field_names)\n            print_('        primary_key = CompositeKey(%s)' % pk_list)\n        elif not primary_keys:\n            print_('        primary_key = False')\n        print_('')\n\n        seen.add(table)\n\n    seen = set()\n    for table in sorted(database.model_names.keys()):\n        if table not in seen:\n            if not tables or table in tables:\n                _print_table(table, seen)\n\ndef print_header(cmd_line, introspector):\n    timestamp = datetime.datetime.now()\n    print_('# Code generated by:')\n    print_('# python -m pwiz %s' % cmd_line)\n    print_('# Date: %s' % timestamp.strftime('%B %d, %Y %I:%M%p'))\n    print_('# Database: %s' % introspector.get_database_name())\n    print_('# Peewee version: %s' % peewee_version)\n    print_('')\n\n\ndef err(msg):\n    sys.stderr.write('\\033[91m%s\\033[0m\\n' % msg)\n    sys.stderr.flush()\n\ndef get_option_parser():\n    parser = OptionParser(usage='usage: %prog [options] database_name')\n    ao = parser.add_option\n    ao('-H', '--host', dest='host')\n    ao('-p', '--port', dest='port', type='int')\n    ao('-u', '--user', dest='user')\n    ao('-P', '--password', dest='password', action='store_true')\n    engines = sorted(DATABASE_MAP)\n    ao('-e', '--engine', dest='engine', choices=engines,\n       help=('Database type, e.g. sqlite, mysql, postgresql or cockroachdb. '\n             'Default is \"postgresql\".'))\n    ao('-s', '--schema', dest='schema')\n    ao('-t', '--tables', dest='tables',\n       help=('Only generate the specified tables. Multiple table names should '\n             'be separated by commas.'))\n    ao('-v', '--views', dest='views', action='store_true',\n       help='Generate model classes for VIEWs in addition to tables.')\n    ao('-i', '--info', dest='info', action='store_true',\n       help=('Add database information and other metadata to top of the '\n             'generated file.'))\n    ao('-o', '--preserve-order', action='store_true', dest='preserve_order',\n       help='Model definition column ordering matches source table.')\n    ao('-I', '--ignore-unknown', action='store_true', dest='ignore_unknown',\n       help='Ignore fields whose type cannot be determined.')\n    ao('-L', '--legacy-naming', action='store_true', dest='legacy_naming',\n       help='Use legacy table- and column-name generation.')\n    return parser\n\ndef get_connect_kwargs(options):\n    ops = ('host', 'port', 'user', 'schema')\n    kwargs = dict((o, getattr(options, o)) for o in ops if getattr(options, o))\n    if options.password:\n        kwargs['password'] = getpass()\n    return kwargs\n\n\nif __name__ == '__main__':\n    raw_argv = sys.argv\n\n    parser = get_option_parser()\n    options, args = parser.parse_args()\n\n    if len(args) < 1:\n        err('Missing required parameter \"database\"')\n        parser.print_help()\n        sys.exit(1)\n\n    connect = get_connect_kwargs(options)\n    database = args[-1]\n\n    tables = None\n    if options.tables:\n        tables = [table.strip() for table in options.tables.split(',')\n                  if table.strip()]\n\n    engine = options.engine\n    if engine is None:\n        engine = 'sqlite' if os.path.exists(database) else 'postgresql'\n\n    introspector = make_introspector(engine, database, **connect)\n    if options.info:\n        cmd_line = ' '.join(raw_argv[1:])\n        print_header(cmd_line, introspector)\n\n    print_models(introspector, tables, options.preserve_order, options.views,\n                 options.ignore_unknown, not options.legacy_naming)\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.0859375,
          "content": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend=\"setuptools.build_meta\"\n"
        },
        {
          "name": "runtests.py",
          "type": "blob",
          "size": 4.111328125,
          "content": "#!/usr/bin/env python\nimport optparse\nimport os\nimport shutil\nimport sys\nimport unittest\n\n\nUSER = os.environ.get('USER') or 'root'\n\n\ndef runtests(suite, verbosity=1, failfast=False):\n    runner = unittest.TextTestRunner(verbosity=verbosity, failfast=failfast)\n    results = runner.run(suite)\n    return results.failures, results.errors\n\ndef get_option_parser():\n    usage = 'usage: %prog [-e engine_name, other options] module1, module2 ...'\n    parser = optparse.OptionParser(usage=usage)\n    basic = optparse.OptionGroup(parser, 'Basic test options')\n    basic.add_option(\n        '-e',\n        '--engine',\n        dest='engine',\n        help=('Database engine to test, one of '\n              '[sqlite, postgres, mysql, mysqlconnector, apsw, sqlcipher,'\n              ' cockroachdb, psycopg3]'))\n    basic.add_option('-v', '--verbosity', dest='verbosity', default=1,\n                     type='int', help='Verbosity of output')\n    basic.add_option('-f', '--failfast', action='store_true', default=False,\n                     dest='failfast', help='Exit on first failure/error.')\n    basic.add_option('-s', '--slow-tests', action='store_true', default=False,\n                     dest='slow_tests', help='Run tests that may be slow.')\n    parser.add_option_group(basic)\n\n    db_param_map = (\n        ('MySQL', 'MYSQL', (\n            # param  default disp default val\n            ('host', 'localhost', 'localhost'),\n            ('port', '3306', ''),\n            ('user', USER, USER),\n            ('password', 'blank', ''))),\n        ('Postgresql', 'PSQL', (\n            ('host', 'localhost', os.environ.get('PGHOST', '')),\n            ('port', '5432', ''),\n            ('user', 'postgres', os.environ.get('PGUSER', '')),\n            ('password', 'blank', os.environ.get('PGPASSWORD', '')))),\n        ('CockroachDB', 'CRDB', (\n            # param  default disp default val\n            ('host', 'localhost', 'localhost'),\n            ('port', '26257', ''),\n            ('user', 'root', 'root'),\n            ('password', 'blank', ''))))\n    for name, prefix, param_list in db_param_map:\n        group = optparse.OptionGroup(parser, '%s connection options' % name)\n        for param, default_disp, default_val in param_list:\n            dest = '%s_%s' % (prefix.lower(), param)\n            opt = '--%s-%s' % (prefix.lower(), param)\n            group.add_option(opt, default=default_val, dest=dest, help=(\n                '%s database %s. Default %s.' % (name, param, default_disp)))\n\n        parser.add_option_group(group)\n    return parser\n\ndef collect_tests(args):\n    suite = unittest.TestSuite()\n\n    if not args:\n        import tests\n        module_suite = unittest.TestLoader().loadTestsFromModule(tests)\n        suite.addTest(module_suite)\n    else:\n        cleaned = ['tests.%s' % arg if not arg.startswith('tests.') else arg\n                   for arg in args]\n        user_suite = unittest.TestLoader().loadTestsFromNames(cleaned)\n        suite.addTest(user_suite)\n\n    return suite\n\nif __name__ == '__main__':\n    parser = get_option_parser()\n    options, args = parser.parse_args()\n\n    if options.engine:\n        os.environ['PEEWEE_TEST_BACKEND'] = options.engine\n\n    for db in ('mysql', 'psql', 'crdb'):\n        for key in ('host', 'port', 'user', 'password'):\n            att_name = '_'.join((db, key))\n            value = getattr(options, att_name, None)\n            if value:\n                os.environ['PEEWEE_%s' % att_name.upper()] = value\n\n    os.environ['PEEWEE_TEST_VERBOSITY'] = str(options.verbosity)\n    if options.slow_tests:\n        os.environ['PEEWEE_SLOW_TESTS'] = '1'\n\n    suite = collect_tests(args)\n    failures, errors = runtests(suite, options.verbosity, options.failfast)\n\n    files_to_delete = [\n        'peewee_test.db',\n        'peewee_test',\n        'tmp.db',\n        'peewee_test.bdb.db',\n        'peewee_test.cipher.db']\n    paths_to_delete = ['peewee_test.bdb.db-journal']\n    for filename in files_to_delete:\n        if os.path.exists(filename):\n            os.unlink(filename)\n    for path in paths_to_delete:\n        if os.path.exists(path):\n            shutil.rmtree(path)\n\n    if errors:\n        sys.exit(2)\n    elif failures:\n        sys.exit(1)\n\n    sys.exit(0)\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 7.326171875,
          "content": "import os\nimport platform\nimport re\nimport sys\nimport warnings\ntry:\n    from distutils.command.build_ext import build_ext\n    from distutils.errors import CCompilerError\n    from distutils.errors import DistutilsExecError\n    from distutils.errors import DistutilsPlatformError\nexcept ImportError:\n    from setuptools._distutils.command.build_ext import build_ext\n    from setuptools._distutils.errors import CCompilerError\n    from setuptools._distutils.errors import DistutilsExecError\n    from setuptools._distutils.errors import DistutilsPlatformError\n\nfrom setuptools import setup\nfrom setuptools.extension import Extension\n\nf = open(os.path.join(os.path.dirname(__file__), 'README.rst'))\nreadme = f.read()\nf.close()\n\nextension_support = True  # Assume we are building C extensions.\n\n# Check if Cython is available and use it to generate extension modules. If\n# Cython is not installed, we will fall back to using the pre-generated C files\n# (so long as we're running on CPython).\ntry:\n    from Cython.Build import cythonize\n    from Cython.Distutils import build_ext\n    from Cython.Distutils.extension import Extension\nexcept ImportError:\n    cython_installed = False\nelse:\n    if platform.python_implementation() != 'CPython':\n        cython_installed = extension_support = False\n        warnings.warn('C extensions disabled as you are not using CPython.')\n    else:\n        cython_installed = True\n\nif 'sdist' in sys.argv and not cython_installed:\n    raise Exception('Building sdist requires that Cython be installed.')\n\nif sys.version_info[0] < 3:\n    FileNotFoundError = EnvironmentError\n\nif cython_installed:\n    src_ext = '.pyx'\nelse:\n    src_ext = '.c'\n    cythonize = lambda obj: obj\n\nsqlite_udf_module = Extension(\n    'playhouse._sqlite_udf',\n    ['playhouse/_sqlite_udf' + src_ext])\nsqlite_ext_module = Extension(\n    'playhouse._sqlite_ext',\n    ['playhouse/_sqlite_ext' + src_ext],\n    libraries=['sqlite3'])\n\n\ndef _have_sqlite_extension_support():\n    import shutil\n    import tempfile\n    try:\n        from distutils.ccompiler import new_compiler\n        from distutils.sysconfig import customize_compiler\n    except ImportError:\n        from setuptools.command.build_ext import customize_compiler\n        from setuptools.command.build_ext import new_compiler\n\n    libraries = ['sqlite3']\n    c_code = ('#include <sqlite3.h>\\n\\n'\n              'int main(int argc, char **argv) { return 0; }')\n    tmp_dir = tempfile.mkdtemp(prefix='tmp_pw_sqlite3_')\n    bin_file = os.path.join(tmp_dir, 'test_pw_sqlite3')\n    src_file = bin_file + '.c'\n    with open(src_file, 'w') as fh:\n        fh.write(c_code)\n\n    compiler = new_compiler()\n    customize_compiler(compiler)\n    success = False\n    try:\n        compiler.link_shared_object(\n            compiler.compile([src_file], output_dir=tmp_dir),\n            bin_file,\n            libraries=['sqlite3'])\n    except CCompilerError:\n        print('unable to compile sqlite3 C extensions - missing headers?')\n    except DistutilsExecError:\n        print('unable to compile sqlite3 C extensions - no c compiler?')\n    except DistutilsPlatformError:\n        print('unable to compile sqlite3 C extensions - platform error')\n    except FileNotFoundError:\n        print('unable to compile sqlite3 C extensions - no compiler!')\n    else:\n        success = True\n    shutil.rmtree(tmp_dir)\n    return success\n\n\n# This is set to True if there is extension support and libsqlite3 is found.\nsqlite_extension_support = False\n\nif extension_support:\n    if os.environ.get('NO_SQLITE'):\n        warnings.warn('SQLite extensions will not be built at users request.')\n    elif not _have_sqlite_extension_support():\n        warnings.warn('Could not find libsqlite3, SQLite extensions will not '\n                      'be built.')\n    else:\n        sqlite_extension_support = True\n\n# Exception we will raise to indicate a failure to build C extensions.\nclass BuildFailure(Exception): pass\n\nclass _PeeweeBuildExt(build_ext):\n    def run(self):\n        try:\n            build_ext.run(self)\n        except DistutilsPlatformError:\n            raise BuildFailure()\n\n    def build_extension(self, ext):\n        try:\n            build_ext.build_extension(self, ext)\n        except (CCompilerError, DistutilsExecError, DistutilsPlatformError):\n            raise BuildFailure()\n\ndef _do_setup(c_extensions, sqlite_extensions):\n    if c_extensions and sqlite_extensions:\n        # Only add modules if the required source files are present. This is to\n        # work-around python 3.11 and pip being janky.\n        if sys.version_info < (3, 11, 0):\n            ext_modules = [sqlite_ext_module, sqlite_udf_module]\n        else:\n            ext_modules = []\n            for m in (sqlite_ext_module, sqlite_udf_module):\n                if all(os.path.exists(src) for src in m.sources):\n                    ext_modules.append(m)\n                else:\n                    print('could not find sources for module: %s!' % m.sources)\n                    print('try adding \"cython\" to your local pyproject.toml')\n    else:\n        ext_modules = None\n\n    with open('peewee.py', 'rt') as fh:\n        version, = [l for l in fh.readlines() if l.startswith('__version__')]\n        version, = re.search(r'\\'([\\d\\.]+)\\'', version).groups()\n\n    setup(\n        name='peewee',\n        version=version,\n        description='a little orm',\n        long_description=readme,\n        author='Charles Leifer',\n        author_email='coleifer@gmail.com',\n        url='https://github.com/coleifer/peewee/',\n        packages=['playhouse'],\n        py_modules=['peewee', 'pwiz'],\n        classifiers=[\n            'Development Status :: 5 - Production/Stable',\n            'Intended Audience :: Developers',\n            'License :: OSI Approved :: MIT License',\n            'Operating System :: OS Independent',\n            'Programming Language :: Python',\n            'Programming Language :: Python :: 2',\n            'Programming Language :: Python :: 2.7',\n            'Programming Language :: Python :: 3',\n            'Programming Language :: Python :: 3.4',\n            'Programming Language :: Python :: 3.5',\n            'Programming Language :: Python :: 3.6',\n            'Programming Language :: Python :: 3.7',\n            'Programming Language :: Python :: 3.8',\n            'Programming Language :: Python :: 3.9',\n            'Programming Language :: Python :: 3.10',\n            'Programming Language :: Python :: 3.11',\n            'Programming Language :: Python :: 3.12',\n            'Programming Language :: Python :: 3.13',\n            #'Programming Language :: Python :: 3.14',\n            #'Programming Language :: Python :: 3.15',\n            #'Programming Language :: Python :: 999.99',\n            'Topic :: Database',\n            'Topic :: Software Development :: Libraries :: Python Modules',\n        ],\n        license='MIT License',\n        platforms=['any'],\n        project_urls={\n            'Documentation': 'http://docs.peewee-orm.com',\n            'Source': 'https://github.com/coleifer/peewee'},\n        scripts=['pwiz.py'],\n        zip_safe=False,\n        cmdclass={'build_ext': _PeeweeBuildExt},\n        ext_modules=cythonize(ext_modules))\n\n\nif extension_support:\n    try:\n        _do_setup(extension_support, sqlite_extension_support)\n    except BuildFailure:\n        print('#' * 75)\n        print('Error compiling C extensions, C extensions will not be built.')\n        print('#' * 75)\n        _do_setup(False, False)\nelse:\n    _do_setup(False, False)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}