{
  "metadata": {
    "timestamp": 1736561358367,
    "page": 392,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "bentoml/OpenLLM",
      "stars": 10362,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.2392578125,
          "content": "root = true\n\n[*]\nend_of_line = lf\ntrim_trailing_whitespace = true\ncharset = utf-8\nindent_style = space\nindent_size = 2\n\n[/node_modules/*]\nindent_size = unset\nindent_style = unset\n\n[{package.json,.travis.yml,.eslintrc.json}]\nindent_style = space\n"
        },
        {
          "name": ".envrc.template",
          "type": "blob",
          "size": 0.0263671875,
          "content": "export PAPERSPACE_API_KEY=\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 1.1240234375,
          "content": "# You can use this file with 'git config blame.ignoreRevsFile .git-blame-ignore-revs'\n# 07/31/2023: Style guidelines\n8c2867d26dfff8a4cf33bc59d5a8dee159f3256a\n# 08/22/2023: Running yapf with guidelines\n1488fbb167a0ae5b0770f33f50a7ee7f7b2223c9\neddbc063743b198d72c21bd7dced59dbd949b9f1\n# 08/23/2023: Synchronize style guidelines\n787ce1b3b63ecbacde371550f46fa7429f3e4db2\n# 08/25/2023: Consistency between yapf and ruff\n46c890480640294c3f34706d595559c7ea97dac5\n# 08/26/2023: Add one blank space between top level definition to similar to Google Style Guide\n806a663e4aa2b174969241f6e310e05762e233f0\n# 08/30/2023: Update to google style\nb545ad2ad1e3acbb69f6578d8a5ee03613867505\n# 09/01/2023: ignore new line split on comma-separated item\n7d893e6cd217ddfe845210503c8f2cf1667d16b6\n# 11/09/2023: running ruff format preview\nac377fe490bd886cf76c3855e6a2a50fc0e03b51\n# 11/26/2023: reduce line overhead\n69aae34cf4e6995edf2e2dc1a669fc4bdecf959a\n# 11/28/2023: compact\nd04309188b8fccec6a3ff36a893099806f560551\n# 12/14/2023: using ruff to 150 LL\nc8c9663d06e49da327ed53a22bea79f78d808aa9\n# 03/15/2024: ignore new ruff formatter\n727361ced761c82351ff539fcafa7af62fb5e2f0\n"
        },
        {
          "name": ".git_archival.txt",
          "type": "blob",
          "size": 0.1220703125,
          "content": "node: $Format:%H$\nnode-date: $Format:%cI$\ndescribe-name: $Format:%(describe:tags=true,match=*[0-9]*)$\nref-names: $Format:%D$\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0888671875,
          "content": "* text=auto eol=lf\n# Needed for setuptools-scm-git-archive\n.git_archival.txt  export-subst\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.048828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n*.whl\n# Environments\nvenv/\n.envrc\n_version.py\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.513671875,
          "content": "ci:\n  autoupdate_schedule: weekly\n  skip: [mypy]\n  autofix_commit_msg: \"ci: auto fixes from pre-commit.ci\\n\\nFor more information, see https://pre-commit.ci\"\n  autoupdate_commit_msg: \"ci: pre-commit autoupdate [pre-commit.ci]\"\n  autofix_prs: true\ndefault_language_version:\n  python: python3.11 # NOTE: sync with .python-version-default\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: \"v0.8.6\"\n    hooks:\n      - id: ruff\n        alias: r\n        verbose: true\n        args: [--exit-non-zero-on-fix, --show-fixes, --fix]\n        types_or: [python, pyi, jupyter]\n      - id: ruff-format\n        alias: rf\n        verbose: true\n        types_or: [python, pyi, jupyter]\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: \"v1.14.1\"\n    hooks:\n      - id: mypy\n        args: [--strict]\n        additional_dependencies: [pydantic]\n  - repo: https://github.com/editorconfig-checker/editorconfig-checker.python\n    rev: \"3.0.3\"\n    hooks:\n      - id: editorconfig-checker\n        verbose: true\n        alias: ec\n        types_or: [python]\n  - repo: meta\n    hooks:\n      - id: check-hooks-apply\n      - id: check-useless-excludes\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: trailing-whitespace\n        verbose: true\n      - id: end-of-file-fixer\n        verbose: true\n      - id: check-yaml\n        args: [\"--unsafe\"]\n      - id: check-toml\n      - id: check-docstring-first\n      - id: check-added-large-files\n      - id: debug-statements\n      - id: check-merge-conflict\n"
        },
        {
          "name": ".python-version-default",
          "type": "blob",
          "size": 0.0048828125,
          "content": "3.11\n"
        },
        {
          "name": ".ruff.toml",
          "type": "blob",
          "size": 0.6533203125,
          "content": "extend-include = [\"*.ipynb\"]\npreview = true\nline-length = 119\nindent-width = 4\n\n[format]\npreview = true\nquote-style = \"single\"\nindent-style = \"space\"\nskip-magic-trailing-comma = true\ndocstring-code-format = true\n\n[lint]\nignore = [\n  \"RUF012\",\n  \"ANN\",    # Mypy is better at this\n  \"E722\",\n]\nselect = [\n  \"F\",\n  \"G\",    # flake8-logging-format\n  \"PERF\", # perflint\n  \"RUF\",  # Ruff-specific rules\n  \"W6\",\n  \"E71\",\n  \"E72\",\n  \"E112\",\n  \"E113\",\n  # \"E124\",\n  \"E203\",\n  \"E272\",\n  # \"E303\",\n  # \"E304\",\n  # \"E501\",\n  # \"E502\",\n  \"E702\",\n  \"E703\",\n  \"E731\",\n  \"W191\",\n  \"W291\",\n  \"W293\",\n  \"UP039\", # unnecessary-class-parentheses\n]\n\n[lint.pydocstyle]\nconvention = \"google\"\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 2.0078125,
          "content": "cff-version: 1.2.0\ntitle: 'OpenLLM: Operating LLMs in production'\nmessage: >-\n  If you use this software, please cite it using these\n  metadata.\ntype: software\nauthors:\n  - given-names: Aaron\n    family-names: Pham\n    email: aarnphm@bentoml.com\n    orcid: 'https://orcid.org/0009-0008-3180-5115'\n  - given-names: Chaoyu\n    family-names: Yang\n    email: chaoyu@bentoml.com\n  - given-names: Sean\n    family-names: Sheng\n    email: ssheng@bentoml.com\n  - given-names: Shenyang\n    family-names: Zhao\n    email: larme@bentoml.com\n  - given-names: Sauyon\n    family-names: Lee\n    email: sauyon@bentoml.com\n  - given-names: Bo\n    family-names: Jiang\n    email: jiang@bentoml.com\n  - given-names: Fog\n    family-names: Dong\n    email: fog@bentoml.com\n  - given-names: Xipeng\n    family-names: Guan\n    email: xipeng@bentoml.com\n  - given-names: Frost\n    family-names: Ming\n    email: frost@bentoml.com\nrepository-code: 'https://github.com/bentoml/OpenLLM'\nurl: 'https://bentoml.com/'\nabstract: >-\n  OpenLLM is an open platform for operating large language\n  models (LLMs) in production. With OpenLLM, you can run\n  inference with any open-source large-language models,\n  deploy to the cloud or on-premises, and build powerful AI\n  apps. It has built-in support for a wide range of\n  open-source LLMs and model runtime, including StableLM,\n  Falcon, Dolly, Flan-T5, ChatGLM, StarCoder and more.\n  OpenLLM helps serve LLMs over RESTful API or gRPC with one\n  command or query via WebUI, CLI, our Python/Javascript\n  client, or any HTTP client. It provides first-class\n  support for LangChain, BentoML and Hugging Face that\n  allows you to easily create your own AI apps by composing\n  LLMs with other models and services. Last but not least,\n  it automatically generates LLM server OCI-compatible\n  Container Images or easily deploys as a serverless\n  endpoint via BentoCloud.\nkeywords:\n  - MLOps\n  - LLMOps\n  - LLM\n  - Infrastructure\n  - Transformers\n  - LLM Serving\n  - Model Serving\n  - Serverless Deployment\nlicense: Apache-2.0\ndate-released: '2023-06-13'\n"
        },
        {
          "name": "DEVELOPMENT.md",
          "type": "blob",
          "size": 4.4921875,
          "content": "# Developer Guide\n\nThis Developer Guide is designed to help you contribute to the OpenLLM project.\nFollow these steps to set up your development environment and learn the process\nof contributing to our open-source project.\n\nJoin our [Discord Channel](https://l.bentoml.com/join-openllm-discord) and reach\nout to us if you have any question!\n\n## Table of Contents\n\n- [Developer Guide](#developer-guide)\n  - [Table of Contents](#table-of-contents)\n  - [Setting Up Your Development Environment](#setting-up-your-development-environment)\n  - [Development Workflow](#development-workflow)\n    - [Adding new models](#adding-new-models)\n    - [Adding bentos](#adding-new-models)\n    - [Adding repos](#adding-new-models)\n\n## Setting Up Your Development Environment\n\nBefore you can start developing, you'll need to set up your environment:\n\n1. Ensure you have [Git](https://git-scm.com/), and\n   [Python3.8+](https://www.python.org/downloads/) installed.\n2. Fork the OpenLLM repository from GitHub.\n3. Clone the forked repository from GitHub:\n\n   ```bash\n   git clone git@github.com:username/OpenLLM.git && cd openllm\n   ```\n\n4. Add the OpenLLM upstream remote to your local OpenLLM clone:\n\n   ```bash\n   git remote add upstream git@github.com:bentoml/OpenLLM.git\n   ```\n\n5. Configure git to pull from the upstream remote:\n\n   ```bash\n   git switch main # ensure you're on the main branch\n   git fetch upstream --tags\n   git branch --set-upstream-to=upstream/main\n   ```\n\n6. (Optional) Link `.python-version-default` to `.python-version`:\n\n   ```bash\n   ln .python-version-default .python-version\n   ```\n\n## Development Workflow\n\nThere are a few ways to contribute to the repository structure for OpenLLM:\n\n### Adding new models\n\n1. [recipe.yaml](./recipe.yaml) contains all related-metadata for generating new LLM-based bentos. To add a new LLM, the following structure should be adhere to:\n\n```yaml\n\"<model_name>:<model_tag>\":\n  project: vllm-chat\n  service_config:\n    name: phi3\n    traffic:\n      timeout: 300\n    resources:\n      gpu: 1\n      gpu_type: nvidia-tesla-l4\n  engine_config:\n    model: microsoft/Phi-3-mini-4k-instruct\n    max_model_len: 4096\n    dtype: half\n  chat_template: phi-3\n```\n\n- `<model_name>` represents the type of model to be supported. Currently supports `phi3`, `llama2`, `llama3`, `gemma`\n\n- `<model_tag>` emphasizes the type of model and its related metadata. The convention would include `<model_size>-<model_type>-<precision>[-<quantization>]`\n  For example:\n\n  - `microsoft/Phi-3-mini-4k-instruct` should be represented as `3.8b-instruct-fp16`.\n  - `TheBloke/Llama-2-7B-Chat-AWQ` would be `7b-chat-awq-4bit`\n\n- `project` would be used as the basis for the generated bento. Currently, most models should use `vllm-chat` as default.\n\n- `service_config` entails all BentoML-related [configuration](https://docs.bentoml.com/en/latest/guides/configurations.html) to run this bento.\n\n> [!NOTE]\n>\n> We recommend to include the following field for `service_config`:\n>\n> - `name` should be the same as `<model_name>`\n> - `resources` includes the available accelerator that can run this models. See more [here](https://docs.bentoml.com/en/latest/guides/configurations.html#resources)\n\n- `engine_config` are fields to be used for vLLM engine. See more supported arguments in [`AsyncEngineArgs`](https://github.com/vllm-project/vllm/blob/7cd2ebb0251fd1fd0eec5c93dac674603a22eddd/vllm/engine/arg_utils.py#L799). We recommend to always include `model`, `max_model_len`, `dtype` and `trust_remote_code`.\n\n- If the model is a chat model, `chat_template` should be used. Add the appropriate `chat_template` under [chat_template directory](./vllm-chat/chat_templates/) should you decide to do so.\n\n2. You can then run `BENTOML_HOME=$(openllm repo default)/bentoml/bentos python make.py <model_name>:<model_tag>` to generate the required bentos.\n\n3. You can then submit a [Pull request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request) to `openllm` with the recipe changes\n\n### Adding bentos\n\nOpenLLM now also manages a [generated bento repository](https://github.com/bentoml/openllm-models/tree/main). If you update and modify and generated bentos, make sure to update the recipe and added the generated bentos under `bentoml/bentos`.\n\n### Adding repos\n\nIf you wish to create a your own managed git repo, you should follow the structure of [bentoml/openllm-models](https://github.com/bentoml/openllm-models/tree/main).\n\nTo add your custom repo, do `openllm repo add <repo_alias> <git_url>`\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.4736328125,
          "content": "# 🦾 OpenLLM: Self-Hosting LLMs Made Easy\n\n[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202-green.svg)](https://github.com/bentoml/OpenLLM/blob/main/LICENSE)\n[![Releases](https://img.shields.io/pypi/v/openllm.svg?logo=pypi&label=PyPI&logoColor=gold)](https://pypi.org/project/openllm)\n[![CI](https://results.pre-commit.ci/badge/github/bentoml/OpenLLM/main.svg)](https://results.pre-commit.ci/latest/github/bentoml/OpenLLM/main)\n[![X](https://badgen.net/badge/icon/@bentomlai/000000?icon=twitter&label=Follow)](https://twitter.com/bentomlai)\n[![Community](https://badgen.net/badge/icon/Community/562f5d?icon=slack&label=Join)](https://l.bentoml.com/join-slack)\n\nOpenLLM allows developers to run **any open-source LLMs** (Llama 3.3, Qwen2.5, Phi3 and [more](#supported-models)) or **custom models** as **OpenAI-compatible APIs** with a single command. It features a [built-in chat UI](#chat-ui), state-of-the-art inference backends, and a simplified workflow for creating enterprise-grade cloud deployment with Docker, Kubernetes, and [BentoCloud](#deploy-to-bentocloud).\n\nUnderstand the [design philosophy of OpenLLM](https://www.bentoml.com/blog/from-ollama-to-openllm-running-llms-in-the-cloud).\n\n## Get Started\n\nRun the following commands to install OpenLLM and explore it interactively.\n\n```bash\npip install openllm  # or pip3 install openllm\nopenllm hello\n```\n\n![hello](https://github.com/user-attachments/assets/5af19f23-1b34-4c45-b1e0-a6798b4586d1)\n\n## Supported models\n\nOpenLLM supports a wide range of state-of-the-art open-source LLMs. You can also add a [model repository to run custom models](#set-up-a-custom-repository) with OpenLLM.\n\n| Model            | Parameters | Quantization | Required GPU  | Start a Server                      |\n| ---------------- | ---------- | ------------ | ------------- | ----------------------------------- |\n| Llama 3.3        | 70B        | -            | 80Gx2         | `openllm serve llama3.3:70b`        |\n| Llama 3.2        | 3B         | -            | 12G           | `openllm serve llama3.2:3b`         |\n| Llama 3.2 Vision | 11B        | -            | 80G           | `openllm serve llama3.2:11b-vision` |\n| Mistral          | 7B         | -            | 24G           | `openllm serve mistral:7b`          |\n| Qwen 2.5         | 1.5B       | -            | 12G           | `openllm serve qwen2.5:1.5b`        |\n| Qwen 2.5 Coder   | 7B         | -            | 24G           | `openllm serve qwen2.5-coder:7b`    |\n| Gemma 2          | 9B         | -            | 24G           | `openllm serve gemma2:9b`           |\n| Phi3             | 3.8B       | -            | 12G           | `openllm serve phi3:3.8b`           |\n\n...\n\nFor the full model list, see the [OpenLLM models repository](https://github.com/bentoml/openllm-models).\n\n## Start an LLM server\n\nTo start an LLM server locally, use the `openllm serve` command and specify the model version.\n\n> [!NOTE]\n> OpenLLM does not store model weights. A Hugging Face token (HF_TOKEN) is required for gated models.\n> 1. Create your Hugging Face token [here](https://huggingface.co/settings/tokens).\n> 2. Request access to the gated model, such as [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B).\n> 3. Set your token as an environment variable by running:\n>    ```bash\n>    export HF_TOKEN=<your token>\n>    ```\n\n```bash\nopenllm serve llama3:8b\n```\n\nThe server will be accessible at [http://localhost:3000](http://localhost:3000/), providing OpenAI-compatible APIs for interaction. You can call the endpoints with different frameworks and tools that support OpenAI-compatible APIs. Typically, you may need to specify the following:\n\n- **The API host address**: By default, the LLM is hosted at [http://localhost:3000](http://localhost:3000/).\n- **The model name:** The name can be different depending on the tool you use.\n- **The API key**: The API key used for client authentication. This is optional.\n\nHere are some examples:\n\n<details>\n\n<summary>OpenAI Python client</summary>\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url='http://localhost:3000/v1', api_key='na')\n\n# Use the following func to get the available models\n# model_list = client.models.list()\n# print(model_list)\n\nchat_completion = client.chat.completions.create(\n    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain superconductors like I'm five years old\"\n        }\n    ],\n    stream=True,\n)\nfor chunk in chat_completion:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n```\n\n</details>\n\n\n<details>\n\n<summary>LlamaIndex</summary>\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(api_bese=\"http://localhost:3000/v1\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", api_key=\"dummy\")\n...\n```\n</details>\n\n## Chat UI\n\nOpenLLM provides a chat UI at the `/chat` endpoint for the launched LLM server at http://localhost:3000/chat.\n\n<img width=\"800\" alt=\"openllm_ui\" src=\"https://github.com/bentoml/OpenLLM/assets/5886138/8b426b2b-67da-4545-8b09-2dc96ff8a707\">\n\n## Chat with a model in the CLI\n\nTo start a chat conversation in the CLI, use the `openllm run` command and specify the model version.\n\n```bash\nopenllm run llama3:8b\n```\n\n## Model repository\n\nA model repository in OpenLLM represents a catalog of available LLMs that you can run. OpenLLM provides a default model repository that includes the latest open-source LLMs like Llama 3, Mistral, and Qwen2, hosted at [this GitHub repository](https://github.com/bentoml/openllm-models). To see all available models from the default and any added repository, use:\n\n```bash\nopenllm model list\n```\n\nTo ensure your local list of models is synchronized with the latest updates from all connected repositories, run:\n\n```bash\nopenllm repo update\n```\n\nTo review a model’s information, run:\n\n```bash\nopenllm model get llama3:8b\n```\n\n### Add a model to the default model repository\n\nYou can contribute to the default model repository by adding new models that others can use. This involves creating and submitting a Bento of the LLM. For more information, check out this [example pull request](https://github.com/bentoml/openllm-models/pull/1).\n\n### Set up a custom repository\n\nYou can add your own repository to OpenLLM with custom models. To do so, follow the format in the default OpenLLM model repository with a `bentos` directory to store custom LLMs. You need to [build your Bentos with BentoML](https://docs.bentoml.com/en/latest/guides/build-options.html) and submit them to your model repository.\n\nFirst, prepare your custom models in a `bentos` directory following the guidelines provided by [BentoML to build Bentos](https://docs.bentoml.com/en/latest/guides/build-options.html). Check out the [default model repository](https://github.com/bentoml/openllm-repo) for an example and read the [Developer Guide](https://github.com/bentoml/OpenLLM/blob/main/DEVELOPMENT.md) for details.\n\nThen, register your custom model repository with OpenLLM:\n\n```bash\nopenllm repo add <repo-name> <repo-url>\n```\n\n**Note**: Currently, OpenLLM only supports adding public repositories.\n\n## Deploy to BentoCloud\n\nOpenLLM supports LLM cloud deployment via BentoML, the unified model serving framework, and BentoCloud, an AI inference platform for enterprise AI teams. BentoCloud provides fully-managed infrastructure optimized for LLM inference with autoscaling, model orchestration, observability, and many more, allowing you to run any AI model in the cloud.\n\n[Sign up for BentoCloud](https://www.bentoml.com/) for free and [log in](https://docs.bentoml.com/en/latest/bentocloud/how-tos/manage-access-token.html). Then, run `openllm deploy` to deploy a model to BentoCloud:\n\n```bash\nopenllm deploy llama3:8b\n```\n\n> [!NOTE]\n> If you are deploying a gated model, make sure to set HF_TOKEN in enviroment variables.\n\nOnce the deployment is complete, you can run model inference on the BentoCloud console:\n\n<img width=\"800\" alt=\"bentocloud_ui\" src=\"https://github.com/bentoml/OpenLLM/assets/65327072/4f7819d9-73ea-488a-a66c-f724e5d063e6\">\n\n## Community\n\nOpenLLM is actively maintained by the BentoML team. Feel free to reach out and join us in our pursuit to make LLMs more accessible and easy to use 👉 [Join our Slack community!](https://l.bentoml.com/join-slack)\n\n## Contributing\n\nAs an open-source project, we welcome contributions of all kinds, such as new features, bug fixes, and documentation. Here are some of the ways to contribute:\n\n- Repost a bug by [creating a GitHub issue](https://github.com/bentoml/OpenLLM/issues/new/choose).\n- [Submit a pull request](https://github.com/bentoml/OpenLLM/compare) or help review other developers’ [pull requests](https://github.com/bentoml/OpenLLM/pulls).\n- Add an LLM to the OpenLLM default model repository so that other users can run your model. See the [pull request template](https://github.com/bentoml/openllm-models/pull/1).\n- Check out the [Developer Guide](https://github.com/bentoml/OpenLLM/blob/main/DEVELOPMENT.md) to learn more.\n\n## Acknowledgements\n\nThis project uses the following open-source projects:\n\n- [bentoml/bentoml](https://github.com/bentoml/bentoml) for production level model serving\n- [vllm-project/vllm](https://github.com/vllm-project/vllm) for production level LLM backend\n- [blrchen/chatgpt-lite](https://github.com/blrchen/chatgpt-lite) for a fancy Web Chat UI\n- [chujiezheng/chat_templates](https://github.com/chujiezheng/chat_templates)\n- [astral-sh/uv](https://github.com/astral-sh/uv) for blazing fast model requirements installing\n\nWe are grateful to the developers and contributors of these projects for their hard work and dedication.\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.8232421875,
          "content": "[project]\nname = \"openllm\"\ndescription = \"OpenLLM: Self-hosting LLMs Made Easy.\"\nreadme = { file = \"README.md\", content-type = \"text/markdown\" }\nauthors = [{ name = \"BentoML Team\", email = \"contact@bentoml.com\" }]\ndynamic = [\"version\"]\nclassifiers = [\n  \"Development Status :: 5 - Production/Stable\",\n  \"Environment :: GPU :: NVIDIA CUDA\",\n  \"Environment :: GPU :: NVIDIA CUDA :: 12\",\n  \"Environment :: GPU :: NVIDIA CUDA :: 11.8\",\n  \"Environment :: GPU :: NVIDIA CUDA :: 11.7\",\n  \"License :: OSI Approved :: Apache Software License\",\n  \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n  \"Topic :: Software Development :: Libraries\",\n  \"Operating System :: OS Independent\",\n  \"Intended Audience :: Developers\",\n  \"Intended Audience :: Science/Research\",\n  \"Intended Audience :: System Administrators\",\n  \"Typing :: Typed\",\n  \"Programming Language :: Python\",\n  \"Programming Language :: Python :: 3\",\n  \"Programming Language :: Python :: 3 :: Only\",\n  \"Programming Language :: Python :: 3.8\",\n  \"Programming Language :: Python :: 3.9\",\n  \"Programming Language :: Python :: 3.10\",\n  \"Programming Language :: Python :: 3.11\",\n  \"Programming Language :: Python :: 3.12\",\n  \"Programming Language :: Python :: Implementation :: CPython\",\n  \"Programming Language :: Python :: Implementation :: PyPy\",\n]\ndependencies = [\n  \"bentoml\",\n  \"typer\",\n  \"questionary\",\n  \"pyaml\",\n  \"psutil\",\n  \"pathlib\",\n  \"pip_requirements_parser\",\n  \"nvidia-ml-py\",\n  \"dulwich\",\n  \"tabulate\",\n  \"uv\",\n  \"openai==1.59.3\",\n]\nkeywords = [\n  \"MLOps\",\n  \"AI\",\n  \"BentoML\",\n  \"Model Serving\",\n  \"Model Deployment\",\n  \"LLMOps\",\n  \"Falcon\",\n  \"Vicuna\",\n  \"Llama 2\",\n  \"Fine tuning\",\n  \"Serverless\",\n  \"Large Language Model\",\n  \"Generative AI\",\n  \"StableLM\",\n  \"Alpaca\",\n  \"PyTorch\",\n  \"Mistral\",\n  \"vLLM\",\n  \"Transformers\",\n]\nlicense = \"Apache-2.0\"\nrequires-python = \">=3.9\"\n\n[project.scripts]\nopenllm = \"openllm.__main__:app\"\n\n[project.urls]\nBlog = \"https://modelserving.com\"\nDocumentation = \"https://github.com/bentoml/OpenLLM#readme\"\nGitHub = \"https://github.com/bentoml/OpenLLM\"\nHomepage = \"https://bentoml.com\"\nTracker = \"https://github.com/bentoml/OpenLLM/issues\"\nTwitter = \"https://twitter.com/bentomlai\"\n\n[tool.typer]\nsrc-dir = \"src/openllm\"\n\n[build-system]\nrequires = [\"hatchling==1.27.0\", \"hatch-vcs==0.4.0\"]\nbuild-backend = 'hatchling.build'\n\n[tool.hatch.version]\nsource = \"vcs\"\nfallback-version = \"0.0.0\"\n[tool.hatch.build.hooks.vcs]\nversion-file = \"src/openllm/_version.py\"\n[tool.hatch.version.raw-options]\ngit_describe_command = [\n  \"git\",\n  \"describe\",\n  \"--dirty\",\n  \"--tags\",\n  \"--long\",\n  \"--first-parent\",\n]\nversion_scheme = \"post-release\"\nfallback_version = \"0.0.0\"\n[tool.hatch.metadata]\nallow-direct-references = true\n[tool.hatch.build.targets.wheel]\nonly-include = [\"src/openllm\"]\nsources = [\"src\"]\n[tool.hatch.build.targets.sdist]\nexclude = [\"/.git_archival.txt\", \"/.python-version-default\"]\n"
        },
        {
          "name": "pyrightconfig.json",
          "type": "blob",
          "size": 0.4296875,
          "content": "{\n  \"useLibraryCodeForTypes\": true,\n  \"verboseOutput\": true,\n  \"define\": {\n    \"MYPY\": true\n  },\n  \"venvPath\": \".\",\n  \"venv\": \".venv\",\n  \"pythonVersion\": \"3.9\",\n  \"enableExperimentalFeatures\": true,\n  \"reportMissingImports\": \"warning\",\n  \"reportMissingTypeStubs\": false,\n  \"reportPrivateUsage\": \"warning\",\n  \"reportUnknownArgumentType\": \"warning\",\n  \"reportUnsupportedDunderAll\": \"warning\",\n  \"reportWildcardImportFromLibrary\": \"warning\"\n}\n"
        },
        {
          "name": "release.sh",
          "type": "blob",
          "size": 2.7431640625,
          "content": "#!/usr/bin/env bash\n\nset -e\n\n# Function to print script usage\nprint_usage() {\n  echo \"Usage: $0 [--release <major|minor|patch|alpha>]\"\n}\n\n# Function to validate release argument\nvalidate_release() {\n  local release=$1\n\n  if [[ $release == \"major\" || $release == \"minor\" || $release == \"patch\" || $release == \"alpha\" ]]; then\n    return 0\n  else\n    return 1\n  fi\n}\n\n# Check if release flag is provided\nif [[ $1 == \"--release\" ]]; then\n  # Check if release argument is provided\n  if [[ -z $2 ]]; then\n    echo \"Error: No release argument provided.\"\n    print_usage\n    exit 1\n  fi\n\n  release=$2\n\n  if ! validate_release \"$release\"; then\n    echo \"Error: Invalid release argument. Only 'major', 'minor', 'patch', or 'alpha' are allowed.\"\n    print_usage\n    exit 1\n  fi\nelse\n  echo \"Error: Unknown option or no option provided.\"\n  print_usage\n  exit 1\nfi\n\n# Get the current version and separate the alpha part if it exists\nversion=\"$(git describe --tags \"$(git rev-list --tags --max-count=1)\")\"\nVERSION=\"${version#v}\"\n\n# Initialize variables for alpha versioning\nALPHA=\"\"\nALPHA_NUM=0\n\n# Check if current version is an alpha version and split accordingly\nif [[ $VERSION =~ -alpha ]]; then\n  IFS='-' read -r BASE_VERSION ALPHA <<<\"$VERSION\"\n  if [[ $ALPHA =~ [.] ]]; then\n    IFS='.' read -r ALPHA ALPHA_NUM <<<\"$ALPHA\"\n  fi\nelse\n  BASE_VERSION=\"$VERSION\"\nfi\n\n# Save the current value of IFS to restore it later and split the base version\nOLD_IFS=$IFS\nIFS='.'\nread -ra VERSION_BITS <<<\"$BASE_VERSION\"\nIFS=$OLD_IFS\n\n# Assign split version numbers\nVNUM1=${VERSION_BITS[0]}\nVNUM2=${VERSION_BITS[1]}\nVNUM3=${VERSION_BITS[2]}\n\n# Adjust the version numbers based on the release type\nif [[ $release == 'major' ]]; then\n  VNUM1=$((VNUM1 + 1))\n  VNUM2=0\n  VNUM3=0\n  ALPHA=\"\" # Reset alpha for major release\nelif [[ $release == 'minor' ]]; then\n  if [[ -n $ALPHA ]]; then\n    ALPHA=\"\" # Remove alpha suffix for minor release from an alpha version\n  else\n    VNUM2=$((VNUM2 + 1))\n    VNUM3=0\n  fi\nelif [[ $release == 'patch' ]]; then\n  VNUM3=$((VNUM3 + 1))\n  ALPHA=\"\" # Reset alpha for patch release\nelif [[ $release == 'alpha' ]]; then\n  if [ -n \"$ALPHA\" ]; then\n    ALPHA_NUM=$((ALPHA_NUM + 1))\n  else\n    VNUM2=$((VNUM2 + 1))\n    VNUM3=0\n    ALPHA=\"alpha\"\n    ALPHA_NUM=0\n  fi\nfi\n\n# Construct the new version string\nif [ -n \"$ALPHA\" ]; then\n  if ((ALPHA_NUM > 0)); then\n    RELEASE_VERSION=\"$VNUM1.$VNUM2.$VNUM3-alpha.$ALPHA_NUM\"\n  else\n    RELEASE_VERSION=\"$VNUM1.$VNUM2.$VNUM3-alpha\"\n  fi\nelse\n  RELEASE_VERSION=\"$VNUM1.$VNUM2.$VNUM3\"\nfi\n\necho \"Commit count: $(git rev-list --count HEAD)\"\necho \"Releasing tag ${RELEASE_VERSION}...\" && git tag -a \"v${RELEASE_VERSION}\" -m \"Release ${RELEASE_VERSION} [generated by GitHub Actions]\"\ngit push origin \"v${RELEASE_VERSION}\"\necho \"Finish releasing OpenLLM ${RELEASE_VERSION}\"\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}