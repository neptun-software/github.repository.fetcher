{
  "metadata": {
    "timestamp": 1736561197392,
    "page": 166,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "openai/evals",
      "stars": 15317,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0654296875,
          "content": "evals/registry/data/**/*.jsonl filter=lfs diff=lfs merge=lfs -text\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2412109375,
          "content": "__pycache__/\nevals.egg-info/\n\n.venv/\nvenv/\n\n# MacOS folder metadata\n.DS_Store\n.vscode/\n\n# PyCharm folder metadata\n.idea/\n\nbuild\n\nopenai-key.txt\n*.code-workspace\n\n# Ignore run_experiments.sh results\nevals/elsuite/**/logs/\nevals/elsuite/**/outputs/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.3994140625,
          "content": "repos:\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: 'v1.3.0'\n    hooks:\n      - id: mypy\n        args: [\"--config-file=mypy.ini\", \"--no-site-packages\"]\n\n  - repo: https://github.com/psf/black\n    rev: 22.8.0\n    hooks:\n      - id: black\n        args: [--line-length=100, --exclude=\"\"]\n\n  # this is not technically always safe but usually is\n  # use comments `# isort: off` and `# isort: on` to disable/re-enable isort\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        args: [--line-length=100, --profile=black]\n\n  # this is slightly dangerous because python imports have side effects\n  # and this tool removes unused imports, which may be providing\n  # necessary side effects for the code to run\n  - repo: https://github.com/PyCQA/autoflake\n    rev: v1.6.1\n    hooks:\n      - id: autoflake\n        args:\n          - \"--in-place\"\n          - \"--expand-star-imports\"\n          - \"--remove-duplicate-keys\"\n          - \"--remove-unused-variables\"\n          - \"--remove-all-unused-imports\"\n        exclude: \"evals/__init__.py\"\n   \n  # This allows ruff to run and autofix the code\n  # The line length is so high because some of the evals are very long\n  # TODO: fix the evals and then reduce the line length here\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.0.277\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix, --line-length=767]"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 6.857421875,
          "content": "MIT License\n\nCopyright (c) 2023 OpenAI\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nNOTE: This license applies to all parts of this repository except for the datasets specified below. See the respective datasets for their individual licenses.\n\n### Dataset Licenses\n\n#### Text Compression\n- **Location**: evals/registry/data/text_compression\n- **Components**:\n    - **c4**:\n        - **License**: Open Data Commons Attribution License: http://opendatacommons.org/licenses/by/1.0/\n        - **Source**: https://huggingface.co/datasets/c4\n    - **openwebtext**:\n        - **License**: Creative Commons CC0 license (“no rights reserved”): https://creativecommons.org/share-your-work/public-domain/cc0/\n        - **Source**: https://huggingface.co/datasets/openwebtext\n    - **oscar**:\n        - **License**: Creative Commons CC0 license (“no rights reserved”): https://creativecommons.org/share-your-work/public-domain/cc0/\n        - **Source**: https://huggingface.co/datasets/oscar\n    - **wikipedia**:\n        - **License**:  Creative Commons Attribution-ShareAlike 3.0 Unported License (CC BY-SA): https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License  and the GNU Free Documentation License (GFDL): https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License\n        - **Source**: https://huggingface.co/datasets/wikipedia\n    - **codeparrot/github-code**:\n        - **License**: MIT License: https://opensource.org/license/mit/\n        - **Source**: https://huggingface.co/datasets/codeparrot/github-code\n    - **Abirate/english_quotes**:\n        - **License**: Creative Commons Attribution 4.0 International License: https://creativecommons.org/licenses/by/4.0/legalcode.txt\n        - **Source**: https://huggingface.co/datasets/Abirate/english_quotes\n\n#### Steganography\n- **Location**: evals/registry/data/steganography\n- **Components**:\n    - **Abirate/english_quotes**:\n        - **License**: Creative Commons Attribution 4.0 International License https://creativecommons.org/licenses/by/4.0/legalcode.txt\n        - **Source**: https://huggingface.co/datasets/Abirate/english_quotes\n    - **PiC/phrase_similarity**:\n        - **License**: Creative Commons NonCommercial (CC BY-NC 4.0) https://creativecommons.org/licenses/by-nc/4.0/legalcode\n        - **Source**: https://huggingface.co/datasets/PiC/phrase_similarity\n    - **wikipedia**:\n        - **License**:  Creative Commons Attribution-ShareAlike 3.0 Unported License (CC BY-SA): https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License  and the GNU Free Documentation License (GFDL): https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License\n        - **Source**: https://huggingface.co/datasets/wikipedia\n    - **c4**:\n        - **License**: Open Data Commons Attribution License: http://opendatacommons.org/licenses/by/1.0/\n        - **Source**: https://huggingface.co/datasets/c4\n    - **akoksal/LongForm**:\n        - **License**: MIT License https://opensource.org/license/mit/\n        - **Source**: https://huggingface.co/datasets/akoksal/LongForm\n    - **alespalla/chatbot_instruction_prompts**:\n        - **License**: Apache License 2.0 https://www.apache.org/licenses/LICENSE-2.0.txt\n        - **Source**: https://huggingface.co/datasets/alespalla/chatbot_instruction_prompts\n    - **lighteval/mmlu**:\n        - **License**: MIT License https://opensource.org/license/mit/\n        - **Source**: https://huggingface.co/datasets/lighteval/mmlu\n    - **vicgalle/alpaca-gpt4**:\n        - **License**: Creative Commons NonCommercial (CC BY-NC 4.0) https://creativecommons.org/licenses/by-nc/4.0/legalcode\n        - **Source**: https://huggingface.co/datasets/vicgalle/alpaca-gpt4\n\n#### Schelling Point\n- **Location**: evals/registry/data/schelling_point\n- **Components**:\n    - **openwebtext**:\n        - **License**: Creative Commons CC0 license (“no rights reserved”): https://creativecommons.org/share-your-work/public-domain/cc0/\n        - **Source**: https://huggingface.co/datasets/openwebtext\n    - **wikipedia**:\n        - **License**:  Creative Commons Attribution-ShareAlike 3.0 Unported License (CC BY-SA): https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License  and the GNU Free Documentation License (GFDL): https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License\n        - **Source**: https://huggingface.co/datasets/wikipedia\n\n#### Ballot Proposals\n- **Location**: evals/registry/data/ballots\n- **Components**:\n    - **California ballot proposals**:\n        - **License**: Public Domain\n        - **Source**: https://repository.uclawsf.edu/ca_ballot_props/\n\n#### Sandbagging\n- **Location**: evals/registry/data/sandbagging\n- **Components**:\n    - **MMLU**:\n        - **License**: MIT License: https://opensource.org/license/mit/\n        - **Source**: https://huggingface.co/datasets/cais/mmlu\n\n#### Theory of Mind\n- **Location**: evals/registry/data/theory_of_mind\n- **Components**:\n    - **ToMi**:\n        - **License**: Creative Commons Attribution-NonCommercial 4.0 International: https://creativecommons.org/licenses/by-nc/4.0/deed.en\n        - **Source**: https://github.com/facebookresearch/ToMi\n    - **Social IQA**:\n        - **License**: Creative Commons Attribution 4.0 International: https://creativecommons.org/licenses/by/4.0/\n        - **Source**: https://allenai.org/data/socialiqa\n          \n#### Already Said That\n\n- **Location**: evals/registry/data/already_said_that\n- **Components**:\n    - **WordNet**:\n        - **License**: WordNet License: https://wordnet.princeton.edu/license-and-commercial-use\n        - **Source**: https://wordnet.princeton.edu/\n\nPlease note: While efforts have been made to accurately represent the licenses associated with each dataset, users should consult the original source of the dataset to ensure compliance with any licensing terms and conditions."
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1328125,
          "content": "recursive-include evals *.py\nrecursive-include evals *.yaml\nrecursive-include evals *.sql\nrecursive-include evals/registry/data *.jsonl\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.06640625,
          "content": ".PHONY: mypy\nmypy:\n\tmypy --config-file=mypy.ini --no-site-packages ."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.3115234375,
          "content": "# OpenAI Evals\n\n> You can now configure and run Evals directly in the OpenAI Dashboard. [Get started →](https://platform.openai.com/docs/guides/evals)\n\nEvals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. We offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly.\n\nIf you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might affect your use case. In the words of [OpenAI's President Greg Brockman](https://twitter.com/gdb/status/1733553161884127435):\n\n<img width=\"596\" alt=\"https://x.com/gdb/status/1733553161884127435?s=20\" src=\"https://github.com/openai/evals/assets/35577566/ce7840ff-43a8-4d88-bb2f-6b207410333b\">\n\n## Setup\n\nTo run evals, you will need to set up and specify your [OpenAI API key](https://platform.openai.com/account/api-keys). After you obtain an API key, specify it using the [`OPENAI_API_KEY` environment variable](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key). Please be aware of the [costs](https://openai.com/pricing) associated with using the API when running evals. You can also run and create evals using [Weights & Biases](https://wandb.ai/wandb_fc/openai-evals/reports/OpenAI-Evals-Demo-Using-W-B-Prompts-to-Run-Evaluations--Vmlldzo0MTI4ODA3).\n\n**Minimum Required Version: Python 3.9**\n\n### Downloading evals\n\nOur evals registry is stored using [Git-LFS](https://git-lfs.com/). Once you have downloaded and installed LFS, you can fetch the evals (from within your local copy of the evals repo) with:\n```sh\ncd evals\ngit lfs fetch --all\ngit lfs pull\n```\n\nThis will populate all the pointer files under `evals/registry/data`.\n\nYou may just want to fetch data for a select eval. You can achieve this via:\n```sh\ngit lfs fetch --include=evals/registry/data/${your eval}\ngit lfs pull\n```\n\n### Making evals\n\nIf you are going to be creating evals, we suggest cloning this repo directly from GitHub and installing the requirements using the following command:\n\n```sh\npip install -e .\n```\n\nUsing `-e`, changes you make to your eval will be reflected immediately without having to reinstall.\n\nOptionally, you can install the formatters for pre-committing with:\n\n```sh\npip install -e .[formatters]\n```\n\nThen run `pre-commit install` to install pre-commit into your git hooks. pre-commit will now run on every commit.\n\nIf you want to manually run all pre-commit hooks on a repository, run `pre-commit run --all-files`. To run individual hooks use `pre-commit run <hook_id>`.\n\n## Running evals\n\nIf you don't want to contribute new evals, but simply want to run them locally, you can install the evals package via pip:\n\n```sh\npip install evals\n```\n\nYou can find the full instructions to run existing evals in [`run-evals.md`](docs/run-evals.md) and our existing eval templates in [`eval-templates.md`](docs/eval-templates.md). For more advanced use cases like prompt chains or tool-using agents, you can use our [Completion Function Protocol](docs/completion-fns.md).\n\nWe provide the option for you to log your eval results to a Snowflake database, if you have one or wish to set one up. For this option, you will further have to specify the `SNOWFLAKE_ACCOUNT`, `SNOWFLAKE_DATABASE`, `SNOWFLAKE_USERNAME`, and `SNOWFLAKE_PASSWORD` environment variables.\n\n## Writing evals\n\nWe suggest getting starting by: \n\n- Walking through the process for building an eval: [`build-eval.md`](docs/build-eval.md)\n- Exploring an example of implementing custom eval logic: [`custom-eval.md`](docs/custom-eval.md)\n- Writing your own completion functions: [`completion-fns.md`](docs/completion-fns.md)\n- Review our starter guide for writing evals: [Getting Started with OpenAI Evals](https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals)\n\nPlease note that we are currently not accepting evals with custom code! While we ask you to not submit such evals at the moment, you can still submit model-graded evals with custom model-graded YAML files.\n\nIf you think you have an interesting eval, please open a pull request with your contribution. OpenAI staff actively review these evals when considering improvements to upcoming models.\n\n## FAQ\n\nDo you have any examples of how to build an eval from start to finish?\n\n- Yes! These are in the `examples` folder. We recommend that you also read through [`build-eval.md`](docs/build-eval.md) in order to gain a deeper understanding of what is happening in these examples.\n\nDo you have any examples of evals implemented in multiple different ways?\n\n- Yes! In particular, see `evals/registry/evals/coqa.yaml`. We have implemented small subsets of the [CoQA](https://stanfordnlp.github.io/coqa/) dataset for various eval templates to help illustrate the differences.\n\nWhen I run an eval, it sometimes hangs at the very end (after the final report). What's going on?\n\n- This is a known issue, but you should be able to interrupt it safely and the eval should finish immediately after.\n\nThere's a lot of code, and I just want to spin up a quick eval. Help? OR,\n\nI am a world-class prompt engineer. I choose not to code. How can I contribute my wisdom?\n\n- If you follow an existing [eval template](docs/eval-templates.md) to build a basic or model-graded eval, you don't need to write any evaluation code at all! Just provide your data in JSON format and specify your eval parameters in YAML. [build-eval.md](docs/build-eval.md) walks you through these steps, and you can supplement these instructions with the Jupyter notebooks in the `examples` folder to help you get started quickly. Keep in mind, though, that a good eval will inevitably require careful thought and rigorous experimentation!\n\n## Disclaimer\n\nBy contributing to evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI evals will be subject to our usual Usage Policies: https://platform.openai.com/docs/usage-policies.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.4033203125,
          "content": "# Security Policy\nFor a more in-depth look at our security policy, please check out our [Coordinated Vulnerability Disclosure Policy](https://openai.com/security/disclosure/#:~:text=Disclosure%20Policy,-Security%20is%20essential&text=OpenAI%27s%20coordinated%20vulnerability%20disclosure%20policy,expect%20from%20us%20in%20return.).\n\nOur PGP key is located [at this address.](https://cdn.openai.com/security.txt)\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "evals",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "mypy.ini",
          "type": "blob",
          "size": 0.990234375,
          "content": "[mypy]\npython_version=3.9\n\nmypy_path=$MYPY_CONFIG_FILE_DIR/typings\n\n; Not all dependencies have type annotations; ignore this.\nignore_missing_imports=True\nnamespace_packages=True\nexplicit_package_bases = True\n\n; Be strict about certain rules.\nstrict_equality=True\nwarn_unused_configs=True\nno_implicit_optional=True\nstrict_optional=True\nwarn_redundant_casts=True\nwarn_unused_ignores=True\ncheck_untyped_defs=True\n\n; By default, code is not checked for type errors.\nignore_errors=True\ndisallow_untyped_defs=False\n\n; However, some directories that are fully type-annotated and don't have type errors have opted in\n; to type checking.\n\n[mypy-evals.registry]\nignore_errors=False\ndisallow_untyped_defs=True\n\n[mypy-evals.cli.oaievalset]\nignore_errors=False\ndisallow_untyped_defs=True\n\n[mypy-evals.cli.oaieval]\nignore_errors=False\ndisallow_untyped_defs=True\n\n[mypy-scripts.*]\nignore_errors=False\ndisallow_untyped_defs=True\n\n[mypy-openai.*]\nignore_errors=False\ndisallow_untyped_defs=True\n\n; TODO: Add the other modules here\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.28515625,
          "content": "[project]\nname = \"evals\"\nversion = \"3.0.1.post1\"\nrequires-python = \">=3.9\"\nreadme = \"README.md\"\ndependencies = [\n    \"aiolimiter\",\n    \"anthropic\",\n    \"backoff\",\n    \"beartype>=0.12.0\",\n    \"blobfile\",\n    \"chess\",\n    \"dacite\",\n    \"datasets\",\n    \"docker\",\n    \"evaluate\",\n    \"filelock\",\n    \"fire\",\n    \"flask\",\n    \"google-generativeai\",\n    \"gymnasium\",\n    \"jiwer\",\n    \"langchain\",\n    \"langdetect\",\n    \"lz4\",\n    \"matplotlib\",\n    \"mock\",\n    \"mypy\",\n    \"networkx\",\n    \"nltk\",\n    \"numexpr\",\n    \"numpy\",\n    \"openai>=1.0.0\",\n    \"pandas\",\n    \"playwright\",\n    \"pydantic\",\n    \"pytest\",\n    \"pyyaml\",\n    \"sacrebleu\",\n    \"seaborn\",\n    \"snowflake-connector-python[pandas]\",\n    \"spacy-universal-sentence-encoder\",\n    \"statsmodels\",\n    \"termcolor\",\n    \"tiktoken\",\n    \"tqdm\",\n    \"types-PyYAML\",\n    \"types-tqdm\",\n    \"zstandard\",\n]\n\n[project.urls]\nrepository = \"https://github.com/openai/evals\"\n\n[project.optional-dependencies]\nformatters = [\"black\", \"isort\", \"autoflake\", \"ruff\"]\n\ntorch = [\"torch\"]\n\n[project.scripts]\noaieval = \"evals.cli.oaieval:main\"\noaievalset = \"evals.cli.oaievalset:main\"\n\n[tool.setuptools]\npackages = [\"evals\"]\n\n[tool.ruff]\n# Allow lines to be as long as 767 characters, this is currently the longest line\n# TODO: This should be brought down in the future\nline-length = 767\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}