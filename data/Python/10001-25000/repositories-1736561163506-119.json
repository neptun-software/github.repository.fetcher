{
  "metadata": {
    "timestamp": 1736561163506,
    "page": 119,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apple/ml-stable-diffusion",
      "stars": 17056,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9052734375,
          "content": "*~\n\n# Swift Package\n.DS_Store\n/.build\n/Packages\n/*.xcodeproj\n.swiftpm\n.vscode\n.*.sw?\n*.docc-build\n*.vs\nPackage.resolved\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# macOS filesystem\n*.DS_Store\n"
        },
        {
          "name": "ACKNOWLEDGEMENTS",
          "type": "blob",
          "size": 30.9365234375,
          "content": "Acknowledgements\nPortions of this software may utilize the following copyrighted \nmaterial, the use of which is hereby acknowledged.\n\n_____________________\nThe Hugging Face team (diffusers)\n                                    Apache License\n                              Version 2.0, January 2004\n                           http://www.apache.org/licenses/\n\n      TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n      1. Definitions.\n\n         \"License\" shall mean the terms and conditions for use, reproduction,\n         and distribution as defined by Sections 1 through 9 of this document.\n\n         \"Licensor\" shall mean the copyright owner or entity authorized by\n         the copyright owner that is granting the License.\n\n         \"Legal Entity\" shall mean the union of the acting entity and all\n         other entities that control, are controlled by, or are under common\n         control with that entity. For the purposes of this definition,\n         \"control\" means (i) the power, direct or indirect, to cause the\n         direction or management of such entity, whether by contract or\n         otherwise, or (ii) ownership of fifty percent (50%) or more of the\n         outstanding shares, or (iii) beneficial ownership of such entity.\n\n         \"You\" (or \"Your\") shall mean an individual or Legal Entity\n         exercising permissions granted by this License.\n\n         \"Source\" form shall mean the preferred form for making modifications,\n         including but not limited to software source code, documentation\n         source, and configuration files.\n\n         \"Object\" form shall mean any form resulting from mechanical\n         transformation or translation of a Source form, including but\n         not limited to compiled object code, generated documentation,\n         and conversions to other media types.\n\n         \"Work\" shall mean the work of authorship, whether in Source or\n         Object form, made available under the License, as indicated by a\n         copyright notice that is included in or attached to the work\n         (an example is provided in the Appendix below).\n\n         \"Derivative Works\" shall mean any work, whether in Source or Object\n         form, that is based on (or derived from) the Work and for which the\n         editorial revisions, annotations, elaborations, or other modifications\n         represent, as a whole, an original work of authorship. For the purposes\n         of this License, Derivative Works shall not include works that remain\n         separable from, or merely link (or bind by name) to the interfaces of,\n         the Work and Derivative Works thereof.\n\n         \"Contribution\" shall mean any work of authorship, including\n         the original version of the Work and any modifications or additions\n         to that Work or Derivative Works thereof, that is intentionally\n         submitted to Licensor for inclusion in the Work by the copyright owner\n         or by an individual or Legal Entity authorized to submit on behalf of\n         the copyright owner. For the purposes of this definition, \"submitted\"\n         means any form of electronic, verbal, or written communication sent\n         to the Licensor or its representatives, including but not limited to\n         communication on electronic mailing lists, source code control systems,\n         and issue tracking systems that are managed by, or on behalf of, the\n         Licensor for the purpose of discussing and improving the Work, but\n         excluding communication that is conspicuously marked or otherwise\n         designated in writing by the copyright owner as \"Not a Contribution.\"\n\n         \"Contributor\" shall mean Licensor and any individual or Legal Entity\n         on behalf of whom a Contribution has been received by Licensor and\n         subsequently incorporated within the Work.\n\n      2. Grant of Copyright License. Subject to the terms and conditions of\n         this License, each Contributor hereby grants to You a perpetual,\n         worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n         copyright license to reproduce, prepare Derivative Works of,\n         publicly display, publicly perform, sublicense, and distribute the\n         Work and such Derivative Works in Source or Object form.\n\n      3. Grant of Patent License. Subject to the terms and conditions of\n         this License, each Contributor hereby grants to You a perpetual,\n         worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n         (except as stated in this section) patent license to make, have made,\n         use, offer to sell, sell, import, and otherwise transfer the Work,\n         where such license applies only to those patent claims licensable\n         by such Contributor that are necessarily infringed by their\n         Contribution(s) alone or by combination of their Contribution(s)\n         with the Work to which such Contribution(s) was submitted. If You\n         institute patent litigation against any entity (including a\n         cross-claim or counterclaim in a lawsuit) alleging that the Work\n         or a Contribution incorporated within the Work constitutes direct\n         or contributory patent infringement, then any patent licenses\n         granted to You under this License for that Work shall terminate\n         as of the date such litigation is filed.\n\n      4. Redistribution. You may reproduce and distribute copies of the\n         Work or Derivative Works thereof in any medium, with or without\n         modifications, and in Source or Object form, provided that You\n         meet the following conditions:\n\n         (a) You must give any other recipients of the Work or\n             Derivative Works a copy of this License; and\n\n         (b) You must cause any modified files to carry prominent notices\n             stating that You changed the files; and\n\n         (c) You must retain, in the Source form of any Derivative Works\n             that You distribute, all copyright, patent, trademark, and\n             attribution notices from the Source form of the Work,\n             excluding those notices that do not pertain to any part of\n             the Derivative Works; and\n\n         (d) If the Work includes a \"NOTICE\" text file as part of its\n             distribution, then any Derivative Works that You distribute must\n             include a readable copy of the attribution notices contained\n             within such NOTICE file, excluding those notices that do not\n             pertain to any part of the Derivative Works, in at least one\n             of the following places: within a NOTICE text file distributed\n             as part of the Derivative Works; within the Source form or\n             documentation, if provided along with the Derivative Works; or,\n             within a display generated by the Derivative Works, if and\n             wherever such third-party notices normally appear. The contents\n             of the NOTICE file are for informational purposes only and\n             do not modify the License. You may add Your own attribution\n             notices within Derivative Works that You distribute, alongside\n             or as an addendum to the NOTICE text from the Work, provided\n             that such additional attribution notices cannot be construed\n             as modifying the License.\n\n         You may add Your own copyright statement to Your modifications and\n         may provide additional or different license terms and conditions\n         for use, reproduction, or distribution of Your modifications, or\n         for any such Derivative Works as a whole, provided Your use,\n         reproduction, and distribution of the Work otherwise complies with\n         the conditions stated in this License.\n\n      5. Submission of Contributions. Unless You explicitly state otherwise,\n         any Contribution intentionally submitted for inclusion in the Work\n         by You to the Licensor shall be under the terms and conditions of\n         this License, without any additional terms or conditions.\n         Notwithstanding the above, nothing herein shall supersede or modify\n         the terms of any separate license agreement you may have executed\n         with Licensor regarding such Contributions.\n\n      6. Trademarks. This License does not grant permission to use the trade\n         names, trademarks, service marks, or product names of the Licensor,\n         except as required for reasonable and customary use in describing the\n         origin of the Work and reproducing the content of the NOTICE file.\n\n      7. Disclaimer of Warranty. Unless required by applicable law or\n         agreed to in writing, Licensor provides the Work (and each\n         Contributor provides its Contributions) on an \"AS IS\" BASIS,\n         WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n         implied, including, without limitation, any warranties or conditions\n         of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n         PARTICULAR PURPOSE. You are solely responsible for determining the\n         appropriateness of using or redistributing the Work and assume any\n         risks associated with Your exercise of permissions under this License.\n\n      8. Limitation of Liability. In no event and under no legal theory,\n         whether in tort (including negligence), contract, or otherwise,\n         unless required by applicable law (such as deliberate and grossly\n         negligent acts) or agreed to in writing, shall any Contributor be\n         liable to You for damages, including any direct, indirect, special,\n         incidental, or consequential damages of any character arising as a\n         result of this License or out of the use or inability to use the\n         Work (including but not limited to damages for loss of goodwill,\n         work stoppage, computer failure or malfunction, or any and all\n         other commercial damages or losses), even if such Contributor\n         has been advised of the possibility of such damages.\n\n      9. Accepting Warranty or Additional Liability. While redistributing\n         the Work or Derivative Works thereof, You may choose to offer,\n         and charge a fee for, acceptance of support, warranty, indemnity,\n         or other liability obligations and/or rights consistent with this\n         License. However, in accepting such obligations, You may act only\n         on Your own behalf and on Your sole responsibility, not on behalf\n         of any other Contributor, and only if You agree to indemnify,\n         defend, and hold each Contributor harmless for any liability\n         incurred by, or claims asserted against, such Contributor by reason\n         of your accepting any such warranty or additional liability.\n\n      END OF TERMS AND CONDITIONS\n\n      APPENDIX: How to apply the Apache License to your work.\n\n         To apply the Apache License to your work, attach the following\n         boilerplate notice, with the fields enclosed by brackets \"[]\"\n         replaced with your own identifying information. (Don't include\n         the brackets!)  The text should be enclosed in the appropriate\n         comment syntax for the file format. We also recommend that a\n         file or class name and description of purpose be included on the\n         same \"printed page\" as the copyright notice for easier\n         identification within third-party archives.\n\n      Copyright [yyyy] [name of copyright owner]\n\n      Licensed under the Apache License, Version 2.0 (the \"License\");\n      you may not use this file except in compliance with the License.\n      You may obtain a copy of the License at\n\n          http://www.apache.org/licenses/LICENSE-2.0\n\n      Unless required by applicable law or agreed to in writing, software\n      distributed under the License is distributed on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n      See the License for the specific language governing permissions and\n      limitations under the License.\n\n\nThe Hugging Face team (transformers)\n        Copyright 2018- The Hugging Face team. All rights reserved.\n\n                                         Apache License\n                                   Version 2.0, January 2004\n                                http://www.apache.org/licenses/\n\n           TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n           1. Definitions.\n\n              \"License\" shall mean the terms and conditions for use, reproduction,\n              and distribution as defined by Sections 1 through 9 of this document.\n\n              \"Licensor\" shall mean the copyright owner or entity authorized by\n              the copyright owner that is granting the License.\n\n              \"Legal Entity\" shall mean the union of the acting entity and all\n              other entities that control, are controlled by, or are under common\n              control with that entity. For the purposes of this definition,\n              \"control\" means (i) the power, direct or indirect, to cause the\n              direction or management of such entity, whether by contract or\n              otherwise, or (ii) ownership of fifty percent (50%) or more of the\n              outstanding shares, or (iii) beneficial ownership of such entity.\n\n              \"You\" (or \"Your\") shall mean an individual or Legal Entity\n              exercising permissions granted by this License.\n\n              \"Source\" form shall mean the preferred form for making modifications,\n              including but not limited to software source code, documentation\n              source, and configuration files.\n\n              \"Object\" form shall mean any form resulting from mechanical\n              transformation or translation of a Source form, including but\n              not limited to compiled object code, generated documentation,\n              and conversions to other media types.\n\n              \"Work\" shall mean the work of authorship, whether in Source or\n              Object form, made available under the License, as indicated by a\n              copyright notice that is included in or attached to the work\n              (an example is provided in the Appendix below).\n\n              \"Derivative Works\" shall mean any work, whether in Source or Object\n              form, that is based on (or derived from) the Work and for which the\n              editorial revisions, annotations, elaborations, or other modifications\n              represent, as a whole, an original work of authorship. For the purposes\n              of this License, Derivative Works shall not include works that remain\n              separable from, or merely link (or bind by name) to the interfaces of,\n              the Work and Derivative Works thereof.\n\n              \"Contribution\" shall mean any work of authorship, including\n              the original version of the Work and any modifications or additions\n              to that Work or Derivative Works thereof, that is intentionally\n              submitted to Licensor for inclusion in the Work by the copyright owner\n              or by an individual or Legal Entity authorized to submit on behalf of\n              the copyright owner. For the purposes of this definition, \"submitted\"\n              means any form of electronic, verbal, or written communication sent\n              to the Licensor or its representatives, including but not limited to\n              communication on electronic mailing lists, source code control systems,\n              and issue tracking systems that are managed by, or on behalf of, the\n              Licensor for the purpose of discussing and improving the Work, but\n              excluding communication that is conspicuously marked or otherwise\n              designated in writing by the copyright owner as \"Not a Contribution.\"\n\n              \"Contributor\" shall mean Licensor and any individual or Legal Entity\n              on behalf of whom a Contribution has been received by Licensor and\n              subsequently incorporated within the Work.\n\n           2. Grant of Copyright License. Subject to the terms and conditions of\n              this License, each Contributor hereby grants to You a perpetual,\n              worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n              copyright license to reproduce, prepare Derivative Works of,\n              publicly display, publicly perform, sublicense, and distribute the\n              Work and such Derivative Works in Source or Object form.\n\n           3. Grant of Patent License. Subject to the terms and conditions of\n              this License, each Contributor hereby grants to You a perpetual,\n              worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n              (except as stated in this section) patent license to make, have made,\n              use, offer to sell, sell, import, and otherwise transfer the Work,\n              where such license applies only to those patent claims licensable\n              by such Contributor that are necessarily infringed by their\n              Contribution(s) alone or by combination of their Contribution(s)\n              with the Work to which such Contribution(s) was submitted. If You\n              institute patent litigation against any entity (including a\n              cross-claim or counterclaim in a lawsuit) alleging that the Work\n              or a Contribution incorporated within the Work constitutes direct\n              or contributory patent infringement, then any patent licenses\n              granted to You under this License for that Work shall terminate\n              as of the date such litigation is filed.\n\n           4. Redistribution. You may reproduce and distribute copies of the\n              Work or Derivative Works thereof in any medium, with or without\n              modifications, and in Source or Object form, provided that You\n              meet the following conditions:\n\n              (a) You must give any other recipients of the Work or\n                  Derivative Works a copy of this License; and\n\n              (b) You must cause any modified files to carry prominent notices\n                  stating that You changed the files; and\n\n              (c) You must retain, in the Source form of any Derivative Works\n                  that You distribute, all copyright, patent, trademark, and\n                  attribution notices from the Source form of the Work,\n                  excluding those notices that do not pertain to any part of\n                  the Derivative Works; and\n\n              (d) If the Work includes a \"NOTICE\" text file as part of its\n                  distribution, then any Derivative Works that You distribute must\n                  include a readable copy of the attribution notices contained\n                  within such NOTICE file, excluding those notices that do not\n                  pertain to any part of the Derivative Works, in at least one\n                  of the following places: within a NOTICE text file distributed\n                  as part of the Derivative Works; within the Source form or\n                  documentation, if provided along with the Derivative Works; or,\n                  within a display generated by the Derivative Works, if and\n                  wherever such third-party notices normally appear. The contents\n                  of the NOTICE file are for informational purposes only and\n                  do not modify the License. You may add Your own attribution\n                  notices within Derivative Works that You distribute, alongside\n                  or as an addendum to the NOTICE text from the Work, provided\n                  that such additional attribution notices cannot be construed\n                  as modifying the License.\n\n              You may add Your own copyright statement to Your modifications and\n              may provide additional or different license terms and conditions\n              for use, reproduction, or distribution of Your modifications, or\n              for any such Derivative Works as a whole, provided Your use,\n              reproduction, and distribution of the Work otherwise complies with\n              the conditions stated in this License.\n\n           5. Submission of Contributions. Unless You explicitly state otherwise,\n              any Contribution intentionally submitted for inclusion in the Work\n              by You to the Licensor shall be under the terms and conditions of\n              this License, without any additional terms or conditions.\n              Notwithstanding the above, nothing herein shall supersede or modify\n              the terms of any separate license agreement you may have executed\n              with Licensor regarding such Contributions.\n\n           6. Trademarks. This License does not grant permission to use the trade\n              names, trademarks, service marks, or product names of the Licensor,\n              except as required for reasonable and customary use in describing the\n              origin of the Work and reproducing the content of the NOTICE file.\n\n           7. Disclaimer of Warranty. Unless required by applicable law or\n              agreed to in writing, Licensor provides the Work (and each\n              Contributor provides its Contributions) on an \"AS IS\" BASIS,\n              WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n              implied, including, without limitation, any warranties or conditions\n              of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n              PARTICULAR PURPOSE. You are solely responsible for determining the\n              appropriateness of using or redistributing the Work and assume any\n              risks associated with Your exercise of permissions under this License.\n\n           8. Limitation of Liability. In no event and under no legal theory,\n              whether in tort (including negligence), contract, or otherwise,\n              unless required by applicable law (such as deliberate and grossly\n              negligent acts) or agreed to in writing, shall any Contributor be\n              liable to You for damages, including any direct, indirect, special,\n              incidental, or consequential damages of any character arising as a\n              result of this License or out of the use or inability to use the\n              Work (including but not limited to damages for loss of goodwill,\n              work stoppage, computer failure or malfunction, or any and all\n              other commercial damages or losses), even if such Contributor\n              has been advised of the possibility of such damages.\n\n           9. Accepting Warranty or Additional Liability. While redistributing\n              the Work or Derivative Works thereof, You may choose to offer,\n              and charge a fee for, acceptance of support, warranty, indemnity,\n              or other liability obligations and/or rights consistent with this\n              License. However, in accepting such obligations, You may act only\n              on Your own behalf and on Your sole responsibility, not on behalf\n              of any other Contributor, and only if You agree to indemnify,\n              defend, and hold each Contributor harmless for any liability\n              incurred by, or claims asserted against, such Contributor by reason\n              of your accepting any such warranty or additional liability.\n\n           END OF TERMS AND CONDITIONS\n\n           APPENDIX: How to apply the Apache License to your work.\n\n              To apply the Apache License to your work, attach the following\n              boilerplate notice, with the fields enclosed by brackets \"[]\"\n              replaced with your own identifying information. (Don't include\n              the brackets!)  The text should be enclosed in the appropriate\n              comment syntax for the file format. We also recommend that a\n              file or class name and description of purpose be included on the\n              same \"printed page\" as the copyright notice for easier\n              identification within third-party archives.\n\n           Copyright [yyyy] [name of copyright owner]\n\n           Licensed under the Apache License, Version 2.0 (the \"License\");\n           you may not use this file except in compliance with the License.\n           You may obtain a copy of the License at\n\n               http://www.apache.org/licenses/LICENSE-2.0\n\n           Unless required by applicable law or agreed to in writing, software\n           distributed under the License is distributed on an \"AS IS\" BASIS,\n           WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n           See the License for the specific language governing permissions and\n           limitations under the License.\n\n\nFacebook, Inc (PyTorch)\n        From PyTorch:\n\n        Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)\n        Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\n        Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n        Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n        Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n        Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n        Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n        Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n        Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\n        From Caffe2:\n\n        Copyright (c) 2016-present, Facebook Inc. All rights reserved.\n\n        All contributions by Facebook:\n        Copyright (c) 2016 Facebook Inc.\n\n        All contributions by Google:\n        Copyright (c) 2015 Google Inc.\n        All rights reserved.\n\n        All contributions by Yangqing Jia:\n        Copyright (c) 2015 Yangqing Jia\n        All rights reserved.\n\n        All contributions by Kakao Brain:\n        Copyright 2019-2020 Kakao Brain\n\n        All contributions by Cruise LLC:\n        Copyright (c) 2022 Cruise LLC.\n        All rights reserved.\n\n        All contributions from Caffe:\n        Copyright(c) 2013, 2014, 2015, the respective contributors\n        All rights reserved.\n\n        All other contributions:\n        Copyright(c) 2015, 2016 the respective contributors\n        All rights reserved.\n\n        Caffe2 uses a copyright model similar to Caffe: each contributor holds\n        copyright over their contributions to Caffe2. The project versioning records\n        all such contribution and copyright details. If a contributor wants to further\n        mark their specific copyright on a particular contribution, they should\n        indicate their copyright solely in the commit message of the change when it is\n        committed.\n\n        All rights reserved.\n\n        Redistribution and use in source and binary forms, with or without\n        modification, are permitted provided that the following conditions are met:\n\n        1. Redistributions of source code must retain the above copyright\n           notice, this list of conditions and the following disclaimer.\n\n        2. Redistributions in binary form must reproduce the above copyright\n           notice, this list of conditions and the following disclaimer in the\n           documentation and/or other materials provided with the distribution.\n\n        3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America\n           and IDIAP Research Institute nor the names of its contributors may be\n           used to endorse or promote products derived from this software without\n           specific prior written permission.\n\n        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n        AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n        IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n        ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n        LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n        CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n        SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n        INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n        CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n        ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n        POSSIBILITY OF SUCH DAMAGE.\n\nNumPy (RandomKit 1.3)\n\n Copyright (c) 2003-2005, Jean-Sebastien Roy (js@jeannot.org)\n\n The rk_random and rk_seed functions algorithms and the original design of\n the Mersenne Twister RNG:\n\n   Copyright (C) 1997 - 2002, Makoto Matsumoto and Takuji Nishimura,\n   All rights reserved.\n\n   Redistribution and use in source and binary forms, with or without\n   modification, are permitted provided that the following conditions\n   are met:\n\n   1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n   2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n   3. The names of its contributors may not be used to endorse or promote\n   products derived from this software without specific prior written\n   permission.\n\n   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n   \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n   A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n   CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n   EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n   PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n   PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n   LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n   NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n Original algorithm for the implementation of rk_interval function from\n Richard J. Wagner's implementation of the Mersenne Twister RNG, optimised by\n Magnus Jonsson.\n\n Constants used in the rk_double implementation by Isaku Wada.\n\n Permission is hereby granted, free of charge, to any person obtaining a\n copy of this software and associated documentation files (the\n \"Software\"), to deal in the Software without restriction, including\n without limitation the rights to use, copy, modify, merge, publish,\n distribute, sublicense, and/or sell copies of the Software, and to\n permit persons to whom the Software is furnished to do so, subject to\n the following conditions:\n\n The above copyright notice and this permission notice shall be included\n in all copies or substantial portions of the Software.\n\n THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.2783203125,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the open source team at [opensource-conduct@group.apple.com](mailto:opensource-conduct@group.apple.com). All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org), version 1.4,\navailable at [https://www.contributor-covenant.org/version/1/4/code-of-conduct.html](https://www.contributor-covenant.org/version/1/4/code-of-conduct.html)"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.712890625,
          "content": "# Contribution Guide\n\nThank you for your interest in contributing to Core ML Stable Diffusion! This project was released for system demonstration purposes and there are limited plans for future development of the repository. While we welcome new pull requests and issues please note that our response may be limited.\n\n\n## Submitting a Pull Request\n\nThe project is licensed under the MIT license. By submitting a pull request, you represent that you have the right to license your contribution to Apple and the community, and agree by submitting the patch that your contributions are licensed under the MIT license.\n\n## Code of Conduct\n\nWe ask that all community members read and observe our [Code of Conduct](CODE_OF_CONDUCT.md).\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2024 Apple Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "Package.swift",
          "type": "blob",
          "size": 1.4794921875,
          "content": "// swift-tools-version: 5.8\n// The swift-tools-version declares the minimum version of Swift required to build this package.\n\nimport PackageDescription\n\nlet package = Package(\n    name: \"stable-diffusion\",\n    platforms: [\n        .macOS(.v13),\n        .iOS(.v16),\n    ],\n    products: [\n        .library(\n            name: \"StableDiffusion\",\n            targets: [\"StableDiffusion\"]),\n        .executable(\n            name: \"StableDiffusionSample\",\n            targets: [\"StableDiffusionCLI\"])\n    ],\n    dependencies: [\n        .package(url: \"https://github.com/apple/swift-argument-parser.git\", from: \"1.2.3\"),\n        .package(url: \"https://github.com/huggingface/swift-transformers.git\", exact: \"0.1.8\"),\n    ],\n    targets: [\n        .target(\n            name: \"StableDiffusion\",\n            dependencies:  [\n                .product(name: \"Transformers\", package: \"swift-transformers\"),\n            ],\n            path: \"swift/StableDiffusion\"),\n        .executableTarget(\n            name: \"StableDiffusionCLI\",\n            dependencies: [\n                \"StableDiffusion\",\n                .product(name: \"ArgumentParser\", package: \"swift-argument-parser\")],\n            path: \"swift/StableDiffusionCLI\"),\n        .testTarget(\n            name: \"StableDiffusionTests\",\n            dependencies: [\"StableDiffusion\"],\n            path: \"swift/StableDiffusionTests\",\n            resources: [\n                .copy(\"Resources/vocab.json\"),\n                .copy(\"Resources/merges.txt\")\n            ]),\n    ]\n)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 57.884765625,
          "content": "# Core ML Stable Diffusion\n\nRun Stable Diffusion on Apple Silicon with Core ML\n\n[\\[Blog Post\\]](https://machinelearning.apple.com/research/stable-diffusion-coreml-apple-silicon) [\\[BibTeX\\]](#bibtex)\n\n\nThis repository comprises:\n\n- `python_coreml_stable_diffusion`, a Python package for converting PyTorch models to Core ML format and performing image generation with Hugging Face [diffusers](https://github.com/huggingface/diffusers) in Python\n- `StableDiffusion`, a Swift package that developers can add to their Xcode projects as a dependency to deploy image generation capabilities in their apps. The Swift package relies on the Core ML model files generated by `python_coreml_stable_diffusion`\n\nIf you run into issues during installation or runtime, please refer to the [FAQ](#faq) section. Please refer to the [System Requirements](#system-requirements) section before getting started.\n\n<img src=\"assets/readme_reel.png\">\n\n## <a name=\"system-requirements\"></a> System Requirements\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\nModel Conversion:\n\n macOS  | Python | coremltools |\n:------:|:------:|:-----------:|\n  13.1  | 3.8    |    7.0      |\n\nProject Build:\n\n  macOS | Xcode | Swift |\n:------:|:-----:|:-----:|\n  13.1  | 14.3  |  5.8  |\n\nTarget Device Runtime:\n\n  macOS | iPadOS, iOS |\n:------:|:-----------:|\n  13.1  |     16.2    |\n\nTarget Device Runtime ([With Memory Improvements](#compression-6-bits-and-higher)):\n\n  macOS | iPadOS, iOS |\n:------:|:-----------:|\n  14.0  |     17.0    |\n\nTarget Device Hardware Generation:\n\n  Mac   |  iPad   | iPhone  |\n:------:|:-------:|:-------:|\n   M1   |   M1    |  A14    |\n\n</details>\n\n\n## <a name=\"performance-benchmark\"></a> Performance Benchmarks\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\n\n[`stabilityai/stable-diffusion-2-1-base`](https://huggingface.co/apple/coreml-stable-diffusion-2-1-base) (512x512)\n\n\n\n|        Device         | `--compute-unit`| `--attention-implementation` | End-to-End Latency (s) | Diffusion Speed (iter/s) |\n| --------------------- | --------------- | ---------------------------- | ---------------------- | ------------------------ |\n| iPhone 12 Mini        | `CPU_AND_NE`    |      `SPLIT_EINSUM_V2`       |      18.5*             |        1.44              |\n| iPhone 12 Pro Max     | `CPU_AND_NE`    |      `SPLIT_EINSUM_V2`       |      15.4              |        1.45              |\n| iPhone 13             | `CPU_AND_NE`    |      `SPLIT_EINSUM_V2`       |      10.8*             |        2.53              |\n| iPhone 13 Pro Max     | `CPU_AND_NE`    |      `SPLIT_EINSUM_V2`       |      10.4              |        2.55              |\n| iPhone 14             | `CPU_AND_NE`    |      `SPLIT_EINSUM_V2`       |      8.6               |        2.57              |\n| iPhone 14 Pro Max     | `CPU_AND_NE`    |      `SPLIT_EINSUM_V2`       |      7.9               |        2.69              |\n| iPad Pro (M1)         | `CPU_AND_NE`    |      `SPLIT_EINSUM_V2`       |      11.2              |        2.19              |\n| iPad Pro (M2)         | `CPU_AND_NE`    |      `SPLIT_EINSUM_V2`       |      7.0               |        3.07              |\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\n- This benchmark was conducted by Apple and Hugging Face using public beta versions of iOS 17.0, iPadOS 17.0 and macOS 14.0 Seed 8 in August 2023.\n- The performance data was collected using the `benchmark` branch of the [Diffusers app](https://github.com/huggingface/swift-coreml-diffusers)\n- Swift code is not fully optimized, introducing up to ~10% overhead unrelated to Core ML model execution.\n- The median latency value across 5 back-to-back end-to-end executions are reported\n- The image generation procedure follows the standard configuration: 20 inference steps, 512x512 output image resolution, 77 text token sequence length, classifier-free guidance (batch size of 2 for unet).\n- The actual prompt length does not impact performance because the Core ML model is converted with a static shape that computes the forward pass for all of the 77 elements (`tokenizer.model_max_length`) in the text token sequence regardless of the actual length of the input text.\n- Weights are compressed to 6 bit precision. Please refer to [this section](#compression-6-bits-and-higher) for details.\n- Activations are in float16 precision for both the GPU and the Neural Engine.\n- `*` indicates that the [reduceMemory](https://github.com/apple/ml-stable-diffusion/blob/main/swift/StableDiffusion/pipeline/StableDiffusionPipeline.swift#L91) option was enabled which loads and unloads models just-in-time to avoid memory shortage. This added up to 2 seconds to the end-to-end latency.\n- In the benchmark table, we report the best performing `--compute-unit` and `--attention-implementation` values per device. The former does not modify the Core ML model and can be applied during runtime. The latter modifies the Core ML model. Note that the best performing compute unit is model version and hardware-specific.\n- Note that the performance optimizations in this repository (e.g. `--attention-implementation`) are generally applicable to Transformers and not customized to Stable Diffusion. Better performance may be observed upon custom kernel tuning. Therefore, these numbers do not represent **peak** HW capability.\n- Performance may vary across different versions of Stable Diffusion due to architecture changes in the model itself. Each reported number is specific to the model version mentioned in that context.\n- Performance may vary due to factors like increased system load from other applications or suboptimal device thermal state.\n\n</details>\n\n\n[`stabilityai/stable-diffusion-xl-base-1.0-ios`](https://huggingface.co/apple/coreml-stable-diffusion-xl-base-ios) (768x768)\n\n|        Device         | `--compute-unit`| `--attention-implementation` | End-to-End Latency (s) | Diffusion Speed (iter/s) |\n| --------------------- | --------------- | ---------------------------- | ---------------------- | ------------------------ |\n| iPhone 12 Pro         | `CPU_AND_NE`    |      `SPLIT_EINSUM`          |            116*        |        0.50              |\n| iPhone 13 Pro Max     | `CPU_AND_NE`    |      `SPLIT_EINSUM`          |            86*         |        0.68              |\n| iPhone 14 Pro Max     | `CPU_AND_NE`    |      `SPLIT_EINSUM`          |            77*         |        0.83              |\n| iPhone 15 Pro Max     | `CPU_AND_NE`    |      `SPLIT_EINSUM`          |            31          |        0.85              |\n| iPad Pro (M1)         | `CPU_AND_NE`    |      `SPLIT_EINSUM`          |            36          |        0.69              |\n| iPad Pro (M2)         | `CPU_AND_NE`    |      `SPLIT_EINSUM`          |            27          |        0.98              |\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\n- This benchmark was conducted by Apple and Hugging Face using iOS 17.0.2 and iPadOS 17.0.2 in September 2023.\n- The performance data was collected using the `benchmark` branch of the [Diffusers app](https://github.com/huggingface/swift-coreml-diffusers)\n- The median latency value across 5 back-to-back end-to-end executions are reported\n- The image generation procedure follows this configuration: 20 inference steps, 768x768 output image resolution, 77 text token sequence length, classifier-free guidance (batch size of 2 for unet).\n- `Unet.mlmodelc` is compressed to 4.04 bit precision following the [Mixed-Bit Palettization](#compression-lower-than-6-bits) algorithm recipe published [here](https://huggingface.co/apple/coreml-stable-diffusion-mixed-bit-palettization/blob/main/recipes/stabilityai-stable-diffusion-xl-base-1.0_palettization_recipe.json)\n- All models except for `Unet.mlmodelc` are compressed to 16 bit precision\n- [madebyollin/sdxl-vae-fp16-fix](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix) by [@madebyollin](https://github.com/madebyollin) was used as the source PyTorch model for `VAEDecoder.mlmodelc` in order to enable float16 weight and activation quantization for the VAE model.\n- `--attention-implementation SPLIT_EINSUM` is chosen in lieu of `SPLIT_EINSUM_V2` due to the prohibitively long compilation time of the latter\n- `*` indicates that the [reduceMemory](https://github.com/apple/ml-stable-diffusion/blob/main/swift/StableDiffusion/pipeline/StableDiffusionPipeline.swift#L91) option was enabled which loads and unloads models just-in-time to avoid memory shortage. This added significant overhead to the end-to-end latency. Note that end-to-end latency difference between `iPad Pro (M1)` and `iPhone 13 Pro Max` despite identical diffusion speed.\n- The actual prompt length does not impact performance because the Core ML model is converted with a static shape that computes the forward pass for all of the 77 elements (`tokenizer.model_max_length`) in the text token sequence regardless of the actual length of the input text.\n- In the benchmark table, we report the best performing `--compute-unit` and `--attention-implementation` values per device. The former does not modify the Core ML model and can be applied during runtime. The latter modifies the Core ML model. Note that the best performing compute unit is model version and hardware-specific.\n- Note that the performance optimizations in this repository (e.g. `--attention-implementation`) are generally applicable to Transformers and not customized to Stable Diffusion. Better performance may be observed upon custom kernel tuning. Therefore, these numbers do not represent **peak** HW capability.\n- Performance may vary across different versions of Stable Diffusion due to architecture changes in the model itself. Each reported number is specific to the model version mentioned in that context.\n- Performance may vary due to factors like increased system load from other applications or suboptimal device thermal state.\n\n\n</details>\n\n\n\n[`stabilityai/stable-diffusion-xl-base-1.0`](https://huggingface.co/apple/coreml-stable-diffusion-xl-base) (1024x1024)\n\n|        Device         | `--compute-unit`| `--attention-implementation` | End-to-End Latency (s) | Diffusion Speed (iter/s) |\n| --------------------- | --------------- | ---------------------------- | ---------------------- | ------------------------ |\n| MacBook Pro (M1 Max)  | `CPU_AND_GPU`   |      `ORIGINAL`              |      46                |        0.46              |\n| MacBook Pro (M2 Max)  | `CPU_AND_GPU`   |      `ORIGINAL`              |      37                |        0.57              |\n| Mac Studio (M1 Ultra) | `CPU_AND_GPU`   |      `ORIGINAL`              |      25                |        0.89              |\n| Mac Studio (M2 Ultra) | `CPU_AND_GPU`   |      `ORIGINAL`              |      20                |        1.11              |\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\n- This benchmark was conducted by Apple and Hugging Face using public beta versions of iOS 17.0, iPadOS 17.0 and macOS 14.0 in July 2023.\n- The performance data was collected by running the `StableDiffusion` Swift pipeline.\n- The median latency value across 3 back-to-back end-to-end executions are reported\n- The image generation procedure follows the standard configuration: 20 inference steps, 1024x1024 output image resolution, classifier-free guidance (batch size of 2 for unet).\n- Weights and activations are in float16 precision\n- Performance may vary across different versions of Stable Diffusion due to architecture changes in the model itself. Each reported number is specific to the model version mentioned in that context.\n- Performance may vary due to factors like increased system load from other applications or suboptimal device thermal state. Given these factors, we do not report sub-second variance in latency.\n\n</details>\n</details>\n\n\n## <a name=\"compression-6-bits-and-higher\"></a> Weight Compression (6-bits and higher)\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\ncoremltools-7.0 supports advanced weight compression techniques for [pruning](https://coremltools.readme.io/v7.0/docs/pruning), [palettization](https://coremltools.readme.io/v7.0/docs/palettization-overview) and [linear 8-bit quantization](https://coremltools.readme.io/v7.0/docs/quantization-aware-training). For these techniques, `coremltools.optimize.torch.*` includes APIs that require fine-tuning to maintain accuracy at higher compression rates whereas `coremltools.optimize.coreml.*` includes APIs that are applied post-training and are data-free.\n\nWe demonstrate how data-free [post-training palettization](https://coremltools.readme.io/v7.0/docs/post-training-palettization) implemented in `coremltools.optimize.coreml.palettize_weights` enables us to achieve greatly improved performance for Stable Diffusion on mobile devices. This API implements the [Fast Exact k-Means](https://arxiv.org/abs/1701.07204) algorithm for optimal weight clustering which yields more accurate palettes. Using `--quantize-nbits {2,4,6,8}` during [conversion](#converting-models-to-coreml) is going to apply this compression to the unet and text_encoder models.\n\nFor best results, we recommend [training-time palettization](https://coremltools.readme.io/v7.0/docs/training-time-palettization): `coremltools.optimize.torch.palettization.DKMPalettizer` if fine-tuning your model is feasible. This API implements the [Differentiable k-Means (DKM)](https://machinelearning.apple.com/research/differentiable-k-means) learned palettization algorithm. In this exercise, we stick to post-training palettization for the sake of simplicity and ease of reproducibility.\n\nThe Neural Engine is capable of accelerating models with low-bit palettization: 1, 2, 4, 6 or 8 bits. With iOS 17 and macOS 14, compressed weights for Core ML models can be just-in-time decompressed during runtime (as opposed to ahead-of-time decompression upon load) to match the precision of activation tensors. This yields significant memory savings and enables models to run on devices with smaller RAM (e.g. iPhone 12 Mini). In addition, compressed weights are faster to fetch from memory which reduces the latency of memory bandwidth-bound layers. The just-in-time decompression behavior depends on the compute unit, layer type and hardware generation.\n\n| Weight Precision | `--compute-unit`   | [`stabilityai/stable-diffusion-2-1-base`](https://huggingface.co/apple/coreml-stable-diffusion-2-1-base) generating *\"a high quality photo of a surfing dog\"* |\n| :---------------:| :----------------: | ------------------------------------------------------  |\n| 6-bit            | cpuAndNeuralEngine | <img src=\"assets/palette6_cpuandne_readmereel.png\"> |\n| 16-bit           | cpuAndNeuralEngine | <img src=\"assets/float16_cpuandne_readmereel.png\">  |\n| 16-bit           | cpuAndGPU          | <img src=\"assets/float16_gpu_readmereel.png\"> |\n\nNote that there are minor differences across 16-bit (float16) and 6-bit results. These differences are comparable to the differences across float16 and float32 or differences across compute units as exemplified above. We recommend a minimum of 6 bits for palettizing Stable Diffusion. Smaller number of bits (1, 2 and 4) will require either fine-tuning or advanced palettization techniques such as [MBP](#compression-lower-than-6-bits).\n\nResources:\n- [Core ML Tools Docs: Optimizing Models](https://coremltools.readme.io/v7.0/docs/optimizing-models)\n- [WWDC23 Session Video: Use Core ML Tools for machine learning model compression](https://developer.apple.com/videos/play/wwdc2023/10047)\n\n</details>\n\n## <a name=\"compression-lower-than-6-bits\"></a> Advanced Weight Compression (Lower than 6-bits)\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\nThis section describes an advanced compression algorithm called [Mixed-Bit Palettization (MBP)](https://huggingface.co/blog/stable-diffusion-xl-coreml#what-is-mixed-bit-palettization) built on top of the [Post-Training Weight Palettization tools](https://apple.github.io/coremltools/docs-guides/source/post-training-palettization.html) and using the [Weights Metadata API](https://apple.github.io/coremltools/docs-guides/source/mlmodel-utilities.html#get-weights-metadata) from [coremltools](https://github.com/apple/coremltools).\n\nMBP builds a per-layer \"palettization recipe\" by picking a suitable number of bits among the Neural Engine supported bit-widths of 1, 2, 4, 6 and 8 in order to achieve the minimum average bit-width while maintaining a desired level of signal strength. The signal strength is measured by comparing the compressed model's output to that of the original float16 model. Given the same random seed and text prompts, PSNR between denoised latents is computed. The compression rate will depend on the model version as well as the tolerance for signal loss (drop in PSNR) since this algorithm is adaptive.\n\n| 3.41-bit | 4.50-bit | 6.55-bit | 16-bit (original) |\n| :-------:| :-------:| :-------:| :----------------:|\n| <img src=\"assets/mbp/a_high_quality_photo_of_a_surfing_dog.7667.final_3.41-bits.png\"> | <img src=\"assets/mbp/a_high_quality_photo_of_a_surfing_dog.7667.final_4.50-bits.png\">  | <img src=\"assets/mbp/a_high_quality_photo_of_a_surfing_dog.7667.final_6.55-bits.png\"> | <img src=\"assets/mbp/a_high_quality_photo_of_a_surfing_dog.7667.final_float16_original.png\"> |\n\n\nFor example, the original float16 [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model has an ~82 dB signal strength. Naively applying [linear 8-bit quantization](https://coremltools.readme.io/docs/data-free-quantization) to the Unet model drops the signal to ~65 dB. Instead, applying MBP yields an average of 2.81-bits quantization while maintaining a signal strength of ~67 dB. This technique generally yields better results compared to using `--quantize-nbits` during model conversion but requires a \"pre-analysis\" run that takes up to a few hours on a single GPU (`mps` or `cuda`).\n\nHere is the signal strength (PSNR in dB) versus model size reduction (% of float16 size) for `stabilityai/stable-diffusion-xl-base-1.0`. The `{1,2,4,6,8}-bit` curves are generated by progressively palettizing more layers using a palette with fixed number of bits. The layers were ordered in ascending order of their isolated impact to end-to-end signal strength so the cumulative compression's impact is delayed as much as possible. The mixed-bit curve is based on falling back to a higher number of bits as soon as a layer's isolated impact to end-to-end signal integrity drops below a threshold. Note that all curves based on palettization outperform linear 8-bit quantization at the same model size except for 1-bit.\n\n<img src=\"assets/mbp/stabilityai_stable-diffusion-xl-base-1.0_psnr_vs_size.png\" width=\"640\">\n\nHere are the steps for applying this technique on another model version:\n\n**Step 1:** Run the pre-analysis script to generate \"recipes\" with varying signal strength:\n\n```python\npython -m python_coreml_stable_diffusion.mixed_bit_compression_pre_analysis --model-version <model-version> -o <output-dir>\n```\n\nFor popular base models, you may find the pre-computed pre-analysis results [here](https://huggingface.co/apple/coreml-stable-diffusion-mixed-bit-palettization/tree/main/recipes). Fine-tuned models models are likely to honor the recipes of their corresponding base models but this is untested.\n\n\n**Step 2:** The resulting JSON file from Step 1 will list \"baselines\", e.g.:\n\n```json\n{\n  \"model_version\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n  \"baselines\": {\n    \"original\": 82.2,\n    \"linear_8bit\": 66.025,\n    \"recipe_6.55_bit_mixedpalette\": 79.9,\n    \"recipe_5.52_bit_mixedpalette\": 78.2,\n    \"recipe_4.89_bit_mixedpalette\": 76.8,\n    \"recipe_4.41_bit_mixedpalette\": 75.5,\n    \"recipe_4.04_bit_mixedpalette\": 73.2,\n    \"recipe_3.67_bit_mixedpalette\": 72.2,\n    \"recipe_3.32_bit_mixedpalette\": 71.4,\n    \"recipe_3.19_bit_mixedpalette\": 70.4,\n    \"recipe_3.08_bit_mixedpalette\": 69.6,\n    \"recipe_2.98_bit_mixedpalette\": 68.6,\n    \"recipe_2.90_bit_mixedpalette\": 67.8,\n    \"recipe_2.83_bit_mixedpalette\": 67.0,\n    \"recipe_2.71_bit_mixedpalette\": 66.3\n  },\n}\n```\n\nAmong these baselines, select a recipe based on your desired signal strength. We recommend palettizing to ~4 bits depending on the use case even if the signal integrity for lower bit values are higher than the linear 8-bit quantization baseline.\n\nFinally, apply the selected recipe to the float16 Core ML model as follows:\n\n```python\npython -m python_coreml_stable_diffusion.mixed_bit_compression_apply --mlpackage-path <path-to-float16-unet-mlpackage> -o <output-dir> --pre-analysis-json-path <path-to--pre-analysis-json> --selected-recipe <selected-recipe-string-key>\n```\n\nAn example `<selected-recipe-string-key>` would be `\"recipe_4.50_bit_mixedpalette\"` which achieves an average of 4.50-bits compression (compressed from ~5.2GB to ~1.46GB for SDXL). Please note that signal strength does not directly map to image-text alignment. Always verify that your MBP-compressed model variant is accurately generating images for your test prompts.\n\n</details>\n\n## <a name=\"activation-quant\"></a> Activation Quantization\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\nOn newer hardware with A17 Pro or M4 chips, such as the iPhone 15 Pro, quantizing both activations and weight to int8 can leverage optimized compute on the Neural Engine which can be used to improve runtime latency in compute-bound models.\n\nIn this section, we demonstrate how to apply [Post Training Activation Quantization](https://apple.github.io/coremltools/docs-guides/source/opt-quantization-algos.html#post-training-data-calibration-activation-quantization), using calibration data, on Stable Diffusion UNet model. \n\nSimilar to Mixed-Bit Palettization (MBP) described [above](#a-namecompression-lower-than-6-bitsa-advanced-weight-compression-lower-than-6-bits), first, a per-layer analysis is run to determine which intermediate activations are more sensitive to 8-bit compression.\nLess sensitive layers are weight and activation quantized (W8A8), whereas more sensitive layers are only weight quantized (W8A16).\n\nHere are the steps for applying this technique:\n\n**Step 1:** Generate calibration data \n            \n```python\npython -m python_coreml_stable_diffusion.activation_quantization --model-version <model-version> --generate-calibration-data  -o <output-dir>\n```\n\nA set of calibration text prompts are run through StableDiffusionPipeline and UNet model inputs are recorded and stored as pickle files in `calibration_data_<model-version>` folder inside specified output directory.\n\n**Step 2:** Run layer-wise sensitivity analysis \n\n```python\npython -m python_coreml_stable_diffusion.activation_quantization --model-version <model-version> --layerwise-sensitivity --calibration-nsamples <num-samples> -o <output-dir>\n```\n\nThis will run the analysis on all Convolutional and Attention (Einsum) modules in the model.\nFor each module, a compressed version is generated by quantizing only that layers weights and activations.\nThen the PSNR between the outputs of the compressed and original model is calculated, using the same random seed and text prompts.\n\nThis analysis takes up to a few hours on a single GPU (cuda). The number of calibration samples used to quantize the model can be reduced to speed up the process. \n \nThe resulting JSON file looks like this:\n\n```json\n{\n  \"conv\": {\n    \"conv_in\": 30.74,\n    \"down_blocks.0.attentions.0.proj_in\": 38.93,\n    \"down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q\": 48.15,\n    \"down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k\": 50.13,\n    \"down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v\": 45.70,\n    \"down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0\": 39.56,\n    ...\n  },\n  \"einsum\": {\n    \"down_blocks.0.attentions.0.transformer_blocks.0.attn1.einsum\": 25.34,\n    \"down_blocks.0.attentions.0.transformer_blocks.0.attn2.einsum\": 31.76,\n    \"down_blocks.0.attentions.1.transformer_blocks.0.attn1.einsum\": 23.40,\n    \"down_blocks.0.attentions.1.transformer_blocks.0.attn2.einsum\": 31.56,\n    ...\n  },\n  \"model_version\": \"stabilityai/stable-diffusion-2-1-base\"\n}\n```\n\n**Step 3:** Generate quantized model \n\nUsing calibration data and layer-wise sensitivity the quantized CoreML model can be generated as follows:\n\n```python\npython -m python_coreml_stable_diffusion.activation_quantization --model-version <model-version> --quantize-pytorch --conv-psnr 38 --attn-psnr 26 -o <output-dir>\n```\n\nThe PSNR thresholds determine which layers will be activation quantized. This number can be tuned to trade-off between output quality and inference latency.  \n\n</details>\n\n## <a name=\"using-stable-diffusion-3\"></a> Using Stable Diffusion 3\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\n### Model Conversion\n\nStable Diffusion 3 uses some new and some old models to run. For the text encoders, the conversion can be done using a similar command as before with the `--sd3-version` flag.\n\n```bash\npython -m python_coreml_stable_diffusion.torch2coreml --model-version stabilityai/stable-diffusion-3-medium --bundle-resources-for-swift-cli --convert-text-encoder --sd3-version -o <output-dir>\n```\n\nFor the new models (MMDiT, a new VAE with 16 channels, and the T5 text encoder), there are a number of new CLI flags that utilize the [DiffusionKit](https://www.github.com/argmaxinc/DiffusionKit) repo:\n\n- `--sd3-version`: Indicates to the converter to treat this as a Stable Diffusion 3 model\n- `--convert-mmdit`: Convert the MMDiT model\n- `--convert-vae-decoder`: Convert the new VAE model (this will use the 16 channel version if --sd3-version is set)\n- `--include-t5`: Downloads and includes a pre-converted T5 text encoder in the conversion\n\ne.g.:\n```bash\npython -m python_coreml_stable_diffusion.torch2coreml --model-version stabilityai/stable-diffusion-3-medium --bundle-resources-for-swift-cli --convert-vae-decoder --convert-mmdit  --include-t5 --sd3-version -o <output-dir>\n```\n\nTo convert the full pipeline with at 1024x1024 resolution, the following command may be used:\n\n```bash\npython -m python_coreml_stable_diffusion.torch2coreml --model-version stabilityai/stable-diffusion-3-medium --bundle-resources-for-swift-cli --convert-text-encoder --convert-vae-decoder --convert-mmdit --include-t5 --sd3-version --latent-h 128 --latent-w 128 -o <output-dir>\n```\n\nKeep in mind that the MMDiT model is quite large and will require increasingly more memory and time to convert as the latent resolution increases.\n\nAlso note that currently the MMDiT model requires fp32 and therefore only supports `CPU_AND_GPU` compute units and `ORIGINAL` attention implementation (the default for this pipeline).\n\n### Swift Inference\n\nSwift inference for Stable Diffusion 3 is similar to the previous versions. The only difference is that the `--sd3` flag should be used to indicate that the model is a Stable Diffusion 3 model.\n\n```bash\nswift run StableDiffusionSample <prompt> --resource-path <output-mlpackages-directory/Resources> --output-path <output-dir> --compute-units cpuAndGPU --sd3\n```\n\n</details>\n\n## <a name=\"using-stable-diffusion-xl\"></a> Using Stable Diffusion XL\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\n### Model Conversion\n\ne.g.:\n\n```bash\npython -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-vae-decoder --convert-text-encoder --xl-version --model-version stabilityai/stable-diffusion-xl-base-1.0 --refiner-version stabilityai/stable-diffusion-xl-refiner-1.0 --bundle-resources-for-swift-cli --attention-implementation {ORIGINAL,SPLIT_EINSUM} -o <output-dir>\n```\n\n- `--xl-version`: Additional argument to pass to the conversion script when specifying an XL model\n- `--refiner-version`: Additional argument to pass to the conversion script when specifying an XL refiner model, required for [\"Ensemble of Expert Denoisers\"](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#1-ensemble-of-expert-denoisers) inference.\n- `--attention-implementation`: `ORIGINAL` is recommended for `cpuAndGPU` for deployment on Mac\n- `--attention-implementation`: `SPLIT_EINSUM` is recommended for `cpuAndNeuralEngine` for deployment on iPhone & iPad\n- `--attention-implementation`: `SPLIT_EINSUM_V2` is not recommended for Stable Diffusion XL because of prohibitively long compilation time\n- **Tip:** Adding `--latent-h 96 --latent-w 96` is recommended for iOS and iPadOS deployment which leads to 768x768 generation as opposed to the default 1024x1024.\n- **Tip:** Due to known float16 overflow issues in the original Stable Diffusion XL VAE, [the model conversion script enforces float32 precision](https://github.com/apple/ml-stable-diffusion/blob/main/python_coreml_stable_diffusion/torch2coreml.py#L486). Using a custom VAE version such as [madebyollin/sdxl-vae-fp16-fix](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix) by [@madebyollin](https://github.com/madebyollin) via `--custom-vae-version madebyollin/sdxl-vae-fp16-fix` will restore the default float16 precision for VAE.\n\n### Swift Inference\n\n```bash\nswift run StableDiffusionSample <prompt> --resource-path <output-mlpackages-directory/Resources> --output-path <output-dir> --compute-units {cpuAndGPU,cpuAndNeuralEngine} --xl\n```\n- Only the `base` model is required, `refiner` model is optional and will be used by default if provided in the resource directory\n- ControlNet for XL is not yet supported\n\n### Python Inference\n\n```bash\npython -m python_coreml_stable_diffusion.pipeline --prompt <prompt> --compute-unit {CPU_AND_GPU,CPU_AND_NE} -o <output-dir> -i <output-mlpackages-directory/Resources> --model-version stabilityai/stable-diffusion-xl-base-1.0\n```\n- `refiner` model is not yet supported\n- ControlNet for XL is not yet supported\n\n</details>\n\n## <a name=\"using-controlnet\"></a> Using ControlNet\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\nExample results using the prompt *\"a high quality photo of a surfing dog\"* conditioned on the scribble (leftmost):\n\n<img src=\"assets/controlnet_readme_reel.png\">\n\n[ControlNet](https://huggingface.co/lllyasviel/ControlNet) allows users to condition image generation with Stable Diffusion on signals such as edge maps, depth maps, segmentation maps, scribbles and pose. Thanks to [@ryu38's contribution](https://github.com/apple/ml-stable-diffusion/pull/153), both the Python CLI and the Swift package support ControlNet models. Please refer to [this section](#converting-models-to-coreml) for details on setting up Stable Diffusion with ControlNet.\n\nNote that ControlNet is not yet supported for Stable Diffusion XL.\n\n</details>\n\n## <a name=\"system-multilingual-text-encoder\"></a> Using the System Multilingual Text Encoder\n\n<details>\n  <summary> Details (Click to expand) </summary>\n\nWith iOS 17 and macOS 14, `NaturalLanguage` framework introduced the [NLContextualEmbedding](https://developer.apple.com/documentation/naturallanguage/nlcontextualembedding) which provides Transformer-based textual embeddings for Latin (20 languages), Cyrillic (4 languages) and CJK (3 languages) scripts. The WWDC23 session titled [Explore Natural Language multilingual models](https://developer.apple.com/videos/play/wwdc2023/10042) demonstrated how this powerful new model can be used by developers to train downstream tasks such as multilingual image generation with Stable Diffusion.\n\nThe code to reproduce this demo workflow is made available in this repository. There are several ways in which this workflow can be implemented. Here is an example:\n\n**Step 1:** Curate an image-text dataset with the desired languages.\n\n**Step 2:** Pre-compute the NLContextualEmbedding values and replace the text strings with these embedding vectors in your dataset.\n\n**Step 3:** Fine-tune a base model from Hugging Face Hub that is compatible with the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview) by using your new dataset and replacing the default text_encoder with your pre-computed NLContextualEmbedding values.\n\n**Step 4:** In order to be able to swap the text_encoder of a base model without training new layers, the base model's `text_encoder.hidden_size` must match that of NLContextualEmbedding. If it doesn't, you will need to train a linear projection layer to map between the two dimensionalities. After fine-tuning, this linear layer should be converted to CoreML as follows:\n\n```shell\npython -m python_coreml_stable_diffusion.multilingual_projection --input-path <path-to-projection-torchscript> --output-dir <output-dir>\n```\n\nThe command above will yield a `MultilingualTextEncoderProjection.mlmodelc` file under `--output-dir` and this should be colocated with the rest of the Core ML model assets that were generated through `--bundle-resources-for-swift-cli`.\n\n**Step 5:** The multilingual system text encoder can now be invoked by setting `useMultilingualTextEncoder` to true when initializing a pipeline or setting `--use-multilingual-text-encoder` in the CLI. Note that the model assets are distributed over-the-air so the first invocation will trigger asset downloads which is less than 100MB.\n\n\nResources:\n- [WWDC23 Session Video: Explore Natural Language multilingual models](https://developer.apple.com/videos/play/wwdc2023/10042)\n- [NLContextualEmbedding API Documentation](https://developer.apple.com/documentation/naturallanguage/nlcontextualembedding)\n\n</details>\n\n## <a name=\"using-converted-weights\"></a> Using Ready-made Core ML Models from Hugging Face Hub\n\n<details>\n  <summary> Click to expand </summary>\n\n Hugging Face ran the [conversion procedure](#converting-models-to-coreml) on the following models and made the Core ML weights publicly available on the Hub. If you would like to convert a version of Stable Diffusion that is not already available on the Hub, please refer to the [Converting Models to Core ML](#converting-models-to-core-ml).\n\n* 6-bit quantized models (suitable for iOS 17 and macOS 14):\n  - [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/apple/coreml-stable-diffusion-1-4-palettized)\n  - [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/apple/coreml-stable-diffusion-v1-5-palettized)\n  - [`stabilityai/stable-diffusion-2-base`](https://huggingface.co/apple/coreml-stable-diffusion-2-base-palettized)\n  - [`stabilityai/stable-diffusion-2-1-base`](https://huggingface.co/apple/coreml-stable-diffusion-2-1-base-palettized)\n\n* Mixed-bit quantized models\n- [`stabilityai/stable-diffusion-xl-base-1.0`](https://huggingface.co/apple/coreml-stable-diffusion-mixed-bit-palettization)\n- [`stabilityai/stable-diffusion-xl-base-1.0-ios`](https://huggingface.co/apple/coreml-stable-diffusion-xl-base-ios)\n\n* Uncompressed models:\n  - [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/apple/coreml-stable-diffusion-v1-4)\n  - [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/apple/coreml-stable-diffusion-v1-5)\n  - [`stabilityai/stable-diffusion-2-base`](https://huggingface.co/apple/coreml-stable-diffusion-2-base)\n  - [`stabilityai/stable-diffusion-2-1-base`](https://huggingface.co/apple/coreml-stable-diffusion-2-1-base)\n  - [`stabilityai/stable-diffusion-xl-base-1.0`](https://huggingface.co/apple/coreml-stable-diffusion-xl-base)\n  - [`stabilityai/stable-diffusion-xl-{base+refiner}-1.0`](https://huggingface.co/apple/coreml-stable-diffusion-xl-base-with-refiner)\n  - [`stabilityai/stable-diffusion-3-medium`](https://huggingface.co/stabilityai/stable-diffusion-3-medium)\n\nIf you want to use any of those models you may download the weights and proceed to [generate images with Python](#image-generation-with-python) or [Swift](#image-generation-with-swift).\n\nThere are several variants in each model repository. You may clone the whole repos using `git` and `git lfs` to download all variants, or selectively download the ones you need.\n\nTo clone the repos using `git`, please follow this process:\n\n**Step 1:** Install the `git lfs` extension for your system.\n\n`git lfs` stores large files outside the main git repo, and it downloads them from the appropriate server after you clone or checkout. It is available in most package managers, check [the installation page](https://git-lfs.com) for details.\n\n**Step 2:** Enable `git lfs` by running this command once:\n\n```bash\ngit lfs install\n```\n\n**Step 3:** Use `git clone` to download a copy of the repo that includes all model variants. For Stable Diffusion version 1.4, you'd issue the following command in your terminal:\n\n```bash\ngit clone https://huggingface.co/apple/coreml-stable-diffusion-v1-4\n```\n\nIf you prefer to download specific variants instead of cloning the repos, you can use the `huggingface_hub` Python library. For example, to do generation in Python using the `ORIGINAL` attention implementation (read [this section](#converting-models-to-core-ml) for details), you could use the following helper code:\n\n```Python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/packages\"\n\nmodel_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\nsnapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\nprint(f\"Model downloaded at {model_path}\")\n```\n\n`model_path` would be the path in your local filesystem where the checkpoint was saved. Please, refer to [this post](https://huggingface.co/blog/diffusers-coreml) for additional details.\n\n</details>\n\n## <a name=\"converting-models-to-coreml\"></a> Converting Models to Core ML\n\n<details>\n  <summary> Click to expand </summary>\n\n**Step 1:** Create a Python environment and install dependencies:\n\n```bash\nconda create -n coreml_stable_diffusion python=3.8 -y\nconda activate coreml_stable_diffusion\ncd /path/to/cloned/ml-stable-diffusion/repository\npip install -e .\n```\n\n**Step 2:** Log in to or register for your [Hugging Face account](https://huggingface.co), generate a [User Access Token](https://huggingface.co/settings/tokens) and use this token to set up Hugging Face API access by running `huggingface-cli login` in a Terminal window.\n\n**Step 3:** Navigate to the version of Stable Diffusion that you would like to use on [Hugging Face Hub](https://huggingface.co/models?search=stable-diffusion) and accept its Terms of Use. The default model version is [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4). The model version may be changed by the user as described in the next step.\n\n**Step 4:** Execute the following command from the Terminal to generate Core ML model files (`.mlpackage`)\n\n```shell\npython -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-text-encoder --convert-vae-decoder --convert-safety-checker --model-version <model-version-string-from-hub> -o <output-mlpackages-directory>\n```\n\n**WARNING:** This command will download several GB worth of PyTorch checkpoints from Hugging Face. Please ensure that you are on Wi-Fi and have enough disk space.\n\nThis generally takes 15-20 minutes on an M1 MacBook Pro. Upon successful execution, the 4 neural network models that comprise Stable Diffusion will have been converted from PyTorch to Core ML (`.mlpackage`) and saved into the specified `<output-mlpackages-directory>`. Some additional notable arguments:\n\n- `--model-version`: The model version name as published on the [Hugging Face Hub](https://huggingface.co/models?search=stable-diffusion)\n\n- `--refiner-version`: The refiner version name as published on the [Hugging Face Hub](https://huggingface.co/models?search=stable-diffusion). This is optional and if specified, this argument will convert and bundle the refiner unet alongside the model unet.\n\n- `--bundle-resources-for-swift-cli`: Compiles all 4 models and bundles them along with necessary resources for text tokenization into `<output-mlpackages-directory>/Resources` which should provided as input to the Swift package. This flag is not necessary for the diffusers-based Python pipeline. [However using these compiled models in Python will significantly speed up inference](https://apple.github.io/coremltools/docs-guides/source/model-prediction.html#why-use-a-compiled-model).\n\n- `--quantize-nbits`: Quantizes the weights of unet and text_encoder models down to 2, 4, 6 or 8 bits using a globally optimal k-means clustering algorithm. By default all models are weight-quantized to 16 bits even if this argument is not specified. Please refer to [this section](#compression-6-bits-and-higher for details and further guidance on weight compression.\n\n- `--chunk-unet`: Splits the Unet model in two approximately equal chunks (each with less than 1GB of weights) for mobile-friendly deployment. This is **required** for Neural Engine deployment on iOS and iPadOS if weights are not quantized to 6-bits or less (`--quantize-nbits {2,4,6}`). This is not required for macOS. Swift CLI is able to consume both the chunked and regular versions of the Unet model but prioritizes the former. Note that chunked unet is not compatible with the Python pipeline because Python pipeline is intended for macOS only.\n\n- `--attention-implementation`: Defaults to `SPLIT_EINSUM` which is the implementation described in [Deploying Transformers on the Apple Neural Engine](https://machinelearning.apple.com/research/neural-engine-transformers). `--attention-implementation SPLIT_EINSUM_V2` yields 10-30% improvement for mobile devices, still targeting the Neural Engine. `--attention-implementation ORIGINAL` will switch to an alternative implementation that should be used for CPU or GPU deployment on some Mac devices. Please refer to the [Performance Benchmark](#performance-benchmark) section for further guidance.\n\n- `--check-output-correctness`: Compares original PyTorch model's outputs to final Core ML model's outputs. This flag increases RAM consumption significantly so it is recommended only for debugging purposes.\n\n- `--convert-controlnet`: Converts ControlNet models specified after this option. This can also convert multiple models if you specify like `--convert-controlnet lllyasviel/sd-controlnet-mlsd lllyasviel/sd-controlnet-depth`.\n\n- `--unet-support-controlnet`: enables a converted UNet model to receive additional inputs from ControlNet. This is required for generating image with using ControlNet and saved with a different name, `*_control-unet.mlpackage`, distinct from normal UNet. On the other hand, this UNet model can not work without ControlNet. Please use normal UNet for just txt2img.\n\n- `--unet-batch-one`: use a batch size of one for the unet, this is needed if you do not want to do classifier free guidance, i.e. using a `guidance-scale` of less than one.\n\n- `--convert-vae-encoder`: not required for text-to-image applications. Required for image-to-image applications in order to map the input image to the latent space.\n\n</details>\n\n## <a name=\"image-generation-with-python\"></a> Image Generation with Python\n\n<details>\n  <summary> Click to expand </summary>\n\nRun text-to-image generation using the example Python pipeline based on [diffusers](https://github.com/huggingface/diffusers):\n\n```shell\npython -m python_coreml_stable_diffusion.pipeline --prompt \"a photo of an astronaut riding a horse on mars\" -i <core-ml-model-directory> -o </path/to/output/image> --compute-unit ALL --seed 93\n```\nPlease refer to the help menu for all available arguments: `python -m python_coreml_stable_diffusion.pipeline -h`. Some notable arguments:\n\n- `-i`: Should point to the `-o` directory from Step 4 of [Converting Models to Core ML](#converting-models-to-coreml) section from above. If you specified `--bundle-resources-for-swift-cli` during conversion, then use the resulting `Resources` folder (which holds the compiled `.mlmodelc` files). [The compiled models load much faster after first use](https://apple.github.io/coremltools/docs-guides/source/model-prediction.html#why-use-a-compiled-model).\n- `--model-version`: If you overrode the default model version while converting models to Core ML, you will need to specify the same model version here.\n- `--compute-unit`: Note that the most performant compute unit for this particular implementation may differ across different hardware. `CPU_AND_GPU` or `CPU_AND_NE` may be faster than `ALL`. Please refer to the [Performance Benchmark](#performance-benchmark) section for further guidance.\n- `--scheduler`: If you would like to experiment with different schedulers, you may specify it here. For available options, please see the help menu. You may also specify a custom number of inference steps by `--num-inference-steps` which defaults to 50.\n- `--controlnet`: ControlNet models specified with this option are used in image generation. Use this option in the format `--controlnet lllyasviel/sd-controlnet-mlsd lllyasviel/sd-controlnet-depth` and make sure to use `--controlnet-inputs` in conjunction.\n- `--controlnet-inputs`: Image inputs corresponding to each ControlNet model. Please provide image paths in same order as models in `--controlnet`, for example: `--controlnet-inputs image_mlsd image_depth`.\n- `--unet-batch-one`: Do not batch unet predictions for the prompt and negative prompt. This requires the unet has been converted with a batch size of one, see `--unet-batch-one` option in conversion script.\n\n</details>\n\n## <a name=\"image-gen-swift\"></a> Image Generation with Swift\n\n<details>\n  <summary> Click to expand </summary>\n\n### Example CLI Usage\n```shell\nswift run StableDiffusionSample \"a photo of an astronaut riding a horse on mars\" --resource-path <output-mlpackages-directory>/Resources/ --seed 93 --output-path </path/to/output/image>\n```\nThe output will be named based on the prompt and random seed:\ne.g. `</path/to/output/image>/a_photo_of_an_astronaut_riding_a_horse_on_mars.93.final.png`\n\nPlease use the `--help` flag to learn about batched generation and more.\n\n### Example Library Usage\n\n```swift\nimport StableDiffusion\n...\nlet pipeline = try StableDiffusionPipeline(resourcesAt: resourceURL)\npipeline.loadResources()\nlet image = try pipeline.generateImages(prompt: prompt, seed: seed).first\n```\nOn iOS, the `reduceMemory` option should be set to `true` when constructing `StableDiffusionPipeline`\n\n### Swift Package Details\n\nThis Swift package contains two products:\n\n- `StableDiffusion` library\n- `StableDiffusionSample` command-line tool\n\nBoth of these products require the Core ML models and tokenization resources to be supplied. When specifying resources via a directory path that directory must contain the following:\n\n- `TextEncoder.mlmodelc` or `TextEncoder2.mlmodelc (text embedding model)\n- `Unet.mlmodelc` or `UnetChunk1.mlmodelc` & `UnetChunk2.mlmodelc` (denoising autoencoder model)\n- `VAEDecoder.mlmodelc` (image decoder model)\n- `vocab.json` (tokenizer vocabulary file)\n- `merges.text` (merges for byte pair encoding file)\n\nOptionally, for image2image, in-painting, or similar:\n\n- `VAEEncoder.mlmodelc` (image encoder model) \n\nOptionally, it may also include the safety checker model that some versions of Stable Diffusion include:\n\n- `SafetyChecker.mlmodelc`\n\nOptionally, for the SDXL refiner:\n\n- `UnetRefiner.mlmodelc` (refiner unet model) \n\nOptionally, for ControlNet:\n\n- `ControlledUNet.mlmodelc` or `ControlledUnetChunk1.mlmodelc` & `ControlledUnetChunk2.mlmodelc` (enabled to receive ControlNet values)\n- `controlnet/` (directory containing ControlNet models)\n  - `LllyasvielSdControlnetMlsd.mlmodelc` (for example, from lllyasviel/sd-controlnet-mlsd)\n  - `LllyasvielSdControlnetDepth.mlmodelc` (for example, from lllyasviel/sd-controlnet-depth)\n  - Other models you converted\n\nNote that the chunked version of Unet is checked for first. Only if it is not present will the full `Unet.mlmodelc` be loaded. Chunking is required for iOS and iPadOS and not necessary for macOS.\n\n</details>\n\n## <a name=\"swift-app\"></a> Example Swift App\n\n<details>\n  <summary> Click to expand </summary>\n\n Hugging Face created an [open-source demo app](https://github.com/huggingface/swift-coreml-diffusers) on top of this library. It's written in native Swift and Swift UI, and runs on macOS, iOS and iPadOS. You can use the code as a starting point for your app, or to see how to integrate this library in your own projects.\n\nHugging Face has made the app [available in the Mac App Store](https://apps.apple.com/app/diffusers/id1666309574?mt=12).\n\n</details>\n\n\n##  <a name=\"faq\"></a> FAQ\n\n<details>\n  <summary> Click to expand </summary>\n<details>\n\n\n<summary> <b> Q1: </b> <code> ERROR: Failed building wheel for tokenizers or error: can't find Rust compiler </code> </summary>\n\n<b> A1: </b> Please review this [potential solution](https://github.com/huggingface/transformers/issues/2831#issuecomment-592724471).\n</details>\n\n\n<details>\n<summary> <b> Q2: </b> <code> RuntimeError: {NSLocalizedDescription = \"Error computing NN outputs.\" </code> </summary>\n\n<b> A2: </b> There are many potential causes for this error. In this context, it is highly likely to be encountered when your system is under increased memory pressure from other applications. Reducing memory utilization of other applications is likely to help alleviate the issue.\n</details>\n\n<details>\n<summary> <b> <a name=\"low-mem-conversion\"></a> Q3: </b> My Mac has 8GB RAM and I am converting models to Core ML using the example command. The process is getting killed because of memory issues. How do I fix this issue? </summary>\n\n<b> A3: </b>  In order to minimize the memory impact of the model conversion process, please execute the following command instead:\n\n```bash\npython -m python_coreml_stable_diffusion.torch2coreml --convert-vae-encoder --model-version <model-version-string-from-hub> -o <output-mlpackages-directory> && \\\npython -m python_coreml_stable_diffusion.torch2coreml --convert-vae-decoder --model-version <model-version-string-from-hub> -o <output-mlpackages-directory> && \\\npython -m python_coreml_stable_diffusion.torch2coreml --convert-unet --model-version <model-version-string-from-hub> -o <output-mlpackages-directory> && \\\npython -m python_coreml_stable_diffusion.torch2coreml --convert-text-encoder --model-version <model-version-string-from-hub> -o <output-mlpackages-directory> && \\\npython -m python_coreml_stable_diffusion.torch2coreml --convert-safety-checker --model-version <model-version-string-from-hub> -o <output-mlpackages-directory> &&\n```\n\nIf you need `--chunk-unet`, you may do so in yet another independent command which will reuse the previously exported Unet model and simply chunk it in place:\n\n```bash\npython -m python_coreml_stable_diffusion.torch2coreml --convert-unet --chunk-unet -o <output-mlpackages-directory>\n```\n\n</details>\n\n<details>\n<summary> <b> Q4: </b> My Mac has 8GB RAM, should image generation work on my machine? </summary>\n\n<b> A4: </b> Yes! Especially the `--compute-unit CPU_AND_NE` option should work under reasonable system load from other applications. Note that part of the [Example Results](#example-results) were generated using an M2 MacBook Air with 8GB RAM.\n</details>\n\n<details>\n<summary> <b> Q5: </b> Every time I generate an image using the Python pipeline, loading all the Core ML models takes 2-3 minutes. Is this expected? </summary>\n\n<b> A5: </b> Both `.mlpackage` and `.mlmodelc` models are compiled (also known as \"model preparation\" in Core ML terms) upon first load when a specific compute unit is specified. `.mlpackage` does not cache this compiled asset so each model load retriggers this compilation which may take up to a few minutes. On the other hand, `.mlmodelc` files do cache this compiled asset and non-first load times are reduced to just a few seconds.\n\nIn order to benefit from compilation caching, you may use the `.mlmodelc` assets instead of `.mlpackage` assets in both Swift (default) and Python (possible thanks to [@lopez-hector](https://github.com/lopez-hector)'s [contribution](https://github.com/apple/ml-stable-diffusion/commit/f3a212491cf531dd88493c89ad3d98d016db407f)) image generation pipelines.\n\n\n</details>\n\n\n<details>\n<summary> <b> <a name=\"q-mobile-app\"></a> Q6: </b> I want to deploy <code>StableDiffusion</code>, the Swift package, in my mobile app. What should I be aware of? </summary>\n\n<b> A6: </b>The [Image Generation with Swift](#image-gen-swift) section describes the minimum SDK and OS versions as well as the device models supported by this package. We recommend carefully testing the package on the device with the least amount of RAM available among your deployment targets.\n\nThe image generation process in `StableDiffusion` can yield over 2 GB of peak memory during runtime depending on the compute units selected.  On iPadOS, we recommend using `.cpuAndNeuralEngine` in your configuration and the `reduceMemory` option when constructing a `StableDiffusionPipeline` to minimize memory pressure.\n\nIf your app crashes during image generation, consider adding the [Increased Memory Limit](https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_kernel_increased-memory-limit) capability to inform the system that some of your apps core features may perform better by exceeding the default app memory limit on supported devices.\n \nOn iOS, depending on the iPhone model, Stable Diffusion model versions, selected compute units, system load and design of your app, this may still not be sufficient to keep your apps peak memory under the limit. Please remember, because the device shares memory between apps and iOS processes, one app using too much memory can compromise the user experience across the whole device.\n\nWe **strongly recommend** compressing your models following the recipes in [Advanced Weight Compression (Lower than 6-bits)](#compression-lower-than-6-bits) for iOS deployment. This reduces the peak RAM usage by up to 75% (from 16-bit to 4-bit) while preserving model output quality.\n\n</details>\n\n<details>\n<summary> <b> Q7: </b> How do I generate images with different resolutions using the same Core ML models? </summary>\n\n<b> A7: </b> The current version of `python_coreml_stable_diffusion` does not support single-model multi-resolution out of the box. However, developers may fork this project and leverage the [flexible shapes](https://coremltools.readme.io/docs/flexible-inputs) support from coremltools to extend the `torch2coreml` script by using `coremltools.EnumeratedShapes`. Note that, while the `text_encoder` is agnostic to the image resolution, the inputs and outputs of `vae_decoder` and `unet` models are dependent on the desired image resolution.\n</details>\n\n<details>\n<summary> <b> Q8: </b> Are the Core ML and PyTorch generated images going to be identical? </summary>\n\n<b> A8: </b> If desired, the generated images across PyTorch and Core ML can be made approximately identical. However, it is not guaranteed by default. There are several factors that might lead to different images across PyTorch and Core ML:\n\n\n  <b> 1. Random Number Generator Behavior </b>\n\n  The main source of potentially different results across PyTorch and Core ML is the Random Number Generator ([RNG](https://en.wikipedia.org/wiki/Random_number_generation)) behavior. PyTorch and Numpy have different sources of randomness. `python_coreml_stable_diffusion` generally relies on Numpy for RNG (e.g. latents initialization) and `StableDiffusion` Swift Library reproduces this RNG behavior by default. However, PyTorch-based pipelines such as Hugging Face `diffusers` relies on PyTorch's RNG behavior. Thanks to @liuliu's [contributions](https://github.com/apple/ml-stable-diffusion/pull/124), one can match the PyTorch (CPU/GPU) RNG behavior in Swift by specifying `--rng torch/cuda` which selects the `torchRNG/cudaRNG` mode.\n\n  <b> 2. PyTorch </b>\n\n  *\"Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.\"* ([source](https://pytorch.org/docs/stable/notes/randomness.html#reproducibility)).\n\n  <b> 3. Model Function Drift During Conversion </b>\n\n  The difference in outputs across corresponding PyTorch and Core ML models is a potential cause. The signal integrity is tested during the conversion process (enabled via `--check-output-correctness` argument to  `python_coreml_stable_diffusion.torch2coreml`) and it is verified to be above a minimum [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio) value as tested on random inputs. Note that this is simply a sanity check and does not guarantee this minimum PSNR across all possible inputs. Furthermore, the results are not guaranteed to be identical when executing the same Core ML models across different compute units. This is not expected to be a major source of difference as the sample visual results indicate in [this section](#compression-6-bits-and-higher).\n\n  <b> 4. Weights and Activations Data Type </b>\n\n  When quantizing models from float32 to lower-precision data types such as float16, the generated images are [known to vary slightly](https://lambdalabs.com/blog/inference-benchmark-stable-diffusion) in semantics even when using the same PyTorch model. Core ML models generated by coremltools have float16 weights and activations by default [unless explicitly overridden](https://github.com/apple/coremltools/blob/main/coremltools/converters/_converters_entry.py#L256). This is not expected to be a major source of difference.\n\n</details>\n\n<details>\n<summary> <b> Q9: </b> The model files are very large, how do I avoid a large binary for my App? </summary>\n\n<b> A9: </b> The recommended option is to prompt the user to download these assets upon first launch of the app. This keeps the app binary size independent of the Core ML models being deployed. Disclosing the size of the download to the user is extremely important as there could be data charges or storage impact that the user might not be comfortable with.\n\n</details>\n\n<details>\n<summary> <b> Q10: </b>  <code> `Could not initialize NNPACK! Reason: Unsupported hardware`  </code> </summary>\n\n<b> A10: </b> This warning is safe to ignore in the context of this repository.\n\n</details>\n\n<details>\n<summary> <b> Q11: </b>  <code> TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect </code> </summary>\n\n<b> A11: </b> This warning is safe to ignore in the context of this repository.\n</details>\n\n<details>\n<summary> <b> Q12: </b>  <code> UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown </code> </summary>\n\n<b> A12: </b> If this warning is printed right after <code> zsh: killed     python -m python_coreml_stable_diffusion.torch2coreml ... </code>, then it is highly likely that your Mac has run out of memory while converting models to Core ML. Please see [Q3](#low-mem-conversion) from above for the solution.\n\n</details>\n\n</details>\n\n</details>\n\n## <a name=\"bibtex\"></a> BibTeX Reference\n\n```latex\n@misc{stable-diffusion-coreml-apple-silicon,\ntitle = {Stable Diffusion with Core ML on Apple Silicon},\nauthor = {Atila Orhon and Michael Siracusa and Aseem Wadhwa},\nyear = {2022},\nURL = {null}\n}\n```\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "python_coreml_stable_diffusion",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.154296875,
          "content": "coremltools>=8.0\ndiffusers[torch]==0.30.2\ndiffusionkit==0.4.0\ntorch\ntransformers==4.44.2\nscipy\nscikit-learn\npytest\ninvisible-watermark\nsafetensors\nmatplotlib\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.3330078125,
          "content": "from setuptools import setup, find_packages\n\nfrom python_coreml_stable_diffusion._version import __version__\n\nwith open('README.md') as f:\n    readme = f.read()\n\nsetup(\n    name='python_coreml_stable_diffusion',\n    version=__version__,\n    url='https://github.com/apple/ml-stable-diffusion',\n    description=\"Run Stable Diffusion on Apple Silicon with Core ML (Python and Swift)\",\n    long_description=readme,\n    long_description_content_type='text/markdown',\n    author='Apple Inc.',\n    install_requires=[\n        \"coremltools>=8.0\",\n        \"diffusers[torch]==0.30.2\",\n        \"torch\",\n        \"transformers==4.44.2\",\n        \"huggingface-hub==0.24.6\",\n        \"scipy\",\n        \"numpy<1.24\",\n        \"pytest\",\n        \"scikit-learn\",\n        \"invisible-watermark\",\n        \"safetensors\",\n        \"matplotlib\",\n        \"diffusionkit==0.4.0\",\n    ],\n    packages=find_packages(),\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"Operating System :: MacOS :: MacOS X\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Topic :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Software Development\",\n    ],\n)\n"
        },
        {
          "name": "swift",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}