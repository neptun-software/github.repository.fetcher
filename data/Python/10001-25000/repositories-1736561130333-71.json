{
  "metadata": {
    "timestamp": 1736561130333,
    "page": 71,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "huggingface/datasets",
      "stars": 19455,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dvc",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dvcignore",
          "type": "blob",
          "size": 0.1357421875,
          "content": "# Add patterns of files dvc should ignore, which could improve\n# the performance. Learn more at\n# https://dvc.org/doc/user-guide/dvcignore\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.60546875,
          "content": "# Locked files\n*.lock\n!dvc.lock\n\n# Extracted dummy data\ndatasets/**/dummy_data-zip-extracted/\n\n# Compiled python modules.\n*.pyc\n\n# Byte-compiled\n_pycache__/\n.cache/\n\n# Python egg metadata, regenerated from source files by setuptools.\n*.egg-info\n.eggs/\n\n# PyPI distribution artifacts.\nbuild/\ndist/\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# pyenv\n.python-version\n\n# Tests\n.pytest_cache/\n\n# Other\n*.DS_Store\n\n# PyCharm/vscode\n.idea\n.vscode\n\n# Vim\n.*.swp\n\n# playground\n/playground\n\n# Sphinx documentation\ndocs/_build/\ndocs/source/_build/\n\n# Benchmark results\nreport.json\nreport.md\n\n# Ruff\n.ruff_cache\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.2509765625,
          "content": "repos:\n  - repo: https://github.com/charliermarsh/ruff-pre-commit # https://github.com/charliermarsh/ruff#usage\n    rev: 'v0.3.0'\n    hooks:\n      # Run the linter.\n      - id: ruff\n        args: [ --fix ]\n      # Run the formatter.\n      - id: ruff-format\n"
        },
        {
          "name": ".zenodo.json",
          "type": "blob",
          "size": 3.1728515625,
          "content": "{\n    \"license\": \"Apache-2.0\",\n    \"creators\": [\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Quentin Lhoest\"\n        },\n        {\n            \"orcid\": \"0000-0003-1727-1045\",\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Albert Villanova del Moral\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Patrick von Platen\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Thomas Wolf\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Mario Å aÅ¡ko\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Yacine Jernite\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Abhishek Thakur\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Lewis Tunstall\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Suraj Patil\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Mariama Drame\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Julien Chaumond\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Julien Plu\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Joe Davison\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Simon Brandeis\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Victor Sanh\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Teven Le Scao\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Kevin Canwen Xu\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Nicolas Patry\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Steven Liu\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Angelina McMillan-Major\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Philipp Schmid\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Sylvain Gugger\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Nathan Raw\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Sylvain Lesage\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Anton Lozhkov\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Matthew Carrigan\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Th\\u00e9o Matussi\\u00e8re\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Leandro von Werra\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Lysandre Debut\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Stas Bekman\"\n        },\n        {\n            \"affiliation\": \"Hugging Face\",\n            \"name\": \"Cl\\u00e9ment Delangue\"\n        }\n    ]\n}"
        },
        {
          "name": "ADD_NEW_DATASET.md",
          "type": "blob",
          "size": 0.373046875,
          "content": "# How to add one new datasets\n\nAdd datasets directly to the ðŸ¤— Hugging Face Hub!\n\nYou can share your dataset on https://huggingface.co/datasets directly using your account, see the documentation:\n\n* [Create a dataset and upload files on the website](https://huggingface.co/docs/datasets/upload_dataset)\n* [Advanced guide using the CLI](https://huggingface.co/docs/datasets/share)\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.3193359375,
          "content": "# This is the list of HuggingFace Datasets authors for copyright purposes.\n#\n# This does not necessarily list everyone who has contributed code, since in\n# some cases, their employer may be the copyright holder.  To see the full list\n# of contributors, see the revision history in source control.\n\nGoogle Inc.\nHuggingFace Inc.\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 3.8046875,
          "content": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\ntitle: \"huggingface/datasets\"\nauthors:\n- family-names: Lhoest\n  given-names: Quentin\n- family-names: Villanova del Moral\n  given-names: Albert\n  orcid: \"https://orcid.org/0000-0003-1727-1045\"\n- family-names: von Platen\n  given-names: Patrick\n- family-names: Wolf\n  given-names: Thomas\n- family-names: Å aÅ¡ko\n  given-names: Mario\n- family-names: Jernite\n  given-names: Yacine\n- family-names: Thakur\n  given-names: Abhishek\n- family-names: Tunstall\n  given-names: Lewis\n- family-names: Patil\n  given-names: Suraj\n- family-names: Drame\n  given-names: Mariama\n- family-names: Chaumond\n  given-names: Julien\n- family-names: Plu\n  given-names: Julien\n- family-names: Davison\n  given-names: Joe\n- family-names: Brandeis\n  given-names: Simon\n- family-names: Sanh\n  given-names: Victor\n- family-names: Le Scao\n  given-names: Teven\n- family-names: Canwen Xu\n  given-names: Kevin\n- family-names: Patry\n  given-names: Nicolas\n- family-names: Liu\n  given-names: Steven\n- family-names: McMillan-Major\n  given-names: Angelina\n- family-names: Schmid\n  given-names: Philipp\n- family-names: Gugger\n  given-names: Sylvain\n- family-names: Raw\n  given-names: Nathan\n- family-names: Lesage\n  given-names: Sylvain\n- family-names: Lozhkov\n  given-names: Anton\n- family-names: Carrigan\n  given-names: Matthew\n- family-names: MatussiÃ¨re\n  given-names: ThÃ©o\n- family-names: von Werra\n  given-names: Leandro\n- family-names: Debut\n  given-names: Lysandre\n- family-names: Bekman\n  given-names: Stas\n- family-names: Delangue\n  given-names: ClÃ©ment\ndoi: 10.5281/zenodo.4817768\nrepository-code: \"https://github.com/huggingface/datasets\"\nlicense: Apache-2.0\npreferred-citation:\n  type: conference-paper\n  title: \"Datasets: A Community Library for Natural Language Processing\"\n  authors:\n  - family-names: Lhoest\n    given-names: Quentin\n  - family-names: Villanova del Moral\n    given-names: Albert\n    orcid: \"https://orcid.org/0000-0003-1727-1045\"\n  - family-names: von Platen\n    given-names: Patrick\n  - family-names: Wolf\n    given-names: Thomas\n  - family-names: Å aÅ¡ko\n    given-names: Mario\n  - family-names: Jernite\n    given-names: Yacine\n  - family-names: Thakur\n    given-names: Abhishek\n  - family-names: Tunstall\n    given-names: Lewis\n  - family-names: Patil\n    given-names: Suraj\n  - family-names: Drame\n    given-names: Mariama\n  - family-names: Chaumond\n    given-names: Julien\n  - family-names: Plu\n    given-names: Julien\n  - family-names: Davison\n    given-names: Joe\n  - family-names: Brandeis\n    given-names: Simon\n  - family-names: Sanh\n    given-names: Victor\n  - family-names: Le Scao\n    given-names: Teven\n  - family-names: Canwen Xu\n    given-names: Kevin\n  - family-names: Patry\n    given-names: Nicolas\n  - family-names: Liu\n    given-names: Steven\n  - family-names: McMillan-Major\n    given-names: Angelina\n  - family-names: Schmid\n    given-names: Philipp\n  - family-names: Gugger\n    given-names: Sylvain\n  - family-names: Raw\n    given-names: Nathan\n  - family-names: Lesage\n    given-names: Sylvain\n  - family-names: Lozhkov\n    given-names: Anton\n  - family-names: Carrigan\n    given-names: Matthew\n  - family-names: MatussiÃ¨re\n    given-names: ThÃ©o\n  - family-names: von Werra\n    given-names: Leandro\n  - family-names: Debut\n    given-names: Lysandre\n  - family-names: Bekman\n    given-names: Stas\n  - family-names: Delangue\n    given-names: ClÃ©ment\n  collection-title: \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"\n  collection-type: proceedings\n  month: 11\n  year: 2021\n  publisher:\n    name: \"Association for Computational Linguistics\"\n  url: \"https://aclanthology.org/2021.emnlp-demo.21\"\n  start: 175\n  end: 184\n  identifiers:\n    - type: other\n      value: \"arXiv:2109.02846\"\n      description: \"The arXiv preprint of the paper\"\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.3623046875,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nfeedback@huggingface.co.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\n[https://www.contributor-covenant.org/version/2/0/code_of_conduct.html][v2.0].\n\nCommunity Impact Guidelines were inspired by \n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available \nat [https://www.contributor-covenant.org/translations][translations].\n\n[homepage]: https://www.contributor-covenant.org\n[v2.0]: https://www.contributor-covenant.org/version/2/0/code_of_conduct.html\n[Mozilla CoC]: https://github.com/mozilla/diversity\n[FAQ]: https://www.contributor-covenant.org/faq\n[translations]: https://www.contributor-covenant.org/translations\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 6.2548828125,
          "content": "# How to contribute to Datasets?\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg)](CODE_OF_CONDUCT.md)\n\nDatasets is an open source project, so all contributions and suggestions are welcome.\n\nYou can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements,\nimproving the documentation, fixing bugs,...\n\nMany thanks in advance to every contributor.\n\nIn order to facilitate healthy, constructive behavior in an open and inclusive community, we all respect and abide by\nour [code of conduct](CODE_OF_CONDUCT.md).\n\n## How to work on an open Issue?\nYou have the list of open Issues at: https://github.com/huggingface/datasets/issues\n\nSome of them may have the label `help wanted`: that means that any contributor is welcomed!\n\nIf you would like to work on any of the open Issues:\n\n1. Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page.\n\n2. You can self-assign it by commenting on the Issue page with the keyword: `#self-assign`.\n\n3. Work on your self-assigned issue and eventually create a Pull Request.\n\n## How to create a Pull Request?\nIf you want to add a dataset see specific instructions in the section [*How to add a dataset*](#how-to-add-a-dataset).\n\n1. Fork the [repository](https://github.com/huggingface/datasets) by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account.\n\n2. Clone your fork to your local disk, and add the base repository as a remote:\n\n    ```bash\n    git clone git@github.com:<your Github handle>/datasets.git\n    cd datasets\n    git remote add upstream https://github.com/huggingface/datasets.git\n    ```\n\n3. Create a new branch to hold your development changes:\n\n    ```bash\n    git checkout -b a-descriptive-name-for-my-changes\n    ```\n\n    **do not** work on the `main` branch.\n\n4. Set up a development environment by running the following command in a virtual environment:\n\n    Simple setup with code formatting only (recommended)\n    ```bash\n    pip install -e \".[quality]\"\n    ```\n    \n    Advanced setup with all the optional dependencies\n    ```bash\n    pip install -e \".[dev]\"\n    ```\n\n   (If datasets was already installed in the virtual environment, remove\n   it with `pip uninstall datasets` before reinstalling it in editable\n   mode with the `-e` flag.)\n\n5. Develop the features on your branch.\n\n6. Format your code. Run `black` and `ruff` so that your newly added files look nice with the following command:\n\n    ```bash\n    make style\n    ```\n   \n7. _(Optional)_ You can also use [`pre-commit`](https://pre-commit.com/) to format your code automatically each time run `git commit`, instead of running `make style` manually. \nTo do this, install `pre-commit` via `pip install pre-commit` and then run `pre-commit install` in the project's root directory to set up the hooks.\nNote that if any files were formatted by `pre-commit` hooks during committing, you have to run `git commit` again .\n\n\n8. Once you're happy with your contribution, add your changed files and make a commit to record your changes locally:\n\n    ```bash\n    git add -u\n    git commit\n    ```\n\n    It is a good idea to sync your copy of the code with the original\n    repository regularly. This way you can quickly account for changes:\n\n    ```bash\n    git fetch upstream\n    git rebase upstream/main\n    ```\n\n9. Once you are satisfied, push the changes to your fork repo using:\n\n   ```bash\n   git push -u origin a-descriptive-name-for-my-changes\n   ```\n\n   Go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.\n\n## Datasets on Hugging Face\n\n### How to add a dataset on Hugging Face\n\nYou can share your dataset on https://huggingface.co/datasets directly using your account (no need to open a PR on GitHub), see the documentation:\n\n* [Create a dataset and upload files on the website](https://huggingface.co/docs/datasets/upload_dataset)\n* [Advanced guide using the CLI](https://huggingface.co/docs/datasets/share)\n\n### How to contribute to the dataset cards\n\nImproving the documentation of datasets is an ever-increasing effort, and we invite users to contribute by sharing their insights with the community in the `README.md` dataset cards provided for each dataset.\n\nIf you see that a dataset card is missing information that you are in a position to provide (as an author of the dataset or as an experienced user), the best thing you can do is to open a Pull Request on the Hugging Face Hub. To do, go to the \"Files and versions\" tab of the dataset page and edit the `README.md` file. We provide:\n\n* a [template](https://github.com/huggingface/datasets/blob/main/templates/README.md)\n* a [guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md) describing what information should go into each of the paragraphs\n* and if you need inspiration, we recommend looking through a [completed example](https://huggingface.co/datasets/eli5/blob/main/README.md)\n\nIf you are a **dataset author**... you know what to do, it is your dataset after all ;) ! We would especially appreciate if you could help us fill in information about the process of creating the dataset, and take a moment to reflect on its social impact and possible limitations if you haven't already done so in the dataset paper or in another data statement.\n\nIf you are a **user of a dataset**, the main source of information should be the dataset paper if it is available: we recommend pulling information from there into the relevant paragraphs of the template. We also eagerly welcome discussions on the [Considerations for Using the Data](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#considerations-for-using-the-data) based on existing scholarship or personal experience that would benefit the whole community.\n\nFinally, if you want more information on the how and why of dataset cards, we strongly recommend reading the foundational works [Datasheets for Datasets](https://arxiv.org/abs/1803.09010) and [Data Statements for NLP](https://www.aclweb.org/anthology/Q18-1041/).\n\nThank you for your contribution!\n\n## Code of conduct\n\nThis project adheres to the HuggingFace [code of conduct](CODE_OF_CONDUCT.md).\nBy participating, you are expected to abide by this code.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.4541015625,
          "content": ".PHONY: quality style test\n\ncheck_dirs := tests src benchmarks utils\n\n# Check that source code meets quality standards\n\nquality:\n\truff check $(check_dirs) setup.py  # linter\n\truff format --check $(check_dirs) setup.py # formatter\n\n# Format source code automatically\n\nstyle:\n\truff check --fix $(check_dirs) setup.py # linter\n\truff format $(check_dirs) setup.py # formatter\n\n# Run tests for the library\n\ntest:\n\tpython -m pytest -n auto --dist=loadfile -s -v ./tests/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.994140625,
          "content": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg\">\n    <img alt=\"Hugging Face Datasets Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/huggingface/datasets/actions/workflows/ci.yml?query=branch%3Amain\"><img alt=\"Build\" src=\"https://github.com/huggingface/datasets/actions/workflows/ci.yml/badge.svg?branch=main\"></a>\n    <a href=\"https://github.com/huggingface/datasets/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/datasets.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/datasets/index.html\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/datasets/index.html.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/datasets/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/datasets.svg\"></a>\n    <a href=\"https://huggingface.co/datasets/\"><img alt=\"Number of datasets\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen\"></a>\n    <a href=\"CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/250213286\"><img src=\"https://zenodo.org/badge/250213286.svg\" alt=\"DOI\"></a>\n</p>\n\nðŸ¤— Datasets is a lightweight library providing **two** main features:\n\n- **one-line dataloaders for many public datasets**: one-liners to download and pre-process any of the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen) major public datasets (image datasets, audio datasets, text datasets in 467 languages and dialects, etc.) provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets). With a simple command like `squad_dataset = load_dataset(\"squad\")`, get any of these datasets ready to use in a dataloader for training/evaluating a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX),\n- **efficient data pre-processing**: simple, fast and reproducible data pre-processing for the public datasets as well as your own local datasets in CSV, JSON, text, PNG, JPEG, WAV, MP3, Parquet, etc. With simple commands like `processed_dataset = dataset.map(process_example)`, efficiently prepare the dataset for inspection and ML model evaluation and training.\n\n[ðŸŽ“ **Documentation**](https://huggingface.co/docs/datasets/) [ðŸ”Ž **Find a dataset in the Hub**](https://huggingface.co/datasets) [ðŸŒŸ **Share a dataset on the Hub**](https://huggingface.co/docs/datasets/share)\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://raw.githubusercontent.com/huggingface/datasets/main/docs/source/imgs/course_banner.png\"></a>\n</h3>\n\nðŸ¤— Datasets is designed to let the community easily add and share new datasets.\n\nðŸ¤— Datasets has many additional interesting features:\n\n- Thrive on large datasets: ðŸ¤— Datasets naturally frees the user from RAM memory limitation, all datasets are memory-mapped using an efficient zero-serialization cost backend (Apache Arrow).\n- Smart caching: never wait for your data to process several times.\n- Lightweight and fast with a transparent and pythonic API (multi-processing/caching/memory-mapping).\n- Built-in interoperability with NumPy, pandas, PyTorch, TensorFlow 2 and JAX.\n- Native support for audio and image data.\n- Enable streaming mode to save disk space and start iterating over the dataset immediately.\n\nðŸ¤— Datasets originated from a fork of the awesome [TensorFlow Datasets](https://github.com/tensorflow/datasets) and the HuggingFace team want to deeply thank the TensorFlow Datasets team for building this amazing library. More details on the differences between ðŸ¤— Datasets and `tfds` can be found in the section [Main differences between ðŸ¤— Datasets and `tfds`](#main-differences-between--datasets-and-tfds).\n\n# Installation\n\n## With pip\n\nðŸ¤— Datasets can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\n\n```bash\npip install datasets\n```\n\n## With conda\n\nðŸ¤— Datasets can be installed using conda as follows:\n\n```bash\nconda install -c huggingface -c conda-forge datasets\n```\n\nFollow the installation pages of TensorFlow and PyTorch to see how to install them with conda.\n\nFor more details on installation, check the installation page in the documentation: https://huggingface.co/docs/datasets/installation\n\n## Installation to use with PyTorch/TensorFlow/pandas\n\nIf you plan to use ðŸ¤— Datasets with PyTorch (1.0+), TensorFlow (2.2+) or pandas, you should also install PyTorch, TensorFlow or pandas.\n\nFor more details on using the library with NumPy, pandas, PyTorch or TensorFlow, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart\n\n# Usage\n\nðŸ¤— Datasets is made to be very simple to use - the API is centered around a single function, `datasets.load_dataset(dataset_name, **kwargs)`, that instantiates a dataset.\n\nThis library can be used for text/image/audio/etc. datasets. Here is an example to load a text dataset:\n\nHere is a quick example:\n\n```python\nfrom datasets import load_dataset\n\n# Print all the available datasets\nfrom huggingface_hub import list_datasets\nprint([dataset.id for dataset in list_datasets()])\n\n# Load a dataset and print the first example in the training set\nsquad_dataset = load_dataset('squad')\nprint(squad_dataset['train'][0])\n\n# Process the dataset - add a column with the length of the context texts\ndataset_with_length = squad_dataset.map(lambda x: {\"length\": len(x[\"context\"])})\n\n# Process the dataset - tokenize the context texts (using a tokenizer from the ðŸ¤— Transformers library)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n\ntokenized_dataset = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)\n```\n\nIf your dataset is bigger than your disk or if you don't want to wait to download the data, you can use streaming:\n\n```python\n# If you want to use the dataset immediately and efficiently stream the data as you iterate over the dataset\nimage_dataset = load_dataset('cifar100', streaming=True)\nfor example in image_dataset[\"train\"]:\n    break\n```\n\nFor more details on using the library, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart and the specific pages on:\n\n- Loading a dataset: https://huggingface.co/docs/datasets/loading\n- What's in a Dataset: https://huggingface.co/docs/datasets/access\n- Processing data with ðŸ¤— Datasets: https://huggingface.co/docs/datasets/process\n    - Processing audio data: https://huggingface.co/docs/datasets/audio_process\n    - Processing image data: https://huggingface.co/docs/datasets/image_process\n    - Processing text data: https://huggingface.co/docs/datasets/nlp_process\n- Streaming a dataset: https://huggingface.co/docs/datasets/stream\n- Writing your own dataset loading script: https://huggingface.co/docs/datasets/dataset_script\n- etc.\n\n# Add a new dataset to the Hub\n\nWe have a very detailed step-by-step guide to add a new dataset to the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen) datasets already provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets).\n\nYou can find:\n- [how to upload a dataset to the Hub using your web browser or Python](https://huggingface.co/docs/datasets/upload_dataset) and also\n- [how to upload it using Git](https://huggingface.co/docs/datasets/share).\n\n# Main differences between ðŸ¤— Datasets and `tfds`\n\nIf you are familiar with the great TensorFlow Datasets, here are the main differences between ðŸ¤— Datasets and `tfds`:\n\n- the scripts in ðŸ¤— Datasets are not provided within the library but are queried, downloaded/cached and dynamically loaded upon request\n- the backend serialization of ðŸ¤— Datasets is based on [Apache Arrow](https://arrow.apache.org/) instead of TF Records and leverage python dataclasses for info and features with some diverging features (we mostly don't do encoding and store the raw data as much as possible in the backend serialization cache).\n- the user-facing dataset object of ðŸ¤— Datasets is not a `tf.data.Dataset` but a built-in framework-agnostic dataset class with methods inspired by what we like in `tf.data` (like a `map()` method). It basically wraps a memory-mapped Arrow table cache.\n\n# Disclaimers\n\nðŸ¤— Datasets may run Python code defined by the dataset authors to parse certain data formats or structures. For security reasons, we ask users to:\n- check the dataset scripts they're going to run beforehand and\n- pin the `revision` of the repositories they use.\n\nIf you're a dataset owner and wish to update any part of it (description, citation, license, etc.), or do not want your dataset to be included in the Hugging Face Hub, please get in touch by opening a discussion or a pull request in the Community tab of the dataset page. Thanks for your contribution to the ML community!\n\n## BibTeX\n\nIf you want to cite our ðŸ¤— Datasets library, you can use our [paper](https://arxiv.org/abs/2109.02846):\n\n```bibtex\n@inproceedings{lhoest-etal-2021-datasets,\n    title = \"Datasets: A Community Library for Natural Language Processing\",\n    author = \"Lhoest, Quentin  and\n      Villanova del Moral, Albert  and\n      Jernite, Yacine  and\n      Thakur, Abhishek  and\n      von Platen, Patrick  and\n      Patil, Suraj  and\n      Chaumond, Julien  and\n      Drame, Mariama  and\n      Plu, Julien  and\n      Tunstall, Lewis  and\n      Davison, Joe  and\n      {\\v{S}}a{\\v{s}}ko, Mario  and\n      Chhablani, Gunjan  and\n      Malik, Bhavitvya  and\n      Brandeis, Simon  and\n      Le Scao, Teven  and\n      Sanh, Victor  and\n      Xu, Canwen  and\n      Patry, Nicolas  and\n      McMillan-Major, Angelina  and\n      Schmid, Philipp  and\n      Gugger, Sylvain  and\n      Delangue, Cl{\\'e}ment  and\n      Matussi{\\`e}re, Th{\\'e}o  and\n      Debut, Lysandre  and\n      Bekman, Stas  and\n      Cistac, Pierric  and\n      Goehringer, Thibault  and\n      Mustar, Victor  and\n      Lagunas, Fran{\\c{c}}ois  and\n      Rush, Alexander  and\n      Wolf, Thomas\",\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.emnlp-demo.21\",\n    pages = \"175--184\",\n    abstract = \"The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.\",\n    eprint={2109.02846},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n}\n```\n\nIf you need to cite a specific version of our ðŸ¤— Datasets library for reproducibility, you can use the corresponding version Zenodo DOI from this [list](https://zenodo.org/search?q=conceptrecid:%224817768%22&sort=-version&all_versions=True).\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.896484375,
          "content": "# Security Policy\n\n## Supported Versions\n<!--\nUse this section to tell people about which versions of your project are\ncurrently being supported with security updates.\n\n| Version | Supported          |\n| ------- | ------------------ |\n| 5.1.x   | :white_check_mark: |\n| 5.0.x   | :x:                |\n| 4.0.x   | :white_check_mark: |\n| < 4.0   | :x:                |\n-->\n\nEach major version is currently being supported with security updates.\n\n| Version | Supported          |\n|---------|--------------------|\n| 1.x.x   | :white_check_mark: |\n| 2.x.x   | :white_check_mark: |\n\n\n## Reporting a Vulnerability\n<!--\nUse this section to tell people how to report a vulnerability.\n\nTell them where to go, how often they can expect to get an update on a\nreported vulnerability, what to expect if the vulnerability is accepted or\ndeclined, etc.\n-->\n\nTo report a security vulnerability, please contact: security@huggingface.co\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.6630859375,
          "content": "[tool.ruff]\nline-length = 119\n\n[tool.ruff.lint]\n# Ignored rules:\n#   \"E501\" -> line length violation\n#   \"F821\" -> undefined named in type annotation (e.g. Literal[\"something\"])\n#   \"C901\" -> `function_name` is too complex\nignore = [\"E501\", \"F821\", \"C901\"]\nselect = [\"C\", \"E\", \"F\", \"I\", \"W\"]\n\n[tool.ruff.lint.isort]\nlines-after-imports = 2\nknown-first-party = [\"datasets\"]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\", \"F403\", \"F405\"]\n\n[tool.pytest.ini_options]\n# Test fails if a FutureWarning is thrown by `huggingface_hub`\nfilterwarnings = [\n    \"error::FutureWarning:huggingface_hub*\",\n]\nmarkers = [\n    \"unit: unit test\",\n    \"integration: integration test\",\n]\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 9.4169921875,
          "content": "# Lint as: python3\n\"\"\"HuggingFace/Datasets is an open library of datasets.\n\nNote:\n\n   VERSION needs to be formatted following the MAJOR.MINOR.PATCH convention\n   (we need to follow this convention to be able to retrieve versioned scripts)\n\nSimple check list for release from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py\n\nSteps to make a release:\n\n0. Prerequisites:\n   - Dependencies:\n     - twine: `pip install twine`\n   - Create an account in (and join the 'datasets' project):\n     - PyPI: https://pypi.org/\n     - Test PyPI: https://test.pypi.org/\n   - Don't break `transformers`: run the `transformers` CI using the `main` branch and make sure it's green.\n     - In `transformers`, use `datasets @ git+https://github.com/huggingface/datasets@main#egg=datasets`\n       Add a step to install `datasets@main` after `save_cache` in .circleci/create_circleci_config.py:\n       ```\n       steps.append({\"run\": {\"name\": \"Install `datasets@main`\", \"command\": 'pip uninstall datasets -y && pip install \"datasets @ git+https://github.com/huggingface/datasets@main#egg=datasets\"'}})\n       ```\n     - and then run the CI\n\n1. Create the release branch from main branch:\n     ```\n     git checkout main\n     git pull upstream main\n     git checkout -b release-VERSION\n     ```\n\n2. Change the version to the release VERSION in:\n   - __init__.py\n   - setup.py\n\n3. Commit these changes, push and create a Pull Request:\n     ```\n     git add -u\n     git commit -m \"Release: VERSION\"\n     git push upstream release-VERSION\n     ```\n   - Go to: https://github.com/huggingface/datasets/pull/new/release-VERSION\n   - Create pull request\n\n4. From your local release branch, build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n   - First, delete any building directories that may exist from previous builds:\n     - build\n     - dist\n   - From the top level directory, build the wheel and the sources:\n       ```\n       python setup.py bdist_wheel\n       python setup.py sdist\n       ```\n   - You should now have a /dist directory with both .whl and .tar.gz source versions.\n\n5. Check that everything looks correct by uploading the package to the test PyPI server:\n     ```\n     twine upload dist/* -r testpypi\n     ```\n   Check that you can install it in a virtualenv/notebook by running:\n     ```\n     pip install huggingface-hub fsspec aiohttp\n     pip install -U tqdm pyarrow\n     pip install -i https://testpypi.python.org/pypi datasets\n     ```\n\n6. Upload the final version to the actual PyPI:\n     ```\n     twine upload dist/* -r pypi\n     ```\n\n7. Make the release on GitHub once everything is looking hunky-dory:\n   - Merge the release Pull Request\n   - Create a new release: https://github.com/huggingface/datasets/releases/new\n   - Choose a tag: Introduce the new VERSION as tag, that will be created when you publish the release\n     - Create new tag VERSION on publish\n   - Release title: Introduce the new VERSION as well\n   - Describe the release\n     - Use \"Generate release notes\" button for automatic generation\n   - Publish release\n\n8. Set the dev version\n   - Create the dev-version branch from the main branch:\n       ```\n       git checkout main\n       git pull upstream main\n       git branch -D dev-version\n       git checkout -b dev-version\n       ```\n   - Change the version to X.X.X+1.dev0 (e.g. VERSION=1.18.3 -> 1.18.4.dev0) in:\n     - __init__.py\n     - setup.py\n   - Commit these changes, push and create a Pull Request:\n       ```\n       git add -u\n       git commit -m \"Set dev version\"\n       git push upstream dev-version\n       ```\n     - Go to: https://github.com/huggingface/datasets/pull/new/dev-version\n     - Create pull request\n   - Merge the dev version Pull Request\n\"\"\"\n\nfrom setuptools import find_packages, setup\n\n\nREQUIRED_PKGS = [\n    # For file locking\n    \"filelock\",\n    # We use numpy>=1.17 to have np.random.Generator (Dataset shuffling)\n    \"numpy>=1.17\",\n    # Backend and serialization.\n    # Minimum 15.0.0 to be able to cast dictionary types to their underlying types\n    \"pyarrow>=15.0.0\",\n    # For smart caching dataset processing\n    \"dill>=0.3.0,<0.3.9\",  # tmp pin until dill has official support for determinism see https://github.com/uqfoundation/dill/issues/19\n    # For performance gains with apache arrow\n    \"pandas\",\n    # for downloading datasets over HTTPS\n    \"requests>=2.32.2\",\n    # progress bars in download and scripts\n    \"tqdm>=4.66.3\",\n    # for fast hashing\n    \"xxhash\",\n    # for better multiprocessing\n    \"multiprocess<0.70.17\",  # to align with dill<0.3.9 (see above)\n    # to save datasets locally or on any filesystem\n    # minimum 2023.1.0 to support protocol=kwargs in fsspec's `open`, `get_fs_token_paths`, etc.: see https://github.com/fsspec/filesystem_spec/pull/1143\n    \"fsspec[http]>=2023.1.0,<=2024.12.0\",\n    # for data streaming via http\n    \"aiohttp\",\n    # To get datasets from the Datasets Hub on huggingface.co\n    \"huggingface-hub>=0.24.0\",\n    # Utilities from PyPA to e.g., compare versions\n    \"packaging\",\n    # To parse YAML metadata from dataset cards\n    \"pyyaml>=5.1\",\n]\n\nAUDIO_REQUIRE = [\n    \"soundfile>=0.12.1\",\n    \"librosa\",\n    \"soxr>=0.4.0; python_version>='3.9'\",  # Supports numpy-2\n]\n\nVISION_REQUIRE = [\n    \"Pillow>=9.4.0\",  # When PIL.Image.ExifTags was introduced\n]\n\nBENCHMARKS_REQUIRE = [\n    \"tensorflow==2.12.0\",\n    \"torch==2.0.1\",\n    \"transformers==4.30.1\",\n]\n\nTESTS_REQUIRE = [\n    # test dependencies\n    \"absl-py\",\n    \"decorator\",\n    \"joblib<1.3.0\",  # joblibspark doesn't support recent joblib versions\n    \"joblibspark\",\n    \"pytest\",\n    \"pytest-datadir\",\n    \"pytest-xdist\",\n    # optional dependencies\n    \"elasticsearch>=7.17.12,<8.0.0\",  # 8.0 asks users to provide hosts or cloud_id when instantiating ElasticSearch(); 7.9.1 has legacy numpy.float_ which was fixed in https://github.com/elastic/elasticsearch-py/pull/2551.\n    \"faiss-cpu>=1.8.0.post1\",  # Pins numpy < 2\n    \"jax>=0.3.14; sys_platform != 'win32'\",\n    \"jaxlib>=0.3.14; sys_platform != 'win32'\",\n    \"lz4\",\n    \"moto[server]\",\n    \"pyspark>=3.4\",  # https://issues.apache.org/jira/browse/SPARK-40991 fixed in 3.4.0\n    \"py7zr\",\n    \"rarfile>=4.0\",\n    \"sqlalchemy\",\n    \"s3fs>=2021.11.1\",  # aligned with fsspec[http]>=2021.11.1; test only on python 3.7 for now\n    \"protobuf<4.0.0\",  # 4.0.0 breaks compatibility with tensorflow<2.12\n    \"tensorflow>=2.6.0; python_version<'3.10'\",  # numpy-2 is not supported for Python < 3.10\n    \"tensorflow>=2.16.0; python_version>='3.10'\",  # Pins numpy < 2\n    \"tiktoken\",\n    \"torch>=2.0.0\",\n    \"torchdata\",\n    \"soundfile>=0.12.1\",\n    \"transformers>=4.42.0\",  # Pins numpy < 2\n    \"zstandard\",\n    \"polars[timezone]>=0.20.0\",\n    \"decord==0.6.0\",\n]\n\n\nTESTS_REQUIRE.extend(VISION_REQUIRE)\nTESTS_REQUIRE.extend(AUDIO_REQUIRE)\n\nNUMPY2_INCOMPATIBLE_LIBRARIES = [\n    \"faiss-cpu\",\n    \"librosa\",  # librosa -> numba-0.60.0 requires numpy < 2.1 (see GH-7111)\n    \"tensorflow\",\n]\nTESTS_NUMPY2_REQUIRE = [\n    library for library in TESTS_REQUIRE if library.partition(\">\")[0] not in NUMPY2_INCOMPATIBLE_LIBRARIES\n]\n\nQUALITY_REQUIRE = [\"ruff>=0.3.0\"]\n\nDOCS_REQUIRE = [\n    # Might need to add doc-builder and some specific deps in the future\n    \"s3fs\",\n    # Following dependencies are required for the Python reference to be built properly\n    \"transformers\",\n    \"torch\",\n    \"tensorflow>=2.6.0\",\n]\n\nEXTRAS_REQUIRE = {\n    \"audio\": AUDIO_REQUIRE,\n    \"vision\": VISION_REQUIRE,\n    \"tensorflow\": [\n        \"tensorflow>=2.6.0\",\n    ],\n    \"tensorflow_gpu\": [\"tensorflow>=2.6.0\"],\n    \"torch\": [\"torch\"],\n    \"jax\": [\"jax>=0.3.14\", \"jaxlib>=0.3.14\"],\n    \"s3\": [\"s3fs\"],\n    \"streaming\": [],  # for backward compatibility\n    \"dev\": TESTS_REQUIRE + QUALITY_REQUIRE + DOCS_REQUIRE,\n    \"tests\": TESTS_REQUIRE,\n    \"tests_numpy2\": TESTS_NUMPY2_REQUIRE,\n    \"quality\": QUALITY_REQUIRE,\n    \"benchmarks\": BENCHMARKS_REQUIRE,\n    \"docs\": DOCS_REQUIRE,\n}\n\nsetup(\n    name=\"datasets\",\n    version=\"3.2.1.dev0\",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)\n    description=\"HuggingFace community-driven open-source library of datasets\",\n    long_description=open(\"README.md\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    author=\"HuggingFace Inc.\",\n    author_email=\"thomas@huggingface.co\",\n    url=\"https://github.com/huggingface/datasets\",\n    download_url=\"https://github.com/huggingface/datasets/tags\",\n    license=\"Apache 2.0\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    package_data={\n        \"datasets\": [\"py.typed\"],\n        \"datasets.utils.resources\": [\"*.json\", \"*.yaml\", \"*.tsv\"],\n    },\n    entry_points={\"console_scripts\": [\"datasets-cli=datasets.commands.datasets_cli:main\"]},\n    python_requires=\">=3.9.0\",\n    install_requires=REQUIRED_PKGS,\n    extras_require=EXTRAS_REQUIRE,\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    keywords=\"datasets machine learning datasets\",\n    zip_safe=False,  # Required for mypy to find the py.typed file\n)\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}