{
  "metadata": {
    "timestamp": 1736561215037,
    "page": 192,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "LlamaFamily/Llama-Chinese",
      "stars": 14336,
      "defaultBranch": "main",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 45.078125,
          "content": "<p align=\"left\">\n    <a href=\"README_EN.md\">English</a> ï½œ ä¸­æ–‡\n</p>\n\n<h1 align=\"center\">\n  Llamaä¸­æ–‡ç¤¾åŒº\n</h1>\n<p align=\"center\" width=\"100%\">\n  <img src=\"assets/llama.jpg\" alt=\"Llama\" style=\"width: 20%; display: block; margin: auto;\"></a>\n</p>\n<p align=\"center\">\n  <font face=\"é»‘ä½“\" color=orange size=\"6\"> Llama3ä½“éªŒå’Œå¾®è°ƒå·²å¼€æ”¾ï¼Œæœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹ </font>\n</p>\n\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/FlagAlpha\" target=\"_blank\">Hugging Face</a> â€¢ ğŸ¤– <a href=\"https://www.modelscope.cn/organization/FlagAlpha/\" target=\"_blank\">ModelScope</a> â€¢ âœ¡ï¸ <a href=\"https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat\" target=\"_blank\">WiseModel</a>\n</p> \n\n<p align=\"center\">\n  <a href=\"https://llama.family\">Llama3.1 åœ¨çº¿ä½“éªŒï¼ˆåŒ…å«Llama2ï¼‰ï¼šhttps://llama.family</a>\n</p>\n<p align=\"center\">\n  <a href=\"https://huggingface.co/FlagAlpha/Atom-7B-Chat\">åŸºäºLlamaçš„å¼€æºä¸­æ–‡é¢„è®­ç»ƒå¤§æ¨¡å‹Atom</a>\n</p>\n\n</br></br>\n\n\n## ğŸ—‚ï¸ ç›®å½•\n- [ğŸ“Œ Llamaä¸­æ–‡ç¤¾åŒº](#-llamaä¸­æ–‡ç¤¾åŒº)\n  * [ğŸ”¥ ç¤¾åŒºä»‹ç»ï¼šLlamaä¸­æ–‡ç¤¾åŒº](#-ç¤¾åŒºä»‹ç»llamaä¸­æ–‡ç¤¾åŒº)\n  * [ğŸ“¢ æœ€æ–°åŠ¨æ€](#-æœ€æ–°åŠ¨æ€)\n  * [ğŸ¤— æ¨¡å‹](#-æ¨¡å‹)\n    + [ğŸ¤— ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹Atom-7B](#-ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹atom)\n    + [ğŸ¤— Llama3å®˜æ–¹æ¨¡å‹](#llama3å®˜æ–¹æ¨¡å‹)\n    + [ğŸ¤— Llama3ä¸­æ–‡å¾®è°ƒæ¨¡å‹](#llama3ä¸­æ–‡å¾®è°ƒæ¨¡å‹)\n    + [ğŸ¤— Llama2å®˜æ–¹æ¨¡å‹](#llama2å®˜æ–¹æ¨¡å‹)\n    + [ğŸ¤— Llama2ä¸­æ–‡å¾®è°ƒæ¨¡å‹](#llama2ä¸­æ–‡å¾®è°ƒæ¨¡å‹)\n  * [ğŸŒŸ ç¤¾åŒºèµ„æº](#ç¤¾åŒºèµ„æº)\n\n\n- [ğŸ“Œ å¦‚ä½•ä½¿ç”¨Llamaæ¨¡å‹?](#-å¦‚ä½•ä½¿ç”¨llamaæ¨¡å‹)\n  - [å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨Anaconda](#å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨anaconda)\n  - [å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨Docker](#å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨docker)\n  - [å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨llama.cpp](#å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨llamacpp)\n  - [å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨gradio](#å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨gradio)\n  - [å¿«é€Ÿä¸Šæ‰‹-æ„å»ºAPIæœåŠ¡](#å¿«é€Ÿä¸Šæ‰‹-æ„å»ºapiæœåŠ¡)\n  - [å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨ollamaè¿è¡Œ](#å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨ollamaè¿è¡Œ)\n\n+ [ğŸ¤– æ¨¡å‹é¢„è®­ç»ƒ](#-æ¨¡å‹é¢„è®­ç»ƒ)\n+ [ğŸ’¡ æ¨¡å‹å¾®è°ƒ](#-æ¨¡å‹å¾®è°ƒ)\n  - [Step1: ç¯å¢ƒå‡†å¤‡](#step1-ç¯å¢ƒå‡†å¤‡)\n  - [Step2: æ•°æ®å‡†å¤‡](#step2-æ•°æ®å‡†å¤‡)\n  - [Step3: å¾®è°ƒè„šæœ¬](#step3-å¾®è°ƒè„šæœ¬)\n    * [LoRAå¾®è°ƒ](#loraå¾®è°ƒ)\n    * [å…¨é‡å‚æ•°å¾®è°ƒ](#å…¨é‡å‚æ•°å¾®è°ƒ)\n  - [Step4: åŠ è½½å¾®è°ƒæ¨¡å‹](#step4-åŠ è½½å¾®è°ƒæ¨¡å‹)\n    * [LoRAå¾®è°ƒ](#loraå¾®è°ƒ-1)\n    * [å…¨é‡å‚æ•°å¾®è°ƒ](#å…¨é‡å‚æ•°å¾®è°ƒ-1)\n+ [ğŸ„ æ¨¡å‹é‡åŒ–](#-æ¨¡å‹é‡åŒ–)\n\n+ [ğŸš€ éƒ¨ç½²åŠ é€Ÿ](#-éƒ¨ç½²åŠ é€Ÿ)\n  - [TensorRT-LLM](#tensorrt-llm)\n  - [vLLM](#vllm)  \n  - [JittorLLMs](#jittorllms)\n  - [lmdeploy](#lmdeploy)\n\n+ [ğŸ’ª å¤–å»¶èƒ½åŠ›](#-å¤–å»¶èƒ½åŠ›)\n  - [LangChain](#langchain)\n    \n* [ğŸ¥‡ æ¨¡å‹è¯„æµ‹](#-æ¨¡å‹è¯„æµ‹)\n  + [Llama2å’ŒLlama3å¯¹æ¯”è¯„æµ‹](#llama2å’Œllama3å¯¹æ¯”è¯„æµ‹)\n  + [Llama3æ¨¡å‹è¯„æµ‹](#llama3æ¨¡å‹è¯„æµ‹)\n  + [Llama2æ¨¡å‹è¯„æµ‹](#llama2æ¨¡å‹è¯„æµ‹)\n\n* [ğŸ“– å­¦ä¹ ä¸­å¿ƒ](#-å­¦ä¹ ä¸­å¿ƒ)\n    + [Llama3](#llama3)\n    + [Llama2](#llama2)\n      - [Metaå®˜æ–¹å¯¹äºLlama2çš„ä»‹ç»](#metaå®˜æ–¹å¯¹äºllama2çš„ä»‹ç»)\n    + [Llamaç›¸å…³è®ºæ–‡](#llamaç›¸å…³è®ºæ–‡)\n\n- [ğŸ“Œ å…¶å®ƒ](#-å…¶å®ƒ)\n  * [ğŸ‰ è‡´è°¢](#-è‡´è°¢)\n  * [ğŸ¤” é—®é¢˜åé¦ˆ](#-é—®é¢˜åé¦ˆ)\n\n## ğŸ“Œ Llamaä¸­æ–‡ç¤¾åŒº\n\n### ğŸ”¥ ç¤¾åŒºä»‹ç»ï¼šllamaä¸­æ–‡ç¤¾åŒº\n\næ¬¢è¿æ¥åˆ°Llamaä¸­æ–‡ç¤¾åŒºï¼æˆ‘ä»¬æ˜¯ä¸€ä¸ªä¸“æ³¨äºLlamaæ¨¡å‹åœ¨ä¸­æ–‡æ–¹é¢çš„ä¼˜åŒ–å’Œä¸Šå±‚å»ºè®¾çš„é«˜çº§æŠ€æœ¯ç¤¾åŒºã€‚\n**å·²ç»åŸºäºå¤§è§„æ¨¡ä¸­æ–‡æ•°æ®ï¼Œä»é¢„è®­ç»ƒå¼€å§‹å¯¹Llama2æ¨¡å‹è¿›è¡Œä¸­æ–‡èƒ½åŠ›çš„æŒç»­è¿­ä»£å‡çº§ã€Doneã€‘**ã€‚**æ­£åœ¨å¯¹Llama3æ¨¡å‹è¿›è¡Œä¸­æ–‡èƒ½åŠ›çš„æŒç»­è¿­ä»£å‡çº§ã€Doingã€‘**\næˆ‘ä»¬çƒ­å¿±æ¬¢è¿å¯¹å¤§æ¨¡å‹LLMå……æ»¡çƒ­æƒ…çš„å¼€å‘è€…å’Œç ”ç©¶è€…åŠ å…¥æˆ‘ä»¬çš„è¡Œåˆ—ã€‚\n\n<details>\n\n#### ä¸ºä»€ä¹ˆé€‰æ‹©Llamaä¸­æ–‡ç¤¾åŒºï¼Ÿ\nğŸš€ **é«˜çº§å·¥ç¨‹å¸ˆå›¢é˜Ÿæ”¯æŒ**ï¼šç¤¾åŒºæœ‰ä¸€æ‰¹ä¸“æ³¨ä¸ºå¤§å®¶æœåŠ¡çš„NLPé«˜çº§å·¥ç¨‹å¸ˆï¼Œæˆ‘ä»¬æœ‰ç€å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒå’Œä¸°å¯Œçš„ç»éªŒï¼Œä¸ºæ‚¨æä¾›ä¸“ä¸šçš„æŒ‡å¯¼å’Œå¸®åŠ©ã€‚\n\nğŸ¯ **ä¸­æ–‡ä¼˜åŒ–**ï¼šæˆ‘ä»¬è‡´åŠ›äºåœ¨Llamaæ¨¡å‹çš„ä¸­æ–‡å¤„ç†æ–¹é¢è¿›è¡Œä¼˜åŒ–ï¼Œæ¢ç´¢é€‚ç”¨äºä¸­æ–‡çš„æœ€ä½³å®è·µï¼Œä»¥æå‡å…¶æ€§èƒ½å’Œé€‚åº”æ€§ã€æ”¯æŒLlama2ã€Llama3ã€‘ã€‚\n\nğŸ’¡ **åˆ›æ–°äº¤æµ**ï¼šæˆ‘ä»¬æ‹¥æœ‰ä¸€æ”¯å¯Œæœ‰åˆ›é€ åŠ›å’Œç»éªŒçš„ç¤¾åŒºæˆå‘˜å›¢é˜Ÿï¼Œå®šæœŸç»„ç»‡çº¿ä¸Šæ´»åŠ¨ã€æŠ€æœ¯ç ”è®¨å’Œç»éªŒåˆ†äº«ï¼Œä¿ƒè¿›æˆå‘˜é—´çš„åˆ›æ–°äº¤æµã€‚\n\nğŸŒ **å…¨çƒè”ç»“**ï¼šæˆ‘ä»¬æ¬¢è¿æ¥è‡ªä¸–ç•Œå„åœ°çš„å¼€å‘è€…åŠ å…¥ç¤¾åŒºï¼Œæ„å»ºä¸€ä¸ªå¼€æ”¾ã€å¤šå…ƒåŒ–çš„å­¦ä¹ å’Œäº¤æµå¹³å°ã€‚\n\nğŸ¤ **å¼€æ”¾å…±äº«**ï¼šæˆ‘ä»¬é¼“åŠ±ç¤¾åŒºæˆå‘˜å¼€æºåˆ†äº«ä»£ç å’Œæ¨¡å‹ï¼Œæ¨åŠ¨åˆä½œå…±èµ¢ï¼Œå…±åŒä¿ƒè¿›ä¸­æ–‡NLPæŠ€æœ¯çš„å‘å±•ã€‚\n\n#### ç¤¾åŒºæ´»åŠ¨\nğŸ—“ï¸ **çº¿ä¸Šè®²åº§**ï¼šé‚€è¯·è¡Œä¸šå†…ä¸“å®¶è¿›è¡Œçº¿ä¸Šè®²åº§ï¼Œåˆ†äº«Llamaåœ¨ä¸­æ–‡NLPé¢†åŸŸçš„æœ€æ–°æŠ€æœ¯å’Œåº”ç”¨ï¼Œæ¢è®¨å‰æ²¿ç ”ç©¶æˆæœã€‚\n\nğŸ’» **é¡¹ç›®å±•ç¤º**ï¼šæˆå‘˜å¯å±•ç¤ºè‡ªå·±åœ¨Llamaä¸­æ–‡ä¼˜åŒ–æ–¹é¢çš„é¡¹ç›®æˆæœï¼Œè·å¾—åé¦ˆå’Œå»ºè®®ï¼Œä¿ƒè¿›é¡¹ç›®åä½œã€‚\n\nğŸ“š **å­¦ä¹ èµ„æº**ï¼šç¤¾åŒºç»´æŠ¤ä¸°å¯Œçš„å­¦ä¹ èµ„æ–™åº“ï¼ŒåŒ…æ‹¬æ•™ç¨‹ã€æ–‡æ¡£å’Œè®ºæ–‡è§£è¯»ï¼Œä¸ºæˆå‘˜æä¾›å…¨é¢çš„å­¦ä¹ æ”¯æŒã€‚\n\nğŸ“ **è®ºæ–‡è§£è¯»**ï¼šç¤¾åŒºæˆå‘˜å…±åŒè§£è¯»ä¸Llamaç›¸å…³çš„æœ€æ–°ç ”ç©¶è®ºæ–‡ï¼Œæ·±å…¥ç†è§£å‰æ²¿ç®—æ³•å’Œæ–¹æ³•ã€‚\n\nğŸ‰ **ä¸»é¢˜æ´»åŠ¨**ï¼šå®šæœŸä¸¾åŠå„ç±»ä¸»é¢˜æ´»åŠ¨ï¼ŒåŒ…æ‹¬æŒ‘æˆ˜èµ›ã€é»‘å®¢é©¬æ‹‰æ¾å’ŒæŠ€æœ¯æ²™é¾™ï¼Œè®©ç¤¾åŒºæˆå‘˜åœ¨è½»æ¾æ„‰å¿«çš„æ°›å›´ä¸­äº¤æµå’Œå­¦ä¹ ã€‚\n\nğŸŒŸ **å¥–åŠ±è®¡åˆ’**ï¼šæˆ‘ä»¬è®¾ç«‹å¥–åŠ±è®¡åˆ’ï¼Œå¯¹ç¤¾åŒºä¸­ç§¯æå‚ä¸ã€è´¡çŒ®ä¼˜ç§€çš„æˆå‘˜ç»™äºˆè£èª‰å’Œå¥–åŠ±ï¼Œæ¿€åŠ±æ›´å¤šä¼˜ç§€äººæ‰çš„åŠ å…¥ã€‚\n\nğŸ“ˆ **æŠ€æœ¯å’¨è¯¢**ï¼šæˆ‘ä»¬æä¾›æŠ€æœ¯å’¨è¯¢æœåŠ¡ï¼Œè§£ç­”æ‚¨åœ¨Llamaå¼€å‘å’Œä¼˜åŒ–è¿‡ç¨‹ä¸­é‡åˆ°çš„é—®é¢˜ï¼ŒåŠ©æ‚¨å¿«é€Ÿæ”»å…‹éš¾å…³ã€‚\n\nğŸš€ **é¡¹ç›®åˆä½œ**ï¼šé¼“åŠ±æˆå‘˜é—´çš„é¡¹ç›®åˆä½œï¼Œå…±åŒæ¢ç´¢Llamaåœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œæ‰“é€ åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚\n\n\n#### ç«‹å³åŠ å…¥æˆ‘ä»¬ï¼\nğŸ“š **æ„¿æ™¯**ï¼šæ— è®ºæ‚¨æ˜¯å¯¹Llamaå·²æœ‰ç ”ç©¶å’Œåº”ç”¨ç»éªŒçš„ä¸“ä¸šå¼€å‘è€…ï¼Œè¿˜æ˜¯å¯¹Llamaä¸­æ–‡ä¼˜åŒ–æ„Ÿå…´è¶£å¹¶å¸Œæœ›æ·±å…¥æ¢ç´¢çš„æ–°æ‰‹ï¼Œæˆ‘ä»¬éƒ½çƒ­åˆ‡æœŸå¾…æ‚¨çš„åŠ å…¥ã€‚åœ¨Llamaä¸­æ–‡ç¤¾åŒºï¼Œæ‚¨å°†æœ‰æœºä¼šä¸è¡Œä¸šå†…é¡¶å°–äººæ‰å…±åŒäº¤æµï¼Œæºæ‰‹æ¨åŠ¨ä¸­æ–‡NLPæŠ€æœ¯çš„è¿›æ­¥ï¼Œå¼€åˆ›æ›´åŠ ç¾å¥½çš„æŠ€æœ¯æœªæ¥ï¼\n\nğŸ”— **æ¸©é¦¨æç¤º**ï¼šæœ¬ç¤¾åŒºä¸ºä¸“ä¸šæŠ€æœ¯äº¤æµå¹³å°ï¼Œæˆ‘ä»¬çƒ­åˆ‡æœŸæœ›å¿—åŒé“åˆçš„å¼€å‘è€…å’Œç ”ç©¶è€…åŠ å…¥ã€‚è¯·éµå®ˆç¤¾åŒºå‡†åˆ™ï¼Œå…±åŒç»´æŠ¤ç§¯æå‘ä¸Šçš„å­¦ä¹ æ°›å›´ã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£å’Œæ”¯æŒï¼\n\n</details>\n\n### ğŸ“¢ æœ€æ–°åŠ¨æ€\n\nã€æœ€æ–°ã€‘2024å¹´07æœˆ24æ—¥ï¼šå¼€æºæœ€å¼º[Llama 3.1](https://llama.meta.com/docs/overview)æ¨¡å‹å‘å¸ƒï¼ŒåŒ…å«8Bã€70Bå’Œ405Bï¼\n\nã€æœ€æ–°ã€‘2024å¹´07æœˆ16æ—¥ï¼š[ç¤¾åŒºè®ºå›](https://forum.llamafamily.cn/)ä¸Šçº¿ï¼Œæœ‰å¤§æ¨¡å‹é—®é¢˜ï¼Œå°±æ‰¾Llamaä¸­æ–‡ç¤¾åŒºï¼\n\nã€æœ€æ–°ã€‘2024å¹´05æœˆ15æ—¥ï¼šæ”¯æŒollamaè¿è¡ŒLlama3-Chinese-8B-Instructã€Atom-7B-Chatï¼Œ[è¯¦ç»†ä½¿ç”¨æ–¹æ³•](https://github.com/LlamaFamily/Llama-Chinese?tab=readme-ov-file#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B-%E4%BD%BF%E7%94%A8ollama%E8%BF%90%E8%A1%8C)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´04æœˆ23æ—¥ï¼šç¤¾åŒºå¢åŠ äº†llama3 8Bä¸­æ–‡å¾®è°ƒæ¨¡å‹[Llama3-Chinese-8B-Instruct](https://github.com/LlamaFamily/Llama-Chinese?tab=readme-ov-file#llama3%E4%B8%AD%E6%96%87%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B)ä»¥åŠå¯¹åº”çš„[å…è´¹APIè°ƒç”¨](https://llama.family/docs/chat-completion-v1)ã€‚\n \nã€æœ€æ–°ã€‘2024å¹´04æœˆ19æ—¥ï¼šç¤¾åŒºå¢åŠ äº†llama3 8Bã€llama3 70B[åœ¨çº¿ä½“éªŒé“¾æ¥](https://llama.family/chat/#/)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´04æœˆ14æ—¥ï¼šç¤¾åŒºæ›´æ–°äº†å››ä¸ªä¸“å®¶è§’è‰²ï¼šå¿ƒç†å’¨è¯¢å¸ˆã€ç¾Šé©¼å¤¸å¤¸ ã€å¾‹å¸ˆã€åŒ»ç”Ÿã€‚é“¾æ¥ï¼š[è§’è‰²role](https://llama.family/tools/#/agent)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´04æœˆ10æ—¥ï¼šAtom-7B-Chat æ¨¡å‹å›ç­”å†…å®¹ç›¸è¾ƒä¹‹å‰æ›´ä¸ºä¸°å¯Œã€å¢å¼ºäº†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œå›ç­”ç¨³å®šæ€§ã€ä¼˜åŒ–äº†ppoçš„å¥–åŠ±æ¨¡å‹ã€‚ä¸‹è½½é“¾æ¥[modelscope](https://modelscope.cn/models/FlagAlpha/Atom-7B-Chat)ã€[Huggingface](https://huggingface.co/FlagAlpha/Atom-7B-Chat)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´04æœˆ01æ—¥ï¼šç¤¾åŒºä¸Šçº¿äº†Llamaä¸­æ–‡[åº”ç”¨å¹³å°](https://llama.family/store)ï¼›åŒæ—¶å¦‚æœä½ æœ‰ä¼˜ç§€çš„çš„åº”ç”¨éœ€è¦æ¨å¹¿å¯ä»¥å¡«å†™[ç”³è¯·è¡¨](https://atomecho.feishu.cn/share/base/form/shrcnFqpN71OmBoXDCT6y0TQgIc)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´03æœˆ08æ—¥ï¼šå¼€æ”¾äº†å…è´¹APIä¾›å¤§å®¶ä½¿ç”¨ï¼ŒåŒ…å«ï¼ˆAtom-1B,7B,13B 3ç§ä¸­æ–‡å¤§æ¨¡å‹ï¼‰[APIä½¿ç”¨é“¾æ¥](https://llama.family/docs/chat-completion-v1)\n\nã€æœ€æ–°ã€‘2024å¹´04æœˆ14æ—¥ï¼šç¤¾åŒºæ›´æ–°äº†å››ä¸ªä¸“å®¶è§’è‰²ï¼šå¿ƒç†å’¨è¯¢å¸ˆã€ç¾Šé©¼å¤¸å¤¸ ã€å¾‹å¸ˆã€åŒ»ç”Ÿã€‚é“¾æ¥ï¼š[è§’è‰²role](https://llama.family/tools/#/agent)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´04æœˆ10æ—¥ï¼šAtom-7B-Chat æ¨¡å‹å›ç­”å†…å®¹ç›¸è¾ƒä¹‹å‰æ›´ä¸ºä¸°å¯Œã€å¢å¼ºäº†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œå›ç­”ç¨³å®šæ€§ã€ä¼˜åŒ–äº†ppoçš„å¥–åŠ±æ¨¡å‹ã€‚ä¸‹è½½é“¾æ¥[modelscope](https://modelscope.cn/models/FlagAlpha/Atom-7B-Chat)ã€[Huggingface](https://huggingface.co/FlagAlpha/Atom-7B-Chat)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´04æœˆ01æ—¥ï¼šç¤¾åŒºä¸Šçº¿äº†Llamaä¸­æ–‡[åº”ç”¨å¹³å°](https://llama.family/store)ï¼›åŒæ—¶å¦‚æœä½ æœ‰ä¼˜ç§€çš„çš„åº”ç”¨éœ€è¦æ¨å¹¿å¯ä»¥å¡«å†™[ç”³è¯·è¡¨](https://atomecho.feishu.cn/share/base/form/shrcnFqpN71OmBoXDCT6y0TQgIc)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´03æœˆ28æ—¥ï¼š[ç¤¾åŒºå…è´¹å…¬å¼€è¯¾](https://mp.weixin.qq.com/s/CsturoU1pOX11CqVnZgu2A)ã€‚\n\nã€æœ€æ–°ã€‘2024å¹´03æœˆ08æ—¥ï¼šå¼€æ”¾äº†å…è´¹APIä¾›å¤§å®¶ä½¿ç”¨ï¼ŒåŒ…å«ï¼ˆAtom-1B,7B,13B 3ç§ä¸­æ–‡å¤§æ¨¡å‹ï¼‰[APIä½¿ç”¨é“¾æ¥](https://llama.family/docs/chat-completion-v1)\n\nã€æœ€æ–°ã€‘2023å¹´10æœˆ8æ—¥ï¼šæ–°å¢æ¸…åå¤§å­¦JittorLLMsçš„æ¨ç†åŠ é€ŸåŠŸèƒ½[JittorLLMs](#jittorllms)ï¼\n\n<details>\n\n- 2023å¹´9æœˆ12æ—¥ï¼šæ›´æ–°é¢„è®­ç»ƒç‰ˆæœ¬[Atom-7B](https://huggingface.co/FlagAlpha/Atom-7B)å’Œå¯¹è¯ç‰ˆæœ¬[Atom-7B-Chat](https://huggingface.co/FlagAlpha/Atom-7B-Chat)æ¨¡å‹å‚æ•°ï¼Œæœ€æ–°çš„ä¸­æ–‡é¢„è®­ç»ƒæ•°æ®é‡ä¸º2.7TB tokenï¼Œè®­ç»ƒè¿›ç¨‹è§[llama.family](https://llama.family/)ï¼\n\n- 2023å¹´9æœˆ2æ—¥ï¼šæ–°å¢æ¨¡å‹[é¢„è®­ç»ƒä»£ç ](#-æ¨¡å‹é¢„è®­ç»ƒ)å’Œ[å…¨é‡å‚æ•°å¾®è°ƒä»£ç ](#-æ¨¡å‹å¾®è°ƒ)ï¼\n  \n- 2023å¹´8æœˆ28æ—¥ï¼šå‘å¸ƒåŸºäºLlama2è¿›è¡Œä¸­æ–‡é¢„è®­ç»ƒçš„å¼€æºå¤§æ¨¡å‹[Atom-7B](https://huggingface.co/FlagAlpha/Atom-7B)ï¼Œå¹¶å°†æŒç»­æ›´æ–°ï¼Œè¯¦æƒ…å‚è€ƒ[ç¤¾åŒºå…¬ä¼—å·æ–‡ç« ](https://mp.weixin.qq.com/s/Bdx0JTVh1kgPn5ydYxIkEw)ï¼\n\n- 2023å¹´8æœˆ26æ—¥ï¼šæä¾›[FastAPI](#fastapiæ¥å£æ­å»º)æ¥å£æ­å»ºè„šæœ¬ï¼\n\n- 2023å¹´8æœˆ26æ—¥ï¼šæä¾›å°†MetaåŸå§‹æ¨¡å‹å‚æ•°è½¬æ¢ä¸ºå…¼å®¹Hugging Faceçš„[æ ¼å¼è½¬åŒ–è„šæœ¬](https://github.com/LlamaFamily/Llama-Chinese/blob/main/scripts/convert2hf/README.md)ï¼\n\n- 2023å¹´8æœˆ26æ—¥ï¼šæ–°å¢[Code Llama](#-ä»£ç æ¨¡å‹)æ¨¡å‹ï¼\n\n- 2023å¹´8æœˆ15æ—¥ï¼šæ–°å¢[PEFTåŠ è½½å¾®è°ƒæ¨¡å‹å‚æ•°](#åŠ è½½å¾®è°ƒæ¨¡å‹)çš„ä»£ç ç¤ºä¾‹ï¼\n\n- 2023å¹´8æœˆ14æ—¥ï¼š[å¤§æ¨¡å‹æ•°æ®å…±äº«è®­ç»ƒå¹³å°](https://llama.family)ä¸Šçº¿ï¼Œæ²¡æœ‰ç®—åŠ›ä¹Ÿèƒ½å‚ä¸å¤§æ¨¡å‹è®­ç»ƒï¼Œç¤¾åŒºæ¯ä½æˆå‘˜è´¡çŒ®çš„æ•°æ®éƒ½å°†å†³å®šæ¨¡å‹èƒ½åŠ›çš„æœªæ¥èµ°å‘ï¼\n\n- 2023å¹´8æœˆ3æ—¥ï¼šæ–°å¢FasterTransformerå’ŒvLLMçš„GPU[æ¨ç†åŠ é€Ÿ](#-æ¨ç†åŠ é€Ÿ)æ”¯æŒï¼\n\n- 2023å¹´7æœˆ31æ—¥ï¼šã€é‡ç£…ã€‘å›½å†…é¦–ä¸ªçœŸæ­£æ„ä¹‰ä¸Šçš„Llama2ä¸­æ–‡å¤§æ¨¡å‹å‘å¸ƒï¼è¯¦æƒ…å‚è§[ç¤¾åŒºå…¬ä¼—å·æ–‡ç« ](https://mp.weixin.qq.com/s/lExUU7z_MvgJ7tzQPF8tUQ)\n\n- 2023å¹´7æœˆ28æ—¥ï¼šé€šè¿‡[Dockeréƒ¨ç½²](#dockeréƒ¨ç½²é—®ç­”æ¥å£)é—®ç­”æ¥å£ï¼\n\n- 2023å¹´7æœˆ27æ—¥ï¼šæ–°å¢[LangChain](#langchain)æ”¯æŒï¼\n\n- 2023å¹´7æœˆ26æ—¥ï¼šæ–°å¢Llama2-13Bä¸­æ–‡å¾®è°ƒå‚æ•°çš„[4bité‡åŒ–å‹ç¼©ç‰ˆæœ¬](#-æ¨¡å‹é‡åŒ–)ï¼\n\n- 2023å¹´7æœˆ25æ—¥ï¼šç¤¾åŒºå¾®ä¿¡å…¬ä¼—å·â€œLlamaä¸­æ–‡ç¤¾åŒºâ€æ¬¢è¿å¤§å®¶å…³æ³¨ï¼Œè·å–æœ€æ–°åˆ†äº«å’ŒåŠ¨æ€ï¼\n\n- 2023å¹´7æœˆ24æ—¥ï¼š[FlagAlpha](https://huggingface.co/FlagAlpha)æ–°å¢Llama2-13Bä¸­æ–‡å¾®è°ƒå‚æ•°ï¼\n\n- 2023å¹´7æœˆ24æ—¥ï¼š[llama.family](https://llama.family/)æ–°å¢Llama2-70Båœ¨çº¿ä½“éªŒï¼\n\n- 2023å¹´7æœˆ23æ—¥ï¼šLlama2ä¸­æ–‡å¾®è°ƒå‚æ•°å‘å¸ƒè‡³Hugging Faceä»“åº“[FlagAlpha](https://huggingface.co/FlagAlpha)ï¼\n\n- 2023å¹´7æœˆ22æ—¥ï¼šLlama2åœ¨çº¿ä½“éªŒé“¾æ¥[llama.family](https://llama.family/)ä¸Šçº¿ï¼ŒåŒæ—¶åŒ…å«MetaåŸç‰ˆå’Œä¸­æ–‡å¾®è°ƒç‰ˆæœ¬ï¼\n\n- 2023å¹´7æœˆ21æ—¥ï¼šè¯„æµ‹äº†MetaåŸå§‹ç‰ˆLlama2 Chatæ¨¡å‹çš„[ä¸­æ–‡é—®ç­”èƒ½åŠ›](#-æ¨¡å‹è¯„æµ‹)ï¼\n\n- 2023å¹´7æœˆ21æ—¥ï¼šæ–°å¢Llama2æ¨¡å‹çš„Hugging Faceç‰ˆæœ¬å›½å†…ä¸‹è½½åœ°å€ï¼\n\n- 2023å¹´7æœˆ20æ—¥ï¼šæ–°å¢[é£ä¹¦çŸ¥è¯†åº“æ–‡æ¡£](https://chinesellama.feishu.cn/wiki/space/7257824476874768388?ccm_open_type=lark_wiki_spaceLink)ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·å…±å»ºï¼\n\n- 2023å¹´7æœˆ20æ—¥ï¼šå›½å†…Llama2æœ€æ–°ä¸‹è½½åœ°å€ä¸Šçº¿ï¼\n\n- 2023å¹´7æœˆ19æ—¥ï¼šæ­£å¼å¯åŠ¨Llama2æ¨¡å‹çš„ä¸­æ–‡é¢„è®­ç»ƒï¼Œå…³æ³¨æˆ‘ä»¬è·å–å®æ—¶åŠ¨æ€ï¼\n\n- 2023å¹´7æœˆ19æ—¥ï¼šLlama2å›½å†…ä¸‹è½½åœ°å€æ­£åœ¨å¯åŠ¨ï¼Œæ•¬è¯·æœŸå¾…ï¼\n\n- 2023å¹´7æœˆ19æ—¥ï¼šå¼€å¯Llama2ä¸­æ–‡ç¤¾åŒºï¼Œæ¬¢è¿å¤§å®¶åŠ å…¥ï¼\n\n</details>\n\n\n### ğŸ¤— æ¨¡å‹\n\n#### ğŸ”µ ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹Atom\n\n**åŸå­å¤§æ¨¡å‹Atom**ç”±Llamaä¸­æ–‡ç¤¾åŒºå’ŒåŸå­å›å£°è”åˆæ‰“é€ ã€‚\n\n|  ç±»åˆ«  | æ¨¡å‹åç§°        | ğŸ¤—æ¨¡å‹åŠ è½½åç§°                  | ä¸‹è½½åœ°å€                                                     |\n| --------------- | --------------- | ------------------------------ | ------------------------------------------------------------ |\n|  é¢„è®­ç»ƒ  | Atom-7B  | FlagAlpha/Atom-7B  | [HuggingFace](https://huggingface.co/FlagAlpha/Atom-7B) \\| [ModelScope](https://modelscope.cn/models/FlagAlpha/Atom-7B) \\| [WiseModel](https://wisemodel.cn/models/FlagAlpha/Atom-7B) |\n|  Chat  | Atom-7B-Chat  | FlagAlpha/Atom-7B-Chat  | [HuggingFace](https://huggingface.co/FlagAlpha/Atom-7B-Chat) \\| [ModelScope](https://modelscope.cn/models/FlagAlpha/Atom-7B-Chat) \\| [WiseModel](https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat)|\n\nAtomç³»åˆ—æ¨¡å‹åŒ…å«Atom-13Bã€Atom-7Bå’ŒAtom-1Bï¼ŒåŸºäºLlama2åšäº†ä¸­æ–‡èƒ½åŠ›çš„æŒç»­ä¼˜åŒ–ã€‚Atom-7Bå’ŒAtom-7B-Chatç›®å‰å·²å®Œå…¨å¼€æºï¼Œæ”¯æŒå•†ç”¨ï¼Œå¯åœ¨[Hugging Face](https://huggingface.co/FlagAlpha)ä»“åº“è·å–æ¨¡å‹ï¼Œè¯¦æƒ…è§[Atom-7Bä¸‹è½½](#åŸºäºllama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹atom)ã€‚Atomå¤§æ¨¡å‹é’ˆå¯¹ä¸­æ–‡åšäº†ä»¥ä¸‹ä¼˜åŒ–ï¼š\n\n- å¤§è§„æ¨¡çš„ä¸­æ–‡æ•°æ®é¢„è®­ç»ƒ\n\nåŸå­å¤§æ¨¡å‹Atomåœ¨Llama2çš„åŸºç¡€ä¸Šï¼Œé‡‡ç”¨å¤§è§„æ¨¡çš„ä¸­æ–‡æ•°æ®è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ŒåŒ…å«ç™¾ç§‘ã€ä¹¦ç±ã€åšå®¢ã€æ–°é—»ã€å…¬å‘Šã€å°è¯´ã€é‡‘èæ•°æ®ã€æ³•å¾‹æ•°æ®ã€åŒ»ç–—æ•°æ®ã€ä»£ç æ•°æ®ã€ä¸“ä¸šè®ºæ–‡æ•°æ®ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ç«èµ›æ•°æ®é›†ç­‰ï¼Œè¯¦è§[ğŸ“ æ•°æ®æ¥æº](#-æ•°æ®æ¥æº)ã€‚\n\nåŒæ—¶å¯¹åºå¤§çš„æ•°æ®è¿›è¡Œäº†è¿‡æ»¤ã€æ‰“åˆ†ã€å»é‡ï¼Œç­›é€‰å‡ºè¶…è¿‡1T tokençš„é«˜è´¨é‡ä¸­æ–‡æ•°æ®ï¼ŒæŒç»­ä¸æ–­åŠ å…¥è®­ç»ƒè¿­ä»£ä¸­ã€‚\n\n- æ›´é«˜æ•ˆçš„ä¸­æ–‡è¯è¡¨\nä¸ºäº†æé«˜ä¸­æ–‡æ–‡æœ¬å¤„ç†çš„æ•ˆç‡ï¼Œæˆ‘ä»¬é’ˆå¯¹Llama2æ¨¡å‹çš„è¯è¡¨è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŸºäºæ•°ç™¾Gçš„ä¸­æ–‡æ–‡æœ¬ï¼Œåœ¨è¯¥æ¨¡å‹è¯è¡¨çš„åŸºç¡€ä¸Šæ‰©å±•è¯åº“è‡³65,000ä¸ªå•è¯ã€‚ç»è¿‡æµ‹è¯•ï¼Œæˆ‘ä»¬çš„æ”¹è¿›ä½¿å¾—ä¸­æ–‡ç¼–ç /è§£ç é€Ÿåº¦æé«˜äº†çº¦350ï¼…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ‰©å¤§äº†ä¸­æ–‡å­—ç¬¦é›†çš„è¦†ç›–èŒƒå›´ï¼ŒåŒ…æ‹¬æ‰€æœ‰emojiç¬¦å·ğŸ˜Šã€‚è¿™ä½¿å¾—ç”Ÿæˆå¸¦æœ‰è¡¨æƒ…ç¬¦å·çš„æ–‡ç« æ›´åŠ é«˜æ•ˆã€‚\n\n- è‡ªé€‚åº”ä¸Šä¸‹æ–‡æ‰©å±•\nAtomå¤§æ¨¡å‹é»˜è®¤æ”¯æŒ4Kä¸Šä¸‹æ–‡ï¼Œåˆ©ç”¨ä½ç½®æ’å€¼PIå’ŒNeural Tangent Kernel ï¼ˆNTKï¼‰æ–¹æ³•ï¼Œç»è¿‡å¾®è°ƒå¯ä»¥å°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å¢åˆ°32Kã€‚\n\n- ğŸ“ ä¸­æ–‡æ•°æ®\n\næˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ•°æ®æ¥ä¼˜åŒ–Llama2çš„ä¸­æ–‡èƒ½åŠ›:\n\n| ç±»å‹                                                       | æè¿°                                                         |\n| ---------------------------------------------------------- | ------------------------------------------------------------ |\n| ç½‘ç»œæ•°æ®                                                   | äº’è”ç½‘ä¸Šå…¬å¼€çš„ç½‘ç»œæ•°æ®ï¼ŒæŒ‘é€‰å‡ºå»é‡åçš„é«˜è´¨é‡ä¸­æ–‡æ•°æ®ï¼Œæ¶‰åŠåˆ°ç™¾ç§‘ã€ä¹¦ç±ã€åšå®¢ã€æ–°é—»ã€å…¬å‘Šã€å°è¯´ç­‰é«˜è´¨é‡é•¿æ–‡æœ¬æ•°æ®ã€‚ |\n| [Wikipedia](https://github.com/goldsmith/Wikipedia)        | ä¸­æ–‡Wikipediaçš„æ•°æ®                                          |\n| [æ‚Ÿé“](https://github.com/BAAI-WuDao/Model)                | ä¸­æ–‡æ‚Ÿé“å¼€æºçš„200Gæ•°æ®                                       |\n| [Clue](https://github.com/CLUEbenchmark/CLUEDatasetSearch) | Clueå¼€æ”¾çš„ä¸­æ–‡é¢„è®­ç»ƒæ•°æ®ï¼Œè¿›è¡Œæ¸…æ´—åçš„é«˜è´¨é‡ä¸­æ–‡é•¿æ–‡æœ¬æ•°æ®   |\n| ç«èµ›æ•°æ®é›†                                                 | è¿‘å¹´æ¥ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å¤šä»»åŠ¡ç«èµ›æ•°æ®é›†ï¼Œçº¦150ä¸ª              |\n| [MNBVC](https://github.com/esbatmop/MNBVC)                 | MNBVC ä¸­æ¸…æ´—å‡ºæ¥çš„éƒ¨åˆ†æ•°æ®é›†\n\nç¤¾åŒºæä¾›é¢„è®­ç»ƒç‰ˆæœ¬Atom-7Bå’ŒåŸºäºAtom-7Bè¿›è¡Œå¯¹è¯å¾®è°ƒçš„æ¨¡å‹å‚æ•°ä¾›å¼€æ”¾ä¸‹è½½ï¼Œå…³äºæ¨¡å‹çš„è¿›å±•è¯¦è§ç¤¾åŒºå®˜ç½‘[llama.family](https://llama.family)ã€‚\n\n#### Llama3å®˜æ–¹æ¨¡å‹\n\n|  ç±»åˆ«  | æ¨¡å‹åç§°   | ğŸ¤—æ¨¡å‹åŠ è½½åç§°             | ä¸‹è½½åœ°å€                                                     |\n|  ----------  | ---------- | ------------------------- | --------------------- |\n|  é¢„è®­ç»ƒ  | Llama3-8B  | meta-llama/Meta-Llama-3-8B  | [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B) \\| [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1gBZ7wEn3gC8VRok0Onh9BQ?pwd=8frq) |\n|  é¢„è®­ç»ƒ  | Llama3-70B | meta-llama/Meta-Llama-3-70B | [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-7B) \\| [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1gBZ7wEn3gC8VRok0Onh9BQ?pwd=8frq) |\n|  å¯¹è¯æ¨¡å‹  | Llama3-8B-Chat  | meta-llama/Meta-Llama-3-8B-Instruct  | [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) \\| [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1gBZ7wEn3gC8VRok0Onh9BQ?pwd=8frq) |\n|  å¯¹è¯æ¨¡å‹  | Llama3-70B-Chat  | meta-llama/Meta-Llama-3-70B-Instruct  | [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) \\| [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1gBZ7wEn3gC8VRok0Onh9BQ?pwd=8frq) |\n\n#### Llama3ä¸­æ–‡å¾®è°ƒæ¨¡å‹\n\n|  ç±»åˆ«  | æ¨¡å‹åç§°   | ğŸ¤—æ¨¡å‹åŠ è½½åç§°             | ä¸‹è½½åœ°å€                                                     |\n|  ----------  | ---------- | ------------------------- | --------------------- |\n|  å¯¹è¯æ¨¡å‹  | Llama3-Chinese-8B-Instruct  | FlagAlpha/Llama3-Chinese-8B-Instruct  | [HuggingFace](https://huggingface.co/FlagAlpha/Llama3-Chinese-8B-Instruct) \\| [modelscope](https://modelscope.cn/models/FlagAlpha/Llama3-Chinese-8B-Instruct/summary) \\| [wisemodel](https://wisemodel.cn/models/FlagAlpha/Llama3-Chinese-8B-Instruct/file) |\n\n\n#### Llama2å®˜æ–¹æ¨¡å‹\n\n<details>\n\n|  ç±»åˆ«  | æ¨¡å‹åç§°   | ğŸ¤—æ¨¡å‹åŠ è½½åç§°             | ä¸‹è½½åœ°å€                                                     |\n|  ----------  | ---------- | ------------------------- | --------------------- |\n|  é¢„è®­ç»ƒ  | Llama2-7B  | meta-llama/Llama-2-7b-hf  | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-7b-hf) \\| [è¿…é›·ç½‘ç›˜](https://pan.xunlei.com/s/VN_t0dUikZqOwt-5DZWHuMvqA1?pwd=66ep) |\n|  é¢„è®­ç»ƒ  | Llama2-13B | meta-llama/Llama-2-13b-hf | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-13b-hf) \\| [è¿…é›·ç½‘ç›˜](https://pan.xunlei.com/s/VN_yT_9G8xNOz0SDWQ7Mb_GZA1?pwd=yvgf) |\n|  é¢„è®­ç»ƒ  | Llama2-70B | meta-llama/Llama-2-70b-hf | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-70b-hf) |\n|  Chat  | Llama2-7B-Chat  | meta-llama/Llama-2-7b-chat-hf  | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) \\| [è¿…é›·ç½‘ç›˜](https://pan.xunlei.com/s/VN_oaV4BpKFgKLto4KgOhBcaA1?pwd=ufir) |\n|  Chat  | Llama2-13B-Chat | meta-llama/Llama-2-13b-chat-hf | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) \\| [è¿…é›·ç½‘ç›˜](https://pan.xunlei.com/s/VN_yA-9G34NGL9B79b3OQZZGA1?pwd=xqrg) |\n|  Chat  | Llama2-70B-Chat | meta-llama/Llama-2-70b-chat-hf | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) \\| [è¿…é›·ç½‘ç›˜](https://pan.xunlei.com/s/VNa_vCGzCy3h3N7oeFXs2W1hA1?pwd=uhxh#) |\n| Code  | CodeLlama-7b    |   meta-llama/Llama-2-70b-chat-hf              | [è¿…é›·ç½‘ç›˜](https://pan.baidu.com/s/1cIPzdNywWLvQI7_2QanOEQ?pwd=zfwi) |\n| Code  | CodeLlama-7b-Python    |   meta-llama/Llama-2-70b-chat-hf              | [è¿…é›·ç½‘ç›˜](https://pan.baidu.com/s/1liY8klGoDagYbpw-g-oFag?pwd=i952) |\n| Code  | CodeLlama-7b-Instruct    |   meta-llama/Llama-2-70b-chat-hf              | [è¿…é›·ç½‘ç›˜](https://pan.baidu.com/s/108o9_DT2E_vfSGtOnDCQVw?pwd=zkt9) |\n| Code  | CodeLlama-13b    |   meta-llama/Llama-2-70b-chat-hf              | [è¿…é›·ç½‘ç›˜](https://pan.baidu.com/s/1lLaeHv0XEBv0iiZzI1dpnw?pwd=qn99) |\n| Code  | CodeLlama-13b-Python    |   meta-llama/Llama-2-70b-chat-hf              | [è¿…é›·ç½‘ç›˜](https://pan.baidu.com/s/1OLVfvZS_oqL3oqMKwsI87w?pwd=a78k) |\n| Code  | CodeLlama-13b-Instruct    |   meta-llama/Llama-2-70b-chat-hf              | [è¿…é›·ç½‘ç›˜](https://pan.baidu.com/s/1HyxJl4w8wElgkZRh2ATrXQ?pwd=seg6) |\n| Code  | CodeLlama-34b    |   meta-llama/Llama-2-70b-chat-hf              | [è¿…é›·ç½‘ç›˜](https://pan.baidu.com/s/1vEw0pFgIkctPUN4_5_6pIQ?pwd=q8eu) |\n\nMetaå®˜æ–¹åœ¨2023å¹´8æœˆ24æ—¥å‘å¸ƒäº†Code Llamaï¼ŒåŸºäºä»£ç æ•°æ®å¯¹Llama2è¿›è¡Œäº†å¾®è°ƒï¼Œæä¾›ä¸‰ä¸ªä¸åŒåŠŸèƒ½çš„ç‰ˆæœ¬ï¼šåŸºç¡€æ¨¡å‹ï¼ˆCode Llamaï¼‰ã€Pythonä¸“ç”¨æ¨¡å‹ï¼ˆCode Llama - Pythonï¼‰å’ŒæŒ‡ä»¤è·Ÿéšæ¨¡å‹ï¼ˆCode Llama - Instructï¼‰ï¼ŒåŒ…å«7Bã€13Bã€34Bä¸‰ç§ä¸åŒå‚æ•°è§„æ¨¡ã€‚ä¸åŒæ¨¡å‹èƒ½åŠ›åŒºåˆ«å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n|  æ¨¡å‹ç±»åˆ«          |        æ¨¡å‹åç§°         | ä»£ç ç»­å†™ | ä»£ç å¡«å…… | æŒ‡ä»¤ç¼–ç¨‹ |\n|-----------------------|------------------------|------|------|------|\n| Code Llama            | CodeLlama-7b           | âœ…    | âœ…    | âŒ    |\n|                       | CodeLlama-13b          | âœ…    | âœ…    | âŒ    |\n|                       | CodeLlama-34b          | âœ…    | âŒ    | âŒ    |\n| Code Llama - Python   | CodeLlama-7b-Python    | âœ…    | âŒ    | âŒ    |\n|                       | CodeLlama-13b-Python   | âœ…    | âŒ    | âŒ    |\n|                       | CodeLlama-34b-Python   | âœ…    | âŒ    | âŒ    |\n| Code Llama - Instruct | CodeLlama-7b-Instruct  | âŒ    | âœ…    | âœ…    |\n|                       | CodeLlama-13b-Instruct | âŒ    | âœ…    | âœ…    |\n|                       | CodeLlama-34b-Instruct | âŒ    | âŒ    | âœ…    |\n\nå…³äºCode Llamaçš„è¯¦ç»†ä¿¡æ¯å¯ä»¥å‚è€ƒå®˜æ–¹Githubä»“åº“[codellama](https://github.com/facebookresearch/codellama)ã€‚\n\n</details>\n\n#### Llama2ä¸­æ–‡å¾®è°ƒæ¨¡å‹\n\næˆ‘ä»¬åŸºäºä¸­æ–‡æŒ‡ä»¤æ•°æ®é›†å¯¹Llama2-Chatæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä½¿å¾—Llama2æ¨¡å‹æœ‰ç€æ›´å¼ºçš„ä¸­æ–‡å¯¹è¯èƒ½åŠ›ã€‚LoRAå‚æ•°ä»¥åŠä¸åŸºç¡€æ¨¡å‹åˆå¹¶çš„å‚æ•°å‡å·²ä¸Šä¼ è‡³[Hugging Face](https://huggingface.co/FlagAlpha)ï¼Œç›®å‰åŒ…å«7Bå’Œ13Bçš„æ¨¡å‹ã€‚\n\n|  ç±»åˆ«  | æ¨¡å‹åç§°   | ğŸ¤—æ¨¡å‹åŠ è½½åç§°             | åŸºç¡€æ¨¡å‹ç‰ˆæœ¬ |    ä¸‹è½½åœ°å€                                                     |\n|  ----------  | ---------- | ------------- |  ----------------- | ------------------- |\n|  åˆå¹¶å‚æ•° | Llama2-Chinese-7b-Chat | FlagAlpha/Llama2-Chinese-7b-Chat  |    meta-llama/Llama-2-7b-chat-hf       |[HuggingFace](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat)  |\n|  åˆå¹¶å‚æ•° | Llama2-Chinese-13b-Chat | FlagAlpha/Llama2-Chinese-13b-Chat|     meta-llama/Llama-2-13b-chat-hf     |[HuggingFace](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat) |\n|  LoRAå‚æ•° | Llama2-Chinese-7b-Chat-LoRA  | FlagAlpha/Llama2-Chinese-7b-Chat-LoRA  |     meta-llama/Llama-2-7b-chat-hf      |[HuggingFace](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat-LoRA) |\n|  LoRAå‚æ•° | Llama2-Chinese-13b-Chat-LoRA | FlagAlpha/Llama2-Chinese-13b-Chat-LoRA |     meta-llama/Llama-2-13b-chat-hf     |[HuggingFace](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-LoRA) |\n\n\n### ç¤¾åŒºèµ„æº\nç¤¾åŒºèµ„æºçš„ä¸°å¯Œæ€§æ˜¯ç¤¾åŒºå‘å±•çš„é‡è¦ä¿éšœï¼Œå®ƒæ¶µç›–äº†å„ç§æ–¹é¢ï¼Œå…¶ä¸­åŒ…æ‹¬ä½†ä¸é™äºä»¥ä¸‹å››ä¸ªæ–¹é¢ï¼šç®—åŠ›ã€æ•°æ®ã€è®ºå›å’Œåº”ç”¨ã€‚åœ¨è¿™äº›æ–¹é¢çš„ç§¯æå‘å±•ä¸å……åˆ†åˆ©ç”¨ï¼Œå°†ä¸ºç¤¾åŒºæˆå‘˜æä¾›æ›´å¤šçš„æœºä¼šå’Œæ”¯æŒï¼Œæ¨åŠ¨æ•´ä¸ªç¤¾åŒºå‘ç€æ›´åŠ ç¹è£çš„æ–¹å‘å‘å±•ã€‚æ›´å¤šçš„å†…å®¹è¯·çœ‹[llama.family](https://llama.family/)\n\n<details>\n\n#### ğŸ’» ç®—åŠ›\n- æä¾›ä½äºå¸‚åœºä»·æ ¼çš„ç®—åŠ›èµ„æºï¼Œå¯ç”¨äºå„ç±»è®¡ç®—ä»»åŠ¡ï¼Œå¦‚æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€æ¨ç†ç­‰ã€‚\n- ä¸ºç¤¾åŒºæˆå‘˜æä¾›ä¸“å±çš„åœ¨çº¿æ¨ç†æœåŠ¡ï¼Œè®©ç”¨æˆ·å¯ä»¥å¿«é€Ÿæœ‰æ•ˆåœ°å¯¹æ¨¡å‹è¿›è¡Œæ¨ç†æ“ä½œã€‚\n- æä¾›ä¸€é”®åœ¨çº¿å¾®è°ƒæœåŠ¡ï¼Œä½¿ç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®ã€‚\n\n#### ğŸ“Š æ•°æ®\n- å¼€æ”¾ä¸°å¯Œçš„è®­ç»ƒæ•°æ®èµ„æºï¼Œè¦†ç›–å¤šä¸ªé¢†åŸŸå’Œè¡Œä¸šï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›å……è¶³çš„æ•°æ®æ”¯æŒã€‚\n- æä¾›é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä»¥æ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶æ”¯æŒæ•°æ®å…±äº«å’Œäº¤æµï¼Œä¿ƒè¿›æ•°æ®èµ„æºçš„å……åˆ†åˆ©ç”¨ã€‚\n\n#### ğŸ’¬ è®ºå›\n- ç¤¾åŒºè®ºå›ä¸ºç¤¾åŒºæˆå‘˜æä¾›äº†ä¸€ä¸ªåœ¨çº¿äº¤æµå’Œè®¨è®ºæŠ€æœ¯é—®é¢˜çš„å¹³å°ã€‚\n- åœ¨è®ºå›ä¸Šï¼Œç”¨æˆ·å¯ä»¥åˆ†äº«ç»éªŒã€æå‡ºé—®é¢˜ã€è§£ç­”ç–‘æƒ‘ï¼Œä¿ƒè¿›æŠ€æœ¯äº¤æµå’Œåˆä½œã€‚\n- è®ºå›è¿˜å¯ä»¥å®šæœŸä¸¾åŠçº¿ä¸Šæ´»åŠ¨ã€ç ”è®¨ä¼šç­‰ï¼Œå¢è¿›ç¤¾åŒºæˆå‘˜ä¹‹é—´çš„è”ç³»å’Œäº†è§£ã€‚\n\n#### ğŸ“± åº”ç”¨\n- å…è´¹æä¾›åº”ç”¨æ¨å¹¿å±•ç¤ºä½ï¼Œè®©å¼€å‘è€…å¯ä»¥å°†ä»–ä»¬çš„åº”ç”¨å……åˆ†å±•ç¤ºç»™ç¤¾åŒºæˆå‘˜ã€‚\n- æä¾›æ¨å¹¿çš„å¸®åŠ©ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå®£ä¼ æ¨å¹¿ã€ç”¨æˆ·å¼•å¯¼ç­‰æœåŠ¡ï¼Œå¸®åŠ©åº”ç”¨è·å¾—æ›´å¤šçš„æ›å…‰å’Œç”¨æˆ·ã€‚\n- é€šè¿‡ç¤¾åŒºå¹³å°ï¼Œä¸ºä¼˜ç§€çš„åº”ç”¨æä¾›åˆä½œæœºä¼šï¼Œä¿ƒè¿›åº”ç”¨å¼€å‘è€…ä¹‹é—´çš„åˆä½œå’Œäº¤æµï¼Œå…±åŒæ¨åŠ¨åº”ç”¨çš„å‘å±•å’Œå£®å¤§ã€‚\n\n</details>\n\n## ğŸ“Œ å¦‚ä½•ä½¿ç”¨Llamaæ¨¡å‹?\n\n\nä½ å¯ä»¥é€‰æ‹©ä¸‹é¢çš„å¿«é€Ÿä¸Šæ‰‹çš„ä»»ä¸€ç§æ–¹å¼ï¼Œå¼€å§‹ä½¿ç”¨ Llama ç³»åˆ—æ¨¡å‹ã€‚æ¨èä½¿ç”¨[ä¸­æ–‡é¢„è®­ç»ƒå¯¹è¯æ¨¡å‹](#llama2ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹atom-7b)è¿›è¡Œä½¿ç”¨ï¼Œå¯¹ä¸­æ–‡çš„æ•ˆæœæ”¯æŒæ›´å¥½ã€‚\n\n\n### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨Anaconda\n\nç¬¬ 0 æ­¥ï¼šå‰ææ¡ä»¶\n- ç¡®ä¿å®‰è£…äº† Python 3.10 ä»¥ä¸Šç‰ˆæœ¬ã€‚\n\nç¬¬ 1 æ­¥ï¼šå‡†å¤‡ç¯å¢ƒ\n\nå¦‚éœ€è®¾ç½®ç¯å¢ƒï¼Œå®‰è£…æ‰€éœ€è¦çš„è½¯ä»¶åŒ…ï¼Œè¿è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚\n```bash\ngit clone https://github.com/LlamaFamily/Llama-Chinese.git\ncd Llama-Chinese\npip install -r requirements.txt\n```\n\nç¬¬ 2 æ­¥ï¼šä¸‹è½½æ¨¡å‹\n\nä½ å¯ä»¥ä»ä»¥ä¸‹æ¥æºä¸‹è½½Atom-7B-Chatæ¨¡å‹ã€‚\n- [HuggingFace](https://huggingface.co/FlagAlpha)\n- [ModelScope](https://modelscope.cn/organization/FlagAlpha)\n- [WiseModel](https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat)\n\nç¬¬ 3 æ­¥ï¼šè¿›è¡Œæ¨ç†\n\nä½¿ç”¨Atom-7B-Chatæ¨¡å‹è¿›è¡Œæ¨ç†\nåˆ›å»ºä¸€ä¸ªåä¸º quick_start.py çš„æ–‡ä»¶ï¼Œå¹¶å°†ä»¥ä¸‹å†…å®¹å¤åˆ¶åˆ°è¯¥æ–‡ä»¶ä¸­ã€‚\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice_map = \"cuda:0\" if torch.cuda.is_available() else \"auto\"\nmodel = AutoModelForCausalLM.from_pretrained('FlagAlpha/Atom-7B-Chat',device_map=device_map,torch_dtype=torch.float16,load_in_8bit=True,trust_remote_code=True,use_flash_attention_2=True)\nmodel =model.eval()\ntokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Atom-7B-Chat',use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\ninput_ids = tokenizer(['<s>Human: ä»‹ç»ä¸€ä¸‹ä¸­å›½\\n</s><s>Assistant: '], return_tensors=\"pt\",add_special_tokens=False).input_ids\nif torch.cuda.is_available():\n  input_ids = input_ids.to('cuda')\ngenerate_input = {\n    \"input_ids\":input_ids,\n    \"max_new_tokens\":512,\n    \"do_sample\":True,\n    \"top_k\":50,\n    \"top_p\":0.95,\n    \"temperature\":0.3,\n    \"repetition_penalty\":1.3,\n    \"eos_token_id\":tokenizer.eos_token_id,\n    \"bos_token_id\":tokenizer.bos_token_id,\n    \"pad_token_id\":tokenizer.pad_token_id\n}\ngenerate_ids  = model.generate(**generate_input)\ntext = tokenizer.decode(generate_ids[0])\nprint(text)\n```\n\nè¿è¡Œ quick_start.py ä»£ç ã€‚\n```bash\npython quick_start.py\n```\n\n### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨Docker\n\nè¯¦æƒ…å‚è§ï¼š[Dockeréƒ¨ç½²](https://github.com/LlamaFamily/Llama-Chinese/blob/main/docs/chat_gradio_guide.md)\n\nç¬¬ 1 æ­¥ï¼šå‡†å¤‡dockeré•œåƒï¼Œé€šè¿‡dockerå®¹å™¨å¯åŠ¨[chat_gradio.py](../examples/chat_gradio.py)\n```bash\ngit clone https://github.com/LlamaFamily/Llama-Chinese.git\n\ncd Llama-Chinese\n\ndocker build -f docker/Dockerfile -t flagalpha/llama2-chinese:gradio .\n```\n\nç¬¬ 2 æ­¥ï¼šé€šè¿‡docker-composeå¯åŠ¨chat_gradio\n```bash\ncd Llama-Chinese/docker\ndocker-compose up -d --build\n```\n\n### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨llama.cpp\nè¯¦æƒ…å‚è§ï¼š[ä½¿ç”¨llama.cpp](https://github.com/LlamaFamily/Llama-Chinese/blob/main/inference-speed/CPU/ggml/README.md)\n\n### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨gradio\nåŸºäºgradioæ­å»ºçš„é—®ç­”ç•Œé¢ï¼Œå®ç°äº†æµå¼çš„è¾“å‡ºï¼Œå°†ä¸‹é¢ä»£ç å¤åˆ¶åˆ°æ§åˆ¶å°è¿è¡Œï¼Œä»¥ä¸‹ä»£ç ä»¥Atom-7B-Chatæ¨¡å‹ä¸ºä¾‹ï¼Œä¸åŒæ¨¡å‹åªéœ€ä¿®æ”¹ä¸€ä¸‹é¢çš„model_name_or_pathå¯¹åº”çš„æ¨¡å‹åç§°å°±å¥½äº†ğŸ˜Š\n```\npython examples/chat_gradio.py --model_name_or_path FlagAlpha/Atom-7B-Chat\n```\n\n### å¿«é€Ÿä¸Šæ‰‹-æ„å»ºAPIæœåŠ¡\nä½¿ç”¨FastChatæ„å»ºå’ŒOpenAIä¸€è‡´çš„æ¨ç†æœåŠ¡æ¥å£ã€‚\n\n<details>\nç¬¬ 0 æ­¥ï¼šå‰ææ¡ä»¶\n\nå®‰è£…fastchat\n```bash\npip3 install \"fschat[model_worker,webui]\"\n```\nç¬¬ 1 æ­¥ï¼šå¯åŠ¨Restful API\n\nå¼€å¯ä¸‰ä¸ªæ§åˆ¶å°åˆ†åˆ«æ‰§è¡Œä¸‹é¢çš„ä¸‰ä¸ªå‘½ä»¤\n- é¦–å…ˆå¯åŠ¨controler\n```bash\npython3 -m fastchat.serve.controller \\\n--host localhost \\\n--port 21001\n```\n\n- å¯åŠ¨æ¨¡å‹\n```bash\nCUDA_VISIBLE_DEVICES=\"0\" python3 -m fastchat.serve.model_worker --model-path /path/Atom-7B-Chat \\\n--host localhost \\\n--port 21002 \\\n--worker-address \"http://localhost:21002\" \\\n--limit-worker-concurrency 5 \\\n--stream-interval 2 \\\n--gpus \"1\" \\\n--load-8bit\n```\n\n- å¯åŠ¨RESTful API æœåŠ¡\n```bash\npython3 -m fastchat.serve.openai_api_server \\\n--host localhost \\\n--port 21003 \\\n--controller-address http://localhost:21001\n```\n\nç¬¬ 2 æ­¥ï¼šæµ‹è¯•apiæœåŠ¡\n\næ‰§è¡Œä¸‹é¢çš„pythonä»£ç æµ‹è¯•ä¸Šé¢éƒ¨ç½²çš„apiæœåŠ¡\n```python\n# coding=utf-8\nimport json\nimport time\nimport urllib.request\nimport sys\nimport requests\n\ndef test_api_server(input_text):\n    header = {'Content-Type': 'application/json'}\n\n    data = {\n          \"messages\": [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": input_text}],\n          \"temperature\": 0.3, \n          \"top_p\" : 0.95, \n          \"max_tokens\": 512, \n          \"model\": \"LLama2-Chinese-13B\",\n          \"stream\" : False,\n          \"n\" : 1,\n          \"best_of\": 1, \n          \"presence_penalty\": 1.2, \n          \"frequency_penalty\": 0.2,           \n          \"top_k\": 50, \n          \"use_beam_search\": False, \n          \"stop\": [], \n          \"ignore_eos\" :False,\n          \"logprobs\": None\n    }\n    response = requests.post(\n        url='http://127.0.0.1:21003/v1/chat/completions',\n        headers=header,\n        data=json.dumps(data).encode('utf-8')\n    )\n\n    result = None\n    try:\n        result = json.loads(response.content)\n        print(json.dumps(data, ensure_ascii=False, indent=2))\n        print(json.dumps(result, ensure_ascii=False, indent=2))\n\n    except Exception as e:\n        print(e)\n\n    return result\n\nif __name__ == \"__main__\":\n    test_api_server(\"å¦‚ä½•å»åŒ—äº¬?\")\n```\n\n</details>\n\n\n### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨ollamaè¿è¡Œ\n\n1. é¦–å…ˆéœ€è¦å®‰è£…ollamaå·¥å…·\n\nå®‰è£…æ–¹æ³•å‚è€ƒï¼š[https://ollama.com](https://ollama.com/)\n\n2. ollamaè¿è¡ŒLlama3-Chinese-8B-Instructã€Atom-7B-Chat\n\nollamaè¿è¡ŒåŸºäºLlama3è¿›è¡Œä¸­æ–‡å¾®è°ƒçš„å¤§æ¨¡å‹[Llama3-Chinese-8B-Instruct](https://huggingface.co/FlagAlpha/Llama3-Chinese-8B-Instruct)\n\næ‰“å¼€å‘½ä»¤è¡Œæ‰§è¡Œå‘½ä»¤\n```\nollama run llamafamily/llama3-chinese-8b-instruct\n```\n\nollamaè¿è¡ŒåŸºäºLlama2è¿›è¡Œä¸­æ–‡é¢„è®­ç»ƒçš„å¼€æºå¤§æ¨¡å‹[Atom-7B-Chat](https://huggingface.co/FlagAlpha/Atom-7B-Chat)\n\næ‰“å¼€å‘½ä»¤è¡Œæ‰§è¡Œå‘½ä»¤\n```\nollama run llamafamily/atom-7b-chat\n```\n\n\n## ğŸ¤– æ¨¡å‹é¢„è®­ç»ƒ\nè™½ç„¶Llama2çš„é¢„è®­ç»ƒæ•°æ®ç›¸å¯¹äºç¬¬ä¸€ä»£LLaMAæ‰©å¤§äº†ä¸€å€ï¼Œä½†æ˜¯ä¸­æ–‡é¢„è®­ç»ƒæ•°æ®çš„æ¯”ä¾‹ä¾ç„¶éå¸¸å°‘ï¼Œä»…å 0.13%ï¼Œè¿™ä¹Ÿå¯¼è‡´äº†åŸå§‹Llama2çš„ä¸­æ–‡èƒ½åŠ›è¾ƒå¼±ã€‚ä¸ºäº†èƒ½å¤Ÿæå‡æ¨¡å‹çš„ä¸­æ–‡èƒ½åŠ›ï¼Œå¯ä»¥é‡‡ç”¨å¾®è°ƒå’Œé¢„è®­ç»ƒä¸¤ç§è·¯å¾„ï¼Œå…¶ä¸­ï¼š\n- å¾®è°ƒéœ€è¦çš„ç®—åŠ›èµ„æºå°‘ï¼Œèƒ½å¤Ÿå¿«é€Ÿå®ç°ä¸€ä¸ªä¸­æ–‡Llamaçš„é›å½¢ã€‚ä½†ç¼ºç‚¹ä¹Ÿæ˜¾è€Œæ˜“è§ï¼Œåªèƒ½æ¿€å‘åŸºåº§æ¨¡å‹å·²æœ‰çš„ä¸­æ–‡èƒ½åŠ›ï¼Œç”±äºLlama2çš„ä¸­æ–‡è®­ç»ƒæ•°æ®æœ¬èº«è¾ƒå°‘ï¼Œæ‰€ä»¥èƒ½å¤Ÿæ¿€å‘çš„èƒ½åŠ›ä¹Ÿæœ‰é™ï¼Œæ²»æ ‡ä¸æ²»æœ¬ã€‚\n\n- åŸºäºå¤§è§„æ¨¡ä¸­æ–‡è¯­æ–™è¿›è¡Œé¢„è®­ç»ƒï¼Œæˆæœ¬é«˜ï¼Œä¸ä»…éœ€è¦å¤§è§„æ¨¡é«˜è´¨é‡çš„ä¸­æ–‡æ•°æ®ï¼Œä¹Ÿéœ€è¦å¤§è§„æ¨¡çš„ç®—åŠ›èµ„æºã€‚ä½†æ˜¯ä¼˜ç‚¹ä¹Ÿæ˜¾è€Œæ˜“è§ï¼Œå°±æ˜¯èƒ½ä»æ¨¡å‹åº•å±‚ä¼˜åŒ–ä¸­æ–‡èƒ½åŠ›ï¼ŒçœŸæ­£è¾¾åˆ°æ²»æœ¬çš„æ•ˆæœï¼Œä»å†…æ ¸ä¸ºå¤§æ¨¡å‹æ³¨å…¥å¼ºå¤§çš„ä¸­æ–‡èƒ½åŠ›ã€‚\n\næˆ‘ä»¬ä¸ºç¤¾åŒºæä¾›äº†Llamaæ¨¡å‹çš„é¢„è®­ç»ƒä»£ç ï¼Œä»¥åŠ[ä¸­æ–‡æµ‹è¯•è¯­æ–™](https://github.com/LlamaFamily/Llama-Chinese/tree/main/data)ï¼Œæ›´å¤šæ•°æ®å¯ä»¥å‚è€ƒ[ä¸­æ–‡è¯­æ–™](#-ä¸­æ–‡æ•°æ®)ã€‚å…·ä½“ä»£ç å’Œé…ç½®å¦‚ä¸‹ï¼š\n- æ¨¡å‹é¢„è®­ç»ƒè„šæœ¬ï¼š[train/pretrain/pretrain.sh](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/pretrain.sh)\n- é¢„è®­ç»ƒå®ç°ä»£ç ï¼š[train/pretrain/pretrain_clm.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/pretrain_clm.py)\n- [DeepSpeed](https://github.com/microsoft/DeepSpeed)åŠ é€Ÿï¼š\n  - å¯¹äºå•å¡è®­ç»ƒï¼Œå¯ä»¥é‡‡ç”¨ZeRO-2çš„æ–¹å¼ï¼Œå‚æ•°é…ç½®è§ [train/pretrain/ds_config_zero2.json](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/ds_config_zero2.json)\n  - å¯¹äºå¤šå¡è®­ç»ƒï¼Œå¯ä»¥é‡‡ç”¨ZeRO-3çš„æ–¹å¼ï¼Œå‚æ•°é…ç½®è§ [train/pretrain/ds_config_zero3.json](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/ds_config_zero3.json)\n- è®­ç»ƒæ•ˆæœåº¦é‡æŒ‡æ ‡ï¼š[train/pretrain/accuracy.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/accuracy.py)\n\n## ğŸ’¡ æ¨¡å‹å¾®è°ƒ\n\næœ¬ä»“åº“ä¸­åŒæ—¶æä¾›äº†LoRAå¾®è°ƒå’Œå…¨é‡å‚æ•°å¾®è°ƒä»£ç ï¼Œå…³äºLoRAçš„è¯¦ç»†ä»‹ç»å¯ä»¥å‚è€ƒè®ºæ–‡â€œ[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)â€ä»¥åŠå¾®è½¯Githubä»“åº“[LoRA](https://github.com/microsoft/LoRA)ã€‚\n\n### Step1: ç¯å¢ƒå‡†å¤‡\n\næ ¹æ®[requirements.txt](https://github.com/LlamaFamily/Llama-Chinese/blob/main/requirements.txt)å®‰è£…å¯¹åº”çš„ç¯å¢ƒä¾èµ–ã€‚\n\n### Step2: æ•°æ®å‡†å¤‡\nåœ¨dataç›®å½•ä¸‹æä¾›äº†ä¸€ä»½ç”¨äºæ¨¡å‹sftçš„æ•°æ®æ ·ä¾‹ï¼š\n- è®­ç»ƒæ•°æ®ï¼š[data/train_sft.csv](https://github.com/LlamaFamily/Llama-Chinese/blob/main/data/train_sft.csv)\n- éªŒè¯æ•°æ®ï¼š[data/dev_sft.csv](https://github.com/LlamaFamily/Llama-Chinese/blob/main/data/dev_sft.csv)\n\næ¯ä¸ªcsvæ–‡ä»¶ä¸­åŒ…å«ä¸€åˆ—â€œtextâ€ï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ¯ä¸ªè®­ç»ƒæ ·ä¾‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å°†é—®é¢˜å’Œç­”æ¡ˆç»„ç»‡ä¸ºæ¨¡å‹è¾“å…¥ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è‡ªå®šä¹‰è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼š\n```\n\"<s>Human: \"+é—®é¢˜+\"\\n</s><s>Assistant: \"+ç­”æ¡ˆ+\"\\n\"</s>\n```\nä¾‹å¦‚ï¼Œ\n```\n<s>Human: ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚</s><s>Assistant: å› ä¸ºåœ°çƒæ˜¯ç›®å‰ä¸ºæ­¢å”¯ä¸€å·²çŸ¥å­˜åœ¨ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚</s>\n```\n\n### Step3: å¾®è°ƒè„šæœ¬\n\n#### LoRAå¾®è°ƒ\nLoRAå¾®è°ƒè„šæœ¬è§ï¼š[train/sft/finetune_lora.sh](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/sft/finetune_lora.sh)ï¼Œå…³äºLoRAå¾®è°ƒçš„å…·ä½“å®ç°ä»£ç è§[train/sft/finetune_clm_lora.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/sft/finetune_clm_lora.py)ï¼Œå•æœºå¤šå¡çš„å¾®è°ƒå¯ä»¥é€šè¿‡ä¿®æ”¹è„šæœ¬ä¸­çš„`--include localhost:0`æ¥å®ç°ã€‚\n\n#### å…¨é‡å‚æ•°å¾®è°ƒ\nå…¨é‡å‚æ•°å¾®è°ƒè„šæœ¬è§ï¼š[train/sft/finetune.sh](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/sft/finetune.sh)ï¼Œå…³äºå…¨é‡å‚æ•°å¾®è°ƒçš„å…·ä½“å®ç°ä»£ç è§[train/sft/finetune_clm.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/sft/finetune_clm.py)ã€‚\n\n\n### Step4: åŠ è½½å¾®è°ƒæ¨¡å‹\n\n#### LoRAå¾®è°ƒ\nåŸºäºLoRAå¾®è°ƒçš„æ¨¡å‹å‚æ•°è§ï¼š[åŸºäºLlama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹](#llama2ä¸­æ–‡å¾®è°ƒæ¨¡å‹)ï¼ŒLoRAå‚æ•°éœ€è¦å’ŒåŸºç¡€æ¨¡å‹å‚æ•°ç»“åˆä½¿ç”¨ã€‚\n\né€šè¿‡[PEFT](https://github.com/huggingface/peft)åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°å’Œå¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä»¥ä¸‹ç¤ºä¾‹ä»£ç ä¸­ï¼Œbase_model_name_or_pathä¸ºé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„ï¼Œfinetune_model_pathä¸ºå¾®è°ƒæ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„ã€‚\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel,PeftConfig\n# ä¾‹å¦‚: finetune_model_path='FlagAlpha/Llama2-Chinese-7b-Chat-LoRA'\nfinetune_model_path=''  \nconfig = PeftConfig.from_pretrained(finetune_model_path)\n# ä¾‹å¦‚: base_model_name_or_path='meta-llama/Llama-2-7b-chat'\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\ndevice_map = \"cuda:0\" if torch.cuda.is_available() else \"auto\"\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map=device_map,torch_dtype=torch.float16,load_in_8bit=True,trust_remote_code=True,use_flash_attention_2=True)\nmodel = PeftModel.from_pretrained(model, finetune_model_path, device_map={\"\": 0})\nmodel =model.eval()\ninput_ids = tokenizer(['<s>Human: ä»‹ç»ä¸€ä¸‹åŒ—äº¬\\n</s><s>Assistant: '], return_tensors=\"pt\",add_special_tokens=False).input_ids\nif torch.cuda.is_available():\n  input_ids = input_ids.to('cuda')\ngenerate_input = {\n    \"input_ids\":input_ids,\n    \"max_new_tokens\":512,\n    \"do_sample\":True,\n    \"top_k\":50,\n    \"top_p\":0.95,\n    \"temperature\":0.3,\n    \"repetition_penalty\":1.3,\n    \"eos_token_id\":tokenizer.eos_token_id,\n    \"bos_token_id\":tokenizer.bos_token_id,\n    \"pad_token_id\":tokenizer.pad_token_id\n}\ngenerate_ids  = model.generate(**generate_input)\ntext = tokenizer.decode(generate_ids[0])\nprint(text)\n```\n\n#### å…¨é‡å‚æ•°å¾®è°ƒ\nå¯¹äºå…¨é‡å‚æ•°å¾®è°ƒçš„æ¨¡å‹ï¼Œè°ƒç”¨æ–¹å¼åŒ[æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹](#æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹)ï¼Œåªéœ€è¦ä¿®æ”¹å…¶ä¸­çš„æ¨¡å‹åç§°æˆ–è€…ä¿å­˜è·¯å¾„å³å¯ã€‚\n\n## ğŸ„ æ¨¡å‹é‡åŒ–\næˆ‘ä»¬å¯¹ä¸­æ–‡å¾®è°ƒçš„æ¨¡å‹å‚æ•°è¿›è¡Œäº†é‡åŒ–ï¼Œæ–¹ä¾¿ä»¥æ›´å°‘çš„è®¡ç®—èµ„æºè¿è¡Œã€‚ç›®å‰å·²ç»åœ¨[Hugging Face](https://huggingface.co/FlagAlpha)ä¸Šä¼ äº†13Bä¸­æ–‡å¾®è°ƒæ¨¡å‹[FlagAlpha/Llama2-Chinese-13b-Chat](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat)çš„4bitå‹ç¼©ç‰ˆæœ¬[FlagAlpha/Llama2-Chinese-13b-Chat-4bit](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-4bit)ï¼Œå…·ä½“è°ƒç”¨æ–¹å¼å¦‚ä¸‹ï¼š\n\nç¯å¢ƒå‡†å¤‡ï¼š\n```\npip install git+https://github.com/PanQiWei/AutoGPTQ.git\n```\n\n```python\nfrom transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\nmodel = AutoGPTQForCausalLM.from_quantized('FlagAlpha/Llama2-Chinese-13b-Chat-4bit', device=\"cuda:0\")\ntokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Llama2-Chinese-13b-Chat-4bit',use_fast=False)\ninput_ids = tokenizer(['<s>Human: æ€ä¹ˆç™»ä¸Šç«æ˜Ÿ\\n</s><s>Assistant: '], return_tensors=\"pt\",add_special_tokens=False).input_ids.to('cuda')        \ngenerate_input = {\n    \"input_ids\":input_ids,\n    \"max_new_tokens\":512,\n    \"do_sample\":True,\n    \"top_k\":50,\n    \"top_p\":0.95,\n    \"temperature\":0.3,\n    \"repetition_penalty\":1.3,\n    \"eos_token_id\":tokenizer.eos_token_id,\n    \"bos_token_id\":tokenizer.bos_token_id,\n    \"pad_token_id\":tokenizer.pad_token_id\n}\ngenerate_ids  = model.generate(**generate_input)\ntext = tokenizer.decode(generate_ids[0])\nprint(text)\n```\n\n## ğŸš€ éƒ¨ç½²åŠ é€Ÿ\néšç€å¤§æ¨¡å‹å‚æ•°è§„æ¨¡çš„ä¸æ–­å¢é•¿ï¼Œåœ¨æœ‰é™çš„ç®—åŠ›èµ„æºä¸‹ï¼Œæå‡æ¨¡å‹çš„æ¨ç†é€Ÿåº¦é€æ¸å˜ä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚å¸¸ç”¨çš„æ¨ç†åŠ é€Ÿæ¡†æ¶åŒ…å« lmdeployã€TensorRT-LLMã€vLLMå’ŒJittorLLMs ç­‰ã€‚\n\n### TensorRT-LLM\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main)ç”±NVIDIAå¼€å‘ï¼Œé«˜æ€§èƒ½æ¨ç†æ¡†æ¶\n\nè¯¦ç»†çš„æ¨ç†æ–‡æ¡£è§ï¼š[inference-speed/GPU/TensorRT-LLM_example](https://github.com/LlamaFamily/Llama-Chinese/tree/main/inference-speed/GPU/TensorRT-LLM_example)\n\n### vLLM\n[vLLM](https://github.com/vllm-project/vllm)ç”±åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡å¼€å‘ï¼Œæ ¸å¿ƒæŠ€æœ¯æ˜¯PageAttentionï¼Œååé‡æ¯”HuggingFace Transformersé«˜å‡º24å€ã€‚ç›¸è¾ƒä¸FasterTrainsformerï¼ŒvLLMæ›´åŠ çš„ç®€å•æ˜“ç”¨ï¼Œä¸éœ€è¦é¢å¤–è¿›è¡Œæ¨¡å‹çš„è½¬æ¢ï¼Œæ”¯æŒfp16æ¨ç†ã€‚\n\nè¯¦ç»†çš„æ¨ç†æ–‡æ¡£è§ï¼š[inference-speed/GPU/vllm_example](https://github.com/LlamaFamily/Llama-Chinese/blob/main/inference-speed/GPU/vllm_example/README.md)\n\n### JittorLLMs\n[JittorLLMs](https://github.com/Jittor/JittorLLMs)ç”±éåç§‘æŠ€é¢†è¡”ï¼Œä¸æ¸…åå¤§å­¦å¯è§†åª’ä½“ç ”ç©¶ä¸­å¿ƒåˆä½œç ”å‘ï¼Œé€šè¿‡åŠ¨æ€swapæœºåˆ¶å¤§å¹…é™ä½ç¡¬ä»¶é…ç½®è¦æ±‚ï¼ˆå‡å°‘80%ï¼‰,å¹¶ä¸”Jittoræ¡†æ¶é€šè¿‡é›¶æ‹·è´æŠ€æœ¯ï¼Œå¤§æ¨¡å‹åŠ è½½ç›¸æ¯”Pytorchå¼€é”€é™ä½40%ï¼ŒåŒæ—¶ï¼Œé€šè¿‡å…ƒç®—å­è‡ªåŠ¨ç¼–è¯‘ä¼˜åŒ–ï¼Œè®¡ç®—æ€§èƒ½æå‡20%ä»¥ä¸Šã€‚\n\nè¯¦ç»†çš„æ¨ç†æ–‡æ¡£è§ï¼š[inference-speed/GPU/JittorLLMs](https://github.com/LlamaFamily/Llama-Chinese/blob/main/inference-speed/GPU/JittorLLMs_example/README.md)\n\n### lmdeploy\n[lmdeploy](https://github.com/InternLM/lmdeploy/) ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å¼€å‘ï¼Œæ¨ç†ä½¿ç”¨ C++/CUDAï¼Œå¯¹å¤–æä¾› python/gRPC/http æ¥å£å’Œ WebUI ç•Œé¢ï¼Œæ”¯æŒ tensor parallel åˆ†å¸ƒå¼æ¨ç†ã€æ”¯æŒ fp16/weight int4/kv cache int8 é‡åŒ–ã€‚\n\nè¯¦ç»†çš„æ¨ç†æ–‡æ¡£è§ï¼š[inference-speed/GPU/lmdeploy_example](https://github.com/LlamaFamily/Llama-Chinese/tree/main/inference-speed/GPU/lmdeploy_example)\n\n## ğŸ’ª å¤–å»¶èƒ½åŠ›\n\né™¤äº†æŒç»­å¢å¼ºå¤§æ¨¡å‹å†…åœ¨çš„çŸ¥è¯†å‚¨å¤‡ã€é€šç”¨ç†è§£ã€é€»è¾‘æ¨ç†å’Œæƒ³è±¡èƒ½åŠ›ç­‰ï¼Œæœªæ¥ï¼Œæˆ‘ä»¬ä¹Ÿä¼šä¸æ–­ä¸°å¯Œå¤§æ¨¡å‹çš„å¤–å»¶èƒ½åŠ›ï¼Œä¾‹å¦‚çŸ¥è¯†åº“æ£€ç´¢ã€è®¡ç®—å·¥å…·ã€WolframAlphaã€æ“ä½œè½¯ä»¶ç­‰ã€‚\næˆ‘ä»¬é¦–å…ˆé›†æˆäº†LangChainæ¡†æ¶ï¼Œå¯ä»¥æ›´æ–¹ä¾¿åœ°åŸºäºLlama2å¼€å‘æ–‡æ¡£æ£€ç´¢ã€é—®ç­”æœºå™¨äººå’Œæ™ºèƒ½ä½“åº”ç”¨ç­‰ï¼Œå…³äºLangChainçš„æ›´å¤šä»‹ç»å‚è§[LangChain](https://github.com/langchain-ai/langchain)ã€‚\n\n### LangChain\né’ˆå¯¹LangChainæ¡†æ¶å°è£…çš„Llama2 LLMç±»è§[examples/llama2_for_langchain.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/examples/llama2_for_langchain.py)ï¼Œç®€å•çš„è°ƒç”¨ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š\n```python\nfrom llama2_for_langchain import Llama2\n\n# è¿™é‡Œä»¥è°ƒç”¨FlagAlpha/Atom-7B-Chatä¸ºä¾‹\nllm = Llama2(model_name_or_path='FlagAlpha/Atom-7B-Chat')\n\nwhile True:\n    human_input = input(\"Human: \")\n    response = llm(human_input)\n    print(f\"Llama2: {response}\")\n```\n\n## ğŸ¥‡ æ¨¡å‹è¯„æµ‹\n\n### Llama2å’ŒLlama3å¯¹æ¯”è¯„æµ‹\nåŸºç¡€æ¨¡å‹å¯¹æ¯”\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/base_eval.png\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\nå¾®è°ƒæ¨¡å‹å¯¹æ¯”\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/tuned_eval.png\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n\n### Llama3æ¨¡å‹è¯„æµ‹\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/llama3_eval.png\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n\n### Llama2æ¨¡å‹è¯„æµ‹\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/llama_eval.jpeg\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n\nä¸ºäº†èƒ½å¤Ÿæ›´åŠ æ¸…æ™°åœ°äº†è§£Llama2æ¨¡å‹çš„ä¸­æ–‡é—®ç­”èƒ½åŠ›ï¼Œæˆ‘ä»¬ç­›é€‰äº†ä¸€äº›å…·æœ‰ä»£è¡¨æ€§çš„ä¸­æ–‡é—®é¢˜ï¼Œå¯¹Llama2æ¨¡å‹è¿›è¡Œæé—®ã€‚æˆ‘ä»¬æµ‹è¯•çš„æ¨¡å‹åŒ…å«Metaå…¬å¼€çš„Llama2-7B-Chatå’ŒLlama2-13B-Chatä¸¤ä¸ªç‰ˆæœ¬ï¼Œæ²¡æœ‰åšä»»ä½•å¾®è°ƒå’Œè®­ç»ƒã€‚æµ‹è¯•é—®é¢˜ç­›é€‰è‡ª[AtomBulb](https://github.com/AtomEcho/AtomBulb)ï¼Œå…±95ä¸ªæµ‹è¯•é—®é¢˜ï¼ŒåŒ…å«ï¼šé€šç”¨çŸ¥è¯†ã€è¯­è¨€ç†è§£ã€åˆ›ä½œèƒ½åŠ›ã€é€»è¾‘æ¨ç†ã€ä»£ç ç¼–ç¨‹ã€å·¥ä½œæŠ€èƒ½ã€ä½¿ç”¨å·¥å…·ã€äººæ ¼ç‰¹å¾å…«ä¸ªå¤§çš„ç±»åˆ«ã€‚\n\næµ‹è¯•ä¸­ä½¿ç”¨çš„Promptå¦‚ä¸‹ï¼Œä¾‹å¦‚å¯¹äºé—®é¢˜â€œåˆ—å‡º5ç§å¯ä»¥æ”¹å–„ç¡çœ è´¨é‡çš„æ–¹æ³•â€ï¼š\n```\n[INST] \n<<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. The answer always been translate into Chinese language.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\nThe answer always been translate into Chinese language.\n<</SYS>>\n\nåˆ—å‡º5ç§å¯ä»¥æ”¹å–„ç¡çœ è´¨é‡çš„æ–¹æ³•\n[/INST]\n```\nLlama2-7B-Chatçš„æµ‹è¯•ç»“æœè§[meta_eval_7B.md](assets/meta_eval_7B.md)ï¼ŒLlama2-13B-Chatçš„æµ‹è¯•ç»“æœè§[meta_eval_13B.md](assets/meta_eval_13B.md)ã€‚\n\né€šè¿‡æµ‹è¯•æˆ‘ä»¬å‘ç°ï¼ŒMetaåŸå§‹çš„Llama2 Chatæ¨¡å‹å¯¹äºä¸­æ–‡é—®ç­”çš„å¯¹é½æ•ˆæœä¸€èˆ¬ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹éƒ½ä¸èƒ½ç»™å‡ºä¸­æ–‡å›ç­”ï¼Œæˆ–è€…æ˜¯ä¸­è‹±æ–‡æ··æ‚çš„å½¢å¼ã€‚å› æ­¤ï¼ŒåŸºäºä¸­æ–‡æ•°æ®å¯¹Llama2æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œå¾®è°ƒååˆ†å¿…è¦ã€‚\n\n\n## ğŸ“– å­¦ä¹ ä¸­å¿ƒ\n\n### å®˜æ–¹æ–‡æ¡£\nMeta Llamaå…¨ç³»åˆ—æ¨¡å‹å®˜æ–¹æ–‡æ¡£ï¼šhttps://llama.meta.com/docs/get-started\n\n### Llama3\n[Llama3å…¨å¥—å­¦ä¹ èµ„æ–™](https://chinesellama.feishu.cn/wiki/XBKPwbhWriWCfrkmJhfcrS9Rnqc?fromScene=spaceOverview)\n\nLlama3å®˜æ–¹é“¾æ¥ï¼šhttps://llama.meta.com/llama3\n\n### Llama2\n\n#### Metaå®˜æ–¹å¯¹äº[Llama2](https://ai.meta.com/llama)çš„ä»‹ç»\nè‡ªä»Metaå…¬å¸å‘å¸ƒç¬¬ä¸€ä»£LLaMAæ¨¡å‹ä»¥æ¥ï¼Œç¾Šé©¼æ¨¡å‹å®¶æ—ç¹è£å‘å±•ã€‚è¿‘æœŸMetaå‘å¸ƒäº†Llama2ç‰ˆæœ¬ï¼Œå¼€æºå¯å•†ç”¨ï¼Œåœ¨æ¨¡å‹å’Œæ•ˆæœä¸Šæœ‰äº†é‡å¤§æ›´æ–°ã€‚Llama2æ€»å…±å…¬å¸ƒäº†7Bã€13Bå’Œ70Bä¸‰ç§å‚æ•°å¤§å°çš„æ¨¡å‹ã€‚ç›¸æ¯”äºLLaMAï¼ŒLlama2çš„è®­ç»ƒæ•°æ®è¾¾åˆ°äº†2ä¸‡äº¿tokenï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¹Ÿç”±ä¹‹å‰çš„2048å‡çº§åˆ°4096ï¼Œå¯ä»¥ç†è§£å’Œç”Ÿæˆæ›´é•¿çš„æ–‡æœ¬ã€‚Llama2 Chatæ¨¡å‹åŸºäº100ä¸‡äººç±»æ ‡è®°æ•°æ®å¾®è°ƒå¾—åˆ°ï¼Œåœ¨è‹±æ–‡å¯¹è¯ä¸Šè¾¾åˆ°äº†æ¥è¿‘ChatGPTçš„æ•ˆæœã€‚\n\n### Llamaç›¸å…³è®ºæ–‡\n* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\n* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)\n* [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\n\n\n## ğŸ“Œ å…¶å®ƒ\n\n### ğŸ‰ è‡´è°¢\n\næ„Ÿè°¢åŸå­å›å£°[AtomEcho](https://github.com/AtomEcho)å›¢é˜Ÿçš„æŠ€æœ¯å’Œèµ„æºæ”¯æŒï¼\n\næ„Ÿè°¢èŠ¯æ ¼[Coremesh](https://coremesh.net)å›¢é˜Ÿçš„æŠ€æœ¯å’Œèµ„æºæ”¯æŒï¼\n\næ„Ÿè°¢ [ç¦å·è¿å¤©æ•™è‚²ç§‘æŠ€æœ‰é™å…¬å¸](www.3class.cc) å¯¹Llamaä¸­æ–‡ç¤¾åŒºçš„è´¡çŒ®ï¼\n\næ„Ÿè°¢ @Z Potentialsç¤¾åŒºå¯¹Llamaä¸­æ–‡ç¤¾åŒºçš„æ”¯æŒï¼\n\n### ğŸ¤” é—®é¢˜åé¦ˆ\n\nå¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ï¼Œåœ¨æäº¤é—®é¢˜ä¹‹å‰ï¼Œè¯·å…ˆæŸ¥é˜…ä»¥å¾€çš„issueæ˜¯å¦èƒ½è§£å†³ä½ çš„é—®é¢˜ã€‚\n\nç¤¼è²Œåœ°æå‡ºé—®é¢˜ï¼Œæ„å»ºå’Œè°çš„è®¨è®ºç¤¾åŒºã€‚\n\nåŠ å…¥[é£ä¹¦çŸ¥è¯†åº“](https://chinesellama.feishu.cn/wiki/space/7257824476874768388?ccm_open_type=lark_wiki_spaceLink)ï¼Œä¸€èµ·å…±å»ºç¤¾åŒºæ–‡æ¡£ã€‚\n\nåŠ å…¥å¾®ä¿¡ç¾¤è®¨è®ºğŸ˜ğŸ˜\n\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/wechat-new.jpeg\" alt=\"Wechat\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n\n<p align=\"center\" width=\"100%\">\n<img src=\"https://api.star-history.com/svg?repos=LlamaFamily/Llama-Chinese&type=Date\" alt=\"Star\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 43.953125,
          "content": "<p align=\"left\">\n    English ï½œ <a href=\"README.md\">ä¸­æ–‡</a>\n</p>\n<br>\n\n<h1 align=\"center\">\n  Llama-Chinese\n</h1>\n<p align=\"center\" width=\"100%\">\n  <img src=\"assets/llama.png\" alt=\"Llama\" style=\"width: 20%; display: block; margin: auto;\"></a>\n</p>\n<p align=\"center\">\n  <font face=\"é»‘ä½“\" color=orange size=\"6\"> The Best Chinese Llama Large Language Model </font>\n</p>\n\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/FlagAlpha\" target=\"_blank\">Hugging Face</a> â€¢ ğŸ¤– <a href=\"https://www.modelscope.cn/organization/FlagAlpha/\" target=\"_blank\">ModelScope</a> â€¢ âœ¡ï¸ <a href=\"https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat\" target=\"_blank\">WiseModel</a>\n</p> \n\n<p align=\"center\">\n  <a href=\"https://llama.family\">Online(Including Llama2, Llama3): llama.family</a>\n</p>\n<p align=\"center\">\n  <a href=\"https://huggingface.co/FlagAlpha/Atom-7B-Chat\">Open-source Chinese Pre-trained LLM Atom based on Llama2</a>\n</p>\n\n</br></br>\n\n\n## ğŸ—‚ï¸ Content Guide\n- [ğŸ“Œ Chinese Llama Community](#-chinese-llama-community)\n  * [ğŸ”¥ Community Introduction: Chinese Llama Community](#-community-introduction-chinese-llama-community)\n  * [ğŸ“¢ Community Announcements](#-community-announcements)\n  * [ğŸ¤— LLM Model](#-models-downloads)\n    + [ğŸ¤— Pre-trained Chinese Model Atom based on Llama2](#-atom-models)\n    + [ğŸ¤— Meta Official Llama2 Model](#meta-official-llama2-models)\n    + [ğŸ¤— Fine-tuned Chinese Models based on Llama2](#fine-tuned-chinese-models-based-on-llama2)\n  * [ğŸŒŸ Community Source](#-community-source)\n    + [GPU Source](#-gpu-source)\n    + [Data Source](#-data-source)\n    + [Discussion](#-discussion)\n    + [Product](#-product)\n\n- [ğŸ“Œ How to use Llama Model?](#-how-to-use-llama-model)\n  * [Setup of Llama3](#setup-of-llama3)\n  * [Setup of Llama2](#setup-of-llama2)\n    + [Simple Setup](#simple-setup)\n      - [Simple Setup-Anaconda](#simple-setup-anaconda)\n      - [Simple Setup-Docker](#simple-setup-docker)\n      - [Simple Setup-llama.cpp](#simple-setup-llamacpp)\n      - [Simple Setup-gradio](#simple-setup-gradio)\n      - [Simple Setup-API](#simple-setup-api)\n    + [ğŸ¤– Model Pretraining](#-model-pretraining)\n    + [ğŸ’¡ Model Fine-tuning](#-model-fine-tuning)\n      - [Step1: Environment Setup](#step1-environment-setup)\n      - [Step2: Data Preparation](#step2-data-preparation)\n      - [Step3: Fine-tuning Scripts](#step3-fine-tuning-script)\n        - [LoRA Fine-tuning](#lora-fine-tuning)\n        - [Full-parameter Fine-tuning](#full-parameter-fine-tuning)\n      - [Step4: Load Fine-tuned Model](#step4-load-fine-tuned-model)\n        - [LoRA Fine-tuning](#lora-fine-tuning-1)\n        - [Full-parameter Fine-tuning](#full-parameter-fine-tuning-1)\n    + [ğŸ„ Model Quantization](#-model-quantization)\n    + [ğŸš€ Inference Acceleration](#-inference-acceleration)\n      - [TensorRT-LLM](#tensorrt-llm)\n      - [vLLM](#vllm)\n      - [JittorLLMs](#jittorllms)\n      - [lmdeploy](#lmdeploy)\n    + [ğŸ’ª Extension Capabilities](#-extension-capabilities)\n      - [LangChain](#langchain)\n  * [ğŸ¥‡ Model Evaluation](#-model-evaluation)\n  * [ğŸ“– Learning Resources](#-learning-resources)\n    + [Llama3](#llama3)\n    + [Llama2](#llama2)\n      - [Meta Official Introduction to Llama2](#meta-official-introduction-to-llama2)\n      - [Llama-related Papers](#llama-related-papers)\n      - [Llama2 Evaluation Results](#llama2-evaluation-results)\n\n- [ğŸ“Œ Others](#-others)\n  * [ğŸ‰ Acknowledgments](#-acknowledgments)\n  * [ğŸ¤” Issue Feedback](#-issue-feedback)\n\n## ğŸ“Œ Chinese Llama Community\n\n### ğŸ”¥ Community Introduction: Chinese Llama Community\n\nWelcome to the Chinese Llama Community! We are a technical community dedicated to optimizing and building on top of the Llama model for Chinese applications.\n**\\*Based on large-scale Chinese data, we start pre-training and continuously upgrade the Llama2 model for Chinese capabilities\\***.\nWe warmly welcome developers and researchers passionate about LLM models to join our community.\n\n<details lang=\"en\">\n\n#### Why Choose the Chinese Llama Community?\nğŸš€ **Support from a Team of Senior Engineers**: The community has a team of dedicated NLP senior engineers who provide strong technical support and rich experience to guide and assist you.\n\nğŸ¯ **Chinese Optimization**: We focus on optimizing Llama2 for Chinese processing, exploring the best practices for Chinese to enhance its performance and adaptability.\n\nğŸ’¡ **Innovative Exchange**: Our community includes a creative and experienced team of members who organize regular online events, technical discussions, and experience sharing to promote innovative exchanges.\n\nğŸŒ **Global Connectivity**: We welcome developers from around the world to join the community, creating an open and diverse platform for learning and communication.\n\nğŸ¤ **Open Sharing**: We encourage community members to open-source and share code and models, promoting collaborative win-win efforts and advancing the development of Chinese NLP technology.\n\n#### Community Activities\nğŸ—“ï¸ **Online Lectures**: Inviting industry experts to conduct online lectures, sharing the latest technology and applications of Llama2 in the Chinese NLP field, and discussing cutting-edge research results.\n\nğŸ’» **Project Showcase**: Members can showcase their project achievements in Llama2 Chinese optimization, receive feedback and suggestions, and promote project collaboration.\n\nğŸ“š **Learning Resources**: The community maintains a rich library of learning materials, including tutorials, documentation, and paper interpretations, providing comprehensive learning support to members.\n\nğŸ“ **Paper Interpretation**: Community members collectively interpret the latest research papers related to Llama2, delving into advanced algorithms and methods.\n\nğŸ‰ **Themed Events**: Regularly organize various themed events, including challenges, hackathons, and technical salons, allowing community members to exchange and learn in a relaxed and enjoyable atmosphere.\n\nğŸŒŸ **Reward Program**: We have established a reward program to honor and reward members who actively participate and contribute outstanding work to the community, motivating more outstanding talents to join.\n\nğŸ“ˆ **Technical Consultation**: We provide technical consulting services to answer your questions and help you overcome challenges in the development and optimization of Llama2.\n\nğŸš€ **Project Collaboration**: Encourage collaboration between members on projects to explore the potential of Llama2 in practical applications and create innovative solutions.\n\n#### Join Us Now!\nğŸ“š **Vision**: Whether you are a professional developer or researcher with experience in Llama2 or a newcomer interested in optimizing Llama2 for Chinese, we eagerly look forward to your joining. In the Chinese Llama Community, you will have the opportunity to exchange ideas with top talents in the industry, work together to advance Chinese NLP technology, and create a brighter technological future!\n\nğŸ”— **Friendly Reminder**: This community is a platform for professional technical exchange. We earnestly hope that like-minded developers and researchers join us. Please adhere to the community guidelines, maintain a positive learning atmosphere, and any content and advertisements unrelated to Llama2 will be removed. Thank you for your understanding and support!\n\n</details>\n\n### ğŸ“¢ Community Announcements\n\nã€Latestã€‘October 8, 2023: Added the inference acceleration feature for JittorLLMs from Tsinghua University [JittorLLMs](#jittorllms)!\n\nã€Latestã€‘September 12, 2023: Updated pre-training versions [Atom-7B](https://huggingface.co/FlagAlpha/Atom-7B) and dialogue version [Atom-7B-Chat](https://huggingface.co/FlagAlpha/Atom-7B-Chat) model parameters. The latest Chinese pre-training data size is 100 billion tokens, and the training progress can be viewed at [llama.family](https://llama.family/)!\n\nã€Latestã€‘September 2, 2023: Added [pre-training code](#-model-pretraining) and [full-parameter fine-tuning code](#-model-fine-tuning)!\n\nã€Latestã€‘August 28, 2023: Released the open-source large model [Atom-7B](https://huggingface.co/FlagAlpha/Atom-7B) based on Llama2 for Chinese pre-training and will continue to be updated. Details can be found in the [community article](https://mp.weixin.qq.com/s/Bdx0JTVh1kgPn5ydYxIkEw)!\n\nã€Latestã€‘August 26, 2023: Provided [FastAPI](#fastapi-interface-setup) interface setup script!\n\nã€Latestã€‘August 26, 2023: Provided a script to convert Meta official model parameters to a format compatible with Hugging Face [Format Conversion Script](https://github.com/LlamaFamily/Llama-Chinese/blob/main/scripts/convert2hf/README.md)!\n\nã€Latestã€‘August 26, 2023: Added [Code Llama](#-code-model) model!\n\n<details lang=\"en\">\n\n- August 15, 2023: Added [PEFT load fine-tuning model parameters](#load-fine-tuned-model) code example!\n\n- August 14, 2023: Launched the [large model data sharing training platform](https://llama.family), allowing everyone to contribute to large model training, even without computing resources. The data contributed by each community member will determine the future capabilities of the model!\n\n- August 3, 2023: Added GPU [inference acceleration](#-inference-acceleration) support for FasterTransformer and vLLM!\n\n- July 31, 2023: ã€Majorã€‘The first truly meaningful Llama2 Chinese large model is released! Details can be found in the [community article](https://mp.weixin.qq.com/s/lExUU7z_MvgJ7tzQPF8tUQ)\n\n- July 28, 2023: Deployed a Q&A interface through [Docker](#docker-deployment-of-qa-interface)!\n\n- July 27, 2023: Added [LangChain](#langchain) support!\n\n- July 26, 2023: Released a [4-bit quantized compressed version](#-model-quantization) of the Llama2-13B Chinese fine-tuning parameters!\n\n- July 25, 2023: The community's WeChat public account \"Llama Chinese Community\" is now live. Feel free to follow for the latest updates and dynamics!\n\n- July 24, 2023: [FlagAlpha](https://huggingface.co/FlagAlpha) added Llama2-13B Chinese fine-tuned parameters!\n\n- July 24, 2023: [llama.family](https://llama.family/) added Llama2-70B online experience!\n\n- July 23, 2023: Released Llama2-13B Chinese fine-tuned parameters to the Hugging Face repository [FlagAlpha](https://huggingface.co/FlagAlpha)!\n\n- July 22, 2023: Llama2 online experience link [llama.family](https://llama.family/) is live, including both Meta original and Chinese fine-tuned versions!\n\n- July 21, 2023: Evaluated the Chinese Q&A capability of the Meta original Llama2 Chat model [Model Evaluation](#-model-evaluation)!\n\n- July 21, 2023: Added the Hugging Face version download link for Llama2 models in China!\n\n- July 20, 2023: Added [Feishu Knowledge Base Documentation](https://chinesellama.feishu.cn/wiki/space/7257824476874768388?ccm_open_type=lark_wiki_spaceLink), welcome everyone to contribute!\n\n- July 20, 2023: Chinese Llama2 latest download links are live!\n\n- July 19, 2023: Officially launched the Llama2 Chinese community, stay tuned for real-time updates!\n\n- July 19, 2023: Chinese Llama2 latest download links are in progress, stay tuned!\n\n- July 19, 2023: Launched the Llama2 Chinese community, welcome everyone to join!\n\n</details>\n\n### ğŸ¤— Models Downloads\n\n#### ğŸ”µ Atom Models\n\nThe Atom models, created jointly by the Chinese Llama Community and AtomEcho, rank in the top ten of the Chinese Large Language Model Evaluation List C-Eval (submission on August 21).\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/ceval.jpg\" alt=\"ceval\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n\n\n|  Category  | Model Name        | ğŸ¤—Model Loading Name                  | Download Link                                                 |\n| --------------- | --------------- | ------------------------------ | ------------------------------------------------------------ |\n|  Pretrained  | Atom-7B  | FlagAlpha/Atom-7B  | [HuggingFace](https://huggingface.co/FlagAlpha/Atom-7B) \\| [ModelScope](https://modelscope.cn/models/FlagAlpha/Atom-7B) \\| [WiseModel](https://wisemodel.cn/models/FlagAlpha/Atom-7B) |\n|  Chat  | Atom-7B-Chat  | FlagAlpha/Atom-7B-Chat  | [HuggingFace](https://huggingface.co/FlagAlpha/Atom-7B-Chat) \\| [ModelScope](https://modelscope.cn/models/FlagAlpha/Atom-7B-Chat) \\| [WiseModel](https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat) |\n\n\nThe Atom series includes Atom-1B, Atom-7B and Atom-13B, with continuous optimization of Chinese language proficiency based on Llama2. Atom-7B and Atom-7B-Chat are fully open source and available for commercial use. You can obtain the models on the [Hugging Face](https://huggingface.co/FlagAlpha) repository. Details are available in [Atom-7B Download](#atom-chinese-pretrained-model-based-on-llama2).\n\nAtom models have the following optimizations for Chinese:\n\n###### Large-scale Chinese Data Pretraining\n\nAtom models are continually pretrained using a large amount of Chinese data, including encyclopedias, books, blogs, news, announcements, novels, financial data, legal data, medical data, code data, professional paper data, and Chinese natural language processing competition datasets. See [ğŸ“ Data Sources](#-data-sources) for details.\n\nThe massive data is filtered, scored, and deduplicated, resulting in high-quality Chinese data exceeding 1T tokens, continuously added to the training iterations.\n\n###### More Efficient Chinese Vocabulary\n\nTo improve the efficiency of Chinese text processing, we optimized the vocabulary of the Llama2 model. First, based on several hundred gigabytes of Chinese text, we expanded the word library to 65,000 words on the basis of the model's vocabulary. Our improvements increased the Chinese encoding/decoding speed by about 350% according to tests. Additionally, we expanded the coverage of the Chinese character set, including all emoji symbols ğŸ˜Š. This makes generating articles with emoji symbols more efficient.\n\n###### Adaptive Context Expansion\n\nAtom large models support a default context of 4K. Through position interpolation (PI) and Neural Tangent Kernel (NTK) methods, the context length can be expanded to 32K after fine-tuning.\n\n###### ğŸ“ Chinese Data\n\nWe optimized the Chinese capabilities of Llama2 using the following data:\n\n| Type                                                       | Description                                                   |\n| ---------------------------------------------------------- | ------------------------------------------------------------ |\n| Web Data                                                   | Publicly available web data on the Internet, selecting deduplicated high-quality Chinese data involving encyclopedias, books, blogs, news, announcements, novels, etc. |\n| [Wikipedia](https://github.com/goldsmith/Wikipedia)        | Chinese Wikipedia data                                        |\n| [Wudao](https://github.com/BAAI-WuDao/Model)               | 200G of Chinese Wudao open-source data                         |\n| [Clue](https://github.com/CLUEbenchmark/CLUEDatasetSearch) | High-quality Chinese long-text data cleaned from Clue's open Chinese pretraining data |\n| Competition Datasets                                       | About 150 Chinese natural language processing multi-task competition datasets in recent years |\n| [MNBVC](https://github.com/esbatmop/MNBVC)                 | Some datasets cleaned from MNBVC                              |\n\n**If you have high-quality datasets, we would greatly appreciate it if you could provide them to us! ğŸ’•ğŸ’•**\n\n\n#### Meta Official Llama2 Models\n\n<details lang=\"en\">\n\nThe Llama2 pretrained models include 7B, 13B, and 70B versions. The Llama2-Chat model is fine-tuned based on the pretrained models and has enhanced conversational capabilities.\n\n|  Category  | Model Name   | ğŸ¤—Model Loading Name             | Download Link                                                 |\n|  ----------  | ---------- | ------------------------- | --------------------- |\n|  Pretrained  | Llama2-7B  | meta-llama/Llama-2-7b-hf  | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-7b-hf) \\| [XunLei](https://pan.xunlei.com/s/VN_t0dUikZqOwt-5DZWHuMvqA1?pwd=66ep) |\n|  Pretrained  | Llama2-13B | meta-llama/Llama-2-13b-hf | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-13b-hf) \\| [XunLei](https://pan.xunlei.com/s/VN_yT_9G8xNOz0SDWQ7Mb_GZA1?pwd=yvgf) |\n|  Pretrained  | Llama2-70B | meta-llama/Llama-2-70b-hf | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-70b-hf) |\n|  Chat  | Llama2-7B-Chat  | meta-llama/Llama-2-7b-chat-hf  | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) \\| [XunLei](https://pan.xunlei.com/s/VN_oaV4BpKFgKLto4KgOhBcaA1?pwd=ufir) |\n|  Chat  | Llama2-13B-Chat | meta-llama/Llama-2-13b-chat-hf | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) \\| [XunLei](https://pan.xunlei.com/s/VN_yA-9G34NGL9B79b3OQZZGA1?pwd=xqrg) |\n|  Chat  | Llama2-70B-Chat | meta-llama/Llama-2-70b-chat-hf | [HuggingFace](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) \\| [XunLei](https://pan.xunlei.com/s/VNa_vCGzCy3h3N7oeFXs2W1hA1?pwd=uhxh#) |\n| Code  | CodeLlama-7b    |   meta-llama/Llama-2-70b-chat-hf              | [XunLei](https://pan.baidu.com/s/1cIPzdNywWLvQI7_2QanOEQ?pwd=zfwi) |\n| Code  | CodeLlama-7b-Python    |   meta-llama/Llama-2-70b-chat-hf              | [XunLei](https://pan.baidu.com/s/1liY8klGoDagYbpw-g-oFag?pwd=i952) |\n| Code  | CodeLlama-7b-Instruct    |   meta-llama/Llama-2-70b-chat-hf              | [XunLei](https://pan.baidu.com/s/108o9_DT2E_vfSGtOnDCQVw?pwd=zkt9) |\n| Code  | CodeLlama-13b    |   meta-llama/Llama-2-70b-chat-hf              | [XunLei](https://pan.baidu.com/s/1lLaeHv0XEBv0iiZzI1dpnw?pwd=qn99) |\n| Code  | CodeLlama-13b-Python    |   meta-llama/Llama-2-70b-chat-hf              | [XunLei](https://pan.baidu.com/s/1OLVfvZS_oqL3oqMKwsI87w?pwd=a78k) |\n| Code  | CodeLlama-13b-Instruct    |   meta-llama/Llama-2-70b-chat-hf              | [XunLei](https://pan.baidu.com/s/1HyxJl4w8wElgkZRh2ATrXQ?pwd=seg6) |\n| Code  | CodeLlama-34b    |   meta-llama/Llama-2-70b-chat-hf              | [XunLei](https://pan.baidu.com/s/1vEw0pFgIkctPUN4_5_6pIQ?pwd=q8eu) |\n\nMeta officially released Code Llama on August 24, 2023, which is a fine-tuned version of Llama2 based on code data. It provides three versions with different functionalities: Base Model (Code Llama), Python-specific Model (Code Llama - Python), and Instruction-following Model (Code Llama - Instruct), each available in 7B, 13B, and 34B parameter sizes. The capabilities of different models are summarized in the following table:\n\n|  Model Category         |        Model Name         | Code Completion | Code Fill | Instruction Programming |\n|-----------------------|------------------------|------|------|------|\n| Code Llama            | CodeLlama-7b           | âœ…    | âœ…    | âŒ    |\n|                       | CodeLlama-13b          | âœ…    | âœ…    | âŒ    |\n|                       | CodeLlama-34b          | âœ…    | âŒ    | âŒ    |\n| Code Llama - Python   | CodeLlama-7b-Python    | âœ…    | âŒ    | âŒ    |\n|                       | CodeLlama-13b-Python   | âœ…    | âŒ    | âŒ    |\n|                       | CodeLlama-34b-Python   | âœ…    | âŒ    | âŒ    |\n| Code Llama - Instruct | CodeLlama-7b-Instruct  | âŒ    | âœ…    | âœ…    |\n|                       | CodeLlama-13b-Instruct | âŒ    | âœ…    | âœ…    |\n|                       | CodeLlama-34b-Instruct | âŒ    | âŒ    | âœ…    |\n\nWe provide a [domestic download link for Code Llama](#-latest-downloads-of-llama2-in-china) and an online experience link at [llama.family](https://llama.family/). For detailed information on Code Llama, refer to the official GitHub repository [codellama](https://github.com/facebookresearch/codellama).\n\n</details>\n\n#### Fine-tuned Chinese Models Based on Llama2\n\nWe fine-tuned the Llama2-Chat model based on a Chinese instruction dataset, enhancing its Chinese conversational abilities. LoRA parameters and merged parameters with the base model have been uploaded to [Hugging Face](https://huggingface.co/FlagAlpha) and currently include models for 7B and 13B.\n\n|  Category  | Model Name   | ğŸ¤—Model Loading Name             | Base Model Version |    Download Link                                                 |\n|  ----------  | ---------- | ------------- |  ----------------- | ------------------- |\n|  Merged Parameters | Llama2-Chinese-7b-Chat | FlagAlpha/Llama2-Chinese-7b-Chat  |    meta-llama/Llama-2-7b-chat-hf       |[HuggingFace](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat)  |\n|  Merged Parameters | Llama2-Chinese-13b-Chat | FlagAlpha/Llama2-Chinese-13b-Chat|     meta-llama/Llama-2-13b-chat-hf     |[HuggingFace](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat) |\n|  LoRA Parameters | Llama2-Chinese-7b-Chat-LoRA  | FlagAlpha/Llama2-Chinese-7b-Chat-LoRA  |     meta-llama/Llama-2-7b-chat-hf      |[HuggingFace](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat-LoRA) |\n|  LoRA Parameters | Llama2-Chinese-13b-Chat-LoRA | FlagAlpha/Llama2-Chinese-13b-Chat-LoRA |     meta-llama/Llama-2-13b-chat-hf     |[HuggingFace](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-LoRA) |\n\n\n### ğŸŒŸ Community Source\nThe abundance of community resources is a crucial guarantee for community development, covering various aspects including but not limited to the following four: computing power, data, forums, and applications. The positive development and full utilization of these aspects will provide community members with more opportunities and support, driving the entire community towards a more prosperous direction. More details in [llama.family](https://llama.family/).\n\n<details lang=\"en\">\n#### ğŸ’» GPU Source\n\n- Provide computing power resources at below-market prices, usable for various computing tasks such as training and inference of deep learning models.\n- Offer exclusive online inference services for community members, enabling users to quickly and effectively perform inference operations on models.\n- Provide one-click online fine-tuning services, allowing users to conveniently fine-tune models to adapt to different tasks and data.\n\n#### ğŸ“Š Data Source\n- Open up abundant training data resources covering multiple domains and industries, providing ample data support for model training.\n- Provide high-quality, diverse datasets to meet the needs of different users, supporting data sharing and exchange to facilitate the full utilization of data resources.\n\n#### ğŸ’¬ Discussion\n- The community forum provides a platform for community members to engage in online discussions and exchange technical issues.\n- On the forum, users can share experiences, ask questions, and provide answers, fostering technical exchange and collaboration.\n- The forum can also host regular online events, seminars, etc., to enhance connections and understanding among community members.\n\n#### ğŸ“± Product\n- Offer free promotion spaces for showcasing applications, allowing developers to fully present their apps to community members.\n- Provide promotional assistance, including but not limited to publicity campaigns, user guidance, etc., to help apps gain more exposure and users.\n- Through the community platform, provide collaboration opportunities for outstanding apps, promoting cooperation and communication among app developers, collectively driving the development and growth of applications.\n\n</details>\n\n## ğŸ“Œ How to use Llama Model?\n\n### Setup of Llama3\n\n### Setup of Llama2\n\n### Simple Setup\nYou can choose a learning path to start using the Llama series models. It is recommended to use the Chinese pre-trained dialogue model for better support of Chinese language effects.\n\n#### Simple Setup: Anaconda\n\n##### Step 0: Prerequisites\n- Make sure that Python version 3.10 or higher is installed.\n\n##### Step 1: Prepare the environment\nIf you need to set up the environment and install required packages, run the following command.\n```bash\ngit clone https://github.com/LlamaFamily/Llama-Chinese.git\ncd Llama-Chinese\npip install -r requirements.txt\n```\n\n##### Step 2: Download Model\nYou can download the Atom-7B-Chat model from the following source.\n- [HuggingFace](https://huggingface.co/FlagAlpha)\n- [ModelScope](https://modelscope.cn/organization/FlagAlpha)\n- [WideModel](https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat)\n\n###### Step 3ï¼šInfer\n1. Create a file named quick_start.py and copy the following content into the file.\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice_map = \"cuda:0\" if torch.cuda.is_available() else \"auto\"\nmodel = AutoModelForCausalLM.from_pretrained('FlagAlpha/Atom-7B-Chat',device_map=device_map,torch_dtype=torch.float16,load_in_8bit=True,trust_remote_code=True,use_flash_attention_2=True)\nmodel =model.eval()\ntokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Atom-7B-Chat',use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\ninput_ids = tokenizer(['<s>Human: ä»‹ç»ä¸€ä¸‹ä¸­å›½\\n</s><s>Assistant: '], return_tensors=\"pt\",add_special_tokens=False).input_ids\nif torch.cuda.is_available():\n  input_ids = input_ids.to('cuda')\ngenerate_input = {\n    \"input_ids\":input_ids,\n    \"max_new_tokens\":512,\n    \"do_sample\":True,\n    \"top_k\":50,\n    \"top_p\":0.95,\n    \"temperature\":0.3,\n    \"repetition_penalty\":1.3,\n    \"eos_token_id\":tokenizer.eos_token_id,\n    \"bos_token_id\":tokenizer.bos_token_id,\n    \"pad_token_id\":tokenizer.pad_token_id\n}\ngenerate_ids  = model.generate(**generate_input)\ntext = tokenizer.decode(generate_ids[0])\nprint(text)\n```\n2. run quick_start.py \n```bash\npython quick_start.py\n```\n\n\n#### Simple Setup: Docker\nFor details, refer to: [Docker Deployment](https://github.com/LlamaFamily/Llama-Chinese/blob/main/docs/chat_gradio_guide.md)\n\nStep 1: Prepare the Docker image and launch [chat_gradio.py](../examples/chat_gradio.py) through a Docker container.\n```bash\ngit clone https://github.com/LlamaFamily/Llama-Chinese.git\n\ncd Llama-Chinese\n\ndocker build -f docker/Dockerfile -t flagalpha/llama2-chinese:gradio .\n```\n\nStep 2: Start chat_gradio through Docker-compose.\n```bash\ncd Llama-Chinese/docker\ndocker-compose up -d --build\n```\n\n#### Simple Setup: llama.cpp\nDetails: [llama.cpp](https://github.com/LlamaFamily/Llama-Chinese/blob/main/inference-speed/CPU/ggml/README.md)\n\n\n##### Simple Setup: gradio\nBuilt on Gradio, the Q&A interface implements fluid output. Copy the following code into the console to run. The code below uses the Atom-7B model as an example, <font color=\"#006600\">simply modify the model name in the code for different models ğŸ˜Š</font><br/>\n\n```\npython examples/chat_gradio.py --model_name_or_path FlagAlpha/Atom-7B-Chat\n```\n\n##### Simple Setup: API\nUse FastChat to build an inference service interface consistent with OpenAI.\n\n<details lang=\"en\">\n\n###### step 0ï¼šPrerequisites\nInstall fastchat\n```bash\npip3 install \"fschat[model_worker,webui]\"\n```\n###### step 1ï¼šRun Restful API\nOpen three terminals and execute the following three commands respectively.\n\n- Fist start controler\n```bash\npython3 -m fastchat.serve.controller \\\n--host localhost \\\n--port 21001\n```\n\n- Start Model\n```bash\nCUDA_VISIBLE_DEVICES=\"0\" python3 -m fastchat.serve.model_worker --model-path /path/Atom-7B-Chat \\\n--host localhost \\\n--port 21002 \\\n--worker-address \"http://localhost:21002\" \\\n--limit-worker-concurrency 5 \\\n--stream-interval 2 \\\n--gpus \"1\" \\\n--load-8bit\n```\n- Start RESTful API Service\n```bash\npython3 -m fastchat.serve.openai_api_server \\\n--host localhost \\\n--port 21003 \\\n--controller-address http://localhost:21001\n```\n\n###### step 2ï¼štest api service\nExecute the Python code below to test the API service deployed above.\n```python\n# coding=utf-8\nimport json\nimport time\nimport urllib.request\nimport sys\nimport requests\n\ndef test_api_server(input_text):\n    header = {'Content-Type': 'application/json'}\n\n    data = {\n          \"messages\": [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": input_text}],\n          \"temperature\": 0.3, \n          \"top_p\" : 0.95, \n          \"max_tokens\": 512, \n          \"model\": \"LLama2-Chinese-13B\",\n          \"stream\" : False,\n          \"n\" : 1,\n          \"best_of\": 1, \n          \"presence_penalty\": 1.2, \n          \"frequency_penalty\": 0.2,           \n          \"top_k\": 50, \n          \"use_beam_search\": False, \n          \"stop\": [], \n          \"ignore_eos\" :False,\n          \"logprobs\": None\n    }\n    response = requests.post(\n        url='http://127.0.0.1:21003/v1/chat/completions',\n        headers=header,\n        data=json.dumps(data).encode('utf-8')\n    )\n\n    result = None\n    try:\n        result = json.loads(response.content)\n        print(json.dumps(data, ensure_ascii=False, indent=2))\n        print(json.dumps(result, ensure_ascii=False, indent=2))\n\n    except Exception as e:\n        print(e)\n\n    return result\n\nif __name__ == \"__main__\":\n    test_api_server(\"å¦‚ä½•å»åŒ—äº¬?\")\n```\n</details>\n\n\n#### ğŸ¤– Model Pretraining\nWhile the pretraining data for Llama2 has doubled compared to the first generation LLaMA, the proportion of Chinese pretraining data is still very low, accounting for only 0.13%. This results in a relatively weak Chinese capability for the original Llama2. To enhance the model's Chinese capability, two approaches can be adopted: fine-tuning and pretraining.\n\n- Fine-tuning requires fewer computational resources and can quickly create a prototype of a Chinese Llama. However, its drawback is evident â€“ it can only leverage the existing Chinese capabilities of the base model. Due to the limited amount of Chinese training data for Llama2, the potential improvement is also restricted, addressing the symptoms rather than the root cause.\n\n- Pretraining based on large-scale Chinese corpora involves high costs, requiring not only large-scale high-quality Chinese data but also substantial computational resources. However, the advantage is clear â€“ it optimizes the Chinese capability from the model's foundational layers, achieving a fundamental improvement, injecting robust Chinese capabilities into the core of the large model.\n\nWe provide the pretraining code for the Llama model to the community, along with [Chinese test data](https://github.com/LlamaFamily/Llama-Chinese/tree/main/data). More data can be found in [Chinese Data](#-chinese-data). The specific code and configurations are as follows:\n\n- Model pretraining script: [train/pretrain/pretrain.sh](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/pretrain.sh)\n- Pretraining implementation code: [train/pretrain/pretrain_clm.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/pretrain_clm.py)\n- [DeepSpeed](https://github.com/microsoft/DeepSpeed) acceleration:\n  - For single-card training, ZeRO-2 can be used. See parameters in [train/pretrain/ds_config_zero2.json](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/ds_config_zero2.json).\n  - For multi-card training, ZeRO-3 can be used. See parameters in [train/pretrain/ds_config_zero3.json](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/ds_config_zero3.json).\n- Training effectiveness metrics: [train/pretrain/accuracy.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/pretrain/accuracy.py)\n\n#### ğŸ’¡ Model Fine-Tuning\n\nThis repository provides both LoRA fine-tuning and full-parameter fine-tuning code. Detailed information about LoRA can be found in the paper \"[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\" and the Microsoft GitHub repository [LoRA](https://github.com/microsoft/LoRA).\n\n##### Step1: Environment Setup\n\nInstall the necessary environment dependencies according to [requirements.txt](https://github.com/LlamaFamily/Llama-Chinese/blob/main/requirements.txt).\n\n##### Step2: Data Preparation\n\nIn the data directory, there is a sample data for the model's SFT:\n- Training data: [data/train_sft.csv](https://github.com/LlamaFamily/Llama-Chinese/blob/main/data/train_sft.csv)\n- Validation data: [data/dev_sft.csv](https://github.com/LlamaFamily/Llama-Chinese/blob/main/data/dev_sft.csv)\n\nEach CSV file contains a \"text\" column, with each row representing a training example. Organize questions and answers in the model's input format, as shown below:\n```\n\"<s>Human: \"+question+\"\\n</s><s>Assistant: \"+answer\n```\nFor example,\n```\n<s>Human: Describe why the Earth is unique in one sentence.</s><s>Assistant: Because the Earth is currently the only known planet with existing life.</s>\n```\n\n##### Step3: Fine-tuning Scripts\n\n###### LoRA Fine-tuning\nLoRA fine-tuning script: [train/sft/finetune_lora.sh](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/sft/finetune_lora.sh). For details on LoRA fine-tuning implementation, refer to [train/sft/finetune_clm_lora.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/sft/finetune_clm_lora.py). Fine-tuning on a single machine with multiple cards can be achieved by modifying the \"--include localhost:0\" in the script.\n\n###### Full-parameter Fine-tuning\nFull-parameter fine-tuning script: [train/sft/finetune.sh](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/sft/finetune.sh). For details on full-parameter fine-tuning implementation, refer to [train/sft/finetune_clm.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/train/sft/finetune_clm.py).\n\n###### Step4: Load Fine-tuned Model\n\n###### LoRA Fine-tuning\nFor LoRA fine-tuned model parameters, see [Chinese Fine-Tuned Model based on Llama2](#chinese-fine-tuned-model-based-on-llama2). LoRA parameters need to be combined with base model parameters.\n\nUse [PEFT](https://github.com/huggingface/peft) to load both pretraining and fine-tuned model parameters. In the example code below, set \"base_model_name_or_path\" to the pretraining model's save path and \"finetune_model_path\" to the fine-tuned model's save path.\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\n\nfinetune_model_path = ''  # For example: 'FlagAlpha/Llama2-Chinese-7b-Chat-LoRA'\nconfig = PeftConfig.from_pretrained(finetune_model_path)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\ndevice_map = \"cuda:0\" if torch.cuda.is_available() else \"auto\"\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map=device_map, torch_dtype=torch.float16, load_in_8bit=True)\nmodel = PeftModel.from_pretrained(model, finetune_model_path, device_map={\"\": 0})\nmodel = model.eval()\ninput_ids = tokenizer(['<s>Human: Introduce Beijing\\n</s><s>Assistant: '], return_tensors=\"pt\", add_special_tokens=False).input_ids\nif torch.cuda.is_available():\n  input_ids = input_ids.to('cuda')\ngenerate_input = {\n    \"input_ids\": input_ids,\n    \"max_new_tokens\": 512,\n    \"do_sample\": True,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"temperature\": 0.3,\n    \"repetition_penalty\": 1.3,\n    \"eos_token_id\": tokenizer.eos_token_id,\n    \"bos_token_id\": tokenizer.bos_token_id,\n    \"pad_token_id\": tokenizer.pad_token_id\n}\ngenerate_ids = model.generate(**generate_input)\ntext = tokenizer.decode(generate_ids[0])\nprint(text)\n```\n###### Full-parameter Fine-tuning\nFor full-parameter fine-tuned models, use the same calling method as in Model Calling Code Example, just modify the model name or save path accordingly.\n\n\n\n#### ğŸ„ Model Quantization\nWe have quantized the parameters of the Chinese fine-tuned model to facilitate running with fewer computational resources. Currently, we have uploaded a 4-bit compressed version of the 13B Chinese fine-tuned model [FlagAlpha/Llama2-Chinese-13b-Chat](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat) as [FlagAlpha/Llama2-Chinese-13b-Chat-4bit](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-4bit) on [Hugging Face](https://huggingface.co/FlagAlpha). The specific calling method is as follows:\n\nEnvironmental requirements:\n```\npip install git+https://github.com/PanQiWei/AutoGPTQ.git\n```\n\n```python\nfrom transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\n\nmodel = AutoGPTQForCausalLM.from_quantized('FlagAlpha/Llama2-Chinese-13b-Chat-4bit', device=\"cuda:0\")\ntokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Llama2-Chinese-13b-Chat-4bit', use_fast=False)\ninput_ids = tokenizer(['<s>Human: How to land on Mars\\n</s><s>Assistant: '], return_tensors=\"pt\", add_special_tokens=False).input_ids.to('cuda')        \ngenerate_input = {\n    \"input_ids\": input_ids,\n    \"max_new_tokens\": 512,\n    \"do_sample\": True,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"temperature\": 0.3,\n    \"repetition_penalty\": 1.3,\n    \"eos_token_id\": tokenizer.eos_token_id,\n    \"bos_token_id\": tokenizer.bos_token_id,\n    \"pad_token_id\": tokenizer.pad_token_id\n}\ngenerate_ids = model.generate(**generate_input)\ntext = tokenizer.decode(generate_ids[0])\nprint(text)\n```\n\n#### ğŸš€ Inference Acceleration\nAs the parameter scale of large models continues to grow, improving model inference speed has become an important research direction with limited computational resources. Common inference acceleration frameworks include lmdeploy, FasterTransformer, vLLM, and JittorLLMs.\n\n##### TensorRT-LLM\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main) is developed by NVIDIA, written in C++/CUDA, and supports distributed inference. \n\nFor detailed inference documentation, visit: [inference-speed/GPU/TensorRT-LLM_example](https://github.com/LlamaFamily/Llama-Chinese/tree/main/inference-speed/GPU/TensorRT-LLM_example)\n\n##### vLLM\n[vLLM](https://github.com/vllm-project/vllm) is developed by the University of California, Berkeley, with its core technology being PageAttention. It achieves 24 times higher throughput compared to HuggingFace Transformers. Unlike FasterTransformer, vLLM is more user-friendly and does not require additional model conversion. It supports FP16 inference.\n\nFor detailed inference documentation, visit: [inference-speed/GPU/vllm_example](https://github.com/LlamaFamily/Llama-Chinese/blob/main/inference-speed/GPU/vllm_example/README.md)\n\n##### JittorLLMs\n[JittorLLMs](https://github.com/Jittor/JittorLLMs) is led by Non-ten Technology in collaboration with the Visual Media Research Center at Tsinghua University. It significantly reduces hardware requirements by 80% through a dynamic swap mechanism. Jittor framework, with zero-copy technology, reduces the loading overhead of large models by 40% compared to PyTorch. Moreover, automatic compilation optimization through meta-operators enhances computational performance by over 20%.\n\nFor detailed inference documentation, visit: [inference-speed/GPU/JittorLLMs](https://github.com/LlamaFamily/Llama-Chinese/blob/main/inference-speed/GPU/JittorLLMs_example/README.md)\n\n##### lmdeploy\n[lmdeploy](https://github.com/InternLM/lmdeploy/) is developed by the Shanghai AI Lab, using C++/CUDA for inference. It provides Python/gRPC/HTTP interfaces and a WebUI for inference, supporting tensor parallel distributed inference and FP16/weight int4/kv cache int8 quantization.\n\nFor detailed inference documentation, visit: [inference-speed/GPU/lmdeploy_example](https://github.com/LlamaFamily/Llama-Chinese/tree/main/inference-speed/GPU/lmdeploy_example)\n\n#### ğŸ’ª Extension Capabilities\n\nIn addition to continually enhancing the intrinsic qualities of large models, such as knowledge base, general understanding, logical reasoning, and imaginative capabilities, we are also actively expanding the extension capabilities of the large models. This includes features like knowledge base retrieval, computational tools, WolframAlpha integration, and software manipulation.\n\nWe have initially integrated the LangChain framework to facilitate the development of applications like document retrieval, question-answering bots, and intelligent agents based on the Llama2 model. For more information on LangChain, please refer to [LangChain](https://github.com/langchain-ai/langchain).\n\n##### LangChain\nFor a simplified implementation using the LangChain framework with the Llama2 LLM class, refer to [examples/llama2_for_langchain.py](https://github.com/LlamaFamily/Llama-Chinese/blob/main/examples/llama2_for_langchain.py). Here is a basic code snippet:\n\n```python\nfrom llama2_for_langchain import Llama2\n\n# Example using FlagAlpha/Atom-7B-Chat\nllm = Llama2(model_name_or_path='FlagAlpha/Atom-7B-Chat')\n\nwhile True:\n    human_input = input(\"Human: \")\n    response = llm(human_input)\n    print(f\"Llama2: {response}\")\n```\n\n\n##### ğŸ¥‡ Model Evaluation\n\n###### Llama3 Evaluation\n\n###### Llama2 Evaluation\n\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/llama_eval.jpeg\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n\nTo gain a clearer understanding of the Chinese question-answering capabilities of the Llama2 model, we selected a set of representative Chinese questions for testing. The tested models include Meta's publicly available versions, namely, Llama2-7B-Chat and Llama2-13B-Chat, without any fine-tuning or training. The test questions were curated from [AtomBulb](https://github.com/AtomEcho/AtomBulb), totaling 95 questions covering eight major categories: general knowledge, language understanding, creative ability, logical reasoning, code programming, work skills, tool usage, and personality traits.\n\nThe prompt used during testing is as follows, for example, for the question \"List 5 methods to improve sleep quality\":\n\n```plaintext\n[INST] \n<<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. The answer always been translate into Chinese language.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\nThe answer always been translate into Chinese language.\n<</SYS>>\n\nList 5 methods to improve sleep quality\n[/INST]\n```\nThe test results for Llama2-7B-Chat can be found at[meta_eval_7B.md](assets/meta_eval_7B.md)ï¼Œand for Llama2-13B-Chat at [meta_eval_13B.md](assets/meta_eval_13B.md)ã€‚\n\nThrough our testing, we observed that Meta's original Llama2 Chat model generally has mediocre alignment with Chinese questions. In most cases, it fails to provide Chinese answers, or the responses are a mixture of Chinese and English. Therefore, it is crucial to train and fine-tune the Llama2 model on Chinese data. Our Chinese version of the Llama2 model is currently undergoing training and will be made available to the community in the near future.\n\n\n#### ğŸ“– Learning Resources\n\n##### Llama3\n\n##### Llama2\n###### Meta Official Introduction to [Llama2](https://ai.meta.com/llama)\nSince the release of Meta's first-generation LLaMA model, the Llama model family has thrived. Recently, Meta released the Llama2 version, which is open-source and commercially available, with significant updates in model and performance. Llama2 has models with parameter sizes of 7B, 13B, and 70B. Compared to LLaMA, Llama2's training data has reached 20 trillion tokens, and the context length has been upgraded from the previous 2048 to 4096, allowing it to understand and generate longer text. The Llama2 Chat model, fine-tuned based on 1 million human-labeled data, achieves results close to ChatGPT in English conversations.\n\n###### Llama-related Papers\n* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\n* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)\n* [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\n\n## ğŸ“Œ Others\n\n## ğŸ‰ Acknowledgments\nSpecial thanks to the AtomEcho team for their technical and resource support!\n\nThanks to @xzsGenius for contributions to the Llama2 Chinese community!\n\nThanks to the Z-Potentials community for supporting the Llama2 Chinese community!\n\n## ğŸ¤” Issue Feedback\nIf you have any issues, please submit them in the GitHub Issues. Before submitting a new issue, please check existing issues to see if your problem has already been addressed.\n\nPlease be polite when raising issues and contribute to building a harmonious discussion community.\n\nJoin the [Feishu Knowledge Base](https://chinesellama.feishu.cn/wiki/space/7257824476874768388?ccm_open_type=lark_wiki_spaceLink) to collaboratively build community documentation.\n\nJoin the WeChat group for discussions ğŸ˜ğŸ˜\n\n\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/wechat-new.jpeg\" alt=\"Wechat\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n\n<p align=\"center\" width=\"100%\">\n<img src=\"https://api.star-history.com/svg?repos=LlamaFamily/Llama-Chinese&type=Date\" alt=\"Wechat\" style=\"width: 100%; display: block; margin: auto;\">\n</p>\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference-speed",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2578125,
          "content": "torch==2.1.2\nbitsandbytes==0.42.0\naccelerate==0.27.2\nnumpy==1.26.4\ngekko==1.0.6\npandas\nscipy\nsentencepiece==0.2.0\ndatasets\nevaluate\npytest\npeft==0.8.2\ntransformers==4.39.0\ndeepspeed==0.14.0\nscikit-learn\ntorchvision\ntorchdata\ntorchaudio\ntensorboard\ngradio\npackaging"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}