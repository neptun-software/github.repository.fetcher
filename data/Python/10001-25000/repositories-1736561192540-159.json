{
  "metadata": {
    "timestamp": 1736561192540,
    "page": 159,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "assafelovic/gpt-researcher",
      "stars": 15617,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".cursorrules",
          "type": "blob",
          "size": 5.7978515625,
          "content": "# Project Overview This project, named GPT-Researcher, LLM based autonomous agent that conducts local and web research on any topic and generates a comprehensive report with citations, is built using Next.js and TypeScript. It integrates various libraries for their strenghts. Your primary goal is to help with Next.js app router patterns, TypeScript type safety, Tailwind CSS best practices, code quality standards, and Python/FastAPI backend optimizations. # Key URLs - Project Home Page: https://gptr.dev/ - GitHub Repository: https://github.com/assafelovic/gpt-researcher - Documentation: https://docs.gptr.dev/ # Project Structure - Frontend user interface built with Next.js, TypeScript, and Tailwind CSS in `/frontend` - Static FastAPI version for lightweight deployments - Next.js version for production use with enhanced features - Multi-agent research system using LangChain and LangGraph in `/backend/multi_agents` - Browser, Editor, Researcher, Reviewer, Revisor, Writer, and Publisher agents - Task configuration and agent coordination - Document processing using Unstructured and PyMuPDF in `/backend/document_processing` - PDF, DOCX, and web content parsing - Text extraction and preprocessing - Report generation using LangChain and Jinja2 templates in `/backend/report_generation` - Template-based report structuring - Dynamic content formatting - Multiple output formats in `/backend/output_formats` - PDF via md2pdf - Markdown via mistune - DOCX via python-docx - Format conversion utilities - Export functionality - GPT Researcher core functionality in `/gpt_researcher` - Web scraping and content aggregation - Research planning and execution - Source validation and tracking - Query processing and response generation - Testing infrastructure in `/tests` - Unit tests for individual components - Integration tests for agent interactions - End-to-end research workflow tests - Mock data and fixtures for testing # Language Model Configuration - Default model: gpt-4-turbo - Alternative models: gpt-3.5-turbo, claude-3-opus - Temperature settings for different tasks - Context window management - Token limit handling - Cost optimization strategies # Error Handling - Research failure recovery - API rate limiting - Network timeout handling - Invalid input management - Source validation errors - Report generation failures # Performance - Parallel processing strategies - Caching mechanisms - Memory management - Response streaming - Resource allocation - Query optimization # Development Workflow - Branch naming conventions - Commit message format - PR review process - Testing requirements - Documentation updates - Version control guidelines # API Documentation - REST endpoints - WebSocket events - Request/Response formats - Authentication methods - Rate limits - Error codes # Monitoring - Performance metrics - Error tracking - Usage statistics - Cost monitoring - Research quality metrics - User feedback tracking # Frontend Components - Static FastAPI version for lightweight deployments - Next.js version for production use with enhanced features # Backend Components - Multi-agent system architecture - Document processing pipeline - Report generation system - Output format handlers # Core Research Components - Web scraping and aggregation - Research planning and execution - Source validation - Query processing # Testing - Unit tests - Integration tests - End-to-end tests - Performance testing # Rule Violation Monitoring - Alert developer when changes conflict with project structure - Warn about deviations from coding standards - Flag unauthorized framework or library additions - Monitor for security and performance anti-patterns - Track API usage patterns that may violate guidelines - Report TypeScript strict mode violations - Identify accessibility compliance issues # Development Guidelines - Use TypeScript with strict mode enabled - Follow ESLint and Prettier configurations - Ensure components are responsive and accessible - Use Tailwind CSS for styling, following the project's design system - Minimize AI-generated comments, prefer self-documenting code - Follow React best practices and hooks guidelines - Validate all user inputs and API responses - Use existing components as reference implementations # Important Scripts - `npm run dev`: Start development server - `npm run build`: Build for production - `npm run test`: Run test suite - `python -m pytest`: Run Python tests - `docker-compose up`: Start all services - `docker-compose run gpt-researcher-tests`: Run test suite in container - `python -m uvicorn backend.server.server:app --host=0.0.0.0 --port=8000`: Start FastAPI server - `python -m uvicorn backend.server.server:app --reload`: Start FastAPI server with auto-reload for development - `python main.py`: Run the main application directly # AI Integration Guidelines - Prioritize type safety in all AI interactions - Follow LangChain and LangGraph best practices - Implement proper error handling for AI responses - Maintain context window limits - Handle rate limiting and API quotas - Validate AI outputs before processing - Log AI interactions for debugging # Lexicon - **GPT Researcher**: Autonomous research agent system - **Multi-Agent System**: Coordinated AI agents for research tasks - **Research Pipeline**: End-to-end research workflow - **Agent Roles**: Browser, Editor, Researcher, Reviewer, Revisor, Writer, Publisher - **Source Validation**: Verification of research sources - **Report Generation**: Process of creating final research output # Additional Resources - [Next.js Documentation](https://nextjs.org/docs) - [TypeScript Handbook](https://www.typescriptlang.org/docs/) - [Tailwind CSS Documentation](https://tailwindcss.com/docs) - [LangChain Documentation](https://python.langchain.com/docs/) - [FastAPI Documentation](https://fastapi.tiangolo.com/) - [Project Documentation](https://docs.gptr.dev/) End all your comments with a :-) symbol."
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0126953125,
          "content": ".git\noutput/\n"
        },
        {
          "name": ".env.example",
          "type": "blob",
          "size": 0.048828125,
          "content": "OPENAI_API_KEY=\nTAVILY_API_KEY=\nDOC_PATH=./my-docs"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.5263671875,
          "content": "#Ignore env containing secrets\n.env\n.venv\n.envrc\n\n#Ignore Virtual Env\nenv/\nvenv/\n.venv/\n\n# Other Environments\nENV/\nenv.bak/\nvenv.bak/\n\n#Ignore generated outputs\noutputs/\n*.lock\ndist/\ngpt_researcher.egg-info/\n\n#Ignore my local docs\nmy-docs/\n\n#Ignore pycache\n**/__pycache__/\n\n#Ignore mypy cache\n.mypy_cache/\nnode_modules\n.idea\n.DS_Store\n.docusaurus\nbuild\ndocs/build\n\n.vscode/launch.json\n.langgraph-data/\n.next/\npackage-lock.json\n\n#Vim swp files\n*.swp\n\n# Log files\nlogs/\n*.orig\n*.log\nserver_log.txt\n\n#Cursor Rules\n.cursorrules\nCURSOR_RULES.md"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.0419921875,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe, as members, contributors, and leaders, pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, sexual identity, or\norientation.\n\nWe commit to acting and interacting in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n- Demonstrating empathy and kindness toward others\n- Being respectful of differing opinions, viewpoints, and experiences\n- Giving and gracefully accepting constructive feedback\n- Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\n- Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n- The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n- Trolling, insulting or derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or email address, without their explicit permission\n- Other conduct that could reasonably be considered inappropriate in a professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior deemed inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that do not\nalign with this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies to all community spaces and also applies when\nan individual is officially representing the community in public spaces.\nExamples include using an official email address, posting via an official\nsocial media account, or acting as an appointed representative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n[Assaf.elovic@gmail.com](mailto:Assaf.elovic@gmail.com).\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period. This includes\navoiding interactions in community spaces and external channels like social media.\nViolating these terms may lead to a temporary or permanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any interaction or public\ncommunication with the community for a specified period. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of groups of individuals.\n\n**Consequence**: A permanent ban from any public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.7197265625,
          "content": "# Contributing to GPT Researcher\n\nFirst off, we'd like to welcome you and thank you for your interest and effort in contributing to our open-source project â¤ï¸. Contributions of all forms are welcomeâ€”from new features and bug fixes to documentation and more.\n\nWe are on a mission to build the #1 AI agent for comprehensive, unbiased, and factual research online, and we need your support to achieve this grand vision.\n\nPlease take a moment to review this document to make the contribution process easy and effective for everyone involved.\n\n## Reporting Issues\n\nIf you come across any issue or have an idea for an improvement, don't hesitate to create an issue on GitHub. Describe your problem in sufficient detail, providing as much relevant information as possible. This way, we can reproduce the issue before attempting to fix it or respond appropriately.\n\n## Contributing Code\n\n1. **Fork the repository and create your branch from `master`.**  \n   If itâ€™s not an urgent bug fix, branch from `master` and work on the feature or fix there.\n\n2. **Make your changes.**  \n   Implement your changes following best practices for coding in the project's language.\n\n3. **Test your changes.**  \n   Ensure that your changes pass all tests if any exist. If the project doesnâ€™t have automated tests, test your changes manually to confirm they behave as expected.\n\n4. **Follow the coding style.**  \n   Ensure your code adheres to the coding conventions used throughout the project, including indentation, accurate comments, etc.\n\n5. **Commit your changes.**  \n   Make your Git commits informative and concise. This is very helpful for others when they look at the Git log.\n\n6. **Push to your fork and submit a pull request.**  \n   When your work is ready and passes tests, push your branch to your fork of the repository and submit a pull request from there.\n\n7. **Pat yourself on the back and wait for review.**  \n   Your work is done, congratulations! Now sit tight. The project maintainers will review your submission as soon as possible. They might suggest changes or ask for improvements. Both constructive conversation and patience are key to the collaboration process.\n\n## Documentation\n\nIf you would like to contribute to the project's documentation, please follow the same steps: fork the repository, make your changes, test them, and submit a pull request.\n\nDocumentation is a vital part of any software. It's not just about having good code; ensuring that users and contributors understand what's going on, how to use the software, or how to contribute is crucial.\n\nWe're grateful for all our contributors, and we look forward to building the world's leading AI research agent hand-in-hand with you. Let's harness the power of open source and AI to change the world together!\n"
        },
        {
          "name": "CURSOR_RULES.md",
          "type": "blob",
          "size": 6.1025390625,
          "content": "> **Note**: This is a readable copy of the `.cursorrules` file maintained for legibility. The actual rules are implemented from the `.cursorrules` file in the root directory.\n\n# GPT-Researcher Cursor Rules\n\n## Project Overview\nThis project, named GPT-Researcher, is an LLM-based autonomous agent that conducts local and web research on any topic and generates a comprehensive report with citations. It is built using Next.js and TypeScript, integrating various libraries for their strengths.\n\nYour primary goal is to help with:\n- Next.js app router patterns\n- TypeScript type safety\n- Tailwind CSS best practices\n- Code quality standards\n- Python/FastAPI backend optimizations\n\n## Key URLs\n- Project Home Page: https://gptr.dev/\n- GitHub Repository: https://github.com/assafelovic/gpt-researcher\n- Documentation: https://docs.gptr.dev/\n\n## Project Structure\n- Frontend user interface built with Next.js, TypeScript, and Tailwind CSS in `/frontend`\n  - Static FastAPI version for lightweight deployments\n  - Next.js version for production use with enhanced features\n\n- Multi-agent research system using LangChain and LangGraph in `/backend/multi_agents`\n  - Browser, Editor, Researcher, Reviewer, Revisor, Writer, and Publisher agents\n  - Task configuration and agent coordination\n\n- Document processing using Unstructured and PyMuPDF in `/backend/document_processing`\n  - PDF, DOCX, and web content parsing\n  - Text extraction and preprocessing\n\n- Report generation using LangChain and Jinja2 templates in `/backend/report_generation`\n  - Template-based report structuring\n  - Dynamic content formatting\n\n- Multiple output formats in `/backend/output_formats`\n  - PDF via md2pdf\n  - Markdown via mistune\n  - DOCX via python-docx\n  - Format conversion utilities\n  - Export functionality\n\n- GPT Researcher core functionality in `/gpt_researcher`\n  - Web scraping and content aggregation\n  - Research planning and execution\n  - Source validation and tracking\n  - Query processing and response generation\n\n- Testing infrastructure in `/tests`\n  - Unit tests for individual components\n  - Integration tests for agent interactions\n  - End-to-end research workflow tests\n  - Mock data and fixtures for testing\n\n## Language Model Configuration\n- Default model: gpt-4-turbo\n- Alternative models: gpt-3.5-turbo, claude-3-opus\n- Temperature settings for different tasks\n- Context window management\n- Token limit handling\n- Cost optimization strategies\n\n## Error Handling\n- Research failure recovery\n- API rate limiting\n- Network timeout handling\n- Invalid input management\n- Source validation errors\n- Report generation failures\n\n## Performance\n- Parallel processing strategies\n- Caching mechanisms\n- Memory management\n- Response streaming\n- Resource allocation\n- Query optimization\n\n## Development Workflow\n- Branch naming conventions\n- Commit message format\n- PR review process\n- Testing requirements\n- Documentation updates\n- Version control guidelines\n\n## API Documentation\n- REST endpoints\n- WebSocket events\n- Request/Response formats\n- Authentication methods\n- Rate limits\n- Error codes\n\n## Monitoring\n- Performance metrics\n- Error tracking\n- Usage statistics\n- Cost monitoring\n- Research quality metrics\n- User feedback tracking\n\n## Frontend Components\n- Static FastAPI version for lightweight deployments\n- Next.js version for production use with enhanced features\n\n## Backend Components\n- Multi-agent system architecture\n- Document processing pipeline\n- Report generation system\n- Output format handlers\n\n## Core Research Components\n- Web scraping and aggregation\n- Research planning and execution\n- Source validation\n- Query processing\n\n## Testing\n- Unit tests\n- Integration tests\n- End-to-end tests\n- Performance testing\n\n## Rule Violation Monitoring\n- Alert developer when changes conflict with project structure\n- Warn about deviations from coding standards\n- Flag unauthorized framework or library additions\n- Monitor for security and performance anti-patterns\n- Track API usage patterns that may violate guidelines\n- Report TypeScript strict mode violations\n- Identify accessibility compliance issues\n\n## Development Guidelines\n- Use TypeScript with strict mode enabled\n- Follow ESLint and Prettier configurations\n- Ensure components are responsive and accessible\n- Use Tailwind CSS for styling, following the project's design system\n- Minimize AI-generated comments, prefer self-documenting code\n- Follow React best practices and hooks guidelines\n- Validate all user inputs and API responses\n- Use existing components as reference implementations\n\n## Important Scripts\n- `npm run dev`: Start development server\n- `npm run build`: Build for production\n- `npm run test`: Run test suite\n- `python -m pytest`: Run Python tests\n- `python -m uvicorn backend.server.server:app --host=0.0.0.0 --port=8000`: Start FastAPI server\n- `python -m uvicorn backend.server.server:app --reload`: Start FastAPI server with auto-reload for development\n- `python main.py`: Run the main application directly\n- `docker-compose up`: Start all services\n- `docker-compose run gpt-researcher-tests`: Run test suite in container\n\n## AI Integration Guidelines\n- Prioritize type safety in all AI interactions\n- Follow LangChain and LangGraph best practices\n- Implement proper error handling for AI responses\n- Maintain context window limits\n- Handle rate limiting and API quotas\n- Validate AI outputs before processing\n- Log AI interactions for debugging\n\n## Lexicon\n- **GPT Researcher**: Autonomous research agent system\n- **Multi-Agent System**: Coordinated AI agents for research tasks\n- **Research Pipeline**: End-to-end research workflow\n- **Agent Roles**: Browser, Editor, Researcher, Reviewer, Revisor, Writer, Publisher\n- **Source Validation**: Verification of research sources\n- **Report Generation**: Process of creating final research output\n\n## Additional Resources\n- [Next.js Documentation](https://nextjs.org/docs)\n- [TypeScript Handbook](https://www.typescriptlang.org/docs/)\n- [Tailwind CSS Documentation](https://tailwindcss.com/docs)\n- [LangChain Documentation](https://python.langchain.com/docs/)\n- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n- [Project Documentation](https://docs.gptr.dev/)\n\n_Note: End all your comments with a :-) symbol._ "
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.7529296875,
          "content": "# Stage 1: Browser and build tools installation\nFROM python:3.11.4-slim-bullseye AS install-browser\n\n# Install Chromium, Chromedriver, Firefox, Geckodriver, and build tools in one layer\nRUN apt-get update && \\\n    apt-get satisfy -y \"chromium, chromium-driver (>= 115.0)\" && \\\n    apt-get install -y --no-install-recommends firefox-esr wget build-essential && \\\n    wget https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux64.tar.gz && \\\n    tar -xvzf geckodriver-v0.33.0-linux64.tar.gz && \\\n    chmod +x geckodriver && \\\n    mv geckodriver /usr/local/bin/ && \\\n    rm geckodriver-v0.33.0-linux64.tar.gz && \\\n    chromium --version && chromedriver --version && \\\n    rm -rf /var/lib/apt/lists/*  # Clean up apt lists to reduce image size\n\n# Stage 2: Python dependencies installation\nFROM install-browser AS gpt-researcher-install\n\nENV PIP_ROOT_USER_ACTION=ignore\nWORKDIR /usr/src/app\n\n# Copy and install Python dependencies in a single layer to optimize cache usage\nCOPY ./requirements.txt ./requirements.txt\nCOPY ./multi_agents/requirements.txt ./multi_agents/requirements.txt\n\nRUN pip install --no-cache-dir -r requirements.txt && \\\n    pip install --no-cache-dir -r multi_agents/requirements.txt\n\n# Stage 3: Final stage with non-root user and app\nFROM gpt-researcher-install AS gpt-researcher\n\n# Create a non-root user for security\nRUN useradd -ms /bin/bash gpt-researcher && \\\n    chown -R gpt-researcher:gpt-researcher /usr/src/app\n\nUSER gpt-researcher\nWORKDIR /usr/src/app\n\n# Copy the rest of the application files with proper ownership\nCOPY --chown=gpt-researcher:gpt-researcher ./ ./\n\n# Expose the application's port\nEXPOSE 8000\n\n# Define the default command to run the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Procfile",
          "type": "blob",
          "size": 0.076171875,
          "content": "web: python -m uvicorn backend.server.server:app --host=0.0.0.0 --port=${PORT}"
        },
        {
          "name": "README-ja_JP.md",
          "type": "blob",
          "size": 12.12109375,
          "content": "<div align=\"center\">\n<!--<h1 style=\"display: flex; align-items: center; gap: 10px;\">\n  <img src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/a45bac7c-092c-42e5-8eb6-69acbf20dde5\" alt=\"Logo\" width=\"25\">\n  GPT Researcher\n</h1>-->\n<img src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3\" alt=\"Logo\" width=\"80\">\n\n\n####\n\n[![å…¬å¼ã‚µã‚¤ãƒˆ](https://img.shields.io/badge/å…¬å¼ã‚µã‚¤ãƒˆ-gptr.dev-blue?style=for-the-badge&logo=world&logoColor=white)](https://gptr.dev)\n[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)\n[![Discord Follow](https://img.shields.io/discord/1127851779011391548?style=for-the-badge&logo=discord&label=Chat%20on%20Discord)](https://discord.gg/QgZXvJAccX)\n\n[![PyPI version](https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&logoColor=white&style=flat)](https://badge.fury.io/py/gpt-researcher)\n![GitHub Release](https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&logo=github)\n[![Open In Colab](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&label=%20&style=flat&logoSize=40)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)\n[![Docker Image Version](https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&style=flat&logo=docker&logoColor=white&color=1D63ED)](https://hub.docker.com/r/gptresearcher/gpt-researcher)\n[![Twitter Follow](https://img.shields.io/twitter/follow/assaf_elovic?style=social)](https://twitter.com/assaf_elovic)\n\n[English](README.md) |\n[ä¸­æ–‡](README-zh_CN.md) |\n[æ—¥æœ¬èª](README-ja_JP.md) |\n[í•œêµ­ì–´](README-ko_KR.md)\n</div>\n\n# ğŸ” GPT Researcher\n\n**GPT Researcher ã¯ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚µãƒ¼ãƒã®ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸè‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚**\n\nã“ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€è©³ç´°ã§äº‹å®Ÿã«åŸºã¥ã„ãŸåã‚Šã®ãªã„ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã€é–¢é€£ã™ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã€ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã€ãŠã‚ˆã³ãƒ¬ãƒƒã‚¹ãƒ³ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚æœ€è¿‘ã® [Plan-and-Solve](https://arxiv.org/abs/2305.04091) ãŠã‚ˆã³ [RAG](https://arxiv.org/abs/2005.11401) è«–æ–‡ã«è§¦ç™ºã•ã‚Œã€GPT Researcher ã¯é€Ÿåº¦ã€æ±ºå®šè«–ã€ãŠã‚ˆã³ä¿¡é ¼æ€§ã®å•é¡Œã«å¯¾å‡¦ã—ã€åŒæœŸæ“ä½œã§ã¯ãªãä¸¦åˆ—åŒ–ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæ¥­ã‚’é€šã˜ã¦ã‚ˆã‚Šå®‰å®šã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨é«˜é€ŸåŒ–ã‚’æä¾›ã—ã¾ã™ã€‚\n\n**ç§ãŸã¡ã®ä½¿å‘½ã¯ã€AIã®åŠ›ã‚’æ´»ç”¨ã—ã¦ã€å€‹äººã‚„çµ„ç¹”ã«æ­£ç¢ºã§åã‚Šã®ãªã„äº‹å®Ÿã«åŸºã¥ã„ãŸæƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚**\n\n## ãªãœGPT Researcherãªã®ã‹ï¼Ÿ\n\n- æ‰‹å‹•ã®ç ”ç©¶ã‚¿ã‚¹ã‚¯ã§å®¢è¦³çš„ãªçµè«–ã‚’å½¢æˆã™ã‚‹ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã€é©åˆ‡ãªãƒªã‚½ãƒ¼ã‚¹ã¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã®ã«æ•°é€±é–“ã‹ã‹ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚\n- ç¾åœ¨ã®LLMã¯éå»ã®æƒ…å ±ã«åŸºã¥ã„ã¦è¨“ç·´ã•ã‚Œã¦ãŠã‚Šã€å¹»è¦šã®ãƒªã‚¹ã‚¯ãŒé«˜ãã€ç ”ç©¶ã‚¿ã‚¹ã‚¯ã«ã¯ã»ã¨ã‚“ã©å½¹ã«ç«‹ã¡ã¾ã›ã‚“ã€‚\n- ç¾åœ¨ã®LLMã¯çŸ­ã„ãƒˆãƒ¼ã‚¯ãƒ³å‡ºåŠ›ã«åˆ¶é™ã•ã‚Œã¦ãŠã‚Šã€é•·ãè©³ç´°ãªç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆï¼ˆ2,000èªä»¥ä¸Šï¼‰ã«ã¯ä¸ååˆ†ã§ã™ã€‚\n- Webæ¤œç´¢ã‚’å¯èƒ½ã«ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆChatGPT + Webãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãªã©ï¼‰ã¯ã€é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’è€ƒæ…®ã—ã€å ´åˆã«ã‚ˆã£ã¦ã¯è¡¨é¢çš„ã§åã£ãŸå›ç­”ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚\n- Webã‚½ãƒ¼ã‚¹ã®é¸æŠã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ç ”ç©¶ã‚¿ã‚¹ã‚¯ã®æ­£ã—ã„çµè«–ã‚’å°ãéš›ã«ãƒã‚¤ã‚¢ã‚¹ãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\n## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\nä¸»ãªã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€ã€Œãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã€ã¨ã€Œå®Ÿè¡Œã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã‚ã‚Šã€ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã¯ç ”ç©¶ã™ã‚‹è³ªå•ã‚’ç”Ÿæˆã—ã€å®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç”Ÿæˆã•ã‚ŒãŸå„ç ”ç©¶è³ªå•ã«åŸºã¥ã„ã¦æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’æ¢ã—ã¾ã™ã€‚æœ€å¾Œã«ã€ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã¯ã™ã¹ã¦ã®é–¢é€£æƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŠã‚ˆã³é›†ç´„ã—ã€ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚<br /> <br /> \nã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€ç ”ç©¶ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã™ã‚‹ãŸã‚ã« gpt-4o-mini ã¨ gpt-4oï¼ˆ128K ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã®ä¸¡æ–¹ã‚’æ´»ç”¨ã—ã¾ã™ã€‚å¿…è¦ã«å¿œã˜ã¦ãã‚Œãã‚Œã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã‚³ã‚¹ãƒˆã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚**å¹³å‡çš„ãªç ”ç©¶ã‚¿ã‚¹ã‚¯ã¯å®Œäº†ã™ã‚‹ã®ã«ç´„3åˆ†ã‹ã‹ã‚Šã€ã‚³ã‚¹ãƒˆã¯ç´„0.1ãƒ‰ãƒ«ã§ã™**ã€‚\n\n<div align=\"center\">\n<img align=\"center\" height=\"500\" src=\"https://cowriter-images.s3.amazonaws.com/architecture.png\">\n</div>\n\n\nè©³ç´°èª¬æ˜:\n* ç ”ç©¶ã‚¯ã‚¨ãƒªã¾ãŸã¯ã‚¿ã‚¹ã‚¯ã«åŸºã¥ã„ã¦ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚\n* ç ”ç©¶ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹å®¢è¦³çš„ãªæ„è¦‹ã‚’å½¢æˆã™ã‚‹ä¸€é€£ã®ç ”ç©¶è³ªå•ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n* å„ç ”ç©¶è³ªå•ã«å¯¾ã—ã¦ã€ä¸ãˆã‚‰ã‚ŒãŸã‚¿ã‚¹ã‚¯ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã™ã‚‹ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã¾ã™ã€‚\n* å„åé›†ã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€é–¢é€£æƒ…å ±ã«åŸºã¥ã„ã¦è¦ç´„ã—ã€ãã®ã‚½ãƒ¼ã‚¹ã‚’è¿½è·¡ã—ã¾ã™ã€‚\n* æœ€å¾Œã«ã€ã™ã¹ã¦ã®è¦ç´„ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŠã‚ˆã³é›†ç´„ã—ã€æœ€çµ‚çš„ãªç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n\n## ãƒ‡ãƒ¢\nhttps://github.com/assafelovic/gpt-researcher/assets/13554167/a00c89a6-a295-4dd0-b58d-098a31c40fda\n\n## ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«\n - [å‹•ä½œåŸç†](https://docs.gptr.dev/blog/building-gpt-researcher)\n - [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•](https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea)\n - [ãƒ©ã‚¤ãƒ–ãƒ‡ãƒ¢](https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8)\n\n## ç‰¹å¾´\n- ğŸ“ ç ”ç©¶ã€ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã€ãƒªã‚½ãƒ¼ã‚¹ã€ãƒ¬ãƒƒã‚¹ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\n- ğŸŒ å„ç ”ç©¶ã§20ä»¥ä¸Šã®Webã‚½ãƒ¼ã‚¹ã‚’é›†ç´„ã—ã€å®¢è¦³çš„ã§äº‹å®Ÿã«åŸºã¥ã„ãŸçµè«–ã‚’å½¢æˆ\n- ğŸ–¥ï¸ ä½¿ã„ã‚„ã™ã„Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆHTML/CSS/JSï¼‰ã‚’å«ã‚€\n- ğŸ” JavaScriptã‚µãƒãƒ¼ãƒˆä»˜ãã®Webã‚½ãƒ¼ã‚¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°\n- ğŸ“‚ è¨ªå•ãŠã‚ˆã³ä½¿ç”¨ã•ã‚ŒãŸWebã‚½ãƒ¼ã‚¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½è·¡\n- ğŸ“„ ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’PDFã€Wordãªã©ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n\n## ğŸ“– ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n\nå®Œå…¨ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼š\n\n- å…¥é–€ï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ç’°å¢ƒè¨­å®šã€ç°¡å˜ãªä¾‹ï¼‰\n- æ“ä½œä¾‹ï¼ˆãƒ‡ãƒ¢ã€çµ±åˆã€dockerã‚µãƒãƒ¼ãƒˆï¼‰\n- å‚è€ƒè³‡æ–™ï¼ˆAPIå®Œå…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰\n- Tavilyã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®çµ±åˆï¼ˆã‚³ã‚¢æ¦‚å¿µã®é«˜åº¦ãªèª¬æ˜ï¼‰\n\n## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ\n> **ã‚¹ãƒ†ãƒƒãƒ— 0** - Python 3.11 ä»¥é™ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚[ã“ã¡ã‚‰](https://www.tutorialsteacher.com/python/install-python)ã‚’å‚ç…§ã—ã¦ã€ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¬ã‚¤ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n<br />\n\n> **ã‚¹ãƒ†ãƒƒãƒ— 1** - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™\n\n```bash\n$ git clone https://github.com/assafelovic/gpt-researcher.git\n$ cd gpt-researcher\n```\n\n<br />\n\n> **ã‚¹ãƒ†ãƒƒãƒ—2** - ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™\n```bash\n$ pip install -r requirements.txt\n```\n<br />\n\n> **ã‚¹ãƒ†ãƒƒãƒ— 3** - OpenAI ã‚­ãƒ¼ã¨ Tavily API ã‚­ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹ã€ç›´æ¥ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™\n\n```bash\n$ export OPENAI_API_KEY={Your OpenAI API Key here}\n```\n```bash\n$ export TAVILY_API_KEY={Your Tavily API Key here}\n```\n\n- **LLMã«ã¯ã€[OpenAI GPT](https://platform.openai.com/docs/guides/gpt) ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™**ãŒã€[Langchain Adapter](https://python.langchain.com/docs/guides/adapters/openai) ãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹ä»–ã® LLM ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚’å«ã‚€ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚llm ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ config/config.py ã§å¤‰æ›´ã™ã‚‹ã ã‘ã§ã™ã€‚[ã“ã®ã‚¬ã‚¤ãƒ‰](https://python.langchain.com/docs/integrations/llms/) ã«å¾“ã£ã¦ã€LLM ã‚’ Langchain ã¨çµ±åˆã™ã‚‹æ–¹æ³•ã‚’å­¦ã‚“ã§ãã ã•ã„ã€‚\n- **æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã«ã¯ã€[Tavily Search API](https://app.tavily.com)ï¼ˆLLM ç”¨ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™**ãŒã€ä»–ã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é¸æŠã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚config/config.py ã§æ¤œç´¢ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ã€Œduckduckgoã€ã€ã€ŒgoogleAPIã€ã€ã€ŒgoogleSerpã€ã€ã€Œsearchapiã€ã€ã€Œsearxã€ã«å¤‰æ›´ã™ã‚‹ã ã‘ã§ã™ã€‚æ¬¡ã«ã€config.py ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ env API ã‚­ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™ã€‚\n- **æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã«ã€[OpenAI GPT](https://platform.openai.com/docs/guides/gpt) ãƒ¢ãƒ‡ãƒ«ã¨ [Tavily Search API](https://app.tavily.com) ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚**\n<br />\n\n> **ã‚¹ãƒ†ãƒƒãƒ— 4** - FastAPI ã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™\n\n```bash\n$ uvicorn main:app --reload\n```\n<br />\n\n> **ã‚¹ãƒ†ãƒƒãƒ— 5** - ä»»æ„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:8000 ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ãƒªã‚µãƒ¼ãƒã‚’æ¥½ã—ã‚“ã§ãã ã•ã„ï¼\n\nDocker ã®ä½¿ã„æ–¹ã‚„æ©Ÿèƒ½ã¨ã‚µãƒ¼ãƒ“ã‚¹ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.gptr.dev) ãƒšãƒ¼ã‚¸ã‚’ã”è¦§ãã ã•ã„ã€‚\n\n## ğŸš€ è²¢çŒ®\nç§ãŸã¡ã¯è²¢çŒ®ã‚’å¤§æ­“è¿ã—ã¾ã™ï¼èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€[è²¢çŒ®](CONTRIBUTING.md) ã‚’ã”è¦§ãã ã•ã„ã€‚\n\nç§ãŸã¡ã®[ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap) ãƒšãƒ¼ã‚¸ã‚’ç¢ºèªã—ã€ç§ãŸã¡ã®ä½¿å‘½ã«å‚åŠ ã™ã‚‹ã“ã¨ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€[Discord ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£](https://discord.gg/QgZXvJAccX) ã‚’é€šã˜ã¦ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚\n\n## âœ‰ï¸ ã‚µãƒãƒ¼ãƒˆ / ãŠå•ã„åˆã‚ã›\n- [ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³](https://discord.gg/spBgZmm3Xe)\n- ç§ãŸã¡ã®ãƒ¡ãƒ¼ãƒ«: support@tavily.com\n\n## ğŸ›¡ å…è²¬äº‹é …\n\nã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ŒGPT Researcherã€ã¯å®Ÿé¨“çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã®ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ãªãã€Œç¾çŠ¶ã®ã¾ã¾ã€æä¾›ã•ã‚Œã¾ã™ã€‚ç§ãŸã¡ã¯å­¦è¡“ç›®çš„ã®ãŸã‚ã«MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§ã‚³ãƒ¼ãƒ‰ã‚’å…±æœ‰ã—ã¦ã„ã¾ã™ã€‚ã“ã“ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å†…å®¹ã¯å­¦è¡“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ã¯ãªãã€å­¦è¡“è«–æ–‡ã‚„ç ”ç©¶è«–æ–‡ã§ã®ä½¿ç”¨ã‚’æ¨å¥¨ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n\nç§ãŸã¡ã®å®¢è¦³çš„ãªç ”ç©¶ä¸»å¼µã«å¯¾ã™ã‚‹è¦‹è§£ï¼š\n1. ç§ãŸã¡ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ä¸»ãªç›®çš„ã¯ã€ä¸æ­£ç¢ºãªäº‹å®Ÿã‚’æ¸›ã‚‰ã™ã“ã¨ã§ã™ã€‚ã©ã†ã‚„ã£ã¦è§£æ±ºã™ã‚‹ã®ã‹ï¼Ÿç§ãŸã¡ãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã‚µã‚¤ãƒˆãŒå¤šã‘ã‚Œã°å¤šã„ã»ã©ã€èª¤ã£ãŸãƒ‡ãƒ¼ã‚¿ã®å¯èƒ½æ€§ã¯ä½ããªã‚Šã¾ã™ã€‚å„ç ”ç©¶ã§20ã®æƒ…å ±ã‚’åé›†ã—ã€ãã‚Œã‚‰ãŒã™ã¹ã¦é–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã¯éå¸¸ã«ä½ã„ã§ã™ã€‚\n2. ç§ãŸã¡ã®ç›®æ¨™ã¯ãƒã‚¤ã‚¢ã‚¹ã‚’æ’é™¤ã™ã‚‹ã“ã¨ã§ã¯ãªãã€å¯èƒ½ãªé™ã‚Šãƒã‚¤ã‚¢ã‚¹ã‚’æ¸›ã‚‰ã™ã“ã¨ã§ã™ã€‚**ç§ãŸã¡ã¯ã“ã“ã§ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨ã—ã¦æœ€ã‚‚åŠ¹æœçš„ãªäººé–“ã¨æ©Ÿæ¢°ã®ç›¸äº’ä½œç”¨ã‚’æ¢æ±‚ã—ã¦ã„ã¾ã™**ã€‚\n3. ç ”ç©¶ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€äººã€…ã‚‚è‡ªåˆ†ãŒç ”ç©¶ã—ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã«å¯¾ã—ã¦ã™ã§ã«æ„è¦‹ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚Šã‚„ã™ã„ã§ã™ã€‚ã“ã®ãƒ„ãƒ¼ãƒ«ã¯å¤šãã®æ„è¦‹ã‚’åé›†ã—ã€åã£ãŸäººãŒæ±ºã—ã¦èª­ã¾ãªã„ã§ã‚ã‚ã†å¤šæ§˜ãªè¦‹è§£ã‚’å‡ç­‰ã«èª¬æ˜ã—ã¾ã™ã€‚\n\n**GPT-4 è¨€èªãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½¿ç”¨ã«ã‚ˆã‚Šé«˜é¡ãªè²»ç”¨ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„**ã€‚ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½¿ç”¨çŠ¶æ³ã¨é–¢é€£ã™ã‚‹è²»ç”¨ã‚’ç›£è¦–ãŠã‚ˆã³ç®¡ç†ã™ã‚‹è²¬ä»»ãŒã‚ã‚‹ã“ã¨ã‚’èªã‚ãŸã“ã¨ã«ãªã‚Šã¾ã™ã€‚OpenAI API ã®ä½¿ç”¨çŠ¶æ³ã‚’å®šæœŸçš„ã«ç¢ºèªã—ã€äºˆæœŸã—ãªã„æ–™é‡‘ãŒç™ºç”Ÿã—ãªã„ã‚ˆã†ã«å¿…è¦ãªåˆ¶é™ã‚„ã‚¢ãƒ©ãƒ¼ãƒˆã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚\n\n---\n\n<p align=\"center\">\n<a href=\"https://star-history.com/#assafelovic/gpt-researcher\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n  </picture>\n</a>\n</p>\n"
        },
        {
          "name": "README-ko_KR.md",
          "type": "blob",
          "size": 15.3583984375,
          "content": "<div align=\"center\">\n<!--<h1 style=\"display: flex; align-items: center; gap: 10px;\">\n  <img src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/a45bac7c-092c-42e5-8eb6-69acbf20dde5\" alt=\"Logo\" width=\"25\">\n  GPT Researcher\n</h1>-->\n<img src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3\" alt=\"Logo\" width=\"80\">\n\n\n####\n\n[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)\n[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)\n[![Discord Follow](https://img.shields.io/discord/1127851779011391548?style=for-the-badge&logo=discord&label=Chat%20on%20Discord)](https://discord.gg/QgZXvJAccX)\n\n[![PyPI version](https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&logoColor=white&style=flat)](https://badge.fury.io/py/gpt-researcher)\n![GitHub Release](https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&logo=github)\n[![Open In Colab](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&label=%20&style=flat&logoSize=40)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)\n[![Docker Image Version](https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&style=flat&logo=docker&logoColor=white&color=1D63ED)](https://hub.docker.com/r/gptresearcher/gpt-researcher)\n[![Twitter Follow](https://img.shields.io/twitter/follow/assaf_elovic?style=social)](https://twitter.com/assaf_elovic)\n\n[English](README.md) |\n[ä¸­æ–‡](README-zh_CN.md) |\n[æ—¥æœ¬èª](README-ja_JP.md) |\n[í•œêµ­ì–´](README-ko_KR.md)\n</div>\n\n# ğŸ” GPT Researcher\n\n**GPT ResearcherëŠ” ë‹¤ì–‘í•œ ì‘ì—…ì„ ëŒ€í•´ í¬ê´„ì ì¸ ì˜¨ë¼ì¸ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœ ììœ¨ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.**\n\nì´ ì—ì´ì „íŠ¸ëŠ” ì„¸ë¶€ì ì´ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ë©° í¸ê²¬ ì—†ëŠ” ì—°êµ¬ ë³´ê³ ì„œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë¦¬ì†ŒìŠ¤ì™€ ê°œìš”ì— ì´ˆì ì„ ë§ì¶˜ ë§ì¶¤í˜• ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.  ìµœê·¼ ë°œí‘œëœ [Plan-and-Solve](https://arxiv.org/abs/2305.04091) ë° [RAG](https://arxiv.org/abs/2005.11401) ë…¼ë¬¸ì—ì„œ ì˜ê°ì„ ë°›ì•„ GPT ResearcherëŠ” ì˜ëª»ëœ ì •ë³´, ì†ë„, ê²°ì •ë¡ ì  ì ‘ê·¼ ë°©ì‹, ì‹ ë¢°ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë™ê¸°í™” ì‘ì—…ì´ ì•„ë‹Œ ë³‘ë ¬ ì—ì´ì „íŠ¸ ì‘ì—…ì„ í†µí•´ ë” ì•ˆì •ì ì´ê³  ë¹ ë¥¸ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n**ìš°ë¦¬ì˜ ëª©í‘œëŠ” AIì˜ í˜ì„ í™œìš©í•˜ì—¬ ê°œì¸ê³¼ ì¡°ì§ì—ê²Œ ì •í™•í•˜ê³  í¸í–¥ ì—†ëŠ” ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.**\n\n## ì™œ GPT Researcherì¸ê°€?\n\n- ì§ì ‘ ìˆ˜í–‰í•˜ëŠ” ì—°êµ¬ ê³¼ì •ì€ ê°ê´€ì ì¸ ê²°ë¡ ì„ ë„ì¶œí•˜ëŠ” ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë©°, ì ì ˆí•œ ë¦¬ì†ŒìŠ¤ì™€ ì •ë³´ë¥¼ ì°¾ëŠ” ë° ëª‡ ì£¼ê°€ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- í˜„ì¬ì˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ê³¼ê±° ì •ë³´ì— ê¸°ë°˜í•´ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, í™˜ê° í˜„ìƒì´ ë°œìƒí•  ìœ„í—˜ì´ ë†’ì•„ ì—°êµ¬ ì‘ì—…ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n- í˜„ì¬ LLMì€ ì§§ì€ í† í° ì¶œë ¥ìœ¼ë¡œ ì œí•œë˜ë©°, 2,000ë‹¨ì–´ ì´ìƒì˜ ê¸¸ê³  ìì„¸í•œ ì—°êµ¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë°ëŠ” ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n- ì›¹ ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ì„œë¹„ìŠ¤(ì˜ˆ: ChatGPT ë˜ëŠ” Perplexity)ëŠ” ì œí•œëœ ë¦¬ì†ŒìŠ¤ì™€ ì½˜í…ì¸ ë§Œì„ ê³ ë ¤í•˜ì—¬ ê²½ìš°ì— ë”°ë¼ í”¼ìƒì ì´ê³  í¸í–¥ëœ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n- ì›¹ ì†ŒìŠ¤ë§Œì„ ì‚¬ìš©í•˜ë©´ ì—°êµ¬ ì‘ì—…ì—ì„œ ì˜¬ë°”ë¥¸ ê²°ë¡ ì„ ë„ì¶œí•  ë•Œ í¸í–¥ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ë°ëª¨\nhttps://github.com/user-attachments/assets/092e9e71-7e27-475d-8c4f-9dddd28934a3\n\n## ì•„í‚¤í…ì²˜\nì£¼ìš” ì•„ì´ë””ì–´ëŠ” \"í”Œë˜ë„ˆ\"ì™€ \"ì‹¤í–‰\" ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ, í”Œë˜ë„ˆëŠ” ì—°êµ¬í•  ì§ˆë¬¸ì„ ìƒì„±í•˜ê³ , ì‹¤í–‰ ì—ì´ì „íŠ¸ëŠ” ìƒì„±ëœ ê° ì—°êµ¬ ì§ˆë¬¸ì— ë”°ë¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ í”Œë˜ë„ˆëŠ” ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ í•„í„°ë§í•˜ê³  ì§‘ê³„í•˜ì—¬ ì—°êµ¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\n<br /> <br /> \nì—ì´ì „íŠ¸ëŠ” `gpt-4o-mini`ì™€ `gpt-4o`(128K ì»¨í…ìŠ¤íŠ¸)ë¥¼ í™œìš©í•˜ì—¬ ì—°êµ¬ ì‘ì—…ì„ ì™„ë£Œí•©ë‹ˆë‹¤. í•„ìš”ì— ë”°ë¼ ê°ê°ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤. **í‰ê·  ì—°êµ¬ ì‘ì—…ì€ ì•½ 2ë¶„ì´ ì†Œìš”ë˜ë©°, ë¹„ìš©ì€ ì•½ $0.005ì…ë‹ˆë‹¤.**.\n\n<div align=\"center\">\n<img align=\"center\" height=\"600\" src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527\">\n</div>\n\nêµ¬ì²´ì ìœ¼ë¡œ:\n* ì—°êµ¬ ì¿¼ë¦¬ ë˜ëŠ” ì‘ì—…ì„ ê¸°ë°˜ìœ¼ë¡œ ë„ë©”ì¸ë³„ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n* ì£¼ì–´ì§„ ì‘ì—…ì— ëŒ€í•´ ê°ê´€ì ì¸ ì˜ê²¬ì„ í˜•ì„±í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ì—°êµ¬ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n* ê° ì—°êµ¬ ì§ˆë¬¸ì— ëŒ€í•´ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‘ì—…ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n* ìˆ˜ì§‘ëœ ê° ë¦¬ì†ŒìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ìš”ì•½í•˜ê³  ì¶œì²˜ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\n* ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš”ì•½ëœ ëª¨ë“  ì •ë³´ë¥¼ í•„í„°ë§í•˜ê³  ì§‘ê³„í•˜ì—¬ ìµœì¢… ì—°êµ¬ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n\n## íŠœí† ë¦¬ì–¼\n - [ë™ì‘ì›ë¦¬](https://docs.gptr.dev/blog/building-gpt-researcher)\n - [ì„¤ì¹˜ë°©ë²•](https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea)\n - [ë¼ì´ë¸Œ ë°ëª¨](https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8)\n\n\n## ê¸°ëŠ¥\n- ğŸ“ ë¡œì»¬ ë¬¸ì„œ ë° ì›¹ ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬, ê°œìš”, ë¦¬ì†ŒìŠ¤ ë° í•™ìŠµ ë³´ê³ ì„œ ìƒì„±\n- ğŸ“œ 2,000ë‹¨ì–´ ì´ìƒì˜ ê¸¸ê³  ìƒì„¸í•œ ì—°êµ¬ ë³´ê³ ì„œ ìƒì„± ê°€ëŠ¥\n- ğŸŒ ì—°êµ¬ë‹¹ 20ê°œ ì´ìƒì˜ ì›¹ ì†ŒìŠ¤ë¥¼ ì§‘ê³„í•˜ì—¬ ê°ê´€ì ì´ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ê²°ë¡  ë„ì¶œ\n- ğŸ–¥ï¸ ê²½ëŸ‰ HTML/CSS/JSì™€ í”„ë¡œë•ì…˜ìš© (NextJS + Tailwind) UX/UI í¬í•¨\n- ğŸ” ìë°”ìŠ¤í¬ë¦½íŠ¸ ì§€ì› ì›¹ ì†ŒìŠ¤ ìŠ¤í¬ë˜í•‘ ê¸°ëŠ¥\n- ğŸ“‚ ì—°êµ¬ ê³¼ì •ì—ì„œ ë§¥ë½ê³¼ ë©”ëª¨ë¦¬ ì¶”ì  ë° ìœ ì§€\n- ğŸ“„ ì—°êµ¬ ë³´ê³ ì„œë¥¼ PDF, Word ë“±ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° ì§€ì›\n\n## ğŸ“– ë¬¸ì„œ\n\nì „ì²´ ë¬¸ì„œ(ì„¤ì¹˜, í™˜ê²½ ì„¤ì •, ê°„ë‹¨í•œ ì˜ˆì‹œ)ë¥¼ ë³´ë ¤ë©´ [ì—¬ê¸°](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n\n- ì‹œì‘í•˜ê¸° (ì„¤ì¹˜, í™˜ê²½ ì„¤ì •, ê°„ë‹¨í•œ ì˜ˆì‹œ)\n- ë§ì¶¤ ì„¤ì • ë° êµ¬ì„±\n- ì‚¬ìš© ë°©ë²• ì˜ˆì‹œ (ë°ëª¨, í†µí•©, ë„ì»¤ ì§€ì›)\n- ì°¸ê³ ìë£Œ (ì „ì²´ API ë¬¸ì„œ)\n\n## âš™ï¸ ì‹œì‘í•˜ê¸°\n### ì„¤ì¹˜\n> **1ë‹¨ê³„** - Python 3.11 ë˜ëŠ” ê·¸ ì´ìƒì˜ ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”. [ì—¬ê¸°](https://www.tutorialsteacher.com/python/install-python)ë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹¨ê³„ë³„ ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.\n\n> **2ë‹¨ê³„** - í”„ë¡œì íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  í•´ë‹¹ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì„¸ìš”.\n\n```bash\ngit clone https://github.com/assafelovic/gpt-researcher.git\ncd gpt-researcher\n```\n\n> **3ë‹¨ê³„** - ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”: ì§ì ‘ exportí•˜ê±°ë‚˜ `.env` íŒŒì¼ì— ì €ì¥í•˜ì„¸ìš”.\n\nLinux/Windowsì—ì„œ ì„ì‹œ ì„¤ì •ì„ í•˜ë ¤ë©´ export ë°©ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”:\n\n```bash\nexport OPENAI_API_KEY={OpenAI API í‚¤ ì…ë ¥}\nexport TAVILY_API_KEY={Tavily API í‚¤ ì…ë ¥}\n```\n\në” ì˜êµ¬ì ì¸ ì„¤ì •ì„ ì›í•œë‹¤ë©´, í˜„ì¬ì˜ `gpt-researcher` ë””ë ‰í† ë¦¬ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  í™˜ê²½ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (export ì—†ì´).\n\n- ê¸°ë³¸ LLMì€ [GPT](https://platform.openai.com/docs/guides/gpt)ì´ì§€ë§Œ, `claude`, `ollama3`, `gemini`, `mistral` ë“± ë‹¤ë¥¸ LLMë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLM ì œê³µìë¥¼ ë³€ê²½í•˜ëŠ” ë°©ë²•ì€ [LLMs ë¬¸ì„œ](https://docs.gptr.dev/docs/gpt-researcher/llms/llms)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ì´ í”„ë¡œì íŠ¸ëŠ” OpenAI GPT ëª¨ë¸ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n- ê¸°ë³¸ ê²€ìƒ‰ê¸°ëŠ” [Tavily](https://app.tavily.com)ì´ì§€ë§Œ, `duckduckgo`, `google`, `bing`, `searchapi`, `serper`, `searx`, `arxiv`, `exa` ë“±ì˜ ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì œê³µìë¥¼ ë³€ê²½í•˜ëŠ” ë°©ë²•ì€ [ê²€ìƒ‰ê¸° ë¬¸ì„œ](https://docs.gptr.dev/docs/gpt-researcher/retrievers)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n\n### ë¹ ë¥¸ ì‹œì‘\n\n> **1ë‹¨ê³„** - í•„ìš”í•œ ì¢…ì†ì„± ì„¤ì¹˜\n\n```bash\npip install -r requirements.txt\n```\n\n> **2ë‹¨ê³„** - FastAPIë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰\n\n```bash\npython -m uvicorn main:app --reload\n```\n\n> **3ë‹¨ê³„** - ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000 ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì—°êµ¬ë¥¼ ì‹œì‘í•˜ì„¸ìš”!\n\n<br />\n\n**[Poetry](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started#poetry) ë˜ëŠ” [ê°€ìƒ í™˜ê²½](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started#virtual-environment)ì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ë‹¤ë©´, [ë¬¸ì„œ](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.**\n\n### PIP íŒ¨í‚¤ì§€ë¡œ ì‹¤í–‰í•˜ê¸°\n```bash\npip install gpt-researcher\n```\n\n```python\n...\nfrom gpt_researcher import GPTResearcher\n\nquery = \"ì™œ Nvidia ì£¼ì‹ì´ ì˜¤ë¥´ê³  ìˆë‚˜ìš”?\"\nresearcher = GPTResearcher(query=query, report_type=\"research_report\")\n# ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ì—°êµ¬ ìˆ˜í–‰\nresearch_result = await researcher.conduct_research()\n# ë³´ê³ ì„œ ì‘ì„±\nreport = await researcher.write_report()\n...\n```\n\n**ë” ë§ì€ ì˜ˆì œì™€ êµ¬ì„± ì˜µì…˜ì€ [PIP ë¬¸ì„œ](https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.**\n\n## Dockerë¡œ ì‹¤í–‰\n\n> **1ë‹¨ê³„** - [Docker ì„¤ì¹˜](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker)\n\n> **2ë‹¨ê³„** - `.env.example` íŒŒì¼ì„ ë³µì‚¬í•˜ê³  API í‚¤ë¥¼ ì¶”ê°€í•œ í›„, íŒŒì¼ì„ `.env`ë¡œ ì €ì¥í•˜ì„¸ìš”.\n\n> **3ë‹¨ê³„** - docker-compose íŒŒì¼ì—ì„œ ì‹¤í–‰í•˜ê³  ì‹¶ì§€ ì•Šì€ ì„œë¹„ìŠ¤ë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.\n\n```bash\n$ docker-compose up --build\n```\n\n> **4ë‹¨ê³„** - docker-compose íŒŒì¼ì—ì„œ ì•„ë¬´ ê²ƒë„ ì£¼ì„ ì²˜ë¦¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ê¸°ë³¸ì ìœ¼ë¡œ ë‘ ê°€ì§€ í”„ë¡œì„¸ìŠ¤ê°€ ì‹œì‘ë©ë‹ˆë‹¤:\n - localhost:8000ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ Python ì„œë²„<br>\n - localhost:3000ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ React ì•±<br>\n\në¸Œë¼ìš°ì €ì—ì„œ localhost:3000ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì—°êµ¬ë¥¼ ì‹œì‘í•˜ì„¸ìš”!\n\n## ğŸ“„ ë¡œì»¬ ë¬¸ì„œë¡œ ì—°êµ¬í•˜ê¸°\n\nGPT Researcherë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—°êµ¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì€ PDF, ì¼ë°˜ í…ìŠ¤íŠ¸, CSV, Excel, Markdown, PowerPoint, Word ë¬¸ì„œì…ë‹ˆë‹¤.\n\n1ë‹¨ê³„: `DOC_PATH` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ ë¬¸ì„œê°€ ìˆëŠ” í´ë”ë¥¼ ì§€ì •í•˜ì„¸ìš”.\n\n```bash\nexport DOC_PATH=\"./my-docs\"\n```\n\n2ë‹¨ê³„:\n - í”„ë¡ íŠ¸ì—”ë“œ ì•±ì„ localhost:8000ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë¼ë©´, \"Report Source\" ë“œë¡­ë‹¤ìš´ ì˜µì…˜ì—ì„œ \"My Documents\"ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n - GPT Researcherë¥¼ [PIP íŒ¨í‚¤ì§€](https://docs.tavily.com/docs/gpt-researcher/pip-package)ë¡œ ì‹¤í–‰ ì¤‘ì´ë¼ë©´, `report_source` ì¸ìˆ˜ë¥¼ \"local\"ë¡œ ì„¤ì •í•˜ì—¬ `GPTResearcher` í´ë˜ìŠ¤ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ì„¸ìš”. [ì½”ë“œ ì˜ˆì œ](https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n\n## ğŸ‘ª ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì–´ì‹œìŠ¤í„´íŠ¸\n\nAIê°€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë° RAGì—ì„œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „í•¨ì— ë”°ë¼, ìš°ë¦¬ëŠ” [LangGraph](https://python.langchain.com/v0.1/docs/langgraph/)ë¡œ êµ¬ì¶•ëœ ìƒˆë¡œìš´ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.\n\nLangGraphë¥¼ ì‚¬ìš©í•˜ë©´ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ì „ë¬¸ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì—°êµ¬ ê³¼ì •ì˜ ê¹Šì´ì™€ ì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœê·¼ [STORM](https://arxiv.org/abs/2402.14207) ë…¼ë¬¸ì—ì„œ ì˜ê°ì„ ë°›ì•„, ì´ í”„ë¡œì íŠ¸ëŠ” AI ì—ì´ì „íŠ¸ íŒ€ì´ ì£¼ì œì— ëŒ€í•œ ì—°êµ¬ë¥¼ ê³„íšì—ì„œ ì¶œíŒê¹Œì§€ í•¨ê»˜ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\ní‰ê·  ì‹¤í–‰ì€ 5-6 í˜ì´ì§€ ë¶„ëŸ‰ì˜ ì—°êµ¬ ë³´ê³ ì„œë¥¼ PDF, Docx, Markdown í˜•ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n\n[ì—¬ê¸°](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents)ì—ì„œ í™•ì¸í•˜ê±°ë‚˜ [ë¬¸ì„œ](https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph)ì—ì„œ ìì„¸í•œ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n\n## ğŸ–¥ï¸ í”„ë¡ íŠ¸ì—”ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜\n\nGPT-ResearcherëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•˜ê³  ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ê¸° ìœ„í•´ í–¥ìƒëœ í”„ë¡ íŠ¸ì—”ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. í”„ë¡ íŠ¸ì—”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:\n\n- ì—°êµ¬ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•  ìˆ˜ ìˆëŠ” ì§ê´€ì ì¸ ì¸í„°í˜ì´ìŠ¤\n- ì—°êµ¬ ì‘ì—…ì˜ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì¶”ì \n- ì—°êµ¬ ê²°ê³¼ì˜ ëŒ€í™”í˜• ë””ìŠ¤í”Œë ˆì´\n- ë§ì¶¤í˜• ì—°êµ¬ ê²½í—˜ì„ ìœ„í•œ ì„¤ì • ê°€ëŠ¥\n\në‘ ê°€ì§€ ë°°í¬ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤:\n1. FastAPIë¡œ ì œê³µë˜ëŠ” ê²½ëŸ‰ ì •ì  í”„ë¡ íŠ¸ì—”ë“œ\n2. ê³ ê¸‰ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” NextJS ì• í”Œë¦¬ì¼€ì´ì…˜\n\ní”„ë¡ íŠ¸ì—”ë“œ ê¸°ëŠ¥ì— ëŒ€í•œ ìì„¸í•œ ì„¤ì¹˜ ë°©ë²• ë° ì •ë³´ë¥¼ ì›í•˜ì‹œë©´ [ë¬¸ì„œ í˜ì´ì§€](https://docs.gptr.dev/docs/gpt-researcher/frontend/frontend)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n\n## ğŸš€ ê¸°ì—¬í•˜ê¸°\nìš°ë¦¬ëŠ” ê¸°ì—¬ë¥¼ ì ê·¹ í™˜ì˜í•©ë‹ˆë‹¤! ê´€ì‹¬ì´ ìˆë‹¤ë©´ [ê¸°ì—¬ ê°€ì´ë“œ](https://github.com/assafelovic/gpt-researcher/blob/master/CONTRIBUTING.md)ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\n\n[ë¡œë“œë§µ](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap) í˜ì´ì§€ë¥¼ í™•ì¸í•˜ê³ , ìš°ë¦¬ [Discord ì»¤ë®¤ë‹ˆí‹°](https://discord.gg/QgZXvJAccX)ì— ê°€ì…í•˜ì—¬ ìš°ë¦¬ì˜ ëª©í‘œì— í•¨ê»˜ ì°¸ì—¬í•´ ì£¼ì„¸ìš”.\n<a href=\"https://github.com/assafelovic/gpt-researcher/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=assafelovic/gpt-researcher\" />\n</a>\n\n## âœ‰ï¸ ì§€ì› / ë¬¸ì˜\n- [ì»¤ë®¤ë‹ˆí‹° Discord](https://discord.gg/spBgZmm3Xe)\n- ì €ì ì´ë©”ì¼: assaf.elovic@gmail.com\n\n## ğŸ›¡ï¸ ë©´ì±… ì¡°í•­\n\nì´ í”„ë¡œì íŠ¸ì¸ GPT ResearcherëŠ” ì‹¤í—˜ì ì¸ ì‘ìš© í”„ë¡œê·¸ë¨ì´ë©°, ëª…ì‹œì ì´ê±°ë‚˜ ë¬µì‹œì ì¸ ë³´ì¦ ì—†ì´ \"ìˆëŠ” ê·¸ëŒ€ë¡œ\" ì œê³µë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ì½”ë“œë¥¼ í•™ìˆ ì  ëª©ì ìœ¼ë¡œ Apache 2 ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì— ìˆëŠ” ê²ƒì€ í•™ìˆ ì  ì¡°ì–¸ì´ ì•„ë‹ˆë©°, í•™ìˆ  ë˜ëŠ” ì—°êµ¬ ë…¼ë¬¸ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\ní¸í–¥ë˜ì§€ ì•Šì€ ì—°êµ¬ ì£¼ì¥ì— ëŒ€í•œ ìš°ë¦¬ì˜ ê²¬í•´:\n1. GPT Researcherì˜ ì£¼ìš” ëª©í‘œëŠ” ì˜ëª»ëœ ì •ë³´ì™€ í¸í–¥ëœ ì‚¬ì‹¤ì„ ì¤„ì´ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ ë°©ë²•ì€ ë¬´ì—‡ì¼ê¹Œìš”? ìš°ë¦¬ëŠ” ë” ë§ì€ ì‚¬ì´íŠ¸ë¥¼ ìŠ¤í¬ë˜í•‘í• ìˆ˜ë¡ ì˜ëª»ëœ ë°ì´í„°ì˜ ê°€ëŠ¥ì„±ì´ ì¤„ì–´ë“ ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê³  ê°€ì¥ ë¹ˆë²ˆí•œ ì •ë³´ë¥¼ ì„ íƒí•˜ë©´, ëª¨ë“  ì •ë³´ê°€ í‹€ë¦´ í™•ë¥ ì€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.\n2. ìš°ë¦¬ëŠ” í¸í–¥ì„ ì™„ì „íˆ ì œê±°í•˜ë ¤ê³  í•˜ì§€ëŠ” ì•Šì§€ë§Œ, ê°€ëŠ¥í•œ í•œ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. **ìš°ë¦¬ëŠ” ì¸ê°„ê³¼ LLMì˜ ê°€ì¥ íš¨ê³¼ì ì¸ ìƒí˜¸ì‘ìš©ì„ ì°¾ê¸° ìœ„í•œ ì»¤ë®¤ë‹ˆí‹°ì…ë‹ˆë‹¤.**\n3. ì—°êµ¬ì—ì„œ ì‚¬ëŒë“¤ë„ ì´ë¯¸ ìì‹ ì´ ì—°êµ¬í•˜ëŠ” ì£¼ì œì— ëŒ€í•´ ì˜ê²¬ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— í¸í–¥ë˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ë§ì€ ì˜ê²¬ì„ ìŠ¤í¬ë˜í•‘í•˜ë©°, í¸í–¥ëœ ì‚¬ëŒì´ë¼ë©´ ê²°ì½” ì½ì§€ ì•Šì•˜ì„ ë‹¤ì–‘í•œ ê²¬í•´ë¥¼ ê³ ë¥´ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.\n\n**GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš°, í† í° ì‚¬ìš©ëŸ‰ ë•Œë¬¸ì— ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.** ì´ í”„ë¡œì íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ìì‹ ì˜ í† í° ì‚¬ìš©ëŸ‰ ë° ê´€ë ¨ ë¹„ìš©ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ë³¸ì¸ì˜ ì±…ì„ì…ë‹ˆë‹¤. OpenAI API ì‚¬ìš©ëŸ‰ì„ ì •ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ê³ , ì˜ˆìƒì¹˜ ëª»í•œ ë¹„ìš©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í•œë„ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì•Œë¦¼ì„ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n\n\n---\n\n<p align=\"center\">\n<a href=\"https://star-history.com/#assafelovic/gpt-researcher\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n  </picture>\n</a>\n</p>\n"
        },
        {
          "name": "README-zh_CN.md",
          "type": "blob",
          "size": 9.3251953125,
          "content": "<div align=\"center\">\n<!--<h1 style=\"display: flex; align-items: center; gap: 10px;\">\n  <img src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/a45bac7c-092c-42e5-8eb6-69acbf20dde5\" alt=\"Logo\" width=\"25\">\n  GPT Researcher\n</h1>-->\n<img src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3\" alt=\"Logo\" width=\"80\">\n\n\n####\n\n[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)\n[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)\n[![Discord Follow](https://img.shields.io/discord/1127851779011391548?style=for-the-badge&logo=discord&label=Chat%20on%20Discord)](https://discord.gg/QgZXvJAccX)\n\n[![PyPI version](https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&logoColor=white&style=flat)](https://badge.fury.io/py/gpt-researcher)\n![GitHub Release](https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&logo=github)\n[![Open In Colab](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&label=%20&style=flat&logoSize=40)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)\n[![Docker Image Version](https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&style=flat&logo=docker&logoColor=white&color=1D63ED)](https://hub.docker.com/r/gptresearcher/gpt-researcher)\n[![Twitter Follow](https://img.shields.io/twitter/follow/assaf_elovic?style=social)](https://twitter.com/assaf_elovic)\n\n[English](README.md) |\n[ä¸­æ–‡](README-zh_CN.md) |\n[æ—¥æœ¬èª](README-ja_JP.md) |\n[í•œêµ­ì–´](README-ko_KR.md)\n</div>\n\n# ğŸ” GPT Researcher\n\n**GPT Researcher æ˜¯ä¸€ä¸ªæ™ºèƒ½ä½“ä»£ç†ï¼Œä¸“ä¸ºå„ç§ä»»åŠ¡çš„ç»¼åˆåœ¨çº¿ç ”ç©¶è€Œè®¾è®¡ã€‚**\n\nä»£ç†å¯ä»¥ç”Ÿæˆè¯¦ç»†ã€æ­£å¼ä¸”å®¢è§‚çš„ç ”ç©¶æŠ¥å‘Šï¼Œå¹¶æä¾›è‡ªå®šä¹‰é€‰é¡¹ï¼Œä¸“æ³¨äºç›¸å…³èµ„æºã€ç»“æ„æ¡†æ¶å’Œç»éªŒæŠ¥å‘Šã€‚å—æœ€è¿‘å‘è¡¨çš„[Plan-and-Solve](https://arxiv.org/abs/2305.04091) å’Œ[RAG](https://arxiv.org/abs/2005.11401) è®ºæ–‡çš„å¯å‘ï¼ŒGPT Researcher è§£å†³äº†é€Ÿåº¦ã€ç¡®å®šæ€§å’Œå¯é æ€§ç­‰é—®é¢˜ï¼Œé€šè¿‡å¹¶è¡ŒåŒ–çš„ä»£ç†è¿è¡Œï¼Œè€Œä¸æ˜¯åŒæ­¥æ“ä½œï¼Œæä¾›äº†æ›´ç¨³å®šçš„æ€§èƒ½å’Œæ›´é«˜çš„é€Ÿåº¦ã€‚\n\n**æˆ‘ä»¬çš„ä½¿å‘½æ˜¯åˆ©ç”¨äººå·¥æ™ºèƒ½çš„åŠ›é‡ï¼Œä¸ºä¸ªäººå’Œç»„ç»‡æä¾›å‡†ç¡®ã€å®¢è§‚å’Œäº‹å®çš„ä¿¡æ¯ã€‚**\n\n## ä¸ºä»€ä¹ˆé€‰æ‹©GPT Researcher?\n\n- å› ä¸ºäººå·¥ç ”ç©¶ä»»åŠ¡å½¢æˆå®¢è§‚ç»“è®ºå¯èƒ½éœ€è¦æ—¶é—´å’Œç»å†ï¼Œæœ‰æ—¶ç”šè‡³éœ€è¦æ•°å‘¨æ‰èƒ½æ‰¾åˆ°æ­£ç¡®çš„èµ„æºå’Œä¿¡æ¯ã€‚\n- ç›®å‰çš„LLMæ˜¯æ ¹æ®å†å²å’Œè¿‡æ—¶çš„ä¿¡æ¯è¿›è¡Œè®­ç»ƒçš„ï¼Œå­˜åœ¨ä¸¥é‡çš„å¹»è§‰é£é™©ï¼Œå› æ­¤å‡ ä¹æ— æ³•èƒœä»»ç ”ç©¶ä»»åŠ¡ã€‚\n- ç½‘ç»œæœç´¢çš„è§£å†³æ–¹æ¡ˆï¼ˆä¾‹å¦‚ ChatGPT + Web æ’ä»¶ï¼‰ä»…è€ƒè™‘æœ‰é™çš„èµ„æºå’Œå†…å®¹ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ä¼šå¯¼è‡´è‚¤æµ…çš„ç»“è®ºæˆ–ä¸å®¢è§‚çš„ç­”æ¡ˆã€‚\n- åªä½¿ç”¨éƒ¨åˆ†èµ„æºå¯èƒ½ä¼šåœ¨ç¡®å®šç ”ç©¶é—®é¢˜æˆ–ä»»åŠ¡çš„æ­£ç¡®ç»“è®ºæ—¶äº§ç”Ÿåå·®ã€‚\n\n## æ¶æ„\nä¸»è¦æ€æƒ³æ˜¯è¿è¡Œâ€œ**è®¡åˆ’è€…**â€å’Œâ€œ**æ‰§è¡Œ**â€ä»£ç†ï¼Œè€Œ**è®¡åˆ’è€…**ç”Ÿæˆé—®é¢˜è¿›è¡Œç ”ç©¶ï¼Œâ€œ**æ‰§è¡Œ**â€ä»£ç†æ ¹æ®æ¯ä¸ªç”Ÿæˆçš„ç ”ç©¶é—®é¢˜å¯»æ‰¾æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚æœ€åï¼Œâ€œ**è®¡åˆ’è€…**â€è¿‡æ»¤å’Œèšåˆæ‰€æœ‰ç›¸å…³ä¿¡æ¯å¹¶åˆ›å»ºç ”ç©¶æŠ¥å‘Šã€‚<br /> <br /> \nä»£ç†åŒæ—¶åˆ©ç”¨ gpt-40-mini å’Œ gpt-4oï¼ˆ128K ä¸Šä¸‹æ–‡ï¼‰æ¥å®Œæˆä¸€é¡¹ç ”ç©¶ä»»åŠ¡ã€‚æˆ‘ä»¬ä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•å¯¹æˆæœ¬è¿›è¡Œä¼˜åŒ–ã€‚**ç ”ç©¶ä»»åŠ¡å¹³å‡è€—æ—¶çº¦ 3 åˆ†é’Ÿï¼Œæˆæœ¬çº¦ä¸º ~0.1 ç¾å…ƒ**ã€‚\n\n<div align=\"center\">\n<img align=\"center\" height=\"500\" src=\"https://cowriter-images.s3.amazonaws.com/architecture.png\">\n</div>\n\n\nè¯¦ç»†è¯´æ˜:\n* æ ¹æ®ç ”ç©¶æœç´¢æˆ–ä»»åŠ¡åˆ›å»ºç‰¹å®šé¢†åŸŸçš„ä»£ç†ã€‚\n* ç”Ÿæˆä¸€ç»„ç ”ç©¶é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å…±åŒå½¢æˆç­”æ¡ˆå¯¹ä»»ä½•ç»™å®šä»»åŠ¡çš„å®¢è§‚æ„è§ã€‚\n* é’ˆå¯¹æ¯ä¸ªç ”ç©¶é—®é¢˜ï¼Œè§¦å‘ä¸€ä¸ªçˆ¬è™«ä»£ç†ï¼Œä»åœ¨çº¿èµ„æºä¸­æœç´¢ä¸ç»™å®šä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚\n* å¯¹äºæ¯ä¸€ä¸ªæŠ“å–çš„èµ„æºï¼Œæ ¹æ®ç›¸å…³ä¿¡æ¯è¿›è¡Œæ±‡æ€»ï¼Œå¹¶è·Ÿè¸ªå…¶æ¥æºã€‚\n* æœ€åï¼Œå¯¹æ‰€æœ‰æ±‡æ€»çš„èµ„æ–™æ¥æºè¿›è¡Œè¿‡æ»¤å’Œæ±‡æ€»ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Šã€‚\n\n## æ¼”ç¤º\nhttps://github.com/assafelovic/gpt-researcher/assets/13554167/a00c89a6-a295-4dd0-b58d-098a31c40fda\n\n## æ•™ç¨‹\n - [è¿è¡ŒåŸç†](https://docs.gptr.dev/blog/building-gpt-researcher)\n - [å¦‚ä½•å®‰è£…](https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea)\n - [ç°åœºæ¼”ç¤º](https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8)\n\n## ç‰¹æ€§\n- ğŸ“ ç”Ÿæˆç ”ç©¶é—®é¢˜ã€å¤§çº²ã€èµ„æºå’Œè¯¾é¢˜æŠ¥å‘Š\n- ğŸŒ æ¯é¡¹ç ”ç©¶æ±‡æ€»è¶…è¿‡20ä¸ªç½‘ç»œèµ„æºï¼Œå½¢æˆå®¢è§‚å’ŒçœŸå®çš„ç»“è®º\n- ğŸ–¥ï¸ åŒ…æ‹¬æ˜“äºä½¿ç”¨çš„webç•Œé¢ (HTML/CSS/JS)\n- ğŸ” æ”¯æŒJavaScriptç½‘ç»œèµ„æºæŠ“å–åŠŸèƒ½\n- ğŸ“‚ è¿½è¸ªè®¿é—®è¿‡å’Œä½¿ç”¨è¿‡çš„ç½‘ç»œèµ„æºå’Œæ¥æº\n- ğŸ“„ å°†ç ”ç©¶æŠ¥å‘Šå¯¼å‡ºä¸ºPDFæˆ–å…¶ä»–æ ¼å¼...\n\n## ğŸ“– æ–‡æ¡£\n\nè¯·å‚é˜…[æ­¤å¤„](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started)ï¼Œäº†è§£å®Œæ•´æ–‡æ¡£ï¼š\n\n- å…¥é—¨ï¼ˆå®‰è£…ã€è®¾ç½®ç¯å¢ƒã€ç®€å•ç¤ºä¾‹ï¼‰\n- æ“ä½œç¤ºä¾‹ï¼ˆæ¼”ç¤ºã€é›†æˆã€docker æ”¯æŒï¼‰\n- å‚è€ƒèµ„æ–™ï¼ˆAPIå®Œæ•´æ–‡æ¡£ï¼‰\n- Tavily åº”ç”¨ç¨‹åºæ¥å£é›†æˆï¼ˆæ ¸å¿ƒæ¦‚å¿µçš„é«˜çº§è§£é‡Šï¼‰\n\n## å¿«é€Ÿå¼€å§‹\n> **æ­¥éª¤ 0** - å®‰è£… Python 3.11 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚[å‚è§æ­¤å¤„](https://www.tutorialsteacher.com/python/install-python) è·å–è¯¦ç»†æŒ‡å—ã€‚\n\n<br />\n\n> **æ­¥éª¤ 1** - ä¸‹è½½é¡¹ç›®\n\n```bash\n$ git clone https://github.com/assafelovic/gpt-researcher.git\n$ cd gpt-researcher\n```\n\n<br />\n\n> **æ­¥éª¤2** -å®‰è£…ä¾èµ–é¡¹\n```bash\n$ pip install -r requirements.txt\n```\n<br />\n\n> **ç¬¬ 3 æ­¥** - ä½¿ç”¨ OpenAI å¯†é’¥å’Œ Tavily API å¯†é’¥åˆ›å»º .env æ–‡ä»¶ï¼Œæˆ–ç›´æ¥å¯¼å‡ºè¯¥æ–‡ä»¶\n\n```bash\n$ export OPENAI_API_KEY={Your OpenAI API Key here}\n```\n```bash\n$ export TAVILY_API_KEY={Your Tavily API Key here}\n```\n\n- **LLMï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ [OpenAI GPT](https://platform.openai.com/docs/guides/gpt)**ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ [Langchain Adapter](https://python.langchain.com/docs/guides/adapters/openai) æ”¯æŒçš„ä»»ä½•å…¶ä»– LLM æ¨¡å‹ï¼ˆåŒ…æ‹¬å¼€æºï¼‰ï¼Œåªéœ€åœ¨ config/config.py ä¸­æ›´æ”¹ llm æ¨¡å‹å’Œæä¾›è€…å³å¯ã€‚è¯·æŒ‰ç…§ [è¿™ä»½æŒ‡å—](https://python.langchain.com/docs/integrations/llms/) å­¦ä¹ å¦‚ä½•å°† LLM ä¸ Langchain é›†æˆã€‚\n- **å¯¹äºæœç´¢å¼•æ“ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ [Tavily Search API](https://app.tavily.com)ï¼ˆå·²é’ˆå¯¹ LLM è¿›è¡Œä¼˜åŒ–ï¼‰**ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–æœç´¢å¼•æ“ï¼Œåªéœ€å°† config/config.py ä¸­çš„æœç´¢æä¾›ç¨‹åºæ›´æ”¹ä¸º \"duckduckgo\"ã€\"googleAPI\"ã€\"searchapi\"ã€\"googleSerp \"æˆ– \"searx \"å³å¯ã€‚ç„¶ååœ¨ config.py æ–‡ä»¶ä¸­æ·»åŠ ç›¸åº”çš„ env API å¯†é’¥ã€‚\n- **æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨ [OpenAI GPT](https://platform.openai.com/docs/guides/gpt) æ¨¡å‹å’Œ [Tavily Search API](https://app.tavily.com) ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚**\n<br />\n\n> **ç¬¬ 4 æ­¥** - ä½¿ç”¨ FastAPI è¿è¡Œä»£ç†\n\n```bash\n$ uvicorn main:app --reload\n```\n<br />\n\n> **ç¬¬ 5 æ­¥** - åœ¨ä»»ä½•æµè§ˆå™¨ä¸Šè®¿é—® http://localhost:8000ï¼Œäº«å—ç ”ç©¶ä¹è¶£ï¼\n\nè¦äº†è§£å¦‚ä½•å¼€å§‹ä½¿ç”¨ Docker æˆ–äº†è§£æœ‰å…³åŠŸèƒ½å’ŒæœåŠ¡çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—® [documentation](https://docs.gptr.dev) é¡µé¢ã€‚\n\n## ğŸš€ è´¡çŒ®\næˆ‘ä»¬éå¸¸æ¬¢è¿æ‚¨çš„è´¡çŒ®ï¼å¦‚æœæ‚¨æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹ [contributing](CONTRIBUTING.md)ã€‚\n\nå¦‚æœæ‚¨æœ‰å…´è¶£åŠ å…¥æˆ‘ä»¬çš„ä»»åŠ¡ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [è·¯çº¿å›¾](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap) é¡µé¢ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬çš„ [Discord ç¤¾åŒº](https://discord.gg/QgZXvJAccX) è”ç³»æˆ‘ä»¬ã€‚\n\n## âœ‰ï¸ æ”¯æŒ / è”ç³»æˆ‘ä»¬\n- [ç¤¾åŒºè®¨è®ºåŒº](https://discord.gg/spBgZmm3Xe)\n- æˆ‘ä»¬çš„é‚®ç®±: support@tavily.com\n\n## ğŸ›¡ å…è´£å£°æ˜\n\næœ¬é¡¹ç›® \"GPT Researcher \"æ˜¯ä¸€ä¸ªå®éªŒæ€§åº”ç”¨ç¨‹åºï¼ŒæŒ‰ \"ç°çŠ¶ \"æä¾›ï¼Œä¸åšä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ã€‚æˆ‘ä»¬æ ¹æ® MIT è®¸å¯åˆ†äº«ç”¨äºå­¦æœ¯ç›®çš„çš„ä»£ç ã€‚æœ¬æ–‡ä¸æä¾›ä»»ä½•å­¦æœ¯å»ºè®®ï¼Œä¹Ÿä¸å»ºè®®åœ¨å­¦æœ¯æˆ–ç ”ç©¶è®ºæ–‡ä¸­ä½¿ç”¨ã€‚\n\næˆ‘ä»¬å¯¹å®¢è§‚ç ”ç©¶ä¸»å¼ çš„çœ‹æ³•ï¼š\n1.  æˆ‘ä»¬æŠ“å–ç³»ç»Ÿçš„å…¨éƒ¨ç›®çš„æ˜¯å‡å°‘ä¸æ­£ç¡®çš„äº‹å®ã€‚å¦‚ä½•è§£å†³ï¼Ÿæˆ‘ä»¬æŠ“å–çš„ç½‘ç«™è¶Šå¤šï¼Œé”™è¯¯æ•°æ®çš„å¯èƒ½æ€§å°±è¶Šå°ã€‚æˆ‘ä»¬æ¯é¡¹ç ”ç©¶éƒ½ä¼šæ”¶é›†20æ¡ä¿¡æ¯ï¼Œå®ƒä»¬å…¨éƒ¨é”™è¯¯çš„å¯èƒ½æ€§æä½ã€‚\n2. æˆ‘ä»¬çš„ç›®æ ‡ä¸æ˜¯æ¶ˆé™¤åè§ï¼Œè€Œæ˜¯å°½å¯èƒ½å‡å°‘åè§ã€‚**ä½œä¸ºä¸€ä¸ªç¤¾åŒºï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ¢ç´¢æœ€æœ‰æ•ˆçš„äººæœºäº’åŠ¨**ã€‚\n3. åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­ï¼Œäººä»¬ä¹Ÿå®¹æ˜“äº§ç”Ÿåè§ï¼Œå› ä¸ºå¤§å¤šæ•°äººå¯¹è‡ªå·±ç ”ç©¶çš„è¯¾é¢˜éƒ½æœ‰è‡ªå·±çš„çœ‹æ³•ã€‚è¿™ä¸ªå·¥å…·å¯ä»¥æœç½—åˆ°è®¸å¤šè§‚ç‚¹ï¼Œå¹¶å‡åŒ€åœ°è§£é‡Šå„ç§ä¸åŒçš„è§‚ç‚¹ï¼Œè€Œæœ‰åè§çš„äººæ˜¯ç»å¯¹è¯»ä¸åˆ°è¿™äº›è§‚ç‚¹çš„ã€‚\n\n**è¯·æ³¨æ„ï¼Œä½¿ç”¨ GPT-4 è¯­è¨€æ¨¡å‹å¯èƒ½ä¼šå› ä½¿ç”¨ä»¤ç‰Œè€Œäº§ç”Ÿé«˜æ˜‚è´¹ç”¨**ã€‚ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨æ‰¿è®¤æœ‰è´£ä»»ç›‘æ§å’Œç®¡ç†è‡ªå·±çš„ä»¤ç‰Œä½¿ç”¨æƒ…å†µåŠç›¸å…³è´¹ç”¨ã€‚å¼ºçƒˆå»ºè®®æ‚¨å®šæœŸæ£€æŸ¥ OpenAI API çš„ä½¿ç”¨æƒ…å†µï¼Œå¹¶è®¾ç½®ä»»ä½•å¿…è¦çš„é™åˆ¶æˆ–è­¦æŠ¥ï¼Œä»¥é˜²æ­¢å‘ç”Ÿæ„å¤–è´¹ç”¨ã€‚\n\n---\n\n<p align=\"center\">\n<a href=\"https://star-history.com/#assafelovic/gpt-researcher\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n  </picture>\n</a>\n</p>\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.4296875,
          "content": "<div align=\"center\" id=\"top\">\n\n<img src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3\" alt=\"Logo\" width=\"80\">\n\n####\n\n[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)\n[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)\n[![Discord Follow](https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&theme=clean-inverted&?compact=true)](https://discord.gg/QgZXvJAccX)\n\n[![PyPI version](https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&logoColor=white&style=flat)](https://badge.fury.io/py/gpt-researcher)\n![GitHub Release](https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&logo=github)\n[![Open In Colab](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&label=%20&style=flat&logoSize=40)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)\n[![Docker Image Version](https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&style=flat&logo=docker&logoColor=white&color=1D63ED)](https://hub.docker.com/r/gptresearcher/gpt-researcher)\n[![Twitter Follow](https://img.shields.io/twitter/follow/assaf_elovic?style=social)](https://twitter.com/assaf_elovic)\n\n[English](README.md) | [ä¸­æ–‡](README-zh_CN.md) | [æ—¥æœ¬èª](README-ja_JP.md) | [í•œêµ­ì–´](README-ko_KR.md)\n\n</div>\n\n# ğŸ” GPT Researcher\n\n**GPT Researcher is an autonomous agent designed for comprehensive web and local research on any given task.** \n\nThe agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent [Plan-and-Solve](https://arxiv.org/abs/2305.04091) and [RAG](https://arxiv.org/abs/2005.11401) papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.\n\n**Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.**\n\n## Why GPT Researcher?\n\n- Objective conclusions for manual research can take weeks, requiring vast resources and time.\n- LLMs trained on outdated information can hallucinate, becoming irrelevant for current research tasks.\n- Current LLMs have token limitations, insufficient for generating long research reports.\n- Limited web sources in existing services lead to misinformation and shallow results.\n- Selective web sources can introduce bias into research tasks.\n\n## Demo\nhttps://github.com/user-attachments/assets/2cc38f6a-9f66-4644-9e69-a46c40e296d4\n\n## Architecture\n\nThe core idea is to utilize 'planner' and 'execution' agents. The planner generates research questions, while the execution agents gather relevant information. The publisher then aggregates all findings into a comprehensive report.\n\n<div align=\"center\">\n<img align=\"center\" height=\"600\" src=\"https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527\">\n</div>\n\nSteps:\n* Create a task-specific agent based on a research query.\n* Generate questions that collectively form an objective opinion on the task.\n* Use a crawler agent for gathering information for each question.\n* Summarize and source-track each resource.\n* Filter and aggregate summaries into a final research report.\n\n## Tutorials\n - [How it Works](https://docs.gptr.dev/blog/building-gpt-researcher)\n - [How to Install](https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea)\n - [Live Demo](https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8)\n\n## Features\n\n- ğŸ“ Generate detailed research reports using web and local documents.\n- ğŸ–¼ï¸ Smart image scraping and filtering for reports.\n- ğŸ“œ Generate detailed reports exceeding 2,000 words.\n- ğŸŒ Aggregate over 20 sources for objective conclusions.\n- ğŸ–¥ï¸ Frontend available in lightweight (HTML/CSS/JS) and production-ready (NextJS + Tailwind) versions.\n- ğŸ” JavaScript-enabled web scraping.\n- ğŸ“‚ Maintains memory and context throughout research.\n- ğŸ“„ Export reports to PDF, Word, and other formats.\n\n## ğŸ“– Documentation\n\nSee the [Documentation](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started) for:\n- Installation and setup guides\n- Configuration and customization options\n- How-To examples\n- Full API references\n\n## âš™ï¸ Getting Started\n\n### Installation\n\n1. Install Python 3.11 or later. [Guide](https://www.tutorialsteacher.com/python/install-python).\n2. Clone the project and navigate to the directory:\n\n    ```bash\n    git clone https://github.com/assafelovic/gpt-researcher.git\n    cd gpt-researcher\n    ```\n\n3. Set up API keys by exporting them or storing them in a `.env` file.\n\n    ```bash\n    export OPENAI_API_KEY={Your OpenAI API Key here}\n    export TAVILY_API_KEY={Your Tavily API Key here}\n    ```\n\n4. Install dependencies and start the server:\n\n    ```bash\n    pip install -r requirements.txt\n    python -m uvicorn main:app --reload\n    ```\n\nVisit [http://localhost:8000](http://localhost:8000) to start.\n\nFor other setups (e.g., Poetry or virtual environments), check the [Getting Started page](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started).\n\n## Run as PIP package\n```bash\npip install gpt-researcher\n\n```\n### Example Usage:\n```python\n...\nfrom gpt_researcher import GPTResearcher\n\nquery = \"why is Nvidia stock going up?\"\nresearcher = GPTResearcher(query=query, report_type=\"research_report\")\n# Conduct research on the given query\nresearch_result = await researcher.conduct_research()\n# Write the report\nreport = await researcher.write_report()\n...\n```\n\n**For more examples and configurations, please refer to the [PIP documentation](https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package) page.**\n\n\n## Run with Docker\n\n> **Step 1** - [Install Docker](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker)\n\n> **Step 2** - Clone the '.env.example' file, add your API Keys to the cloned file and save the file as '.env'\n\n> **Step 3** - Within the docker-compose file comment out services that you don't want to run with Docker.\n\n```bash\ndocker-compose up --build\n```\n\nIf that doesn't work, try running it without the dash:\n```bash\ndocker compose up --build\n```\n\n\n> **Step 4** - By default, if you haven't uncommented anything in your docker-compose file, this flow will start 2 processes:\n - the Python server running on localhost:8000<br>\n - the React app running on localhost:3000<br>\n\nVisit localhost:3000 on any browser and enjoy researching!\n\n\n\n## ğŸ“„ Research on Local Documents\n\nYou can instruct the GPT Researcher to run research tasks based on your local documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.\n\nStep 1: Add the env variable `DOC_PATH` pointing to the folder where your documents are located.\n\n```bash\nexport DOC_PATH=\"./my-docs\"\n```\n\nStep 2: \n - If you're running the frontend app on localhost:8000, simply select \"My Documents\" from the \"Report Source\" Dropdown Options.\n - If you're running GPT Researcher with the [PIP package](https://docs.tavily.com/docs/gpt-researcher/pip-package), pass the `report_source` argument as \"local\" when you instantiate the `GPTResearcher` class [code sample here](https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research).\n\n\n## ğŸ‘ª Multi-Agent Assistant\nAs AI evolves from prompt engineering and RAG to multi-agent systems, we're excited to introduce our new multi-agent assistant built with [LangGraph](https://python.langchain.com/v0.1/docs/langgraph/).\n\nBy using LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Inspired by the recent [STORM](https://arxiv.org/abs/2402.14207) paper, this project showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.\n\nAn average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.\n\nCheck it out [here](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents) or head over to our [documentation](https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph) for more information.\n\n## ğŸ–¥ï¸ Frontend Applications\n\nGPT-Researcher now features an enhanced frontend to improve the user experience and streamline the research process. The frontend offers:\n\n- An intuitive interface for inputting research queries\n- Real-time progress tracking of research tasks\n- Interactive display of research findings\n- Customizable settings for tailored research experiences\n\nTwo deployment options are available:\n1. A lightweight static frontend served by FastAPI\n2. A feature-rich NextJS application for advanced functionality\n\nFor detailed setup instructions and more information about the frontend features, please visit our [documentation page](https://docs.gptr.dev/docs/gpt-researcher/frontend/frontend).\n\n## ğŸš€ Contributing\nWe highly welcome contributions! Please check out [contributing](https://github.com/assafelovic/gpt-researcher/blob/master/CONTRIBUTING.md) if you're interested.\n\nPlease check out our [roadmap](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap) page and reach out to us via our [Discord community](https://discord.gg/QgZXvJAccX) if you're interested in joining our mission.\n<a href=\"https://github.com/assafelovic/gpt-researcher/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=assafelovic/gpt-researcher\" />\n</a>\n## âœ‰ï¸ Support / Contact us\n- [Community Discord](https://discord.gg/spBgZmm3Xe)\n- Author Email: assaf.elovic@gmail.com\n\n## ğŸ›¡ Disclaimer\n\nThis project, GPT Researcher, is an experimental application and is provided \"as-is\" without any warranty, express or implied. We are sharing codes for academic purposes under the Apache 2 license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.\n\nOur view on unbiased research claims:\n1. The main goal of GPT Researcher is to reduce incorrect and biased facts. How? We assume that the more sites we scrape the less chances of incorrect data. By scraping multiple sites per research, and choosing the most frequent information, the chances that they are all wrong is extremely low.\n2. We do not aim to eliminate biases; we aim to reduce it as much as possible. **We are here as a community to figure out the most effective human/llm interactions.**\n3. In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.\n\n---\n\n<p align=\"center\">\n<a href=\"https://star-history.com/#assafelovic/gpt-researcher\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n  </picture>\n</a>\n</p>\n\n\n<p align=\"right\">\n  <a href=\"#top\">â¬†ï¸ Back to Top</a>\n</p>\n"
        },
        {
          "name": "backend",
          "type": "tree",
          "content": null
        },
        {
          "name": "citation.cff",
          "type": "blob",
          "size": 0.2783203125,
          "content": "cff-version: 1.0.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n  - family-names: Elovic\n    given-names: Assaf\ntitle: gpt-researcher\nversion: 0.5.4\ndate-released: 2023-07-23\nrepository-code: https://github.com/assafelovic/gpt-researcher\nurl: https://gptr.dev"
        },
        {
          "name": "cli.py",
          "type": "blob",
          "size": 2.8642578125,
          "content": "\"\"\"\nProvides a command line interface for the GPTResearcher class.\n\nUsage:\n\n```shell\npython cli.py \"<query>\" --report_type <report_type>\n```\n\n\"\"\"\nimport asyncio\nimport argparse\nfrom argparse import RawTextHelpFormatter\nfrom uuid import uuid4\nimport os\n\nfrom dotenv import load_dotenv\n\nfrom gpt_researcher import GPTResearcher\nfrom gpt_researcher.utils.enum import ReportType\nfrom backend.report_type import DetailedReport\n\n# =============================================================================\n# CLI\n# =============================================================================\n\ncli = argparse.ArgumentParser(\n    description=\"Generate a research report.\",\n    # Enables the use of newlines in the help message\n    formatter_class=RawTextHelpFormatter)\n\n# =====================================\n# Arg: Query\n# =====================================\n\ncli.add_argument(\n    # Position 0 argument\n    \"query\",\n    type=str,\n    help=\"The query to conduct research on.\")\n\n# =====================================\n# Arg: Report Type\n# =====================================\n\nchoices = [report_type.value for report_type in ReportType]\n\nreport_type_descriptions = {\n    ReportType.ResearchReport.value: \"Summary - Short and fast (~2 min)\",\n    ReportType.DetailedReport.value: \"Detailed - In depth and longer (~5 min)\",\n    ReportType.ResourceReport.value: \"\",\n    ReportType.OutlineReport.value: \"\",\n    ReportType.CustomReport.value: \"\",\n    ReportType.SubtopicReport.value: \"\"\n}\n\ncli.add_argument(\n    \"--report_type\",\n    type=str,\n    help=\"The type of report to generate. Options:\\n\" + \"\\n\".join(\n        f\"  {choice}: {report_type_descriptions[choice]}\" for choice in choices\n    ),\n    # Deserialize ReportType as a List of strings:\n    choices=choices,\n    required=True)\n\n# =============================================================================\n# Main\n# =============================================================================\n\n\nasync def main(args):\n    \"\"\" \n    Conduct research on the given query, generate the report, and write\n    it as a markdown file to the output directory.\n    \"\"\"\n    if args.report_type == 'detailed_report':\n        detailed_report = DetailedReport(\n            query=args.query,\n            report_type=\"research_report\",\n            report_source=\"web_search\",\n        )\n\n        report = await detailed_report.run()\n    else:\n        researcher = GPTResearcher(\n            query=args.query,\n            report_type=args.report_type)\n\n        await researcher.conduct_research()\n\n        report = await researcher.write_report()\n\n    # Write the report to a file\n    artifact_filepath = f\"outputs/{uuid4()}.md\"\n    os.makedirs(\"outputs\", exist_ok=True)\n    with open(artifact_filepath, \"w\") as f:\n        f.write(report)\n\n    print(f\"Report written to '{artifact_filepath}'\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n    args = cli.parse_args()\n    asyncio.run(main(args))\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 1.208984375,
          "content": "services:\n  gpt-researcher:\n    pull_policy: build\n    image: gptresearcher/gpt-researcher\n    build: ./\n    environment: \n      OPENAI_API_KEY: ${OPENAI_API_KEY}\n      TAVILY_API_KEY: ${TAVILY_API_KEY}\n      LANGCHAIN_API_KEY: ${LANGCHAIN_API_KEY}\n      LOGGING_LEVEL: INFO\n    volumes:\n      - ./outputs:/usr/src/app/outputs\n    restart: always\n    ports:\n      - 8000:8000\n  gptr-nextjs:\n    pull_policy: build\n    image: gptresearcher/gptr-nextjs\n    stdin_open: true\n    environment:\n      CHOKIDAR_USEPOLLING: true\n      LOGGING_LEVEL: INFO\n    build:\n      dockerfile: Dockerfile.dev\n      context: frontend/nextjs\n    volumes:\n      - /app/node_modules\n      - ./frontend/nextjs:/app\n      - ./outputs:/app/outputs\n    restart: always\n    ports:\n      - 3000:3000\n\n  gpt-researcher-tests:\n    image: gptresearcher/gpt-researcher-tests\n    build: ./\n    environment: \n      OPENAI_API_KEY: ${OPENAI_API_KEY}\n      TAVILY_API_KEY: ${TAVILY_API_KEY}\n      LANGCHAIN_API_KEY: ${LANGCHAIN_API_KEY}\n      LOGGING_LEVEL: INFO\n    profiles: [\"test\"]\n    command: >\n      /bin/sh -c \"\n      pip install pytest pytest-asyncio faiss-cpu &&\n      python -m pytest tests/report-types.py &&\n      python -m pytest tests/vector-store.py\n      \"\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "frontend",
          "type": "tree",
          "content": null
        },
        {
          "name": "gpt_researcher",
          "type": "tree",
          "content": null
        },
        {
          "name": "langgraph.json",
          "type": "blob",
          "size": 0.1533203125,
          "content": "{\n  \"python_version\": \"3.11\",\n  \"dependencies\": [\n    \"./multi_agents\"\n  ],\n  \"graphs\": {\n    \"agent\": \"./multi_agents/agent.py:graph\"\n  },\n  \"env\": \".env\"\n}"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 0.9521484375,
          "content": "from dotenv import load_dotenv\nimport logging\nfrom pathlib import Path\n\n# Create logs directory if it doesn't exist\nlogs_dir = Path(\"logs\")\nlogs_dir.mkdir(exist_ok=True)\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        # File handler for general application logs\n        logging.FileHandler('logs/app.log'),\n        # Stream handler for console output\n        logging.StreamHandler()\n    ]\n)\n\n# Suppress verbose fontTools logging\nlogging.getLogger('fontTools').setLevel(logging.WARNING)\nlogging.getLogger('fontTools.subset').setLevel(logging.WARNING)\nlogging.getLogger('fontTools.ttLib').setLevel(logging.WARNING)\n\n# Create logger instance\nlogger = logging.getLogger(__name__)\n\nload_dotenv()\n\nfrom backend.server.server import app\n\nif __name__ == \"__main__\":\n    import uvicorn\n    \n    logger.info(\"Starting server...\")\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
        },
        {
          "name": "multi_agents",
          "type": "tree",
          "content": null
        },
        {
          "name": "poetry.toml",
          "type": "blob",
          "size": 0.0302734375,
          "content": "[virtualenvs]\nin-project = true"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.33203125,
          "content": "[tool.poetry]\nname = \"gpt-researcher\"\nversion = \"0.8.5\"\ndescription = \"GPT Researcher is an autonomous agent designed for comprehensive online research on a variety of tasks.\"\nauthors = [\"Assaf Elovic <assaf.elovic@gmail.com>\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \">=3.10,<3.12\"\nbeautifulsoup4 = \">=4.12.2\"\ncolorama = \">=0.4.6\"\nduckduckgo_search = \">=4.1.1\"\nmd2pdf = \">=1.0.1\"\nopenai = \">=1.3.3\"\npython-dotenv = \">=1.0.0\"\npyyaml = \">=6.0.1\"\nuvicorn = \">=0.24.0.post1\"\npydantic = \">=2.5.1\"\nfastapi = \">=0.104.1\"\npython-multipart = \">=0.0.6\"\nmarkdown = \">=3.5.1\"\nlangchain = \"^0.2\"\nlanggraph = \">=0.0.29,<0.3\"\nlangchain_community = \"^0.2\"\nlangchain-openai = \"^0.1\"\ntavily-python = \">=0.2.8\"\npermchain = \">=0.0.6\"\narxiv = \">=2.0.0\"\nPyMuPDF = \">=1.23.6\"\nrequests = \">=2.31.0\"\njinja2 = \">=3.1.2\"\naiofiles = \">=23.2.1\"\nSQLAlchemy = \">=2.0.28\"\nmistune = \"^3.0.2\"\nhtmldocx = \"^0.0.6\"\npython-docx = \"^1.1.0\"\nlxml = { version = \">=4.9.2\", extras = [\"html_clean\"] }\nunstructured = \">=0.13,<0.16\"\ntiktoken = \">=0.7.0\"\njson-repair = \"^0.29.8\"\njson5 = \"^0.9.25\"\nloguru = \"^0.7.2\"\nwebsockets = \"^13.1\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.pytest.ini_options]\nasyncio_mode = \"strict\"\naddopts = \"-v\"\ntestpaths = [\"tests\"]\npython_files = \"test_*.py\"\nasyncio_fixture_loop_scope = \"function\""
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.390625,
          "content": "# dependencies\nbeautifulsoup4\ncolorama\nmd2pdf\npython-dotenv\npyyaml\nuvicorn\npydantic\nfastapi\npython-multipart\nmarkdown\nlangchain\nlangchain_community\nlangchain-openai\nlangchain-ollama\nlanggraph\ntiktoken\ngpt-researcher\narxiv\nPyMuPDF\nrequests\njinja2\naiofiles\nmistune\npython-docx\nhtmldocx\nlxml_html_clean\nwebsockets\nunstructured\njson_repair\njson5\nloguru\n\n# uncomment for testing\n# pytest\n# pytest-asyncio\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.3408203125,
          "content": "from setuptools import find_packages, setup\n\nLATEST_VERSION = \"0.10.10\"\n\nexclude_packages = [\n    \"selenium\",\n    \"webdriver\",\n    \"fastapi\",\n    \"fastapi.*\",\n    \"uvicorn\",\n    \"jinja2\",\n    \"gpt-researcher\",\n    \"langgraph\"\n]\n\nwith open(r\"README.md\", \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nwith open(\"requirements.txt\", \"r\") as f:\n    reqs = [line.strip() for line in f if not any(pkg in line for pkg in exclude_packages)]\n\nsetup(\n    name=\"gpt-researcher\",\n    version=LATEST_VERSION,\n    description=\"GPT Researcher is an autonomous agent designed for comprehensive web research on any task\",\n    package_dir={'gpt_researcher': 'gpt_researcher'},\n    packages=find_packages(exclude=exclude_packages),\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/assafelovic/gpt-researcher\",\n    author=\"Assaf Elovic\",\n    author_email=\"assaf.elovic@gmail.com\",\n    license=\"MIT\",\n    classifiers=[\n        \"License :: OSI Approved :: MIT License\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    install_requires=reqs,\n\n\n)"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}