{
  "metadata": {
    "timestamp": 1736561261640,
    "page": 254,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "google-deepmind/alphafold",
      "stars": 13072,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.03125,
          "content": ".dockerignore\ndocker/Dockerfile\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.9501953125,
          "content": "# How to Contribute\n\nWe welcome small patches related to bug fixes and documentation, but we do not\nplan to make any major changes to this repository.\n\n## Contributor License Agreement\n\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution,\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to <https://cla.developers.google.com/> to see\nyour current agreements on file or to sign a new one.\n\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\n\n## Code reviews\n\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\n[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more\ninformation on using pull requests.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 34.8310546875,
          "content": "![header](imgs/header.jpg)\n\n# AlphaFold\n\nThis package provides an implementation of the inference pipeline of AlphaFold\nv2. For simplicity, we refer to this model as AlphaFold throughout the rest of\nthis document.\n\nWe also provide:\n\n1.  An implementation of AlphaFold-Multimer. This represents a work in progress\n    and AlphaFold-Multimer isn't expected to be as stable as our monomer\n    AlphaFold system. [Read the guide](#updating-existing-installation) for how\n    to upgrade and update code.\n2.  The [technical note](docs/technical_note_v2.3.0.md) containing the models\n    and inference procedure for an updated AlphaFold v2.3.0.\n3.  A [CASP15 baseline](docs/casp15_predictions.zip) set of predictions along\n    with documentation of any manual interventions performed.\n\nAny publication that discloses findings arising from using this source code or\nthe model parameters should [cite](#citing-this-work) the\n[AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2) and, if\napplicable, the\n[AlphaFold-Multimer paper](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1).\n\nPlease also refer to the\n[Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM1_ESM.pdf)\nfor a detailed description of the method.\n\n**You can use a slightly simplified version of AlphaFold with\n[this Colab notebook](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb)**\nor community-supported versions (see below).\n\nIf you have any questions, please contact the AlphaFold team at\n[alphafold@deepmind.com](mailto:alphafold@deepmind.com).\n\n![CASP14 predictions](imgs/casp14_predictions.gif)\n\n## Installation and running your first prediction\n\nYou will need a machine running Linux, AlphaFold does not support other\noperating systems. Full installation requires up to 3 TB of disk space to keep\ngenetic databases (SSD storage is recommended) and a modern NVIDIA GPU (GPUs\nwith more memory can predict larger protein structures).\n\nPlease follow these steps:\n\n1.  Install [Docker](https://www.docker.com/).\n\n    *   Install\n        [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)\n        for GPU support.\n    *   Setup running\n        [Docker as a non-root user](https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user).\n\n1.  Clone this repository and `cd` into it.\n\n    ```bash\n    git clone https://github.com/deepmind/alphafold.git\n    cd ./alphafold\n    ```\n\n1.  Download genetic databases and model parameters:\n\n    *   Install `aria2c`. On most Linux distributions it is available via the\n        package manager as the `aria2` package (on Debian-based distributions\n        this can be installed by running `sudo apt install aria2`).\n\n    *   Please use the script `scripts/download_all_data.sh` to download and set\n        up full databases. This may take substantial time (download size is 556\n        GB), so we recommend running this script in the background:\n\n    ```bash\n    scripts/download_all_data.sh <DOWNLOAD_DIR> > download.log 2> download_all.log &\n    ```\n\n    *   **Note: The download directory `<DOWNLOAD_DIR>` should *not* be a\n        subdirectory in the AlphaFold repository directory.** If it is, the\n        Docker build will be slow as the large databases will be copied into the\n        docker build context.\n\n    *   It is possible to run AlphaFold with reduced databases; please refer to\n        the [complete documentation](#genetic-databases).\n\n1.  Check that AlphaFold will be able to use a GPU by running:\n\n    ```bash\n    docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\n    ```\n\n    The output of this command should show a list of your GPUs. If it doesn't,\n    check if you followed all steps correctly when setting up the\n    [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)\n    or take a look at the following\n    [NVIDIA Docker issue](https://github.com/NVIDIA/nvidia-docker/issues/1447#issuecomment-801479573).\n\n    If you wish to run AlphaFold using Singularity (a common containerization\n    platform on HPC systems) we recommend using some of the third party\n    Singularity setups as linked in\n    https://github.com/deepmind/alphafold/issues/10 or\n    https://github.com/deepmind/alphafold/issues/24.\n\n1.  Build the Docker image:\n\n    ```bash\n    docker build -f docker/Dockerfile -t alphafold .\n    ```\n\n    If you encounter the following error:\n\n    ```\n    W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n    E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease' is not signed.\n    ```\n\n    use the workaround described in\n    https://github.com/deepmind/alphafold/issues/463#issuecomment-1124881779.\n\n1.  Install the `run_docker.py` dependencies. Note: You may optionally wish to\n    create a\n    [Python Virtual Environment](https://docs.python.org/3/tutorial/venv.html)\n    to prevent conflicts with your system's Python environment.\n\n    ```bash\n    pip3 install -r docker/requirements.txt\n    ```\n\n1.  Make sure that the output directory exists (the default is `/tmp/alphafold`)\n    and that you have sufficient permissions to write into it.\n\n1.  Run `run_docker.py` pointing to a FASTA file containing the protein\n    sequence(s) for which you wish to predict the structure (`--fasta_paths`\n    parameter). AlphaFold will search for the available templates before the\n    date specified by the `--max_template_date` parameter; this could be used to\n    avoid certain templates during modeling. `--data_dir` is the directory with\n    downloaded genetic databases and `--output_dir` is the absolute path to the\n    output directory.\n\n    ```bash\n    python3 docker/run_docker.py \\\n      --fasta_paths=your_protein.fasta \\\n      --max_template_date=2022-01-01 \\\n      --data_dir=$DOWNLOAD_DIR \\\n      --output_dir=/home/user/absolute_path_to_the_output_dir\n    ```\n\n1.  Once the run is over, the output directory shall contain predicted\n    structures of the target protein. Please check the documentation below for\n    additional options and troubleshooting tips.\n\n### Genetic databases\n\nThis step requires `aria2c` to be installed on your machine.\n\nAlphaFold needs multiple genetic (sequence) databases to run:\n\n*   [BFD](https://bfd.mmseqs.com/),\n*   [MGnify](https://www.ebi.ac.uk/metagenomics/),\n*   [PDB70](http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/),\n*   [PDB](https://www.rcsb.org/) (structures in the mmCIF format),\n*   [PDB seqres](https://www.rcsb.org/) – only for AlphaFold-Multimer,\n*   [UniRef30 (FKA UniClust30)](https://uniclust.mmseqs.com/),\n*   [UniProt](https://www.uniprot.org/uniprot/) – only for AlphaFold-Multimer,\n*   [UniRef90](https://www.uniprot.org/help/uniref).\n\nWe provide a script `scripts/download_all_data.sh` that can be used to download\nand set up all of these databases:\n\n*   Recommended default:\n\n    ```bash\n    scripts/download_all_data.sh <DOWNLOAD_DIR>\n    ```\n\n    will download the full databases.\n\n*   With `reduced_dbs` parameter:\n\n    ```bash\n    scripts/download_all_data.sh <DOWNLOAD_DIR> reduced_dbs\n    ```\n\n    will download a reduced version of the databases to be used with the\n    `reduced_dbs` database preset. This shall be used with the corresponding\n    AlphaFold parameter `--db_preset=reduced_dbs` later during the AlphaFold run\n    (please see [AlphaFold parameters](#running-alphafold) section).\n\n:ledger: **Note: The download directory `<DOWNLOAD_DIR>` should *not* be a\nsubdirectory in the AlphaFold repository directory.** If it is, the Docker build\nwill be slow as the large databases will be copied during the image creation.\n\nWe don't provide exactly the database versions used in CASP14 – see the\n[note on reproducibility](#note-on-casp14-reproducibility). Some of the\ndatabases are mirrored for speed, see [mirrored databases](#mirrored-databases).\n\n:ledger: **Note: The total download size for the full databases is around 556 GB\nand the total size when unzipped is 2.62 TB. Please make sure you have a large\nenough hard drive space, bandwidth and time to download. We recommend using an\nSSD for better genetic search performance.**\n\n:ledger: **Note: If the download directory and datasets don't have full read and\nwrite permissions, it can cause errors with the MSA tools, with opaque\n(external) error messages. Please ensure the required permissions are applied,\ne.g. with the `sudo chmod 755 --recursive \"$DOWNLOAD_DIR\"` command.**\n\nThe `download_all_data.sh` script will also download the model parameter files.\nOnce the script has finished, you should have the following directory structure:\n\n```\n$DOWNLOAD_DIR/                             # Total: ~ 2.62 TB (download: 556 GB)\n    bfd/                                   # ~ 1.8 TB (download: 271.6 GB)\n        # 6 files.\n    mgnify/                                # ~ 120 GB (download: 67 GB)\n        mgy_clusters_2022_05.fa\n    params/                                # ~ 5.3 GB (download: 5.3 GB)\n        # 5 CASP14 models,\n        # 5 pTM models,\n        # 5 AlphaFold-Multimer models,\n        # LICENSE,\n        # = 16 files.\n    pdb70/                                 # ~ 56 GB (download: 19.5 GB)\n        # 9 files.\n    pdb_mmcif/                             # ~ 238 GB (download: 43 GB)\n        mmcif_files/\n            # About 199,000 .cif files.\n        obsolete.dat\n    pdb_seqres/                            # ~ 0.2 GB (download: 0.2 GB)\n        pdb_seqres.txt\n    small_bfd/                             # ~ 17 GB (download: 9.6 GB)\n        bfd-first_non_consensus_sequences.fasta\n    uniref30/                              # ~ 206 GB (download: 52.5 GB)\n        # 7 files.\n    uniprot/                               # ~ 105 GB (download: 53 GB)\n        uniprot.fasta\n    uniref90/                              # ~ 67 GB (download: 34 GB)\n        uniref90.fasta\n```\n\n`bfd/` is only downloaded if you download the full databases, and `small_bfd/`\nis only downloaded if you download the reduced databases.\n\n### Model parameters\n\nWhile the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold\nparameters and CASP15 prediction data are made available under the terms of the\nCC BY 4.0 license. Please see the [Disclaimer](#license-and-disclaimer) below\nfor more detail.\n\nThe AlphaFold parameters are available from\nhttps://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar, and\nare downloaded as part of the `scripts/download_all_data.sh` script. This script\nwill download parameters for:\n\n*   5 models which were used during CASP14, and were extensively validated for\n    structure prediction quality (see Jumper et al. 2021, Suppl. Methods 1.12\n    for details).\n*   5 pTM models, which were fine-tuned to produce pTM (predicted TM-score) and\n    (PAE) predicted aligned error values alongside their structure predictions\n    (see Jumper et al. 2021, Suppl. Methods 1.9.7 for details).\n*   5 AlphaFold-Multimer models that produce pTM and PAE values alongside their\n    structure predictions.\n\n### Updating existing installation\n\nIf you have a previous version you can either reinstall fully from scratch\n(remove everything and run the setup from scratch) or you can do an incremental\nupdate that will be significantly faster but will require a bit more work. Make\nsure you follow these steps in the exact order they are listed below:\n\n1.  **Update the code.**\n    *   Go to the directory with the cloned AlphaFold repository and run `git\n        fetch origin main` to get all code updates.\n1.  **Update the UniProt, UniRef, MGnify and PDB seqres databases.**\n    *   Remove `<DOWNLOAD_DIR>/uniprot`.\n    *   Run `scripts/download_uniprot.sh <DOWNLOAD_DIR>`.\n    *   Remove `<DOWNLOAD_DIR>/uniclust30`.\n    *   Run `scripts/download_uniref30.sh <DOWNLOAD_DIR>`.\n    *   Remove `<DOWNLOAD_DIR>/uniref90`.\n    *   Run `scripts/download_uniref90.sh <DOWNLOAD_DIR>`.\n    *   Remove `<DOWNLOAD_DIR>/mgnify`.\n    *   Run `scripts/download_mgnify.sh <DOWNLOAD_DIR>`.\n    *   Remove `<DOWNLOAD_DIR>/pdb_mmcif`. It is needed to have PDB SeqRes and\n        PDB from exactly the same date. Failure to do this step will result in\n        potential errors when searching for templates when running\n        AlphaFold-Multimer.\n    *   Run `scripts/download_pdb_mmcif.sh <DOWNLOAD_DIR>`.\n    *   Run `scripts/download_pdb_seqres.sh <DOWNLOAD_DIR>`.\n1.  **Update the model parameters.**\n    *   Remove the old model parameters in `<DOWNLOAD_DIR>/params`.\n    *   Download new model parameters using\n        `scripts/download_alphafold_params.sh <DOWNLOAD_DIR>`.\n1.  **Follow [Running AlphaFold](#running-alphafold).**\n\n#### Using deprecated model weights\n\nTo use the deprecated v2.2.0 AlphaFold-Multimer model weights:\n\n1.  Change `SOURCE_URL` in `scripts/download_alphafold_params.sh` to\n    `https://storage.googleapis.com/alphafold/alphafold_params_2022-03-02.tar`,\n    and download the old parameters.\n2.  Change the `_v3` to `_v2` in the multimer `MODEL_PRESETS` in `config.py`.\n\nTo use the deprecated v2.1.0 AlphaFold-Multimer model weights:\n\n1.  Change `SOURCE_URL` in `scripts/download_alphafold_params.sh` to\n    `https://storage.googleapis.com/alphafold/alphafold_params_2022-01-19.tar`,\n    and download the old parameters.\n2.  Remove the `_v3` in the multimer `MODEL_PRESETS` in `config.py`.\n\n## Running AlphaFold\n\n**The simplest way to run AlphaFold is using the provided Docker script.** This\nwas tested on Google Cloud with a machine using the `nvidia-gpu-cloud-image`\nwith 12 vCPUs, 85 GB of RAM, a 100 GB boot disk, the databases on an additional\n3 TB disk, and an A100 GPU. For your first run, please follow the instructions\nfrom\n[Installation and running your first prediction](#installation-and-running-your-first-prediction)\nsection.\n\n1.  By default, Alphafold will attempt to use all visible GPU devices. To use a\n    subset, specify a comma-separated list of GPU UUID(s) or index(es) using the\n    `--gpu_devices` flag. See\n    [GPU enumeration](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html#gpu-enumeration)\n    for more details.\n\n1.  You can control which AlphaFold model to run by adding the `--model_preset=`\n    flag. We provide the following models:\n\n    *   **monomer**: This is the original model used at CASP14 with no\n        ensembling.\n\n    *   **monomer\\_casp14**: This is the original model used at CASP14 with\n        `num_ensemble=8`, matching our CASP14 configuration. This is largely\n        provided for reproducibility as it is 8x more computationally expensive\n        for limited accuracy gain (+0.1 average GDT gain on CASP14 domains).\n\n    *   **monomer\\_ptm**: This is the original CASP14 model fine tuned with the\n        pTM head, providing a pairwise confidence measure. It is slightly less\n        accurate than the normal monomer model.\n\n    *   **multimer**: This is the [AlphaFold-Multimer](#citing-this-work) model.\n        To use this model, provide a multi-sequence FASTA file. In addition, the\n        UniProt database should have been downloaded.\n\n1.  You can control MSA speed/quality tradeoff by adding\n    `--db_preset=reduced_dbs` or `--db_preset=full_dbs` to the run command. We\n    provide the following presets:\n\n    *   **reduced\\_dbs**: This preset is optimized for speed and lower hardware\n        requirements. It runs with a reduced version of the BFD database. It\n        requires 8 CPU cores (vCPUs), 8 GB of RAM, and 600 GB of disk space.\n\n    *   **full\\_dbs**: This runs with all genetic databases used at CASP14.\n\n    Running the command above with the `monomer` model preset and the\n    `reduced_dbs` data preset would look like this:\n\n    ```bash\n    python3 docker/run_docker.py \\\n      --fasta_paths=T1050.fasta \\\n      --max_template_date=2020-05-14 \\\n      --model_preset=monomer \\\n      --db_preset=reduced_dbs \\\n      --data_dir=$DOWNLOAD_DIR \\\n      --output_dir=/home/user/absolute_path_to_the_output_dir\n    ```\n\n1.  After generating the predicted model, AlphaFold runs a relaxation step to\n    improve local geometry. By default, only the best model (by pLDDT) is\n    relaxed (`--models_to_relax=best`), but also all of the models\n    (`--models_to_relax=all`) or none of the models (`--models_to_relax=none`)\n    can be relaxed.\n\n1.  The relaxation step can be run on GPU (faster, but could be less stable) or\n    CPU (slow, but stable). This can be controlled with\n    `--enable_gpu_relax=true` (default) or `--enable_gpu_relax=false`.\n\n1.  AlphaFold can re-use MSAs (multiple sequence alignments) for the same\n    sequence via `--use_precomputed_msas=true` option; this can be useful for\n    trying different AlphaFold parameters. This option assumes that the\n    directory structure generated by the first AlphaFold run in the output\n    directory exists and that the protein sequence is the same.\n\n### Running AlphaFold-Multimer\n\nAll steps are the same as when running the monomer system, but you will have to\n\n*   provide an input fasta with multiple sequences,\n*   set `--model_preset=multimer`,\n\nAn example that folds a protein complex `multimer.fasta`:\n\n```bash\npython3 docker/run_docker.py \\\n  --fasta_paths=multimer.fasta \\\n  --max_template_date=2020-05-14 \\\n  --model_preset=multimer \\\n  --data_dir=$DOWNLOAD_DIR \\\n  --output_dir=/home/user/absolute_path_to_the_output_dir\n```\n\nBy default the multimer system will run 5 seeds per model (25 total predictions)\nfor a small drop in accuracy you may wish to run a single seed per model. This\ncan be done via the `--num_multimer_predictions_per_model` flag, e.g. set it to\n`--num_multimer_predictions_per_model=1` to run a single seed per model.\n\n### AlphaFold prediction speed\n\nThe table below reports prediction runtimes for proteins of various lengths. We\nonly measure unrelaxed structure prediction with three recycles while excluding\nruntimes from MSA and template search. When running `docker/run_docker.py` with\n`--benchmark=true`, this runtime is stored in `timings.json`. All runtimes are\nfrom a single A100 NVIDIA GPU. Prediction speed on A100 for smaller structures\ncan be improved by increasing `global_config.subbatch_size` in\n`alphafold/model/config.py`.\n\nNo. residues | Prediction time (s)\n-----------: | ------------------:\n100          | 4.9\n200          | 7.7\n300          | 13\n400          | 18\n500          | 29\n600          | 36\n700          | 53\n800          | 60\n900          | 91\n1,000        | 96\n1,100        | 140\n1,500        | 280\n2,000        | 450\n2,500        | 969\n3,000        | 1,240\n3,500        | 2,465\n4,000        | 5,660\n4,500        | 12,475\n5,000        | 18,824\n\n### Examples\n\nBelow are examples on how to use AlphaFold in different scenarios.\n\n#### Folding a monomer\n\nSay we have a monomer with the sequence `<SEQUENCE>`. The input fasta should be:\n\n```fasta\n>sequence_name\n<SEQUENCE>\n```\n\nThen run the following command:\n\n```bash\npython3 docker/run_docker.py \\\n  --fasta_paths=monomer.fasta \\\n  --max_template_date=2021-11-01 \\\n  --model_preset=monomer \\\n  --data_dir=$DOWNLOAD_DIR \\\n  --output_dir=/home/user/absolute_path_to_the_output_dir\n```\n\n#### Folding a homomer\n\nSay we have a homomer with 3 copies of the same sequence `<SEQUENCE>`. The input\nfasta should be:\n\n```fasta\n>sequence_1\n<SEQUENCE>\n>sequence_2\n<SEQUENCE>\n>sequence_3\n<SEQUENCE>\n```\n\nThen run the following command:\n\n```bash\npython3 docker/run_docker.py \\\n  --fasta_paths=homomer.fasta \\\n  --max_template_date=2021-11-01 \\\n  --model_preset=multimer \\\n  --data_dir=$DOWNLOAD_DIR \\\n  --output_dir=/home/user/absolute_path_to_the_output_dir\n```\n\n#### Folding a heteromer\n\nSay we have an A2B3 heteromer, i.e. with 2 copies of `<SEQUENCE A>` and 3 copies\nof `<SEQUENCE B>`. The input fasta should be:\n\n```fasta\n>sequence_1\n<SEQUENCE A>\n>sequence_2\n<SEQUENCE A>\n>sequence_3\n<SEQUENCE B>\n>sequence_4\n<SEQUENCE B>\n>sequence_5\n<SEQUENCE B>\n```\n\nThen run the following command:\n\n```bash\npython3 docker/run_docker.py \\\n  --fasta_paths=heteromer.fasta \\\n  --max_template_date=2021-11-01 \\\n  --model_preset=multimer \\\n  --data_dir=$DOWNLOAD_DIR \\\n  --output_dir=/home/user/absolute_path_to_the_output_dir\n```\n\n#### Folding multiple monomers one after another\n\nSay we have a two monomers, `monomer1.fasta` and `monomer2.fasta`.\n\nWe can fold both sequentially by using the following command:\n\n```bash\npython3 docker/run_docker.py \\\n  --fasta_paths=monomer1.fasta,monomer2.fasta \\\n  --max_template_date=2021-11-01 \\\n  --model_preset=monomer \\\n  --data_dir=$DOWNLOAD_DIR \\\n  --output_dir=/home/user/absolute_path_to_the_output_dir\n```\n\n#### Folding multiple multimers one after another\n\nSay we have a two multimers, `multimer1.fasta` and `multimer2.fasta`.\n\nWe can fold both sequentially by using the following command:\n\n```bash\npython3 docker/run_docker.py \\\n  --fasta_paths=multimer1.fasta,multimer2.fasta \\\n  --max_template_date=2021-11-01 \\\n  --model_preset=multimer \\\n  --data_dir=$DOWNLOAD_DIR \\\n  --output_dir=/home/user/absolute_path_to_the_output_dir\n```\n\n### AlphaFold output\n\nThe outputs will be saved in a subdirectory of the directory provided via the\n`--output_dir` flag of `run_docker.py` (defaults to `/tmp/alphafold/`). The\noutputs include the computed MSAs, unrelaxed structures, relaxed structures,\nranked structures, raw model outputs, prediction metadata, and section timings.\nThe `--output_dir` directory will have the following structure:\n\n```\n<target_name>/\n    features.pkl\n    ranked_{0,1,2,3,4}.pdb\n    ranking_debug.json\n    relax_metrics.json\n    relaxed_model_{1,2,3,4,5}.pdb\n    result_model_{1,2,3,4,5}.pkl\n    timings.json\n    unrelaxed_model_{1,2,3,4,5}.pdb\n    msas/\n        bfd_uniref_hits.a3m\n        mgnify_hits.sto\n        uniref90_hits.sto\n```\n\nThe contents of each output file are as follows:\n\n*   `features.pkl` – A `pickle` file containing the input feature NumPy arrays\n    used by the models to produce the structures.\n*   `unrelaxed_model_*.pdb` – A PDB format text file containing the predicted\n    structure, exactly as outputted by the model.\n*   `relaxed_model_*.pdb` – A PDB format text file containing the predicted\n    structure, after performing an Amber relaxation procedure on the unrelaxed\n    structure prediction (see Jumper et al. 2021, Suppl. Methods 1.8.6 for\n    details).\n*   `ranked_*.pdb` – A PDB format text file containing the predicted structures,\n    after reordering by model confidence. Here `ranked_i.pdb` should contain the\n    prediction with the (`i + 1`)-th highest confidence (so that `ranked_0.pdb`\n    has the highest confidence). To rank model confidence, we use predicted LDDT\n    (pLDDT) scores (see Jumper et al. 2021, Suppl. Methods 1.9.6 for details).\n    If `--models_to_relax=all` then all ranked structures are relaxed. If\n    `--models_to_relax=best` then only `ranked_0.pdb` is relaxed (the rest are\n    unrelaxed). If `--models_to_relax=none`, then the ranked structures are all\n    unrelaxed.\n*   `ranking_debug.json` – A JSON format text file containing the pLDDT values\n    used to perform the model ranking, and a mapping back to the original model\n    names.\n*   `relax_metrics.json` – A JSON format text file containing relax metrics, for\n    instance remaining violations.\n*   `timings.json` – A JSON format text file containing the times taken to run\n    each section of the AlphaFold pipeline.\n*   `msas/` - A directory containing the files describing the various genetic\n    tool hits that were used to construct the input MSA.\n*   `result_model_*.pkl` – A `pickle` file containing a nested dictionary of the\n    various NumPy arrays directly produced by the model. In addition to the\n    output of the structure module, this includes auxiliary outputs such as:\n\n    *   Distograms (`distogram/logits` contains a NumPy array of shape [N_res,\n        N_res, N_bins] and `distogram/bin_edges` contains the definition of the\n        bins).\n    *   Per-residue pLDDT scores (`plddt` contains a NumPy array of shape\n        [N_res] with the range of possible values from `0` to `100`, where `100`\n        means most confident). This can serve to identify sequence regions\n        predicted with high confidence or as an overall per-target confidence\n        score when averaged across residues.\n    *   Present only if using pTM models: predicted TM-score (`ptm` field\n        contains a scalar). As a predictor of a global superposition metric,\n        this score is designed to also assess whether the model is confident in\n        the overall domain packing.\n    *   Present only if using pTM models: predicted pairwise aligned errors\n        (`predicted_aligned_error` contains a NumPy array of shape [N_res,\n        N_res] with the range of possible values from `0` to\n        `max_predicted_aligned_error`, where `0` means most confident). This can\n        serve for a visualisation of domain packing confidence within the\n        structure.\n\nThe pLDDT confidence measure is stored in the B-factor field of the output PDB\nfiles (although unlike a B-factor, higher pLDDT is better, so care must be taken\nwhen using for tasks such as molecular replacement).\n\nThis code has been tested to match mean top-1 accuracy on a CASP14 test set with\npLDDT ranking over 5 model predictions (some CASP targets were run with earlier\nversions of AlphaFold and some had manual interventions; see our forthcoming\npublication for details). Some targets such as T1064 may also have high\nindividual run variance over random seeds.\n\n## Inferencing many proteins\n\nThe provided inference script is optimized for predicting the structure of a\nsingle protein, and it will compile the neural network to be specialized to\nexactly the size of the sequence, MSA, and templates. For large proteins, the\ncompile time is a negligible fraction of the runtime, but it may become more\nsignificant for small proteins or if the multi-sequence alignments are already\nprecomputed. In the bulk inference case, it may make sense to use our\n`make_fixed_size` function to pad the inputs to a uniform size, thereby reducing\nthe number of compilations required.\n\nWe do not provide a bulk inference script, but it should be straightforward to\ndevelop on top of the `RunModel.predict` method with a parallel system for\nprecomputing multi-sequence alignments. Alternatively, this script can be run\nrepeatedly with only moderate overhead.\n\n## Note on CASP14 reproducibility\n\nAlphaFold's output for a small number of proteins has high inter-run variance,\nand may be affected by changes in the input data. The CASP14 target T1064 is a\nnotable example; the large number of SARS-CoV-2-related sequences recently\ndeposited changes its MSA significantly. This variability is somewhat mitigated\nby the model selection process; running 5 models and taking the most confident.\n\nTo reproduce the results of our CASP14 system as closely as possible you must\nuse the same database versions we used in CASP. These may not match the default\nversions downloaded by our scripts.\n\nFor genetics:\n\n*   UniRef90:\n    [v2020_01](https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2020_01/uniref/)\n*   MGnify:\n    [v2018_12](http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/)\n*   Uniclust30: [v2018_08](http://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/)\n*   BFD: [only version available](https://bfd.mmseqs.com/)\n\nFor templates:\n\n*   PDB: (downloaded 2020-05-14)\n*   PDB70:\n    [2020-05-13](http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/old-releases/pdb70_from_mmcif_200513.tar.gz)\n\nAn alternative for templates is to use the latest PDB and PDB70, but pass the\nflag `--max_template_date=2020-05-14`, which restricts templates only to\nstructures that were available at the start of CASP14.\n\n## Citing this work\n\nIf you use the code or data in this package, please cite:\n\n```bibtex\n@Article{AlphaFold2021,\n  author  = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'\\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n  journal = {Nature},\n  title   = {Highly accurate protein structure prediction with {AlphaFold}},\n  year    = {2021},\n  volume  = {596},\n  number  = {7873},\n  pages   = {583--589},\n  doi     = {10.1038/s41586-021-03819-2}\n}\n```\n\nIn addition, if you use the AlphaFold-Multimer mode, please cite:\n\n```bibtex\n@article {AlphaFold-Multimer2021,\n  author       = {Evans, Richard and O{\\textquoteright}Neill, Michael and Pritzel, Alexander and Antropova, Natasha and Senior, Andrew and Green, Tim and {\\v{Z}}{\\'\\i}dek, Augustin and Bates, Russ and Blackwell, Sam and Yim, Jason and Ronneberger, Olaf and Bodenstein, Sebastian and Zielinski, Michal and Bridgland, Alex and Potapenko, Anna and Cowie, Andrew and Tunyasuvunakool, Kathryn and Jain, Rishub and Clancy, Ellen and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},\n  journal      = {bioRxiv},\n  title        = {Protein complex prediction with AlphaFold-Multimer},\n  year         = {2021},\n  elocation-id = {2021.10.04.463034},\n  doi          = {10.1101/2021.10.04.463034},\n  URL          = {https://www.biorxiv.org/content/early/2021/10/04/2021.10.04.463034},\n  eprint       = {https://www.biorxiv.org/content/early/2021/10/04/2021.10.04.463034.full.pdf},\n}\n```\n\n## Community contributions\n\nColab notebooks provided by the community (please note that these notebooks may\nvary from our full AlphaFold system and we did not validate their accuracy):\n\n*   The\n    [ColabFold AlphaFold2 notebook](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb)\n    by Martin Steinegger, Sergey Ovchinnikov and Milot Mirdita, which uses an\n    API hosted at the Södinglab based on the MMseqs2 server\n    [(Mirdita et al. 2019, Bioinformatics)](https://academic.oup.com/bioinformatics/article/35/16/2856/5280135)\n    for the multiple sequence alignment creation.\n\n## Acknowledgements\n\nAlphaFold communicates with and/or references the following separate libraries\nand packages:\n\n*   [Abseil](https://github.com/abseil/abseil-py)\n*   [Biopython](https://biopython.org)\n*   [Chex](https://github.com/deepmind/chex)\n*   [Colab](https://research.google.com/colaboratory/)\n*   [Docker](https://www.docker.com)\n*   [HH Suite](https://github.com/soedinglab/hh-suite)\n*   [HMMER Suite](http://eddylab.org/software/hmmer)\n*   [Haiku](https://github.com/deepmind/dm-haiku)\n*   [Immutabledict](https://github.com/corenting/immutabledict)\n*   [JAX](https://github.com/google/jax/)\n*   [Kalign](https://msa.sbc.su.se/cgi-bin/msa.cgi)\n*   [matplotlib](https://matplotlib.org/)\n*   [ML Collections](https://github.com/google/ml_collections)\n*   [NumPy](https://numpy.org)\n*   [OpenMM](https://github.com/openmm/openmm)\n*   [OpenStructure](https://openstructure.org)\n*   [pandas](https://pandas.pydata.org/)\n*   [pymol3d](https://github.com/avirshup/py3dmol)\n*   [SciPy](https://scipy.org)\n*   [Sonnet](https://github.com/deepmind/sonnet)\n*   [TensorFlow](https://github.com/tensorflow/tensorflow)\n*   [Tree](https://github.com/deepmind/tree)\n*   [tqdm](https://github.com/tqdm/tqdm)\n\nWe thank all their contributors and maintainers!\n\n## Get in Touch\n\nIf you have any questions not covered in this overview, please contact the\nAlphaFold team at [alphafold@deepmind.com](mailto:alphafold@deepmind.com).\n\nWe would love to hear your feedback and understand how AlphaFold has been useful\nin your research. Share your stories with us at\n[alphafold@deepmind.com](mailto:alphafold@deepmind.com).\n\n## License and Disclaimer\n\nThis is not an officially supported Google product.\n\nCopyright 2022 DeepMind Technologies Limited.\n\nAlphaFold 2 and its output are for theoretical modeling only. They are not\nintended, validated, or approved for clinical use. You should not use the\nAlphaFold 2 or its output for clinical purposes or rely on them for medical or\nother professional advice. Any content regarding those topics is provided for\ninformational purposes only and is not a substitute for advice from a qualified\nprofessional.\n\nOutput of AlphaFold 2 are predictions with varying levels of confidence and\nshould be interpreted carefully. Use discretion before relying on, publishing,\ndownloading or otherwise using AlphaFold 2 and its output.\n\n### AlphaFold Code License\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at https://www.apache.org/licenses/LICENSE-2.0.\n\nUnless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n### Model Parameters License\n\nThe AlphaFold parameters are made available under the terms of the Creative\nCommons Attribution 4.0 International (CC BY 4.0) license. You can find details\nat: https://creativecommons.org/licenses/by/4.0/legalcode\n\n### Third-party software\n\nUse of the third-party software, libraries or code referred to in the\n[Acknowledgements](#acknowledgements) section above may be governed by separate\nterms and conditions or license provisions. Your use of the third-party\nsoftware, libraries or code is subject to any such terms and you should check\nthat you can comply with any applicable restrictions or terms and conditions\nbefore use.\n\n### Mirrored Databases\n\nThe following databases have been mirrored by DeepMind, and are available with\nreference to the following:\n\n*   [BFD](https://bfd.mmseqs.com/) (unmodified), by Steinegger M. and Söding J.,\n    available under a\n    [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).\n\n*   [BFD](https://bfd.mmseqs.com/) (modified), by Steinegger M. and Söding J.,\n    modified by DeepMind, available under a\n    [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).\n    See the Methods section of the\n    [AlphaFold proteome paper](https://www.nature.com/articles/s41586-021-03828-1)\n    for details.\n\n*   [Uniref30: v2021_03](http://wwwuser.gwdg.de/~compbiol/uniclust/2021_03/)\n    (unmodified), by Mirdita M. et al., available under a\n    [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).\n\n*   [MGnify: v2022_05](http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2022_05/README.txt)\n    (unmodified), by Mitchell AL et al., available free of all copyright\n    restrictions and made fully and freely available for both non-commercial and\n    commercial use under\n    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n"
        },
        {
          "name": "afdb",
          "type": "tree",
          "content": null
        },
        {
          "name": "alphafold",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.22265625,
          "content": "absl-py==1.0.0\nbiopython==1.79\nchex==0.1.86\ndm-haiku==0.0.12\ndm-tree==0.1.8\ndocker==5.0.0\nimmutabledict==2.0.0\njax==0.4.26\nml-collections==0.1.0\nnumpy==1.24.3\npandas==2.0.3\nsetuptools<72.0.0\nscipy==1.11.1\ntensorflow-cpu==2.16.1\n"
        },
        {
          "name": "run_alphafold.py",
          "type": "blob",
          "size": 22.0458984375,
          "content": "# Copyright 2021 DeepMind Technologies Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Full AlphaFold protein structure prediction script.\"\"\"\nimport enum\nimport json\nimport os\nimport pathlib\nimport pickle\nimport random\nimport shutil\nimport sys\nimport time\nfrom typing import Any, Dict, Union\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom alphafold.common import confidence\nfrom alphafold.common import protein\nfrom alphafold.common import residue_constants\nfrom alphafold.data import pipeline\nfrom alphafold.data import pipeline_multimer\nfrom alphafold.data import templates\nfrom alphafold.data.tools import hhsearch\nfrom alphafold.data.tools import hmmsearch\nfrom alphafold.model import config\nfrom alphafold.model import data\nfrom alphafold.model import model\nfrom alphafold.relax import relax\nimport jax.numpy as jnp\nimport numpy as np\n\n# Internal import (7716).\n\nlogging.set_verbosity(logging.INFO)\n\n\n@enum.unique\nclass ModelsToRelax(enum.Enum):\n  ALL = 0\n  BEST = 1\n  NONE = 2\n\nflags.DEFINE_list(\n    'fasta_paths', None, 'Paths to FASTA files, each containing a prediction '\n    'target that will be folded one after another. If a FASTA file contains '\n    'multiple sequences, then it will be folded as a multimer. Paths should be '\n    'separated by commas. All FASTA paths must have a unique basename as the '\n    'basename is used to name the output directories for each prediction.')\n\nflags.DEFINE_string('data_dir', None, 'Path to directory of supporting data.')\nflags.DEFINE_string('output_dir', None, 'Path to a directory that will '\n                    'store the results.')\nflags.DEFINE_string('jackhmmer_binary_path', shutil.which('jackhmmer'),\n                    'Path to the JackHMMER executable.')\nflags.DEFINE_string('hhblits_binary_path', shutil.which('hhblits'),\n                    'Path to the HHblits executable.')\nflags.DEFINE_string('hhsearch_binary_path', shutil.which('hhsearch'),\n                    'Path to the HHsearch executable.')\nflags.DEFINE_string('hmmsearch_binary_path', shutil.which('hmmsearch'),\n                    'Path to the hmmsearch executable.')\nflags.DEFINE_string('hmmbuild_binary_path', shutil.which('hmmbuild'),\n                    'Path to the hmmbuild executable.')\nflags.DEFINE_string('kalign_binary_path', shutil.which('kalign'),\n                    'Path to the Kalign executable.')\nflags.DEFINE_string('uniref90_database_path', None, 'Path to the Uniref90 '\n                    'database for use by JackHMMER.')\nflags.DEFINE_string('mgnify_database_path', None, 'Path to the MGnify '\n                    'database for use by JackHMMER.')\nflags.DEFINE_string('bfd_database_path', None, 'Path to the BFD '\n                    'database for use by HHblits.')\nflags.DEFINE_string('small_bfd_database_path', None, 'Path to the small '\n                    'version of BFD used with the \"reduced_dbs\" preset.')\nflags.DEFINE_string('uniref30_database_path', None, 'Path to the UniRef30 '\n                    'database for use by HHblits.')\nflags.DEFINE_string('uniprot_database_path', None, 'Path to the Uniprot '\n                    'database for use by JackHMMer.')\nflags.DEFINE_string('pdb70_database_path', None, 'Path to the PDB70 '\n                    'database for use by HHsearch.')\nflags.DEFINE_string('pdb_seqres_database_path', None, 'Path to the PDB '\n                    'seqres database for use by hmmsearch.')\nflags.DEFINE_string('template_mmcif_dir', None, 'Path to a directory with '\n                    'template mmCIF structures, each named <pdb_id>.cif')\nflags.DEFINE_string('max_template_date', None, 'Maximum template release date '\n                    'to consider. Important if folding historical test sets.')\nflags.DEFINE_string('obsolete_pdbs_path', None, 'Path to file containing a '\n                    'mapping from obsolete PDB IDs to the PDB IDs of their '\n                    'replacements.')\nflags.DEFINE_enum('db_preset', 'full_dbs',\n                  ['full_dbs', 'reduced_dbs'],\n                  'Choose preset MSA database configuration - '\n                  'smaller genetic database config (reduced_dbs) or '\n                  'full genetic database config  (full_dbs)')\nflags.DEFINE_enum('model_preset', 'monomer',\n                  ['monomer', 'monomer_casp14', 'monomer_ptm', 'multimer'],\n                  'Choose preset model configuration - the monomer model, '\n                  'the monomer model with extra ensembling, monomer model with '\n                  'pTM head, or multimer model')\nflags.DEFINE_boolean('benchmark', False, 'Run multiple JAX model evaluations '\n                     'to obtain a timing that excludes the compilation time, '\n                     'which should be more indicative of the time required for '\n                     'inferencing many proteins.')\nflags.DEFINE_integer('random_seed', None, 'The random seed for the data '\n                     'pipeline. By default, this is randomly generated. Note '\n                     'that even if this is set, Alphafold may still not be '\n                     'deterministic, because processes like GPU inference are '\n                     'nondeterministic.')\nflags.DEFINE_integer('num_multimer_predictions_per_model', 5, 'How many '\n                     'predictions (each with a different random seed) will be '\n                     'generated per model. E.g. if this is 2 and there are 5 '\n                     'models then there will be 10 predictions per input. '\n                     'Note: this FLAG only applies if model_preset=multimer')\nflags.DEFINE_boolean('use_precomputed_msas', False, 'Whether to read MSAs that '\n                     'have been written to disk instead of running the MSA '\n                     'tools. The MSA files are looked up in the output '\n                     'directory, so it must stay the same between multiple '\n                     'runs that are to reuse the MSAs. WARNING: This will not '\n                     'check if the sequence, database or configuration have '\n                     'changed.')\nflags.DEFINE_enum_class('models_to_relax', ModelsToRelax.BEST, ModelsToRelax,\n                        'The models to run the final relaxation step on. '\n                        'If `all`, all models are relaxed, which may be time '\n                        'consuming. If `best`, only the most confident model '\n                        'is relaxed. If `none`, relaxation is not run. Turning '\n                        'off relaxation might result in predictions with '\n                        'distracting stereochemical violations but might help '\n                        'in case you are having issues with the relaxation '\n                        'stage.')\nflags.DEFINE_boolean('use_gpu_relax', None, 'Whether to relax on GPU. '\n                     'Relax on GPU can be much faster than CPU, so it is '\n                     'recommended to enable if possible. GPUs must be available'\n                     ' if this setting is enabled.')\n\nFLAGS = flags.FLAGS\n\nMAX_TEMPLATE_HITS = 20\nRELAX_MAX_ITERATIONS = 0\nRELAX_ENERGY_TOLERANCE = 2.39\nRELAX_STIFFNESS = 10.0\nRELAX_EXCLUDE_RESIDUES = []\nRELAX_MAX_OUTER_ITERATIONS = 3\n\n\ndef _check_flag(flag_name: str,\n                other_flag_name: str,\n                should_be_set: bool):\n  if should_be_set != bool(FLAGS[flag_name].value):\n    verb = 'be' if should_be_set else 'not be'\n    raise ValueError(f'{flag_name} must {verb} set when running with '\n                     f'\"--{other_flag_name}={FLAGS[other_flag_name].value}\".')\n\n\ndef _jnp_to_np(output: Dict[str, Any]) -> Dict[str, Any]:\n  \"\"\"Recursively changes jax arrays to numpy arrays.\"\"\"\n  for k, v in output.items():\n    if isinstance(v, dict):\n      output[k] = _jnp_to_np(v)\n    elif isinstance(v, jnp.ndarray):\n      output[k] = np.array(v)\n  return output\n\n\ndef _save_confidence_json_file(\n    plddt: np.ndarray, output_dir: str, model_name: str\n) -> None:\n  confidence_json = confidence.confidence_json(plddt)\n\n  # Save the confidence json.\n  confidence_json_output_path = os.path.join(\n      output_dir, f'confidence_{model_name}.json'\n  )\n  with open(confidence_json_output_path, 'w') as f:\n    f.write(confidence_json)\n\n\ndef _save_mmcif_file(\n    prot: protein.Protein,\n    output_dir: str,\n    model_name: str,\n    file_id: str,\n    model_type: str,\n) -> None:\n  \"\"\"Crate mmCIF string and save to a file.\n\n  Args:\n    prot: Protein object.\n    output_dir: Directory to which files are saved.\n    model_name: Name of a model.\n    file_id: The file ID (usually the PDB ID) to be used in the mmCIF.\n    model_type: Monomer or multimer.\n  \"\"\"\n\n  mmcif_string = protein.to_mmcif(prot, file_id, model_type)\n\n  # Save the MMCIF.\n  mmcif_output_path = os.path.join(output_dir, f'{model_name}.cif')\n  with open(mmcif_output_path, 'w') as f:\n    f.write(mmcif_string)\n\n\ndef _save_pae_json_file(\n    pae: np.ndarray, max_pae: float, output_dir: str, model_name: str\n) -> None:\n  \"\"\"Check prediction result for PAE data and save to a JSON file if present.\n\n  Args:\n    pae: The n_res x n_res PAE array.\n    max_pae: The maximum possible PAE value.\n    output_dir: Directory to which files are saved.\n    model_name: Name of a model.\n  \"\"\"\n  pae_json = confidence.pae_json(pae, max_pae)\n\n  # Save the PAE json.\n  pae_json_output_path = os.path.join(output_dir, f'pae_{model_name}.json')\n  with open(pae_json_output_path, 'w') as f:\n    f.write(pae_json)\n\n\ndef predict_structure(\n    fasta_path: str,\n    fasta_name: str,\n    output_dir_base: str,\n    data_pipeline: Union[pipeline.DataPipeline, pipeline_multimer.DataPipeline],\n    model_runners: Dict[str, model.RunModel],\n    amber_relaxer: relax.AmberRelaxation,\n    benchmark: bool,\n    random_seed: int,\n    models_to_relax: ModelsToRelax,\n    model_type: str,\n):\n  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n  logging.info('Predicting %s', fasta_name)\n  timings = {}\n  output_dir = os.path.join(output_dir_base, fasta_name)\n  if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n  msa_output_dir = os.path.join(output_dir, 'msas')\n  if not os.path.exists(msa_output_dir):\n    os.makedirs(msa_output_dir)\n\n  # Get features.\n  t_0 = time.time()\n  feature_dict = data_pipeline.process(\n      input_fasta_path=fasta_path,\n      msa_output_dir=msa_output_dir)\n  timings['features'] = time.time() - t_0\n\n  # Write out features as a pickled dictionary.\n  features_output_path = os.path.join(output_dir, 'features.pkl')\n  with open(features_output_path, 'wb') as f:\n    pickle.dump(feature_dict, f, protocol=4)\n\n  unrelaxed_pdbs = {}\n  unrelaxed_proteins = {}\n  relaxed_pdbs = {}\n  relax_metrics = {}\n  ranking_confidences = {}\n\n  # Run the models.\n  num_models = len(model_runners)\n  for model_index, (model_name, model_runner) in enumerate(\n      model_runners.items()):\n    logging.info('Running model %s on %s', model_name, fasta_name)\n    t_0 = time.time()\n    model_random_seed = model_index + random_seed * num_models\n    processed_feature_dict = model_runner.process_features(\n        feature_dict, random_seed=model_random_seed)\n    timings[f'process_features_{model_name}'] = time.time() - t_0\n\n    t_0 = time.time()\n    prediction_result = model_runner.predict(processed_feature_dict,\n                                             random_seed=model_random_seed)\n    t_diff = time.time() - t_0\n    timings[f'predict_and_compile_{model_name}'] = t_diff\n    logging.info(\n        'Total JAX model %s on %s predict time (includes compilation time, see --benchmark): %.1fs',\n        model_name, fasta_name, t_diff)\n\n    if benchmark:\n      t_0 = time.time()\n      model_runner.predict(processed_feature_dict,\n                           random_seed=model_random_seed)\n      t_diff = time.time() - t_0\n      timings[f'predict_benchmark_{model_name}'] = t_diff\n      logging.info(\n          'Total JAX model %s on %s predict time (excludes compilation time): %.1fs',\n          model_name, fasta_name, t_diff)\n\n    plddt = prediction_result['plddt']\n    _save_confidence_json_file(plddt, output_dir, model_name)\n    ranking_confidences[model_name] = prediction_result['ranking_confidence']\n\n    if (\n        'predicted_aligned_error' in prediction_result\n        and 'max_predicted_aligned_error' in prediction_result\n    ):\n      pae = prediction_result['predicted_aligned_error']\n      max_pae = prediction_result['max_predicted_aligned_error']\n      _save_pae_json_file(pae, float(max_pae), output_dir, model_name)\n\n    # Remove jax dependency from results.\n    np_prediction_result = _jnp_to_np(dict(prediction_result))\n\n    # Save the model outputs.\n    result_output_path = os.path.join(output_dir, f'result_{model_name}.pkl')\n    with open(result_output_path, 'wb') as f:\n      pickle.dump(np_prediction_result, f, protocol=4)\n\n    # Add the predicted LDDT in the b-factor column.\n    # Note that higher predicted LDDT value means higher model confidence.\n    plddt_b_factors = np.repeat(\n        plddt[:, None], residue_constants.atom_type_num, axis=-1)\n    unrelaxed_protein = protein.from_prediction(\n        features=processed_feature_dict,\n        result=prediction_result,\n        b_factors=plddt_b_factors,\n        remove_leading_feature_dimension=not model_runner.multimer_mode)\n\n    unrelaxed_proteins[model_name] = unrelaxed_protein\n    unrelaxed_pdbs[model_name] = protein.to_pdb(unrelaxed_protein)\n    unrelaxed_pdb_path = os.path.join(output_dir, f'unrelaxed_{model_name}.pdb')\n    with open(unrelaxed_pdb_path, 'w') as f:\n      f.write(unrelaxed_pdbs[model_name])\n\n    _save_mmcif_file(\n        prot=unrelaxed_protein,\n        output_dir=output_dir,\n        model_name=f'unrelaxed_{model_name}',\n        file_id=str(model_index),\n        model_type=model_type,\n    )\n\n  # Rank by model confidence.\n  ranked_order = [\n      model_name for model_name, confidence in\n      sorted(ranking_confidences.items(), key=lambda x: x[1], reverse=True)]\n\n  # Relax predictions.\n  if models_to_relax == ModelsToRelax.BEST:\n    to_relax = [ranked_order[0]]\n  elif models_to_relax == ModelsToRelax.ALL:\n    to_relax = ranked_order\n  elif models_to_relax == ModelsToRelax.NONE:\n    to_relax = []\n\n  for model_name in to_relax:\n    t_0 = time.time()\n    relaxed_pdb_str, _, violations = amber_relaxer.process(\n        prot=unrelaxed_proteins[model_name])\n    relax_metrics[model_name] = {\n        'remaining_violations': violations,\n        'remaining_violations_count': sum(violations)\n    }\n    timings[f'relax_{model_name}'] = time.time() - t_0\n\n    relaxed_pdbs[model_name] = relaxed_pdb_str\n\n    # Save the relaxed PDB.\n    relaxed_output_path = os.path.join(\n        output_dir, f'relaxed_{model_name}.pdb')\n    with open(relaxed_output_path, 'w') as f:\n      f.write(relaxed_pdb_str)\n\n    relaxed_protein = protein.from_pdb_string(relaxed_pdb_str)\n    _save_mmcif_file(\n        prot=relaxed_protein,\n        output_dir=output_dir,\n        model_name=f'relaxed_{model_name}',\n        file_id='0',\n        model_type=model_type,\n    )\n\n  # Write out relaxed PDBs in rank order.\n  for idx, model_name in enumerate(ranked_order):\n    ranked_output_path = os.path.join(output_dir, f'ranked_{idx}.pdb')\n    with open(ranked_output_path, 'w') as f:\n      if model_name in relaxed_pdbs:\n        f.write(relaxed_pdbs[model_name])\n      else:\n        f.write(unrelaxed_pdbs[model_name])\n\n    if model_name in relaxed_pdbs:\n      protein_instance = protein.from_pdb_string(relaxed_pdbs[model_name])\n    else:\n      protein_instance = protein.from_pdb_string(unrelaxed_pdbs[model_name])\n\n    _save_mmcif_file(\n        prot=protein_instance,\n        output_dir=output_dir,\n        model_name=f'ranked_{idx}',\n        file_id=str(idx),\n        model_type=model_type,\n    )\n\n  ranking_output_path = os.path.join(output_dir, 'ranking_debug.json')\n  with open(ranking_output_path, 'w') as f:\n    label = 'iptm+ptm' if 'iptm' in prediction_result else 'plddts'\n    f.write(json.dumps(\n        {label: ranking_confidences, 'order': ranked_order}, indent=4))\n\n  logging.info('Final timings for %s: %s', fasta_name, timings)\n\n  timings_output_path = os.path.join(output_dir, 'timings.json')\n  with open(timings_output_path, 'w') as f:\n    f.write(json.dumps(timings, indent=4))\n  if models_to_relax != ModelsToRelax.NONE:\n    relax_metrics_path = os.path.join(output_dir, 'relax_metrics.json')\n    with open(relax_metrics_path, 'w') as f:\n      f.write(json.dumps(relax_metrics, indent=4))\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError('Too many command-line arguments.')\n\n  for tool_name in (\n      'jackhmmer', 'hhblits', 'hhsearch', 'hmmsearch', 'hmmbuild', 'kalign'):\n    if not FLAGS[f'{tool_name}_binary_path'].value:\n      raise ValueError(f'Could not find path to the \"{tool_name}\" binary. Make '\n                       'sure it is installed on your system.')\n\n  use_small_bfd = FLAGS.db_preset == 'reduced_dbs'\n  _check_flag('small_bfd_database_path', 'db_preset',\n              should_be_set=use_small_bfd)\n  _check_flag('bfd_database_path', 'db_preset',\n              should_be_set=not use_small_bfd)\n  _check_flag('uniref30_database_path', 'db_preset',\n              should_be_set=not use_small_bfd)\n\n  run_multimer_system = 'multimer' in FLAGS.model_preset\n  model_type = 'Multimer' if run_multimer_system else 'Monomer'\n  _check_flag('pdb70_database_path', 'model_preset',\n              should_be_set=not run_multimer_system)\n  _check_flag('pdb_seqres_database_path', 'model_preset',\n              should_be_set=run_multimer_system)\n  _check_flag('uniprot_database_path', 'model_preset',\n              should_be_set=run_multimer_system)\n\n  if FLAGS.model_preset == 'monomer_casp14':\n    num_ensemble = 8\n  else:\n    num_ensemble = 1\n\n  # Check for duplicate FASTA file names.\n  fasta_names = [pathlib.Path(p).stem for p in FLAGS.fasta_paths]\n  if len(fasta_names) != len(set(fasta_names)):\n    raise ValueError('All FASTA paths must have a unique basename.')\n\n  if run_multimer_system:\n    template_searcher = hmmsearch.Hmmsearch(\n        binary_path=FLAGS.hmmsearch_binary_path,\n        hmmbuild_binary_path=FLAGS.hmmbuild_binary_path,\n        database_path=FLAGS.pdb_seqres_database_path)\n    template_featurizer = templates.HmmsearchHitFeaturizer(\n        mmcif_dir=FLAGS.template_mmcif_dir,\n        max_template_date=FLAGS.max_template_date,\n        max_hits=MAX_TEMPLATE_HITS,\n        kalign_binary_path=FLAGS.kalign_binary_path,\n        release_dates_path=None,\n        obsolete_pdbs_path=FLAGS.obsolete_pdbs_path)\n  else:\n    template_searcher = hhsearch.HHSearch(\n        binary_path=FLAGS.hhsearch_binary_path,\n        databases=[FLAGS.pdb70_database_path])\n    template_featurizer = templates.HhsearchHitFeaturizer(\n        mmcif_dir=FLAGS.template_mmcif_dir,\n        max_template_date=FLAGS.max_template_date,\n        max_hits=MAX_TEMPLATE_HITS,\n        kalign_binary_path=FLAGS.kalign_binary_path,\n        release_dates_path=None,\n        obsolete_pdbs_path=FLAGS.obsolete_pdbs_path)\n\n  monomer_data_pipeline = pipeline.DataPipeline(\n      jackhmmer_binary_path=FLAGS.jackhmmer_binary_path,\n      hhblits_binary_path=FLAGS.hhblits_binary_path,\n      uniref90_database_path=FLAGS.uniref90_database_path,\n      mgnify_database_path=FLAGS.mgnify_database_path,\n      bfd_database_path=FLAGS.bfd_database_path,\n      uniref30_database_path=FLAGS.uniref30_database_path,\n      small_bfd_database_path=FLAGS.small_bfd_database_path,\n      template_searcher=template_searcher,\n      template_featurizer=template_featurizer,\n      use_small_bfd=use_small_bfd,\n      use_precomputed_msas=FLAGS.use_precomputed_msas)\n\n  if run_multimer_system:\n    num_predictions_per_model = FLAGS.num_multimer_predictions_per_model\n    data_pipeline = pipeline_multimer.DataPipeline(\n        monomer_data_pipeline=monomer_data_pipeline,\n        jackhmmer_binary_path=FLAGS.jackhmmer_binary_path,\n        uniprot_database_path=FLAGS.uniprot_database_path,\n        use_precomputed_msas=FLAGS.use_precomputed_msas)\n  else:\n    num_predictions_per_model = 1\n    data_pipeline = monomer_data_pipeline\n\n  model_runners = {}\n  model_names = config.MODEL_PRESETS[FLAGS.model_preset]\n  for model_name in model_names:\n    model_config = config.model_config(model_name)\n    if run_multimer_system:\n      model_config.model.num_ensemble_eval = num_ensemble\n    else:\n      model_config.data.eval.num_ensemble = num_ensemble\n    model_params = data.get_model_haiku_params(\n        model_name=model_name, data_dir=FLAGS.data_dir)\n    model_runner = model.RunModel(model_config, model_params)\n    for i in range(num_predictions_per_model):\n      model_runners[f'{model_name}_pred_{i}'] = model_runner\n\n  logging.info('Have %d models: %s', len(model_runners),\n               list(model_runners.keys()))\n\n  amber_relaxer = relax.AmberRelaxation(\n      max_iterations=RELAX_MAX_ITERATIONS,\n      tolerance=RELAX_ENERGY_TOLERANCE,\n      stiffness=RELAX_STIFFNESS,\n      exclude_residues=RELAX_EXCLUDE_RESIDUES,\n      max_outer_iterations=RELAX_MAX_OUTER_ITERATIONS,\n      use_gpu=FLAGS.use_gpu_relax)\n\n  random_seed = FLAGS.random_seed\n  if random_seed is None:\n    random_seed = random.randrange(sys.maxsize // len(model_runners))\n  logging.info('Using random seed %d for the data pipeline', random_seed)\n\n  # Predict structure for each of the sequences.\n  for i, fasta_path in enumerate(FLAGS.fasta_paths):\n    fasta_name = fasta_names[i]\n    predict_structure(\n        fasta_path=fasta_path,\n        fasta_name=fasta_name,\n        output_dir_base=FLAGS.output_dir,\n        data_pipeline=data_pipeline,\n        model_runners=model_runners,\n        amber_relaxer=amber_relaxer,\n        benchmark=FLAGS.benchmark,\n        random_seed=random_seed,\n        models_to_relax=FLAGS.models_to_relax,\n        model_type=model_type,\n    )\n\n\nif __name__ == '__main__':\n  flags.mark_flags_as_required([\n      'fasta_paths',\n      'output_dir',\n      'data_dir',\n      'uniref90_database_path',\n      'mgnify_database_path',\n      'template_mmcif_dir',\n      'max_template_date',\n      'obsolete_pdbs_path',\n      'use_gpu_relax',\n  ])\n\n  app.run(main)\n"
        },
        {
          "name": "run_alphafold_test.py",
          "type": "blob",
          "size": 4.18359375,
          "content": "# Copyright 2021 DeepMind Technologies Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for run_alphafold.\"\"\"\n\nimport json\nimport os\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport run_alphafold\nimport mock\nimport numpy as np\n# Internal import (7716).\n\nTEST_DATA_DIR = 'alphafold/common/testdata/'\n\n\nclass RunAlphafoldTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      ('relax', run_alphafold.ModelsToRelax.ALL),\n      ('no_relax', run_alphafold.ModelsToRelax.NONE),\n  )\n  def test_end_to_end(self, models_to_relax):\n\n    data_pipeline_mock = mock.Mock()\n    model_runner_mock = mock.Mock()\n    amber_relaxer_mock = mock.Mock()\n\n    data_pipeline_mock.process.return_value = {}\n    model_runner_mock.process_features.return_value = {\n        'aatype': np.zeros((12, 10), dtype=np.int32),\n        'residue_index': np.tile(np.arange(10, dtype=np.int32)[None], (12, 1)),\n    }\n    model_runner_mock.predict.return_value = {\n        'structure_module': {\n            'final_atom_positions': np.zeros((10, 37, 3)),\n            'final_atom_mask': np.ones((10, 37)),\n        },\n        'predicted_lddt': {\n            'logits': np.ones((10, 50)),\n        },\n        'plddt': np.ones(10) * 42,\n        'ranking_confidence': 90,\n        'ptm': np.array(0.),\n        'aligned_confidence_probs': np.zeros((10, 10, 50)),\n        'predicted_aligned_error': np.zeros((10, 10)),\n        'max_predicted_aligned_error': np.array(0.),\n    }\n    model_runner_mock.multimer_mode = False\n\n    with open(\n        os.path.join(\n            absltest.get_default_test_srcdir(), TEST_DATA_DIR, 'glucagon.pdb'\n        )\n    ) as f:\n      pdb_string = f.read()\n    amber_relaxer_mock.process.return_value = (\n        pdb_string,\n        None,\n        [1.0, 0.0, 0.0],\n    )\n\n    out_dir = self.create_tempdir().full_path\n    fasta_path = os.path.join(out_dir, 'target.fasta')\n    with open(fasta_path, 'wt') as f:\n      f.write('>A\\nAAAAAAAAAAAAA')\n    fasta_name = 'test'\n\n    run_alphafold.predict_structure(\n        fasta_path=fasta_path,\n        fasta_name=fasta_name,\n        output_dir_base=out_dir,\n        data_pipeline=data_pipeline_mock,\n        model_runners={'model1': model_runner_mock},\n        amber_relaxer=amber_relaxer_mock,\n        benchmark=False,\n        random_seed=0,\n        models_to_relax=models_to_relax,\n        model_type='Monomer',\n    )\n\n    base_output_files = os.listdir(out_dir)\n    self.assertIn('target.fasta', base_output_files)\n    self.assertIn('test', base_output_files)\n\n    target_output_files = os.listdir(os.path.join(out_dir, 'test'))\n    expected_files = [\n        'confidence_model1.json',\n        'features.pkl',\n        'msas',\n        'pae_model1.json',\n        'ranked_0.cif',\n        'ranked_0.pdb',\n        'ranking_debug.json',\n        'result_model1.pkl',\n        'timings.json',\n        'unrelaxed_model1.cif',\n        'unrelaxed_model1.pdb',\n    ]\n    if models_to_relax == run_alphafold.ModelsToRelax.ALL:\n      expected_files.extend(\n          ['relaxed_model1.cif', 'relaxed_model1.pdb', 'relax_metrics.json']\n      )\n      with open(os.path.join(out_dir, 'test', 'relax_metrics.json')) as f:\n        relax_metrics = json.loads(f.read())\n      self.assertDictEqual({'model1': {'remaining_violations': [1.0, 0.0, 0.0],\n                                       'remaining_violations_count': 1.0}},\n                           relax_metrics)\n    self.assertCountEqual(expected_files, target_output_files)\n\n    # Check that pLDDT is set in the B-factor column.\n    with open(os.path.join(out_dir, 'test', 'unrelaxed_model1.pdb')) as f:\n      for line in f:\n        if line.startswith('ATOM'):\n          self.assertEqual(line[61:66], '42.00')\n\n\nif __name__ == '__main__':\n  absltest.main()\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "server",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.0810546875,
          "content": "# Copyright 2021 DeepMind Technologies Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Install script for setuptools.\"\"\"\n\nfrom alphafold import version\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nsetup(\n    name='alphafold',\n    version=version.__version__,\n    description=(\n        'An implementation of the inference pipeline of AlphaFold v2.0. This is'\n        ' a completely new model that was entered as AlphaFold2 in CASP14 and'\n        ' published in Nature.'\n    ),\n    author='DeepMind',\n    author_email='alphafold@deepmind.com',\n    license='Apache License, Version 2.0',\n    url='https://github.com/deepmind/alphafold',\n    packages=find_packages(),\n    install_requires=[\n        'absl-py',\n        'biopython',\n        'chex',\n        'dm-haiku',\n        'dm-tree',\n        'docker',\n        'immutabledict',\n        'jax',\n        'ml-collections',\n        'numpy',\n        'pandas',\n        'scipy',\n        'tensorflow-cpu',\n    ],\n    tests_require=[\n        'matplotlib',  # For notebook_utils_test.\n        'mock',\n    ],\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: Apache Software License',\n        'Operating System :: POSIX :: Linux',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    ],\n)\n"
        }
      ]
    }
  ]
}