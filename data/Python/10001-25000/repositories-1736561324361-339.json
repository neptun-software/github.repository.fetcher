{
  "metadata": {
    "timestamp": 1736561324361,
    "page": 339,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "instantX-research/InstantID",
      "stars": 11294,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.0732421875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\nhuggingface/\ncheckpoints/\nmodels/\n\n# Cog\n.cog\n\ngradio_cached_examples"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08984375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.271484375,
          "content": "<div align=\"center\">\n<h1>InstantID: Zero-shot Identity-Preserving Generation in Seconds</h1>\n\n[**Qixun Wang**](https://github.com/wangqixun)<sup>12</sup> 路 [**Xu Bai**](https://huggingface.co/baymin0220)<sup>12</sup> 路 [**Haofan Wang**](https://haofanwang.github.io/)<sup>12*</sup> 路 [**Zekui Qin**](https://github.com/ZekuiQin)<sup>12</sup> 路 [**Anthony Chen**](https://antonioo-c.github.io/)<sup>123</sup>\n\nHuaxia Li<sup>2</sup> 路 Xu Tang<sup>2</sup> 路 Yao Hu<sup>2</sup>\n\n<sup>1</sup>InstantX Team 路 <sup>2</sup>Xiaohongshu Inc 路 <sup>3</sup>Peking University\n\n<sup>*</sup>corresponding authors\n\n<a href='https://instantid.github.io/'><img src='https://img.shields.io/badge/Project-Page-green'></a>\n<a href='https://arxiv.org/abs/2401.07519'><img src='https://img.shields.io/badge/Technique-Report-red'></a>\n<a href='https://huggingface.co/papers/2401.07519'><img src='https://img.shields.io/static/v1?label=Paper&message=Huggingface&color=orange'></a> \n[![GitHub](https://img.shields.io/github/stars/InstantID/InstantID?style=social)](https://github.com/InstantID/InstantID)\n\n<a href='https://huggingface.co/spaces/InstantX/InstantID'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a>\n[![ModelScope](https://img.shields.io/badge/ModelScope-Studios-blue)](https://modelscope.cn/studios/instantx/InstantID/summary)\n[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/InstantX/InstantID)\n\n</div>\n\nInstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks.\n\n<img src='assets/applications.png'>\n\n## Release\n- [2024/07/18]  We are training InstantID for [Kolors](https://huggingface.co/Kwai-Kolors/Kolors-diffusers). The weight requires significant computational power, which is currently in the process of iteration. After the model training is completed, it will be open-sourced. The latest checkpoint results are referenced in [Kolors Version](#kolors-version). \n- [2024/04/03]  We release our recent work [InstantStyle](https://github.com/InstantStyle/InstantStyle) for style transfer, compatible with InstantID!\n- [2024/02/01]  We have supported LCM acceleration and Multi-ControlNets on our [Huggingface Spaces Demo](https://huggingface.co/spaces/InstantX/InstantID)! Our depth estimator is supported by [Depth-Anything](https://github.com/LiheYoung/Depth-Anything).\n- [2024/01/31]  [OneDiff](https://github.com/siliconflow/onediff?tab=readme-ov-file#easy-to-use) now supports accelerated inference for InstantID, check [this](https://github.com/siliconflow/onediff/blob/main/benchmarks/instant_id.py) for details!\n- [2024/01/23]  Our pipeline has been merged into [diffusers](https://github.com/huggingface/diffusers/blob/main/examples/community/pipeline_stable_diffusion_xl_instantid.py)!\n- [2024/01/22]  We release the [pre-trained checkpoints](https://huggingface.co/InstantX/InstantID), [inference code](https://github.com/InstantID/InstantID/blob/main/infer.py) and [gradio demo](https://huggingface.co/spaces/InstantX/InstantID)!\n- [2024/01/15]  We release the [technical report](https://arxiv.org/abs/2401.07519).\n- [2023/12/11]  We launch the [project page](https://instantid.github.io/).\n\n## Demos\n\n### Stylized Synthesis\n\n<p align=\"center\">\n  <img src=\"assets/StylizedSynthesis.png\">\n</p>\n\n### Comparison with Previous Works\n\n<p align=\"center\">\n  <img src=\"assets/compare-a.png\">\n</p>\n\nComparison with existing tuning-free state-of-the-art techniques. InstantID achieves better fidelity and retain good text editability (faces and styles blend better).\n\n<p align=\"center\">\n  <img src=\"assets/compare-c.png\">\n</p>\n\nComparison with pre-trained character LoRAs. We don't need multiple images and still can achieve competitive results as LoRAs without any training.\n\n<p align=\"center\">\n  <img src=\"assets/compare-b.png\">\n</p>\n\nComparison with InsightFace Swapper (also known as ROOP or Refactor). However, in non-realistic style, our work is more flexible on the integration of face and background.\n\n### Kolors Version\n\nWe have adapted InstantID for [Kolors](https://huggingface.co/Kwai-Kolors/Kolors-diffusers). Leveraging Kolors' robust text generation capabilities , InstantID can be integrated with Kolors to simultaneously generate **ID** and **text**.\n\n\n| demo | demo | demo |\n|:-----:|:-----:|:-----:|\n<img src=\"./assets/kolor/demo_1.jpg\" >|<img src=\"./assets/kolor/demo_2.jpg\" >|<img src=\"./assets/kolor/demo_3.jpg\" >|\n\n\n\n## Download\n\nYou can directly download the model from [Huggingface](https://huggingface.co/InstantX/InstantID).\nYou also can download the model in python script:\n\n```python\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/config.json\", local_dir=\"./checkpoints\")\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/diffusion_pytorch_model.safetensors\", local_dir=\"./checkpoints\")\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ip-adapter.bin\", local_dir=\"./checkpoints\")\n```\n\nOr run the following command to download all models:\n\n```python\npip install -r gradio_demo/requirements.txt\npython gradio_demo/download_models.py\n```\n\nIf you cannot access to Huggingface, you can use [hf-mirror](https://hf-mirror.com/) to download models.\n```python\nexport HF_ENDPOINT=https://hf-mirror.com\nhuggingface-cli download --resume-download InstantX/InstantID --local-dir checkpoints --local-dir-use-symlinks False\n```\n\nFor face encoder, you need to manually download via this [URL](https://github.com/deepinsight/insightface/issues/1896#issuecomment-1023867304) to `models/antelopev2` as the default link is invalid. Once you have prepared all models, the folder tree should be like:\n\n```\n  .\n   models\n   checkpoints\n   ip_adapter\n   pipeline_stable_diffusion_xl_instantid.py\n   README.md\n```\n\n## Usage\n\nIf you want to reproduce results in the paper, please refer to the code in [infer_full.py](infer_full.py). If you want to compare the results with other methods, even without using depth-controlnet, it is recommended that you use this code. \n\nIf you are pursuing better results, it is recommended to follow [InstantID-Rome](https://github.com/instantX-research/InstantID-Rome).\n\nThe following code comes from [infer.py](infer.py). If you want to quickly experience InstantID, please refer to the code in [infer.py](infer.py). \n\n\n\n```python\n# !pip install opencv-python transformers accelerate insightface\nimport diffusers\nfrom diffusers.utils import load_image\nfrom diffusers.models import ControlNetModel\n\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\n# prepare 'antelopev2' under ./models\napp = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\n# prepare models under ./checkpoints\nface_adapter = f'./checkpoints/ip-adapter.bin'\ncontrolnet_path = f'./checkpoints/ControlNetModel'\n\n# load IdentityNet\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n\nbase_model = 'wangqixun/YamerMIX_v8'  # from https://civitai.com/models/84040?modelVersionId=196039\npipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n    base_model,\n    controlnet=controlnet,\n    torch_dtype=torch.float16\n)\npipe.cuda()\n\n# load adapter\npipe.load_ip_adapter_instantid(face_adapter)\n```\n\nThen, you can customized your own face images\n\n```python\n# load an image\nface_image = load_image(\"./examples/yann-lecun_resize.jpg\")\n\n# prepare face emb\nface_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\nface_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1]  # only use the maximum face\nface_emb = face_info['embedding']\nface_kps = draw_kps(face_image, face_info['kps'])\n\n# prompt\nprompt = \"film noir style, ink sketch|vector, male man, highly detailed, sharp focus, ultra sharpness, monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic\"\nnegative_prompt = \"ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, vibrant, colorful\"\n\n# generate image\nimage = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    image_embeds=face_emb,\n    image=face_kps,\n    controlnet_conditioning_scale=0.8,\n    ip_adapter_scale=0.8,\n).images[0]\n```\n\nTo save VRAM, you can enable CPU offloading\n```python\npipe.enable_model_cpu_offload()\npipe.enable_vae_tiling()\n```\n\n## Speed Up with LCM-LoRA\n\nOur work is compatible with [LCM-LoRA](https://github.com/luosiallen/latent-consistency-model). First, download the model.\n\n```python\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"latent-consistency/lcm-lora-sdxl\", filename=\"pytorch_lora_weights.safetensors\", local_dir=\"./checkpoints\")\n```\n\nTo use it, you just need to load it and infer with a small num_inference_steps. Note that it is recommendated to set guidance_scale between [0, 1].\n```python\nfrom diffusers import LCMScheduler\n\nlcm_lora_path = \"./checkpoints/pytorch_lora_weights.safetensors\"\n\npipe.load_lora_weights(lcm_lora_path)\npipe.fuse_lora()\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\nnum_inference_steps = 10\nguidance_scale = 0\n```\n\n## Start a local gradio demo <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>\nRun the following command:\n\n```python\npython gradio_demo/app.py\n```\n\nor MultiControlNet version:\n```python\ngradio_demo/app-multicontrolnet.py \n```\n\n## Usage Tips\n- For higher similarity, increase the weight of controlnet_conditioning_scale (IdentityNet) and ip_adapter_scale (Adapter).\n- For over-saturation, decrease the ip_adapter_scale. If not work, decrease controlnet_conditioning_scale.\n- For higher text control ability, decrease ip_adapter_scale.\n- For specific styles, choose corresponding base model makes differences.\n- We have not supported multi-person yet, only use the largest face as reference facial landmarks.\n- We provide a [style template](https://github.com/ahgsql/StyleSelectorXL/blob/main/sdxl_styles.json) for reference.\n\n## Community Resources\n\n### Replicate Demo\n- [zsxkib/instant-id](https://replicate.com/zsxkib/instant-id)\n\n### WebUI\n- [Mikubill/sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet/discussions/2589)\n\n### ComfyUI\n- [cubiq/ComfyUI_InstantID](https://github.com/cubiq/ComfyUI_InstantID)\n- [ZHO-ZHO-ZHO/ComfyUI-InstantID](https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID)\n- [huxiuhan/ComfyUI-InstantID](https://github.com/huxiuhan/ComfyUI-InstantID)\n\n### Windows\n- [sdbds/InstantID-for-windows](https://github.com/sdbds/InstantID-for-windows)\n\n## Acknowledgements\n- InstantID is developed by InstantX Team, all copyright reserved.\n- Our work is highly inspired by [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter) and [ControlNet](https://github.com/lllyasviel/ControlNet). Thanks for their great works!\n- Thanks [Yamer](https://civitai.com/user/Yamer) for developing [YamerMIX](https://civitai.com/models/84040?modelVersionId=196039), we use it as base model in our demo.\n- Thanks [ZHO-ZHO-ZHO](https://github.com/ZHO-ZHO-ZHO), [huxiuhan](https://github.com/huxiuhan), [sdbds](https://github.com/sdbds), [zsxkib](https://replicate.com/zsxkib) for their generous contributions.\n- Thanks to the [HuggingFace](https://github.com/huggingface) gradio team for their free GPU support!\n- Thanks to the [ModelScope](https://github.com/modelscope/modelscope) team for their free GPU support!\n- Thanks to the [OpenXLab](https://openxlab.org.cn/apps/detail/InstantX/InstantID) team for their free GPU support!\n- Thanks to [SiliconFlow](https://github.com/siliconflow) for their OneDiff integration of InstantID! \n\n## Disclaimer\nThe code of InstantID is released under [Apache License](https://github.com/InstantID/InstantID?tab=Apache-2.0-1-ov-file#readme) for both academic and commercial usage. **However, both manual-downloading and auto-downloading face models from insightface are for non-commercial research purposes only** according to their [license](https://github.com/deepinsight/insightface?tab=readme-ov-file#license). **Our released checkpoints are also for research purposes only**. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=InstantID/InstantID&type=Date)](https://star-history.com/#InstantID/InstantID&Date)\n\n\n## Sponsor Us\nIf you find this project useful, you can buy us a coffee via Github Sponsor! We support [Paypal](https://ko-fi.com/instantx) and [WeChat Pay](https://tinyurl.com/instantx-pay).\n\n## Cite\nIf you find InstantID useful for your research and applications, please cite us using this BibTeX:\n\n```bibtex\n@article{wang2024instantid,\n  title={InstantID: Zero-shot Identity-Preserving Generation in Seconds},\n  author={Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},\n  journal={arXiv preprint arXiv:2401.07519},\n  year={2024}\n}\n```\n\nFor any question, please feel free to contact us via haofanwang.ai@gmail.com or wangqixun.ai@gmail.com.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.8984375,
          "content": "# Configuration for Cog 锔\n# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md\n\nbuild:\n  # set to true if your model requires a GPU\n  gpu: true\n  # cuda: \"12.1\"\n\n  # a list of ubuntu apt packages to install\n  system_packages:\n    - \"libgl1-mesa-glx\"\n    - \"libglib2.0-0\"\n\n  # python version in the form '3.11' or '3.11.4'\n  python_version: \"3.11\"\n\n  # a list of packages in the format <package-name>==<version>\n  python_packages:\n    - \"opencv-python==4.9.0.80\"\n    - \"transformers==4.37.0\"\n    - \"accelerate==0.26.1\"\n    - \"insightface==0.7.3\"\n    - \"diffusers==0.25.1\"\n    - \"onnxruntime==1.16.3\"\n\n  # commands run after the environment is setup\n  run:\n    - curl -o /usr/local/bin/pget -L \"https://github.com/replicate/pget/releases/download/v0.6.0/pget_linux_x86_64\" && chmod +x /usr/local/bin/pget\n\n# predict.py defines how predictions are run on your model\npredict: \"cog/predict.py:Predictor\"\n"
        },
        {
          "name": "cog",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradio_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "infer.py",
          "type": "blob",
          "size": 3.2900390625,
          "content": "import cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers.utils import load_image\nfrom diffusers.models import ControlNetModel\n\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\ndef resize_img(input_image, max_side=1280, min_side=1024, size=None, \n               pad_to_max_side=False, mode=Image.BILINEAR, base_pixel_number=64):\n\n    w, h = input_image.size\n    if size is not None:\n        w_resize_new, h_resize_new = size\n    else:\n        ratio = min_side / min(h, w)\n        w, h = round(ratio*w), round(ratio*h)\n        ratio = max_side / max(h, w)\n        input_image = input_image.resize([round(ratio*w), round(ratio*h)], mode)\n        w_resize_new = (round(ratio * w) // base_pixel_number) * base_pixel_number\n        h_resize_new = (round(ratio * h) // base_pixel_number) * base_pixel_number\n    input_image = input_image.resize([w_resize_new, h_resize_new], mode)\n\n    if pad_to_max_side:\n        res = np.ones([max_side, max_side, 3], dtype=np.uint8) * 255\n        offset_x = (max_side - w_resize_new) // 2\n        offset_y = (max_side - h_resize_new) // 2\n        res[offset_y:offset_y+h_resize_new, offset_x:offset_x+w_resize_new] = np.array(input_image)\n        input_image = Image.fromarray(res)\n    return input_image\n\n\nif __name__ == \"__main__\":\n\n    # Load face encoder\n    app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n    app.prepare(ctx_id=0, det_size=(640, 640))\n\n    # Path to InstantID models\n    face_adapter = f'./checkpoints/ip-adapter.bin'\n    controlnet_path = f'./checkpoints/ControlNetModel'\n\n    # Load pipeline\n    controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n\n    base_model_path = 'stabilityai/stable-diffusion-xl-base-1.0'\n\n    pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n        base_model_path,\n        controlnet=controlnet,\n        torch_dtype=torch.float16,\n    )\n    pipe.cuda()\n    pipe.load_ip_adapter_instantid(face_adapter)\n\n    # Infer setting\n    prompt = \"analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality\"\n    n_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured\"\n\n    face_image = load_image(\"./examples/yann-lecun_resize.jpg\")\n    face_image = resize_img(face_image)\n\n    face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\n    face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1] # only use the maximum face\n    face_emb = face_info['embedding']\n    face_kps = draw_kps(face_image, face_info['kps'])\n\n    image = pipe(\n        prompt=prompt,\n        negative_prompt=n_prompt,\n        image_embeds=face_emb,\n        image=face_kps,\n        controlnet_conditioning_scale=0.8,\n        ip_adapter_scale=0.8,\n        num_inference_steps=30,\n        guidance_scale=5,\n    ).images[0]\n\n    image.save('result.jpg')"
        },
        {
          "name": "infer_full.py",
          "type": "blob",
          "size": 4.8525390625,
          "content": "import cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers.utils import load_image\nfrom diffusers.models import ControlNetModel\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\n\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid_full import StableDiffusionXLInstantIDPipeline, draw_kps\n\nfrom controlnet_aux import MidasDetector\n\ndef convert_from_image_to_cv2(img: Image) -> np.ndarray:\n    return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n\ndef resize_img(input_image, max_side=1280, min_side=1024, size=None, \n               pad_to_max_side=False, mode=Image.BILINEAR, base_pixel_number=64):\n\n    w, h = input_image.size\n    if size is not None:\n        w_resize_new, h_resize_new = size\n    else:\n        ratio = min_side / min(h, w)\n        w, h = round(ratio*w), round(ratio*h)\n        ratio = max_side / max(h, w)\n        input_image = input_image.resize([round(ratio*w), round(ratio*h)], mode)\n        w_resize_new = (round(ratio * w) // base_pixel_number) * base_pixel_number\n        h_resize_new = (round(ratio * h) // base_pixel_number) * base_pixel_number\n    input_image = input_image.resize([w_resize_new, h_resize_new], mode)\n\n    if pad_to_max_side:\n        res = np.ones([max_side, max_side, 3], dtype=np.uint8) * 255\n        offset_x = (max_side - w_resize_new) // 2\n        offset_y = (max_side - h_resize_new) // 2\n        res[offset_y:offset_y+h_resize_new, offset_x:offset_x+w_resize_new] = np.array(input_image)\n        input_image = Image.fromarray(res)\n    return input_image\n\n\nif __name__ == \"__main__\":\n\n    # Load face encoder\n    app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n    app.prepare(ctx_id=0, det_size=(640, 640))\n\n    # Path to InstantID models\n    face_adapter = f'./checkpoints/ip-adapter.bin'\n    controlnet_path = f'./checkpoints/ControlNetModel'\n    controlnet_depth_path = f'diffusers/controlnet-depth-sdxl-1.0-small'\n    \n    # Load depth detector\n    midas = MidasDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n    # Load pipeline\n    controlnet_list = [controlnet_path, controlnet_depth_path]\n    controlnet_model_list = []\n    for controlnet_path in controlnet_list:\n        controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n        controlnet_model_list.append(controlnet)\n    controlnet = MultiControlNetModel(controlnet_model_list)\n    \n    base_model_path = 'stabilityai/stable-diffusion-xl-base-1.0'\n\n    pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n        base_model_path,\n        controlnet=controlnet,\n        torch_dtype=torch.float16,\n    )\n    pipe.cuda()\n    pipe.load_ip_adapter_instantid(face_adapter)\n\n    # Infer setting\n    prompt = \"analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality\"\n    n_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured\"\n\n    face_image = load_image(\"./examples/yann-lecun_resize.jpg\")\n    face_image = resize_img(face_image)\n\n    face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\n    face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1] # only use the maximum face\n    face_emb = face_info['embedding']\n\n    # use another reference image\n    pose_image = load_image(\"./examples/poses/pose.jpg\")\n    pose_image = resize_img(pose_image)\n\n    face_info = app.get(cv2.cvtColor(np.array(pose_image), cv2.COLOR_RGB2BGR))\n    pose_image_cv2 = convert_from_image_to_cv2(pose_image)\n    face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1] # only use the maximum face\n    face_kps = draw_kps(pose_image, face_info['kps'])\n\n    width, height = face_kps.size\n\n    # use depth control\n    processed_image_midas = midas(pose_image)\n    processed_image_midas = processed_image_midas.resize(pose_image.size)\n    \n    # enhance face region\n    control_mask = np.zeros([height, width, 3])\n    x1, y1, x2, y2 = face_info[\"bbox\"]\n    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n    control_mask[y1:y2, x1:x2] = 255\n    control_mask = Image.fromarray(control_mask.astype(np.uint8))\n\n    image = pipe(\n        prompt=prompt,\n        negative_prompt=n_prompt,\n        image_embeds=face_emb,\n        control_mask=control_mask,\n        image=[face_kps, processed_image_midas],\n        controlnet_conditioning_scale=[0.8,0.8],\n        ip_adapter_scale=0.8,\n        num_inference_steps=30,\n        guidance_scale=5,\n    ).images[0]\n\n    image.save('result.jpg')"
        },
        {
          "name": "infer_img2img.py",
          "type": "blob",
          "size": 3.3701171875,
          "content": "import cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers.utils import load_image\nfrom diffusers.models import ControlNetModel\n\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid_img2img import StableDiffusionXLInstantIDImg2ImgPipeline, draw_kps\n\ndef resize_img(input_image, max_side=1280, min_side=1024, size=None, \n               pad_to_max_side=False, mode=Image.BILINEAR, base_pixel_number=64):\n\n    w, h = input_image.size\n    if size is not None:\n        w_resize_new, h_resize_new = size\n    else:\n        ratio = min_side / min(h, w)\n        w, h = round(ratio*w), round(ratio*h)\n        ratio = max_side / max(h, w)\n        input_image = input_image.resize([round(ratio*w), round(ratio*h)], mode)\n        w_resize_new = (round(ratio * w) // base_pixel_number) * base_pixel_number\n        h_resize_new = (round(ratio * h) // base_pixel_number) * base_pixel_number\n    input_image = input_image.resize([w_resize_new, h_resize_new], mode)\n\n    if pad_to_max_side:\n        res = np.ones([max_side, max_side, 3], dtype=np.uint8) * 255\n        offset_x = (max_side - w_resize_new) // 2\n        offset_y = (max_side - h_resize_new) // 2\n        res[offset_y:offset_y+h_resize_new, offset_x:offset_x+w_resize_new] = np.array(input_image)\n        input_image = Image.fromarray(res)\n    return input_image\n\n\nif __name__ == \"__main__\":\n    \n    # Load face encoder\n    app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n    app.prepare(ctx_id=0, det_size=(640, 640))\n\n    # Path to InstantID models\n    face_adapter = f'./checkpoints/ip-adapter.bin'\n    controlnet_path = f'./checkpoints/ControlNetModel'\n\n    # Load pipeline\n    controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n\n    base_model_path = 'stabilityai/stable-diffusion-xl-base-1.0'\n\n    pipe = StableDiffusionXLInstantIDImg2ImgPipeline.from_pretrained(\n        base_model_path,\n        controlnet=controlnet,\n        torch_dtype=torch.float16,\n    )\n    pipe.cuda()\n    pipe.load_ip_adapter_instantid(face_adapter)\n\n    # Infer setting\n    prompt = \"analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality\"\n    n_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured\"\n\n    face_image = load_image(\"./examples/yann-lecun_resize.jpg\")\n    face_image = resize_img(face_image)\n\n    face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\n    face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1] # only use the maximum face\n    face_emb = face_info['embedding']\n    face_kps = draw_kps(face_image, face_info['kps'])\n\n    image = pipe(\n        prompt=prompt,\n        negative_prompt=n_prompt,\n        image=face_image,\n        image_embeds=face_emb,\n        control_image=face_kps,\n        controlnet_conditioning_scale=0.8,\n        ip_adapter_scale=0.8,\n        num_inference_steps=30,\n        guidance_scale=5,\n        strength=0.85\n    ).images[0]\n\n    image.save('result.jpg')"
        },
        {
          "name": "ip_adapter",
          "type": "tree",
          "content": null
        },
        {
          "name": "pipeline_stable_diffusion_xl_instantid.py",
          "type": "blob",
          "size": 40.4326171875,
          "content": "# Copyright 2024 The InstantX Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport cv2\nimport math\n\nimport numpy as np\nimport PIL.Image\nimport torch\nimport torch.nn.functional as F\n\nfrom diffusers.image_processor import PipelineImageInput\n\nfrom diffusers.models import ControlNetModel\n\nfrom diffusers.utils import (\n    deprecate,\n    logging,\n    replace_example_docstring,\n)\nfrom diffusers.utils.torch_utils import is_compiled_module, is_torch_version\nfrom diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput\n\nfrom diffusers import StableDiffusionXLControlNetPipeline\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\nfrom diffusers.utils.import_utils import is_xformers_available\n\nfrom ip_adapter.resampler import Resampler\nfrom ip_adapter.utils import is_torch2_available\n\nif is_torch2_available():\n    from ip_adapter.attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor\nelse:\n    from ip_adapter.attention_processor import IPAttnProcessor, AttnProcessor\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\nEXAMPLE_DOC_STRING = \"\"\"\n    Examples:\n        ```py\n        >>> # !pip install opencv-python transformers accelerate insightface\n        >>> import diffusers\n        >>> from diffusers.utils import load_image\n        >>> from diffusers.models import ControlNetModel\n\n        >>> import cv2\n        >>> import torch\n        >>> import numpy as np\n        >>> from PIL import Image\n        \n        >>> from insightface.app import FaceAnalysis\n        >>> from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\n        >>> # download 'antelopev2' under ./models\n        >>> app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n        >>> app.prepare(ctx_id=0, det_size=(640, 640))\n        \n        >>> # download models under ./checkpoints\n        >>> face_adapter = f'./checkpoints/ip-adapter.bin'\n        >>> controlnet_path = f'./checkpoints/ControlNetModel'\n        \n        >>> # load IdentityNet\n        >>> controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n        \n        >>> pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n        ...     \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, torch_dtype=torch.float16\n        ... )\n        >>> pipe.cuda()\n        \n        >>> # load adapter\n        >>> pipe.load_ip_adapter_instantid(face_adapter)\n\n        >>> prompt = \"analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality\"\n        >>> negative_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured\"\n\n        >>> # load an image\n        >>> image = load_image(\"your-example.jpg\")\n        \n        >>> face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))[-1]\n        >>> face_emb = face_info['embedding']\n        >>> face_kps = draw_kps(face_image, face_info['kps'])\n        \n        >>> pipe.set_ip_adapter_scale(0.8)\n\n        >>> # generate image\n        >>> image = pipe(\n        ...     prompt, image_embeds=face_emb, image=face_kps, controlnet_conditioning_scale=0.8\n        ... ).images[0]\n        ```\n\"\"\"\n\ndef draw_kps(image_pil, kps, color_list=[(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255)]):\n    \n    stickwidth = 4\n    limbSeq = np.array([[0, 2], [1, 2], [3, 2], [4, 2]])\n    kps = np.array(kps)\n\n    w, h = image_pil.size\n    out_img = np.zeros([h, w, 3])\n\n    for i in range(len(limbSeq)):\n        index = limbSeq[i]\n        color = color_list[index[0]]\n\n        x = kps[index][:, 0]\n        y = kps[index][:, 1]\n        length = ((x[0] - x[1]) ** 2 + (y[0] - y[1]) ** 2) ** 0.5\n        angle = math.degrees(math.atan2(y[0] - y[1], x[0] - x[1]))\n        polygon = cv2.ellipse2Poly((int(np.mean(x)), int(np.mean(y))), (int(length / 2), stickwidth), int(angle), 0, 360, 1)\n        out_img = cv2.fillConvexPoly(out_img.copy(), polygon, color)\n    out_img = (out_img * 0.6).astype(np.uint8)\n\n    for idx_kp, kp in enumerate(kps):\n        color = color_list[idx_kp]\n        x, y = kp\n        out_img = cv2.circle(out_img.copy(), (int(x), int(y)), 10, color, -1)\n\n    out_img_pil = PIL.Image.fromarray(out_img.astype(np.uint8))\n    return out_img_pil\n    \nclass StableDiffusionXLInstantIDPipeline(StableDiffusionXLControlNetPipeline):\n    \n    def cuda(self, dtype=torch.float16, use_xformers=False):\n        self.to('cuda', dtype)\n        \n        if hasattr(self, 'image_proj_model'):\n            self.image_proj_model.to(self.unet.device).to(self.unet.dtype)\n        \n        if use_xformers:\n            if is_xformers_available():\n                import xformers\n                from packaging import version\n\n                xformers_version = version.parse(xformers.__version__)\n                if xformers_version == version.parse(\"0.0.16\"):\n                    logger.warn(\n                        \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n                    )\n                self.enable_xformers_memory_efficient_attention()\n            else:\n                raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n    \n    def load_ip_adapter_instantid(self, model_ckpt, image_emb_dim=512, num_tokens=16, scale=0.5):     \n        self.set_image_proj_model(model_ckpt, image_emb_dim, num_tokens)\n        self.set_ip_adapter(model_ckpt, num_tokens, scale)\n        \n    def set_image_proj_model(self, model_ckpt, image_emb_dim=512, num_tokens=16):\n        \n        image_proj_model = Resampler(\n            dim=1280,\n            depth=4,\n            dim_head=64,\n            heads=20,\n            num_queries=num_tokens,\n            embedding_dim=image_emb_dim,\n            output_dim=self.unet.config.cross_attention_dim,\n            ff_mult=4,\n        )\n\n        image_proj_model.eval()\n        \n        self.image_proj_model = image_proj_model.to(self.device, dtype=self.dtype)\n        state_dict = torch.load(model_ckpt, map_location=\"cpu\")\n        if 'image_proj' in state_dict:\n            state_dict = state_dict[\"image_proj\"]\n        self.image_proj_model.load_state_dict(state_dict)\n        \n        self.image_proj_model_in_features = image_emb_dim\n    \n    def set_ip_adapter(self, model_ckpt, num_tokens, scale):\n        \n        unet = self.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = AttnProcessor().to(unet.device, dtype=unet.dtype)\n            else:\n                attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, \n                                                   cross_attention_dim=cross_attention_dim, \n                                                   scale=scale,\n                                                   num_tokens=num_tokens).to(unet.device, dtype=unet.dtype)\n        unet.set_attn_processor(attn_procs)\n        \n        state_dict = torch.load(model_ckpt, map_location=\"cpu\")\n        ip_layers = torch.nn.ModuleList(self.unet.attn_processors.values())\n        if 'ip_adapter' in state_dict:\n            state_dict = state_dict['ip_adapter']\n        ip_layers.load_state_dict(state_dict)\n    \n    def set_ip_adapter_scale(self, scale):\n        unet = getattr(self, self.unet_name) if not hasattr(self, \"unet\") else self.unet\n        for attn_processor in unet.attn_processors.values():\n            if isinstance(attn_processor, IPAttnProcessor):\n                attn_processor.scale = scale\n\n    def _encode_prompt_image_emb(self, prompt_image_emb, device, num_images_per_prompt, dtype, do_classifier_free_guidance):\n        \n        if isinstance(prompt_image_emb, torch.Tensor):\n            prompt_image_emb = prompt_image_emb.clone().detach()\n        else:\n            prompt_image_emb = torch.tensor(prompt_image_emb)\n            \n        prompt_image_emb = prompt_image_emb.reshape([1, -1, self.image_proj_model_in_features])\n        \n        if do_classifier_free_guidance:\n            prompt_image_emb = torch.cat([torch.zeros_like(prompt_image_emb), prompt_image_emb], dim=0)\n        else:\n            prompt_image_emb = torch.cat([prompt_image_emb], dim=0)\n        \n        prompt_image_emb = prompt_image_emb.to(device=self.image_proj_model.latents.device, \n                                               dtype=self.image_proj_model.latents.dtype)\n        prompt_image_emb = self.image_proj_model(prompt_image_emb)\n\n        bs_embed, seq_len, _ = prompt_image_emb.shape\n        prompt_image_emb = prompt_image_emb.repeat(1, num_images_per_prompt, 1)\n        prompt_image_emb = prompt_image_emb.view(bs_embed * num_images_per_prompt, seq_len, -1)\n        \n        return prompt_image_emb.to(device=device, dtype=dtype)\n\n    @torch.no_grad()\n    @replace_example_docstring(EXAMPLE_DOC_STRING)\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        prompt_2: Optional[Union[str, List[str]]] = None,\n        image: PipelineImageInput = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 5.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        image_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n        guess_mode: bool = False,\n        control_guidance_start: Union[float, List[float]] = 0.0,\n        control_guidance_end: Union[float, List[float]] = 1.0,\n        original_size: Tuple[int, int] = None,\n        crops_coords_top_left: Tuple[int, int] = (0, 0),\n        target_size: Tuple[int, int] = None,\n        negative_original_size: Optional[Tuple[int, int]] = None,\n        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n        negative_target_size: Optional[Tuple[int, int]] = None,\n        clip_skip: Optional[int] = None,\n        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n\n        # IP adapter\n        ip_adapter_scale=None,\n\n        **kwargs,\n    ):\n        r\"\"\"\n        The call function to the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n            prompt_2 (`str` or `List[str]`, *optional*):\n                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n                used in both text-encoders.\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, `List[np.ndarray]`,:\n                    `List[List[torch.FloatTensor]]`, `List[List[np.ndarray]]` or `List[List[PIL.Image.Image]]`):\n                The ControlNet input condition to provide guidance to the `unet` for generation. If the type is\n                specified as `torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can also be\n                accepted as an image. The dimensions of the output image defaults to `image`'s dimensions. If height\n                and/or width are passed, `image` is resized accordingly. If multiple ControlNets are specified in\n                `init`, images must be passed as a list such that each element of the list can be correctly batched for\n                input to a single ControlNet.\n            height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n                The height in pixels of the generated image. Anything below 512 pixels won't work well for\n                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n                and checkpoints that are not specifically fine-tuned on low resolutions.\n            width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n                The width in pixels of the generated image. Anything below 512 pixels won't work well for\n                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n                and checkpoints that are not specifically fine-tuned on low resolutions.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 5.0):\n                A higher guidance scale value encourages the model to generate images closely linked to the text\n                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n            negative_prompt_2 (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. This is sent to `tokenizer_2`\n                and `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (畏) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n                generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor is generated by sampling using the supplied random `generator`.\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n                provided, text embeddings are generated from the `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, pooled text embeddings are generated from `prompt` input argument.\n            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs (prompt\n                weighting). If not provided, pooled `negative_prompt_embeds` are generated from `negative_prompt` input\n                argument.\n            image_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated image embeddings.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in\n                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 1.0):\n                The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale` before they are added\n                to the residual in the original `unet`. If multiple ControlNets are specified in `init`, you can set\n                the corresponding scale as a list.\n            guess_mode (`bool`, *optional*, defaults to `False`):\n                The ControlNet encoder tries to recognize the content of the input image even if you remove all\n                prompts. A `guidance_scale` value between 3.0 and 5.0 is recommended.\n            control_guidance_start (`float` or `List[float]`, *optional*, defaults to 0.0):\n                The percentage of total steps at which the ControlNet starts applying.\n            control_guidance_end (`float` or `List[float]`, *optional*, defaults to 1.0):\n                The percentage of total steps at which the ControlNet stops applying.\n            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.\n                `original_size` defaults to `(height, width)` if not specified. Part of SDXL's micro-conditioning as\n                explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                `crops_coords_top_left` can be used to generate an image that appears to be \"cropped\" from the position\n                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting\n                `crops_coords_top_left` to (0, 0). Part of SDXL's micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                For most cases, `target_size` should be set to the desired height and width of the generated image. If\n                not specified it will default to `(height, width)`. Part of SDXL's micro-conditioning as explained in\n                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                To negatively condition the generation process based on a specific image resolution. Part of SDXL's\n                micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL's\n                micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                To negatively condition the generation process based on a target image resolution. It should be as same\n                as the `target_size` for most cases. Part of SDXL's micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            clip_skip (`int`, *optional*):\n                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n                the output of the pre-final layer will be used for computing the prompt embeddings.\n            callback_on_step_end (`Callable`, *optional*):\n                A function that calls at the end of each denoising steps during the inference. The function is called\n                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n                `callback_on_step_end_tensor_inputs`.\n            callback_on_step_end_tensor_inputs (`List`, *optional*):\n                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n                `._callback_tensor_inputs` attribute of your pipeine class.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n                If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,\n                otherwise a `tuple` is returned containing the output images.\n        \"\"\"\n\n        callback = kwargs.pop(\"callback\", None)\n        callback_steps = kwargs.pop(\"callback_steps\", None)\n\n        if callback is not None:\n            deprecate(\n                \"callback\",\n                \"1.0.0\",\n                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n        if callback_steps is not None:\n            deprecate(\n                \"callback_steps\",\n                \"1.0.0\",\n                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n\n        controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n\n        # align format for control guidance\n        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n            mult = len(controlnet.nets) if isinstance(controlnet, MultiControlNetModel) else 1\n            control_guidance_start, control_guidance_end = (\n                mult * [control_guidance_start],\n                mult * [control_guidance_end],\n            )\n        \n        # 0. set ip_adapter_scale\n        if ip_adapter_scale is not None:\n            self.set_ip_adapter_scale(ip_adapter_scale)\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt=prompt,\n            prompt_2=prompt_2,\n            image=image,\n            callback_steps=callback_steps,\n            negative_prompt=negative_prompt,\n            negative_prompt_2=negative_prompt_2,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n        )\n\n        self._guidance_scale = guidance_scale\n        self._clip_skip = clip_skip\n        self._cross_attention_kwargs = cross_attention_kwargs\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n\n        if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n            controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n\n        global_pool_conditions = (\n            controlnet.config.global_pool_conditions\n            if isinstance(controlnet, ControlNetModel)\n            else controlnet.nets[0].config.global_pool_conditions\n        )\n        guess_mode = guess_mode or global_pool_conditions\n\n        # 3.1 Encode input prompt\n        text_encoder_lora_scale = (\n            self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n        )\n        (\n            prompt_embeds,\n            negative_prompt_embeds,\n            pooled_prompt_embeds,\n            negative_pooled_prompt_embeds,\n        ) = self.encode_prompt(\n            prompt,\n            prompt_2,\n            device,\n            num_images_per_prompt,\n            self.do_classifier_free_guidance,\n            negative_prompt,\n            negative_prompt_2,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            lora_scale=text_encoder_lora_scale,\n            clip_skip=self.clip_skip,\n        )\n        \n        # 3.2 Encode image prompt\n        prompt_image_emb = self._encode_prompt_image_emb(image_embeds, \n                                                         device,\n                                                         num_images_per_prompt,\n                                                         self.unet.dtype,\n                                                         self.do_classifier_free_guidance)\n        \n        # 4. Prepare image\n        if isinstance(controlnet, ControlNetModel):\n            image = self.prepare_image(\n                image=image,\n                width=width,\n                height=height,\n                batch_size=batch_size * num_images_per_prompt,\n                num_images_per_prompt=num_images_per_prompt,\n                device=device,\n                dtype=controlnet.dtype,\n                do_classifier_free_guidance=self.do_classifier_free_guidance,\n                guess_mode=guess_mode,\n            )\n            height, width = image.shape[-2:]\n        elif isinstance(controlnet, MultiControlNetModel):\n            images = []\n\n            for image_ in image:\n                image_ = self.prepare_image(\n                    image=image_,\n                    width=width,\n                    height=height,\n                    batch_size=batch_size * num_images_per_prompt,\n                    num_images_per_prompt=num_images_per_prompt,\n                    device=device,\n                    dtype=controlnet.dtype,\n                    do_classifier_free_guidance=self.do_classifier_free_guidance,\n                    guess_mode=guess_mode,\n                )\n\n                images.append(image_)\n\n            image = images\n            height, width = image[0].shape[-2:]\n        else:\n            assert False\n\n        # 5. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n        self._num_timesteps = len(timesteps)\n\n        # 6. Prepare latent variables\n        num_channels_latents = self.unet.config.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6.5 Optionally get Guidance Scale Embedding\n        timestep_cond = None\n        if self.unet.config.time_cond_proj_dim is not None:\n            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n            timestep_cond = self.get_guidance_scale_embedding(\n                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n            ).to(device=device, dtype=latents.dtype)\n\n        # 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 7.1 Create tensor stating which controlnets to keep\n        controlnet_keep = []\n        for i in range(len(timesteps)):\n            keeps = [\n                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n                for s, e in zip(control_guidance_start, control_guidance_end)\n            ]\n            controlnet_keep.append(keeps[0] if isinstance(controlnet, ControlNetModel) else keeps)\n\n        # 7.2 Prepare added time ids & embeddings\n        if isinstance(image, list):\n            original_size = original_size or image[0].shape[-2:]\n        else:\n            original_size = original_size or image.shape[-2:]\n        target_size = target_size or (height, width)\n\n        add_text_embeds = pooled_prompt_embeds\n        if self.text_encoder_2 is None:\n            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n        else:\n            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n\n        add_time_ids = self._get_add_time_ids(\n            original_size,\n            crops_coords_top_left,\n            target_size,\n            dtype=prompt_embeds.dtype,\n            text_encoder_projection_dim=text_encoder_projection_dim,\n        )\n\n        if negative_original_size is not None and negative_target_size is not None:\n            negative_add_time_ids = self._get_add_time_ids(\n                negative_original_size,\n                negative_crops_coords_top_left,\n                negative_target_size,\n                dtype=prompt_embeds.dtype,\n                text_encoder_projection_dim=text_encoder_projection_dim,\n            )\n        else:\n            negative_add_time_ids = add_time_ids\n\n        if self.do_classifier_free_guidance:\n            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n\n        prompt_embeds = prompt_embeds.to(device)\n        add_text_embeds = add_text_embeds.to(device)\n        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n        encoder_hidden_states = torch.cat([prompt_embeds, prompt_image_emb], dim=1)\n\n        # 8. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        is_unet_compiled = is_compiled_module(self.unet)\n        is_controlnet_compiled = is_compiled_module(self.controlnet)\n        is_torch_higher_equal_2_1 = is_torch_version(\">=\", \"2.1\")\n                \n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # Relevant thread:\n                # https://dev-discuss.pytorch.org/t/cudagraphs-in-pytorch-2-0/1428\n                if (is_unet_compiled and is_controlnet_compiled) and is_torch_higher_equal_2_1:\n                    torch._inductor.cudagraph_mark_step_begin()\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n\n                # controlnet(s) inference\n                if guess_mode and self.do_classifier_free_guidance:\n                    # Infer ControlNet only for the conditional batch.\n                    control_model_input = latents\n                    control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                    controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n                    controlnet_added_cond_kwargs = {\n                        \"text_embeds\": add_text_embeds.chunk(2)[1],\n                        \"time_ids\": add_time_ids.chunk(2)[1],\n                    }\n                else:\n                    control_model_input = latent_model_input\n                    controlnet_prompt_embeds = prompt_embeds\n                    controlnet_added_cond_kwargs = added_cond_kwargs\n                \n                if isinstance(controlnet_keep[i], list):\n                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n                else:\n                    controlnet_cond_scale = controlnet_conditioning_scale\n                    if isinstance(controlnet_cond_scale, list):\n                        controlnet_cond_scale = controlnet_cond_scale[0]\n                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n\n                down_block_res_samples, mid_block_res_sample = self.controlnet(\n                    control_model_input,\n                    t,\n                    encoder_hidden_states=prompt_image_emb,\n                    controlnet_cond=image,\n                    conditioning_scale=cond_scale,\n                    guess_mode=guess_mode,\n                    added_cond_kwargs=controlnet_added_cond_kwargs,\n                    return_dict=False,\n                )\n\n                if guess_mode and self.do_classifier_free_guidance:\n                    # Infered ControlNet only for the conditional batch.\n                    # To apply the output of ControlNet to both the unconditional and conditional batches,\n                    # add 0 to the unconditional batch to keep it unchanged.\n                    down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                    mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n\n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=encoder_hidden_states,\n                    timestep_cond=timestep_cond,\n                    cross_attention_kwargs=self.cross_attention_kwargs,\n                    down_block_additional_residuals=down_block_res_samples,\n                    mid_block_additional_residual=mid_block_res_sample,\n                    added_cond_kwargs=added_cond_kwargs,\n                    return_dict=False,\n                )[0]\n\n                # perform guidance\n                if self.do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n\n                if callback_on_step_end is not None:\n                    callback_kwargs = {}\n                    for k in callback_on_step_end_tensor_inputs:\n                        callback_kwargs[k] = locals()[k]\n                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n\n                    latents = callback_outputs.pop(\"latents\", latents)\n                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n                        callback(step_idx, t, latents)\n        \n        if not output_type == \"latent\":\n            # make sure the VAE is in float32 mode, as it overflows in float16\n            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n\n            if needs_upcasting:\n                self.upcast_vae()\n                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n\n            # unscale/denormalize the latents\n            # denormalize with the mean and std if available and not None\n            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n            if has_latents_mean and has_latents_std:\n                latents_mean = (\n                    torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n                )\n                latents_std = (\n                    torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n                )\n                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n            else:\n                latents = latents / self.vae.config.scaling_factor\n\n            image = self.vae.decode(latents, return_dict=False)[0]\n\n            # cast back to fp16 if needed\n            if needs_upcasting:\n                self.vae.to(dtype=torch.float16)\n        else:\n            image = latents\n\n        if not output_type == \"latent\":\n            # apply watermark if available\n            if self.watermark is not None:\n                image = self.watermark.apply_watermark(image)\n\n            image = self.image_processor.postprocess(image, output_type=output_type)\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (image,)\n\n        return StableDiffusionXLPipelineOutput(images=image)"
        },
        {
          "name": "pipeline_stable_diffusion_xl_instantid_full.py",
          "type": "blob",
          "size": 59.8583984375,
          "content": "# Copyright 2024 The InstantX Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport cv2\nimport math\n\nimport numpy as np\nimport PIL.Image\nimport torch\nimport torch.nn.functional as F\n\nfrom diffusers.image_processor import PipelineImageInput\n\nfrom diffusers.models import ControlNetModel\n\nfrom diffusers.utils import (\n    deprecate,\n    logging,\n    replace_example_docstring,\n)\nfrom diffusers.utils.torch_utils import is_compiled_module, is_torch_version\nfrom diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput\n\nfrom diffusers import StableDiffusionXLControlNetPipeline\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\nfrom diffusers.utils.import_utils import is_xformers_available\n\nfrom ip_adapter.resampler import Resampler\nfrom ip_adapter.utils import is_torch2_available\n\nif is_torch2_available():\n    from ip_adapter.attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor\nelse:\n    from ip_adapter.attention_processor import IPAttnProcessor, AttnProcessor\nfrom ip_adapter.attention_processor import region_control\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\nEXAMPLE_DOC_STRING = \"\"\"\n    Examples:\n        ```py\n        >>> # !pip install opencv-python transformers accelerate insightface\n        >>> import diffusers\n        >>> from diffusers.utils import load_image\n        >>> from diffusers.models import ControlNetModel\n\n        >>> import cv2\n        >>> import torch\n        >>> import numpy as np\n        >>> from PIL import Image\n        \n        >>> from insightface.app import FaceAnalysis\n        >>> from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\n        >>> # download 'antelopev2' under ./models\n        >>> app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n        >>> app.prepare(ctx_id=0, det_size=(640, 640))\n        \n        >>> # download models under ./checkpoints\n        >>> face_adapter = f'./checkpoints/ip-adapter.bin'\n        >>> controlnet_path = f'./checkpoints/ControlNetModel'\n        \n        >>> # load IdentityNet\n        >>> controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n        \n        >>> pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n        ...     \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, torch_dtype=torch.float16\n        ... )\n        >>> pipe.cuda()\n        \n        >>> # load adapter\n        >>> pipe.load_ip_adapter_instantid(face_adapter)\n\n        >>> prompt = \"analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality\"\n        >>> negative_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured\"\n\n        >>> # load an image\n        >>> image = load_image(\"your-example.jpg\")\n        \n        >>> face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))[-1]\n        >>> face_emb = face_info['embedding']\n        >>> face_kps = draw_kps(face_image, face_info['kps'])\n        \n        >>> pipe.set_ip_adapter_scale(0.8)\n\n        >>> # generate image\n        >>> image = pipe(\n        ...     prompt, image_embeds=face_emb, image=face_kps, controlnet_conditioning_scale=0.8\n        ... ).images[0]\n        ```\n\"\"\"\n\nfrom transformers import CLIPTokenizer\nfrom diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipeline\nclass LongPromptWeight(object):\n    \n    \"\"\"\n    Copied from https://github.com/huggingface/diffusers/blob/main/examples/community/lpw_stable_diffusion_xl.py\n    \"\"\"\n    \n    def __init__(self) -> None:\n        pass\n\n    def parse_prompt_attention(self, text):\n        \"\"\"\n        Parses a string with attention tokens and returns a list of pairs: text and its associated weight.\n        Accepted tokens are:\n        (abc) - increases attention to abc by a multiplier of 1.1\n        (abc:3.12) - increases attention to abc by a multiplier of 3.12\n        [abc] - decreases attention to abc by a multiplier of 1.1\n        \\( - literal character '('\n        \\[ - literal character '['\n        \\) - literal character ')'\n        \\] - literal character ']'\n        \\\\ - literal character '\\'\n        anything else - just text\n\n        >>> parse_prompt_attention('normal text')\n        [['normal text', 1.0]]\n        >>> parse_prompt_attention('an (important) word')\n        [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n        >>> parse_prompt_attention('(unbalanced')\n        [['unbalanced', 1.1]]\n        >>> parse_prompt_attention('\\(literal\\]')\n        [['(literal]', 1.0]]\n        >>> parse_prompt_attention('(unnecessary)(parens)')\n        [['unnecessaryparens', 1.1]]\n        >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n        [['a ', 1.0],\n        ['house', 1.5730000000000004],\n        [' ', 1.1],\n        ['on', 1.0],\n        [' a ', 1.1],\n        ['hill', 0.55],\n        [', sun, ', 1.1],\n        ['sky', 1.4641000000000006],\n        ['.', 1.1]]\n        \"\"\"\n        import re\n\n        re_attention = re.compile(\n            r\"\"\"\n                \\\\\\(|\\\\\\)|\\\\\\[|\\\\]|\\\\\\\\|\\\\|\\(|\\[|:([+-]?[.\\d]+)\\)|\n                \\)|]|[^\\\\()\\[\\]:]+|:\n            \"\"\",\n            re.X,\n        )\n\n        re_break = re.compile(r\"\\s*\\bBREAK\\b\\s*\", re.S)\n\n        res = []\n        round_brackets = []\n        square_brackets = []\n\n        round_bracket_multiplier = 1.1\n        square_bracket_multiplier = 1 / 1.1\n\n        def multiply_range(start_position, multiplier):\n            for p in range(start_position, len(res)):\n                res[p][1] *= multiplier\n\n        for m in re_attention.finditer(text):\n            text = m.group(0)\n            weight = m.group(1)\n\n            if text.startswith(\"\\\\\"):\n                res.append([text[1:], 1.0])\n            elif text == \"(\":\n                round_brackets.append(len(res))\n            elif text == \"[\":\n                square_brackets.append(len(res))\n            elif weight is not None and len(round_brackets) > 0:\n                multiply_range(round_brackets.pop(), float(weight))\n            elif text == \")\" and len(round_brackets) > 0:\n                multiply_range(round_brackets.pop(), round_bracket_multiplier)\n            elif text == \"]\" and len(square_brackets) > 0:\n                multiply_range(square_brackets.pop(), square_bracket_multiplier)\n            else:\n                parts = re.split(re_break, text)\n                for i, part in enumerate(parts):\n                    if i > 0:\n                        res.append([\"BREAK\", -1])\n                    res.append([part, 1.0])\n\n        for pos in round_brackets:\n            multiply_range(pos, round_bracket_multiplier)\n\n        for pos in square_brackets:\n            multiply_range(pos, square_bracket_multiplier)\n\n        if len(res) == 0:\n            res = [[\"\", 1.0]]\n\n        # merge runs of identical weights\n        i = 0\n        while i + 1 < len(res):\n            if res[i][1] == res[i + 1][1]:\n                res[i][0] += res[i + 1][0]\n                res.pop(i + 1)\n            else:\n                i += 1\n\n        return res\n\n    def get_prompts_tokens_with_weights(self, clip_tokenizer: CLIPTokenizer, prompt: str):\n        \"\"\"\n        Get prompt token ids and weights, this function works for both prompt and negative prompt\n\n        Args:\n            pipe (CLIPTokenizer)\n                A CLIPTokenizer\n            prompt (str)\n                A prompt string with weights\n\n        Returns:\n            text_tokens (list)\n                A list contains token ids\n            text_weight (list)\n                A list contains the correspodent weight of token ids\n\n        Example:\n            import torch\n            from transformers import CLIPTokenizer\n\n            clip_tokenizer = CLIPTokenizer.from_pretrained(\n                \"stablediffusionapi/deliberate-v2\"\n                , subfolder = \"tokenizer\"\n                , dtype = torch.float16\n            )\n\n            token_id_list, token_weight_list = get_prompts_tokens_with_weights(\n                clip_tokenizer = clip_tokenizer\n                ,prompt = \"a (red:1.5) cat\"*70\n            )\n        \"\"\"\n        texts_and_weights = self.parse_prompt_attention(prompt)\n        text_tokens, text_weights = [], []\n        for word, weight in texts_and_weights:\n            # tokenize and discard the starting and the ending token\n            token = clip_tokenizer(word, truncation=False).input_ids[1:-1]  # so that tokenize whatever length prompt\n            # the returned token is a 1d list: [320, 1125, 539, 320]\n\n            # merge the new tokens to the all tokens holder: text_tokens\n            text_tokens = [*text_tokens, *token]\n\n            # each token chunk will come with one weight, like ['red cat', 2.0]\n            # need to expand weight for each token.\n            chunk_weights = [weight] * len(token)\n\n            # append the weight back to the weight holder: text_weights\n            text_weights = [*text_weights, *chunk_weights]\n        return text_tokens, text_weights\n\n    def group_tokens_and_weights(self, token_ids: list, weights: list, pad_last_block=False):\n        \"\"\"\n        Produce tokens and weights in groups and pad the missing tokens\n\n        Args:\n            token_ids (list)\n                The token ids from tokenizer\n            weights (list)\n                The weights list from function get_prompts_tokens_with_weights\n            pad_last_block (bool)\n                Control if fill the last token list to 75 tokens with eos\n        Returns:\n            new_token_ids (2d list)\n            new_weights (2d list)\n\n        Example:\n            token_groups,weight_groups = group_tokens_and_weights(\n                token_ids = token_id_list\n                , weights = token_weight_list\n            )\n        \"\"\"\n        bos, eos = 49406, 49407\n\n        # this will be a 2d list\n        new_token_ids = []\n        new_weights = []\n        while len(token_ids) >= 75:\n            # get the first 75 tokens\n            head_75_tokens = [token_ids.pop(0) for _ in range(75)]\n            head_75_weights = [weights.pop(0) for _ in range(75)]\n\n            # extract token ids and weights\n            temp_77_token_ids = [bos] + head_75_tokens + [eos]\n            temp_77_weights = [1.0] + head_75_weights + [1.0]\n\n            # add 77 token and weights chunk to the holder list\n            new_token_ids.append(temp_77_token_ids)\n            new_weights.append(temp_77_weights)\n\n        # padding the left\n        if len(token_ids) >= 0:\n            padding_len = 75 - len(token_ids) if pad_last_block else 0\n\n            temp_77_token_ids = [bos] + token_ids + [eos] * padding_len + [eos]\n            new_token_ids.append(temp_77_token_ids)\n\n            temp_77_weights = [1.0] + weights + [1.0] * padding_len + [1.0]\n            new_weights.append(temp_77_weights)\n\n        return new_token_ids, new_weights\n\n    def get_weighted_text_embeddings_sdxl(\n        self,\n        pipe: StableDiffusionXLPipeline,\n        prompt: str = \"\",\n        prompt_2: str = None,\n        neg_prompt: str = \"\",\n        neg_prompt_2: str = None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n        pooled_prompt_embeds=None,\n        negative_pooled_prompt_embeds=None,\n        extra_emb=None,\n        extra_emb_alpha=0.6,\n    ):\n        \"\"\"\n        This function can process long prompt with weights, no length limitation\n        for Stable Diffusion XL\n\n        Args:\n            pipe (StableDiffusionPipeline)\n            prompt (str)\n            prompt_2 (str)\n            neg_prompt (str)\n            neg_prompt_2 (str)\n        Returns:\n            prompt_embeds (torch.Tensor)\n            neg_prompt_embeds (torch.Tensor)\n        \"\"\"\n        # \n        if prompt_embeds is not None and \\\n            negative_prompt_embeds is not None and \\\n            pooled_prompt_embeds is not None and \\\n            negative_pooled_prompt_embeds is not None:\n            return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds\n\n        if prompt_2:\n            prompt = f\"{prompt} {prompt_2}\"\n\n        if neg_prompt_2:\n            neg_prompt = f\"{neg_prompt} {neg_prompt_2}\"\n\n        eos = pipe.tokenizer.eos_token_id\n\n        # tokenizer 1\n        prompt_tokens, prompt_weights = self.get_prompts_tokens_with_weights(pipe.tokenizer, prompt)\n        neg_prompt_tokens, neg_prompt_weights = self.get_prompts_tokens_with_weights(pipe.tokenizer, neg_prompt)\n\n        # tokenizer 2\n        # prompt_tokens_2, prompt_weights_2 = self.get_prompts_tokens_with_weights(pipe.tokenizer_2, prompt)\n        # neg_prompt_tokens_2, neg_prompt_weights_2 = self.get_prompts_tokens_with_weights(pipe.tokenizer_2, neg_prompt)\n        # tokenizer 2  !! !!!! 绛澶瑰峰tokenizer 1涓涓\n        prompt_tokens_2, prompt_weights_2 = self.get_prompts_tokens_with_weights(pipe.tokenizer, prompt)\n        neg_prompt_tokens_2, neg_prompt_weights_2 = self.get_prompts_tokens_with_weights(pipe.tokenizer, neg_prompt)\n\n        # padding the shorter one for prompt set 1\n        prompt_token_len = len(prompt_tokens)\n        neg_prompt_token_len = len(neg_prompt_tokens)\n\n        if prompt_token_len > neg_prompt_token_len:\n            # padding the neg_prompt with eos token\n            neg_prompt_tokens = neg_prompt_tokens + [eos] * abs(prompt_token_len - neg_prompt_token_len)\n            neg_prompt_weights = neg_prompt_weights + [1.0] * abs(prompt_token_len - neg_prompt_token_len)\n        else:\n            # padding the prompt\n            prompt_tokens = prompt_tokens + [eos] * abs(prompt_token_len - neg_prompt_token_len)\n            prompt_weights = prompt_weights + [1.0] * abs(prompt_token_len - neg_prompt_token_len)\n\n        # padding the shorter one for token set 2\n        prompt_token_len_2 = len(prompt_tokens_2)\n        neg_prompt_token_len_2 = len(neg_prompt_tokens_2)\n\n        if prompt_token_len_2 > neg_prompt_token_len_2:\n            # padding the neg_prompt with eos token\n            neg_prompt_tokens_2 = neg_prompt_tokens_2 + [eos] * abs(prompt_token_len_2 - neg_prompt_token_len_2)\n            neg_prompt_weights_2 = neg_prompt_weights_2 + [1.0] * abs(prompt_token_len_2 - neg_prompt_token_len_2)\n        else:\n            # padding the prompt\n            prompt_tokens_2 = prompt_tokens_2 + [eos] * abs(prompt_token_len_2 - neg_prompt_token_len_2)\n            prompt_weights_2 = prompt_weights + [1.0] * abs(prompt_token_len_2 - neg_prompt_token_len_2)\n\n        embeds = []\n        neg_embeds = []\n\n        prompt_token_groups, prompt_weight_groups = self.group_tokens_and_weights(prompt_tokens.copy(), prompt_weights.copy())\n\n        neg_prompt_token_groups, neg_prompt_weight_groups = self.group_tokens_and_weights(\n            neg_prompt_tokens.copy(), neg_prompt_weights.copy()\n        )\n\n        prompt_token_groups_2, prompt_weight_groups_2 = self.group_tokens_and_weights(\n            prompt_tokens_2.copy(), prompt_weights_2.copy()\n        )\n\n        neg_prompt_token_groups_2, neg_prompt_weight_groups_2 = self.group_tokens_and_weights(\n            neg_prompt_tokens_2.copy(), neg_prompt_weights_2.copy()\n        )\n\n        # get prompt embeddings one by one is not working.\n        for i in range(len(prompt_token_groups)):\n            # get positive prompt embeddings with weights\n            token_tensor = torch.tensor([prompt_token_groups[i]], dtype=torch.long, device=pipe.device)\n            weight_tensor = torch.tensor(prompt_weight_groups[i], dtype=torch.float16, device=pipe.device)\n\n            token_tensor_2 = torch.tensor([prompt_token_groups_2[i]], dtype=torch.long, device=pipe.device)\n\n            # use first text encoder\n            prompt_embeds_1 = pipe.text_encoder(token_tensor.to(pipe.device), output_hidden_states=True)\n            prompt_embeds_1_hidden_states = prompt_embeds_1.hidden_states[-2]\n\n            # use second text encoder\n            prompt_embeds_2 = pipe.text_encoder_2(token_tensor_2.to(pipe.device), output_hidden_states=True)\n            prompt_embeds_2_hidden_states = prompt_embeds_2.hidden_states[-2]\n            pooled_prompt_embeds = prompt_embeds_2[0]\n\n            prompt_embeds_list = [prompt_embeds_1_hidden_states, prompt_embeds_2_hidden_states]\n            token_embedding = torch.concat(prompt_embeds_list, dim=-1).squeeze(0)\n\n            for j in range(len(weight_tensor)):\n                if weight_tensor[j] != 1.0:\n                    token_embedding[j] = (\n                        token_embedding[-1] + (token_embedding[j] - token_embedding[-1]) * weight_tensor[j]\n                    )\n\n            token_embedding = token_embedding.unsqueeze(0)\n            embeds.append(token_embedding)\n\n            # get negative prompt embeddings with weights\n            neg_token_tensor = torch.tensor([neg_prompt_token_groups[i]], dtype=torch.long, device=pipe.device)\n            neg_token_tensor_2 = torch.tensor([neg_prompt_token_groups_2[i]], dtype=torch.long, device=pipe.device)\n            neg_weight_tensor = torch.tensor(neg_prompt_weight_groups[i], dtype=torch.float16, device=pipe.device)\n\n            # use first text encoder\n            neg_prompt_embeds_1 = pipe.text_encoder(neg_token_tensor.to(pipe.device), output_hidden_states=True)\n            neg_prompt_embeds_1_hidden_states = neg_prompt_embeds_1.hidden_states[-2]\n\n            # use second text encoder\n            neg_prompt_embeds_2 = pipe.text_encoder_2(neg_token_tensor_2.to(pipe.device), output_hidden_states=True)\n            neg_prompt_embeds_2_hidden_states = neg_prompt_embeds_2.hidden_states[-2]\n            negative_pooled_prompt_embeds = neg_prompt_embeds_2[0]\n\n            neg_prompt_embeds_list = [neg_prompt_embeds_1_hidden_states, neg_prompt_embeds_2_hidden_states]\n            neg_token_embedding = torch.concat(neg_prompt_embeds_list, dim=-1).squeeze(0)\n\n            for z in range(len(neg_weight_tensor)):\n                if neg_weight_tensor[z] != 1.0:\n                    neg_token_embedding[z] = (\n                        neg_token_embedding[-1] + (neg_token_embedding[z] - neg_token_embedding[-1]) * neg_weight_tensor[z]\n                    )\n\n            neg_token_embedding = neg_token_embedding.unsqueeze(0)\n            neg_embeds.append(neg_token_embedding)\n\n        prompt_embeds = torch.cat(embeds, dim=1)\n        negative_prompt_embeds = torch.cat(neg_embeds, dim=1)\n\n        if extra_emb is not None:\n            extra_emb = extra_emb.to(prompt_embeds.device, dtype=prompt_embeds.dtype) * extra_emb_alpha\n            prompt_embeds = torch.cat([prompt_embeds, extra_emb], 1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds, torch.zeros_like(extra_emb)], 1)\n            print(f'fix prompt_embeds, extra_emb_alpha={extra_emb_alpha}')\n\n        return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds\n\n    def get_prompt_embeds(self, *args, **kwargs):\n        prompt_embeds, negative_prompt_embeds, _, _ = self.get_weighted_text_embeddings_sdxl(*args, **kwargs)\n        prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n        return prompt_embeds\n\ndef draw_kps(image_pil, kps, color_list=[(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255)]):\n    \n    stickwidth = 4\n    limbSeq = np.array([[0, 2], [1, 2], [3, 2], [4, 2]])\n    kps = np.array(kps)\n\n    w, h = image_pil.size\n    out_img = np.zeros([h, w, 3])\n\n    for i in range(len(limbSeq)):\n        index = limbSeq[i]\n        color = color_list[index[0]]\n\n        x = kps[index][:, 0]\n        y = kps[index][:, 1]\n        length = ((x[0] - x[1]) ** 2 + (y[0] - y[1]) ** 2) ** 0.5\n        angle = math.degrees(math.atan2(y[0] - y[1], x[0] - x[1]))\n        polygon = cv2.ellipse2Poly((int(np.mean(x)), int(np.mean(y))), (int(length / 2), stickwidth), int(angle), 0, 360, 1)\n        out_img = cv2.fillConvexPoly(out_img.copy(), polygon, color)\n    out_img = (out_img * 0.6).astype(np.uint8)\n\n    for idx_kp, kp in enumerate(kps):\n        color = color_list[idx_kp]\n        x, y = kp\n        out_img = cv2.circle(out_img.copy(), (int(x), int(y)), 10, color, -1)\n\n    out_img_pil = PIL.Image.fromarray(out_img.astype(np.uint8))\n    return out_img_pil\n    \nclass StableDiffusionXLInstantIDPipeline(StableDiffusionXLControlNetPipeline):\n    \n    def cuda(self, dtype=torch.float16, use_xformers=False):\n        self.to('cuda', dtype)\n        \n        if hasattr(self, 'image_proj_model'):\n            self.image_proj_model.to(self.unet.device).to(self.unet.dtype)\n        \n        if use_xformers:\n            if is_xformers_available():\n                import xformers\n                from packaging import version\n\n                xformers_version = version.parse(xformers.__version__)\n                if xformers_version == version.parse(\"0.0.16\"):\n                    logger.warn(\n                        \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n                    )\n                self.enable_xformers_memory_efficient_attention()\n            else:\n                raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n    \n    def load_ip_adapter_instantid(self, model_ckpt, image_emb_dim=512, num_tokens=16, scale=0.5):     \n        self.set_image_proj_model(model_ckpt, image_emb_dim, num_tokens)\n        self.set_ip_adapter(model_ckpt, num_tokens, scale)\n        \n    def set_image_proj_model(self, model_ckpt, image_emb_dim=512, num_tokens=16):\n        \n        image_proj_model = Resampler(\n            dim=1280,\n            depth=4,\n            dim_head=64,\n            heads=20,\n            num_queries=num_tokens,\n            embedding_dim=image_emb_dim,\n            output_dim=self.unet.config.cross_attention_dim,\n            ff_mult=4,\n        )\n\n        image_proj_model.eval()\n        \n        self.image_proj_model = image_proj_model.to(self.device, dtype=self.dtype)\n        state_dict = torch.load(model_ckpt, map_location=\"cpu\")\n        if 'image_proj' in state_dict:\n            state_dict = state_dict[\"image_proj\"]\n        self.image_proj_model.load_state_dict(state_dict)\n        \n        self.image_proj_model_in_features = image_emb_dim\n    \n    def set_ip_adapter(self, model_ckpt, num_tokens, scale):\n        \n        unet = self.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = AttnProcessor().to(unet.device, dtype=unet.dtype)\n            else:\n                attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, \n                                                   cross_attention_dim=cross_attention_dim, \n                                                   scale=scale,\n                                                   num_tokens=num_tokens).to(unet.device, dtype=unet.dtype)\n        unet.set_attn_processor(attn_procs)\n        \n        state_dict = torch.load(model_ckpt, map_location=\"cpu\")\n        ip_layers = torch.nn.ModuleList(self.unet.attn_processors.values())\n        if 'ip_adapter' in state_dict:\n            state_dict = state_dict['ip_adapter']\n        ip_layers.load_state_dict(state_dict)\n    \n    def set_ip_adapter_scale(self, scale):\n        unet = getattr(self, self.unet_name) if not hasattr(self, \"unet\") else self.unet\n        for attn_processor in unet.attn_processors.values():\n            if isinstance(attn_processor, IPAttnProcessor):\n                attn_processor.scale = scale\n\n    def _encode_prompt_image_emb(self, prompt_image_emb, device, num_images_per_prompt, dtype, do_classifier_free_guidance):\n        \n        if isinstance(prompt_image_emb, torch.Tensor):\n            prompt_image_emb = prompt_image_emb.clone().detach()\n        else:\n            prompt_image_emb = torch.tensor(prompt_image_emb)\n            \n        prompt_image_emb = prompt_image_emb.reshape([1, -1, self.image_proj_model_in_features])\n        \n        if do_classifier_free_guidance:\n            prompt_image_emb = torch.cat([torch.zeros_like(prompt_image_emb), prompt_image_emb], dim=0)\n        else:\n            prompt_image_emb = torch.cat([prompt_image_emb], dim=0)\n        \n        prompt_image_emb = prompt_image_emb.to(device=self.image_proj_model.latents.device, \n                                               dtype=self.image_proj_model.latents.dtype)\n        prompt_image_emb = self.image_proj_model(prompt_image_emb)\n\n        bs_embed, seq_len, _ = prompt_image_emb.shape\n        prompt_image_emb = prompt_image_emb.repeat(1, num_images_per_prompt, 1)\n        prompt_image_emb = prompt_image_emb.view(bs_embed * num_images_per_prompt, seq_len, -1)\n        \n        return prompt_image_emb.to(device=device, dtype=dtype)\n\n    @torch.no_grad()\n    @replace_example_docstring(EXAMPLE_DOC_STRING)\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        prompt_2: Optional[Union[str, List[str]]] = None,\n        image: PipelineImageInput = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 5.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        image_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n        guess_mode: bool = False,\n        control_guidance_start: Union[float, List[float]] = 0.0,\n        control_guidance_end: Union[float, List[float]] = 1.0,\n        original_size: Tuple[int, int] = None,\n        crops_coords_top_left: Tuple[int, int] = (0, 0),\n        target_size: Tuple[int, int] = None,\n        negative_original_size: Optional[Tuple[int, int]] = None,\n        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n        negative_target_size: Optional[Tuple[int, int]] = None,\n        clip_skip: Optional[int] = None,\n        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n\n        # IP adapter\n        ip_adapter_scale=None,\n\n        # Enhance Face Region\n        control_mask = None,\n\n        **kwargs,\n    ):\n        r\"\"\"\n        The call function to the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n            prompt_2 (`str` or `List[str]`, *optional*):\n                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n                used in both text-encoders.\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, `List[np.ndarray]`,:\n                    `List[List[torch.FloatTensor]]`, `List[List[np.ndarray]]` or `List[List[PIL.Image.Image]]`):\n                The ControlNet input condition to provide guidance to the `unet` for generation. If the type is\n                specified as `torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can also be\n                accepted as an image. The dimensions of the output image defaults to `image`'s dimensions. If height\n                and/or width are passed, `image` is resized accordingly. If multiple ControlNets are specified in\n                `init`, images must be passed as a list such that each element of the list can be correctly batched for\n                input to a single ControlNet.\n            height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n                The height in pixels of the generated image. Anything below 512 pixels won't work well for\n                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n                and checkpoints that are not specifically fine-tuned on low resolutions.\n            width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n                The width in pixels of the generated image. Anything below 512 pixels won't work well for\n                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n                and checkpoints that are not specifically fine-tuned on low resolutions.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 5.0):\n                A higher guidance scale value encourages the model to generate images closely linked to the text\n                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n            negative_prompt_2 (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. This is sent to `tokenizer_2`\n                and `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (畏) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n                generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor is generated by sampling using the supplied random `generator`.\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n                provided, text embeddings are generated from the `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, pooled text embeddings are generated from `prompt` input argument.\n            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs (prompt\n                weighting). If not provided, pooled `negative_prompt_embeds` are generated from `negative_prompt` input\n                argument.\n            image_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated image embeddings.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in\n                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 1.0):\n                The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale` before they are added\n                to the residual in the original `unet`. If multiple ControlNets are specified in `init`, you can set\n                the corresponding scale as a list.\n            guess_mode (`bool`, *optional*, defaults to `False`):\n                The ControlNet encoder tries to recognize the content of the input image even if you remove all\n                prompts. A `guidance_scale` value between 3.0 and 5.0 is recommended.\n            control_guidance_start (`float` or `List[float]`, *optional*, defaults to 0.0):\n                The percentage of total steps at which the ControlNet starts applying.\n            control_guidance_end (`float` or `List[float]`, *optional*, defaults to 1.0):\n                The percentage of total steps at which the ControlNet stops applying.\n            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.\n                `original_size` defaults to `(height, width)` if not specified. Part of SDXL's micro-conditioning as\n                explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                `crops_coords_top_left` can be used to generate an image that appears to be \"cropped\" from the position\n                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting\n                `crops_coords_top_left` to (0, 0). Part of SDXL's micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                For most cases, `target_size` should be set to the desired height and width of the generated image. If\n                not specified it will default to `(height, width)`. Part of SDXL's micro-conditioning as explained in\n                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                To negatively condition the generation process based on a specific image resolution. Part of SDXL's\n                micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL's\n                micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                To negatively condition the generation process based on a target image resolution. It should be as same\n                as the `target_size` for most cases. Part of SDXL's micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            clip_skip (`int`, *optional*):\n                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n                the output of the pre-final layer will be used for computing the prompt embeddings.\n            callback_on_step_end (`Callable`, *optional*):\n                A function that calls at the end of each denoising steps during the inference. The function is called\n                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n                `callback_on_step_end_tensor_inputs`.\n            callback_on_step_end_tensor_inputs (`List`, *optional*):\n                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n                `._callback_tensor_inputs` attribute of your pipeine class.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n                If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,\n                otherwise a `tuple` is returned containing the output images.\n        \"\"\"\n\n        lpw = LongPromptWeight()\n\n        callback = kwargs.pop(\"callback\", None)\n        callback_steps = kwargs.pop(\"callback_steps\", None)\n\n        if callback is not None:\n            deprecate(\n                \"callback\",\n                \"1.0.0\",\n                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n        if callback_steps is not None:\n            deprecate(\n                \"callback_steps\",\n                \"1.0.0\",\n                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n\n        controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n\n        # align format for control guidance\n        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n            mult = len(controlnet.nets) if isinstance(controlnet, MultiControlNetModel) else 1\n            control_guidance_start, control_guidance_end = (\n                mult * [control_guidance_start],\n                mult * [control_guidance_end],\n            )\n        \n        # 0. set ip_adapter_scale\n        if ip_adapter_scale is not None:\n            self.set_ip_adapter_scale(ip_adapter_scale)\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt=prompt,\n            prompt_2=prompt_2,\n            image=image,\n            callback_steps=callback_steps,\n            negative_prompt=negative_prompt,\n            negative_prompt_2=negative_prompt_2,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n        )\n        \n        self._guidance_scale = guidance_scale\n        self._clip_skip = clip_skip\n        self._cross_attention_kwargs = cross_attention_kwargs\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n\n        if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n            controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n\n        global_pool_conditions = (\n            controlnet.config.global_pool_conditions\n            if isinstance(controlnet, ControlNetModel)\n            else controlnet.nets[0].config.global_pool_conditions\n        )\n        guess_mode = guess_mode or global_pool_conditions\n\n        # 3.1 Encode input prompt\n        (\n            prompt_embeds,\n            negative_prompt_embeds,\n            pooled_prompt_embeds,\n            negative_pooled_prompt_embeds,\n        ) = lpw.get_weighted_text_embeddings_sdxl(\n            pipe=self, \n            prompt=prompt, \n            neg_prompt=negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n        )\n        \n        # 3.2 Encode image prompt\n        prompt_image_emb = self._encode_prompt_image_emb(image_embeds, \n                                                         device,\n                                                         num_images_per_prompt,\n                                                         self.unet.dtype,\n                                                         self.do_classifier_free_guidance)\n        \n        # 4. Prepare image\n        if isinstance(controlnet, ControlNetModel):\n            image = self.prepare_image(\n                image=image,\n                width=width,\n                height=height,\n                batch_size=batch_size * num_images_per_prompt,\n                num_images_per_prompt=num_images_per_prompt,\n                device=device,\n                dtype=controlnet.dtype,\n                do_classifier_free_guidance=self.do_classifier_free_guidance,\n                guess_mode=guess_mode,\n            )\n            height, width = image.shape[-2:]\n        elif isinstance(controlnet, MultiControlNetModel):\n            images = []\n\n            for image_ in image:\n                image_ = self.prepare_image(\n                    image=image_,\n                    width=width,\n                    height=height,\n                    batch_size=batch_size * num_images_per_prompt,\n                    num_images_per_prompt=num_images_per_prompt,\n                    device=device,\n                    dtype=controlnet.dtype,\n                    do_classifier_free_guidance=self.do_classifier_free_guidance,\n                    guess_mode=guess_mode,\n                )\n\n                images.append(image_)\n\n            image = images\n            height, width = image[0].shape[-2:]\n        else:\n            assert False\n\n        # 4.1 Region control\n        if control_mask is not None:\n            mask_weight_image = control_mask\n            mask_weight_image = np.array(mask_weight_image)\n            mask_weight_image_tensor = torch.from_numpy(mask_weight_image).to(device=device, dtype=prompt_embeds.dtype)\n            mask_weight_image_tensor = mask_weight_image_tensor[:, :, 0] / 255.\n            mask_weight_image_tensor = mask_weight_image_tensor[None, None]\n            h, w = mask_weight_image_tensor.shape[-2:]\n            control_mask_wight_image_list = []\n            for scale in [8, 8, 8, 16, 16, 16, 32, 32, 32]:\n                scale_mask_weight_image_tensor = F.interpolate(\n                    mask_weight_image_tensor,(h // scale, w // scale), mode='bilinear')\n                control_mask_wight_image_list.append(scale_mask_weight_image_tensor)\n            region_mask = torch.from_numpy(np.array(control_mask)[:, :, 0]).to(self.unet.device, dtype=self.unet.dtype) / 255.\n            region_control.prompt_image_conditioning = [dict(region_mask=region_mask)]\n        else:\n            control_mask_wight_image_list = None\n            region_control.prompt_image_conditioning = [dict(region_mask=None)]\n\n        # 5. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n        self._num_timesteps = len(timesteps)\n\n        # 6. Prepare latent variables\n        num_channels_latents = self.unet.config.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6.5 Optionally get Guidance Scale Embedding\n        timestep_cond = None\n        if self.unet.config.time_cond_proj_dim is not None:\n            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n            timestep_cond = self.get_guidance_scale_embedding(\n                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n            ).to(device=device, dtype=latents.dtype)\n\n        # 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 7.1 Create tensor stating which controlnets to keep\n        controlnet_keep = []\n        for i in range(len(timesteps)):\n            keeps = [\n                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n                for s, e in zip(control_guidance_start, control_guidance_end)\n            ]\n            controlnet_keep.append(keeps[0] if isinstance(controlnet, ControlNetModel) else keeps)\n\n        # 7.2 Prepare added time ids & embeddings\n        if isinstance(image, list):\n            original_size = original_size or image[0].shape[-2:]\n        else:\n            original_size = original_size or image.shape[-2:]\n        target_size = target_size or (height, width)\n\n        add_text_embeds = pooled_prompt_embeds\n        if self.text_encoder_2 is None:\n            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n        else:\n            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n\n        add_time_ids = self._get_add_time_ids(\n            original_size,\n            crops_coords_top_left,\n            target_size,\n            dtype=prompt_embeds.dtype,\n            text_encoder_projection_dim=text_encoder_projection_dim,\n        )\n\n        if negative_original_size is not None and negative_target_size is not None:\n            negative_add_time_ids = self._get_add_time_ids(\n                negative_original_size,\n                negative_crops_coords_top_left,\n                negative_target_size,\n                dtype=prompt_embeds.dtype,\n                text_encoder_projection_dim=text_encoder_projection_dim,\n            )\n        else:\n            negative_add_time_ids = add_time_ids\n\n        if self.do_classifier_free_guidance:\n            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n\n        prompt_embeds = prompt_embeds.to(device)\n        add_text_embeds = add_text_embeds.to(device)\n        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n        encoder_hidden_states = torch.cat([prompt_embeds, prompt_image_emb], dim=1)\n\n        # 8. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        is_unet_compiled = is_compiled_module(self.unet)\n        is_controlnet_compiled = is_compiled_module(self.controlnet)\n        is_torch_higher_equal_2_1 = is_torch_version(\">=\", \"2.1\")\n                \n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # Relevant thread:\n                # https://dev-discuss.pytorch.org/t/cudagraphs-in-pytorch-2-0/1428\n                if (is_unet_compiled and is_controlnet_compiled) and is_torch_higher_equal_2_1:\n                    torch._inductor.cudagraph_mark_step_begin()\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n\n                # controlnet(s) inference\n                if guess_mode and self.do_classifier_free_guidance:\n                    # Infer ControlNet only for the conditional batch.\n                    control_model_input = latents\n                    control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                    controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n                    controlnet_added_cond_kwargs = {\n                        \"text_embeds\": add_text_embeds.chunk(2)[1],\n                        \"time_ids\": add_time_ids.chunk(2)[1],\n                    }\n                else:\n                    control_model_input = latent_model_input\n                    controlnet_prompt_embeds = prompt_embeds\n                    controlnet_added_cond_kwargs = added_cond_kwargs\n                \n                if isinstance(controlnet_keep[i], list):\n                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n                else:\n                    controlnet_cond_scale = controlnet_conditioning_scale\n                    if isinstance(controlnet_cond_scale, list):\n                        controlnet_cond_scale = controlnet_cond_scale[0]\n                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n\n                if isinstance(self.controlnet, MultiControlNetModel):\n                    down_block_res_samples_list, mid_block_res_sample_list = [], []\n                    for control_index in range(len(self.controlnet.nets)):\n                        controlnet = self.controlnet.nets[control_index]\n                        if control_index == 0:\n                            # assume fhe first controlnet is IdentityNet\n                            controlnet_prompt_embeds = prompt_image_emb\n                        else:\n                            controlnet_prompt_embeds = prompt_embeds\n                        down_block_res_samples, mid_block_res_sample = controlnet(control_model_input,\n                                                                                  t,\n                                                                                  encoder_hidden_states=controlnet_prompt_embeds,\n                                                                                  controlnet_cond=image[control_index],\n                                                                                  conditioning_scale=cond_scale[control_index],\n                                                                                  guess_mode=guess_mode,\n                                                                                  added_cond_kwargs=controlnet_added_cond_kwargs,\n                                                                                  return_dict=False)\n\n                        # controlnet mask\n                        if control_index == 0 and control_mask_wight_image_list is not None:\n                            down_block_res_samples = [\n                                down_block_res_sample * mask_weight\n                                for down_block_res_sample, mask_weight in zip(down_block_res_samples, control_mask_wight_image_list)\n                            ]\n                            mid_block_res_sample *= control_mask_wight_image_list[-1]\n\n                        down_block_res_samples_list.append(down_block_res_samples)\n                        mid_block_res_sample_list.append(mid_block_res_sample)\n\n                    mid_block_res_sample = torch.stack(mid_block_res_sample_list).sum(dim=0)\n                    down_block_res_samples = [torch.stack(down_block_res_samples).sum(dim=0) for down_block_res_samples in\n                                              zip(*down_block_res_samples_list)]\n                else:\n                    down_block_res_samples, mid_block_res_sample = self.controlnet(\n                        control_model_input,\n                        t,\n                        encoder_hidden_states=prompt_image_emb,\n                        controlnet_cond=image,\n                        conditioning_scale=cond_scale,\n                        guess_mode=guess_mode,\n                        added_cond_kwargs=controlnet_added_cond_kwargs,\n                        return_dict=False,\n                    )\n\n                    # controlnet mask\n                    if control_mask_wight_image_list is not None:\n                        down_block_res_samples = [\n                            down_block_res_sample * mask_weight\n                            for down_block_res_sample, mask_weight in zip(down_block_res_samples, control_mask_wight_image_list)\n                        ]\n                        mid_block_res_sample *= control_mask_wight_image_list[-1]\n\n                if guess_mode and self.do_classifier_free_guidance:\n                    # Infered ControlNet only for the conditional batch.\n                    # To apply the output of ControlNet to both the unconditional and conditional batches,\n                    # add 0 to the unconditional batch to keep it unchanged.\n                    down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                    mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n\n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=encoder_hidden_states,\n                    timestep_cond=timestep_cond,\n                    cross_attention_kwargs=self.cross_attention_kwargs,\n                    down_block_additional_residuals=down_block_res_samples,\n                    mid_block_additional_residual=mid_block_res_sample,\n                    added_cond_kwargs=added_cond_kwargs,\n                    return_dict=False,\n                )[0]\n\n                # perform guidance\n                if self.do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n\n                if callback_on_step_end is not None:\n                    callback_kwargs = {}\n                    for k in callback_on_step_end_tensor_inputs:\n                        callback_kwargs[k] = locals()[k]\n                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n\n                    latents = callback_outputs.pop(\"latents\", latents)\n                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n                        callback(step_idx, t, latents)\n        \n        if not output_type == \"latent\":\n            # make sure the VAE is in float32 mode, as it overflows in float16\n            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n\n            if needs_upcasting:\n                self.upcast_vae()\n                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n\n            # unscale/denormalize the latents\n            # denormalize with the mean and std if available and not None\n            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n            if has_latents_mean and has_latents_std:\n                latents_mean = (\n                    torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n                )\n                latents_std = (\n                    torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n                )\n                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n            else:\n                latents = latents / self.vae.config.scaling_factor\n\n            image = self.vae.decode(latents, return_dict=False)[0]\n\n            # cast back to fp16 if needed\n            if needs_upcasting:\n                self.vae.to(dtype=torch.float16)\n        else:\n            image = latents\n\n        if not output_type == \"latent\":\n            # apply watermark if available\n            if self.watermark is not None:\n                image = self.watermark.apply_watermark(image)\n\n            image = self.image_processor.postprocess(image, output_type=output_type)\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (image,)\n\n        return StableDiffusionXLPipelineOutput(images=image)\n"
        },
        {
          "name": "pipeline_stable_diffusion_xl_instantid_img2img.py",
          "type": "blob",
          "size": 49.0244140625,
          "content": "# Copyright 2024 The InstantX Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport math\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport cv2\nimport numpy as np\nimport PIL.Image\nimport torch\nimport torch.nn as nn\n\nfrom diffusers import StableDiffusionXLControlNetImg2ImgPipeline\nfrom diffusers.image_processor import PipelineImageInput\nfrom diffusers.models import ControlNetModel\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\nfrom diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput\nfrom diffusers.utils import (\n    deprecate,\n    logging,\n    replace_example_docstring,\n)\nfrom diffusers.utils.import_utils import is_xformers_available\nfrom diffusers.utils.torch_utils import is_compiled_module, is_torch_version\n\n\ntry:\n    import xformers\n    import xformers.ops\n\n    xformers_available = True\nexcept Exception:\n    xformers_available = False\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\ndef FeedForward(dim, mult=4):\n    inner_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, inner_dim, bias=False),\n        nn.GELU(),\n        nn.Linear(inner_dim, dim, bias=False),\n    )\n\n\ndef reshape_tensor(x, heads):\n    bs, length, width = x.shape\n    # (bs, length, width) --> (bs, length, n_heads, dim_per_head)\n    x = x.view(bs, length, heads, -1)\n    # (bs, length, n_heads, dim_per_head) --> (bs, n_heads, length, dim_per_head)\n    x = x.transpose(1, 2)\n    # (bs, n_heads, length, dim_per_head) --> (bs*n_heads, length, dim_per_head)\n    x = x.reshape(bs, heads, length, -1)\n    return x\n\n\nclass PerceiverAttention(nn.Module):\n    def __init__(self, *, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.dim_head = dim_head\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n    def forward(self, x, latents):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, n1, D)\n            latent (torch.Tensor): latent features\n                shape (b, n2, D)\n        \"\"\"\n        x = self.norm1(x)\n        latents = self.norm2(latents)\n\n        b, l, _ = latents.shape\n\n        q = self.to_q(latents)\n        kv_input = torch.cat((x, latents), dim=-2)\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n        q = reshape_tensor(q, self.heads)\n        k = reshape_tensor(k, self.heads)\n        v = reshape_tensor(v, self.heads)\n\n        # attention\n        scale = 1 / math.sqrt(math.sqrt(self.dim_head))\n        weight = (q * scale) @ (k * scale).transpose(-2, -1)  # More stable with f16 than dividing afterwards\n        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n        out = weight @ v\n\n        out = out.permute(0, 2, 1, 3).reshape(b, l, -1)\n\n        return self.to_out(out)\n\n\nclass Resampler(nn.Module):\n    def __init__(\n        self,\n        dim=1024,\n        depth=8,\n        dim_head=64,\n        heads=16,\n        num_queries=8,\n        embedding_dim=768,\n        output_dim=1024,\n        ff_mult=4,\n    ):\n        super().__init__()\n\n        self.latents = nn.Parameter(torch.randn(1, num_queries, dim) / dim**0.5)\n\n        self.proj_in = nn.Linear(embedding_dim, dim)\n\n        self.proj_out = nn.Linear(dim, output_dim)\n        self.norm_out = nn.LayerNorm(output_dim)\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, x):\n        latents = self.latents.repeat(x.size(0), 1, 1)\n        x = self.proj_in(x)\n\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n\n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n\n\nclass AttnProcessor(nn.Module):\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size=None,\n        cross_attention_dim=None,\n    ):\n        super().__init__()\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.norm_cross:\n            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass IPAttnProcessor(nn.Module):\n    r\"\"\"\n    Attention processor for IP-Adapater.\n    Args:\n        hidden_size (`int`):\n            The hidden size of the attention layer.\n        cross_attention_dim (`int`):\n            The number of channels in the `encoder_hidden_states`.\n        scale (`float`, defaults to 1.0):\n            the weight scale of image prompt.\n        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):\n            The context length of the image features.\n    \"\"\"\n\n    def __init__(self, hidden_size, cross_attention_dim=None, scale=1.0, num_tokens=4):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.scale = scale\n        self.num_tokens = num_tokens\n\n        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            # get encoder_hidden_states, ip_hidden_states\n            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n            encoder_hidden_states, ip_hidden_states = (\n                encoder_hidden_states[:, :end_pos, :],\n                encoder_hidden_states[:, end_pos:, :],\n            )\n            if attn.norm_cross:\n                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        if xformers_available:\n            hidden_states = self._memory_efficient_attention_xformers(query, key, value, attention_mask)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # for ip-adapter\n        ip_key = self.to_k_ip(ip_hidden_states)\n        ip_value = self.to_v_ip(ip_hidden_states)\n\n        ip_key = attn.head_to_batch_dim(ip_key)\n        ip_value = attn.head_to_batch_dim(ip_value)\n\n        if xformers_available:\n            ip_hidden_states = self._memory_efficient_attention_xformers(query, ip_key, ip_value, None)\n        else:\n            ip_attention_probs = attn.get_attention_scores(query, ip_key, None)\n            ip_hidden_states = torch.bmm(ip_attention_probs, ip_value)\n        ip_hidden_states = attn.batch_to_head_dim(ip_hidden_states)\n\n        hidden_states = hidden_states + self.scale * ip_hidden_states\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n    def _memory_efficient_attention_xformers(self, query, key, value, attention_mask):\n        # TODO attention_mask\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n        return hidden_states\n\n\nEXAMPLE_DOC_STRING = \"\"\"\n    Examples:\n        ```py\n        >>> # !pip install opencv-python transformers accelerate insightface\n        >>> import diffusers\n        >>> from diffusers.utils import load_image\n        >>> from diffusers.models import ControlNetModel\n\n        >>> import cv2\n        >>> import torch\n        >>> import numpy as np\n        >>> from PIL import Image\n\n        >>> from insightface.app import FaceAnalysis\n        >>> from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\n        >>> # download 'antelopev2' under ./models\n        >>> app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n        >>> app.prepare(ctx_id=0, det_size=(640, 640))\n\n        >>> # download models under ./checkpoints\n        >>> face_adapter = f'./checkpoints/ip-adapter.bin'\n        >>> controlnet_path = f'./checkpoints/ControlNetModel'\n\n        >>> # load IdentityNet\n        >>> controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n\n        >>> pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n        ...     \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, torch_dtype=torch.float16\n        ... )\n        >>> pipe.cuda()\n\n        >>> # load adapter\n        >>> pipe.load_ip_adapter_instantid(face_adapter)\n\n        >>> prompt = \"analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality\"\n        >>> negative_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured\"\n\n        >>> # load an image\n        >>> image = load_image(\"your-example.jpg\")\n\n        >>> face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))[-1]\n        >>> face_emb = face_info['embedding']\n        >>> face_kps = draw_kps(face_image, face_info['kps'])\n\n        >>> pipe.set_ip_adapter_scale(0.8)\n\n        >>> # generate image\n        >>> image = pipe(\n        ...     prompt, image_embeds=face_emb, image=face_kps, controlnet_conditioning_scale=0.8\n        ... ).images[0]\n        ```\n\"\"\"\n\n\ndef draw_kps(image_pil, kps, color_list=[(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]):\n    stickwidth = 4\n    limbSeq = np.array([[0, 2], [1, 2], [3, 2], [4, 2]])\n    kps = np.array(kps)\n\n    w, h = image_pil.size\n    out_img = np.zeros([h, w, 3])\n\n    for i in range(len(limbSeq)):\n        index = limbSeq[i]\n        color = color_list[index[0]]\n\n        x = kps[index][:, 0]\n        y = kps[index][:, 1]\n        length = ((x[0] - x[1]) ** 2 + (y[0] - y[1]) ** 2) ** 0.5\n        angle = math.degrees(math.atan2(y[0] - y[1], x[0] - x[1]))\n        polygon = cv2.ellipse2Poly(\n            (int(np.mean(x)), int(np.mean(y))), (int(length / 2), stickwidth), int(angle), 0, 360, 1\n        )\n        out_img = cv2.fillConvexPoly(out_img.copy(), polygon, color)\n    out_img = (out_img * 0.6).astype(np.uint8)\n\n    for idx_kp, kp in enumerate(kps):\n        color = color_list[idx_kp]\n        x, y = kp\n        out_img = cv2.circle(out_img.copy(), (int(x), int(y)), 10, color, -1)\n\n    out_img_pil = PIL.Image.fromarray(out_img.astype(np.uint8))\n    return out_img_pil\n\n\nclass StableDiffusionXLInstantIDImg2ImgPipeline(StableDiffusionXLControlNetImg2ImgPipeline):\n    def cuda(self, dtype=torch.float16, use_xformers=False):\n        self.to(\"cuda\", dtype)\n\n        if hasattr(self, \"image_proj_model\"):\n            self.image_proj_model.to(self.unet.device).to(self.unet.dtype)\n\n        if use_xformers:\n            if is_xformers_available():\n                import xformers\n                from packaging import version\n\n                xformers_version = version.parse(xformers.__version__)\n                if xformers_version == version.parse(\"0.0.16\"):\n                    logger.warning(\n                        \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n                    )\n                self.enable_xformers_memory_efficient_attention()\n            else:\n                raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n\n    def load_ip_adapter_instantid(self, model_ckpt, image_emb_dim=512, num_tokens=16, scale=0.5):\n        self.set_image_proj_model(model_ckpt, image_emb_dim, num_tokens)\n        self.set_ip_adapter(model_ckpt, num_tokens, scale)\n\n    def set_image_proj_model(self, model_ckpt, image_emb_dim=512, num_tokens=16):\n        image_proj_model = Resampler(\n            dim=1280,\n            depth=4,\n            dim_head=64,\n            heads=20,\n            num_queries=num_tokens,\n            embedding_dim=image_emb_dim,\n            output_dim=self.unet.config.cross_attention_dim,\n            ff_mult=4,\n        )\n\n        image_proj_model.eval()\n\n        self.image_proj_model = image_proj_model.to(self.device, dtype=self.dtype)\n        state_dict = torch.load(model_ckpt, map_location=\"cpu\")\n        if \"image_proj\" in state_dict:\n            state_dict = state_dict[\"image_proj\"]\n        self.image_proj_model.load_state_dict(state_dict)\n\n        self.image_proj_model_in_features = image_emb_dim\n\n    def set_ip_adapter(self, model_ckpt, num_tokens, scale):\n        unet = self.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = AttnProcessor().to(unet.device, dtype=unet.dtype)\n            else:\n                attn_procs[name] = IPAttnProcessor(\n                    hidden_size=hidden_size,\n                    cross_attention_dim=cross_attention_dim,\n                    scale=scale,\n                    num_tokens=num_tokens,\n                ).to(unet.device, dtype=unet.dtype)\n        unet.set_attn_processor(attn_procs)\n\n        state_dict = torch.load(model_ckpt, map_location=\"cpu\")\n        ip_layers = torch.nn.ModuleList(self.unet.attn_processors.values())\n        if \"ip_adapter\" in state_dict:\n            state_dict = state_dict[\"ip_adapter\"]\n        ip_layers.load_state_dict(state_dict)\n\n    def set_ip_adapter_scale(self, scale):\n        unet = getattr(self, self.unet_name) if not hasattr(self, \"unet\") else self.unet\n        for attn_processor in unet.attn_processors.values():\n            if isinstance(attn_processor, IPAttnProcessor):\n                attn_processor.scale = scale\n\n    def _encode_prompt_image_emb(self, prompt_image_emb, device, dtype, do_classifier_free_guidance):\n        if isinstance(prompt_image_emb, torch.Tensor):\n            prompt_image_emb = prompt_image_emb.clone().detach()\n        else:\n            prompt_image_emb = torch.tensor(prompt_image_emb)\n\n        prompt_image_emb = prompt_image_emb.to(device=device, dtype=dtype)\n        prompt_image_emb = prompt_image_emb.reshape([1, -1, self.image_proj_model_in_features])\n\n        if do_classifier_free_guidance:\n            prompt_image_emb = torch.cat([torch.zeros_like(prompt_image_emb), prompt_image_emb], dim=0)\n        else:\n            prompt_image_emb = torch.cat([prompt_image_emb], dim=0)\n        image_proj_model_device = self.image_proj_model.to(device)\n        prompt_image_emb = image_proj_model_device(prompt_image_emb)\n        return prompt_image_emb\n\n    @torch.no_grad()\n    @replace_example_docstring(EXAMPLE_DOC_STRING)\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        prompt_2: Optional[Union[str, List[str]]] = None,\n        image: PipelineImageInput = None,\n        control_image: PipelineImageInput = None,\n        strength: float = 0.8,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 5.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        image_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n        guess_mode: bool = False,\n        control_guidance_start: Union[float, List[float]] = 0.0,\n        control_guidance_end: Union[float, List[float]] = 1.0,\n        original_size: Tuple[int, int] = None,\n        crops_coords_top_left: Tuple[int, int] = (0, 0),\n        target_size: Tuple[int, int] = None,\n        negative_original_size: Optional[Tuple[int, int]] = None,\n        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n        negative_target_size: Optional[Tuple[int, int]] = None,\n        aesthetic_score: float = 6.0,\n        negative_aesthetic_score: float = 2.5,\n        clip_skip: Optional[int] = None,\n        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n        **kwargs,\n    ):\n        r\"\"\"\n        The call function to the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n            prompt_2 (`str` or `List[str]`, *optional*):\n                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n                used in both text-encoders.\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, `List[np.ndarray]`,:\n                    `List[List[torch.FloatTensor]]`, `List[List[np.ndarray]]` or `List[List[PIL.Image.Image]]`):\n                The ControlNet input condition to provide guidance to the `unet` for generation. If the type is\n                specified as `torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can also be\n                accepted as an image. The dimensions of the output image defaults to `image`'s dimensions. If height\n                and/or width are passed, `image` is resized accordingly. If multiple ControlNets are specified in\n                `init`, images must be passed as a list such that each element of the list can be correctly batched for\n                input to a single ControlNet.\n            height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n                The height in pixels of the generated image. Anything below 512 pixels won't work well for\n                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n                and checkpoints that are not specifically fine-tuned on low resolutions.\n            width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n                The width in pixels of the generated image. Anything below 512 pixels won't work well for\n                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n                and checkpoints that are not specifically fine-tuned on low resolutions.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 5.0):\n                A higher guidance scale value encourages the model to generate images closely linked to the text\n                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n            negative_prompt_2 (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. This is sent to `tokenizer_2`\n                and `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (畏) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n                generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor is generated by sampling using the supplied random `generator`.\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n                provided, text embeddings are generated from the `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, pooled text embeddings are generated from `prompt` input argument.\n            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs (prompt\n                weighting). If not provided, pooled `negative_prompt_embeds` are generated from `negative_prompt` input\n                argument.\n            image_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated image embeddings.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in\n                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 1.0):\n                The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale` before they are added\n                to the residual in the original `unet`. If multiple ControlNets are specified in `init`, you can set\n                the corresponding scale as a list.\n            guess_mode (`bool`, *optional*, defaults to `False`):\n                The ControlNet encoder tries to recognize the content of the input image even if you remove all\n                prompts. A `guidance_scale` value between 3.0 and 5.0 is recommended.\n            control_guidance_start (`float` or `List[float]`, *optional*, defaults to 0.0):\n                The percentage of total steps at which the ControlNet starts applying.\n            control_guidance_end (`float` or `List[float]`, *optional*, defaults to 1.0):\n                The percentage of total steps at which the ControlNet stops applying.\n            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.\n                `original_size` defaults to `(height, width)` if not specified. Part of SDXL's micro-conditioning as\n                explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                `crops_coords_top_left` can be used to generate an image that appears to be \"cropped\" from the position\n                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting\n                `crops_coords_top_left` to (0, 0). Part of SDXL's micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                For most cases, `target_size` should be set to the desired height and width of the generated image. If\n                not specified it will default to `(height, width)`. Part of SDXL's micro-conditioning as explained in\n                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                To negatively condition the generation process based on a specific image resolution. Part of SDXL's\n                micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL's\n                micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                To negatively condition the generation process based on a target image resolution. It should be as same\n                as the `target_size` for most cases. Part of SDXL's micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            clip_skip (`int`, *optional*):\n                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n                the output of the pre-final layer will be used for computing the prompt embeddings.\n            callback_on_step_end (`Callable`, *optional*):\n                A function that calls at the end of each denoising steps during the inference. The function is called\n                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n                `callback_on_step_end_tensor_inputs`.\n            callback_on_step_end_tensor_inputs (`List`, *optional*):\n                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n                `._callback_tensor_inputs` attribute of your pipeline class.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n                If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,\n                otherwise a `tuple` is returned containing the output images.\n        \"\"\"\n\n        callback = kwargs.pop(\"callback\", None)\n        callback_steps = kwargs.pop(\"callback_steps\", None)\n\n        if callback is not None:\n            deprecate(\n                \"callback\",\n                \"1.0.0\",\n                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n        if callback_steps is not None:\n            deprecate(\n                \"callback_steps\",\n                \"1.0.0\",\n                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n\n        controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n\n        # align format for control guidance\n        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n            mult = len(controlnet.nets) if isinstance(controlnet, MultiControlNetModel) else 1\n            control_guidance_start, control_guidance_end = (\n                mult * [control_guidance_start],\n                mult * [control_guidance_end],\n            )\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt,\n            prompt_2,\n            control_image,\n            strength,\n            num_inference_steps,\n            callback_steps,\n            negative_prompt,\n            negative_prompt_2,\n            prompt_embeds,\n            negative_prompt_embeds,\n            pooled_prompt_embeds,\n            negative_pooled_prompt_embeds,\n            None,\n            None,\n            controlnet_conditioning_scale,\n            control_guidance_start,\n            control_guidance_end,\n            callback_on_step_end_tensor_inputs,\n        )\n\n        self._guidance_scale = guidance_scale\n        self._clip_skip = clip_skip\n        self._cross_attention_kwargs = cross_attention_kwargs\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n\n        if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n            controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n\n        global_pool_conditions = (\n            controlnet.config.global_pool_conditions\n            if isinstance(controlnet, ControlNetModel)\n            else controlnet.nets[0].config.global_pool_conditions\n        )\n        guess_mode = guess_mode or global_pool_conditions\n\n        # 3.1 Encode input prompt\n        text_encoder_lora_scale = (\n            self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n        )\n        (\n            prompt_embeds,\n            negative_prompt_embeds,\n            pooled_prompt_embeds,\n            negative_pooled_prompt_embeds,\n        ) = self.encode_prompt(\n            prompt,\n            prompt_2,\n            device,\n            num_images_per_prompt,\n            self.do_classifier_free_guidance,\n            negative_prompt,\n            negative_prompt_2,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            lora_scale=text_encoder_lora_scale,\n            clip_skip=self.clip_skip,\n        )\n\n        # 3.2 Encode image prompt\n        prompt_image_emb = self._encode_prompt_image_emb(\n            image_embeds, device, self.unet.dtype, self.do_classifier_free_guidance\n        )\n        bs_embed, seq_len, _ = prompt_image_emb.shape\n        prompt_image_emb = prompt_image_emb.repeat(1, num_images_per_prompt, 1)\n        prompt_image_emb = prompt_image_emb.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n        # 4. Prepare image and controlnet_conditioning_image\n        image = self.image_processor.preprocess(image, height=height, width=width).to(dtype=torch.float32)\n\n        if isinstance(controlnet, ControlNetModel):\n            control_image = self.prepare_control_image(\n                image=control_image,\n                width=width,\n                height=height,\n                batch_size=batch_size * num_images_per_prompt,\n                num_images_per_prompt=num_images_per_prompt,\n                device=device,\n                dtype=controlnet.dtype,\n                do_classifier_free_guidance=self.do_classifier_free_guidance,\n                guess_mode=guess_mode,\n            )\n            height, width = control_image.shape[-2:]\n        elif isinstance(controlnet, MultiControlNetModel):\n            control_images = []\n\n            for control_image_ in control_image:\n                control_image_ = self.prepare_control_image(\n                    image=control_image_,\n                    width=width,\n                    height=height,\n                    batch_size=batch_size * num_images_per_prompt,\n                    num_images_per_prompt=num_images_per_prompt,\n                    device=device,\n                    dtype=controlnet.dtype,\n                    do_classifier_free_guidance=self.do_classifier_free_guidance,\n                    guess_mode=guess_mode,\n                )\n\n                control_images.append(control_image_)\n\n            control_image = control_images\n            height, width = control_image[0].shape[-2:]\n        else:\n            assert False\n\n        # 5. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n        self._num_timesteps = len(timesteps)\n\n        # 6. Prepare latent variables\n        latents = self.prepare_latents(\n            image,\n            latent_timestep,\n            batch_size,\n            num_images_per_prompt,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            True,\n        )\n\n        # # 6.5 Optionally get Guidance Scale Embedding\n        timestep_cond = None\n        if self.unet.config.time_cond_proj_dim is not None:\n            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n            timestep_cond = self.get_guidance_scale_embedding(\n                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n            ).to(device=device, dtype=latents.dtype)\n\n        # 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 7.1 Create tensor stating which controlnets to keep\n        controlnet_keep = []\n        for i in range(len(timesteps)):\n            keeps = [\n                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n                for s, e in zip(control_guidance_start, control_guidance_end)\n            ]\n            controlnet_keep.append(keeps[0] if isinstance(controlnet, ControlNetModel) else keeps)\n\n        # 7.2 Prepare added time ids & embeddings\n        if isinstance(control_image, list):\n            original_size = original_size or control_image[0].shape[-2:]\n        else:\n            original_size = original_size or control_image.shape[-2:]\n        target_size = target_size or (height, width)\n\n        if negative_original_size is None:\n            negative_original_size = original_size\n        if negative_target_size is None:\n            negative_target_size = target_size\n        add_text_embeds = pooled_prompt_embeds\n\n        if self.text_encoder_2 is None:\n            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n        else:\n            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n\n        add_time_ids, add_neg_time_ids = self._get_add_time_ids(\n            original_size,\n            crops_coords_top_left,\n            target_size,\n            aesthetic_score,\n            negative_aesthetic_score,\n            negative_original_size,\n            negative_crops_coords_top_left,\n            negative_target_size,\n            dtype=prompt_embeds.dtype,\n            text_encoder_projection_dim=text_encoder_projection_dim,\n        )\n        add_time_ids = add_time_ids.repeat(batch_size * num_images_per_prompt, 1)\n\n        if self.do_classifier_free_guidance:\n            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n            add_neg_time_ids = add_neg_time_ids.repeat(batch_size * num_images_per_prompt, 1)\n            add_time_ids = torch.cat([add_neg_time_ids, add_time_ids], dim=0)\n\n        prompt_embeds = prompt_embeds.to(device)\n        add_text_embeds = add_text_embeds.to(device)\n        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n        encoder_hidden_states = torch.cat([prompt_embeds, prompt_image_emb], dim=1)\n\n        # 8. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        is_unet_compiled = is_compiled_module(self.unet)\n        is_controlnet_compiled = is_compiled_module(self.controlnet)\n        is_torch_higher_equal_2_1 = is_torch_version(\">=\", \"2.1\")\n\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # Relevant thread:\n                # https://dev-discuss.pytorch.org/t/cudagraphs-in-pytorch-2-0/1428\n                if (is_unet_compiled and is_controlnet_compiled) and is_torch_higher_equal_2_1:\n                    torch._inductor.cudagraph_mark_step_begin()\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n\n                # controlnet(s) inference\n                if guess_mode and self.do_classifier_free_guidance:\n                    # Infer ControlNet only for the conditional batch.\n                    control_model_input = latents\n                    control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                    controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n                    controlnet_added_cond_kwargs = {\n                        \"text_embeds\": add_text_embeds.chunk(2)[1],\n                        \"time_ids\": add_time_ids.chunk(2)[1],\n                    }\n                else:\n                    control_model_input = latent_model_input\n                    controlnet_prompt_embeds = prompt_embeds\n                    controlnet_added_cond_kwargs = added_cond_kwargs\n\n                if isinstance(controlnet_keep[i], list):\n                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n                else:\n                    controlnet_cond_scale = controlnet_conditioning_scale\n                    if isinstance(controlnet_cond_scale, list):\n                        controlnet_cond_scale = controlnet_cond_scale[0]\n                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n\n                down_block_res_samples, mid_block_res_sample = self.controlnet(\n                    control_model_input,\n                    t,\n                    encoder_hidden_states=prompt_image_emb,\n                    controlnet_cond=control_image,\n                    conditioning_scale=cond_scale,\n                    guess_mode=guess_mode,\n                    added_cond_kwargs=controlnet_added_cond_kwargs,\n                    return_dict=False,\n                )\n\n                if guess_mode and self.do_classifier_free_guidance:\n                    # Infered ControlNet only for the conditional batch.\n                    # To apply the output of ControlNet to both the unconditional and conditional batches,\n                    # add 0 to the unconditional batch to keep it unchanged.\n                    down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                    mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n\n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=encoder_hidden_states,\n                    timestep_cond=timestep_cond,\n                    cross_attention_kwargs=self.cross_attention_kwargs,\n                    down_block_additional_residuals=down_block_res_samples,\n                    mid_block_additional_residual=mid_block_res_sample,\n                    added_cond_kwargs=added_cond_kwargs,\n                    return_dict=False,\n                )[0]\n\n                # perform guidance\n                if self.do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n\n                if callback_on_step_end is not None:\n                    callback_kwargs = {}\n                    for k in callback_on_step_end_tensor_inputs:\n                        callback_kwargs[k] = locals()[k]\n                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n\n                    latents = callback_outputs.pop(\"latents\", latents)\n                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n                        callback(step_idx, t, latents)\n\n        if not output_type == \"latent\":\n            # make sure the VAE is in float32 mode, as it overflows in float16\n            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n            if needs_upcasting:\n                self.upcast_vae()\n                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n\n            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n\n            # cast back to fp16 if needed\n            if needs_upcasting:\n                self.vae.to(dtype=torch.float16)\n        else:\n            image = latents\n\n        if not output_type == \"latent\":\n            # apply watermark if available\n            if self.watermark is not None:\n                image = self.watermark.apply_watermark(image)\n\n            image = self.image_processor.postprocess(image, output_type=output_type)\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (image,)\n\n        return StableDiffusionXLPipelineOutput(images=image)\n"
        }
      ]
    }
  ]
}