{
  "metadata": {
    "timestamp": 1736561087435,
    "page": 17,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "microsoft/JARVIS",
      "stars": 23866,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.3310546875,
          "content": "# ALL\n*.dev.yaml\n\nlegacy/\n\n# for server\nserver/models/*\n!server/models/download.sh\n!server/models/download.ps1\nserver/logs/\nserver/models_dev\nserver/public/*\n!server/public/examples/\nserver/public/examples/*\n!server/public/examples/a.jpg\n!server/public/examples/b.jpg\n!server/public/examples/c.jpg\n!server/public/examples/d.jpg\n!server/public/examples/e.jpg\n!server/public/examples/f.jpg\n!server/public/examples/g.jpg\n\n# docker\nDockerfile\ndocker-compose.yml\n\n# for gradio\n# server/run_gradio.py\n\n# for web\nweb/node_modules\nweb/package-lock.json\nweb/dist\nweb/electron-dist\nweb/yarn.lock\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 1.1025390625,
          "content": "cff-version: 1.2.0\nmessage: \"If you find this work useful in your method, you can cite the paper as below.\"\nauthors:\n  - family-names: Shen\n    given-names: Yongliang\n  - family-names: Song\n    given-names: Kaitao\n  - family-names: Tan\n    given-names: Xu\n  - family-names: Li\n    given-names: Dongsheng\n  - family-names: Lu\n    given-names: Weiming\n  - family-names: Zhuang\n    given-names: Yueting\ntitle: \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace\"\nversion: 2.0.4\nlicense: MIT\nstatus: preprint\ndate-released: 2023-03-30\nurl: https://github.com/microsoft/JARVIS\npreferred-citation:\n  type: article\n  authors:\n    - family-names: Shen\n      given-names: Yongliang\n    - family-names: Song\n      given-names: Kaitao\n    - family-names: Tan\n      given-names: Xu\n    - family-names: Li\n      given-names: Dongsheng\n    - family-names: Lu\n      given-names: Weiming\n    - family-names: Zhuang\n      given-names: Yueting\n  journal: \"arXiv preprint arXiv:2303.17580\"\n  title: \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace\"\n  year: 2023\n  url: https://arxiv.org/abs/2303.17580"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.43359375,
          "content": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.66015625,
          "content": "\nTo contribute to this GitHub project, you can follow these steps:\n\n1. Fork the repository you want to contribute to by clicking the \"Fork\" button on the project page.\n\n2. Clone the repository to your local machine and enter the newly created repo using the following commands:\n\n```\ngit clone https://github.com/YOUR-GITHUB-USERNAME/JARVIS\ncd JARVIS\n```\n3. Create a new branch for your changes using the following command:\n\n```\ngit checkout -b \"branch-name\"\n```\n4. Make your changes to the code or documentation.\n\n5. Add the changes to the staging area using the following command:\n```\ngit add . \n```\n\n6. Commit the changes with a meaningful commit message using the following command:\n```\ngit commit -m \"your commit message\"\n```\n7. Push the changes to your forked repository using the following command:\n```\ngit push origin branch-name\n```\n8. Go to the GitHub website and navigate to your forked repository.\n\n9. Click the \"New pull request\" button.\n\n10. Select the branch you just pushed to and the branch you want to merge into on the original repository.\n\n11. Add a description of your changes and click the \"Create pull request\" button.\n\n12. Wait for the project maintainer to review your changes and provide feedback.\n\n13. Make any necessary changes based on feedback and repeat steps 5-12 until your changes are accepted and merged into the main project.\n\n14. Once your changes are merged, you can update your forked repository and local copy of the repository with the following commands:\n\n```\ngit fetch upstream\ngit checkout main\ngit merge upstream/main\n```\nFinally, delete the branch you created with the following command:\n```\ngit branch -d branch-name\n```\nThat's it you made it üê£‚≠ê‚≠ê\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1142578125,
          "content": "    MIT License\n\n    Copyright (c) Microsoft Corporation.\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.6357421875,
          "content": "# JARVIS\n\n\n[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2303.17580)\n[![Open in Spaces](https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/microsoft/HuggingGPT)\n\nThe mission of JARVIS is to explore artificial general intelligence (AGI) and deliver cutting-edge research to the whole community.\n\n## What's New\n\n+  [2024.01.15] We release Easytool for easier tool usage.\n   + The code and datasets are available at [EasyTool](/easytool).\n   + The paper is available at [EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction](https://arxiv.org/abs/2401.06201).\n+  [2023.11.30] We release TaskBench for evaluating task automation capability of LLMs.\n   + The code and datasets are available at [TaskBench](/taskbench).\n   + The paper is available at [TaskBench: Benchmarking Large Language Models for Task Automation](https://arxiv.org/abs/2311.18760).\n+  [2023.07.28] We are now in the process of planning evaluation and project rebuilding. We will release a new version of Jarvis in the near future.\n+  [2023.07.24] We released a light langchain version of Jarvis. See <a href=\"https://github.com/langchain-ai/langchain/tree/master/libs/experimental/langchain_experimental/autonomous_agents/hugginggpt\">here</a>.\n+  [2023.04.16] Jarvis now supports the OpenAI service on the Azure platform and the GPT-4 model.\n+  [2023.04.06] We added the Gradio demo and built the web API for `/tasks` and `/results` in `server` mode.\n   +  The Gradio demo is now hosted on Hugging Face Space. (Build with `inference_mode=hybrid` and `local_deployment=standard`)\n   +  The Web API `/tasks` and `/results` access intermediate results for `Stage #1`: task planning and `Stage #1-3`: model selection with execution results. See <a href=\"#Server\">here</a>.\n+  [2023.04.03] We added the CLI mode and provided parameters for configuring the scale of local endpoints.\n   +  You can enjoy a lightweight experience with Jarvis without deploying the models locally. See <a href=\"#Configuration\">here</a>.\n   +  Just run `python awesome_chat.py --config configs/config.lite.yaml` to experience it.\n+  [2023.04.01] We updated a version of code for building.\n\n\n### Overview\n\nLanguage serves as an interface for LLMs to connect numerous AI models for solving complicated AI tasks!\n\n<p align=\"center\">\n<img width=\"100%\" alt=\"image\" src=\"./hugginggpt/assets/intro.png\">    \n</p>\n\n\nSee our paper: [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace](http://arxiv.org/abs/2303.17580), Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu and Yueting Zhuang (the first two authors contribute equally)\n\nWe introduce a collaborative system that consists of **an LLM as the controller** and **numerous expert models as collaborative executors** (from HuggingFace Hub). The workflow of our system consists of four stages:\n+ **Task Planning**: Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks.\n+ **Model Selection**: To solve the planned tasks, ChatGPT selects expert models hosted on Hugging Face based on their descriptions.\n+ **Task Execution**: Invokes and executes each selected model, and return the results to ChatGPT.\n+ **Response Generation**: Finally, using ChatGPT to integrate the prediction of all models, and generate responses.\n\n<p align=\"center\"><img src=\"./hugginggpt/assets/overview.jpg\"></p>\n\n### System Requirements\n\n#### Default (Recommended)\n\nFor `configs/config.default.yaml`:\n\n+ Ubuntu 16.04 LTS\n+ VRAM >= 24GB\n+ RAM > 12GB (minimal), 16GB (standard), 80GB (full)\n+ Disk > 284GB \n  + 42GB for `damo-vilab/text-to-video-ms-1.7b`\n  + 126GB for `ControlNet`\n  + 66GB for `stable-diffusion-v1-5`\n  + 50GB for others\n  \n#### Minimum (Lite)\n\nFor `configs/config.lite.yaml`:\n\n+ Ubuntu 16.04 LTS\n+ Nothing else\n\nThe configuration `configs/config.lite.yaml` does not require any expert models to be downloaded and deployed locally. However, it means that Jarvis is restricted to models running stably on HuggingFace Inference Endpoints.\n\n### Quick Start\n\nFirst replace `openai.key` and `huggingface.token` in `server/configs/config.default.yaml` with **your personal OpenAI Key** and **your Hugging Face Token**, or put them in the environment variables `OPENAI_API_KEY` and `HUGGINGFACE_ACCESS_TOKEN` respectively. Then run the following commands:\n\n<span id=\"Server\"></span>\n\n#### For Server:\n\n```bash\n# setup env\ncd server\nconda create -n jarvis python=3.8\nconda activate jarvis\nconda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\npip install -r requirements.txt\n\n# download models. Make sure that `git-lfs` is installed.\ncd models\nbash download.sh # required when `inference_mode` is `local` or `hybrid`. \n\n# run server\ncd ..\npython models_server.py --config configs/config.default.yaml # required when `inference_mode` is `local` or `hybrid`\npython awesome_chat.py --config configs/config.default.yaml --mode server # for text-davinci-003\n```\n\nNow you can access Jarvis' services by the Web API. \n\n+ `/hugginggpt` --method `POST`, access the full service.\n+ `/tasks` --method `POST`, access intermediate results for Stage #1.\n+ `/results` --method `POST`, access intermediate results for Stage #1-3.\n\nFor example:\n\n```bash\n# request\ncurl --location 'http://localhost:8004/tasks' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"based on pose of /examples/d.jpg and content of /examples/e.jpg, please show me a new image\"\n        }\n    ]\n}'\n\n# response\n[{\"args\":{\"image\":\"/examples/d.jpg\"},\"dep\":[-1],\"id\":0,\"task\":\"openpose-control\"},{\"args\":{\"image\":\"/examples/e.jpg\"},\"dep\":[-1],\"id\":1,\"task\":\"image-to-text\"},{\"args\":{\"image\":\"<GENERATED>-0\",\"text\":\"<GENERATED>-1\"},\"dep\":[1,0],\"id\":2,\"task\":\"openpose-text-to-image\"}]\n```\n\n\n#### For Web:\n\nWe provide a user-friendly web page. After starting `awesome_chat.py` in a server mode, you can run the commands to communicate with Jarvis in your browser:\n \n- you need to install `nodejs` and `npm` first.\n- [ IMPORTANT ] if you are running the web client on another machine, you need set `http://{LAN_IP_of_the_server}:{port}/` to `HUGGINGGPT_BASE_URL` of `web/src/config/index.ts`.\n- if you want to use the video generation feature, you need to compile `ffmpeg` manually with H.264.\n- you can switch to ChatGPT by `double click` on the setting icon!\n\n```bash\ncd web\nnpm install\nnpm run dev\n```\n\n```bash\n# Optional: Install ffmpeg\n# This command need be executed without errors.\nLD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/ffmpeg -i input.mp4 -vcodec libx264 output.mp4\n```\n\n<span id=\"Gradio\"></span>\n\n#### For Gradio\n\nThe Gradio demo is now hosted on Hugging Face Space. You can also run the following commands to start the demo locally:\n\n```bash\npython models_server.py --config configs/config.gradio.yaml\npython run_gradio_demo.py --config configs/config.gradio.yaml\n\n# or run the HF Space as a Docker image (Build with `inference_mode=hibrid` and `local_deployment=standard`)\ndocker run -it -p 7860:7860 --platform=linux/amd64 registry.hf.space/microsoft-hugginggpt:latest python app.py\n```\n\n#### For CLI:\n\nYou can also run Jarvis more easily in CLI mode:\n\n```bash\ncd server\npython awesome_chat.py --config configs/config.default.yaml --mode cli\n```\n\nExamples of CLI mode:\n\n```\nWelcome to Jarvis! A collaborative system that consists of an LLM as the controller and numerous expert models as collaborative executors. Jarvis can plan tasks, schedule Hugging Face models, generate friendly responses based on your requests, and help you with many things. Please enter your request (`exit` to exit).\n[ User ]: Given a collection of image A: /examples/a.jpg, B: /examples/b.jpg, C: /examples/c.jpg, please tell me how many zebras in these picture?\n[ Jarvis ]: Based on your request, I have used several models to process the images you provided. For the image A: /examples/a.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text \"a cat sitting on a window sill looking out\". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box '/images/a3dc.jpg', which contains two objects with labels 'potted plant' and 'cat'. For the image B: /examples/b.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text \"two zebras standing in a field of dry grass\". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box '/images/f5df.jpg', which contains three objects with labels 'zebra'. For the image C: /examples/c.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text \"a zebra standing in a field of tall grass\". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box '/images/69c7.jpg', which contains one object with label 'zebra'. Finally, I used the canny-control model canny-control to generate the image '/images/7e18.png' based on the image /examples/f.jpg. Therefore, there are four zebras in these pictures. Is there anything else I can help you with?\n[ User ]: Please answer all the named entities in the sentence: Iron Man is a superhero appearing in American comic books published by Marvel Comics. The character was co-created by writer and editor Stan Lee, developed by scripter Larry Lieber, and designed by artists Don Heck and Jack Kirby.\n[ Jarvis ]: Sure, I understand your request. Based on the inference results of the models, I will answer your request step by step. Firstly, I used the token-classification model dslim/bert-base-NER to recognize the named entities in the sentence. The inference result is that the named entities are Iron Man, Stan Lee, Larry Lieber, Don Heck and Jack Kirby. Then, I used the question-answering model deepset/roberta-base-squad2 to answer your request. The inference result is that there is no output since the context cannot be empty. Therefore, I cannot make it. I hope this answer your request. Is there anything else I can help you with?\n[ User ]: Please generate a canny image based on /examples/f.jpg\n[ Jarvis ]: Sure. I have generated a canny image based on /examples/f.jpg. To do this, I first used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text description of the image, which is \"a herd of giraffes and zebras grazing in a field\". Then I used the canny-control model to generate the canny image of the original image, which is stored in the path \"/images/1ea2.png\". Finally, I used the canny-text-to-image model lllyasviel/sd-controlnet-canny to generate the canny image based on the text description and the canny image, which is stored in the path \"/images/ba66.png\". Do you need anything else?\n```\n\n\n### Configuration\n\nThe server-side configuration file is `server/configs/config.default.yaml`, and some parameters are presented as follows:\n\n+ `model`: LLM, currently supports `text-davinci-003`. We are working on integrating more open-source LLMs.\n+ `inference_mode`: mode of inference endpoints\n  + `local`: only use the local inference endpoints\n  + `huggingface`: only use the Hugging Face Inference Endpoints **(free of local inference endpoints)**\n  + `hybrid`: both of `local` and `huggingface`\n+ `local_deployment`: scale of locally deployed models, works under `local` or `hybrid` inference mode:\n  +  `minimal` (RAM>12GB, ControlNet only)\n  +  `standard` (RAM>16GB, ControlNet + Standard Pipelines)\n  +  `full` (RAM>42GB, All registered models)\n\nOn a personal laptop, we recommend the configuration of `inference_mode: hybrid `and `local_deployment: minimal`. But the available models under this setting may be limited due to the instability of remote Hugging Face Inference Endpoints.\n\n### NVIDIA Jetson Embedded Device Support\nA [Dockerfile](./Dockerfile.jetson) is included that provides experimental support for [NVIDIA Jetson embedded devices](https://developer.nvidia.com/embedded-computing).  This image provides accelerated ffmpeg, pytorch, torchaudio, and torchvision dependencies.  To build the docker image, [ensure that the default docker runtime is set to 'nvidia'](https://github.com/NVIDIA/nvidia-docker/wiki/Advanced-topics#default-runtime).  A pre-built image is provided at https://hub.docker.com/r/toolboc/nv-jarvis.\n\n```bash\n#Build the docker image\ndocker build --pull --rm -f \"Dockerfile.jetson\" -t toolboc/nv-jarvis:r35.2.1 \n```\n\nDue to to memory requirements, JARVIS is required to run on Jetson AGX Orin family devices (64G on-board RAM device preferred) with config options set to:\n* `inference_mode: local` \n* `local_deployment: standard`  \n\nModels and configs are recommended to be provided through a volume mount from the host to the container as shown in the `docker run` step below.  It is possible to uncomment the `# Download local models` section of the [Dockerfile](./Dockerfile.jetson) to build a container with models included.\n\n#### Start the model server, awesomechat, and web app on Jetson Orin AGX\n\n```bash\n# run the container which will automatically start the model server\ndocker run --name jarvis --net=host --gpus all -v ~/jarvis/configs:/app/server/configs -v ~/src/JARVIS/server/models:/app/server/models toolboc/nv-jarvis:r35.2.1\n\n# (wait for model server to complete initialization)\n\n# start awesome_chat.py \ndocker exec jarvis python3 awesome_chat.py --config configs/config.default.yaml --mode server\n\n#start the web application (application will be acessible at http://localhost:9999)\ndocker exec jarvis npm run dev --prefix=/app/web\n```\n\n### Screenshots\n\n<p align=\"center\"><img src=\"./hugginggpt/assets/screenshot_q.jpg\"><img src=\"./hugginggpt/assets/screenshot_a.jpg\"></p>\n\n\n\n\n## Citation\nIf you find this work useful in your method, you can cite the paper as below:\n\n    @inproceedings{shen2023hugginggpt,\n      author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},\n      booktitle = {Advances in Neural Information Processing Systems},\n      title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},\n      year = {2023}\n    }\n\n>\n    @article{shen2023taskbench,\n      title   = {TaskBench: Benchmarking Large Language Models for Task Automation},\n      author  = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},\n      journal = {arXiv preprint arXiv:2311.18760},\n      year    = {2023}\n    }\n\n>\n    @article{yuan2024easytool,\n      title   = {EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction},\n      author  = {Siyu Yuan and Kaitao Song and Jiangjie Chen and Xu Tan and Yongliang Shen and Ren Kan and Dongsheng Li and Deqing Yang},\n      journal = {arXiv preprint arXiv:2401.06201},\n      year    = {2024}\n    }\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.6923828125,
          "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.8 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/opensource/security/definition), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/opensource/security/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/opensource/security/pgpkey).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://aka.ms/opensource/security/msrc). \n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/opensource/security/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/opensource/security/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->\n"
        },
        {
          "name": "SUPPORT.md",
          "type": "blob",
          "size": 1.21484375,
          "content": "# TODO: The maintainer of this repo has not yet edited this file\r\n\r\n**REPO OWNER**: Do you want Customer Service & Support (CSS) support for this product/project?\r\n\r\n- **No CSS support:** Fill out this template with information about how to file issues and get help.\r\n- **Yes CSS support:** Fill out an intake form at [aka.ms/onboardsupport](https://aka.ms/onboardsupport). CSS will work with/help you to determine next steps.\r\n- **Not sure?** Fill out an intake as though the answer were \"Yes\". CSS will help you decide.\r\n\r\n*Then remove this first heading from this SUPPORT.MD file before publishing your repo.*\r\n\r\n# Support\r\n\r\n## How to file issues and get help  \r\n\r\nThis project uses GitHub Issues to track bugs and feature requests. Please search the existing \r\nissues before filing new issues to avoid duplicates.  For new issues, file your bug or \r\nfeature request as a new Issue.\r\n\r\nFor help and questions about using this project, please **REPO MAINTAINER: INSERT INSTRUCTIONS HERE \r\nFOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER\r\nCHANNEL. WHERE WILL YOU HELP PEOPLE?**.\r\n\r\n## Microsoft Support Policy  \r\n\r\nSupport for this **PROJECT or PRODUCT** is limited to the resources listed above.\r\n"
        },
        {
          "name": "easytool",
          "type": "tree",
          "content": null
        },
        {
          "name": "hugginggpt",
          "type": "tree",
          "content": null
        },
        {
          "name": "taskbench",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}