{
  "metadata": {
    "timestamp": 1736561237375,
    "page": 222,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "nltk/nltk",
      "stars": 13753,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.01171875,
          "content": "* text=auto\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.7265625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*.class\n*.jar\n\n*.egg\nbuild/\ndist/\nnltk.egg-info/\nweb/_build\n\n# Test artifacts and coverage reports\n*.tox\n*.errs\n.hypothesis\n.noseids\n.coverage*\nnltk/test/*.html\nnltk/test/tweets*\nmodel.crf.tagger\nbrown.embedding\npylintoutput\nnosetests.xml\nnosetests_scrubbed.xml\ncoverage.xml\n\n# editor temporary files\n*.*.sw[op]\n.idea\n*~\n\n# git mergetools backups\n*.orig\n\n# emacs backups\n*#\n\n# spell-check backups\n*.bak\n\n# automatically built files for website\nweb/api/*.rst\nweb/howto/*.rst\n\n# iPython notebooks\n.ipynb_checkpoints\n\n# pyenv files\n.python-version\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n#vscode\n.vscode/\n\n# Direnv\n.envrc\n\n# Mypy\n.mypy_cache\n\n# macOS\n.DS_Store\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.775390625,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n      - id: fix-byte-order-marker\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: requirements-txt-fixer\n      - id: check-yaml\n  - repo: https://github.com/asottile/pyupgrade\n    rev: v3.16.0\n    hooks:\n    - id: pyupgrade\n      args: [\"--py38-plus\"]\n  - repo: https://github.com/ambv/black\n    rev: 24.4.2\n    hooks:\n    - id: black\n  - repo: local\n    hooks:\n      - id: isort\n        name: isort\n        entry: isort\n        require_serial: true\n        language: python\n        language_version: python3\n        types_or: [cython, pyi, python]\n        args: ['--filter-files']\n        minimum_pre_commit_version: '2.9.2'\n        additional_dependencies: ['isort==5.13.2']\n"
        },
        {
          "name": "AUTHORS.md",
          "type": "blob",
          "size": 7.443359375,
          "content": "# Natural Language Toolkit (NLTK) Authors\n\n## Original Authors\n\n- Steven Bird <stevenbird1@gmail.com>\n- Edward Loper <edloper@gmail.com>\n- Ewan Klein <ewan@inf.ed.ac.uk>\n\n## Contributors\n\n- Tom Aarsen\n- Rami Al-Rfou'\n- Mark Amery\n- Greg Aumann\n- Ivan Barria\n- Ingolf Becker\n- Yonatan Becker\n- Paul Bedaride\n- Steven Bethard\n- Robert Berwick\n- Dan Blanchard\n- Nathan Bodenstab\n- Alexander Böhm\n- Francis Bond\n- Paul Bone\n- Jordan Boyd-Graber\n- Daniel Blanchard\n- Phil Blunsom\n- Lars Buitinck\n- Cristian Capdevila\n- Steve Cassidy\n- Chen-Fu Chiang\n- Dmitry Chichkov\n- Jinyoung Choi\n- Andrew Clausen\n- Lucas Champollion\n- Graham Christensen\n- Trevor Cohn\n- David Coles\n- Tom Conroy <https://github.com/tconroy>\n- Claude Coulombe\n- Lucas Cooper\n- Robin Cooper\n- Chris Crowner\n- James Curran\n- Arthur Darcet\n- Dariel Dato-on\n- Selina Dennis\n- Leon Derczynski\n- Alexis Dimitriadis\n- Nikhil Dinesh\n- Liang Dong\n- David Doukhan\n- Rebecca Dridan\n- Pablo Duboue\n- Long Duong\n- Christian Federmann\n- Campion Fellin\n- Michelle Fullwood\n- Dan Garrette\n- Maciej Gawinecki\n- Jean Mark Gawron\n- Sumukh Ghodke\n- Yoav Goldberg\n- Michael Wayne Goodman\n- Dougal Graham\n- Brent Gray\n- Simon Greenhill\n- Clark Grubb\n- Eduardo Pereira Habkost\n- Masato Hagiwara\n- Lauri Hallila\n- Michael Hansen\n- Yurie Hara\n- Will Hardy\n- Tyler Hartley\n- Peter Hawkins\n- Saimadhav Heblikar\n- Fredrik Hedman\n- Helder\n- Michael Heilman\n- Ofer Helman\n- Christopher Hench\n- Bruce Hill\n- Amy Holland\n- Kristy Hollingshead\n- Marcus Huderle\n- Baden Hughes\n- Nancy Ide\n- Rebecca Ingram\n- Edward Ivanovic\n- Thomas Jakobsen\n- Nick Johnson\n- Eric Kafe\n- Piotr Kasprzyk\n- Angelos Katharopoulos\n- Sudharshan Kaushik\n- Chris Koenig\n- Mikhail Korobov\n- Denis Krusko\n- Ilia Kurenkov\n- Stefano Lattarini\n- Pierre-François Laquerre\n- Stefano Lattarini\n- Haejoong Lee\n- Jackson Lee\n- Max Leonov\n- Chris Liechti\n- Hyuckin David Lim\n- Tom Lippincott\n- Peter Ljunglöf\n- Alex Louden\n- David Lukeš\n- Joseph Lynch\n- Nitin Madnani\n- Felipe Madrigal\n- Bjørn Mæland\n- Dean Malmgren\n- Christopher Maloof\n- Rob Malouf\n- Iker Manterola\n- Carl de Marcken\n- Mitch Marcus\n- Torsten Marek\n- Robert Marshall\n- Marius Mather\n- Duncan McGreggor\n- David McClosky\n- Xinfan Meng\n- Dmitrijs Milajevs\n- Matt Miller\n- Margaret Mitchell\n- Tomonori Nagano\n- Jason Narad\n- Shari A’aidil Nasruddin\n- Lance Nathan\n- Morten Neergaard\n- David Nemeskey\n- Eric Nichols\n- Joel Nothman\n- Alireza Nourian\n- Alexander Oleynikov\n- Pierpaolo Pantone\n- Ted Pedersen\n- Jacob Perkins\n- Alberto Planas\n- Ondrej Platek\n- Alessandro Presta\n- Qi Liu\n- Martin Thorsen Ranang\n- Michael Recachinas\n- Brandon Rhodes\n- Joshua Ritterman\n- Will Roberts\n- Stuart Robinson\n- Carlos Rodriguez\n- Lorenzo Rubio\n- Alex Rudnick\n- Jussi Salmela\n- Geoffrey Sampson\n- Kepa Sarasola\n- Kevin Scannell\n- Nathan Schneider\n- Rico Sennrich\n- Thomas Skardal\n- Eric Smith\n- Lynn Soe\n- Rob Speer\n- Peter Spiller\n- Richard Sproat\n- Ceri Stagg\n- Peter Stahl\n- Oliver Steele\n- Thomas Stieglmaier\n- Jan Strunk\n- Liling Tan\n- Claire Taylor\n- Louis Tiao\n- Steven Tomcavage\n- Tiago Tresoldi\n- Marcus Uneson\n- Yu Usami\n- Petro Verkhogliad\n- Peter Wang\n- Zhe Wang\n- Charlotte Wilson\n- Chuck Wooters\n- Steven Xu\n- Beracah Yankama\n- Lei Ye (叶磊)\n- Patrick Ye\n- Geraldine Sim Wei Ying\n- Jason Yoder\n- Thomas Zieglier\n- 0ssifrage\n- ducki13\n- kiwipi\n- lade\n- isnowfy\n- onesandzeros\n- pquentin\n- wvanlint\n- Álvaro Justen <https://github.com/turicas>\n- bjut-hz\n- Sergio Oller\n- Izam Mohammed <https://github.com/izam-mohammed>\n- Will Monroe\n- Elijah Rippeth\n- Emil Manukyan\n- Casper Lehmann-Strøm\n- Andrew Giel\n- Tanin Na Nakorn\n- Linghao Zhang\n- Colin Carroll\n- Heguang Miao\n- Hannah Aizenman (story645)\n- George Berry\n- Adam Nelson\n- J Richard Snape\n- Alex Constantin <alex@keyworder.ch>\n- Tsolak Ghukasyan\n- Prasasto Adi\n- Safwan Kamarrudin\n- Arthur Tilley\n- Vilhjalmur Thorsteinsson\n- Jaehoon Hwang <https://github.com/jaehoonhwang>\n- Chintan Shah <https://github.com/chintanshah24>\n- sbagan\n- Zicheng Xu\n- Albert Au Yeung <https://github.com/albertauyeung>\n- Shenjian Zhao\n- Deng Wang <https://github.com/lmatt-bit>\n- Ali Abdullah\n- Stoytcho Stoytchev\n- Lakhdar Benzahia\n- Kheireddine Abainia <https://github.com/xprogramer>\n- Yibin Lin <https://github.com/yibinlin>\n- Artiem Krinitsyn\n- Björn Mattsson\n- Oleg Chislov\n- Pavan Gururaj Joshi <https://github.com/PavanGJ>\n- Ethan Hill <https://github.com/hill1303>\n- Vivek Lakshmanan\n- Somnath Rakshit <https://github.com/somnathrakshit>\n- Anlan Du\n- Pulkit Maloo <https://github.com/pulkitmaloo>\n- Brandon M. Burroughs <https://github.com/brandonmburroughs>\n- John Stewart <https://github.com/free-variation>\n- Iaroslav Tymchenko <https://github.com/myproblemchild>\n- Aleš Tamchyna\n- Tim Gianitsos <https://github.com/timgianitsos>\n- Philippe Partarrieu <https://github.com/ppartarr>\n- Andrew Owen Martin\n- Adrian Ellis <https://github.com/adrianjellis>\n- Nat Quayle Nelson <https://github.com/nqnstudios>\n- Yanpeng Zhao <https://github.com/zhaoyanpeng>\n- Matan Rak <https://github.com/matanrak>\n- Nick Ulle <https://github.com/nick-ulle>\n- Uday Krishna <https://github.com/udaykrishna>\n- Osman Zubair <https://github.com/okz12>\n- Viresh Gupta <https://github.com/virresh>\n- Ondřej Cífka <https://github.com/cifkao>\n- Iris X. Zhou <https://github.com/irisxzhou>\n- Devashish Lal <https://github.com/BLaZeKiLL>\n- Gerhard Kremer <https://github.com/GerhardKa>\n- Nicolas Darr <https://github.com/ndarr>\n- Hervé Nicol <https://github.com/hervenicol>\n- Alexandre H. T. Dias <https://github.com/alexandredias3d>\n- Daksh Shah <https://github.com/Daksh>\n- Jacob Weightman <https://github.com/jacobdweightman>\n- Bonifacio de Oliveira <https://github.com/Bonifacio2>\n- Armins Bagrats Stepanjans <https://github.com/ab-10>\n- Vassilis Palassopoulos <https://github.com/palasso>\n- Ram Rachum <https://github.com/cool-RR>\n- Or Sharir <https://github.com/orsharir>\n- Denali Molitor <https://github.com/dmmolitor>\n- Jacob Moorman <https://github.com/jdmoorman>\n- Cory Nezin <https://github.com/corynezin>\n- Matt Chaput\n- Danny Sepler <https://github.com/dannysepler>\n- Akshita Bhagia <https://github.com/AkshitaB>\n- Pratap Yadav <https://github.com/prtpydv>\n- Hiroki Teranishi <https://github.com/chantera>\n- Ruben Cartuyvels <https://github.com/rubencart>\n- Dalton Pearson <https://github.com/daltonpearson>\n- Robby Horvath <https://github.com/robbyhorvath>\n- Gavish Poddar <https://github.com/gavishpoddar>\n- Saibo Geng <https://github.com/Saibo-creator>\n- Ahmet Yildirim <https://github.com/RnDevelover>\n- Yuta Nakamura <https://github.com/yutanakamura-tky>\n- Adam Hawley <https://github.com/adamjhawley>\n- Panagiotis Simakis <https://github.com/sp1thas>\n- Richard Wang <https://github.com/richarddwang>\n- Alexandre Perez-Lebel <https://github.com/aperezlebel>\n- Fernando Carranza <https://github.com/fernandocar86>\n- Martin Kondratzky <https://github.com/martinkondra>\n- Heungson Lee <https://github.com/heungson>\n- M.K. Pawelkiewicz <https://github.com/hamiltonianflow>\n- Steven Thomas Smith <https://github.com/essandess>\n- Jan Lennartz <https://github.com/Madnex>\n- Tim Sockel <https://github.com/TiMauzi>\n- Akihiro Yamazaki <https://github.com/zakkie>\n- Ron Urbach <https://github.com/sharpblade4>\n- Vivek Kalyan <https://github.com/vivekkalyan>\n- Tom Strange https://github.com/strangetom\n\n## Others whose work we've taken and included in NLTK, but who didn't directly contribute it:\n\n### Contributors to the Porter Stemmer\n\n- Martin Porter\n- Vivake Gupta\n- Barry Wilkins\n- Hiranmay Ghosh\n- Chris Emerson\n\n### Authors of snowball arabic stemmer algorithm\n\n- Assem Chelli\n- Abdelkrim Aries\n- Lakhdar Benzahia\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 1.1083984375,
          "content": "cff-version: 1.2.0\ntitle: >-\n  Natural Language ToolKit (NLTK)\nmessage: >-\n  Please cite this software using the metadata from\n  'preferred-citation'.\ntype: software\nauthors:\n  - name: \"NLTK Team\"\n    email: \"nltk.team@gmail.com\"\nrepository-code: \"https://github.com/nltk/nltk\"\nurl: \"https://www.nltk.org\"\nlicense: Apache-2.0\nkeywords:\n  - \"NLP\"\n  - \"CL\"\n  - \"natural language processing\"\n  - \"computational linguistics\"\n  - \"parsing\"\n  - \"tagging\"\n  - \"tokenizing\"\n  - \"syntax\"\n  - \"linguistics\"\n  - \"language\"\n  - \"natural language\"\n  - \"text analytics\"\npreferred-citation:\n  title: >-\n    Natural Language Processing with Python: Analyzing\n    Text with the Natural Language Toolkit\n  type: book\n  authors:\n    - given-names: Steven\n      family-names: Bird\n      orcid: https://orcid.org/0000-0003-3782-7733\n    - given-names: Ewan\n      family-names: Klein\n      orcid: https://orcid.org/0000-0002-0520-8447\n    - given-names: Edward\n      family-names: Loper\n  year: 2009\n  month: 6\n  url: \"https://www.nltk.org/book/\"\n  isbn: \"9780596516499\"\n  publisher:\n    name: \"O'Reilly Media, Inc.\"\n    website: \"https://www.oreilly.com/\"\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 9.115234375,
          "content": "# Contributing to NLTK\n\nHi! Thanks for your interest in contributing to [NLTK](https://www.nltk.org/).\n:-) You'll be joining a [long list of contributors](https://github.com/nltk/nltk/blob/develop/AUTHORS.md).\nIn this document, we'll try to summarize everything that you need to know to\ndo a good job.\n\n\n## Code and Issues\n\nWe use [GitHub](https://www.github.com/) to host our code repositories and\nissues. The [NLTK organization on GitHub](https://github.com/nltk) has many\nrepositories, so we can manage better the issues and development. The most\nimportant are:\n\n- [nltk/nltk](https://github.com/nltk/nltk/), the main repository with code\n  related to the library;\n- [nltk/nltk_data](https://github.com/nltk/nltk_data), repository with data\n  related to corpora, taggers and other useful data that are not shipped by\n  default with the library, which can be downloaded by `nltk.downloader`;\n- [nltk/nltk.github.com](https://github.com/nltk/nltk.github.com), NLTK website\n  with information about the library, documentation, link for downloading NLTK\n  Book etc.;\n- [nltk/nltk_book](https://github.com/nltk/nltk_book), source code for the NLTK\n  Book.\n\n## Development priorities\n\nNLTK consists of the functionality that the Python/NLP community is motivated to contribute.\nSome priority areas for development are listed in the [NLTK Wiki](https://github.com/nltk/nltk/wiki#development).\n\n## Git and our Branching model\n\n### Git\n\nWe use [Git](https://git-scm.com/) as our [version control\nsystem](https://en.wikipedia.org/wiki/Revision_control), so the best way to\ncontribute is to learn how to use it and put your changes on a Git repository.\nThere's plenty of documentation about Git -- you can start with the [Pro Git\nbook](https://git-scm.com/book/).\n\n\n### Setting up a Development Environment\n\nTo set up your local development environment for contributing to the main\nrepository [nltk/nltk](https://github.com/nltk/nltk/):\n\n- Fork the [nltk/nltk](https://github.com/nltk/nltk/) repository on GitHub\n  to your account;\n- Clone your forked repository locally\n  (`git clone https://github.com/<your-github-username>/nltk.git`);\n- Run `cd nltk` to get to the root directory of the `nltk` code base;\n- Install the dependencies (`pip install -r pip-req.txt`);\n- Install the [pre-commit](https://pre-commit.com) hooks: (`pre-commit install`)\n- Download the datasets for running tests\n  (`python -m nltk.downloader all`);\n- Create a remote link from your local repository to the\n  upstream `nltk/nltk` on GitHub\n  (`git remote add upstream https://github.com/nltk/nltk.git`) --\n  you will need to use this `upstream` link when updating your local repository\n  with all the latest contributions.\n\n### GitHub Pull requests\n\nWe use the famous\n[gitflow](https://nvie.com/posts/a-successful-git-branching-model/) to manage our\nbranches.\n\nSummary of our git branching model:\n- Go to the `develop` branch (`git checkout develop`);\n- Get all the latest work from the upstream `nltk/nltk` repository\n  (`git pull upstream develop`);\n- Create a new branch off of `develop` with a descriptive name (for example:\n  `feature/portuguese-sentiment-analysis`, `hotfix/bug-on-downloader`). You can\n  do it by switching to the `develop` branch (`git checkout develop`) and then\n  creating a new branch (`git checkout -b name-of-the-new-branch`);\n- Do many small commits on that branch locally (`git add files-changed`,\n  `git commit -m \"Add some change\"`);\n- Run the tests to make sure nothing breaks\n  (`tox -e py312` if you are on Python 3.12);\n- Add your name to the `AUTHORS.md` file as a contributor;\n- Push to your fork on GitHub (with the name as your local branch:\n  `git push origin branch-name`);\n- Create a pull request using the GitHub Web interface (asking us to pull the\n  changes from your new branch and add to them our `develop` branch);\n- Wait for comments.\n\n\n### Tips\n\n- Write [helpful commit\n  messages](https://robots.thoughtbot.com/5-useful-tips-for-a-better-commit-message).\n- Anything in the `develop` branch should be deployable (no failing tests).\n- Never use `git add .`: it can add unwanted files;\n- Avoid using `git commit -a` unless you know what you're doing;\n- Check every change with `git diff` before adding them to the index (stage\n  area) and with `git diff --cached` before committing;\n- Make sure you add your name to our [list of contributors](https://github.com/nltk/nltk/blob/develop/AUTHORS.md);\n- If you have push access to the main repository, please do not commit directly\n  to `develop`: your access should be used only to accept pull requests; if you\n  want to make a new feature, you should use the same process as other\n  developers so your code will be reviewed.\n- See [RELEASE-HOWTO.txt](RELEASE-HOWTO.txt) to see everything you\n  need before creating a new NLTK release.\n\n\n## Code Guidelines\n\n- Use [PEP8](https://www.python.org/dev/peps/pep-0008/);\n- Write tests for your new features (please see \"Tests\" topic below);\n- Always remember that [commented code is dead\n  code](https://blog.codinghorror.com/coding-without-comments/);\n- Name identifiers (variables, classes, functions, module names) with readable\n  names (`x` is always wrong);\n- When manipulating strings, we prefer either [f-string\n  formatting](https://docs.python.org/3/tutorial/inputoutput.html#formatted-string-literals)\n  (f`'{a} = {b}'`) or [new-style\n  formatting](https://docs.python.org/library/string.html#format-string-syntax)\n  (`'{} = {}'.format(a, b)`), instead of the old-style formatting (`'%s = %s' % (a, b)`);\n- All `#TODO` comments should be turned into issues (use our\n  [GitHub issue system](https://github.com/nltk/nltk/issues));\n- Run all tests before pushing (just execute `tox`) so you will know if your\n  changes broke something;\n\nSee also our [developer's\nguide](https://github.com/nltk/nltk/wiki/Developers-Guide).\n\n\n## Tests\n\nYou should write tests for every feature you add or bug you solve in the code.\nHaving automated tests for every line of our code lets us make big changes\nwithout worries: there will always be tests to verify if the changes introduced\nbugs or lack of features. If we don't have tests we will be blind and every\nchange will come with some fear of possibly breaking something.\n\nFor a better design of your code, we recommend using a technique called\n[test-driven development](https://en.wikipedia.org/wiki/Test-driven_development),\nwhere you write your tests **before** writing the actual code that implements\nthe desired feature.\n\nYou can use `pytest` to run your tests, no matter which type of test it is:\n\n```\ncd nltk/test\npytest util.doctest  # doctest\npytest unit/translate/test_nist.py  # unittest\npytest  # all tests\n```\n\n\n## Continuous Integration\n\n**Deprecated:** NLTK uses [Cloudbees](https://nltk.ci.cloudbees.com/) for continuous integration.\n\n**Deprecated:** NLTK uses [Travis](https://travis-ci.org/nltk/nltk/) for continuous integration.\n\nNLTK uses [GitHub Actions](https://github.com/nltk/nltk/actions) for continuous integration. See [here](https://docs.github.com/en/actions) for GitHub's documentation.\n\nThe [`.github/workflows/ci.yaml`](https://github.com/nltk/nltk/blob/develop/.github/workflows/ci.yaml) file configures the CI:\n\n - `on:` section\n   - ensures that this CI is run on code pushes, pull request, or through the GitHub website via a button.\n\n - The `cache_nltk_data` job\n   - performs these steps:\n     - Downloads the `nltk` source code.\n     - Load `nltk_data` via cache.\n       - Otherwise, download all the data packages through `nltk.download('all')`.\n\n  - The `test` job\n    - tests against supported Python versions (`3.8`, `3.9`, `3.10`, `3.11`, `3.12`).\n    - tests on `ubuntu-latest` and `macos-latest`.\n    - relies on the `cache_nltk_data` job to ensure that `nltk_data` is available.\n    - performs these steps:\n      - Downloads the `nltk` source code.\n      - Set up Python using whatever version is being checked in the current execution.\n      - Load module dependencies via cache.\n        - Otherwise, install dependencies via `pip install -U -r requirements-ci.txt`.\n      - Load cached `nltk_data` loaded via `cache_nltk_data`.\n      - Run `pytest --numprocesses auto -rsx nltk/test`.\n\n - The `pre-commit` job\n   - performs these steps:\n     - Downloads the `nltk` source code.\n     - Runs pre-commit on all files in the repository. (Similar to `pre-commit run --all-files`)\n     - Fails if any hooks performed a change.\n\n#### To test with `tox` locally\n\nFirst setup a new virtual environment, see https://docs.python-guide.org/dev/virtualenvs/\nThen run `tox -e py312`.\n\nFor example, using `pipenv`:\n\n```\ngit clone https://github.com/nltk/nltk.git\ncd nltk\npipenv install -r pip-req.txt\npipenv install tox\ntox -e py312\n```\n\n\n# Discussion\n\nWe have three mail lists on Google Groups:\n\n- [nltk][nltk-announce], for announcements only;\n- [nltk-users][nltk-users], for general discussion and user questions;\n- [nltk-dev][nltk-dev], for people interested in NLTK development.\n\nPlease feel free to contact us through the [nltk-dev][nltk-dev] mail list if\nyou have any questions or suggestions. Every contribution is very welcome!\n\nHappy hacking! (;\n\n[nltk-announce]: https://groups.google.com/forum/#!forum/nltk\n[nltk-dev]: https://groups.google.com/forum/#!forum/nltk-dev\n[nltk-users]: https://groups.google.com/forum/#!forum/nltk-users\n"
        },
        {
          "name": "ChangeLog",
          "type": "blob",
          "size": 60.5517578125,
          "content": "Version 3.9.1 2024-08-19\n* Fixed bug that prevented wordnet from loading\n\nVersion 3.9 2024-08-18\n* Fix security vulnerability CVE-2024-39705 (breaking change)\n* Replace pickled models (punkt, chunker, taggers) by new pickle-free \"_tab\" packages\n* No longer sort Wordnet synsets and relations (sort in calling function when required)\n* Only strip the last suffix in Wordnet Morphy, thus restricting synsets() results\n* Add Python 3.12 support\n* Many other minor fixes\n\nThanks to the following contributors to 3.8.2:\nTom Aarsen, Cat Lee Ball, Veralara Bernhard, Carlos Brandt, Konstantin Chernyshev, Michael Higgins,\nEric Kafe, Vivek Kalyan, David Lukes, Rob Malouf, purificant, Alex Rudnick, Liling Tan, Akihiro Yamazaki.\n\nVersion 3.8.1 2023-01-02\n\n* Resolve RCE vulnerability in localhost WordNet Browser (#3100)\n* Remove unused tool scripts (#3099)\n* Resolve XSS vulnerability in localhost WordNet Browser (#3096)\n* Add Python 3.11 support (#3090)\n\nThanks to the following contributors to 3.8.1:\nFrancis Bond, John Vandenberg, Tom Aarsen\n\nVersion 3.8 2022-12-12\n\n* Refactor dispersion plot (#3082)\n* Provide type hints for LazyCorpusLoader variables (#3081)\n* Throw warning when LanguageModel is initialized with incorrect vocabulary (#3080)\n* Fix WordNet's all_synsets() function (#3078)\n* Resolve TreebankWordDetokenizer inconsistency with end-of-string contractions (#3070)\n* Support both iso639-3 codes and BCP-47 language tags (#3060)\n* Avoid DeprecationWarning in Regexp tokenizer (#3055)\n* Fix many doctests, add doctests to CI (#3054, #3050, #3048)\n* Fix bool field not being read in VerbNet (#3044)\n* Greatly improve time efficiency of SyllableTokenizer when tokenizing numbers (#3042)\n* Fix encodings of Polish udhr corpus reader (#3038)\n* Allow TweetTokenizer to tokenize emoji flag sequences (#3034)\n* Prevent LazyModule from increasing the size of nltk.__dict__ (#3033)\n* Fix CoreNLPServer non-default port issue (#3031)\n* Add \"acion\" suffix to the Spanish SnowballStemmer (#3030)\n* Allow loading WordNet without OMW (#3026)\n* Use input() in nltk.chat.chatbot() for Jupyter support (#3022)\n* Fix edit_distance_align() in distance.py (#3017)\n* Tackle performance and accuracy regression of sentence tokenizer since NLTK 3.6.6 (#3014)\n* Add the Iota operator to semantic logic (#3010)\n* Resolve critical errors in WordNet app (#3008)\n* Resolve critical error in CHILDES Corpus (#2998)\n* Make WordNet information_content() accept adjective satellites (#2995)\n* Add \"strict=True\" parameter to CoreNLP (#2993, #3043)\n* Resolve issue with WordNet's synset_from_sense_key (#2988)\n* Handle WordNet synsets that were lost in mapping (#2985)\n* Resolve TypeError in Boxer (#2979)\n* Add function to retrieve WordNet synonyms (#2978)\n* Warn about nonexistent OMW offsets instead of raising an error (#2974)\n* Fix missing ic argument in res, jcn and lin similarity functions of WordNet (#2970)\n* Add support for the extended OMW (#2946)\n* Fix LC cutoff policy of text tiling (#2936)\n* Optimize ConditionalFreqDist.__add__ performance (#2939)\n* Add Markdown corpus reader (#2902)\n\nThanks to the following contributors to 3.8:\nAlexandre Perez-Lebel, David Lukes, Eric Kafe, Fernando Carranza, Heungson Lee,\nHoyeol Kim, James Huang, Jelle Zijlstra, Louis-Justin Tallot, M.K. Pawelkiewicz,\nJan Lennartz, Malinda Dilhara, Martin Kondratzky, Rob Malouf, Saud Kadiri,\nSiddhesh Mhadnak, Stephan Hasler, Steve Smith, Tom Aarsen, Tyler Sheaffer,\nYue Zhao, cestwc, elespike, purificant, richardyy1188\n\nVersion 3.7 2022-02-09\n\n* Improve and update the NLTK team page on nltk.org (#2855, #2941)\n* Drop support for Python 3.6, support Python 3.10 (#2920)\n\nThanks to the following contributors to 3.7:\nTom Aarsen\n\nVersion 3.6.7 2021-12-28\n\n* Resolve IndexError in `sent_tokenize` and `word_tokenize` (#2922)\n\nThanks to the following contributors to 3.6.7:\nTom Aarsen\n\nVersion 3.6.6 2021-12-21\n\n* Refactor `gensim.doctest` to work for gensim 4.0.0 and up (#2914)\n* Add Precision, Recall, F-measure, Confusion Matrix to Taggers (#2862)\n* Added warnings if .zip files exist without any corresponding .csv files. (#2908)\n* Fix `FileNotFoundError` when the `download_dir` is a non-existing nested folder (#2910)\n* Rename omw to omw-1.4 (#2907)\n* Resolve ReDoS opportunity by fixing incorrectly specified regex (#2906)\n* Support OMW 1.4 (#2899)\n* Deprecate Tree get and set node methods (#2900)\n* Fix broken inaugural test case (#2903)\n* Use Multilingual Wordnet Data from OMW with newer Wordnet versions (#2889)\n* Keep NLTKs \"tokenize\" module working with pathlib (#2896)\n* Make prettyprinter to be more readable (#2893)\n* Update links to the nltk book (#2895)\n* Add `CITATION.cff` to nltk (#2880)\n* Resolve serious ReDoS in PunktSentenceTokenizer (#2869)\n* Delete old CI config files (#2881)\n* Improve Tokenize documentation + add TokenizerI as superclass for TweetTokenizer (#2878)\n* Fix expected value for BLEU score doctest after changes from #2572\n* Add multi Bleu functionality and tests (#2793)\n* Deprecate 'return_str' parameter in NLTKWordTokenizer and TreebankWordTokenizer (#2883)\n* Allow empty string in CFG's + more (#2888)\n* Partition `tree.py` module into `tree` package + pickle fix (#2863)\n* Fix several TreebankWordTokenizer and NLTKWordTokenizer bugs (#2877)\n* Rewind Wordnet data file after each lookup (#2868)\n* Correct __init__ call for SyntaxCorpusReader subclasses (#2872)\n* Documentation fixes (#2873)\n* Fix levenstein distance for duplicated letters (#2849)\n* Support alternative Wordnet versions (#2860)\n* Remove hundreds of formatting warnings for nltk.org (#2859)\n* Modernize `nltk.org/howto` pages (#2856)\n* Fix Bleu Score smoothing function from taking log(0) (#2839)\n* Update third party tools to newer versions and removing MaltParser fixed version (#2832)\n* Fix TypeError: _pretty() takes 1 positional argument but 2 were given in sem/drt.py (#2854)\n* Replace `http` with `https` in most URLs (#2852)\n\nThanks to the following contributors to 3.6.6:\nAdam Hawley, BatMrE, Danny Sepler, Eric Kafe, Gavish Poddar, Panagiotis Simakis,\nRnDevelover, Robby Horvath, Tom Aarsen, Yuta Nakamura, Mohaned Mashaly\n\nVersion 3.6.5 2021-10-11\n\n* modernised nltk.org website\n* addressed LGTM.com issues\n* support ZWJ sequences emoji and skin tone modifer emoji in TweetTokenizer\n* METEOR evaluation now requires pre-tokenized input\n* Code linting and type hinting\n* implement get_refs function for DrtLambdaExpression\n* Enable automated CoreNLP, Senna, Prover9/Mace4, Megam, MaltParser CI tests\n* specify minimum regex version that supports regex.Pattern\n* avoid re.Pattern and regex.Pattern which fail for Python 3.6, 3.7\n\nThanks to the following contributors to 3.6.5:\nTom Aarsen, Saibo Geng, Mohaned Mashaly, Dimitri Papadopoulos, Danny Sepler,\nAhmet Yildirim, RnDevelover, yutanakamura\n\nVersion 3.6.4 2021-10-01\n\n* deprecate `nltk.usage(obj)` in favor of `help(obj)`\n* resolve ReDoS vulnerability in Corpus Reader\n* solidify performance tests\n* improve phone number recognition in tweet tokenizer\n* refactored CISTEM stemmer for German\n* identify NLTK Team as the author\n* replace travis badge with github actions badge\n* add SECURITY.md\n\nThanks to the following contributors to 3.6.4:\nTom Aarsen, Mohaned Mashaly, Dimitri Papadopoulos Orfanos, purificant, Danny Sepler\n\nVersion 3.6.3 2021-09-19\n* Dropped support for Python 3.5\n* Run CI tests on Windows, too\n* Moved from Travis CI to GitHub Actions\n* Code and comment cleanups\n* Visualize WordNet relation graphs using Graphviz\n* Fixed large error in METEOR score\n* Apply isort, pyupgrade, black, added as pre-commit hooks\n* Prevent debug_decisions in Punkt from throwing IndexError\n* Resolved ZeroDivisionError in RIBES with dissimilar sentences\n* Initialize WordNet IC total counts with smoothing value\n* Fixed AttributeError for Arabic ARLSTem2 stemmer\n* Many fixes and improvements to lm language model package\n* Fix bug in nltk.metrics.aline, C_skip = -10\n* Improvements to TweetTokenizer\n* Optional show arg for FreqDist.plot, ConditionalFreqDist.plot\n* edit_distance now computes Damerau-Levenshtein edit-distance\n\nThanks to the following contributors to 3.6.3:\nTom Aarsen, Abhijnan Bajpai, Michael Wayne Goodman, Michał Górny, Maarten ter Huurne,\nManu Joseph, Eric Kafe, Ilia Kurenkov, Daniel Loney, Rob Malouf, Mohaned Mashaly,\npurificant, Danny Sepler, Anthony Sottile\n\nVersion 3.6.2 2021-04-20\n* move test code to nltk/test\n* clean up some doctests\n* fix bug in NgramAssocMeasures (order preserving fix)\n* fixes for compatibility with Pypy 7.3.4\n\nThanks to the following contributors to 3.6.2:\nRuben Cartuyvels, Rob Malouf, Dalton Pearson, Danny Sepler\n\nVersion 3.6 2021-04-07\n* add support for Python 3.9\n* add Tree.fromlist\n* compute Minimum Spanning Tree of unweighted graph using BFS\n* fix bug with infinite loop in Wordnet closure and tree\n* fix bug in calculating BLEU using smoothing method 4\n* Wordnet synset similarities work for all pos\n* new Arabic light stemmer (ARLSTem2)\n* new syllable tokenizer (LegalitySyllableTokenizer)\n* remove nose in favor of pytest\n* misc bug fixes, code cleanups, test cleanups, efficiency improvements\n\nThanks to the following contributors to 3.6:\nTom Aarsen, K Abainia, Akshita Bhagia, Andrew Bird, Thomas Bird,\nTom Conroy, Christopher Hench, Andrew Jorgensen, Eric Kafe,\nIlia Kurenkov, Yeting Li, Joseph Manu, Marius Mather, Denali Molitor,\nJacob Moorman, Philippe Ombredanne, Vassilis Palassopoulos, Ram Rachum,\nDanny Sepler, Or Sharir, Brad Solomon, Hiroki Teranishi, Constantin Weisser,\nPratap Yadav, Louis Yang\n\nVersion 3.5 2020-04-13\n* add support for Python 3.8\n* drop support for Python 2\n* create NLTK's own Tokenizer class distinct from the Treebank reference tokeniser\n* update Vader sentiment analyser\n* fix JSON serialization of some PoS taggers\n* minor improvements in grammar.CFG, Vader, pl196x corpus reader, StringTokenizer\n* change implementation <= and >= for FreqDist so they are partial orders\n* make FreqDist iterable\n* correctly handle Penn Treebank trees with a unlabeled branching top node.\n\nThanks to the following contributors to 3.5:\nNicolas Darr, Gerhard Kremer, Liling Tan, Christopher Hench, Alexandre Dias, Hervé Nicol,\nPierpaolo Pantone, Bonifacio de Oliveira, Maciej Gawinecki, BLKSerene, hoefling, alvations,\npyfisch, srhrshr\n\nVersion 3.4.5 2019-08-20\n* Fixed security bug in downloader: Zip slip vulnerability - for the unlikely\n  situation where a user configures their downloader to use a compromised server\n  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-14751)\n\nThanks to the following contributors to 3.4.5:\nMike Salvatore\n\nVersion 3.4.4 2019-07-04\n* fix bug in plot function (probability.py)\n* add improved PanLex Swadesh corpus reader\n\nThanks to the following contributors to 3.4.4:\nDevashish Lal, Liling Tan\n\nVersion 3.4.3 2019-06-07\n\n* add Text.generate()\n* add QuadgramAssocMeasures\n* add SSP to tokenizers\n* return confidence of best tag from AveragedPerceptron\n* make plot methods return Axes objects\n* don't require list arguments to PositiveNaiveBayesClassifier.train\n* fix Tree classes to work with native Python copy library\n* fix inconsistency for NomBank\n* fix random seeding in LanguageModel.generate\n* fix ConditionalFreqDist mutation on tabulate/plot call\n* fix broken links in documentation\n* fix misc Wordnet issues\n* update installation instructions\n\nThanks to the following contributors to 3.4.3:\nalvations, Bharat123rox, cifkao, drewmiller, free-variation, henchc\nirisxzhou, nick-ulle, ppartarr, simonepri, yigitsever, zhaoyanpeng\n\nVersion 3.4.1 2019-04-17\n\n* add chomsky_normal_form for CFGs\n* add meteor score\n* add minimum edit/Levenshtein distance based alignment function\n* allow access to collocation list via text.collocation_list()\n* support corenlp server options\n* drop support for Python 3.4\n* other minor fixes\n\nThanks to the following contributors to 3.4.1:\nAdrian Ellis, Andrew Martin, Ayush Kaushal, BLKSerene, Bharat\nRaghunathan, Franklin Chen, KMiNT21 Kevin Brown, Liling Tan,\nMatan Rak, Nat Quayle Nelson, Osman Zubair, Purificant,\nUday Krishna, Viresh Gupta\n\nVersion 3.4 2018-11-17\n* Support Python 3.7\n* Language Modeling incl Kneser-Ney, Witten-Bell, Good-Turing\n* Cistem Stemmer for German\n* Support Russian National Corpus incl POS tag model\n* Decouple sentiment and twitter packages\n* Minor extensions for WordNet\n* K-alpha\n* Fix warning messages for corenlp\n* Comprehensive code cleanups\n* Many other minor fixes\n* Switch continuous integration from Jenkins to Travis\n\nSpecial thanks to Ilia Kurenkov (Language Model package), Liling Tan (Python 3.7, Travis-CI),\nand purificant (code cleanups). Thanks also to: Afshin Sadeghi, Ales Tamchyna, Alok Debnath,\naquatiko, Coykto, Denis Kataev, dnc1994, Fabian Howard, Frankie Robertson, Iaroslav Tymchenko,\nJayakrishna Sahit, LBenzahia, Leonie Weißweiler, Linghao Zhang, Rohit Kumar, sahitpj,\nTim Gianitsos, vagrant, 53X\n\nVersion 3.3 2018-05-06\n* Support Python 3.6\n* New interface to CoreNLP\n* Support synset retrieval by sense key\n* Minor fixes to CoNLL Corpus Reader, AlignedSent\n* Fixed minor inconsistencies in APIs and API documentation\n* Better conformance to PEP8\n* Drop moses.py (incompatible license)\n\nSpecial thanks to Liling Tan for leading our transition to Python 3.6.\nThanks to other contributors listed here: https://github.com/nltk/nltk/blob/develop/AUTHORS.md\n\nVersion 3.2.5 2017-09-24\n\n* Arabic stemmers (ARLSTem, Snowball)\n* NIST MT evaluation metric and added NIST international_tokenize\n* Moses tokenizer\n* Document Russian tagger\n* Fix to Stanford segmenter\n* Improve treebank detokenizer, VerbNet, Vader\n* Misc code and documentation cleanups\n* Implement fixes suggested by LGTM\n\nThanks to the following contributors to 3.2.5:\nAli Abdullah, Lakhdar Benzahia, Henry Elder, Campion Fellin,\nTsolak Ghukasyan, Thanh Ha, Jean Helie, Nelson Liu,\nNathan Schneider, Chintan Shah, Fábio Silva, Liling Tan,\nZiyao Wei, Zicheng Xu, Albert Au Yeung, AbdealiJK,\nporqupine, sbagan, xprogramer\n\nVersion 3.2.4 2017-05-21\n\n* remove load-time dependency on Python requests library\n* add support for Arabic in StanfordSegmenter\n* fix MosesDetokenizer on irregular quote tokens\n\nThanks to the following contributors to 3.2.4:\nAlex Constantin, Hatem Nassrat, Liling Tan\n\nVersion 3.2.3 2017-05-16\n\n* new interface to Stanford CoreNLP Web API\n* improved Lancaster stemmer with customizable rules from Whoosh\n* improved Treebank tokenizer\n* improved support for GLEU score\n* adopt new Abstract base class style\n* support custom tab files for extending WordNet\n* make synset_from_pos_and_offset a public method\n* make non-English WordNet lemma lookups case-insensitive\n* speed up TnT tagger\n* speed up FreqDist and ConditionalFreqDist\n* support additional quotes in TreebankWordTokenizer\n* clean up Tk's postscript output\n* drop explicit support for corpora not distributed with NLTK to streamline testing\n* allow iterator in perceptron tagger training\n* allow for curly bracket quantifiers in chunk.regexp.CHUNK_TAG_PATTERN\n* new corpus reader for MWA subset of PPDB\n* improved testing framework\n\nThanks to the following contributors to 3.2.3:\nMark Amery, Carl Bolz, Abdelhak Bougouffa, Matt Chaput, Michael Goodman,\nJaehoon Hwang, Naoya Kanai, Jackson Lee, Christian Meyer, Dmitrijs Milajevs,\nAdam Nelson, Pierpaolo Pantone, Liling Tan, Vilhjalmur Thorsteinsson,\nArthur Tilley, jmhutch, Yorwba, eromoe and others\n\nVersion 3.2.2 2016-12-31\n* added Kondrak's Aline algorithm\n* added ChrF and GLEU MT evaluation metrics\n* added Russian pos tagger model\n* added Moses detokenizer\n* rewrite Porter Stemmer\n* rewrite FrameNet corpus reader\n  (adds frame parameter to fes(), lus(), exemplars()\n  see https://www.nltk.org/howto/framenet.html)\n* updated FrameNet Corpus to version 1.7\n* fixes to stanford_segmenter.py, SentiText, CoNLL Corpus Reader\n* fixes to BLEU, naivebayes, Krippendorff's alpha, Punkt\n* fixes to tests for TransitionParser, Senna, edit distance\n* fixes to Moses Tokenizer and Detokenizer\n* improved TweetTokenizer\n* strip trailing whitespace when splitting sentences\n* handle inverted exclamation mark in ToktokTokenizer\n* resolved some issues with Python 3.5 support\n* improvements to testing framework\n* clean up dependencies\n\nThanks to the following contributors to 3.2.2:\n\nPrasasto Adi, Mark Amery, Geoff Bacon, George Berry, Colin Carroll, Alexis Dimitriadis,\nNicholas Fabina, German Ferrero, Tsolak Ghukasyan, Hyuckin David Lim, Naoya Kanai,\nGreg Kondrak, Igor Korolev, Tim Leslie, Rob Malouf, Heguang Miao, Dmitrijs Milajevs,\nAdam Nelson, Dennis O'Brien, Qi Liu, Pierpaolo Pantone, Andy Reagan, Mike Recachinas,\nNathan Schneider, Jānis Šlapiņš, Richard Snape, Liling Tan, Marcus Uneson,\nLinghao Zhang, drevicko, SaintNazaire\n\nVersion 3.2.1 2016-04-09\n* Support for CCG semantics, Stanford segmenter, VADER lexicon\n* Fixes to BLEU score calculation, CHILDES corpus reader\n* Other miscellaneous fixes\n\nThanks to the following contributors to 3.2.1:\nAndrew Giel, Casper Lehmann-Strøm, David Madl, Tanin Na Nakorn,\nGuilherme Nardari, Philippe Ombredanne, Nathan Schneider, Liling Tan,\nJosiah Wang, venticello\n\nVersion 3.2 2016-03-03\n* Fixes for Python 3.5\n* Code cleanups now Python 2.6 is no longer supported\n* Improvements to documentation\n* Comprehensive use of os.path for platform-specific path handling\n* Support for PanLex\n* Support for third party download locations for NLTK data\n* Fix bugs in IBM method 3 smoothing and BLEU calculation\n* Support smoothing for BLEU score and corpus-level BLEU\n* Support RIBES score\n* Improvements to TweetTokenizer\n* Updates for Stanford API\n* Add mathematical operators to ConditionalFreqDist\n* Fix bug in sentiwordnet for adjectives\n* Merged internal implementations of Trie\n\nThanks to the following contributors to 3.2:\nSantiago Castro, Jihun Choi, Graham Christensen, Andrew Drozdov, Long\nDuong, Kyriakos Georgiou, Michael Wayne Goodman, Clark Grubb, Tah Wei\nHoon, David Kamholz, Ewan Klein, Reed Loden, Rob Malouf, Philippe\nOmbredanne, Josh Owen, Pierpaolo Pantone, Mike Recachinas, Elijah\nRippeth, Thomas Stieglmaier, Liling Tan, Philip Tzou, Pratap Vardhan.\n\nVersion 3.1 2015-10-15\n* Fixes for Python 3.5 (drop support for capturing groups in regexp tokenizer)\n* Drop support for Python 2.6\n* Adopt perceptron tagger for new default POS tagger nltk.pos_tag\n* Stanford Neural Dependency Parser wrapper\n* Sentiment analysis package incl VADER\n* Improvements to twitter package\n* Multi word expression tokenizer\n* Support for everygram and skipgram\n* consistent evaluation metric interfaces, putting reference before hypothesis\n* new nltk.translate module, incorporating the old align module\n* implement stack decoder\n* clean up Alignment interface\n* CorpusReader method to support access to license and citation\n* Multext East Corpus and MTECorpusReader\n* include six module to streamline installation on MS Windows\n\nThanks to the following contributors to 3.1:\nLe Tuan Anh, Petra Barancikova, Alexander Böhm, Francis Bond,\nLong Duong, Anna Garbar, Matthew Honnibal, Tah Wei Hoon, Ewan Klein,\nRob Malouf, Dmitrijs Milajevs, Will Monroe, Sergio Oller, Pierpaolo\nPantone, Jacob Perkins, Lorenzo Rubio, Thomas Stieglmaier, Liling Tan,\nPratap Vardhan\n\nVersion 3.0.5 2015-09-05\n* rewritten IBM models, and new IBM Model 4 and 5 implementations\n* new Twitter package\n* stabilized MaltParser API\n* improved regex tagger\n* improved documentation on contributing\n* minor improvements to documentation and testing\n\nThanks to the following contributors to 3.0.5:\nÁlvaro Justen, Dmitrijs Milajevs, Ewan Klein, Heran Lin, Justin Hammar,\nLiling Tan, Long Duong, Lorenzo Rubio, Pierpaolo Pantone, Tah Wei Hoon\n\nVersion 3.0.4 2015-07-13\n* minor bug fixes and enhancements\n\nThanks to the following contributors to 3.0.4:\nNicola Bova, Santiago Castro, Len Remmerswaal, Keith Suderman, kabayan55,\npln-fing-udelar (NLP Group, Instituto de Computación, Facultad de Ingeniería, Universidad de la República, Uruguay).\n\nVersion 3.0.3 2015-06-12\n* bug fixes (Stanford NER, Boxer, Snowball, treebank tokenizer,\n    dependency graph, KneserNey, BLEU)\n* code clean-ups\n* default POS tagger permits tagset to be specified\n* gensim illustration\n* tgrep implementation\n* added PanLex Swadesh corpora\n* visualisation for aligned bitext\n* support for Google App Engine\n* POSTagger renamed StanfordPOSTagger, NERTagger renamed StanfordNERTagger\n\nThanks to the following contributors to 3.0.3:\n\nLong Duong, Pedro Fialho, Dan Garrette, Helder, Saimadhav Heblikar,\nChris Inskip, David Kamholz, Dmitrijs Milajevs, Smitha Milli,\nTom Mortimer-Jones, Avital Pekker, Jonathan Pool, Sam Raker,\nWill Roberts, Dmitry Sadovnychyi, Nathan Schneider, Anirudh W\n\nVersion 3.0.2 2015-03-13\n* make pretty-printing method names consistent\n* improvements to Portuguese stemmer\n* transition-based dependency parsers\n* dependency graph visualisation for ipython notebook\n* interfaces for Senna, BLLIP, python-crfsuite\n* NKJP corpus reader\n* code clean ups, minor bug fixes\n\nThanks to the following contributors to 3.0.2:\n\nLong Duong, Saimadhav Heblikar, Helder, Mikhail Korobov, Denis Krusko,\nAlex Louden, Felipe Madrigal, David McClosky, Dmitrijs Milajevs,\nOndrej Platek, Nathan Schneider, Dávid Márk Nemeskey, 0ssifrage, ducki13, kiwipi.\n\nVersion 3.0.1 2015-01-12\n* fix setup.py for new version of setuptools\n\nVersion 3.0.0 2014-09-07\n* minor bugfixes\n* added phrase extraction code by Liling Tan and Fredrik Hedman\n\nThanks to the following contributors to 3.0.0:\nMark Amery, Ivan Barria, Ingolf Becker, Francis Bond, Lars\nBuitinck, Cristian Capdevila, Arthur Darcet, Michelle Fullwood,\nDan Garrette, Dougal Graham, Dan Garrette, Dougal Graham, Lauri\nHallila, Tyler Hartley, Fredrik Hedman, Ofer Helman, Bruce Hill,\nMarcus Huderle, Nancy Ide, Nick Johnson, Angelos Katharopoulos,\nEwan Klein, Mikhail Korobov, Chris Liechti, Peter Ljunglof,\nJoseph Lynch, Haejoong Lee, Peter Ljunglöf, Dean Malmgren, Rob\nMalouf, Thorsten Marek, Dmitrijs Milajevs, Shari A’aidil\nNasruddin, Lance Nathan, Joel Nothman, Alireza Nourian, Alexander\nOleynikov, Ted Pedersen, Jacob Perkins, Will Roberts, Alex\nRudnick, Nathan Schneider, Geraldine Sim Wei Ying, Lynn Soe,\nLiling Tan, Louis Tiao, Marcus Uneson, Yu Usami, Steven Xu, Zhe\nWang, Chuck Wooters, lade, isnowfy, onesandzeros, pquentin, wvanlint\n\nVersion 3.0b2 2014-08-21\n* minor bugfixes and clean-ups\n* renamed remaining parse_ methods to read_ or load_, cf issue #656\n* added Paice's method of evaluating stemming algorithms\n\nThanks to the following contributors to 3.0.0b2: Lars Buitinck,\nCristian Capdevila, Lauri Hallila, Ofer Helman, Dmitrijs Milajevs,\nlade, Liling Tan, Steven Xu\n\nVersion 3.0.0b1 2014-07-11\n* Added SentiWordNet corpus and corpus reader\n* Fixed support for 10-column dependency file format\n* Changed Tree initialization to use fromstring\n\nThanks to the following contributors to 3.0b1: Mark Amery, Ivan\nBarria, Ingolf Becker, Francis Bond, Lars Buitinck, Arthur Darcet,\nMichelle Fullwood, Dan Garrette, Dougal Graham, Dan Garrette, Dougal\nGraham, Tyler Hartley, Ofer Helman, Bruce Hill, Marcus Huderle, Nancy\nIde, Nick Johnson, Angelos Katharopoulos, Ewan Klein, Mikhail Korobov,\nChris Liechti, Peter Ljunglof, Joseph Lynch, Haejoong Lee, Peter\nLjunglöf, Dean Malmgren, Rob Malouf, Thorsten Marek, Dmitrijs\nMilajevs, Shari A’aidil Nasruddin, Lance Nathan, Joel Nothman, Alireza\nNourian, Alexander Oleynikov, Ted Pedersen, Jacob Perkins, Will\nRoberts, Alex Rudnick, Nathan Schneider, Geraldine Sim Wei Ying, Lynn\nSoe, Liling Tan, Louis Tiao, Marcus Uneson, Yu Usami, Steven Xu, Zhe\nWang, Chuck Wooters, isnowfy, onesandzeros, pquentin, wvanlint\n\nVersion 3.0a4 2014-05-25\n* IBM Models 1-3, BLEU, Gale-Church aligner\n* Lesk algorithm for WSD\n* Open Multilingual WordNet\n* New implementation of Brill Tagger\n* Extend BNCCorpusReader to parse the whole BNC\n* MASC Tagged Corpus and corpus reader\n* Interface to Stanford Parser\n* Code speed-ups and clean-ups\n* API standardisation, including fromstring method for many objects\n* Improved regression testing setup\n* Removed PyYAML dependency\n\nThanks to the following contributors to 3.0a4:\nIvan Barria, Ingolf Becker, Francis Bond, Arthur Darcet, Dan Garrette,\nOfer Helman, Dougal Graham, Nancy Ide, Ewan Klein, Mikhail Korobov,\nChris Liechti, Peter Ljunglof, Joseph Lynch, Rob Malouf, Thorsten Marek,\nDmitrijs Milajevs, Shari A’aidil Nasruddin, Lance Nathan, Joel Nothman,\nJacob Perkins, Lynn Soe, Liling Tan, Louis Tiao, Marcus Uneson, Steven Xu,\nGeraldine Sim Wei Ying\n\nVersion 3.0a3 2013-11-02\n* support for FrameNet contributed by Chuck Wooters\n* support for Universal Declaration of Human Rights Corpus (udhr2)\n* major API changes:\n  - Tree.node -> Tree.label() / Tree.set_label()\n  - Chunk parser: top_node -> root_label; chunk_node -> chunk_label\n  - WordNet properties are now access methods, e.g. Synset.definition -> Synset.definition()\n  - relextract: show_raw_rtuple() -> rtuple(), show_clause() -> clause()\n* bugfix in texttiling\n* replaced simplify_tags with support for universal tagset (simplify_tags=True -> tagset='universal')\n* Punkt default behavior changed to realign sentence boundaries after trailing parenthesis and quotes\n* deprecated classify.svm (use scikit-learn instead)\n* various efficiency improvements\n\nThanks to the following contributors to 3.0a3:\nLars Buitinck, Marcus Huderle, Nick Johnson, Dougal Graham, Ewan Klein,\nMikhail Korobov, Haejoong Lee, Peter Ljunglöf, Dean Malmgren, Lance Nathan,\nAlexander Oleynikov, Nathan Schneider, Chuck Wooters, Yu Usami, Steven Xu,\npquentin, wvanlint\n\nVersion 3.0a2 2013-07-12\n* speed improvements in word_tokenize, GAAClusterer, TnT tagger, Baum Welch, HMM tagger\n* small improvements in collocation finders, probability, modelling, Porter Stemmer\n* bugfix in lowest common hypernyn calculation (used in path similarity measures)\n* code cleanups, docstring cleanups, demo fixes\n\nThanks to the following contributors to 3.0a2:\nMark Amery, Lars Buitinck, Michelle Fullwood, Dan Garrette, Dougal Graham,\nTyler Hartley, Bruce Hill, Angelos Katharopoulos, Mikhail Korobov,\nRob Malouf, Joel Nothman, Ted Pedersen, Will Roberts, Alex Rudnick,\nSteven Xu, isnowfy, onesandzeros\n\nVersion 3.0a1 2013-02-14\n* reinstated tkinter support (Haejoong Lee)\n\nVersion 3.0a0 2013-01-14\n* alpha release of first version to support Python 2.6, 2.7, and 3.\n\nVersion 2.0.4 2012-11-07\n* minor bugfix (removed numpy dependency)\n\nVersion 2.0.3 2012-09-24\n\n* fixed corpus/reader/util.py to support Python 2.5\n* make MaltParser safe to use in parallel\n* fixed bug in inter-annotator agreement\n* updates to various doctests (nltk/test)\n* minor bugfixes\n\nThanks to the following contributors to 2.0.3:\nRobin Cooper, Pablo Duboue, Christian Federmann, Dan Garrette, Ewan Klein,\nPierre-François Laquerre, Max Leonov, Peter Ljunglöf, Nitin Madnani, Ceri Stagg\n\nVersion 2.0.2 2012-07-05\n\n* improvements to PropBank, NomBank, and SemCor corpus readers\n* interface to full Penn Treebank Corpus V3 (corpus.ptb)\n* made wordnet.lemmas case-insensitive\n* more flexible padding in model.ngram\n* minor bugfixes and documentation enhancements\n* better support for automated testing\n\nThanks to the following contributors to 2.0.2:\nDaniel Blanchard, Mikhail Korobov, Nitin Madnani, Duncan McGreggor,\nMorten Neergaard, Nathan Schneider, Rico Sennrich.\n\nVersion 2.0.1 2012-05-15\n\n* moved NLTK to GitHub: https://github.com/nltk\n* set up integration testing: https://jenkins.shiningpanda.com/nltk/ (Morten Neergaard)\n* converted documentation to Sphinx format: https://www.nltk.org/api/nltk.html\n* dozens of minor enhancements and bugfixes: https://github.com/nltk/nltk/commits/\n* dozens of fixes for conformance with PEP-8\n* dozens of fixes to ensure operation with Python 2.5\n* added interface to Lin's Dependency Thesaurus (Dan Blanchard)\n* added interface to scikit-learn classifiers (Lars Buitinck)\n* added segmentation evaluation measures (David Doukhan)\n\nThanks to the following\tcontributors to\t2.0.1 (since 2.0b9, July 2010):\nRami Al-Rfou', Yonatan Becker, Steven Bethard, Daniel Blanchard, Lars\nBuitinck, David Coles, Lucas Cooper, David Doukhan, Dan Garrette,\nMasato Hagiwara, Michael Hansen, Michael Heilman, Rebecca Ingram,\nSudharshan Kaushik, Mikhail Korobov, Peter Ljunglof, Nitin Madnani,\nRob Malouf, Tomonori Nagano, Morten Neergaard, David Nemeskey,\nJoel Nothman, Jacob Perkins, Alessandro Presta, Alex Rudnick,\nNathan Schneider, Stefano Lattarini, Peter Stahl, Jason Yoder\n\nVersion 2.0.1 (rc1) 2011-04-11\n\nNLTK:\n* added interface to the Stanford POS Tagger\n* updates to sem.Boxer, sem.drt.DRS\n* allow unicode strings in grammars\n* allow non-string features in classifiers\n* modifications to HunposTagger\n* issues with DRS printing\n* fixed bigram collocation finder for window_size > 2\n* doctest paths no longer presume unix-style pathname separators\n* fixed issue with NLTK's tokenize module colliding with the Python tokenize module\n* fixed issue with stemming Unicode strings\n* changed ViterbiParser.nbest_parse to parse\n* ChaSen and KNBC Japanese corpus readers\n* preserve case in concordance display\n* fixed bug in simplification of Brown tags\n* a version of IBM Model 1 as described in Koehn 2010\n* new class AlignedSent for aligned sentence data and evaluation metrics\n* new nltk.util.set_proxy to allow easy configuration of HTTP proxy\n* improvements to downloader user interface to catch URL and HTTP errors\n* added CHILDES corpus reader\n* created special exception hierarchy for Prover9 errors\n* significant changes to the underlying code of the boxer interface\n* path-based wordnet similarity metrics use a fake root node for verbs, following the Perl version\n* added ability to handle multi-sentence discourses in Boxer\n* added the 'english' Snowball stemmer\n* simplifications and corrections of Earley Chart Parser rules\n* several changes to the feature chart parsers for correct unification\n* bugfixes: FreqDist.plot, FreqDist.max, NgramModel.entropy, CategorizedCorpusReader, DecisionTreeClassifier\n* removal of Python >2.4 language features for 2.4 compatibility\n* removal of deprecated functions and associated warnings\n* added semantic domains to wordnet corpus reader\n* changed wordnet similarity functions to include instance hyponyms\n* updated to use latest version of Boxer\n\nData:\n* JEITA Public Morphologically Tagged Corpus (in ChaSen format)\n* KNB Annotated corpus of Japanese blog posts\n* Fixed some minor bugs in alvey.fcfg, and added number of parse trees in alvey_sentences.txt\n* added more comtrans data\n\nDocumentation:\n* minor fixes to documentation\n* NLTK Japanese book (chapter 12) by Masato Hagiwara\n\nNLTK-Contrib:\n* Viethen and Dale referring expression algorithms\n\n\nVersion 2.0b9 2010-07-25\n\nNLTK:\n* many code and documentation cleanups\n* Added port of Snowball stemmers\n* Fixed loading of pickled tokenizers (issue 556)\n* DecisionTreeClassifier now handles unknown features (issue 570)\n* Added error messages to LogicParser\n* Replaced max_models with end_size to prevent Mace from hanging\n* Added interface to Boxer\n* Added nltk.corpus.semcor to give access to SemCor 3.0 corpus (issue 530)\n* Added support for integer- and float-valued features in maxent classifiers\n* Permit NgramModels to be pickled\n* Added Sourced Strings (see test/sourcedstring.doctest for details)\n* Fixed bugs in with Good-Turing and Simple Good-Turing Estimation (issue 26)\n* Add support for span tokenization, aka standoff annotation of segmentation (incl Punkt)\n* allow unicode nodes in Tree.productions()\n* Fixed WordNet's morphy to be consistent with the original implementation,\n  taking the shortest returned form instead of an arbitrary one (issues 427, 487)\n* Fixed bug in MaxentClassifier\n* Accepted bugfixes for YCOE corpus reader (issue 435)\n* Added test to _cumulative_frequencies() to correctly handle the case when no arguments are supplied\n* Added a TaggerI interface to the HunPos open-source tagger\n* Return 0, not None, when no count is present for a lemma in WordNet\n* fixed pretty-printing of unicode leaves\n* More efficient calculation of the leftcorner relation for left corner parsers\n* Added two functions for graph calculations: transitive closure and inversion.\n* FreqDist.pop() and FreqDist.popitems() now invalidate the caches (issue 511)\n\nData:\n* Added SemCor 3.0 corpus (Brown Corpus tagged with WordNet synsets)\n* Added LanguageID corpus (trigram counts for 451 languages)\n* Added grammar for a^n b^n c^n\n\nNLTK-Contrib:\n* minor updates\n\nThanks to the following\tcontributors to\t2.0b9:\n\nSteven Bethard,\tFrancis Bond, Dmitry Chichkov, Liang Dong, Dan Garrette,\nSimon Greenhill, Bjorn Maeland, Rob Malouf, Joel Nothman, Jacob Perkins,\nAlberto Planas, Alex Rudnick, Geoffrey Sampson, Kevin Scannell, Richard Sproat\n\n\nVersion 2.0b8 2010-02-05\n\nNLTK:\n* fixed copyright and license statements\n* removed PyYAML, and added dependency to installers and download instructions\n* updated to LogicParser, DRT (Dan Garrette)\n* WordNet similarity metrics return None instead of -1 when\n  they fail to find a path (Steve Bethard)\n* shortest_path_distance uses instance hypernyms (Jordan Boyd-Graber)\n* clean_html improved (Bjorn Maeland)\n* batch_parse, batch_interpret and batch_evaluate functions allow\n    grammar or grammar filename as argument\n* more Portuguese examples (portuguese_en.doctest, examples/pt.py)\n\nNLTK-Contrib:\n* Aligner implementations (Christopher Crowner, Torsten Marek)\n* ScriptTranscriber package (Richard Sproat and Kristy Hollingshead)\n\nBook:\n* updates for second printing, correcting errata\n  https://nltk.googlecode.com/svn/trunk/nltk/doc/book/errata.txt\n\nData:\n* added Europarl sample, with 10 docs for each of 11 langs (Nitin Madnani)\n* added SMULTRON sample corpus (Torsten Marek, Martin Volk)\n\n\nVersion 2.0b7 2009-11-09\n\nNLTK:\n* minor bugfixes and enhancements: data loader, inference package, FreqDist, Punkt\n* added Portuguese example module, similar to nltk.book for English (examples/pt.py)\n* added all_lemma_names() method to WordNet corpus reader\n* added update() and __add__() extensions to FreqDist (enhances alignment with Python 3.0 counters)\n* reimplemented clean_html\n* added test-suite runner for automatic/manual regression testing\n\nNLTK-Data:\n* updated Punkt models for sentence segmentation\n* added corpus of the works of Machado de Assis (Brazilian Portuguese)\n\nBook:\n* Added translation of preface into Portuguese, contributed by Tiago Tresoldi.\n\nVersion 2.0b6 2009-09-20\n\nNLTK:\n* minor fixes for Python 2.4 compatibility\n* added words() method to XML corpus reader\n* minor bugfixes and code clean-ups\n* fixed downloader to put data in %APPDATA% on Windows\n\nData:\n* Updated Punkt models\n* Fixed utf8 encoding issues with UDHR and Stopwords Corpora\n* Renamed CoNLL \"cat\" files to \"esp\" (different language)\n* Added Alvey NLT feature-based grammar\n* Added Polish PL196x corpus\n\nVersion 2.0b5 2009-07-19\n\nNLTK:\n* minor bugfixes (incl FreqDist, Python eggs)\n* added reader for Europarl Corpora (contributed by Nitin Madnani)\n* added reader for IPI PAN Polish Corpus (contributed by Konrad Goluchowski)\n* fixed data.py so that it doesn't generate a warning for Windows Python 2.6\n\nNLTK-Contrib:\n* updated Praat reader (contributed by Margaret Mitchell)\n\nVersion 2.0b4 2009-07-10\n\nNLTK:\n* switched to Apache License, Version 2.0\n* minor bugfixes in semantics and inference packages\n* support for Python eggs\n* fixed stale regression tests\n\nData:\n* added NomBank 1.0\n* uppercased feature names in some grammars\n\nVersion 2.0b3 2009-06-25\n\nNLTK:\n* several bugfixes\n* added nombank corpus reader (Paul Bedaride)\n\nVersion 2.0b2 2009-06-15\n\nNLTK:\n* minor bugfixes and optimizations for parsers, updated some doctests\n* added bottom-up filtered left corner parsers,\n  LeftCornerChartParser and IncrementalLeftCornerChartParser.\n* fixed dispersion plot bug which prevented empty plots\n\nVersion 2.0b1 2009-06-09\n\nNLTK:\n* major refactor of chart parser code and improved API (Peter Ljungl喃)\n* added new bottom-up left-corner chart parser strategy\n* misc bugfixes (ChunkScore, chart rules, chatbots, jcn-similarity)\n* improved efficiency of \"import nltk\" using lazy module imports\n* moved CCG package and ISRI Arabic stemmer from NLTK-Contrib into core NLTK\n* misc code cleanups\n\nContrib:\n* moved out of the main NLTK distribution into a separate distribution\n\nBook:\n* Ongoing polishing ahead of print publication\n\nVersion 0.9.9 2009-05-06\n\nNLTK:\n* Finalized API for NLTK 2.0 and the book, incl dozens of small fixes\n* Names of the form nltk.foo.Bar now available as nltk.Bar\n  for significant functionality; in some cases the name was modified\n  (using old names will produce a deprecation warning)\n* Bugfixes in downloader, WordNet\n* Expanded functionality in DecisionTree\n* Bigram collocations extended for discontiguous bigrams\n* Translation toy nltk.misc.babelfish\n* New module nltk.help giving access to tagset documentation\n* Fix imports so that NLTK builds without Tkinter (Bjorn Maeland)\n\nData:\n* new maxent NE chunker model\n* updated grammar packages for the book\n* data for new tagsets collection, documenting several tagsets\n* added lolcat translation to the Genesis collection\n\nContrib (work in progress):\n* Updates to coreference package (Joseph Frazee)\n* New ISRI Arabic stemmer (Hosam Algasaier)\n* Updates to Toolbox package (Greg Aumann)\n\nBook:\n* Substantial editorial corrections ahead of final submission\n\nVersion 0.9.8 2009-02-18\n\nNLTK:\n* New off-the-shelf tokenizer, POS tagger, and named-entity tagger\n* New metrics package with inter-annotator agreement scores,\n  distance metrics, rank correlation\n* New collocations package (Joel Nothman)\n* Many clean-ups to WordNet package (Steven Bethard, Jordan Boyd-Graber)\n* Moved old pywordnet-based WordNet package to nltk_contrib\n* WordNet browser (Paul Bone)\n* New interface to dependency treebank corpora\n* Moved MinimalSet class into nltk.misc package\n* Put NLTK applications in new nltk.app package\n* Many other improvements incl semantics package, toolbox, MaltParser\n* Misc changes to many API names in preparation for 1.0, old names deprecated\n* Most classes now available in the top-level namespace\n* Work on Python egg distribution (Brandon Rhodes)\n* Removed deprecated code remaining from 0.8.* versions\n* Fixes for Python 2.4 compatibility\n\nData:\n* Corrected identifiers in Dependency Treebank corpus\n* Basque and Catalan Dependency Treebanks (CoNLL 2007)\n* PE08 Parser Evaluation data\n* New models for POS tagger and named-entity tagger\n\nBook:\n* Substantial editorial corrections\n\nVersion 0.9.7 2008-12-19\n\nNLTK:\n* fixed problems with accessing zipped corpora\n* improved design and efficiency of grammars and chart parsers\n  including new bottom-up combine strategy and a redesigned\n  Earley strategy (Peter Ljunglof)\n* fixed bugs in smoothed probability distributions and added\n  regression tests (Peter Ljunglof)\n* improvements to Punkt (Joel Nothman)\n* improvements to text classifiers\n* simple word-overlap RTE classifier\n\nData:\n* A new package of large grammars (Peter Ljunglof)\n* A small gazetteer corpus and corpus reader\n* Organized example grammars into separate packages\n* Childrens' stories added to gutenberg package\n\nContrib (work in progress):\n* fixes and demonstration for named-entity feature extractors in nltk_contrib.coref\n\nBook:\n* extensive changes throughout, including new chapter 5 on classification\n  and substantially revised chapter 11 on managing linguistic data\n\nVersion 0.9.6 2008-12-07\n\nNLTK:\n* new WordNet corpus reader (contributed by Steven Bethard)\n* incorporated dependency parsers into NLTK (was NLTK-Contrib) (contributed by Jason Narad)\n* moved nltk/cfg.py to nltk/grammar.py and incorporated dependency grammars\n* improved efficiency of unification algorithm\n* various enhancements to the semantics package\n* added plot() and tabulate() methods to FreqDist and ConditionalFreqDist\n* FreqDist.keys() and list(FreqDist) provide keys reverse-sorted by value,\n  to avoid the confusion caused by FreqDist.sorted()\n* new downloader module to support interactive data download: nltk.download()\n  run using \"python -m nltk.downloader all\"\n* fixed WordNet bug that caused min_depth() to sometimes give incorrect result\n* added nltk.util.Index as a wrapper around defaultdict(list) plus\n  a functional-style initializer\n* fixed bug in Earley chart parser that caused it to break\n* added basic TnT tagger nltk.tag.tnt\n* new corpus reader for CoNLL dependency format (contributed by Kepa Sarasola and Iker Manterola)\n* misc other bugfixes\n\nContrib (work in progress):\n* TIGERSearch implementation by Torsten Marek\n* extensions to hole and glue semantics modules by Dan Garrette\n* new coreference package by Joseph Frazee\n* MapReduce interface by Xinfan Meng\n\nData:\n* Corpora are stored in compressed format if this will not compromise speed of access\n* Swadesh Corpus of comparative wordlists in 23 languages\n* Split grammar collection into separate packages\n* New Basque and Spanish grammar samples (contributed by Kepa Sarasola and Iker Manterola)\n* Brown Corpus sections now have meaningful names (e.g. 'a' is now 'news')\n* Fixed bug that forced users to manually unzip the WordNet corpus\n* New dependency-parsed version of Treebank corpus sample\n* Added movie script \"Monty Python and the Holy Grail\" to webtext corpus\n* Replaced words corpus data with a much larger list of English words\n* New URL for list of available NLTK corpora\n  https://nltk.googlecode.com/svn/trunk/nltk_data/index.xml\n\nBook:\n* complete rewrite of first three chapters to make the book accessible\n  to a wider audience\n* new chapter on data-intensive language processing\n* extensive reworking of most chapters\n* Dropped subsection numbering; moved exercises to end of chapters\n\nDistributions:\n* created Portfile to support Mac installation\n\n\nVersion 0.9.5 2008-08-27\n\nNLTK:\n* text module with support for concordancing, text generation, plotting\n* book module\n* Major reworking of the automated theorem proving modules (Dan Garrette)\n* draw.dispersion now uses pylab\n* draw.concordance GUI tool\n* nltk.data supports for reading corpora and other data files from within zipfiles\n* trees can be constructed from strings with Tree(s) (cf Tree.parse(s))\n\nContrib (work in progress):\n* many updates to student projects\n  - nltk_contrib.agreement (Thomas Lippincott)\n  - nltk_contrib.coref (Joseph Frazee)\n  - nltk_contrib.depparser (Jason Narad)\n  - nltk_contrib.fuf (Petro Verkhogliad)\n  - nltk_contrib.hadoop (Xinfan Meng)\n* clean-ups: deleted stale files; moved some packages to misc\n\nData\n* Cleaned up Gutenberg text corpora\n* added Moby Dick; removed redundant copy of Blake songs.\n* more tagger models\n* renamed to nltk_data to facilitate installation\n* stored each corpus as a zip file for quicker installation\n  and access, and to solve a problem with the Propbank\n  corpus including a file with an illegal name for MSWindows\n  (con.xml).\n\nBook:\n* changed filenames to chNN format\n* reworked opening chapters (work in progress)\n\nDistributions:\n* fixed problem with mac installer that arose when Python binary\n  couldn't be found\n* removed dependency of NLTK on nltk_data so that NLTK code can be\n  installed before the data\n\nVersion 0.9.4 2008-08-01\n\nNLTK:\n- Expanded semantics package for first order logic, linear logic,\n  glue semantics, DRT, LFG (Dan Garrette)\n- new WordSense class in wordnet.synset supporting access to synsets\n  from sense keys and accessing sense counts (Joel Nothman)\n- interface to Mallet's linear chain CRF implementation (nltk.tag.crf)\n- misc bugfixes incl Punkt, synsets, maxent\n- improved support for chunkers incl flexible chunk corpus reader,\n  new rule type: ChunkRuleWithContext\n- new GUI for pos-tagged concordancing nltk.draw.pos_concordance\n- new GUI for developing regexp chunkers nltk.draw.rechunkparser\n- added bio_sents() and bio_words() methods to ConllChunkCorpusReader in conll.py\n    to allow reading (word, tag, chunk_typ) tuples off of CoNLL-2000 corpus. Also\n    modified ConllChunkCorpusView to support these changes.\n- feature structures support values with custom unification methods\n- new flag on tagged corpus readers to use simplified tagsets\n- new package for ngram language modeling with Katz backoff nltk.model\n- added classes for single-parented and multi-parented trees that\n  automatically maintain parent pointers (nltk.tree.ParentedTree and\n  nltk.tree.MultiParentedTree)\n- new WordNet browser GUI (Jussi Salmela, Paul Bone)\n- improved support for lazy sequences\n- added generate() method to probability distributions\n- more flexible parser for converting bracketed strings to trees\n- made fixes to docstrings to improve API documentation\n\nContrib (work in progress)\n- new NLG package, FUF/SURGE (Petro Verkhogliad)\n- new dependency parser package (Jason Narad)\n- new Coreference package, incl support for\n  ACE-2, MUC-6 and MUC-7 corpora (Joseph Frazee)\n- CCG Parser (Graeme Gange)\n- first order resolution theorem prover (Dan Garrette)\n\nData:\n- Nnw NPS Chat Corpus and corpus reader (nltk.corpus.nps_chat)\n- ConllCorpusReader can now be used to read CoNLL 2004 and 2005 corpora.\n- Implemented HMM-based Treebank POS tagger and phrase chunker for\n  nltk_contrib.coref in api.py. Pickled versions of these objects are checked\n  in in data/taggers and data/chunkers.\n\nBook:\n- misc corrections in response to feedback from readers\n\nVersion 0.9.3 2008-06-03\n\nNLTK:\n- modified WordNet similarity code to use pre-built information content files\n- new classifier-based tagger, BNC corpus reader\n- improved unicode support for corpus readers\n- improved interfaces to Weka, Prover9/Mace4\n- new support for using MEGAM and SciPy to train maxent classifiers\n- rewrite of Punkt sentence segmenter (Joel Nothman)\n- bugfixes for WordNet information content module (Jordan Boyd-Graber)\n- code clean-ups throughout\n\nBook:\n- miscellaneous fixes in response to feedback from readers\n\nContrib:\n- implementation of incremental algorithm for generating\n  referring expressions (contributed by Margaret Mitchell)\n- refactoring WordNet browser (Paul Bone)\n\nCorpora:\n- included WordNet information content files\n\nVersion 0.9.2 2008-03-04\n\nNLTK:\n- new theorem-prover and model-checker module nltk.inference,\n  including interface to Prover9/Mace4 (Dan Garrette, Ewan Klein)\n- bugfix in Reuters corpus reader that causes Python\n  to complain about too many open files\n- VerbNet and PropBank corpus readers\n\nData:\n- VerbNet Corpus version 2.1: hierarchical, verb lexicon linked to WordNet\n- PropBank Corpus: predicate-argument structures, as stand-off annotation of Penn Treebank\n\nContrib:\n- New work on WordNet browser, incorporating a client-server model (Jussi Salmela)\n\nDistributions:\n- Mac OS 10.5 distribution\n\nVersion 0.9.1 2008-01-24\n\nNLTK:\n- new interface for text categorization corpora\n- new corpus readers: RTE, Movie Reviews, Question Classification, Brown Corpus\n- bugfix in ConcatenatedCorpusView that caused iteration to fail if it didn't start from the beginning of the corpus\n\nData:\n- Question classification data, included with permission of Li & Roth\n- Reuters 21578 Corpus, ApteMod version, from CPAN\n- Movie Reviews corpus (sentiment polarity), included with permission of Lillian Lee\n- Corpus for Recognising Textual Entailment (RTE) Challenges 1, 2 and 3\n- Brown Corpus (reverted to original file structure: ca01-cr09)\n- Penn Treebank corpus sample (simplified implementation, new readers treebank_raw and treebank_chunk)\n- Minor redesign of corpus readers, to use filenames instead of \"items\" to identify parts of a corpus\n\nContrib:\n- theorem_prover: Prover9, tableau, MaltParser, Mace4, glue semantics, docs (Dan Garrette, Ewan Klein)\n- drt: improved drawing, conversion to FOL (Dan Garrette)\n- gluesemantics: GUI demonstration, abstracted LFG code, documentation (Dan Garrette)\n- readability: various text readability scores (Thomas Jakobsen, Thomas Skardal)\n- toolbox: code to normalize toolbox databases (Greg Aumann)\n\nBook:\n- many improvements in early chapters in response to reader feedback\n- updates for revised corpus readers\n- moved unicode section to chapter 3\n- work on engineering.txt (not included in 0.9.1)\n\nDistributions:\n- Fixed installation for Mac OS 10.5 (Joshua Ritterman)\n- Generalize doctest_driver to work with doc_contrib\n\nVersion 0.9 2007-10-12\n\nNLTK:\n- New naming of packages and modules, and more functions imported into\n  top-level nltk namespace, e.g. nltk.chunk.Regexp -> nltk.RegexpParser,\n    nltk.tokenize.Line -> nltk.LineTokenizer, nltk.stem.Porter -> nltk.PorterStemmer,\n    nltk.parse.ShiftReduce -> nltk.ShiftReduceParser\n- processing class names changed from verbs to nouns, e.g.\n  StemI -> StemmerI, ParseI -> ParserI, ChunkParseI -> ChunkParserI, ClassifyI -> ClassifierI\n- all tokenizers are now available as subclasses of TokenizeI,\n  selected tokenizers are also available as functions, e.g. wordpunct_tokenize()\n- rewritten ngram tagger code, collapsed lookup tagger with unigram tagger\n- improved tagger API, permitting training in the initializer\n- new system for deprecating code so that users are notified of name changes.\n- support for reading feature cfgs to parallel reading cfgs (parse_featcfg())\n- text classifier package, maxent (GIS, IIS), naive Bayes, decision trees, weka support\n- more consistent tree printing\n- wordnet's morphy stemmer now accessible via stemmer package\n- RSLP Portuguese stemmer (originally developed by Viviane Moreira Orengo, reimplemented by Tiago Tresoldi)\n- promoted ieer_rels.py to the sem package\n- improvements to WordNet package (Jussi Salmela)\n- more regression tests, and support for checking coverage of tests\n- miscellaneous bugfixes\n- remove numpy dependency\n\nData:\n- new corpus reader implementation, refactored syntax corpus readers\n- new data package: corpora, grammars, tokenizers, stemmers, samples\n- CESS-ESP Spanish Treebank and corpus reader\n- CESS-CAT Catalan Treebank and corpus reader\n- Alpino Dutch Treebank and corpus reader\n- MacMorpho POS-tagged Brazilian Portuguese news text and corpus reader\n- trained model for Portuguese sentence segmenter\n- Floresta Portuguese Treebank version 7.4 and corpus reader\n- TIMIT player audio support\n\nContrib:\n- BioReader (contributed by Carlos Rodriguez)\n- TnT tagger (contributed by Sam Huston)\n- wordnet browser (contributed by Jussi Salmela, requires wxpython)\n- lpath interpreter (contributed by Haejoong Lee)\n- timex -- regular expression-based temporal expression tagger\n\nBook:\n- polishing of early chapters\n- introductions to parts 1, 2, 3\n- improvements in book processing software (xrefs, avm & gloss formatting, javascript clipboard)\n- updates to book organization, chapter contents\n- corrections throughout suggested by readers (acknowledged in preface)\n- more consistent use of US spelling throughout\n- all examples redone to work with single import statement: \"import nltk\"\n- reordered chapters: 5->7->8->9->11->12->5\n  * language engineering in part 1 to broaden the appeal\n    of the earlier part of the book and to talk more about\n    evaluation and baselines at an earlier stage\n  * concentrate the partial and full parsing material in part 2,\n    and remove the specialized feature-grammar material into part 3\n\nDistributions:\n- streamlined mac installation (Joshua Ritterman)\n- included mac distribution with ISO image\n\nVersion 0.8 2007-07-01\n\nCode:\n- changed nltk.__init__ imports to explicitly import names from top-level modules\n- changed corpus.util to use the 'rb' flag for opening files, to fix problems\n  reading corpora under MSWindows\n- updated stale examples in engineering.txt\n- extended feature structure interface to permit chained features, e.g. fs['F','G']\n- further misc improvements to test code plus some bugfixes\nTutorials:\n- rewritten opening section of tagging chapter\n- reorganized some exercises\n\nVersion 0.8b2 2007-06-26\n\nCode (major):\n- new corpus package, obsoleting old corpora package\n  - supports caching, slicing, corpus search path\n  - more flexible API\n  - global updates so all NLTK modules use new corpus package\n- moved nltk/contrib to separate top-level package nltk_contrib\n- changed wordpunct tokenizer to use \\w instead of a-zA-Z0-9\n  as this will be more robust for languages other than English,\n  with implications for many corpus readers that use it\n- known bug: certain re-entrant structures in featstruct\n- known bug: when the LHS of an edge contains an ApplicationExpression,\n    variable values in the RHS bindings aren't copied over when the\n    fundamental rule applies\n- known bug: HMM tagger is broken\nTutorials:\n- global updates to NLTK and docs\n- ongoing polishing\nCorpora:\n- treebank sample reverted to published multi-file structure\nContrib:\n- DRT and Glue Semantics code (nltk_contrib.drt, nltk_contrib.gluesemantics, by Dan Garrette)\n\nVersion 0.8b1 2007-06-18\n\nCode (major):\n- changed package name to nltk\n- import all top-level modules into nltk, reducing need for import statements\n- reorganization of sub-package structures to simplify imports\n- new featstruct module, unifying old featurelite and featurestructure modules\n- FreqDist now inherits from dict, fd.count(sample) becomes fd[sample]\n- FreqDist initializer permits: fd = FreqDist(len(token) for token in text)\n- made numpy optional\nCode (minor):\n- changed GrammarFile initializer to accept filename\n- consistent tree display format\n- fixed loading process for WordNet and TIMIT that prevented code installation if data not installed\n- taken more care with unicode types\n- incorporated pcfg code into cfg module\n- moved cfg, tree, featstruct to top level\n- new filebroker module to make handling of example grammar files more transparent\n- more corpus readers (webtext, abc)\n- added cfg.covers() to check that a grammar covers a sentence\n- simple text-based wordnet browser\n- known bug: parse/featurechart.py uses incorrect apply() function\nCorpora:\n- csv data file to document NLTK corpora\nContrib:\n- added Glue semantics code (contrib.glue, by Dan Garrette)\n- Punkt sentence segmenter port (contrib.punkt, by Willy)\n- added LPath interpreter (contrib.lpath, by Haejoong Lee)\n- extensive work on classifiers (contrib.classifier*, Sumukh Ghodke)\nTutorials:\n- polishing on parts I, II\n- more illustrations, data plots, summaries, exercises\n- continuing to make prose more accessible to non-linguistic audience\n- new default import that all chapters presume: from nltk.book import *\nDistributions:\n- updated to latest version of numpy\n- removed WordNet installation instructions as WordNet is now included in corpus distribution\n- added pylab (matplotlib)\n\nVersion 0.7.5 2007-05-16\n\nCode:\n- improved WordNet and WordNet-Similarity interface\n- the Lancaster Stemmer (contributed by Steven Tomcavage)\nCorpora:\n- Web text samples\n- BioCreAtIvE-PPI - a corpus for protein-protein interactions\n- Switchboard Telephone Speech Corpus Sample (via Talkbank)\n- CMU Problem Reports Corpus sample\n- CONLL2002 POS+NER data\n- Patient Information Leaflet corpus\n- WordNet 3.0 data files\n- English wordlists: basic English, frequent words\nTutorials:\n- more improvements to text and images\n\nVersion 0.7.4 2007-05-01\n\nCode:\n- Indian POS tagged corpus reader: corpora.indian\n- Sinica Treebank corpus reader: corpora.sinica_treebank\n- new web corpus reader corpora.web\n- tag package now supports pickling\n- added function to utilities.py to guess character encoding\nCorpora:\n- Rotokas texts from Stuart Robinson\n- POS-tagged corpora for several Indian languages (Bangla, Hindi, Marathi, Telugu) from A Kumaran\nTutorials:\n- Substantial work on Part II of book on structured programming, parsing and grammar\n- More bibliographic citations\n- Improvements in typesetting, cross references\n- Redimensioned images and tables for better use of page space\n- Moved project list to wiki\nContrib:\n- validation of toolbox entries using chunking\n- improved classifiers\nDistribution:\n- updated for Python 2.5.1, Numpy 1.0.2\n\nVersion 0.7.3 2007-04-02\n\n* Code:\n - made chunk.Regexp.parse() more flexible about its input\n - developed new syntax for PCFG grammars, e.g. A -> B C [0.3] | D [0.7]\n - fixed CFG parser to support grammars with slash categories\n - moved beta classify package from main NLTK to contrib\n - Brill taggers loaded correctly\n - misc bugfixes\n* Corpora:\n - Shakespeare XML corpus sample and corpus reader\n* Tutorials:\n - improvements to prose, exercises, plots, images\n - expanded and reorganized tutorial on structured programming\n - formatting improvements for Python listings\n - improved plots (using pylab)\n - categorization of problems by difficulty\nContrib:\n - more work on kimmo lexicon and grammar\n - more work on classifiers\n\nVersion 0.7.2 2007-03-01\n\n* Code:\n - simple feature detectors (detect module)\n - fixed problem when token generators are passed to a parser (parse package)\n - fixed bug in Grammar.productions() (identified by Lucas Champollion and Mitch Marcus)\n - fixed import bug in category.GrammarFile.earley_parser\n - added utilities.OrderedDict\n - initial port of old NLTK classifier package (by Sam Huston)\n - UDHR corpus reader\n* Corpora:\n - added UDHR corpus (Universal Declaration of Human Rights)\n     with 10k text samples in 300+ languages\n* Tutorials:\n - improved images\n - improved book formatting, including new support for:\n   - javascript to copy program examples to clipboard in HTML version,\n   - bibliography, chapter cross-references, colorization, index, table-of-contents\n\n* Contrib:\n  - new Kimmo system: contrib.mit.six863.kimmo (Rob Speer)\n  - fixes for: contrib.fsa (Rob Speer)\n  - demonstration of text classifiers trained on UDHR corpus for\n      language identification: contrib.langid (Sam Huston)\n  - new Lambek calculus system: contrib.lambek\n  - new tree implementation based on elementtree: contrib.tree\n\nVersion 0.7.1 2007-01-14\n\n* Code:\n  - bugfixes (HMM, WordNet)\n\nVersion 0.7 2006-12-22\n\n* Code:\n  - bugfixes, including fixed bug in Brown corpus reader\n  - cleaned up wordnet 2.1 interface code and similarity measures\n  - support for full Penn treebank format contributed by Yoav Goldberg\n* Tutorials:\n  - expanded tutorials on advanced parsing and structured programming\n  - checked all doctest code\n  - improved images for chart parsing\n\nVersion 0.7b1 2006-12-06\n\n* Code:\n  - expanded semantic interpretation package\n  - new high-level chunking interface, with cascaded chunking\n  - split chunking code into new chunk package\n  - updated wordnet package to support version 2.1 of Wordnet.\n  - prototyped basic wordnet similarity measures\n    (path distance, Wu + Palmer and Leacock + Chodorow, Resnik similarity measures.)\n  - bugfixes (tag.Window, tag.ngram)\n  - more doctests\n* Contrib:\n  - toolbox language settings module\n* Tutorials:\n  - rewrite of chunking chapter, switched from Treebank to CoNLL format as main focus,\n    simplified evaluation framework, added ngram chunking section\n  - substantial updates throughout (esp programming and semantics chapters)\n* Corpora:\n  - Chat-80 Prolog data files provided as corpora, plus corpus reader\n\nVersion 0.7a2 2006-11-13\n\n* Code:\n  - more doctest\n  - code to read Chat-80 data\n  - HMM bugfix\n* Tutorials:\n  - continued updates and polishing\n* Corpora:\n  - toolbox MDF sample data\n\nVersion 0.7a1 2006-10-29\n\n* Code:\n  - new toolbox module (Greg Aumann)\n  - new semantics package (Ewan Klein)\n  - bugfixes\n* Tutorials\n  - substantial revision, especially in preface, introduction, words,\n    and semantics chapters.\n\nVersion 0.6.6 2006-10-06\n\n* Code:\n  - bugfixes (probability, shoebox, draw)\n* Contrib:\n  - new work on shoebox package (Stuart Robinson)\n* Tutorials:\n  - continual expansion and revision, especially on introduction to\n    programming, advanced programming and the feature-based grammar chapters.\n\nVersion 0.6.5 2006-07-09\n\n* Code:\n  - improvements to shoebox module (Stuart Robinson, Greg Aumann)\n  - incorporated feature-based parsing into core NLTK-Lite\n  - corpus reader for Sinica treebank sample\n  - new stemmer package\n* Contrib:\n  - hole semantics implementation (Peter Wang)\n  - Incorporating yaml\n  - new work on feature structures, unification, lambda calculus\n  - new work on shoebox package (Stuart Robinson, Greg Aumann)\n* Corpora:\n  - Sinica treebank sample\n* Tutorials:\n  - expanded discussion throughout, incl: left-recursion, trees, grammars,\n    feature-based grammar, agreement, unification, PCFGs,\n    baseline performance, exercises, improved display of trees\n\nVersion 0.6.4 2006-04-20\n\n* Code:\n  - corpus readers for Senseval 2 and TIMIT\n  - clusterer (ported from old NLTK)\n  - support for cascaded chunkers\n  - bugfix suggested by Brent Payne\n  - new SortedDict class for regression testing\n* Contrib:\n  - CombinedTagger tagger and marshalling taggers, contributed by Tiago Tresoldi\n* Corpora:\n  - new: Senseval 2, TIMIT sample\n* Tutorials:\n  - major revisions to programming, words, tagging, chunking, and parsing tutorials\n  - many new exercises\n  - formatting improvements, including colorized program examples\n  - fixed problem with testing on training data, reported by Jason Baldridge\n\nVersion 0.6.3 2006-03-09\n\n* switch to new style classes\n* repair FSA model sufficiently for Kimmo module to work\n* port of MIT Kimmo morphological analyzer; still needs lots of code clean-up and inline docs\n* expanded support for shoebox format, developed with Stuart Robinson\n* fixed bug in indexing CFG productions, for empty right-hand-sides\n* efficiency improvements, suggested by Martin Ranang\n* replaced classeq with isinstance, for efficiency improvement, as suggested by Martin Ranang\n* bugfixes in chunk eval\n* simplified call to draw_trees\n* names, stopwords corpora\n\nVersion 0.6.2 2006-01-29\n\n* Peter Spiller's concordancer\n* Will Hardy's implementation of Penton's paradigm visualization system\n* corpus readers for presidential speeches\n* removed NLTK dependency\n* generalized CFG terminals to permit full range of characters\n* used fully qualified names in demo code, for portability\n* bugfixes from Yoav Goldberg, Eduardo Pereira Habkost\n* fixed obscure quoting bug in tree displays and conversions\n* simplified demo code, fixed import bug\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.162109375,
          "content": "include *.md\ninclude *.txt\ninclude *.ini\ninclude setup.*\ninclude ChangeLog\ninclude Makefile\ninclude MANIFEST.in\n\n\ngraft nltk\ngraft tools\ngraft web\n\nglobal-exclude *~\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.5771484375,
          "content": "# Natural Language Toolkit: source Makefile\n#\n# Copyright (C) 2001-2024 NLTK Project\n# Author: Steven Bird <stevenbird1@gmail.com>\n#\t Edward Loper <edloper@gmail.com>\n# URL: <https://www.nltk.org/>\n# For license information, see LICENSE.TXT\n\nPYTHON = python\nVERSION = $(shell $(PYTHON) -c 'import nltk; print(nltk.__version__)' | sed '/^Warning: */d')\nNLTK_URL = $(shell $(PYTHON) -c 'import nltk; print(nltk.__url__)' | sed '/^Warning: */d')\n\n.PHONY: all clean clean_code\n\nall: dist\n\n########################################################################\n# TESTING\n########################################################################\nDOCTEST_FILES = nltk/test/*.doctest\nDOCTEST_CODE_FILES = nltk/*.py nltk/*/*.py\n\ndoctest:\n\tpytest $(DOCTEST_FILES)\n\ndoctest_code:\n\tpytest $(DOCTEST_CODE_FILES)\n\ndemotest:\n\tfind nltk -name \"*.py\"\\\n        -and -not -path *misc* \\\n        -and -not -name brown_ic.py \\\n        -exec echo ==== '{}' ==== \\; -exec python '{}' \\;\n\n########################################################################\n# DISTRIBUTIONS\n########################################################################\n\ndist: clean_code\n\t$(PYTHON) -m build\n\n########################################################################\n# CLEAN\n########################################################################\n\nclean: clean_code\n\trm -rf build web/_build iso dist api MANIFEST nltk-$(VERSION) nltk.egg-info\n\nclean_code:\n\trm -f `find nltk -name '*.pyc'`\n\trm -f `find nltk -name '*.pyo'`\n\trm -f `find . -name '*~'`\n\trm -rf `find . -name '__pycache__'`\n\trm -f MANIFEST # regenerate manifest from MANIFEST.in\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.69921875,
          "content": "# Natural Language Toolkit (NLTK)\n[![PyPI](https://img.shields.io/pypi/v/nltk.svg)](https://pypi.python.org/pypi/nltk)\n![CI](https://github.com/nltk/nltk/actions/workflows/ci.yaml/badge.svg?branch=develop)\n\nNLTK -- the Natural Language Toolkit -- is a suite of open source Python\nmodules, data sets, and tutorials supporting research and development in Natural\nLanguage Processing. NLTK requires Python version 3.8, 3.9, 3.10, 3.11 or 3.12.\n\nFor documentation, please visit [nltk.org](https://www.nltk.org/).\n\n\n## Contributing\n\nDo you want to contribute to NLTK development? Great!\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for more details.\n\nSee also [how to contribute to NLTK](https://www.nltk.org/contribute.html).\n\n\n## Donate\n\nHave you found the toolkit helpful?  Please support NLTK development by donating\nto the project via PayPal, using the link on the NLTK homepage.\n\n\n## Citing\n\nIf you publish work that uses NLTK, please cite the NLTK book, as follows:\n\n    Bird, Steven, Edward Loper and Ewan Klein (2009).\n    Natural Language Processing with Python.  O'Reilly Media Inc.\n\n\n## Copyright\n\nCopyright (C) 2001-2024 NLTK Project\n\nFor license information, see [LICENSE.txt](LICENSE.txt).\n\n[AUTHORS.md](AUTHORS.md) contains a list of everyone who has contributed to NLTK.\n\n\n### Redistributing\n\n- NLTK source code is distributed under the Apache 2.0 License.\n- NLTK documentation is distributed under the Creative Commons\n  Attribution-Noncommercial-No Derivative Works 3.0 United States license.\n- NLTK corpora are provided under the terms given in the README file for each\n  corpus; all are redistributable and available for non-commercial use.\n- NLTK may be freely redistributed, subject to the provisions of these licenses.\n"
        },
        {
          "name": "RELEASE-HOWTO.txt",
          "type": "blob",
          "size": 3.0419921875,
          "content": "Building an NLTK distribution\n----------------------------------\n\n0. Packages required to build, test, and distribute NLTK\n   - py312-pytest py312-requests py312-sphinx\n   - py312-numpy py312-matplotlib py312-tkinter py312-black\n   - sqlite3-tcl\n   - pip install sphinxcontrib-apidoc twython build twine pytest-mock pre-commit\n\n1. Testing\n   - Check no errors are reported in our continuous integration service:\n     https://github.com/nltk/nltk/actions\n   - Optionally test demonstration code locally\n     make demotest\n   - Optionally test individual modules:\n     tox-3.8 -e py38 nltk.package.module\n   - Check the data index is up-to-date:\n     cd ../nltk_data; make; push\n\n2. Update Version Number and ChangeLog\n   - Update version number\n     edit nltk/VERSION and web/conf.py (version and release)\n   - Check web/install.rst mentions latest version of Python\n   - Check setup.py lists correct range of Python versions\n   - Add a new entry to the news page in nltk/web/news.rst\n   - Update the ChangeLog (for nltk, nltk_data)\n     git log \"$(git describe --tags --abbrev=0)..HEAD\" --oneline\n     edit ChangeLog\n\n3. Build Documentation\n   - Check the copyright year is correct and update if necessary\n     e.g. ./tools/global_replace.py 2001-2022 2001-2024\n     check web/conf.py copyright line\n   - Check that installation instructions are up-to-date\n     (including the range of Python versions that are supported)\n     edit web/install.rst setup.py\n   - Ensure that nltk_theme is installed and updated\n     pip install -U nltk_theme\n   - Rebuild the API docs\n     sphinx-build -E ./web ./build\n   - Publish them\n     cd ../nltk.github.com; git pull (begin with current docs repo)\n     cp -r ../nltk/build/* .\n     git add .\n     git commit -m \"updates for version 3.X.Y\"\n     git push origin master\n\n4. Create a new version\n   - Tag this version:\n     cd ../nltk\n     git tag -a 3.X.Y -m \"version 3.X.Y\"\n     git push --tags\n     verify that it shows up here: https://github.com/nltk/nltk/tags\n     This is important for the website, as the footer will link to the\n     tag with the version from web/conf.py.\n\n5. Release\n   - Make the distributions\n     make clean; make dist; ls dist/\n   - Upload the distributions\n     python -m twine upload dist/*\n   - Check upload\n     https://pypi.python.org/pypi/nltk\n\n6. Announce\n   - Post announcement to NLTK the mailing lists:\n     nltk-dev (for beta releases)\n     nltk-users (for final releases)\n     nltk twitter account\n\n7. Optionally update repo version\n   - we don't want builds from the repository to have the same release number\n     e.g. after release X.Y.4, update repository version to X.Y.5a (alpha)\n\n\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@@@ BOOK BUILD\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\nThe build requires docutils, pdflatex, python imaging library, epydoc,\n  cdrtools, ImageMagick\n\n  1. Check out a clean copy of the subversion repository (or make clean)\n     and install locally with pip install; make clean\n  2. make doc (slow; see doc/ for the results) and commit\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.1015625,
          "content": "# Security Policy\n\n## Reporting a Vulnerability\n\nPlease report security issues to `nltk.team@gmail.com`\n"
        },
        {
          "name": "nltk",
          "type": "tree",
          "content": null
        },
        {
          "name": "pip-req.txt",
          "type": "blob",
          "size": 0.2294921875,
          "content": "pytest>=6.0.1\ntox>=1.6.1\npylint>=1.1.0\nnumpy>=1.8.0\nscipy>=0.13.2\nmatplotlib>=1.3.1\nscikit-learn>=0.14.1\npython-crfsuite>=0.8.2\npyparsing>=2.0.3\ntwython>=3.2.0\nregex>=2021.8.3\nclick>=7.1.2\njoblib>=1.0.1\ntqdm>=4.59.0\npre-commit>=2.13.0\n"
        },
        {
          "name": "requirements-ci.txt",
          "type": "blob",
          "size": 0.1484375,
          "content": "click\ngensim>=4.0.0\nmarkdown-it-py\nmatplotlib\nmdit-plain\nmdit-py-plugins\npytest\npytest-mock\npytest-xdist[psutil]\npyyaml\nregex\nscikit-learn\ntqdm\ntwython\n"
        },
        {
          "name": "requirements-test.txt",
          "type": "blob",
          "size": 0.0546875,
          "content": "pylint\npytest>=6.0.1\npytest-cov>=2.10.1\npytest-mock\ntox\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.1025390625,
          "content": "[metadata]\nlicense_files =\n    LICENSE.txt\n    AUTHORS.md\n    README.md\n\n[build_sphinx]\nsource-dir = web\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.6640625,
          "content": "#!/usr/bin/env python\n#\n# Setup script for the Natural Language Toolkit\n#\n# Copyright (C) 2001-2024 NLTK Project\n# Author: NLTK Team <nltk.team@gmail.com>\n# URL: <https://www.nltk.org/>\n# For license information, see LICENSE.TXT\n\n# Work around mbcs bug in distutils.\n# https://bugs.python.org/issue10945\nimport codecs\n\ntry:\n    codecs.lookup(\"mbcs\")\nexcept LookupError:\n    ascii = codecs.lookup(\"ascii\")\n    func = lambda name, enc=ascii: {True: enc}.get(name == \"mbcs\")\n    codecs.register(func)\n\nimport os\n\n# Use the VERSION file to get NLTK version\nversion_file = os.path.join(os.path.dirname(__file__), \"nltk\", \"VERSION\")\nwith open(version_file) as fh:\n    nltk_version = fh.read().strip()\n\n# setuptools\nfrom setuptools import find_packages, setup\n\n# Specify groups of optional dependencies\nextras_require = {\n    \"machine_learning\": [\n        \"numpy\",\n        \"python-crfsuite\",\n        \"scikit-learn\",\n        \"scipy\",\n    ],\n    \"plot\": [\"matplotlib\"],\n    \"tgrep\": [\"pyparsing\"],\n    \"twitter\": [\"twython\"],\n    \"corenlp\": [\"requests\"],\n}\n\n# Add a group made up of all optional dependencies\nextras_require[\"all\"] = {\n    package for group in extras_require.values() for package in group\n}\n\n# Adds CLI commands\nconsole_scripts = \"\"\"\n[console_scripts]\nnltk=nltk.cli:cli\n\"\"\"\n\n_project_homepage = \"https://www.nltk.org/\"\n\nsetup(\n    name=\"nltk\",\n    description=\"Natural Language Toolkit\",\n    version=nltk_version,\n    url=_project_homepage,\n    project_urls={\n        \"Documentation\": _project_homepage,\n        \"Source Code\": \"https://github.com/nltk/nltk\",\n        \"Issue Tracker\": \"https://github.com/nltk/nltk/issues\",\n    },\n    long_description=\"\"\"\\\nThe Natural Language Toolkit (NLTK) is a Python package for\nnatural language processing.  NLTK requires Python 3.8, 3.9, 3.10, 3.11 or 3.12.\"\"\",\n    license=\"Apache License, Version 2.0\",\n    keywords=[\n        \"NLP\",\n        \"CL\",\n        \"natural language processing\",\n        \"computational linguistics\",\n        \"parsing\",\n        \"tagging\",\n        \"tokenizing\",\n        \"syntax\",\n        \"linguistics\",\n        \"language\",\n        \"natural language\",\n        \"text analytics\",\n    ],\n    maintainer=\"NLTK Team\",\n    maintainer_email=\"nltk.team@gmail.com\",\n    author=\"NLTK Team\",\n    author_email=\"nltk.team@gmail.com\",\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Information Technology\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering :: Human Machine Interfaces\",\n        \"Topic :: Scientific/Engineering :: Information Analysis\",\n        \"Topic :: Text Processing\",\n        \"Topic :: Text Processing :: Filters\",\n        \"Topic :: Text Processing :: General\",\n        \"Topic :: Text Processing :: Indexing\",\n        \"Topic :: Text Processing :: Linguistic\",\n    ],\n    package_data={\"nltk\": [\"test/*.doctest\", \"VERSION\"]},\n    python_requires=\">=3.8\",\n    install_requires=[\n        \"click\",\n        \"joblib\",\n        \"regex>=2021.8.3\",\n        \"tqdm\",\n    ],\n    extras_require=extras_require,\n    packages=find_packages(),\n    zip_safe=False,  # since normal files will be present too?\n    entry_points=console_scripts,\n)\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 2.3720703125,
          "content": "[tox]\nenvlist =\n    py{38,39,310,311,312}\n    pypy\n    py{38,39,310,311,312}-nodeps\n    py{38,39,310,311,312}-jenkins\n    py-travis\n\n[testenv]\n; simplify numpy installation\nsetenv =\n    LAPACK=\n    ATLAS=None\n    PYTHONWARNINGS=ignore\n\n; Copy all environment variables to the tox test environment\npassenv = *\n\ndeps =\n    numpy\n    text-unidecode\n    twython\n    pyparsing\n    pytest\n    pytest-cov\n    pytest-mock\n    python-crfsuite\n    regex\n    click\n    joblib\n    tqdm\n    matplotlib\n    markdown-it-py\n    mdit-py-plugins\n    mdit-plain\n    pyyaml\n\nchangedir = nltk/test\ncommands =\n    ; scipy and scikit-learn requires numpy even to run setup.py so\n    ; they can't be installed in one command\n    pip install scipy scikit-learn\n\n    ; pytest --cov=nltk --cov-report html:{envdir}/docs nltk/test/\n    pytest\n\n[testenv:pypy]\n; numpy is bundled with pypy; coverage is extra slow and\n; the coverage results are not that different from CPython.\ndeps =\n    pytest\n    pytest-mock\n    twython\n\ncommands =\n    pytest\n\n[testenv:py38-nodeps]\nbasepython = python3.8\ndeps =\n    pytest\n    pytest-mock\ncommands = pytest\n\n[testenv:py39-nodeps]\nbasepython = python3.9\ndeps =\n    pytest\n    pytest-mock\ncommands = pytest\n\n[testenv:py310-nodeps]\nbasepython = python3.10\ndeps =\n    pytest\n    pytest-mock\ncommands = pytest\n\n[testenv:py311-nodeps]\nbasepython = python3.11\ndeps =\n    pytest\n    pytest-mock\ncommands = pytest\n\n[testenv:py312-nodeps]\nbasepython = python3.12\ndeps =\n    pytest\n    pytest-mock\ncommands = pytest\n\n# Use minor version agnostic basepython, but specify testenv\n# control Python2/3 versions using jenkins' user-defined matrix instead.\n# Available Python versions: http://repository-cloudbees.forge.cloudbees.com/distributions/ci-addons/python/fc25/\n\n[testenv:py-travis]\nextras = all\nsetenv =\n    NLTK_DATA = {homedir}/nltk_data/\ncommands = {toxinidir}/tools/travis/coverage-pylint.sh\n\n[testenv:py-travis-third-party]\nextras = all\nsetenv =\n\tSTANFORD_MODELS = {homedir}/third/stanford-parser/\n\tSTANFORD_PARSER = {homedir}/third/stanford-parser/\n\tSTANFORD_POSTAGGER = {homedir}/third/stanford-postagger/\n    NLTK_DATA = {homedir}/nltk_data/\n\ncommands =\n    {toxinidir}/tools/travis/third-party.sh\n    {toxinidir}/tools/travis/coverage-pylint.sh\n\n[testenv:py3-runtime-check]\n; nltk should be runnable in an env with nothing installed\nbasepython = python3\ndeps =\ncommands = python -c \"import nltk\"\n\n[isort]\nprofile=black\n"
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}