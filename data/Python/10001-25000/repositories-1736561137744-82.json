{
  "metadata": {
    "timestamp": 1736561137744,
    "page": 82,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "wistbean/learn_python3_spider",
      "stars": 18948,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0908203125,
          "content": "*.js linguist-language=python\n*.css linguist-language=python\n*.html linguist-language=python\n"
        },
        {
          "name": "Brigtdata1.png",
          "type": "blob",
          "size": 85.94921875,
          "content": null
        },
        {
          "name": "CloudCreat",
          "type": "blob",
          "size": 0.7158203125,
          "content": "# coding: utf-8\n\nfrom wordcloud import WordCloud\nimport cv2\nimport jieba\nimport matplotlib.pyplot as plt\n\nwith open('test.txt', 'r') as f:\n    text = f.read()\n\ncut_text = \" \".join(jieba.cut(text))\n\ncolor_mask = cv2.imread('back.jpeg')\n\ncloud = WordCloud(\n    # 设置字体，不指定就会出现乱码\n    font_path=\"/Users/caichenyang/Desktop/爬虫尝试/Baoli.ttc\",\n    # font_path=path.join(d,'simsun.ttc'),\n    # 设置背景色\n    background_color='white',\n    # 词云形状\n    mask=color_mask,\n    # 允许最大词汇\n    max_words=120,\n    # 最大号字体\n    max_font_size=2000\n)\n\nwCloud = cloud.generate(cut_text)\nwCloud.to_file('cloud.jpg')\nplt.imshow(wCloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\n"
        },
        {
          "name": "GaoKao_Score",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2019 wistbean\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.27734375,
          "content": "\n# learn_python3_spider\n接下来就是，学习python的正确姿势！\n\n[等下，阿里云服务器/2核2G/3M/40g，99元/年？？？](https://t.aliyun.com/U/DYTxRF)\n\npeace.\n\n# 如果你也想要会 Python\n\n可以加入我的 [Python 会员网站](https://fxxkpython.com)!!\n\n# python爬虫教程从0到1\n\n## 爬虫负基础\n\n- [python爬虫系列教程-1 ｜ 不会代码也想爬数据？这就教你！](https://mp.weixin.qq.com/s?__biz=MzkyNTExNzY4NA==&mid=2247484935&idx=1&sn=ad9f68845455ca35c08c0e11f92aa4a6&chksm=c1ca3b9cf6bdb28a8647bc911079221b790780611e019e628613657ebfbc38e1e317f53ab00f&token=1453775207&lang=zh_CN#rd)\n\n\n## python爬虫前，抓包\n\n- [python爬虫系列教程00 | 什么是爬虫，怎么玩爬虫？](http://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489892&amp;idx=1&amp;sn=40f3f6b70d467ca72b838939aa63d720&amp;chksm=ceb9e378f9ce6a6e089459fad40e2ef8bdce9f46a0a7b9e8332cdbe6d2bc09a47879dc99dd4c&amp;scene=27#wechat_redirect)\n- [python爬虫系列教程01 | 教你在 Chrome 浏览器轻松抓包](http://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489893&amp;idx=1&amp;sn=32cc4fe30066a148485f40629aff598a&amp;chksm=ceb9e379f9ce6a6f609b95a729d01ff1745c101c14fe005fd2ed73e32dec08e1ed4d102bc9c9&amp;scene=27#wechat_redirect)\n- [python爬虫系列教程02 | 教你通过 Fiddler 进行手机抓包？](https://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489894&amp;idx=1&amp;sn=d620c16bf3fcb4657c8c44152d936fc7&amp;chksm=ceb9e37af9ce6a6c3017158256b06afd5fb1945a4cd05f9db7e27c31606626ee73d0cc44a074&amp;scene=27#wechat_redirect)\n\n\n## python爬虫库的使用\n- [python爬虫系列教程03 | 那个叫做 Urllib 的库让我们的 python 假装是浏览器](http://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489895&amp;idx=1&amp;sn=9cddfab13d7d251ab3f7e81a882961ce&amp;chksm=ceb9e37bf9ce6a6d1014e00a7d4730249dc1e9104a8fd70ebc34211714a136bc003a962853b8&amp;scene=27#wechat_redirect)\n- [python爬虫系列教程04 | 长江后浪推前浪，Requests库把urllib库拍在沙滩上](http://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489896&amp;idx=1&amp;sn=a4686a0cefb12a9bc5d41b062327f545&amp;chksm=ceb9e374f9ce6a622723e99c8e6c04dc25b268d474f259f85ec8da73d755f0de562bb584c63c&amp;scene=27#wechat_redirect)\n- [python爬虫系列教程05 | 年轻人，不会正则表达式你睡得着觉？有点出息没有？](https://vip.fxxkpython.com/?p=1928)\n- [python爬虫系列教程06 | 你的第一个爬虫，爬取当当网 Top 500 本五星好评书籍](https://vip.fxxkpython.com/?p=1903)\n- [python爬虫系列教程07 | 有了 BeautifulSoup ，妈妈再也不用担心我的正则表达式了](https://vip.fxxkpython.com/?p=1891)\n- [python爬虫系列教程08 | 你的第二个爬虫，要过年了，爬取豆瓣最受欢迎的250部电影慢慢看](https://vip.fxxkpython.com/?p=1871)\n- [python爬虫系列教程09 | 上来，自己动 ！这就是 selenium 的牛逼之处](https://vip.fxxkpython.com/?p=1854)\n- [python爬虫系列教程10 | 这次，将带你使用 selenium+ phantomJS 爬取b站上的NBA形象大使蔡徐坤和他的球友们](https://vip.fxxkpython.com/?p=4699)\n- [python爬虫系列教程11 | python爬虫的时候对Json数据的解析](https://vip.fxxkpython.com/?p=4723)\n- [python爬虫系列教程12 | 秒爬，python爬虫中的多线程，多进程，协程](https://vip.fxxkpython.com/?p=4736)\n- [python爬虫系列教程13 | 就这么说吧，如果你不懂python多线程和线程池，那就去河边摸鱼！](https://vip.fxxkpython.com/?p=4756)\n- [python爬虫系列教程14 | 害羞，用多线程秒爬那些万恶的妹纸们，纸巾呢？](https://vip.fxxkpython.com/?p=5293)\n- [python爬虫系列教程15 | 你，快去试试用多进程的方式重新去爬取豆瓣上的电影](https://vip.fxxkpython.com/?p=4793)\n- [python爬虫系列教程16 | 听说你又被封 ip 了，你要学会伪装好自己，这次说说伪装你的头部](https://vip.fxxkpython.com/?p=4803)\n- [python爬虫系列教程17 | 就算你被封了也能继续爬，使用IP代理池伪装你的IP地址，让IP飘一会](https://vip.fxxkpython.com/?p=4818)\n- [海外评比第一的代理 IP 提供商，代理抓取成功率 99%。](https://get.brightdata.com/k47ir0cpnkh2)\n- [python爬虫系列教程18 | 遇到需要的登录的网站怎么办？用这3招轻松搞定！](https://vip.fxxkpython.com/?p=4833)\n- [python爬虫系列教程19 | 小帅b教你如何识别图片验证码](https://vip.fxxkpython.com/?p=4848)\n- [python爬虫系列教程20 | 对于b站这样的滑动验证码，不好意思，照样自动识别](https://vip.fxxkpython.com/?p=4878)\n- [python爬虫系列教程21 | 以后我再讲「模拟登录」我就是狗](https://vip.fxxkpython.com/?p=4919)\n- [python爬虫系列教程22 | 手机，这次要让你上来自己动了。这就是 Appium+Python 的牛x之处](https://vip.fxxkpython.com/?p=4929)\n- [python爬虫系列教程23 | 搞事情了，用 Appium 爬取你的微信朋友圈。](https://vip.fxxkpython.com/?p=4950)\n- [python爬虫系列教程24 |爬取下来的数据怎么保存？ CSV 了解一下](https://vip.fxxkpython.com/?p=4975)\n- [python爬虫系列教程25 | 把数据爬取下来之后就存储到你的MySQL数据库。](https://vip.fxxkpython.com/?p=4990)\n- [python爬虫系列教程26 | 当Python遇到MongoDB的时候，存储av女优的数据变得如此顺滑爽～](https://vip.fxxkpython.com/?p=5005)\n- [python爬虫系列教程27 | 你爬下的数据不分析一波可就亏了啊，使用python进行数据可视化](https://vip.fxxkpython.com/?p=5020)\n- [python爬虫系列教程28 | 使用scrapy爬取糗事百科的例子，告诉你它有多厉害！](https://vip.fxxkpython.com/?p=5038)\n- [python爬虫系列教程30 | scrapy后续，把「糗事百科」的段子爬下来然后存到数据库中](https://vip.fxxkpython.com/?p=5059)\n- [mitmproxy | 那个站在中间的男人，使用Python就能直接操控你的上网请求](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485104&idx=1&sn=5ee4a04e6ce2854e5507cd320517fd0d&chksm=fc8bbe21cbfc373738d926e0ca3250f44079449a85c1fe88f307805e28a3cc4ada07d9e322bb&token=2085568099&lang=zh_CN#rd)\n- [mitmproxy | 如何使用 mitmproxy 监控你的手机](https://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&mid=2247490016&idx=1&sn=4749ad1707fd33be167118d5f3aadeab&source=41#wechat_redirect)\n\n\n## python爬虫进阶：python爬虫反爬\n\n- [python爬虫反爬 | 对方是如何丧心病狂的通过 css 加密让你爬不到数据的](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484810&amp;idx=1&amp;sn=ed3297773c1eeb741bdabfb31c3ea00e&amp;chksm=fc8bbd1bcbfc340d6ae0166e035dd8c8e106afae8adc5fc32162a17b68916b69383b0ab67265&amp;scene=27#wechat_redirect) \n- [python爬虫反反爬 | 看完这篇，你几乎可以横扫大部分 css 字体加密的网站！](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484921&amp;idx=1&amp;sn=72a707c5bc67eede144947829cab4dc6&amp;chksm=fc8bbd68cbfc347eca6727ff90f85ef58a4fdd7c2f75a962aee3ccd5e9c4266dbe5f4e6e2262&amp;scene=27#wechat_redirect) \n- [python爬虫反反爬 | 像有道词典这样的 JS 混淆加密应该怎么破](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484997&amp;idx=1&amp;sn=b304304aacb3cba31f5f7a6c6bb1ba69&amp;chksm=fc8bbed4cbfc37c29db631c187295757c164ae75ff3e0381dbbf685a9f3d1410098e5b751e33&amp;scene=27#wechat_redirect) \n- [你想逆向我的 js 代码？呵呵，先过了我的反 debug 再说吧！](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485338&idx=1&sn=5b4d6ed34a27ed5e81a3e5d8ccf8bee9&scene=19&token=464856977&lang=zh_CN#wechat_redirect)\n- [js 逆向分析，代码扣取](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247488874&idx=1&sn=709dc1b0005bb3e39b59a90d75cccfa7&chksm=fc8badfbcbfc24ede7a35f6b0d6b1becddb5ca57ab9efe2b394d962581e84804c05208c94f79&token=1628227326&lang=zh_CN#rd)\n- [Python 逆向抓取 APP 数据](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247488910&idx=1&sn=2b5cc2571220086b9ac3e2c95fd41cde&chksm=fc8bad1fcbfc24090c06a86da2725e506d17993bddf0dfa57330d3069a003a5f0d66fb602d11&token=1628227326&lang=zh_CN#rd)\n- [这次有点骚了，破解安卓某 APP 低层加密](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247489255&idx=1&sn=4abbd544ff0a63f4d24eb06a0f954df1&chksm=fc8bae76cbfc2760a71267cca2cade5f6847a7291fa4adc0abf37743c878bfbda65a496f2305&scene=178&cur_album_id=1321044729160859650#rd)\n- [当你通过 Python 请求网站得到一堆无厘头的 JS 时...](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247489477&idx=1&sn=891944cbceab3eb77966055604645457&chksm=fc8baf54cbfc2642a171cf8851c20f8a05f3eb2b844e8d0b1a6e7539590ce7c69ffae21f10f5&scene=178&cur_album_id=1321044729160859650#rd)\n\n## Python websocket 爬虫：\n- [哇靠，这些数据疯狂变化，该怎么爬取？](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485466&idx=1&sn=1e4db96f3ca1d3a263dd7e075cbd7600&scene=19&token=464856977&lang=zh_CN#wechat_redirect)\n\n## Python 分布式爬虫\n- [说说分布式爬虫](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485718&idx=1&sn=2d42d1c7408b14781ef4c1e97fbac8f6&scene=19&token=464856977&lang=zh_CN#wechat_redirect)\n- [我整来了几台服务器，就是为了给你演示一下分布式爬虫的整个过程](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485863&idx=1&sn=34f9fb196c77dffdcce4a610b622270d&scene=19&token=464856977&lang=zh_CN#wechat_redirect)\n\n## 爬虫实战教程\n- [python爬取 20w 表情包之后，从此你就成为了微信斗图届的高手](https://fxxkpython.com/python-pa-qu-biao-qing-bao.html)\n- [python爬取你喜欢的公众号的所有原创文章，然后搞成PDF慢慢看](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484657&amp;idx=1&amp;sn=998bfcce6cd22b7fedff29e68a46fe3f&amp;chksm=fc8bbc60cbfc3576f117d3566fbea8a042ee573d840bbe6a3d4ec9bffef815c691b7f9a59711&amp;scene=27#wechat_redirect) \n- [当 python 遇到你的微信的时候，你才发现原来你的微信好友是这样的](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484710&amp;idx=1&amp;sn=cf17f2e87405ebffb20edd0ca0a7315b&amp;chksm=fc8bbdb7cbfc34a1389e17d4485b677d5ada497a404dc8f14107914e50382c640e7bd3cb93a4&amp;scene=27#wechat_redirect) \n- [高考要来了，扒一扒历年高考录取分数来压压惊](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484745&amp;idx=1&amp;sn=24362e73605d30e06ebe05d1fe7225f2&amp;chksm=fc8bbdd8cbfc34ce100b9461f46c8a1c0008172f101b34b38e146f56323bc40bbd373a127ee8&amp;scene=27#wechat_redirect) \n- [随着身子的一阵颤抖，Python爬取抖音上的小姐姐突然变得索然无味](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485150&idx=1&sn=b813993925a1031d4e85eb8841ccdb37&scene=19#wechat_redirect)\n- [使用 scrapy 爬取 stackoverflow 上的所有 Python 问答](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485754&idx=1&sn=3e52aa0ac13f3a23c6dee2b75424f0f5&scene=19&token=464856977&lang=zh_CN#wechat_redirect)\n- [爬取周杰伦新歌《说好不哭》的所有评论，然后生成词云图](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485571&idx=1&sn=094517114b22a4684988008aecab2639&scene=19&token=464856977&lang=zh_CN#wechat_redirect)\n- [我整来了几台服务器，就是为了给你演示一下分布式爬虫的整个过程](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485863&idx=1&sn=34f9fb196c77dffdcce4a610b622270d&scene=19&token=464856977&lang=zh_CN#wechat_redirect)\n\n\n## 爬虫实例源代码\n\n图文教程 | 相关源码\n---- | ---\n[1、爬取当当网 Top 500 本五星好评书籍](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247484142&idx=1&sn=d4893c734e44a16db871f7904910bdcb&scene=19#wechat_redirect) | [源码](https://github.com/wistbean/learn_python3_spider/blob/master/dangdang_top_500.py)\n[2、爬取豆瓣最受欢迎的250部电影慢慢看](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247484156&idx=1&sn=dc732b380d162f39ff63d55cac5a0dd6&scene=19#wechat_redirect) | [源码](https://github.com/wistbean/learn_python3_spider/blob/master/douban_top_250_books.py)   \n[3、爬取b站上的NBA形象大使蔡徐坤和他的球友们](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247484182&idx=1&sn=1b1c0058e402a9dc559d16ab37a30e98&scene=19#wechat_redirect) | [源码](https://github.com/wistbean/learn_python3_spider/blob/master/ikun_basketball.py)\n[4、用多线程秒爬那些万恶的妹纸们，纸巾呢？](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247484225&idx=1&sn=077fba66aaa1d806193403ce51e75279&scene=19#wechat_redirect) | [源码](https://github.com/wistbean/learn_python3_spider/blob/master/meizitu.py)\n[5、自动识别b站滑动验证码](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247484321&idx=1&sn=4bc73324acfacda7d3bc82120b19d11a&scene=19#wechat_redirect) | [源码](https://github.com/wistbean/learn_python3_spider/blob/master/fuck_bilibili_captcha.py)\n[6、搞事情了，用 Appium 爬取你的微信朋友圈](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247484386&idx=1&sn=7f0545f27f095f20d69deedfa9f606a1&scene=19#wechat_redirect) | [源码](https://github.com/wistbean/learn_python3_spider/blob/master/wechat_moment.py)\n[7、scrapy爬取糗事百科段子到MongoDB（上）](https://fxxkpython.com/python3-web-fxxkpython-spider-tutorial-29.html)、[scrapy爬取糗事百科段子到MongoDB(下)](https://fxxkpython.com/python3-web-fxxkpython-spider-tutorial-30.html) | [源码](https://github.com/wistbean/learn_python3_spider/tree/master/qiushibaike) \n[8、python爬取 20w 表情包之后，从此你就成为了微信斗图届的高手](https://fxxkpython.com/python-pa-qu-biao-qing-bao.html) | [源码](https://github.com/wistbean/learn_python3_spider/tree/master/biaoqingbao)\n[9、python爬取你喜欢的公众号的所有原创文章，然后搞成PDF慢慢看](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247484657&idx=1&sn=998bfcce6cd22b7fedff29e68a46fe3f&scene=19&token=464856977&lang=zh_CN#wechat_redirect) | [源码](https://github.com/wistbean/learn_python3_spider/blob/master/wechat_public_account.py)\n[10、当 python 遇到你的微信的时候，你才发现原来你的微信好友是这样的](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247484710&idx=1&sn=cf17f2e87405ebffb20edd0ca0a7315b&scene=19&token=464856977&lang=zh_CN#wechat_redirect) | [--](https://wistbean.github.io)\n> 未完待续...\n\n## 爬虫技巧\n- [给你们说几点鲜有人知的爬虫技巧](https://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&mid=2247485129&idx=1&sn=56a9aecafa73162c639a873b5bbdf534&chksm=fc8bbe58cbfc374e5c033a37a82b94e8391855d85f1db26975579ddb3cf0882f1157e37f224c&token=2111372640&lang=zh_CN#rd)\n\n\n## python爬虫段子\n\n- [网站维护人员：真的求求你们了，不要再来爬取了！！](https://vip.fxxkpython.com/?p=4679)\n\n## python相关\n- [python如何赚钱？ python爬虫如何进阶？ python就业？ 如何快速入门python？ .....](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484770&amp;idx=1&amp;sn=16427865c7b2785594acfbcf4505e26f&amp;chksm=fc8bbdf3cbfc34e5856dd36dd825f9b89b05a4ab3def08dac48b760771e4ee0454fdf9ddee72&amp;scene=27#wechat_redirect) \n- [ 如何自学 Python 高效一些](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484608&amp;idx=1&amp;sn=0ebde7cbfea6e42e9e8e316bbec35b2b&amp;chksm=fc8bbc51cbfc35475daa15a026c44727bc7954bd722b24870eab567ef937a8f175369c546962&amp;scene=27#wechat_redirect) \n- [python教程资源](https://wistbean.github.io/categories/python/)\n- [吐血分享这两个爬虫用到的 Chrome 牛逼插件](http://mp.weixin.qq.com/s?__biz=MzU2ODYzNTkwMg==&amp;mid=2247484859&amp;idx=1&amp;sn=b5f91ab1dc027d06e34dea1b37091b34&amp;chksm=fc8bbd2acbfc343ce3123fb3e3aec1fa1e34c96b15e998a34e01a75f4ca90b0089e1692f5a31&amp;scene=27#wechat_redirect) \n\n## 公众号获取 Python 相关帅书\n\n微信搜索id：fxxkpython\n名称：学习 Python 的正确姿势\n\n<a href=\"https://fxxkpython.com\"><img src=\"https://user-images.githubusercontent.com/11805948/156555426-44ad6d54-4b9b-47bb-b6c9-44b0f7a0609b.JPG\" align=\"left\" height=\"250\" width=\"250\"></a>\n\n\n*进去发送「帅书」即可领取。*\n\n\n\n## Python 视频号\n\n\n<a href=\"https://fxxkpython.com\"><img src=\"https://user-images.githubusercontent.com/11805948/148672594-3beb1694-e4d8-4d50-bc30-db0495379ea3.JPG\" align=\"left\" height=\"250\" width=\"250\"></a>\n\n## 通往Python高手之路\n小帅b手把手带你：[通往Python高手之路](http://vip.fxxkpython.com/?page_id=18)\n\n\n\n\n"
        },
        {
          "name": "biaoqingbao",
          "type": "tree",
          "content": null
        },
        {
          "name": "dangdang_top_500.py",
          "type": "blob",
          "size": 1.4228515625,
          "content": "import requests\nimport re\nimport json\n\n\ndef request_dandan(url):\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            return response.text\n    except requests.RequestException as e:\n        print(e)\n        return None\n\n\ndef parse_result(html):\n    pattern = re.compile(\n        '<li.*?list_num.*?(\\d+)\\.</div>.*?<img src=\"(.*?)\".*?class=\"name\".*?title=\"(.*?)\">.*?class=\"star\">.*?class=\"tuijian\">(.*?)</span>.*?class=\"publisher_info\">.*?target=\"_blank\">(.*?)</a>.*?class=\"biaosheng\">.*?<span>(.*?)</span></div>.*?<p><span class=\"price_n\">(.*?)</span>.*?</li>', re.S)\n    items = re.findall(pattern, html)\n\n    for item in items:\n        yield {\n            'range': item[0],\n            'image': item[1],\n            'title': item[2],\n            'recommend': item[3],\n            'author': item[4],\n            'times': item[5],\n            'price': item[6]\n        }\n\n\ndef write_item_to_file(item):\n    print('开始写入数据 ====> ' + str(item))\n    with open('book.txt', 'a', encoding='UTF-8') as f:\n        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n\n\ndef main(page):\n    url = 'http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-' + str(page)\n    html = request_dandan(url)\n    items = parse_result(html)  # 解析过滤我们想要的信息\n    for item in items:\n        write_item_to_file(item)\n\n\nif __name__ == \"__main__\":\n    for i in range(1, 26):\n        main(i)\n"
        },
        {
          "name": "douban_top_250_books.py",
          "type": "blob",
          "size": 2.09375,
          "content": "import requests\nfrom bs4 import BeautifulSoup\nimport xlwt\n\n\ndef request_douban(url):\nheaders = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '\n                      'Chrome/88.0.4324.146 Safari/537.36',\n    }\n\n    try:\n        response = requests.get(url=url, headers=headers)\n        if response.status_code == 200:\n            return response.text\n    except requests.RequestException:\n        return None\n\n\nbook = xlwt.Workbook(encoding='utf-8', style_compression=0)\n\nsheet = book.add_sheet('豆瓣电影Top250', cell_overwrite_ok=True)\nsheet.write(0, 0, '名称')\nsheet.write(0, 1, '图片')\nsheet.write(0, 2, '排名')\nsheet.write(0, 3, '评分')\nsheet.write(0, 4, '作者')\nsheet.write(0, 5, '简介')\n\nn = 1\n\n\ndef save_to_excel(soup):\n    list = soup.find(class_='grid_view').find_all('li')\n\n    for item in list:\n        item_name = item.find(class_='title').string\n        item_img = item.find('a').find('img').get('src')\n        item_index = item.find(class_='').string\n        item_score = item.find(class_='rating_num').string\n        item_author = item.find('p').text\n        if item.find(class_='inq') is not None:\n            item_intr = item.find(class_='inq').string\n        else:\n            item_intr = 'NOT AVAILABLE'        \n\n        # print('爬取电影：' + item_index + ' | ' + item_name +' | ' + item_img +' | ' + item_score +' | ' + item_author +' | ' + item_intr )\n        print('爬取电影：' + item_index + ' | ' + item_name + ' | ' + item_score + ' | ' + item_intr)\n\n        global n\n\n        sheet.write(n, 0, item_name)\n        sheet.write(n, 1, item_img)\n        sheet.write(n, 2, item_index)\n        sheet.write(n, 3, item_score)\n        sheet.write(n, 4, item_author)\n        sheet.write(n, 5, item_intr)\n\n        n = n + 1\n\n\ndef main(page):\n    url = 'https://movie.douban.com/top250?start=' + str(page * 25) + '&filter='\n    html = request_douban(url)\n    soup = BeautifulSoup(html, 'lxml')\n    save_to_excel(soup)\n\n\nif __name__ == '__main__':\n\n    for i in range(0, 10):\n        main(i)\n\nbook.save(u'豆瓣最受欢迎的250部电影.xlsx')\n"
        },
        {
          "name": "douban_top_250_books_mul_process.py",
          "type": "blob",
          "size": 2.607421875,
          "content": "import requests\nfrom bs4 import BeautifulSoup\nimport xlwt\nimport multiprocessing\nimport time\nimport sys\n\ndef request_douban(url):\n    try:\n        response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})\n        if response.status_code == 200:\n            return response.text\n    except requests.RequestException:\n        return None\n\ndef main(url):\n    sys.setrecursionlimit(1000000)\n    data = []\n    html = request_douban(url)\n    # soup = BeautifulSoup(html, 'lxml')\n    soup = BeautifulSoup(html, 'html.parser')\n    list = soup.find(class_='grid_view').find_all('li')\n    for item in list:\n        item_name = item.find(class_='title').string\n        item_img = item.find('a').find('img').get('src')\n        item_index = item.find(class_='').string\n        item_score = item.find(class_='rating_num').string\n        item_author = item.find('p').text\n        item_intr = ''\n        if (item.find(class_='inq') != None):\n            item_intr = item.find(class_='inq').string\n        print('爬取电影：' + item_index + ' | ' + item_name + ' | ' + item_score + ' | ' + item_intr)\n        item = {\n            'item_index': item_index,\n            'item_name': item_name,\n            'item_score': item_score,\n            'item_intr': item_intr,\n            'item_img': item_img,\n            'item_author': item_author\n        }\n        data.append(item)\n    return data\n    \nif __name__ == '__main__':\n    startTime = time.time()\n    data = []\n    urls = []\n    pool = multiprocessing.Pool(multiprocessing.cpu_count()-1)\n    for i in range(0, 10):\n        url = 'https://movie.douban.com/top250?start=' + str(i * 25) + '&filter='\n        urls.append(url)\n    pool.map(main, urls)\n    for pageItem in pool.map(main, urls):\n        data.extend(pageItem)\n    book = xlwt.Workbook(encoding='utf-8', style_compression=0)\n    sheet = book.add_sheet('豆瓣电影Top250-test', cell_overwrite_ok=True)\n    sheet.write(0, 0, '名称')\n    sheet.write(0, 1, '图片')\n    sheet.write(0, 2, '排名')\n    sheet.write(0, 3, '评分')\n    sheet.write(0, 4, '作者')\n    sheet.write(0, 5, '简介')\n    for index,item in enumerate(data):\n        sheet.write(index+1, 0, item['item_name'])\n        sheet.write(index+1, 1, item['item_img'])\n        sheet.write(index+1, 2, item['item_index'])\n        sheet.write(index+1, 3, item['item_score'])\n        sheet.write(index+1, 4, item['item_author'])\n        sheet.write(index+1, 5, item['item_intr'])\n    book.save(u'豆瓣最受欢迎的250部电影-mul.xlsx')\n\n    endTime = time.time()\n    dtime = endTime - startTime\n    print(\"程序运行时间：%s s\" % dtime)  # 4.036666631698608 s"
        },
        {
          "name": "fuck_bilibili_captcha.py",
          "type": "blob",
          "size": 5.9423828125,
          "content": "import time\nimport requests\nfrom PIL import Image\nfrom selenium import webdriver\nfrom selenium.webdriver import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nimport re\nfrom io import BytesIO\n\ndriver = webdriver.Chrome('/usr/lib/chromium-browser/chromedriver')\nWAIT = WebDriverWait(driver, 10)\nurl = 'https://passport.bilibili.com/login'\n\n\ndef mergy_Image(image_file, location_list):\n    \"\"\"\n    将原始图片进行合成\n    :param image_file: 图片文件\n    :param location_list: 图片位置\n    :return: 合成新的图片\n    \"\"\"\n\n    # 存放上下部分的各个小块\n    upper_half_list = []\n    down_half_list = []\n\n    image = Image.open(image_file)\n\n    # 通过 y 的位置来判断是上半部分还是下半部分,然后切割\n    for location in location_list:\n        if location['y'] == -58:\n            # 间距为10，y：58-116\n            im = image.crop((abs(location['x']), 58, abs(location['x'])+10, 116))\n            upper_half_list.append(im)\n        if location['y'] == 0:\n            # 间距为10，y：0-58\n            im = image.crop((abs(location['x']), 0, abs(location['x']) + 10, 58))\n            down_half_list.append(im)\n\n    # 创建一张大小一样的图片\n    new_image = Image.new('RGB', (260, 116))\n\n    # 粘贴好上半部分 y坐标是从上到下（0-116）\n    offset = 0\n    for im in upper_half_list:\n        new_image.paste(im, (offset, 0))\n        offset += 10\n\n    # 粘贴好下半部分\n    offset = 0\n    for im in down_half_list:\n        new_image.paste(im, (offset, 58))\n        offset += 10\n\n    return new_image\n\n\ndef get_distance(bg_Image, fullbg_Image):\n\n    # 阈值\n    threshold = 200\n\n    print(bg_Image.size[0])\n    print(bg_Image.size[1])\n\n\n    for i in range(60, bg_Image.size[0]):\n        for j in range(bg_Image.size[1]):\n            bg_pix = bg_Image.getpixel((i, j))\n            fullbg_pix = fullbg_Image.getpixel((i, j))\n            r = abs(bg_pix[0] - fullbg_pix[0])\n            g = abs(bg_pix[1] - fullbg_pix[1])\n            b = abs(bg_pix[2] - fullbg_pix[2])\n\n            if r + g + b > threshold:\n               return i\n\n\n\n\ndef get_path(distance):\n        result = []\n        current = 0\n        mid = distance * 4 / 5\n        t = 0.2\n        v = 0\n        while current < (distance - 10):\n            if current < mid:\n                a = 2\n            else:\n                a = -3\n            v0 = v\n            v = v0 + a * t\n            s = v0 * t + 0.5 * a * t * t\n            current += s\n            result.append(round(s))\n        return result\n\n\ndef start_drag(driver, distance):\n\n    # 被妖怪吃掉了\n    # knob =  WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show\")))\n    # ActionChains(driver).click_and_hold(knob).perform()\n    # ActionChains(driver).move_by_offset(xoffset=distance, yoffset=0.1).perform()\n    # time.sleep(0.5)\n    # ActionChains(driver).release(knob).perform()\n\n    # 被妖怪吃掉了\n    # ActionChains(driver).drag_and_drop_by_offset(knob, distance-10, 0).perform()\n\n    knob = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show\")))\n    result = get_path(distance)\n    ActionChains(driver).click_and_hold(knob).perform()\n\n    for x in result:\n        ActionChains(driver).move_by_offset(xoffset=x, yoffset=0).perform()\n\n    time.sleep(0.5)\n    ActionChains(driver).release(knob).perform()\n\n\ndef recognize_code(driver):\n    \"\"\"\n    识别滑动验证码\n    :param driver: selenium驱动\n    :return:\n    \"\"\"\n\n    bs = BeautifulSoup(driver.page_source,'lxml')\n    # 找到背景图片和缺口图片的div\n    bg_div = bs.find_all(class_='gt_cut_bg_slice')\n    fullbg_div = bs.find_all(class_='gt_cut_fullbg_slice')\n\n    # 获取缺口背景图片url\n    bg_url = re.findall('background-image:\\surl\\(\"(.*?)\"\\)',bg_div[0].get('style'))\n    # 获取背景图片url\n    fullbg_url = re.findall('background-image:\\surl\\(\"(.*?)\"\\)',fullbg_div[0].get('style'))\n\n    # 存放每个合成缺口背景图片的位置\n    bg_location_list = []\n    # 存放每个合成背景图片的位置\n    fullbg_location_list = []\n\n    for bg in bg_div:\n        location = {}\n        location['x'] = int(re.findall('background-position:\\s(.*?)px\\s(.*?)px;', bg.get('style'))[0][0])\n        location['y'] = int(re.findall('background-position:\\s(.*?)px\\s(.*?)px;', bg.get('style'))[0][1])\n        bg_location_list.append(location)\n\n    for fullbg in fullbg_div:\n        location = {}\n        location['x'] = int(re.findall('background-position:\\s(.*?)px\\s(.*?)px;', fullbg.get('style'))[0][0])\n        location['y'] = int(re.findall('background-position:\\s(.*?)px\\s(.*?)px;', fullbg.get('style'))[0][1])\n        fullbg_location_list.append(location)\n\n    print(bg_location_list)\n    print(fullbg_location_list)\n\n    # 将图片格式存为 jpg 格式\n    bg_url = bg_url[0].replace('webp', 'jpg')\n    fullbg_url = fullbg_url[0].replace('webp', 'jpg')\n    # print(bg_url)\n    # print(fullbg_url)\n\n    # 下载图片\n    bg_image = requests.get(bg_url).content\n    fullbg_image = requests.get(fullbg_url).content\n    print('完成图片下载')\n\n    # 写入图片\n    bg_image_file = BytesIO(bg_image)\n    fullbg_image_file = BytesIO(fullbg_image)\n\n    # 合成图片\n    bg_Image = mergy_Image(bg_image_file, bg_location_list)\n    fullbg_Image = mergy_Image(fullbg_image_file, fullbg_location_list)\n    # bg_Image.show()\n    # fullbg_Image.show()\n\n    # 计算缺口偏移距离\n    distance = get_distance(bg_Image, fullbg_Image)\n    print('得到距离：%s' % str(distance))\n\n    start_drag(driver, distance)\n\n\n\n\nif __name__ == '__main__':\n\n    # 获取滑块按钮\n    driver.get(url)\n    slider = WAIT.until(EC.element_to_be_clickable(\n        (By.CSS_SELECTOR, \"#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show\")))\n\n    recognize_code(driver)\n\n\n    # driver.close()\n\n"
        },
        {
          "name": "ikun_basketball.py",
          "type": "blob",
          "size": 3.896484375,
          "content": "# coding=utf-8\n\n# 最新版的selenium(4.x.x)已经不支持PhantomJS。如要用PhantomJS，可用旧版本selenium。如pip install selenium==3.8.0。\nfrom selenium import webdriver\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nimport xlwt\n\n# browser = webdriver.PhantomJS()\nbrowser = webdriver.Chrome()\nWAIT = WebDriverWait(browser, 10)\nbrowser.set_window_size(1400, 900)\n\nbook = xlwt.Workbook(encoding='utf-8', style_compression=0)\n\nsheet = book.add_sheet('蔡徐坤篮球', cell_overwrite_ok=True)\nsheet.write(0, 0, '名称')\nsheet.write(0, 1, '地址')\nsheet.write(0, 2, '描述')\nsheet.write(0, 3, '观看次数')\nsheet.write(0, 4, '弹幕数')\nsheet.write(0, 5, '发布时间')\n\nn = 1\n\n\ndef search():\n    try:\n        print('开始访问b站....')\n        browser.get(\"https://www.bilibili.com/\")\n\n        # 被那个破登录遮住了\n        # index = WAIT.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#primary_menu > ul > li.home > a\")))\n        # index.click()\n\n        input = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#nav_searchform > input\")))\n        submit = WAIT.until(EC.element_to_be_clickable(\n            (By.XPATH, '/html/body/div[2]/div/div[1]/div[1]/div/div[2]/div/form/div/button')))\n\n        input.send_keys('蔡徐坤 篮球')\n        submit.click()\n\n        # 跳转到新的窗口\n        print('跳转到新窗口')\n        all_h = browser.window_handles\n        browser.switch_to.window(all_h[1])\n        get_source()\n\n        total = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR,\n                                                           \"#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.last > button\")))\n        return int(total.text)\n    except TimeoutException:\n        return search()\n\n\ndef next_page(page_num):\n    try:\n        print('获取下一页数据')\n        next_btn = WAIT.until(EC.element_to_be_clickable((By.CSS_SELECTOR,\n                                                          '#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.next > button')))\n        next_btn.click()\n        WAIT.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR,\n                                                     '#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.active > button'),\n                                                    str(page_num)))\n        get_source()\n    except TimeoutException:\n        browser.refresh()\n        return next_page(page_num)\n\n\ndef save_to_excel(soup):\n    list = soup.find(class_='video-list clearfix').find_all(class_='video-item matrix')\n\n    for item in list:\n        item_title = item.find('a').get('title')\n        item_link = item.find('a').get('href')\n        item_dec = item.find(class_='des hide').text\n        item_view = item.find(class_='so-icon watch-num').text\n        item_biubiu = item.find(class_='so-icon hide').text\n        item_date = item.find(class_='so-icon time').text\n\n        print('爬取：' + item_title)\n\n        global n\n\n        sheet.write(n, 0, item_title)\n        sheet.write(n, 1, item_link)\n        sheet.write(n, 2, item_dec)\n        sheet.write(n, 3, item_view)\n        sheet.write(n, 4, item_biubiu)\n        sheet.write(n, 5, item_date)\n\n        n = n + 1\n\n\ndef get_source():\n    WAIT.until(EC.presence_of_element_located(\n        (By.CSS_SELECTOR, '#all-list > div.flow-loader > div.filter-wrap')))\n\n    html = browser.page_source\n    soup = BeautifulSoup(html, 'lxml')\n    print('到这')\n\n    save_to_excel(soup)\n\n\ndef main():\n    try:\n        total = search()\n        print(total)\n\n        for i in range(2, int(total + 1)):\n            next_page(i)\n\n    finally:\n        browser.close()\n\n\nif __name__ == '__main__':\n    main()\n    book.save('蔡徐坤篮球.xlsx')\n"
        },
        {
          "name": "meizitu.py",
          "type": "blob",
          "size": 2.5458984375,
          "content": "# encoding = utf-8\nimport concurrent\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef header(referer):\n\n    headers = {\n        'Host': 'i.meizitu.net',\n        'Pragma': 'no-cache',\n        'Accept-Encoding': 'gzip, deflate',\n        'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6',\n        'Cache-Control': 'no-cache',\n        'Connection': 'keep-alive',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',\n        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',\n        'Referer': '{}'.format(referer),\n    }\n\n    return headers\n\n\ndef request_page(url):\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            return response.text\n    except requests.RequestException:\n        return None\n\n\ndef get_page_urls():\n\n    for i in range(1, 2):\n        baseurl = 'https://www.mzitu.com/page/{}'.format(i)\n        html = request_page(baseurl)\n        soup = BeautifulSoup(html, 'lxml')\n        elements = soup.find(class_='postlist').find_all('li')\n        urls = []\n        for item in elements:\n            url = item.find('span').find('a').get('href')\n            print('页面链接：%s' % url)\n            urls.append(url)\n\n    return urls\n\n\ndef download_Pic(title, image_list):\n    # 新建文件夹\n    os.mkdir(title)\n    j = 1\n    # 下载图片\n    for item in image_list:\n        filename = '%s/%s.jpg' % (title, str(j))\n        print('downloading....%s : NO.%s' % (title, str(j)))\n        with open(filename, 'wb') as f:\n            img = requests.get(item, headers=header(item)).content\n            f.write(img)\n        j += 1\n\ndef download(url):\n    html = request_page(url)\n    soup = BeautifulSoup(html, 'lxml')\n    total = soup.find(class_='pagenavi').find_all('a')[-2].find('span').string\n    title = soup.find('h2').string\n    image_list = []\n\n    for i in range(int(total)):\n        html = request_page(url + '/%s' % (i + 1))\n        soup = BeautifulSoup(html, 'lxml')\n        img_url = soup.find('img').get('src')\n        image_list.append(img_url)\n\n    download_Pic(title, image_list)\n\n\ndef download_all_images(list_page_urls):\n    # 获取每一个详情妹纸\n    # works = len(list_page_urls)\n    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as exector:\n        for url in list_page_urls:\n            exector.submit(download, url)\n\n\nif __name__ == '__main__':\n    # 获取每一页的链接和名称\n    list_page_urls = get_page_urls()\n    download_all_images(list_page_urls)"
        },
        {
          "name": "qiushibaike",
          "type": "tree",
          "content": null
        },
        {
          "name": "stackoverflow",
          "type": "tree",
          "content": null
        },
        {
          "name": "wechat_moment.py",
          "type": "blob",
          "size": 4.0234375,
          "content": "import time\n\nfrom appium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nclass Wechat_Moment():\n    def __init__(self):\n        desired_caps = {}\n        desired_caps['platformName'] = 'Android'\n        desired_caps['platformVersion'] = '5.1'\n        desired_caps['deviceName'] = '88CKBM622PAM'\n        desired_caps['appPackage'] = 'com.tencent.mm'\n        desired_caps['appActivity'] = '.ui.LauncherUI'\n\n        # 定义在朋友圈的时候滑动位置\n        self.start_x = 300\n        self.start_y = 800\n        self.end_x = 300\n        self.end_y = 300\n\n        # 启动微信\n        self.driver = webdriver.Remote('http://localhost:4723/wd/hub', desired_caps)\n        # 设置等待\n        self.wait = WebDriverWait(self.driver, 300)\n        print('微信启动...')\n\n\n    def login(self):\n        # 获取到登录按钮后点击\n        login_btn = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/e4g\")))\n        login_btn.click()\n        # 获取使用微信号登录按钮\n        change_login_btn = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/cou\")))\n        change_login_btn.click()\n        # 获取输入账号元素并输入\n        account = self.wait.until(EC.presence_of_element_located((By.XPATH, '//*[@resource-id=\"com.tencent.mm:id/cos\"]/android.widget.EditText')))\n        account.send_keys(\"xxxxxxxx\")\n        # 获取密码元素并输入\n        password = self.wait.until(EC.presence_of_element_located((By.XPATH,  '//*[@resource-id=\"com.tencent.mm:id/cot\"]/android.widget.EditText')))\n        password.send_keys(\"xxxxxx\")\n        # 登录\n        login = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/cov\")))\n        login.click()\n        # 点击去掉通讯录提示框\n        no_btn = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/az9\")))\n        no_btn.click()\n        print('登录成功...')\n\n\n    def find_xiaoshuaib(self):\n        # 获取到搜索按钮后点击\n        search_btn = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/iq\")))\n        # 等搜索建立索引再点击\n        time.sleep(10)\n        search_btn.click()\n        # 获取搜索框并输入\n        search_input = self.wait.until(EC.presence_of_element_located((By.ID, \"com.tencent.mm:id/kh\")))\n        search_input.send_keys(\"wistbean\")\n        print('搜索小帅b...')\n        # 点击头像进入\n        xiaoshuaib_btn = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/py\")))\n        xiaoshuaib_btn.click()\n        # 点击右上角...进入\n        menu_btn = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/jy\")))\n        menu_btn.click()\n        # 再点击头像\n        icon_btn = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/e0c\")))\n        icon_btn.click()\n        # 点击朋友圈\n        moment_btn = self.wait.until(EC.element_to_be_clickable((By.ID, \"com.tencent.mm:id/d86\")))\n        moment_btn.click()\n        print('进入朋友圈...')\n\n    def get_data(self):\n        while True:\n            # 获取 ListView\n            items = self.wait.until(EC.presence_of_all_elements_located((By.ID, 'com.tencent.mm:id/eew')))\n            # 滑动\n            self.driver.swipe(self.start_x, self.start_y, self.end_x, self.end_y, 2000)\n            #遍历获取每个List数据\n            for item in items:\n                moment_text = item.find_element_by_id('com.tencent.mm:id/kt').text\n                day_text = item.find_element_by_id('com.tencent.mm:id/eke').text\n                month_text = item.find_element_by_id('com.tencent.mm:id/ekf').text\n                print('抓取到小帅b朋友圈数据： %s' % moment_text)\n                print('抓取到小帅b发布时间： %s月%s' % (month_text, day_text))\n\nif __name__ == '__main__':\n    wc_moment = Wechat_Moment()\n    wc_moment.login()\n    wc_moment.find_xiaoshuaib()\n    wc_moment.get_data()\n\n\n\n\n\n\n\n\n\n\n\n"
        },
        {
          "name": "wechat_public_account.py",
          "type": "blob",
          "size": 1.9521484375,
          "content": "#-*- coding:UTF-8 -*-\nimport json\nimport time\nimport pdfkit\n\nimport requests\n\nbase_url = 'https://mp.weixin.qq.com/mp/profile_ext'\n\n\n# 这些信息不能抄我的，要用你自己的才有效\nheaders = {\n    'Connection': 'keep - alive',\n    'Accept': '* / *',\n    'User-Agent': '写你自己的',\n    'Referer': '写你自己的',\n    'Accept-Encoding': 'br, gzip, deflate'\n}\n\ncookies = {\n    'devicetype': 'iOS12.2',\n    'lang': 'zh_CN',\n    'pass_ticket': '写你自己的',\n    'version': '1700042b',\n    'wap_sid2': '写你自己的',\n    'wxuin': '3340537333'\n}\n\n\n\ndef get_params(offset):\n    params = {\n        'action': 'getmsg',\n        '__biz': '写你自己的',\n        'f': 'json',\n        'offset': '{}'.format(offset),\n        'count': '10',\n        'is_ok': '1',\n        'scene': '126',\n        'uin': '777',\n        'key': '777',\n        'pass_ticket': '写你自己的',\n        'appmsg_token': '写你自己的',\n        'x5': '0',\n        'f': 'json',\n    }\n\n    return params\n\n\ndef get_list_data(offset):\n    res = requests.get(base_url, headers=headers, params=get_params(offset), cookies=cookies)\n    data = json.loads(res.text)\n    can_msg_continue = data['can_msg_continue']\n    next_offset = data['next_offset']\n\n    general_msg_list = data['general_msg_list']\n    list_data = json.loads(general_msg_list)['list']\n\n    for data in list_data:\n        try:\n            if data['app_msg_ext_info']['copyright_stat'] == 11:\n                msg_info = data['app_msg_ext_info']\n                title = msg_info['title']\n                content_url = msg_info['content_url']\n                # 自己定义存储路径\n                pdfkit.from_url(content_url, '/home/wistbean/wechat_article/'+title+'.pdf')\n                print('获取到原创文章：%s ： %s' % (title, content_url))\n        except:\n            print('不是图文')\n\n    if can_msg_continue == 1:\n        time.sleep(1)\n        get_list_data(next_offset)\n\n\nif __name__ == '__main__':\n    get_list_data(0)"
        }
      ]
    }
  ]
}