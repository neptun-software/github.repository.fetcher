{
  "metadata": {
    "timestamp": 1736561377786,
    "page": 417,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "kedro-org/kedro",
      "stars": 10090,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.1484375,
          "content": "\ntmp/\n# CMake\ncmake-build-debug/\n\n## File-based project format:\n*.iws\n\n## Plugin-specific files:\n\n# IntelliJ\n.idea/\n*.iml\nout/\n\n### macOS\n*.DS_Store\n.AppleDouble\n.LSOverride\n.Trashes\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n### Python template\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ntest-output.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n.static_storage/\n.media/\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.envrc\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.cenv\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n/kedro/framework/html\n\n# Sphinx documentation\n# Additional files created by sphinx.ext.autosummary\n# Some of them are actually tracked to control the output\n/docs/source/api/kedro.*\n\n# mypy\n.mypy_cache/\n\n# Visual Studio Code\n.vscode/\n# end to end tests assets\nkedro.db\n\n# Vim\n*~\n.*.swo\n.*.swp\n\n# Prettier\n.prettierignore\n\n.pytest_cache/\nkedro/html\ndocs/tmp-build-artifacts\ndocs/build\ndocs/temp\ndocs/node_modules\ndocs/source/04_user_guide/source/.ipynb\ntests/template/fake_project/\n\ndefault.profraw\npackage-lock.json\n\n# Kedro-Datasets plugin\nkedro/datasets/*\n"
        },
        {
          "name": ".gitpod.yml",
          "type": "blob",
          "size": 1.03125,
          "content": "image: gitpod/workspace-python-3.11\n\ntasks:\n  - name: kedro\n\n    init: |\n      make sign-off\n      pip install uv\n      uv venv\n      source .venv/bin/activate\n      uv pip install -e /workspace/kedro[test]\n      cd /workspace\n      kedro new --name project -s spaceflights-pandas --checkout main\n      uv pip install -e project\n      cd /workspace/kedro\n      pre-commit install --install-hooks\n\n    command: |\n      clear\n      source .venv/bin/activate\n      kedro info\n\n\ngithub:\n  prebuilds:\n    # enable for the master/default branch (defaults to true)\n    master: true\n    # enable for all branches in this repo (defaults to false)\n    branches: true\n    # enable for pull requests coming from this repo (defaults to true)\n    pullRequests: true\n    # enable for pull requests coming from forks (defaults to false)\n    pullRequestsFromForks: true\n    # add a \"Review in Gitpod\" button as a comment to pull requests (defaults to true)\n    addComment: false\n    # add a \"Review in Gitpod\" button to pull requests (defaults to false)\n    addBadge: true\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.9248046875,
          "content": "# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\n\ndefault_stages: [pre-commit, manual]\n\nrepos:\n    - repo: https://github.com/astral-sh/ruff-pre-commit\n      rev: v0.7.1\n      hooks:\n        - id: ruff\n          name: \"ruff on kedro/, tests/ and docs/\"\n          args: [\"--fix\", \"--exit-non-zero-on-fix\"]\n          exclude: \"^kedro/templates/|^features/steps/test_starter/\"\n        - id: ruff-format\n          name: \"ruff format on kedro/, features/ and tests/\"\n          files: \"^kedro/|^features/|^tests/\"\n          exclude: \"^features/steps/test_starter|^kedro/templates/\"\n\n    - repo: https://github.com/pre-commit/pre-commit-hooks\n      rev: v4.6.0\n      hooks:\n          - id: trailing-whitespace\n          - id: end-of-file-fixer\n          - id: check-yaml # Checks yaml files for parseable syntax.\n            exclude: \"^kedro/templates/|^features/steps/test_starter/\"\n          - id: check-json # Checks json files for parseable syntax.\n          - id: check-added-large-files\n          - id: check-case-conflict # Check for files that would conflict in case-insensitive filesystems\n          - id: check-merge-conflict # Check for files that contain merge conflict strings.\n          - id: debug-statements # Check for debugger imports and py37+ `breakpoint()` calls in python source.\n            exclude: \"^kedro/templates/|^features/steps/test_starter/\"\n          - id: requirements-txt-fixer # Sorts entries in requirements.txt\n            exclude: \"^kedro/templates/|^features/steps/test_starter/\"\n\n    - repo: local\n      hooks:\n          - id: imports\n            name: \"Import Linter\"\n            language: system\n            pass_filenames: false\n            entry: lint-imports\n\n    - repo: https://github.com/Yelp/detect-secrets\n      rev: v1.5.0\n      hooks:\n          - id: detect-secrets\n            args: ['--baseline', '.secrets.baseline']\n            exclude: ^features/steps/test_starter\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.75,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.9\"\n    nodejs: \"19\"\n  jobs:\n    post_checkout:\n      - git fetch --unshallow || true\n    pre_build:\n      - pip freeze\n      - python -m sphinx -WETan -j auto -D language=en -b linkcheck -d _build/doctrees docs/source _build/linkcheck\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  builder: html\n  configuration: docs/source/conf.py\n  fail_on_warning: true\n\npython:\n  install:\n    - method: pip\n      path: .\n      extra_requirements:\n        - docs\n\nsearch:\n  ranking:\n    # Push API docs in the /api directory down the ranking\n    api/*: -5\n"
        },
        {
          "name": ".secrets.baseline",
          "type": "blob",
          "size": 5.478515625,
          "content": "{\n  \"version\": \"1.5.0\",\n  \"plugins_used\": [\n    {\n      \"name\": \"ArtifactoryDetector\"\n    },\n    {\n      \"name\": \"AWSKeyDetector\"\n    },\n    {\n      \"name\": \"AzureStorageKeyDetector\"\n    },\n    {\n      \"name\": \"Base64HighEntropyString\",\n      \"limit\": 4.5\n    },\n    {\n      \"name\": \"BasicAuthDetector\"\n    },\n    {\n      \"name\": \"CloudantDetector\"\n    },\n    {\n      \"name\": \"DiscordBotTokenDetector\"\n    },\n    {\n      \"name\": \"GitHubTokenDetector\"\n    },\n    {\n      \"name\": \"GitLabTokenDetector\"\n    },\n    {\n      \"name\": \"HexHighEntropyString\",\n      \"limit\": 3.0\n    },\n    {\n      \"name\": \"IbmCloudIamDetector\"\n    },\n    {\n      \"name\": \"IbmCosHmacDetector\"\n    },\n    {\n      \"name\": \"IPPublicDetector\"\n    },\n    {\n      \"name\": \"JwtTokenDetector\"\n    },\n    {\n      \"name\": \"KeywordDetector\",\n      \"keyword_exclude\": \"\"\n    },\n    {\n      \"name\": \"MailchimpDetector\"\n    },\n    {\n      \"name\": \"NpmDetector\"\n    },\n    {\n      \"name\": \"OpenAIDetector\"\n    },\n    {\n      \"name\": \"PrivateKeyDetector\"\n    },\n    {\n      \"name\": \"PypiTokenDetector\"\n    },\n    {\n      \"name\": \"SendGridDetector\"\n    },\n    {\n      \"name\": \"SlackDetector\"\n    },\n    {\n      \"name\": \"SoftlayerDetector\"\n    },\n    {\n      \"name\": \"SquareOAuthDetector\"\n    },\n    {\n      \"name\": \"StripeDetector\"\n    },\n    {\n      \"name\": \"TelegramBotTokenDetector\"\n    },\n    {\n      \"name\": \"TwilioKeyDetector\"\n    }\n  ],\n  \"filters_used\": [\n    {\n      \"path\": \"detect_secrets.filters.allowlist.is_line_allowlisted\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.common.is_ignored_due_to_verification_policies\",\n      \"min_level\": 2\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_indirect_reference\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_likely_id_string\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_lock_file\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_not_alphanumeric_string\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_potential_uuid\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_prefixed_with_dollar_sign\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_sequential_string\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_swagger_file\"\n    },\n    {\n      \"path\": \"detect_secrets.filters.heuristic.is_templated_secret\"\n    }\n  ],\n  \"results\": {\n    \"features/steps/test_starter/{{ cookiecutter.repo_name }}/conf/local/credentials.yml\": [\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"features/steps/test_starter/{{ cookiecutter.repo_name }}/conf/local/credentials.yml\",\n        \"hashed_secret\": \"a62f2225bf70bfaccbc7f1ef2a397836717377de\",\n        \"is_verified\": false,\n        \"line_number\": 8\n      },\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"features/steps/test_starter/{{ cookiecutter.repo_name }}/conf/local/credentials.yml\",\n        \"hashed_secret\": \"d033e22ae348aeb5660fc2140aec35850c4da997\",\n        \"is_verified\": false,\n        \"line_number\": 16\n      }\n    ],\n    \"kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml\": [\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml\",\n        \"hashed_secret\": \"e5e9fa1ba31ecd1ae84f75caaa474f3a663f05f4\",\n        \"is_verified\": false,\n        \"line_number\": 9\n      },\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml\",\n        \"hashed_secret\": \"d033e22ae348aeb5660fc2140aec35850c4da997\",\n        \"is_verified\": false,\n        \"line_number\": 18\n      }\n    ],\n    \"tests/config/test_omegaconf_config.py\": [\n      {\n        \"type\": \"Basic Auth Credentials\",\n        \"filename\": \"tests/config/test_omegaconf_config.py\",\n        \"hashed_secret\": \"9d4e1e23bd5b727046a9e3b4b7db57bd8d6ee684\",\n        \"is_verified\": false,\n        \"line_number\": 39\n      }\n    ],\n    \"tests/framework/context/test_context.py\": [\n      {\n        \"type\": \"Basic Auth Credentials\",\n        \"filename\": \"tests/framework/context/test_context.py\",\n        \"hashed_secret\": \"9d4e1e23bd5b727046a9e3b4b7db57bd8d6ee684\",\n        \"is_verified\": false,\n        \"line_number\": 63\n      }\n    ],\n    \"tests/io/conftest.py\": [\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"tests/io/conftest.py\",\n        \"hashed_secret\": \"adb5fabe51f5b45e83fdd91b71c92156fec4a63e\",\n        \"is_verified\": false,\n        \"line_number\": 71\n      },\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"tests/io/conftest.py\",\n        \"hashed_secret\": \"3c3b274d119ff5a5ec6c1e215c1cb794d9973ac1\",\n        \"is_verified\": false,\n        \"line_number\": 117\n      },\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"tests/io/conftest.py\",\n        \"hashed_secret\": \"15dd2c9ccec914f1470b4dccb45789844e49cf70\",\n        \"is_verified\": false,\n        \"line_number\": 131\n      }\n    ],\n    \"tests/io/test_data_catalog.py\": [\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"tests/io/test_data_catalog.py\",\n        \"hashed_secret\": \"15dd2c9ccec914f1470b4dccb45789844e49cf70\",\n        \"is_verified\": false,\n        \"line_number\": 529\n      }\n    ],\n    \"tests/io/test_kedro_data_catalog.py\": [\n      {\n        \"type\": \"Secret Keyword\",\n        \"filename\": \"tests/io/test_kedro_data_catalog.py\",\n        \"hashed_secret\": \"15dd2c9ccec914f1470b4dccb45789844e49cf70\",\n        \"is_verified\": false,\n        \"line_number\": 482\n      }\n    ]\n  },\n  \"generated_at\": \"2025-01-08T12:21:43Z\"\n}\n"
        },
        {
          "name": ".vale.ini",
          "type": "blob",
          "size": 0.1083984375,
          "content": "StylesPath = .github/styles\n\nMinAlertLevel = suggestion\n\n[*.md]\nBasedOnStyles = Vale, Kedro\nVale.Spelling = NO\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 1.181640625,
          "content": "cff-version: 1.2.0\nmessage: If you use this software, please cite it as below.\nauthors:\n- family-names: Alam\n  given-names: Sajid\n- family-names: Chan\n  given-names: Nok Lam\n- family-names: Couto\n  given-names: Laura\n- family-names: Dada\n  given-names: Yetunde\n  orcid: https://orcid.org/0000-0002-5273-954X\n- family-names: Danov\n  given-names: Ivan\n- family-names: Datta\n  given-names: Deepyaman\n- family-names: DeBold\n  given-names: Tynan\n- family-names: Gundaniya\n  given-names: Jitendra\n- family-names: Honor√©-Roug√©\n  given-names: Yolan\n- family-names: Kaiser\n  given-names: Stephanie\n- family-names: Kanchwala\n  given-names: Rashida\n- family-names: Katiyar\n  given-names: Ankita\n- family-names: Pilla\n  given-names: Ravi Kumar\n- family-names: Nguyen\n  given-names: Huong\n- family-names: Cano Rodr√≠guez\n  given-names: Juan Luis\n  orcid: https://orcid.org/0000-0002-2187-161X\n- family-names: Schwarzmann\n  given-names: Joel\n- family-names: Sorokin\n  given-names: Dmitry\n- family-names: Theisen\n  given-names: Merel\n- family-names: Zab≈Çocki\n  given-names: Marcin\n- family-names: Brugman\n  given-names: Simon\ntitle: Kedro\nversion: 0.19.10\ndate-released: 2024-11-26\nurl: https://github.com/kedro-org/kedro\n"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 0.0615234375,
          "content": "*              @merelcht\ndocs/          @yetudada @astrojuanlu\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.5244140625,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behaviour that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behaviour by participants include:\n\n* The use of sexualised language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehaviour and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behaviour.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviours that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behaviour may be\nreported by contacting the project team on [Slack](https://slack.kedro.org). All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n**Investigation Timeline:** The project team will make all reasonable efforts to initiate and conclude the investigation in a timely fashion. Depending on the type of investigation the steps and timeline for each investigation will vary.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.0126953125,
          "content": "# Introduction\n\nWe welcome any and all contributions to Kedro, at whatever level you can manage. Here are a few suggestions, but you are welcome to suggest anything else that you think improves the community for us all!\n\n## Join the community\n\nYou can find the Kedro community on our [Slack organisation](https://slack.kedro.org/), which is where we share news and announcements, and answer technical questions. You're welcome to post links to any articles or videos about Kedro that you create or find, such as how-tos, showcases, demos, blog posts or tutorials.\n\nWe also curate a [GitHub repo that lists content created by the Kedro community](https://github.com/kedro-org/awesome-kedro). If you've made something with Kedro, simply add it to the list with a PR!\n\n## Contribute to the project\n\nThere are quite a few ways to contribute to Kedro, such as answering questions about Kedro to help others, fixing a typo on the documentation, reporting a bug, reviewing pull requests or adding a feature.\n\nTake a look at some of our [contribution suggestions on the Kedro GitHub Wiki](https://github.com/kedro-org/kedro/wiki/Contribute-to-Kedro)!\n\n## Join the Technical Steering Committee\nKedro is an incubating project in [LF AI & Data](https://lfaidata.foundation/), a sub-organisation within the Linux Foundation that focuses on open innovation within the data and AI space.\n\nThe project is governed by a group of maintainers, known as the Technical Steering Committee (TSC); read more about the structure of our TSC in our [Technical Charter](./kedro_technical_charter.pdf).\n\nWe regularly invite community members to join the TSC and help define the future of the Kedro project. Read the [guidance on becoming a Kedro maintainer](https://docs.kedro.org/en/stable/contribution/technical_steering_committee.html) to understand the process of joining the TSC.\n\n## Code of conduct\n\nThe Kedro team pledges to foster and maintain a friendly community. We enforce a [Code of Conduct](./CODE_OF_CONDUCT.md) to ensure every Kedroid is welcomed and treated with respect.\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.224609375,
          "content": "include README.md\ninclude LICENSE.md\ninclude kedro/framework/project/default_logging.yml\ninclude kedro/framework/project/rich_logging.yml\ninclude kedro/ipython/*.png\ninclude kedro/ipython/*.svg\nrecursive-include kedro/templates *\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.7275390625,
          "content": "install:\n\tuv pip install --system -e .\n\nclean:\n\trm -rf build dist docs/build kedro/html pip-wheel-metadata .mypy_cache .pytest_cache features/steps/test_plugin/test_plugin.egg-info\n\tfind . -regex \".*/__pycache__\" -exec rm -rf {} +\n\tfind . -regex \".*\\.egg-info\" -exec rm -rf {} +\n\tpre-commit clean || true\n\nlint:\n\tpre-commit run -a --hook-stage manual $(hook)\n\tmypy kedro --strict --allow-any-generics --no-warn-unused-ignores\ntest:\n\tpytest --numprocesses 4 --dist loadfile\n\nshow-coverage:\n\tcoverage html --show-contexts || true\n\topen htmlcov/index.html\n\ne2e-tests:\n\tbehave --tags=-skip\n\ne2e-tests-fast: export BEHAVE_LOCAL_ENV=TRUE\ne2e-tests-fast:\n\tbehave --tags=-skip --no-capture\n\npip-compile:\n\tpip-compile -q -o -\n\nbuild-docs:\n\tuv pip install -e \".[docs]\"\n\t./docs/build-docs.sh \"docs\"\n\nshow-docs:\n\topen docs/build/html/index.html\n\nlinkcheck:\n\tuv pip install --system \"kedro[docs] @ .\"\n\t./docs/build-docs.sh \"linkcheck\"\n\npackage: clean install\n\tpython -m pip install build && python -m build\n\ninstall-test-requirements:\n\tpython -m pip install \"uv==0.4.29\"\n\tuv pip install --system \"kedro[test] @ .\"\n\ninstall-pre-commit:\n\tpre-commit install --install-hooks\n\nuninstall-pre-commit:\n\tpre-commit uninstall\n\nprint-python-env:\n\t@./tools/print_env.sh\n\ndatabricks-build:\n\tpython -m pip install build && python -m build\n\tpython ./tools/databricks_build.py\n\nsign-off:\n\techo \"git interpret-trailers --if-exists doNothing \\c\" >> .git/hooks/commit-msg\n\techo '--trailer \"Signed-off-by: $$(git config user.name) <$$(git config user.email)>\" \\c' >> .git/hooks/commit-msg\n\techo '--in-place \"$$1\"' >> .git/hooks/commit-msg\n\tchmod +x .git/hooks/commit-msg\n\nlanguage-lint: dir ?= docs\n\n# Pattern rule to allow \"make language-lint dir=doc/source/hooks>\" syntax\nlanguage-lint:\n\tvale $(dir)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.5107421875,
          "content": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-light.png\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-dark.png\">\n    <img src=\"https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-light.png\" alt=\"Kedro\">\n  </picture>\n</p>\n\n[![Python version](https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg)](https://pypi.org/project/kedro/)\n[![PyPI version](https://badge.fury.io/py/kedro.svg)](https://pypi.org/project/kedro/)\n[![Conda version](https://img.shields.io/conda/vn/conda-forge/kedro.svg)](https://anaconda.org/conda-forge/kedro)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/kedro-org/kedro/blob/main/LICENSE.md)\n[![Slack Organisation](https://img.shields.io/badge/slack-chat-blueviolet.svg?label=Kedro%20Slack&logo=slack)](https://slack.kedro.org)\n[![Slack Archive](https://img.shields.io/badge/slack-archive-blueviolet.svg?label=Kedro%20Slack%20)](https://linen-slack.kedro.org/)\n![GitHub Actions Workflow Status - Main](https://img.shields.io/github/actions/workflow/status/kedro-org/kedro/all-checks.yml?label=main)\n![GitHub Actions Workflow Status - Develop](https://img.shields.io/github/actions/workflow/status/kedro-org/kedro/all-checks.yml?branch=develop&label=develop)\n[![Documentation](https://readthedocs.org/projects/kedro/badge/?version=stable)](https://docs.kedro.org/)\n[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/6711/badge)](https://bestpractices.coreinfrastructure.org/projects/6711)\n[![Monthly downloads](https://static.pepy.tech/badge/kedro/month)](https://pepy.tech/project/kedro)\n[![Total downloads](https://static.pepy.tech/badge/kedro)](https://pepy.tech/project/kedro)\n\n[![Powered by Kedro](https://img.shields.io/badge/powered_by-kedro-ffc900?logo=kedro)](https://kedro.org)\n\n## What is Kedro?\n\nKedro is a toolbox for production-ready data science. It uses software engineering best practices to help you create data engineering and data science pipelines that are reproducible, maintainable, and modular. You can find out more at [kedro.org](https://kedro.org).\n\nKedro is an open-source Python framework hosted by the [LF AI & Data Foundation](https://lfaidata.foundation/).\n\n## How do I install Kedro?\n\nTo install Kedro from the Python Package Index (PyPI) run:\n\n```\npip install kedro\n```\n\nIt is also possible to install Kedro using `conda`:\n\n```\nconda install -c conda-forge kedro\n```\n\nOur [Get Started guide](https://docs.kedro.org/en/stable/get_started/install.html) contains full installation instructions, and includes how to set up Python virtual environments.\n\n### Installation from source\nTo access the latest Kedro version before its official release, install it from the `main` branch.\n```\npip install git+https://github.com/kedro-org/kedro@main\n```\n\n## What are the main features of Kedro?\n\n| Feature              | What is this?                                                                                                                                                                                                                                                                                                                                                                                      |\n| -------------------- |----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Project Template     | A standard, modifiable and easy-to-use project template based on [Cookiecutter Data Science](https://github.com/drivendata/cookiecutter-data-science/).                                                                                                                                                                                                                                            |\n| Data Catalog         | A series of lightweight data connectors used to save and load data across many different file formats and file systems, including local and network file systems, cloud object stores, and HDFS. The Data Catalog also includes data and model versioning for file-based systems.                                                                                                                  |\n| Pipeline Abstraction | Automatic resolution of dependencies between pure Python functions and data pipeline visualisation using [Kedro-Viz](https://github.com/kedro-org/kedro-viz).                                                                                                                                                                                                                                      |\n| Coding Standards     | Test-driven development using [`pytest`](https://github.com/pytest-dev/pytest), produce well-documented code using [Sphinx](http://www.sphinx-doc.org/en/master/), create linted code with support for [`ruff`](https://github.com/astral-sh/ruff) and make use of the standard Python logging library. |\n| Flexible Deployment  | Deployment strategies that include single or distributed-machine deployment as well as additional support for deploying on Argo, Prefect, Kubeflow, AWS Batch, and Databricks.                                                                                                                                                                                                                      |\n\n## How do I use Kedro?\n\nThe [Kedro documentation](https://docs.kedro.org/en/stable/) first explains [how to install Kedro](https://docs.kedro.org/en/stable/get_started/install.html) and then introduces [key Kedro concepts](https://docs.kedro.org/en/stable/get_started/kedro_concepts.html).\n\nYou can then review the [spaceflights tutorial](https://docs.kedro.org/en/stable/tutorial/spaceflights_tutorial.html) to build a Kedro project for hands-on experience.\n\nFor new and intermediate Kedro users, there's a comprehensive section on [how to visualise Kedro projects using Kedro-Viz](https://docs.kedro.org/en/stable/visualisation/index.html).\n\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/kedro-org/kedro-viz/main/.github/img/banner.png\" alt>\n    <em>A pipeline visualisation generated using Kedro-Viz</em>\n</p>\n\nAdditional documentation explains [how to work with Kedro and Jupyter notebooks](https://docs.kedro.org/en/stable/notebooks_and_ipython/index.html), and there are a set of advanced user guides for advanced for key Kedro features. We also recommend the [API reference documentation](/kedro) for further information.\n\n\n## Why does Kedro exist?\n\nKedro is built upon our collective best-practice (and mistakes) trying to deliver real-world ML applications that have vast amounts of raw unvetted data. We developed Kedro to achieve the following:\n\n- To address the main shortcomings of Jupyter notebooks, one-off scripts, and glue-code because there is a focus on\n  creating **maintainable data science code**\n- To enhance **team collaboration** when different team members have varied exposure to software engineering concepts\n- To increase efficiency, because applied concepts like modularity and separation of concerns inspire the creation of\n  **reusable analytics code**\n\nFind out more about how Kedro can answer your use cases from the [product FAQs on the Kedro website](https://kedro.org/#faq).\n\n## The humans behind Kedro\n\nThe [Kedro product team](https://docs.kedro.org/en/stable/contribution/technical_steering_committee.html#kedro-maintainers) and a number of [open source contributors from across the world](https://github.com/kedro-org/kedro/releases) maintain Kedro.\n\n## Can I contribute?\n\nYes! We welcome all kinds of contributions. Check out our [guide to contributing to Kedro](https://github.com/kedro-org/kedro/wiki/Contribute-to-Kedro).\n\n## Where can I learn more?\n\nThere is a growing community around Kedro. We encourage you to ask and answer technical questions on [Slack](https://slack.kedro.org/) and bookmark the [Linen archive of past discussions](https://linen-slack.kedro.org/).\n\nWe keep a list of [technical FAQs in the Kedro documentation](https://docs.kedro.org/en/stable/faq/faq.html) and you can find a  growing list of blog posts, videos and projects that use Kedro over on the [`awesome-kedro` GitHub repository](https://github.com/kedro-org/awesome-kedro). If you have created anything with Kedro we'd love to include it on the list. Just make a PR to add it!\n\n## How can I cite Kedro?\n\nIf you're an academic, Kedro can also help you, for example, as a tool to solve the problem of reproducible research. Use the \"Cite this repository\" button on [our repository](https://github.com/kedro-org/kedro) to generate a citation from the [CITATION.cff file](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files).\n\n## Python version support policy\n* The core [Kedro Framework](https://github.com/kedro-org/kedro) supports all Python versions that are actively maintained by the CPython core team. When a [Python version reaches end of life](https://devguide.python.org/versions/#versions), support for that version is dropped from Kedro. This is not considered a breaking change.\n* The [Kedro Datasets](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-datasets) package follows the [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) Python version support policy. This means that `kedro-datasets` generally drops Python version support before `kedro`. This is because `kedro-datasets` has a lot of dependencies that follow NEP 29 and the more conservative version support approach of the Kedro Framework makes it hard to manage those dependencies properly.\n\n\n## ‚òïÔ∏è Kedro Coffee Chat üî∂\n\nWe appreciate our community and want to stay connected. For that, we offer a public Coffee Chat format where we share updates and cool stuff around Kedro once every two weeks and give you time to ask your questions live.\n\nCheck out the upcoming demo topics and dates at the [Kedro Coffee Chat wiki page](https://github.com/kedro-org/kedro/wiki/Kedro-Coffee-Chat).\n\nFollow our Slack [announcement channel](https://kedro-org.slack.com/archives/C03RKAQ0MGQ) to see Kedro Coffee Chat announcements and access demo recordings.\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 151.931640625,
          "content": "# Upcoming Release\n\n## Major features and improvements\n* Implemented `KedroDataCatalog.to_config()` method that converts the catalog instance into a configuration format suitable for serialization.\n* Improve OmegaConfigLoader performance.\n* Replaced `trufflehog` with `detect-secrets` for detecting secrets within a code base.\n\n## Bug fixes and other changes\n* Added validation to ensure dataset versions consistency across catalog.\n* Fixed a bug in project creation when using a custom starter template offline.\n* Added `node` import to the pipeline template.\n* Update error message when executing kedro run without pipeline.\n* Safeguard hooks when user incorrectly registers a hook class in settings.py.\n\n## Breaking changes to the API\n## Documentation changes\n## Community contributions\n\n# Release 0.19.10\n\n## Major features and improvements\n* Add official support for Python 3.13.\n* Implemented dict-like interface for `KedroDataCatalog`.\n* Implemented lazy dataset initializing for `KedroDataCatalog`.\n* Project dependencies on both the default template and on starter templates are now explicitly declared on the `pyproject.toml` file, allowing Kedro projects to work with project management tools like `uv`, `pdm`, and `rye`.\n\n**Note:** ``KedroDataCatalog`` is an experimental feature and is under active development. Therefore, it is possible we'll introduce breaking changes to this class, so be mindful of that if you decide to use it already. Let us know if you have any feedback about the ``KedroDataCatalog`` or ideas for new features.\n\n## Bug fixes and other changes\n* Added I/O support for Oracle Cloud Infrastructure (OCI) Object Storage filesystem.\n* Fixed `DatasetAlreadyExistsError` for `ThreadRunner` when Kedro project run and using runner separately.\n\n## Breaking changes to the API\n## Documentation changes\n* Added Databricks Asset Bundles deployment guide.\n* Added a new minimal Kedro project creation guide.\n* Added example to explain how dataset factories work.\n* Updated CLI autocompletion docs with new Click syntax.\n* Standardised `.parquet` suffix in docs and tests.\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n* [G. D. McBain](https://github.com/gdmcbain)\n* [Greg Vaslowski](https://github.com/Vaslo)\n* [Hyewon Choi](https://github.com/hyew0nChoi)\n* [Pedro Antonacio](https://github.com/antonacio)\n\n# Release 0.19.9\n\n## Major features and improvements\n* Dropped Python 3.8 support.\n* Implemented `KedroDataCatalog` repeating `DataCatalog` functionality with a few API enhancements:\n  * Removed `_FrozenDatasets` and access datasets as properties;\n  * Added get dataset by name feature;\n  * `add_feed_dict()` was simplified to only add raw data;\n  * Datasets' initialisation was moved out from `from_config()` method to the constructor.\n* Moved development requirements from `requirements.txt` to the dedicated section in `pyproject.toml` for project template.\n* Implemented `Protocol` abstraction for the current `DataCatalog` and adding new catalog implementations.\n* Refactored `kedro run` and `kedro catalog` commands.\n* Moved pattern resolution logic from `DataCatalog` to a separate component - `CatalogConfigResolver`. Updated `DataCatalog` to use `CatalogConfigResolver` internally.\n* Made packaged Kedro projects return `session.run()` output to be used when running it in the interactive environment.\n* Enhanced `OmegaConfigLoader` configuration validation to detect duplicate keys at all parameter levels, ensuring comprehensive nested key checking.\n\n**Note:** ``KedroDataCatalog`` is an experimental feature and is under active development. Therefore, it is possible we'll introduce breaking changes to this class, so be mindful of that if you decide to use it already. Let us know if you have any feedback about the ``KedroDataCatalog`` or ideas for new features.\n\n## Bug fixes and other changes\n* Fixed bug where using dataset factories breaks with `ThreadRunner`.\n* Fixed a bug where `SharedMemoryDataset.exists` would not call the underlying `MemoryDataset`.\n* Fixed template projects example tests.\n* Made credentials loading consistent between `KedroContext._get_catalog()` and `resolve_patterns` so that both use `_get_config_credentials()`\n\n## Breaking changes to the API\n* Removed `ShelveStore` to address a security vulnerability.\n\n## Documentation changes\n* Fix logo on PyPI page.\n* Minor language/styling updates.\n\n\n## Community contributions\n* [Puneet](https://github.com/puneeter)\n* [ethanknights](https://github.com/ethanknights)\n* [Manezki](https://github.com/Manezki)\n* [MigQ2](https://github.com/MigQ2)\n* [Felix Scherz](https://github.com/felixscherz)\n* [Yu-Sheng Li](https://github.com/kevin1kevin1k)\n\n# Release 0.19.8\n\n## Major features and improvements\n* Made default run entrypoint in `__main__.py` work in interactive environments such as IPyhon and Databricks.\n\n## Bug fixes and other changes\n* Fixed a bug that caused tracebacks disappeared from CLI runs.\n* Moved `_find_run_command()` and `_find_run_command_in_plugins()` from `__main__.py` in the project template to the framework itself.\n* Fixed a bug where `%load_node` breaks with multi-lines import statements.\n* Fixed a regression where `rich` mark up logs stop showing since 0.19.7.\n\n## Breaking changes to the API\n\n## Documentation changes\n* Add clarifications in docs explaining how runtime parameter resolution works.\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n* [cclauss](https://github.com/cclauss)\n* [eltociear](https://github.com/eltociear)\n* [ltalirz](https://github.com/ltalirz)\n\n# Release 0.19.7\n\n## Major features and improvements\n* Exposed `load` and `save` publicly for each dataset in the core `kedro` library, and enabled other datasets to do the same. If a dataset doesn't expose `load` or `save` publicly, Kedro will fall back to using `_load` or `_save`, respectively.\n* Kedro commands are now lazily loaded to add performance gains when running Kedro commands.\n* Implemented key completion support for accessing datasets in the `DataCatalog`.\n* Implemented dataset pretty printing.\n* Implemented `DataCatalog` pretty printing.\n* Moved to an opt-out model for telemetry, enabling it by default without requiring prior consent.\n\n## Bug fixes and other changes\n* Updated error message for invalid catalog entries.\n* Updated error message for catalog entries when the dataset class is not found with hints on how to resolve the issue.\n* Fixed a bug in the `DataCatalog` `shallow_copy()` method to ensure it returns the type of the used catalog and doesn't cast it to `DataCatalog`.\n* Made [kedro-telemetry](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-telemetry) a core dependency.\n* Fixed a bug when `OmegaConfigLoader` is printed, there are few missing arguments.\n* Fixed a bug when where iterating `OmegaConfigLoader`'s `keys` return empty dictionary.\n\n## Breaking changes to the API\n\n## Upcoming deprecations for Kedro 0.20.0\n* The utility method `get_pkg_version()` is deprecated and will be removed in Kedro 0.20.0.\n* `LambdaDataset` is deprecated and will be removed in Kedro 0.20.0.\n\n## Documentation changes\n* Improved documentation for configuring dataset parameters in the data catalog\n* Extended documentation with an example of logging customisation at runtime\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n* [nickolasrm](https://github.com/nickolasrm)\n* [yury-fedotov](https://github.com/yury-fedotov)\n\n# Release 0.19.6\n\n## Major features and improvements\n* Added `raise_errors` argument to `find_pipelines`. If `True`, the first pipeline for which autodiscovery fails will cause an error to be raised. The default behaviour is still to raise a warning for each failing pipeline.\n* It is now possible to use Kedro without having `rich` installed.\n* Updated custom logging behavior: `conf/logging.yml` will be used if it exists and `KEDRO_LOGGING_CONFIG` is not set; otherwise, `default_logging.yml` will be used.\n\n## Bug fixes and other changes\n* User defined catch-all dataset factory patterns now override the default pattern provided by the runner.\n\n## Breaking changes to the API\n\n## Upcoming deprecations for Kedro 0.20.0\n* All micro-packaging commands (`kedro micropkg pull`, `kedro micropkg package`) are deprecated and will be removed in Kedro 0.20.0.\n\n## Documentation changes\n* Improved documentation for custom starters\n* Added a new docs section on deploying Kedro project on AWS Airflow MWAA\n* Detailed instructions on using `globals` and `runtime_params` with the `OmegaConfigLoader`\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n* [doxenix](https://github.com/doxenix)\n* [cleeeks](https://github.com/cleeeks)\n\n# Release 0.19.5\n\n## Bug fixes and other changes\n* Fixed breaking import issue when working on a project with `kedro-viz` on python 3.8.\n\n## Documentation changes\n* Updated the documentation for deploying a Kedro project with Astronomer Airflow.\n* Used `kedro-sphinx-theme` for documentation.\n* Add mentions about correct usage of `configure_project` with `multiprocessing`.\n*\n# Release 0.19.4\n\n## Major features and improvements\n* Kedro commands now work from any subdirectory within a Kedro project.\n* Kedro CLI now provides a better error message when project commands are run outside of a project i.e. `kedro run`\n* Added the `--telemetry` flag to `kedro new`, allowing the user to register consent to have user analytics collected at the same time as the project is created.\n* Improved the performance of `Pipeline` object creation and summing.\n* Improved suggestions to resume failed pipeline runs.\n* Dropped the dependency on `toposort` in favour of the built-in `graphlib` module.\n* Cookiecutter errors are shown in short format without the `--verbose` flag.\n\n## Bug fixes and other changes\n* Updated `kedro pipeline create` and `kedro pipeline delete` to read the base environment from the project settings.\n* Updated CLI command `kedro catalog resolve` to read credentials properly.\n* Changed the path of where pipeline tests generated with `kedro pipeline create` from `<project root>/src/tests/pipelines/<pipeline name>` to `<project root>/tests/pipelines/<pipeline name>`.\n* Updated ``.gitignore`` to prevent pushing MLflow local runs folder to a remote forge when using MLflow and Git.\n* Fixed error handling message for malformed yaml/json files in OmegaConfigLoader.\n* Fixed a bug in `node`-creation allowing self-dependencies when using transcoding, that is datasets named like `name@format`.\n* Improved error message when passing wrong value to node.\n\n## Breaking changes to the API\n* Methods `_is_project` and `_find_kedro_project` have been moved to `kedro.utils`. We recommend not using private methods in your code, but if you do, please update your code to use the new location.\n\n## Documentation changes\n* Added missing description for `merge_strategy` argument in OmegaConfigLoader.\n* Added documentation on best practices for testing nodes and pipelines.\n* Clarified docs around using custom resolvers without a full Kedro project.\n\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n* [ondrejzacha](https://github.com/ondrejzacha)\n* [Puneet](https://github.com/puneeter)\n\n# Release 0.19.3\n\n## Major features and improvements\n* Create the debugging line magic `%load_node` for Jupyter Notebook and Jupyter Lab.\n* Add official support for Python 3.12.\n* Add better IPython, VS Code Notebook support for `%load_node` and minimal support for Databricks.\n* Add full Kedro Node input syntax for `%load_node`.\n\n## Bug fixes and other changes\n* Updated CLI Command `kedro catalog resolve` to work with dataset factories that use `PartitionedDataset`.\n* Addressed arbitrary file write via archive extraction security vulnerability in micropackaging.\n* Added the `_EPHEMERAL` attribute to `AbstractDataset` and other Dataset classes that inherit from it.\n* Added new JSON Schema that works with Kedro versions 0.19.*\n\n## Breaking changes to the API\n\n## Documentation changes\n* Enable read-the-docs search when user presses Command/Ctrl + K.\n* Added documentation for `kedro-telemetry` and the data collected by it.\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n* [MosaicMan](https://github.com/MosaicMan)\n* [Fazil](https://github.com/lordsoffallen)\n\n# Release 0.19.2\n\n## Bug fixes and other changes\n* Removed example pipeline requirements when examples are not selected in `tools`.\n* Allowed modern versions of JupyterLab and Jupyter Notebooks.\n* Removed setuptools dependency\n* Added `source_dir` explicitly in `pyproject.toml` for non-src layout project.\n* `MemoryDataset` entries are now included in free outputs.\n* Removed black dependency and replaced it functionality with `ruff format`.\n\n## Breaking changes to the API\n* Added logging about not using async mode in `SequentiallRunner` and `ParallelRunner`.\n* Changed input format for tools option obtained from --config file from numbers to short names.\n\n## Documentation changes\n* Added documentation about `bootstrap_project` and `configure_project`.\n* Added documentation about `kedro run` and hook execution order.\n\n## Migration guide from Kedro 0.18.* to 0.19.*\n[See the migration guide for 0.19 in the Kedro documentation](https://docs.kedro.org/en/latest/resources/migration.html).\n\n# Release 0.19.1\n\n## Bug fixes and other changes\n* Loosened pin for `kedro-telemetry` to fix dependency issues in `0.19.0`.\n\n## Migration guide from Kedro 0.18.* to 0.19.*\n[See the migration guide for 0.19 in the Kedro documentation](https://docs.kedro.org/en/latest/resources/migration.html).\n\n\n# Release 0.19.0\n\n## Major features and improvements\n* Dropped Python 3.7 support.\n* [Introduced project tools and example to the `kedro new` CLI flow](docs/source/get_started/new_project.md#project-tools).\n* The new spaceflights starters, `spaceflights-pandas`, `spaceflights-pandas-viz`, `spaceflights-pyspark`, and `spaceflights-pyspark-viz` can be used with the `kedro new` command with the `--starter` flag.\n* Added the `--conf-source` option to `%reload_kedro`, allowing users to specify a source for project configuration.\n* [Added the functionality to choose a merging strategy for config files loaded with `OmegaConfigLoader`](docs/source/configuration/advanced_configuration.md#how-to-change-the-merge-strategy-used-by-omegaconfigloader).\n* Modified the mechanism of importing datasets, raise more explicit error when dependencies are missing.\n* Added validation for configuration file used to override run commands via the CLI.\n* Moved the default environment `base` and `local` from config loader to `_ProjectSettings`. This enables the use of config loader as a standalone class without affecting existing Kedro Framework users.\n\n## Bug fixes and other changes\n* Added a new field `tools` to `pyproject.toml` when a project is created.\n* Reduced `spaceflights` data to minimise waiting times during tutorial execution.\n* Added validation to node tags to be consistent with node names.\n* Removed `pip-tools` as a dependency.\n* Accepted path-like filepaths more broadly for datasets.\n* Removed support for defining the `layer` attribute at top-level within DataCatalog.\n* Bumped `kedro-datasets` to latest `2.0.0`.\n\n## Breaking changes to the API\n* Renamed the `data_sets` argument and the `_data_sets` attribute in `Catalog` and their references to `datasets` and `_datasets` respectively.\n* Renamed the `data_sets()` method in `Pipeline` and all references to it to `datasets()`.\n* Renamed all other uses of `data_set` and `data_sets` in the codebase to `dataset` and `datasets` respectively.\n* Remove deprecated `project_version` from `ProjectMetadata`.\n* Removed `package_name` argument from `KedroSession.create`.\n* Removed the `create_default_data_set()` method in the `Runner` in favour of using dataset factories to create default dataset instances.\n* Removed `layer` argument from the DataCatalog.\n\n### Datasets\n* Removed `kedro.extras.datasets` and tests.\n* Reduced constructor arguments for `APIDataset` by replacing most arguments with a single constructor argument `load_args`. This makes it more consistent with other Kedro DataSets and the underlying `requests` API, and automatically enables the full configuration domain: stream, certificates, proxies, and more.\n* Removed `PartitionedDataset` and `IncrementalDataset` from `kedro.io`\n\n### CLI\n* Removed deprecated commands:\n   * `kedro docs`\n   * `kedro jupyter convert`\n   * `kedro activate-nbstripout`\n   * `kedro build-docs`\n   * `kedro build-reqs`\n   * `kedro lint`\n   * `kedro test`\n* Added the `--addons` flag to the `kedro new` command.\n* Added the `--name` flag to the `kedro new` command.\n* Removed `kedro run` flags `--node`, `--tag`, and `--load-version` in favour of `--nodes`, `--tags`, and `--load-versions`.\n\n### ConfigLoader\n* Made `OmegaConfigLoader` the default config loader.\n* Removed `ConfigLoader` and `TemplatedConfigLoader`.\n* `logging` is removed from `ConfigLoader` in favour of the environment variable `KEDRO_LOGGING_CONFIG`.\n\n### Other\n* Removed deprecated `kedro.extras.ColorHandler`.\n* The Kedro IPython extension is no longer available as `%load_ext kedro.extras.extensions.ipython`; use `%load_ext kedro.ipython` instead.\n* Anonymous nodes are given default names of the form `<function_name>([in1;in2;...]) -> [out1;out2;...]`, with the names of inputs and outputs separated by semicolons.\n* The default project template now has one `pyproject.toml` at the root of the project (containing both the packaging metadata and the Kedro build config).\n* The `requirements.txt` in the default project template moved to the root of the project as well (hence dependencies are now installed with `pip install -r requirements.txt` instead of `pip install -r src/requirements.txt`).\n* The `spaceflights` starter has been renamed to `spaceflights-pandas`.\n* The starters `pandas-iris`, `pyspark-iris`, `pyspark`, and `standalone-datacatalog` have been archived.\n\n## Migration guide from Kedro 0.18.* to 0.19.*\n[See the migration guide for 0.19 in the Kedro documentation](https://docs.kedro.org/en/latest/resources/migration.html).\n\n\n### Logging\n`logging.yml` is now independent of Kedro's run environment and only used if `KEDRO_LOGGING_CONFIG` is set to point to it.\n\n## Community contributors\nWe are grateful to every community member who made a PR to Kedro that's found its way into 0.19.0, and give particular thanks to those who contributed between 0.18.14 and this release, either as part of their ongoing Kedro community involvement or as part of Hacktoberfest 2023 üéÉ\n\n* [Jeroldine Akuye Oakley](https://github.com/JayOaks) üéÉ\n* [La√≠za Milena Scheid Parizotto](https://github.com/laizaparizotto) üéÉ\n* [Mustapha Abdullahi](https://github.com/mustious)\n* [Adam Kells](https://github.com/adamkells)\n* [Ajay Gonepuri](https://github.com/HKABIG)\n\n# Release 0.18.14\n\n## Major features and improvements\n* Allowed using of custom cookiecutter templates for creating pipelines with `--template` flag for `kedro pipeline create` or via `template/pipeline` folder.\n* Allowed overriding of configuration keys with runtime parameters using the `runtime_params` resolver with `OmegaConfigLoader`.\n\n## Bug fixes and other changes\n* Updated dataset factories to resolve nested catalog config properly.\n* Updated `OmegaConfigLoader` to handle paths containing dots outside of `conf_source`.\n* Made `settings.py` optional.\n\n## Documentation changes\n* Added documentation to clarify execution order of hooks.\n* Added a notebook example for spaceflights to illustrate how to incrementally add Kedro features.\n* Moved documentation for the `standalone-datacatalog` starter into its [README file](https://github.com/kedro-org/kedro-starters/tree/main/standalone-datacatalog).\n* Added new documentation about deploying a Kedro project with Amazon EMR.\n* Added new documentation about how to publish a Kedro-Viz project to make it shareable.\n* New TSC members added to the page and the organisation of each member is also now listed.\n* Plus some minor bug fixes and changes across the documentation.\n\n## Upcoming deprecations for Kedro 0.19.0\n* All dataset classes will be removed from the core Kedro repository (`kedro.extras.datasets`). Install and import them from the [`kedro-datasets`](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-datasets) package instead.\n* All dataset classes ending with `DataSet` are deprecated and will be removed in Kedro `0.19.0` and `kedro-datasets` `2.0.0`. Instead, use the updated class names ending with `Dataset`.\n* The starters `pandas-iris`, `pyspark-iris`, `pyspark`, and `standalone-datacatalog` are deprecated and will be archived in Kedro 0.19.0.\n* `PartitionedDataset` and `IncrementalDataset` have been moved to `kedro-datasets` and will be removed in Kedro `0.19.0`. Install and import them from the [`kedro-datasets`](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-datasets) package instead.\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n* [Jason Hite](https://github.com/jasonmhite)\n* [IngerMathilde](https://github.com/IngerMathilde)\n* [La√≠za Milena Scheid Parizotto](https://github.com/laizaparizotto)\n* [Richard](https://github.com/CF-FHB-X)\n* [flpvvvv](https://github.com/flpvvvv)\n* [qheuristics](https://github.com/qheuristics)\n* [Miguel Ortiz](https://github.com/miguel-ortiz-marin)\n* [rxm7706](https://github.com/rxm7706)\n* [I√±igo Hidalgo](https://github.com/inigohidalgo)\n* [harmonys-qb](https://github.com/harmonys-qb)\n* [Yi Kuang](https://github.com/lvxhnat)\n* [Jens Lord√©n](https://github.com/Celsuss)\n\n# Release 0.18.13\n\n## Major features and improvements\n* Added support for Python 3.11. This includes tackling challenges like dependency pinning and test adjustments to ensure a smooth experience. Detailed migration tips are provided below for further context.\n* Added new `OmegaConfigLoader` features:\n  * Allowed registering of custom resolvers to `OmegaConfigLoader` through `CONFIG_LOADER_ARGS`.\n  * Added support for global variables to `OmegaConfigLoader`.\n* Added `kedro catalog resolve` CLI command that resolves dataset factories in the catalog with any explicit entries in the project pipeline.\n* Implemented a flat `conf/` structure for modular pipelines, and accordingly, updated the `kedro pipeline create` and `kedro catalog create` command.\n* Updated new Kedro project template and Kedro starters:\n  * Change Kedro starters and new Kedro projects to use `OmegaConfigLoader`.\n  * Converted `setup.py` in new Kedro project template and Kedro starters to `pyproject.toml` and moved flake8 configuration\n  to dedicated file `.flake8`.\n  * Updated the spaceflights starter to use the new flat `conf/` structure.\n\n## Bug fixes and other changes\n* Updated `OmegaConfigLoader` to ignore config from hidden directories like `.ipynb_checkpoints`.\n\n## Documentation changes\n* Revised the `data` section to restructure beginner and advanced pages about the Data Catalog and datasets.\n* Moved contributor documentation to the [GitHub wiki](https://github.com/kedro-org/kedro/wiki/Contribute-to-Kedro).\n* Updated example of using generator functions in nodes.\n* Added migration guide from the `ConfigLoader` and the `TemplatedConfigLoader` to the `OmegaConfigLoader`. The `ConfigLoader` and the `TemplatedConfigLoader` are deprecated and will be removed in the `0.19.0` release.\n\n## Migration Tips for Python 3.11:\n* PyTables on Windows: Users on Windows with Python >=3.8 should note we've pinned `pytables` to `3.8.0` due to compatibility issues.\n* Spark Dependency: We've set an upper version limit for `pyspark` at <3.4 due to breaking changes in 3.4.\n* Testing with Python 3.10: The latest `moto` version now supports parallel test execution for Python 3.10, resolving previous issues.\n\n## Breaking changes to the API\n\n## Upcoming deprecations for Kedro 0.19.0\n* Renamed abstract dataset classes, in accordance with the [Kedro lexicon](https://github.com/kedro-org/kedro/wiki/Kedro-documentation-style-guide#kedro-lexicon). Dataset classes ending with \"DataSet\" are deprecated and will be removed in 0.19.0. Note that all of the below classes are also importable from `kedro.io`; only the module where they are defined is listed as the location.\n\n| Type                       | Deprecated Alias           | Location        |\n| -------------------------- | -------------------------- | --------------- |\n| `AbstractDataset`          | `AbstractDataSet`          | `kedro.io.core` |\n| `AbstractVersionedDataset` | `AbstractVersionedDataSet` | `kedro.io.core` |\n\n* Using the `layer` attribute at the top level is deprecated; it will be removed in Kedro version 0.19.0. Please move `layer` inside the `metadata` -> `kedro-viz` attributes.\n\n## Community contributions\nThanks to [La√≠za Milena Scheid Parizotto](https://github.com/laizaparizotto) and [Jonathan Cohen](https://github.com/JonathanDCohen).\n\n# Release 0.18.12\n\n## Major features and improvements\n* Added dataset factories feature which uses pattern matching to reduce the number of catalog entries.\n* Activated all built-in resolvers by default for `OmegaConfigLoader` except for `oc.env`.\n* Added `kedro catalog rank` CLI command that ranks dataset factories in the catalog by matching priority.\n\n## Bug fixes and other changes\n* Consolidated dependencies and optional dependencies in `pyproject.toml`.\n* Made validation of unique node outputs much faster.\n* Updated `kedro catalog list` to show datasets generated with factories.\n\n## Documentation changes\n* Recommended `ruff` as the linter and removed mentions of `pylint`, `isort`, `flake8`.\n\n## Community contributions\nThanks to [La√≠za Milena Scheid Parizotto](https://github.com/laizaparizotto) and [Chris Schopp](https://github.com/cschopp-simwell).\n\n## Breaking changes to the API\n\n## Upcoming deprecations for Kedro 0.19.0\n* `ConfigLoader` and `TemplatedConfigLoader` will be deprecated. Please use `OmegaConfigLoader` instead.\n\n# Release 0.18.11\n\n## Major features and improvements\n* Added `databricks-iris` as an official starter.\n\n## Bug fixes and other changes\n* Reworked micropackaging workflow to use standard Python packaging practices.\n* Make `kedro micropkg package` accept `--verbose`.\n* Compare for protocol and delimiter in `PartitionedDataSet` to be able to pass the protocol to partitions which paths starts with the same characters as the protocol (e.g. `s3://s3-my-bucket`).\n\n## Documentation changes\n* Significant improvements to the documentation that covers working with Databricks and Kedro, including a new page for workspace-only development, and a guide to choosing the best workflow for your use case.\n* Updated documentation for deploying with Prefect for version 2.0.\n* Added documentation for developing a Kedro project using a Databricks workspace.\n\n## Breaking changes to the API\n* Logging is decoupled from `ConfigLoader`, use `KEDRO_LOGGING_CONFIG` to configure logging.\n\n## Upcoming deprecations for Kedro 0.19.0\n* Renamed dataset and error classes, in accordance with the [Kedro lexicon](https://github.com/kedro-org/kedro/wiki/Kedro-documentation-style-guide#kedro-lexicon). Dataset classes ending with \"DataSet\" and error classes starting with \"DataSet\" are deprecated and will be removed in 0.19.0. Note that all of the below classes are also importable from `kedro.io`; only the module where they are defined is listed as the location.\n\n| Type                        | Deprecated Alias            | Location                       |\n| --------------------------- | --------------------------- | ------------------------------ |\n| `CachedDataset`             | `CachedDataSet`             | `kedro.io.cached_dataset`      |\n| `LambdaDataset`             | `LambdaDataSet`             | `kedro.io.lambda_dataset`      |\n| `IncrementalDataset`        | `IncrementalDataSet`        | `kedro.io.partitioned_dataset` |\n| `MemoryDataset`             | `MemoryDataSet`             | `kedro.io.memory_dataset`      |\n| `PartitionedDataset`        | `PartitionedDataSet`        | `kedro.io.partitioned_dataset` |\n| `DatasetError`              | `DataSetError`              | `kedro.io.core`                |\n| `DatasetAlreadyExistsError` | `DataSetAlreadyExistsError` | `kedro.io.core`                |\n| `DatasetNotFoundError`      | `DataSetNotFoundError`      | `kedro.io.core`                |\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n\n* [jmalovera10](https://github.com/jmalovera10)\n* [debugger24](https://github.com/debugger24)\n* [juliushetzel](https://github.com/juliushetzel)\n* [jacobweiss2305](https://github.com/jacobweiss2305)\n* [eduardoconto](https://github.com/eduardoconto)\n\n# Release 0.18.10\n\n## Major features and improvements\n* Rebrand across all documentation and Kedro assets.\n* Added support for variable interpolation in the catalog with the `OmegaConfigLoader`.\n\n# Release 0.18.9\n\n## Major features and improvements\n* `kedro run --params` now updates interpolated parameters correctly when using `OmegaConfigLoader`.\n* Added `metadata` attribute to `kedro.io` datasets. This is ignored by Kedro, but may be consumed by users or external plugins.\n* Added `kedro.logging.RichHandler`. This replaces the default `rich.logging.RichHandler` and is more flexible, user can turn off the `rich` traceback if needed.\n\n## Bug fixes and other changes\n* `OmegaConfigLoader` will return a `dict` instead of `DictConfig`.\n* `OmegaConfigLoader` does not show a `MissingConfigError` when the config files exist but are empty.\n\n## Documentation changes\n* Added documentation for collaborative experiment tracking within Kedro-Viz.\n* Revised section on deployment to better organise content and reflect how recently docs have been updated.\n* Minor improvements to fix typos and revise docs to align with engineering changes.\n\n## Breaking changes to the API\n* `kedro package` does not produce `.egg` files anymore, and now relies exclusively on `.whl` files.\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n\n* [tomasvanpottelbergh](https://github.com/tomasvanpottelbergh)\n* [https://github.com/debugger24](https://github.com/debugger24)\n\n# Release 0.18.8\n\n## Major features and improvements\n* Added `KEDRO_LOGGING_CONFIG` environment variable, which can be used to configure logging from the beginning of the `kedro` process.\n* Removed logs folder from the kedro new project template. File-based logging will remain but just be level INFO and above and go to project root instead.\n\n\n## Bug fixes and other changes\n* Improvements to Jupyter E2E tests.\n* Added full `kedro run` CLI command to session store to improve run reproducibility using `Kedro-Viz` experiment tracking.\n\n### Documentation changes\n* Improvements to documentation about configuration.\n* Improvements to Sphinx toolchain including incrementing to use a newer version.\n* Improvements to documentation on visualising Kedro projects on Databricks, and additional documentation about the development workflow for Kedro projects on Databricks.\n* Updated Technical Steering Committee membership documentation.\n* Revised documentation section about linting and formatting and extended to give details of `flake8` configuration.\n* Updated table of contents for documentation to reduce scrolling.\n* Expanded FAQ documentation.\n* Added a 404 page to documentation.\n* Added deprecation warnings about the removal of `kedro.extras.datasets`.\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n\n* [MaximeSteinmetz](https://github.com/MaximeSteinmetz)\n\n\n# Release 0.18.7\n\n## Major features and improvements\n* Added new Kedro CLI `kedro jupyter setup` to setup Jupyter Kernel for Kedro.\n* `kedro package` now includes the project configuration in a compressed `tar.gz` file.\n* Added functionality to the `OmegaConfigLoader` to load configuration from compressed files of `zip` or `tar` format. This feature requires `fsspec>=2023.1.0`.\n* Significant improvements to on-boarding documentation that covers setup for new Kedro users. Also some major changes to the spaceflights tutorial to make it faster to work through. We think it's a better read. Tell us if it's not.\n\n## Bug fixes and other changes\n* Added a guide and tooling for developing Kedro for Databricks.\n* Implemented missing dict-like interface for `_ProjectPipeline`.\n\n\n# Release 0.18.6\n\n## Bug fixes and other changes\n* Fixed bug that didn't allow to read or write datasets with `s3a` or `s3n` filepaths\n* Fixed bug with overriding nested parameters using the `--params` flag\n* Fixed bug that made session store incompatible with `Kedro-Viz` experiment tracking\n\n## Migration guide from Kedro 0.18.5 to 0.18.6\nA regression introduced in Kedro version `0.18.5` caused the `Kedro-Viz` console to fail to show experiment tracking correctly. If you experienced this issue, you will need to:\n* upgrade to Kedro version `0.18.6`\n* delete any erroneous session entries created with Kedro 0.18.5 from your session_store.db stored at `<project-path>/data/session_store.db`.\n\nThanks to Kedroids tomohiko kato, [tsanikgr](https://github.com/tsanikgr) and [maddataanalyst](https://github.com/maddataanalyst) for very detailed reports about the bug.\n\n\n# Release 0.18.5\n\n> This release introduced a bug that causes a failure in experiment tracking within the `Kedro-Viz` console. We recommend that you use Kedro version `0.18.6` in preference.\n\n## Major features and improvements\n* Added new `OmegaConfigLoader` which uses `OmegaConf` for loading and merging configuration.\n* Added the `--conf-source` option to `kedro run`, allowing users to specify a source for project configuration for the run.\n* Added `omegaconf` syntax as option for `--params`. Keys and values can now be separated by colons or equals signs.\n* Added support for generator functions as nodes, i.e. using `yield` instead of return.\n  * Enable chunk-wise processing in nodes with generator functions.\n  * Save node outputs after every `yield` before proceeding with next chunk.\n* Fixed incorrect parsing of Azure Data Lake Storage Gen2 URIs used in datasets.\n* Added support for loading credentials from environment variables using `OmegaConfigLoader`.\n* Added new `--namespace` flag to `kedro run` to enable filtering by node namespace.\n* Added a new argument `node` for all four dataset hooks.\n* Added the `kedro run` flags `--nodes`, `--tags`, and `--load-versions` to replace `--node`, `--tag`, and `--load-version`.\n\n## Bug fixes and other changes\n* Commas surrounded by square brackets (only possible for nodes with default names) will no longer split the arguments to `kedro run` options which take a list of nodes as inputs (`--from-nodes` and `--to-nodes`).\n* Fixed bug where `micropkg` manifest section in `pyproject.toml` isn't recognised as allowed configuration.\n* Fixed bug causing `load_ipython_extension` not to register the `%reload_kedro` line magic when called in a directory that does not contain a Kedro project.\n* Added `anyconfig`'s `ac_context` parameter to `kedro.config.commons` module functions for more flexible `ConfigLoader` customizations.\n* Change reference to `kedro.pipeline.Pipeline` object throughout test suite with `kedro.modular_pipeline.pipeline` factory.\n* Fixed bug causing the `after_dataset_saved` hook only to be called for one output dataset when multiple are saved in a single node and async saving is in use.\n* Log level for \"Credentials not found in your Kedro project config\" was changed from `WARNING` to `DEBUG`.\n* Added safe extraction of tar files in `micropkg pull` to fix vulnerability caused by [CVE-2007-4559](https://github.com/advisories/GHSA-gw9q-c7gh-j9vm).\n* Documentation improvements\n    * Bug fix in table font size\n    * Updated API docs links for datasets\n    * Improved CLI docs for `kedro run`\n    * Revised documentation for visualisation to build plots and for experiment tracking\n    * Added example for loading external credentials to the Hooks documentation\n\n## Breaking changes to the API\n\n## Community contributions\nMany thanks to the following Kedroids for contributing PRs to this release:\n\n* [adamfrly](https://github.com/adamfrly)\n* [corymaklin](https://github.com/corymaklin)\n* [Emiliopb](https://github.com/Emiliopb)\n* [grhaonan](https://github.com/grhaonan)\n* [JStumpp](https://github.com/JStumpp)\n* [michalbrys](https://github.com/michalbrys)\n* [sbrugman](https://github.com/sbrugman)\n\n## Upcoming deprecations for Kedro 0.19.0\n* `project_version` will be deprecated in `pyproject.toml` please use `kedro_init_version` instead.\n* Deprecated `kedro run` flags `--node`, `--tag`, and `--load-version` in favour of `--nodes`, `--tags`, and `--load-versions`.\n\n# Release 0.18.4\n\n## Major features and improvements\n* Make Kedro instantiate datasets from `kedro_datasets` with higher priority than `kedro.extras.datasets`. `kedro_datasets` is the namespace for the new `kedro-datasets` python package.\n* The config loader objects now implement `UserDict` and the configuration is accessed through `conf_loader['catalog']`.\n* You can configure config file patterns through `settings.py` without creating a custom config loader.\n* Added the following new datasets:\n\n| Type                                 | Description                                                                | Location                         |\n| ------------------------------------ | -------------------------------------------------------------------------- | -------------------------------- |\n| `svmlight.SVMLightDataSet`           | Work with svmlight/libsvm files using scikit-learn library                 | `kedro.extras.datasets.svmlight` |\n| `video.VideoDataSet`                 | Read and write video files from a filesystem                               | `kedro.extras.datasets.video`    |\n| `video.video_dataset.SequenceVideo`  | Create a video object from an iterable sequence to use with `VideoDataSet` | `kedro.extras.datasets.video`    |\n| `video.video_dataset.GeneratorVideo` | Create a video object from a generator to use with `VideoDataSet`          | `kedro.extras.datasets.video`    |\n* Implemented support for a functional definition of schema in `dask.ParquetDataSet` to work with the `dask.to_parquet` API.\n\n## Bug fixes and other changes\n* Fixed `kedro micropkg pull` for packages on PyPI.\n* Fixed `format` in `save_args` for `SparkHiveDataSet`, previously it didn't allow you to save it as delta format.\n* Fixed save errors in `TensorFlowModelDataset` when used without versioning; previously, it wouldn't overwrite an existing model.\n* Added support for `tf.device` in `TensorFlowModelDataset`.\n* Updated error message for `VersionNotFoundError` to handle insufficient permission issues for cloud storage.\n* Updated Experiment Tracking docs with working examples.\n* Updated `MatplotlibWriter`, `text.TextDataSet`, `plotly.PlotlyDataSet` and `plotly.JSONDataSet` docs with working examples.\n* Modified implementation of the Kedro IPython extension to use `local_ns` rather than a global variable.\n* Refactored `ShelveStore` to its own module to ensure multiprocessing works with it.\n* `kedro.extras.datasets.pandas.SQLQueryDataSet` now takes optional argument `execution_options`.\n* Removed `attrs` upper bound to support newer versions of Airflow.\n* Bumped the lower bound for the `setuptools` dependency to <=61.5.1.\n\n## Minor breaking changes to the API\n\n## Upcoming deprecations for Kedro 0.19.0\n* `kedro test` and `kedro lint` will be deprecated.\n\n## Documentation\n* Revised the Introduction to shorten it\n* Revised the Get Started section to remove unnecessary information and clarify the learning path\n* Updated the spaceflights tutorial to simplify the later stages and clarify what the reader needed to do in each phase\n* Moved some pages that covered advanced materials into more appropriate sections\n* Moved visualisation into its own section\n* Fixed a bug that degraded user experience: the table of contents is now sticky when you navigate between pages\n* Added redirects where needed on ReadTheDocs for legacy links and bookmarks\n\n## Contributions from the Kedroid community\nWe are grateful to the following for submitting PRs that contributed to this release: [jstammers](https://github.com/jstammers), [FlorianGD](https://github.com/FlorianGD), [yash6318](https://github.com/yash6318), [carlaprv](https://github.com/carlaprv), [dinotuku](https://github.com/dinotuku), [williamcaicedo](https://github.com/williamcaicedo), [avan-sh](https://github.com/avan-sh), [Kastakin](https://github.com/Kastakin), [amaralbf](https://github.com/amaralbf), [BSGalvan](https://github.com/BSGalvan), [levimjoseph](https://github.com/levimjoseph), [daniel-falk](https://github.com/daniel-falk), [clotildeguinard](https://github.com/clotildeguinard), [avsolatorio](https://github.com/avsolatorio), and [picklejuicedev](https://github.com/picklejuicedev) for comments and input to documentation changes\n\n# Release 0.18.3\n\n## Major features and improvements\n* Implemented autodiscovery of project pipelines. A pipeline created with `kedro pipeline create <pipeline_name>` can now be accessed immediately without needing to explicitly register it in `src/<package_name>/pipeline_registry.py`, either individually by name (e.g. `kedro run --pipeline=<pipeline_name>`) or as part of the combined default pipeline (e.g. `kedro run`). By default, the simplified `register_pipelines()` function in `pipeline_registry.py` looks like:\n\n    ```python\n    def register_pipelines() -> Dict[str, Pipeline]:\n        \"\"\"Register the project's pipelines.\n\n        Returns:\n            A mapping from pipeline names to ``Pipeline`` objects.\n        \"\"\"\n        pipelines = find_pipelines()\n        pipelines[\"__default__\"] = sum(pipelines.values())\n        return pipelines\n    ```\n\n* The Kedro IPython extension should now be loaded with `%load_ext kedro.ipython`.\n* The line magic `%reload_kedro` now accepts keywords arguments, e.g. `%reload_kedro --env=prod`.\n* Improved resume pipeline suggestion for `SequentialRunner`, it will backtrack the closest persisted inputs to resume.\n\n## Bug fixes and other changes\n\n* Changed default `False` value for rich logging `show_locals`, to make sure credentials and other sensitive data isn't shown in logs.\n* Rich traceback handling is disabled on Databricks so that exceptions now halt execution as expected. This is a workaround for a [bug in `rich`](https://github.com/Textualize/rich/issues/2455).\n* When using `kedro run -n [some_node]`, if `some_node` is missing a namespace the resulting error message will suggest the correct node name.\n* Updated documentation for `rich` logging.\n* Updated Prefect deployment documentation to allow for reruns with saved versioned datasets.\n* The Kedro IPython extension now surfaces errors when it cannot load a Kedro project.\n* Relaxed `delta-spark` upper bound to allow compatibility with Spark 3.1.x and 3.2.x.\n* Added `gdrive` to list of cloud protocols, enabling Google Drive paths for datasets.\n* Added svg logo resource for ipython kernel.\n\n## Upcoming deprecations for Kedro 0.19.0\n* The Kedro IPython extension will no longer be available as `%load_ext kedro.extras.extensions.ipython`; use `%load_ext kedro.ipython` instead.\n* `kedro jupyter convert`, `kedro build-docs`, `kedro build-reqs` and `kedro activate-nbstripout` will be deprecated.\n\n# Release 0.18.2\n\n## Major features and improvements\n* Added `abfss` to list of cloud protocols, enabling abfss paths.\n* Kedro now uses the [Rich](https://github.com/Textualize/rich) library to format terminal logs and tracebacks.\n* The file `conf/base/logging.yml` is now optional. See [our documentation](https://docs.kedro.org/en/0.18.2/logging/logging.html) for details.\n* Introduced a `kedro.starters` entry point. This enables plugins to create custom starter aliases used by `kedro starter list` and `kedro new`.\n* Reduced the `kedro new` prompts to just one question asking for the project name.\n\n## Bug fixes and other changes\n* Bumped `pyyaml` upper bound to make Kedro compatible with the [pyodide](https://pyodide.org/en/stable/usage/loading-packages.html#micropip) stack.\n* Updated project template's Sphinx configuration to use `myst_parser` instead of `recommonmark`.\n* Reduced number of log lines by changing the logging level from `INFO` to `DEBUG` for low priority messages.\n* Kedro's framework-side logging configuration no longer performs file-based logging. Hence superfluous `info.log`/`errors.log` files are no longer created in your project root, and running Kedro on read-only file systems such as Databricks Repos is now possible.\n* The `root` logger is now set to the Python default level of `WARNING` rather than `INFO`. Kedro's logger is still set to emit `INFO` level messages.\n* `SequentialRunner` now has consistent execution order across multiple runs with sorted nodes.\n* Bumped the upper bound for the Flake8 dependency to <5.0.\n* `kedro jupyter notebook/lab` no longer reuses a Jupyter kernel.\n* Required `cookiecutter>=2.1.1` to address a [known command injection vulnerability](https://security.snyk.io/vuln/SNYK-PYTHON-COOKIECUTTER-2414281).\n* The session store no longer fails if a username cannot be found with `getpass.getuser`.\n* Added generic typing for `AbstractDataSet` and `AbstractVersionedDataSet` as well as typing to all datasets.\n* Rendered the deployment guide flowchart as a Mermaid diagram, and added Dask.\n\n## Minor breaking changes to the API\n* The module `kedro.config.default_logger` no longer exists; default logging configuration is now set automatically through `kedro.framework.project.LOGGING`. Unless you explicitly import `kedro.config.default_logger` you do not need to make any changes.\n\n## Upcoming deprecations for Kedro 0.19.0\n* `kedro.extras.ColorHandler` will be removed in 0.19.0.\n\n# Release 0.18.1\n\n## Major features and improvements\n* Added a new hook `after_context_created` that passes the `KedroContext` instance as `context`.\n* Added a new CLI hook `after_command_run`.\n* Added more detail to YAML `ParserError` exception error message.\n* Added option to `SparkDataSet` to specify a `schema` load argument that allows for supplying a user-defined schema as opposed to relying on the schema inference of Spark.\n* The Kedro package no longer contains a built version of the Kedro documentation significantly reducing the package size.\n\n## Bug fixes and other changes\n* Removed fatal error from being logged when a Kedro session is created in a directory without git.\n* `KedroContext` is now a attr's dataclass, `config_loader` is available as public attribute.\n* Fixed `CONFIG_LOADER_CLASS` validation so that `TemplatedConfigLoader` can be specified in settings.py. Any `CONFIG_LOADER_CLASS` must be a subclass of `AbstractConfigLoader`.\n* Added runner name to the `run_params` dictionary used in pipeline hooks.\n* Updated [Databricks documentation](https://docs.kedro.org/en/0.18.1/deployment/databricks.html) to include how to get it working with IPython extension and Kedro-Viz.\n* Update sections on visualisation, namespacing, and experiment tracking in the spaceflight tutorial to correspond to the complete spaceflights starter.\n* Fixed `Jinja2` syntax loading with `TemplatedConfigLoader` using `globals.yml`.\n* Removed global `_active_session`, `_activate_session` and `_deactivate_session`. Plugins that need to access objects such as the config loader should now do so through `context` in the new `after_context_created` hook.\n* `config_loader` is available as a public read-only attribute of `KedroContext`.\n* Made `hook_manager` argument optional for `runner.run`.\n* `kedro docs` now opens an online version of the Kedro documentation instead of a locally built version.\n\n## Upcoming deprecations for Kedro 0.19.0\n* `kedro docs` will be removed in 0.19.0.\n\n\n# Release 0.18.0\n\n## TL;DR ‚ú®\nKedro 0.18.0 strives to reduce the complexity of the project template and get us closer to a stable release of the framework. We've introduced the full [micro-packaging workflow](https://docs.kedro.org/en/0.18.0/nodes_and_pipelines/micro_packaging.html) üì¶, which allows you to import packages, utility functions and existing pipelines into your Kedro project. [Integration with IPython and Jupyter](https://docs.kedro.org/en/0.18.0/tools_integration/ipython.html) has been streamlined in preparation for enhancements to Kedro's interactive workflow. Additionally, the release comes with long-awaited Python 3.9 and 3.10 support üêç.\n\n## Major features and improvements\n\n### Framework\n* Added `kedro.config.abstract_config.AbstractConfigLoader` as an abstract base class for all `ConfigLoader` implementations. `ConfigLoader` and `TemplatedConfigLoader` now inherit directly from this base class.\n* Streamlined the `ConfigLoader.get` and `TemplatedConfigLoader.get` API and delegated the actual `get` method functional implementation to the `kedro.config.common` module.\n* The `hook_manager` is no longer a global singleton. The `hook_manager` lifecycle is now managed by the `KedroSession`, and a new `hook_manager` will be created every time a `session` is instantiated.\n* Added support for specifying parameters mapping in `pipeline()` without the `params:` prefix.\n* Added new API `Pipeline.filter()` (previously in `KedroContext._filter_pipeline()`) to filter parts of a pipeline.\n* Added `username` to Session store for logging during Experiment Tracking.\n* A packaged Kedro project can now be imported and run from another Python project as following:\n```python\nfrom my_package.__main__ import main\n\nmain(\n    [\"--pipleine\", \"my_pipeline\"]\n)  # or just main() if no parameters are needed for the run\n```\n\n### Project template\n* Removed `cli.py` from the Kedro project template. By default, all CLI commands, including `kedro run`, are now defined on the Kedro framework side. You can still define custom CLI commands by creating your own `cli.py`.\n* Removed `hooks.py` from the Kedro project template. Registration hooks have been removed in favour of `settings.py` configuration, but you can still define execution timeline hooks by creating your own `hooks.py`.\n* Removed `.ipython` directory from the Kedro project template. The IPython/Jupyter workflow no longer uses IPython profiles; it now uses an IPython extension.\n* The default `kedro` run configuration environment names can now be set in `settings.py` using the `CONFIG_LOADER_ARGS` variable. The relevant keyword arguments to supply are `base_env` and `default_run_env`, which are set to `base` and `local` respectively by default.\n\n### DataSets\n* Added the following new datasets:\n\n| Type                      | Description                                                   | Location                         |\n| ------------------------- | ------------------------------------------------------------- | -------------------------------- |\n| `pandas.XMLDataSet`       | Read XML into Pandas DataFrame. Write Pandas DataFrame to XML | `kedro.extras.datasets.pandas`   |\n| `networkx.GraphMLDataSet` | Work with NetworkX using GraphML files                        | `kedro.extras.datasets.networkx` |\n| `networkx.GMLDataSet`     | Work with NetworkX using Graph Modelling Language files       | `kedro.extras.datasets.networkx` |\n| `redis.PickleDataSet`     | loads/saves data from/to a Redis database                     | `kedro.extras.datasets.redis`    |\n\n* Added `partitionBy` support and exposed `save_args` for `SparkHiveDataSet`.\n* Exposed `open_args_save` in `fs_args` for `pandas.ParquetDataSet`.\n* Refactored the `load` and `save` operations for `pandas` datasets in order to leverage `pandas` own API and delegate `fsspec` operations to them. This reduces the need to have our own `fsspec` wrappers.\n* Merged `pandas.AppendableExcelDataSet` into `pandas.ExcelDataSet`.\n* Added `save_args` to `feather.FeatherDataSet`.\n\n### Jupyter and IPython integration\n* The [only recommended way to work with Kedro in Jupyter or IPython is now the Kedro IPython extension](https://docs.kedro.org/en/0.18.0/tools_integration/ipython.html). Managed Jupyter instances should load this via `%load_ext kedro.ipython` and use the line magic `%reload_kedro`.\n* `kedro ipython` launches an IPython session that preloads the Kedro IPython extension.\n* `kedro jupyter notebook/lab` creates a custom Jupyter kernel that preloads the Kedro IPython extension and launches a notebook with that kernel selected. There is no longer a need to specify `--all-kernels` to show all available kernels.\n\n### Dependencies\n* Bumped the minimum version of `pandas` to 1.3. Any `storage_options` should continue to be specified under `fs_args` and/or `credentials`.\n* Added support for Python 3.9 and 3.10, dropped support for Python 3.6.\n* Updated `black` dependency in the project template to a non pre-release version.\n\n### Other\n* Documented distribution of Kedro pipelines with Dask.\n\n## Breaking changes to the API\n\n### Framework\n* Removed `RegistrationSpecs` and its associated `register_config_loader` and `register_catalog` hook specifications in favour of `CONFIG_LOADER_CLASS`/`CONFIG_LOADER_ARGS` and `DATA_CATALOG_CLASS` in `settings.py`.\n* Removed deprecated functions `load_context` and `get_project_context`.\n* Removed deprecated `CONF_SOURCE`, `package_name`, `pipeline`, `pipelines`, `config_loader` and `io` attributes from `KedroContext` as well as the deprecated `KedroContext.run` method.\n* Added the `PluginManager` `hook_manager` argument to `KedroContext` and the `Runner.run()` method, which will be provided by the `KedroSession`.\n* Removed the public method `get_hook_manager()` and replaced its functionality by `_create_hook_manager()`.\n* Enforced that only one run can be successfully executed as part of a `KedroSession`. `run_id` has been renamed to `session_id` as a result.\n\n### Configuration loaders\n* The `settings.py` setting `CONF_ROOT` has been renamed to `CONF_SOURCE`. Default value of `conf` remains unchanged.\n* `ConfigLoader` and `TemplatedConfigLoader` argument `conf_root` has been renamed to `conf_source`.\n* `extra_params` has been renamed to `runtime_params` in `kedro.config.config.ConfigLoader` and `kedro.config.templated_config.TemplatedConfigLoader`.\n* The environment defaulting behaviour has been removed from `KedroContext` and is now implemented in a `ConfigLoader` class (or equivalent) with the `base_env` and `default_run_env` attributes.\n\n### DataSets\n* `pandas.ExcelDataSet` now uses `openpyxl` engine instead of `xlrd`.\n* `pandas.ParquetDataSet` now calls `pd.to_parquet()` upon saving. Note that the argument `partition_cols` is not supported.\n* `spark.SparkHiveDataSet` API has been updated to reflect `spark.SparkDataSet`. The `write_mode=insert` option has also been replaced with `write_mode=append` as per Spark styleguide. This change addresses [Issue 725](https://github.com/kedro-org/kedro/issues/725) and [Issue 745](https://github.com/kedro-org/kedro/issues/745). Additionally, `upsert` mode now leverages `checkpoint` functionality and requires a valid `checkpointDir` be set for current `SparkContext`.\n* `yaml.YAMLDataSet` can no longer save a `pandas.DataFrame` directly, but it can save a dictionary. Use `pandas.DataFrame.to_dict()` to convert your `pandas.DataFrame` to a dictionary before you attempt to save it to YAML.\n* Removed `open_args_load` and `open_args_save` from the following datasets:\n  * `pandas.CSVDataSet`\n  * `pandas.ExcelDataSet`\n  * `pandas.FeatherDataSet`\n  * `pandas.JSONDataSet`\n  * `pandas.ParquetDataSet`\n* `storage_options` are now dropped if they are specified under `load_args` or `save_args` for the following datasets:\n  * `pandas.CSVDataSet`\n  * `pandas.ExcelDataSet`\n  * `pandas.FeatherDataSet`\n  * `pandas.JSONDataSet`\n  * `pandas.ParquetDataSet`\n* Renamed `lambda_data_set`, `memory_data_set`, and `partitioned_data_set` to `lambda_dataset`, `memory_dataset`, and `partitioned_dataset`, respectively, in `kedro.io`.\n* The dataset `networkx.NetworkXDataSet` has been renamed to `networkx.JSONDataSet`.\n\n### CLI\n* Removed `kedro install` in favour of `pip install -r src/requirements.txt` to install project dependencies.\n* Removed `--parallel` flag from `kedro run` in favour of `--runner=ParallelRunner`. The `-p` flag is now an alias for `--pipeline`.\n* `kedro pipeline package` has been replaced by `kedro micropkg package` and, in addition to the `--alias` flag used to rename the package, now accepts a module name and path to the pipeline or utility module to package, relative to `src/<package_name>/`. The `--version` CLI option has been removed in favour of setting a `__version__` variable in the micro-package's `__init__.py` file.\n* `kedro pipeline pull` has been replaced by `kedro micropkg pull` and now also supports `--destination` to provide a location for pulling the package.\n* Removed `kedro pipeline list` and `kedro pipeline describe` in favour of `kedro registry list` and `kedro registry describe`.\n* `kedro package` and `kedro micropkg package` now save `egg` and `whl` or `tar` files in the `<project_root>/dist` folder (previously `<project_root>/src/dist`).\n* Changed the behaviour of `kedro build-reqs` to compile requirements from `requirements.txt` instead of `requirements.in` and save them to `requirements.lock` instead of `requirements.txt`.\n* `kedro jupyter notebook/lab` no longer accept `--all-kernels` or `--idle-timeout` flags. `--all-kernels` is now the default behaviour.\n* `KedroSession.run` now raises `ValueError` rather than `KedroContextError` when the pipeline contains no nodes. The same `ValueError` is raised when there are no matching tags.\n* `KedroSession.run` now raises `ValueError` rather than `KedroContextError` when the pipeline name doesn't exist in the pipeline registry.\n\n### Other\n* Added namespace to parameters in a modular pipeline, which addresses [Issue 399](https://github.com/kedro-org/kedro/issues/399).\n* Switched from packaging pipelines as wheel files to tar archive files compressed with gzip (`.tar.gz`).\n* Removed decorator API from `Node` and `Pipeline`, as well as the modules `kedro.extras.decorators` and `kedro.pipeline.decorators`.\n* Removed transformer API from `DataCatalog`, as well as the modules `kedro.extras.transformers` and `kedro.io.transformers`.\n* Removed the `Journal` and `DataCatalogWithDefault`.\n* Removed `%init_kedro` IPython line magic, with its functionality incorporated into `%reload_kedro`. This means that if `%reload_kedro` is called with a filepath, that will be set as default for subsequent calls.\n\n## Migration guide from Kedro 0.17.* to 0.18.*\n\n### Hooks\n* Remove any existing `hook_impl` of the `register_config_loader` and `register_catalog` methods from `ProjectHooks` in `hooks.py` (or custom alternatives).\n* If you use `run_id` in the `after_catalog_created` hook, replace it with `save_version` instead.\n* If you use `run_id` in any of the `before_node_run`, `after_node_run`, `on_node_error`, `before_pipeline_run`, `after_pipeline_run` or `on_pipeline_error` hooks, replace it with `session_id` instead.\n\n### `settings.py` file\n* If you use a custom config loader class such as `kedro.config.TemplatedConfigLoader`, alter `CONFIG_LOADER_CLASS` to specify the class and `CONFIG_LOADER_ARGS` to specify keyword arguments. If not set, these default to `kedro.config.ConfigLoader` and an empty dictionary respectively.\n* If you use a custom data catalog class, alter `DATA_CATALOG_CLASS` to specify the class. If not set, this defaults to `kedro.io.DataCatalog`.\n* If you have a custom config location (i.e. not `conf`), update `CONF_ROOT` to `CONF_SOURCE` and set it to a string with the expected configuration location. If not set, this defaults to `\"conf\"`.\n\n### Modular pipelines\n* If you use any modular pipelines with parameters, make sure they are declared with the correct namespace. See example below:\n\nFor a given pipeline:\n```python\nactive_pipeline = pipeline(\n    pipe=[\n        node(\n            func=some_func,\n            inputs=[\"model_input_table\", \"params:model_options\"],\n            outputs=[\"**my_output\"],\n        ),\n        ...,\n    ],\n    inputs=\"model_input_table\",\n    namespace=\"candidate_modelling_pipeline\",\n)\n```\n\nThe parameters should look like this:\n\n```diff\n-model_options:\n-    test_size: 0.2\n-    random_state: 8\n-    features:\n-    - engines\n-    - passenger_capacity\n-    - crew\n+candidate_modelling_pipeline:\n+    model_options:\n+      test_size: 0.2\n+      random_state: 8\n+      features:\n+        - engines\n+        - passenger_capacity\n+        - crew\n\n```\n* Optional: You can now remove all `params:` prefix when supplying values to `parameters` argument in a `pipeline()` call.\n* If you pull modular pipelines with `kedro pipeline pull my_pipeline --alias other_pipeline`, now use `kedro micropkg pull my_pipeline --alias pipelines.other_pipeline` instead.\n* If you package modular pipelines with `kedro pipeline package my_pipeline`, now use `kedro micropkg package pipelines.my_pipeline` instead.\n* Similarly, if you package any modular pipelines using `pyproject.toml`, you should modify the keys to include the full module path, and wrapped in double-quotes, e.g:\n\n```diff\n[tool.kedro.micropkg.package]\n-data_engineering = {destination = \"path/to/here\"}\n-data_science = {alias = \"ds\", env = \"local\"}\n+\"pipelines.data_engineering\" = {destination = \"path/to/here\"}\n+\"pipelines.data_science\" = {alias = \"ds\", env = \"local\"}\n\n[tool.kedro.micropkg.pull]\n-\"s3://my_bucket/my_pipeline\" = {alias = \"aliased_pipeline\"}\n+\"s3://my_bucket/my_pipeline\" = {alias = \"pipelines.aliased_pipeline\"}\n```\n\n### DataSets\n* If you use `pandas.ExcelDataSet`, make sure you have `openpyxl` installed in your environment. This is automatically installed if you specify `kedro[pandas.ExcelDataSet]==0.18.0` in your `requirements.txt`. You can uninstall `xlrd` if you were only using it for this dataset.\n* If you use`pandas.ParquetDataSet`, pass pandas saving arguments directly to `save_args` instead of nested in `from_pandas` (e.g. `save_args = {\"preserve_index\": False}` instead of `save_args = {\"from_pandas\": {\"preserve_index\": False}}`).\n* If you use `spark.SparkHiveDataSet` with `write_mode` option set to `insert`, change this to `append` in line with the Spark styleguide. If you use `spark.SparkHiveDataSet` with `write_mode` option set to `upsert`, make sure that your `SparkContext` has a valid `checkpointDir` set either by `SparkContext.setCheckpointDir` method or directly in the `conf` folder.\n* If you use `pandas~=1.2.0` and pass `storage_options` through `load_args` or `savs_args`, specify them under `fs_args` or via `credentials` instead.\n* If you import from `kedro.io.lambda_data_set`, `kedro.io.memory_data_set`, or `kedro.io.partitioned_data_set`, change the import to `kedro.io.lambda_dataset`, `kedro.io.memory_dataset`, or `kedro.io.partitioned_dataset`, respectively (or import the dataset directly from `kedro.io`).\n* If you have any `pandas.AppendableExcelDataSet` entries in your catalog, replace them with `pandas.ExcelDataSet`.\n* If you have any `networkx.NetworkXDataSet` entries in your catalog, replace them with `networkx.JSONDataSet`.\n\n### Other\n* Edit any scripts containing `kedro pipeline package --version` to use `kedro micropkg package` instead. If you wish to set a specific pipeline package version, set the `__version__` variable in the pipeline package's `__init__.py` file.\n* To run a pipeline in parallel, use `kedro run --runner=ParallelRunner` rather than `--parallel` or `-p`.\n* If you call `ConfigLoader` or `TemplatedConfigLoader` directly, update the keyword arguments `conf_root` to `conf_source` and `extra_params` to `runtime_params`.\n* If you use `KedroContext` to access `ConfigLoader`, use `settings.CONFIG_LOADER_CLASS` to access the currently used `ConfigLoader` instead.\n* The signature of `KedroContext` has changed and now needs `config_loader` and `hook_manager` as additional arguments of type `ConfigLoader` and `PluginManager` respectively.\n\n# Release 0.17.7\n\n## Major features and improvements\n* `pipeline` now accepts `tags` and a collection of `Node`s and/or `Pipeline`s rather than just a single `Pipeline` object. `pipeline` should be used in preference to `Pipeline` when creating a Kedro pipeline.\n* `pandas.SQLTableDataSet` and `pandas.SQLQueryDataSet` now only open one connection per database, at instantiation time (therefore at catalog creation time), rather than one per load/save operation.\n* Added new command group, `micropkg`, to replace `kedro pipeline pull` and `kedro pipeline package` with `kedro micropkg pull` and `kedro micropkg package` for Kedro 0.18.0. `kedro micropkg package` saves packages to `project/dist` while `kedro pipeline package` saves packages to `project/src/dist`.\n\n## Bug fixes and other changes\n* Added tutorial documentation for [experiment tracking](https://docs.kedro.org/en/0.17.7/08_logging/02_experiment_tracking.html).\n* Added [Plotly dataset documentation](https://docs.kedro.org/en/0.17.7/03_tutorial/05_visualise_pipeline.html#visualise-plotly-charts-in-kedro-viz).\n* Added the upper limit `pandas<1.4` to maintain compatibility with `xlrd~=1.0`.\n* Bumped the `Pillow` minimum version requirement to 9.0 (Python 3.7+ only) following [CVE-2022-22817](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22817).\n* Fixed `PickleDataSet` to be copyable and hence work with the parallel runner.\n* Upgraded `pip-tools`, which is used by `kedro build-reqs`, to 6.5 (Python 3.7+ only). This `pip-tools` version is compatible with `pip>=21.2`, including the most recent releases of `pip`. Python 3.6 users should continue to use `pip-tools` 6.4 and `pip<22`.\n* Added `astro-iris` as alias for `astro-airlow-iris`, so that old tutorials can still be followed.\n* Added details about [Kedro's Technical Steering Committee and governance model](https://docs.kedro.org/en/0.17.7/14_contribution/technical_steering_committee.html).\n\n## Upcoming deprecations for Kedro 0.18.0\n* `kedro pipeline pull` and `kedro pipeline package` will be deprecated. Please use `kedro micropkg` instead.\n\n\n# Release 0.17.6\n\n## Major features and improvements\n* Added `pipelines` global variable to IPython extension, allowing you to access the project's pipelines in `kedro ipython` or `kedro jupyter notebook`.\n* Enabled overriding nested parameters with `params` in CLI, i.e. `kedro run --params=\"model.model_tuning.booster:gbtree\"` updates parameters to `{\"model\": {\"model_tuning\": {\"booster\": \"gbtree\"}}}`.\n* Added option to `pandas.SQLQueryDataSet` to specify a `filepath` with a SQL query, in addition to the current method of supplying the query itself in the `sql` argument.\n* Extended `ExcelDataSet` to support saving Excel files with multiple sheets.\n* Added the following new datasets:\n\n| Type                      | Description                                                                                                            | Location                       |\n| ------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ------------------------------ |\n| `plotly.JSONDataSet`      | Works with plotly graph object Figures (saves as json file)                                                            | `kedro.extras.datasets.plotly` |\n| `pandas.GenericDataSet`   | Provides a 'best effort' facility to read / write any format provided by the `pandas` library                          | `kedro.extras.datasets.pandas` |\n| `pandas.GBQQueryDataSet`  | Loads data from a Google Bigquery table using provided SQL query                                                       | `kedro.extras.datasets.pandas` |\n| `spark.DeltaTableDataSet` | Dataset designed to handle Delta Lake Tables and their CRUD-style operations, including `update`, `merge` and `delete` | `kedro.extras.datasets.spark`  |\n\n## Bug fixes and other changes\n* Fixed an issue where `kedro new --config config.yml` was ignoring the config file when `prompts.yml` didn't exist.\n* Added documentation for `kedro viz --autoreload`.\n* Added support for arbitrary backends (via importable module paths) that satisfy the `pickle` interface to `PickleDataSet`.\n* Added support for `sum` syntax for connecting pipeline objects.\n* Upgraded `pip-tools`, which is used by `kedro build-reqs`, to 6.4. This `pip-tools` version requires `pip>=21.2` while [adding support for `pip>=21.3`](https://github.com/jazzband/pip-tools/pull/1501). To upgrade `pip`, please refer to [their documentation](https://pip.pypa.io/en/stable/installing/#upgrading-pip).\n* Relaxed the bounds on the `plotly` requirement for `plotly.PlotlyDataSet` and the `pyarrow` requirement for `pandas.ParquetDataSet`.\n* `kedro pipeline package <pipeline>` now raises an error if the `<pipeline>` argument doesn't look like a valid Python module path (e.g. has `/` instead of `.`).\n* Added new `overwrite` argument to `PartitionedDataSet` and `MatplotlibWriter` to enable deletion of existing partitions and plots on dataset `save`.\n* `kedro pipeline pull` now works when the project requirements contains entries such as `-r`, `--extra-index-url` and local wheel files ([Issue #913](https://github.com/kedro-org/kedro/issues/913)).\n* Fixed slow startup because of catalog processing by reducing the exponential growth of extra processing during `_FrozenDatasets` creations.\n* Removed `.coveragerc` from the Kedro project template. `coverage` settings are now given in `pyproject.toml`.\n* Fixed a bug where packaging or pulling a modular pipeline with the same name as the project's package name would throw an error (or silently pass without including the pipeline source code in the wheel file).\n* Removed unintentional dependency on `git`.\n* Fixed an issue where nested pipeline configuration was not included in the packaged pipeline.\n* Deprecated the \"Thanks for supporting contributions\" section of release notes to simplify the contribution process; Kedro 0.17.6 is the last release that includes this. This process has been replaced with the [automatic GitHub feature](https://github.com/kedro-org/kedro/graphs/contributors).\n* Fixed a bug where the version on the tracking datasets didn't match the session id and the versions of regular versioned datasets.\n* Fixed an issue where datasets in `load_versions` that are not found in the data catalog would silently pass.\n* Altered the string representation of nodes so that node inputs/outputs order is preserved rather than being alphabetically sorted.\n* Update `APIDataSet` to accept `auth` through `credentials` and allow any iterable for `auth`.\n\n## Upcoming deprecations for Kedro 0.18.0\n* `kedro.extras.decorators` and `kedro.pipeline.decorators` are being deprecated in favour of Hooks.\n* `kedro.extras.transformers` and `kedro.io.transformers` are being deprecated in favour of Hooks.\n* The `--parallel` flag on `kedro run` is being removed in favour of `--runner=ParallelRunner`. The `-p` flag will change to be an alias for `--pipeline`.\n* `kedro.io.DataCatalogWithDefault` is being deprecated, to be removed entirely in 0.18.0.\n\n## Thanks for supporting contributions\n[Deepyaman Datta](https://github.com/deepyaman),\n[Brites](https://github.com/brites101),\n[Manish Swami](https://github.com/ManishS6),\n[Avaneesh Yembadi](https://github.com/avan-sh),\n[Zain Patel](https://github.com/mzjp2),\n[Simon Brugman](https://github.com/sbrugman),\n[Kiyo Kunii](https://github.com/921kiyo),\n[Benjamin Levy](https://github.com/BenjaminLevyQB),\n[Louis de Charsonville](https://github.com/louisdecharson),\n[Simon Picard](https://github.com/simonpicard)\n\n# Release 0.17.5\n\n## Major features and improvements\n* Added new CLI group `registry`, with the associated commands `kedro registry list` and `kedro registry describe`, to replace `kedro pipeline list` and `kedro pipeline describe`.\n* Added support for dependency management at a modular pipeline level. When a pipeline with `requirements.txt` is packaged, its dependencies are embedded in the modular pipeline wheel file. Upon pulling the pipeline, Kedro will append dependencies to the project's `requirements.in`. More information is available in [our documentation](https://docs.kedro.org/en/0.17.5/06_nodes_and_pipelines/03_modular_pipelines.html).\n* Added support for bulk packaging/pulling modular pipelines using `kedro pipeline package/pull --all` and `pyproject.toml`.\n* Removed `cli.py` from the Kedro project template. By default all CLI commands, including `kedro run`, are now defined on the Kedro framework side. These can be overridden in turn by a plugin or a `cli.py` file in your project. A packaged Kedro project will respect the same hierarchy when executed with `python -m my_package`.\n* Removed `.ipython/profile_default/startup/` from the Kedro project template in favour of `.ipython/profile_default/ipython_config.py` and the `kedro.extras.extensions.ipython`.\n* Added support for `dill` backend to `PickleDataSet`.\n* Imports are now refactored at `kedro pipeline package` and `kedro pipeline pull` time, so that _aliasing_ a modular pipeline doesn't break it.\n* Added the following new datasets to support basic Experiment Tracking:\n\n| Type                      | Description                                              | Location                         |\n| ------------------------- | -------------------------------------------------------- | -------------------------------- |\n| `tracking.MetricsDataSet` | Dataset to track numeric metrics for experiment tracking | `kedro.extras.datasets.tracking` |\n| `tracking.JSONDataSet`    | Dataset to track data for experiment tracking            | `kedro.extras.datasets.tracking` |\n\n## Bug fixes and other changes\n* Bumped minimum required `fsspec` version to 2021.04.\n* Fixed the `kedro install` and `kedro build-reqs` flows when uninstalled dependencies are present in a project's `settings.py`, `context.py` or `hooks.py` ([Issue #829](https://github.com/kedro-org/kedro/issues/829)).\n* Imports are now refactored at `kedro pipeline package` and `kedro pipeline pull` time, so that _aliasing_ a modular pipeline doesn't break it.\n\n## Minor breaking changes to the API\n* Pinned `dynaconf` to `<3.1.6` because the method signature for `_validate_items` changed which is used in Kedro.\n\n## Upcoming deprecations for Kedro 0.18.0\n* `kedro pipeline list` and `kedro pipeline describe` are being deprecated in favour of new commands `kedro registry list ` and `kedro registry describe`.\n* `kedro install` is being deprecated in favour of using `pip install -r src/requirements.txt` to install project dependencies.\n\n## Thanks for supporting contributions\n[Moussa Taifi](https://github.com/moutai),\n[Deepyaman Datta](https://github.com/deepyaman)\n\n# Release 0.17.4\n\n## Major features and improvements\n* Added the following new datasets:\n\n| Type                   | Description                                                 | Location                       |\n| ---------------------- | ----------------------------------------------------------- | ------------------------------ |\n| `plotly.PlotlyDataSet` | Works with plotly graph object Figures (saves as json file) | `kedro.extras.datasets.plotly` |\n\n## Bug fixes and other changes\n* Defined our set of Kedro Principles! Have a read through [our docs](https://docs.kedro.org/en/0.17.4/12_faq/03_kedro_principles.html).\n* `ConfigLoader.get()` now raises a `BadConfigException`, with a more helpful error message, if a configuration file cannot be loaded (for instance due to wrong syntax or poor formatting).\n* `run_id` now defaults to `save_version` when `after_catalog_created` is called, similarly to what happens during a `kedro run`.\n* Fixed a bug where `kedro ipython` and `kedro jupyter notebook` didn't work if the `PYTHONPATH` was already set.\n* Update the IPython extension to allow passing `env` and `extra_params` to `reload_kedro`  similar to how the IPython script works.\n* `kedro info` now outputs if a plugin has any `hooks` or `cli_hooks` implemented.\n* `PartitionedDataSet` now supports lazily materializing data on save.\n* `kedro pipeline describe` now defaults to the `__default__` pipeline when no pipeline name is provided and also shows the namespace the nodes belong to.\n* Fixed an issue where spark.SparkDataSet with enabled versioning would throw a VersionNotFoundError when using databricks-connect from a remote machine and saving to dbfs filesystem.\n* `EmailMessageDataSet` added to doctree.\n* When node inputs do not pass validation, the error message is now shown as the most recent exception in the traceback ([Issue #761](https://github.com/kedro-org/kedro/issues/761)).\n* `kedro pipeline package` now only packages the parameter file that exactly matches the pipeline name specified and the parameter files in a directory with the pipeline name.\n* Extended support to newer versions of third-party dependencies ([Issue #735](https://github.com/kedro-org/kedro/issues/735)).\n* Ensured consistent references to `model input` tables in accordance with our Data Engineering convention.\n* Changed behaviour where `kedro pipeline package` takes the pipeline package version, rather than the kedro package version. If the pipeline package version is not present, then the package version is used.\n* Launched [GitHub Discussions](https://github.com/kedro-org/kedro/discussions/) and [Kedro Discord Server](https://discord.gg/akJDeVaxnB)\n* Improved error message when versioning is enabled for a dataset previously saved as non-versioned ([Issue #625](https://github.com/kedro-org/kedro/issues/625)).\n\n## Minor breaking changes to the API\n\n## Upcoming deprecations for Kedro 0.18.0\n\n## Thanks for supporting contributions\n[Lou Kratz](https://github.com/lou-k),\n[Lucas Jamar](https://github.com/lucasjamar)\n\n# Release 0.17.3\n\n## Major features and improvements\n* Kedro plugins can now override built-in CLI commands.\n* Added a `before_command_run` hook for plugins to add extra behaviour before Kedro CLI commands run.\n* `pipelines` from `pipeline_registry.py` and `register_pipeline` hooks are now loaded lazily when they are first accessed, not on startup:\n\n    ```python\n    from kedro.framework.project import pipelines\n\n    print(pipelines[\"__default__\"])  # pipeline loading is only triggered here\n    ```\n\n## Bug fixes and other changes\n* `TemplatedConfigLoader` now correctly inserts default values when no globals are supplied.\n* Fixed a bug where the `KEDRO_ENV` environment variable had no effect on instantiating the `context` variable in an iPython session or a Jupyter notebook.\n* Plugins with empty CLI groups are no longer displayed in the Kedro CLI help screen.\n* Duplicate commands will no longer appear twice in the Kedro CLI help screen.\n* CLI commands from sources with the same name will show under one list in the help screen.\n* The setup of a Kedro project, including adding src to path and configuring settings, is now handled via the `bootstrap_project` method.\n* `configure_project` is invoked if a `package_name` is supplied to `KedroSession.create`. This is added for backward-compatibility purpose to support a workflow that creates `Session` manually. It will be removed in `0.18.0`.\n* Stopped swallowing up all `ModuleNotFoundError` if `register_pipelines` not found, so that a more helpful error message will appear when a dependency is missing, e.g. [Issue #722](https://github.com/kedro-org/kedro/issues/722).\n* When `kedro new` is invoked using a configuration yaml file, `output_dir` is no longer a required key; by default the current working directory will be used.\n* When `kedro new` is invoked using a configuration yaml file, the appropriate `prompts.yml` file is now used for validating the provided configuration. Previously, validation was always performed against the kedro project template `prompts.yml` file.\n* When a relative path to a starter template is provided, `kedro new` now generates user prompts to obtain configuration rather than supplying empty configuration.\n* Fixed error when using starters on Windows with Python 3.7 (Issue [#722](https://github.com/kedro-org/kedro/issues/722)).\n* Fixed decoding error of config files that contain accented characters by opening them for reading in UTF-8.\n* Fixed an issue where `after_dataset_loaded` run would finish before a dataset is actually loaded when using `--async` flag.\n\n## Upcoming deprecations for Kedro 0.18.0\n\n* `kedro.versioning.journal.Journal` will be removed.\n* The following properties on `kedro.framework.context.KedroContext` will be removed:\n  * `io` in favour of `KedroContext.catalog`\n  * `pipeline` (equivalent to `pipelines[\"__default__\"]`)\n  * `pipelines` in favour of `kedro.framework.project.pipelines`\n\n# Release 0.17.2\n\n## Major features and improvements\n* Added support for `compress_pickle` backend to `PickleDataSet`.\n* Enabled loading pipelines without creating a `KedroContext` instance:\n\n    ```python\n    from kedro.framework.project import pipelines\n\n    print(pipelines)\n    ```\n\n* Projects generated with kedro>=0.17.2:\n  - should define pipelines in `pipeline_registry.py` rather than `hooks.py`.\n  - when run as a package, will behave the same as `kedro run`\n\n## Bug fixes and other changes\n* If `settings.py` is not importable, the errors will be surfaced earlier in the process, rather than at runtime.\n\n## Minor breaking changes to the API\n* `kedro pipeline list` and `kedro pipeline describe` no longer accept redundant `--env` parameter.\n* `from kedro.framework.cli.cli import cli` no longer includes the `new` and `starter` commands.\n\n## Upcoming deprecations for Kedro 0.18.0\n\n* `kedro.framework.context.KedroContext.run` will be removed in release 0.18.0.\n\n## Thanks for supporting contributions\n[Sasaki Takeru](https://github.com/takeru)\n\n# Release 0.17.1\n\n## Major features and improvements\n* Added `env` and `extra_params` to `reload_kedro()` line magic.\n* Extended the `pipeline()` API to allow strings and sets of strings as `inputs` and `outputs`, to specify when a dataset name remains the same (not namespaced).\n* Added the ability to add custom prompts with regexp validator for starters by repurposing `default_config.yml` as `prompts.yml`.\n* Added the `env` and `extra_params` arguments to `register_config_loader` hook.\n* Refactored the way `settings` are loaded. You will now be able to run:\n\n    ```python\n    from kedro.framework.project import settings\n\n    print(settings.CONF_ROOT)\n    ```\n\n* Added a check on `kedro.runner.parallel_runner.ParallelRunner` which checks datasets for the `_SINGLE_PROCESS` attribute in the `_validate_catalog` method. If this attribute is set to `True` in an instance of a dataset (e.g. `SparkDataSet`), the `ParallelRunner` will raise an `AttributeError`.\n* Any user-defined dataset that should not be used with `ParallelRunner` may now have the `_SINGLE_PROCESS` attribute set to `True`.\n\n## Bug fixes and other changes\n* The version of a packaged modular pipeline now defaults to the version of the project package.\n* Added fix to prevent new lines being added to pandas CSV datasets.\n* Fixed issue with loading a versioned `SparkDataSet` in the interactive workflow.\n* Kedro CLI now checks `pyproject.toml` for a `tool.kedro` section before treating the project as a Kedro project.\n* Added fix to `DataCatalog::shallow_copy` now it should copy layers.\n* `kedro pipeline pull` now uses `pip download` for protocols that are not supported by `fsspec`.\n* Cleaned up documentation to fix broken links and rewrite permanently redirected ones.\n* Added a `jsonschema` schema definition for the Kedro 0.17 catalog.\n* `kedro install` now waits on Windows until all the requirements are installed.\n* Exposed `--to-outputs` option in the CLI, throughout the codebase, and as part of hooks specifications.\n* Fixed a bug where `ParquetDataSet` wasn't creating parent directories on the fly.\n* Updated documentation.\n\n## Breaking changes to the API\n* This release has broken the `kedro ipython` and `kedro jupyter` workflows. To fix this, follow the instructions in the migration guide below.\n* You will also need to upgrade `kedro-viz` to 3.10.1 if you use the `%run_viz` line magic in Jupyter Notebook.\n\n> *Note:* If you're using the `ipython` [extension](https://docs.kedro.org/en/0.17.1/11_tools_integration/02_ipython.html#ipython-extension) instead, you will not encounter this problem.\n\n## Migration guide\nYou will have to update the file `<your_project>/.ipython/profile_default/startup/00-kedro-init.py` in order to make `kedro ipython` and/or `kedro jupyter` work. Add the following line before the `KedroSession` is created:\n\n```python\nconfigure_project(metadata.package_name)  # to add\n\nsession = KedroSession.create(metadata.package_name, path)\n```\n\nMake sure that the associated import is provided in the same place as others in the file:\n\n```python\nfrom kedro.framework.project import configure_project  # to add\nfrom kedro.framework.session import KedroSession\n```\n\n## Thanks for supporting contributions\n[Mariana Silva](https://github.com/marianansilva),\n[Kiyohito Kunii](https://github.com/921kiyo),\n[noklam](https://github.com/noklam),\n[Ivan Doroshenko](https://github.com/imdoroshenko),\n[Zain Patel](https://github.com/mzjp2),\n[Deepyaman Datta](https://github.com/deepyaman),\n[Sam Hiscox](https://github.com/samhiscoxqb),\n[Pascal Brokmeier](https://github.com/pascalwhoop)\n\n# Release 0.17.0\n\n## Major features and improvements\n\n* In a significant change, [we have introduced `KedroSession`](https://docs.kedro.org/en/0.17.0/04_kedro_project_setup/03_session.html) which is responsible for managing the lifecycle of a Kedro run.\n* Created a new Kedro Starter: `kedro new --starter=mini-kedro`. It is possible to [use the DataCatalog as a standalone component](https://github.com/kedro-org/kedro-starters/tree/master/mini-kedro) in a Jupyter notebook and transition into the rest of the Kedro framework.\n* Added `DatasetSpecs` with Hooks to run before and after datasets are loaded from/saved to the catalog.\n* Added a command: `kedro catalog create`. For a registered pipeline, it creates a `<conf_root>/<env>/catalog/<pipeline_name>.yml` configuration file with `MemoryDataSet` datasets for each dataset that is missing from `DataCatalog`.\n* Added `settings.py` and `pyproject.toml` (to replace `.kedro.yml`) for project configuration, in line with Python best practice.\n* `ProjectContext` is no longer needed, unless for very complex customisations. `KedroContext`, `ProjectHooks` and `settings.py` together implement sensible default behaviour. As a result `context_path` is also now an _optional_ key in `pyproject.toml`.\n* Removed `ProjectContext` from `src/<package_name>/run.py`.\n* `TemplatedConfigLoader` now supports [Jinja2 template syntax](https://jinja.palletsprojects.com/en/2.11.x/templates/) alongside its original syntax.\n* Made [registration Hooks](https://docs.kedro.org/en/0.17.0/07_extend_kedro/02_hooks.html#registration-hooks) mandatory, as the only way to customise the `ConfigLoader` or the `DataCatalog` used in a project. If no such Hook is provided in `src/<package_name>/hooks.py`, a `KedroContextError` is raised. There are sensible defaults defined in any project generated with Kedro >= 0.16.5.\n\n## Bug fixes and other changes\n\n* `ParallelRunner` no longer results in a run failure, when triggered from a notebook, if the run is started using `KedroSession` (`session.run()`).\n* `before_node_run` can now overwrite node inputs by returning a dictionary with the corresponding updates.\n* Added minimal, black-compatible flake8 configuration to the project template.\n* Moved `isort` and `pytest` configuration from `<project_root>/setup.cfg` to `<project_root>/pyproject.toml`.\n* Extra parameters are no longer incorrectly passed from `KedroSession` to `KedroContext`.\n* Relaxed `pyspark` requirements to allow for installation of `pyspark` 3.0.\n* Added a `--fs-args` option to the `kedro pipeline pull` command to specify configuration options for the `fsspec` filesystem arguments used when pulling modular pipelines from non-PyPI locations.\n* Bumped maximum required `fsspec` version to 0.9.\n* Bumped maximum supported `s3fs` version to 0.5 (`S3FileSystem` interface has changed since 0.4.1 version).\n\n## Deprecations\n* In Kedro 0.17.0 we have deleted the deprecated `kedro.cli` and `kedro.context` modules in favour of `kedro.framework.cli` and `kedro.framework.context` respectively.\n\n## Other breaking changes to the API\n* `kedro.io.DataCatalog.exists()` returns `False` when the dataset does not exist, as opposed to raising an exception.\n* The pipeline-specific `catalog.yml` file is no longer automatically created for modular pipelines when running `kedro pipeline create`. Use `kedro catalog create` to replace this functionality.\n* Removed `include_examples` prompt from `kedro new`. To generate boilerplate example code, you should use a Kedro starter.\n* Changed the `--verbose` flag from a global command to a project-specific command flag (e.g `kedro --verbose new` becomes `kedro new --verbose`).\n* Dropped support of the `dataset_credentials` key in credentials in `PartitionedDataSet`.\n* `get_source_dir()` was removed from `kedro/framework/cli/utils.py`.\n* Dropped support of `get_config`, `create_catalog`, `create_pipeline`, `template_version`, `project_name` and `project_path` keys by `get_project_context()` function (`kedro/framework/cli/cli.py`).\n* `kedro new --starter` now defaults to fetching the starter template matching the installed Kedro version.\n* Renamed `kedro_cli.py` to `cli.py` and moved it inside the Python package (`src/<package_name>/`), for a better packaging and deployment experience.\n* Removed `.kedro.yml` from the project template and replaced it with `pyproject.toml`.\n* Removed `KEDRO_CONFIGS` constant (previously residing in `kedro.framework.context.context`).\n* Modified `kedro pipeline create` CLI command to add a boilerplate parameter config file in `conf/<env>/parameters/<pipeline_name>.yml` instead of `conf/<env>/pipelines/<pipeline_name>/parameters.yml`. CLI commands `kedro pipeline delete` / `package` / `pull` were updated accordingly.\n* Removed `get_static_project_data` from `kedro.framework.context`.\n* Removed `KedroContext.static_data`.\n* The `KedroContext` constructor now takes `package_name` as first argument.\n* Replaced `context` property on `KedroSession` with `load_context()` method.\n* Renamed `_push_session` and `_pop_session` in `kedro.framework.session.session` to `_activate_session` and `_deactivate_session` respectively.\n* Custom context class is set via `CONTEXT_CLASS` variable in `src/<your_project>/settings.py`.\n* Removed `KedroContext.hooks` attribute. Instead, hooks should be registered in `src/<your_project>/settings.py` under the `HOOKS` key.\n* Restricted names given to nodes to match the regex pattern `[\\w\\.-]+$`.\n* Removed `KedroContext._create_config_loader()` and `KedroContext._create_data_catalog()`. They have been replaced by registration hooks, namely `register_config_loader()` and `register_catalog()` (see also [upcoming deprecations](#upcoming_deprecations_for_kedro_0.18.0)).\n\n\n## Upcoming deprecations for Kedro 0.18.0\n\n* `kedro.framework.context.load_context` will be removed in release 0.18.0.\n* `kedro.framework.cli.get_project_context` will be removed in release 0.18.0.\n* We've added a `DeprecationWarning` to the decorator API for both `node` and `pipeline`. These will be removed in release 0.18.0. Use Hooks to extend a node's behaviour instead.\n* We've added a `DeprecationWarning` to the Transformers API when adding a transformer to the catalog. These will be removed in release 0.18.0. Use Hooks to customise the `load` and `save` methods.\n\n## Thanks for supporting contributions\n[Deepyaman Datta](https://github.com/deepyaman),\n[Zach Schuster](https://github.com/zschuster)\n\n## Migration guide from Kedro 0.16.* to 0.17.*\n\n**Reminder:** Our documentation on [how to upgrade Kedro](https://docs.kedro.org/en/0.17.0/12_faq/01_faq.html#how-do-i-upgrade-kedro) covers a few key things to remember when updating any Kedro version.\n\nThe Kedro 0.17.0 release contains some breaking changes. If you update Kedro to 0.17.0 and then try to work with projects created against earlier versions of Kedro, you may encounter some issues when trying to run `kedro` commands in the terminal for that project. Here's a short guide to getting your projects running against the new version of Kedro.\n\n\n>*Note*: As always, if you hit any problems, please check out our documentation:\n>* [How can I find out more about Kedro?](https://docs.kedro.org/en/0.17.0/12_faq/01_faq.html#how-can-i-find-out-more-about-kedro)\n>* [How can I get my questions answered?](https://docs.kedro.org/en/0.17.0/12_faq/01_faq.html#how-can-i-get-my-question-answered).\n\nTo get an existing Kedro project to work after you upgrade to Kedro 0.17.0, we recommend that you create a new project against Kedro 0.17.0 and move the code from your existing project into it. Let's go through the changes, but first, note that if you create a new Kedro project with Kedro 0.17.0 you will not be asked whether you want to include the boilerplate code for the Iris dataset example. We've removed this option (you should now use a Kedro starter if you want to create a project that is pre-populated with code).\n\nTo create a new, blank Kedro 0.17.0 project to drop your existing code into, you can create one, as always, with `kedro new`. We also recommend creating a new virtual environment for your new project, or you might run into conflicts with existing dependencies.\n\n* **Update `pyproject.toml`**: Copy the following three keys from the `.kedro.yml` of your existing Kedro project into the `pyproject.toml` file of your new Kedro 0.17.0 project:\n\n\n    ```toml\n    [tools.kedro]\n    package_name = \"<package_name>\"\n    project_name = \"<project_name>\"\n    project_version = \"0.17.0\"\n    ```\n\nCheck your source directory. If you defined a different source directory (`source_dir`), make sure you also move that to `pyproject.toml`.\n\n\n* **Copy files from your existing project**:\n\n  + Copy subfolders of `project/src/project_name/pipelines` from existing to new project\n  + Copy subfolders of `project/src/test/pipelines` from existing to new project\n  + Copy the requirements your project needs into `requirements.txt` and/or `requirements.in`.\n  + Copy your project configuration from the `conf` folder. Take note of the new locations needed for modular pipeline configuration (move it from `conf/<env>/pipeline_name/catalog.yml` to `conf/<env>/catalog/pipeline_name.yml` and likewise for `parameters.yml`).\n  + Copy from the `data/` folder of your existing project, if needed, into the same location in your new project.\n  + Copy any Hooks from `src/<package_name>/hooks.py`.\n\n* **Update your new project's README and docs as necessary**.\n\n* **Update `settings.py`**: For example, if you specified additional Hook implementations in `hooks`, or listed plugins under `disable_hooks_by_plugin` in your `.kedro.yml`, you will need to move them to `settings.py` accordingly:\n\n    ```python\n    from <package_name>.hooks import MyCustomHooks, ProjectHooks\n\n    HOOKS = (ProjectHooks(), MyCustomHooks())\n\n    DISABLE_HOOKS_FOR_PLUGINS = (\"my_plugin1\",)\n    ```\n\n* **Migration for `node` names**. From 0.17.0 the only allowed characters for node names are letters, digits, hyphens, underscores and/or fullstops. If you have previously defined node names that have special characters, spaces or other characters that are no longer permitted, you will need to rename those nodes.\n\n* **Copy changes to `kedro_cli.py`**. If you previously customised the `kedro run` command or added more CLI commands to your `kedro_cli.py`, you should move them into `<project_root>/src/<package_name>/cli.py`. Note, however, that the new way to run a Kedro pipeline is via a `KedroSession`, rather than using the `KedroContext`:\n\n    ```python\n    with KedroSession.create(package_name=...) as session:\n        session.run()\n    ```\n\n* **Copy changes made to `ConfigLoader`**. If you have defined a custom class, such as `TemplatedConfigLoader`, by overriding `ProjectContext._create_config_loader`, you should move the contents of the function in `src/<package_name>/hooks.py`, under `register_config_loader`.\n\n* **Copy changes made to `DataCatalog`**. Likewise, if you have `DataCatalog` defined with `ProjectContext._create_catalog`, you should copy-paste the contents into `register_catalog`.\n\n* **Optional**: If you have plugins such as [Kedro-Viz](https://github.com/kedro-org/kedro-viz) installed, it's likely that Kedro 0.17.0 won't work with their older versions, so please either upgrade to the plugin's newest version or follow their migration guides.\n\n# Release 0.16.6\n\n## Major features and improvements\n\n* Added documentation with a focus on single machine and distributed environment deployment; the series includes Docker, Argo, Prefect, Kubeflow, AWS Batch, AWS Sagemaker and extends our section on Databricks.\n* Added [kedro-starter-spaceflights](https://github.com/kedro-org/kedro-starter-spaceflights/) alias for generating a project: `kedro new --starter spaceflights`.\n\n## Bug fixes and other changes\n* Fixed `TypeError` when converting dict inputs to a node made from a wrapped `partial` function.\n* `PartitionedDataSet` improvements:\n  - Supported passing arguments to the underlying filesystem.\n* Improved handling of non-ASCII word characters in dataset names.\n  - For example, a dataset named `jalape√±o` will be accessible as `DataCatalog.datasets.jalape√±o` rather than `DataCatalog.datasets.jalape__o`.\n* Fixed `kedro install` for an Anaconda environment defined in `environment.yml`.\n* Fixed backwards compatibility with templates generated with older Kedro versions <0.16.5. No longer need to update `.kedro.yml` to use `kedro lint` and `kedro jupyter notebook convert`.\n* Improved documentation.\n* Added documentation using MinIO with Kedro.\n* Improved error messages for incorrect parameters passed into a node.\n* Fixed issue with saving a `TensorFlowModelDataset` in the HDF5 format with versioning enabled.\n* Added missing `run_result` argument in `after_pipeline_run` Hooks spec.\n* Fixed a bug in IPython script that was causing context hooks to be registered twice. To apply this fix to a project generated with an older Kedro version, apply the same changes made in [this PR](https://github.com/kedro-org/kedro-starter-pandas-iris/pull/16) to your `00-kedro-init.py` file.\n* Improved documentation.\n\n## Breaking changes to the API\n\n## Thanks for supporting contributions\n[Deepyaman Datta](https://github.com/deepyaman), [Bhavya Merchant](https://github.com/bnmerchant), [Lovkush Agarwal](https://github.com/Lovkush-A), [Varun Krishna S](https://github.com/vhawk19), [Sebastian Bertoli](https://github.com/sebastianbertoli), [noklam](https://github.com/noklam), [Daniel Petti](https://github.com/djpetti), [Waylon Walker](https://github.com/waylonwalker), [Saran Balaji C](https://github.com/csaranbalaji)\n\n# Release 0.16.5\n\n## Major features and improvements\n* Added the following new datasets.\n\n| Type                        | Description                                                                                             | Location                      |\n| --------------------------- | ------------------------------------------------------------------------------------------------------- | ----------------------------- |\n| `email.EmailMessageDataSet` | Manage email messages using [the Python standard library](https://docs.python.org/3/library/email.html) | `kedro.extras.datasets.email` |\n\n* Added support for `pyproject.toml` to configure Kedro. `pyproject.toml` is used if `.kedro.yml` doesn't exist (Kedro configuration should be under `[tool.kedro]` section).\n* Projects created with this version will have no `pipeline.py`, having been replaced by `hooks.py`.\n* Added a set of registration hooks, as the new way of registering library components with a Kedro project:\n    * `register_pipelines()`, to replace `_get_pipelines()`\n    * `register_config_loader()`, to replace `_create_config_loader()`\n    * `register_catalog()`, to replace `_create_catalog()`\nThese can be defined in `src/<python_package>/hooks.py` and added to `.kedro.yml` (or `pyproject.toml`). The order of execution is: plugin hooks, `.kedro.yml` hooks, hooks in `ProjectContext.hooks`.\n* Added ability to disable auto-registered Hooks using `.kedro.yml` (or `pyproject.toml`) configuration file.\n\n## Bug fixes and other changes\n* Added option to run asynchronously via the Kedro CLI.\n* Absorbed `.isort.cfg` settings into `setup.cfg`.\n* Packaging a modular pipeline raises an error if the pipeline directory is empty or non-existent.\n\n## Breaking changes to the API\n* `project_name`, `project_version` and `package_name` now have to be defined in `.kedro.yml` for projects using Kedro 0.16.5+.\n\n## Migration Guide\nThis release has accidentally broken the usage of `kedro lint` and `kedro jupyter notebook convert` on a project template generated with previous versions of Kedro (<=0.16.4). To amend this, please either upgrade to `kedro==0.16.6` or update `.kedro.yml` within your project root directory to include the following keys:\n\n```yaml\nproject_name: \"<your_project_name>\"\nproject_version: \"<kedro_version_of_the_project>\"\npackage_name: \"<your_package_name>\"\n```\n\n## Thanks for supporting contributions\n[Deepyaman Datta](https://github.com/deepyaman), [Bas Nijholt](https://github.com/basnijholt), [Sebastian Bertoli](https://github.com/sebastianbertoli)\n\n# Release 0.16.4\n\n## Major features and improvements\n* Fixed a bug for using `ParallelRunner` on Windows.\n* Enabled auto-discovery of hooks implementations coming from installed plugins.\n\n## Bug fixes and other changes\n* Fixed a bug for using `ParallelRunner` on Windows.\n* Modified `GBQTableDataSet` to load customized results using customized queries from Google Big Query tables.\n* Documentation improvements.\n\n## Breaking changes to the API\n\n## Thanks for supporting contributions\n[Ajay Bisht](https://github.com/ajb7), [Vijay Sajjanar](https://github.com/vjkr), [Deepyaman Datta](https://github.com/deepyaman), [Sebastian Bertoli](https://github.com/sebastianbertoli), [Shahil Mawjee](https://github.com/s-mawjee), [Louis Guitton](https://github.com/louisguitton), [Emanuel Ferm](https://github.com/eferm)\n\n# Release 0.16.3\n\n## Major features and improvements\n* Added the `kedro pipeline pull` CLI command to extract a packaged modular pipeline, and place the contents in a Kedro project.\n* Added the `--version` option to `kedro pipeline package` to allow specifying alternative versions to package under.\n* Added the `--starter` option to `kedro new` to create a new project from a local, remote or aliased starter template.\n* Added the `kedro starter list` CLI command to list all starter templates that can be used to bootstrap a new Kedro project.\n* Added the following new datasets.\n\n| Type               | Description                                                                                           | Location                     |\n| ------------------ | ----------------------------------------------------------------------------------------------------- | ---------------------------- |\n| `json.JSONDataSet` | Work with JSON files using [the Python standard library](https://docs.python.org/3/library/json.html) | `kedro.extras.datasets.json` |\n\n## Bug fixes and other changes\n* Removed `/src/nodes` directory from the project template and made `kedro jupyter convert` create it on the fly if necessary.\n* Fixed a bug in `MatplotlibWriter` which prevented saving lists and dictionaries of plots locally on Windows.\n* Closed all pyplot windows after saving in `MatplotlibWriter`.\n* Documentation improvements:\n  - Added [kedro-wings](https://github.com/tamsanh/kedro-wings) and [kedro-great](https://github.com/tamsanh/kedro-great) to the list of community plugins.\n* Fixed broken versioning for Windows paths.\n* Fixed `DataSet` string representation for falsy values.\n* Improved the error message when duplicate nodes are passed to the `Pipeline` initializer.\n* Fixed a bug where `kedro docs` would fail because the built docs were located in a different directory.\n* Fixed a bug where `ParallelRunner` would fail on Windows machines whose reported CPU count exceeded 61.\n* Fixed an issue with saving TensorFlow model to `h5` file on Windows.\n* Added a `json` parameter to `APIDataSet` for the convenience of generating requests with JSON bodies.\n* Fixed dependencies for `SparkDataSet` to include spark.\n\n## Breaking changes to the API\n\n## Thanks for supporting contributions\n[Deepyaman Datta](https://github.com/deepyaman), [Tam-Sanh Nguyen](https://github.com/tamsanh), [DataEngineerOne](http://youtube.com/DataEngineerOne)\n\n# Release 0.16.2\n\n## Major features and improvements\n* Added the following new datasets.\n\n| Type                                | Description                                                                                                          | Location                           |\n| ----------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ---------------------------------- |\n| `pandas.AppendableExcelDataSet`     | Work with `Excel` files opened in append mode                                                                        | `kedro.extras.datasets.pandas`     |\n| `tensorflow.TensorFlowModelDataset` | Work with `TensorFlow` models using [TensorFlow 2.X](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save) | `kedro.extras.datasets.tensorflow` |\n| `holoviews.HoloviewsWriter`         | Work with `Holoviews` objects (saves as image file)                                                                  | `kedro.extras.datasets.holoviews`  |\n\n* `kedro install` will now compile project dependencies (by running `kedro build-reqs` behind the scenes) before the installation if the `src/requirements.in` file doesn't exist.\n* Added `only_nodes_with_namespace` in `Pipeline` class to filter only nodes with a specified namespace.\n* Added the `kedro pipeline delete` command to help delete unwanted or unused pipelines (it won't remove references to the pipeline in your `create_pipelines()` code).\n* Added the `kedro pipeline package` command to help package up a modular pipeline. It will bundle up the pipeline source code, tests, and parameters configuration into a .whl file.\n\n## Bug fixes and other changes\n* `DataCatalog` improvements:\n  - Introduced regex filtering to the `DataCatalog.list()` method.\n  - Non-alphanumeric characters (except underscore) in dataset name are replaced with `__` in `DataCatalog.datasets`, for ease of access to transcoded datasets.\n* Dataset improvements:\n  - Improved initialization speed of `spark.SparkHiveDataSet`.\n  - Improved S3 cache in `spark.SparkDataSet`.\n  - Added support of options for building `pyarrow` table in `pandas.ParquetDataSet`.\n* `kedro build-reqs` CLI command improvements:\n  - `kedro build-reqs` is now called with `-q` option and will no longer print out compiled requirements to the console for security reasons.\n  - All unrecognized CLI options in `kedro build-reqs` command are now passed to [pip-compile](https://github.com/jazzband/pip-tools#example-usage-for-pip-compile) call (e.g. `kedro build-reqs --generate-hashes`).\n* `kedro jupyter` CLI command improvements:\n  - Improved error message when running `kedro jupyter notebook`, `kedro jupyter lab` or `kedro ipython` with Jupyter/IPython dependencies not being installed.\n  - Fixed `%run_viz` line magic for showing kedro viz inside a Jupyter notebook. For the fix to be applied on existing Kedro project, please see the migration guide.\n  - Fixed the bug in IPython startup script ([issue 298](https://github.com/kedro-org/kedro/issues/298)).\n* Documentation improvements:\n  - Updated community-generated content in FAQ.\n  - Added [find-kedro](https://github.com/WaylonWalker/find-kedro) and [kedro-static-viz](https://github.com/WaylonWalker/kedro-static-viz) to the list of community plugins.\n  - Add missing `pillow.ImageDataSet` entry to the documentation.\n\n## Breaking changes to the API\n\n### Migration guide from Kedro 0.16.1 to 0.16.2\n\n#### Guide to apply the fix for `%run_viz` line magic in existing project\n\nEven though this release ships a fix for project generated with `kedro==0.16.2`, after upgrading, you will still need to make a change in your existing project if it was generated with `kedro>=0.16.0,<=0.16.1` for the fix to take effect. Specifically, please change the content of your project's IPython init script located at `.ipython/profile_default/startup/00-kedro-init.py` with the content of [this file](https://github.com/kedro-org/kedro/blob/0.16.2/kedro/templates/project/%7B%7B%20cookiecutter.repo_name%20%7D%7D/.ipython/profile_default/startup/00-kedro-init.py). You will also need `kedro-viz>=3.3.1`.\n\n## Thanks for supporting contributions\n[Miguel Rodriguez Gutierrez](https://github.com/MigQ2), [Joel Schwarzmann](https://github.com/datajoely), [w0rdsm1th](https://github.com/w0rdsm1th), [Deepyaman Datta](https://github.com/deepyaman), [Tam-Sanh Nguyen](https://github.com/tamsanh), [Marcus Gawronsky](https://github.com/marcusinthesky)\n\n# 0.16.1\n\n## Major features and improvements\n\n## Bug fixes and other changes\n* Fixed deprecation warnings from `kedro.cli` and `kedro.context` when running `kedro jupyter notebook`.\n* Fixed a bug where `catalog` and `context` were not available in Jupyter Lab and Notebook.\n* Fixed a bug where `kedro build-reqs` would fail if you didn't have your project dependencies installed.\n\n## Breaking changes to the API\n\n## Thanks for supporting contributions\n\n# 0.16.0\n\n## Major features and improvements\n### CLI\n* Added new CLI commands (only available for the projects created using Kedro 0.16.0 or later):\n  - `kedro catalog list` to list datasets in your catalog\n  - `kedro pipeline list` to list pipelines\n  - `kedro pipeline describe` to describe a specific pipeline\n  - `kedro pipeline create` to create a modular pipeline\n* Improved the CLI speed by up to 50%.\n* Improved error handling when making a typo on the CLI. We now suggest some of the possible commands you meant to type, in `git`-style.\n\n### Framework\n* All modules in `kedro.cli` and `kedro.context` have been moved into `kedro.framework.cli` and `kedro.framework.context` respectively. `kedro.cli` and `kedro.context` will be removed in future releases.\n* Added `Hooks`, which is a new mechanism for extending Kedro.\n* Fixed `load_context` changing user's current working directory.\n* Allowed the source directory to be configurable in `.kedro.yml`.\n* Added the ability to specify nested parameter values inside your node inputs, e.g. `node(func, \"params:a.b\", None)`\n### DataSets\n* Added the following new datasets.\n\n| Type                       | Description                                 | Location                          |\n| -------------------------- | ------------------------------------------- | --------------------------------- |\n| `pillow.ImageDataSet`      | Work with image files using `Pillow`        | `kedro.extras.datasets.pillow`    |\n| `geopandas.GeoJSONDataSet` | Work with geospatial data using `GeoPandas` | `kedro.extras.datasets.geopandas` |\n| `api.APIDataSet`           | Work with data from HTTP(S) API requests    | `kedro.extras.datasets.api`       |\n\n* Added `joblib` backend support to `pickle.PickleDataSet`.\n* Added versioning support to `MatplotlibWriter` dataset.\n* Added the ability to install dependencies for a given dataset with more granularity, e.g. `pip install \"kedro[pandas.ParquetDataSet]\"`.\n* Added the ability to specify extra arguments, e.g. `encoding` or `compression`, for `fsspec.spec.AbstractFileSystem.open()` calls when loading/saving a dataset. See Example 3 under [docs](https://docs.kedro.org/en/0.16.0/04_user_guide/04_data_catalog.html#use-the-data-catalog-with-the-yaml-api).\n\n### Other\n* Added `namespace` property on ``Node``, related to the modular pipeline where the node belongs.\n* Added an option to enable asynchronous loading inputs and saving outputs in both `SequentialRunner(is_async=True)` and `ParallelRunner(is_async=True)` class.\n* Added `MemoryProfiler` transformer.\n* Removed the requirement to have all dependencies for a dataset module to use only a subset of the datasets within.\n* Added support for `pandas>=1.0`.\n* Enabled Python 3.8 compatibility. _Please note that a Spark workflow may be unreliable for this Python version as `pyspark` is not fully-compatible with 3.8 yet._\n* Renamed \"features\" layer to \"feature\" layer to be consistent with (most) other layers and the [relevant FAQ](https://docs.kedro.org/en/0.16.0/06_resources/01_faq.html#what-is-data-engineering-convention).\n\n## Bug fixes and other changes\n* Fixed a bug where a new version created mid-run by an external system caused inconsistencies in the load versions used in the current run.\n* Documentation improvements\n  * Added instruction in the documentation on how to create a custom runner).\n  * Updated contribution process in `CONTRIBUTING.md` - added Developer Workflow.\n  * Documented installation of development version of Kedro in the [FAQ section](https://docs.kedro.org/en/0.16.0/06_resources/01_faq.html#how-can-i-use-development-version-of-kedro).\n  * Added missing `_exists` method to `MyOwnDataSet` example in 04_user_guide/08_advanced_io.\n* Fixed a bug where `PartitionedDataSet` and `IncrementalDataSet` were not working with `s3a` or `s3n` protocol.\n* Added ability to read partitioned parquet file from a directory in `pandas.ParquetDataSet`.\n* Replaced `functools.lru_cache` with `cachetools.cachedmethod` in `PartitionedDataSet` and `IncrementalDataSet` for per-instance cache invalidation.\n* Implemented custom glob function for `SparkDataSet` when running on Databricks.\n* Fixed a bug in `SparkDataSet` not allowing for loading data from DBFS in a Windows machine using Databricks-connect.\n* Improved the error message for `DataSetNotFoundError` to suggest possible dataset names user meant to type.\n* Added the option for contributors to run Kedro tests locally without Spark installation with `make test-no-spark`.\n* Added option to lint the project without applying the formatting changes (`kedro lint --check-only`).\n\n## Breaking changes to the API\n### Datasets\n* Deleted obsolete datasets from `kedro.io`.\n* Deleted `kedro.contrib` and `extras` folders.\n* Deleted obsolete `CSVBlobDataSet` and `JSONBlobDataSet` dataset types.\n* Made `invalidate_cache` method on datasets private.\n* `get_last_load_version` and `get_last_save_version` methods are no longer available on `AbstractDataSet`.\n* `get_last_load_version` and `get_last_save_version` have been renamed to `resolve_load_version` and `resolve_save_version` on ``AbstractVersionedDataSet``, the results of which are cached.\n* The `release()` method on datasets extending ``AbstractVersionedDataSet`` clears the cached load and save version. All custom datasets must call `super()._release()` inside `_release()`.\n* ``TextDataSet`` no longer has `load_args` and `save_args`. These can instead be specified under `open_args_load` or `open_args_save` in `fs_args`.\n* `PartitionedDataSet` and `IncrementalDataSet` method `invalidate_cache` was made private: `_invalidate_caches`.\n\n### Other\n* Removed `KEDRO_ENV_VAR` from `kedro.context` to speed up the CLI run time.\n* `Pipeline.name` has been removed in favour of `Pipeline.tag()`.\n* Dropped `Pipeline.transform()` in favour of `kedro.pipeline.modular_pipeline.pipeline()` helper function.\n* Made constant `PARAMETER_KEYWORDS` private, and moved it from `kedro.pipeline.pipeline` to `kedro.pipeline.modular_pipeline`.\n* Layers are no longer part of the dataset object, as they've moved to the `DataCatalog`.\n* Python 3.5 is no longer supported by the current and all future versions of Kedro.\n\n### Migration guide from Kedro 0.15.* to 0.16.*\n\n#### General Migration\n\n**reminder** [How do I upgrade Kedro](https://docs.kedro.org/en/0.16.0/06_resources/01_faq.html#how-do-i-upgrade-kedro) covers a few key things to remember when updating any kedro version.\n\n#### Migration for datasets\n\nSince all the datasets (from `kedro.io` and `kedro.contrib.io`) were moved to `kedro/extras/datasets` you must update the type of all datasets in `<project>/conf/base/catalog.yml` file.\nHere how it should be changed: `type: <SomeDataSet>` -> `type: <subfolder of kedro/extras/datasets>.<SomeDataSet>` (e.g. `type: CSVDataSet` -> `type: pandas.CSVDataSet`).\n\nIn addition, all the specific datasets like `CSVLocalDataSet`, `CSVS3DataSet` etc. were deprecated. Instead, you must use generalized datasets like `CSVDataSet`.\nE.g. `type: CSVS3DataSet` -> `type: pandas.CSVDataSet`.\n\n> Note: No changes required if you are using your custom dataset.\n\n#### Migration for Pipeline.transform()\n`Pipeline.transform()` has been dropped in favour of the `pipeline()` constructor. The following changes apply:\n- Remember to import `from kedro.pipeline import pipeline`\n- The `prefix` argument has been renamed to `namespace`\n- And `datasets` has been broken down into more granular arguments:\n  - `inputs`: Independent inputs to the pipeline\n  - `outputs`: Any output created in the pipeline, whether an intermediary dataset or a leaf output\n  - `parameters`: `params:...` or `parameters`\n\nAs an example, code that used to look like this with the `Pipeline.transform()` constructor:\n```python\nresult = my_pipeline.transform(\n    datasets={\"input\": \"new_input\", \"output\": \"new_output\", \"params:x\": \"params:y\"},\n    prefix=\"pre\",\n)\n```\n\nWhen used with the new `pipeline()` constructor, becomes:\n```python\nfrom kedro.pipeline import pipeline\n\nresult = pipeline(\n    my_pipeline,\n    inputs={\"input\": \"new_input\"},\n    outputs={\"output\": \"new_output\"},\n    parameters={\"params:x\": \"params:y\"},\n    namespace=\"pre\",\n)\n```\n\n#### Migration for decorators, color logger, transformers etc.\nSince some modules were moved to other locations you need to update import paths appropriately.\nYou can find the list of moved files in the [`0.15.6` release notes](https://github.com/kedro-org/kedro/releases/tag/0.15.6) under the section titled `Files with a new location`.\n\n#### Migration for CLI and KEDRO_ENV environment variable\n> Note: If you haven't made significant changes to your `kedro_cli.py`, it may be easier to simply copy the updated `kedro_cli.py` `.ipython/profile_default/startup/00-kedro-init.py` and from GitHub or a newly generated project into your old project.\n\n* We've removed `KEDRO_ENV_VAR` from `kedro.context`. To get your existing project template working, you'll need to remove all instances of `KEDRO_ENV_VAR` from your project template:\n  - From the imports in `kedro_cli.py` and `.ipython/profile_default/startup/00-kedro-init.py`: `from kedro.context import KEDRO_ENV_VAR, load_context` -> `from kedro.framework.context import load_context`\n  - Remove the `envvar=KEDRO_ENV_VAR` line from the click options in `run`, `jupyter_notebook` and `jupyter_lab` in `kedro_cli.py`\n  - Replace `KEDRO_ENV_VAR` with `\"KEDRO_ENV\"` in `_build_jupyter_env`\n  - Replace `context = load_context(path, env=os.getenv(KEDRO_ENV_VAR))` with `context = load_context(path)` in `.ipython/profile_default/startup/00-kedro-init.py`\n\n #### Migration for `kedro build-reqs`\n\n We have upgraded `pip-tools` which is used by `kedro build-reqs` to 5.x. This `pip-tools` version requires `pip>=20.0`. To upgrade `pip`, please refer to [their documentation](https://pip.pypa.io/en/stable/installing/#upgrading-pip).\n\n## Thanks for supporting contributions\n[@foolsgold](https://github.com/foolsgold), [Mani Sarkar](https://github.com/neomatrix369), [Priyanka Shanbhag](https://github.com/priyanka1414), [Luis Blanche](https://github.com/LuisBlanche), [Deepyaman Datta](https://github.com/deepyaman), [Antony Milne](https://github.com/AntonyMilneQB), [Panos Psimatikas](https://github.com/ppsimatikas), [Tam-Sanh Nguyen](https://github.com/tamsanh), [Tomasz Kaczmarczyk](https://github.com/TomaszKaczmarczyk), [Kody Fischer](https://github.com/Klio-Foxtrot187), [Waylon Walker](https://github.com/waylonwalker)\n\n# 0.15.9\n\n## Major features and improvements\n\n## Bug fixes and other changes\n\n* Pinned `fsspec>=0.5.1, <0.7.0` and `s3fs>=0.3.0, <0.4.1` to fix incompatibility issues with their latest release.\n\n## Breaking changes to the API\n\n## Thanks for supporting contributions\n\n# 0.15.8\n\n## Major features and improvements\n\n## Bug fixes and other changes\n\n* Added the additional libraries to our `requirements.txt` so `pandas.CSVDataSet` class works out of box with `pip install kedro`.\n* Added `pandas` to our `extra_requires` in `setup.py`.\n* Improved the error message when dependencies of a `DataSet` class are missing.\n\n## Breaking changes to the API\n\n## Thanks for supporting contributions\n\n# 0.15.7\n\n## Major features and improvements\n\n* Added in documentation on how to contribute a custom `AbstractDataSet` implementation.\n\n## Bug fixes and other changes\n\n* Fixed the link to the Kedro banner image in the documentation.\n\n## Breaking changes to the API\n\n## Thanks for supporting contributions\n\n# 0.15.6\n\n## Major features and improvements\n> _TL;DR_ We're launching [`kedro.extras`](https://github.com/kedro-org/kedro/tree/master/extras), the new home for our revamped series of datasets, decorators and dataset transformers. The datasets in [`kedro.extras.datasets`](https://github.com/kedro-org/kedro/tree/master/extras/datasets) use [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to access a variety of data stores including local file systems, network file systems, cloud object stores (including S3 and GCP), and Hadoop, read more about this [**here**](https://docs.kedro.org/en/0.15.6/04_user_guide/04_data_catalog.html#specifying-the-location-of-the-dataset). The change will allow [#178](https://github.com/kedro-org/kedro/issues/178) to happen in the next major release of Kedro.\n\nAn example of this new system can be seen below, loading the CSV `SparkDataSet` from S3:\n\n```yaml\nweather:\n  type: spark.SparkDataSet  # Observe the specified type, this  affects all datasets\n  filepath: s3a://your_bucket/data/01_raw/weather*  # filepath uses fsspec to indicate the file storage system\n  credentials: dev_s3\n  file_format: csv\n```\n\nYou can also load data incrementally whenever it is dumped into a directory with the extension to [`PartionedDataSet`](https://docs.kedro.org/en/0.15.6/04_user_guide/08_advanced_io.html#partitioned-dataset), a feature that allows you to load a directory of files. The [`IncrementalDataSet`](https://docs.kedro.org/en/0.15.6/04_user_guide/08_advanced_io.html#incremental-loads-with-incrementaldataset) stores the information about the last processed partition in a `checkpoint`, read more about this feature [**here**](https://docs.kedro.org/en/0.15.6/04_user_guide/08_advanced_io.html#incremental-loads-with-incrementaldataset).\n\n### New features\n\n* Added `layer` attribute for datasets in `kedro.extras.datasets` to specify the name of a layer according to [data engineering convention](https://docs.kedro.org/en/0.15.6/06_resources/01_faq.html#what-is-data-engineering-convention), this feature will be passed to [`kedro-viz`](https://github.com/kedro-org/kedro-viz) in future releases.\n* Enabled loading a particular version of a dataset in Jupyter Notebooks and iPython, using `catalog.load(\"dataset_name\", version=\"<2019-12-13T15.08.09.255Z>\")`.\n* Added property `run_id` on `ProjectContext`, used for versioning using the [`Journal`](https://docs.kedro.org/en/0.15.6/04_user_guide/13_journal.html). To customise your journal `run_id` you can override the private method `_get_run_id()`.\n* Added the ability to install all optional kedro dependencies via `pip install \"kedro[all]\"`.\n* Modified the `DataCatalog`'s load order for datasets, loading order is the following:\n  - `kedro.io`\n  - `kedro.extras.datasets`\n  - Import path, specified in `type`\n* Added an optional `copy_mode` flag to `CachedDataSet` and `MemoryDataSet` to specify (`deepcopy`, `copy` or `assign`) the copy mode to use when loading and saving.\n\n### New Datasets\n\n| Type                             | Description                                                                                                                                      | Location                            |\n| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------- |\n| `dask.ParquetDataSet`            | Handles parquet datasets using Dask                                                                                                              | `kedro.extras.datasets.dask`        |\n| `pickle.PickleDataSet`           | Work with Pickle files using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem         | `kedro.extras.datasets.pickle`      |\n| `pandas.CSVDataSet`              | Work with CSV files using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem            | `kedro.extras.datasets.pandas`      |\n| `pandas.TextDataSet`             | Work with text files using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem           | `kedro.extras.datasets.pandas`      |\n| `pandas.ExcelDataSet`            | Work with Excel files using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem          | `kedro.extras.datasets.pandas`      |\n| `pandas.HDFDataSet`              | Work with HDF using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem                  | `kedro.extras.datasets.pandas`      |\n| `yaml.YAMLDataSet`               | Work with YAML files using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem           | `kedro.extras.datasets.yaml`        |\n| `matplotlib.MatplotlibWriter`    | Save with Matplotlib images using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem    | `kedro.extras.datasets.matplotlib`  |\n| `networkx.NetworkXDataSet`       | Work with NetworkX files using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem       | `kedro.extras.datasets.networkx`    |\n| `biosequence.BioSequenceDataSet` | Work with bio-sequence objects using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem | `kedro.extras.datasets.biosequence` |\n| `pandas.GBQTableDataSet`         | Work with Google BigQuery                                                                                                                        | `kedro.extras.datasets.pandas`      |\n| `pandas.FeatherDataSet`          | Work with feather files using [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem        | `kedro.extras.datasets.pandas`      |\n| `IncrementalDataSet`             | Inherit from `PartitionedDataSet` and remembers the last processed partition                                                                     | `kedro.io`                          |\n\n### Files with a new location\n\n| Type                                                                 | New Location                                 |\n| -------------------------------------------------------------------- | -------------------------------------------- |\n| `JSONDataSet`                                                        | `kedro.extras.datasets.pandas`               |\n| `CSVBlobDataSet`                                                     | `kedro.extras.datasets.pandas`               |\n| `JSONBlobDataSet`                                                    | `kedro.extras.datasets.pandas`               |\n| `SQLTableDataSet`                                                    | `kedro.extras.datasets.pandas`               |\n| `SQLQueryDataSet`                                                    | `kedro.extras.datasets.pandas`               |\n| `SparkDataSet`                                                       | `kedro.extras.datasets.spark`                |\n| `SparkHiveDataSet`                                                   | `kedro.extras.datasets.spark`                |\n| `SparkJDBCDataSet`                                                   | `kedro.extras.datasets.spark`                |\n| `kedro/contrib/decorators/retry.py`                                  | `kedro/extras/decorators/retry_node.py`      |\n| `kedro/contrib/decorators/memory_profiler.py`                        | `kedro/extras/decorators/memory_profiler.py` |\n| `kedro/contrib/io/transformers/transformers.py`                      | `kedro/extras/transformers/time_profiler.py` |\n| `kedro/contrib/colors/logging/color_logger.py`                       | `kedro/extras/logging/color_logger.py`       |\n| `extras/ipython_loader.py`                                           | `tools/ipython/ipython_loader.py`            |\n| `kedro/contrib/io/cached/cached_dataset.py`                          | `kedro/io/cached_dataset.py`                 |\n| `kedro/contrib/io/catalog_with_default/data_catalog_with_default.py` | `kedro/io/data_catalog_with_default.py`      |\n| `kedro/contrib/config/templated_config.py`                           | `kedro/config/templated_config.py`           |\n\n## Upcoming deprecations\n\n| Category                  | Type                                                           |\n| ------------------------- | -------------------------------------------------------------- |\n| **Datasets**              | `BioSequenceLocalDataSet`                                      |\n|                           | `CSVGCSDataSet`                                                |\n|                           | `CSVHTTPDataSet`                                               |\n|                           | `CSVLocalDataSet`                                              |\n|                           | `CSVS3DataSet`                                                 |\n|                           | `ExcelLocalDataSet`                                            |\n|                           | `FeatherLocalDataSet`                                          |\n|                           | `JSONGCSDataSet`                                               |\n|                           | `JSONLocalDataSet`                                             |\n|                           | `HDFLocalDataSet`                                              |\n|                           | `HDFS3DataSet`                                                 |\n|                           | `kedro.contrib.io.cached.CachedDataSet`                        |\n|                           | `kedro.contrib.io.catalog_with_default.DataCatalogWithDefault` |\n|                           | `MatplotlibLocalWriter`                                        |\n|                           | `MatplotlibS3Writer`                                           |\n|                           | `NetworkXLocalDataSet`                                         |\n|                           | `ParquetGCSDataSet`                                            |\n|                           | `ParquetLocalDataSet`                                          |\n|                           | `ParquetS3DataSet`                                             |\n|                           | `PickleLocalDataSet`                                           |\n|                           | `PickleS3DataSet`                                              |\n|                           | `TextLocalDataSet`                                             |\n|                           | `YAMLLocalDataSet`                                             |\n| **Decorators**            | `kedro.contrib.decorators.memory_profiler`                     |\n|                           | `kedro.contrib.decorators.retry`                               |\n|                           | `kedro.contrib.decorators.pyspark.spark_to_pandas`             |\n|                           | `kedro.contrib.decorators.pyspark.pandas_to_spark`             |\n| **Transformers**          | `kedro.contrib.io.transformers.transformers`                   |\n| **Configuration Loaders** | `kedro.contrib.config.TemplatedConfigLoader`                   |\n\n## Bug fixes and other changes\n* Added the option to set/overwrite params in `config.yaml` using YAML dict style instead of string CLI formatting only.\n* Kedro CLI arguments `--node` and `--tag` support comma-separated values, alternative methods will be deprecated in future releases.\n* Fixed a bug in the `invalidate_cache` method of `ParquetGCSDataSet` and `CSVGCSDataSet`.\n* `--load-version` now won't break if version value contains a colon.\n* Enabled running `node`s with duplicate inputs.\n* Improved error message when empty credentials are passed into `SparkJDBCDataSet`.\n* Fixed bug that caused an empty project to fail unexpectedly with ImportError in `template/.../pipeline.py`.\n* Fixed bug related to saving dataframe with categorical variables in table mode using `HDFS3DataSet`.\n* Fixed bug that caused unexpected behavior when using `from_nodes` and `to_nodes` in pipelines using transcoding.\n* Credentials nested in the dataset config are now also resolved correctly.\n* Bumped minimum required pandas version to 0.24.0 to make use of `pandas.DataFrame.to_numpy` (recommended alternative to `pandas.DataFrame.values`).\n* Docs improvements.\n* `Pipeline.transform` skips modifying node inputs/outputs containing `params:` or `parameters` keywords.\n* Support for `dataset_credentials` key in the credentials for `PartitionedDataSet` is now deprecated. The dataset credentials should be specified explicitly inside the dataset config.\n* Datasets can have a new `confirm` function which is called after a successful node function execution if the node contains `confirms` argument with such dataset name.\n* Make the resume prompt on pipeline run failure use `--from-nodes` instead of `--from-inputs` to avoid unnecessarily re-running nodes that had already executed.\n* When closed, Jupyter notebook kernels are automatically terminated after 30 seconds of inactivity by default. Use `--idle-timeout` option to update it.\n* Added `kedro-viz` to the Kedro project template `requirements.txt` file.\n* Removed the `results` and `references` folder from the project template.\n* Updated contribution process in `CONTRIBUTING.md`.\n\n## Breaking changes to the API\n* Existing `MatplotlibWriter` dataset in `contrib` was renamed to `MatplotlibLocalWriter`.\n* `kedro/contrib/io/matplotlib/matplotlib_writer.py` was renamed to `kedro/contrib/io/matplotlib/matplotlib_local_writer.py`.\n* `kedro.contrib.io.bioinformatics.sequence_dataset.py` was renamed to `kedro.contrib.io.bioinformatics.biosequence_local_dataset.py`.\n\n## Thanks for supporting contributions\n[Andrii Ivaniuk](https://github.com/andrii-ivaniuk), [Jonas Kemper](https://github.com/jonasrk), [Yuhao Zhu](https://github.com/yhzqb), [Balazs Konig](https://github.com/BalazsKonigQB), [Pedro Abreu](https://github.com/PedroAbreuQB), [Tam-Sanh Nguyen](https://github.com/tamsanh), [Peter Zhao](https://github.com/zxpeter), [Deepyaman Datta](https://github.com/deepyaman), [Florian Roessler](https://github.com/fdroessler/), [Miguel Rodriguez Gutierrez](https://github.com/MigQ2)\n\n# 0.15.5\n\n## Major features and improvements\n* New CLI commands and command flags:\n  - Load multiple `kedro run` CLI flags from a configuration file with the `--config` flag (e.g. `kedro run --config run_config.yml`)\n  - Run parametrised pipeline runs with the `--params` flag (e.g. `kedro run --params param1:value1,param2:value2`).\n  - Lint your project code using the `kedro lint` command, your project is linted with [`black`](https://github.com/psf/black) (Python 3.6+), [`flake8`](https://gitlab.com/pycqa/flake8) and [`isort`](https://github.com/PyCQA/isort).\n* Load specific environments with Jupyter notebooks using `KEDRO_ENV` which will globally set `run`, `jupyter notebook` and `jupyter lab` commands using environment variables.\n* Added the following datasets:\n  - `CSVGCSDataSet` dataset in `contrib` for working with CSV files in Google Cloud Storage.\n  - `ParquetGCSDataSet` dataset in `contrib` for working with Parquet files in Google Cloud Storage.\n  - `JSONGCSDataSet` dataset in `contrib` for working with JSON files in Google Cloud Storage.\n  - `MatplotlibS3Writer` dataset in `contrib` for saving Matplotlib images to S3.\n  - `PartitionedDataSet` for working with datasets split across multiple files.\n  - `JSONDataSet` dataset for working with JSON files that uses [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) to communicate with the underlying filesystem. It doesn't support `http(s)` protocol for now.\n* Added `s3fs_args` to all S3 datasets.\n* Pipelines can be deducted with `pipeline1 - pipeline2`.\n\n## Bug fixes and other changes\n* `ParallelRunner` now works with `SparkDataSet`.\n* Allowed the use of nulls in `parameters.yml`.\n* Fixed an issue where `%reload_kedro` wasn't reloading all user modules.\n* Fixed `pandas_to_spark` and `spark_to_pandas` decorators to work with functions with kwargs.\n* Fixed a bug where `kedro jupyter notebook` and `kedro jupyter lab` would run a different Jupyter installation to the one in the local environment.\n* Implemented Databricks-compatible dataset versioning for `SparkDataSet`.\n* Fixed a bug where `kedro package` would fail in certain situations where `kedro build-reqs` was used to generate `requirements.txt`.\n* Made `bucket_name` argument optional for the following datasets: `CSVS3DataSet`, `HDFS3DataSet`, `PickleS3DataSet`, `contrib.io.parquet.ParquetS3DataSet`, `contrib.io.gcs.JSONGCSDataSet` - bucket name can now be included into the filepath along with the filesystem protocol (e.g. `s3://bucket-name/path/to/key.csv`).\n* Documentation improvements and fixes.\n\n## Breaking changes to the API\n* Renamed entry point for running pip-installed projects to `run_package()` instead of `main()` in `src/<package>/run.py`.\n* `bucket_name` key has been removed from the string representation of the following datasets: `CSVS3DataSet`, `HDFS3DataSet`, `PickleS3DataSet`, `contrib.io.parquet.ParquetS3DataSet`, `contrib.io.gcs.JSONGCSDataSet`.\n* Moved the `mem_profiler` decorator to `contrib` and separated the `contrib` decorators so that dependencies are modular. You may need to update your import paths, for example the pyspark decorators should be imported as `from kedro.contrib.decorators.pyspark import <pyspark_decorator>` instead of `from kedro.contrib.decorators import <pyspark_decorator>`.\n\n## Thanks for supporting contributions\n[Sheldon Tsen](https://github.com/sheldontsen-qb), [@roumail](https://github.com/roumail), [Karlson Lee](https://github.com/i25959341), [Waylon Walker](https://github.com/WaylonWalker), [Deepyaman Datta](https://github.com/deepyaman), [Giovanni](https://github.com/plauto), [Zain Patel](https://github.com/mzjp2)\n\n# 0.15.4\n\n## Major features and improvements\n* `kedro jupyter` now gives the default kernel a sensible name.\n* `Pipeline.name` has been deprecated in favour of `Pipeline.tags`.\n* Reuse pipelines within a Kedro project using `Pipeline.transform`, it simplifies dataset and node renaming.\n* Added Jupyter Notebook line magic (`%run_viz`) to run `kedro viz` in a Notebook cell (requires [`kedro-viz`](https://github.com/kedro-org/kedro-viz) version 3.0.0 or later).\n* Added the following datasets:\n  - `NetworkXLocalDataSet` in `kedro.contrib.io.networkx` to load and save local graphs (JSON format) via NetworkX. (by [@josephhaaga](https://github.com/josephhaaga))\n  - `SparkHiveDataSet` in `kedro.contrib.io.pyspark.SparkHiveDataSet` allowing usage of Spark and insert/upsert on non-transactional Hive tables.\n* `kedro.contrib.config.TemplatedConfigLoader` now supports name/dict key templating and default values.\n\n## Bug fixes and other changes\n* `get_last_load_version()` method for versioned datasets now returns exact last load version if the dataset has been loaded at least once and `None` otherwise.\n* Fixed a bug in `_exists` method for versioned `SparkDataSet`.\n* Enabled the customisation of the ExcelWriter in `ExcelLocalDataSet` by specifying options under `writer` key in `save_args`.\n* Fixed a bug in IPython startup script, attempting to load context from the incorrect location.\n* Removed capping the length of a dataset's string representation.\n* Fixed `kedro install` command failing on Windows if `src/requirements.txt` contains a different version of Kedro.\n* Enabled passing a single tag into a node or a pipeline without having to wrap it in a list (i.e. `tags=\"my_tag\"`).\n\n## Breaking changes to the API\n* Removed `_check_paths_consistency()` method from `AbstractVersionedDataSet`. Version consistency check is now done in `AbstractVersionedDataSet.save()`. Custom versioned datasets should modify `save()` method implementation accordingly.\n\n## Thanks for supporting contributions\n[Joseph Haaga](https://github.com/josephhaaga), [Deepyaman Datta](https://github.com/deepyaman), [Joost Duisters](https://github.com/JoostDuisters), [Zain Patel](https://github.com/mzjp2), [Tom Vigrass](https://github.com/tomvigrass)\n\n# 0.15.3\n\n## Bug Fixes and other changes\n* Narrowed the requirements for `PyTables` so that we maintain support for Python 3.5.\n\n# 0.15.2\n\n## Major features and improvements\n* Added `--load-version`, a `kedro run` argument that allows you run the pipeline with a particular load version of a dataset.\n* Support for modular pipelines in `src/`, break the pipeline into isolated parts with reusability in mind.\n* Support for multiple pipelines, an ability to have multiple entry point pipelines and choose one with `kedro run --pipeline NAME`.\n* Added a `MatplotlibWriter` dataset in `contrib` for saving Matplotlib images.\n* An ability to template/parameterize configuration files with `kedro.contrib.config.TemplatedConfigLoader`.\n* Parameters are exposed as a context property for ease of access in iPython / Jupyter Notebooks with `context.params`.\n* Added `max_workers` parameter for ``ParallelRunner``.\n\n## Bug fixes and other changes\n* Users will override the `_get_pipeline` abstract method in `ProjectContext(KedroContext)` in `run.py` rather than the `pipeline` abstract property. The `pipeline` property is not abstract anymore.\n* Improved an error message when versioned local dataset is saved and unversioned path already exists.\n* Added `catalog` global variable to `00-kedro-init.py`, allowing you to load datasets with `catalog.load()`.\n* Enabled tuples to be returned from a node.\n* Disallowed the ``ConfigLoader`` loading the same file more than once, and deduplicated the `conf_paths` passed in.\n* Added a `--open` flag to `kedro build-docs` that opens the documentation on build.\n* Updated the ``Pipeline`` representation to include name of the pipeline, also making it readable as a context property.\n* `kedro.contrib.io.pyspark.SparkDataSet` and `kedro.contrib.io.azure.CSVBlobDataSet` now support versioning.\n\n## Breaking changes to the API\n* `KedroContext.run()` no longer accepts `catalog` and `pipeline` arguments.\n* `node.inputs` now returns the node's inputs in the order required to bind them properly to the node's function.\n\n## Thanks for supporting contributions\n[Deepyaman Datta](https://github.com/deepyaman), [Luciano Issoe](https://github.com/Lucianois), [Joost Duisters](https://github.com/JoostDuisters), [Zain Patel](https://github.com/mzjp2), [William Ashford](https://github.com/williamashfordQB), [Karlson Lee](https://github.com/i25959341)\n\n# 0.15.1\n\n## Major features and improvements\n* Extended `versioning` support to cover the tracking of environment setup, code and datasets.\n* Added the following datasets:\n  - `FeatherLocalDataSet` in `contrib` for usage with pandas. (by [@mdomarsaleem](https://github.com/mdomarsaleem))\n* Added `get_last_load_version` and `get_last_save_version` to `AbstractVersionedDataSet`.\n* Implemented `__call__` method on `Node` to allow for users to execute `my_node(input1=1, input2=2)` as an alternative to `my_node.run(dict(input1=1, input2=2))`.\n* Added new `--from-inputs` run argument.\n\n## Bug fixes and other changes\n* Fixed a bug in `load_context()` not loading context in non-Kedro Jupyter Notebooks.\n* Fixed a bug in `ConfigLoader.get()` not listing nested files for `**`-ending glob patterns.\n* Fixed a logging config error in Jupyter Notebook.\n* Updated documentation in `03_configuration` regarding how to modify the configuration path.\n* Documented the architecture of Kedro showing how we think about library, project and framework components.\n* `extras/kedro_project_loader.py` renamed to `extras/ipython_loader.py` and now runs any IPython startup scripts without relying on the Kedro project structure.\n* Fixed TypeError when validating partial function's signature.\n* After a node failure during a pipeline run, a resume command will be suggested in the logs. This command will not work if the required inputs are MemoryDataSets.\n\n## Breaking changes to the API\n\n## Thanks for supporting contributions\n[Omar Saleem](https://github.com/mdomarsaleem), [Mariana Silva](https://github.com/marianansilva), [Anil Choudhary](https://github.com/aniryou), [Craig](https://github.com/cfranklin11)\n\n# 0.15.0\n\n## Major features and improvements\n* Added `KedroContext` base class which holds the configuration and Kedro's main functionality (catalog, pipeline, config, runner).\n* Added a new CLI command `kedro jupyter convert` to facilitate converting Jupyter Notebook cells into Kedro nodes.\n* Added support for `pip-compile` and new Kedro command `kedro build-reqs` that generates `requirements.txt` based on `requirements.in`.\n* Running `kedro install` will install packages to conda environment if `src/environment.yml` exists in your project.\n* Added a new `--node` flag to `kedro run`, allowing users to run only the nodes with the specified names.\n* Added new `--from-nodes` and `--to-nodes` run arguments, allowing users to run a range of nodes from the pipeline.\n* Added prefix `params:` to the parameters specified in `parameters.yml` which allows users to differentiate between their different parameter node inputs and outputs.\n* Jupyter Lab/Notebook now starts with only one kernel by default.\n* Added the following datasets:\n  -  `CSVHTTPDataSet` to load CSV using HTTP(s) links.\n  - `JSONBlobDataSet` to load json (-delimited) files from Azure Blob Storage.\n  - `ParquetS3DataSet` in `contrib` for usage with pandas. (by [@mmchougule](https://github.com/mmchougule))\n  - `CachedDataSet` in `contrib` which will cache data in memory to avoid io/network operations. It will clear the cache once a dataset is no longer needed by a pipeline. (by [@tsanikgr](https://github.com/tsanikgr))\n  - `YAMLLocalDataSet` in `contrib` to load and save local YAML files. (by [@Minyus](https://github.com/Minyus))\n\n## Bug fixes and other changes\n* Documentation improvements including instructions on how to initialise a Spark session using YAML configuration.\n* `anyconfig` default log level changed from `INFO` to `WARNING`.\n* Added information on installed plugins to `kedro info`.\n* Added style sheets for project documentation, so the output of `kedro build-docs` will resemble the style of `kedro docs`.\n\n## Breaking changes to the API\n* Simplified the Kedro template in `run.py` with the introduction of `KedroContext` class.\n* Merged `FilepathVersionMixIn` and `S3VersionMixIn` under one abstract class `AbstractVersionedDataSet` which extends`AbstractDataSet`.\n* `name` changed to be a keyword-only argument for `Pipeline`.\n* `CSVLocalDataSet` no longer supports URLs. `CSVHTTPDataSet` supports URLs.\n\n### Migration guide from Kedro 0.14.* to Kedro 0.15.0\n#### Migration for Kedro project template\nThis guide assumes that:\n  * The framework specific code has not been altered significantly\n  * Your project specific code is stored in the dedicated python package under `src/`.\n\nThe breaking changes were introduced in the following project template files:\n- `<project-name>/.ipython/profile_default/startup/00-kedro-init.py`\n- `<project-name>/kedro_cli.py`\n- `<project-name>/src/tests/test_run.py`\n- `<project-name>/src/<python_package>/run.py`\n- `<project-name>/.kedro.yml` (new file)\n\nThe easiest way to migrate your project from Kedro 0.14.* to Kedro 0.15.0 is to create a new project (by using `kedro new`) and move code and files bit by bit as suggested in the detailed guide below:\n\n1. Create a new project with the same name by running `kedro new`\n\n2. Copy the following folders to the new project:\n - `results/`\n - `references/`\n - `notebooks/`\n - `logs/`\n - `data/`\n - `conf/`\n\n3. If you customised your `src/<package>/run.py`, make sure you apply the same customisations to `src/<package>/run.py`\n - If you customised `get_config()`, you can override `config_loader` property in `ProjectContext` derived class\n - If you customised `create_catalog()`, you can override `catalog()` property in `ProjectContext` derived class\n - If you customised `run()`, you can override `run()` method in `ProjectContext` derived class\n - If you customised default `env`, you can override it in `ProjectContext` derived class or pass it at construction. By default, `env` is `local`.\n - If you customised default `root_conf`, you can override `CONF_ROOT` attribute in `ProjectContext` derived class. By default, `KedroContext` base class has `CONF_ROOT` attribute set to `conf`.\n\n4. The following syntax changes are introduced in ipython or Jupyter notebook/labs:\n - `proj_dir` -> `context.project_path`\n - `proj_name` -> `context.project_name`\n - `conf` -> `context.config_loader`.\n - `io` -> `context.catalog` (e.g., `io.load()` -> `context.catalog.load()`)\n\n5. If you customised your `kedro_cli.py`, you need to apply the same customisations to your `kedro_cli.py` in the new project.\n\n6. Copy the contents of the old project's `src/requirements.txt` into the new project's `src/requirements.in` and, from the project root directory, run the `kedro build-reqs` command in your terminal window.\n\n#### Migration for versioning custom dataset classes\n\nIf you defined any custom dataset classes which support versioning in your project, you need to apply the following changes:\n\n1. Make sure your dataset inherits from `AbstractVersionedDataSet` only.\n2. Call `super().__init__()` with the appropriate arguments in the dataset's `__init__`. If storing on local filesystem, providing the filepath and the version is enough. Otherwise, you should also pass in an `exists_function` and a `glob_function` that emulate `exists` and `glob` in a different filesystem (see `CSVS3DataSet` as an example).\n3. Remove setting of the `_filepath` and `_version` attributes in the dataset's `__init__`, as this is taken care of in the base abstract class.\n4. Any calls to `_get_load_path` and `_get_save_path` methods should take no arguments.\n5. Ensure you convert the output of `_get_load_path` and `_get_save_path` appropriately, as these now return [`PurePath`s](https://docs.python.org/3/library/pathlib.html#pure-paths) instead of strings.\n6. Make sure `_check_paths_consistency` is called with [`PurePath`s](https://docs.python.org/3/library/pathlib.html#pure-paths) as input arguments, instead of strings.\n\nThese steps should have brought your project to Kedro 0.15.0. There might be some more minor tweaks needed as every project is unique, but now you have a pretty solid base to work with. If you run into any problems, please consult the [Kedro documentation](https://docs.kedro.org).\n\n## Thanks for supporting contributions\n[Dmitry Vukolov](https://github.com/dvukolov), [Jo Stichbury](https://github.com/stichbury), [Angus Williams](https://github.com/awqb), [Deepyaman Datta](https://github.com/deepyaman), [Mayur Chougule](https://github.com/mmchougule), [Marat Kopytjuk](https://github.com/kopytjuk), [Evan Miller](https://github.com/evanmiller29), [Yusuke Minami](https://github.com/Minyus)\n\n# 0.14.3\n\n## Major features and improvements\n* Tab completion for catalog datasets in `ipython` or `jupyter` sessions. (Thank you [@datajoely](https://github.com/datajoely) and [@WaylonWalker](https://github.com/WaylonWalker))\n* Added support for transcoding, an ability to decouple loading/saving mechanisms of a dataset from its storage location, denoted by adding '@' to the dataset name.\n* Datasets have a new `release` function that instructs them to free any cached data. The runners will call this when the dataset is no longer needed downstream.\n\n## Bug fixes and other changes\n* Add support for pipeline nodes made up from partial functions.\n* Expand user home directory `~` for TextLocalDataSet (see issue #19).\n* Add a `short_name` property to `Node`s for a display-friendly (but not necessarily unique) name.\n* Add Kedro project loader for IPython: `extras/kedro_project_loader.py`.\n* Fix source file encoding issues with Python 3.5 on Windows.\n* Fix local project source not having priority over the same source installed as a package, leading to local updates not being recognised.\n\n## Breaking changes to the API\n* Remove the max_loads argument from the `MemoryDataSet` constructor and from the `AbstractRunner.create_default_data_set` method.\n\n## Thanks for supporting contributions\n[Joel Schwarzmann](https://github.com/datajoely), [Alex Kalmikov](https://github.com/kalexqb)\n\n# 0.14.2\n\n## Major features and improvements\n* Added Data Set transformer support in the form of AbstractTransformer and DataCatalog.add_transformer.\n\n## Breaking changes to the API\n* Merged the `ExistsMixin` into `AbstractDataSet`.\n* `Pipeline.node_dependencies` returns a dictionary keyed by node, with sets of parent nodes as values; `Pipeline` and `ParallelRunner` were refactored to make use of this for topological sort for node dependency resolution and running pipelines respectively.\n* `Pipeline.grouped_nodes` returns a list of sets, rather than a list of lists.\n\n## Thanks for supporting contributions\n\n[Darren Gallagher](https://github.com/dazzag24), [Zain Patel](https://github.com/mzjp2)\n\n# 0.14.1\n\n## Major features and improvements\n* New I/O module `HDFS3DataSet`.\n\n## Bug fixes and other changes\n* Improved API docs.\n* Template `run.py` will throw a warning instead of error if `credentials.yml`\n  is not present.\n\n## Breaking changes to the API\nNone\n\n\n# 0.14.0\n\nThe initial release of Kedro.\n\n\n## Thanks for supporting contributions\n\nJo Stichbury, Aris Valtazanos, Fabian Peters, Guilherme Braccialli, Joel Schwarzmann, Miguel Beltre, Mohammed ElNabawy, Deepyaman Datta, Shubham Agrawal, Oleg Andreyev, Mayur Chougule, William Ashford, Ed Cannon, Nikhilesh Nukala, Sean Bailey, Vikram Tegginamath, Thomas Huijskens, Musa Bilal\n\nWe are also grateful to everyone who advised and supported us, filed issues or helped resolve them, asked and answered questions and were part of inspiring discussions.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.6748046875,
          "content": "# Security policy\n\nKedro and its community take security bugs seriously. We appreciate efforts to improve the security of all Kedro products\nand follow the [GitHub coordinated disclosure of security vulnerabilities](https://docs.github.com/en/code-security/security-advisories/about-coordinated-disclosure-of-security-vulnerabilities#about-reporting-and-disclosing-vulnerabilities-in-projects-on-github)\nfor responsible disclosure and prompt mitigation. We are committed to working with security researchers to\nresolve the vulnerabilities they discover.\n\n## Supported versions\n\nThe latest versions of [Kedro](https://github.com/kedro-org/kedro), [Kedro-Viz](https://github.com/kedro-org/kedro-viz/), [Kedro Starters](https://github.com/kedro-org/kedro-starters) and the [Kedro plugins](https://github.com/kedro-org/kedro-plugins) have continued support. Any critical vulnerability will be fixed and a release will be done for the affected project as soon as possible.\n\n## Reporting a vulnerability\n\nWhen finding a security vulnerability in [Kedro](https://github.com/kedro-org/kedro), [Kedro-Viz](https://github.com/kedro-org/kedro-viz/), [Kedro Starters](https://github.com/kedro-org/kedro-starters) or any of the official [Kedro plugins](https://github.com/kedro-org/kedro-plugins), perform the following actions:\n\n- [Open an issue](https://github.com/kedro-org/kedro/issues/new?assignees=&labels=Issue%3A%20Bug%20Report%20%F0%9F%90%9E&template=bug-report.md&title=%28security%29%20Security%20Vulnerability) on the Kedro repository. Ensure that you use `(security) Security Vulnerability` as the title and _do not_ mention any vulnerability details in the issue post.\n- Send a notification [email](mailto:kedro-framework@mckinsey.com) to the Kedro Framework maintainers that contains, at a minimum:\n  - The link to the filed issue stub.\n  - Your GitHub handle.\n  - Detailed information about the security vulnerability, evidence that supports the relevance of the finding and any reproducibility instructions for independent confirmation.\n\nThis first stage of reporting is to ensure that a rapid validation can occur without wasting the time and effort of a reporter. Future communication and vulnerability resolution will be conducted after validating\nthe veracity of the reported issue.\n\nA Kedro maintainer will, after validating the report:\n\n- Acknowledge the bug\n- Mark the issue with a `Blockerüìõ` priority\n- Open a draft [GitHub Security Advisory](https://docs.github.com/en/code-security/security-advisories/creating-a-security-advisory)\n  to discuss the vulnerability details in private.\n\nThe private Security Advisory will be used to confirm the issue, prepare a fix, and publicly disclose it after the fix has been released.\n"
        },
        {
          "name": "asv.conf.json",
          "type": "blob",
          "size": 0.55859375,
          "content": "{\n    \"version\": 1,\n    \"project\": \"Kedro\",\n    \"project_url\": \"https://kedro.org/\",\n    \"repo\": \".\",\n    \"install_command\": [\n        \"pip install -e .  kedro-datasets[pandas-csvdataset]\"\n    ],\n    \"branches\": [\n        \"main\"\n    ],\n    \"environment_name\": \"kedro\",\n    \"environment_type\": \"virtualenv\",\n    \"show_commit_url\": \"http://github.com/kedro-org/kedro/commit/\",\n    \"results_dir\": \".asv/results\",\n    \"benchmark_dir\": \"kedro_benchmarks\",\n    \"html_dir\": \".asv/html\",\n    \"matrix\": {\n        \"req\": {\n            \"kedro-datasets[pandas]\": []\n        }\n    }\n}\n"
        },
        {
          "name": "behave.ini",
          "type": "blob",
          "size": 0.33203125,
          "content": "[behave]\n\n# These commands force behave to display all output regardless of the outcome of the test\nstderr_capture=False\nstdout_capture=False\n\n# Single line print statements will not be shown even if output capture is off,\n# to avoid that behaviour either add a new line at the end of the string ('\\n') or disable color with:\n# color=False\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "features",
          "type": "tree",
          "content": null
        },
        {
          "name": "kedro",
          "type": "tree",
          "content": null
        },
        {
          "name": "kedro_benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "kedro_technical_charter.pdf",
          "type": "blob",
          "size": 106.9775390625,
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 5.9736328125,
          "content": "# PEP-518 https://peps.python.org/pep-0518/\n[build-system]\n# Minimum requirements for the build system to execute.\nrequires = [\"setuptools>=65.5.1\"]  # PEP 518 specifications\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"kedro\"\nauthors = [\n    {name = \"Kedro\"}\n]\ndescription = \"Kedro helps you build production-ready data and analytics pipelines\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"attrs>=21.3\",\n    \"build>=0.7.0\",\n    \"cachetools>=4.1\",\n    \"click>=4.0\",\n    \"cookiecutter>=2.1.1,<3.0\",\n    \"dynaconf>=3.1.2,<4.0\",\n    \"fsspec>=2021.4\",\n    \"gitpython>=3.0\",\n    \"importlib-metadata>=3.6,<9.0\",\n    \"importlib_resources>=1.3,<7.0\",  # The `files()` API was introduced in `importlib_resources` 1.3 and Python 3.9.\n    \"kedro-telemetry>=0.5.0\",\n    \"more_itertools>=8.14.0\",\n    \"omegaconf>=2.1.1\",\n    \"parse>=1.19.0\",\n    \"pluggy>=1.0\",\n    \"pre-commit-hooks\",\n    \"PyYAML>=4.2,<7.0\",\n    \"rich>=12.0,<14.0\",\n    \"rope>=0.21,<2.0\",  # subject to LGPLv3 license\n    \"toml>=0.10.0\",\n    \"typing_extensions>=4.0\",\n]\nkeywords = [\n    \"pipelines\",\n    \"machine learning\",\n    \"data pipelines\",\n    \"data science\",\n    \"data engineering\",\n]\nlicense = {text = \"Apache Software License (Apache 2.0)\"}\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n]\ndynamic = [\"readme\", \"version\"]\n\n[project.optional-dependencies]\ntest = [\n    \"behave==1.2.6\",\n    \"coverage[toml]\",\n    \"detect-secrets~=1.5.0\",\n    \"import-linter==2.1\",\n    \"ipylab>=1.0.0\",\n    \"ipython~=8.10\",\n    \"jupyterlab_server>=2.11.1\",\n    \"jupyterlab>=3,<5\",\n    \"jupyter~=1.0\",\n    \"kedro-datasets\",\n    \"mypy~=1.0\",\n    \"pandas~=2.0\",\n    \"pluggy>=1.0\",\n    \"pre-commit>=2.9.2, <5.0\",  # The hook `mypy` requires pre-commit version 2.9.2.\n    \"pytest-cov>=3,<7\",\n    \"pytest-mock>=1.7.1, <4.0\",\n    \"pytest-xdist[psutil]~=2.2.1\",\n    \"pytest>=7.2,<9.0\",\n    \"s3fs>=2021.4, <2025.1\",  # Upper bound set arbitrarily, to be reassessed in late 2024\n    \"requests_mock\",\n    # mypy related dependencies\n    \"pandas-stubs\",\n    \"types-PyYAML\",\n    \"types-cachetools\",\n    \"types-requests\",\n    \"types-toml\",\n]\ndocs = [\n    \"ipykernel>=5.3, <7.0\",\n    \"Jinja2<3.2.0\",\n    \"kedro-sphinx-theme==2024.10.2\",\n    \"sphinx-notfound-page!=1.0.3\",  # Required by kedro-sphinx-theme. 1.0.3 results in `AttributeError: 'tuple' object has no attribute 'default'`.\n]\njupyter = [\n    \"ipylab>=1.0.0\",\n    \"notebook>=7.0.0\"  # requires the new share backend of notebook and labs\"\n]\nbenchmark = [\n    \"asv\"\n]\nall = [ \"kedro[test,docs,jupyter,benchmark]\" ]\n\n[project.urls]\nHomepage = \"https://kedro.org\"\nSource = \"https://github.com/kedro-org/kedro\"\nDocumentation = \"https://docs.kedro.org\"\nTracker = \"https://github.com/kedro-org/kedro/issues\"\n\n[project.scripts]\nkedro = \"kedro.framework.cli:main\"\n\n[tool.setuptools]\nzip-safe = false\n\n[tool.setuptools.packages.find]\ninclude = [\"kedro*\"]\n\n[tool.setuptools.package-data]\nkedro = [\"py.typed\"]\n\n[tool.setuptools.dynamic]\nreadme = {file = \"README.md\", content-type = \"text/markdown\"}\nversion = {attr = \"kedro.__version__\"}\n\n[tool.ruff.format]\nexclude = [\"**/templates\", \"features/steps/test_starter\"]\ndocstring-code-format = true\n\n[tool.coverage.report]\nfail_under = 100\nshow_missing = true\nomit = [\n    \"kedro/templates/*\",\n    \"kedro/extras/logging/color_logger.py\",\n    \"kedro/extras/extensions/ipython.py\",\n    \"kedro/framework/cli/hooks/specs.py\",\n    \"kedro/framework/hooks/specs.py\",\n    \"tests/*\",\n    \"kedro/runner/parallel_runner.py\",\n    \"*/site-packages/*\",\n]\nexclude_also = [\"raise NotImplementedError\", \"if TYPE_CHECKING:\", \"class CatalogProtocol\"]\n\n[tool.coverage.run]\nconcurrency = [\"multiprocessing\", \"thread\"]\nparallel = true\nsigterm = true\n\n[tool.pytest.ini_options]\naddopts=\"\"\"\n--cov-context test  \\\n--cov-config pyproject.toml \\\n--cov-report xml:coverage.xml \\\n--cov-report term-missing \\\n--cov kedro \\\n--cov tests \\\n--ignore tests/template/fake_repo \\\n--ignore features \\\n--ignore kedro/templates \\\n--no-cov-on-fail \\\n-ra \\\n-W ignore\"\"\"\ntestpaths = [\n  \"tests\"\n]\n\n[tool.importlinter]\nroot_package = \"kedro\"\n\n[[tool.importlinter.contracts]]\nname = \"CLI > Context > Library, Runner > Extras > IO & Pipeline\"\ntype = \"layers\"\ncontainers = \"kedro\"\nlayers = [\n    \"framework.cli\",\n    \"framework.session\",\n    \"framework.context\",\n    \"framework.project\",\n    \"runner\",\n    \"io\",\n    \"pipeline\",\n    \"config\"\n]\nignore_imports = [\n    \"kedro.runner.task -> kedro.framework.project\",\n    \"kedro.framework.hooks.specs -> kedro.framework.context\"\n]\n\n[[tool.importlinter.contracts]]\nname = \"Pipeline and IO are independent\"\ntype = \"independence\"\nmodules = [\n    \"kedro.pipeline\",\n    \"kedro.io\"\n]\n\n[[tool.importlinter.contracts]]\nname = \"Config cannot import Runner et al\"\ntype = \"forbidden\"\nsource_modules = [\n    \"kedro.config\"\n]\nforbidden_modules = [\n    \"kedro.runner\",\n    \"kedro.io\",\n    \"kedro.pipeline\",\n]\n\n[[tool.importlinter.contracts]]\nname = \"Runner et al cannot import Config\"\ntype = \"forbidden\"\nsource_modules = [\n    \"kedro.runner\",\n    \"kedro.io\",\n    \"kedro.pipeline\",\n]\nforbidden_modules = [\n    \"kedro.config\"\n]\nignore_imports = [\n    \"kedro.framework.context.context -> kedro.config\",\n    \"kedro.framework.session.session -> kedro.config\",\n]\n\n[tool.ruff]\nline-length = 88\nshow-fixes = true\nlint.select = [\n    \"F\",    # Pyflakes\n    \"W\",    # pycodestyle\n    \"E\",    # pycodestyle\n    \"I\",    # isort\n    \"UP\",   # pyupgrade\n    \"PL\",   # Pylint\n    \"T201\", # Print Statement\n    \"S\",    # flake8-bandit\n    \"TCH\",  # flake8-type-checking\n    \"RUF\",  # Ruff-specific rules\n]\nlint.ignore = [\"E501\"]\n\n[tool.ruff.lint.isort]\nknown-first-party = [\"kedro\"]\n\n[tool.ruff.lint.per-file-ignores]\n\"{tests,docs}/*\" = [\"PLR2004\",\"PLR0913\"]\n\"{tests,docs,tools,static,features,docs}/*\" = [\"T201\", \"S101\", \"S108\"]  # Check print statement for kedro/ only\n\n[tool.mypy]\nignore_missing_imports = true\ndisable_error_code = ['misc']\nexclude = ['^kedro/templates/', '^docs/', '^features/steps/test_starter/']\n"
        },
        {
          "name": "static",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}