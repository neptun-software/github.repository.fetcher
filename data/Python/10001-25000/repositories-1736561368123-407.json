{
  "metadata": {
    "timestamp": 1736561368123,
    "page": 407,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dbt-labs/dbt-core",
      "stars": 10198,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".bumpversion.cfg",
          "type": "blob",
          "size": 0.9462890625,
          "content": "[bumpversion]\ncurrent_version = 1.10.0a1\nparse = (?P<major>[\\d]+) # major version number\n\t\\.(?P<minor>[\\d]+) # minor version number\n\t\\.(?P<patch>[\\d]+) # patch version number\n\t(?P<prerelease>  # optional pre-release - ex: a1, b2, rc25\n\t(?P<prekind>a|b|rc) # pre-release type\n\t(?P<num>[\\d]+) # pre-release version number\n\t)?\n\t( # optional nightly release indicator\n\t\\.(?P<nightly>dev[0-9]+) # ex: .dev02142023\n\t)? # expected matches: `1.15.0`, `1.5.0a11`, `1.5.0a1.dev123`, `1.5.0.dev123457`, expected failures: `1`, `1.5`, `1.5.2-a1`, `text1.5.0`\nserialize =\n\t{major}.{minor}.{patch}{prekind}{num}.{nightly}\n\t{major}.{minor}.{patch}.{nightly}\n\t{major}.{minor}.{patch}{prekind}{num}\n\t{major}.{minor}.{patch}\ncommit = False\ntag = False\n\n[bumpversion:part:prekind]\nfirst_value = a\noptional_value = final\nvalues =\n\ta\n\tb\n\trc\n\tfinal\n\n[bumpversion:part:num]\nfirst_value = 1\n\n[bumpversion:part:nightly]\n\n[bumpversion:file:core/setup.py]\n\n[bumpversion:file:core/dbt/version.py]\n"
        },
        {
          "name": ".changes",
          "type": "tree",
          "content": null
        },
        {
          "name": ".changie.yaml",
          "type": "blob",
          "size": 3.8193359375,
          "content": "changesDir: .changes\nunreleasedDir: unreleased\nheaderPath: header.tpl.md\nversionHeaderPath: \"\"\nchangelogPath: CHANGELOG.md\nversionExt: md\nenvPrefix: \"CHANGIE_\"\nversionFormat: '## dbt-core {{.Version}} - {{.Time.Format \"January 02, 2006\"}}'\nkindFormat: '### {{.Kind}}'\nchangeFormat: |-\n  {{- $IssueList := list }}\n  {{- $changes := splitList \" \" $.Custom.Issue }}\n  {{- range $issueNbr := $changes }}\n    {{- $changeLink := \"[#nbr](https://github.com/dbt-labs/dbt-core/issues/nbr)\" | replace \"nbr\" $issueNbr }}\n    {{- $IssueList = append $IssueList $changeLink  }}\n  {{- end -}}\n  - {{.Body}} ({{ range $index, $element := $IssueList }}{{if $index}}, {{end}}{{$element}}{{end}})\n\nkinds:\n  - label: Breaking Changes\n  - label: Features\n  - label: Fixes\n  - label: Docs\n    changeFormat: |-\n      {{- $IssueList := list }}\n      {{- $changes := splitList \" \" $.Custom.Issue }}\n      {{- range $issueNbr := $changes }}\n        {{- $changeLink := \"[dbt-docs/#nbr](https://github.com/dbt-labs/dbt-docs/issues/nbr)\" | replace \"nbr\" $issueNbr }}\n        {{- $IssueList = append $IssueList $changeLink }}\n      {{- end -}}\n      - {{.Body}} ({{ range $index, $element := $IssueList }}{{if $index}}, {{end}}{{$element}}{{end}})\n  - label: Under the Hood\n  - label: Dependencies\n  - label: Security\n\nnewlines:\n  afterChangelogHeader: 1\n  afterKind: 1\n  afterChangelogVersion: 1\n  beforeKind: 1\n  endOfVersion: 1\n\ncustom:\n- key: Author\n  label: GitHub Username(s) (separated by a single space if multiple)\n  type: string\n  minLength: 3\n- key: Issue\n  label: GitHub Issue Number (separated by a single space if multiple)\n  type: string\n  minLength: 1\n\nfooterFormat: |\n  {{- $contributorDict := dict }}\n  {{- /* ensure all names in this list are all lowercase for later matching purposes */}}\n  {{- $core_team := splitList \" \" .Env.CORE_TEAM }}\n  {{- /* ensure we always skip snyk and dependabot in addition to the core team */}}\n  {{- $maintainers := list \"dependabot[bot]\" \"snyk-bot\"}}\n  {{- range $team_member := $core_team }}\n    {{- $team_member_lower := lower $team_member }}\n    {{- $maintainers = append $maintainers $team_member_lower }}\n  {{- end }}\n  {{- range $change := .Changes }}\n    {{- $authorList := splitList \" \" $change.Custom.Author }}\n    {{- /* loop through all authors for a single changelog */}}\n    {{- range $author := $authorList }}\n      {{- $authorLower := lower $author }}\n      {{- /* we only want to include non-core team contributors */}}\n      {{- if not (has $authorLower $maintainers)}}\n        {{- $changeList := splitList \" \" $change.Custom.Author }}\n          {{- $IssueList := list }}\n          {{- $changeLink := $change.Kind }}\n          {{- $changes := splitList \" \" $change.Custom.Issue }}\n          {{- range $issueNbr := $changes }}\n            {{- $changeLink := \"[#nbr](https://github.com/dbt-labs/dbt-core/issues/nbr)\" | replace \"nbr\" $issueNbr }}\n            {{- $IssueList = append $IssueList $changeLink  }}\n          {{- end }}\n          {{- /* check if this contributor has other changes associated with them already */}}\n          {{- if hasKey $contributorDict $author }}\n            {{- $contributionList := get $contributorDict $author }}\n            {{- $contributionList = concat $contributionList $IssueList  }}\n            {{- $contributorDict := set $contributorDict $author $contributionList }}\n          {{- else }}\n            {{- $contributionList := $IssueList }}\n            {{- $contributorDict := set $contributorDict $author $contributionList }}\n          {{- end }}\n        {{- end}}\n    {{- end}}\n  {{- end }}\n  {{- /* no indentation here for formatting so the final markdown doesn't have unneeded indentations */}}\n  {{- if $contributorDict}}\n  ### Contributors\n  {{- range $k,$v := $contributorDict }}\n  - [@{{$k}}](https://github.com/{{$k}}) ({{ range $index, $element := $v }}{{if $index}}, {{end}}{{$element}}{{end}})\n  {{- end }}\n  {{- end }}\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0341796875,
          "content": "*\n!docker/requirements/*.txt\n!dist\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.2822265625,
          "content": "[flake8]\nselect =\n    E\n    W\n    F\nignore =\n    W503 # makes Flake8 work like black\n    W504\n    E203 # makes Flake8 work like black\n    E704 # makes Flake8 work like black\n    E741\n    E501 # long line checking is done in black\nexclude = test/\nper-file-ignores =\n    */__init__.py: F401\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 0.1181640625,
          "content": "# Reformatting dbt-core via black, flake8, mypy, and assorted pre-commit hooks.\n43e3fc22c4eae4d3d901faba05e33c40f1f1dc5a\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.2548828125,
          "content": "core/dbt/task/docs/index.html binary\ntests/functional/artifacts/data/state/*/manifest.json binary\ncore/dbt/docs/build/html/searchindex.js binary\ncore/dbt/docs/build/html/index.html binary\nperformance/runner/Cargo.lock binary\ncore/dbt/events/types_pb2.py binary\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1689453125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv*/\ndbt_env/\nbuild/\n!tests/functional/build\n!core/dbt/docs/build\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n.mypy_cache/\n.dmypy.json\nlogs/\n.user.yml\nprofiles.yml\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\n.env\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\ntest.env\nmakefile.test.env\n*.pytest_cache/\n\n# Unit test artifacts\nindex.html\n\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Ipython Notebook\n.ipynb_checkpoints\n\n# Emacs\n*~\n\n# Sublime Text\n*.sublime-*\n\n# Vim\n*.sw*\n\n# Pyenv\n.python-version\n\n# Vim\n*.sw*\n\n# pycharm\n.idea/\nvenv/\n\n# AWS credentials\n.aws/\n\n# MacOS\n.DS_Store\n\n# vscode\n.vscode/\n*.code-workspace\n\n# poetry\npoetry.lock\n\n# asdf\n.tool-versions\n"
        },
        {
          "name": ".isort.cfg",
          "type": "blob",
          "size": 0.162109375,
          "content": "[settings]\nprofile=black\nextend_skip_glob=.github/*,third-party-stubs/*,scripts/*\nknown_first_party=dbt,dbt_adapters,dbt_common,dbt_extractor,dbt_semantic_interfaces\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.037109375,
          "content": "# Configuration for pre-commit hooks (see https://pre-commit.com/).\n# Eventually the hooks described here will be run as tests before merging each PR.\n\nexclude: ^(core/dbt/docs/build/|core/dbt/common/events/types_pb2.py|core/dbt/events/core_types_pb2.py|core/dbt/adapters/events/adapter_types_pb2.py)\n\n# Force all unspecified python hooks to run python 3.9\ndefault_language_version:\n  python: python3\n\nrepos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v3.2.0\n  hooks:\n  - id: check-yaml\n    args: [--unsafe]\n  - id: check-json\n  - id: end-of-file-fixer\n    exclude: schemas/dbt/manifest/\n  - id: trailing-whitespace\n    exclude_types:\n      - \"markdown\"\n  - id: check-case-conflict\n- repo: https://github.com/pycqa/isort\n  # rev must match what's in dev-requirements.txt\n  rev: 5.13.2\n  hooks:\n    - id: isort\n- repo: https://github.com/psf/black\n  # rev must match what's in dev-requirements.txt\n  rev: 24.3.0\n  hooks:\n  - id: black\n  - id: black\n    alias: black-check\n    stages: [manual]\n    args:\n    - \"--check\"\n    - \"--diff\"\n- repo: https://github.com/pycqa/flake8\n  # rev must match what's in dev-requirements.txt\n  rev: 4.0.1\n  hooks:\n  - id: flake8\n  - id: flake8\n    alias: flake8-check\n    stages: [manual]\n- repo: https://github.com/pre-commit/mirrors-mypy\n  # rev must match what's in dev-requirements.txt\n  rev: v1.4.1\n  hooks:\n  - id: mypy\n    # N.B.: Mypy is... a bit fragile.\n    #\n    # By using `language: system` we run this hook in the local\n    # environment instead of a pre-commit isolated one.  This is needed\n    # to ensure mypy correctly parses the project.\n\n    # It may cause trouble\n    # in that it adds environmental variables out of our control to the\n    # mix.  Unfortunately, there's nothing we can do about per pre-commit's\n    # author.\n    # See https://github.com/pre-commit/pre-commit/issues/730 for details.\n    args: [--show-error-codes]\n    files: ^core/dbt/\n    language: system\n  - id: mypy\n    alias: mypy-check\n    stages: [manual]\n    args: [--show-error-codes, --pretty]\n    files: ^core/dbt/\n    language: system\n"
        },
        {
          "name": "ARCHITECTURE.md",
          "type": "blob",
          "size": 4.65625,
          "content": "The core function of dbt is SQL compilation and execution. Users create projects of dbt resources (models, tests, seeds, snapshots, ...), defined in SQL and YAML files, and they invoke dbt to create, update, or query associated views and tables. Today, dbt makes heavy use of Jinja2 to enable the templating of SQL, and to construct a DAG (Directed Acyclic Graph) from all of the resources in a project. Users can also extend their projects by installing resources (including Jinja macros) from other projects, called \"packages.\"\n\n## dbt-core\n\nMost of the python code in the repository is within the `core/dbt` directory.\n- [`single python files`](core/dbt/README.md): A number of individual files, such as 'compilation.py' and 'exceptions.py'\n\nThe main subdirectories of core/dbt:\n- [`adapters`](core/dbt/adapters/README.md): Define base classes for behavior that is likely to differ across databases\n- [`clients`](core/dbt/clients/README.md): Interface with dependencies (agate, jinja) or across operating systems\n- [`config`](core/dbt/config/README.md): Reconcile user-supplied configuration from connection profiles, project files, and Jinja macros\n- [`context`](core/dbt/context/README.md): Build and expose dbt-specific Jinja functionality\n- [`contracts`](core/dbt/contracts/README.md): Define Python objects (dataclasses) that dbt expects to create and validate\n- [`deps`](core/dbt/deps/README.md): Package installation and dependency resolution\n- [`events`](core/dbt/events/README.md): Logging events\n- [`graph`](core/dbt/graph/README.md): Produce a `networkx` DAG of project resources, and selecting those resources given user-supplied criteria\n- [`include`](core/dbt/include/README.md): The dbt \"global project,\" which defines default implementations of Jinja2 macros\n- [`parser`](core/dbt/parser/README.md): Read project files, validate, construct python objects\n- [`task`](core/dbt/task/README.md): Set forth the actions that dbt can perform when invoked\n\nLegacy tests are found in the 'test' directory:\n- [`unit tests`](core/dbt/test/unit/README.md): Unit tests\n- [`integration tests`](core/dbt/test/integration/README.md): Integration tests\n\n### Invoking dbt\n\nThe \"tasks\" map to top-level dbt commands. So `dbt run` => task.run.RunTask, etc. Some are more like abstract base classes (GraphRunnableTask, for example) but all the concrete types outside of task should map to tasks. Currently one executes at a time. The tasks kick off their “Runners” and those do execute in parallel. The parallelism is managed via a thread pool, in GraphRunnableTask.\n\ncore/dbt/task/docs/index.html\nThis is the docs website code. It comes from the dbt-docs repository, and is generated when a release is packaged.\n\n## Adapters\n\ndbt uses an adapter-plugin pattern to extend support to different databases, warehouses, query engines, etc. \nNote: dbt-postgres used to exist in dbt-core but is now in [its own repo](https://github.com/dbt-labs/dbt-postgres) \n\nEach adapter is a mix of python, Jinja2, and SQL. The adapter code also makes heavy use of Jinja2 to wrap modular chunks of SQL functionality, define default implementations, and allow plugins to override it.\n\nEach adapter plugin is a standalone python package that includes:\n\n- `dbt/include/[name]`: A \"sub-global\" dbt project, of YAML and SQL files, that reimplements Jinja macros to use the adapter's supported SQL syntax\n- `dbt/adapters/[name]`: Python modules that inherit, and optionally reimplement, the base adapter classes defined in dbt-core\n- `setup.py`\n\nThe Postgres adapter code is the most central, and many of its implementations are used as the default defined in the dbt-core global project. The greater the distance of a data technology from Postgres, the more its adapter plugin may need to reimplement.\n\n## Testing dbt\n\nThe [`test/`](test/) subdirectory includes unit and integration tests that run as continuous integration checks against open pull requests. Unit tests check mock inputs and outputs of specific python functions. Integration tests perform end-to-end dbt invocations against real adapters (Postgres, Redshift, Snowflake, BigQuery) and assert that the results match expectations. See [the contributing guide](CONTRIBUTING.md) for a step-by-step walkthrough of setting up a local development and testing environment.\n\n## Everything else\n\n- [docker](docker/): All dbt versions are published as Docker images on DockerHub. This subfolder contains the `Dockerfile` (constant) and `requirements.txt` (one for each version).\n- [etc](etc/): Images for README\n- [scripts](scripts/): Helper scripts for testing, releasing, and producing JSON schemas. These are not included in distributions of dbt, nor are they rigorously tested—they're just handy tools for the dbt maintainers :)\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 2.2802734375,
          "content": "# dbt Core Changelog\n\n- This file provides a full account of all changes to `dbt-core`\n- Changes are listed under the (pre)release in which they first appear. Subsequent releases include changes from previous releases.\n- \"Breaking changes\" listed under a version may require action from end users or external maintainers when upgrading to that version.\n- Do not edit this file directly. This file is auto-generated using [changie](https://github.com/miniscruff/changie). For details on how to document a change, see [the contributing guide](https://github.com/dbt-labs/dbt-core/blob/main/CONTRIBUTING.md#adding-changelog-entry)\n\n## Previous Releases\n\nFor information on prior major and minor releases, see their changelogs:\n\n\n* [1.9](https://github.com/dbt-labs/dbt-core/blob/1.9.latest/CHANGELOG.md)\n* [1.8](https://github.com/dbt-labs/dbt-core/blob/1.8.latest/CHANGELOG.md)\n* [1.7](https://github.com/dbt-labs/dbt-core/blob/1.7.latest/CHANGELOG.md)\n* [1.6](https://github.com/dbt-labs/dbt-core/blob/1.6.latest/CHANGELOG.md)\n* [1.5](https://github.com/dbt-labs/dbt-core/blob/1.5.latest/CHANGELOG.md)\n* [1.4](https://github.com/dbt-labs/dbt-core/blob/1.4.latest/CHANGELOG.md)\n* [1.3](https://github.com/dbt-labs/dbt-core/blob/1.3.latest/CHANGELOG.md)\n* [1.2](https://github.com/dbt-labs/dbt-core/blob/1.2.latest/CHANGELOG.md)\n* [1.1](https://github.com/dbt-labs/dbt-core/blob/1.1.latest/CHANGELOG.md)\n* [1.0](https://github.com/dbt-labs/dbt-core/blob/1.0.latest/CHANGELOG.md)\n* [0.21](https://github.com/dbt-labs/dbt-core/blob/0.21.latest/CHANGELOG.md)\n* [0.20](https://github.com/dbt-labs/dbt-core/blob/0.20.latest/CHANGELOG.md)\n* [0.19](https://github.com/dbt-labs/dbt-core/blob/0.19.latest/CHANGELOG.md)\n* [0.18](https://github.com/dbt-labs/dbt-core/blob/0.18.latest/CHANGELOG.md)\n* [0.17](https://github.com/dbt-labs/dbt-core/blob/0.17.latest/CHANGELOG.md)\n* [0.16](https://github.com/dbt-labs/dbt-core/blob/0.16.latest/CHANGELOG.md)\n* [0.15](https://github.com/dbt-labs/dbt-core/blob/0.15.latest/CHANGELOG.md)\n* [0.14](https://github.com/dbt-labs/dbt-core/blob/0.14.latest/CHANGELOG.md)\n* [0.13](https://github.com/dbt-labs/dbt-core/blob/0.13.latest/CHANGELOG.md)\n* [0.12](https://github.com/dbt-labs/dbt-core/blob/0.12.latest/CHANGELOG.md)\n* [0.11 and earlier](https://github.com/dbt-labs/dbt-core/blob/0.11.latest/CHANGELOG.md)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 15.7333984375,
          "content": "# Contributing to `dbt-core`\n\n`dbt-core` is open source software. It is what it is today because community members have opened issues, provided feedback, and [contributed to the knowledge loop](https://www.getdbt.com/dbt-labs/values/). Whether you are a seasoned open source contributor or a first-time committer, we welcome and encourage you to contribute code, documentation, ideas, or problem statements to this project.\n\n1. [About this document](#about-this-document)\n2. [Getting the code](#getting-the-code)\n3. [Setting up an environment](#setting-up-an-environment)\n4. [Running dbt-core in development](#running-dbt-core-in-development)\n5. [Testing dbt-core](#testing)\n6. [Debugging](#debugging)\n7. [Adding or modifying a changelog entry](#adding-or-modifying-a-changelog-entry)\n8. [Submitting a Pull Request](#submitting-a-pull-request)\n9. [Troubleshooting Tips](#troubleshooting-tips)\n\n## About this document\n\nThere are many ways to contribute to the ongoing development of `dbt-core`, such as by participating in discussions and issues. We encourage you to first read our higher-level document: [\"Expectations for Open Source Contributors\"](https://docs.getdbt.com/docs/contributing/oss-expectations).\n\nThe rest of this document serves as a more granular guide for contributing code changes to `dbt-core` (this repository). It is not intended as a guide for using `dbt-core`, and some pieces assume a level of familiarity with Python development (virtualenvs, `pip`, etc). Specific code snippets in this guide assume you are using macOS or Linux and are comfortable with the command line.\n\nIf you get stuck, we're happy to help! Drop us a line in the `#dbt-core-development` channel in the [dbt Community Slack](https://community.getdbt.com).\n\n### Notes\n\n- **Adapters:** Is your issue or proposed code change related to a specific [database adapter](https://docs.getdbt.com/docs/available-adapters)? If so, please open issues, PRs, and discussions in that adapter's repository instead.\n- **CLA:** Please note that anyone contributing code to `dbt-core` must sign the [Contributor License Agreement](https://docs.getdbt.com/docs/contributor-license-agreements). If you are unable to sign the CLA, the `dbt-core` maintainers will unfortunately be unable to merge any of your Pull Requests. We welcome you to participate in discussions, open issues, and comment on existing ones.\n- **Branches:** All pull requests from community contributors should target the `main` branch (default). If the change is needed as a patch for a minor version of dbt that has already been released (or is already a release candidate), a maintainer will backport the changes in your PR to the relevant \"latest\" release branch (`1.0.latest`, `1.1.latest`, ...). If an issue fix applies to a release branch, that fix should be first committed to the development branch and then to the release branch (rarely release-branch fixes may not apply to `main`).\n- **Releases**: Before releasing a new minor version of Core, we prepare a series of alphas and release candidates to allow users (especially employees of dbt Labs!) to test the new version in live environments. This is an important quality assurance step, as it exposes the new code to a wide variety of complicated deployments and can surface bugs before official release. Releases are accessible via our [supported installation methods](https://docs.getdbt.com/docs/core/installation-overview#install-dbt-core).\n\n## Getting the code\n\n### Installing git\n\nYou will need `git` in order to download and modify the `dbt-core` source code. On macOS, the best way to download git is to just install [Xcode](https://developer.apple.com/support/xcode/).\n\n### External contributors\n\nIf you are not a member of the `dbt-labs` GitHub organization, you can contribute to `dbt-core` by forking the `dbt-core` repository. For a detailed overview on forking, check out the [GitHub docs on forking](https://help.github.com/en/articles/fork-a-repo). In short, you will need to:\n\n1. Fork the `dbt-core` repository\n2. Clone your fork locally\n3. Check out a new branch for your proposed changes\n4. Push changes to your fork\n5. Open a pull request against `dbt-labs/dbt-core` from your forked repository\n\n### dbt Labs contributors\n\nIf you are a member of the `dbt-labs` GitHub organization, you will have push access to the `dbt-core` repo. Rather than forking `dbt-core` to make your changes, just clone the repository, check out a new branch, and push directly to that branch.\n\n## Setting up an environment\n\nThere are some tools that will be helpful to you in developing locally. While this is the list relevant for `dbt-core` development, many of these tools are used commonly across open-source python projects.\n\n### Tools\n\nThese are the tools used in `dbt-core` development and testing:\n\n- [`tox`](https://tox.readthedocs.io/en/latest/) to manage virtualenvs across python versions. We currently target the latest patch releases for Python 3.8, 3.9, 3.10 and 3.11\n- [`pytest`](https://docs.pytest.org/en/latest/) to define, discover, and run tests\n- [`flake8`](https://flake8.pycqa.org/en/latest/) for code linting\n- [`black`](https://github.com/psf/black) for code formatting\n- [`mypy`](https://mypy.readthedocs.io/en/stable/) for static type checking\n- [`pre-commit`](https://pre-commit.com) to easily run those checks\n- [`changie`](https://changie.dev/) to create changelog entries, without merge conflicts\n- [`make`](https://users.cs.duke.edu/~ola/courses/programming/Makefiles/Makefiles.html) to run multiple setup or test steps in combination. Don't worry too much, nobody _really_ understands how `make` works, and our Makefile aims to be super simple.\n- [GitHub Actions](https://github.com/features/actions) for automating tests and checks, once a PR is pushed to the `dbt-core` repository\n\nA deep understanding of these tools in not required to effectively contribute to `dbt-core`, but we recommend checking out the attached documentation if you're interested in learning more about each one.\n\n#### Virtual environments\n\nWe strongly recommend using virtual environments when developing code in `dbt-core`. We recommend creating this virtualenv\nin the root of the `dbt-core` repository. To create a new virtualenv, run:\n```sh\npython3 -m venv env\nsource env/bin/activate\n```\n\nThis will create and activate a new Python virtual environment.\n\n#### Docker and `docker-compose`\n\nDocker and `docker-compose` are both used in testing. Specific instructions for you OS can be found [here](https://docs.docker.com/get-docker/).\n\n\n#### Postgres (optional)\n\nFor testing, and later in the examples in this document, you may want to have `psql` available so you can poke around in the database and see what happened. We recommend that you use [homebrew](https://brew.sh/) for that on macOS, and your package manager on Linux. You can install any version of the postgres client that you'd like. On macOS, with homebrew setup, you can run:\n\n```sh\nbrew install postgresql\n```\n\n## Running `dbt-core` in development\n\n### Installation\n\nFirst make sure that you set up your `virtualenv` as described in [Setting up an environment](#setting-up-an-environment).  Also ensure you have the latest version of pip installed with `pip install --upgrade pip`. Next, install `dbt-core` (and its dependencies):\n\n```sh\nmake dev\n```\nor, alternatively:\n```sh\npip install -r dev-requirements.txt -r editable-requirements.txt\npre-commit install\n```\n\nWhen installed in this way, any changes you make to your local copy of the source code will be reflected immediately in your next `dbt` run.\n\n### Running `dbt-core`\n\nWith your virtualenv activated, the `dbt` script should point back to the source code you've cloned on your machine. You can verify this by running `which dbt`. This command should show you a path to an executable in your virtualenv.\n\nConfigure your [profile](https://docs.getdbt.com/docs/configure-your-profile) as necessary to connect to your target databases. It may be a good idea to add a new profile pointing to a local Postgres instance, or a specific test sandbox within your data warehouse if appropriate. Make sure to create a profile before running integration tests.\n\n## Testing\n\nOnce you're able to manually test that your code change is working as expected, it's important to run existing automated tests, as well as adding some new ones. These tests will ensure that:\n- Your code changes do not unexpectedly break other established functionality\n- Your code changes can handle all known edge cases\n- The functionality you're adding will _keep_ working in the future\n\nAlthough `dbt-core` works with a number of different databases, you won't need to supply credentials for every one of these databases in your test environment. Instead, you can test most `dbt-core` code changes with Python and Postgres.\n\n### Initial setup\n\nPostgres offers the easiest way to test most `dbt-core` functionality today. They are the fastest to run, and the easiest to set up. To run the Postgres integration tests, you'll have to do one extra step of setting up the test database:\n\n```sh\nmake setup-db\n```\nor, alternatively:\n```sh\ndocker-compose up -d database\nPGHOST=localhost PGUSER=root PGPASSWORD=password PGDATABASE=postgres bash test/setup_db.sh\n```\n\n### Test commands\n\nThere are a few methods for running tests locally.\n\n#### Makefile\n\nThere are multiple targets in the Makefile to run common test suites and code\nchecks, most notably:\n\n```sh\n# Runs unit tests with py38 and code checks in parallel.\nmake test\n# Runs postgres integration tests with py38 in \"fail fast\" mode.\nmake integration\n```\n> These make targets assume you have a local installation of a recent version of [`tox`](https://tox.readthedocs.io/en/latest/) for unit/integration testing and pre-commit for code quality checks,\n> unless you use choose a Docker container to run tests. Run `make help` for more info.\n\nCheck out the other targets in the Makefile to see other commonly used test\nsuites.\n\n#### `pre-commit`\n[`pre-commit`](https://pre-commit.com) takes care of running all code-checks for formatting and linting. Run `make dev` to install `pre-commit` in your local environment (we recommend running this command with a python virtual environment active). This command installs several pip executables including black, mypy, and flake8. Once this is done you can use any of the linter-based make targets as well as a git pre-commit hook that will ensure proper formatting and linting.\n\n#### `tox`\n\n[`tox`](https://tox.readthedocs.io/en/latest/) takes care of managing virtualenvs and install dependencies in order to run tests. You can also run tests in parallel, for example, you can run unit tests for Python 3.8, Python 3.9, Python 3.10 and Python 3.11 checks in parallel with `tox -p`. Also, you can run unit tests for specific python versions with `tox -e py38`. The configuration for these tests in located in `tox.ini`.\n\n#### `pytest`\n\nFinally, you can also run a specific test or group of tests using [`pytest`](https://docs.pytest.org/en/latest/) directly. With a virtualenv active and dev dependencies installed you can do things like:\n\n```sh\n# run all unit tests in a file\npython3 -m pytest tests/unit/test_invocation_id.py\n# run a specific unit test\npython3 -m pytest tests/unit/test_invocation_id.py::TestInvocationId::test_invocation_id\n# run specific Postgres functional tests\npython3 -m pytest tests/functional/sources\n```\n\n> See [pytest usage docs](https://docs.pytest.org/en/6.2.x/usage.html) for an overview of useful command-line options.\n\n### Unit, Integration, Functional?\n\nHere are some general rules for adding tests:\n* unit tests (`tests/unit`) don’t need to access a database; \"pure Python\" tests should be written as unit tests\n* functional tests (`tests/functional`) cover anything that interacts with a database, namely adapter\n\n## Debugging\n\n1. The logs for a `dbt run` have stack traces and other information for debugging errors (in `logs/dbt.log` in your project directory).\n2. Try using a debugger, like `ipdb`. For pytest: `--pdb --pdbcls=IPython.terminal.debugger:pdb`\n3. Sometimes, it’s easier to debug on a single thread: `dbt --single-threaded run`\n4. To make print statements from Jinja macros:  `{{ log(msg, info=true) }}`\n5. You can also add `{{ debug() }}` statements, which will drop you into some auto-generated code that the macro wrote.\n6. The dbt “artifacts” are written out to the ‘target’ directory of your dbt project. They are in unformatted json, which can be hard to read. Format them with:\n> python -m json.tool target/run_results.json > run_results.json\n\n### Assorted development tips\n* Append `# type: ignore` to the end of a line if you need to disable `mypy` on that line.\n* Sometimes flake8 complains about lines that are actually fine, in which case you can put a comment on the line such as: # noqa or # noqa: ANNN, where ANNN is the error code that flake8 issues.\n* To collect output for `CProfile`, run dbt with the `-r` option and the name of an output file, i.e. `dbt -r dbt.cprof run`. If you just want to profile parsing, you can do: `dbt -r dbt.cprof parse`. `pip` install `snakeviz` to view the output. Run `snakeviz dbt.cprof` and output will be rendered in a browser window.\n\n## Adding or modifying a CHANGELOG Entry\n\nWe use [changie](https://changie.dev) to generate `CHANGELOG` entries. **Note:** Do not edit the `CHANGELOG.md` directly. Your modifications will be lost.\n\nFollow the steps to [install `changie`](https://changie.dev/guide/installation/) for your system.\n\nOnce changie is installed and your PR is created for a new feature, simply run the following command and changie will walk you through the process of creating a changelog entry:\n\n```shell\nchangie new\n```\n\nCommit the file that's created and your changelog entry is complete!\n\nIf you are contributing to a feature already in progress, you will modify the changie yaml file in dbt/.changes/unreleased/ related to your change. If you need help finding this file, please ask within the discussion for the pull request!\n\nYou don't need to worry about which `dbt-core` version your change will go into. Just create the changelog entry with `changie`, and open your PR against the `main` branch. All merged changes will be included in the next minor version of `dbt-core`. The Core maintainers _may_ choose to \"backport\" specific changes in order to patch older minor versions. In that case, a maintainer will take care of that backport after merging your PR, before releasing the new version of `dbt-core`.\n\n## Submitting a Pull Request\n\nCode can be merged into the current development branch `main` by opening a pull request. If the proposal looks like it's on the right track, then a `dbt-core` maintainer will triage the PR and label it as `ready_for_review`. From this point, two code reviewers will be assigned with the aim of responding to any updates to the PR within about one week. They may suggest code revision for style or clarity, or request that you add unit or integration test(s). These are good things! We believe that, with a little bit of help, anyone can contribute high-quality code. Once merged, your contribution will be available for the next release of `dbt-core`.\n\nAutomated tests run via GitHub Actions. If you're a first-time contributor, all tests (including code checks and unit tests) will require a maintainer to approve. Changes in the `dbt-core` repository trigger integration tests against Postgres. dbt Labs also provides CI environments in which to test changes to other adapters, triggered by PRs in those adapters' repositories, as well as periodic maintenance checks of each adapter in concert with the latest `dbt-core` code changes.\n\nOnce all tests are passing and your PR has been approved, a `dbt-core` maintainer will merge your changes into the active development branch. And that's it! Happy developing :tada:\n\n## Troubleshooting Tips\n\nSometimes, the content license agreement auto-check bot doesn't find a user's entry in its roster. If you need to force a rerun, add `@cla-bot check` in a comment on the pull request.\n"
        },
        {
          "name": "Dockerfile.test",
          "type": "blob",
          "size": 2.091796875,
          "content": "##\n#  This dockerfile is used for local development and adapter testing only.\n#  See `/docker` for a generic and production-ready docker file\n##\n\nFROM ubuntu:22.04\n\nENV DEBIAN_FRONTEND noninteractive\n\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n    software-properties-common gpg-agent \\\n    && add-apt-repository ppa:git-core/ppa -y \\\n    && apt-get dist-upgrade -y \\\n    && apt-get install -y --no-install-recommends \\\n    netcat \\\n    postgresql \\\n    curl \\\n    git \\\n    ssh \\\n    software-properties-common \\\n    make \\\n    build-essential \\\n    ca-certificates \\\n    libpq-dev \\\n    libsasl2-dev \\\n    libsasl2-2 \\\n    libsasl2-modules-gssapi-mit \\\n    libyaml-dev \\\n    unixodbc-dev \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get install -y \\\n    python-is-python3 \\\n    python-dev-is-python3 \\\n    python3-pip \\\n    python3.9 \\\n    python3.9-dev \\\n    python3.9-venv \\\n    python3.10 \\\n    python3.10-dev \\\n    python3.10-venv \\\n    python3.11 \\\n    python3.11-dev \\\n    python3.11-venv \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\nARG DOCKERIZE_VERSION=v0.6.1\nRUN curl -LO https://github.com/jwilder/dockerize/releases/download/$DOCKERIZE_VERSION/dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz \\\n    && tar -C /usr/local/bin -xzvf dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz \\\n    && rm dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz\n\nRUN pip3 install -U tox wheel six setuptools\n\n# These args are passed in via docker-compose, which reads then from the .env file.\n# On Linux, run `make .env` to create the .env file for the current user.\n# On MacOS and Windows, these can stay unset.\nARG USER_ID\nARG GROUP_ID\n\nRUN if [ ${USER_ID:-0} -ne 0 ] && [ ${GROUP_ID:-0} -ne 0 ]; then \\\n    groupadd -g ${GROUP_ID} dbt_test_user && \\\n    useradd -m -l -u ${USER_ID} -g ${GROUP_ID} dbt_test_user; \\\n    else \\\n    useradd -mU -l dbt_test_user; \\\n    fi\nRUN mkdir /usr/app && chown dbt_test_user /usr/app\nRUN mkdir /home/tox && chown dbt_test_user /home/tox\n\nWORKDIR /usr/app\nVOLUME /usr/app\n\nUSER dbt_test_user\n\nENV PYTHONIOENCODING=utf-8\nENV LANG C.UTF-8\n"
        },
        {
          "name": "License.md",
          "type": "blob",
          "size": 11.078125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2021 dbt Labs, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.890625,
          "content": ".DEFAULT_GOAL:=help\n\n# Optional flag to run target in a docker container.\n# (example `make test USE_DOCKER=true`)\nifeq ($(USE_DOCKER),true)\n\tDOCKER_CMD := docker-compose run --rm test\nendif\n\n#\n# To override CI_flags, create a file at this repo's root dir named `makefile.test.env`. Fill it\n# with any ENV_VAR overrides required by your test environment, e.g.\n#    DBT_TEST_USER_1=user\n#    LOG_DIR=\"dir with a space in it\"\n#\n# Warn: Restrict each line to one variable only.\n#\nifeq (./makefile.test.env,$(wildcard ./makefile.test.env))\n\tinclude ./makefile.test.env\nendif\n\nCI_FLAGS =\\\n\tDBT_TEST_USER_1=$(if $(DBT_TEST_USER_1),$(DBT_TEST_USER_1),dbt_test_user_1)\\\n\tDBT_TEST_USER_2=$(if $(DBT_TEST_USER_2),$(DBT_TEST_USER_2),dbt_test_user_2)\\\n\tDBT_TEST_USER_3=$(if $(DBT_TEST_USER_3),$(DBT_TEST_USER_3),dbt_test_user_3)\\\n\tRUSTFLAGS=$(if $(RUSTFLAGS),$(RUSTFLAGS),\"-D warnings\")\\\n\tLOG_DIR=$(if $(LOG_DIR),$(LOG_DIR),./logs)\\\n\tDBT_LOG_FORMAT=$(if $(DBT_LOG_FORMAT),$(DBT_LOG_FORMAT),json)\n\n\n.PHONY: dev_req\ndev_req: ## Installs dbt-* packages in develop mode along with only development dependencies.\n\t@\\\n\tpip install -r dev-requirements.txt -r editable-requirements.txt\n\n.PHONY: dev\ndev: dev_req ## Installs dbt-* packages in develop mode along with development dependencies and pre-commit.\n\t@\\\n\tpre-commit install\n\n.PHONY: dev-uninstall\ndev-uninstall: ## Uninstall all packages in venv except for build tools\n\t@\\\n    pip freeze | grep -v \"^-e\" | cut -d \"@\" -f1 | xargs pip uninstall -y; \\\n    pip uninstall -y dbt-core\n\n.PHONY: core_proto_types\ncore_proto_types:  ## generates google protobuf python file from core_types.proto\n\tprotoc -I=./core/dbt/events --python_out=./core/dbt/events ./core/dbt/events/core_types.proto\n\n.PHONY: mypy\nmypy: .env ## Runs mypy against staged changes for static type checking.\n\t@\\\n\t$(DOCKER_CMD) pre-commit run --hook-stage manual mypy-check | grep -v \"INFO\"\n\n.PHONY: flake8\nflake8: .env ## Runs flake8 against staged changes to enforce style guide.\n\t@\\\n\t$(DOCKER_CMD) pre-commit run --hook-stage manual flake8-check | grep -v \"INFO\"\n\n.PHONY: black\nblack: .env ## Runs black  against staged changes to enforce style guide.\n\t@\\\n\t$(DOCKER_CMD) pre-commit run --hook-stage manual black-check -v | grep -v \"INFO\"\n\n.PHONY: lint\nlint: .env ## Runs flake8 and mypy code checks against staged changes.\n\t@\\\n\t$(DOCKER_CMD) pre-commit run flake8-check --hook-stage manual | grep -v \"INFO\"; \\\n\t$(DOCKER_CMD) pre-commit run mypy-check --hook-stage manual | grep -v \"INFO\"\n\n.PHONY: unit\nunit: .env ## Runs unit tests with py\n\t@\\\n\t$(DOCKER_CMD) tox -e py\n\n.PHONY: test\ntest: .env ## Runs unit tests with py and code checks against staged changes.\n\t@\\\n\t$(DOCKER_CMD) tox -e py; \\\n\t$(DOCKER_CMD) pre-commit run black-check --hook-stage manual | grep -v \"INFO\"; \\\n\t$(DOCKER_CMD) pre-commit run flake8-check --hook-stage manual | grep -v \"INFO\"; \\\n\t$(DOCKER_CMD) pre-commit run mypy-check --hook-stage manual | grep -v \"INFO\"\n\n.PHONY: integration\nintegration: .env ## Runs core integration tests using postgres with py-integration\n\t@\\\n\t$(CI_FLAGS) $(DOCKER_CMD) tox -e py-integration -- -nauto\n\n.PHONY: integration-fail-fast\nintegration-fail-fast: .env ## Runs core integration tests using postgres with py-integration in \"fail fast\" mode.\n\t@\\\n\t$(DOCKER_CMD) tox -e py-integration -- -x -nauto\n\n.PHONY: interop\ninterop: clean\n\t@\\\n\tmkdir $(LOG_DIR) && \\\n\t$(CI_FLAGS) $(DOCKER_CMD) tox -e py-integration -- -nauto && \\\n\tLOG_DIR=$(LOG_DIR) cargo run --manifest-path test/interop/log_parsing/Cargo.toml\n\n.PHONY: setup-db\nsetup-db: ## Setup Postgres database with docker-compose for system testing.\n\t@\\\n\tdocker-compose up -d database && \\\n\tPGHOST=localhost PGUSER=root PGPASSWORD=password PGDATABASE=postgres bash test/setup_db.sh\n\n# This rule creates a file named .env that is used by docker-compose for passing\n# the USER_ID and GROUP_ID arguments to the Docker image.\n.env: ## Setup step for using using docker-compose with make target.\n\t@touch .env\nifneq ($(OS),Windows_NT)\nifneq ($(shell uname -s), Darwin)\n\t@echo USER_ID=$(shell id -u) > .env\n\t@echo GROUP_ID=$(shell id -g) >> .env\nendif\nendif\n\n.PHONY: clean\nclean: ## Resets development environment.\n\t@echo 'cleaning repo...'\n\t@rm -f .coverage\n\t@rm -f .coverage.*\n\t@rm -rf .eggs/\n\t@rm -f .env\n\t@rm -rf .tox/\n\t@rm -rf build/\n\t@rm -rf dbt.egg-info/\n\t@rm -f dbt_project.yml\n\t@rm -rf dist/\n\t@rm -f htmlcov/*.{css,html,js,json,png}\n\t@rm -rf logs/\n\t@rm -rf target/\n\t@find . -type f -name '*.pyc' -delete\n\t@find . -type d -name '__pycache__' -depth -delete\n\t@echo 'done.'\n\n\n.PHONY: help\nhelp: ## Show this help message.\n\t@echo 'usage: make [target] [USE_DOCKER=true]'\n\t@echo\n\t@echo 'targets:'\n\t@grep -E '^[8+a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n\t@echo\n\t@echo 'options:'\n\t@echo 'use USE_DOCKER=true to run target in a docker container'\n\n.PHONY: json_schema\njson_schema: ## Update generated JSON schema using code changes.\n\tscripts/collect-artifact-schema.py --path schemas\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.517578125,
          "content": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/dbt-labs/dbt-core/fa1ea14ddfb1d5ae319d5141844910dd53ab2834/etc/dbt-core.svg\" alt=\"dbt logo\" width=\"750\"/>\n</p>\n<p align=\"center\">\n  <a href=\"https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml\">\n    <img src=\"https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml/badge.svg?event=push\" alt=\"CI Badge\"/>\n  </a>\n</p>\n\n**[dbt](https://www.getdbt.com/)** enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.\n\n![architecture](https://github.com/dbt-labs/dbt-core/blob/202cb7e51e218c7b29eb3b11ad058bd56b7739de/etc/dbt-transform.png)\n\n## Understanding dbt\n\nAnalysts using dbt can transform their data by simply writing select statements, while dbt handles turning these statements into tables and views in a data warehouse.\n\nThese select statements, or \"models\", form a dbt project. Models frequently build on top of one another – dbt makes it easy to [manage relationships](https://docs.getdbt.com/docs/ref) between models, and [visualize these relationships](https://docs.getdbt.com/docs/documentation), as well as assure the quality of your transformations through [testing](https://docs.getdbt.com/docs/testing).\n\n![dbt dag](https://raw.githubusercontent.com/dbt-labs/dbt-core/6c6649f9129d5d108aa3b0526f634cd8f3a9d1ed/etc/dbt-dag.png)\n\n## Getting started\n\n- [Install dbt Core](https://docs.getdbt.com/docs/get-started/installation) or explore the [dbt Cloud CLI](https://docs.getdbt.com/docs/cloud/cloud-cli-installation), a command-line interface powered by [dbt Cloud](https://docs.getdbt.com/docs/cloud/about-cloud/dbt-cloud-features) that enhances collaboration.\n- Read the [introduction](https://docs.getdbt.com/docs/introduction/) and [viewpoint](https://docs.getdbt.com/docs/about/viewpoint/)\n\n## Join the dbt Community\n\n- Be part of the conversation in the [dbt Community Slack](http://community.getdbt.com/)\n- Read more on the [dbt Community Discourse](https://discourse.getdbt.com)\n\n## Reporting bugs and contributing code\n\n- Want to report a bug or request a feature? Let us know and open [an issue](https://github.com/dbt-labs/dbt-core/issues/new/choose)\n- Want to help us build dbt? Check out the [Contributing Guide](https://github.com/dbt-labs/dbt-core/blob/HEAD/CONTRIBUTING.md)\n\n## Code of Conduct\n\nEveryone interacting in the dbt project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the [dbt Code of Conduct](https://community.getdbt.com/code-of-conduct).\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.072265625,
          "content": "[About dbt Core versions](https://docs.getdbt.com/docs/dbt-versions/core)\n"
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.9619140625,
          "content": "ignore:\n  - \".github\"\n  - \".changes\"\n\ncoverage:\n  status:\n    project:\n      default:\n        target: auto\n        threshold: 0.1% # Reduce noise by ignoring rounding errors in coverage drops\n    patch:\n      default:\n        target: auto\n        threshold: 80%\n\ncomment:\n  layout: \"header, diff, flags, components\"  # show component info in the PR comment\n\ncomponent_management:\n  default_rules:  # default rules that will be inherited by all components\n    statuses:\n      - type: project # in this case every component that doens't have a status defined will have a project type one\n        target: auto\n        threshold: 0.1%\n      - type: patch\n        target: 80%\n  individual_components:\n    - component_id: unittests\n      name: \"Unit Tests\"\n      flag_regexes:\n        - \"unit\"\n      statuses:\n        - type: patch\n          target: 80%\n          threshold: 5%\n    - component_id: integrationtests\n      name: \"Integration Tests\"\n      flag_regexes:\n        - \"integration\"\n"
        },
        {
          "name": "core",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev-requirements.txt",
          "type": "blob",
          "size": 0.98046875,
          "content": "git+https://github.com/dbt-labs/dbt-adapters.git@main\ngit+https://github.com/dbt-labs/dbt-adapters.git@main#subdirectory=dbt-tests-adapter\ngit+https://github.com/dbt-labs/dbt-common.git@main\ngit+https://github.com/dbt-labs/dbt-postgres.git@main\n# black must match what's in .pre-commit-config.yaml to be sure local env matches CI\nblack==24.3.0\nbumpversion\nddtrace==2.3.0\ndocutils\n# flake8 must match what's in .pre-commit-config.yaml to be sure local env matches CI\nflake8==4.0.1\nflaky\nfreezegun>=1.4.0,<1.5\nhypothesis\nipdb\n# isort must match what's in .pre-commit-config.yaml to be sure local env matches CI\nisort==5.13.2\n# mypy must match what's in .pre-commit-config.yaml to be sure local env matches CI\nmypy==1.4.1\npip-tools\npre-commit\nprotobuf>=5.0,<6.0\npytest>=7.4,<8.0\npytest-cov\npytest-csv>=3.0,<4.0\npytest-dotenv\npytest-mock\npytest-split\npytest-xdist\nsphinx\ntox>=3.13\ntypes-docutils\ntypes-PyYAML\ntypes-Jinja2\ntypes-mock\ntypes-protobuf>=5.0,<6.0\ntypes-pytz\ntypes-requests\ntypes-setuptools\nmocker\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.7197265625,
          "content": "##\n#  This compose file is used for local development and adapter testing only.\n#  See `/docker` for a generic and production-ready docker file\n##\n\nversion: \"3.5\"\nservices:\n  database:\n    image: postgres\n    environment:\n      POSTGRES_USER: \"root\"\n      POSTGRES_PASSWORD: \"password\"\n      POSTGRES_DB: \"dbt\"\n    ports:\n      - \"5432:5432\"\n\n  test:\n    build:\n      context: .\n      dockerfile: Dockerfile.test\n      args:\n        # Run `make .env` to set $USER_ID and $GROUP_ID\n        USER_ID: ${USER_ID:-}\n        GROUP_ID: ${GROUP_ID:-}\n    command: \"/root/.virtualenvs/dbt/bin/pytest\"\n    environment:\n      POSTGRES_TEST_HOST: \"database\"\n    volumes:\n      - .:/usr/app\n    working_dir: /usr/app\n    depends_on:\n      - database\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "editable-requirements.txt",
          "type": "blob",
          "size": 0.009765625,
          "content": "-e ./core\n"
        },
        {
          "name": "etc",
          "type": "tree",
          "content": null
        },
        {
          "name": "events",
          "type": "tree",
          "content": null
        },
        {
          "name": "performance",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.1884765625,
          "content": "[tool.mypy]\n# TODO: widen range of files as we fix issues\nfiles = 'core/dbt'\nmypy_path = \"third-party-stubs/\"\nnamespace_packages = true\n\n[tool.black]\nline-length = 99\ntarget-version = ['py38']\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.234375,
          "content": "[pytest]\nfilterwarnings =\n    ignore:.*'soft_unicode' has been renamed to 'soft_str'*:DeprecationWarning\n    ignore:unclosed file .*:ResourceWarning\nenv_files =\n    test.env\ntestpaths =\n    tests/functional\n    tests/unit\npythonpath = core\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0068359375,
          "content": "./core\n"
        },
        {
          "name": "schemas",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third-party-stubs",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.943359375,
          "content": "[tox]\nskipsdist = True\nenvlist = unit,integration\n\n[testenv:{unit,py38,py39,py310,py311,py}]\ndescription = unit testing\ndownload = true\nskip_install = true\npassenv =\n  DBT_*\n  PYTEST_ADDOPTS\ncommands =\n  {envpython} -m pytest --cov=core --cov-report=xml {posargs} tests/unit\ndeps =\n  -rdev-requirements.txt\n  -reditable-requirements.txt\n\n[testenv:{integration,py38-integration,py39-integration,py310-integration,py311-integration,py-integration}]\ndescription = functional testing\ndownload = true\nskip_install = true\npassenv =\n  DBT_*\n  POSTGRES_TEST_*\n  PYTEST_ADDOPTS\n  DD_CIVISIBILITY_AGENTLESS_ENABLED\n  DD_API_KEY\n  DD_SITE\n  DD_ENV\n  DD_SERVICE\ncommands =\n  {envpython} -m pytest --cov=core --cov-append --cov-report=xml {posargs} tests/functional -k \"not tests/functional/graph_selection\"\n  {envpython} -m pytest --cov=core --cov-append --cov-report=xml {posargs} tests/functional/graph_selection\n\ndeps =\n  -rdev-requirements.txt\n  -reditable-requirements.txt\n"
        }
      ]
    }
  ]
}