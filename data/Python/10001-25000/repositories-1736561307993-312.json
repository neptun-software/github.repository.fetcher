{
  "metadata": {
    "timestamp": 1736561307993,
    "page": 312,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Embedding/Chinese-Word-Vectors",
      "stars": 11908,
      "defaultBranch": "master",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.61328125,
          "content": "# Chinese Word Vectors 中文词向量\n[中文](https://github.com/Embedding/Chinese-Word-Vectors/blob/master/README_zh.md)\n\nThis project provides 100+ Chinese Word Vectors (embeddings) trained with different **representations** (dense and sparse), **context features** (word, ngram, character, and more), and **corpora**. One can easily obtain pre-trained vectors with different properties and use them for downstream tasks. \n\nMoreover, we provide a Chinese analogical reasoning dataset **CA8** and an evaluation toolkit for users to evaluate the quality of their word vectors.\n\n## Reference\nPlease cite the paper, if using these embeddings and CA8 dataset.\n\nShen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href=\"http://aclweb.org/anthology/P18-2023\"><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.\n\n```\n@InProceedings{P18-2023,\n  author =  \"Li, Shen\n    and Zhao, Zhe\n    and Hu, Renfen\n    and Li, Wensi\n    and Liu, Tao\n    and Du, Xiaoyong\",\n  title =   \"Analogical Reasoning on Chinese Morphological and Semantic Relations\",\n  booktitle =   \"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n  year =  \"2018\",\n  publisher =   \"Association for Computational Linguistics\",\n  pages =   \"138--143\",\n  location =  \"Melbourne, Australia\",\n  url =   \"http://aclweb.org/anthology/P18-2023\"\n}\n```\n\n&nbsp;\n\nA detailed analysis of the relation between the intrinsic and extrinsic evaluations of Chinese word embeddings is shown in the paper:\n\nYuanyuan Qiu, Hongzheng Li, Shen Li, Yingdi Jiang, Renfen Hu, Lijiao Yang. <a href=\"http://www.cips-cl.org/static/anthology/CCL-2018/CCL-18-086.pdf\"><em>Revisiting Correlations between Intrinsic and Extrinsic Evaluations of Word Embeddings</em></a>. Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. Springer, Cham, 2018. 209-221. (CCL & NLP-NABD 2018 Best Paper)\n\n```\n@incollection{qiu2018revisiting,\n  title={Revisiting Correlations between Intrinsic and Extrinsic Evaluations of Word Embeddings},\n  author={Qiu, Yuanyuan and Li, Hongzheng and Li, Shen and Jiang, Yingdi and Hu, Renfen and Yang, Lijiao},\n  booktitle={Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data},\n  pages={209--221},\n  year={2018},\n  publisher={Springer}\n}\n```\n\n## Format\nThe pre-trained vector files are in text format. Each line contains a word and its vector. Each value is separated by space. The first line records the meta information: the first number indicates the number of words in the file and the second indicates the dimension size. \n\nBesides dense word vectors (trained with SGNS), we also provide sparse vectors (trained with PPMI). They are in the same format with liblinear, where the number before \" : \" denotes dimension index and the number after the \" : \" denotes the value. \n\n## Pre-trained Chinese Word Vectors\n\n### Basic Settings\n\n<table align=\"center\">\n  <tr align=\"center\">\n    <td><b>Window Size</b></td>\n    <td><b>Dynamic Window</b></td>\n    <td><b>Sub-sampling</b></td>\n    <td><b>Low-Frequency Word</b></td>\n    <td><b>Iteration</b></td>\n    <td><b>Negative Sampling<sup>*</sup></b></td>\n  </tr>\n  <tr align=\"center\">\n    <td>5</td>\n    <td>Yes</td>\n    <td>1e-5</td>\n    <td>10</td>\n    <td>5</td>\n    <td>5</td>\n  </tr>\n</table>\n\n<sup>\\*</sup>Only for SGNS.\n\n### Various Domains\n\nChinese Word Vectors trained with different representations, context features, and corpora.\n\n<table align=\"center\">\n    <tr align=\"center\">\n        <td colspan=\"5\"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>\n    </tr>\n    <tr align=\"center\">\n        <td rowspan=\"2\">Corpus</td>\n        <td colspan=\"4\">Context Features</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Word</td>\n      <td>Word + Ngram</td>\n      <td>Word + Character</td>\n      <td>Word + Character + Ngram</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Baidu Encyclopedia 百度百科</td>\n      <td><a href=\"https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1IiIbQGJ_AooTj5s8aZYcvA\">300d</a> / PWD: 5555</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Wikipedia_zh 中文维基百科</td>\n      <td><a href=\"https://pan.baidu.com/s/11hSZJN-NWBEvryIED6Donw?pwd=qfgv\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1RWcPWQEiCrwna7xmhI8ARg?pwd=jp7e\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1DKvgg0RgtqwyDPs1IbS0TQ?pwd=s22w\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1OTfYo_sQamCYwJLdp3KHnw?pwd=k6p9\">300d</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>People's Daily News 人民日报</td>\n      <td><a href=\"https://pan.baidu.com/s/19sqMz-JAhhxh3o6ecvQxQw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1upPkA8KJnxTZBfjuNDtaeQ\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1BvKk2QjbtQMch7EISppW2A\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/19Vso_k79FZb5OZCWQPAnFQ\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Sogou News 搜狗新闻</td>\n      <td><a href=\"https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/13yVrXeGYkxdGW3P6juiQmA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1pUqyn7mnPcUmzxT64gGpSw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Financial News 金融新闻</td>\n      <td><a href=\"https://pan.baidu.com/s/1c8wmsqdrfUbQQ6j2Dx5NwQ?pwd=nakr\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1EXVpN8-vMr1-f2l4kZICLg?pwd=ki7t\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1EXVpN8-vMr1-f2l4kZICLg?pwd=ki7t\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/19JWtZL6U8P-XfE5LsTlftg?pwd=gbnb\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Zhihu_QA 知乎问答 </td>\n      <td><a href=\"https://pan.baidu.com/s/1VGOs0RH7DXE5vRrtw6boQA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1OQ6fQLCgqT43WTwh5fh_lg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1_xogqF9kJT6tmQHSAYrYeg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1Fo27Lv_0nz8FXg-xbOz14Q\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Weibo 微博</td>\n      <td><a href=\"https://pan.baidu.com/s/1zbuUJEEEpZRNHxZ7Gezzmw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/11PWBcvruXEDvKf2TiIXntg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/10bhJpaXMCUK02nHvRAttqA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1FHl_bQkYucvVk-j2KG4dxA\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Literature 文学作品</td>\n      <td><a href=\"https://pan.baidu.com/s/1ciq8iXtcrHpu3ir_VhK0zg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1Oa4CkPd8o2xd6LEAaa4gmg\">300d</a> / PWD: z5b4</td>\n      <td><a href=\"https://pan.baidu.com/s/1IG8IxNp2s7vVklz-vyZR9A\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1SEOKrJYS14HpqIaQT462kA\">300d</a> / PWD: yenb</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>\n      <td><a href=\"https://pan.baidu.com/s/1vPSeUsSiWYXEWAuokLR0qQ\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1sS9E7sclvS_UZcBgHN7xLQ\">300d</a></td>\n      <td>NAN</td>\n      <td>NAN</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Mixed-large 综合<br>Baidu Netdisk / Google Drive</td>\n      <td>\n        <a href=\"https://pan.baidu.com/s/1luy-GlTdqqvJ3j-A4FcIOw\">300d</a><br>\n        <a href=\"https://drive.google.com/open?id=1Zh9ZCEu8_eSQ-qkYVQufQDNKPC4mtEKR\">300d</a>\n      </td>\n      <td>\n        <a href=\"https://pan.baidu.com/s/1oJol-GaRMk4-8Ejpzxo6Gw\">300d</a><br>\n        <a href=\"https://drive.google.com/open?id=1WUU9LnoAjs--1E_WqcghLJ-Pp8bb38oS\">300d</a>\n      </td>\n      <td>\n        <a href=\"https://pan.baidu.com/s/1DjIGENlhRbsVyHW-caRePg\">300d</a><br>\n        <a href=\"https://drive.google.com/open?id=1aVAK0Z2E5DkdIH6-JHbiWSL5dbAcz6c3\">300d</a>\n      </td>\n      <td>\n        <a href=\"https://pan.baidu.com/s/14JP1gD7hcmsWdSpTvA3vKA\">300d</a><br>\n        <a href=\"https://drive.google.com/open?id=1kSAl4_AOg3_6ayU7KRM0Nk66uGdSZdnk\">300d</a>\n      </td>\n    </tr>\n</table>\n\n<table align=\"center\">\n    <tr align=\"center\">\n        <td colspan=\"5\"><b>Positive Pointwise Mutual Information (PPMI)</b></td>\n    </tr>\n    <tr align=\"center\">\n        <td rowspan=\"2\">Corpus</td>\n        <td colspan=\"4\">Context Features</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Word</td>\n      <td>Word + Ngram</td>\n      <td>Word + Character</td>\n      <td>Word + Character + Ngram</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Baidu Encyclopedia 百度百科</td>\n      <td><a href=\"https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1KcfFdyO0-kE9S9CwzIisfw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1FXYM3CY161_4QMgiH8vasQ\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Wikipedia_zh 中文维基百科</td>\n      <td><a href=\"https://pan.baidu.com/s/172vD1NljxnbeubgXkuja4Q?pwd=k2hr\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1taIMttirPOw9Df51epIWBg?pwd=rmfh\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1-l9pdeUOwVzRVT4utvszfQ?pwd=ameb\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1VYI5GrKWR16gHvah38I3SQ?pwd=gzj8\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>People's Daily News 人民日报</td>\n      <td><a href=\"https://pan.baidu.com/s/1NLr1K7aapU2sYBvzbVny5g\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1LJl3Br0ccGDHP0XX2k3pVw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1GQQXGMn1AHh-BlifT0JD2g\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1Xm9Ec3O3rJ6ayrwVwonC7g\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Sogou News 搜狗新闻</td>\n      <td><a href=\"https://pan.baidu.com/s/1ECA51CZLp9_JB_me7YZ9-Q\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1FO39ZYy1mStERf_b53Y_yQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1lLBFBk8nn3spFAvKY9IJ6A\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1f-dLQZlZo_-B5ZKcPIc6rw\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Financial News 金融新闻</td>\n      <td><a href=\"https://pan.baidu.com/s/1yyJ7NZl-GabDJLbP-eYdCQ?pwd=9efk\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/17ZLOJpLXSQFxN0SZTITdIw?pwd=sjzy\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1rRGLUkA01kGceFDBOG9wlA?pwd=yve5\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1X-150CjeUPdQBq--Gr7w3A?pwd=qqc7\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Zhihu_QA 知乎问答 </td>\n      <td><a href=\"https://pan.baidu.com/s/1VaUP3YJC0IZKTbJ-1_8HZg\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1g39PKwT0kSmpneKOgXR5YQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1d8Bsuak0fyXxQOVUiNr-2w\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1D5fteBX0Vy4czEqpxXjlrQ\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Weibo 微博</td>\n      <td><a href=\"https://pan.baidu.com/s/15O2EbToOzjNSkzJwAOk_Ug\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/11Dqywn0hfMhysto7bZS1Dw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1wY-7mfV6nwDj_tru6W9h4Q\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1DMW-MgLApbQnWwDd-pT_qw\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Literature 文学作品</td>\n      <td><a href=\"https://pan.baidu.com/s/1HTHhlr8zvzhTwed7dO0sDg\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1jAuGJBxKqgapt__urGsBOQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/173AJfCoAV0ZA8Z31tKBdTA\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1dFCxke_Su3lLsuwZr7co3A\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>\n      <td><a href=\"https://pan.baidu.com/s/1NJ1Gc99oE0-GV0QxBqy-qw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1YGEgyXIbw0O4NtoM1ohjdA\">Sparse</a></td>\n      <td>NAN</td>\n      <td>NAN</td>\n    </tr>\n    </tr>\n    <tr  align=\"center\">\n      <td>Mixed-large 综合</td>\n      <td>Sparse</td>\n      <td>Sparse</td>\n      <td>Sparse</td>\n      <td>Sparse</td>\n    </tr>\n</table>\n\n<sup>\\*</sup>Character embeddings are provided, since most of Hanzi are words in the archaic Chinese.\n\n### Various Co-occurrence Information\n\nWe release word vectors upon different co-occurrence statistics. Target and context vectors are often called input and output vectors in some related papers. \n\nIn this part, one can obtain vectors of arbitrary linguistic units beyond word. For example, character vectors is in the context vectors of word-character.\n\nAll vectors are trained by SGNS on Baidu Encyclopedia.\n\n<table align=\"center\">\n  <tr align=\"center\">\n    <td><b>Feature</b></td>\n    <td><b>Co-occurrence Type</b></td>\n    <td><b>Target Word Vectors</b></td>\n    <td><b>Context Word Vectors</b></td>\n  </tr>\n  \n  <tr align=\"center\">\n  \t<td rowspan=\"1\">Word</td>\n    <td>Word → Word</td>\n    <td><a href=\"https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/18T6DRVmS_cZu5u64EbbESQ\">300d</a></td>\n  </tr>\n\n  <tr align=\"center\">\n    <td rowspan=\"3\">Ngram</td>\n    <td>Word → Ngram (1-2)</td>\n    <td><a href=\"https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/12asujjAaaqxNFYRNP-MThw\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>Word → Ngram (1-3)</td>\n    <td><a href=\"https://pan.baidu.com/s/1oUmbxsnSuXf2jU8Jxu7U8A\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1ylg6FfFHa0kXbiVz8bIL8g\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>Ngram (1-2) → Ngram (1-2)</td>\n    <td><a href=\"https://pan.baidu.com/s/1Za7DIGVhE6dMsTmxHb-izg\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1oKI4Cs9eo7bg5mqfY1hdmg\">300d</a></td>\n  </tr>\n  \n  <tr align=\"center\">\n    <td rowspan=\"3\">Character</td>\n    <td>Word → Character (1)</td>\n \t  <td><a href=\"https://pan.baidu.com/s/1c9yiosHKNIZwRlLzD_F1ig\">300d</a></td>\n    <td><a href=\"https://pan.baidu.com/s/1KGZ_x8r-lq-AuElLCSVzvQ\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>Word → Character (1-2)</td>\n \t  <td><a href=\"https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw\">300d</a></td>\n    <td><a href=\"https://pan.baidu.com/s/1q0ItLzbn5Tfb3LhepRCeEA\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>Word → Character (1-4)</td>\n    <td><a href=\"https://pan.baidu.com/s/1WNWAnba56Rqjmx-FAN_7_g\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1hJKTAz6PwS7wmz9wQgmYeg\">300d</a></td>\n  </tr>\n  \n  <tr align=\"center\">\n  \t<td rowspan=\"1\">Radical</td>\n    <td>Radical</td>\n    <td>300d</td>\n \t  <td>300d</td>\n  </tr>\n  \n  <tr align=\"center\">\n    <td rowspan=\"2\">Position</td>\n    <td>Word → Word (left/right)</td>\n    <td><a href=\"https://pan.baidu.com/s/1JvjcrXFZPknT5H5Xw6KRVg\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1m6K9CnIIS8FrQZdDuF6hPQ\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>Word → Word (distance)</td>\n    <td><a href=\"https://pan.baidu.com/s/1c29BDu4R1hyUX-sgvlHJnA\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1sMZHIc-7eU6gRalHwtBHZw\">300d</a></td>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>Global</td>\n    <td>Word → Text</td>\n    <td>300d</td>\n \t  <td>300d</td>\n  </tr>\n    \n  <tr align=\"center\">\n    <td rowspan=\"2\">Syntactic Feature</td>\n    <td>Word → POS</td>\n    <td>300d</td>\n \t  <td>300d</td>\n  </tr>\n  <tr align=\"center\">\n    <td>Word → Dependency</td>\n    <td>300d</td>\n \t  <td>300d</td>\n  </tr>\n</table>\n\n## Representations\nExisting word representation methods fall into one of the two classes, **dense** and **sparse** represnetations. SGNS model (a model in word2vec toolkit) and PPMI model are respectively typical methods of these two classes. SGNS model trains low-dimensional real (dense) vectors through a shallow neural network. It is also called neural embedding method. PPMI model is a sparse bag-of-feature representation weighted by positive-pointwise-mutual-information (PPMI) weighting scheme.\n\n## Context Features\nThree context features: **word**, **ngram**, and **character** are commonly used in the word embedding literature. Most word representation methods essentially exploit word-word co-occurrence statistics, namely using word as context feature **(word feature)**. Inspired by language modeling problem, we introduce ngram feature into the context. Both word-word and word-ngram co-occurrence statistics are used for training **(ngram feature)**. For Chinese, characters (Hanzi) often convey strong semantics. To this end, we consider using word-word and word-character co-occurrence statistics for learning word vectors. The length of character-level ngrams ranges from 1 to 4 **(character feature)**.\n\nBesides word, ngram, and character, there are other features which have substantial influence on properties of word vectors. For example, using entire text as context feature could introduce more topic information into word vectors; using dependency parse as context feature could add syntactic constraint to word vectors. 17 co-occurrence types are considered in this project.\n\n## Corpus\nWe made great efforts to collect corpus across various domains. All text data are preprocessed by removing html and xml tags. Only the plain text are kept and [HanLP(v_1.5.3)](https://github.com/hankcs/HanLP) is used for word segmentation. In addition, traditional Chinese characters are converted into simplified characters with [Open Chinese Convert (OpenCC)](https://github.com/BYVoid/OpenCC). The detailed corpora information is listed as follows:\n\n<table align=\"center\">\n\t<tr align=\"center\">\n\t\t<td><b>Corpus</b></td>\n\t\t<td><b>Size</b></td>\n\t\t<td><b>Tokens</b></td>\n\t\t<td><b>Vocabulary Size</b></td>\n\t\t<td><b>Description</b></td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Baidu Encyclopedia<br />百度百科</td>\n\t\t<td>4.1G</td>\n\t\t<td>745M</td>\n\t\t<td>5422K</td>\n\t\t<td>Chinese Encyclopedia data from<br />https://baike.baidu.com/</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Wikipedia_zh<br />中文维基百科</td>\n\t\t<td>1.3G</td>\n\t\t<td>223M</td>\n\t\t<td>2129K</td>\n\t\t<td>Chinese Wikipedia data from<br />https://dumps.wikimedia.org/</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>People's Daily News<br />人民日报</td>\n\t\t<td>3.9G</td>\n\t\t<td>668M</td>\n\t\t<td>1664K</td>\n\t\t<td>News data from People's Daily(1946-2017)<br />http://data.people.com.cn/</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Sogou News<br />搜狗新闻</td>\n\t\t<td>3.7G</td>\n\t\t<td>649M</td>\n\t\t<td>1226K</td>\n\t\t<td>News data provided by Sogou labs<br />http://www.sogou.com/labs/</td>\n\t</tr>\n  <tr align=\"center\">\n    <td>Financial News<br />金融新闻</td>\n    <td>6.2G</td>\n    <td>1055M</td>\n    <td>2785K</td>\n    <td>Financial news collected from multiple news websites</td>\n  </tr>\n\t<tr align=\"center\">\n\t\t<td>Zhihu_QA<br />知乎问答</td>\n\t\t<td>2.1G</td>\n\t\t<td>384M</td>\n\t\t<td>1117K</td>\n\t\t<td>Chinese QA data from<br />https://www.zhihu.com/</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Weibo<br />微博</td>\n\t\t<td>0.73G</td>\n\t\t<td>136M</td>\n\t\t<td>850K</td>\n\t\t<td>Chinese microblog data provided by NLPIR Lab<br />http://www.nlpir.org/wordpress/download/weibo.7z</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Literature<br />文学作品</td>\n\t\t<td>0.93G</td>\n\t\t<td>177M</td>\n\t\t<td>702K</td>\n\t\t<td>8599 modern Chinese literature works</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Mixed-large<br />综合</td>\n\t\t<td>22.6G</td>\n    <td>4037M</td>\n    <td>10653K</td>\n\t\t<td>We build the large corpus by merging the above corpora.</td>\n\t</tr>\n  <tr align=\"center\">\n    <td>Complete Library in Four Sections<br />四库全书</td>\n    <td>1.5G</td>\n    <td>714M</td>\n    <td>21.8K</td>\n    <td>The largest collection of texts in pre-modern China.</td>\n  </tr>\n</table>\n\nAll words are concerned, including low frequency words.\n\n## Toolkits\nAll word vectors are trained by [ngram2vec](https://github.com/zhezhaoa/ngram2vec/) toolkit. Ngram2vec toolkit is a superset of [word2vec](https://github.com/svn2github/word2vec) and [fasttext](https://github.com/facebookresearch/fastText) toolkit, where arbitrary context features and models are supported.\n\n## Chinese Word Analogy Benchmarks\nThe quality of word vectors is often evaluated by analogy question tasks. In this project, two benchmarks are exploited for evaluation. The first is CA-translated, where most analogy questions are directly translated from English benchmark. Although CA-translated has been widely used in many Chinese word embedding papers, it only contains questions of three semantic questions and covers 134 Chinese words. In contrast, CA8 is specifically designed for Chinese language. It contains 17813 analogy questions and covers comprehensive morphological and semantic relations. The CA-translated, CA8, and their detailed descriptions are provided in [**testsets**](https://github.com/Embedding/Chinese-Word-Vectors/tree/master/testsets) folder.\n\n## Evaluation Toolkit\nWe present an evaluation toolkit in [**evaluation**](https://github.com/Embedding/Chinese-Word-Vectors/tree/master/evaluation) folder. \n\nRun the following codes to evaluate dense vectors.\n```\n$ python ana_eval_dense.py -v <vector.txt> -a CA8/morphological.txt\n$ python ana_eval_dense.py -v <vector.txt> -a CA8/semantic.txt\n```\nRun the following codes to evaluate sparse vectors.\n```\n$ python ana_eval_sparse.py -v <vector.txt> -a CA8/morphological.txt\n$ python ana_eval_sparse.py -v <vector.txt> -a CA8/semantic.txt\n```\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 21.0087890625,
          "content": "# Chinese Word Vectors 中文词向量\n[For English](https://github.com/Embedding/Chinese-Word-Vectors/blob/master/README.md)\n\n本项目提供超过100种中文词向量，其中包括不同的表示方式（稠密和稀疏）、不同的上下文特征（词、N元组、字等等）、以及不同的训练语料。获取预训练词向量非常方便，下载后即可用于下游任务。\n\n此外，我们还提供了中文词类比任务数据集**CA8**和配套的评测工具，以便对中文词向量进行评估。\n\n## 参考文献\n如果使用了本项目的词向量和CA8数据集请进行如下引用：\n\nShen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href=\"http://aclweb.org/anthology/P18-2023\"><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.\n\n```\n@InProceedings{P18-2023,\n  author =  \"Li, Shen\n    and Zhao, Zhe\n    and Hu, Renfen\n    and Li, Wensi\n    and Liu, Tao\n    and Du, Xiaoyong\",\n  title =   \"Analogical Reasoning on Chinese Morphological and Semantic Relations\",\n  booktitle =   \"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n  year =  \"2018\",\n  publisher =   \"Association for Computational Linguistics\",\n  pages =   \"138--143\",\n  location =  \"Melbourne, Australia\",\n  url =   \"http://aclweb.org/anthology/P18-2023\"\n}\n```\n\n&nbsp;\n\n我们对中文词向量的内部和外部评估任务做了一个非常详尽的分析和对比，参见：\n\nYuanyuan Qiu, Hongzheng Li, Shen Li, Yingdi Jiang, Renfen Hu, Lijiao Yang. <a href=\"http://www.cips-cl.org/static/anthology/CCL-2018/CCL-18-086.pdf\"><em>Revisiting Correlations between Intrinsic and Extrinsic Evaluations of Word Embeddings</em></a>. Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. Springer, Cham, 2018. 209-221. (CCL & NLP-NABD 2018 Best Paper)\n\n```\n@incollection{qiu2018revisiting,\n  title={Revisiting Correlations between Intrinsic and Extrinsic Evaluations of Word Embeddings},\n  author={Qiu, Yuanyuan and Li, Hongzheng and Li, Shen and Jiang, Yingdi and Hu, Renfen and Yang, Lijiao},\n  booktitle={Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data},\n  pages={209--221},\n  year={2018},\n  publisher={Springer}\n}\n```\n\n## 格式\n所有的预训练词向量文件均为文本格式。每一行都包括一个词和它对应的词向量。所有的值均用空格分开。每个文件的第一行记录了基本信息：第一个数值是文件中总词数，第二个数值是向量维度。\n\n除了稠密的词向量（用SGNS方式训练的），我们也提供了稀疏的词向量（用PPMI方式训练的）。稀疏的词向量格式同liblinear中的一样，以“位置:数值”的方式存储。\n\n## 预训练中文词向量\n\n### 基本参数\n\n<table align=\"center\">\n  <tr align=\"center\">\n    <td><b>窗口大小</b></td>\n    <td><b>动态窗口</b></td>\n    <td><b>子采样</b></td>\n    <td><b>低频词阈值</b></td>\n    <td><b>迭代次数</b></td>\n    <td><b>负采样<sup>*</sup></b></td>\n  </tr>\n  <tr align=\"center\">\n    <td>5</td>\n    <td>是</td>\n    <td>1e-5</td>\n    <td>10</td>\n    <td>5</td>\n    <td>5</td>\n  </tr>\n</table>\n\n<sup>\\*</sup>仅适用于SGNS.\n\n### 不同领域\n\n下列词向量基于不同的表示方式、不同的上下文特征以及不同领域的语料训练而成。\n\n<table align=\"center\">\n    <tr align=\"center\">\n        <td colspan=\"5\"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>\n    </tr>\n    <tr align=\"center\">\n        <td rowspan=\"2\">语料</td>\n        <td colspan=\"4\">上下文特征</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>词</td>\n      <td>词 + N元组</td>\n      <td>词 + 字</td>\n      <td>词 + 字 + N元组</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Baidu Encyclopedia 百度百科</td>\n      <td><a href=\"https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1IiIbQGJ_AooTj5s8aZYcvA\">300d</a> / PWD: 5555</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Wikipedia_zh 中文维基百科</td>\n      <td><a href=\"https://pan.baidu.com/s/1AmXYWVgkxrG4GokevPtNgA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1ZKePwxwsDdzNrfkc6WKdGQ\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1ZBVVD4mUSUuXOxlZ3V71ZA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/19wQrclyynOnco3JBvnI5pA\">300d</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>People's Daily News 人民日报</td>\n      <td><a href=\"https://pan.baidu.com/s/19sqMz-JAhhxh3o6ecvQxQw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1upPkA8KJnxTZBfjuNDtaeQ\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1BvKk2QjbtQMch7EISppW2A\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/19Vso_k79FZb5OZCWQPAnFQ\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Sogou News 搜狗新闻</td>\n      <td><a href=\"https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/13yVrXeGYkxdGW3P6juiQmA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1pUqyn7mnPcUmzxT64gGpSw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Financial News 金融新闻</td>\n      <td><a href=\"https://pan.baidu.com/s/1EhtsbDa3ekzZPODWNLHcXA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1FcPHv7S4vUgnL7WeWf4_PA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/13CAxY5ffRFuOcHZu8VmArw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1sqvrUtGBAZ7YWEsGz41DRQ\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Zhihu_QA 知乎问答 </td>\n      <td><a href=\"https://pan.baidu.com/s/1VGOs0RH7DXE5vRrtw6boQA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1OQ6fQLCgqT43WTwh5fh_lg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1_xogqF9kJT6tmQHSAYrYeg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1Fo27Lv_0nz8FXg-xbOz14Q\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Weibo 微博</td>\n      <td><a href=\"https://pan.baidu.com/s/1zbuUJEEEpZRNHxZ7Gezzmw\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/11PWBcvruXEDvKf2TiIXntg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/10bhJpaXMCUK02nHvRAttqA\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1FHl_bQkYucvVk-j2KG4dxA\">300d</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Literature 文学作品</td>\n      <td><a href=\"https://pan.baidu.com/s/1ciq8iXtcrHpu3ir_VhK0zg\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1Oa4CkPd8o2xd6LEAaa4gmg\">300d</a> / PWD: z5b4</td>\n      <td><a href=\"https://pan.baidu.com/s/1IG8IxNp2s7vVklz-vyZR9A\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1SEOKrJYS14HpqIaQT462kA\">300d</a> / PWD: yenb</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>\n      <td><a href=\"https://pan.baidu.com/s/1vPSeUsSiWYXEWAuokLR0qQ\">300d</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1sS9E7sclvS_UZcBgHN7xLQ\">300d</a></td>\n      <td>NAN</td>\n      <td>NAN</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Mixed-large 综合<br>Baidu Netdisk / Google Drive</td>\n      <td>\n        <a href=\"https://pan.baidu.com/s/1luy-GlTdqqvJ3j-A4FcIOw\">300d</a><br>\n        <a href=\"https://drive.google.com/open?id=1Zh9ZCEu8_eSQ-qkYVQufQDNKPC4mtEKR\">300d</a>\n      </td>\n      <td>\n        <a href=\"https://pan.baidu.com/s/1oJol-GaRMk4-8Ejpzxo6Gw\">300d</a><br>\n        <a href=\"https://drive.google.com/open?id=1WUU9LnoAjs--1E_WqcghLJ-Pp8bb38oS\">300d</a>\n      </td>\n      <td>\n        <a href=\"https://pan.baidu.com/s/1DjIGENlhRbsVyHW-caRePg\">300d</a><br>\n        <a href=\"https://drive.google.com/open?id=1aVAK0Z2E5DkdIH6-JHbiWSL5dbAcz6c3\">300d</a>\n      </td>\n      <td>\n        <a href=\"https://pan.baidu.com/s/14JP1gD7hcmsWdSpTvA3vKA\">300d</a><br>\n        <a href=\"https://drive.google.com/open?id=1kSAl4_AOg3_6ayU7KRM0Nk66uGdSZdnk\">300d</a>\n      </td>\n    </tr>\n</table>\n\n<table align=\"center\">\n    <tr align=\"center\">\n        <td colspan=\"5\"><b>Positive Pointwise Mutual Information (PPMI)</b></td>\n    </tr>\n    <tr align=\"center\">\n        <td rowspan=\"2\">语料</td>\n        <td colspan=\"4\">上下文特征</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>词</td>\n      <td>词 + N元组</td>\n      <td>词 + 字</td>\n      <td>词 + 字 + N元组</td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Baidu Encyclopedia 百度百科</td>\n      <td><a href=\"https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1KcfFdyO0-kE9S9CwzIisfw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1FXYM3CY161_4QMgiH8vasQ\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Wikipedia_zh 中文维基百科</td>\n      <td><a href=\"https://pan.baidu.com/s/1MGXRrc54nITPzQ7sfEUjMA\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1mtxZna8UJ7xBIxhBFntumQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1dDImpAx41V73Byl2julOGA\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1bsBQHXFpxMHGBexYof1_rw\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>People's Daily News 人民日报</td>\n      <td><a href=\"https://pan.baidu.com/s/1NLr1K7aapU2sYBvzbVny5g\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1LJl3Br0ccGDHP0XX2k3pVw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1GQQXGMn1AHh-BlifT0JD2g\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1Xm9Ec3O3rJ6ayrwVwonC7g\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Sogou News 搜狗新闻</td>\n      <td><a href=\"https://pan.baidu.com/s/1ECA51CZLp9_JB_me7YZ9-Q\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1FO39ZYy1mStERf_b53Y_yQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1lLBFBk8nn3spFAvKY9IJ6A\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1f-dLQZlZo_-B5ZKcPIc6rw\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Financial News 金融新闻</td>\n      <td><a href=\"https://pan.baidu.com/s/10wtgdmrTsTrjpSDvI0KzOw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1b6zjvhOIqTdACSSbriisVw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1w24vCfgqcoJvPxsB5VrRvw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1b9BPiDRhiEZ-6ybTcovrqQ\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Zhihu_QA 知乎问答 </td>\n      <td><a href=\"https://pan.baidu.com/s/1VaUP3YJC0IZKTbJ-1_8HZg\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1g39PKwT0kSmpneKOgXR5YQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1d8Bsuak0fyXxQOVUiNr-2w\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1D5fteBX0Vy4czEqpxXjlrQ\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Weibo 微博</td>\n      <td><a href=\"https://pan.baidu.com/s/15O2EbToOzjNSkzJwAOk_Ug\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/11Dqywn0hfMhysto7bZS1Dw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1wY-7mfV6nwDj_tru6W9h4Q\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1DMW-MgLApbQnWwDd-pT_qw\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Literature 文学作品</td>\n      <td><a href=\"https://pan.baidu.com/s/1HTHhlr8zvzhTwed7dO0sDg\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1jAuGJBxKqgapt__urGsBOQ\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/173AJfCoAV0ZA8Z31tKBdTA\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1dFCxke_Su3lLsuwZr7co3A\">Sparse</a></td>\n    </tr>\n    <tr  align=\"center\">\n      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>\n      <td><a href=\"https://pan.baidu.com/s/1NJ1Gc99oE0-GV0QxBqy-qw\">Sparse</a></td>\n      <td><a href=\"https://pan.baidu.com/s/1YGEgyXIbw0O4NtoM1ohjdA\">Sparse</a></td>\n      <td>NAN</td>\n      <td>NAN</td>\n    </tr>\n    </tr>\n    <tr  align=\"center\">\n      <td>Mixed-large 综合</td>\n      <td>Sparse</td>\n      <td>Sparse</td>\n      <td>Sparse</td>\n      <td>Sparse</td>\n    </tr>\n</table>\n\n<sup>\\*</sup>由于古汉语中绝大部份词均为单字词，因此只需字向量。\n\n### 不同的上下文共现信息\n\n我们提供了基于不同共现信息训练而成的词向量。下述提到的中心向量和上下文向量在类似的论文中也被称为输入和输出向量。\n\n这个部分中的向量不仅仅是词向量，还有其它的语言单位对应的向量。比如，在上下文是“词-字”的条件下，上下文向量会包含字向量。\n\n所有的向量均采用SGNS在百度百科语料上训练而成。\n\n<table align=\"center\">\n  <tr align=\"center\">\n    <td><b>特征</b></td>\n    <td><b>共现信息</b></td>\n    <td><b>中心向量</b></td>\n    <td><b>上下文向量</b></td>\n  </tr>\n  \n  <tr align=\"center\">\n  \t<td rowspan=\"1\">词</td>\n    <td>词 → 词</td>\n    <td><a href=\"https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/18T6DRVmS_cZu5u64EbbESQ\">300d</a></td>\n  </tr>\n\n  <tr align=\"center\">\n    <td rowspan=\"3\">N元组</td>\n    <td>词 → N元组 (1-2)</td>\n    <td><a href=\"https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/12asujjAaaqxNFYRNP-MThw\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>词 → N元组 (1-3)</td>\n    <td><a href=\"https://pan.baidu.com/s/1oUmbxsnSuXf2jU8Jxu7U8A\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1ylg6FfFHa0kXbiVz8bIL8g\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>N元组 (1-2) → N元组 (1-2)</td>\n    <td><a href=\"https://pan.baidu.com/s/1Za7DIGVhE6dMsTmxHb-izg\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1oKI4Cs9eo7bg5mqfY1hdmg\">300d</a></td>\n  </tr>\n  \n  <tr align=\"center\">\n    <td rowspan=\"3\">字</td>\n    <td>词 → 字 (1)</td>\n \t  <td><a href=\"https://pan.baidu.com/s/1c9yiosHKNIZwRlLzD_F1ig\">300d</a></td>\n    <td><a href=\"https://pan.baidu.com/s/1KGZ_x8r-lq-AuElLCSVzvQ\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>词 → 字 (1-2)</td>\n \t  <td><a href=\"https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw\">300d</a></td>\n    <td><a href=\"https://pan.baidu.com/s/1q0ItLzbn5Tfb3LhepRCeEA\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>词 → 字 (1-4)</td>\n    <td><a href=\"https://pan.baidu.com/s/1WNWAnba56Rqjmx-FAN_7_g\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1hJKTAz6PwS7wmz9wQgmYeg\">300d</a></td>\n  </tr>\n  \n  <tr align=\"center\">\n  \t<td rowspan=\"1\">偏旁部首</td>\n    <td>偏旁部首</td>\n    <td>300d</td>\n \t  <td>300d</td>\n  </tr>\n  \n  <tr align=\"center\">\n    <td rowspan=\"2\">位置</td>\n    <td>词 → 词 (左/右)</td>\n    <td><a href=\"https://pan.baidu.com/s/1JvjcrXFZPknT5H5Xw6KRVg\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1m6K9CnIIS8FrQZdDuF6hPQ\">300d</a></td>\n  </tr>\n  <tr align=\"center\">\n    <td>词 → 词 (距离)</td>\n    <td><a href=\"https://pan.baidu.com/s/1c29BDu4R1hyUX-sgvlHJnA\">300d</a></td>\n \t  <td><a href=\"https://pan.baidu.com/s/1sMZHIc-7eU6gRalHwtBHZw\">300d</a></td>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>全局信息</td>\n    <td>词 → 文章</td>\n    <td>300d</td>\n \t  <td>300d</td>\n  </tr>\n    \n  <tr align=\"center\">\n    <td rowspan=\"2\">语法特征</td>\n    <td>词 → 词性</td>\n    <td>300d</td>\n \t  <td>300d</td>\n  </tr>\n  <tr align=\"center\">\n    <td>词 → 依存关系</td>\n    <td>300d</td>\n \t  <td>300d</td>\n  </tr>\n</table>\n\n## 表示方式\n目前有两种词向量的表示方式：**稠密**和**稀疏**的表示方式。SGNS模型（word2vec中提出的一种模型）和PPMI模型分别是二者的代表。SGNS是通过浅层神经网络训练而成的一种低维实向量来表示词语。它通常也被称之为神经词嵌入方法。PPMI模型是一种基于正值逐点互信息并且以稀疏方式表示的特征汇总模型。\n\n## 上下文特征\n在词向量领域通常有三种主要的上下文特征：**词**、**N元组**、**字**。大部分词表示方式本质上都是在利用词和词的共现统计信息，换句话说就是把词来当作上下文特征，即上文**词特征**中的向量。受到语言模型的启发，我们将N元组信息也引入了上下文特征中。不只是词和词的共现信息，词和N元组的共现信息也被用在了上文**N元组特征**向量的训练中。对于中文来说，汉字承载了较强的含义，因此我们利用了词和词的共现信息以及词和字的共现信息来训练了一些向量。在上文**字特征**中的向量包括了长度1至4的字级别N元组。\n\n除了词、N元组和字以外。还有一些特征对词向量的性质有重要的影响。比如，把整篇文本作为特征引入训练中会使得词向量受到文章主题的影响；用依存分析的结果作为上下文特征来训练词向量会让词向量受到语法的影响。本项目共涉及17种不同的共现信息。\n\n## 语料\n我们花费了大量精力来收集了来自多个领域的语料。所有的文本数据均移除了html和xml标记，仅保留了纯文本。之后采用了[HanLP(v_1.5.3)](https://github.com/hankcs/HanLP)对文本进行了分词。此外，我们将繁体中文用[Open Chinese Convert (OpenCC)](https://github.com/BYVoid/OpenCC)转换为了简体中文。更详细的语料信息如下所示：\n\n<table align=\"center\">\n\t<tr align=\"center\">\n\t\t<td><b>语料</b></td>\n\t\t<td><b>大小</b></td>\n\t\t<td><b>词数量</b></td>\n\t\t<td><b>词汇量</b></td>\n\t\t<td><b>详情</b></td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Baidu Encyclopedia<br />百度百科</td>\n\t\t<td>4.1G</td>\n\t\t<td>745M</td>\n\t\t<td>5422K</td>\n\t\t<td>中文百科<br />https://baike.baidu.com/</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Wikipedia_zh<br />中文维基百科</td>\n\t\t<td>1.3G</td>\n\t\t<td>223M</td>\n\t\t<td>2129K</td>\n\t\t<td>中文维基百科<br />https://dumps.wikimedia.org/</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>People's Daily News<br />人民日报</td>\n\t\t<td>3.9G</td>\n\t\t<td>668M</td>\n\t\t<td>1664K</td>\n\t\t<td>人民日报新闻数据(1946-2017)<br />http://data.people.com.cn/</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Sogou News<br />搜狗新闻</td>\n\t\t<td>3.7G</td>\n\t\t<td>649M</td>\n\t\t<td>1226K</td>\n\t\t<td>Sogou labs的新闻数据<br />http://www.sogou.com/labs/</td>\n\t</tr>\n  <tr align=\"center\">\n    <td>Financial News<br />金融新闻</td>\n    <td>6.2G</td>\n    <td>1055M</td>\n    <td>2785K</td>\n    <td>从多个网站收集到的金融新闻</td>\n  </tr>\n\t<tr align=\"center\">\n\t\t<td>Zhihu_QA<br />知乎问答</td>\n\t\t<td>2.1G</td>\n\t\t<td>384M</td>\n\t\t<td>1117K</td>\n\t\t<td>中文问答数据<br />https://www.zhihu.com/</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Weibo<br />微博</td>\n\t\t<td>0.73G</td>\n\t\t<td>136M</td>\n\t\t<td>850K</td>\n\t\t<td>NLPIR Lab提供的微博数据<br />http://www.nlpir.org/wordpress/download/weibo.7z</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Literature<br />文学作品</td>\n\t\t<td>0.93G</td>\n\t\t<td>177M</td>\n\t\t<td>702K</td>\n\t\t<td>8599篇现代文学作品</td>\n\t</tr>\n\t<tr align=\"center\">\n\t\t<td>Mixed-large<br />综合</td>\n\t\t<td>22.6G</td>\n    <td>4037M</td>\n    <td>10653K</td>\n\t\t<td>上述所有数据的汇总</td>\n\t</tr>\n  <tr align=\"center\">\n    <td>Complete Library in Four Sections<br />四库全书</td>\n    <td>1.5G</td>\n    <td>714M</td>\n    <td>21.8K</td>\n    <td>目前最大的古代文献汇总</td>\n  </tr>\n</table>\n\n上述统计结果中，所有词都被计算在内，包括低频词。\n\n## 训练工具\n所有词向量均采用[ngram2vec](https://github.com/zhezhaoa/ngram2vec/)训练而成。Ngram2vec是[word2vec](https://github.com/svn2github/word2vec)和[fasttext](https://github.com/facebookresearch/fastText)的超集。它可以兼容各种上下文特征并且支持多种模型。\n\n## 中文词类比评测\n通常人们利用词类比任务来评测词向量的好坏。本项目中，包含有两个词类比任务。其一是CA-translated，即由英文词类比任务翻译得到的中文词类比任务集。虽然CA-translated被广泛应用于中文词向量相关的论文中，但是它仅仅只有语义相关任务没有语法相关的任务，并且其中词汇量仅有134。因此，我们提供了另一个中文词类比任务集CA8。CA8是特别为中文而设计的词类比任务集。它包括了17813个词类比问题，并且同时涵盖了语法和语义任务。对上述两个数据集更详细的介绍可以参见：[**testsets**](https://github.com/Embedding/Chinese-Word-Vectors/tree/master/testsets)文件夹。\n\n## 评测工具\n我们提供了配套的评测工具，详见[**evaluation**](https://github.com/Embedding/Chinese-Word-Vectors/tree/master/evaluation)文件夹。\n\n评测稠密的向量，可以运行：\n```\n$ python ana_eval_dense.py -v <vector.txt> -a CA8/morphological.txt\n$ python ana_eval_dense.py -v <vector.txt> -a CA8/semantic.txt\n```\n评测稀疏的向量，可以运行：\n```\n$ python ana_eval_sparse.py -v <vector.txt> -a CA8/morphological.txt\n$ python ana_eval_sparse.py -v <vector.txt> -a CA8/semantic.txt\n```\n"
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "testsets",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}