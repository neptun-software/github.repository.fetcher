{
  "metadata": {
    "timestamp": 1736561315475,
    "page": 325,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "xmu-xiaoma666/External-Attention-pytorch",
      "stars": 11668,
      "defaultBranch": "master",
      "files": [
        {
          "name": "FightingCVimg",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.044921875,
          "content": "MIT License\n\nCopyright (c) 2021 xmu-xiaoma666\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 63.41015625,
          "content": "\n<img src=\"./FightingCVimg/LOGO.gif\" height=\"200\" width=\"400\"/>\n\n简体中文 | [English](./README_EN.md)\n\n# FightingCV 代码库， 包含 [***Attention***](#attention-series),[***Backbone***](#backbone-series), [***MLP***](#mlp-series), [***Re-parameter***](#re-parameter-series), [**Convolution**](#convolution-series)\n\n![](https://img.shields.io/badge/fightingcv-v0.0.1-brightgreen)\n![](https://img.shields.io/badge/python->=v3.0-blue)\n![](https://img.shields.io/badge/pytorch->=v1.4-red)\n\n<!--\n-------\n*If this project is helpful to you, welcome to give a ***star***.* \n\n*Don't forget to ***follow*** me to learn about project updates.*\n\n-->\n\n\n\n\nHello，大家好，我是小马🚀🚀🚀\n\n***For 小白（Like Me）：***\n最近在读论文的时候会发现一个问题，有时候论文核心思想非常简单，核心代码可能也就十几行。但是打开作者release的源码时，却发现提出的模块嵌入到分类、检测、分割等任务框架中，导致代码比较冗余，对于特定任务框架不熟悉的我，**很难找到核心代码**，导致在论文和网络思想的理解上会有一定困难。\n\n***For 进阶者（Like You）：***\n如果把Conv、FC、RNN这些基本单元看做小的Lego积木，把Transformer、ResNet这些结构看成已经搭好的Lego城堡。那么本项目提供的模块就是一个个具有完整语义信息的Lego组件。**让科研工作者们避免反复造轮子**，只需思考如何利用这些“Lego组件”，搭建出更多绚烂多彩的作品。\n\n***For 大神（May Be Like You）：***\n能力有限，**不喜轻喷**！！！\n\n***For All：***\n本项目致力于实现一个既能**让深度学习小白也能搞懂**，又能**服务科研和工业社区**的代码库。\n\n\n<!--\n\n作为[**FightingCV公众号**](https://mp.weixin.qq.com/s/m9RiivbbDPdjABsTd6q8FA)和 **[FightingCV-Paper-Reading](https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading)** 的补充，本项目的宗旨是从代码角度，实现🚀**让世界上没有难读的论文**🚀。\n\n\n（同时也非常欢迎各位科研工作者将自己的工作的核心代码整理到本项目中，推动科研社区的发展，会在readme中注明代码的作者~）\n\n\n\n\n## 技术交流 <img title=\"\" src=\"https://user-images.githubusercontent.com/48054808/157800467-2a9946ad-30d1-49a9-b9db-ba33413d9c90.png\" alt=\"\" width=\"20\">\n\n欢迎大家关注公众号：**FightingCV**\n\n\n\n| FightingCV公众号 | 小助手微信 （备注【**公司/学校+方向+ID**】）|\n:-------------------------:|:-------------------------:\n<img src='./FightingCVimg/FightingCV.jpg' width='200px'>  |  <img src='./FightingCVimg/xiaozhushou.jpg' width='200px'> \n\n- 公众号**每天**都会进行**论文、算法和代码的干货分享**哦~\n\n- **交流群每天分享一些最新的论文和解析**，欢迎大家一起**学习交流**哈~~~\n\n- 强烈推荐大家关注[**知乎**](https://www.zhihu.com/people/jason-14-58-38/posts)账号和[**FightingCV公众号**](https://mp.weixin.qq.com/s/m9RiivbbDPdjABsTd6q8FA)，可以快速了解到最新优质的干货资源。\n\n\n-------\n\n\n-->\n\n## 🌟 Star History\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=xmu-xiaoma666/External-Attention-pytorch&type=Date)](https://star-history.com/#xmu-xiaoma666/External-Attention-pytorch&Date)\n\n## 使用\n\n### 安装\n\n 直接通过 pip 安装\n\n  ```shell\n  pip install fightingcv-attention\n  ```\n\n\n或克隆该仓库\n\n  ```shell\n  git clone https://github.com/xmu-xiaoma666/External-Attention-pytorch.git\n\n  cd External-Attention-pytorch\n  ```\n\n### 演示\n\n#### 使用 pip 方式\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# 使用 pip 方式\n\nfrom fightingcv_attention.attention.MobileViTv2Attention import *\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = MobileViTv2Attention(d_model=512)\n    output=sa(input)\n    print(output.shape)\n```\n\n - pip包 内置模块使用参考: [fightingcv-attention 说明文档](./README_pip.md)\n\n#### 使用 git 方式\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# 与 pip方式 区别在于 将 `fightingcv_attention` 替换 `model`\n\nfrom model.attention.MobileViTv2Attention import *\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = MobileViTv2Attention(d_model=512)\n    output=sa(input)\n    print(output.shape)\n```\n\n-------\n\n\n\n# 目录\n\n- [Attention Series](#attention-series)\n    - [1. External Attention Usage](#1-external-attention-usage)\n\n    - [2. Self Attention Usage](#2-self-attention-usage)\n\n    - [3. Simplified Self Attention Usage](#3-simplified-self-attention-usage)\n\n    - [4. Squeeze-and-Excitation Attention Usage](#4-squeeze-and-excitation-attention-usage)\n\n    - [5. SK Attention Usage](#5-sk-attention-usage)\n\n    - [6. CBAM Attention Usage](#6-cbam-attention-usage)\n\n    - [7. BAM Attention Usage](#7-bam-attention-usage)\n    \n    - [8. ECA Attention Usage](#8-eca-attention-usage)\n\n    - [9. DANet Attention Usage](#9-danet-attention-usage)\n\n    - [10. Pyramid Split Attention (PSA) Usage](#10-Pyramid-Split-Attention-Usage)\n\n    - [11. Efficient Multi-Head Self-Attention(EMSA) Usage](#11-Efficient-Multi-Head-Self-Attention-Usage)\n\n    - [12. Shuffle Attention Usage](#12-Shuffle-Attention-Usage)\n    \n    - [13. MUSE Attention Usage](#13-MUSE-Attention-Usage)\n  \n    - [14. SGE Attention Usage](#14-SGE-Attention-Usage)\n\n    - [15. A2 Attention Usage](#15-A2-Attention-Usage)\n\n    - [16. AFT Attention Usage](#16-AFT-Attention-Usage)\n\n    - [17. Outlook Attention Usage](#17-Outlook-Attention-Usage)\n\n    - [18. ViP Attention Usage](#18-ViP-Attention-Usage)\n\n    - [19. CoAtNet Attention Usage](#19-CoAtNet-Attention-Usage)\n\n    - [20. HaloNet Attention Usage](#20-HaloNet-Attention-Usage)\n\n    - [21. Polarized Self-Attention Usage](#21-Polarized-Self-Attention-Usage)\n\n    - [22. CoTAttention Usage](#22-CoTAttention-Usage)\n\n    - [23. Residual Attention Usage](#23-Residual-Attention-Usage)\n  \n    - [24. S2 Attention Usage](#24-S2-Attention-Usage)\n\n    - [25. GFNet Attention Usage](#25-GFNet-Attention-Usage)\n\n    - [26. Triplet Attention Usage](#26-TripletAttention-Usage)\n\n    - [27. Coordinate Attention Usage](#27-Coordinate-Attention-Usage)\n\n    - [28. MobileViT Attention Usage](#28-MobileViT-Attention-Usage)\n\n    - [29. ParNet Attention Usage](#29-ParNet-Attention-Usage)\n\n    - [30. UFO Attention Usage](#30-UFO-Attention-Usage)\n\n    - [31. ACmix Attention Usage](#31-Acmix-Attention-Usage)\n  \n    - [32. MobileViTv2 Attention Usage](#32-MobileViTv2-Attention-Usage)\n\n    - [33. DAT Attention Usage](#33-DAT-Attention-Usage)\n\n    - [34. CrossFormer Attention Usage](#34-CrossFormer-Attention-Usage)\n\n    - [35. MOATransformer Attention Usage](#35-MOATransformer-Attention-Usage)\n\n    - [36. CrissCrossAttention Attention Usage](#36-CrissCrossAttention-Attention-Usage)\n\n    - [37. Axial_attention Attention Usage](#37-Axial_attention-Attention-Usage)\n\n- [Backbone Series](#Backbone-series)\n\n    - [1. ResNet Usage](#1-ResNet-Usage)\n\n    - [2. ResNeXt Usage](#2-ResNeXt-Usage)\n\n    - [3. MobileViT Usage](#3-MobileViT-Usage)\n\n    - [4. ConvMixer Usage](#4-ConvMixer-Usage)\n\n    - [5. ShuffleTransformer Usage](#5-ShuffleTransformer-Usage)\n\n    - [6. ConTNet Usage](#6-ConTNet-Usage)\n\n    - [7. HATNet Usage](#7-HATNet-Usage)\n\n    - [8. CoaT Usage](#8-CoaT-Usage)\n\n    - [9. PVT Usage](#9-PVT-Usage)\n\n    - [10. CPVT Usage](#10-CPVT-Usage)\n\n    - [11. PIT Usage](#11-PIT-Usage)\n\n    - [12. CrossViT Usage](#12-CrossViT-Usage)\n\n    - [13. TnT Usage](#13-TnT-Usage)\n\n    - [14. DViT Usage](#14-DViT-Usage)\n\n    - [15. CeiT Usage](#15-CeiT-Usage)\n\n    - [16. ConViT Usage](#16-ConViT-Usage)\n\n    - [17. CaiT Usage](#17-CaiT-Usage)\n\n    - [18. PatchConvnet Usage](#18-PatchConvnet-Usage)\n\n    - [19. DeiT Usage](#19-DeiT-Usage)\n\n    - [20. LeViT Usage](#20-LeViT-Usage)\n\n    - [21. VOLO Usage](#21-VOLO-Usage)\n    \n    - [22. Container Usage](#22-Container-Usage)\n\n    - [23. CMT Usage](#23-CMT-Usage)\n\n    - [24. EfficientFormer Usage](#24-EfficientFormer-Usage)\n\n    - [25. ConvNeXtV2 Usage](#25-ConvNeXtV2-Usage)\n\n\n\n- [MLP Series](#mlp-series)\n\n    - [1. RepMLP Usage](#1-RepMLP-Usage)\n\n    - [2. MLP-Mixer Usage](#2-MLP-Mixer-Usage)\n\n    - [3. ResMLP Usage](#3-ResMLP-Usage)\n\n    - [4. gMLP Usage](#4-gMLP-Usage)\n\n    - [5. sMLP Usage](#5-sMLP-Usage)\n\n    - [6. vip-mlp Usage](#6-vip-mlp-Usage)\n\n- [Re-Parameter(ReP) Series](#Re-Parameter-series)\n\n    - [1. RepVGG Usage](#1-RepVGG-Usage)\n\n    - [2. ACNet Usage](#2-ACNet-Usage)\n\n    - [3. Diverse Branch Block(DDB) Usage](#3-Diverse-Branch-Block-Usage)\n\n- [Convolution Series](#Convolution-series)\n\n    - [1. Depthwise Separable Convolution Usage](#1-Depthwise-Separable-Convolution-Usage)\n\n    - [2. MBConv Usage](#2-MBConv-Usage)\n\n    - [3. Involution Usage](#3-Involution-Usage)\n\n    - [4. DynamicConv Usage](#4-DynamicConv-Usage)\n\n    - [5. CondConv Usage](#5-CondConv-Usage)\n\n***\n\n\n\n# Attention Series\n\n- Pytorch implementation of [\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks---arXiv 2021.05.05\"](https://arxiv.org/abs/2105.02358)\n\n- Pytorch implementation of [\"Attention Is All You Need---NIPS2017\"](https://arxiv.org/pdf/1706.03762.pdf)\n\n- Pytorch implementation of [\"Squeeze-and-Excitation Networks---CVPR2018\"](https://arxiv.org/abs/1709.01507)\n\n- Pytorch implementation of [\"Selective Kernel Networks---CVPR2019\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n- Pytorch implementation of [\"CBAM: Convolutional Block Attention Module---ECCV2018\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n\n- Pytorch implementation of [\"BAM: Bottleneck Attention Module---BMCV2018\"](https://arxiv.org/pdf/1807.06514.pdf)\n\n- Pytorch implementation of [\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020\"](https://arxiv.org/pdf/1910.03151.pdf)\n\n- Pytorch implementation of [\"Dual Attention Network for Scene Segmentation---CVPR2019\"](https://arxiv.org/pdf/1809.02983.pdf)\n\n- Pytorch implementation of [\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30\"](https://arxiv.org/pdf/2105.14447.pdf)\n\n- Pytorch implementation of [\"ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28\"](https://arxiv.org/abs/2105.13677)\n\n- Pytorch implementation of [\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021\"](https://arxiv.org/pdf/2102.00240.pdf)\n\n- Pytorch implementation of [\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17\"](https://arxiv.org/abs/1911.09483)\n\n- Pytorch implementation of [\"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23\"](https://arxiv.org/pdf/1905.09646.pdf)\n\n- Pytorch implementation of [\"A2-Nets: Double Attention Networks---NIPS2018\"](https://arxiv.org/pdf/1810.11579.pdf)\n\n\n- Pytorch implementation of [\"An Attention Free Transformer---ICLR2021 (Apple New Work)\"](https://arxiv.org/pdf/2105.14103v1.pdf)\n\n\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24\"](https://arxiv.org/abs/2106.13112) \n  [【论文解析】](https://zhuanlan.zhihu.com/p/385561050)\n\n\n- Pytorch implementation of [Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23](https://arxiv.org/abs/2106.12368) \n  [【论文解析】](https://mp.weixin.qq.com/s/5gonUQgBho_m2O54jyXF_Q)\n\n\n- Pytorch implementation of [CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09](https://arxiv.org/abs/2106.04803) \n  [【论文解析】](https://zhuanlan.zhihu.com/p/385578588)\n\n\n- Pytorch implementation of [Scaling Local Self-Attention for Parameter Efficient Visual Backbones---CVPR2021 Oral](https://arxiv.org/pdf/2103.12731.pdf)  [【论文解析】](https://zhuanlan.zhihu.com/p/388598744)\n\n\n\n- Pytorch implementation of [Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02](https://arxiv.org/abs/2107.00782)  [【论文解析】](https://zhuanlan.zhihu.com/p/389770482) \n\n\n- Pytorch implementation of [Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292)  [【论文解析】](https://zhuanlan.zhihu.com/p/394795481) \n\n\n- Pytorch implementation of [Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \n\n\n- Pytorch implementation of [S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) [【论文解析】](https://zhuanlan.zhihu.com/p/397003638) \n\n- Pytorch implementation of [Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \n\n- Pytorch implementation of [Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021](https://arxiv.org/abs/2010.03045) \n\n- Pytorch implementation of [Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2110.02178)\n\n- Pytorch implementation of [Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\n\n- Pytorch implementation of [UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2109.14382)\n\n- Pytorch implementation of [Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06](https://arxiv.org/abs/2206.02680)\n\n- Pytorch implementation of [On the Integration of Self-Attention and Convolution---ArXiv 2022.03.14](https://arxiv.org/pdf/2111.14556.pdf)\n\n- Pytorch implementation of [CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION---ICLR 2022](https://arxiv.org/pdf/2108.00154.pdf)\n\n- Pytorch implementation of [Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903)\n\n- Pytorch implementation of [CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n\n- Pytorch implementation of [Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)\n***\n\n\n### 1. External Attention Usage\n#### 1.1. Paper\n[\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\"](https://arxiv.org/abs/2105.02358)\n\n#### 1.2. Overview\n![](./model/img/External_Attention.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.attention.ExternalAttention import ExternalAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nea = ExternalAttention(d_model=512,S=8)\noutput=ea(input)\nprint(output.shape)\n```\n\n***\n\n\n### 2. Self Attention Usage\n#### 2.1. Paper\n[\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf)\n\n#### 1.2. Overview\n![](./model/img/SA.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.attention.SelfAttention import ScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nsa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n```\n\n***\n\n### 3. Simplified Self Attention Usage\n#### 3.1. Paper\n[None]()\n\n#### 3.2. Overview\n![](./model/img/SSA.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\noutput=ssa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n### 4. Squeeze-and-Excitation Attention Usage\n#### 4.1. Paper\n[\"Squeeze-and-Excitation Networks\"](https://arxiv.org/abs/1709.01507)\n\n#### 4.2. Overview\n![](./model/img/SE.png)\n\n#### 4.3. Usage Code\n```python\nfrom model.attention.SEAttention import SEAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SEAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n\n***\n\n### 5. SK Attention Usage\n#### 5.1. Paper\n[\"Selective Kernel Networks\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n#### 5.2. Overview\n![](./model/img/SK.png)\n\n#### 5.3. Usage Code\n```python\nfrom model.attention.SKAttention import SKAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SKAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n***\n\n### 6. CBAM Attention Usage\n#### 6.1. Paper\n[\"CBAM: Convolutional Block Attention Module\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n\n#### 6.2. Overview\n![](./model/img/CBAM1.png)\n\n![](./model/img/CBAM2.png)\n\n#### 6.3. Usage Code\n```python\nfrom model.attention.CBAM import CBAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nkernel_size=input.shape[2]\ncbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)\noutput=cbam(input)\nprint(output.shape)\n\n```\n\n***\n\n### 7. BAM Attention Usage\n#### 7.1. Paper\n[\"BAM: Bottleneck Attention Module\"](https://arxiv.org/pdf/1807.06514.pdf)\n\n#### 7.2. Overview\n![](./model/img/BAM.png)\n\n#### 7.3. Usage Code\n```python\nfrom model.attention.BAM import BAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nbam = BAMBlock(channel=512,reduction=16,dia_val=2)\noutput=bam(input)\nprint(output.shape)\n\n```\n\n***\n\n### 8. ECA Attention Usage\n#### 8.1. Paper\n[\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\"](https://arxiv.org/pdf/1910.03151.pdf)\n\n#### 8.2. Overview\n![](./model/img/ECA.png)\n\n#### 8.3. Usage Code\n```python\nfrom model.attention.ECAAttention import ECAAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\neca = ECAAttention(kernel_size=3)\noutput=eca(input)\nprint(output.shape)\n\n```\n\n***\n\n### 9. DANet Attention Usage\n#### 9.1. Paper\n[\"Dual Attention Network for Scene Segmentation\"](https://arxiv.org/pdf/1809.02983.pdf)\n\n#### 9.2. Overview\n![](./model/img/danet.png)\n\n#### 9.3. Usage Code\n```python\nfrom model.attention.DANet import DAModule\nimport torch\n\ninput=torch.randn(50,512,7,7)\ndanet=DAModule(d_model=512,kernel_size=3,H=7,W=7)\nprint(danet(input).shape)\n\n```\n\n***\n\n### 10. Pyramid Split Attention Usage\n\n#### 10.1. Paper\n[\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network\"](https://arxiv.org/pdf/2105.14447.pdf)\n\n#### 10.2. Overview\n![](./model/img/psa.png)\n\n#### 10.3. Usage Code\n```python\nfrom model.attention.PSA import PSA\nimport torch\n\ninput=torch.randn(50,512,7,7)\npsa = PSA(channel=512,reduction=8)\noutput=psa(input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 11. Efficient Multi-Head Self-Attention Usage\n\n#### 11.1. Paper\n[\"ResT: An Efficient Transformer for Visual Recognition\"](https://arxiv.org/abs/2105.13677)\n\n#### 11.2. Overview\n![](./model/img/EMSA.png)\n\n#### 11.3. Usage Code\n```python\n\nfrom model.attention.EMSA import EMSA\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,64,512)\nemsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)\noutput=emsa(input,input,input)\nprint(output.shape)\n    \n```\n\n***\n\n\n### 12. Shuffle Attention Usage\n\n#### 12.1. Paper\n[\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS\"](https://arxiv.org/pdf/2102.00240.pdf)\n\n#### 12.2. Overview\n![](./model/img/ShuffleAttention.jpg)\n\n#### 12.3. Usage Code\n```python\n\nfrom model.attention.ShuffleAttention import ShuffleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,512,7,7)\nse = ShuffleAttention(channel=512,G=8)\noutput=se(input)\nprint(output.shape)\n\n    \n```\n\n\n***\n\n\n### 13. MUSE Attention Usage\n\n#### 13.1. Paper\n[\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\"](https://arxiv.org/abs/1911.09483)\n\n#### 13.2. Overview\n![](./model/img/MUSE.png)\n\n#### 13.3. Usage Code\n```python\nfrom model.attention.MUSEAttention import MUSEAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,49,512)\nsa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 14. SGE Attention Usage\n\n#### 14.1. Paper\n[Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks](https://arxiv.org/pdf/1905.09646.pdf)\n\n#### 14.2. Overview\n![](./model/img/SGE.png)\n\n#### 14.3. Usage Code\n```python\nfrom model.attention.SGE import SpatialGroupEnhance\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nsge = SpatialGroupEnhance(groups=8)\noutput=sge(input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 15. A2 Attention Usage\n\n#### 15.1. Paper\n[A2-Nets: Double Attention Networks](https://arxiv.org/pdf/1810.11579.pdf)\n\n#### 15.2. Overview\n![](./model/img/A2.png)\n\n#### 15.3. Usage Code\n```python\nfrom model.attention.A2Atttention import DoubleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\na2 = DoubleAttention(512,128,128,True)\noutput=a2(input)\nprint(output.shape)\n\n```\n\n\n\n### 16. AFT Attention Usage\n\n#### 16.1. Paper\n[An Attention Free Transformer](https://arxiv.org/pdf/2105.14103v1.pdf)\n\n#### 16.2. Overview\n![](./model/img/AFT.jpg)\n\n#### 16.3. Usage Code\n```python\nfrom model.attention.AFT import AFT_FULL\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,49,512)\naft_full = AFT_FULL(d_model=512, n=49)\noutput=aft_full(input)\nprint(output.shape)\n\n```\n\n\n\n\n\n\n### 17. Outlook Attention Usage\n\n#### 17.1. Paper\n\n\n[VOLO: Vision Outlooker for Visual Recognition\"](https://arxiv.org/abs/2106.13112)\n\n\n#### 17.2. Overview\n![](./model/img/OutlookAttention.png)\n\n#### 17.3. Usage Code\n```python\nfrom model.attention.OutlookAttention import OutlookAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,28,28,512)\noutlook = OutlookAttention(dim=512)\noutput=outlook(input)\nprint(output.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n### 18. ViP Attention Usage\n\n#### 18.1. Paper\n\n\n[Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\n\n\n#### 18.2. Overview\n![](./model/img/ViP.png)\n\n#### 18.3. Usage Code\n```python\n\nfrom model.attention.ViP import WeightedPermuteMLP\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(64,8,8,512)\nseg_dim=8\nvip=WeightedPermuteMLP(512,seg_dim)\nout=vip(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n### 19. CoAtNet Attention Usage\n\n#### 19.1. Paper\n\n\n[CoAtNet: Marrying Convolution and Attention for All Data Sizes\"](https://arxiv.org/abs/2106.04803) \n\n\n#### 19.2. Overview\nNone\n\n\n#### 19.3. Usage Code\n```python\n\nfrom model.attention.CoAtNet import CoAtNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=CoAtNet(in_ch=3,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n### 20. HaloNet Attention Usage\n\n#### 20.1. Paper\n\n\n[Scaling Local Self-Attention for Parameter Efficient Visual Backbones\"](https://arxiv.org/pdf/2103.12731.pdf) \n\n\n#### 20.2. Overview\n\n![](./model/img/HaloNet.png)\n\n#### 20.3. Usage Code\n```python\n\nfrom model.attention.HaloAttention import HaloAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,8,8)\nhalo = HaloAttention(dim=512,\n    block_size=2,\n    halo_size=1,)\noutput=halo(input)\nprint(output.shape)\n\n```\n\n\n***\n\n### 21. Polarized Self-Attention Usage\n\n#### 21.1. Paper\n\n[Polarized Self-Attention: Towards High-quality Pixel-wise Regression\"](https://arxiv.org/abs/2107.00782)  \n\n\n#### 21.2. Overview\n\n![](./model/img/PoSA.png)\n\n#### 21.3. Usage Code\n```python\n\nfrom model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,7,7)\npsa = SequentialPolarizedSelfAttention(channel=512)\noutput=psa(input)\nprint(output.shape)\n\n\n```\n\n\n***\n\n\n### 22. CoTAttention Usage\n\n#### 22.1. Paper\n\n[Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292) \n\n\n#### 22.2. Overview\n\n![](./model/img/CoT.png)\n\n#### 22.3. Usage Code\n```python\n\nfrom model.attention.CoTAttention import CoTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ncot = CoTAttention(dim=512,kernel_size=3)\noutput=cot(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n### 23. Residual Attention Usage\n\n#### 23.1. Paper\n\n[Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \n\n\n#### 23.2. Overview\n\n![](./model/img/ResAtt.png)\n\n#### 23.3. Usage Code\n```python\n\nfrom model.attention.ResidualAttention import ResidualAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nresatt = ResidualAttention(channel=512,num_class=1000,la=0.2)\noutput=resatt(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n\n### 24. S2 Attention Usage\n\n#### 24.1. Paper\n\n[S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) \n\n\n#### 24.2. Overview\n\n![](./model/img/S2Attention.png)\n\n#### 24.3. Usage Code\n```python\nfrom model.attention.S2Attention import S2Attention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ns2att = S2Attention(channels=512)\noutput=s2att(input)\nprint(output.shape)\n\n```\n\n***\n\n\n\n### 25. GFNet Attention Usage\n\n#### 25.1. Paper\n\n[Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \n\n\n#### 25.2. Overview\n\n![](./model/img/GFNet.jpg)\n\n#### 25.3. Usage Code - Implemented by [Wenliang Zhao (Author)](https://scholar.google.com/citations?user=lyPWvuEAAAAJ&hl=en)\n\n```python\nfrom model.attention.gfnet import GFNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nx = torch.randn(1, 3, 224, 224)\ngfnet = GFNet(embed_dim=384, img_size=224, patch_size=16, num_classes=1000)\nout = gfnet(x)\nprint(out.shape)\n\n```\n\n***\n\n\n### 26. TripletAttention Usage\n\n#### 26.1. Paper\n\n[Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021](https://arxiv.org/abs/2010.03045) \n\n#### 26.2. Overview\n\n![](./model/img/triplet.png)\n\n#### 26.3. Usage Code - Implemented by [digantamisra98](https://github.com/digantamisra98)\n\n```python\nfrom model.attention.TripletAttention import TripletAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\ninput=torch.randn(50,512,7,7)\ntriplet = TripletAttention()\noutput=triplet(input)\nprint(output.shape)\n```\n\n\n***\n\n\n### 27. Coordinate Attention Usage\n\n#### 27.1. Paper\n\n[Coordinate Attention for Efficient Mobile Network Design---CVPR 2021](https://arxiv.org/abs/2103.02907)\n\n\n#### 27.2. Overview\n\n![](./model/img/CoordAttention.png)\n\n#### 27.3. Usage Code - Implemented by [Andrew-Qibin](https://github.com/Andrew-Qibin)\n\n```python\nfrom model.attention.CoordAttention import CoordAtt\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninp=torch.rand([2, 96, 56, 56])\ninp_dim, oup_dim = 96, 96\nreduction=32\n\ncoord_attention = CoordAtt(inp_dim, oup_dim, reduction=reduction)\noutput=coord_attention(inp)\nprint(output.shape)\n```\n\n***\n\n\n### 28. MobileViT Attention Usage\n\n#### 28.1. Paper\n\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2103.02907)\n\n\n#### 28.2. Overview\n\n![](./model/img/MobileViTAttention.png)\n\n#### 28.3. Usage Code\n\n```python\nfrom model.attention.MobileViTAttention import MobileViTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    m=MobileViTAttention()\n    input=torch.randn(1,3,49,49)\n    output=m(input)\n    print(output.shape)  #output:(1,3,49,49)\n    \n```\n\n***\n\n\n### 29. ParNet Attention Usage\n\n#### 29.1. Paper\n\n[Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\n\n\n#### 29.2. Overview\n\n![](./model/img/ParNet.png)\n\n#### 29.3. Usage Code\n\n```python\nfrom model.attention.ParNetAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,512,7,7)\n    pna = ParNetAttention(channel=512)\n    output=pna(input)\n    print(output.shape) #50,512,7,7\n    \n```\n\n***\n\n\n### 30. UFO Attention Usage\n\n#### 30.1. Paper\n\n[UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2110.07641)\n\n\n#### 30.2. Overview\n\n![](./model/img/UFO.png)\n\n#### 30.3. Usage Code\n\n```python\nfrom model.attention.UFOAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)\n    output=ufo(input,input,input)\n    print(output.shape) #[50, 49, 512]\n    \n```\n\n-\n\n### 31. ACmix Attention Usage\n\n#### 31.1. Paper\n\n[On the Integration of Self-Attention and Convolution](https://arxiv.org/pdf/2111.14556.pdf)\n\n#### 31.2. Usage Code\n\n```python\nfrom model.attention.ACmix import ACmix\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,256,7,7)\n    acmix = ACmix(in_planes=256, out_planes=256)\n    output=acmix(input)\n    print(output.shape)\n    \n```\n\n### 32. MobileViTv2 Attention Usage\n\n#### 32.1. Paper\n\n[Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06](https://arxiv.org/abs/2206.02680)\n\n\n#### 32.2. Overview\n\n![](./model/img/MobileViTv2.png)\n\n#### 32.3. Usage Code\n\n```python\nfrom model.attention.MobileViTv2Attention import MobileViTv2Attention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = MobileViTv2Attention(d_model=512)\n    output=sa(input)\n    print(output.shape)\n    \n```\n\n### 33. DAT Attention Usage\n\n#### 33.1. Paper\n\n[Vision Transformer with Deformable Attention---CVPR2022](https://arxiv.org/abs/2201.00520)\n\n#### 33.2. Usage Code\n\n```python\nfrom model.attention.DAT import DAT\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DAT(\n        img_size=224,\n        patch_size=4,\n        num_classes=1000,\n        expansion=4,\n        dim_stem=96,\n        dims=[96, 192, 384, 768],\n        depths=[2, 2, 6, 2],\n        stage_spec=[['L', 'S'], ['L', 'S'], ['L', 'D', 'L', 'D', 'L', 'D'], ['L', 'D']],\n        heads=[3, 6, 12, 24],\n        window_sizes=[7, 7, 7, 7] ,\n        groups=[-1, -1, 3, 6],\n        use_pes=[False, False, True, True],\n        dwc_pes=[False, False, False, False],\n        strides=[-1, -1, 1, 1],\n        sr_ratios=[-1, -1, -1, -1],\n        offset_range_factor=[-1, -1, 2, 2],\n        no_offs=[False, False, False, False],\n        fixed_pes=[False, False, False, False],\n        use_dwc_mlps=[False, False, False, False],\n        use_conv_patches=False,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.2,\n    )\n    output=model(input)\n    print(output[0].shape)\n    \n```\n\n### 34. CrossFormer Attention Usage\n\n#### 34.1. Paper\n\n[CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION---ICLR 2022](https://arxiv.org/pdf/2108.00154.pdf)\n\n#### 34.2. Usage Code\n\n```python\nfrom model.attention.Crossformer import CrossFormer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CrossFormer(img_size=224,\n        patch_size=[4, 8, 16, 32],\n        in_chans= 3,\n        num_classes=1000,\n        embed_dim=48,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        group_size=[7, 7, 7, 7],\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        ape=False,\n        patch_norm=True,\n        use_checkpoint=False,\n        merge_size=[[2, 4], [2,4], [2, 4]]\n    )\n    output=model(input)\n    print(output.shape)\n    \n```\n\n### 35. MOATransformer Attention Usage\n\n#### 35.1. Paper\n\n[Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903)\n\n#### 35.2. Usage Code\n\n```python\nfrom model.attention.MOATransformer import MOATransformer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = MOATransformer(\n        img_size=224,\n        patch_size=4,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=96,\n        depths=[2, 2, 6],\n        num_heads=[3, 6, 12],\n        window_size=14,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        ape=False,\n        patch_norm=True,\n        use_checkpoint=False\n    )\n    output=model(input)\n    print(output.shape)\n    \n```\n\n### 36. CrissCrossAttention Attention Usage\n\n#### 36.1. Paper\n\n[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n\n#### 36.2. Usage Code\n\n```python\nfrom model.attention.CrissCrossAttention import CrissCrossAttention\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(3, 64, 7, 7)\n    model = CrissCrossAttention(64)\n    outputs = model(input)\n    print(outputs.shape)\n    \n```\n\n### 37. Axial_attention Attention Usage\n\n#### 37.1. Paper\n\n[Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)\n\n#### 37.2. Usage Code\n\n```python\nfrom model.attention.Axial_attention import AxialImageTransformer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(3, 128, 7, 7)\n    model = AxialImageTransformer(\n        dim = 128,\n        depth = 12,\n        reversible = True\n    )\n    outputs = model(input)\n    print(outputs.shape)\n    \n```\n\n***\n\n\n# Backbone Series\n\n- Pytorch implementation of [\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n- Pytorch implementation of [\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\n\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\n\n- Pytorch implementation of [Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer---ArXiv 2021.06.07](https://arxiv.org/abs/2106.03650)\n\n- Pytorch implementation of [ConTNet: Why not use convolution and transformer at the same time?---ArXiv 2021.04.27](https://arxiv.org/abs/2104.13497)\n\n- Pytorch implementation of [Vision Transformers with Hierarchical Attention---ArXiv 2022.06.15](https://arxiv.org/abs/2106.03180)\n\n- Pytorch implementation of [Co-Scale Conv-Attentional Image Transformers---ArXiv 2021.08.26](https://arxiv.org/abs/2104.06399)\n\n- Pytorch implementation of [Conditional Positional Encodings for Vision Transformers](https://arxiv.org/abs/2102.10882)\n\n- Pytorch implementation of [Rethinking Spatial Dimensions of Vision Transformers---ICCV 2021](https://arxiv.org/abs/2103.16302)\n\n- Pytorch implementation of [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification---ICCV 2021](https://arxiv.org/abs/2103.14899)\n\n- Pytorch implementation of [Transformer in Transformer---NeurIPS 2021](https://arxiv.org/abs/2103.00112)\n\n- Pytorch implementation of [DeepViT: Towards Deeper Vision Transformer](https://arxiv.org/abs/2103.11886)\n\n- Pytorch implementation of [Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)\n***\n\n- Pytorch implementation of [ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)\n\n- Pytorch implementation of [Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\n\n- Pytorch implementation of [Going deeper with Image Transformers---ICCV 2021 (Oral)](https://arxiv.org/abs/2103.17239)\n\n- Pytorch implementation of [Training data-efficient image transformers & distillation through attention---ICML 2021](https://arxiv.org/abs/2012.12877)\n\n- Pytorch implementation of [LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\n\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/abs/2106.13112)\n\n- Pytorch implementation of [Container: Context Aggregation Network---NeuIPS 2021](https://arxiv.org/abs/2106.01401)\n\n- Pytorch implementation of [CMT: Convolutional Neural Networks Meet Vision Transformers---CVPR 2022](https://arxiv.org/abs/2107.06263)\n\n- Pytorch implementation of [Vision Transformer with Deformable Attention---CVPR 2022](https://arxiv.org/abs/2201.00520)\n\n- Pytorch implementation of [EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)\n\n- Pytorch implementation of [ConvNeXtV2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808)\n\n\n### 1. ResNet Usage\n#### 1.1. Paper\n[\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n#### 1.2. Overview\n![](./model/img/resnet.png)\n![](./model/img/resnet2.jpg)\n\n#### 1.3. Usage Code\n```python\n\nfrom model.backbone.resnet import ResNet50,ResNet101,ResNet152\nimport torch\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnet50=ResNet50(1000)\n    # resnet101=ResNet101(1000)\n    # resnet152=ResNet152(1000)\n    out=resnet50(input)\n    print(out.shape)\n\n```\n\n\n### 2. ResNeXt Usage\n#### 2.1. Paper\n\n[\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\n\n#### 2.2. Overview\n![](./model/img/resnext.png)\n\n#### 2.3. Usage Code\n```python\n\nfrom model.backbone.resnext import ResNeXt50,ResNeXt101,ResNeXt152\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnext50=ResNeXt50(1000)\n    # resnext101=ResNeXt101(1000)\n    # resnext152=ResNeXt152(1000)\n    out=resnext50(input)\n    print(out.shape)\n\n\n```\n\n\n\n### 3. MobileViT Usage\n#### 3.1. Paper\n\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\n\n#### 3.2. Overview\n![](./model/img/mobileViT.jpg)\n\n#### 3.3. Usage Code\n```python\n\nfrom model.backbone.MobileViT import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n\n    ### mobilevit_xxs\n    mvit_xxs=mobilevit_xxs()\n    out=mvit_xxs(input)\n    print(out.shape)\n\n    ### mobilevit_xs\n    mvit_xs=mobilevit_xs()\n    out=mvit_xs(input)\n    print(out.shape)\n\n\n    ### mobilevit_s\n    mvit_s=mobilevit_s()\n    out=mvit_s(input)\n    print(out.shape)\n\n```\n\n\n\n\n\n### 4. ConvMixer Usage\n#### 4.1. Paper\n[Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\n#### 4.2. Overview\n![](./model/img/ConvMixer.png)\n\n#### 4.3. Usage Code\n```python\n\nfrom model.backbone.ConvMixer import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    x=torch.randn(1,3,224,224)\n    convmixer=ConvMixer(dim=512,depth=12)\n    out=convmixer(x)\n    print(out.shape)  #[1, 1000]\n\n\n```\n\n### 5. ShuffleTransformer Usage\n#### 5.1. Paper\n[Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer](https://arxiv.org/pdf/2106.03650.pdf)\n\n#### 5.2. Usage Code\n```python\n\nfrom model.backbone.ShuffleTransformer import ShuffleTransformer\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    sft = ShuffleTransformer()\n    output=sft(input)\n    print(output.shape)\n\n\n```\n\n### 6. ConTNet Usage\n#### 6.1. Paper\n[ConTNet: Why not use convolution and transformer at the same time?](https://arxiv.org/abs/2104.13497)\n\n#### 6.2. Usage Code\n```python\n\nfrom model.backbone.ConTNet import ConTNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == \"__main__\":\n    model = build_model(use_avgdown=True, relative=True, qkv_bias=True, pre_norm=True)\n    input = torch.randn(1, 3, 224, 224)\n    out = model(input)\n    print(out.shape)\n\n\n```\n\n### 7 HATNet Usage\n#### 7.1. Paper\n[Vision Transformers with Hierarchical Attention](https://arxiv.org/abs/2106.03180)\n\n#### 7.2. Usage Code\n```python\n\nfrom model.backbone.HATNet import HATNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    hat = HATNet(dims=[48, 96, 240, 384], head_dim=48, expansions=[8, 8, 4, 4],\n        grid_sizes=[8, 7, 7, 1], ds_ratios=[8, 4, 2, 1], depths=[2, 2, 6, 3])\n    output=hat(input)\n    print(output.shape)\n\n\n```\n\n### 8 CoaT Usage\n#### 8.1. Paper\n[Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/abs/2104.06399)\n\n#### 8.2. Usage Code\n```python\n\nfrom model.backbone.CoaT import CoaT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CoaT(patch_size=4, embed_dims=[152, 152, 152, 152], serial_depths=[2, 2, 2, 2], parallel_depth=6, num_heads=8, mlp_ratios=[4, 4, 4, 4])\n    output=model(input)\n    print(output.shape) # torch.Size([1, 1000])\n\n```\n\n### 9 PVT Usage\n#### 9.1. Paper\n[PVT v2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/pdf/2106.13797.pdf)\n\n#### 9.2. Usage Code\n```python\n\nfrom model.backbone.PVT import PyramidVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PyramidVisionTransformer(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1])\n    output=model(input)\n    print(output.shape)\n\n```\n\n\n### 10 CPVT Usage\n#### 10.1. Paper\n[Conditional Positional Encodings for Vision Transformers](https://arxiv.org/abs/2102.10882)\n\n#### 10.2. Usage Code\n```python\n\nfrom model.backbone.CPVT import CPVTV2\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CPVTV2(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1])\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 11 PIT Usage\n#### 11.1. Paper\n[Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)\n\n#### 11.2. Usage Code\n```python\n\nfrom model.backbone.PIT import PoolingTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=14,\n        stride=7,\n        base_dims=[64, 64, 64],\n        depth=[3, 6, 4],\n        heads=[4, 8, 16],\n        mlp_ratio=4\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 12 CrossViT Usage\n#### 12.1. Paper\n[CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899)\n\n#### 12.2. Usage Code\n```python\n\nfrom model.backbone.CrossViT import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == \"__main__\":\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        img_size=[240, 224],\n        patch_size=[12, 16], \n        embed_dim=[192, 384], \n        depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],\n        num_heads=[6, 6], \n        mlp_ratio=[4, 4, 1], \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 13 TnT Usage\n#### 13.1. Paper\n[Transformer in Transformer](https://arxiv.org/abs/2103.00112)\n\n#### 13.2. Usage Code\n```python\n\nfrom model.backbone.TnT import TNT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = TNT(\n        img_size=224, \n        patch_size=16, \n        outer_dim=384, \n        inner_dim=24, \n        depth=12,\n        outer_num_heads=6, \n        inner_num_heads=4, \n        qkv_bias=False,\n        inner_stride=4)\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 14 DViT Usage\n#### 14.1. Paper\n[DeepViT: Towards Deeper Vision Transformer](https://arxiv.org/abs/2103.11886)\n\n#### 14.2. Usage Code\n```python\n\nfrom model.backbone.DViT import DeepVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DeepVisionTransformer(\n        patch_size=16, embed_dim=384, \n        depth=[False] * 16, \n        apply_transform=[False] * 0 + [True] * 32, \n        num_heads=12, \n        mlp_ratio=3, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 15 CeiT Usage\n#### 15.1. Paper\n[Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)\n\n#### 15.2. Usage Code\n```python\n\nfrom model.backbone.CeiT import CeIT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CeIT(\n        hybrid_backbone=Image2Tokens(),\n        patch_size=4, \n        embed_dim=192, \n        depth=12, \n        num_heads=3, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 16 ConViT Usage\n#### 16.1. Paper\n[ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)\n\n#### 16.2. Usage Code\n```python\n\nfrom model.backbone.ConViT import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        num_heads=16,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 17 CaiT Usage\n#### 17.1. Paper\n[Going deeper with Image Transformers](https://arxiv.org/abs/2103.17239)\n\n#### 17.2. Usage Code\n```python\n\nfrom model.backbone.CaiT import CaiT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CaiT(\n        img_size= 224,\n        patch_size=16, \n        embed_dim=192, \n        depth=24, \n        num_heads=4, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 18 PatchConvnet Usage\n#### 18.1. Paper\n[Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\n\n#### 18.2. Usage Code\n```python\n\nfrom model.backbone.PatchConvnet import PatchConvnet\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=384,\n        depth=60,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        depth_token_only=1,\n        mlp_ratio_clstk=3.0,\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 19 DeiT Usage\n#### 19.1. Paper\n[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n\n#### 19.2. Usage Code\n```python\n\nfrom model.backbone.DeiT import DistilledVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DistilledVisionTransformer(\n        patch_size=16, \n        embed_dim=384, \n        depth=12, \n        num_heads=6, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 20 LeViT Usage\n#### 20.1. Paper\n[LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\n\n#### 20.2. Usage Code\n```python\n\nfrom model.backbone.LeViT import *\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    for name in specification:\n        input=torch.randn(1,3,224,224)\n        model = globals()[name](fuse=True, pretrained=False)\n        model.eval()\n        output = model(input)\n        print(output.shape)\n\n```\n\n### 21 VOLO Usage\n#### 21.1. Paper\n[VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/abs/2106.13112)\n\n#### 21.2. Usage Code\n```python\n\nfrom model.backbone.VOLO import VOLO\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VOLO([4, 4, 8, 2],\n                 embed_dims=[192, 384, 384, 384],\n                 num_heads=[6, 12, 12, 12],\n                 mlp_ratios=[3, 3, 3, 3],\n                 downsamples=[True, False, False, False],\n                 outlook_attention=[True, False, False, False ],\n                 post_layers=['ca', 'ca'],\n                 )\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 22 Container Usage\n#### 22.1. Paper\n[Container: Context Aggregation Network](https://arxiv.org/abs/2106.01401)\n\n#### 22.2. Usage Code\n```python\n\nfrom model.backbone.Container import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        img_size=[224, 56, 28, 14], \n        patch_size=[4, 2, 2, 2], \n        embed_dim=[64, 128, 320, 512], \n        depth=[3, 4, 8, 3], \n        num_heads=16, \n        mlp_ratio=[8, 8, 4, 4], \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6))\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 23 CMT Usage\n#### 23.1. Paper\n[CMT: Convolutional Neural Networks Meet Vision Transformers](https://arxiv.org/abs/2107.06263)\n\n#### 23.2. Usage Code\n```python\n\nfrom model.backbone.CMT import CMT_Tiny\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CMT_Tiny()\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 24 EfficientFormer Usage\n#### 24.1. Paper\n[EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)\n\n#### 24.2. Usage Code\n```python\n\nfrom model.backbone.EfficientFormer import EfficientFormer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = EfficientFormer(\n        layers=EfficientFormer_depth['l1'],\n        embed_dims=EfficientFormer_width['l1'],\n        downsamples=[True, True, True, True],\n        vit_num=1,\n    )\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 25 ConvNeXtV2 Usage\n#### 25.1. Paper\n[ConvNeXtV2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808)\n\n#### 25.2. Usage Code\n```python\n\nfrom model.backbone.convnextv2 import convnextv2_atto\nimport torch\nfrom torch import nn\n\nif __name__ == \"__main__\":\n    model = convnextv2_atto()\n    input = torch.randn(1, 3, 224, 224)\n    out = model(input)\n    print(out.shape)\n\n```\n\n\n\n\n\n# MLP Series\n\n- Pytorch implementation of [\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition---arXiv 2021.05.05\"](https://arxiv.org/pdf/2105.01883v1.pdf)\n\n- Pytorch implementation of [\"MLP-Mixer: An all-MLP Architecture for Vision---arXiv 2021.05.17\"](https://arxiv.org/pdf/2105.01601.pdf)\n\n- Pytorch implementation of [\"ResMLP: Feedforward networks for image classification with data-efficient training---arXiv 2021.05.07\"](https://arxiv.org/pdf/2105.03404.pdf)\n\n- Pytorch implementation of [\"Pay Attention to MLPs---arXiv 2021.05.17\"](https://arxiv.org/abs/2105.08050)\n\n\n- Pytorch implementation of [\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?---arXiv 2021.09.12\"](https://arxiv.org/abs/2109.05422)\n\n### 1. RepMLP Usage\n#### 1.1. Paper\n[\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition\"](https://arxiv.org/pdf/2105.01883v1.pdf)\n\n#### 1.2. Overview\n![](./model/img/repmlp.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.mlp.repmlp import RepMLP\nimport torch\nfrom torch import nn\n\nN=4 #batch size\nC=512 #input dim\nO=1024 #output dim\nH=14 #image height\nW=14 #image width\nh=7 #patch height\nw=7 #patch width\nfc1_fc2_reduction=1 #reduction ratio\nfc3_groups=8 # groups\nrepconv_kernels=[1,3,5,7] #kernel list\nrepmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels)\nx=torch.randn(N,C,H,W)\nrepmlp.eval()\nfor module in repmlp.modules():\n    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n        nn.init.uniform_(module.running_mean, 0, 0.1)\n        nn.init.uniform_(module.running_var, 0, 0.1)\n        nn.init.uniform_(module.weight, 0, 0.1)\n        nn.init.uniform_(module.bias, 0, 0.1)\n\n#training result\nout=repmlp(x)\n#inference result\nrepmlp.switch_to_deploy()\ndeployout = repmlp(x)\n\nprint(((deployout-out)**2).sum())\n```\n\n### 2. MLP-Mixer Usage\n#### 2.1. Paper\n[\"MLP-Mixer: An all-MLP Architecture for Vision\"](https://arxiv.org/pdf/2105.01601.pdf)\n\n#### 2.2. Overview\n![](./model/img/mlpmixer.png)\n\n#### 2.3. Usage Code\n```python\nfrom model.mlp.mlp_mixer import MlpMixer\nimport torch\nmlp_mixer=MlpMixer(num_classes=1000,num_blocks=10,patch_size=10,tokens_hidden_dim=32,channels_hidden_dim=1024,tokens_mlp_dim=16,channels_mlp_dim=1024)\ninput=torch.randn(50,3,40,40)\noutput=mlp_mixer(input)\nprint(output.shape)\n```\n\n***\n\n### 3. ResMLP Usage\n#### 3.1. Paper\n[\"ResMLP: Feedforward networks for image classification with data-efficient training\"](https://arxiv.org/pdf/2105.03404.pdf)\n\n#### 3.2. Overview\n![](./model/img/resmlp.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.mlp.resmlp import ResMLP\nimport torch\n\ninput=torch.randn(50,3,14,14)\nresmlp=ResMLP(dim=128,image_size=14,patch_size=7,class_num=1000)\nout=resmlp(input)\nprint(out.shape) #the last dimention is class_num\n```\n\n***\n\n### 4. gMLP Usage\n#### 4.1. Paper\n[\"Pay Attention to MLPs\"](https://arxiv.org/abs/2105.08050)\n\n#### 4.2. Overview\n![](./model/img/gMLP.jpg)\n\n#### 4.3. Usage Code\n```python\nfrom model.mlp.g_mlp import gMLP\nimport torch\n\nnum_tokens=10000\nbs=50\nlen_sen=49\nnum_layers=6\ninput=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen\ngmlp = gMLP(num_tokens=num_tokens,len_sen=len_sen,dim=512,d_ff=1024)\noutput=gmlp(input)\nprint(output.shape)\n```\n\n***\n\n### 5. sMLP Usage\n#### 5.1. Paper\n[\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?\"](https://arxiv.org/abs/2109.05422)\n\n#### 5.2. Overview\n![](./model/img/sMLP.jpg)\n\n#### 5.3. Usage Code\n```python\nfrom model.mlp.sMLP_block import sMLPBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    smlp=sMLPBlock(h=224,w=224)\n    out=smlp(input)\n    print(out.shape)\n```\n\n### 6. vip-mlp Usage\n#### 6.1. Paper\n[\"Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\n\n#### 6.2. Usage Code\n```python\nfrom model.mlp.vip-mlp import VisionPermutator\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionPermutator(\n        layers=[4, 3, 8, 3], \n        embed_dims=[384, 384, 384, 384], \n        patch_size=14, \n        transitions=[False, False, False, False],\n        segment_dim=[16, 16, 16, 16], \n        mlp_ratios=[3, 3, 3, 3], \n        mlp_fn=WeightedPermuteMLP\n    )\n    output=model(input)\n    print(output.shape)\n```\n\n\n# Re-Parameter Series\n\n- Pytorch implementation of [\"RepVGG: Making VGG-style ConvNets Great Again---CVPR2021\"](https://arxiv.org/abs/2101.03697)\n\n- Pytorch implementation of [\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks---ICCV2019\"](https://arxiv.org/abs/1908.03930)\n\n- Pytorch implementation of [\"Diverse Branch Block: Building a Convolution as an Inception-like Unit---CVPR2021\"](https://arxiv.org/abs/2103.13425)\n\n\n***\n\n### 1. RepVGG Usage\n#### 1.1. Paper\n[\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/abs/2101.03697)\n\n#### 1.2. Overview\n![](./model/img/repvgg.png)\n\n#### 1.3. Usage Code\n```python\n\nfrom model.rep.repvgg import RepBlock\nimport torch\n\n\ninput=torch.randn(50,512,49,49)\nrepblock=RepBlock(512,512)\nrepblock.eval()\nout=repblock(input)\nrepblock._switch_to_deploy()\nout2=repblock(input)\nprint('difference between vgg and repvgg')\nprint(((out2-out)**2).sum())\n```\n\n\n\n***\n\n### 2. ACNet Usage\n#### 2.1. Paper\n[\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks\"](https://arxiv.org/abs/1908.03930)\n\n#### 2.2. Overview\n![](./model/img/acnet.png)\n\n#### 2.3. Usage Code\n```python\nfrom model.rep.acnet import ACNet\nimport torch\nfrom torch import nn\n\ninput=torch.randn(50,512,49,49)\nacnet=ACNet(512,512)\nacnet.eval()\nout=acnet(input)\nacnet._switch_to_deploy()\nout2=acnet(input)\nprint('difference:')\nprint(((out2-out)**2).sum())\n\n```\n\n\n\n***\n\n### 2. Diverse Branch Block Usage\n#### 2.1. Paper\n[\"Diverse Branch Block: Building a Convolution as an Inception-like Unit\"](https://arxiv.org/abs/2103.13425)\n\n#### 2.2. Overview\n![](./model/img/ddb.png)\n\n#### 2.3. Usage Code\n##### 2.3.1 Transform I\n```python\nfrom model.rep.ddb import transI_conv_bn\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n#conv+bn\nconv1=nn.Conv2d(64,64,3,padding=1)\nbn1=nn.BatchNorm2d(64)\nbn1.eval()\nout1=bn1(conv1(input))\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.2 Transform II\n```python\nfrom model.rep.ddb import transII_conv_branch\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,64,3,padding=1)\nconv2=nn.Conv2d(64,64,3,padding=1)\nout1=conv1(input)+conv2(input)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.3 Transform III\n```python\nfrom model.rep.ddb import transIII_conv_sequential\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,64,1,padding=0,bias=False)\nconv2=nn.Conv2d(64,64,3,padding=1,bias=False)\nout1=conv2(conv1(input))\n\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1,bias=False)\nconv_fuse.weight.data=transIII_conv_sequential(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.4 Transform IV\n```python\nfrom model.rep.ddb import transIV_conv_concat\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,32,3,padding=1)\nconv2=nn.Conv2d(64,32,3,padding=1)\nout1=torch.cat([conv1(input),conv2(input)],dim=1)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.5 Transform V\n```python\nfrom model.rep.ddb import transV_avg\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\navg=nn.AvgPool2d(kernel_size=3,stride=1)\nout1=avg(input)\n\nconv=transV_avg(64,3)\nout2=conv(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n\n##### 2.3.6 Transform VI\n```python\nfrom model.rep.ddb import transVI_conv_scale\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1x1=nn.Conv2d(64,64,1)\nconv1x3=nn.Conv2d(64,64,(1,3),padding=(0,1))\nconv3x1=nn.Conv2d(64,64,(3,1),padding=(1,0))\nout1=conv1x1(input)+conv1x3(input)+conv3x1(input)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n\n\n\n\n# Convolution Series\n\n- Pytorch implementation of [\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications---CVPR2017\"](https://arxiv.org/abs/1704.04861)\n\n- Pytorch implementation of [\"Efficientnet: Rethinking model scaling for convolutional neural networks---PMLR2019\"](http://proceedings.mlr.press/v97/tan19a.html)\n\n- Pytorch implementation of [\"Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021\"](https://arxiv.org/abs/2103.06255)\n\n- Pytorch implementation of [\"Dynamic Convolution: Attention over Convolution Kernels---CVPR2020 Oral\"](https://arxiv.org/abs/1912.03458)\n\n- Pytorch implementation of [\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference---NeurIPS2019\"](https://arxiv.org/abs/1904.04971)\n\n***\n\n### 1. Depthwise Separable Convolution Usage\n#### 1.1. Paper\n[\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"](https://arxiv.org/abs/1704.04861)\n\n#### 1.2. Overview\n![](./model/img/DepthwiseSeparableConv.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\ndsconv=DepthwiseSeparableConvolution(3,64)\nout=dsconv(input)\nprint(out.shape)\n```\n\n***\n\n\n### 2. MBConv Usage\n#### 2.1. Paper\n[\"Efficientnet: Rethinking model scaling for convolutional neural networks\"](http://proceedings.mlr.press/v97/tan19a.html)\n\n#### 2.2. Overview\n![](./model/img/MBConv.jpg)\n\n#### 2.3. Usage Code\n```python\nfrom model.conv.MBConv import MBConvBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=512,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n\n```\n\n***\n\n\n### 3. Involution Usage\n#### 3.1. Paper\n[\"Involution: Inverting the Inherence of Convolution for Visual Recognition\"](https://arxiv.org/abs/2103.06255)\n\n#### 3.2. Overview\n![](./model/img/Involution.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.conv.Involution import Involution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,4,64,64)\ninvolution=Involution(kernel_size=3,in_channel=4,stride=2)\nout=involution(input)\nprint(out.shape)\n```\n\n***\n\n\n### 4. DynamicConv Usage\n#### 4.1. Paper\n[\"Dynamic Convolution: Attention over Convolution Kernels\"](https://arxiv.org/abs/1912.03458)\n\n#### 4.2. Overview\n![](./model/img/DynamicConv.png)\n\n#### 4.3. Usage Code\n```python\nfrom model.conv.DynamicConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=DynamicConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape) # 2,32,64,64\n\n```\n\n***\n\n\n### 5. CondConv Usage\n#### 5.1. Paper\n[\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference\"](https://arxiv.org/abs/1904.04971)\n\n#### 5.2. Overview\n![](./model/img/CondConv.png)\n\n#### 5.3. Usage Code\n```python\nfrom model.conv.CondConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\n\n\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=CondConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape)\n\n```\n\n\n\n## 其他项目推荐\n\n-------\n\n🔥🔥🔥 重磅！！！作为项目补充，更多论文层面的解析，可以关注新开源的项目 **[FightingCV-Paper-Reading](https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading)** ，里面汇集和整理了各大顶会顶刊的论文解析\n\n\n\n🔥🔥🔥重磅！！！ 最近为大家整理了网上的各种AI相关的视频教程和必读论文 **[FightingCV-Course\n](https://github.com/xmu-xiaoma666/FightingCV-Course)**\n\n\n🔥🔥🔥 重磅！！！最近全新开源了一个 **[YOLOAir](https://github.com/iscyy/yoloair)** 目标检测代码库 ，里面集成了多种YOLO模型，包括YOLOv5, YOLOv7,YOLOR, YOLOX,YOLOv4, YOLOv3以及其他YOLO模型，还包括多种现有Attention机制。\n\n\n🔥🔥🔥 **ECCV2022论文汇总：[ECCV2022-Paper-List](https://github.com/xmu-xiaoma666/ECCV2022-Paper-List/blob/master/README.md)**\n\n\n<!-- ![image](https://user-images.githubusercontent.com/33897496/184842902-9acff374-b3e7-401a-80fd-9d484e40c637.png) -->\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 60.21875,
          "content": "\n<img src=\"./FightingCVimg/LOGO.gif\" height=\"200\" width=\"400\"/>\n\nEnglish | [简体中文](./README.md)\n\n# FightingCV Codebase For [***Attention***](#attention-series),[***Backbone***](#backbone-series), [***MLP***](#mlp-series), [***Re-parameter***](#re-parameter-series), [**Convolution**](#convolution-series)\n\n![](https://img.shields.io/badge/fightingcv-v0.0.1-brightgreen)\n![](https://img.shields.io/badge/python->=v3.0-blue)\n![](https://img.shields.io/badge/pytorch->=v1.4-red)\n\n<!--\n-------\n*If this project is helpful to you, welcome to give a*star***.* \n\n*Don't forget to*follow*me to learn about project updates.*\n\n-->\n\n-------\n\n\n🔥🔥🔥As a supplement to the project, a object detection codebase [YOLOAir](https://github.com/iscyy/yoloair) has recently been newly opened, which integrates various attention mechanisms in the object detection algorithm. The code is simple and easy to read. Welcome to play and star🌟!**\n\n\n<!-- ![image](https://user-images.githubusercontent.com/33897496/184842902-9acff374-b3e7-401a-80fd-9d484e40c637.png) -->\n\n\n\n-------\n\nHello, everyone, I'm Xiaoma 🚀🚀🚀\n\n***For beginners (like me):***\nRecently, I found a problem when reading the paper. Sometimes the core idea of the paper is very simple, and the core code may be just a dozen lines. However, when I open the source code of the author's release, I find that the proposed module is embedded in the task framework such as classification, detection and segmentation, resulting in redundant code. For me who is not familiar with the specific task framework, it is difficult to find the core code, resulting in some difficulties in understanding the paper and network ideas.\n\n***For advanced (like you):***\nIf the basic units conv, FC and RNN are regarded as small Lego blocks, and the structures transformer and RESNET are regarded as LEGO castles that have been built. The modules provided by this project are LEGO components with complete semantic informationLet scientific researchers avoid repeatedly building wheels, just think about how to use these \"LEGO components\" to build more colorful works.\n\n***For proficient (may be like you):***\nLimited capacity, do not like light spraying!!!\n\n***For All：***\nThis project aims to realize a code base that can make beginners of deep learning understand and serve scientific research and industrial communities. As [fightingcv wechat official account]( https://mp.weixin.qq.com/s/m9RiivbbDPdjABsTd6q8FA )The purpose of this project is to achieve 🚀Let there be no hard to read papers in the world🚀。\n(at the same time, we also welcome all scientific researchers to sort out the core code of their work into this project, promote the development of the scientific research community, and indicate the author of the code in readme ~)\n\n\n<!--\n\n\n## Wechat Official account &  communication group\n\n\n\nWelcome to pay attention to wechat official account: **fightingcv**\n\n\n\nThe official account shares papers, algorithms and codes every day Oh~\n\n\n\n\n**Share some recent papers and analysis in the group every day. Welcome to study and exchange ha~~~\n\n(if you can't add it, you can add wechat: **775629340**, remember the remarks **[company / school + direction + ID])**\n\n![](./FightingCVimg/wechat.jpg)\n\nWe strongly recommend that you pay attention to [Zhihu]( https://www.zhihu.com/people/jason-14-58-38/posts )Account number and **[fightingcv Wechat official account**]( https://mp.weixin.qq.com/s/m9RiivbbDPdjABsTd6q8FA )** to quickly learn about the latest high-quality dry goods resources.\n\n-->\n\n***\n\n# Contents\n\n- [Attention Series](#attention-series)\n    - [1. External Attention Usage](#1-external-attention-usage)\n\n    - [2. Self Attention Usage](#2-self-attention-usage)\n\n    - [3. Simplified Self Attention Usage](#3-simplified-self-attention-usage)\n\n    - [4. Squeeze-and-Excitation Attention Usage](#4-squeeze-and-excitation-attention-usage)\n\n    - [5. SK Attention Usage](#5-sk-attention-usage)\n\n    - [6. CBAM Attention Usage](#6-cbam-attention-usage)\n\n    - [7. BAM Attention Usage](#7-bam-attention-usage)\n    \n    - [8. ECA Attention Usage](#8-eca-attention-usage)\n\n    - [9. DANet Attention Usage](#9-danet-attention-usage)\n\n    - [10. Pyramid Split Attention (PSA) Usage](#10-Pyramid-Split-Attention-Usage)\n\n    - [11. Efficient Multi-Head Self-Attention(EMSA) Usage](#11-Efficient-Multi-Head-Self-Attention-Usage)\n\n    - [12. Shuffle Attention Usage](#12-Shuffle-Attention-Usage)\n    \n    - [13. MUSE Attention Usage](#13-MUSE-Attention-Usage)\n  \n    - [14. SGE Attention Usage](#14-SGE-Attention-Usage)\n\n    - [15. A2 Attention Usage](#15-A2-Attention-Usage)\n\n    - [16. AFT Attention Usage](#16-AFT-Attention-Usage)\n\n    - [17. Outlook Attention Usage](#17-Outlook-Attention-Usage)\n\n    - [18. ViP Attention Usage](#18-ViP-Attention-Usage)\n\n    - [19. CoAtNet Attention Usage](#19-CoAtNet-Attention-Usage)\n\n    - [20. HaloNet Attention Usage](#20-HaloNet-Attention-Usage)\n\n    - [21. Polarized Self-Attention Usage](#21-Polarized-Self-Attention-Usage)\n\n    - [22. CoTAttention Usage](#22-CoTAttention-Usage)\n\n    - [23. Residual Attention Usage](#23-Residual-Attention-Usage)\n  \n    - [24. S2 Attention Usage](#24-S2-Attention-Usage)\n\n    - [25. GFNet Attention Usage](#25-GFNet-Attention-Usage)\n\n    - [26. Triplet Attention Usage](#26-TripletAttention-Usage)\n\n    - [27. Coordinate Attention Usage](#27-Coordinate-Attention-Usage)\n\n    - [28. MobileViT Attention Usage](#28-MobileViT-Attention-Usage)\n\n    - [29. ParNet Attention Usage](#29-ParNet-Attention-Usage)\n\n    - [30. UFO Attention Usage](#30-UFO-Attention-Usage)\n\n    - [31. ACmix Attention Usage](#31-Acmix-Attention-Usage)\n  \n    - [32. MobileViTv2 Attention Usage](#32-MobileViTv2-Attention-Usage)\n\n    - [33. DAT Attention Usage](#33-DAT-Attention-Usage)\n\n    - [34. CrossFormer Attention Usage](#34-CrossFormer-Attention-Usage)\n\n    - [35. MOATransformer Attention Usage](#35-MOATransformer-Attention-Usage)\n\n    - [36. CrissCrossAttention Attention Usage](#36-CrissCrossAttention-Attention-Usage)\n\n    - [37. Axial_attention Attention Usage](#37-Axial_attention-Attention-Usage)\n\n- [Backbone Series](#Backbone-series)\n\n    - [1. ResNet Usage](#1-ResNet-Usage)\n\n    - [2. ResNeXt Usage](#2-ResNeXt-Usage)\n\n    - [3. MobileViT Usage](#3-MobileViT-Usage)\n\n    - [4. ConvMixer Usage](#4-ConvMixer-Usage)\n\n    - [5. ShuffleTransformer Usage](#5-ShuffleTransformer-Usage)\n\n    - [6. ConTNet Usage](#6-ConTNet-Usage)\n\n    - [7. HATNet Usage](#7-HATNet-Usage)\n\n    - [8. CoaT Usage](#8-CoaT-Usage)\n\n    - [9. PVT Usage](#9-PVT-Usage)\n\n    - [10. CPVT Usage](#10-CPVT-Usage)\n\n    - [11. PIT Usage](#11-PIT-Usage)\n\n    - [12. CrossViT Usage](#12-CrossViT-Usage)\n\n    - [13. TnT Usage](#13-TnT-Usage)\n\n    - [14. DViT Usage](#14-DViT-Usage)\n\n    - [15. CeiT Usage](#15-CeiT-Usage)\n\n    - [16. ConViT Usage](#16-ConViT-Usage)\n\n    - [17. CaiT Usage](#17-CaiT-Usage)\n\n    - [18. PatchConvnet Usage](#18-PatchConvnet-Usage)\n\n    - [19. DeiT Usage](#19-DeiT-Usage)\n\n    - [20. LeViT Usage](#20-LeViT-Usage)\n\n    - [21. VOLO Usage](#21-VOLO-Usage)\n    \n    - [22. Container Usage](#22-Container-Usage)\n\n    - [23. CMT Usage](#23-CMT-Usage)\n\n\n- [MLP Series](#mlp-series)\n\n    - [1. RepMLP Usage](#1-RepMLP-Usage)\n\n    - [2. MLP-Mixer Usage](#2-MLP-Mixer-Usage)\n\n    - [3. ResMLP Usage](#3-ResMLP-Usage)\n\n    - [4. gMLP Usage](#4-gMLP-Usage)\n\n    - [5. sMLP Usage](#5-sMLP-Usage)\n\n    - [6. vip-mlp Usage](#6-vip-mlp-Usage)\n\n- [Re-Parameter(ReP) Series](#Re-Parameter-series)\n\n    - [1. RepVGG Usage](#1-RepVGG-Usage)\n\n    - [2. ACNet Usage](#2-ACNet-Usage)\n\n    - [3. Diverse Branch Block(DDB) Usage](#3-Diverse-Branch-Block-Usage)\n\n- [Convolution Series](#Convolution-series)\n\n    - [1. Depthwise Separable Convolution Usage](#1-Depthwise-Separable-Convolution-Usage)\n\n    - [2. MBConv Usage](#2-MBConv-Usage)\n\n    - [3. Involution Usage](#3-Involution-Usage)\n\n    - [4. DynamicConv Usage](#4-DynamicConv-Usage)\n\n    - [5. CondConv Usage](#5-CondConv-Usage)\n\n***\n\n\n# Attention Series\n\n- Pytorch implementation of [\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks---arXiv 2021.05.05\"](https://arxiv.org/abs/2105.02358)\n\n- Pytorch implementation of [\"Attention Is All You Need---NIPS2017\"](https://arxiv.org/pdf/1706.03762.pdf)\n\n- Pytorch implementation of [\"Squeeze-and-Excitation Networks---CVPR2018\"](https://arxiv.org/abs/1709.01507)\n\n- Pytorch implementation of [\"Selective Kernel Networks---CVPR2019\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n- Pytorch implementation of [\"CBAM: Convolutional Block Attention Module---ECCV2018\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n\n- Pytorch implementation of [\"BAM: Bottleneck Attention Module---BMCV2018\"](https://arxiv.org/pdf/1807.06514.pdf)\n\n- Pytorch implementation of [\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020\"](https://arxiv.org/pdf/1910.03151.pdf)\n\n- Pytorch implementation of [\"Dual Attention Network for Scene Segmentation---CVPR2019\"](https://arxiv.org/pdf/1809.02983.pdf)\n\n- Pytorch implementation of [\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30\"](https://arxiv.org/pdf/2105.14447.pdf)\n\n- Pytorch implementation of [\"ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28\"](https://arxiv.org/abs/2105.13677)\n\n- Pytorch implementation of [\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021\"](https://arxiv.org/pdf/2102.00240.pdf)\n\n- Pytorch implementation of [\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17\"](https://arxiv.org/abs/1911.09483)\n\n- Pytorch implementation of [\"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23\"](https://arxiv.org/pdf/1905.09646.pdf)\n\n- Pytorch implementation of [\"A2-Nets: Double Attention Networks---NIPS2018\"](https://arxiv.org/pdf/1810.11579.pdf)\n\n\n- Pytorch implementation of [\"An Attention Free Transformer---ICLR2021 (Apple New Work)\"](https://arxiv.org/pdf/2105.14103v1.pdf)\n\n\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24\"](https://arxiv.org/abs/2106.13112) \n  [【论文解析】](https://zhuanlan.zhihu.com/p/385561050)\n\n\n- Pytorch implementation of [Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23](https://arxiv.org/abs/2106.12368) \n  [【论文解析】](https://mp.weixin.qq.com/s/5gonUQgBho_m2O54jyXF_Q)\n\n\n- Pytorch implementation of [CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09](https://arxiv.org/abs/2106.04803) \n  [【论文解析】](https://zhuanlan.zhihu.com/p/385578588)\n\n\n- Pytorch implementation of [Scaling Local Self-Attention for Parameter Efficient Visual Backbones---CVPR2021 Oral](https://arxiv.org/pdf/2103.12731.pdf)  [【论文解析】](https://zhuanlan.zhihu.com/p/388598744)\n\n\n\n- Pytorch implementation of [Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02](https://arxiv.org/abs/2107.00782)  [【论文解析】](https://zhuanlan.zhihu.com/p/389770482) \n\n\n- Pytorch implementation of [Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292)  [【论文解析】](https://zhuanlan.zhihu.com/p/394795481) \n\n\n- Pytorch implementation of [Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \n\n\n- Pytorch implementation of [S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) [【论文解析】](https://zhuanlan.zhihu.com/p/397003638) \n\n- Pytorch implementation of [Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \n\n- Pytorch implementation of [Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021](https://arxiv.org/abs/2010.03045) \n\n- Pytorch implementation of [Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2110.02178)\n\n- Pytorch implementation of [Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\n\n- Pytorch implementation of [UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2109.14382)\n\n- Pytorch implementation of [Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06](https://arxiv.org/abs/2206.02680)\n\n- Pytorch implementation of [On the Integration of Self-Attention and Convolution---ArXiv 2022.03.14](https://arxiv.org/pdf/2111.14556.pdf)\n\n- Pytorch implementation of [CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION---ICLR 2022](https://arxiv.org/pdf/2108.00154.pdf)\n\n- Pytorch implementation of [Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903)\n\n- Pytorch implementation of [CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n\n- Pytorch implementation of [Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)\n***\n\n\n### 1. External Attention Usage\n#### 1.1. Paper\n[\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\"](https://arxiv.org/abs/2105.02358)\n\n#### 1.2. Overview\n![](./model/img/External_Attention.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.attention.ExternalAttention import ExternalAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nea = ExternalAttention(d_model=512,S=8)\noutput=ea(input)\nprint(output.shape)\n```\n\n***\n\n\n### 2. Self Attention Usage\n#### 2.1. Paper\n[\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf)\n\n#### 1.2. Overview\n![](./model/img/SA.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.attention.SelfAttention import ScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nsa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n```\n\n***\n\n### 3. Simplified Self Attention Usage\n#### 3.1. Paper\n[None]()\n\n#### 3.2. Overview\n![](./model/img/SSA.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\noutput=ssa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n### 4. Squeeze-and-Excitation Attention Usage\n#### 4.1. Paper\n[\"Squeeze-and-Excitation Networks\"](https://arxiv.org/abs/1709.01507)\n\n#### 4.2. Overview\n![](./model/img/SE.png)\n\n#### 4.3. Usage Code\n```python\nfrom model.attention.SEAttention import SEAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SEAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n\n***\n\n### 5. SK Attention Usage\n#### 5.1. Paper\n[\"Selective Kernel Networks\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n#### 5.2. Overview\n![](./model/img/SK.png)\n\n#### 5.3. Usage Code\n```python\nfrom model.attention.SKAttention import SKAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SKAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n***\n\n### 6. CBAM Attention Usage\n#### 6.1. Paper\n[\"CBAM: Convolutional Block Attention Module\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n\n#### 6.2. Overview\n![](./model/img/CBAM1.png)\n\n![](./model/img/CBAM2.png)\n\n#### 6.3. Usage Code\n```python\nfrom model.attention.CBAM import CBAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nkernel_size=input.shape[2]\ncbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)\noutput=cbam(input)\nprint(output.shape)\n\n```\n\n***\n\n### 7. BAM Attention Usage\n#### 7.1. Paper\n[\"BAM: Bottleneck Attention Module\"](https://arxiv.org/pdf/1807.06514.pdf)\n\n#### 7.2. Overview\n![](./model/img/BAM.png)\n\n#### 7.3. Usage Code\n```python\nfrom model.attention.BAM import BAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nbam = BAMBlock(channel=512,reduction=16,dia_val=2)\noutput=bam(input)\nprint(output.shape)\n\n```\n\n***\n\n### 8. ECA Attention Usage\n#### 8.1. Paper\n[\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\"](https://arxiv.org/pdf/1910.03151.pdf)\n\n#### 8.2. Overview\n![](./model/img/ECA.png)\n\n#### 8.3. Usage Code\n```python\nfrom model.attention.ECAAttention import ECAAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\neca = ECAAttention(kernel_size=3)\noutput=eca(input)\nprint(output.shape)\n\n```\n\n***\n\n### 9. DANet Attention Usage\n#### 9.1. Paper\n[\"Dual Attention Network for Scene Segmentation\"](https://arxiv.org/pdf/1809.02983.pdf)\n\n#### 9.2. Overview\n![](./model/img/danet.png)\n\n#### 9.3. Usage Code\n```python\nfrom model.attention.DANet import DAModule\nimport torch\n\ninput=torch.randn(50,512,7,7)\ndanet=DAModule(d_model=512,kernel_size=3,H=7,W=7)\nprint(danet(input).shape)\n\n```\n\n***\n\n### 10. Pyramid Split Attention Usage\n\n#### 10.1. Paper\n[\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network\"](https://arxiv.org/pdf/2105.14447.pdf)\n\n#### 10.2. Overview\n![](./model/img/psa.png)\n\n#### 10.3. Usage Code\n```python\nfrom model.attention.PSA import PSA\nimport torch\n\ninput=torch.randn(50,512,7,7)\npsa = PSA(channel=512,reduction=8)\noutput=psa(input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 11. Efficient Multi-Head Self-Attention Usage\n\n#### 11.1. Paper\n[\"ResT: An Efficient Transformer for Visual Recognition\"](https://arxiv.org/abs/2105.13677)\n\n#### 11.2. Overview\n![](./model/img/EMSA.png)\n\n#### 11.3. Usage Code\n```python\n\nfrom model.attention.EMSA import EMSA\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,64,512)\nemsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)\noutput=emsa(input,input,input)\nprint(output.shape)\n    \n```\n\n***\n\n\n### 12. Shuffle Attention Usage\n\n#### 12.1. Paper\n[\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS\"](https://arxiv.org/pdf/2102.00240.pdf)\n\n#### 12.2. Overview\n![](./model/img/ShuffleAttention.jpg)\n\n#### 12.3. Usage Code\n```python\n\nfrom model.attention.ShuffleAttention import ShuffleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,512,7,7)\nse = ShuffleAttention(channel=512,G=8)\noutput=se(input)\nprint(output.shape)\n\n    \n```\n\n\n***\n\n\n### 13. MUSE Attention Usage\n\n#### 13.1. Paper\n[\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\"](https://arxiv.org/abs/1911.09483)\n\n#### 13.2. Overview\n![](./model/img/MUSE.png)\n\n#### 13.3. Usage Code\n```python\nfrom model.attention.MUSEAttention import MUSEAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,49,512)\nsa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 14. SGE Attention Usage\n\n#### 14.1. Paper\n[Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks](https://arxiv.org/pdf/1905.09646.pdf)\n\n#### 14.2. Overview\n![](./model/img/SGE.png)\n\n#### 14.3. Usage Code\n```python\nfrom model.attention.SGE import SpatialGroupEnhance\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nsge = SpatialGroupEnhance(groups=8)\noutput=sge(input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 15. A2 Attention Usage\n\n#### 15.1. Paper\n[A2-Nets: Double Attention Networks](https://arxiv.org/pdf/1810.11579.pdf)\n\n#### 15.2. Overview\n![](./model/img/A2.png)\n\n#### 15.3. Usage Code\n```python\nfrom model.attention.A2Atttention import DoubleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\na2 = DoubleAttention(512,128,128,True)\noutput=a2(input)\nprint(output.shape)\n\n```\n\n\n\n### 16. AFT Attention Usage\n\n#### 16.1. Paper\n[An Attention Free Transformer](https://arxiv.org/pdf/2105.14103v1.pdf)\n\n#### 16.2. Overview\n![](./model/img/AFT.jpg)\n\n#### 16.3. Usage Code\n```python\nfrom model.attention.AFT import AFT_FULL\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,49,512)\naft_full = AFT_FULL(d_model=512, n=49)\noutput=aft_full(input)\nprint(output.shape)\n\n```\n\n\n\n\n\n\n### 17. Outlook Attention Usage\n\n#### 17.1. Paper\n\n\n[VOLO: Vision Outlooker for Visual Recognition\"](https://arxiv.org/abs/2106.13112)\n\n\n#### 17.2. Overview\n![](./model/img/OutlookAttention.png)\n\n#### 17.3. Usage Code\n```python\nfrom model.attention.OutlookAttention import OutlookAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,28,28,512)\noutlook = OutlookAttention(dim=512)\noutput=outlook(input)\nprint(output.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n### 18. ViP Attention Usage\n\n#### 18.1. Paper\n\n\n[Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\n\n\n#### 18.2. Overview\n![](./model/img/ViP.png)\n\n#### 18.3. Usage Code\n```python\n\nfrom model.attention.ViP import WeightedPermuteMLP\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(64,8,8,512)\nseg_dim=8\nvip=WeightedPermuteMLP(512,seg_dim)\nout=vip(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n### 19. CoAtNet Attention Usage\n\n#### 19.1. Paper\n\n\n[CoAtNet: Marrying Convolution and Attention for All Data Sizes\"](https://arxiv.org/abs/2106.04803) \n\n\n#### 19.2. Overview\nNone\n\n\n#### 19.3. Usage Code\n```python\n\nfrom model.attention.CoAtNet import CoAtNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=CoAtNet(in_ch=3,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n### 20. HaloNet Attention Usage\n\n#### 20.1. Paper\n\n\n[Scaling Local Self-Attention for Parameter Efficient Visual Backbones\"](https://arxiv.org/pdf/2103.12731.pdf) \n\n\n#### 20.2. Overview\n\n![](./model/img/HaloNet.png)\n\n#### 20.3. Usage Code\n```python\n\nfrom model.attention.HaloAttention import HaloAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,8,8)\nhalo = HaloAttention(dim=512,\n    block_size=2,\n    halo_size=1,)\noutput=halo(input)\nprint(output.shape)\n\n```\n\n\n***\n\n### 21. Polarized Self-Attention Usage\n\n#### 21.1. Paper\n\n[Polarized Self-Attention: Towards High-quality Pixel-wise Regression\"](https://arxiv.org/abs/2107.00782)  \n\n\n#### 21.2. Overview\n\n![](./model/img/PoSA.png)\n\n#### 21.3. Usage Code\n```python\n\nfrom model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,7,7)\npsa = SequentialPolarizedSelfAttention(channel=512)\noutput=psa(input)\nprint(output.shape)\n\n\n```\n\n\n***\n\n\n### 22. CoTAttention Usage\n\n#### 22.1. Paper\n\n[Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292) \n\n\n#### 22.2. Overview\n\n![](./model/img/CoT.png)\n\n#### 22.3. Usage Code\n```python\n\nfrom model.attention.CoTAttention import CoTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ncot = CoTAttention(dim=512,kernel_size=3)\noutput=cot(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n### 23. Residual Attention Usage\n\n#### 23.1. Paper\n\n[Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \n\n\n#### 23.2. Overview\n\n![](./model/img/ResAtt.png)\n\n#### 23.3. Usage Code\n```python\n\nfrom model.attention.ResidualAttention import ResidualAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nresatt = ResidualAttention(channel=512,num_class=1000,la=0.2)\noutput=resatt(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n\n### 24. S2 Attention Usage\n\n#### 24.1. Paper\n\n[S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) \n\n\n#### 24.2. Overview\n\n![](./model/img/S2Attention.png)\n\n#### 24.3. Usage Code\n```python\nfrom model.attention.S2Attention import S2Attention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ns2att = S2Attention(channels=512)\noutput=s2att(input)\nprint(output.shape)\n\n```\n\n***\n\n\n\n### 25. GFNet Attention Usage\n\n#### 25.1. Paper\n\n[Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \n\n\n#### 25.2. Overview\n\n![](./model/img/GFNet.jpg)\n\n#### 25.3. Usage Code - Implemented by [Wenliang Zhao (Author)](https://scholar.google.com/citations?user=lyPWvuEAAAAJ&hl=en)\n\n```python\nfrom model.attention.gfnet import GFNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nx = torch.randn(1, 3, 224, 224)\ngfnet = GFNet(embed_dim=384, img_size=224, patch_size=16, num_classes=1000)\nout = gfnet(x)\nprint(out.shape)\n\n```\n\n***\n\n\n### 26. TripletAttention Usage\n\n#### 26.1. Paper\n\n[Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021](https://arxiv.org/abs/2010.03045) \n\n#### 26.2. Overview\n\n![](./model/img/triplet.png)\n\n#### 26.3. Usage Code - Implemented by [digantamisra98](https://github.com/digantamisra98)\n\n```python\nfrom model.attention.TripletAttention import TripletAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\ninput=torch.randn(50,512,7,7)\ntriplet = TripletAttention()\noutput=triplet(input)\nprint(output.shape)\n```\n\n\n***\n\n\n### 27. Coordinate Attention Usage\n\n#### 27.1. Paper\n\n[Coordinate Attention for Efficient Mobile Network Design---CVPR 2021](https://arxiv.org/abs/2103.02907)\n\n\n#### 27.2. Overview\n\n![](./model/img/CoordAttention.png)\n\n#### 27.3. Usage Code - Implemented by [Andrew-Qibin](https://github.com/Andrew-Qibin)\n\n```python\nfrom model.attention.CoordAttention import CoordAtt\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninp=torch.rand([2, 96, 56, 56])\ninp_dim, oup_dim = 96, 96\nreduction=32\n\ncoord_attention = CoordAtt(inp_dim, oup_dim, reduction=reduction)\noutput=coord_attention(inp)\nprint(output.shape)\n```\n\n***\n\n\n### 28. MobileViT Attention Usage\n\n#### 28.1. Paper\n\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2103.02907)\n\n\n#### 28.2. Overview\n\n![](./model/img/MobileViTAttention.png)\n\n#### 28.3. Usage Code\n\n```python\nfrom model.attention.MobileViTAttention import MobileViTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    m=MobileViTAttention()\n    input=torch.randn(1,3,49,49)\n    output=m(input)\n    print(output.shape)  #output:(1,3,49,49)\n    \n```\n\n***\n\n\n### 29. ParNet Attention Usage\n\n#### 29.1. Paper\n\n[Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\n\n\n#### 29.2. Overview\n\n![](./model/img/ParNet.png)\n\n#### 29.3. Usage Code\n\n```python\nfrom model.attention.ParNetAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,512,7,7)\n    pna = ParNetAttention(channel=512)\n    output=pna(input)\n    print(output.shape) #50,512,7,7\n    \n```\n\n***\n\n\n### 30. UFO Attention Usage\n\n#### 30.1. Paper\n\n[UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2110.07641)\n\n\n#### 30.2. Overview\n\n![](./model/img/UFO.png)\n\n#### 30.3. Usage Code\n\n```python\nfrom model.attention.UFOAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)\n    output=ufo(input,input,input)\n    print(output.shape) #[50, 49, 512]\n    \n```\n\n-\n\n### 31. ACmix Attention Usage\n\n#### 31.1. Paper\n\n[On the Integration of Self-Attention and Convolution](https://arxiv.org/pdf/2111.14556.pdf)\n\n#### 31.2. Usage Code\n\n```python\nfrom model.attention.ACmix import ACmix\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,256,7,7)\n    acmix = ACmix(in_planes=256, out_planes=256)\n    output=acmix(input)\n    print(output.shape)\n    \n```\n\n### 32. MobileViTv2 Attention Usage\n\n#### 32.1. Paper\n\n[Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06](https://arxiv.org/abs/2206.02680)\n\n\n#### 32.2. Overview\n\n![](./model/img/MobileViTv2.png)\n\n#### 32.3. Usage Code\n\n```python\nfrom model.attention.MobileViTv2Attention import MobileViTv2Attention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = MobileViTv2Attention(d_model=512)\n    output=sa(input)\n    print(output.shape)\n    \n```\n\n### 33. DAT Attention Usage\n\n#### 33.1. Paper\n\n[Vision Transformer with Deformable Attention---CVPR2022](https://arxiv.org/abs/2201.00520)\n\n#### 33.2. Usage Code\n\n```python\nfrom model.attention.DAT import DAT\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DAT(\n        img_size=224,\n        patch_size=4,\n        num_classes=1000,\n        expansion=4,\n        dim_stem=96,\n        dims=[96, 192, 384, 768],\n        depths=[2, 2, 6, 2],\n        stage_spec=[['L', 'S'], ['L', 'S'], ['L', 'D', 'L', 'D', 'L', 'D'], ['L', 'D']],\n        heads=[3, 6, 12, 24],\n        window_sizes=[7, 7, 7, 7] ,\n        groups=[-1, -1, 3, 6],\n        use_pes=[False, False, True, True],\n        dwc_pes=[False, False, False, False],\n        strides=[-1, -1, 1, 1],\n        sr_ratios=[-1, -1, -1, -1],\n        offset_range_factor=[-1, -1, 2, 2],\n        no_offs=[False, False, False, False],\n        fixed_pes=[False, False, False, False],\n        use_dwc_mlps=[False, False, False, False],\n        use_conv_patches=False,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.2,\n    )\n    output=model(input)\n    print(output[0].shape)\n    \n```\n\n### 34. CrossFormer Attention Usage\n\n#### 34.1. Paper\n\n[CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION---ICLR 2022](https://arxiv.org/pdf/2108.00154.pdf)\n\n#### 34.2. Usage Code\n\n```python\nfrom model.attention.Crossformer import CrossFormer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CrossFormer(img_size=224,\n        patch_size=[4, 8, 16, 32],\n        in_chans= 3,\n        num_classes=1000,\n        embed_dim=48,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        group_size=[7, 7, 7, 7],\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        ape=False,\n        patch_norm=True,\n        use_checkpoint=False,\n        merge_size=[[2, 4], [2,4], [2, 4]]\n    )\n    output=model(input)\n    print(output.shape)\n    \n```\n\n### 35. MOATransformer Attention Usage\n\n#### 35.1. Paper\n\n[Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903)\n\n#### 35.2. Usage Code\n\n```python\nfrom model.attention.MOATransformer import MOATransformer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = MOATransformer(\n        img_size=224,\n        patch_size=4,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=96,\n        depths=[2, 2, 6],\n        num_heads=[3, 6, 12],\n        window_size=14,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        ape=False,\n        patch_norm=True,\n        use_checkpoint=False\n    )\n    output=model(input)\n    print(output.shape)\n    \n```\n\n### 36. CrissCrossAttention Attention Usage\n\n#### 36.1. Paper\n\n[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n\n#### 36.2. Usage Code\n\n```python\nfrom model.attention.CrissCrossAttention import CrissCrossAttention\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(3, 64, 7, 7)\n    model = CrissCrossAttention(64)\n    outputs = model(input)\n    print(outputs.shape)\n    \n```\n\n### 37. Axial_attention Attention Usage\n\n#### 37.1. Paper\n\n[Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)\n\n#### 37.2. Usage Code\n\n```python\nfrom model.attention.Axial_attention import AxialImageTransformer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(3, 128, 7, 7)\n    model = AxialImageTransformer(\n        dim = 128,\n        depth = 12,\n        reversible = True\n    )\n    outputs = model(input)\n    print(outputs.shape)\n    \n```\n\n***\n\n\n# Backbone Series\n\n- Pytorch implementation of [\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n- Pytorch implementation of [\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\n\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\n\n- Pytorch implementation of [Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer---ArXiv 2021.06.07](https://arxiv.org/abs/2106.03650)\n\n- Pytorch implementation of [ConTNet: Why not use convolution and transformer at the same time?---ArXiv 2021.04.27](https://arxiv.org/abs/2104.13497)\n\n- Pytorch implementation of [Vision Transformers with Hierarchical Attention---ArXiv 2022.06.15](https://arxiv.org/abs/2106.03180)\n\n- Pytorch implementation of [Co-Scale Conv-Attentional Image Transformers---ArXiv 2021.08.26](https://arxiv.org/abs/2104.06399)\n\n- Pytorch implementation of [Conditional Positional Encodings for Vision Transformers](https://arxiv.org/abs/2102.10882)\n\n- Pytorch implementation of [Rethinking Spatial Dimensions of Vision Transformers---ICCV 2021](https://arxiv.org/abs/2103.16302)\n\n- Pytorch implementation of [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification---ICCV 2021](https://arxiv.org/abs/2103.14899)\n\n- Pytorch implementation of [Transformer in Transformer---NeurIPS 2021](https://arxiv.org/abs/2103.00112)\n\n- Pytorch implementation of [DeepViT: Towards Deeper Vision Transformer](https://arxiv.org/abs/2103.11886)\n\n- Pytorch implementation of [Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)\n***\n\n- Pytorch implementation of [ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)\n\n- Pytorch implementation of [Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\n\n- Pytorch implementation of [Going deeper with Image Transformers---ICCV 2021 (Oral)](https://arxiv.org/abs/2103.17239)\n\n- Pytorch implementation of [Training data-efficient image transformers & distillation through attention---ICML 2021](https://arxiv.org/abs/2012.12877)\n\n- Pytorch implementation of [LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\n\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/abs/2106.13112)\n\n- Pytorch implementation of [Container: Context Aggregation Network---NeuIPS 2021](https://arxiv.org/abs/2106.01401)\n\n- Pytorch implementation of [CMT: Convolutional Neural Networks Meet Vision Transformers---CVPR 2022](https://arxiv.org/abs/2107.06263)\n\n- Pytorch implementation of [Vision Transformer with Deformable Attention---CVPR 2022](https://arxiv.org/abs/2201.00520)\n\n\n### 1. ResNet Usage\n#### 1.1. Paper\n[\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n#### 1.2. Overview\n![](./model/img/resnet.png)\n![](./model/img/resnet2.jpg)\n\n#### 1.3. Usage Code\n```python\n\nfrom model.backbone.resnet import ResNet50,ResNet101,ResNet152\nimport torch\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnet50=ResNet50(1000)\n    # resnet101=ResNet101(1000)\n    # resnet152=ResNet152(1000)\n    out=resnet50(input)\n    print(out.shape)\n\n```\n\n\n### 2. ResNeXt Usage\n#### 2.1. Paper\n\n[\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\n\n#### 2.2. Overview\n![](./model/img/resnext.png)\n\n#### 2.3. Usage Code\n```python\n\nfrom model.backbone.resnext import ResNeXt50,ResNeXt101,ResNeXt152\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnext50=ResNeXt50(1000)\n    # resnext101=ResNeXt101(1000)\n    # resnext152=ResNeXt152(1000)\n    out=resnext50(input)\n    print(out.shape)\n\n\n```\n\n\n\n### 3. MobileViT Usage\n#### 3.1. Paper\n\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\n\n#### 3.2. Overview\n![](./model/img/mobileViT.jpg)\n\n#### 3.3. Usage Code\n```python\n\nfrom model.backbone.MobileViT import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n\n    ### mobilevit_xxs\n    mvit_xxs=mobilevit_xxs()\n    out=mvit_xxs(input)\n    print(out.shape)\n\n    ### mobilevit_xs\n    mvit_xs=mobilevit_xs()\n    out=mvit_xs(input)\n    print(out.shape)\n\n\n    ### mobilevit_s\n    mvit_s=mobilevit_s()\n    out=mvit_s(input)\n    print(out.shape)\n\n```\n\n\n\n\n\n### 4. ConvMixer Usage\n#### 4.1. Paper\n[Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\n#### 4.2. Overview\n![](./model/img/ConvMixer.png)\n\n#### 4.3. Usage Code\n```python\n\nfrom model.backbone.ConvMixer import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    x=torch.randn(1,3,224,224)\n    convmixer=ConvMixer(dim=512,depth=12)\n    out=convmixer(x)\n    print(out.shape)  #[1, 1000]\n\n\n```\n\n### 5. ShuffleTransformer Usage\n#### 5.1. Paper\n[Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer](https://arxiv.org/pdf/2106.03650.pdf)\n\n#### 5.2. Usage Code\n```python\n\nfrom model.backbone.ShuffleTransformer import ShuffleTransformer\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    sft = ShuffleTransformer()\n    output=sft(input)\n    print(output.shape)\n\n\n```\n\n### 6. ConTNet Usage\n#### 6.1. Paper\n[ConTNet: Why not use convolution and transformer at the same time?](https://arxiv.org/abs/2104.13497)\n\n#### 6.2. Usage Code\n```python\n\nfrom model.backbone.ConTNet import ConTNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == \"__main__\":\n    model = build_model(use_avgdown=True, relative=True, qkv_bias=True, pre_norm=True)\n    input = torch.randn(1, 3, 224, 224)\n    out = model(input)\n    print(out.shape)\n\n\n```\n\n### 7 HATNet Usage\n#### 7.1. Paper\n[Vision Transformers with Hierarchical Attention](https://arxiv.org/abs/2106.03180)\n\n#### 7.2. Usage Code\n```python\n\nfrom model.backbone.HATNet import HATNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    hat = HATNet(dims=[48, 96, 240, 384], head_dim=48, expansions=[8, 8, 4, 4],\n        grid_sizes=[8, 7, 7, 1], ds_ratios=[8, 4, 2, 1], depths=[2, 2, 6, 3])\n    output=hat(input)\n    print(output.shape)\n\n\n```\n\n### 8 CoaT Usage\n#### 8.1. Paper\n[Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/abs/2104.06399)\n\n#### 8.2. Usage Code\n```python\n\nfrom model.backbone.CoaT import CoaT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CoaT(patch_size=4, embed_dims=[152, 152, 152, 152], serial_depths=[2, 2, 2, 2], parallel_depth=6, num_heads=8, mlp_ratios=[4, 4, 4, 4])\n    output=model(input)\n    print(output.shape) # torch.Size([1, 1000])\n\n```\n\n### 9 PVT Usage\n#### 9.1. Paper\n[PVT v2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/pdf/2106.13797.pdf)\n\n#### 9.2. Usage Code\n```python\n\nfrom model.backbone.PVT import PyramidVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PyramidVisionTransformer(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1])\n    output=model(input)\n    print(output.shape)\n\n```\n\n\n### 10 CPVT Usage\n#### 10.1. Paper\n[Conditional Positional Encodings for Vision Transformers](https://arxiv.org/abs/2102.10882)\n\n#### 10.2. Usage Code\n```python\n\nfrom model.backbone.CPVT import CPVTV2\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CPVTV2(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1])\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 11 PIT Usage\n#### 11.1. Paper\n[Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)\n\n#### 11.2. Usage Code\n```python\n\nfrom model.backbone.PIT import PoolingTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=14,\n        stride=7,\n        base_dims=[64, 64, 64],\n        depth=[3, 6, 4],\n        heads=[4, 8, 16],\n        mlp_ratio=4\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 12 CrossViT Usage\n#### 12.1. Paper\n[CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899)\n\n#### 12.2. Usage Code\n```python\n\nfrom model.backbone.CrossViT import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == \"__main__\":\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        img_size=[240, 224],\n        patch_size=[12, 16], \n        embed_dim=[192, 384], \n        depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],\n        num_heads=[6, 6], \n        mlp_ratio=[4, 4, 1], \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 13 TnT Usage\n#### 13.1. Paper\n[Transformer in Transformer](https://arxiv.org/abs/2103.00112)\n\n#### 13.2. Usage Code\n```python\n\nfrom model.backbone.TnT import TNT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = TNT(\n        img_size=224, \n        patch_size=16, \n        outer_dim=384, \n        inner_dim=24, \n        depth=12,\n        outer_num_heads=6, \n        inner_num_heads=4, \n        qkv_bias=False,\n        inner_stride=4)\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 14 DViT Usage\n#### 14.1. Paper\n[DeepViT: Towards Deeper Vision Transformer](https://arxiv.org/abs/2103.11886)\n\n#### 14.2. Usage Code\n```python\n\nfrom model.backbone.DViT import DeepVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DeepVisionTransformer(\n        patch_size=16, embed_dim=384, \n        depth=[False] * 16, \n        apply_transform=[False] * 0 + [True] * 32, \n        num_heads=12, \n        mlp_ratio=3, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 15 CeiT Usage\n#### 15.1. Paper\n[Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)\n\n#### 15.2. Usage Code\n```python\n\nfrom model.backbone.CeiT import CeIT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CeIT(\n        hybrid_backbone=Image2Tokens(),\n        patch_size=4, \n        embed_dim=192, \n        depth=12, \n        num_heads=3, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 16 ConViT Usage\n#### 16.1. Paper\n[ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)\n\n#### 16.2. Usage Code\n```python\n\nfrom model.backbone.ConViT import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        num_heads=16,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 17 CaiT Usage\n#### 17.1. Paper\n[Going deeper with Image Transformers](https://arxiv.org/abs/2103.17239)\n\n#### 17.2. Usage Code\n```python\n\nfrom model.backbone.CaiT import CaiT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CaiT(\n        img_size= 224,\n        patch_size=16, \n        embed_dim=192, \n        depth=24, \n        num_heads=4, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 18 PatchConvnet Usage\n#### 18.1. Paper\n[Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\n\n#### 18.2. Usage Code\n```python\n\nfrom model.backbone.PatchConvnet import PatchConvnet\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=384,\n        depth=60,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        depth_token_only=1,\n        mlp_ratio_clstk=3.0,\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 19 DeiT Usage\n#### 19.1. Paper\n[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n\n#### 19.2. Usage Code\n```python\n\nfrom model.backbone.DeiT import DistilledVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DistilledVisionTransformer(\n        patch_size=16, \n        embed_dim=384, \n        depth=12, \n        num_heads=6, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 20 LeViT Usage\n#### 20.1. Paper\n[LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\n\n#### 20.2. Usage Code\n```python\n\nfrom model.backbone.LeViT import *\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    for name in specification:\n        input=torch.randn(1,3,224,224)\n        model = globals()[name](fuse=True, pretrained=False)\n        model.eval()\n        output = model(input)\n        print(output.shape)\n\n```\n\n### 21 VOLO Usage\n#### 21.1. Paper\n[VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/abs/2106.13112)\n\n#### 21.2. Usage Code\n```python\n\nfrom model.backbone.VOLO import VOLO\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VOLO([4, 4, 8, 2],\n                 embed_dims=[192, 384, 384, 384],\n                 num_heads=[6, 12, 12, 12],\n                 mlp_ratios=[3, 3, 3, 3],\n                 downsamples=[True, False, False, False],\n                 outlook_attention=[True, False, False, False ],\n                 post_layers=['ca', 'ca'],\n                 )\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 22 Container Usage\n#### 22.1. Paper\n[Container: Context Aggregation Network](https://arxiv.org/abs/2106.01401)\n\n#### 22.2. Usage Code\n```python\n\nfrom model.backbone.Container import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        img_size=[224, 56, 28, 14], \n        patch_size=[4, 2, 2, 2], \n        embed_dim=[64, 128, 320, 512], \n        depth=[3, 4, 8, 3], \n        num_heads=16, \n        mlp_ratio=[8, 8, 4, 4], \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6))\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 23 CMT Usage\n#### 23.1. Paper\n[CMT: Convolutional Neural Networks Meet Vision Transformers](https://arxiv.org/abs/2107.06263)\n\n#### 23.2. Usage Code\n```python\n\nfrom model.backbone.CMT import CMT_Tiny\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CMT_Tiny()\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n\n\n\n\n\n# MLP Series\n\n- Pytorch implementation of [\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition---arXiv 2021.05.05\"](https://arxiv.org/pdf/2105.01883v1.pdf)\n\n- Pytorch implementation of [\"MLP-Mixer: An all-MLP Architecture for Vision---arXiv 2021.05.17\"](https://arxiv.org/pdf/2105.01601.pdf)\n\n- Pytorch implementation of [\"ResMLP: Feedforward networks for image classification with data-efficient training---arXiv 2021.05.07\"](https://arxiv.org/pdf/2105.03404.pdf)\n\n- Pytorch implementation of [\"Pay Attention to MLPs---arXiv 2021.05.17\"](https://arxiv.org/abs/2105.08050)\n\n\n- Pytorch implementation of [\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?---arXiv 2021.09.12\"](https://arxiv.org/abs/2109.05422)\n\n### 1. RepMLP Usage\n#### 1.1. Paper\n[\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition\"](https://arxiv.org/pdf/2105.01883v1.pdf)\n\n#### 1.2. Overview\n![](./model/img/repmlp.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.mlp.repmlp import RepMLP\nimport torch\nfrom torch import nn\n\nN=4 #batch size\nC=512 #input dim\nO=1024 #output dim\nH=14 #image height\nW=14 #image width\nh=7 #patch height\nw=7 #patch width\nfc1_fc2_reduction=1 #reduction ratio\nfc3_groups=8 # groups\nrepconv_kernels=[1,3,5,7] #kernel list\nrepmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels)\nx=torch.randn(N,C,H,W)\nrepmlp.eval()\nfor module in repmlp.modules():\n    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n        nn.init.uniform_(module.running_mean, 0, 0.1)\n        nn.init.uniform_(module.running_var, 0, 0.1)\n        nn.init.uniform_(module.weight, 0, 0.1)\n        nn.init.uniform_(module.bias, 0, 0.1)\n\n#training result\nout=repmlp(x)\n#inference result\nrepmlp.switch_to_deploy()\ndeployout = repmlp(x)\n\nprint(((deployout-out)**2).sum())\n```\n\n### 2. MLP-Mixer Usage\n#### 2.1. Paper\n[\"MLP-Mixer: An all-MLP Architecture for Vision\"](https://arxiv.org/pdf/2105.01601.pdf)\n\n#### 2.2. Overview\n![](./model/img/mlpmixer.png)\n\n#### 2.3. Usage Code\n```python\nfrom model.mlp.mlp_mixer import MlpMixer\nimport torch\nmlp_mixer=MlpMixer(num_classes=1000,num_blocks=10,patch_size=10,tokens_hidden_dim=32,channels_hidden_dim=1024,tokens_mlp_dim=16,channels_mlp_dim=1024)\ninput=torch.randn(50,3,40,40)\noutput=mlp_mixer(input)\nprint(output.shape)\n```\n\n***\n\n### 3. ResMLP Usage\n#### 3.1. Paper\n[\"ResMLP: Feedforward networks for image classification with data-efficient training\"](https://arxiv.org/pdf/2105.03404.pdf)\n\n#### 3.2. Overview\n![](./model/img/resmlp.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.mlp.resmlp import ResMLP\nimport torch\n\ninput=torch.randn(50,3,14,14)\nresmlp=ResMLP(dim=128,image_size=14,patch_size=7,class_num=1000)\nout=resmlp(input)\nprint(out.shape) #the last dimention is class_num\n```\n\n***\n\n### 4. gMLP Usage\n#### 4.1. Paper\n[\"Pay Attention to MLPs\"](https://arxiv.org/abs/2105.08050)\n\n#### 4.2. Overview\n![](./model/img/gMLP.jpg)\n\n#### 4.3. Usage Code\n```python\nfrom model.mlp.g_mlp import gMLP\nimport torch\n\nnum_tokens=10000\nbs=50\nlen_sen=49\nnum_layers=6\ninput=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen\ngmlp = gMLP(num_tokens=num_tokens,len_sen=len_sen,dim=512,d_ff=1024)\noutput=gmlp(input)\nprint(output.shape)\n```\n\n***\n\n### 5. sMLP Usage\n#### 5.1. Paper\n[\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?\"](https://arxiv.org/abs/2109.05422)\n\n#### 5.2. Overview\n![](./model/img/sMLP.jpg)\n\n#### 5.3. Usage Code\n```python\nfrom model.mlp.sMLP_block import sMLPBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    smlp=sMLPBlock(h=224,w=224)\n    out=smlp(input)\n    print(out.shape)\n```\n\n### 6. vip-mlp Usage\n#### 6.1. Paper\n[\"Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\n\n#### 6.2. Usage Code\n```python\nfrom model.mlp.vip-mlp import VisionPermutator\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionPermutator(\n        layers=[4, 3, 8, 3], \n        embed_dims=[384, 384, 384, 384], \n        patch_size=14, \n        transitions=[False, False, False, False],\n        segment_dim=[16, 16, 16, 16], \n        mlp_ratios=[3, 3, 3, 3], \n        mlp_fn=WeightedPermuteMLP\n    )\n    output=model(input)\n    print(output.shape)\n```\n\n\n# Re-Parameter Series\n\n- Pytorch implementation of [\"RepVGG: Making VGG-style ConvNets Great Again---CVPR2021\"](https://arxiv.org/abs/2101.03697)\n\n- Pytorch implementation of [\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks---ICCV2019\"](https://arxiv.org/abs/1908.03930)\n\n- Pytorch implementation of [\"Diverse Branch Block: Building a Convolution as an Inception-like Unit---CVPR2021\"](https://arxiv.org/abs/2103.13425)\n\n\n***\n\n### 1. RepVGG Usage\n#### 1.1. Paper\n[\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/abs/2101.03697)\n\n#### 1.2. Overview\n![](./model/img/repvgg.png)\n\n#### 1.3. Usage Code\n```python\n\nfrom model.rep.repvgg import RepBlock\nimport torch\n\n\ninput=torch.randn(50,512,49,49)\nrepblock=RepBlock(512,512)\nrepblock.eval()\nout=repblock(input)\nrepblock._switch_to_deploy()\nout2=repblock(input)\nprint('difference between vgg and repvgg')\nprint(((out2-out)**2).sum())\n```\n\n\n\n***\n\n### 2. ACNet Usage\n#### 2.1. Paper\n[\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks\"](https://arxiv.org/abs/1908.03930)\n\n#### 2.2. Overview\n![](./model/img/acnet.png)\n\n#### 2.3. Usage Code\n```python\nfrom model.rep.acnet import ACNet\nimport torch\nfrom torch import nn\n\ninput=torch.randn(50,512,49,49)\nacnet=ACNet(512,512)\nacnet.eval()\nout=acnet(input)\nacnet._switch_to_deploy()\nout2=acnet(input)\nprint('difference:')\nprint(((out2-out)**2).sum())\n\n```\n\n\n\n***\n\n### 2. Diverse Branch Block Usage\n#### 2.1. Paper\n[\"Diverse Branch Block: Building a Convolution as an Inception-like Unit\"](https://arxiv.org/abs/2103.13425)\n\n#### 2.2. Overview\n![](./model/img/ddb.png)\n\n#### 2.3. Usage Code\n##### 2.3.1 Transform I\n```python\nfrom model.rep.ddb import transI_conv_bn\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n#conv+bn\nconv1=nn.Conv2d(64,64,3,padding=1)\nbn1=nn.BatchNorm2d(64)\nbn1.eval()\nout1=bn1(conv1(input))\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.2 Transform II\n```python\nfrom model.rep.ddb import transII_conv_branch\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,64,3,padding=1)\nconv2=nn.Conv2d(64,64,3,padding=1)\nout1=conv1(input)+conv2(input)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.3 Transform III\n```python\nfrom model.rep.ddb import transIII_conv_sequential\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,64,1,padding=0,bias=False)\nconv2=nn.Conv2d(64,64,3,padding=1,bias=False)\nout1=conv2(conv1(input))\n\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1,bias=False)\nconv_fuse.weight.data=transIII_conv_sequential(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.4 Transform IV\n```python\nfrom model.rep.ddb import transIV_conv_concat\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,32,3,padding=1)\nconv2=nn.Conv2d(64,32,3,padding=1)\nout1=torch.cat([conv1(input),conv2(input)],dim=1)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.5 Transform V\n```python\nfrom model.rep.ddb import transV_avg\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\navg=nn.AvgPool2d(kernel_size=3,stride=1)\nout1=avg(input)\n\nconv=transV_avg(64,3)\nout2=conv(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n\n##### 2.3.6 Transform VI\n```python\nfrom model.rep.ddb import transVI_conv_scale\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1x1=nn.Conv2d(64,64,1)\nconv1x3=nn.Conv2d(64,64,(1,3),padding=(0,1))\nconv3x1=nn.Conv2d(64,64,(3,1),padding=(1,0))\nout1=conv1x1(input)+conv1x3(input)+conv3x1(input)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n\n\n\n\n# Convolution Series\n\n- Pytorch implementation of [\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications---CVPR2017\"](https://arxiv.org/abs/1704.04861)\n\n- Pytorch implementation of [\"Efficientnet: Rethinking model scaling for convolutional neural networks---PMLR2019\"](http://proceedings.mlr.press/v97/tan19a.html)\n\n- Pytorch implementation of [\"Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021\"](https://arxiv.org/abs/2103.06255)\n\n- Pytorch implementation of [\"Dynamic Convolution: Attention over Convolution Kernels---CVPR2020 Oral\"](https://arxiv.org/abs/1912.03458)\n\n- Pytorch implementation of [\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference---NeurIPS2019\"](https://arxiv.org/abs/1904.04971)\n\n***\n\n### 1. Depthwise Separable Convolution Usage\n#### 1.1. Paper\n[\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"](https://arxiv.org/abs/1704.04861)\n\n#### 1.2. Overview\n![](./model/img/DepthwiseSeparableConv.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\ndsconv=DepthwiseSeparableConvolution(3,64)\nout=dsconv(input)\nprint(out.shape)\n```\n\n***\n\n\n### 2. MBConv Usage\n#### 2.1. Paper\n[\"Efficientnet: Rethinking model scaling for convolutional neural networks\"](http://proceedings.mlr.press/v97/tan19a.html)\n\n#### 2.2. Overview\n![](./model/img/MBConv.jpg)\n\n#### 2.3. Usage Code\n```python\nfrom model.conv.MBConv import MBConvBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=512,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n\n```\n\n***\n\n\n### 3. Involution Usage\n#### 3.1. Paper\n[\"Involution: Inverting the Inherence of Convolution for Visual Recognition\"](https://arxiv.org/abs/2103.06255)\n\n#### 3.2. Overview\n![](./model/img/Involution.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.conv.Involution import Involution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,4,64,64)\ninvolution=Involution(kernel_size=3,in_channel=4,stride=2)\nout=involution(input)\nprint(out.shape)\n```\n\n***\n\n\n### 4. DynamicConv Usage\n#### 4.1. Paper\n[\"Dynamic Convolution: Attention over Convolution Kernels\"](https://arxiv.org/abs/1912.03458)\n\n#### 4.2. Overview\n![](./model/img/DynamicConv.png)\n\n#### 4.3. Usage Code\n```python\nfrom model.conv.DynamicConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=DynamicConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape) # 2,32,64,64\n\n```\n\n***\n\n\n### 5. CondConv Usage\n#### 5.1. Paper\n[\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference\"](https://arxiv.org/abs/1904.04971)\n\n#### 5.2. Overview\n![](./model/img/CondConv.png)\n\n#### 5.3. Usage Code\n```python\nfrom model.conv.CondConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=CondConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape)\n\n```\n\n***\n"
        },
        {
          "name": "README_pip.md",
          "type": "blob",
          "size": 59.1982421875,
          "content": "## pip使用文档\n\n### 安装\n\n 直接通过 pip 安装，可直接在其他任务中使用\n\n  ```shell\n  pip install fightingcv-attention\n  ```\n\n### 演示\n\n#### 使用 pip 方式\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# 使用 pip 方式\n\nfrom fightingcv_attention.attention.MobileViTv2Attention import *\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = MobileViTv2Attention(d_model=512)\n    output=sa(input)\n    print(output.shape)\n```\n\n## pip包 fightingcv-attention 包含以下模块\n\n# 目录\n\n- [Attention Series](#attention-series)\n    - [1. External Attention Usage](#1-external-attention-usage)\n\n    - [2. Self Attention Usage](#2-self-attention-usage)\n\n    - [3. Simplified Self Attention Usage](#3-simplified-self-attention-usage)\n\n    - [4. Squeeze-and-Excitation Attention Usage](#4-squeeze-and-excitation-attention-usage)\n\n    - [5. SK Attention Usage](#5-sk-attention-usage)\n\n    - [6. CBAM Attention Usage](#6-cbam-attention-usage)\n\n    - [7. BAM Attention Usage](#7-bam-attention-usage)\n    \n    - [8. ECA Attention Usage](#8-eca-attention-usage)\n\n    - [9. DANet Attention Usage](#9-danet-attention-usage)\n\n    - [10. Pyramid Split Attention (PSA) Usage](#10-Pyramid-Split-Attention-Usage)\n\n    - [11. Efficient Multi-Head Self-Attention(EMSA) Usage](#11-Efficient-Multi-Head-Self-Attention-Usage)\n\n    - [12. Shuffle Attention Usage](#12-Shuffle-Attention-Usage)\n    \n    - [13. MUSE Attention Usage](#13-MUSE-Attention-Usage)\n  \n    - [14. SGE Attention Usage](#14-SGE-Attention-Usage)\n\n    - [15. A2 Attention Usage](#15-A2-Attention-Usage)\n\n    - [16. AFT Attention Usage](#16-AFT-Attention-Usage)\n\n    - [17. Outlook Attention Usage](#17-Outlook-Attention-Usage)\n\n    - [18. ViP Attention Usage](#18-ViP-Attention-Usage)\n\n    - [19. CoAtNet Attention Usage](#19-CoAtNet-Attention-Usage)\n\n    - [20. HaloNet Attention Usage](#20-HaloNet-Attention-Usage)\n\n    - [21. Polarized Self-Attention Usage](#21-Polarized-Self-Attention-Usage)\n\n    - [22. CoTAttention Usage](#22-CoTAttention-Usage)\n\n    - [23. Residual Attention Usage](#23-Residual-Attention-Usage)\n  \n    - [24. S2 Attention Usage](#24-S2-Attention-Usage)\n\n    - [25. GFNet Attention Usage](#25-GFNet-Attention-Usage)\n\n    - [26. Triplet Attention Usage](#26-TripletAttention-Usage)\n\n    - [27. Coordinate Attention Usage](#27-Coordinate-Attention-Usage)\n\n    - [28. MobileViT Attention Usage](#28-MobileViT-Attention-Usage)\n\n    - [29. ParNet Attention Usage](#29-ParNet-Attention-Usage)\n\n    - [30. UFO Attention Usage](#30-UFO-Attention-Usage)\n\n    - [31. ACmix Attention Usage](#31-Acmix-Attention-Usage)\n  \n    - [32. MobileViTv2 Attention Usage](#32-MobileViTv2-Attention-Usage)\n\n    - [33. DAT Attention Usage](#33-DAT-Attention-Usage)\n\n    - [34. CrossFormer Attention Usage](#34-CrossFormer-Attention-Usage)\n\n    - [35. MOATransformer Attention Usage](#35-MOATransformer-Attention-Usage)\n\n    - [36. CrissCrossAttention Attention Usage](#36-CrissCrossAttention-Attention-Usage)\n\n    - [37. Axial_attention Attention Usage](#37-Axial_attention-Attention-Usage)\n\n- [Backbone Series](#Backbone-series)\n\n    - [1. ResNet Usage](#1-ResNet-Usage)\n\n    - [2. ResNeXt Usage](#2-ResNeXt-Usage)\n\n    - [3. MobileViT Usage](#3-MobileViT-Usage)\n\n    - [4. ConvMixer Usage](#4-ConvMixer-Usage)\n\n    - [5. ShuffleTransformer Usage](#5-ShuffleTransformer-Usage)\n\n    - [6. ConTNet Usage](#6-ConTNet-Usage)\n\n    - [7. HATNet Usage](#7-HATNet-Usage)\n\n    - [8. CoaT Usage](#8-CoaT-Usage)\n\n    - [9. PVT Usage](#9-PVT-Usage)\n\n    - [10. CPVT Usage](#10-CPVT-Usage)\n\n    - [11. PIT Usage](#11-PIT-Usage)\n\n    - [12. CrossViT Usage](#12-CrossViT-Usage)\n\n    - [13. TnT Usage](#13-TnT-Usage)\n\n    - [14. DViT Usage](#14-DViT-Usage)\n\n    - [15. CeiT Usage](#15-CeiT-Usage)\n\n    - [16. ConViT Usage](#16-ConViT-Usage)\n\n    - [17. CaiT Usage](#17-CaiT-Usage)\n\n    - [18. PatchConvnet Usage](#18-PatchConvnet-Usage)\n\n    - [19. DeiT Usage](#19-DeiT-Usage)\n\n    - [20. LeViT Usage](#20-LeViT-Usage)\n\n    - [21. VOLO Usage](#21-VOLO-Usage)\n    \n    - [22. Container Usage](#22-Container-Usage)\n\n    - [23. CMT Usage](#23-CMT-Usage)\n\n    - [24. EfficientFormer Usage](#24-EfficientFormer-Usage)\n\n\n- [MLP Series](#mlp-series)\n\n    - [1. RepMLP Usage](#1-RepMLP-Usage)\n\n    - [2. MLP-Mixer Usage](#2-MLP-Mixer-Usage)\n\n    - [3. ResMLP Usage](#3-ResMLP-Usage)\n\n    - [4. gMLP Usage](#4-gMLP-Usage)\n\n    - [5. sMLP Usage](#5-sMLP-Usage)\n\n    - [6. vip-mlp Usage](#6-vip-mlp-Usage)\n\n- [Re-Parameter(ReP) Series](#Re-Parameter-series)\n\n    - [1. RepVGG Usage](#1-RepVGG-Usage)\n\n    - [2. ACNet Usage](#2-ACNet-Usage)\n\n    - [3. Diverse Branch Block(DDB) Usage](#3-Diverse-Branch-Block-Usage)\n\n- [Convolution Series](#Convolution-series)\n\n    - [1. Depthwise Separable Convolution Usage](#1-Depthwise-Separable-Convolution-Usage)\n\n    - [2. MBConv Usage](#2-MBConv-Usage)\n\n    - [3. Involution Usage](#3-Involution-Usage)\n\n    - [4. DynamicConv Usage](#4-DynamicConv-Usage)\n\n    - [5. CondConv Usage](#5-CondConv-Usage)\n\n***\n\n\n\n# Attention Series\n\n- Pytorch implementation of [\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks---arXiv 2021.05.05\"](https://arxiv.org/abs/2105.02358)\n\n- Pytorch implementation of [\"Attention Is All You Need---NIPS2017\"](https://arxiv.org/pdf/1706.03762.pdf)\n\n- Pytorch implementation of [\"Squeeze-and-Excitation Networks---CVPR2018\"](https://arxiv.org/abs/1709.01507)\n\n- Pytorch implementation of [\"Selective Kernel Networks---CVPR2019\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n- Pytorch implementation of [\"CBAM: Convolutional Block Attention Module---ECCV2018\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n\n- Pytorch implementation of [\"BAM: Bottleneck Attention Module---BMCV2018\"](https://arxiv.org/pdf/1807.06514.pdf)\n\n- Pytorch implementation of [\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020\"](https://arxiv.org/pdf/1910.03151.pdf)\n\n- Pytorch implementation of [\"Dual Attention Network for Scene Segmentation---CVPR2019\"](https://arxiv.org/pdf/1809.02983.pdf)\n\n- Pytorch implementation of [\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30\"](https://arxiv.org/pdf/2105.14447.pdf)\n\n- Pytorch implementation of [\"ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28\"](https://arxiv.org/abs/2105.13677)\n\n- Pytorch implementation of [\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021\"](https://arxiv.org/pdf/2102.00240.pdf)\n\n- Pytorch implementation of [\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17\"](https://arxiv.org/abs/1911.09483)\n\n- Pytorch implementation of [\"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23\"](https://arxiv.org/pdf/1905.09646.pdf)\n\n- Pytorch implementation of [\"A2-Nets: Double Attention Networks---NIPS2018\"](https://arxiv.org/pdf/1810.11579.pdf)\n\n\n- Pytorch implementation of [\"An Attention Free Transformer---ICLR2021 (Apple New Work)\"](https://arxiv.org/pdf/2105.14103v1.pdf)\n\n\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24\"](https://arxiv.org/abs/2106.13112) \n  [【论文解析】](https://zhuanlan.zhihu.com/p/385561050)\n\n\n- Pytorch implementation of [Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23](https://arxiv.org/abs/2106.12368) \n  [【论文解析】](https://mp.weixin.qq.com/s/5gonUQgBho_m2O54jyXF_Q)\n\n\n- Pytorch implementation of [CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09](https://arxiv.org/abs/2106.04803) \n  [【论文解析】](https://zhuanlan.zhihu.com/p/385578588)\n\n\n- Pytorch implementation of [Scaling Local Self-Attention for Parameter Efficient Visual Backbones---CVPR2021 Oral](https://arxiv.org/pdf/2103.12731.pdf)  [【论文解析】](https://zhuanlan.zhihu.com/p/388598744)\n\n\n\n- Pytorch implementation of [Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02](https://arxiv.org/abs/2107.00782)  [【论文解析】](https://zhuanlan.zhihu.com/p/389770482) \n\n\n- Pytorch implementation of [Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292)  [【论文解析】](https://zhuanlan.zhihu.com/p/394795481) \n\n\n- Pytorch implementation of [Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \n\n\n- Pytorch implementation of [S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) [【论文解析】](https://zhuanlan.zhihu.com/p/397003638) \n\n- Pytorch implementation of [Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \n\n- Pytorch implementation of [Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021](https://arxiv.org/abs/2010.03045) \n\n- Pytorch implementation of [Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2110.02178)\n\n- Pytorch implementation of [Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\n\n- Pytorch implementation of [UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2109.14382)\n\n- Pytorch implementation of [Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06](https://arxiv.org/abs/2206.02680)\n\n- Pytorch implementation of [On the Integration of Self-Attention and Convolution---ArXiv 2022.03.14](https://arxiv.org/pdf/2111.14556.pdf)\n\n- Pytorch implementation of [CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION---ICLR 2022](https://arxiv.org/pdf/2108.00154.pdf)\n\n- Pytorch implementation of [Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903)\n\n- Pytorch implementation of [CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n\n- Pytorch implementation of [Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)\n***\n\n\n### 1. External Attention Usage\n#### 1.1. Paper\n[\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\"](https://arxiv.org/abs/2105.02358)\n\n#### 1.2. Overview\n![](./model/img/External_Attention.png)\n\n#### 1.3. Usage Code\n```python\nfrom fightingcv_attention.attention.ExternalAttention import ExternalAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nea = ExternalAttention(d_model=512,S=8)\noutput=ea(input)\nprint(output.shape)\n```\n\n***\n\n\n### 2. Self Attention Usage\n#### 2.1. Paper\n[\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf)\n\n#### 1.2. Overview\n![](./model/img/SA.png)\n\n#### 1.3. Usage Code\n```python\nfrom fightingcv_attention.attention.SelfAttention import ScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nsa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n```\n\n***\n\n### 3. Simplified Self Attention Usage\n#### 3.1. Paper\n[None]()\n\n#### 3.2. Overview\n![](./model/img/SSA.png)\n\n#### 3.3. Usage Code\n```python\nfrom fightingcv_attention.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\noutput=ssa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n### 4. Squeeze-and-Excitation Attention Usage\n#### 4.1. Paper\n[\"Squeeze-and-Excitation Networks\"](https://arxiv.org/abs/1709.01507)\n\n#### 4.2. Overview\n![](./model/img/SE.png)\n\n#### 4.3. Usage Code\n```python\nfrom fightingcv_attention.attention.SEAttention import SEAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SEAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n\n***\n\n### 5. SK Attention Usage\n#### 5.1. Paper\n[\"Selective Kernel Networks\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n#### 5.2. Overview\n![](./model/img/SK.png)\n\n#### 5.3. Usage Code\n```python\nfrom fightingcv_attention.attention.SKAttention import SKAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SKAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n***\n\n### 6. CBAM Attention Usage\n#### 6.1. Paper\n[\"CBAM: Convolutional Block Attention Module\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n\n#### 6.2. Overview\n![](./model/img/CBAM1.png)\n\n![](./model/img/CBAM2.png)\n\n#### 6.3. Usage Code\n```python\nfrom fightingcv_attention.attention.CBAM import CBAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nkernel_size=input.shape[2]\ncbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)\noutput=cbam(input)\nprint(output.shape)\n\n```\n\n***\n\n### 7. BAM Attention Usage\n#### 7.1. Paper\n[\"BAM: Bottleneck Attention Module\"](https://arxiv.org/pdf/1807.06514.pdf)\n\n#### 7.2. Overview\n![](./model/img/BAM.png)\n\n#### 7.3. Usage Code\n```python\nfrom fightingcv_attention.attention.BAM import BAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nbam = BAMBlock(channel=512,reduction=16,dia_val=2)\noutput=bam(input)\nprint(output.shape)\n\n```\n\n***\n\n### 8. ECA Attention Usage\n#### 8.1. Paper\n[\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\"](https://arxiv.org/pdf/1910.03151.pdf)\n\n#### 8.2. Overview\n![](./model/img/ECA.png)\n\n#### 8.3. Usage Code\n```python\nfrom fightingcv_attention.attention.ECAAttention import ECAAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\neca = ECAAttention(kernel_size=3)\noutput=eca(input)\nprint(output.shape)\n\n```\n\n***\n\n### 9. DANet Attention Usage\n#### 9.1. Paper\n[\"Dual Attention Network for Scene Segmentation\"](https://arxiv.org/pdf/1809.02983.pdf)\n\n#### 9.2. Overview\n![](./model/img/danet.png)\n\n#### 9.3. Usage Code\n```python\nfrom fightingcv_attention.attention.DANet import DAModule\nimport torch\n\ninput=torch.randn(50,512,7,7)\ndanet=DAModule(d_model=512,kernel_size=3,H=7,W=7)\nprint(danet(input).shape)\n\n```\n\n***\n\n### 10. Pyramid Split Attention Usage\n\n#### 10.1. Paper\n[\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network\"](https://arxiv.org/pdf/2105.14447.pdf)\n\n#### 10.2. Overview\n![](./model/img/psa.png)\n\n#### 10.3. Usage Code\n```python\nfrom fightingcv_attention.attention.PSA import PSA\nimport torch\n\ninput=torch.randn(50,512,7,7)\npsa = PSA(channel=512,reduction=8)\noutput=psa(input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 11. Efficient Multi-Head Self-Attention Usage\n\n#### 11.1. Paper\n[\"ResT: An Efficient Transformer for Visual Recognition\"](https://arxiv.org/abs/2105.13677)\n\n#### 11.2. Overview\n![](./model/img/EMSA.png)\n\n#### 11.3. Usage Code\n```python\n\nfrom fightingcv_attention.attention.EMSA import EMSA\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,64,512)\nemsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)\noutput=emsa(input,input,input)\nprint(output.shape)\n    \n```\n\n***\n\n\n### 12. Shuffle Attention Usage\n\n#### 12.1. Paper\n[\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS\"](https://arxiv.org/pdf/2102.00240.pdf)\n\n#### 12.2. Overview\n![](./model/img/ShuffleAttention.jpg)\n\n#### 12.3. Usage Code\n```python\n\nfrom fightingcv_attention.attention.ShuffleAttention import ShuffleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,512,7,7)\nse = ShuffleAttention(channel=512,G=8)\noutput=se(input)\nprint(output.shape)\n\n    \n```\n\n\n***\n\n\n### 13. MUSE Attention Usage\n\n#### 13.1. Paper\n[\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\"](https://arxiv.org/abs/1911.09483)\n\n#### 13.2. Overview\n![](./model/img/MUSE.png)\n\n#### 13.3. Usage Code\n```python\nfrom fightingcv_attention.attention.MUSEAttention import MUSEAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,49,512)\nsa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 14. SGE Attention Usage\n\n#### 14.1. Paper\n[Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks](https://arxiv.org/pdf/1905.09646.pdf)\n\n#### 14.2. Overview\n![](./model/img/SGE.png)\n\n#### 14.3. Usage Code\n```python\nfrom fightingcv_attention.attention.SGE import SpatialGroupEnhance\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nsge = SpatialGroupEnhance(groups=8)\noutput=sge(input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 15. A2 Attention Usage\n\n#### 15.1. Paper\n[A2-Nets: Double Attention Networks](https://arxiv.org/pdf/1810.11579.pdf)\n\n#### 15.2. Overview\n![](./model/img/A2.png)\n\n#### 15.3. Usage Code\n```python\nfrom fightingcv_attention.attention.A2Atttention import DoubleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\na2 = DoubleAttention(512,128,128,True)\noutput=a2(input)\nprint(output.shape)\n\n```\n\n\n\n### 16. AFT Attention Usage\n\n#### 16.1. Paper\n[An Attention Free Transformer](https://arxiv.org/pdf/2105.14103v1.pdf)\n\n#### 16.2. Overview\n![](./model/img/AFT.jpg)\n\n#### 16.3. Usage Code\n```python\nfrom fightingcv_attention.attention.AFT import AFT_FULL\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,49,512)\naft_full = AFT_FULL(d_model=512, n=49)\noutput=aft_full(input)\nprint(output.shape)\n\n```\n\n\n\n\n\n\n### 17. Outlook Attention Usage\n\n#### 17.1. Paper\n\n\n[VOLO: Vision Outlooker for Visual Recognition\"](https://arxiv.org/abs/2106.13112)\n\n\n#### 17.2. Overview\n![](./model/img/OutlookAttention.png)\n\n#### 17.3. Usage Code\n```python\nfrom fightingcv_attention.attention.OutlookAttention import OutlookAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,28,28,512)\noutlook = OutlookAttention(dim=512)\noutput=outlook(input)\nprint(output.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n### 18. ViP Attention Usage\n\n#### 18.1. Paper\n\n\n[Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\n\n\n#### 18.2. Overview\n![](./model/img/ViP.png)\n\n#### 18.3. Usage Code\n```python\n\nfrom fightingcv_attention.attention.ViP import WeightedPermuteMLP\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(64,8,8,512)\nseg_dim=8\nvip=WeightedPermuteMLP(512,seg_dim)\nout=vip(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n### 19. CoAtNet Attention Usage\n\n#### 19.1. Paper\n\n\n[CoAtNet: Marrying Convolution and Attention for All Data Sizes\"](https://arxiv.org/abs/2106.04803) \n\n\n#### 19.2. Overview\nNone\n\n\n#### 19.3. Usage Code\n```python\n\nfrom fightingcv_attention.attention.CoAtNet import CoAtNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=CoAtNet(in_ch=3,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n### 20. HaloNet Attention Usage\n\n#### 20.1. Paper\n\n\n[Scaling Local Self-Attention for Parameter Efficient Visual Backbones\"](https://arxiv.org/pdf/2103.12731.pdf) \n\n\n#### 20.2. Overview\n\n![](./model/img/HaloNet.png)\n\n#### 20.3. Usage Code\n```python\n\nfrom fightingcv_attention.attention.HaloAttention import HaloAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,8,8)\nhalo = HaloAttention(dim=512,\n    block_size=2,\n    halo_size=1,)\noutput=halo(input)\nprint(output.shape)\n\n```\n\n\n***\n\n### 21. Polarized Self-Attention Usage\n\n#### 21.1. Paper\n\n[Polarized Self-Attention: Towards High-quality Pixel-wise Regression\"](https://arxiv.org/abs/2107.00782)  \n\n\n#### 21.2. Overview\n\n![](./model/img/PoSA.png)\n\n#### 21.3. Usage Code\n```python\n\nfrom fightingcv_attention.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,7,7)\npsa = SequentialPolarizedSelfAttention(channel=512)\noutput=psa(input)\nprint(output.shape)\n\n\n```\n\n\n***\n\n\n### 22. CoTAttention Usage\n\n#### 22.1. Paper\n\n[Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292) \n\n\n#### 22.2. Overview\n\n![](./model/img/CoT.png)\n\n#### 22.3. Usage Code\n```python\n\nfrom fightingcv_attention.attention.CoTAttention import CoTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ncot = CoTAttention(dim=512,kernel_size=3)\noutput=cot(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n### 23. Residual Attention Usage\n\n#### 23.1. Paper\n\n[Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \n\n\n#### 23.2. Overview\n\n![](./model/img/ResAtt.png)\n\n#### 23.3. Usage Code\n```python\n\nfrom fightingcv_attention.attention.ResidualAttention import ResidualAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nresatt = ResidualAttention(channel=512,num_class=1000,la=0.2)\noutput=resatt(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n\n### 24. S2 Attention Usage\n\n#### 24.1. Paper\n\n[S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) \n\n\n#### 24.2. Overview\n\n![](./model/img/S2Attention.png)\n\n#### 24.3. Usage Code\n```python\nfrom fightingcv_attention.attention.S2Attention import S2Attention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ns2att = S2Attention(channels=512)\noutput=s2att(input)\nprint(output.shape)\n\n```\n\n***\n\n\n\n### 25. GFNet Attention Usage\n\n#### 25.1. Paper\n\n[Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \n\n\n#### 25.2. Overview\n\n![](./model/img/GFNet.jpg)\n\n#### 25.3. Usage Code - Implemented by [Wenliang Zhao (Author)](https://scholar.google.com/citations?user=lyPWvuEAAAAJ&hl=en)\n\n```python\nfrom fightingcv_attention.attention.gfnet import GFNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nx = torch.randn(1, 3, 224, 224)\ngfnet = GFNet(embed_dim=384, img_size=224, patch_size=16, num_classes=1000)\nout = gfnet(x)\nprint(out.shape)\n\n```\n\n***\n\n\n### 26. TripletAttention Usage\n\n#### 26.1. Paper\n\n[Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021](https://arxiv.org/abs/2010.03045) \n\n#### 26.2. Overview\n\n![](./model/img/triplet.png)\n\n#### 26.3. Usage Code - Implemented by [digantamisra98](https://github.com/digantamisra98)\n\n```python\nfrom fightingcv_attention.attention.TripletAttention import TripletAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\ninput=torch.randn(50,512,7,7)\ntriplet = TripletAttention()\noutput=triplet(input)\nprint(output.shape)\n```\n\n\n***\n\n\n### 27. Coordinate Attention Usage\n\n#### 27.1. Paper\n\n[Coordinate Attention for Efficient Mobile Network Design---CVPR 2021](https://arxiv.org/abs/2103.02907)\n\n\n#### 27.2. Overview\n\n![](./model/img/CoordAttention.png)\n\n#### 27.3. Usage Code - Implemented by [Andrew-Qibin](https://github.com/Andrew-Qibin)\n\n```python\nfrom fightingcv_attention.attention.CoordAttention import CoordAtt\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninp=torch.rand([2, 96, 56, 56])\ninp_dim, oup_dim = 96, 96\nreduction=32\n\ncoord_attention = CoordAtt(inp_dim, oup_dim, reduction=reduction)\noutput=coord_attention(inp)\nprint(output.shape)\n```\n\n***\n\n\n### 28. MobileViT Attention Usage\n\n#### 28.1. Paper\n\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2103.02907)\n\n\n#### 28.2. Overview\n\n![](./model/img/MobileViTAttention.png)\n\n#### 28.3. Usage Code\n\n```python\nfrom fightingcv_attention.attention.MobileViTAttention import MobileViTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    m=MobileViTAttention()\n    input=torch.randn(1,3,49,49)\n    output=m(input)\n    print(output.shape)  #output:(1,3,49,49)\n    \n```\n\n***\n\n\n### 29. ParNet Attention Usage\n\n#### 29.1. Paper\n\n[Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\n\n\n#### 29.2. Overview\n\n![](./model/img/ParNet.png)\n\n#### 29.3. Usage Code\n\n```python\nfrom fightingcv_attention.attention.ParNetAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,512,7,7)\n    pna = ParNetAttention(channel=512)\n    output=pna(input)\n    print(output.shape) #50,512,7,7\n    \n```\n\n***\n\n\n### 30. UFO Attention Usage\n\n#### 30.1. Paper\n\n[UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2110.07641)\n\n\n#### 30.2. Overview\n\n![](./model/img/UFO.png)\n\n#### 30.3. Usage Code\n\n```python\nfrom fightingcv_attention.attention.UFOAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)\n    output=ufo(input,input,input)\n    print(output.shape) #[50, 49, 512]\n    \n```\n\n-\n\n### 31. ACmix Attention Usage\n\n#### 31.1. Paper\n\n[On the Integration of Self-Attention and Convolution](https://arxiv.org/pdf/2111.14556.pdf)\n\n#### 31.2. Usage Code\n\n```python\nfrom fightingcv_attention.attention.ACmix import ACmix\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,256,7,7)\n    acmix = ACmix(in_planes=256, out_planes=256)\n    output=acmix(input)\n    print(output.shape)\n    \n```\n\n### 32. MobileViTv2 Attention Usage\n\n#### 32.1. Paper\n\n[Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06](https://arxiv.org/abs/2206.02680)\n\n\n#### 32.2. Overview\n\n![](./model/img/MobileViTv2.png)\n\n#### 32.3. Usage Code\n\n```python\nfrom fightingcv_attention.attention.MobileViTv2Attention import MobileViTv2Attention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = MobileViTv2Attention(d_model=512)\n    output=sa(input)\n    print(output.shape)\n    \n```\n\n### 33. DAT Attention Usage\n\n#### 33.1. Paper\n\n[Vision Transformer with Deformable Attention---CVPR2022](https://arxiv.org/abs/2201.00520)\n\n#### 33.2. Usage Code\n\n```python\nfrom fightingcv_attention.attention.DAT import DAT\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DAT(\n        img_size=224,\n        patch_size=4,\n        num_classes=1000,\n        expansion=4,\n        dim_stem=96,\n        dims=[96, 192, 384, 768],\n        depths=[2, 2, 6, 2],\n        stage_spec=[['L', 'S'], ['L', 'S'], ['L', 'D', 'L', 'D', 'L', 'D'], ['L', 'D']],\n        heads=[3, 6, 12, 24],\n        window_sizes=[7, 7, 7, 7] ,\n        groups=[-1, -1, 3, 6],\n        use_pes=[False, False, True, True],\n        dwc_pes=[False, False, False, False],\n        strides=[-1, -1, 1, 1],\n        sr_ratios=[-1, -1, -1, -1],\n        offset_range_factor=[-1, -1, 2, 2],\n        no_offs=[False, False, False, False],\n        fixed_pes=[False, False, False, False],\n        use_dwc_mlps=[False, False, False, False],\n        use_conv_patches=False,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.2,\n    )\n    output=model(input)\n    print(output[0].shape)\n    \n```\n\n### 34. CrossFormer Attention Usage\n\n#### 34.1. Paper\n\n[CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION---ICLR 2022](https://arxiv.org/pdf/2108.00154.pdf)\n\n#### 34.2. Usage Code\n\n```python\nfrom fightingcv_attention.attention.Crossformer import CrossFormer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CrossFormer(img_size=224,\n        patch_size=[4, 8, 16, 32],\n        in_chans= 3,\n        num_classes=1000,\n        embed_dim=48,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        group_size=[7, 7, 7, 7],\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        ape=False,\n        patch_norm=True,\n        use_checkpoint=False,\n        merge_size=[[2, 4], [2,4], [2, 4]]\n    )\n    output=model(input)\n    print(output.shape)\n    \n```\n\n### 35. MOATransformer Attention Usage\n\n#### 35.1. Paper\n\n[Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903)\n\n#### 35.2. Usage Code\n\n```python\nfrom fightingcv_attention.attention.MOATransformer import MOATransformer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = MOATransformer(\n        img_size=224,\n        patch_size=4,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=96,\n        depths=[2, 2, 6],\n        num_heads=[3, 6, 12],\n        window_size=14,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        ape=False,\n        patch_norm=True,\n        use_checkpoint=False\n    )\n    output=model(input)\n    print(output.shape)\n    \n```\n\n### 36. CrissCrossAttention Attention Usage\n\n#### 36.1. Paper\n\n[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n\n#### 36.2. Usage Code\n\n```python\nfrom fightingcv_attention.attention.CrissCrossAttention import CrissCrossAttention\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(3, 64, 7, 7)\n    model = CrissCrossAttention(64)\n    outputs = model(input)\n    print(outputs.shape)\n    \n```\n\n### 37. Axial_attention Attention Usage\n\n#### 37.1. Paper\n\n[Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)\n\n#### 37.2. Usage Code\n\n```python\nfrom fightingcv_attention.attention.Axial_attention import AxialImageTransformer\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(3, 128, 7, 7)\n    model = AxialImageTransformer(\n        dim = 128,\n        depth = 12,\n        reversible = True\n    )\n    outputs = model(input)\n    print(outputs.shape)\n    \n```\n\n***\n\n\n# Backbone Series\n\n- Pytorch implementation of [\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n- Pytorch implementation of [\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\n\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\n\n- Pytorch implementation of [Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer---ArXiv 2021.06.07](https://arxiv.org/abs/2106.03650)\n\n- Pytorch implementation of [ConTNet: Why not use convolution and transformer at the same time?---ArXiv 2021.04.27](https://arxiv.org/abs/2104.13497)\n\n- Pytorch implementation of [Vision Transformers with Hierarchical Attention---ArXiv 2022.06.15](https://arxiv.org/abs/2106.03180)\n\n- Pytorch implementation of [Co-Scale Conv-Attentional Image Transformers---ArXiv 2021.08.26](https://arxiv.org/abs/2104.06399)\n\n- Pytorch implementation of [Conditional Positional Encodings for Vision Transformers](https://arxiv.org/abs/2102.10882)\n\n- Pytorch implementation of [Rethinking Spatial Dimensions of Vision Transformers---ICCV 2021](https://arxiv.org/abs/2103.16302)\n\n- Pytorch implementation of [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification---ICCV 2021](https://arxiv.org/abs/2103.14899)\n\n- Pytorch implementation of [Transformer in Transformer---NeurIPS 2021](https://arxiv.org/abs/2103.00112)\n\n- Pytorch implementation of [DeepViT: Towards Deeper Vision Transformer](https://arxiv.org/abs/2103.11886)\n\n- Pytorch implementation of [Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)\n***\n\n- Pytorch implementation of [ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)\n\n- Pytorch implementation of [Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\n\n- Pytorch implementation of [Going deeper with Image Transformers---ICCV 2021 (Oral)](https://arxiv.org/abs/2103.17239)\n\n- Pytorch implementation of [Training data-efficient image transformers & distillation through attention---ICML 2021](https://arxiv.org/abs/2012.12877)\n\n- Pytorch implementation of [LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\n\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/abs/2106.13112)\n\n- Pytorch implementation of [Container: Context Aggregation Network---NeuIPS 2021](https://arxiv.org/abs/2106.01401)\n\n- Pytorch implementation of [CMT: Convolutional Neural Networks Meet Vision Transformers---CVPR 2022](https://arxiv.org/abs/2107.06263)\n\n- Pytorch implementation of [Vision Transformer with Deformable Attention---CVPR 2022](https://arxiv.org/abs/2201.00520)\n\n- Pytorch implementation of [EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)\n\n\n### 1. ResNet Usage\n#### 1.1. Paper\n[\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n#### 1.2. Overview\n![](./model/img/resnet.png)\n![](./model/img/resnet2.jpg)\n\n#### 1.3. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.resnet import ResNet50,ResNet101,ResNet152\nimport torch\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnet50=ResNet50(1000)\n    # resnet101=ResNet101(1000)\n    # resnet152=ResNet152(1000)\n    out=resnet50(input)\n    print(out.shape)\n\n```\n\n\n### 2. ResNeXt Usage\n#### 2.1. Paper\n\n[\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\n\n#### 2.2. Overview\n![](./model/img/resnext.png)\n\n#### 2.3. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.resnext import ResNeXt50,ResNeXt101,ResNeXt152\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnext50=ResNeXt50(1000)\n    # resnext101=ResNeXt101(1000)\n    # resnext152=ResNeXt152(1000)\n    out=resnext50(input)\n    print(out.shape)\n\n\n```\n\n\n\n### 3. MobileViT Usage\n#### 3.1. Paper\n\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\n\n#### 3.2. Overview\n![](./model/img/mobileViT.jpg)\n\n#### 3.3. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.MobileViT import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n\n    ### mobilevit_xxs\n    mvit_xxs=mobilevit_xxs()\n    out=mvit_xxs(input)\n    print(out.shape)\n\n    ### mobilevit_xs\n    mvit_xs=mobilevit_xs()\n    out=mvit_xs(input)\n    print(out.shape)\n\n\n    ### mobilevit_s\n    mvit_s=mobilevit_s()\n    out=mvit_s(input)\n    print(out.shape)\n\n```\n\n\n\n\n\n### 4. ConvMixer Usage\n#### 4.1. Paper\n[Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\n#### 4.2. Overview\n![](./model/img/ConvMixer.png)\n\n#### 4.3. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.ConvMixer import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    x=torch.randn(1,3,224,224)\n    convmixer=ConvMixer(dim=512,depth=12)\n    out=convmixer(x)\n    print(out.shape)  #[1, 1000]\n\n\n```\n\n### 5. ShuffleTransformer Usage\n#### 5.1. Paper\n[Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer](https://arxiv.org/pdf/2106.03650.pdf)\n\n#### 5.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.ShuffleTransformer import ShuffleTransformer\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    sft = ShuffleTransformer()\n    output=sft(input)\n    print(output.shape)\n\n\n```\n\n### 6. ConTNet Usage\n#### 6.1. Paper\n[ConTNet: Why not use convolution and transformer at the same time?](https://arxiv.org/abs/2104.13497)\n\n#### 6.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.ConTNet import ConTNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == \"__main__\":\n    model = build_model(use_avgdown=True, relative=True, qkv_bias=True, pre_norm=True)\n    input = torch.randn(1, 3, 224, 224)\n    out = model(input)\n    print(out.shape)\n\n\n```\n\n### 7 HATNet Usage\n#### 7.1. Paper\n[Vision Transformers with Hierarchical Attention](https://arxiv.org/abs/2106.03180)\n\n#### 7.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.HATNet import HATNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    hat = HATNet(dims=[48, 96, 240, 384], head_dim=48, expansions=[8, 8, 4, 4],\n        grid_sizes=[8, 7, 7, 1], ds_ratios=[8, 4, 2, 1], depths=[2, 2, 6, 3])\n    output=hat(input)\n    print(output.shape)\n\n\n```\n\n### 8 CoaT Usage\n#### 8.1. Paper\n[Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/abs/2104.06399)\n\n#### 8.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.CoaT import CoaT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CoaT(patch_size=4, embed_dims=[152, 152, 152, 152], serial_depths=[2, 2, 2, 2], parallel_depth=6, num_heads=8, mlp_ratios=[4, 4, 4, 4])\n    output=model(input)\n    print(output.shape) # torch.Size([1, 1000])\n\n```\n\n### 9 PVT Usage\n#### 9.1. Paper\n[PVT v2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/pdf/2106.13797.pdf)\n\n#### 9.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.PVT import PyramidVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PyramidVisionTransformer(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1])\n    output=model(input)\n    print(output.shape)\n\n```\n\n\n### 10 CPVT Usage\n#### 10.1. Paper\n[Conditional Positional Encodings for Vision Transformers](https://arxiv.org/abs/2102.10882)\n\n#### 10.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.CPVT import CPVTV2\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CPVTV2(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1])\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 11 PIT Usage\n#### 11.1. Paper\n[Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)\n\n#### 11.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.PIT import PoolingTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=14,\n        stride=7,\n        base_dims=[64, 64, 64],\n        depth=[3, 6, 4],\n        heads=[4, 8, 16],\n        mlp_ratio=4\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 12 CrossViT Usage\n#### 12.1. Paper\n[CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899)\n\n#### 12.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.CrossViT import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == \"__main__\":\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        img_size=[240, 224],\n        patch_size=[12, 16], \n        embed_dim=[192, 384], \n        depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],\n        num_heads=[6, 6], \n        mlp_ratio=[4, 4, 1], \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 13 TnT Usage\n#### 13.1. Paper\n[Transformer in Transformer](https://arxiv.org/abs/2103.00112)\n\n#### 13.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.TnT import TNT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = TNT(\n        img_size=224, \n        patch_size=16, \n        outer_dim=384, \n        inner_dim=24, \n        depth=12,\n        outer_num_heads=6, \n        inner_num_heads=4, \n        qkv_bias=False,\n        inner_stride=4)\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 14 DViT Usage\n#### 14.1. Paper\n[DeepViT: Towards Deeper Vision Transformer](https://arxiv.org/abs/2103.11886)\n\n#### 14.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.DViT import DeepVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DeepVisionTransformer(\n        patch_size=16, embed_dim=384, \n        depth=[False] * 16, \n        apply_transform=[False] * 0 + [True] * 32, \n        num_heads=12, \n        mlp_ratio=3, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 15 CeiT Usage\n#### 15.1. Paper\n[Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)\n\n#### 15.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.CeiT import CeIT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CeIT(\n        hybrid_backbone=Image2Tokens(),\n        patch_size=4, \n        embed_dim=192, \n        depth=12, \n        num_heads=3, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 16 ConViT Usage\n#### 16.1. Paper\n[ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)\n\n#### 16.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.ConViT import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        num_heads=16,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 17 CaiT Usage\n#### 17.1. Paper\n[Going deeper with Image Transformers](https://arxiv.org/abs/2103.17239)\n\n#### 17.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.CaiT import CaiT\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CaiT(\n        img_size= 224,\n        patch_size=16, \n        embed_dim=192, \n        depth=24, \n        num_heads=4, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2\n        )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 18 PatchConvnet Usage\n#### 18.1. Paper\n[Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\n\n#### 18.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.PatchConvnet import PatchConvnet\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=384,\n        depth=60,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        depth_token_only=1,\n        mlp_ratio_clstk=3.0,\n    )\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 19 DeiT Usage\n#### 19.1. Paper\n[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n\n#### 19.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.DeiT import DistilledVisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = DistilledVisionTransformer(\n        patch_size=16, \n        embed_dim=384, \n        depth=12, \n        num_heads=6, \n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n        )\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 20 LeViT Usage\n#### 20.1. Paper\n[LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\n\n#### 20.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.LeViT import *\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    for name in specification:\n        input=torch.randn(1,3,224,224)\n        model = globals()[name](fuse=True, pretrained=False)\n        model.eval()\n        output = model(input)\n        print(output.shape)\n\n```\n\n### 21 VOLO Usage\n#### 21.1. Paper\n[VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/abs/2106.13112)\n\n#### 21.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.VOLO import VOLO\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VOLO([4, 4, 8, 2],\n                 embed_dims=[192, 384, 384, 384],\n                 num_heads=[6, 12, 12, 12],\n                 mlp_ratios=[3, 3, 3, 3],\n                 downsamples=[True, False, False, False],\n                 outlook_attention=[True, False, False, False ],\n                 post_layers=['ca', 'ca'],\n                 )\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 22 Container Usage\n#### 22.1. Paper\n[Container: Context Aggregation Network](https://arxiv.org/abs/2106.01401)\n\n#### 22.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.Container import VisionTransformer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionTransformer(\n        img_size=[224, 56, 28, 14], \n        patch_size=[4, 2, 2, 2], \n        embed_dim=[64, 128, 320, 512], \n        depth=[3, 4, 8, 3], \n        num_heads=16, \n        mlp_ratio=[8, 8, 4, 4], \n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6))\n    output=model(input)\n    print(output.shape)\n\n```\n\n### 23 CMT Usage\n#### 23.1. Paper\n[CMT: Convolutional Neural Networks Meet Vision Transformers](https://arxiv.org/abs/2107.06263)\n\n#### 23.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.CMT import CMT_Tiny\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = CMT_Tiny()\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n### 24 EfficientFormer Usage\n#### 24.1. Paper\n[EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)\n\n#### 24.2. Usage Code\n```python\n\nfrom fightingcv_attention.backbone.EfficientFormer import EfficientFormer\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = EfficientFormer(\n        layers=EfficientFormer_depth['l1'],\n        embed_dims=EfficientFormer_width['l1'],\n        downsamples=[True, True, True, True],\n        vit_num=1,\n    )\n    output=model(input)\n    print(output[0].shape)\n\n```\n\n\n\n\n\n\n# MLP Series\n\n- Pytorch implementation of [\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition---arXiv 2021.05.05\"](https://arxiv.org/pdf/2105.01883v1.pdf)\n\n- Pytorch implementation of [\"MLP-Mixer: An all-MLP Architecture for Vision---arXiv 2021.05.17\"](https://arxiv.org/pdf/2105.01601.pdf)\n\n- Pytorch implementation of [\"ResMLP: Feedforward networks for image classification with data-efficient training---arXiv 2021.05.07\"](https://arxiv.org/pdf/2105.03404.pdf)\n\n- Pytorch implementation of [\"Pay Attention to MLPs---arXiv 2021.05.17\"](https://arxiv.org/abs/2105.08050)\n\n\n- Pytorch implementation of [\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?---arXiv 2021.09.12\"](https://arxiv.org/abs/2109.05422)\n\n### 1. RepMLP Usage\n#### 1.1. Paper\n[\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition\"](https://arxiv.org/pdf/2105.01883v1.pdf)\n\n#### 1.2. Overview\n![](./model/img/repmlp.png)\n\n#### 1.3. Usage Code\n```python\nfrom fightingcv_attention.mlp.repmlp import RepMLP\nimport torch\nfrom torch import nn\n\nN=4 #batch size\nC=512 #input dim\nO=1024 #output dim\nH=14 #image height\nW=14 #image width\nh=7 #patch height\nw=7 #patch width\nfc1_fc2_reduction=1 #reduction ratio\nfc3_groups=8 # groups\nrepconv_kernels=[1,3,5,7] #kernel list\nrepmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels)\nx=torch.randn(N,C,H,W)\nrepmlp.eval()\nfor module in repmlp.modules():\n    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n        nn.init.uniform_(module.running_mean, 0, 0.1)\n        nn.init.uniform_(module.running_var, 0, 0.1)\n        nn.init.uniform_(module.weight, 0, 0.1)\n        nn.init.uniform_(module.bias, 0, 0.1)\n\n#training result\nout=repmlp(x)\n#inference result\nrepmlp.switch_to_deploy()\ndeployout = repmlp(x)\n\nprint(((deployout-out)**2).sum())\n```\n\n### 2. MLP-Mixer Usage\n#### 2.1. Paper\n[\"MLP-Mixer: An all-MLP Architecture for Vision\"](https://arxiv.org/pdf/2105.01601.pdf)\n\n#### 2.2. Overview\n![](./model/img/mlpmixer.png)\n\n#### 2.3. Usage Code\n```python\nfrom fightingcv_attention.mlp.mlp_mixer import MlpMixer\nimport torch\nmlp_mixer=MlpMixer(num_classes=1000,num_blocks=10,patch_size=10,tokens_hidden_dim=32,channels_hidden_dim=1024,tokens_mlp_dim=16,channels_mlp_dim=1024)\ninput=torch.randn(50,3,40,40)\noutput=mlp_mixer(input)\nprint(output.shape)\n```\n\n***\n\n### 3. ResMLP Usage\n#### 3.1. Paper\n[\"ResMLP: Feedforward networks for image classification with data-efficient training\"](https://arxiv.org/pdf/2105.03404.pdf)\n\n#### 3.2. Overview\n![](./model/img/resmlp.png)\n\n#### 3.3. Usage Code\n```python\nfrom fightingcv_attention.mlp.resmlp import ResMLP\nimport torch\n\ninput=torch.randn(50,3,14,14)\nresmlp=ResMLP(dim=128,image_size=14,patch_size=7,class_num=1000)\nout=resmlp(input)\nprint(out.shape) #the last dimention is class_num\n```\n\n***\n\n### 4. gMLP Usage\n#### 4.1. Paper\n[\"Pay Attention to MLPs\"](https://arxiv.org/abs/2105.08050)\n\n#### 4.2. Overview\n![](./model/img/gMLP.jpg)\n\n#### 4.3. Usage Code\n```python\nfrom fightingcv_attention.mlp.g_mlp import gMLP\nimport torch\n\nnum_tokens=10000\nbs=50\nlen_sen=49\nnum_layers=6\ninput=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen\ngmlp = gMLP(num_tokens=num_tokens,len_sen=len_sen,dim=512,d_ff=1024)\noutput=gmlp(input)\nprint(output.shape)\n```\n\n***\n\n### 5. sMLP Usage\n#### 5.1. Paper\n[\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?\"](https://arxiv.org/abs/2109.05422)\n\n#### 5.2. Overview\n![](./model/img/sMLP.jpg)\n\n#### 5.3. Usage Code\n```python\nfrom fightingcv_attention.mlp.sMLP_block import sMLPBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    smlp=sMLPBlock(h=224,w=224)\n    out=smlp(input)\n    print(out.shape)\n```\n\n### 6. vip-mlp Usage\n#### 6.1. Paper\n[\"Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\n\n#### 6.2. Usage Code\n```python\nfrom fightingcv_attention.mlp.vip-mlp import VisionPermutator\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    model = VisionPermutator(\n        layers=[4, 3, 8, 3], \n        embed_dims=[384, 384, 384, 384], \n        patch_size=14, \n        transitions=[False, False, False, False],\n        segment_dim=[16, 16, 16, 16], \n        mlp_ratios=[3, 3, 3, 3], \n        mlp_fn=WeightedPermuteMLP\n    )\n    output=model(input)\n    print(output.shape)\n```\n\n\n# Re-Parameter Series\n\n- Pytorch implementation of [\"RepVGG: Making VGG-style ConvNets Great Again---CVPR2021\"](https://arxiv.org/abs/2101.03697)\n\n- Pytorch implementation of [\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks---ICCV2019\"](https://arxiv.org/abs/1908.03930)\n\n- Pytorch implementation of [\"Diverse Branch Block: Building a Convolution as an Inception-like Unit---CVPR2021\"](https://arxiv.org/abs/2103.13425)\n\n\n***\n\n### 1. RepVGG Usage\n#### 1.1. Paper\n[\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/abs/2101.03697)\n\n#### 1.2. Overview\n![](./model/img/repvgg.png)\n\n#### 1.3. Usage Code\n```python\n\nfrom fightingcv_attention.rep.repvgg import RepBlock\nimport torch\n\n\ninput=torch.randn(50,512,49,49)\nrepblock=RepBlock(512,512)\nrepblock.eval()\nout=repblock(input)\nrepblock._switch_to_deploy()\nout2=repblock(input)\nprint('difference between vgg and repvgg')\nprint(((out2-out)**2).sum())\n```\n\n\n\n***\n\n### 2. ACNet Usage\n#### 2.1. Paper\n[\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks\"](https://arxiv.org/abs/1908.03930)\n\n#### 2.2. Overview\n![](./model/img/acnet.png)\n\n#### 2.3. Usage Code\n```python\nfrom fightingcv_attention.rep.acnet import ACNet\nimport torch\nfrom torch import nn\n\ninput=torch.randn(50,512,49,49)\nacnet=ACNet(512,512)\nacnet.eval()\nout=acnet(input)\nacnet._switch_to_deploy()\nout2=acnet(input)\nprint('difference:')\nprint(((out2-out)**2).sum())\n\n```\n\n\n\n***\n\n### 2. Diverse Branch Block Usage\n#### 2.1. Paper\n[\"Diverse Branch Block: Building a Convolution as an Inception-like Unit\"](https://arxiv.org/abs/2103.13425)\n\n#### 2.2. Overview\n![](./model/img/ddb.png)\n\n#### 2.3. Usage Code\n##### 2.3.1 Transform I\n```python\nfrom fightingcv_attention.rep.ddb import transI_conv_bn\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n#conv+bn\nconv1=nn.Conv2d(64,64,3,padding=1)\nbn1=nn.BatchNorm2d(64)\nbn1.eval()\nout1=bn1(conv1(input))\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.2 Transform II\n```python\nfrom fightingcv_attention.rep.ddb import transII_conv_branch\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,64,3,padding=1)\nconv2=nn.Conv2d(64,64,3,padding=1)\nout1=conv1(input)+conv2(input)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.3 Transform III\n```python\nfrom fightingcv_attention.rep.ddb import transIII_conv_sequential\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,64,1,padding=0,bias=False)\nconv2=nn.Conv2d(64,64,3,padding=1,bias=False)\nout1=conv2(conv1(input))\n\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1,bias=False)\nconv_fuse.weight.data=transIII_conv_sequential(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.4 Transform IV\n```python\nfrom fightingcv_attention.rep.ddb import transIV_conv_concat\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,32,3,padding=1)\nconv2=nn.Conv2d(64,32,3,padding=1)\nout1=torch.cat([conv1(input),conv2(input)],dim=1)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.5 Transform V\n```python\nfrom fightingcv_attention.rep.ddb import transV_avg\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\navg=nn.AvgPool2d(kernel_size=3,stride=1)\nout1=avg(input)\n\nconv=transV_avg(64,3)\nout2=conv(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n\n##### 2.3.6 Transform VI\n```python\nfrom fightingcv_attention.rep.ddb import transVI_conv_scale\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1x1=nn.Conv2d(64,64,1)\nconv1x3=nn.Conv2d(64,64,(1,3),padding=(0,1))\nconv3x1=nn.Conv2d(64,64,(3,1),padding=(1,0))\nout1=conv1x1(input)+conv1x3(input)+conv3x1(input)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n\n\n\n\n# Convolution Series\n\n- Pytorch implementation of [\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications---CVPR2017\"](https://arxiv.org/abs/1704.04861)\n\n- Pytorch implementation of [\"Efficientnet: Rethinking model scaling for convolutional neural networks---PMLR2019\"](http://proceedings.mlr.press/v97/tan19a.html)\n\n- Pytorch implementation of [\"Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021\"](https://arxiv.org/abs/2103.06255)\n\n- Pytorch implementation of [\"Dynamic Convolution: Attention over Convolution Kernels---CVPR2020 Oral\"](https://arxiv.org/abs/1912.03458)\n\n- Pytorch implementation of [\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference---NeurIPS2019\"](https://arxiv.org/abs/1904.04971)\n\n***\n\n### 1. Depthwise Separable Convolution Usage\n#### 1.1. Paper\n[\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"](https://arxiv.org/abs/1704.04861)\n\n#### 1.2. Overview\n![](./model/img/DepthwiseSeparableConv.png)\n\n#### 1.3. Usage Code\n```python\nfrom fightingcv_attention.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\ndsconv=DepthwiseSeparableConvolution(3,64)\nout=dsconv(input)\nprint(out.shape)\n```\n\n***\n\n\n### 2. MBConv Usage\n#### 2.1. Paper\n[\"Efficientnet: Rethinking model scaling for convolutional neural networks\"](http://proceedings.mlr.press/v97/tan19a.html)\n\n#### 2.2. Overview\n![](./model/img/MBConv.jpg)\n\n#### 2.3. Usage Code\n```python\nfrom fightingcv_attention.conv.MBConv import MBConvBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=512,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n\n```\n\n***\n\n\n### 3. Involution Usage\n#### 3.1. Paper\n[\"Involution: Inverting the Inherence of Convolution for Visual Recognition\"](https://arxiv.org/abs/2103.06255)\n\n#### 3.2. Overview\n![](./model/img/Involution.png)\n\n#### 3.3. Usage Code\n```python\nfrom fightingcv_attention.conv.Involution import Involution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,4,64,64)\ninvolution=Involution(kernel_size=3,in_channel=4,stride=2)\nout=involution(input)\nprint(out.shape)\n```\n\n***\n\n\n### 4. DynamicConv Usage\n#### 4.1. Paper\n[\"Dynamic Convolution: Attention over Convolution Kernels\"](https://arxiv.org/abs/1912.03458)\n\n#### 4.2. Overview\n![](./model/img/DynamicConv.png)\n\n#### 4.3. Usage Code\n```python\nfrom fightingcv_attention.conv.DynamicConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=DynamicConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape) # 2,32,64,64\n\n```\n\n***\n\n\n### 5. CondConv Usage\n#### 5.1. Paper\n[\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference\"](https://arxiv.org/abs/1904.04971)\n\n#### 5.2. Overview\n![](./model/img/CondConv.png)\n\n#### 5.3. Usage Code\n```python\nfrom fightingcv_attention.conv.CondConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\n\n\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=CondConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape)\n\n```\n"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 0.265625,
          "content": "from model.attention.MobileViTv2Attention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = MobileViTv2Attention(d_model=512)\n    output=sa(input)\n    print(output.shape)\n "
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.19140625,
          "content": "from setuptools import find_packages, setup\n\n\n\nsetup(\n    name=\"fighingcv\",\n    version=\"1.0.0\",\n    author=\"xmu-xiaoma666\",\n    author_email=\"julien@huggingface.co\",\n    description=(\n        \"FightingCV Codebase For Attention,Backbone, MLP, Re-parameter, Convolution\"\n    ),\n    long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    keywords=(\n        \"Attention\"\n        \"Backbone\"\n    ),\n    license=\"Apache\",\n    url=\"https://github.com/xmu-xiaoma666/External-Attention-pytorch\",\n    package_dir={\"\": \".\"},\n    packages=find_packages(\".\"),\n    # entry_points={\n    #     \"console_scripts\": [\n    #         \"huggingface-cli=huggingface_hub.commands.huggingface_cli:main\"\n    #     ]\n    # },\n    python_requires=\">=3.7.0\",\n    # install_requires=install_requires,\n    classifiers=[\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n)"
        }
      ]
    }
  ]
}