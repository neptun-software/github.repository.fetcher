{
  "metadata": {
    "timestamp": 1736561335722,
    "page": 355,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVlabs/stylegan2",
      "stars": 11024,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.03125,
          "content": "/.stylegan2-cache/\n__pycache__/\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.353515625,
          "content": "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, visit\n# https://nvlabs.github.io/stylegan2/license.html\n\nFROM tensorflow/tensorflow:1.14.0-gpu-py3\n\nRUN pip install scipy==1.3.3\nRUN pip install requests==2.22.0\nRUN pip install Pillow==6.2.1\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 4.6552734375,
          "content": "Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\r\n\r\n\r\nNvidia Source Code License-NC\r\n\r\n=======================================================================\r\n\r\n1. Definitions\r\n\r\n\"Licensor\" means any person or entity that distributes its Work.\r\n\r\n\"Software\" means the original work of authorship made available under\r\nthis License.\r\n\r\n\"Work\" means the Software and any additions to or derivative works of\r\nthe Software that are made available under this License.\r\n\r\n\"Nvidia Processors\" means any central processing unit (CPU), graphics\r\nprocessing unit (GPU), field-programmable gate array (FPGA),\r\napplication-specific integrated circuit (ASIC) or any combination\r\nthereof designed, made, sold, or provided by Nvidia or its affiliates.\r\n\r\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and\r\n\"distribution\" have the meaning as provided under U.S. copyright law;\r\nprovided, however, that for the purposes of this License, derivative\r\nworks shall not include works that remain separable from, or merely\r\nlink (or bind by name) to the interfaces of, the Work.\r\n\r\nWorks, including the Software, are \"made available\" under this License\r\nby including in or with the Work either (a) a copyright notice\r\nreferencing the applicability of this License to the Work, or (b) a\r\ncopy of this License.\r\n\r\n2. License Grants\r\n\r\n    2.1 Copyright Grant. Subject to the terms and conditions of this\r\n    License, each Licensor grants to you a perpetual, worldwide,\r\n    non-exclusive, royalty-free, copyright license to reproduce,\r\n    prepare derivative works of, publicly display, publicly perform,\r\n    sublicense and distribute its Work and any resulting derivative\r\n    works in any form.\r\n\r\n3. Limitations\r\n\r\n    3.1 Redistribution. You may reproduce or distribute the Work only\r\n    if (a) you do so under this License, (b) you include a complete\r\n    copy of this License with your distribution, and (c) you retain\r\n    without modification any copyright, patent, trademark, or\r\n    attribution notices that are present in the Work.\r\n\r\n    3.2 Derivative Works. You may specify that additional or different\r\n    terms apply to the use, reproduction, and distribution of your\r\n    derivative works of the Work (\"Your Terms\") only if (a) Your Terms\r\n    provide that the use limitation in Section 3.3 applies to your\r\n    derivative works, and (b) you identify the specific derivative\r\n    works that are subject to Your Terms. Notwithstanding Your Terms,\r\n    this License (including the redistribution requirements in Section\r\n    3.1) will continue to apply to the Work itself.\r\n\r\n    3.3 Use Limitation. The Work and any derivative works thereof only\r\n    may be used or intended for use non-commercially. The Work or\r\n    derivative works thereof may be used or intended for use by Nvidia\r\n    or its affiliates commercially or non-commercially. As used herein,\r\n    \"non-commercially\" means for research or evaluation purposes only.\r\n\r\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim\r\n    against any Licensor (including any claim, cross-claim or\r\n    counterclaim in a lawsuit) to enforce any patents that you allege\r\n    are infringed by any Work, then your rights under this License from\r\n    such Licensor (including the grants in Sections 2.1 and 2.2) will\r\n    terminate immediately.\r\n\r\n    3.5 Trademarks. This License does not grant any rights to use any\r\n    Licensor's or its affiliates' names, logos, or trademarks, except\r\n    as necessary to reproduce the notices described in this License.\r\n\r\n    3.6 Termination. If you violate any term of this License, then your\r\n    rights under this License (including the grants in Sections 2.1 and\r\n    2.2) will terminate immediately.\r\n\r\n4. Disclaimer of Warranty.\r\n\r\nTHE WORK IS PROVIDED \"AS IS\" WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\r\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR\r\nNON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER\r\nTHIS LICENSE. \r\n\r\n5. Limitation of Liability.\r\n\r\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL\r\nTHEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE\r\nSHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\r\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF\r\nOR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK\r\n(INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION,\r\nLOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER\r\nCOMMERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF\r\nTHE POSSIBILITY OF SUCH DAMAGES.\r\n\r\n=======================================================================\r\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.447265625,
          "content": "## StyleGAN2 &mdash; Official TensorFlow Implementation\n\n![Teaser image](./docs/stylegan2-teaser-1024x256.png)\n\n**Analyzing and Improving the Image Quality of StyleGAN**<br>\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila<br>\n\nPaper: http://arxiv.org/abs/1912.04958<br>\nVideo: https://youtu.be/c-NJtV9Jvp0<br>\n\nAbstract: *The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.*\n\nFor business inquiries, please visit our website and submit the form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)\n\n**&#9733;&#9733;&#9733; NEW: [StyleGAN2-ADA-PyTorch](https://github.com/NVlabs/stylegan2-ada-pytorch) is now available; see the full list of versions [here](https://nvlabs.github.io/stylegan2/versions.html) &#9733;&#9733;&#9733;**\n\n| Additional material | &nbsp;\n| :--- | :----------\n| [StyleGAN2](https://drive.google.com/open?id=1QHc-yF5C3DChRwSdZKcx1w6K8JvSxQi7) | Main Google Drive folder\n| &boxvr;&nbsp; [stylegan2-paper.pdf](https://drive.google.com/open?id=1fnF-QsiQeKaxF-HbvFiGtzHF_Bf3CzJu) | High-quality version of the paper\n| &boxvr;&nbsp; [stylegan2-video.mp4](https://drive.google.com/open?id=1f_gbKW6FUUHKkUxciJ_lQx29mCq_fSBy) | High-quality version of the video\n| &boxvr;&nbsp; [images](https://drive.google.com/open?id=1Sak157_DLX84ytqHHqZaH_59HoEWzfB7) | Example images produced using our method\n| &boxv;&nbsp; &boxvr;&nbsp;  [curated-images](https://drive.google.com/open?id=1ydWb8xCHzDKMTW9kQ7sL-B1R0zATHVHp) | Hand-picked images showcasing our results\n| &boxv;&nbsp; &boxur;&nbsp;  [100k-generated-images](https://drive.google.com/open?id=1BA2OZ1GshdfFZGYZPob5QWOGBuJCdu5q) | Random images with and without truncation\n| &boxvr;&nbsp; [videos](https://drive.google.com/open?id=1yXDV96SFXoUiZKU7AyE6DyKgDpIk4wUZ) | Individual clips of the video as high-quality MP4\n| &boxur;&nbsp; [networks](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/) | Pre-trained networks\n| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-ffhq-config-f.pkl | StyleGAN2 for <span style=\"font-variant:small-caps\">FFHQ</span> dataset at 1024&times;1024\n| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-car-config-f.pkl | StyleGAN2 for <span style=\"font-variant:small-caps\">LSUN Car</span> dataset at 512&times;384\n| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-cat-config-f.pkl | StyleGAN2 for <span style=\"font-variant:small-caps\">LSUN Cat</span> dataset at 256&times;256\n| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-church-config-f.pkl | StyleGAN2 for <span style=\"font-variant:small-caps\">LSUN Church</span> dataset at 256&times;256\n| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-horse-config-f.pkl | StyleGAN2 for <span style=\"font-variant:small-caps\">LSUN Horse</span> dataset at 256&times;256\n| &ensp;&ensp; &boxur;&nbsp;&#x22ef;  | Other training configurations used in the paper\n\n## Requirements\n\n* Both Linux and Windows are supported. Linux is recommended for performance and compatibility reasons.\n* 64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer.\n* We recommend TensorFlow 1.14, which we used for all experiments in the paper, but TensorFlow 1.15 is also supported on Linux. TensorFlow 2.x is not supported.\n* On Windows you need to use TensorFlow 1.14, as the standard 1.15 installation does not include necessary C++ headers.\n* One or more high-end NVIDIA GPUs, NVIDIA drivers, CUDA 10.0 toolkit and cuDNN 7.5. To reproduce the results reported in the paper, you need an NVIDIA GPU with at least 16 GB of DRAM.\n* Docker users: use the [provided Dockerfile](./Dockerfile) to build an image with the required library dependencies.\n\nStyleGAN2 relies on custom TensorFlow ops that are compiled on the fly using [NVCC](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html). To test that your NVCC installation is working correctly, run:\n\n```.bash\nnvcc test_nvcc.cu -o test_nvcc -run\n| CPU says hello.\n| GPU says hello.\n```\n\nOn Windows, the compilation requires Microsoft Visual Studio to be in `PATH`. We recommend installing [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/) and adding into `PATH` using `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"`.\n\n## Using pre-trained networks\n\nPre-trained networks are stored as `*.pkl` files on the [StyleGAN2 Google Drive folder](https://drive.google.com/open?id=1QHc-yF5C3DChRwSdZKcx1w6K8JvSxQi7). Below, you can either reference them directly using the syntax `gdrive:networks/<filename>.pkl`, or download them manually and reference by filename.\n\n```.bash\n# Generate uncurated ffhq images (matches paper Figure 12)\npython run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n  --seeds=6600-6625 --truncation-psi=0.5\n\n# Generate curated ffhq images (matches paper Figure 11)\npython run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n  --seeds=66,230,389,1518 --truncation-psi=1.0\n\n# Generate uncurated car images\npython run_generator.py generate-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\\n  --seeds=6000-6025 --truncation-psi=0.5\n\n# Example of style mixing (matches the corresponding video clip)\npython run_generator.py style-mixing-example --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n  --row-seeds=85,100,75,458,1500 --col-seeds=55,821,1789,293 --truncation-psi=1.0\n```\n\nThe results are placed in `results/<RUNNING_ID>/*.png`. You can change the location with `--result-dir`. For example, `--result-dir=~/my-stylegan2-results`.\n\nYou can import the networks in your own Python code using `pickle.load()`. For this to work, you need to include the `dnnlib` source directory in `PYTHONPATH` and create a default TensorFlow session by calling `dnnlib.tflib.init_tf()`. See [run_generator.py](./run_generator.py) and [pretrained_networks.py](./pretrained_networks.py) for examples.\n\n## Preparing datasets\n\nDatasets are stored as multi-resolution TFRecords, similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory, e.g., `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections, the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments, e.g., `--dataset=ffhq --data-dir=~/datasets`.\n\n**FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords, run:\n\n```.bash\npushd ~\ngit clone https://github.com/NVlabs/ffhq-dataset.git\ncd ffhq-dataset\npython download_ffhq.py --tfrecords\npopd\npython dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq\n```\n\n**LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords, run:\n\n```.bash\npython dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384\npython dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256\npython dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256\npython dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256\n```\n\n**Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords, run:\n\n```.bash\npython dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images\npython dataset_tool.py display ~/datasets/my-custom-dataset\n```\n\n## Projecting images to latent space\n\nTo find the matching latent vectors for a set of images, run:\n\n```.bash\n# Project generated images\npython run_projector.py project-generated-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\\n  --seeds=0,1,5\n\n# Project real images\npython run_projector.py project-real-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\\n  --dataset=car --data-dir=~/datasets\n```\n\n## Training networks\n\nTo reproduce the training runs for config F in Tables 1 and 3, run:\n\n```.bash\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=ffhq --mirror-augment=true\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=car --total-kimg=57000\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=cat --total-kimg=88000\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=church --total-kimg 88000 --gamma=100\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=horse --total-kimg 100000 --gamma=100\n```\n\nFor other configurations, see `python run_training.py --help`.\n\nWe have verified that the results match the paper when training with 1, 2, 4, or 8 GPUs. Note that training FFHQ at 1024&times;1024 resolution requires GPU(s) with at least 16 GB of memory. The following table lists typical training times using NVIDIA DGX-1 with 8 Tesla V100 GPUs:\n\n| Configuration | Resolution      | Total kimg | 1 GPU   | 2 GPUs  | 4 GPUs  | 8 GPUs | GPU mem |\n| :------------ | :-------------: | :--------: | :-----: | :-----: | :-----: | :----: | :-----: |\n| `config-f`    | 1024&times;1024 | 25000      | 69d 23h | 36d 4h  | 18d 14h | 9d 18h | 13.3 GB |\n| `config-f`    | 1024&times;1024 | 10000      | 27d 23h | 14d 11h | 7d 10h  | 3d 22h | 13.3 GB |\n| `config-e`    | 1024&times;1024 | 25000      | 35d 11h | 18d 15h | 9d 15h  | 5d 6h  | 8.6 GB  |\n| `config-e`    | 1024&times;1024 | 10000      | 14d 4h  | 7d 11h  | 3d 20h  | 2d 3h  | 8.6 GB  |\n| `config-f`    | 256&times;256   | 25000      | 32d 13h | 16d 23h | 8d 21h  | 4d 18h | 6.4 GB  |\n| `config-f`    | 256&times;256   | 10000      | 13d 0h  | 6d 19h  | 3d 13h  | 1d 22h | 6.4 GB  |\n\nTraining curves for FFHQ config F (StyleGAN2) compared to original StyleGAN using 8 GPUs:\n\n![Training curves](./docs/stylegan2-training-curves.png)\n\nAfter training, the resulting networks can be used the same way as the official pre-trained networks:\n\n```.bash\n# Generate 1000 random images without truncation\npython run_generator.py generate-images --seeds=0-999 --truncation-psi=1.0 \\\n  --network=results/00006-stylegan2-ffhq-8gpu-config-f/networks-final.pkl\n```\n\n## Evaluation metrics\n\nTo reproduce the numbers for config F in Tables 1 and 3, run:\n\n```.bash\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n  --metrics=fid50k,ppl_wend --dataset=ffhq --mirror-augment=true\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-car-config-f.pkl \\\n  --metrics=fid50k,ppl2_wend --dataset=car\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-cat-config-f.pkl \\\n  --metrics=fid50k,ppl2_wend --dataset=cat\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-church-config-f.pkl \\\n  --metrics=fid50k,ppl2_wend --dataset=church\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-horse-config-f.pkl \\\n  --metrics=fid50k,ppl2_wend --dataset=horse\n```\n\nFor other configurations, see the [StyleGAN2 Google Drive folder](https://drive.google.com/open?id=1QHc-yF5C3DChRwSdZKcx1w6K8JvSxQi7).\n\nNote that the metrics are evaluated using a different random seed each time, so the results will vary between runs. In the paper, we reported the average result of running each metric 10 times. The following table lists the available metrics along with their expected runtimes and random variation:\n\n| Metric      | FFHQ config F  | 1 GPU  | 2 GPUs  | 4 GPUs | Description |\n| :---------- | :------------: | :----: | :-----: | :----: | :---------- |\n| `fid50k`    | 2.84 &pm; 0.03 | 22 min | 14 min  | 10 min | [Fr&eacute;chet Inception Distance](https://arxiv.org/abs/1706.08500)\n| `is50k`     | 5.13 &pm; 0.02 | 23 min | 14 min  | 8 min  | [Inception Score](https://arxiv.org/abs/1606.03498)\n| `ppl_zfull` | 348.0 &pm; 3.8 | 41 min | 22 min  | 14 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) in Z, full paths\n| `ppl_wfull` | 126.9 &pm; 0.2 | 42 min | 22 min  | 13 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) in W, full paths\n| `ppl_zend`  | 348.6 &pm; 3.0 | 41 min | 22 min  | 14 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) in Z, path endpoints\n| `ppl_wend`  | 129.4 &pm; 0.8 | 40 min | 23 min  | 13 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) in W, path endpoints\n| `ppl2_wend` | 145.0 &pm; 0.5 | 41 min | 23 min  | 14 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) without center crop\n| `ls`        | 154.2 / 4.27   | 10 hrs | 6 hrs   | 4 hrs  | [Linear Separability](https://arxiv.org/abs/1812.04948)\n| `pr50k3`    | 0.689 / 0.492  | 26 min | 17 min  | 12 min | [Precision and Recall](https://arxiv.org/abs/1904.06991)\n\nNote that some of the metrics cache dataset-specific data on the disk, and they will take somewhat longer when run for the first time.\n\n## License\n\nCopyright &copy; 2019, NVIDIA Corporation. All rights reserved.\n\nThis work is made available under the Nvidia Source Code License-NC. To view a copy of this license, visit https://nvlabs.github.io/stylegan2/license.html\n\n## Citation\n\n```\n@inproceedings{Karras2019stylegan2,\n  title     = {Analyzing and Improving the Image Quality of {StyleGAN}},\n  author    = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n  booktitle = {Proc. CVPR},\n  year      = {2020}\n}\n```\n\n## Acknowledgements\n\nWe thank Ming-Yu Liu for an early review, Timo Viitanen for his help with code release, and Tero Kuosmanen for compute infrastructure.\n"
        },
        {
          "name": "dataset_tool.py",
          "type": "blob",
          "size": 29.572265625,
          "content": "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, visit\n# https://nvlabs.github.io/stylegan2/license.html\n\n\"\"\"Tool for creating multi-resolution TFRecords datasets.\"\"\"\n\n# pylint: disable=too-many-lines\nimport os\nimport sys\nimport glob\nimport argparse\nimport threading\nimport six.moves.queue as Queue # pylint: disable=import-error\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport PIL.Image\nimport dnnlib.tflib as tflib\n\nfrom training import dataset\n\n#----------------------------------------------------------------------------\n\ndef error(msg):\n    print('Error: ' + msg)\n    exit(1)\n\n#----------------------------------------------------------------------------\n\nclass TFRecordExporter:\n    def __init__(self, tfrecord_dir, expected_images, print_progress=True, progress_interval=10):\n        self.tfrecord_dir       = tfrecord_dir\n        self.tfr_prefix         = os.path.join(self.tfrecord_dir, os.path.basename(self.tfrecord_dir))\n        self.expected_images    = expected_images\n        self.cur_images         = 0\n        self.shape              = None\n        self.resolution_log2    = None\n        self.tfr_writers        = []\n        self.print_progress     = print_progress\n        self.progress_interval  = progress_interval\n\n        if self.print_progress:\n            print('Creating dataset \"%s\"' % tfrecord_dir)\n        if not os.path.isdir(self.tfrecord_dir):\n            os.makedirs(self.tfrecord_dir)\n        assert os.path.isdir(self.tfrecord_dir)\n\n    def close(self):\n        if self.print_progress:\n            print('%-40s\\r' % 'Flushing data...', end='', flush=True)\n        for tfr_writer in self.tfr_writers:\n            tfr_writer.close()\n        self.tfr_writers = []\n        if self.print_progress:\n            print('%-40s\\r' % '', end='', flush=True)\n            print('Added %d images.' % self.cur_images)\n\n    def choose_shuffled_order(self): # Note: Images and labels must be added in shuffled order.\n        order = np.arange(self.expected_images)\n        np.random.RandomState(123).shuffle(order)\n        return order\n\n    def add_image(self, img):\n        if self.print_progress and self.cur_images % self.progress_interval == 0:\n            print('%d / %d\\r' % (self.cur_images, self.expected_images), end='', flush=True)\n        if self.shape is None:\n            self.shape = img.shape\n            self.resolution_log2 = int(np.log2(self.shape[1]))\n            assert self.shape[0] in [1, 3]\n            assert self.shape[1] == self.shape[2]\n            assert self.shape[1] == 2**self.resolution_log2\n            tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n            for lod in range(self.resolution_log2 - 1):\n                tfr_file = self.tfr_prefix + '-r%02d.tfrecords' % (self.resolution_log2 - lod)\n                self.tfr_writers.append(tf.python_io.TFRecordWriter(tfr_file, tfr_opt))\n        assert img.shape == self.shape\n        for lod, tfr_writer in enumerate(self.tfr_writers):\n            if lod:\n                img = img.astype(np.float32)\n                img = (img[:, 0::2, 0::2] + img[:, 0::2, 1::2] + img[:, 1::2, 0::2] + img[:, 1::2, 1::2]) * 0.25\n            quant = np.rint(img).clip(0, 255).astype(np.uint8)\n            ex = tf.train.Example(features=tf.train.Features(feature={\n                'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=quant.shape)),\n                'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n            tfr_writer.write(ex.SerializeToString())\n        self.cur_images += 1\n\n    def add_labels(self, labels):\n        if self.print_progress:\n            print('%-40s\\r' % 'Saving labels...', end='', flush=True)\n        assert labels.shape[0] == self.cur_images\n        with open(self.tfr_prefix + '-rxx.labels', 'wb') as f:\n            np.save(f, labels.astype(np.float32))\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n#----------------------------------------------------------------------------\n\nclass ExceptionInfo(object):\n    def __init__(self):\n        self.value = sys.exc_info()[1]\n        self.traceback = traceback.format_exc()\n\n#----------------------------------------------------------------------------\n\nclass WorkerThread(threading.Thread):\n    def __init__(self, task_queue):\n        threading.Thread.__init__(self)\n        self.task_queue = task_queue\n\n    def run(self):\n        while True:\n            func, args, result_queue = self.task_queue.get()\n            if func is None:\n                break\n            try:\n                result = func(*args)\n            except:\n                result = ExceptionInfo()\n            result_queue.put((result, args))\n\n#----------------------------------------------------------------------------\n\nclass ThreadPool(object):\n    def __init__(self, num_threads):\n        assert num_threads >= 1\n        self.task_queue = Queue.Queue()\n        self.result_queues = dict()\n        self.num_threads = num_threads\n        for _idx in range(self.num_threads):\n            thread = WorkerThread(self.task_queue)\n            thread.daemon = True\n            thread.start()\n\n    def add_task(self, func, args=()):\n        assert hasattr(func, '__call__') # must be a function\n        if func not in self.result_queues:\n            self.result_queues[func] = Queue.Queue()\n        self.task_queue.put((func, args, self.result_queues[func]))\n\n    def get_result(self, func): # returns (result, args)\n        result, args = self.result_queues[func].get()\n        if isinstance(result, ExceptionInfo):\n            print('\\n\\nWorker thread caught an exception:\\n' + result.traceback)\n            raise result.value\n        return result, args\n\n    def finish(self):\n        for _idx in range(self.num_threads):\n            self.task_queue.put((None, (), None))\n\n    def __enter__(self): # for 'with' statement\n        return self\n\n    def __exit__(self, *excinfo):\n        self.finish()\n\n    def process_items_concurrently(self, item_iterator, process_func=lambda x: x, pre_func=lambda x: x, post_func=lambda x: x, max_items_in_flight=None):\n        if max_items_in_flight is None: max_items_in_flight = self.num_threads * 4\n        assert max_items_in_flight >= 1\n        results = []\n        retire_idx = [0]\n\n        def task_func(prepared, _idx):\n            return process_func(prepared)\n\n        def retire_result():\n            processed, (_prepared, idx) = self.get_result(task_func)\n            results[idx] = processed\n            while retire_idx[0] < len(results) and results[retire_idx[0]] is not None:\n                yield post_func(results[retire_idx[0]])\n                results[retire_idx[0]] = None\n                retire_idx[0] += 1\n\n        for idx, item in enumerate(item_iterator):\n            prepared = pre_func(item)\n            results.append(None)\n            self.add_task(func=task_func, args=(prepared, idx))\n            while retire_idx[0] < idx - max_items_in_flight + 2:\n                for res in retire_result(): yield res\n        while retire_idx[0] < len(results):\n            for res in retire_result(): yield res\n\n#----------------------------------------------------------------------------\n\ndef display(tfrecord_dir):\n    print('Loading dataset \"%s\"' % tfrecord_dir)\n    tflib.init_tf({'gpu_options.allow_growth': True})\n    dset = dataset.TFRecordDataset(tfrecord_dir, max_label_size='full', repeat=False, shuffle_mb=0)\n    tflib.init_uninitialized_vars()\n    import cv2  # pip install opencv-python\n\n    idx = 0\n    while True:\n        try:\n            images, labels = dset.get_minibatch_np(1)\n        except tf.errors.OutOfRangeError:\n            break\n        if idx == 0:\n            print('Displaying images')\n            cv2.namedWindow('dataset_tool')\n            print('Press SPACE or ENTER to advance, ESC to exit')\n        print('\\nidx = %-8d\\nlabel = %s' % (idx, labels[0].tolist()))\n        cv2.imshow('dataset_tool', images[0].transpose(1, 2, 0)[:, :, ::-1]) # CHW => HWC, RGB => BGR\n        idx += 1\n        if cv2.waitKey() == 27:\n            break\n    print('\\nDisplayed %d images.' % idx)\n\n#----------------------------------------------------------------------------\n\ndef extract(tfrecord_dir, output_dir):\n    print('Loading dataset \"%s\"' % tfrecord_dir)\n    tflib.init_tf({'gpu_options.allow_growth': True})\n    dset = dataset.TFRecordDataset(tfrecord_dir, max_label_size=0, repeat=False, shuffle_mb=0)\n    tflib.init_uninitialized_vars()\n\n    print('Extracting images to \"%s\"' % output_dir)\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n    idx = 0\n    while True:\n        if idx % 10 == 0:\n            print('%d\\r' % idx, end='', flush=True)\n        try:\n            images, _labels = dset.get_minibatch_np(1)\n        except tf.errors.OutOfRangeError:\n            break\n        if images.shape[1] == 1:\n            img = PIL.Image.fromarray(images[0][0], 'L')\n        else:\n            img = PIL.Image.fromarray(images[0].transpose(1, 2, 0), 'RGB')\n        img.save(os.path.join(output_dir, 'img%08d.png' % idx))\n        idx += 1\n    print('Extracted %d images.' % idx)\n\n#----------------------------------------------------------------------------\n\ndef compare(tfrecord_dir_a, tfrecord_dir_b, ignore_labels):\n    max_label_size = 0 if ignore_labels else 'full'\n    print('Loading dataset \"%s\"' % tfrecord_dir_a)\n    tflib.init_tf({'gpu_options.allow_growth': True})\n    dset_a = dataset.TFRecordDataset(tfrecord_dir_a, max_label_size=max_label_size, repeat=False, shuffle_mb=0)\n    print('Loading dataset \"%s\"' % tfrecord_dir_b)\n    dset_b = dataset.TFRecordDataset(tfrecord_dir_b, max_label_size=max_label_size, repeat=False, shuffle_mb=0)\n    tflib.init_uninitialized_vars()\n\n    print('Comparing datasets')\n    idx = 0\n    identical_images = 0\n    identical_labels = 0\n    while True:\n        if idx % 100 == 0:\n            print('%d\\r' % idx, end='', flush=True)\n        try:\n            images_a, labels_a = dset_a.get_minibatch_np(1)\n        except tf.errors.OutOfRangeError:\n            images_a, labels_a = None, None\n        try:\n            images_b, labels_b = dset_b.get_minibatch_np(1)\n        except tf.errors.OutOfRangeError:\n            images_b, labels_b = None, None\n        if images_a is None or images_b is None:\n            if images_a is not None or images_b is not None:\n                print('Datasets contain different number of images')\n            break\n        if images_a.shape == images_b.shape and np.all(images_a == images_b):\n            identical_images += 1\n        else:\n            print('Image %d is different' % idx)\n        if labels_a.shape == labels_b.shape and np.all(labels_a == labels_b):\n            identical_labels += 1\n        else:\n            print('Label %d is different' % idx)\n        idx += 1\n    print('Identical images: %d / %d' % (identical_images, idx))\n    if not ignore_labels:\n        print('Identical labels: %d / %d' % (identical_labels, idx))\n\n#----------------------------------------------------------------------------\n\ndef create_mnist(tfrecord_dir, mnist_dir):\n    print('Loading MNIST from \"%s\"' % mnist_dir)\n    import gzip\n    with gzip.open(os.path.join(mnist_dir, 'train-images-idx3-ubyte.gz'), 'rb') as file:\n        images = np.frombuffer(file.read(), np.uint8, offset=16)\n    with gzip.open(os.path.join(mnist_dir, 'train-labels-idx1-ubyte.gz'), 'rb') as file:\n        labels = np.frombuffer(file.read(), np.uint8, offset=8)\n    images = images.reshape(-1, 1, 28, 28)\n    images = np.pad(images, [(0,0), (0,0), (2,2), (2,2)], 'constant', constant_values=0)\n    assert images.shape == (60000, 1, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (60000,) and labels.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n    onehot = np.zeros((labels.size, np.max(labels) + 1), dtype=np.float32)\n    onehot[np.arange(labels.size), labels] = 1.0\n\n    with TFRecordExporter(tfrecord_dir, images.shape[0]) as tfr:\n        order = tfr.choose_shuffled_order()\n        for idx in range(order.size):\n            tfr.add_image(images[order[idx]])\n        tfr.add_labels(onehot[order])\n\n#----------------------------------------------------------------------------\n\ndef create_mnistrgb(tfrecord_dir, mnist_dir, num_images=1000000, random_seed=123):\n    print('Loading MNIST from \"%s\"' % mnist_dir)\n    import gzip\n    with gzip.open(os.path.join(mnist_dir, 'train-images-idx3-ubyte.gz'), 'rb') as file:\n        images = np.frombuffer(file.read(), np.uint8, offset=16)\n    images = images.reshape(-1, 28, 28)\n    images = np.pad(images, [(0,0), (2,2), (2,2)], 'constant', constant_values=0)\n    assert images.shape == (60000, 32, 32) and images.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n\n    with TFRecordExporter(tfrecord_dir, num_images) as tfr:\n        rnd = np.random.RandomState(random_seed)\n        for _idx in range(num_images):\n            tfr.add_image(images[rnd.randint(images.shape[0], size=3)])\n\n#----------------------------------------------------------------------------\n\ndef create_cifar10(tfrecord_dir, cifar10_dir):\n    print('Loading CIFAR-10 from \"%s\"' % cifar10_dir)\n    import pickle\n    images = []\n    labels = []\n    for batch in range(1, 6):\n        with open(os.path.join(cifar10_dir, 'data_batch_%d' % batch), 'rb') as file:\n            data = pickle.load(file, encoding='latin1')\n        images.append(data['data'].reshape(-1, 3, 32, 32))\n        labels.append(data['labels'])\n    images = np.concatenate(images)\n    labels = np.concatenate(labels)\n    assert images.shape == (50000, 3, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (50000,) and labels.dtype == np.int32\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n    onehot = np.zeros((labels.size, np.max(labels) + 1), dtype=np.float32)\n    onehot[np.arange(labels.size), labels] = 1.0\n\n    with TFRecordExporter(tfrecord_dir, images.shape[0]) as tfr:\n        order = tfr.choose_shuffled_order()\n        for idx in range(order.size):\n            tfr.add_image(images[order[idx]])\n        tfr.add_labels(onehot[order])\n\n#----------------------------------------------------------------------------\n\ndef create_cifar100(tfrecord_dir, cifar100_dir):\n    print('Loading CIFAR-100 from \"%s\"' % cifar100_dir)\n    import pickle\n    with open(os.path.join(cifar100_dir, 'train'), 'rb') as file:\n        data = pickle.load(file, encoding='latin1')\n    images = data['data'].reshape(-1, 3, 32, 32)\n    labels = np.array(data['fine_labels'])\n    assert images.shape == (50000, 3, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (50000,) and labels.dtype == np.int32\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 99\n    onehot = np.zeros((labels.size, np.max(labels) + 1), dtype=np.float32)\n    onehot[np.arange(labels.size), labels] = 1.0\n\n    with TFRecordExporter(tfrecord_dir, images.shape[0]) as tfr:\n        order = tfr.choose_shuffled_order()\n        for idx in range(order.size):\n            tfr.add_image(images[order[idx]])\n        tfr.add_labels(onehot[order])\n\n#----------------------------------------------------------------------------\n\ndef create_svhn(tfrecord_dir, svhn_dir):\n    print('Loading SVHN from \"%s\"' % svhn_dir)\n    import pickle\n    images = []\n    labels = []\n    for batch in range(1, 4):\n        with open(os.path.join(svhn_dir, 'train_%d.pkl' % batch), 'rb') as file:\n            data = pickle.load(file, encoding='latin1')\n        images.append(data[0])\n        labels.append(data[1])\n    images = np.concatenate(images)\n    labels = np.concatenate(labels)\n    assert images.shape == (73257, 3, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (73257,) and labels.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n    onehot = np.zeros((labels.size, np.max(labels) + 1), dtype=np.float32)\n    onehot[np.arange(labels.size), labels] = 1.0\n\n    with TFRecordExporter(tfrecord_dir, images.shape[0]) as tfr:\n        order = tfr.choose_shuffled_order()\n        for idx in range(order.size):\n            tfr.add_image(images[order[idx]])\n        tfr.add_labels(onehot[order])\n\n#----------------------------------------------------------------------------\n\ndef create_lsun(tfrecord_dir, lmdb_dir, resolution=256, max_images=None):\n    print('Loading LSUN dataset from \"%s\"' % lmdb_dir)\n    import lmdb # pip install lmdb # pylint: disable=import-error\n    import cv2 # pip install opencv-python\n    import io\n    with lmdb.open(lmdb_dir, readonly=True).begin(write=False) as txn:\n        total_images = txn.stat()['entries'] # pylint: disable=no-value-for-parameter\n        if max_images is None:\n            max_images = total_images\n        with TFRecordExporter(tfrecord_dir, max_images) as tfr:\n            for _idx, (_key, value) in enumerate(txn.cursor()):\n                try:\n                    try:\n                        img = cv2.imdecode(np.fromstring(value, dtype=np.uint8), 1)\n                        if img is None:\n                            raise IOError('cv2.imdecode failed')\n                        img = img[:, :, ::-1] # BGR => RGB\n                    except IOError:\n                        img = np.asarray(PIL.Image.open(io.BytesIO(value)))\n                    crop = np.min(img.shape[:2])\n                    img = img[(img.shape[0] - crop) // 2 : (img.shape[0] + crop) // 2, (img.shape[1] - crop) // 2 : (img.shape[1] + crop) // 2]\n                    img = PIL.Image.fromarray(img, 'RGB')\n                    img = img.resize((resolution, resolution), PIL.Image.ANTIALIAS)\n                    img = np.asarray(img)\n                    img = img.transpose([2, 0, 1]) # HWC => CHW\n                    tfr.add_image(img)\n                except:\n                    print(sys.exc_info()[1])\n                if tfr.cur_images == max_images:\n                    break\n\n#----------------------------------------------------------------------------\n\ndef create_lsun_wide(tfrecord_dir, lmdb_dir, width=512, height=384, max_images=None):\n    assert width == 2 ** int(np.round(np.log2(width)))\n    assert height <= width\n    print('Loading LSUN dataset from \"%s\"' % lmdb_dir)\n    import lmdb # pip install lmdb # pylint: disable=import-error\n    import cv2 # pip install opencv-python\n    import io\n    with lmdb.open(lmdb_dir, readonly=True).begin(write=False) as txn:\n        total_images = txn.stat()['entries'] # pylint: disable=no-value-for-parameter\n        if max_images is None:\n            max_images = total_images\n        with TFRecordExporter(tfrecord_dir, max_images, print_progress=False) as tfr:\n            for idx, (_key, value) in enumerate(txn.cursor()):\n                try:\n                    try:\n                        img = cv2.imdecode(np.fromstring(value, dtype=np.uint8), 1)\n                        if img is None:\n                            raise IOError('cv2.imdecode failed')\n                        img = img[:, :, ::-1] # BGR => RGB\n                    except IOError:\n                        img = np.asarray(PIL.Image.open(io.BytesIO(value)))\n\n                    ch = int(np.round(width * img.shape[0] / img.shape[1]))\n                    if img.shape[1] < width or ch < height:\n                        continue\n\n                    img = img[(img.shape[0] - ch) // 2 : (img.shape[0] + ch) // 2]\n                    img = PIL.Image.fromarray(img, 'RGB')\n                    img = img.resize((width, height), PIL.Image.ANTIALIAS)\n                    img = np.asarray(img)\n                    img = img.transpose([2, 0, 1]) # HWC => CHW\n\n                    canvas = np.zeros([3, width, width], dtype=np.uint8)\n                    canvas[:, (width - height) // 2 : (width + height) // 2] = img\n                    tfr.add_image(canvas)\n                    print('\\r%d / %d => %d ' % (idx + 1, total_images, tfr.cur_images), end='')\n\n                except:\n                    print(sys.exc_info()[1])\n                if tfr.cur_images == max_images:\n                    break\n    print()\n\n#----------------------------------------------------------------------------\n\ndef create_celeba(tfrecord_dir, celeba_dir, cx=89, cy=121):\n    print('Loading CelebA from \"%s\"' % celeba_dir)\n    glob_pattern = os.path.join(celeba_dir, 'img_align_celeba_png', '*.png')\n    image_filenames = sorted(glob.glob(glob_pattern))\n    expected_images = 202599\n    if len(image_filenames) != expected_images:\n        error('Expected to find %d images' % expected_images)\n\n    with TFRecordExporter(tfrecord_dir, len(image_filenames)) as tfr:\n        order = tfr.choose_shuffled_order()\n        for idx in range(order.size):\n            img = np.asarray(PIL.Image.open(image_filenames[order[idx]]))\n            assert img.shape == (218, 178, 3)\n            img = img[cy - 64 : cy + 64, cx - 64 : cx + 64]\n            img = img.transpose(2, 0, 1) # HWC => CHW\n            tfr.add_image(img)\n\n#----------------------------------------------------------------------------\n\ndef create_from_images(tfrecord_dir, image_dir, shuffle):\n    print('Loading images from \"%s\"' % image_dir)\n    image_filenames = sorted(glob.glob(os.path.join(image_dir, '*')))\n    if len(image_filenames) == 0:\n        error('No input images found')\n\n    img = np.asarray(PIL.Image.open(image_filenames[0]))\n    resolution = img.shape[0]\n    channels = img.shape[2] if img.ndim == 3 else 1\n    if img.shape[1] != resolution:\n        error('Input images must have the same width and height')\n    if resolution != 2 ** int(np.floor(np.log2(resolution))):\n        error('Input image resolution must be a power-of-two')\n    if channels not in [1, 3]:\n        error('Input images must be stored as RGB or grayscale')\n\n    with TFRecordExporter(tfrecord_dir, len(image_filenames)) as tfr:\n        order = tfr.choose_shuffled_order() if shuffle else np.arange(len(image_filenames))\n        for idx in range(order.size):\n            img = np.asarray(PIL.Image.open(image_filenames[order[idx]]))\n            if channels == 1:\n                img = img[np.newaxis, :, :] # HW => CHW\n            else:\n                img = img.transpose([2, 0, 1]) # HWC => CHW\n            tfr.add_image(img)\n\n#----------------------------------------------------------------------------\n\ndef create_from_hdf5(tfrecord_dir, hdf5_filename, shuffle):\n    print('Loading HDF5 archive from \"%s\"' % hdf5_filename)\n    import h5py # conda install h5py\n    with h5py.File(hdf5_filename, 'r') as hdf5_file:\n        hdf5_data = max([value for key, value in hdf5_file.items() if key.startswith('data')], key=lambda lod: lod.shape[3])\n        with TFRecordExporter(tfrecord_dir, hdf5_data.shape[0]) as tfr:\n            order = tfr.choose_shuffled_order() if shuffle else np.arange(hdf5_data.shape[0])\n            for idx in range(order.size):\n                tfr.add_image(hdf5_data[order[idx]])\n            npy_filename = os.path.splitext(hdf5_filename)[0] + '-labels.npy'\n            if os.path.isfile(npy_filename):\n                tfr.add_labels(np.load(npy_filename)[order])\n\n#----------------------------------------------------------------------------\n\ndef execute_cmdline(argv):\n    prog = argv[0]\n    parser = argparse.ArgumentParser(\n        prog        = prog,\n        description = 'Tool for creating multi-resolution TFRecords datasets for StyleGAN and ProGAN.',\n        epilog      = 'Type \"%s <command> -h\" for more information.' % prog)\n\n    subparsers = parser.add_subparsers(dest='command')\n    subparsers.required = True\n    def add_command(cmd, desc, example=None):\n        epilog = 'Example: %s %s' % (prog, example) if example is not None else None\n        return subparsers.add_parser(cmd, description=desc, help=desc, epilog=epilog)\n\n    p = add_command(    'display',          'Display images in dataset.',\n                                            'display datasets/mnist')\n    p.add_argument(     'tfrecord_dir',     help='Directory containing dataset')\n\n    p = add_command(    'extract',          'Extract images from dataset.',\n                                            'extract datasets/mnist mnist-images')\n    p.add_argument(     'tfrecord_dir',     help='Directory containing dataset')\n    p.add_argument(     'output_dir',       help='Directory to extract the images into')\n\n    p = add_command(    'compare',          'Compare two datasets.',\n                                            'compare datasets/mydataset datasets/mnist')\n    p.add_argument(     'tfrecord_dir_a',   help='Directory containing first dataset')\n    p.add_argument(     'tfrecord_dir_b',   help='Directory containing second dataset')\n    p.add_argument(     '--ignore_labels',  help='Ignore labels (default: 0)', type=int, default=0)\n\n    p = add_command(    'create_mnist',     'Create dataset for MNIST.',\n                                            'create_mnist datasets/mnist ~/downloads/mnist')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'mnist_dir',        help='Directory containing MNIST')\n\n    p = add_command(    'create_mnistrgb',  'Create dataset for MNIST-RGB.',\n                                            'create_mnistrgb datasets/mnistrgb ~/downloads/mnist')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'mnist_dir',        help='Directory containing MNIST')\n    p.add_argument(     '--num_images',     help='Number of composite images to create (default: 1000000)', type=int, default=1000000)\n    p.add_argument(     '--random_seed',    help='Random seed (default: 123)', type=int, default=123)\n\n    p = add_command(    'create_cifar10',   'Create dataset for CIFAR-10.',\n                                            'create_cifar10 datasets/cifar10 ~/downloads/cifar10')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'cifar10_dir',      help='Directory containing CIFAR-10')\n\n    p = add_command(    'create_cifar100',  'Create dataset for CIFAR-100.',\n                                            'create_cifar100 datasets/cifar100 ~/downloads/cifar100')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'cifar100_dir',     help='Directory containing CIFAR-100')\n\n    p = add_command(    'create_svhn',      'Create dataset for SVHN.',\n                                            'create_svhn datasets/svhn ~/downloads/svhn')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'svhn_dir',         help='Directory containing SVHN')\n\n    p = add_command(    'create_lsun',      'Create dataset for single LSUN category.',\n                                            'create_lsun datasets/lsun-car-100k ~/downloads/lsun/car_lmdb --resolution 256 --max_images 100000')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'lmdb_dir',         help='Directory containing LMDB database')\n    p.add_argument(     '--resolution',     help='Output resolution (default: 256)', type=int, default=256)\n    p.add_argument(     '--max_images',     help='Maximum number of images (default: none)', type=int, default=None)\n\n    p = add_command(    'create_lsun_wide', 'Create LSUN dataset with non-square aspect ratio.',\n                                            'create_lsun_wide datasets/lsun-car-512x384 ~/downloads/lsun/car_lmdb --width 512 --height 384')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'lmdb_dir',         help='Directory containing LMDB database')\n    p.add_argument(     '--width',          help='Output width (default: 512)', type=int, default=512)\n    p.add_argument(     '--height',         help='Output height (default: 384)', type=int, default=384)\n    p.add_argument(     '--max_images',     help='Maximum number of images (default: none)', type=int, default=None)\n\n    p = add_command(    'create_celeba',    'Create dataset for CelebA.',\n                                            'create_celeba datasets/celeba ~/downloads/celeba')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'celeba_dir',       help='Directory containing CelebA')\n    p.add_argument(     '--cx',             help='Center X coordinate (default: 89)', type=int, default=89)\n    p.add_argument(     '--cy',             help='Center Y coordinate (default: 121)', type=int, default=121)\n\n    p = add_command(    'create_from_images', 'Create dataset from a directory full of images.',\n                                            'create_from_images datasets/mydataset myimagedir')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'image_dir',        help='Directory containing the images')\n    p.add_argument(     '--shuffle',        help='Randomize image order (default: 1)', type=int, default=1)\n\n    p = add_command(    'create_from_hdf5', 'Create dataset from legacy HDF5 archive.',\n                                            'create_from_hdf5 datasets/celebahq ~/downloads/celeba-hq-1024x1024.h5')\n    p.add_argument(     'tfrecord_dir',     help='New dataset directory to be created')\n    p.add_argument(     'hdf5_filename',    help='HDF5 archive containing the images')\n    p.add_argument(     '--shuffle',        help='Randomize image order (default: 1)', type=int, default=1)\n\n    args = parser.parse_args(argv[1:] if len(argv) > 1 else ['-h'])\n    func = globals()[args.command]\n    del args.command\n    func(**vars(args))\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    execute_cmdline(sys.argv)\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "dnnlib",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "metrics",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretrained_networks.py",
          "type": "blob",
          "size": 7.3447265625,
          "content": "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, visit\n# https://nvlabs.github.io/stylegan2/license.html\n\n\"\"\"List of pre-trained StyleGAN2 networks located on Google Drive.\"\"\"\n\nimport pickle\nimport dnnlib\nimport dnnlib.tflib as tflib\n\n#----------------------------------------------------------------------------\n# StyleGAN2 Google Drive root: https://drive.google.com/open?id=1QHc-yF5C3DChRwSdZKcx1w6K8JvSxQi7\n\ngdrive_urls = {\n    'gdrive:networks/stylegan2-car-config-a.pkl':                           'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-a.pkl',\n    'gdrive:networks/stylegan2-car-config-b.pkl':                           'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-b.pkl',\n    'gdrive:networks/stylegan2-car-config-c.pkl':                           'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-c.pkl',\n    'gdrive:networks/stylegan2-car-config-d.pkl':                           'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-d.pkl',\n    'gdrive:networks/stylegan2-car-config-e.pkl':                           'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-e.pkl',\n    'gdrive:networks/stylegan2-car-config-f.pkl':                           'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-f.pkl',\n    'gdrive:networks/stylegan2-cat-config-a.pkl':                           'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-a.pkl',\n    'gdrive:networks/stylegan2-cat-config-f.pkl':                           'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl',\n    'gdrive:networks/stylegan2-church-config-a.pkl':                        'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-church-config-a.pkl',\n    'gdrive:networks/stylegan2-church-config-f.pkl':                        'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-church-config-f.pkl',\n    'gdrive:networks/stylegan2-ffhq-config-a.pkl':                          'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-a.pkl',\n    'gdrive:networks/stylegan2-ffhq-config-b.pkl':                          'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-b.pkl',\n    'gdrive:networks/stylegan2-ffhq-config-c.pkl':                          'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-c.pkl',\n    'gdrive:networks/stylegan2-ffhq-config-d.pkl':                          'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-d.pkl',\n    'gdrive:networks/stylegan2-ffhq-config-e.pkl':                          'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-e.pkl',\n    'gdrive:networks/stylegan2-ffhq-config-f.pkl':                          'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl',\n    'gdrive:networks/stylegan2-horse-config-a.pkl':                         'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-horse-config-a.pkl',\n    'gdrive:networks/stylegan2-horse-config-f.pkl':                         'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-horse-config-f.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gorig-Dorig.pkl':        'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gorig-Dorig.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gorig-Dresnet.pkl':      'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gorig-Dresnet.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gorig-Dskip.pkl':        'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gorig-Dskip.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gresnet-Dorig.pkl':      'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gresnet-Dorig.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gresnet-Dresnet.pkl':    'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gresnet-Dresnet.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gresnet-Dskip.pkl':      'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gresnet-Dskip.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gskip-Dorig.pkl':        'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gskip-Dorig.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gskip-Dresnet.pkl':      'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gskip-Dresnet.pkl',\n    'gdrive:networks/table2/stylegan2-car-config-e-Gskip-Dskip.pkl':        'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-car-config-e-Gskip-Dskip.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gorig-Dorig.pkl':       'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gorig-Dorig.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gorig-Dresnet.pkl':     'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gorig-Dresnet.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gorig-Dskip.pkl':       'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gorig-Dskip.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gresnet-Dorig.pkl':     'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gresnet-Dorig.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gresnet-Dresnet.pkl':   'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gresnet-Dresnet.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gresnet-Dskip.pkl':     'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gresnet-Dskip.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gskip-Dorig.pkl':       'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gskip-Dorig.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gskip-Dresnet.pkl':     'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gskip-Dresnet.pkl',\n    'gdrive:networks/table2/stylegan2-ffhq-config-e-Gskip-Dskip.pkl':       'https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/table2/stylegan2-ffhq-config-e-Gskip-Dskip.pkl',\n}\n\n#----------------------------------------------------------------------------\n\ndef get_path_or_url(path_or_gdrive_path):\n    return gdrive_urls.get(path_or_gdrive_path, path_or_gdrive_path)\n\n#----------------------------------------------------------------------------\n\n_cached_networks = dict()\n\ndef load_networks(path_or_gdrive_path):\n    path_or_url = get_path_or_url(path_or_gdrive_path)\n    if path_or_url in _cached_networks:\n        return _cached_networks[path_or_url]\n\n    if dnnlib.util.is_url(path_or_url):\n        stream = dnnlib.util.open_url(path_or_url, cache_dir='.stylegan2-cache')\n    else:\n        stream = open(path_or_url, 'rb')\n\n    tflib.init_tf()\n    with stream:\n        G, D, Gs = pickle.load(stream, encoding='latin1')\n    _cached_networks[path_or_url] = G, D, Gs\n    return G, D, Gs\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "projector.py",
          "type": "blob",
          "size": 8.7646484375,
          "content": "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, visit\n# https://nvlabs.github.io/stylegan2/license.html\n\nimport numpy as np\nimport tensorflow as tf\nimport dnnlib\nimport dnnlib.tflib as tflib\n\nfrom training import misc\n\n#----------------------------------------------------------------------------\n\nclass Projector:\n    def __init__(self):\n        self.num_steps                  = 1000\n        self.dlatent_avg_samples        = 10000\n        self.initial_learning_rate      = 0.1\n        self.initial_noise_factor       = 0.05\n        self.lr_rampdown_length         = 0.25\n        self.lr_rampup_length           = 0.05\n        self.noise_ramp_length          = 0.75\n        self.regularize_noise_weight    = 1e5\n        self.verbose                    = False\n        self.clone_net                  = True\n\n        self._Gs                    = None\n        self._minibatch_size        = None\n        self._dlatent_avg           = None\n        self._dlatent_std           = None\n        self._noise_vars            = None\n        self._noise_init_op         = None\n        self._noise_normalize_op    = None\n        self._dlatents_var          = None\n        self._noise_in              = None\n        self._dlatents_expr         = None\n        self._images_expr           = None\n        self._target_images_var     = None\n        self._lpips                 = None\n        self._dist                  = None\n        self._loss                  = None\n        self._reg_sizes             = None\n        self._lrate_in              = None\n        self._opt                   = None\n        self._opt_step              = None\n        self._cur_step              = None\n\n    def _info(self, *args):\n        if self.verbose:\n            print('Projector:', *args)\n\n    def set_network(self, Gs, minibatch_size=1):\n        assert minibatch_size == 1\n        self._Gs = Gs\n        self._minibatch_size = minibatch_size\n        if self._Gs is None:\n            return\n        if self.clone_net:\n            self._Gs = self._Gs.clone()\n\n        # Find dlatent stats.\n        self._info('Finding W midpoint and stddev using %d samples...' % self.dlatent_avg_samples)\n        latent_samples = np.random.RandomState(123).randn(self.dlatent_avg_samples, *self._Gs.input_shapes[0][1:])\n        dlatent_samples = self._Gs.components.mapping.run(latent_samples, None)[:, :1, :] # [N, 1, 512]\n        self._dlatent_avg = np.mean(dlatent_samples, axis=0, keepdims=True) # [1, 1, 512]\n        self._dlatent_std = (np.sum((dlatent_samples - self._dlatent_avg) ** 2) / self.dlatent_avg_samples) ** 0.5\n        self._info('std = %g' % self._dlatent_std)\n\n        # Find noise inputs.\n        self._info('Setting up noise inputs...')\n        self._noise_vars = []\n        noise_init_ops = []\n        noise_normalize_ops = []\n        while True:\n            n = 'G_synthesis/noise%d' % len(self._noise_vars)\n            if not n in self._Gs.vars:\n                break\n            v = self._Gs.vars[n]\n            self._noise_vars.append(v)\n            noise_init_ops.append(tf.assign(v, tf.random_normal(tf.shape(v), dtype=tf.float32)))\n            noise_mean = tf.reduce_mean(v)\n            noise_std = tf.reduce_mean((v - noise_mean)**2)**0.5\n            noise_normalize_ops.append(tf.assign(v, (v - noise_mean) / noise_std))\n            self._info(n, v)\n        self._noise_init_op = tf.group(*noise_init_ops)\n        self._noise_normalize_op = tf.group(*noise_normalize_ops)\n\n        # Image output graph.\n        self._info('Building image output graph...')\n        self._dlatents_var = tf.Variable(tf.zeros([self._minibatch_size] + list(self._dlatent_avg.shape[1:])), name='dlatents_var')\n        self._noise_in = tf.placeholder(tf.float32, [], name='noise_in')\n        dlatents_noise = tf.random.normal(shape=self._dlatents_var.shape) * self._noise_in\n        self._dlatents_expr = tf.tile(self._dlatents_var + dlatents_noise, [1, self._Gs.components.synthesis.input_shape[1], 1])\n        self._images_expr = self._Gs.components.synthesis.get_output_for(self._dlatents_expr, randomize_noise=False)\n\n        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n        proc_images_expr = (self._images_expr + 1) * (255 / 2)\n        sh = proc_images_expr.shape.as_list()\n        if sh[2] > 256:\n            factor = sh[2] // 256\n            proc_images_expr = tf.reduce_mean(tf.reshape(proc_images_expr, [-1, sh[1], sh[2] // factor, factor, sh[2] // factor, factor]), axis=[3,5])\n\n        # Loss graph.\n        self._info('Building loss graph...')\n        self._target_images_var = tf.Variable(tf.zeros(proc_images_expr.shape), name='target_images_var')\n        if self._lpips is None:\n            self._lpips = misc.load_pkl('https://nvlabs-fi-cdn.nvidia.com/stylegan/networks/metrics/vgg16_zhang_perceptual.pkl')\n        self._dist = self._lpips.get_output_for(proc_images_expr, self._target_images_var)\n        self._loss = tf.reduce_sum(self._dist)\n\n        # Noise regularization graph.\n        self._info('Building noise regularization graph...')\n        reg_loss = 0.0\n        for v in self._noise_vars:\n            sz = v.shape[2]\n            while True:\n                reg_loss += tf.reduce_mean(v * tf.roll(v, shift=1, axis=3))**2 + tf.reduce_mean(v * tf.roll(v, shift=1, axis=2))**2\n                if sz <= 8:\n                    break # Small enough already\n                v = tf.reshape(v, [1, 1, sz//2, 2, sz//2, 2]) # Downscale\n                v = tf.reduce_mean(v, axis=[3, 5])\n                sz = sz // 2\n        self._loss += reg_loss * self.regularize_noise_weight\n\n        # Optimizer.\n        self._info('Setting up optimizer...')\n        self._lrate_in = tf.placeholder(tf.float32, [], name='lrate_in')\n        self._opt = dnnlib.tflib.Optimizer(learning_rate=self._lrate_in)\n        self._opt.register_gradients(self._loss, [self._dlatents_var] + self._noise_vars)\n        self._opt_step = self._opt.apply_updates()\n\n    def run(self, target_images):\n        # Run to completion.\n        self.start(target_images)\n        while self._cur_step < self.num_steps:\n            self.step()\n\n        # Collect results.\n        pres = dnnlib.EasyDict()\n        pres.dlatents = self.get_dlatents()\n        pres.noises = self.get_noises()\n        pres.images = self.get_images()\n        return pres\n\n    def start(self, target_images):\n        assert self._Gs is not None\n\n        # Prepare target images.\n        self._info('Preparing target images...')\n        target_images = np.asarray(target_images, dtype='float32')\n        target_images = (target_images + 1) * (255 / 2)\n        sh = target_images.shape\n        assert sh[0] == self._minibatch_size\n        if sh[2] > self._target_images_var.shape[2]:\n            factor = sh[2] // self._target_images_var.shape[2]\n            target_images = np.reshape(target_images, [-1, sh[1], sh[2] // factor, factor, sh[3] // factor, factor]).mean((3, 5))\n\n        # Initialize optimization state.\n        self._info('Initializing optimization state...')\n        tflib.set_vars({self._target_images_var: target_images, self._dlatents_var: np.tile(self._dlatent_avg, [self._minibatch_size, 1, 1])})\n        tflib.run(self._noise_init_op)\n        self._opt.reset_optimizer_state()\n        self._cur_step = 0\n\n    def step(self):\n        assert self._cur_step is not None\n        if self._cur_step >= self.num_steps:\n            return\n        if self._cur_step == 0:\n            self._info('Running...')\n\n        # Hyperparameters.\n        t = self._cur_step / self.num_steps\n        noise_strength = self._dlatent_std * self.initial_noise_factor * max(0.0, 1.0 - t / self.noise_ramp_length) ** 2\n        lr_ramp = min(1.0, (1.0 - t) / self.lr_rampdown_length)\n        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n        lr_ramp = lr_ramp * min(1.0, t / self.lr_rampup_length)\n        learning_rate = self.initial_learning_rate * lr_ramp\n\n        # Train.\n        feed_dict = {self._noise_in: noise_strength, self._lrate_in: learning_rate}\n        _, dist_value, loss_value = tflib.run([self._opt_step, self._dist, self._loss], feed_dict)\n        tflib.run(self._noise_normalize_op)\n\n        # Print status.\n        self._cur_step += 1\n        if self._cur_step == self.num_steps or self._cur_step % 10 == 0:\n            self._info('%-8d%-12g%-12g' % (self._cur_step, dist_value, loss_value))\n        if self._cur_step == self.num_steps:\n            self._info('Done.')\n\n    def get_cur_step(self):\n        return self._cur_step\n\n    def get_dlatents(self):\n        return tflib.run(self._dlatents_expr, {self._noise_in: 0})\n\n    def get_noises(self):\n        return tflib.run(self._noise_vars)\n\n    def get_images(self):\n        return tflib.run(self._images_expr, {self._noise_in: 0})\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "run_generator.py",
          "type": "blob",
          "size": 8.0517578125,
          "content": "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, visit\n# https://nvlabs.github.io/stylegan2/license.html\n\nimport argparse\nimport numpy as np\nimport PIL.Image\nimport dnnlib\nimport dnnlib.tflib as tflib\nimport re\nimport sys\n\nimport pretrained_networks\n\n#----------------------------------------------------------------------------\n\ndef generate_images(network_pkl, seeds, truncation_psi):\n    print('Loading networks from \"%s\"...' % network_pkl)\n    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n\n    Gs_kwargs = dnnlib.EasyDict()\n    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n    Gs_kwargs.randomize_noise = False\n    if truncation_psi is not None:\n        Gs_kwargs.truncation_psi = truncation_psi\n\n    for seed_idx, seed in enumerate(seeds):\n        print('Generating image for seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n        rnd = np.random.RandomState(seed)\n        z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n        tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n        images = Gs.run(z, None, **Gs_kwargs) # [minibatch, height, width, channel]\n        PIL.Image.fromarray(images[0], 'RGB').save(dnnlib.make_run_dir_path('seed%04d.png' % seed))\n\n#----------------------------------------------------------------------------\n\ndef style_mixing_example(network_pkl, row_seeds, col_seeds, truncation_psi, col_styles, minibatch_size=4):\n    print('Loading networks from \"%s\"...' % network_pkl)\n    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n    w_avg = Gs.get_var('dlatent_avg') # [component]\n\n    Gs_syn_kwargs = dnnlib.EasyDict()\n    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n    Gs_syn_kwargs.randomize_noise = False\n    Gs_syn_kwargs.minibatch_size = minibatch_size\n\n    print('Generating W vectors...')\n    all_seeds = list(set(row_seeds + col_seeds))\n    all_z = np.stack([np.random.RandomState(seed).randn(*Gs.input_shape[1:]) for seed in all_seeds]) # [minibatch, component]\n    all_w = Gs.components.mapping.run(all_z, None) # [minibatch, layer, component]\n    all_w = w_avg + (all_w - w_avg) * truncation_psi # [minibatch, layer, component]\n    w_dict = {seed: w for seed, w in zip(all_seeds, list(all_w))} # [layer, component]\n\n    print('Generating images...')\n    all_images = Gs.components.synthesis.run(all_w, **Gs_syn_kwargs) # [minibatch, height, width, channel]\n    image_dict = {(seed, seed): image for seed, image in zip(all_seeds, list(all_images))}\n\n    print('Generating style-mixed images...')\n    for row_seed in row_seeds:\n        for col_seed in col_seeds:\n            w = w_dict[row_seed].copy()\n            w[col_styles] = w_dict[col_seed][col_styles]\n            image = Gs.components.synthesis.run(w[np.newaxis], **Gs_syn_kwargs)[0]\n            image_dict[(row_seed, col_seed)] = image\n\n    print('Saving images...')\n    for (row_seed, col_seed), image in image_dict.items():\n        PIL.Image.fromarray(image, 'RGB').save(dnnlib.make_run_dir_path('%d-%d.png' % (row_seed, col_seed)))\n\n    print('Saving image grid...')\n    _N, _C, H, W = Gs.output_shape\n    canvas = PIL.Image.new('RGB', (W * (len(col_seeds) + 1), H * (len(row_seeds) + 1)), 'black')\n    for row_idx, row_seed in enumerate([None] + row_seeds):\n        for col_idx, col_seed in enumerate([None] + col_seeds):\n            if row_seed is None and col_seed is None:\n                continue\n            key = (row_seed, col_seed)\n            if row_seed is None:\n                key = (col_seed, col_seed)\n            if col_seed is None:\n                key = (row_seed, row_seed)\n            canvas.paste(PIL.Image.fromarray(image_dict[key], 'RGB'), (W * col_idx, H * row_idx))\n    canvas.save(dnnlib.make_run_dir_path('grid.png'))\n\n#----------------------------------------------------------------------------\n\ndef _parse_num_range(s):\n    '''Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.'''\n\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    m = range_re.match(s)\n    if m:\n        return list(range(int(m.group(1)), int(m.group(2))+1))\n    vals = s.split(',')\n    return [int(x) for x in vals]\n\n#----------------------------------------------------------------------------\n\n_examples = '''examples:\n\n  # Generate ffhq uncurated images (matches paper Figure 12)\n  python %(prog)s generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl --seeds=6600-6625 --truncation-psi=0.5\n\n  # Generate ffhq curated images (matches paper Figure 11)\n  python %(prog)s generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl --seeds=66,230,389,1518 --truncation-psi=1.0\n\n  # Generate uncurated car images (matches paper Figure 12)\n  python %(prog)s generate-images --network=gdrive:networks/stylegan2-car-config-f.pkl --seeds=6000-6025 --truncation-psi=0.5\n\n  # Generate style mixing example (matches style mixing video clip)\n  python %(prog)s style-mixing-example --network=gdrive:networks/stylegan2-ffhq-config-f.pkl --row-seeds=85,100,75,458,1500 --col-seeds=55,821,1789,293 --truncation-psi=1.0\n'''\n\n#----------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='''StyleGAN2 generator.\n\nRun 'python %(prog)s <subcommand> --help' for subcommand help.''',\n        epilog=_examples,\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n\n    subparsers = parser.add_subparsers(help='Sub-commands', dest='command')\n\n    parser_generate_images = subparsers.add_parser('generate-images', help='Generate images')\n    parser_generate_images.add_argument('--network', help='Network pickle filename', dest='network_pkl', required=True)\n    parser_generate_images.add_argument('--seeds', type=_parse_num_range, help='List of random seeds', required=True)\n    parser_generate_images.add_argument('--truncation-psi', type=float, help='Truncation psi (default: %(default)s)', default=0.5)\n    parser_generate_images.add_argument('--result-dir', help='Root directory for run results (default: %(default)s)', default='results', metavar='DIR')\n\n    parser_style_mixing_example = subparsers.add_parser('style-mixing-example', help='Generate style mixing video')\n    parser_style_mixing_example.add_argument('--network', help='Network pickle filename', dest='network_pkl', required=True)\n    parser_style_mixing_example.add_argument('--row-seeds', type=_parse_num_range, help='Random seeds to use for image rows', required=True)\n    parser_style_mixing_example.add_argument('--col-seeds', type=_parse_num_range, help='Random seeds to use for image columns', required=True)\n    parser_style_mixing_example.add_argument('--col-styles', type=_parse_num_range, help='Style layer range (default: %(default)s)', default='0-6')\n    parser_style_mixing_example.add_argument('--truncation-psi', type=float, help='Truncation psi (default: %(default)s)', default=0.5)\n    parser_style_mixing_example.add_argument('--result-dir', help='Root directory for run results (default: %(default)s)', default='results', metavar='DIR')\n\n    args = parser.parse_args()\n    kwargs = vars(args)\n    subcmd = kwargs.pop('command')\n\n    if subcmd is None:\n        print ('Error: missing subcommand.  Re-run with --help for usage.')\n        sys.exit(1)\n\n    sc = dnnlib.SubmitConfig()\n    sc.num_gpus = 1\n    sc.submit_target = dnnlib.SubmitTarget.LOCAL\n    sc.local.do_not_copy_source_files = True\n    sc.run_dir_root = kwargs.pop('result_dir')\n    sc.run_desc = subcmd\n\n    func_name_map = {\n        'generate-images': 'run_generator.generate_images',\n        'style-mixing-example': 'run_generator.style_mixing_example'\n    }\n    dnnlib.submit_run(sc, func_name_map[subcmd], **kwargs)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "run_metrics.py",
          "type": "blob",
          "size": 3.3662109375,
          "content": "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, visit\n# https://nvlabs.github.io/stylegan2/license.html\n\nimport argparse\nimport os\nimport sys\n\nimport dnnlib\nimport dnnlib.tflib as tflib\n\nimport pretrained_networks\nfrom metrics import metric_base\nfrom metrics.metric_defaults import metric_defaults\n\n#----------------------------------------------------------------------------\n\ndef run(network_pkl, metrics, dataset, data_dir, mirror_augment):\n    print('Evaluating metrics \"%s\" for \"%s\"...' % (','.join(metrics), network_pkl))\n    tflib.init_tf()\n    network_pkl = pretrained_networks.get_path_or_url(network_pkl)\n    dataset_args = dnnlib.EasyDict(tfrecord_dir=dataset, shuffle_mb=0)\n    num_gpus = dnnlib.submit_config.num_gpus\n    metric_group = metric_base.MetricGroup([metric_defaults[metric] for metric in metrics])\n    metric_group.run(network_pkl, data_dir=data_dir, dataset_args=dataset_args, mirror_augment=mirror_augment, num_gpus=num_gpus)\n\n#----------------------------------------------------------------------------\n\ndef _str_to_bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\n#----------------------------------------------------------------------------\n\n_examples = '''examples:\n\n  python %(prog)s --data-dir=~/datasets --network=gdrive:networks/stylegan2-ffhq-config-f.pkl --metrics=fid50k,ppl_wend --dataset=ffhq --mirror-augment=true\n\nvalid metrics:\n\n  ''' + ', '.join(sorted([x for x in metric_defaults.keys()])) + '''\n'''\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Run StyleGAN2 metrics.',\n        epilog=_examples,\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    parser.add_argument('--result-dir', help='Root directory for run results (default: %(default)s)', default='results', metavar='DIR')\n    parser.add_argument('--network', help='Network pickle filename', dest='network_pkl', required=True)\n    parser.add_argument('--metrics', help='Metrics to compute (default: %(default)s)', default='fid50k', type=lambda x: x.split(','))\n    parser.add_argument('--dataset', help='Training dataset', required=True)\n    parser.add_argument('--data-dir', help='Dataset root directory', required=True)\n    parser.add_argument('--mirror-augment', help='Mirror augment (default: %(default)s)', default=False, type=_str_to_bool, metavar='BOOL')\n    parser.add_argument('--num-gpus', help='Number of GPUs to use', type=int, default=1, metavar='N')\n\n    args = parser.parse_args()\n\n    if not os.path.exists(args.data_dir):\n        print ('Error: dataset root directory does not exist.')\n        sys.exit(1)\n\n    kwargs = vars(args)\n    sc = dnnlib.SubmitConfig()\n    sc.num_gpus = kwargs.pop('num_gpus')\n    sc.submit_target = dnnlib.SubmitTarget.LOCAL\n    sc.local.do_not_copy_source_files = True\n    sc.run_dir_root = kwargs.pop('result_dir')\n    sc.run_desc = 'run-metrics'\n    dnnlib.submit_run(sc, 'run_metrics.run', **kwargs)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "run_projector.py",
          "type": "blob",
          "size": 6.7998046875,
          "content": "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, visit\n# https://nvlabs.github.io/stylegan2/license.html\n\nimport argparse\nimport numpy as np\nimport dnnlib\nimport dnnlib.tflib as tflib\nimport re\nimport sys\n\nimport projector\nimport pretrained_networks\nfrom training import dataset\nfrom training import misc\n\n#----------------------------------------------------------------------------\n\ndef project_image(proj, targets, png_prefix, num_snapshots):\n    snapshot_steps = set(proj.num_steps - np.linspace(0, proj.num_steps, num_snapshots, endpoint=False, dtype=int))\n    misc.save_image_grid(targets, png_prefix + 'target.png', drange=[-1,1])\n    proj.start(targets)\n    while proj.get_cur_step() < proj.num_steps:\n        print('\\r%d / %d ... ' % (proj.get_cur_step(), proj.num_steps), end='', flush=True)\n        proj.step()\n        if proj.get_cur_step() in snapshot_steps:\n            misc.save_image_grid(proj.get_images(), png_prefix + 'step%04d.png' % proj.get_cur_step(), drange=[-1,1])\n    print('\\r%-30s\\r' % '', end='', flush=True)\n\n#----------------------------------------------------------------------------\n\ndef project_generated_images(network_pkl, seeds, num_snapshots, truncation_psi):\n    print('Loading networks from \"%s\"...' % network_pkl)\n    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n    proj = projector.Projector()\n    proj.set_network(Gs)\n    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n\n    Gs_kwargs = dnnlib.EasyDict()\n    Gs_kwargs.randomize_noise = False\n    Gs_kwargs.truncation_psi = truncation_psi\n\n    for seed_idx, seed in enumerate(seeds):\n        print('Projecting seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n        rnd = np.random.RandomState(seed)\n        z = rnd.randn(1, *Gs.input_shape[1:])\n        tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars})\n        images = Gs.run(z, None, **Gs_kwargs)\n        project_image(proj, targets=images, png_prefix=dnnlib.make_run_dir_path('seed%04d-' % seed), num_snapshots=num_snapshots)\n\n#----------------------------------------------------------------------------\n\ndef project_real_images(network_pkl, dataset_name, data_dir, num_images, num_snapshots):\n    print('Loading networks from \"%s\"...' % network_pkl)\n    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n    proj = projector.Projector()\n    proj.set_network(Gs)\n\n    print('Loading images from \"%s\"...' % dataset_name)\n    dataset_obj = dataset.load_dataset(data_dir=data_dir, tfrecord_dir=dataset_name, max_label_size=0, repeat=False, shuffle_mb=0)\n    assert dataset_obj.shape == Gs.output_shape[1:]\n\n    for image_idx in range(num_images):\n        print('Projecting image %d/%d ...' % (image_idx, num_images))\n        images, _labels = dataset_obj.get_minibatch_np(1)\n        images = misc.adjust_dynamic_range(images, [0, 255], [-1, 1])\n        project_image(proj, targets=images, png_prefix=dnnlib.make_run_dir_path('image%04d-' % image_idx), num_snapshots=num_snapshots)\n\n#----------------------------------------------------------------------------\n\ndef _parse_num_range(s):\n    '''Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.'''\n\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    m = range_re.match(s)\n    if m:\n        return list(range(int(m.group(1)), int(m.group(2))+1))\n    vals = s.split(',')\n    return [int(x) for x in vals]\n\n#----------------------------------------------------------------------------\n\n_examples = '''examples:\n\n  # Project generated images\n  python %(prog)s project-generated-images --network=gdrive:networks/stylegan2-car-config-f.pkl --seeds=0,1,5\n\n  # Project real images\n  python %(prog)s project-real-images --network=gdrive:networks/stylegan2-car-config-f.pkl --dataset=car --data-dir=~/datasets\n\n'''\n\n#----------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='''StyleGAN2 projector.\n\nRun 'python %(prog)s <subcommand> --help' for subcommand help.''',\n        epilog=_examples,\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n\n    subparsers = parser.add_subparsers(help='Sub-commands', dest='command')\n\n    project_generated_images_parser = subparsers.add_parser('project-generated-images', help='Project generated images')\n    project_generated_images_parser.add_argument('--network', help='Network pickle filename', dest='network_pkl', required=True)\n    project_generated_images_parser.add_argument('--seeds', type=_parse_num_range, help='List of random seeds', default=range(3))\n    project_generated_images_parser.add_argument('--num-snapshots', type=int, help='Number of snapshots (default: %(default)s)', default=5)\n    project_generated_images_parser.add_argument('--truncation-psi', type=float, help='Truncation psi (default: %(default)s)', default=1.0)\n    project_generated_images_parser.add_argument('--result-dir', help='Root directory for run results (default: %(default)s)', default='results', metavar='DIR')\n\n    project_real_images_parser = subparsers.add_parser('project-real-images', help='Project real images')\n    project_real_images_parser.add_argument('--network', help='Network pickle filename', dest='network_pkl', required=True)\n    project_real_images_parser.add_argument('--data-dir', help='Dataset root directory', required=True)\n    project_real_images_parser.add_argument('--dataset', help='Training dataset', dest='dataset_name', required=True)\n    project_real_images_parser.add_argument('--num-snapshots', type=int, help='Number of snapshots (default: %(default)s)', default=5)\n    project_real_images_parser.add_argument('--num-images', type=int, help='Number of images to project (default: %(default)s)', default=3)\n    project_real_images_parser.add_argument('--result-dir', help='Root directory for run results (default: %(default)s)', default='results', metavar='DIR')\n\n    args = parser.parse_args()\n    subcmd = args.command\n    if subcmd is None:\n        print ('Error: missing subcommand.  Re-run with --help for usage.')\n        sys.exit(1)\n\n    kwargs = vars(args)\n    sc = dnnlib.SubmitConfig()\n    sc.num_gpus = 1\n    sc.submit_target = dnnlib.SubmitTarget.LOCAL\n    sc.local.do_not_copy_source_files = True\n    sc.run_dir_root = kwargs.pop('result_dir')\n    sc.run_desc = kwargs.pop('command')\n\n    func_name_map = {\n        'project-generated-images': 'run_projector.project_generated_images',\n        'project-real-images': 'run_projector.project_real_images'\n    }\n    dnnlib.submit_run(sc, func_name_map[subcmd], **kwargs)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "run_training.py",
          "type": "blob",
          "size": 8.2744140625,
          "content": "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, visit\n# https://nvlabs.github.io/stylegan2/license.html\n\nimport argparse\nimport copy\nimport os\nimport sys\n\nimport dnnlib\nfrom dnnlib import EasyDict\n\nfrom metrics.metric_defaults import metric_defaults\n\n#----------------------------------------------------------------------------\n\n_valid_configs = [\n    # Table 1\n    'config-a', # Baseline StyleGAN\n    'config-b', # + Weight demodulation\n    'config-c', # + Lazy regularization\n    'config-d', # + Path length regularization\n    'config-e', # + No growing, new G & D arch.\n    'config-f', # + Large networks (default)\n\n    # Table 2\n    'config-e-Gorig-Dorig',   'config-e-Gorig-Dresnet',   'config-e-Gorig-Dskip',\n    'config-e-Gresnet-Dorig', 'config-e-Gresnet-Dresnet', 'config-e-Gresnet-Dskip',\n    'config-e-Gskip-Dorig',   'config-e-Gskip-Dresnet',   'config-e-Gskip-Dskip',\n]\n\n#----------------------------------------------------------------------------\n\ndef run(dataset, data_dir, result_dir, config_id, num_gpus, total_kimg, gamma, mirror_augment, metrics):\n    train     = EasyDict(run_func_name='training.training_loop.training_loop') # Options for training loop.\n    G         = EasyDict(func_name='training.networks_stylegan2.G_main')       # Options for generator network.\n    D         = EasyDict(func_name='training.networks_stylegan2.D_stylegan2')  # Options for discriminator network.\n    G_opt     = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8)                  # Options for generator optimizer.\n    D_opt     = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8)                  # Options for discriminator optimizer.\n    G_loss    = EasyDict(func_name='training.loss.G_logistic_ns_pathreg')      # Options for generator loss.\n    D_loss    = EasyDict(func_name='training.loss.D_logistic_r1')              # Options for discriminator loss.\n    sched     = EasyDict()                                                     # Options for TrainingSchedule.\n    grid      = EasyDict(size='8k', layout='random')                           # Options for setup_snapshot_image_grid().\n    sc        = dnnlib.SubmitConfig()                                          # Options for dnnlib.submit_run().\n    tf_config = {'rnd.np_random_seed': 1000}                                   # Options for tflib.init_tf().\n\n    train.data_dir = data_dir\n    train.total_kimg = total_kimg\n    train.mirror_augment = mirror_augment\n    train.image_snapshot_ticks = train.network_snapshot_ticks = 10\n    sched.G_lrate_base = sched.D_lrate_base = 0.002\n    sched.minibatch_size_base = 32\n    sched.minibatch_gpu_base = 4\n    D_loss.gamma = 10\n    metrics = [metric_defaults[x] for x in metrics]\n    desc = 'stylegan2'\n\n    desc += '-' + dataset\n    dataset_args = EasyDict(tfrecord_dir=dataset)\n\n    assert num_gpus in [1, 2, 4, 8]\n    sc.num_gpus = num_gpus\n    desc += '-%dgpu' % num_gpus\n\n    assert config_id in _valid_configs\n    desc += '-' + config_id\n\n    # Configs A-E: Shrink networks to match original StyleGAN.\n    if config_id != 'config-f':\n        G.fmap_base = D.fmap_base = 8 << 10\n\n    # Config E: Set gamma to 100 and override G & D architecture.\n    if config_id.startswith('config-e'):\n        D_loss.gamma = 100\n        if 'Gorig'   in config_id: G.architecture = 'orig'\n        if 'Gskip'   in config_id: G.architecture = 'skip' # (default)\n        if 'Gresnet' in config_id: G.architecture = 'resnet'\n        if 'Dorig'   in config_id: D.architecture = 'orig'\n        if 'Dskip'   in config_id: D.architecture = 'skip'\n        if 'Dresnet' in config_id: D.architecture = 'resnet' # (default)\n\n    # Configs A-D: Enable progressive growing and switch to networks that support it.\n    if config_id in ['config-a', 'config-b', 'config-c', 'config-d']:\n        sched.lod_initial_resolution = 8\n        sched.G_lrate_base = sched.D_lrate_base = 0.001\n        sched.G_lrate_dict = sched.D_lrate_dict = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n        sched.minibatch_size_base = 32 # (default)\n        sched.minibatch_size_dict = {8: 256, 16: 128, 32: 64, 64: 32}\n        sched.minibatch_gpu_base = 4 # (default)\n        sched.minibatch_gpu_dict = {8: 32, 16: 16, 32: 8, 64: 4}\n        G.synthesis_func = 'G_synthesis_stylegan_revised'\n        D.func_name = 'training.networks_stylegan2.D_stylegan'\n\n    # Configs A-C: Disable path length regularization.\n    if config_id in ['config-a', 'config-b', 'config-c']:\n        G_loss = EasyDict(func_name='training.loss.G_logistic_ns')\n\n    # Configs A-B: Disable lazy regularization.\n    if config_id in ['config-a', 'config-b']:\n        train.lazy_regularization = False\n\n    # Config A: Switch to original StyleGAN networks.\n    if config_id == 'config-a':\n        G = EasyDict(func_name='training.networks_stylegan.G_style')\n        D = EasyDict(func_name='training.networks_stylegan.D_basic')\n\n    if gamma is not None:\n        D_loss.gamma = gamma\n\n    sc.submit_target = dnnlib.SubmitTarget.LOCAL\n    sc.local.do_not_copy_source_files = True\n    kwargs = EasyDict(train)\n    kwargs.update(G_args=G, D_args=D, G_opt_args=G_opt, D_opt_args=D_opt, G_loss_args=G_loss, D_loss_args=D_loss)\n    kwargs.update(dataset_args=dataset_args, sched_args=sched, grid_args=grid, metric_arg_list=metrics, tf_config=tf_config)\n    kwargs.submit_config = copy.deepcopy(sc)\n    kwargs.submit_config.run_dir_root = result_dir\n    kwargs.submit_config.run_desc = desc\n    dnnlib.submit_run(**kwargs)\n\n#----------------------------------------------------------------------------\n\ndef _str_to_bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\ndef _parse_comma_sep(s):\n    if s is None or s.lower() == 'none' or s == '':\n        return []\n    return s.split(',')\n\n#----------------------------------------------------------------------------\n\n_examples = '''examples:\n\n  # Train StyleGAN2 using the FFHQ dataset\n  python %(prog)s --num-gpus=8 --data-dir=~/datasets --config=config-f --dataset=ffhq --mirror-augment=true\n\nvalid configs:\n\n  ''' + ', '.join(_valid_configs) + '''\n\nvalid metrics:\n\n  ''' + ', '.join(sorted([x for x in metric_defaults.keys()])) + '''\n\n'''\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Train StyleGAN2.',\n        epilog=_examples,\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    parser.add_argument('--result-dir', help='Root directory for run results (default: %(default)s)', default='results', metavar='DIR')\n    parser.add_argument('--data-dir', help='Dataset root directory', required=True)\n    parser.add_argument('--dataset', help='Training dataset', required=True)\n    parser.add_argument('--config', help='Training config (default: %(default)s)', default='config-f', required=True, dest='config_id', metavar='CONFIG')\n    parser.add_argument('--num-gpus', help='Number of GPUs (default: %(default)s)', default=1, type=int, metavar='N')\n    parser.add_argument('--total-kimg', help='Training length in thousands of images (default: %(default)s)', metavar='KIMG', default=25000, type=int)\n    parser.add_argument('--gamma', help='R1 regularization weight (default is config dependent)', default=None, type=float)\n    parser.add_argument('--mirror-augment', help='Mirror augment (default: %(default)s)', default=False, metavar='BOOL', type=_str_to_bool)\n    parser.add_argument('--metrics', help='Comma-separated list of metrics or \"none\" (default: %(default)s)', default='fid50k', type=_parse_comma_sep)\n\n    args = parser.parse_args()\n\n    if not os.path.exists(args.data_dir):\n        print ('Error: dataset root directory does not exist.')\n        sys.exit(1)\n\n    if args.config_id not in _valid_configs:\n        print ('Error: --config value must be one of: ', ', '.join(_valid_configs))\n        sys.exit(1)\n\n    for metric in args.metrics:\n        if metric not in metric_defaults:\n            print ('Error: unknown metric \\'%s\\'' % metric)\n            sys.exit(1)\n\n    run(**vars(args))\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n\n"
        },
        {
          "name": "test_nvcc.cu",
          "type": "blob",
          "size": 0.6982421875,
          "content": "// Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\r\n//\r\n// This work is made available under the Nvidia Source Code License-NC.\r\n// To view a copy of this license, visit\r\n// https://nvlabs.github.io/stylegan2/license.html\r\n\r\n#include <cstdio>\r\n\r\nvoid checkCudaError(cudaError_t err)\r\n{\r\n    if (err != cudaSuccess)\r\n    {\r\n        printf(\"%s: %s\\n\", cudaGetErrorName(err), cudaGetErrorString(err));\r\n        exit(1);\r\n    }\r\n}\r\n\r\n__global__ void cudaKernel(void)\r\n{\r\n    printf(\"GPU says hello.\\n\");\r\n}\r\n\r\nint main(void)\r\n{\r\n    printf(\"CPU says hello.\\n\");\r\n    checkCudaError(cudaLaunchKernel((void*)cudaKernel, 1, 1, NULL, 0, NULL));\r\n    checkCudaError(cudaDeviceSynchronize());\r\n    return 0;\r\n}\r\n"
        },
        {
          "name": "training",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}