{
  "metadata": {
    "timestamp": 1736561146653,
    "page": 94,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Jack-Cherish/python-spider",
      "stars": 18443,
      "defaultBranch": "master",
      "files": [
        {
          "name": "12306.py",
          "type": "blob",
          "size": 3.2431640625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author: liuyw\n\"\"\"\nfrom splinter.browser import Browser\nfrom time import sleep\nimport traceback\nimport time, sys\n\nclass huoche(object):\n\tdriver_name = ''\n\texecutable_path = ''\n\t#用户名，密码\n\tusername = u\"xxx\"\n\tpasswd = u\"xxx\"\n\t# cookies值得自己去找, 下面两个分别是沈阳, 哈尔滨\n\tstarts = u\"%u6C88%u9633%2CSYT\"\n\tends = u\"%u54C8%u5C14%u6EE8%2CHBB\"\n\t\n\t# 时间格式2018-01-19\n\tdtime = u\"2018-01-19\"\n\t# 车次，选择第几趟，0则从上之下依次点击\n\torder = 0\n\t###乘客名\n\tusers = [u\"xxx\",u\"xxx\"]\n\t##席位\n\txb = u\"二等座\"\n\tpz = u\"成人票\"\n\n\t\"\"\"网址\"\"\"\n\tticket_url = \"https://kyfw.12306.cn/otn/leftTicket/init\"\n\tlogin_url = \"https://kyfw.12306.cn/otn/login/init\"\n\tinitmy_url = \"https://kyfw.12306.cn/otn/index/initMy12306\"\n\tbuy = \"https://kyfw.12306.cn/otn/confirmPassenger/initDc\"\n\t\n\tdef __init__(self):\n\t\tself.driver_name = 'chrome'\n\t\tself.executable_path = 'D:/chromedriver'\n\n\tdef login(self):\n\t\tself.driver.visit(self.login_url)\n\t\tself.driver.fill(\"loginUserDTO.user_name\", self.username)\n\t\t# sleep(1)\n\t\tself.driver.fill(\"userDTO.password\", self.passwd)\n\t\tprint(u\"等待验证码，自行输入...\")\n\t\twhile True:\n\t\t\tif self.driver.url != self.initmy_url:\n\t\t\t\tsleep(1)\n\t\t\telse:\n\t\t\t\tbreak\n\n\tdef start(self):\n\t\tself.driver = Browser(driver_name=self.driver_name,executable_path=self.executable_path)\n\t\tself.driver.driver.set_window_size(1400, 1000)\n\t\tself.login()\n\t\t# sleep(1)\n\t\tself.driver.visit(self.ticket_url)\n\t\ttry:\n\t\t\tprint(u\"购票页面开始...\")\n\t\t\t# sleep(1)\n\t\t\t# 加载查询信息\n\t\t\tself.driver.cookies.add({\"_jc_save_fromStation\": self.starts})\n\t\t\tself.driver.cookies.add({\"_jc_save_toStation\": self.ends})\n\t\t\tself.driver.cookies.add({\"_jc_save_fromDate\": self.dtime})\n\n\t\t\tself.driver.reload()\n\n\t\t\tcount = 0\n\t\t\tif self.order != 0:\n\t\t\t\twhile self.driver.url == self.ticket_url:\n\t\t\t\t\tself.driver.find_by_text(u\"查询\").click()\n\t\t\t\t\tcount += 1\n\t\t\t\t\tprint(u\"循环点击查询... 第 %s 次\" % count)\n\t\t\t\t\t# sleep(1)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tself.driver.find_by_text(u\"预订\")[self.order - 1].click()\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t\tprint(e)\n\t\t\t\t\t\tprint(u\"还没开始预订\")\n\t\t\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\twhile self.driver.url == self.ticket_url:\n\t\t\t\t\tself.driver.find_by_text(u\"查询\").click()\n\t\t\t\t\tcount += 1\n\t\t\t\t\tprint(u\"循环点击查询... 第 %s 次\" % count)\n\t\t\t\t\t# sleep(0.8)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfor i in self.driver.find_by_text(u\"预订\"):\n\t\t\t\t\t\t\ti.click()\n\t\t\t\t\t\t\tsleep(1)\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t\tprint(e)\n\t\t\t\t\t\tprint(u\"还没开始预订 %s\" % count)\n\t\t\t\t\t\tcontinue\n\t\t\tprint(u\"开始预订...\")\n\t\t\t# sleep(3)\n\t\t\t# self.driver.reload()\n\t\t\tsleep(1)\n\t\t\tprint(u'开始选择用户...')\n\t\t\tfor user in self.users:\n\t\t\t\tself.driver.find_by_text(user).last.click()\n\n\t\t\tprint(u\"提交订单...\")\n\t\t\tsleep(1)\n\t\t\tself.driver.find_by_text(self.pz).click()\n\t\t\tself.driver.find_by_id('').select(self.pz)\n\t\t\t# sleep(1)\n\t\t\tself.driver.find_by_text(self.xb).click()\n\t\t\tsleep(1)\n\t\t\tself.driver.find_by_id('submitOrder_id').click()\n\t\t\tprint(u\"开始选座...\")\n\t\t\tself.driver.find_by_id('1D').last.click()\n\t\t\tself.driver.find_by_id('1F').last.click()\n\n\t\t\tsleep(1.5)\n\t\t\tprint(u\"确认选座...\")\n\t\t\tself.driver.find_by_id('qr_submit_id').click()\n\n\t\texcept Exception as e:\n\t\t\tprint(e)\n\nif __name__ == '__main__':\n\thuoche = huoche()\n\thuoche.start()"
        },
        {
          "name": "2020",
          "type": "tree",
          "content": null
        },
        {
          "name": "Netease",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.3974609375,
          "content": "# 注：2020年最新连载教程请移步：[Python Spider 2020](https://github.com/Jack-Cherish/python-spider/tree/master/2020 \"Python Spider 2020\")\n\n免责声明：\n\n大家请以学习为目的使用本仓库，爬虫违法违规的案件：https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China\n\n本仓库的所有内容仅供学习和参考之用，禁止用于商业用途。任何人或组织不得将本仓库的内容用于非法用途或侵犯他人合法权益。本仓库所涉及的爬虫技术仅用于学习和研究，不得用于对其他平台进行大规模爬虫或其他非法行为。对于因使用本仓库内容而引起的任何法律责任，本仓库不承担任何责任。使用本仓库的内容即表示您同意本免责声明的所有条款和条件。\n\n# Python Spider\n\n原创文章每周最少两篇，**后续最新文章**会在[【公众号】](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)首发，视频[【B站】](https://space.bilibili.com/331507846)首发，大家可以加我[【微信】](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)进**交流群**，技术交流或提意见都可以，欢迎**Star**！\n\n<p align=\"center\">\n  <a href=\"https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg\" target=\"_blank\"><img src=\"https://img.shields.io/badge/weChat-微信群-blue.svg\" alt=\"微信群\"></a>\n  <a href=\"https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg\" target=\"_blank\"><img src=\"https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-Jack%20Cui-lightgrey.svg\" alt=\"公众号\"></a>\n  <a href=\"https://space.bilibili.com/331507846\"><img src=\"https://img.shields.io/badge/bilibili-哔哩哔哩-critical\" alt=\"B站\"></a>\n  <a href=\"https://www.zhihu.com/people/Jack--Cui\" target=\"_blank\"><img src=\"https://img.shields.io/badge/zhihu-知乎-informational\" alt=\"知乎\"></a>\n  <a href=\"https://blog.csdn.net/c406495762\" target=\"_blank\"><img src=\"https://img.shields.io/badge/csdn-CSDN-red.svg\" alt=\"CSDN\"></a>\n  <a href=\"https://www.toutiao.com/c/user/token/MS4wLjABAAAA5gJtmezUJ6vli2hZvnN13iLnzKLpuF8gGHeS0iVlmNs/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/toutiao-%E5%A4%B4%E6%9D%A1-important.svg\" alt=\"头条\"></a>\n  <a href=\"https://juejin.im/user/5ea2ca74e51d4546b50d5f9f\" target=\"_blank\"><img src=\"https://img.shields.io/badge/juejin-掘金-blue.svg\" alt=\"掘金\"></a>\n</p>\n\n## 声明\n\n* 代码、教程**仅限于学习交流，请勿用于任何商业用途！**\n\n## 目录\n\n* [爬虫小工具](#爬虫小工具)\n    * [文件下载小助手](https://github.com/Jack-Cherish/python-spider/blob/master/downloader.py \"悬停显示\")\n* [爬虫实战](#爬虫实战)\n    * [笔趣看小说下载](https://github.com/Jack-Cherish/python-spider/blob/master/biqukan.py \"悬停显示\")\n    * [百度文库免费文章下载助手_rev1](https://github.com/Jack-Cherish/python-spider/blob/master/baiduwenku.py \"悬停显示\")\n    * [百度文库免费文章下载助手_rev2](https://github.com/Jack-Cherish/python-spider/blob/master/baiduwenku_pro_1.py \"悬停显示\")\n    * [《帅啊》网帅哥图片下载](https://github.com/Jack-Cherish/python-spider/blob/master/shuaia.py \"悬停显示\")\n    * [构建代理IP池](https://github.com/Jack-Cherish/python-spider/blob/master/daili.py \"悬停显示\")\n    * [《火影忍者》漫画下载](https://github.com/Jack-Cherish/python-spider/tree/master/cartoon \"悬停显示\")\n    * [财务报表下载小助手](https://github.com/Jack-Cherish/python-spider/blob/master/financical.py \"悬停显示\")\n    * [一小时入门网络爬虫](https://github.com/Jack-Cherish/python-spider/tree/master/one_hour_spider \"悬停显示\")\n    * [抖音App视频下载](https://github.com/Jack-Cherish/python-spider/tree/master/douyin \"悬停显示\")\n    * [GEETEST验证码识别](https://github.com/Jack-Cherish/python-spider/blob/master/geetest.py \"悬停显示\")\n    * [12306抢票小助手](https://github.com/Jack-Cherish/python-spider/blob/master/12306.py \"悬停显示\")\n    * [百万英雄答题辅助系统](https://github.com/Jack-Cherish/python-spider/tree/master/baiwan \"悬停显示\")   \n    * [网易云音乐免费音乐批量下载](https://github.com/Jack-Cherish/python-spider/tree/master/Netease \"悬停显示\")\n    * [B站免费视频和弹幕批量下载](https://github.com/Jack-Cherish/python-spider/tree/master/bilibili \"悬停显示\")\n    * [京东商品晒单图下载](https://github.com/Jack-Cherish/python-spider/tree/master/dingdong \"悬停显示\")\n    * [正方教务管理系统个人信息查询](https://github.com/Jack-Cherish/python-spider/tree/master/zhengfang_system_spider \"悬停显示\")\n* [其它](#其它)\n\n## 爬虫小工具\n\n* downloader.py:文件下载小助手\n\n\t一个可以用于下载图片、视频、文件的小工具，有下载进度显示功能。稍加修改即可添加到自己的爬虫中。\n\t\n\t动态示意图：\n\t\n\t![image](https://raw.githubusercontent.com/Jack-Cherish/Pictures/master/9.gif)\n\n## 爬虫实战\n \n * biqukan.py:《笔趣看》盗版小说网站，爬取小说工具\n\n\t第三方依赖库安装：\n\n\t\tpip3 install beautifulsoup4\n\n\t使用方法：\n\n\t\tpython biqukan.py\n\n * baiduwenku.py: 百度文库word文章爬取\n\t\n\t原理说明：http://blog.csdn.net/c406495762/article/details/72331737\n\t\n\t代码不完善，没有进行打包，不具通用性，纯属娱乐。\n\t\n * shuaia.py: 爬取《帅啊》网，帅哥图片\n\n\t《帅啊》网URL：http://www.shuaia.net/index.html\n\n\t原理说明：http://blog.csdn.net/c406495762/article/details/72597755\n\t\n\t第三方依赖库安装：\n\t\n\t\tpip3 install requests beautifulsoup4\n\t\t\n * daili.py: 构建代理IP池\n\n\t原理说明：http://blog.csdn.net/c406495762/article/details/72793480\n\t\n\t\n * carton: 使用Scrapy爬取《火影忍者》漫画\n\n\t代码可以爬取整个《火影忍者》漫画所有章节的内容，保存到本地。更改地址，可以爬取其他漫画。保存地址可以在settings.py中修改。\n\t\n\t动漫网站：http://comic.kukudm.com/\n\t\n\t原理说明：http://blog.csdn.net/c406495762/article/details/72858983\n\t\n * hero.py: 《王者荣耀》推荐出装查询小助手\n\n\t网页爬取已经会了，想过爬取手机APP里的内容吗？\n\t\n\t原理说明：http://blog.csdn.net/c406495762/article/details/76850843\n\t\n * financical.py: 财务报表下载小助手\n\n\t爬取的数据存入数据库会吗？《跟股神巴菲特学习炒股之财务报表入库(MySQL)》也许能给你一些思路。\n\t\n\t原理说明：http://blog.csdn.net/c406495762/article/details/77801899\n\t\n\t动态示意图：\n\t\n\t![image](https://raw.githubusercontent.com/Jack-Cherish/Pictures/master/10.gif)\n\t\n * one_hour_spider:一小时入门Python3网络爬虫。\n\n\t原理说明:\n\t\n\t * 知乎：https://zhuanlan.zhihu.com/p/29809609\n\t * CSDN：http://blog.csdn.net/c406495762/article/details/78123502\n\t\n\t本次实战内容有：\n\t\n\t * 网络小说下载(静态网站)-biqukan\n\t * 优美壁纸下载(动态网站)-unsplash\n\t * 视频下载\n\t \n * douyin.py:抖音App视频下载\n \n\t抖音App的视频下载，就是普通的App爬取。\n\n\t原理说明:\n\t\n\t * 个人网站：http://cuijiahua.com/blog/2018/03/spider-5.html\n\t\n * douyin_pro:抖音App视频下载（升级版）\n \n\t抖音App的视频下载，添加视频解析网站，支持无水印视频下载，使用第三方平台解析。\n\n\t原理说明:\n\t\n\t * 个人网站：http://cuijiahua.com/blog/2018/03/spider-5.html\n\t \n * douyin:抖音App视频下载（升级版2）\n \n\t抖音App的视频下载，添加视频解析网站，支持无水印视频下载，通过url解析，无需第三方平台。\n\t\n\t原理说明:\n\t\n\t * 个人网站：http://cuijiahua.com/blog/2018/03/spider-5.html\n\t \n\t动态示意图：\n\t\n\t![image](https://github.com/Jack-Cherish/Pictures/blob/master/14.gif)\n\t\n * geetest.py:GEETEST验证码识别\n \n \t原理说明:\n\t\n\t 无\n\t\n * 12306.py:用Python抢火车票简单代码\n \n\t可以自己慢慢丰富，蛮简单，有爬虫基础很好操作，没有原理说明。\n\t\n * baiwan:百万英雄辅助答题\n \n\t效果图：\n\t\n\t![image](https://github.com/Jack-Cherish/Pictures/blob/master/11.gif)\n\t\n\t原理说明：\n\t\n\t* 个人网站：http://cuijiahua.com/blog/2018/01/spider_3.html\n\t\n  \t功能介绍：\n\t\n\t服务器端，使用Python（baiwan.py）通过抓包获得的接口获取答题数据，解析之后通过百度知道搜索接口匹配答案，将最终匹配的结果写入文件（file.txt)。\n\t\n\t手机抓包不会的朋友，可以看下我的早期[手机APP抓包教程](http://blog.csdn.net/c406495762/article/details/76850843 \"悬停显示\")。\n\t\n\tNode.js（app.js）每隔1s读取一次file.txt文件，并将读取结果通过socket.io推送给客户端（index.html）。\n\t\n\t亲测答题延时在3s左右。\n\t\n\t声明：没做过后端和前端，花了一天时间，现学现卖弄好的，javascript也是现看现用，百度的程序，调试调试而已。可能有很多用法比较low的地方，用法不对，请勿见怪，有大牛感兴趣，可以自行完善。\n\n * Netease:根据歌单下载网易云音乐\n \t\n\t效果图：\n\t\n\t![image](https://github.com/Jack-Cherish/Pictures/blob/master/13.gif)\n\t\n\t原理说明：\n\t\n\t暂无\n\t\n\t功能介绍：\n\t\n\t根据music_list.txt文件里的歌单的信息下载网易云音乐，将自己喜欢的音乐进行批量下载。\n\n * bilibili：B站视频和弹幕批量下载\n \t\n\t原理说明：\n\t\n\t暂无\n\t\n\t使用说明：\n\t\n        python bilibili.py -d 猫 -k 猫 -p 10\n\n        三个参数：\n        -d\t保存视频的文件夹名\n        -k\tB站搜索的关键字\n        -p\t下载搜索结果前多少页\n\t\n * jingdong：京东商品晒单图下载\n \n \t效果图：\n\t\n\t![image](https://github.com/Jack-Cherish/Pictures/blob/master/jd.gif)\n \t\n\t原理说明：\n\t\n\t暂无\n\t\n\t使用说明：\n\t\n        python jd.py -k 芒果\n\t\n         三个参数：\n        -d\t保存图片的路径，默认为fd.py文件所在文件夹\n        -k\t搜索关键词\n        -n  \t下载商品的晒单图个数，即n个商店的晒单图\n\n * zhengfang_system_spider：对正方教务管理系统个人课表，个人学生成绩，绩点等简单爬取\n \n \t效果图：\n\t\n\t![image](/zhengfang_system_spider/screenshot/zf.png)\n \t\n\t原理说明：\n\t\n\t暂无\n\t\n\t使用说明：\n\t\n        cd zhengfang_system_spider\n        pip install -r requirements.txt\n        python spider.py\n\n## 其它\n\n * 欢迎 Pull requests，感谢贡献。\n \n 更多精彩，敬请期待！\n\n<a name=\"微信\"></a>  <a name=\"公众号\"></a>\n\n<img src=\"https://ftp.bmp.ovh/imgs/2020/07/112254f0199e3d4f.jpg\" alt=\"wechat\" width=\"400\" height=\"200\" align=\"bottom\" />\n"
        },
        {
          "name": "baiduwenku.py",
          "type": "blob",
          "size": 1.7197265625,
          "content": "# -*- coding:UTF-8 -*-\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport re\nimport time\n\nif __name__ == '__main__':\n\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('user-agent=\"Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19\"')\n\tdriver = webdriver.Chrome('J:\\迅雷下载\\chromedriver.exe', chrome_options=options)\n\tdriver.get('https://wenku.baidu.com/view/aa31a84bcf84b9d528ea7a2c.html')\n\n\thtml = driver.page_source\n\tbf1 = BeautifulSoup(html, 'lxml')\n\tresult = bf1.find_all(class_='rtcspage')\n\tbf2 = BeautifulSoup(str(result[0]), 'lxml')\n\ttitle = bf2.div.div.h1.string\n\tpagenum = bf2.find_all(class_='size')\n\tpagenum = BeautifulSoup(str(pagenum), 'lxml').span.string\n\tpagepattern = re.compile('页数：(\\d+)页')\n\tnum = int(pagepattern.findall(pagenum)[0])\n\tprint('文章标题：%s' % title)\n\tprint('文章页数：%d' % num)\n\n\n\twhile True:\n\t\tnum = num / 5.0\n\t\thtml = driver.page_source\n\t\tbf1 = BeautifulSoup(html, 'lxml')\n\t\tresult = bf1.find_all(class_='rtcspage')\n\t\tfor each_result in result:\n\t\t\tbf2 = BeautifulSoup(str(each_result), 'lxml')\n\t\t\ttexts = bf2.find_all('p')\n\t\t\tfor each_text in texts:\n\t\t\t\tmain_body = BeautifulSoup(str(each_text), 'lxml')\n\t\t\t\tfor each in main_body.find_all(True):\n\t\t\t\t\tif each.name == 'span':\n\t\t\t\t\t\tprint(each.string.replace('\\xa0',''),end='')\n\t\t\t\t\telif each.name == 'br':\n\t\t\t\t\t\tprint('')\n\t\t\tprint('\\n')\n\t\tif num > 1:\n\t\t\tpage = driver.find_elements_by_xpath(\"//div[@class='page']\")\n\t\t\tdriver.execute_script('arguments[0].scrollIntoView();', page[-1]) #拖动到可见的元素去\n\t\t\tnextpage = driver.find_element_by_xpath(\"//a[@data-fun='next']\")\n\t\t\tnextpage.click()\n\t\t\ttime.sleep(3)\n\t\telse:\n\t\t\tbreak"
        },
        {
          "name": "baiduwenku_pro_1.py",
          "type": "blob",
          "size": 3.1787109375,
          "content": "import requests\nimport re\nimport json\nimport os\n\nsession = requests.session()\n\n\ndef fetch_url(url):\n    return session.get(url).content.decode('gbk')\n\n\ndef get_doc_id(url):\n    return re.findall('view/(.*).html', url)[0]\n\n\ndef parse_type(content):\n    return re.findall(r\"docType.*?\\:.*?\\'(.*?)\\'\\,\", content)[0]\n\n\ndef parse_title(content):\n    return re.findall(r\"title.*?\\:.*?\\'(.*?)\\'\\,\", content)[0]\n\n\ndef parse_doc(content):\n    result = ''\n    url_list = re.findall('(https.*?0.json.*?)\\\\\\\\x22}', content)\n    url_list = [addr.replace(\"\\\\\\\\\\\\/\", \"/\") for addr in url_list]\n    for url in url_list[:-5]:\n        content = fetch_url(url)\n        y = 0\n        txtlists = re.findall('\"c\":\"(.*?)\".*?\"y\":(.*?),', content)\n        for item in txtlists:\n            if not y == item[1]:\n                y = item[1]\n                n = '\\n'\n            else:\n                n = ''\n            result += n\n            result += item[0].encode('utf-8').decode('unicode_escape', 'ignore')\n    return result\n\n\ndef parse_txt(doc_id):\n    content_url = 'https://wenku.baidu.com/api/doc/getdocinfo?callback=cb&doc_id=' + doc_id\n    content = fetch_url(content_url)\n    md5 = re.findall('\"md5sum\":\"(.*?)\"', content)[0]\n    pn = re.findall('\"totalPageNum\":\"(.*?)\"', content)[0]\n    rsign = re.findall('\"rsign\":\"(.*?)\"', content)[0]\n    content_url = 'https://wkretype.bdimg.com/retype/text/' + doc_id + '?rn=' + pn + '&type=txt' + md5 + '&rsign=' + rsign\n    content = json.loads(fetch_url(content_url))\n    result = ''\n    for item in content:\n        for i in item['parags']:\n            result += i['c'].replace('\\\\r', '\\r').replace('\\\\n', '\\n')\n    return result\n\n\ndef parse_other(doc_id):\n    content_url = \"https://wenku.baidu.com/browse/getbcsurl?doc_id=\" + doc_id + \"&pn=1&rn=99999&type=ppt\"\n    content = fetch_url(content_url)\n    url_list = re.findall('{\"zoom\":\"(.*?)\",\"page\"', content)\n    url_list = [item.replace(\"\\\\\", '') for item in url_list]\n    if not os.path.exists(doc_id):\n        os.mkdir(doc_id)\n    for index, url in enumerate(url_list):\n        content = session.get(url).content\n        path = os.path.join(doc_id, str(index) + '.jpg')\n        with open(path, 'wb') as f:\n            f.write(content)\n    print(\"图片保存在\" + doc_id + \"文件夹\")\n\n\ndef save_file(filename, content):\n    with open(filename, 'w', encoding='utf8') as f:\n        f.write(content)\n        print('已保存为:' + filename)\n\n\n# test_txt_url = 'https://wenku.baidu.com/view/cbb4af8b783e0912a3162a89.html?from=search'\n# test_ppt_url = 'https://wenku.baidu.com/view/2b7046e3f78a6529657d5376.html?from=search'\n# test_pdf_url = 'https://wenku.baidu.com/view/dd6e15c1227916888586d795.html?from=search'\n# test_xls_url = 'https://wenku.baidu.com/view/eb4a5bb7312b3169a551a481.html?from=search'\ndef main():\n    url = input('请输入要下载的文库URL地址')\n    content = fetch_url(url)\n    doc_id = get_doc_id(url)\n    type = parse_type(content)\n    title = parse_title(content)\n    if type == 'doc':\n        result = parse_doc(content)\n        save_file(title + '.txt', result)\n    elif type == 'txt':\n        result = parse_txt(doc_id)\n        save_file(title + '.txt', result)\n    else:\n        parse_other(doc_id)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "baiwan",
          "type": "tree",
          "content": null
        },
        {
          "name": "bilibili",
          "type": "tree",
          "content": null
        },
        {
          "name": "bilibili_luckyman",
          "type": "tree",
          "content": null
        },
        {
          "name": "biqukan.py",
          "type": "blob",
          "size": 3.880859375,
          "content": "# -*- coding:UTF-8 -*-\nfrom urllib import request\nfrom bs4 import BeautifulSoup\nimport collections\nimport re\nimport os\nimport time\nimport sys\nimport types\n\n\"\"\"\n类说明:下载《笔趣看》网小说: url:https://www.biqukan.com/\n\nParameters:\n\ttarget - 《笔趣看》网指定的小说目录地址(string)\n\nReturns:\n\t无\n\nModify:\n\t2017-05-06\n\"\"\"\nclass download(object):\n\tdef __init__(self, target):\n\t\tself.__target_url = target\n\t\tself.__head = {'User-Agent':'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19',}\n\n\t\"\"\"\n\t函数说明:获取下载链接\n\n\tParameters:\n\t\t无\n\n\tReturns:\n\t\tnovel_name + '.txt' - 保存的小说名(string)\n\t\tnumbers - 章节数(int)\n\t\tdownload_dict - 保存章节名称和下载链接的字典(dict)\n\n\tModify:\n\t\t2017-05-06\n\t\"\"\"\n\tdef get_download_url(self):\n\t\tcharter = re.compile(u'[第弟](.+)章', re.IGNORECASE)\n\t\ttarget_req = request.Request(url = self.__target_url, headers = self.__head)\n\t\ttarget_response = request.urlopen(target_req)\n\t\ttarget_html = target_response.read().decode('gbk','ignore')\n\t\tlistmain_soup = BeautifulSoup(target_html,'lxml')\n\t\tchapters = listmain_soup.find_all('div',class_ = 'listmain')\n\t\tdownload_soup = BeautifulSoup(str(chapters), 'lxml')\n\t\tnovel_name = str(download_soup.dl.dt).split(\"》\")[0][5:]\n\t\tflag_name = \"《\" + novel_name + \"》\" + \"正文卷\"\n\t\tnumbers = (len(download_soup.dl.contents) - 1) / 2 - 8\n\t\tdownload_dict = collections.OrderedDict()\n\t\tbegin_flag = False\n\t\tnumbers = 1\n\t\tfor child in download_soup.dl.children:\n\t\t\tif child != '\\n':\n\t\t\t\tif child.string == u\"%s\" % flag_name:\n\t\t\t\t\tbegin_flag = True\n\t\t\t\tif begin_flag == True and child.a != None:\n\t\t\t\t\tdownload_url = \"https://www.biqukan.com\" + child.a.get('href')\n\t\t\t\t\tdownload_name = child.string\n\t\t\t\t\tnames = str(download_name).split('章')\n\t\t\t\t\tname = charter.findall(names[0] + '章')\n\t\t\t\t\tif name:\n\t\t\t\t\t\t\tdownload_dict['第' + str(numbers) + '章 ' + names[1]] = download_url\n\t\t\t\t\t\t\tnumbers += 1\n\t\treturn novel_name + '.txt', numbers, download_dict\n\t\n\t\"\"\"\n\t函数说明:爬取文章内容\n\n\tParameters:\n\t\turl - 下载连接(string)\n\n\tReturns:\n\t\tsoup_text - 章节内容(string)\n\n\tModify:\n\t\t2017-05-06\n\t\"\"\"\n\tdef Downloader(self, url):\n\t\tdownload_req = request.Request(url = url, headers = self.__head)\n\t\tdownload_response = request.urlopen(download_req)\n\t\tdownload_html = download_response.read().decode('gbk','ignore')\n\t\tsoup_texts = BeautifulSoup(download_html, 'lxml')\n\t\ttexts = soup_texts.find_all(id = 'content', class_ = 'showtxt')\n\t\tsoup_text = BeautifulSoup(str(texts), 'lxml').div.text.replace('\\xa0','')\n\t\treturn soup_text\n\n\t\"\"\"\n\t函数说明:将爬取的文章内容写入文件\n\n\tParameters:\n\t\tname - 章节名称(string)\n\t\tpath - 当前路径下,小说保存名称(string)\n\t\ttext - 章节内容(string)\n\n\tReturns:\n\t\t无\n\n\tModify:\n\t\t2017-05-06\n\t\"\"\"\n\tdef Writer(self, name, path, text):\n\t\twrite_flag = True\n\t\twith open(path, 'a', encoding='utf-8') as f:\n\t\t\tf.write(name + '\\n\\n')\n\t\t\tfor each in text:\n\t\t\t\tif each == 'h':\n\t\t\t\t\twrite_flag = False\n\t\t\t\tif write_flag == True and each != ' ':\n\t\t\t\t\tf.write(each)\n\t\t\t\tif write_flag == True and each == '\\r':\n\t\t\t\t\tf.write('\\n')\t\t\t\n\t\t\tf.write('\\n\\n')\n\nif __name__ == \"__main__\":\n\tprint(\"\\n\\t\\t欢迎使用《笔趣看》小说下载小工具\\n\\n\\t\\t作者:Jack-Cui\\t时间:2017-05-06\\n\")\n\tprint(\"*************************************************************************\")\n\t\n\t#小说地址\n\ttarget_url = str(input(\"请输入小说目录下载地址:\\n\"))\n\n\t#实例化下载类\n\td = download(target = target_url)\n\tname, numbers, url_dict = d.get_download_url()\n\tif name in os.listdir():\n\t\tos.remove(name)\n\tindex = 1\n\n\t#下载中\n\tprint(\"《%s》下载中:\" % name[:-4])\n\tfor key, value in url_dict.items():\n\t\td.Writer(key, name, d.Downloader(value))\n\t\tsys.stdout.write(\"已下载:%.3f%%\" %  float(index/numbers) + '\\r')\n\t\tsys.stdout.flush()\n\t\tindex += 1\t\n\n\tprint(\"《%s》下载完成！\" % name[:-4])\n\n\t\n"
        },
        {
          "name": "cartoon",
          "type": "tree",
          "content": null
        },
        {
          "name": "daili.py",
          "type": "blob",
          "size": 3.9951171875,
          "content": "# -*- coding:UTF-8 -*-\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nimport subprocess as sp\nfrom lxml import etree\nimport requests\nimport random\nimport re\n\n\"\"\"\n函数说明:获取IP代理\nParameters:\n\tpage - 高匿代理页数,默认获取第一页\nReturns:\n\tproxys_list - 代理列表\nModify:\n\t2017-05-27\n\"\"\"\ndef get_proxys(page = 1):\n\t#requests的Session可以自动保持cookie,不需要自己维护cookie内容\n\tS = requests.Session()\n\t#西祠代理高匿IP地址\n\ttarget_url = 'http://www.xicidaili.com/nn/%d' % page\n\t#完善的headers\n\ttarget_headers = {'Upgrade-Insecure-Requests':'1',\n\t\t'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n\t\t'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n\t\t'Referer':'http://www.xicidaili.com/nn/',\n\t\t'Accept-Encoding':'gzip, deflate, sdch',\n\t\t'Accept-Language':'zh-CN,zh;q=0.8',\n\t}\n\t#get请求\n\ttarget_response = S.get(url = target_url, headers = target_headers)\n\t#utf-8编码\n\ttarget_response.encoding = 'utf-8'\n\t#获取网页信息\n\ttarget_html = target_response.text\n\t#获取id为ip_list的table\n\tbf1_ip_list = BeautifulSoup(target_html, 'lxml')\n\tbf2_ip_list = BeautifulSoup(str(bf1_ip_list.find_all(id = 'ip_list')), 'lxml')\n\tip_list_info = bf2_ip_list.table.contents\n\t#存储代理的列表\n\tproxys_list = []\n\t#爬取每个代理信息\n\tfor index in range(len(ip_list_info)):\n\t\tif index % 2 == 1 and index != 1:\n\t\t\tdom = etree.HTML(str(ip_list_info[index]))\n\t\t\tip = dom.xpath('//td[2]')\n\t\t\tport = dom.xpath('//td[3]')\n\t\t\tprotocol = dom.xpath('//td[6]')\n\t\t\tproxys_list.append(protocol[0].text.lower() + '#' + ip[0].text + '#' + port[0].text)\n\t#返回代理列表\n\treturn proxys_list\n\n\"\"\"\n函数说明:检查代理IP的连通性\nParameters:\n\tip - 代理的ip地址\n\tlose_time - 匹配丢包数\n\twaste_time - 匹配平均时间\nReturns:\n\taverage_time - 代理ip平均耗时\nModify:\n\t2017-05-27\n\"\"\"\ndef check_ip(ip, lose_time, waste_time):\n\t#命令 -n 要发送的回显请求数 -w 等待每次回复的超时时间(毫秒)\n\tcmd = \"ping -n 3 -w 3 %s\"\n\t#执行命令\n\tp = sp.Popen(cmd % ip, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE, shell=True) \n\t#获得返回结果并解码\n\tout = p.stdout.read().decode(\"gbk\")\n\t#丢包数\n\tlose_time = lose_time.findall(out)\n\t#当匹配到丢失包信息失败,默认为三次请求全部丢包,丢包数lose赋值为3\n\tif len(lose_time) == 0:\n\t\tlose = 3\n\telse:\n\t\tlose = int(lose_time[0])\n\t#如果丢包数目大于2个,则认为连接超时,返回平均耗时1000ms\n\tif lose > 2:\n\t\t#返回False\n\t\treturn 1000\n\t#如果丢包数目小于等于2个,获取平均耗时的时间\n\telse:\n\t\t#平均时间\n\t\taverage = waste_time.findall(out)\n\t\t#当匹配耗时时间信息失败,默认三次请求严重超时,返回平均好使1000ms\n\t\tif len(average) == 0:\n\t\t\treturn 1000\n\t\telse:\n\t\t\t#\n\t\t\taverage_time = int(average[0])\n\t\t\t#返回平均耗时\n\t\t\treturn average_time\n\n\"\"\"\n函数说明:初始化正则表达式\nParameters:\n\t无\nReturns:\n\tlose_time - 匹配丢包数\n\twaste_time - 匹配平均时间\nModify:\n\t2017-05-27\n\"\"\"\ndef initpattern():\n\t#匹配丢包数\n\tlose_time = re.compile(u\"丢失 = (\\d+)\", re.IGNORECASE)\n\t#匹配平均时间\n\twaste_time = re.compile(u\"平均 = (\\d+)ms\", re.IGNORECASE)\n\treturn lose_time, waste_time\n\nif __name__ == '__main__':\n\t#初始化正则表达式\n\tlose_time, waste_time = initpattern()\n\t#获取IP代理\n\tproxys_list = get_proxys(1)\n\n\t#如果平均时间超过200ms重新选取ip\n\twhile True:\n\t\t#从100个IP中随机选取一个IP作为代理进行访问\n\t\tproxy = random.choice(proxys_list)\n\t\tsplit_proxy = proxy.split('#')\n\t\t#获取IP\n\t\tip = split_proxy[1]\n\t\t#检查ip\n\t\taverage_time = check_ip(ip, lose_time, waste_time)\n\t\tif average_time > 200:\n\t\t\t#去掉不能使用的IP\n\t\t\tproxys_list.remove(proxy)\n\t\t\tprint(\"ip连接超时, 重新获取中!\")\n\t\tif average_time < 200:\n\t\t\tbreak\n\n\t#去掉已经使用的IP\n\tproxys_list.remove(proxy)\n\tproxy_dict = {split_proxy[0]:split_proxy[1] + ':' + split_proxy[2]}\n\tprint(\"使用代理:\", proxy_dict)\n"
        },
        {
          "name": "dingdong",
          "type": "tree",
          "content": null
        },
        {
          "name": "douyin.py",
          "type": "blob",
          "size": 4.025390625,
          "content": "# -*- coding:utf-8 -*-\nfrom bs4 import BeautifulSoup\nfrom contextlib import closing\nimport requests, json, time, re, os, sys, time\n\nclass DouYin(object):\n\tdef __init__(self):\n\t\t\"\"\"\n\t\t抖音App视频下载\n\t\t\"\"\"\n\t\t#SSL认证\n\t\tpass\n\n\tdef get_video_urls(self, user_id):\n\t\t\"\"\"\n\t\t获得视频播放地址\n\t\tParameters:\n\t\t\tnickname：查询的用户名\n\t\tReturns:\n\t\t\tvideo_names: 视频名字列表\n\t\t\tvideo_urls: 视频链接列表\n\t\t\taweme_count: 视频数量\n\t\t\"\"\"\n\t\tvideo_names = []\n\t\tvideo_urls = []\n\t\tunique_id = ''\n\t\twhile unique_id != user_id:\n\t\t\tsearch_url = 'https://api.amemv.com/aweme/v1/discover/search/?cursor=0&keyword=%s&count=10&type=1&retry_type=no_retry&iid=17900846586&device_id=34692364855&ac=wifi&channel=xiaomi&aid=1128&app_name=aweme&version_code=162&version_name=1.6.2&device_platform=android&ssmix=a&device_type=MI+5&device_brand=Xiaomi&os_api=24&os_version=7.0&uuid=861945034132187&openudid=dc451556fc0eeadb&manifest_version_code=162&resolution=1080*1920&dpi=480&update_version_code=1622' % user_id\n\t\t\treq = requests.get(url = search_url, verify = False)\n\t\t\thtml = json.loads(req.text)\n\t\t\taweme_count = html['user_list'][0]['user_info']['aweme_count']\n\t\t\tuid = html['user_list'][0]['user_info']['uid']\n\t\t\tnickname = html['user_list'][0]['user_info']['nickname']\n\t\t\tunique_id = html['user_list'][0]['user_info']['unique_id']\n\t\tuser_url = 'https://www.douyin.com/aweme/v1/aweme/post/?user_id=%s&max_cursor=0&count=%s' % (uid, aweme_count)\n\t\treq = requests.get(url = user_url, verify = False)\n\t\thtml = json.loads(req.text)\n\t\ti = 1\n\t\tfor each in html['aweme_list']:\n\t\t\tshare_desc = each['share_info']['share_desc']\n\t\t\tif '抖音-原创音乐短视频社区' == share_desc:\n\t\t\t\tvideo_names.append(str(i) + '.mp4')\n\t\t\t\ti += 1\n\t\t\telse:\n\t\t\t\tvideo_names.append(share_desc + '.mp4')\n\t\t\tvideo_urls.append(each['share_info']['share_url'])\n\n\t\treturn video_names, video_urls, nickname\n\n\tdef get_download_url(self, video_url):\n\t\t\"\"\"\n\t\t获得视频播放地址\n\t\tParameters:\n\t\t\tvideo_url：视频播放地址\n\t\tReturns:\n\t\t\tdownload_url: 视频下载地址\n\t\t\"\"\"\n\t\treq = requests.get(url = video_url, verify = False)\n\t\tbf = BeautifulSoup(req.text, 'lxml')\n\t\tscript = bf.find_all('script')[-1]\n\t\tvideo_url_js = re.findall('var data = \\[(.+)\\];', str(script))[0]\n\t\tvideo_html = json.loads(video_url_js)\n\t\tdownload_url = video_html['video']['play_addr']['url_list'][0]\n\t\treturn download_url\n\n\tdef video_downloader(self, video_url, video_name):\n\t\t\"\"\"\n\t\t视频下载\n\t\tParameters:\n\t\t\tNone\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\"\n\t\tsize = 0\n\t\twith closing(requests.get(video_url, stream=True, verify = False)) as response:\n\t\t\tchunk_size = 1024\n\t\t\tcontent_size = int(response.headers['content-length']) \n\t\t\tif response.status_code == 200:\n\t\t\t\tsys.stdout.write('  [文件大小]:%0.2f MB\\n' % (content_size / chunk_size / 1024))\n\n\t\t\t\twith open(video_name, \"wb\") as file:  \n\t\t\t\t\tfor data in response.iter_content(chunk_size = chunk_size):\n\t\t\t\t\t\tfile.write(data)\n\t\t\t\t\t\tsize += len(data)\n\t\t\t\t\t\tfile.flush()\n\n\t\t\t\t\tsys.stdout.write('    [下载进度]:%.2f%%' % float(size / content_size * 100))\n\t\t\t\t\tsys.stdout.flush()\n\t\ttime.sleep(1)\n\n\n\tdef run(self):\n\t\t\"\"\"\n\t\t运行函数\n\t\tParameters:\n\t\t\tNone\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\"\n\t\tself.hello()\n\t\t# user_id = input('请输入ID(例如13978338):')\n\t\tuser_id = 'sm666888'\n\t\tvideo_names, video_urls, nickname = self.get_video_urls(user_id)\n\t\tif nickname not in os.listdir():\n\t\t\tos.mkdir(nickname)\n\t\tsys.stdout.write('视频下载中:\\n')\n\t\tfor num in range(len(video_urls)):\n\t\t\tprint('  %s\\n' % video_urls[num])\n\t\t\tvideo_url = self.get_download_url(video_urls[num])\n\t\t\tif '\\\\' in video_names[num]:\n\t\t\t\tvideo_name = video_names[num].replace('\\\\', '')\n\t\t\telif '/' in video_names[num]:\n\t\t\t\tvideo_name = video_names[num].replace('/', '')\n\t\t\telse:\n\t\t\t\tvideo_name = video_names[num]\n\t\t\tself.video_downloader(video_url, os.path.join(nickname, video_name))\n\t\t\tprint('')\n\n\tdef hello(self):\n\t\t\"\"\"\n\t\t打印欢迎界面\n\t\tParameters:\n\t\t\tNone\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\"\n\t\tprint('*' * 100)\n\t\tprint('\\t\\t\\t\\t抖音App视频下载小助手')\n\t\tprint('*' * 100)\n\n\t\t\nif __name__ == '__main__':\n\tdouyin = DouYin()\n\tdouyin.run()"
        },
        {
          "name": "douyin",
          "type": "tree",
          "content": null
        },
        {
          "name": "douyin_pro.py",
          "type": "blob",
          "size": 5.28515625,
          "content": "# -*- coding:utf-8 -*-\nfrom splinter.driver.webdriver.chrome import Options, Chrome\nfrom splinter.browser import Browser\nfrom contextlib import closing\nimport requests, json, time, re, os, sys, time\nfrom bs4 import BeautifulSoup\n\nclass DouYin(object):\n\tdef __init__(self, width = 500, height = 300):\n\t\t\"\"\"\n\t\t抖音App视频下载\n\t\t\"\"\"\n\t\t# 无头浏览器\n\t\tchrome_options = Options()\n\t\tchrome_options.add_argument('user-agent=\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\"')\n\t\tself.driver = Browser(driver_name='chrome', executable_path='D:/chromedriver', options=chrome_options, headless=True)\n\n\tdef get_video_urls(self, user_id):\n\t\t\"\"\"\n\t\t获得视频播放地址\n\t\tParameters:\n\t\t\tuser_id：查询的用户ID\n\t\tReturns:\n\t\t\tvideo_names: 视频名字列表\n\t\t\tvideo_urls: 视频链接列表\n\t\t\tnickname: 用户昵称\n\t\t\"\"\"\n\t\tvideo_names = []\n\t\tvideo_urls = []\n\t\tunique_id = ''\n\t\twhile unique_id != user_id:\n\t\t\tsearch_url = 'https://api.amemv.com/aweme/v1/discover/search/?cursor=0&keyword=%s&count=10&type=1&retry_type=no_retry&iid=17900846586&device_id=34692364855&ac=wifi&channel=xiaomi&aid=1128&app_name=aweme&version_code=162&version_name=1.6.2&device_platform=android&ssmix=a&device_type=MI+5&device_brand=Xiaomi&os_api=24&os_version=7.0&uuid=861945034132187&openudid=dc451556fc0eeadb&manifest_version_code=162&resolution=1080*1920&dpi=480&update_version_code=1622' % user_id\n\t\t\treq = requests.get(url = search_url, verify = False)\n\t\t\thtml = json.loads(req.text)\n\t\t\taweme_count = html['user_list'][0]['user_info']['aweme_count']\n\t\t\tuid = html['user_list'][0]['user_info']['uid']\n\t\t\tnickname = html['user_list'][0]['user_info']['nickname']\n\t\t\tunique_id = html['user_list'][0]['user_info']['unique_id']\n\t\tuser_url = 'https://www.douyin.com/aweme/v1/aweme/post/?user_id=%s&max_cursor=0&count=%s' % (uid, aweme_count)\n\t\treq = requests.get(url = user_url, verify = False)\n\t\thtml = json.loads(req.text)\n\t\ti = 1\n\t\tfor each in html['aweme_list']:\n\t\t\tshare_desc = each['share_info']['share_desc']\n\t\t\tif '抖音-原创音乐短视频社区' == share_desc:\n\t\t\t\tvideo_names.append(str(i) + '.mp4')\n\t\t\t\ti += 1\n\t\t\telse:\n\t\t\t\tvideo_names.append(share_desc + '.mp4')\n\t\t\tvideo_urls.append(each['share_info']['share_url'])\n\n\t\treturn video_names, video_urls, nickname\n\n\tdef get_download_url(self, video_url):\n\t\t\"\"\"\n\t\t获得带水印的视频播放地址\n\t\tParameters:\n\t\t\tvideo_url：带水印的视频播放地址\n\t\tReturns:\n\t\t\tdownload_url: 带水印的视频下载地址\n\t\t\"\"\"\n\t\treq = requests.get(url = video_url, verify = False)\n\t\tbf = BeautifulSoup(req.text, 'lxml')\n\t\tscript = bf.find_all('script')[-1]\n\t\tvideo_url_js = re.findall('var data = \\[(.+)\\];', str(script))[0]\n\t\tvideo_html = json.loads(video_url_js)\n\t\tdownload_url = video_html['video']['play_addr']['url_list'][0]\n\t\treturn download_url\n\n\tdef video_downloader(self, video_url, video_name, watermark_flag=True):\n\t\t\"\"\"\n\t\t视频下载\n\t\tParameters:\n\t\t\tvideo_url: 带水印的视频地址\n\t\t\tvideo_name: 视频名\n\t\t\twatermark_flag: 是否下载不带水印的视频\n\t\tReturns:\n\t\t\t无\n\t\t\"\"\"\n\t\tsize = 0\n\t\tif watermark_flag == True:\n\t\t\tvideo_url = self.remove_watermark(video_url)\n\t\telse:\n\t\t\tvideo_url = self.get_download_url(video_url)\n\t\twith closing(requests.get(video_url, stream=True, verify = False)) as response:\n\t\t\tchunk_size = 1024\n\t\t\tcontent_size = int(response.headers['content-length']) \n\t\t\tif response.status_code == 200:\n\t\t\t\tsys.stdout.write('  [文件大小]:%0.2f MB\\n' % (content_size / chunk_size / 1024))\n\n\t\t\t\twith open(video_name, \"wb\") as file:  \n\t\t\t\t\tfor data in response.iter_content(chunk_size = chunk_size):\n\t\t\t\t\t\tfile.write(data)\n\t\t\t\t\t\tsize += len(data)\n\t\t\t\t\t\tfile.flush()\n\n\t\t\t\t\t\tsys.stdout.write('  [下载进度]:%.2f%%' % float(size / content_size * 100) + '\\r')\n\t\t\t\t\t\tsys.stdout.flush()\n\n\n\tdef remove_watermark(self, video_url):\n\t\t\"\"\"\n\t\t获得无水印的视频播放地址\n\t\tParameters:\n\t\t\tvideo_url: 带水印的视频地址\n\t\tReturns:\n\t\t\t无水印的视频下载地址\n\t\t\"\"\"\n\t\tself.driver.visit('http://douyin.iiilab.com/')\n\t\tself.driver.find_by_tag('input').fill(video_url)\n\t\tself.driver.find_by_xpath('//button[@class=\"btn btn-default\"]').click()\n\t\thtml = self.driver.find_by_xpath('//div[@class=\"thumbnail\"]/div/p')[0].html\n\t\tbf = BeautifulSoup(html, 'lxml')\n\t\treturn bf.find('a').get('href')\n\n\tdef run(self):\n\t\t\"\"\"\n\t\t运行函数\n\t\tParameters:\n\t\t\tNone\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\"\n\t\tself.hello()\n\t\tuser_id = input('请输入ID(例如40103580):')\n\t\tvideo_names, video_urls, nickname = self.get_video_urls(user_id)\n\t\tif nickname not in os.listdir():\n\t\t\tos.mkdir(nickname)\n\t\tprint('视频下载中:共有%d个作品!\\n' % len(video_urls))\n\t\tfor num in range(len(video_urls)):\n\t\t\tprint('  解析第%d个视频链接 [%s] 中，请稍后!\\n' % (num+1, video_urls[num]))\n\t\t\tif '\\\\' in video_names[num]:\n\t\t\t\tvideo_name = video_names[num].replace('\\\\', '')\n\t\t\telif '/' in video_names[num]:\n\t\t\t\tvideo_name = video_names[num].replace('/', '')\n\t\t\telse:\n\t\t\t\tvideo_name = video_names[num]\n\t\t\tself.video_downloader(video_urls[num], os.path.join(nickname, video_name))\n\t\t\tprint('\\n')\n\n\t\tprint('下载完成!')\n\n\tdef hello(self):\n\t\t\"\"\"\n\t\t打印欢迎界面\n\t\tParameters:\n\t\t\tNone\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\"\n\t\tprint('*' * 100)\n\t\tprint('\\t\\t\\t\\t抖音App视频下载小助手')\n\t\tprint('\\t\\t作者:Jack Cui')\n\t\tprint('*' * 100)\n\n\nif __name__ == '__main__':\n\tdouyin = DouYin()\n\tdouyin.run()\n"
        },
        {
          "name": "downloader.py",
          "type": "blob",
          "size": 2.2421875,
          "content": "#-*- coding: UTF-8 -*-\nimport requests  \nfrom contextlib import closing\n\nclass ProgressBar(object):  \n    def __init__(self, title, count=0.0, run_status=None, fin_status=None, total=100.0, unit='', sep='/', chunk_size=1.0):  \n        super(ProgressBar, self).__init__()  \n        self.info = \"[%s] %s %.2f %s %s %.2f %s\"  \n        self.title = title  \n        self.total = total  \n        self.count = count  \n        self.chunk_size = chunk_size  \n        self.status = run_status or \"\"  \n        self.fin_status = fin_status or \" \" * len(self.status)  \n        self.unit = unit  \n        self.seq = sep  \n  \n    def __get_info(self):  \n        #[名称] 状态 进度 单位 分割线 总数 单位  \n        _info = self.info % (self.title, self.status, self.count/self.chunk_size, self.unit, self.seq, self.total/self.chunk_size, self.unit)  \n        return _info  \n  \n    def refresh(self, count = 1, status = None):  \n        self.count += count  \n        self.status = status or self.status  \n        end_str = \"\\r\"  \n        if self.count >= self.total:  \n            end_str = '\\n'  \n            self.status = status or self.fin_status  \n        print(self.__get_info(), end=end_str, )  \n\n\nif __name__ == '__main__':\n\t#url = 'http://www.demongan.com/source/game/二十四点.zip'\n\t#filename = '二十四点.zip'\n\tprint('*' * 100)\n\tprint('\\t\\t\\t\\t欢迎使用文件下载小助手')\n\tprint('作者:Jack-Cui\\n博客:http://blog.csdn.net/c406495762')\n\tprint('*' * 100)\n\turl  = input('请输入需要下载的文件链接:\\n')\n\tfilename = url.split('/')[-1]\n\twith closing(requests.get(url, stream=True)) as response:  \n\t\tchunk_size = 1024  \n\t\tcontent_size = int(response.headers['content-length'])  \n\t\tif response.status_code == 200:\n\t\t\tprint('文件大小:%0.2f KB' % (content_size / chunk_size))\n\t\t\tprogress = ProgressBar(\"%s下载进度\" % filename\n\t\t\t            , total = content_size  \n\t\t\t            , unit = \"KB\"  \n\t\t\t            , chunk_size = chunk_size  \n\t\t\t            , run_status = \"正在下载\"  \n\t\t\t            , fin_status = \"下载完成\")  \n\n\t\t\twith open(filename, \"wb\") as file:  \n\t\t\t        for data in response.iter_content(chunk_size=chunk_size):  \n\t\t\t            file.write(data)  \n\t\t\t            progress.refresh(count=len(data))  \n\t\telse:\n\t\t\tprint('链接异常')"
        },
        {
          "name": "financical.py",
          "type": "blob",
          "size": 7.3154296875,
          "content": "#-*- coding:UTF-8 -*-\nimport sys\nimport pymysql\nimport requests\nimport json\nimport re\nfrom bs4 import BeautifulSoup\n\n\"\"\"\n类说明:获取财务数据\n\nAuthor:\n\tJack Cui\nBlog:\n\thttp://blog.csdn.net/c406495762\nZhihu:\n\thttps://www.zhihu.com/people/Jack--Cui/\nModify:\n\t2017-08-31\n\"\"\"\nclass FinancialData():\n\n\tdef __init__(self):\n\t\t#服务器域名\n\t\tself.server = 'http://quotes.money.163.com/'\n\t\tself.cwnb = 'http://quotes.money.163.com/hkstock/cwsj_'\n\t\t#主要财务指标\n\t\tself.cwzb_dict = {'EPS':'基本每股收益','EPS_DILUTED':'摊薄每股收益','GROSS_MARGIN':'毛利率',\n\t\t'CAPITAL_ADEQUACY':'资本充足率','LOANS_DEPOSITS':'贷款回报率','ROTA':'总资产收益率',\n\t\t'ROEQUITY':'净资产收益率','CURRENT_RATIO':'流动比率','QUICK_RATIO':'速动比率',\n\t\t'ROLOANS':'存贷比','INVENTORY_TURNOVER':'存货周转率','GENERAL_ADMIN_RATIO':'管理费用比率',\n\t\t'TOTAL_ASSET2TURNOVER':'资产周转率','FINCOSTS_GROSSPROFIT':'财务费用比率','TURNOVER_CASH':'销售现金比率','YEAREND_DATE':'报表日期'}\n\t\t#利润表\n\t\tself.lrb_dict = {'TURNOVER':'总营收','OPER_PROFIT':'经营利润','PBT':'除税前利润',\n\t\t'NET_PROF':'净利润','EPS':'每股基本盈利','DPS':'每股派息',\n\t\t'INCOME_INTEREST':'利息收益','INCOME_NETTRADING':'交易收益','INCOME_NETFEE':'费用收益','YEAREND_DATE':'报表日期'}\n\t\t#资产负债表\n\t\tself.fzb_dict = {\n\t\t\t'FIX_ASS':'固定资产','CURR_ASS':'流动资产','CURR_LIAB':'流动负债',\n\t\t\t'INVENTORY':'存款','CASH':'现金及银行存结','OTHER_ASS':'其他资产',\n\t\t\t'TOTAL_ASS':'总资产','TOTAL_LIAB':'总负债','EQUITY':'股东权益',\n\t\t\t'CASH_SHORTTERMFUND':'库存现金及短期资金','DEPOSITS_FROM_CUSTOMER':'客户存款',\n\t\t\t'FINANCIALASSET_SALE':'可供出售之证券','LOAN_TO_BANK':'银行同业存款及贷款',\n\t\t\t'DERIVATIVES_LIABILITIES':'金融负债','DERIVATIVES_ASSET':'金融资产','YEAREND_DATE':'报表日期'}\n\t\t#现金流表\n\t\tself.llb_dict = {\n\t\t\t'CF_NCF_OPERACT':'经营活动产生的现金流','CF_INT_REC':'已收利息','CF_INT_PAID':'已付利息',\n\t\t\t'CF_INT_REC':'已收股息','CF_DIV_PAID':'已派股息','CF_INV':'投资活动产生现金流',\n\t\t\t'CF_FIN_ACT':'融资活动产生现金流','CF_BEG':'期初现金及现金等价物','CF_CHANGE_CSH':'现金及现金等价物净增加额',\n\t\t\t'CF_END':'期末现金及现金等价物','CF_EXCH':'汇率变动影响','YEAREND_DATE':'报表日期'}\n\t\t#总表\n\t\tself.table_dict = {'cwzb':self.cwzb_dict,'lrb':self.lrb_dict,'fzb':self.fzb_dict,'llb':self.llb_dict}\n\t\t#请求头\n\t\tself.headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n\t\t\t'Accept-Encoding': 'gzip, deflate',\n\t\t\t'Accept-Language': 'zh-CN,zh;q=0.8',\n\t\t\t'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36'}\n\t\n\t\"\"\"\n\t函数说明:获取股票页面信息\n\n\tAuthor:\n\t\tJack Cui\n\tParameters:\n\t    url - 股票财务数据界面地址\n\tReturns:\n\t    name - 股票名\n\t    table_name_list - 财务报表名称\n\t    table_date_list - 财务报表年限\n\t    url_list - 财务报表查询连接\n\tBlog:\n\t\thttp://blog.csdn.net/c406495762\n\tZhihu:\n\t\thttps://www.zhihu.com/people/Jack--Cui/\n\tModify:\n\t\t2017-08-31\n\t\"\"\"\n\tdef get_informations(self, url):\n\t\treq = requests.get(url = url, headers = self.headers)\n\t\treq.encoding = 'utf-8'\n\t\thtml = req.text\n\t\tpage_bf = BeautifulSoup(html, 'lxml')\n\t\t#股票名称，股票代码\n\t\tname = page_bf.find_all('span', class_ = 'name')[0].string\n\t\t# code = page_bf.find_all('span', class_ = 'code')[0].string\n\t\t# code = re.findall('\\d+',code)[0]\n\n\t\t#存储各个表名的列表\n\t\ttable_name_list = []\n\t\ttable_date_list = []\n\t\teach_date_list = []\n\t\turl_list = []\n\t\t#表名和表时间\n\t\ttable_name = page_bf.find_all('div', class_ = 'titlebar3')\n\t\tfor each_table_name in table_name:\n\t\t\t#表名\n\t\t\ttable_name_list.append(each_table_name.span.string)\n\t\t\t#表时间\n\t\t\tfor each_table_date in each_table_name.div.find_all('select', id = re.compile('.+1$')):\n\t\t\t\turl_list.append(re.findall('(\\w+)1',each_table_date.get('id'))[0])\n\t\t\t\tfor each_date in each_table_date.find_all('option'):\n\t\t\t\t\teach_date_list.append(each_date.string)\n\t\t\t\ttable_date_list.append(each_date_list)\n\t\t\t\teach_date_list = []\n\t\treturn name,table_name_list,table_date_list,url_list\n\n\t\"\"\"\n\t函数说明:财务报表入库\n\n\tAuthor:\n\t\tJack Cui\n\tParameters:\n\t    name - 股票名\n\t    table_name_list - 财务报表名称\n\t    table_date_list - 财务报表年限\n\t    url_list - 财务报表查询连接\n\tReturns:\n\t\t无\n\tBlog:\n\t\thttp://blog.csdn.net/c406495762\n\tZhihu:\n\t\thttps://www.zhihu.com/people/Jack--Cui/\n\tModify:\n\t\t2017-08-31\n\t\"\"\"\n\tdef insert_tables(self, name, table_name_list,table_date_list, url_list):\n\t\t#打开数据库连接:host-连接主机地址,port-端口号,user-用户名,passwd-用户密码,db-数据库名,charset-编码\n\t\tconn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='yourpasswd',db='financialdata',charset='utf8')\n\t\t#使用cursor()方法获取操作游标\n\t\tcursor = conn.cursor()  \n\t\t#插入信息\n\t\tfor i in range(len(table_name_list)):\n\t\t\tsys.stdout.write('    [正在下载       ]    %s' % table_name_list[i] + '\\r')\n\t\t\t#获取数据地址\n\t\t\turl = self.server + 'hk/service/cwsj_service.php?symbol={}&start={}&end={}&type={}&unit=yuan'.format(code,table_date_list[i][-1],table_date_list[i][0],url_list[i])\n\t\t\treq_table = requests.get(url = url, headers = self.headers)\n\t\t\ttable = req_table.json()\n\t\t\tnums = len(table)\n\t\t\tvalue_dict = {}\n\t\t\tfor num in range(nums):\n\t\t\t\tsys.stdout.write('    [正在下载 %.2f%%]   ' % (((num+1) / nums)*100) + '\\r')\n\t\t\t\tsys.stdout.flush()\n\t\t\t\tvalue_dict['股票名'] = name\n\t\t\t\tvalue_dict['股票代码'] = code\n\t\t\t\tfor key, value in table[i].items():\n\t\t\t\t\tif key in self.table_dict[url_list[i]]:\n\t\t\t\t\t\tvalue_dict[self.table_dict[url_list[i]][key]] = value\n\n\t\t\t\tsql1 = \"\"\"\n\t\t\t\tINSERT INTO %s (`股票名`,`股票代码`,`报表日期`) VALUES ('%s','%s','%s')\"\"\" % (url_list[i],value_dict['股票名'],value_dict['股票代码'],value_dict['报表日期'])\n\t\t\t\ttry:\n\t\t\t\t\tcursor.execute(sql1)\n\t\t\t\t\t# 执行sql语句\n\t\t\t\t\tconn.commit()\n\t\t\t\texcept:\n\t\t\t\t\t# 发生错误时回滚\n\t\t\t\t\tconn.rollback()\n\n\t\t\t\tfor key, value in value_dict.items():\n\t\t\t\t\tif key not in ['股票名','股票代码','报表日期']:\n\t\t\t\t\t\tsql2 = \"\"\"\n\t\t\t\t\t\tUPDATE %s SET %s='%s' WHERE `股票名`='%s' AND `报表日期`='%s'\"\"\" % (url_list[i],key,value,value_dict['股票名'],value_dict['报表日期'])\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tcursor.execute(sql2)\n\t\t\t\t\t\t\t# 执行sql语句\n\t\t\t\t\t\t\tconn.commit()\n\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t# 发生错误时回滚\n\t\t\t\t\t\t\tconn.rollback()\n\t\t\t\tvalue_dict = {}\n\t\t\tprint('    [下载完成 ')\n\n\t\t# 关闭数据库连接\n\t\tcursor.close()  \n\t\tconn.close()\n\nif __name__ == '__main__':\n\tprint('*' * 100)\n\tprint('\\t\\t\\t\\t\\t财务数据下载助手\\n')\n\tprint('作者:Jack-Cui\\n')\n\tprint('About Me:\\n')\n\tprint('  知乎:https://www.zhihu.com/people/Jack--Cui')\n\tprint('  Blog:http://blog.csdn.net/c406495762')\n\tprint('  Gihub:https://github.com/Jack-Cherish\\n')\n\tprint('*' * 100)\n\tfd = FinancialData()\n\t#上市股票地址\n\tcode = input('请输入股票代码:')\n\n\tname,table_name_list,table_date_list,url_list = fd.get_informations(fd.cwnb + code + '.html')\n\tprint('\\n  %s:(%s)财务数据下载中！\\n' % (name,code))\n\tfd.insert_tables(name,table_name_list,table_date_list,url_list)\n\tprint('\\n  %s:(%s)财务数据下载完成！' % (name,code))"
        },
        {
          "name": "geetest.py",
          "type": "blob",
          "size": 8.833984375,
          "content": "# -*-coding:utf-8 -*-\nimport random\nimport re\nimport time\n# 图片转换\nimport base64\nfrom urllib.request import urlretrieve\n\nfrom bs4 import BeautifulSoup\n\nimport PIL.Image as image\nfrom selenium import webdriver\nfrom selenium.webdriver import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\ndef save_base64img(data_str, save_name):\n    \"\"\"\n    将 base64 数据转化为图片保存到指定位置\n    :param data_str: base64 数据，不包含类型\n    :param save_name: 保存的全路径\n    \"\"\"\n    img_data = base64.b64decode(data_str)\n    file = open(save_name, 'wb')\n    file.write(img_data)\n    file.close()\n\n\ndef get_base64_by_canvas(driver, class_name, contain_type):\n    \"\"\"\n    将 canvas 标签内容转换为 base64 数据\n    :param driver: webdriver 对象\n    :param class_name: canvas 标签的类名\n    :param contain_type: 返回的数据是否包含类型\n    :return: base64 数据\n    \"\"\"\n    # 防止图片未加载完就下载一张空图\n    bg_img = ''\n    while len(bg_img) < 5000:\n        getImgJS = 'return document.getElementsByClassName(\"' + class_name + '\")[0].toDataURL(\"image/png\");'\n        bg_img = driver.execute_script(getImgJS)\n        time.sleep(0.5)\n    # print(bg_img)\n    if contain_type:\n        return bg_img\n    else:\n        return bg_img[bg_img.find(',') + 1:]\n\n\ndef save_bg(driver, bg_path=\"bg.png\", bg_class=\"geetest_canvas_bg geetest_absolute\"):\n    \"\"\"\n    保存包含缺口的背景图\n    :param driver: webdriver 对象\n    :param bg_path: 保存路径\n    :param bg_class: 背景图的 class 属性\n    :return: 保存路径\n    \"\"\"\n    bg_img_data = get_base64_by_canvas(driver, bg_class, False)\n    save_base64img(bg_img_data, bg_path)\n    return bg_path\n\n\ndef save_full_bg(driver, full_bg_path=\"fbg.png\", full_bg_class=\"geetest_canvas_fullbg geetest_fade geetest_absolute\"):\n    \"\"\"\n    保存完整的的背景图\n    :param driver: webdriver 对象\n    :param full_bg_path: 保存路径\n    :param full_bg_class: 完整背景图的 class 属性\n    :return: 保存路径\n    \"\"\"\n    bg_img_data = get_base64_by_canvas(driver, full_bg_class, False)\n    save_base64img(bg_img_data, full_bg_path)\n    return full_bg_path\n\nclass Crack():\n\tdef __init__(self,keyword):\n\t\tself.url = '*'\n\t\tself.browser = webdriver.Chrome('D:\\\\chromedriver.exe')\n\t\tself.wait = WebDriverWait(self.browser, 100)\n\t\tself.keyword = keyword\n\t\tself.BORDER = 6\n\n\tdef open(self):\n\t\t\"\"\"\n\t\t打开浏览器,并输入查询内容\n\t\t\"\"\"\n\t\tself.browser.get(self.url)\n\t\tkeyword = self.wait.until(EC.presence_of_element_located((By.ID, 'keyword_qycx')))\n\t\tbowton = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'btn')))\n\t\tkeyword.send_keys(self.keyword)\n\t\tbowton.click()\n\n\tdef get_images(self, bg_filename = 'bg.jpg', fullbg_filename = 'fullbg.jpg'):\n\t\t\"\"\"\n\t\t获取验证码图片\n\t\t:return: 图片的location信息\n\t\t\"\"\"\n\t\tbg = []\n\t\tfullgb = []\n\t\twhile bg == [] and fullgb == []:\n\t\t\tbf = BeautifulSoup(self.browser.page_source, 'lxml')\n\t\t\tbg = bf.find_all('div', class_ = 'gt_cut_bg_slice')\n\t\t\tfullgb = bf.find_all('div', class_ = 'gt_cut_fullbg_slice')\n\t\tbg_url = re.findall('url\\(\\\"(.*)\\\"\\);', bg[0].get('style'))[0].replace('webp', 'jpg')\n\t\tfullgb_url = re.findall('url\\(\\\"(.*)\\\"\\);', fullgb[0].get('style'))[0].replace('webp', 'jpg')\n\t\tbg_location_list = []\n\t\tfullbg_location_list = []\n\t\tfor each_bg in bg:\n\t\t\tlocation = {}\n\t\t\tlocation['x'] = int(re.findall('background-position: (.*)px (.*)px;',each_bg.get('style'))[0][0])\n\t\t\tlocation['y'] = int(re.findall('background-position: (.*)px (.*)px;',each_bg.get('style'))[0][1])\n\t\t\tbg_location_list.append(location)\n\t\tfor each_fullgb in fullgb:\n\t\t\tlocation = {}\n\t\t\tlocation['x'] = int(re.findall('background-position: (.*)px (.*)px;',each_fullgb.get('style'))[0][0])\n\t\t\tlocation['y'] = int(re.findall('background-position: (.*)px (.*)px;',each_fullgb.get('style'))[0][1])\n\t\t\tfullbg_location_list.append(location)\n\n\t\turlretrieve(url = bg_url, filename = bg_filename)\n\t\tprint('缺口图片下载完成')\n\t\turlretrieve(url = fullgb_url, filename = fullbg_filename)\n\t\tprint('背景图片下载完成')\n\t\treturn bg_location_list, fullbg_location_list\n\n\tdef get_merge_image(self, filename, location_list):\n\t\t\"\"\"\n\t\t根据位置对图片进行合并还原\n\t\t:filename:图片\n\t\t:location_list:图片位置\n\t\t\"\"\"\n\t\tim = image.open(filename)\n\t\tnew_im = image.new('RGB', (260,116))\n\t\tim_list_upper=[]\n\t\tim_list_down=[]\n\n\t\tfor location in location_list:\n\t\t\tif location['y'] == -58:\n\t\t\t\tim_list_upper.append(im.crop((abs(location['x']),58,abs(location['x']) + 10, 166)))\n\t\t\tif location['y'] == 0:\n\t\t\t\tim_list_down.append(im.crop((abs(location['x']),0,abs(location['x']) + 10, 58)))\n\n\t\tnew_im = image.new('RGB', (260,116))\n\n\t\tx_offset = 0\n\t\tfor im in im_list_upper:\n\t\t\tnew_im.paste(im, (x_offset,0))\n\t\t\tx_offset += im.size[0]\n\n\t\tx_offset = 0\n\t\tfor im in im_list_down:\n\t\t\tnew_im.paste(im, (x_offset,58))\n\t\t\tx_offset += im.size[0]\n\n\t\tnew_im.save(filename)\n\n\t\treturn new_im\n\n\tdef get_merge_image(self, filename, location_list):\n\t\t\"\"\"\n\t\t根据位置对图片进行合并还原\n\t\t:filename:图片\n\t\t:location_list:图片位置\n\t\t\"\"\"\n\t\tim = image.open(filename)\n\t\tnew_im = image.new('RGB', (260,116))\n\t\tim_list_upper=[]\n\t\tim_list_down=[]\n\n\t\tfor location in location_list:\n\t\t\tif location['y']==-58:\n\t\t\t\tim_list_upper.append(im.crop((abs(location['x']),58,abs(location['x'])+10,166)))\n\t\t\tif location['y']==0:\n\t\t\t\tim_list_down.append(im.crop((abs(location['x']),0,abs(location['x'])+10,58)))\n\n\t\tnew_im = image.new('RGB', (260,116))\n\n\t\tx_offset = 0\n\t\tfor im in im_list_upper:\n\t\t\tnew_im.paste(im, (x_offset,0))\n\t\t\tx_offset += im.size[0]\n\n\t\tx_offset = 0\n\t\tfor im in im_list_down:\n\t\t\tnew_im.paste(im, (x_offset,58))\n\t\t\tx_offset += im.size[0]\n\n\t\tnew_im.save(filename)\n\n\t\treturn new_im\n\n\tdef is_pixel_equal(self, img1, img2, x, y):\n\t\t\"\"\"\n\t\t判断两个像素是否相同\n\t\t:param image1: 图片1\n\t\t:param image2: 图片2\n\t\t:param x: 位置x\n\t\t:param y: 位置y\n\t\t:return: 像素是否相同\n\t\t\"\"\"\n\t\t# 取两个图片的像素点\n\t\tpix1 = img1.load()[x, y]\n\t\tpix2 = img2.load()[x, y]\n\t\tthreshold = 60\n\t\tif (abs(pix1[0] - pix2[0] < threshold) and abs(pix1[1] - pix2[1] < threshold) and abs(pix1[2] - pix2[2] < threshold)):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef get_gap(self, img1, img2):\n\t\t\"\"\"\n\t\t获取缺口偏移量\n\t\t:param img1: 不带缺口图片\n\t\t:param img2: 带缺口图片\n\t\t:return:\n\t\t\"\"\"\n\t\tleft = 43\n\t\tfor i in range(left, img1.size[0]):\n\t\t\tfor j in range(img1.size[1]):\n\t\t\t\tif not self.is_pixel_equal(img1, img2, i, j):\n\t\t\t\t\tleft = i\n\t\t\t\t\treturn left\n\t\treturn left\t\n\n\tdef get_track(self, distance):\n\t\t\"\"\"\n\t\t根据偏移量获取移动轨迹\n\t\t:param distance: 偏移量\n\t\t:return: 移动轨迹\n\t\t\"\"\"\n\t\t# 移动轨迹\n\t\ttrack = []\n\t\t# 当前位移\n\t\tcurrent = 0\n\t\t# 减速阈值\n\t\tmid = distance * 4 / 5\n\t\t# 计算间隔\n\t\tt = 0.2\n\t\t# 初速度\n\t\tv = 0\n        \n\t\twhile current < distance:\n\t\t\tif current < mid:\n\t\t\t\t# 加速度为正2\n\t\t\t\ta = 2\n\t\t\telse:\t\n\t\t\t\t# 加速度为负3\n\t\t\t\ta = -3\n\t\t\t# 初速度v0\n\t\t\tv0 = v\n\t\t\t# 当前速度v = v0 + at\n\t\t\tv = v0 + a * t\n\t\t\t# 移动距离x = v0t + 1/2 * a * t^2\n\t\t\tmove = v0 * t + 1 / 2 * a * t * t\n\t\t\t# 当前位移\n\t\t\tcurrent += move\n\t\t\t# 加入轨迹\n\t\t\ttrack.append(round(move))\n\t\treturn track\n\n\tdef get_slider(self):\n\t\t\"\"\"\n\t\t获取滑块\n\t\t:return: 滑块对象\n\t\t\"\"\"\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tslider = self.browser.find_element_by_xpath(\"//div[@class='gt_slider_knob gt_show']\")\n\t\t\t\tbreak\n\t\t\texcept:\n\t\t\t\ttime.sleep(0.5)\n\t\treturn slider\n\n\tdef move_to_gap(self, slider, track):\n\t\t\"\"\"\n\t\t拖动滑块到缺口处\n\t\t:param slider: 滑块\n\t\t:param track: 轨迹\n\t\t:return:\n\t\t\"\"\"\n\t\tActionChains(self.browser).click_and_hold(slider).perform()\n\t\twhile track:\n\t\t\tx = random.choice(track)\n\t\t\tActionChains(self.browser).move_by_offset(xoffset=x, yoffset=0).perform()\n\t\t\ttrack.remove(x)\n\t\ttime.sleep(0.5)\n\t\tActionChains(self.browser).release().perform()\n\n\tdef crack(self):\n\t\t# 打开浏览器\n\t\tself.open()\n\n\t\t# 保存的图片名字\n\t\tbg_filename = 'bg.jpg'\n\t\tfullbg_filename = 'fullbg.jpg'\n\n\t\t# 获取图片\n\t\tbg_location_list, fullbg_location_list = self.get_images(bg_filename, fullbg_filename)\n\n\t\t# 根据位置对图片进行合并还原\n\t\t# 方法1\n\t\t# bg_img = self.get_merge_image(bg_filename, bg_location_list)\n\t\t# fullbg_img = self.get_merge_image(fullbg_filename, fullbg_location_list)\n\t\t# 方法2\n\t\tbg_img = save_bg(self.browser)\n\t\tfull_bg_img = save_full_bg(self.browser)\n\n\t\t# 获取缺口位置\n\t\t# 方法1\n\t\t# gap = self.get_gap(fullbg_img, bg_img)\n\t\t# 方法2\n\t\tgap = self.get_gap(image.open(full_bg_img), image.open(bg_img))\n\t\tprint('缺口位置', gap)\n\n\t\ttrack = self.get_track(gap-self.BORDER)\n\t\tprint('滑动滑块')\n\t\tprint(track)\n\n\t\t# # 点按呼出缺口\n\t\t# slider = self.get_slider()\n\t\t# # 拖动滑块到缺口处\n\t\t# self.move_to_gap(slider, track)\n\nif __name__ == '__main__':\n\tprint('开始验证')\n\tcrack = Crack(u'中国移动')\n\tcrack.crack()\n\tprint('验证成功')\n"
        },
        {
          "name": "hero.py",
          "type": "blob",
          "size": 6.00390625,
          "content": "#-*- coding: UTF-8 -*-\nfrom urllib.request import urlretrieve\nimport requests\nimport os\n\n\"\"\"\n函数说明:下载《英雄联盟盒子》中的英雄图片\n\nParameters:\n    url - GET请求地址，通过Fiddler抓包获取\n    header - headers信息\nReturns:\n    无\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nModify:\n    2017-08-07\n\"\"\"\ndef hero_imgs_download(url, header):\n    req = requests.get(url = url, headers = header).json()\n    hero_num = len(req['list'])\n    print('一共有%d个英雄' % hero_num)\n    hero_images_path = 'hero_images'\n    for each_hero in req['list']:\n        hero_photo_url = each_hero['cover']\n        hero_name = each_hero['name'] + '.jpg'\n        filename = hero_images_path + '/' + hero_name\n        if hero_images_path not in os.listdir():\n            os.makedirs(hero_images_path)\n        urlretrieve(url = hero_photo_url, filename = filename)\n\n\"\"\"\n函数说明:打印所有英雄的名字和ID\n\nParameters:\n    url - GET请求地址，通过Fiddler抓包获取\n    header - headers信息\nReturns:\n    无\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nModify:\n    2017-08-07\n\"\"\"\ndef hero_list(url, header):\n\tprint('*' * 100)\n\tprint('\\t\\t\\t\\t欢迎使用《王者荣耀》出装下助手！')\n\tprint('*' * 100)\n\treq = requests.get(url = url, headers = header).json()\n\tflag = 0\n\tfor each_hero in req['list']:\n\t\tflag += 1\n\t\tprint('%s的ID为:%-7s' % (each_hero['name'], each_hero['hero_id']), end = '\\t\\t')\n\t\tif flag == 3:\n\t\t\tprint('\\n', end = '')\n\t\t\tflag = 0\n\n\"\"\"\n函数说明:根据equip_id查询武器名字和价格\n\nParameters:\n    equip_id - 武器的ID\n    weapon_info - 存储所有武器的字典\nReturns:\n    weapon_name - 武器的名字\n    weapon_price - 武器的价格\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nModify:\n    2017-08-07\n\"\"\"\ndef seek_weapon(equip_id, weapon_info):\n\tfor each_weapon in weapon_info:\n\t\tif each_weapon['equip_id'] == str(equip_id):\n\t\t\tweapon_name = each_weapon['name']\n\t\t\tweapon_price = each_weapon['price']\n\t\t\treturn weapon_name, weapon_price\n\n\n\"\"\"\n函数说明:获取并打印出装信息\n\nParameters:\n    url - GET请求地址，通过Fiddler抓包获取\n    header - headers信息\n    weapon_info - 存储所有武器的字典\nReturns:\n\t无\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nModify:\n    2017-08-07\n\"\"\"\ndef hero_info(url, header, weapon_info):\n\treq = requests.get(url = url, headers = header).json()\n\tprint('\\n历史上的%s:\\n    %s' % (req['info']['name'], req['info']['history_intro']))\n\tfor each_equip_choice in req['info']['equip_choice']:\n\t\tprint('\\n%s:\\n   %s' % (each_equip_choice['title'], each_equip_choice['description']))\n\t\ttotal_price = 0\n\t\tflag = 0\n\t\tfor each_weapon in each_equip_choice['list']:\n\t\t\tflag += 1\n\t\t\tweapon_name, weapon_price = seek_weapon(each_weapon['equip_id'], weapon_info)\n\t\t\tprint('%s:%s' % (weapon_name, weapon_price), end = '\\t')\n\t\t\tif flag == 3:\n\t\t\t\tprint('\\n', end = '')\n\t\t\t\tflag = 0\n\t\t\ttotal_price += int(weapon_price)\n\t\tprint('神装套件价格共计:%d' % total_price)\n\n\n\"\"\"\n函数说明:获取武器信息\n\nParameters:\n    url - GET请求地址，通过Fiddler抓包获取\n    header - headers信息\nReturns:\n    weapon_info_dict - 武器信息\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nModify:\n    2017-08-07\n\"\"\"\ndef hero_weapon(url, header):\n    req = requests.get(url = url, headers = header).json()\n    weapon_info_dict = req['list']\n    return weapon_info_dict\n\n\nif __name__ == '__main__':\n    headers = {'Accept-Charset': 'UTF-8',\n            'Accept-Encoding': 'gzip,deflate',\n            'User-Agent': 'Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI 5 MIUI/V8.1.6.0.MAACNDI)',\n            'X-Requested-With': 'XMLHttpRequest',\n            'Content-type': 'application/x-www-form-urlencoded',\n            'Connection': 'Keep-Alive',\n            'Host': 'gamehelper.gm825.com'}\n    weapon_url = \"http://gamehelper.gm825.com/wzry/equip/list?channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8\"\n    heros_url = \"http://gamehelper.gm825.com/wzry/hero/list?channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8\"\n    hero_list(heros_url, headers)\n    hero_id = input(\"请输入要查询的英雄ID:\")\n    hero_url = \"http://gamehelper.gm825.com/wzry/hero/detail?hero_id={}&channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8\".format(hero_id)\n    weapon_info_dict = hero_weapon(weapon_url, headers)\n    hero_info(hero_url, headers, weapon_info_dict)"
        },
        {
          "name": "one_hour_spider",
          "type": "tree",
          "content": null
        },
        {
          "name": "shuaia.py",
          "type": "blob",
          "size": 1.5458984375,
          "content": "# -*- coding:UTF-8 -*-\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlretrieve\nimport requests\nimport os\nimport time\n\nif __name__ == '__main__':\n\tlist_url = []\n\tfor num in range(1,3):\n\t\tif num == 1:\n\t\t\turl = 'http://www.shuaia.net/index.html'\n\t\telse:\n\t\t\turl = 'http://www.shuaia.net/index_%d.html' % num\n\t\theaders = {\n\t\t\t\t\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n\t\t}\n\t\treq = requests.get(url = url,headers = headers)\n\t\treq.encoding = 'utf-8'\n\t\thtml = req.text\n\t\tbf = BeautifulSoup(html, 'lxml')\n\t\ttargets_url = bf.find_all(class_='item-img')\n\t\t\n\t\tfor each in targets_url:\n\t\t\tlist_url.append(each.img.get('alt') + '=' + each.get('href'))\n\n\tprint('连接采集完成')\n\n\tfor each_img in list_url:\n\t\timg_info = each_img.split('=')\n\t\ttarget_url = img_info[1]\n\t\tfilename = img_info[0] + '.jpg'\n\t\tprint('下载：' + filename)\n\t\theaders = {\n\t\t\t\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n\t\t}\n\t\timg_req = requests.get(url = target_url,headers = headers)\n\t\timg_req.encoding = 'utf-8'\n\t\timg_html = img_req.text\n\t\timg_bf_1 = BeautifulSoup(img_html, 'lxml')\n\t\timg_url = img_bf_1.find_all('div', class_='wr-single-content-list')\n\t\timg_bf_2 = BeautifulSoup(str(img_url), 'lxml')\n\t\timg_url = 'http://www.shuaia.net' + img_bf_2.div.img.get('src')\n\t\tif 'images' not in os.listdir():\n\t\t\tos.makedirs('images')\n\t\turlretrieve(url = img_url,filename = 'images/' + filename)\n\t\ttime.sleep(1)\n\n\tprint('下载完成！')"
        },
        {
          "name": "video_downloader",
          "type": "tree",
          "content": null
        },
        {
          "name": "zhengfang_system_spider",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}