{
  "metadata": {
    "timestamp": 1736561188041,
    "page": 153,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/ChatGLM2-6B",
      "stars": 15748,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 0.923828125,
          "content": "## Q1\n\n**Macç›´æ¥åŠ è½½é‡åŒ–åçš„æ¨¡å‹å‡ºç°æç¤º `clang: error: unsupported option '-fopenmp'**\n\nè¿™æ˜¯ç”±äºMacç”±äºæœ¬èº«ç¼ºä¹ompå¯¼è‡´çš„ï¼Œæ­¤æ—¶å¯è¿è¡Œä½†æ˜¯å•æ ¸ã€‚éœ€è¦å•ç‹¬å®‰è£… openmp ä¾èµ–ï¼Œå³å¯åœ¨Macä¸‹ä½¿ç”¨OMPï¼š\n\n```bash\n# å‚è€ƒ`https://mac.r-project.org/openmp/`\n## å‡è®¾: gcc(clang)æ˜¯14.xç‰ˆæœ¬ï¼Œå…¶ä»–ç‰ˆæœ¬è§R-Projectæä¾›çš„è¡¨æ ¼\ncurl -O https://mac.r-project.org/openmp/openmp-14.0.6-darwin20-Release.tar.gz\nsudo tar fvxz openmp-14.0.6-darwin20-Release.tar.gz -C /\n```\næ­¤æ—¶ä¼šå®‰è£…ä¸‹é¢å‡ ä¸ªæ–‡ä»¶ï¼š`/usr/local/lib/libomp.dylib`, `/usr/local/include/ompt.h`, `/usr/local/include/omp.h`, `/usr/local/include/omp-tools.h`ã€‚\n\n> æ³¨æ„ï¼šå¦‚æœä½ ä¹‹å‰è¿è¡Œ`ChatGLM2-6B`é¡¹ç›®å¤±è´¥è¿‡ï¼Œæœ€å¥½æ¸…ä¸€ä¸‹Hugging Faceçš„ç¼“å­˜ï¼Œi.e. é»˜è®¤ä¸‹æ˜¯ `rm -rf ${HOME}/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4`ã€‚ç”±äºä½¿ç”¨äº†`rm`å‘½ä»¤ï¼Œè¯·æ˜ç¡®çŸ¥é“è‡ªå·±åœ¨åˆ é™¤ä»€ä¹ˆã€‚\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 4.0361328125,
          "content": "The ChatGLM2-6B License\n\n1. å®šä¹‰\n\nâ€œè®¸å¯æ–¹â€æ˜¯æŒ‡åˆ†å‘å…¶è½¯ä»¶çš„ ChatGLM2-6B æ¨¡å‹å›¢é˜Ÿã€‚\n\nâ€œè½¯ä»¶â€æ˜¯æŒ‡æ ¹æ®æœ¬è®¸å¯æä¾›çš„ ChatGLM2-6B æ¨¡å‹å‚æ•°ã€‚\n\n2. è®¸å¯æˆäºˆ\n\næ ¹æ®æœ¬è®¸å¯çš„æ¡æ¬¾å’Œæ¡ä»¶ï¼Œè®¸å¯æ–¹ç‰¹æ­¤æˆäºˆæ‚¨éæ’ä»–æ€§ã€å…¨çƒæ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯å†è®¸å¯ã€å¯æ’¤é”€ã€å…ç‰ˆç¨çš„ç‰ˆæƒè®¸å¯ã€‚\n\nä¸Šè¿°ç‰ˆæƒå£°æ˜å’Œæœ¬è®¸å¯å£°æ˜åº”åŒ…å«åœ¨æœ¬è½¯ä»¶çš„æ‰€æœ‰å‰¯æœ¬æˆ–é‡è¦éƒ¨åˆ†ä¸­ã€‚\n\n3.é™åˆ¶\n\næ‚¨ä¸å¾—å‡ºäºä»»ä½•å†›äº‹æˆ–éæ³•ç›®çš„ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹ã€åˆå¹¶ã€å‘å¸ƒã€åˆ†å‘ã€å¤åˆ¶æˆ–åˆ›å»ºæœ¬è½¯ä»¶çš„å…¨éƒ¨æˆ–éƒ¨åˆ†è¡ç”Ÿä½œå“ã€‚\n\næ‚¨ä¸å¾—åˆ©ç”¨æœ¬è½¯ä»¶ä»äº‹ä»»ä½•å±å®³å›½å®¶å®‰å…¨å’Œå›½å®¶ç»Ÿä¸€ã€å±å®³ç¤¾ä¼šå…¬å…±åˆ©ç›Šã€ä¾µçŠ¯äººèº«æƒç›Šçš„è¡Œä¸ºã€‚\n\n4.å…è´£å£°æ˜\n\næœ¬è½¯ä»¶â€œæŒ‰åŸæ ·â€æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¯¹é€‚é”€æ€§ã€ç‰¹å®šç”¨é€”çš„é€‚ç”¨æ€§å’Œéä¾µæƒæ€§çš„ä¿è¯ã€‚ åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œä½œè€…æˆ–ç‰ˆæƒæŒæœ‰äººå‡ä¸å¯¹ä»»ä½•ç´¢èµ”ã€æŸå®³æˆ–å…¶ä»–è´£ä»»è´Ÿè´£ï¼Œæ— è®ºæ˜¯åœ¨åˆåŒè¯‰è®¼ã€ä¾µæƒè¡Œä¸ºè¿˜æ˜¯å…¶ä»–æ–¹é¢ï¼Œç”±è½¯ä»¶æˆ–è½¯ä»¶çš„ä½¿ç”¨æˆ–å…¶ä»–äº¤æ˜“å¼•èµ·ã€ç”±è½¯ä»¶å¼•èµ·æˆ–ä¸ä¹‹ç›¸å…³ è½¯ä»¶ã€‚\n\n5. è´£ä»»é™åˆ¶\n\né™¤é€‚ç”¨æ³•å¾‹ç¦æ­¢çš„èŒƒå›´å¤–ï¼Œåœ¨ä»»ä½•æƒ…å†µä¸‹ä¸”æ ¹æ®ä»»ä½•æ³•å¾‹ç†è®ºï¼Œæ— è®ºæ˜¯åŸºäºä¾µæƒè¡Œä¸ºã€ç–å¿½ã€åˆåŒã€è´£ä»»æˆ–å…¶ä»–åŸå› ï¼Œä»»ä½•è®¸å¯æ–¹å‡ä¸å¯¹æ‚¨æ‰¿æ‹…ä»»ä½•ç›´æ¥ã€é—´æ¥ã€ç‰¹æ®Šã€å¶ç„¶ã€ç¤ºèŒƒæ€§ã€ æˆ–é—´æ¥æŸå®³ï¼Œæˆ–ä»»ä½•å…¶ä»–å•†ä¸šæŸå¤±ï¼Œå³ä½¿è®¸å¯äººå·²è¢«å‘ŠçŸ¥æ­¤ç±»æŸå®³çš„å¯èƒ½æ€§ã€‚\n\n6.äº‰è®®è§£å†³\n\næœ¬è®¸å¯å—ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹ç®¡è¾–å¹¶æŒ‰å…¶è§£é‡Šã€‚ å› æœ¬è®¸å¯å¼•èµ·çš„æˆ–ä¸æœ¬è®¸å¯æœ‰å…³çš„ä»»ä½•äº‰è®®åº”æäº¤åŒ—äº¬å¸‚æµ·æ·€åŒºäººæ°‘æ³•é™¢ã€‚\n\nè¯·æ³¨æ„ï¼Œè®¸å¯è¯å¯èƒ½ä¼šæ›´æ–°åˆ°æ›´å…¨é¢çš„ç‰ˆæœ¬ã€‚ æœ‰å…³è®¸å¯å’Œç‰ˆæƒçš„ä»»ä½•é—®é¢˜ï¼Œè¯·é€šè¿‡ license@zhipuai.cn ä¸æˆ‘ä»¬è”ç³»ã€‚\n\n1. Definitions\n\nâ€œLicensorâ€ means the ChatGLM2-6B Model Team that distributes its Software.\n\nâ€œSoftwareâ€ means the ChatGLM2-6B model parameters made available under this license.\n\n2. License Grant\n\nSubject to the terms and conditions of this License, the Licensor hereby grants to you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to use the Software.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of Peopleâ€™s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us at license@zhipuai.cn.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.5703125,
          "content": "# ChatGLM2-6B\n\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/THUDM/chatglm2-6b\" target=\"_blank\">HF Repo</a> â€¢ ğŸ¦ <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">[GLM-130B@ICLR 23]</a> <a href=\"https://github.com/THUDM/GLM-130B\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n<p align=\"center\">\n    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„  <a href=\"https://discord.gg/fK2dz4bg\" target=\"_blank\">Discord</a> å’Œ <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<p align=\"center\">\nğŸ“åœ¨ <a href=\"https://www.chatglm.cn\">chatglm.cn</a> ä½“éªŒæ›´å¤§è§„æ¨¡çš„ ChatGLM æ¨¡å‹ã€‚\n</p>\n\n\n*Read this in [English](README_EN.md)*\n\n## GLM-4 å¼€æºæ¨¡å‹å’ŒAPI\n\næˆ‘ä»¬å·²ç»å‘å¸ƒæœ€æ–°çš„ **GLM-4** æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæœ‰äº†æ–°çš„çªç ´ï¼Œæ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ¸ é“ä½“éªŒæˆ‘ä»¬çš„æœ€æ–°æ¨¡å‹ã€‚\n\n+ [GLM-4 å¼€æºæ¨¡å‹](https://github.com/THUDM/GLM-4) æˆ‘ä»¬å·²ç»å¼€æºäº† GLM-4-9B ç³»åˆ—æ¨¡å‹ï¼Œåœ¨å„é¡¹æŒ‡æ ‡çš„ceæ˜¯ä¸Šæœ‰æ˜æ˜¾æå‡ï¼Œæ¬¢è¿å°è¯•ã€‚\n+ [æ™ºè°±æ¸…è¨€](https://chatglm.cn/main/detail?fr=ecology_x) ä½“éªŒæœ€æ–°ç‰ˆ GLM-4ï¼ŒåŒ…æ‹¬ **GLMsï¼ŒAll tools**ç­‰åŠŸèƒ½ã€‚\n+ [APIå¹³å°](https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9) æ–°ä¸€ä»£ API å¹³å°å·²ç»ä¸Šçº¿ï¼Œæ‚¨å¯ä»¥ç›´æ¥åœ¨\n  API\n  å¹³å°ä¸Šä½“éªŒ `GLM-4-0520`ã€`GLM-4-air`ã€`GLM-4-airx`ã€`GLM-4-flash`ã€`GLM-4`ã€`GLM-3-Turbo`ã€`CharacterGLM-3`ï¼Œ`CogView-3`\n  ç­‰æ–°æ¨¡å‹ã€‚\n  å…¶ä¸­`GLM-4`ã€`GLM-3-Turbo`ä¸¤ä¸ªæ¨¡å‹æ”¯æŒäº† `System Prompt`ã€`Function Call`ã€ `Retrieval`ã€`Web_Search`ç­‰æ–°åŠŸèƒ½ï¼Œæ¬¢è¿ä½“éªŒã€‚\n\n+ [GLM-4 API å¼€æºæ•™ç¨‹](https://github.com/MetaGLM/glm-cookbook/) GLM-4 APIæ•™ç¨‹å’ŒåŸºç¡€åº”ç”¨ï¼Œæ¬¢è¿å°è¯•ã€‚\n  APIç›¸å…³é—®é¢˜å¯ä»¥åœ¨æœ¬å¼€æºæ•™ç¨‹ç–‘é—®ï¼Œæˆ–è€…ä½¿ç”¨ [GLM-4 API AIåŠ©æ‰‹](https://open.bigmodel.cn/shareapp/v1/?share_code=sQwt5qyqYVaNh1O_87p8O)\n  æ¥è·å¾—å¸¸è§é—®é¢˜çš„å¸®åŠ©ã€‚\n\n-----\n\n## ä»‹ç»\n\nChatGLM**2**-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM**2**-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§ï¼š\n\n1. **æ›´å¼ºå¤§çš„æ€§èƒ½**ï¼šåŸºäº ChatGLM åˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œæˆ‘ä»¬å…¨é¢å‡çº§äº† ChatGLM2-6B çš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2-6B ä½¿ç”¨äº† [GLM](https://github.com/THUDM/GLM) çš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº† 1.4T ä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œ[è¯„æµ‹ç»“æœ](#è¯„æµ‹ç»“æœ)æ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B åœ¨ MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ ã€BBHï¼ˆ+60%ï¼‰ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚\n2. **æ›´é•¿çš„ä¸Šä¸‹æ–‡**ï¼šåŸºäº [FlashAttention](https://github.com/HazyResearch/flash-attention) æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ç”± ChatGLM-6B çš„ 2K æ‰©å±•åˆ°äº† 32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨ 8K çš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒã€‚å¯¹äºæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬å‘å¸ƒäº† [ChatGLM2-6B-32K](https://huggingface.co/THUDM/chatglm2-6b-32k) æ¨¡å‹ã€‚[LongBench](https://github.com/THUDM/LongBench) çš„æµ‹è¯„ç»“æœè¡¨æ˜ï¼Œåœ¨ç­‰é‡çº§çš„å¼€æºæ¨¡å‹ä¸­ï¼ŒChatGLM2-6B-32K æœ‰ç€è¾ƒä¸ºæ˜æ˜¾çš„ç«äº‰ä¼˜åŠ¿ã€‚\n3. **æ›´é«˜æ•ˆçš„æ¨ç†**ï¼šåŸºäº [Multi-Query Attention](http://arxiv.org/abs/1911.02150) æŠ€æœ¯ï¼ŒChatGLM2-6B æœ‰æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ï¼šåœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº† 42%ï¼ŒINT4 é‡åŒ–ä¸‹ï¼Œ6G æ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”± 1K æå‡åˆ°äº† 8Kã€‚\n4. **æ›´å¼€æ”¾çš„åè®®**ï¼šChatGLM2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾**ï¼Œåœ¨å¡«å†™[é—®å·](https://open.bigmodel.cn/mla/form)è¿›è¡Œç™»è®°å**äº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨**ã€‚\n\n-----\n\nChatGLM2-6B å¼€æºæ¨¡å‹æ—¨åœ¨ä¸å¼€æºç¤¾åŒºä¸€èµ·æ¨åŠ¨å¤§æ¨¡å‹æŠ€æœ¯å‘å±•ï¼Œæ³è¯·å¼€å‘è€…å’Œå¤§å®¶éµå®ˆ[å¼€æºåè®®](MODEL_LICENSE)ï¼Œå‹¿å°†å¼€æºæ¨¡å‹å’Œä»£ç åŠåŸºäºå¼€æºé¡¹ç›®äº§ç”Ÿçš„è¡ç”Ÿç‰©ç”¨äºä»»ä½•å¯èƒ½ç»™å›½å®¶å’Œç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ä»¥åŠç”¨äºä»»ä½•æœªç»è¿‡å®‰å…¨è¯„ä¼°å’Œå¤‡æ¡ˆçš„æœåŠ¡ã€‚**ç›®å‰ï¼Œæœ¬é¡¹ç›®å›¢é˜ŸæœªåŸºäº ChatGLM2-6B å¼€å‘ä»»ä½•åº”ç”¨ï¼ŒåŒ…æ‹¬ç½‘é¡µç«¯ã€å®‰å“ã€è‹¹æœ iOS åŠ Windows App ç­‰åº”ç”¨ã€‚**\n\nå°½ç®¡æ¨¡å‹åœ¨è®­ç»ƒçš„å„ä¸ªé˜¶æ®µéƒ½å°½åŠ›ç¡®ä¿æ•°æ®çš„åˆè§„æ€§å’Œå‡†ç¡®æ€§ï¼Œä½†ç”±äº ChatGLM2-6B æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä¸”æ¨¡å‹å—æ¦‚ç‡éšæœºæ€§å› ç´ å½±å“ï¼Œæ— æ³•ä¿è¯è¾“å‡ºå†…å®¹çš„å‡†ç¡®æ€§ï¼Œä¸”æ¨¡å‹æ˜“è¢«è¯¯å¯¼ã€‚**æœ¬é¡¹ç›®ä¸æ‰¿æ‹…å¼€æºæ¨¡å‹å’Œä»£ç å¯¼è‡´çš„æ•°æ®å®‰å…¨ã€èˆ†æƒ…é£é™©æˆ–å‘ç”Ÿä»»ä½•æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­ã€ä¸å½“åˆ©ç”¨è€Œäº§ç”Ÿçš„é£é™©å’Œè´£ä»»ã€‚**\n\n## æ›´æ–°ä¿¡æ¯\n**[2023/07/31]** å‘å¸ƒ [ChatGLM2-6B-32K](https://huggingface.co/THUDM/chatglm2-6b-32k) æ¨¡å‹ï¼Œæå‡å¯¹äºé•¿æ–‡æœ¬çš„ç†è§£èƒ½åŠ›ã€‚\n\n**[2023/07/25]** å‘å¸ƒ [CodeGeeX2](https://github.com/THUDM/CodeGeeX2) æ¨¡å‹ï¼ŒåŸºäº ChatGLM2-6B åŠ å…¥ä»£ç é¢„è®­ç»ƒå®ç°ï¼Œä»£ç èƒ½åŠ›å…¨é¢æå‡ã€‚\n\n**[2023/07/04]** å‘å¸ƒ P-Tuning v2 ä¸ å…¨å‚æ•°å¾®è°ƒè„šæœ¬ï¼Œå‚è§ [P-Tuning](./ptuning)ã€‚\n\n## å‹æƒ…é“¾æ¥\nå¯¹ ChatGLM2 è¿›è¡ŒåŠ é€Ÿçš„å¼€æºé¡¹ç›®ï¼š\n* [fastllm](https://github.com/ztxz16/fastllm/): å…¨å¹³å°åŠ é€Ÿæ¨ç†æ–¹æ¡ˆï¼Œå•GPUæ‰¹é‡æ¨ç†æ¯ç§’å¯è¾¾10000+tokenï¼Œæ‰‹æœºç«¯æœ€ä½3Gå†…å­˜å®æ—¶è¿è¡Œï¼ˆéªé¾™865ä¸Šçº¦4~5 token/sï¼‰\n* [chatglm.cpp](https://github.com/li-plus/chatglm.cpp): ç±»ä¼¼ llama.cpp çš„ CPU é‡åŒ–åŠ é€Ÿæ¨ç†æ–¹æ¡ˆï¼Œå®ç° Mac ç¬”è®°æœ¬ä¸Šå®æ—¶å¯¹è¯\n* [ChatGLM2-TPU](https://github.com/sophgo/ChatGLM2-TPU): é‡‡ç”¨TPUåŠ é€Ÿæ¨ç†æ–¹æ¡ˆï¼Œåœ¨ç®—èƒ½ç«¯ä¾§èŠ¯ç‰‡BM1684Xï¼ˆ16T@FP16ï¼Œå†…å­˜16Gï¼‰ä¸Šå®æ—¶è¿è¡Œçº¦5 token/s\n\nåŸºäºæˆ–ä½¿ç”¨äº† ChatGLM2-6B çš„å¼€æºé¡¹ç›®ï¼š\n* [Chuanhu Chat](https://github.com/GaiZhenbiao/ChuanhuChatGPT): ä¸ºå„ä¸ªå¤§è¯­è¨€æ¨¡å‹å’Œåœ¨çº¿æ¨¡å‹APIæä¾›ç¾è§‚æ˜“ç”¨ã€åŠŸèƒ½ä¸°å¯Œã€å¿«é€Ÿéƒ¨ç½²çš„ç”¨æˆ·ç•Œé¢ï¼Œæ”¯æŒChatGLM2-6Bã€‚\n\næ”¯æŒ ChatGLM-6B å’Œç›¸å…³åº”ç”¨åœ¨çº¿è®­ç»ƒçš„ç¤ºä¾‹é¡¹ç›®ï¼š\n* [ChatGLM2-6B åœ¨è…¾è®¯äº‘éƒ¨ç½²æ•™ç¨‹](https://cloud.tencent.com/document/product/1721/104848)\n* [ChatGLM2-6B çš„éƒ¨ç½²ä¸å¾®è°ƒæ•™ç¨‹](https://www.heywhale.com/mw/project/64984a7b72ebe240516ae79c)\n\n## è¯„æµ‹ç»“æœ\næˆ‘ä»¬é€‰å–äº†éƒ¨åˆ†ä¸­è‹±æ–‡å…¸å‹æ•°æ®é›†è¿›è¡Œäº†è¯„æµ‹ï¼Œä»¥ä¸‹ä¸º ChatGLM2-6B æ¨¡å‹åœ¨ [MMLU](https://github.com/hendrycks/test) (è‹±æ–‡)ã€[C-Eval](https://cevalbenchmark.com/static/leaderboard.html)ï¼ˆä¸­æ–‡ï¼‰ã€[GSM8K](https://github.com/openai/grade-school-math)ï¼ˆæ•°å­¦ï¼‰ã€[BBH](https://github.com/suzgunmirac/BIG-Bench-Hard)ï¼ˆè‹±æ–‡ï¼‰ ä¸Šçš„æµ‹è¯„ç»“æœã€‚åœ¨ [evaluation](./evaluation/README.md) ä¸­æä¾›äº†åœ¨ C-Eval ä¸Šè¿›è¡Œæµ‹è¯„çš„è„šæœ¬ã€‚\n\n### MMLU\n\n| Model | Average | STEM | Social Sciences | Humanities | Others |\n| ----- |------| ---- |------|-------| ----- |\n| ChatGLM-6B | 40.63 | 33.89 | 44.84 | 39.02 | 45.71 |\n| ChatGLM2-6B (base) | 47.86 | 41.20 | 54.44 | 43.66 | 54.46 |\n| ChatGLM2-6B | 45.46 | 40.06 | 51.61 | 41.23 | 51.24 |\n| ChatGLM2-12B (base) | 56.18 | 48.18 | 65.13 | 52.58 | 60.93 |\n| ChatGLM2-12B | 52.13 | 47.00 | 61.00 | 46.10 | 56.05 |\n\n> Chat æ¨¡å‹ä½¿ç”¨ zero-shot CoT (Chain-of-Thought) çš„æ–¹æ³•æµ‹è¯•ï¼ŒBase æ¨¡å‹ä½¿ç”¨ few-shot answer-only çš„æ–¹æ³•æµ‹è¯•\n\n### C-Eval\n\n| Model | Average | STEM  | Social Sciences | Humanities | Others |\n| ----- |---------|-------| ----- |------------|--------|\n| ChatGLM-6B | 38.9    | 33.3  | 48.3 | 41.3       | 38.0   |\n| ChatGLM2-6B (base) | 51.7    | 48.6  | 60.5 | 51.3       | 49.8   |\n| ChatGLM2-6B | 50.1    | 46.4\t | 60.4 | 50.6       | 46.9   |\n| ChatGLM2-12B (base) | 61.6    | 55.4\t | 73.7 | 64.2       | 59.4   | \n| ChatGLM2-12B | 57.0    | 52.1\t | 69.3 | 58.5       | 53.2   | \n\n> Chat æ¨¡å‹ä½¿ç”¨ zero-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒBase æ¨¡å‹ä½¿ç”¨ few-shot answer only çš„æ–¹æ³•æµ‹è¯•\n\n### GSM8K\n\n| Model        | Accuracy | Accuracy (Chinese)* |\n|--------------|----------| - |\n| ChatGLM-6B   | 4.82     | 5.85 |\n| ChatGLM2-6B (base) | 32.37    | 28.95 |\n| ChatGLM2-6B  | 28.05    | 20.45 |\n| ChatGLM2-12B (base) | 40.94    | 42.71 |\n| ChatGLM2-12B | 38.13    | 23.43 |\n\n> æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ few-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒCoT prompt æ¥è‡ª http://arxiv.org/abs/2201.11903\n> \n> \\* æˆ‘ä»¬ä½¿ç”¨ç¿»è¯‘ API ç¿»è¯‘äº† GSM8K ä¸­çš„ 500 é“é¢˜ç›®å’Œ CoT prompt å¹¶è¿›è¡Œäº†äººå·¥æ ¡å¯¹\n\n\n### BBH\n\n| Model        | Accuracy |\n|--------------|-------|\n| ChatGLM-6B   | 18.73 |\n| ChatGLM2-6B (base) | 33.68 |\n| ChatGLM2-6B  | 30.00 |\n| ChatGLM2-12B (base) | 36.02 |\n| ChatGLM2-12B | 39.98 |\n\n> æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ few-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒCoT prompt æ¥è‡ª https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts\n\n## æ¨ç†æ€§èƒ½\nChatGLM2-6B ä½¿ç”¨äº† [Multi-Query Attention](http://arxiv.org/abs/1911.02150)ï¼Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚ç”Ÿæˆ 2000 ä¸ªå­—ç¬¦çš„å¹³å‡é€Ÿåº¦å¯¹æ¯”å¦‚ä¸‹\n\n| Model | æ¨ç†é€Ÿåº¦ (å­—ç¬¦/ç§’) |\n| ----  | -----  |\n| ChatGLM-6B  | 31.49 |\n| ChatGLM2-6B | 44.62 |\n\n> ä½¿ç”¨å®˜æ–¹å®ç°ï¼Œbatch size = 1ï¼Œmax length = 2048ï¼Œbf16 ç²¾åº¦ï¼Œæµ‹è¯•ç¡¬ä»¶ä¸º A100-SXM4-80Gï¼Œè½¯ä»¶ç¯å¢ƒä¸º PyTorch 2.0.1\n\nMulti-Query Attention åŒæ—¶ä¹Ÿé™ä½äº†ç”Ÿæˆè¿‡ç¨‹ä¸­ KV Cache çš„æ˜¾å­˜å ç”¨ï¼Œæ­¤å¤–ï¼ŒChatGLM2-6B é‡‡ç”¨ Causal Mask è¿›è¡Œå¯¹è¯è®­ç»ƒï¼Œè¿ç»­å¯¹è¯æ—¶å¯å¤ç”¨å‰é¢è½®æ¬¡çš„ KV Cacheï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†æ˜¾å­˜å ç”¨ã€‚å› æ­¤ï¼Œä½¿ç”¨ 6GB æ˜¾å­˜çš„æ˜¾å¡è¿›è¡Œ INT4 é‡åŒ–çš„æ¨ç†æ—¶ï¼Œåˆä»£çš„ ChatGLM-6B æ¨¡å‹æœ€å¤šèƒ½å¤Ÿç”Ÿæˆ 1119 ä¸ªå­—ç¬¦å°±ä¼šæç¤ºæ˜¾å­˜è€—å°½ï¼Œè€Œ ChatGLM2-6B èƒ½å¤Ÿç”Ÿæˆè‡³å°‘ 8192 ä¸ªå­—ç¬¦ã€‚\n\n| **é‡åŒ–ç­‰çº§** | **ç¼–ç  2048 é•¿åº¦çš„æœ€å°æ˜¾å­˜** | **ç”Ÿæˆ 8192 é•¿åº¦çš„æœ€å°æ˜¾å­˜** |\n| -------------- |---------------------|---------------------|\n| FP16 / BF16 | 13.1 GB             | 12.8 GB             | \n| INT8           | 8.2 GB              | 8.1 GB              |\n| INT4           | 5.5 GB              | 5.1 GB              |\n\n> ChatGLM2-6B åˆ©ç”¨äº† PyTorch 2.0 å¼•å…¥çš„ `torch.nn.functional.scaled_dot_product_attention` å®ç°é«˜æ•ˆçš„ Attention è®¡ç®—ï¼Œå¦‚æœ PyTorch ç‰ˆæœ¬è¾ƒä½åˆ™ä¼š fallback åˆ°æœ´ç´ çš„ Attention å®ç°ï¼Œå‡ºç°æ˜¾å­˜å ç”¨é«˜äºä¸Šè¡¨çš„æƒ…å†µã€‚\n\næˆ‘ä»¬ä¹Ÿæµ‹è¯•äº†é‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œé‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚\n\n| é‡åŒ–ç­‰çº§ | Accuracy (MMLU) | Accuracy (C-Eval dev) |\n| ----- | ----- |-----------------------|\n| BF16 | 45.47 | 53.57                 |\n| INT4 | 43.13 | 50.30                 |\n\n\n\n## ChatGLM2-6B ç¤ºä¾‹\n\nç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B å¤šä¸ªç»´åº¦çš„èƒ½åŠ›éƒ½å–å¾—äº†æå‡ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¯¹æ¯”ç¤ºä¾‹ã€‚æ›´å¤š ChatGLM2-6B çš„å¯èƒ½ï¼Œç­‰å¾…ä½ æ¥æ¢ç´¢å‘ç°ï¼\n\n<details><summary><b>æ•°ç†é€»è¾‘</b></summary>\n\n![](resources/math.png)\n\n</details>\n\n<details><summary><b>çŸ¥è¯†æ¨ç†</b></summary>\n\n![](resources/knowledge.png)\n\n</details>\n\n<details><summary><b>é•¿æ–‡æ¡£ç†è§£</b></summary>\n\n![](resources/long-context.png)\n\n</details>\n\n## ä½¿ç”¨æ–¹å¼\n### ç¯å¢ƒå®‰è£…\né¦–å…ˆéœ€è¦ä¸‹è½½æœ¬ä»“åº“ï¼š\n```shell\ngit clone https://github.com/THUDM/ChatGLM2-6B\ncd ChatGLM2-6B\n```\n\nç„¶åä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š\n```\npip install -r requirements.txt\n```\nå…¶ä¸­ `transformers` åº“ç‰ˆæœ¬æ¨èä¸º `4.30.2`ï¼Œ`torch` æ¨èä½¿ç”¨ 2.0 åŠä»¥ä¸Šçš„ç‰ˆæœ¬ï¼Œä»¥è·å¾—æœ€ä½³çš„æ¨ç†æ€§èƒ½ã€‚\n\n### ä»£ç è°ƒç”¨ \n\nå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM2-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š\n\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device='cuda')\n>>> model = model.eval()\n>>> response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n>>> print(response)\nä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n>>> response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n>>> print(response)\næ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:\n\n1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚\n2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚\n3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚\n4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚\n5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚\n6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚\n\nå¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚\n```\n\n#### ä»æœ¬åœ°åŠ è½½æ¨¡å‹\nä»¥ä¸Šä»£ç ä¼šç”± `transformers` è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å®ç°å’Œå‚æ•°ã€‚å®Œæ•´çš„æ¨¡å‹å®ç°åœ¨ [Hugging Face Hub](https://huggingface.co/THUDM/chatglm2-6b)ã€‚å¦‚æœä½ çš„ç½‘ç»œç¯å¢ƒè¾ƒå·®ï¼Œä¸‹è½½æ¨¡å‹å‚æ•°å¯èƒ½ä¼šèŠ±è´¹è¾ƒé•¿æ—¶é—´ç”šè‡³å¤±è´¥ã€‚æ­¤æ—¶å¯ä»¥å…ˆå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶åä»æœ¬åœ°åŠ è½½ã€‚\n\nä» Hugging Face Hub ä¸‹è½½æ¨¡å‹éœ€è¦å…ˆ[å®‰è£…Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)ï¼Œç„¶åè¿è¡Œ\n```Shell\ngit clone https://huggingface.co/THUDM/chatglm2-6b\n```\n\nå¦‚æœä½ ä» Hugging Face Hub ä¸Šä¸‹è½½ checkpoint çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯ä»¥åªä¸‹è½½æ¨¡å‹å®ç°\n```Shell\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm2-6b\n```\nç„¶åä»[è¿™é‡Œ](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/)æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å‚æ•°æ–‡ä»¶ï¼Œå¹¶å°†ä¸‹è½½çš„æ–‡ä»¶æ›¿æ¢åˆ°æœ¬åœ°çš„ `chatglm2-6b` ç›®å½•ä¸‹ã€‚\n\n\nå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ä¹‹åï¼Œå°†ä»¥ä¸Šä»£ç ä¸­çš„ `THUDM/chatglm2-6b` æ›¿æ¢ä¸ºä½ æœ¬åœ°çš„ `chatglm2-6b` æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œå³å¯ä»æœ¬åœ°åŠ è½½æ¨¡å‹ã€‚\n\næ¨¡å‹çš„å®ç°ä»ç„¶å¤„åœ¨å˜åŠ¨ä¸­ã€‚å¦‚æœå¸Œæœ›å›ºå®šä½¿ç”¨çš„æ¨¡å‹å®ç°ä»¥ä¿è¯å…¼å®¹æ€§ï¼Œå¯ä»¥åœ¨ `from_pretrained` çš„è°ƒç”¨ä¸­å¢åŠ  `revision=\"v1.0\"` å‚æ•°ã€‚`v1.0` æ˜¯å½“å‰æœ€æ–°çš„ç‰ˆæœ¬å·ï¼Œå®Œæ•´çš„ç‰ˆæœ¬åˆ—è¡¨å‚è§ [Change Log](https://huggingface.co/THUDM/chatglm2-6b#change-log)ã€‚\n\n### ç½‘é¡µç‰ˆ Demo\n![web-demo](resources/web-demo.gif)\nå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨åŸºäº Gradio çš„ç½‘é¡µç‰ˆ demoï¼š\n```shell\npython web_demo.py\n```\n\n![web-demo](resources/web-demo2.gif)\n\nå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨åŸºäº Streamlit çš„ç½‘é¡µç‰ˆ demoï¼š\n```shell\nstreamlit run web_demo2.py\n```\n\nç½‘é¡µç‰ˆ demo ä¼šè¿è¡Œä¸€ä¸ª Web Serverï¼Œå¹¶è¾“å‡ºåœ°å€ã€‚åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡ºçš„åœ°å€å³å¯ä½¿ç”¨ã€‚ ç»æµ‹è¯•ï¼ŒåŸºäº Streamlit çš„ç½‘é¡µç‰ˆ Demo ä¼šæ›´æµç•…ã€‚\n\n### å‘½ä»¤è¡Œ Demo\n\n![cli-demo](resources/cli-demo.png)\n\nè¿è¡Œä»“åº“ä¸­ [cli_demo.py](cli_demo.py)ï¼š\n\n```shell\npython cli_demo.py\n```\n\nç¨‹åºä¼šåœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ `clear` å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥ `stop` ç»ˆæ­¢ç¨‹åºã€‚\n\n### API éƒ¨ç½²\né¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ– `pip install fastapi uvicorn`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ [api.py](api.py)ï¼š\n```shell\npython api.py\n```\né»˜è®¤éƒ¨ç½²åœ¨æœ¬åœ°çš„ 8000 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨\n```shell\ncurl -X POST \"http://127.0.0.1:8000\" \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"ä½ å¥½\", \"history\": []}'\n```\nå¾—åˆ°çš„è¿”å›å€¼ä¸º\n```shell\n{\n  \"response\":\"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\",\n  \"history\":[[\"ä½ å¥½\",\"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\"]],\n  \"status\":200,\n  \"time\":\"2023-03-23 21:38:40\"\n}\n```\næ„Ÿè°¢ [@hiyouga]() å®ç°äº† OpenAI æ ¼å¼çš„æµå¼ API éƒ¨ç½²ï¼Œå¯ä»¥ä½œä¸ºä»»æ„åŸºäº ChatGPT çš„åº”ç”¨çš„åç«¯ï¼Œæ¯”å¦‚ [ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web)ã€‚å¯ä»¥é€šè¿‡è¿è¡Œä»“åº“ä¸­çš„[openai_api.py](openai_api.py) è¿›è¡Œéƒ¨ç½²ï¼š\n```shell\npython openai_api.py\n```\nè¿›è¡Œ API è°ƒç”¨çš„ç¤ºä¾‹ä»£ç ä¸º\n```python\nimport openai\nif __name__ == \"__main__\":\n    openai.api_base = \"http://localhost:8000/v1\"\n    openai.api_key = \"none\"\n    for chunk in openai.ChatCompletion.create(\n        model=\"chatglm2-6b\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n        ],\n        stream=True\n    ):\n        if hasattr(chunk.choices[0].delta, \"content\"):\n            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n\n## ä½æˆæœ¬éƒ¨ç½²\n\n### æ¨¡å‹é‡åŒ–\n\né»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 13GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b-int4\",trust_remote_code=True).cuda()\n```\n\næ¨¡å‹é‡åŒ–ä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½æŸå¤±ï¼Œç»è¿‡æµ‹è¯•ï¼ŒChatGLM2-6B åœ¨ 4-bit é‡åŒ–ä¸‹ä»ç„¶èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶æµç•…çš„ç”Ÿæˆã€‚ é‡åŒ–æ¨¡å‹çš„å‚æ•°æ–‡ä»¶ä¹Ÿå¯ä»¥ä»[è¿™é‡Œ](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/)æ‰‹åŠ¨ä¸‹è½½ã€‚\n\n### CPU éƒ¨ç½²\n\nå¦‚æœä½ æ²¡æœ‰ GPU ç¡¬ä»¶çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½†æ˜¯æ¨ç†é€Ÿåº¦ä¼šæ›´æ…¢ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼ˆéœ€è¦å¤§æ¦‚ 32GB å†…å­˜ï¼‰\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).float()\n```\nå¦‚æœä½ çš„å†…å­˜ä¸è¶³çš„è¯ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b-int4\",trust_remote_code=True).float()\n```\nåœ¨ cpu ä¸Šè¿è¡Œé‡åŒ–åçš„æ¨¡å‹éœ€è¦å®‰è£… `gcc` ä¸ `openmp`ã€‚å¤šæ•° Linux å‘è¡Œç‰ˆé»˜è®¤å·²å®‰è£…ã€‚å¯¹äº Windows ï¼Œå¯åœ¨å®‰è£… [TDM-GCC](https://jmeubank.github.io/tdm-gcc/) æ—¶å‹¾é€‰ `openmp`ã€‚ Windows æµ‹è¯•ç¯å¢ƒ `gcc` ç‰ˆæœ¬ä¸º `TDM-GCC 10.3.0`ï¼Œ Linux ä¸º `gcc 11.3.0`ã€‚åœ¨ MacOS ä¸Šè¯·å‚è€ƒ [Q1](FAQ.md#q1)ã€‚\n\n### Mac éƒ¨ç½²\n\nå¯¹äºæ­è½½äº† Apple Silicon æˆ–è€… AMD GPU çš„ Macï¼Œå¯ä»¥ä½¿ç”¨ MPS åç«¯æ¥åœ¨ GPU ä¸Šè¿è¡Œ ChatGLM2-6Bã€‚éœ€è¦å‚è€ƒ Apple çš„ [å®˜æ–¹è¯´æ˜](https://developer.apple.com/metal/pytorch) å®‰è£… PyTorch-Nightlyï¼ˆæ­£ç¡®çš„ç‰ˆæœ¬å·åº”è¯¥æ˜¯2.x.x.dev2023xxxxï¼Œè€Œä¸æ˜¯ 2.x.xï¼‰ã€‚\n\nç›®å‰åœ¨ MacOS ä¸Šåªæ”¯æŒ[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](README.md#ä»æœ¬åœ°åŠ è½½æ¨¡å‹)ã€‚å°†ä»£ç ä¸­çš„æ¨¡å‹åŠ è½½æ”¹ä¸ºä»æœ¬åœ°åŠ è½½ï¼Œå¹¶ä½¿ç”¨ mps åç«¯ï¼š\n```python\nmodel = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).to('mps')\n```\n\nåŠ è½½åŠç²¾åº¦çš„ ChatGLM2-6B æ¨¡å‹éœ€è¦å¤§æ¦‚ 13GB å†…å­˜ã€‚å†…å­˜è¾ƒå°çš„æœºå™¨ï¼ˆæ¯”å¦‚ 16GB å†…å­˜çš„ MacBook Proï¼‰ï¼Œåœ¨ç©ºä½™å†…å­˜ä¸è¶³çš„æƒ…å†µä¸‹ä¼šä½¿ç”¨ç¡¬ç›˜ä¸Šçš„è™šæ‹Ÿå†…å­˜ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ä¸¥é‡å˜æ…¢ã€‚\næ­¤æ—¶å¯ä»¥ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹ chatglm2-6b-int4ã€‚å› ä¸º GPU ä¸Šé‡åŒ–çš„ kernel æ˜¯ä½¿ç”¨ CUDA ç¼–å†™çš„ï¼Œå› æ­¤æ— æ³•åœ¨ MacOS ä¸Šä½¿ç”¨ï¼Œåªèƒ½ä½¿ç”¨ CPU è¿›è¡Œæ¨ç†ã€‚\nä¸ºäº†å……åˆ†ä½¿ç”¨ CPU å¹¶è¡Œï¼Œè¿˜éœ€è¦[å•ç‹¬å®‰è£… OpenMP](FAQ.md#q1)ã€‚\n\nåœ¨ Mac ä¸Šè¿›è¡Œæ¨ç†ä¹Ÿå¯ä»¥ä½¿ç”¨ [ChatGLM.cpp](https://github.com/li-plus/chatglm.cpp)\n\n### å¤šå¡éƒ¨ç½²\nå¦‚æœä½ æœ‰å¤šå¼  GPUï¼Œä½†æ˜¯æ¯å¼  GPU çš„æ˜¾å­˜å¤§å°éƒ½ä¸è¶³ä»¥å®¹çº³å®Œæ•´çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå¯ä»¥å°†æ¨¡å‹åˆ‡åˆ†åœ¨å¤šå¼ GPUä¸Šã€‚é¦–å…ˆå®‰è£… accelerate: `pip install accelerate`ï¼Œç„¶åé€šè¿‡å¦‚ä¸‹æ–¹æ³•åŠ è½½æ¨¡å‹ï¼š\n```python\nfrom utils import load_model_on_gpus\nmodel = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\n```\nå³å¯å°†æ¨¡å‹éƒ¨ç½²åˆ°ä¸¤å¼  GPU ä¸Šè¿›è¡Œæ¨ç†ã€‚ä½ å¯ä»¥å°† `num_gpus` æ”¹ä¸ºä½ å¸Œæœ›ä½¿ç”¨çš„ GPU æ•°ã€‚é»˜è®¤æ˜¯å‡åŒ€åˆ‡åˆ†çš„ï¼Œä½ ä¹Ÿå¯ä»¥ä¼ å…¥ `device_map` å‚æ•°æ¥è‡ªå·±æŒ‡å®šã€‚ \n\n## åè®®\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) åè®®å¼€æºï¼ŒChatGLM2-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE)ã€‚ChatGLM2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾**ï¼Œåœ¨å¡«å†™[é—®å·](https://open.bigmodel.cn/mla/form)è¿›è¡Œç™»è®°å**äº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨**ã€‚\n\n\n## å¼•ç”¨\n\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ï¼ŒChatGLM2-6B çš„è®ºæ–‡ä¼šåœ¨è¿‘æœŸå…¬å¸ƒï¼Œæ•¬è¯·æœŸå¾…ï½\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 17.8798828125,
          "content": "# ChatGLM2-6B\n\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/THUDM/chatglm2-6b\" target=\"_blank\">HF Repo</a> â€¢ ğŸ¦ <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> â€¢ ğŸ“„<a href=\"https://arxiv.org/pdf/2406.12793\" target=\"_blank\"> Report </a> <br>\n</p>\n<p align=\"center\">\n    ğŸ‘‹ Join our  <a href=\"https://discord.gg/fK2dz4bg\" target=\"_blank\">Discord</a> and <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n\n## GLM-4 Open Source Model and API\n\nWe have released the latest **GLM-4** model, which has made new breakthroughs in multiple indicators. You can directly\nexperience our latest model in the following two channels.\n\n+ [GLM-4 open source model](https://github.com/THUDM/GLM-4) We have open sourced the GLM-4-9B series models, which have\n  significantly improved the performance of various indicators. Welcome to try.\n+ [Zhipu Qingyan](https://chatglm.cn/main/detail?fr=ecology_x) Experience the latest version of GLM-4, including **GLMs,\n  All tools** and other functions.\n+ [API platform](https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9) The new generation of API\n  platform has been launched. You can directly experience new models such\n  as `GLM-4-0520`, `GLM-4-air`, `GLM-4-airx`, `GLM-4-flash`, `GLM-4`, `GLM-3-Turbo`, `CharacterGLM-3`, `CogView-3` on\n  the API platform.\n  Among them, the two models `GLM-4` and `GLM-3-Turbo` support new functions such\n  as `System Prompt`, `Function Call`, `Retrieval`, and `Web_Search`. You are welcome to experience them.\n\n+ [GLM-4 API open source tutorial](https://github.com/MetaGLM/glm-cookbook/) GLM-4 API tutorial and basic applications,\n  welcome to try.\n  API-related questions can be asked in this open source tutorial, or\n  use [GLM-4 API AI Assistant](https://open.bigmodel.cn/shareapp/v1/?share_code=sQwt5qyqYVaNh1O_87p8O)\n  to get help with common problems.\n\n\n## Introduction\n\nChatGLM**2**-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B). It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:\n\n1. **Stronger Performance**: Based on the development experience of the first-generation ChatGLM model, we have fully upgraded the base model of ChatGLM2-6B. ChatGLM2-6B uses the hybrid objective function of [GLM](https://github.com/THUDM/GLM), and has undergone pre-training with 1.4T bilingual tokens and human preference alignment training. The [evaluation results](README.md#evaluation-results) show that, compared to the first-generation model, ChatGLM2-6B has achieved substantial improvements in performance on datasets like MMLU (+23%), CEval (+33%), GSM8K (+571%), BBH (+60%), showing strong competitiveness among models of the same size.\n2. **Longer Context**: Based on [FlashAttention](https://github.com/HazyResearch/flash-attention) technique, we have extended the context length of the base model from 2K in ChatGLM-6B to 32K, and trained with a context length of 8K during the dialogue alignment, allowing for more rounds of dialogue. However, the current version of ChatGLM2-6B has limited understanding of single-round ultra-long documents, which we will focus on optimizing in future iterations.\n3. **More Efficient Inference**: Based on [Multi-Query Attention](http://arxiv.org/abs/1911.02150) technique, ChatGLM2-6B has more efficient inference speed and lower GPU memory usage: under the official  implementation, the inference speed has increased by 42% compared to the first generation; under INT4 quantization, the dialogue length supported by 6G GPU memory has increased from 1K to 8K.\n4. **More Open License**: ChatGLM2-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form).\n\nWelcome to use the larger ChatGLM model on [chatglm.cn](https://chatglm.cn) \n\n-----\n\nThe open-source ChatGLM2-6B is intended to promote the development of LLMs together with the open-source community. We earnestly request developers and everyone to abide by the [open-source license](MODEL_LICENSE). Do not use the open-source model, code, or any derivatives from the open-source project for any purposes that may harm nations or societies, or for any services that have not undergone safety assessments and legal approval. **At present, our project team has not developed any applications based on ChatGLM2-6B, including web, Android, Apple iOS, and Windows App applications.**\n\nAlthough the model strives to ensure the compliance and accuracy of data at each stage of training, due to the smaller scale of the ChatGLM2-6B model, and its susceptibility to probabilistic randomness, the accuracy of output content cannot be guaranteed, and the model can easily be misled. **Our project does not assume any risks or responsibilities arising from data security, public opinion risks, or any instances of the model being misled, abused, disseminated, or improperly used due to the open-source model and code.**\n\n## Projects\nOpen source projects that accelerate ChatGLM2:\n\n* [fastllm](https://github.com/ztxz16/fastllm/): Universal platform acceleration inference solution, single GPU batch inference can reach 10,000+ tokens per second, and it can run in real-time on mobile devices with a minimum of 3GB of memory (about 4~5 tokens/s on Snapdragon 865).\n* [chatglm.cpp](https://github.com/li-plus/chatglm.cpp): Real-time CPU inference on a MacBook accelerated by quantization, similar to llama.cpp.\n* [ChatGLM2-TPU](https://github.com/sophgo/ChatGLM2-TPU): Using the TPU accelerated inference solution, it runs about 5 token/s in real time on the end-side chip BM1684X (16T@FP16, 16G DDR).\n\nExample projects supporting online training of ChatGLM-6B and related applications:\n* [ChatGLM-6B deployment and fine-tuning tutorial](https://www.heywhale.com/mw/project/64984a7b72ebe240516ae79c)\n\n## Evaluation\nWe selected some typical Chinese and English datasets for evaluation. Below are the evaluation results of the ChatGLM2-6B model on [MMLU](https://github.com/hendrycks/test) (English), [C-Eval](https://cevalbenchmark.com/static/leaderboard.html) (Chinese), [GSM8K](https://github.com/openai/grade-school-math) (Mathematics), [BBH](https://github.com/suzgunmirac/BIG-Bench-Hard) (English).\n\n### MMLU\n\n| Model | Average | STEM | Social Sciences | Humanities | Others |\n| ----- | ----- | ---- | ----- | ----- | ----- |\n| ChatGLM-6B | 40.63 | 33.89 | 44.84 | 39.02 | 45.71 |\n| ChatGLM2-6B (base) | 47.86 | 41.20 | 54.44 | 43.66 | 54.46 |\n| ChatGLM2-6B | 45.46 | 40.06 | 51.61 | 41.23 | 51.24 |\n\n> Chat-aligned version is evaluated under zero-shot CoT (Chain-of-Thought), and Base version is evaluated under few-shot answer-only\n\n### C-Eval\n\n| Model | Average | STEM | Social Sciences | Humanities | Others |\n| ----- | ---- | ---- | ----- | ----- | ----- |\n| ChatGLM-6B | 38.9 | 33.3 | 48.3 | 41.3 | 38.0 |\n| ChatGLM2-6B (base) | 51.7 | 48.6 | 60.5 | 51.3 | 49.8 |\n| ChatGLM2-6B | 50.1 | 46.4\t| 60.4 | 50.6 | 46.9 | \n\n> Chat-aligned version is evaluated under zero-shot CoT (Chain-of-Thought), and Base version is evaluated under few-shot answer-only\n\n### GSM8K\n\n| Model | Accuracy | Accuracy (Chinese)* |\n| ----- | ----- | ----- |\n| ChatGLM-6B | 4.82 | 5.85 |\n| ChatGLM2-6B (base) | 32.37 | 28.95 |\n| ChatGLM2-6B | 28.05 | 20.45 |\n\n> All model versions are evaluated under few-shot CoT, and CoT prompts are from http://arxiv.org/abs/2201.11903\n> \\* We translate a 500-query subset of GSM8K and its corresponding CoT prompts using machine translation API and subsequent human proofreading.\n\n\n### BBH\n\n| Model | Accuracy |\n| ----- | ----- |\n| ChatGLM-6B | 18.73 |\n| ChatGLM2-6B (base) | 33.68 |\n| ChatGLM2-6B | 30.00 |\n\n> All model versions are evaluated under few-shot CoT, and CoT prompts are from https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts\n\n## Inference Efficiency\nChatGLM2-6B employs [Multi-Query Attention](http://arxiv.org/abs/1911.02150) to improve inference speed. Here is a comparison of the average speed for generating 2000 tokens.\n\n\n| Model | Inference Speed (tokens/s) |\n| ----  | -----  |\n| ChatGLM-6B  | 31.49 |\n| ChatGLM2-6B | 44.62 |\n\n> Under our official implementation, batch size = 1, max length = 2048, bf16 precision, tested with an A100-SXM-80G and PyTorch 2.0 environment\n\nMulti-Query Attention also reduces the GPU memory usage of the KV Cache during inference. Additionally, ChatGLM2-6B uses Causal Mask for dialogue training, which allows the reuse of the KV Cache from previous rounds in continuous dialogues, further optimizing GPU memory usage. Therefore, when performing INT4 quantization inference with a 6GB GPU, while the first-generation ChatGLM-6B can only generate a maximum of 1119 tokens before running out of memory, ChatGLM2-6B can generate at least 8192 tokens.\n\n| **Quantization** | **Encoding 2048 Tokens** | **Decoding 8192 Tokens** |\n| -------------- | --------------------- | --------------- |\n| FP16 / BF16 | 13.1 GB             | 12.8 GB             | \n| INT8           | 8.2 GB              | 8.1 GB              |\n| INT4           | 5.5 GB              | 5.1 GB              |\n\n> ChatGLM2-6B takes advantage of `torch.nn.functional.scaled_dot_product_attention` introduced in PyTorch 2.0 for efficient Attention computation. If the PyTorch version is lower, it will fallback to the naive Attention implementation, which may result in higher GPU memory usage than shown in the table above.\n\nWe also tested the impact of quantization on model performance. The results show that the impact of quantization on model performance is within an acceptable range.\n\n| Quantization | Accuracy (MMLU) | Accuracy (C-Eval dev) |\n| ----- | ----- |-----------------------|\n| BF16 | 45.47 | 53.57                 |\n| INT4 | 43.13 | 50.30                 |\n\n\n## ChatGLM2-6B Examples\n\nCompared to the first-generation model, ChatGLM2-6B has made improvements in multiple dimensions. Below are some comparison examples. More possibilities with ChatGLM2-6B are waiting for you to explore and discover!\n\n<details><summary><b>Mathematics and Logic</b></summary>\n\n![](examples/math.png)\n\n</details>\n\n<details><summary><b>Knowledge Reasoning</b></summary>\n\n![](examples/knowledge.png)\n\n</details>\n\n<details><summary><b>Long Document Understanding</b></summary>\n\n![](examples/long-context.png)\n\n</details>\n\n## Getting Started\n### Environment Setup\n\nInstall dependencies with pip: `pip install -r requirements.txt`. It's recommended to use version `4.27.1` for the `transformers` library and use version 2.0 or higher for `torch` to achieve the best inference performance.\n\nWe provide a web page demo and a command line demo. You need to download this repository to use them:\n\n```shell\ngit clone https://github.com/THUDM/ChatGLM2-6B\ncd ChatGLM2-6B\n```\n\n### Usage\n\nGenerate dialogue with the following code:\n\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device='cuda').eval()\n>>> response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n>>> print(response)\nä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n>>> response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n>>> print(response)\næ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:\n\n1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚\n2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚\n3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚\n4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚\n5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚\n6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚\n\nå¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚\n```\nThe implementation of the model is still in development. If you want to fix the used model implementation to ensure compatibility, you can add the `revision=\"v1.0\"` parameter in the `from_pretrained` call. `v1.0` is the latest version number. For a complete list of versions, see [Change Log](https://huggingface.co/THUDM/chatglm2-6b#change-log).\n\n### Web Demo\n\n![web-demo](resources/web-demo.gif)\n\nInstall Gradio `pip install gradio`ï¼Œand run [web_demo.py](web_demo.py):\n\n```shell\npython web_demo.py\n```\n\nThe program runs a web server and outputs the URL. Open the URL in the browser to use the web demo.\n\n#### CLI Demo\n\n![cli-demo](resources/cli-demo.png)\n\nRun [cli_demo.py](cli_demo.py) in the repo:\n\n```shell\npython cli_demo.py\n```\n\nThe command runs an interactive program in the shell. Type your instruction in the shell and hit enter to generate the response. Type `clear` to clear the dialogue history and `stop` to terminate the program.\n\n## API Deployment\nFirst install the additional dependency `pip install fastapi uvicorn`. The run [api.py](api.py) in the repo.\n```shell\npython api.py\n```\nBy default the api runs at the`8000`port of the local machine. You can call the API via \n```shell\ncurl -X POST \"http://127.0.0.1:8000\" \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"ä½ å¥½\", \"history\": []}'\n```\nThe returned value is\n```shell\n{\n  \"response\":\"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\",\n  \"history\":[[\"ä½ å¥½\",\"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\"]],\n  \"status\":200,\n  \"time\":\"2023-03-23 21:38:40\"\n}\n```\n## Deployment\n\n### Quantization\n\nBy default, the model parameters are loaded with FP16 precision, which require about 13GB of GPU memory. It your GPU memory is limited, you can try to load the model parameters with quantization:\n\n```python\n# hange according to your hardware. Only support 4/8 bit quantization now.\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).quantize(8).cuda()\n```\n\nModel quantization will bring some performance loss on datasets. But after testing, ChatGLM2-6B can still perform natural and smooth generation under 4-bit quantization.\n\n### CPU Deployment\n\nIf your computer is not equipped with GPU, you can also conduct inference on CPU, but the inference speed is slow (and taking about 32GB of memory):\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).float()\n```\n\n### Inference on Mac\n\nFor Macs (and MacBooks) with Apple Silicon, it is possible to use the MPS backend to run ChatGLM-6B on the GPU. First, you need to refer to Apple's [official instructions](https://developer.apple.com/metal/pytorch) to install PyTorch-Nightly. (The correct version number should be 2.1.0.dev2023xxxx, not 2.0.0).\n\nCurrently you must [load the model locally](README_en.md#load-the-model-locally) on MacOS. Change the code to load the model from your local path, and use the mps backend:\n```python\nmodel = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).to('mps')\n```\n\nLoading a FP16 ChatGLM-6B model requires about 13GB of memory. Machines with less memory (such as a MacBook Pro with 16GB of memory) will use the virtual memory on the hard disk when there is insufficient free memory, resulting in a serious slowdown in inference speed.\n\n## License\n\nThe code of this repository is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0). The use of the ChatGLM2-6B model weights is subject to the [Model License](MODEL_LICENSE). ChatGLM2-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form).\n\n## Citation\n\nIf you find our work useful, please consider citing the following papers. The technical report for ChatGLM2-6B will be out soon.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 2.0947265625,
          "content": "from fastapi import FastAPI, Request\nfrom transformers import AutoTokenizer, AutoModel\nimport uvicorn, json, datetime\nimport torch\n\nDEVICE = \"cuda\"\nDEVICE_ID = \"0\"\nCUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n\n\ndef torch_gc():\n    if torch.cuda.is_available():\n        with torch.cuda.device(CUDA_DEVICE):\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n\n\napp = FastAPI()\n\n\n@app.post(\"/\")\nasync def create_item(request: Request):\n    global model, tokenizer\n    json_post_raw = await request.json()\n    json_post = json.dumps(json_post_raw)\n    json_post_list = json.loads(json_post)\n    prompt = json_post_list.get('prompt')\n    history = json_post_list.get('history')\n    max_length = json_post_list.get('max_length')\n    top_p = json_post_list.get('top_p')\n    temperature = json_post_list.get('temperature')\n    response, history = model.chat(tokenizer,\n                                   prompt,\n                                   history=history,\n                                   max_length=max_length if max_length else 2048,\n                                   top_p=top_p if top_p else 0.7,\n                                   temperature=temperature if temperature else 0.95)\n    now = datetime.datetime.now()\n    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    answer = {\n        \"response\": response,\n        \"history\": history,\n        \"status\": 200,\n        \"time\": time\n    }\n    log = \"[\" + time + \"] \" + '\", prompt:\"' + prompt + '\", response:\"' + repr(response) + '\"'\n    print(log)\n    torch_gc()\n    return answer\n\n\nif __name__ == '__main__':\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n    model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n    # å¤šæ˜¾å¡æ”¯æŒï¼Œä½¿ç”¨ä¸‹é¢ä¸‰è¡Œä»£æ›¿ä¸Šé¢ä¸¤è¡Œï¼Œå°†num_gpusæ”¹ä¸ºä½ å®é™…çš„æ˜¾å¡æ•°é‡\n    # model_path = \"THUDM/chatglm2-6b\"\n    # tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    # model = load_model_on_gpus(model_path, num_gpus=2)\n    model.eval()\n    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)\n"
        },
        {
          "name": "cli_demo.py",
          "type": "blob",
          "size": 2.173828125,
          "content": "import os\nimport platform\nimport signal\nfrom transformers import AutoTokenizer, AutoModel\nimport readline\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n# å¤šæ˜¾å¡æ”¯æŒï¼Œä½¿ç”¨ä¸‹é¢ä¸¤è¡Œä»£æ›¿ä¸Šé¢ä¸€è¡Œï¼Œå°†num_gpusæ”¹ä¸ºä½ å®é™…çš„æ˜¾å¡æ•°é‡\n# from utils import load_model_on_gpus\n# model = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\nmodel = model.eval()\n\nos_name = platform.system()\nclear_command = 'cls' if os_name == 'Windows' else 'clear'\nstop_stream = False\n\n\ndef build_prompt(history):\n    prompt = \"æ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\"\n    for query, response in history:\n        prompt += f\"\\n\\nç”¨æˆ·ï¼š{query}\"\n        prompt += f\"\\n\\nChatGLM2-6Bï¼š{response}\"\n    return prompt\n\n\ndef signal_handler(signal, frame):\n    global stop_stream\n    stop_stream = True\n\n\ndef main():\n    past_key_values, history = None, []\n    global stop_stream\n    print(\"æ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\")\n    while True:\n        query = input(\"\\nç”¨æˆ·ï¼š\")\n        if query.strip() == \"stop\":\n            break\n        if query.strip() == \"clear\":\n            past_key_values, history = None, []\n            os.system(clear_command)\n            print(\"æ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\")\n            continue\n        print(\"\\nChatGLMï¼š\", end=\"\")\n        current_length = 0\n        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history,\n                                                                    past_key_values=past_key_values,\n                                                                    return_past_key_values=True):\n            if stop_stream:\n                stop_stream = False\n                break\n            else:\n                print(response[current_length:], end=\"\", flush=True)\n                current_length = len(response)\n        print(\"\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_api.py",
          "type": "blob",
          "size": 5.5654296875,
          "content": "# coding=utf-8\n# Implements API for ChatGLM2-6B in OpenAI's format. (https://platform.openai.com/docs/api-reference/chat)\n# Usage: python openai_api.py\n# Visit http://localhost:8000/docs for documents.\n\n\nimport time\nimport torch\nimport uvicorn\nfrom pydantic import BaseModel, Field\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom contextlib import asynccontextmanager\nfrom typing import Any, Dict, List, Literal, Optional, Union\nfrom transformers import AutoTokenizer, AutoModel\nfrom sse_starlette.sse import ServerSentEvent, EventSourceResponse\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI): # collects GPU memory\n    yield\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n\napp = FastAPI(lifespan=lifespan)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"owner\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass ChatMessage(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    content: str\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal[\"user\", \"assistant\", \"system\"]] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessage]\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    max_length: Optional[int] = None\n    stream: Optional[bool] = False\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Literal[\"stop\", \"length\"]\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]]\n\n\nclass ChatCompletionResponse(BaseModel):\n    model: str\n    object: Literal[\"chat.completion\", \"chat.completion.chunk\"]\n    choices: List[Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]]\n    created: Optional[int] = Field(default_factory=lambda: int(time.time()))\n\n\n@app.get(\"/v1/models\", response_model=ModelList)\nasync def list_models():\n    global model_args\n    model_card = ModelCard(id=\"gpt-3.5-turbo\")\n    return ModelList(data=[model_card])\n\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    global model, tokenizer\n\n    if request.messages[-1].role != \"user\":\n        raise HTTPException(status_code=400, detail=\"Invalid request\")\n    query = request.messages[-1].content\n\n    prev_messages = request.messages[:-1]\n    if len(prev_messages) > 0 and prev_messages[0].role == \"system\":\n        query = prev_messages.pop(0).content + query\n\n    history = []\n    if len(prev_messages) % 2 == 0:\n        for i in range(0, len(prev_messages), 2):\n            if prev_messages[i].role == \"user\" and prev_messages[i+1].role == \"assistant\":\n                history.append([prev_messages[i].content, prev_messages[i+1].content])\n\n    if request.stream:\n        generate = predict(query, history, request.model)\n        return EventSourceResponse(generate, media_type=\"text/event-stream\")\n\n    response, _ = model.chat(tokenizer, query, history=history)\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=ChatMessage(role=\"assistant\", content=response),\n        finish_reason=\"stop\"\n    )\n\n    return ChatCompletionResponse(model=request.model, choices=[choice_data], object=\"chat.completion\")\n\n\nasync def predict(query: str, history: List[List[str]], model_id: str):\n    global model, tokenizer\n\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0,\n        delta=DeltaMessage(role=\"assistant\"),\n        finish_reason=None\n    )\n    chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n    yield \"{}\".format(chunk.json(exclude_unset=True, ensure_ascii=False))\n\n    current_length = 0\n\n    for new_response, _ in model.stream_chat(tokenizer, query, history):\n        if len(new_response) == current_length:\n            continue\n\n        new_text = new_response[current_length:]\n        current_length = len(new_response)\n\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0,\n            delta=DeltaMessage(content=new_text),\n            finish_reason=None\n        )\n        chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n        yield \"{}\".format(chunk.json(exclude_unset=True, ensure_ascii=False))\n\n\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0,\n        delta=DeltaMessage(),\n        finish_reason=\"stop\"\n    )\n    chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n    yield \"{}\".format(chunk.json(exclude_unset=True, ensure_ascii=False))\n    yield '[DONE]'\n\n\n\nif __name__ == \"__main__\":\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n    model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n    # å¤šæ˜¾å¡æ”¯æŒï¼Œä½¿ç”¨ä¸‹é¢ä¸¤è¡Œä»£æ›¿ä¸Šé¢ä¸€è¡Œï¼Œå°†num_gpusæ”¹ä¸ºä½ å®é™…çš„æ˜¾å¡æ•°é‡\n    # from utils import load_model_on_gpus\n    # model = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\n    model.eval()\n\n    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)\n"
        },
        {
          "name": "ptuning",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1240234375,
          "content": "protobuf\ntransformers==4.30.2\ncpm_kernels\ntorch>=2.0\ngradio\nmdtex2html\nsentencepiece\naccelerate\nsse-starlette\nstreamlit>=1.24.0"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 2.2197265625,
          "content": "import os\nfrom typing import Dict, Tuple, Union, Optional\n\nfrom torch.nn import Module\nfrom transformers import AutoModel\n\n\ndef auto_configure_device_map(num_gpus: int) -> Dict[str, int]:\n    # transformer.word_embeddings å ç”¨1å±‚\n    # transformer.final_layernorm å’Œ lm_head å ç”¨1å±‚\n    # transformer.layers å ç”¨ 28 å±‚\n    # æ€»å…±30å±‚åˆ†é…åˆ°num_gpuså¼ å¡ä¸Š\n    num_trans_layers = 28\n    per_gpu_layers = 30 / num_gpus\n\n    # bugfix: åœ¨linuxä¸­è°ƒç”¨torch.embeddingä¼ å…¥çš„weight,inputä¸åœ¨åŒä¸€deviceä¸Š,å¯¼è‡´RuntimeError\n    # windowsä¸‹ model.device ä¼šè¢«è®¾ç½®æˆ transformer.word_embeddings.device\n    # linuxä¸‹ model.device ä¼šè¢«è®¾ç½®æˆ lm_head.device\n    # åœ¨è°ƒç”¨chatæˆ–è€…stream_chatæ—¶,input_idsä¼šè¢«æ”¾åˆ°model.deviceä¸Š\n    # å¦‚æœtransformer.word_embeddings.deviceå’Œmodel.deviceä¸åŒ,åˆ™ä¼šå¯¼è‡´RuntimeError\n    # å› æ­¤è¿™é‡Œå°†transformer.word_embeddings,transformer.final_layernorm,lm_headéƒ½æ”¾åˆ°ç¬¬ä¸€å¼ å¡ä¸Š\n    # æœ¬æ–‡ä»¶æ¥æºäºhttps://github.com/THUDM/ChatGLM-6B/blob/main/utils.py\n    # ä»…æ­¤å¤„åšå°‘è®¸ä¿®æ”¹ä»¥æ”¯æŒChatGLM2\n    device_map = {\n        'transformer.embedding.word_embeddings': 0,\n        'transformer.encoder.final_layernorm': 0,\n        'transformer.output_layer': 0,\n        'transformer.rotary_pos_emb': 0,\n        'lm_head': 0\n    }\n\n    used = 2\n    gpu_target = 0\n    for i in range(num_trans_layers):\n        if used >= per_gpu_layers:\n            gpu_target += 1\n            used = 0\n        assert gpu_target < num_gpus\n        device_map[f'transformer.encoder.layers.{i}'] = gpu_target\n        used += 1\n\n    return device_map\n\n\ndef load_model_on_gpus(checkpoint_path: Union[str, os.PathLike], num_gpus: int = 2,\n                       device_map: Optional[Dict[str, int]] = None, **kwargs) -> Module:\n    if num_gpus < 2 and device_map is None:\n        model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True, **kwargs).half().cuda()\n    else:\n        from accelerate import dispatch_model\n\n        model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True, **kwargs).half()\n\n        if device_map is None:\n            device_map = auto_configure_device_map(num_gpus)\n\n        model = dispatch_model(model, device_map=device_map)\n\n    return model\n"
        },
        {
          "name": "web_demo.py",
          "type": "blob",
          "size": 4.1220703125,
          "content": "from transformers import AutoModel, AutoTokenizer\nimport gradio as gr\nimport mdtex2html\nfrom utils import load_model_on_gpus\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n# å¤šæ˜¾å¡æ”¯æŒï¼Œä½¿ç”¨ä¸‹é¢ä¸¤è¡Œä»£æ›¿ä¸Šé¢ä¸€è¡Œï¼Œå°†num_gpusæ”¹ä¸ºä½ å®é™…çš„æ˜¾å¡æ•°é‡\n# from utils import load_model_on_gpus\n# model = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\nmodel = model.eval()\n\n\"\"\"Override Chatbot.postprocess\"\"\"\n\n\ndef postprocess(self, y):\n    if y is None:\n        return []\n    for i, (message, response) in enumerate(y):\n        y[i] = (\n            None if message is None else mdtex2html.convert((message)),\n            None if response is None else mdtex2html.convert(response),\n        )\n    return y\n\n\ngr.Chatbot.postprocess = postprocess\n\n\ndef parse_text(text):\n    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n    lines = text.split(\"\\n\")\n    lines = [line for line in lines if line != \"\"]\n    count = 0\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            count += 1\n            items = line.split('`')\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = f'<br></code></pre>'\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace(\"`\", \"\\`\")\n                    line = line.replace(\"<\", \"&lt;\")\n                    line = line.replace(\">\", \"&gt;\")\n                    line = line.replace(\" \", \"&nbsp;\")\n                    line = line.replace(\"*\", \"&ast;\")\n                    line = line.replace(\"_\", \"&lowbar;\")\n                    line = line.replace(\"-\", \"&#45;\")\n                    line = line.replace(\".\", \"&#46;\")\n                    line = line.replace(\"!\", \"&#33;\")\n                    line = line.replace(\"(\", \"&#40;\")\n                    line = line.replace(\")\", \"&#41;\")\n                    line = line.replace(\"$\", \"&#36;\")\n                lines[i] = \"<br>\"+line\n    text = \"\".join(lines)\n    return text\n\n\ndef predict(input, chatbot, max_length, top_p, temperature, history, past_key_values):\n    chatbot.append((parse_text(input), \"\"))\n    for response, history, past_key_values in model.stream_chat(tokenizer, input, history, past_key_values=past_key_values,\n                                                                return_past_key_values=True,\n                                                                max_length=max_length, top_p=top_p,\n                                                                temperature=temperature):\n        chatbot[-1] = (parse_text(input), parse_text(response))\n\n        yield chatbot, history, past_key_values\n\n\ndef reset_user_input():\n    return gr.update(value='')\n\n\ndef reset_state():\n    return [], [], None\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\"\"\"<h1 align=\"center\">ChatGLM2-6B</h1>\"\"\")\n\n    chatbot = gr.Chatbot()\n    with gr.Row():\n        with gr.Column(scale=4):\n            with gr.Column(scale=12):\n                user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=10).style(\n                    container=False)\n            with gr.Column(min_width=32, scale=1):\n                submitBtn = gr.Button(\"Submit\", variant=\"primary\")\n        with gr.Column(scale=1):\n            emptyBtn = gr.Button(\"Clear History\")\n            max_length = gr.Slider(0, 32768, value=8192, step=1.0, label=\"Maximum length\", interactive=True)\n            top_p = gr.Slider(0, 1, value=0.8, step=0.01, label=\"Top P\", interactive=True)\n            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)\n\n    history = gr.State([])\n    past_key_values = gr.State(None)\n\n    submitBtn.click(predict, [user_input, chatbot, max_length, top_p, temperature, history, past_key_values],\n                    [chatbot, history, past_key_values], show_progress=True)\n    submitBtn.click(reset_user_input, [], [user_input])\n\n    emptyBtn.click(reset_state, outputs=[chatbot, history, past_key_values], show_progress=True)\n\ndemo.queue().launch(share=False, inbrowser=True)\n"
        },
        {
          "name": "web_demo2.py",
          "type": "blob",
          "size": 2.44921875,
          "content": "from transformers import AutoModel, AutoTokenizer\nimport streamlit as st\n\n\nst.set_page_config(\n    page_title=\"ChatGLM2-6b æ¼”ç¤º\",\n    page_icon=\":robot:\",\n    layout='wide'\n)\n\n\n@st.cache_resource\ndef get_model():\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n    model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n    # å¤šæ˜¾å¡æ”¯æŒï¼Œä½¿ç”¨ä¸‹é¢ä¸¤è¡Œä»£æ›¿ä¸Šé¢ä¸€è¡Œï¼Œå°†num_gpusæ”¹ä¸ºä½ å®é™…çš„æ˜¾å¡æ•°é‡\n    # from utils import load_model_on_gpus\n    # model = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\n    model = model.eval()\n    return tokenizer, model\n\n\ntokenizer, model = get_model()\n\nst.title(\"ChatGLM2-6B\")\n\nmax_length = st.sidebar.slider(\n    'max_length', 0, 32768, 8192, step=1\n)\ntop_p = st.sidebar.slider(\n    'top_p', 0.0, 1.0, 0.8, step=0.01\n)\ntemperature = st.sidebar.slider(\n    'temperature', 0.0, 1.0, 0.8, step=0.01\n)\n\nif 'history' not in st.session_state:\n    st.session_state.history = []\n\nif 'past_key_values' not in st.session_state:\n    st.session_state.past_key_values = None\n\nfor i, (query, response) in enumerate(st.session_state.history):\n    with st.chat_message(name=\"user\", avatar=\"user\"):\n        st.markdown(query)\n    with st.chat_message(name=\"assistant\", avatar=\"assistant\"):\n        st.markdown(response)\nwith st.chat_message(name=\"user\", avatar=\"user\"):\n    input_placeholder = st.empty()\nwith st.chat_message(name=\"assistant\", avatar=\"assistant\"):\n    message_placeholder = st.empty()\n\nprompt_text = st.text_area(label=\"ç”¨æˆ·å‘½ä»¤è¾“å…¥\",\n                           height=100,\n                           placeholder=\"è¯·åœ¨è¿™å„¿è¾“å…¥æ‚¨çš„å‘½ä»¤\")\n\nbutton = st.button(\"å‘é€\", key=\"predict\")\n\nif button:\n    input_placeholder.markdown(prompt_text)\n    history, past_key_values = st.session_state.history, st.session_state.past_key_values\n    for response, history, past_key_values in model.stream_chat(tokenizer, prompt_text, history,\n                                                                past_key_values=past_key_values,\n                                                                max_length=max_length, top_p=top_p,\n                                                                temperature=temperature,\n                                                                return_past_key_values=True):\n        message_placeholder.markdown(response)\n\n    st.session_state.history = history\n    st.session_state.past_key_values = past_key_values\n"
        }
      ]
    }
  ]
}