{
  "metadata": {
    "timestamp": 1736561188041,
    "page": 153,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/ChatGLM2-6B",
      "stars": 15748,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 0.923828125,
          "content": "## Q1\n\n**Mac直接加载量化后的模型出现提示 `clang: error: unsupported option '-fopenmp'**\n\n这是由于Mac由于本身缺乏omp导致的，此时可运行但是单核。需要单独安装 openmp 依赖，即可在Mac下使用OMP：\n\n```bash\n# 参考`https://mac.r-project.org/openmp/`\n## 假设: gcc(clang)是14.x版本，其他版本见R-Project提供的表格\ncurl -O https://mac.r-project.org/openmp/openmp-14.0.6-darwin20-Release.tar.gz\nsudo tar fvxz openmp-14.0.6-darwin20-Release.tar.gz -C /\n```\n此时会安装下面几个文件：`/usr/local/lib/libomp.dylib`, `/usr/local/include/ompt.h`, `/usr/local/include/omp.h`, `/usr/local/include/omp-tools.h`。\n\n> 注意：如果你之前运行`ChatGLM2-6B`项目失败过，最好清一下Hugging Face的缓存，i.e. 默认下是 `rm -rf ${HOME}/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4`。由于使用了`rm`命令，请明确知道自己在删除什么。\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 4.0361328125,
          "content": "The ChatGLM2-6B License\n\n1. 定义\n\n“许可方”是指分发其软件的 ChatGLM2-6B 模型团队。\n\n“软件”是指根据本许可提供的 ChatGLM2-6B 模型参数。\n\n2. 许可授予\n\n根据本许可的条款和条件，许可方特此授予您非排他性、全球性、不可转让、不可再许可、可撤销、免版税的版权许可。\n\n上述版权声明和本许可声明应包含在本软件的所有副本或重要部分中。\n\n3.限制\n\n您不得出于任何军事或非法目的使用、复制、修改、合并、发布、分发、复制或创建本软件的全部或部分衍生作品。\n\n您不得利用本软件从事任何危害国家安全和国家统一、危害社会公共利益、侵犯人身权益的行为。\n\n4.免责声明\n\n本软件“按原样”提供，不提供任何明示或暗示的保证，包括但不限于对适销性、特定用途的适用性和非侵权性的保证。 在任何情况下，作者或版权持有人均不对任何索赔、损害或其他责任负责，无论是在合同诉讼、侵权行为还是其他方面，由软件或软件的使用或其他交易引起、由软件引起或与之相关 软件。\n\n5. 责任限制\n\n除适用法律禁止的范围外，在任何情况下且根据任何法律理论，无论是基于侵权行为、疏忽、合同、责任或其他原因，任何许可方均不对您承担任何直接、间接、特殊、偶然、示范性、 或间接损害，或任何其他商业损失，即使许可人已被告知此类损害的可能性。\n\n6.争议解决\n\n本许可受中华人民共和国法律管辖并按其解释。 因本许可引起的或与本许可有关的任何争议应提交北京市海淀区人民法院。\n\n请注意，许可证可能会更新到更全面的版本。 有关许可和版权的任何问题，请通过 license@zhipuai.cn 与我们联系。\n\n1. Definitions\n\n“Licensor” means the ChatGLM2-6B Model Team that distributes its Software.\n\n“Software” means the ChatGLM2-6B model parameters made available under this license.\n\n2. License Grant\n\nSubject to the terms and conditions of this License, the Licensor hereby grants to you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to use the Software.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of People’s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us at license@zhipuai.cn.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.5703125,
          "content": "# ChatGLM2-6B\n\n<p align=\"center\">\n🤗 <a href=\"https://huggingface.co/THUDM/chatglm2-6b\" target=\"_blank\">HF Repo</a> • 🐦 <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> • 📃 <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> • 📃 <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">[GLM-130B@ICLR 23]</a> <a href=\"https://github.com/THUDM/GLM-130B\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n<p align=\"center\">\n    👋 加入我们的  <a href=\"https://discord.gg/fK2dz4bg\" target=\"_blank\">Discord</a> 和 <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<p align=\"center\">\n📍在 <a href=\"https://www.chatglm.cn\">chatglm.cn</a> 体验更大规模的 ChatGLM 模型。\n</p>\n\n\n*Read this in [English](README_EN.md)*\n\n## GLM-4 开源模型和API\n\n我们已经发布最新的 **GLM-4** 模型，该模型在多个指标上有了新的突破，您可以在以下两个渠道体验我们的最新模型。\n\n+ [GLM-4 开源模型](https://github.com/THUDM/GLM-4) 我们已经开源了 GLM-4-9B 系列模型，在各项指标的ce是上有明显提升，欢迎尝试。\n+ [智谱清言](https://chatglm.cn/main/detail?fr=ecology_x) 体验最新版 GLM-4，包括 **GLMs，All tools**等功能。\n+ [API平台](https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9) 新一代 API 平台已经上线，您可以直接在\n  API\n  平台上体验 `GLM-4-0520`、`GLM-4-air`、`GLM-4-airx`、`GLM-4-flash`、`GLM-4`、`GLM-3-Turbo`、`CharacterGLM-3`，`CogView-3`\n  等新模型。\n  其中`GLM-4`、`GLM-3-Turbo`两个模型支持了 `System Prompt`、`Function Call`、 `Retrieval`、`Web_Search`等新功能，欢迎体验。\n\n+ [GLM-4 API 开源教程](https://github.com/MetaGLM/glm-cookbook/) GLM-4 API教程和基础应用，欢迎尝试。\n  API相关问题可以在本开源教程疑问，或者使用 [GLM-4 API AI助手](https://open.bigmodel.cn/shareapp/v1/?share_code=sQwt5qyqYVaNh1O_87p8O)\n  来获得常见问题的帮助。\n\n-----\n\n## 介绍\n\nChatGLM**2**-6B 是开源中英双语对话模型 [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM**2**-6B 引入了如下新特性：\n\n1. **更强大的性能**：基于 ChatGLM 初代模型的开发经验，我们全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B 使用了 [GLM](https://github.com/THUDM/GLM) 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，[评测结果](#评测结果)显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。\n2. **更长的上下文**：基于 [FlashAttention](https://github.com/HazyResearch/flash-attention) 技术，我们将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练。对于更长的上下文，我们发布了 [ChatGLM2-6B-32K](https://huggingface.co/THUDM/chatglm2-6b-32k) 模型。[LongBench](https://github.com/THUDM/LongBench) 的测评结果表明，在等量级的开源模型中，ChatGLM2-6B-32K 有着较为明显的竞争优势。\n3. **更高效的推理**：基于 [Multi-Query Attention](http://arxiv.org/abs/1911.02150) 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。\n4. **更开放的协议**：ChatGLM2-6B 权重对学术研究**完全开放**，在填写[问卷](https://open.bigmodel.cn/mla/form)进行登记后**亦允许免费商业使用**。\n\n-----\n\nChatGLM2-6B 开源模型旨在与开源社区一起推动大模型技术发展，恳请开发者和大家遵守[开源协议](MODEL_LICENSE)，勿将开源模型和代码及基于开源项目产生的衍生物用于任何可能给国家和社会带来危害的用途以及用于任何未经过安全评估和备案的服务。**目前，本项目团队未基于 ChatGLM2-6B 开发任何应用，包括网页端、安卓、苹果 iOS 及 Windows App 等应用。**\n\n尽管模型在训练的各个阶段都尽力确保数据的合规性和准确性，但由于 ChatGLM2-6B 模型规模较小，且模型受概率随机性因素影响，无法保证输出内容的准确性，且模型易被误导。**本项目不承担开源模型和代码导致的数据安全、舆情风险或发生任何模型被误导、滥用、传播、不当利用而产生的风险和责任。**\n\n## 更新信息\n**[2023/07/31]** 发布 [ChatGLM2-6B-32K](https://huggingface.co/THUDM/chatglm2-6b-32k) 模型，提升对于长文本的理解能力。\n\n**[2023/07/25]** 发布 [CodeGeeX2](https://github.com/THUDM/CodeGeeX2) 模型，基于 ChatGLM2-6B 加入代码预训练实现，代码能力全面提升。\n\n**[2023/07/04]** 发布 P-Tuning v2 与 全参数微调脚本，参见 [P-Tuning](./ptuning)。\n\n## 友情链接\n对 ChatGLM2 进行加速的开源项目：\n* [fastllm](https://github.com/ztxz16/fastllm/): 全平台加速推理方案，单GPU批量推理每秒可达10000+token，手机端最低3G内存实时运行（骁龙865上约4~5 token/s）\n* [chatglm.cpp](https://github.com/li-plus/chatglm.cpp): 类似 llama.cpp 的 CPU 量化加速推理方案，实现 Mac 笔记本上实时对话\n* [ChatGLM2-TPU](https://github.com/sophgo/ChatGLM2-TPU): 采用TPU加速推理方案，在算能端侧芯片BM1684X（16T@FP16，内存16G）上实时运行约5 token/s\n\n基于或使用了 ChatGLM2-6B 的开源项目：\n* [Chuanhu Chat](https://github.com/GaiZhenbiao/ChuanhuChatGPT): 为各个大语言模型和在线模型API提供美观易用、功能丰富、快速部署的用户界面，支持ChatGLM2-6B。\n\n支持 ChatGLM-6B 和相关应用在线训练的示例项目：\n* [ChatGLM2-6B 在腾讯云部署教程](https://cloud.tencent.com/document/product/1721/104848)\n* [ChatGLM2-6B 的部署与微调教程](https://www.heywhale.com/mw/project/64984a7b72ebe240516ae79c)\n\n## 评测结果\n我们选取了部分中英文典型数据集进行了评测，以下为 ChatGLM2-6B 模型在 [MMLU](https://github.com/hendrycks/test) (英文)、[C-Eval](https://cevalbenchmark.com/static/leaderboard.html)（中文）、[GSM8K](https://github.com/openai/grade-school-math)（数学）、[BBH](https://github.com/suzgunmirac/BIG-Bench-Hard)（英文） 上的测评结果。在 [evaluation](./evaluation/README.md) 中提供了在 C-Eval 上进行测评的脚本。\n\n### MMLU\n\n| Model | Average | STEM | Social Sciences | Humanities | Others |\n| ----- |------| ---- |------|-------| ----- |\n| ChatGLM-6B | 40.63 | 33.89 | 44.84 | 39.02 | 45.71 |\n| ChatGLM2-6B (base) | 47.86 | 41.20 | 54.44 | 43.66 | 54.46 |\n| ChatGLM2-6B | 45.46 | 40.06 | 51.61 | 41.23 | 51.24 |\n| ChatGLM2-12B (base) | 56.18 | 48.18 | 65.13 | 52.58 | 60.93 |\n| ChatGLM2-12B | 52.13 | 47.00 | 61.00 | 46.10 | 56.05 |\n\n> Chat 模型使用 zero-shot CoT (Chain-of-Thought) 的方法测试，Base 模型使用 few-shot answer-only 的方法测试\n\n### C-Eval\n\n| Model | Average | STEM  | Social Sciences | Humanities | Others |\n| ----- |---------|-------| ----- |------------|--------|\n| ChatGLM-6B | 38.9    | 33.3  | 48.3 | 41.3       | 38.0   |\n| ChatGLM2-6B (base) | 51.7    | 48.6  | 60.5 | 51.3       | 49.8   |\n| ChatGLM2-6B | 50.1    | 46.4\t | 60.4 | 50.6       | 46.9   |\n| ChatGLM2-12B (base) | 61.6    | 55.4\t | 73.7 | 64.2       | 59.4   | \n| ChatGLM2-12B | 57.0    | 52.1\t | 69.3 | 58.5       | 53.2   | \n\n> Chat 模型使用 zero-shot CoT 的方法测试，Base 模型使用 few-shot answer only 的方法测试\n\n### GSM8K\n\n| Model        | Accuracy | Accuracy (Chinese)* |\n|--------------|----------| - |\n| ChatGLM-6B   | 4.82     | 5.85 |\n| ChatGLM2-6B (base) | 32.37    | 28.95 |\n| ChatGLM2-6B  | 28.05    | 20.45 |\n| ChatGLM2-12B (base) | 40.94    | 42.71 |\n| ChatGLM2-12B | 38.13    | 23.43 |\n\n> 所有模型均使用 few-shot CoT 的方法测试，CoT prompt 来自 http://arxiv.org/abs/2201.11903\n> \n> \\* 我们使用翻译 API 翻译了 GSM8K 中的 500 道题目和 CoT prompt 并进行了人工校对\n\n\n### BBH\n\n| Model        | Accuracy |\n|--------------|-------|\n| ChatGLM-6B   | 18.73 |\n| ChatGLM2-6B (base) | 33.68 |\n| ChatGLM2-6B  | 30.00 |\n| ChatGLM2-12B (base) | 36.02 |\n| ChatGLM2-12B | 39.98 |\n\n> 所有模型均使用 few-shot CoT 的方法测试，CoT prompt 来自 https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts\n\n## 推理性能\nChatGLM2-6B 使用了 [Multi-Query Attention](http://arxiv.org/abs/1911.02150)，提高了生成速度。生成 2000 个字符的平均速度对比如下\n\n| Model | 推理速度 (字符/秒) |\n| ----  | -----  |\n| ChatGLM-6B  | 31.49 |\n| ChatGLM2-6B | 44.62 |\n\n> 使用官方实现，batch size = 1，max length = 2048，bf16 精度，测试硬件为 A100-SXM4-80G，软件环境为 PyTorch 2.0.1\n\nMulti-Query Attention 同时也降低了生成过程中 KV Cache 的显存占用，此外，ChatGLM2-6B 采用 Causal Mask 进行对话训练，连续对话时可复用前面轮次的 KV Cache，进一步优化了显存占用。因此，使用 6GB 显存的显卡进行 INT4 量化的推理时，初代的 ChatGLM-6B 模型最多能够生成 1119 个字符就会提示显存耗尽，而 ChatGLM2-6B 能够生成至少 8192 个字符。\n\n| **量化等级** | **编码 2048 长度的最小显存** | **生成 8192 长度的最小显存** |\n| -------------- |---------------------|---------------------|\n| FP16 / BF16 | 13.1 GB             | 12.8 GB             | \n| INT8           | 8.2 GB              | 8.1 GB              |\n| INT4           | 5.5 GB              | 5.1 GB              |\n\n> ChatGLM2-6B 利用了 PyTorch 2.0 引入的 `torch.nn.functional.scaled_dot_product_attention` 实现高效的 Attention 计算，如果 PyTorch 版本较低则会 fallback 到朴素的 Attention 实现，出现显存占用高于上表的情况。\n\n我们也测试了量化对模型性能的影响。结果表明，量化对模型性能的影响在可接受范围内。\n\n| 量化等级 | Accuracy (MMLU) | Accuracy (C-Eval dev) |\n| ----- | ----- |-----------------------|\n| BF16 | 45.47 | 53.57                 |\n| INT4 | 43.13 | 50.30                 |\n\n\n\n## ChatGLM2-6B 示例\n\n相比于初代模型，ChatGLM2-6B 多个维度的能力都取得了提升，以下是一些对比示例。更多 ChatGLM2-6B 的可能，等待你来探索发现！\n\n<details><summary><b>数理逻辑</b></summary>\n\n![](resources/math.png)\n\n</details>\n\n<details><summary><b>知识推理</b></summary>\n\n![](resources/knowledge.png)\n\n</details>\n\n<details><summary><b>长文档理解</b></summary>\n\n![](resources/long-context.png)\n\n</details>\n\n## 使用方式\n### 环境安装\n首先需要下载本仓库：\n```shell\ngit clone https://github.com/THUDM/ChatGLM2-6B\ncd ChatGLM2-6B\n```\n\n然后使用 pip 安装依赖：\n```\npip install -r requirements.txt\n```\n其中 `transformers` 库版本推荐为 `4.30.2`，`torch` 推荐使用 2.0 及以上的版本，以获得最佳的推理性能。\n\n### 代码调用 \n\n可以通过如下代码调用 ChatGLM2-6B 模型来生成对话：\n\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device='cuda')\n>>> model = model.eval()\n>>> response, history = model.chat(tokenizer, \"你好\", history=[])\n>>> print(response)\n你好👋!我是人工智能助手 ChatGLM2-6B,很高兴见到你,欢迎问我任何问题。\n>>> response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n>>> print(response)\n晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:\n\n1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。\n2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。\n3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。\n4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。\n5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。\n6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。\n\n如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。\n```\n\n#### 从本地加载模型\n以上代码会由 `transformers` 自动下载模型实现和参数。完整的模型实现在 [Hugging Face Hub](https://huggingface.co/THUDM/chatglm2-6b)。如果你的网络环境较差，下载模型参数可能会花费较长时间甚至失败。此时可以先将模型下载到本地，然后从本地加载。\n\n从 Hugging Face Hub 下载模型需要先[安装Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)，然后运行\n```Shell\ngit clone https://huggingface.co/THUDM/chatglm2-6b\n```\n\n如果你从 Hugging Face Hub 上下载 checkpoint 的速度较慢，可以只下载模型实现\n```Shell\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm2-6b\n```\n然后从[这里](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/)手动下载模型参数文件，并将下载的文件替换到本地的 `chatglm2-6b` 目录下。\n\n\n将模型下载到本地之后，将以上代码中的 `THUDM/chatglm2-6b` 替换为你本地的 `chatglm2-6b` 文件夹的路径，即可从本地加载模型。\n\n模型的实现仍然处在变动中。如果希望固定使用的模型实现以保证兼容性，可以在 `from_pretrained` 的调用中增加 `revision=\"v1.0\"` 参数。`v1.0` 是当前最新的版本号，完整的版本列表参见 [Change Log](https://huggingface.co/THUDM/chatglm2-6b#change-log)。\n\n### 网页版 Demo\n![web-demo](resources/web-demo.gif)\n可以通过以下命令启动基于 Gradio 的网页版 demo：\n```shell\npython web_demo.py\n```\n\n![web-demo](resources/web-demo2.gif)\n\n可以通过以下命令启动基于 Streamlit 的网页版 demo：\n```shell\nstreamlit run web_demo2.py\n```\n\n网页版 demo 会运行一个 Web Server，并输出地址。在浏览器中打开输出的地址即可使用。 经测试，基于 Streamlit 的网页版 Demo 会更流畅。\n\n### 命令行 Demo\n\n![cli-demo](resources/cli-demo.png)\n\n运行仓库中 [cli_demo.py](cli_demo.py)：\n\n```shell\npython cli_demo.py\n```\n\n程序会在命令行中进行交互式的对话，在命令行中输入指示并回车即可生成回复，输入 `clear` 可以清空对话历史，输入 `stop` 终止程序。\n\n### API 部署\n首先需要安装额外的依赖 `pip install fastapi uvicorn`，然后运行仓库中的 [api.py](api.py)：\n```shell\npython api.py\n```\n默认部署在本地的 8000 端口，通过 POST 方法进行调用\n```shell\ncurl -X POST \"http://127.0.0.1:8000\" \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"你好\", \"history\": []}'\n```\n得到的返回值为\n```shell\n{\n  \"response\":\"你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\",\n  \"history\":[[\"你好\",\"你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\"]],\n  \"status\":200,\n  \"time\":\"2023-03-23 21:38:40\"\n}\n```\n感谢 [@hiyouga]() 实现了 OpenAI 格式的流式 API 部署，可以作为任意基于 ChatGPT 的应用的后端，比如 [ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web)。可以通过运行仓库中的[openai_api.py](openai_api.py) 进行部署：\n```shell\npython openai_api.py\n```\n进行 API 调用的示例代码为\n```python\nimport openai\nif __name__ == \"__main__\":\n    openai.api_base = \"http://localhost:8000/v1\"\n    openai.api_key = \"none\"\n    for chunk in openai.ChatCompletion.create(\n        model=\"chatglm2-6b\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"你好\"}\n        ],\n        stream=True\n    ):\n        if hasattr(chunk.choices[0].delta, \"content\"):\n            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n\n## 低成本部署\n\n### 模型量化\n\n默认情况下，模型以 FP16 精度加载，运行上述代码需要大概 13GB 显存。如果你的 GPU 显存有限，可以尝试以量化方式加载模型，使用方法如下：\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b-int4\",trust_remote_code=True).cuda()\n```\n\n模型量化会带来一定的性能损失，经过测试，ChatGLM2-6B 在 4-bit 量化下仍然能够进行自然流畅的生成。 量化模型的参数文件也可以从[这里](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/)手动下载。\n\n### CPU 部署\n\n如果你没有 GPU 硬件的话，也可以在 CPU 上进行推理，但是推理速度会更慢。使用方法如下（需要大概 32GB 内存）\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).float()\n```\n如果你的内存不足的话，也可以使用量化后的模型\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b-int4\",trust_remote_code=True).float()\n```\n在 cpu 上运行量化后的模型需要安装 `gcc` 与 `openmp`。多数 Linux 发行版默认已安装。对于 Windows ，可在安装 [TDM-GCC](https://jmeubank.github.io/tdm-gcc/) 时勾选 `openmp`。 Windows 测试环境 `gcc` 版本为 `TDM-GCC 10.3.0`， Linux 为 `gcc 11.3.0`。在 MacOS 上请参考 [Q1](FAQ.md#q1)。\n\n### Mac 部署\n\n对于搭载了 Apple Silicon 或者 AMD GPU 的 Mac，可以使用 MPS 后端来在 GPU 上运行 ChatGLM2-6B。需要参考 Apple 的 [官方说明](https://developer.apple.com/metal/pytorch) 安装 PyTorch-Nightly（正确的版本号应该是2.x.x.dev2023xxxx，而不是 2.x.x）。\n\n目前在 MacOS 上只支持[从本地加载模型](README.md#从本地加载模型)。将代码中的模型加载改为从本地加载，并使用 mps 后端：\n```python\nmodel = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).to('mps')\n```\n\n加载半精度的 ChatGLM2-6B 模型需要大概 13GB 内存。内存较小的机器（比如 16GB 内存的 MacBook Pro），在空余内存不足的情况下会使用硬盘上的虚拟内存，导致推理速度严重变慢。\n此时可以使用量化后的模型 chatglm2-6b-int4。因为 GPU 上量化的 kernel 是使用 CUDA 编写的，因此无法在 MacOS 上使用，只能使用 CPU 进行推理。\n为了充分使用 CPU 并行，还需要[单独安装 OpenMP](FAQ.md#q1)。\n\n在 Mac 上进行推理也可以使用 [ChatGLM.cpp](https://github.com/li-plus/chatglm.cpp)\n\n### 多卡部署\n如果你有多张 GPU，但是每张 GPU 的显存大小都不足以容纳完整的模型，那么可以将模型切分在多张GPU上。首先安装 accelerate: `pip install accelerate`，然后通过如下方法加载模型：\n```python\nfrom utils import load_model_on_gpus\nmodel = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\n```\n即可将模型部署到两张 GPU 上进行推理。你可以将 `num_gpus` 改为你希望使用的 GPU 数。默认是均匀切分的，你也可以传入 `device_map` 参数来自己指定。 \n\n## 协议\n\n本仓库的代码依照 [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) 协议开源，ChatGLM2-6B 模型的权重的使用则需要遵循 [Model License](MODEL_LICENSE)。ChatGLM2-6B 权重对学术研究**完全开放**，在填写[问卷](https://open.bigmodel.cn/mla/form)进行登记后**亦允许免费商业使用**。\n\n\n## 引用\n\n如果你觉得我们的工作有帮助的话，请考虑引用下列论文，ChatGLM2-6B 的论文会在近期公布，敬请期待～\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 17.8798828125,
          "content": "# ChatGLM2-6B\n\n<p align=\"center\">\n🤗 <a href=\"https://huggingface.co/THUDM/chatglm2-6b\" target=\"_blank\">HF Repo</a> • 🐦 <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> • 📄<a href=\"https://arxiv.org/pdf/2406.12793\" target=\"_blank\"> Report </a> <br>\n</p>\n<p align=\"center\">\n    👋 Join our  <a href=\"https://discord.gg/fK2dz4bg\" target=\"_blank\">Discord</a> and <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n\n## GLM-4 Open Source Model and API\n\nWe have released the latest **GLM-4** model, which has made new breakthroughs in multiple indicators. You can directly\nexperience our latest model in the following two channels.\n\n+ [GLM-4 open source model](https://github.com/THUDM/GLM-4) We have open sourced the GLM-4-9B series models, which have\n  significantly improved the performance of various indicators. Welcome to try.\n+ [Zhipu Qingyan](https://chatglm.cn/main/detail?fr=ecology_x) Experience the latest version of GLM-4, including **GLMs,\n  All tools** and other functions.\n+ [API platform](https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9) The new generation of API\n  platform has been launched. You can directly experience new models such\n  as `GLM-4-0520`, `GLM-4-air`, `GLM-4-airx`, `GLM-4-flash`, `GLM-4`, `GLM-3-Turbo`, `CharacterGLM-3`, `CogView-3` on\n  the API platform.\n  Among them, the two models `GLM-4` and `GLM-3-Turbo` support new functions such\n  as `System Prompt`, `Function Call`, `Retrieval`, and `Web_Search`. You are welcome to experience them.\n\n+ [GLM-4 API open source tutorial](https://github.com/MetaGLM/glm-cookbook/) GLM-4 API tutorial and basic applications,\n  welcome to try.\n  API-related questions can be asked in this open source tutorial, or\n  use [GLM-4 API AI Assistant](https://open.bigmodel.cn/shareapp/v1/?share_code=sQwt5qyqYVaNh1O_87p8O)\n  to get help with common problems.\n\n\n## Introduction\n\nChatGLM**2**-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B). It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:\n\n1. **Stronger Performance**: Based on the development experience of the first-generation ChatGLM model, we have fully upgraded the base model of ChatGLM2-6B. ChatGLM2-6B uses the hybrid objective function of [GLM](https://github.com/THUDM/GLM), and has undergone pre-training with 1.4T bilingual tokens and human preference alignment training. The [evaluation results](README.md#evaluation-results) show that, compared to the first-generation model, ChatGLM2-6B has achieved substantial improvements in performance on datasets like MMLU (+23%), CEval (+33%), GSM8K (+571%), BBH (+60%), showing strong competitiveness among models of the same size.\n2. **Longer Context**: Based on [FlashAttention](https://github.com/HazyResearch/flash-attention) technique, we have extended the context length of the base model from 2K in ChatGLM-6B to 32K, and trained with a context length of 8K during the dialogue alignment, allowing for more rounds of dialogue. However, the current version of ChatGLM2-6B has limited understanding of single-round ultra-long documents, which we will focus on optimizing in future iterations.\n3. **More Efficient Inference**: Based on [Multi-Query Attention](http://arxiv.org/abs/1911.02150) technique, ChatGLM2-6B has more efficient inference speed and lower GPU memory usage: under the official  implementation, the inference speed has increased by 42% compared to the first generation; under INT4 quantization, the dialogue length supported by 6G GPU memory has increased from 1K to 8K.\n4. **More Open License**: ChatGLM2-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form).\n\nWelcome to use the larger ChatGLM model on [chatglm.cn](https://chatglm.cn) \n\n-----\n\nThe open-source ChatGLM2-6B is intended to promote the development of LLMs together with the open-source community. We earnestly request developers and everyone to abide by the [open-source license](MODEL_LICENSE). Do not use the open-source model, code, or any derivatives from the open-source project for any purposes that may harm nations or societies, or for any services that have not undergone safety assessments and legal approval. **At present, our project team has not developed any applications based on ChatGLM2-6B, including web, Android, Apple iOS, and Windows App applications.**\n\nAlthough the model strives to ensure the compliance and accuracy of data at each stage of training, due to the smaller scale of the ChatGLM2-6B model, and its susceptibility to probabilistic randomness, the accuracy of output content cannot be guaranteed, and the model can easily be misled. **Our project does not assume any risks or responsibilities arising from data security, public opinion risks, or any instances of the model being misled, abused, disseminated, or improperly used due to the open-source model and code.**\n\n## Projects\nOpen source projects that accelerate ChatGLM2:\n\n* [fastllm](https://github.com/ztxz16/fastllm/): Universal platform acceleration inference solution, single GPU batch inference can reach 10,000+ tokens per second, and it can run in real-time on mobile devices with a minimum of 3GB of memory (about 4~5 tokens/s on Snapdragon 865).\n* [chatglm.cpp](https://github.com/li-plus/chatglm.cpp): Real-time CPU inference on a MacBook accelerated by quantization, similar to llama.cpp.\n* [ChatGLM2-TPU](https://github.com/sophgo/ChatGLM2-TPU): Using the TPU accelerated inference solution, it runs about 5 token/s in real time on the end-side chip BM1684X (16T@FP16, 16G DDR).\n\nExample projects supporting online training of ChatGLM-6B and related applications:\n* [ChatGLM-6B deployment and fine-tuning tutorial](https://www.heywhale.com/mw/project/64984a7b72ebe240516ae79c)\n\n## Evaluation\nWe selected some typical Chinese and English datasets for evaluation. Below are the evaluation results of the ChatGLM2-6B model on [MMLU](https://github.com/hendrycks/test) (English), [C-Eval](https://cevalbenchmark.com/static/leaderboard.html) (Chinese), [GSM8K](https://github.com/openai/grade-school-math) (Mathematics), [BBH](https://github.com/suzgunmirac/BIG-Bench-Hard) (English).\n\n### MMLU\n\n| Model | Average | STEM | Social Sciences | Humanities | Others |\n| ----- | ----- | ---- | ----- | ----- | ----- |\n| ChatGLM-6B | 40.63 | 33.89 | 44.84 | 39.02 | 45.71 |\n| ChatGLM2-6B (base) | 47.86 | 41.20 | 54.44 | 43.66 | 54.46 |\n| ChatGLM2-6B | 45.46 | 40.06 | 51.61 | 41.23 | 51.24 |\n\n> Chat-aligned version is evaluated under zero-shot CoT (Chain-of-Thought), and Base version is evaluated under few-shot answer-only\n\n### C-Eval\n\n| Model | Average | STEM | Social Sciences | Humanities | Others |\n| ----- | ---- | ---- | ----- | ----- | ----- |\n| ChatGLM-6B | 38.9 | 33.3 | 48.3 | 41.3 | 38.0 |\n| ChatGLM2-6B (base) | 51.7 | 48.6 | 60.5 | 51.3 | 49.8 |\n| ChatGLM2-6B | 50.1 | 46.4\t| 60.4 | 50.6 | 46.9 | \n\n> Chat-aligned version is evaluated under zero-shot CoT (Chain-of-Thought), and Base version is evaluated under few-shot answer-only\n\n### GSM8K\n\n| Model | Accuracy | Accuracy (Chinese)* |\n| ----- | ----- | ----- |\n| ChatGLM-6B | 4.82 | 5.85 |\n| ChatGLM2-6B (base) | 32.37 | 28.95 |\n| ChatGLM2-6B | 28.05 | 20.45 |\n\n> All model versions are evaluated under few-shot CoT, and CoT prompts are from http://arxiv.org/abs/2201.11903\n> \\* We translate a 500-query subset of GSM8K and its corresponding CoT prompts using machine translation API and subsequent human proofreading.\n\n\n### BBH\n\n| Model | Accuracy |\n| ----- | ----- |\n| ChatGLM-6B | 18.73 |\n| ChatGLM2-6B (base) | 33.68 |\n| ChatGLM2-6B | 30.00 |\n\n> All model versions are evaluated under few-shot CoT, and CoT prompts are from https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts\n\n## Inference Efficiency\nChatGLM2-6B employs [Multi-Query Attention](http://arxiv.org/abs/1911.02150) to improve inference speed. Here is a comparison of the average speed for generating 2000 tokens.\n\n\n| Model | Inference Speed (tokens/s) |\n| ----  | -----  |\n| ChatGLM-6B  | 31.49 |\n| ChatGLM2-6B | 44.62 |\n\n> Under our official implementation, batch size = 1, max length = 2048, bf16 precision, tested with an A100-SXM-80G and PyTorch 2.0 environment\n\nMulti-Query Attention also reduces the GPU memory usage of the KV Cache during inference. Additionally, ChatGLM2-6B uses Causal Mask for dialogue training, which allows the reuse of the KV Cache from previous rounds in continuous dialogues, further optimizing GPU memory usage. Therefore, when performing INT4 quantization inference with a 6GB GPU, while the first-generation ChatGLM-6B can only generate a maximum of 1119 tokens before running out of memory, ChatGLM2-6B can generate at least 8192 tokens.\n\n| **Quantization** | **Encoding 2048 Tokens** | **Decoding 8192 Tokens** |\n| -------------- | --------------------- | --------------- |\n| FP16 / BF16 | 13.1 GB             | 12.8 GB             | \n| INT8           | 8.2 GB              | 8.1 GB              |\n| INT4           | 5.5 GB              | 5.1 GB              |\n\n> ChatGLM2-6B takes advantage of `torch.nn.functional.scaled_dot_product_attention` introduced in PyTorch 2.0 for efficient Attention computation. If the PyTorch version is lower, it will fallback to the naive Attention implementation, which may result in higher GPU memory usage than shown in the table above.\n\nWe also tested the impact of quantization on model performance. The results show that the impact of quantization on model performance is within an acceptable range.\n\n| Quantization | Accuracy (MMLU) | Accuracy (C-Eval dev) |\n| ----- | ----- |-----------------------|\n| BF16 | 45.47 | 53.57                 |\n| INT4 | 43.13 | 50.30                 |\n\n\n## ChatGLM2-6B Examples\n\nCompared to the first-generation model, ChatGLM2-6B has made improvements in multiple dimensions. Below are some comparison examples. More possibilities with ChatGLM2-6B are waiting for you to explore and discover!\n\n<details><summary><b>Mathematics and Logic</b></summary>\n\n![](examples/math.png)\n\n</details>\n\n<details><summary><b>Knowledge Reasoning</b></summary>\n\n![](examples/knowledge.png)\n\n</details>\n\n<details><summary><b>Long Document Understanding</b></summary>\n\n![](examples/long-context.png)\n\n</details>\n\n## Getting Started\n### Environment Setup\n\nInstall dependencies with pip: `pip install -r requirements.txt`. It's recommended to use version `4.27.1` for the `transformers` library and use version 2.0 or higher for `torch` to achieve the best inference performance.\n\nWe provide a web page demo and a command line demo. You need to download this repository to use them:\n\n```shell\ngit clone https://github.com/THUDM/ChatGLM2-6B\ncd ChatGLM2-6B\n```\n\n### Usage\n\nGenerate dialogue with the following code:\n\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device='cuda').eval()\n>>> response, history = model.chat(tokenizer, \"你好\", history=[])\n>>> print(response)\n你好👋!我是人工智能助手 ChatGLM2-6B,很高兴见到你,欢迎问我任何问题。\n>>> response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n>>> print(response)\n晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:\n\n1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。\n2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。\n3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。\n4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。\n5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。\n6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。\n\n如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。\n```\nThe implementation of the model is still in development. If you want to fix the used model implementation to ensure compatibility, you can add the `revision=\"v1.0\"` parameter in the `from_pretrained` call. `v1.0` is the latest version number. For a complete list of versions, see [Change Log](https://huggingface.co/THUDM/chatglm2-6b#change-log).\n\n### Web Demo\n\n![web-demo](resources/web-demo.gif)\n\nInstall Gradio `pip install gradio`，and run [web_demo.py](web_demo.py):\n\n```shell\npython web_demo.py\n```\n\nThe program runs a web server and outputs the URL. Open the URL in the browser to use the web demo.\n\n#### CLI Demo\n\n![cli-demo](resources/cli-demo.png)\n\nRun [cli_demo.py](cli_demo.py) in the repo:\n\n```shell\npython cli_demo.py\n```\n\nThe command runs an interactive program in the shell. Type your instruction in the shell and hit enter to generate the response. Type `clear` to clear the dialogue history and `stop` to terminate the program.\n\n## API Deployment\nFirst install the additional dependency `pip install fastapi uvicorn`. The run [api.py](api.py) in the repo.\n```shell\npython api.py\n```\nBy default the api runs at the`8000`port of the local machine. You can call the API via \n```shell\ncurl -X POST \"http://127.0.0.1:8000\" \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"你好\", \"history\": []}'\n```\nThe returned value is\n```shell\n{\n  \"response\":\"你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\",\n  \"history\":[[\"你好\",\"你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\"]],\n  \"status\":200,\n  \"time\":\"2023-03-23 21:38:40\"\n}\n```\n## Deployment\n\n### Quantization\n\nBy default, the model parameters are loaded with FP16 precision, which require about 13GB of GPU memory. It your GPU memory is limited, you can try to load the model parameters with quantization:\n\n```python\n# hange according to your hardware. Only support 4/8 bit quantization now.\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).quantize(8).cuda()\n```\n\nModel quantization will bring some performance loss on datasets. But after testing, ChatGLM2-6B can still perform natural and smooth generation under 4-bit quantization.\n\n### CPU Deployment\n\nIf your computer is not equipped with GPU, you can also conduct inference on CPU, but the inference speed is slow (and taking about 32GB of memory):\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).float()\n```\n\n### Inference on Mac\n\nFor Macs (and MacBooks) with Apple Silicon, it is possible to use the MPS backend to run ChatGLM-6B on the GPU. First, you need to refer to Apple's [official instructions](https://developer.apple.com/metal/pytorch) to install PyTorch-Nightly. (The correct version number should be 2.1.0.dev2023xxxx, not 2.0.0).\n\nCurrently you must [load the model locally](README_en.md#load-the-model-locally) on MacOS. Change the code to load the model from your local path, and use the mps backend:\n```python\nmodel = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).to('mps')\n```\n\nLoading a FP16 ChatGLM-6B model requires about 13GB of memory. Machines with less memory (such as a MacBook Pro with 16GB of memory) will use the virtual memory on the hard disk when there is insufficient free memory, resulting in a serious slowdown in inference speed.\n\n## License\n\nThe code of this repository is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0). The use of the ChatGLM2-6B model weights is subject to the [Model License](MODEL_LICENSE). ChatGLM2-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form).\n\n## Citation\n\nIf you find our work useful, please consider citing the following papers. The technical report for ChatGLM2-6B will be out soon.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 2.0947265625,
          "content": "from fastapi import FastAPI, Request\nfrom transformers import AutoTokenizer, AutoModel\nimport uvicorn, json, datetime\nimport torch\n\nDEVICE = \"cuda\"\nDEVICE_ID = \"0\"\nCUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n\n\ndef torch_gc():\n    if torch.cuda.is_available():\n        with torch.cuda.device(CUDA_DEVICE):\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n\n\napp = FastAPI()\n\n\n@app.post(\"/\")\nasync def create_item(request: Request):\n    global model, tokenizer\n    json_post_raw = await request.json()\n    json_post = json.dumps(json_post_raw)\n    json_post_list = json.loads(json_post)\n    prompt = json_post_list.get('prompt')\n    history = json_post_list.get('history')\n    max_length = json_post_list.get('max_length')\n    top_p = json_post_list.get('top_p')\n    temperature = json_post_list.get('temperature')\n    response, history = model.chat(tokenizer,\n                                   prompt,\n                                   history=history,\n                                   max_length=max_length if max_length else 2048,\n                                   top_p=top_p if top_p else 0.7,\n                                   temperature=temperature if temperature else 0.95)\n    now = datetime.datetime.now()\n    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    answer = {\n        \"response\": response,\n        \"history\": history,\n        \"status\": 200,\n        \"time\": time\n    }\n    log = \"[\" + time + \"] \" + '\", prompt:\"' + prompt + '\", response:\"' + repr(response) + '\"'\n    print(log)\n    torch_gc()\n    return answer\n\n\nif __name__ == '__main__':\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n    model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n    # 多显卡支持，使用下面三行代替上面两行，将num_gpus改为你实际的显卡数量\n    # model_path = \"THUDM/chatglm2-6b\"\n    # tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    # model = load_model_on_gpus(model_path, num_gpus=2)\n    model.eval()\n    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)\n"
        },
        {
          "name": "cli_demo.py",
          "type": "blob",
          "size": 2.173828125,
          "content": "import os\nimport platform\nimport signal\nfrom transformers import AutoTokenizer, AutoModel\nimport readline\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n# 多显卡支持，使用下面两行代替上面一行，将num_gpus改为你实际的显卡数量\n# from utils import load_model_on_gpus\n# model = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\nmodel = model.eval()\n\nos_name = platform.system()\nclear_command = 'cls' if os_name == 'Windows' else 'clear'\nstop_stream = False\n\n\ndef build_prompt(history):\n    prompt = \"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n    for query, response in history:\n        prompt += f\"\\n\\n用户：{query}\"\n        prompt += f\"\\n\\nChatGLM2-6B：{response}\"\n    return prompt\n\n\ndef signal_handler(signal, frame):\n    global stop_stream\n    stop_stream = True\n\n\ndef main():\n    past_key_values, history = None, []\n    global stop_stream\n    print(\"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\")\n    while True:\n        query = input(\"\\n用户：\")\n        if query.strip() == \"stop\":\n            break\n        if query.strip() == \"clear\":\n            past_key_values, history = None, []\n            os.system(clear_command)\n            print(\"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\")\n            continue\n        print(\"\\nChatGLM：\", end=\"\")\n        current_length = 0\n        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history,\n                                                                    past_key_values=past_key_values,\n                                                                    return_past_key_values=True):\n            if stop_stream:\n                stop_stream = False\n                break\n            else:\n                print(response[current_length:], end=\"\", flush=True)\n                current_length = len(response)\n        print(\"\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_api.py",
          "type": "blob",
          "size": 5.5654296875,
          "content": "# coding=utf-8\n# Implements API for ChatGLM2-6B in OpenAI's format. (https://platform.openai.com/docs/api-reference/chat)\n# Usage: python openai_api.py\n# Visit http://localhost:8000/docs for documents.\n\n\nimport time\nimport torch\nimport uvicorn\nfrom pydantic import BaseModel, Field\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom contextlib import asynccontextmanager\nfrom typing import Any, Dict, List, Literal, Optional, Union\nfrom transformers import AutoTokenizer, AutoModel\nfrom sse_starlette.sse import ServerSentEvent, EventSourceResponse\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI): # collects GPU memory\n    yield\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n\napp = FastAPI(lifespan=lifespan)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"owner\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass ChatMessage(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    content: str\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal[\"user\", \"assistant\", \"system\"]] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessage]\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    max_length: Optional[int] = None\n    stream: Optional[bool] = False\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Literal[\"stop\", \"length\"]\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]]\n\n\nclass ChatCompletionResponse(BaseModel):\n    model: str\n    object: Literal[\"chat.completion\", \"chat.completion.chunk\"]\n    choices: List[Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]]\n    created: Optional[int] = Field(default_factory=lambda: int(time.time()))\n\n\n@app.get(\"/v1/models\", response_model=ModelList)\nasync def list_models():\n    global model_args\n    model_card = ModelCard(id=\"gpt-3.5-turbo\")\n    return ModelList(data=[model_card])\n\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    global model, tokenizer\n\n    if request.messages[-1].role != \"user\":\n        raise HTTPException(status_code=400, detail=\"Invalid request\")\n    query = request.messages[-1].content\n\n    prev_messages = request.messages[:-1]\n    if len(prev_messages) > 0 and prev_messages[0].role == \"system\":\n        query = prev_messages.pop(0).content + query\n\n    history = []\n    if len(prev_messages) % 2 == 0:\n        for i in range(0, len(prev_messages), 2):\n            if prev_messages[i].role == \"user\" and prev_messages[i+1].role == \"assistant\":\n                history.append([prev_messages[i].content, prev_messages[i+1].content])\n\n    if request.stream:\n        generate = predict(query, history, request.model)\n        return EventSourceResponse(generate, media_type=\"text/event-stream\")\n\n    response, _ = model.chat(tokenizer, query, history=history)\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=ChatMessage(role=\"assistant\", content=response),\n        finish_reason=\"stop\"\n    )\n\n    return ChatCompletionResponse(model=request.model, choices=[choice_data], object=\"chat.completion\")\n\n\nasync def predict(query: str, history: List[List[str]], model_id: str):\n    global model, tokenizer\n\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0,\n        delta=DeltaMessage(role=\"assistant\"),\n        finish_reason=None\n    )\n    chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n    yield \"{}\".format(chunk.json(exclude_unset=True, ensure_ascii=False))\n\n    current_length = 0\n\n    for new_response, _ in model.stream_chat(tokenizer, query, history):\n        if len(new_response) == current_length:\n            continue\n\n        new_text = new_response[current_length:]\n        current_length = len(new_response)\n\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0,\n            delta=DeltaMessage(content=new_text),\n            finish_reason=None\n        )\n        chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n        yield \"{}\".format(chunk.json(exclude_unset=True, ensure_ascii=False))\n\n\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0,\n        delta=DeltaMessage(),\n        finish_reason=\"stop\"\n    )\n    chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n    yield \"{}\".format(chunk.json(exclude_unset=True, ensure_ascii=False))\n    yield '[DONE]'\n\n\n\nif __name__ == \"__main__\":\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n    model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n    # 多显卡支持，使用下面两行代替上面一行，将num_gpus改为你实际的显卡数量\n    # from utils import load_model_on_gpus\n    # model = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\n    model.eval()\n\n    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)\n"
        },
        {
          "name": "ptuning",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1240234375,
          "content": "protobuf\ntransformers==4.30.2\ncpm_kernels\ntorch>=2.0\ngradio\nmdtex2html\nsentencepiece\naccelerate\nsse-starlette\nstreamlit>=1.24.0"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 2.2197265625,
          "content": "import os\nfrom typing import Dict, Tuple, Union, Optional\n\nfrom torch.nn import Module\nfrom transformers import AutoModel\n\n\ndef auto_configure_device_map(num_gpus: int) -> Dict[str, int]:\n    # transformer.word_embeddings 占用1层\n    # transformer.final_layernorm 和 lm_head 占用1层\n    # transformer.layers 占用 28 层\n    # 总共30层分配到num_gpus张卡上\n    num_trans_layers = 28\n    per_gpu_layers = 30 / num_gpus\n\n    # bugfix: 在linux中调用torch.embedding传入的weight,input不在同一device上,导致RuntimeError\n    # windows下 model.device 会被设置成 transformer.word_embeddings.device\n    # linux下 model.device 会被设置成 lm_head.device\n    # 在调用chat或者stream_chat时,input_ids会被放到model.device上\n    # 如果transformer.word_embeddings.device和model.device不同,则会导致RuntimeError\n    # 因此这里将transformer.word_embeddings,transformer.final_layernorm,lm_head都放到第一张卡上\n    # 本文件来源于https://github.com/THUDM/ChatGLM-6B/blob/main/utils.py\n    # 仅此处做少许修改以支持ChatGLM2\n    device_map = {\n        'transformer.embedding.word_embeddings': 0,\n        'transformer.encoder.final_layernorm': 0,\n        'transformer.output_layer': 0,\n        'transformer.rotary_pos_emb': 0,\n        'lm_head': 0\n    }\n\n    used = 2\n    gpu_target = 0\n    for i in range(num_trans_layers):\n        if used >= per_gpu_layers:\n            gpu_target += 1\n            used = 0\n        assert gpu_target < num_gpus\n        device_map[f'transformer.encoder.layers.{i}'] = gpu_target\n        used += 1\n\n    return device_map\n\n\ndef load_model_on_gpus(checkpoint_path: Union[str, os.PathLike], num_gpus: int = 2,\n                       device_map: Optional[Dict[str, int]] = None, **kwargs) -> Module:\n    if num_gpus < 2 and device_map is None:\n        model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True, **kwargs).half().cuda()\n    else:\n        from accelerate import dispatch_model\n\n        model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True, **kwargs).half()\n\n        if device_map is None:\n            device_map = auto_configure_device_map(num_gpus)\n\n        model = dispatch_model(model, device_map=device_map)\n\n    return model\n"
        },
        {
          "name": "web_demo.py",
          "type": "blob",
          "size": 4.1220703125,
          "content": "from transformers import AutoModel, AutoTokenizer\nimport gradio as gr\nimport mdtex2html\nfrom utils import load_model_on_gpus\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n# 多显卡支持，使用下面两行代替上面一行，将num_gpus改为你实际的显卡数量\n# from utils import load_model_on_gpus\n# model = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\nmodel = model.eval()\n\n\"\"\"Override Chatbot.postprocess\"\"\"\n\n\ndef postprocess(self, y):\n    if y is None:\n        return []\n    for i, (message, response) in enumerate(y):\n        y[i] = (\n            None if message is None else mdtex2html.convert((message)),\n            None if response is None else mdtex2html.convert(response),\n        )\n    return y\n\n\ngr.Chatbot.postprocess = postprocess\n\n\ndef parse_text(text):\n    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n    lines = text.split(\"\\n\")\n    lines = [line for line in lines if line != \"\"]\n    count = 0\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            count += 1\n            items = line.split('`')\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = f'<br></code></pre>'\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace(\"`\", \"\\`\")\n                    line = line.replace(\"<\", \"&lt;\")\n                    line = line.replace(\">\", \"&gt;\")\n                    line = line.replace(\" \", \"&nbsp;\")\n                    line = line.replace(\"*\", \"&ast;\")\n                    line = line.replace(\"_\", \"&lowbar;\")\n                    line = line.replace(\"-\", \"&#45;\")\n                    line = line.replace(\".\", \"&#46;\")\n                    line = line.replace(\"!\", \"&#33;\")\n                    line = line.replace(\"(\", \"&#40;\")\n                    line = line.replace(\")\", \"&#41;\")\n                    line = line.replace(\"$\", \"&#36;\")\n                lines[i] = \"<br>\"+line\n    text = \"\".join(lines)\n    return text\n\n\ndef predict(input, chatbot, max_length, top_p, temperature, history, past_key_values):\n    chatbot.append((parse_text(input), \"\"))\n    for response, history, past_key_values in model.stream_chat(tokenizer, input, history, past_key_values=past_key_values,\n                                                                return_past_key_values=True,\n                                                                max_length=max_length, top_p=top_p,\n                                                                temperature=temperature):\n        chatbot[-1] = (parse_text(input), parse_text(response))\n\n        yield chatbot, history, past_key_values\n\n\ndef reset_user_input():\n    return gr.update(value='')\n\n\ndef reset_state():\n    return [], [], None\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\"\"\"<h1 align=\"center\">ChatGLM2-6B</h1>\"\"\")\n\n    chatbot = gr.Chatbot()\n    with gr.Row():\n        with gr.Column(scale=4):\n            with gr.Column(scale=12):\n                user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=10).style(\n                    container=False)\n            with gr.Column(min_width=32, scale=1):\n                submitBtn = gr.Button(\"Submit\", variant=\"primary\")\n        with gr.Column(scale=1):\n            emptyBtn = gr.Button(\"Clear History\")\n            max_length = gr.Slider(0, 32768, value=8192, step=1.0, label=\"Maximum length\", interactive=True)\n            top_p = gr.Slider(0, 1, value=0.8, step=0.01, label=\"Top P\", interactive=True)\n            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)\n\n    history = gr.State([])\n    past_key_values = gr.State(None)\n\n    submitBtn.click(predict, [user_input, chatbot, max_length, top_p, temperature, history, past_key_values],\n                    [chatbot, history, past_key_values], show_progress=True)\n    submitBtn.click(reset_user_input, [], [user_input])\n\n    emptyBtn.click(reset_state, outputs=[chatbot, history, past_key_values], show_progress=True)\n\ndemo.queue().launch(share=False, inbrowser=True)\n"
        },
        {
          "name": "web_demo2.py",
          "type": "blob",
          "size": 2.44921875,
          "content": "from transformers import AutoModel, AutoTokenizer\nimport streamlit as st\n\n\nst.set_page_config(\n    page_title=\"ChatGLM2-6b 演示\",\n    page_icon=\":robot:\",\n    layout='wide'\n)\n\n\n@st.cache_resource\ndef get_model():\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n    model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n    # 多显卡支持，使用下面两行代替上面一行，将num_gpus改为你实际的显卡数量\n    # from utils import load_model_on_gpus\n    # model = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\n    model = model.eval()\n    return tokenizer, model\n\n\ntokenizer, model = get_model()\n\nst.title(\"ChatGLM2-6B\")\n\nmax_length = st.sidebar.slider(\n    'max_length', 0, 32768, 8192, step=1\n)\ntop_p = st.sidebar.slider(\n    'top_p', 0.0, 1.0, 0.8, step=0.01\n)\ntemperature = st.sidebar.slider(\n    'temperature', 0.0, 1.0, 0.8, step=0.01\n)\n\nif 'history' not in st.session_state:\n    st.session_state.history = []\n\nif 'past_key_values' not in st.session_state:\n    st.session_state.past_key_values = None\n\nfor i, (query, response) in enumerate(st.session_state.history):\n    with st.chat_message(name=\"user\", avatar=\"user\"):\n        st.markdown(query)\n    with st.chat_message(name=\"assistant\", avatar=\"assistant\"):\n        st.markdown(response)\nwith st.chat_message(name=\"user\", avatar=\"user\"):\n    input_placeholder = st.empty()\nwith st.chat_message(name=\"assistant\", avatar=\"assistant\"):\n    message_placeholder = st.empty()\n\nprompt_text = st.text_area(label=\"用户命令输入\",\n                           height=100,\n                           placeholder=\"请在这儿输入您的命令\")\n\nbutton = st.button(\"发送\", key=\"predict\")\n\nif button:\n    input_placeholder.markdown(prompt_text)\n    history, past_key_values = st.session_state.history, st.session_state.past_key_values\n    for response, history, past_key_values in model.stream_chat(tokenizer, prompt_text, history,\n                                                                past_key_values=past_key_values,\n                                                                max_length=max_length, top_p=top_p,\n                                                                temperature=temperature,\n                                                                return_past_key_values=True):\n        message_placeholder.markdown(response)\n\n    st.session_state.history = history\n    st.session_state.past_key_values = past_key_values\n"
        }
      ]
    }
  ]
}