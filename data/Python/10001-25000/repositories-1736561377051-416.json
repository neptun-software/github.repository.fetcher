{
  "metadata": {
    "timestamp": 1736561377051,
    "page": 416,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "great-expectations/great_expectations",
      "stars": 10100,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".coderabbit.yaml",
          "type": "blob",
          "size": 0.4716796875,
          "content": "# Only enables summaries for PRs. Disables auto-review\nlanguage: \"en\"\nearly_access: false\nreviews:\n  request_changes_workflow: false\n  high_level_summary: true\n  poem: true\n  review_status: true\n  collapse_walkthrough: false\n  path_filters:\n    - \"!**/.xml\"\n    - \"great_expectations/**\"\n  auto_review:\n    enabled: false\n    ignore_title_keywords:\n      - \"WIP\"\n      - \"DO NOT MERGE\"\n    drafts: false\n    base_branches:\n      - \"develop\"\n      - \"feat/*\"\nchat:\n  auto_reply: true\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 2.884765625,
          "content": "# This contains a list of git commit hashes that should be ignored when viewing the git blame on github.\n# It can also be passed to `--ignore-revs-file` if using git locally.\n# https://git-scm.com/docs/git-blame#Documentation/git-blame.txt---ignore-revltrevgt\n# A typical use of these would be if a large formatting change was applied across the whole codebase but otherwise\n# no runtime changes were made. Each entry should be preceded with a comment explaining the change.\n#\n# https://black.readthedocs.io/en/stable/guides/introducing_black_to_your_project.html#avoiding-ruining-git-blame\n# Apply noqa markers for all TCH001 violations\n# https://github.com/great-expectations/great_expectations/pull/7072\nf5e7df1846102d9a62cc9b9110387925ffae60cc\n# Apply noqa markers for all PTH (use-pathlib) violations\n# https://github.com/great-expectations/great_expectations/pull/7290\n597b2b625569b6f5f110f8230ac26ab405167da6\n# Apply noqa markers for TID251 (sqlalchemy) violations\n# https://github.com/great-expectations/great_expectations/pull/7564\ne55b3484a86f654e8b819041dd6cc73730e01a8f\n# python 3.7 -> 3.8 Ruff auto-fixes\n# https://github.com/great-expectations/great_expectations/pull/7945\n85b81029c759dc2b6a8c8956a5babb19e3280494\n# black 2.2 -> 2.3 formatting changes\n# https://github.com/great-expectations/great_expectations/pull/7946\n582fe60179b9b8bf31ba0a394bfa669f495ce8c6\n# lint ci, expectations and rule_based_profiler\n# https://github.com/great-expectations/great_expectations/pull/8422\n8bd2dea55e22188b74d1b20abee480f2c298239d\n# ruff 0.0.280\n# https://github.com/great-expectations/great_expectations/pull/8179\n544f90d0e90807032e517283c1c0971904ebd4e0\n# ruff 0.0.284\n# https://github.com/great-expectations/great_expectations/pull/8552\n02224a3c9f53552207682c54dde70f9bb4cae1c4\n# add @overrides\n# https://github.com/great-expectations/great_expectations/pull/8589\nc3df7cde688332cb101432909145b37ff4108803\n# prettier yaml formatting\n# https://github.com/great-expectations/great_expectations/pull/9562\n179c974eb23cbd5197459e0c95749f22041f806e\n# black -> ruff format\n# https://github.com/great-expectations/great_expectations/pull/9536\n697d5910c8331721e7dc299b1c0a294de84275b5\n# Ban unittest.mock usage\n# https://github.com/great-expectations/great_expectations/pull/9586\nfc5023b4e1f1dad2e9c6faa8897fa92991b821ab\n# Reduce max cyclomatic complexity from 10 -> 8\n# https://github.com/great-expectations/great_expectations/pull/9622\nedf5dbdc77b418d659dc9d87462cd6df9a85436c\n# Change line-length from 88 -> 100\n# https://github.com/great-expectations/great_expectations/pull/9584\nd170c89167a96b702edc02b16dbf5984619d0e8f\n# enable TRYceratops linting; add noqa comments\n# https://github.com/great-expectations/great_expectations/pull/9684\n2bbfb50a6458f09ef197ee1174666a4c4726a850\n# Add \"FIXME CoP\" comments to type ignore and noqa ignore lines\n# https://github.com/great-expectations/great_expectations/pull/10817\n078c1611dd9c70fec6d7b6318f21a063ff6aa9b0\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.04296875,
          "content": "great_expectations/_version.py export-subst\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9130859375,
          "content": "# Documentation build\ndocs/source/_build\nnode_modules\ndocs/build\n.docusaurus\n.cache-loader\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\n\n# Render test output\ntests/render/output/*\n\n# DataContext test output\ntests/data_context/output/*\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\n.idea/\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports / profiling data\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\nsrc/dgtest/\ntype_cov/\n.profiles/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\nge_dev/\ngx_dev/\n.gx_dev/\n\n# direnv\n.envrc\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\n\n# vim backup files\n*~\n\n# VS code\n.vscode\n\n.DS_Store\n**/.pytest_cache\n\n# Vim swap files\n*.swp\ntags\n\n# Special \"scratchpad\" directory for notes, code samples, experiments, etc.\nDO_NOT_COMMIT\n\n# Hive Metastore from Spark\nmetastore_db\n\n# dependency management\nPipfile\nPipfile.lock\n\n# contract testing files\ntests/integration/cloud/rest_contracts/pacts\n\n.databricks\n\n# Local Netlify folder\n.netlify\n\n# mercury container volume\nassets/docker/mercury/volume/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.8349609375,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: check-ast\n      - id: check-json\n        exclude: invalid_json_file.json|tests/test_sets/fixed_distributional_test_dataset.json|tests/test_sets/expected_cli_results_default.json\n      - id: check-yaml\n        exclude: docs/.*|tests/data_context/fixtures/bad_yml/great_expectations\\.yml\n      - id: end-of-file-fixer\n        exclude: docs/.*|tests/test_sets/broken_excel_file\\.xls|.*.ge_store_backend_id\n      - id: trailing-whitespace\n        exclude: docs/.*|tests/.*.fixture\n      - id: requirements-txt-fixer\n        exclude: docs/.*\n      - id: no-commit-to-branch\n        args: [--branch, develop, --branch, main]\n      - id: detect-private-key\n        exclude: tests/test_fixtures/database_key_test*|tests/datasource/fluent/test_snowflake_datasource\\.py\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: \"v0.7.2\"\n    hooks:\n      - id: ruff\n        args: [\"--fix\"]\n        exclude: ^(build|examples|docs/docusaurus/versioned_docs/version-0.17|docs/docusaurus/versioned_docs/version-0.18)\n      - id: ruff-format\n  - repo: https://github.com/pre-commit/mirrors-prettier\n    rev: v4.0.0-alpha.8\n    hooks:\n      - id: prettier\n        types_or: [yaml]\n        exclude: tests/data_context/fixtures/bad_yml\n  # Github Actions Linter\n  # Need to be able to inline ignore errors\n  # https://github.com/rhysd/actionlint/issues/237\n  # - repo: https://github.com/rhysd/actionlint\n  #   rev: main\n  #   hooks:\n  #     - id: actionlint\n# https://pre-commit.ci/\nci:\n  autofix_commit_msg: |\n    [pre-commit.ci] auto fixes from pre-commit.com hooks\n\n    for more information, see https://pre-commit.ci\n  autofix_prs: true\n  autoupdate_branch: \"develop\"\n  autoupdate_commit_msg: \"[pre-commit.ci] pre-commit autoupdate\"\n  autoupdate_schedule: monthly\n  submodules: false\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.6806640625,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs_rtd/conf.py\n  fail_on_warning: false # set to true once builds are no longer producing warnings\n\n# Build documentation with MkDocs\n#mkdocs:\n#  configuration: mkdocs.yml\n\n# Optionally build your docs in additional formats such as PDF and ePub\nformats:\n  - htmlzip\n\n# Optionally set the version of Python and requirements required to build your docs\npython:\n  version: 3.8\n  #   system_packages: true\n  install:\n    - requirements: docs_rtd/requirements.txt\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.875,
          "content": "abstract: Great Expectations is a shared, open standard for data quality. It helps\n  data teams eliminate pipeline debt, through data testing, documentation, and profiling.\nauthors:\n- family-names: Gong\n  given-names: Abe\n- family-names: Campbell\n  given-names: James\n- name: Great Expectations\n  website: https://greatexpectations.io\n  email: team@greatexpectations.io\ncff-version: 1.2.0\nidentifiers:\n- description: This is the collection of all archived snapshots of all versions of\n    Great Expectations\n  type: doi\n  value: 10.5281/zenodo.5683574\nkeywords:\n- data quality\n- pipeline testing\n- data testing\n- pipeline debt\n- data observability\n- data monitoring\n- data profiling\n- data documentation\nlicense: Apache-2.0\nmessage: If you use this software, please cite it using these metadata.\nrepository-code: https://github.com/great-expectations/great_expectations\ntitle: Great Expectations\n"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 0.2080078125,
          "content": "reqs/ @great-expectations/timber\nrequirements-dev.txt @great-expectations/timber\nrequirements-types.txt @great-expectations/timber\nrequirements.txt @great-expectations/timber\nCODEOWNERS @great-expectations/timber\n"
        },
        {
          "name": "CONTRIBUTING_CODE.md",
          "type": "blob",
          "size": 25.53125,
          "content": "# Contribute a code change\n\nTo modify existing Great Expectations code, you complete the following tasks:\n\n- [Fork and clone the Great Expectations repository](#fork-and-clone-the-great-expectations-repository)\n\n- [Create a virtual environment (optional)](#create-a-virtual-environment-optional)\n\n- [Install great_expectations](#install-great-expectations-and-extra-requirements-from-your-local-repository)\n\n- [Configure backends for testing (optional)](#configure-backends-for-testing-optional)\n\n- [Test code changes](#test-code-changes)\n\n- [Test performance](#test-performance)\n\n- [Submit a pull request](#submit-a-pull-request)\n\n- [Sign the contributor license agreement (CLA)](#contributor-license-agreement-cla)\n\nTo discuss your code change before you implement it, join the [Great Expectations Slack community](https://greatexpectations.io/slack) and make your suggestion in the [#contributing](https://greatexpectationstalk.slack.com/archives/CV828B2UX) channel.\n\nTo request a documentation change, or a change that doesn't require local testing, see the [README](https://github.com/great-expectations/great_expectations/tree/develop/docs) in the `docs` repository.\n\nTo create and submit a Custom Expectation to Great Expectations for consideration, see [CONTRIBUTING_EXPECTATIONS](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_EXPECTATIONS.md) in the `great_expectations` repository.\n\nTo submit a custom package to Great Expectations for consideration, see [CONTRIBUTING_PACKAGES](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_PACKAGES.md) in the `great_expectations` repository.\n\n## Prerequisites\n\n- A GitHub account.\n\n- A working version of Git on your computer. See [Getting Started - Installing Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n\n- A new SSH (Secure Shell Protocol) key. See [Generating a new SSH key and adding it to the ssh-agent](https://help.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).\n\n- The latest Python version installed and configured. See [Python downloads](https://www.python.org/downloads/).\n\n## Fork and clone the Great Expectations repository\n\n1. Open a browser and go to the [Great Expectations repository](https://github.com/great-expectations/great_expectations).\n\n2. Click **Fork** and then **Create Fork**.\n\n3. Click **Code** and then select the **HTTPS** or **SSH** tabs.\n\n4. Copy the URL, open a Git terminal, and then run the following command:\n\n    ```sh\n    git clone <url>\n    ```\n5. Run the following command to specify a new remote upstream repository that will be synced with the fork:\n\n    ```sh\n    git remote add upstream git@github.com:great-expectations/great_expectations.git\n    ```\n6. Run the following command to create a branch for your changes:\n\n    ```sh\n    git checkout -b <branch-name>\n    ```\n\n## Create a virtual environment (optional)\n\nA [virtual environment](https://peps.python.org/pep-0405) allows you to install an independent set of Python packages to their own site directory, isolated from the base/system install of Python.\n\nGreat Expectations requires a Python version from 3.9 to 3.12.\n\n### Python\n\n1. Run the following command to create a virtual environment named `gx_dev`:\n\n   ```sh\n   python3 -m venv <path_to_environments_folder>/gx_dev\n   ```\n2. Run the following command to activate the virtual environment:\n\n   ```sh\n   source <path_to_environments_folder>/gx_dev/bin/activate\n   ```\n3. Run the following command to upgrade pip, the \"package installer\" for Python:\n    ```sh\n    pip install --upgrade pip\n    ```\n\n### Anaconda\n\n1. Run the following command to create a virtual environment named `gx_dev`:\n\n   ```sh\n   conda create --name gx_dev\n   ```\n2. Run the following command to activate the virtual environment:\n\n   ```sh\n   conda activate gx_dev\n   ```\n\n## Install Great Expectations and extra requirements from your local repository\n\n1. Run the following command from the root of the `great_expectations` repository to install Great Expectations in [editable mode](https://setuptools.pypa.io/en/latest/userguide/development_mode.html), with extra requirements for testing:\n\n    ```sh\n\n    pip install -c constraints-dev.txt -e \".[test]\"\n    ```\n\n    To specify other dependencies, add a comma after `test` and enter the dependency name(s). **For example, \".[test, postgresql, trino]\"**.\n\n    The supported extra dependencies include: `arrow`, `athena`, `aws_secrets`, `azure`, `azure_secrets`, `bigquery`, `clickhouse`, `cloud`, `dremio`, `excel`, `gcp`, `hive`, `mssql`, `mysql`, `postgresql`, `redshift`, `s3`, `snowflake`, `spark`, `teradata`, `test`, `trino`, `vertica`.\n\n    Check below to see if any of your desired dependencies need system packages installed, **before `pip install`**.\n\n2. Optional. If you're using Amazon Redshift (`redshift`) or PostgreSQL (`postgresql`), run one of the following commands to install the `pg_config` executable, which is required to install the `psycopg2-binary` Python package:\n\n    ```sh\n    sudo apt-get install -y libpq-dev\n    ```\n    or\n    ```sh\n    brew install postgresql\n    ```\n\n3. Optional. If you're using Microsoft SQL Server (`mssql`) or Dremio (`dremio`), run one of the following commands to install `unixodbc`, which is required to install the `pyodbc` Python package:\n\n    ```sh\n    sudo apt-get install -y unixodbc-dev\n    ```\n    or\n    ```sh\n    brew install unixodbc\n    ```\n\n    If your Mac computer has an Apple Silicon chip, you might need to\n\n    1. specify additional compiler or linker options. For example:\n\n    ```sh\n    export LDFLAGS=\"-L/opt/homebrew/Cellar/unixodbc/[your version]/lib\"\n    export CPPFLAGS=\"-I/opt/homebrew/Cellar/unixodbc/[your version]/include\"`\n    ```\n\n    2. reinstall pyodbc:\n\n    ```\n    python -m pip install --force-reinstall --no-binary :all: pyodbc\n    python -c \"import pyodbc; print(pyodbc.version)\"\n    ```\n\n    3. install the ODBC 17 driver: https://learn.microsoft.com/en-us/sql/connect/odbc/linux-mac/install-microsoft-odbc-driver-sql-server-macos?view=sql-server-ver15\n\n    4. see the following resources for more information:\n\n    - [Installing Microsoft ODBC driver for macOS](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/install-microsoft-odbc-driver-sql-server-macos)\n    - [Installing Microsoft ODBC driver for Linux](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server)\n\n4. Add `ulimit -n 4096` to the `~/.zshrc` or `~/.bashrc` files to prevent `OSError: [Errno 24] Too many open files` errors.\n\n5. Run the following command to confirm pandas and SQLAlchemy with SQLite tests are passing:\n\n   ```sh\n    ulimit -n 4096\n\n    pytest -v\n   ```\n\n## Configure backends for testing (optional)\n\nSome Great Expectations features require specific backends for local testing.\n\n### Prerequisites\n\n- Docker (PostgreSQL and MySQL). See [Get Docker](https://docs.docker.com/get-docker/).\n\n### PostgreSQL\n\n1. CD to `assets/docker/postgresql` in your `great_expectations` repository, and then and run the following command:\n\n    ```sh\n    docker-compose up -d\n    ```\n2. Run the following command to verify the PostgreSQL container is running:\n\n    ```sh\n    docker-compose ps\n    ```\n    The command should return results similar to the following example:\n\n    ````console\n\t\tName                       Command              State           Ports\n\t———————————————————————————————————————————\n\tpostgresql_travis_db_1   docker-entrypoint.sh postgres   Up      0.0.0.0:5432->5432/tcp\n\t````\n3. Run the following command to run tests on the PostgreSQL container:\n\n    ```sh\n    pytest -v --postgresql\n    ```\n\n4. When you finish testing, run the following command to shut down the PostgreSQL container:\n\n     ```sh\n    docker-compose down\n    ```\n#### Troubleshooting\n\nErrors similar to the following are returned when you try to start the PostgreSQL container and another service is using port 5432:\n\n````console\n\tpsycopg2.OperationalError: could not connect to server: Connection refused\n\t    Is the server running on host \"localhost\" (::1) and accepting\n\t    TCP/IP connections on port 5432?\n\tcould not connect to server: Connection refused\n\t    Is the server running on host \"localhost\" (127.0.0.1) and accepting\n\t    TCP/IP connections on port 5432?\n````\n````console\n\tsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) FATAL:  database \"test_ci\" does not exist\n\t(Background on this error at: http://sqlalche.me/e/e3q8)\n````\nTo resolve these errors, configure Docker to run on another port and confirm the server details are correct.\n\n### MySQL\n\nIf another service is using port 3306, Docker might start the container but silently fail to set up the port.\n\n1. CD to `assets/docker/mysql` in your `great_expectations` repository, and then and run the following command:\n\n    ```sh\n    docker-compose up -d\n    ```\n2. Run the following command to verify the MySQL container is running:\n\n    ```sh\n    docker-compose ps\n    ```\n    The command should return results similar to the following example:\n\n    ```console\n\t      Name                   Command             State                 Ports\n\t------------------------------------------------------------------------------------------\n\tmysql_mysql_db_1   docker-entrypoint.sh mysqld   Up      0.0.0.0:3306->3306/tcp, 33060/tcp\n\t````\n3. Run the following command to run tests on the MySQL container:\n\n    ```sh\n    pytest -v --mysql\n    ```\n\n4. When you finish testing, run the following command to shut down the MySQL container:\n\n    ```sh\n    docker-compose down\n    ```\n\n### Spark\n\nUse the following information to use Spark for code testing.\n\n#### Prerequisites\n\n- Java. See [Java downloads](https://www.java.com/en/download/).\n\n- The PATH or JAVA_HOME environment variables set correctly. See [Setting Java variables in Windows](https://www.ibm.com/docs/en/b2b-integrator/5.2?topic=installation-setting-java-variables-in-windows) or [Setting Java variables in Linux](https://www.ibm.com/docs/en/b2b-integrator/5.2?topic=installation-setting-java-variables-in-linux).\n\nOn Mac, run the following commands to set the PATH and JAVA_HOME environment variables:\n```\nexport JAVA_HOME=`/usr/libexec/java_home`\nexport PATH=$PATH:$JAVA_HOME/bin\n```\n\n\n#### Install PySpark\n\nWhen you install PySpark, Spark is also installed. See [Spark Overview](https://spark.apache.org/docs/latest/index.html#downloading).\n\nRun the following command to install PySpark and Apache Spark:\n\n```console\npip install pyspark\n```\n\n## Test code changes\n\nGreat Expectations production code must be thoroughly tested, and you must perform unit testing on all branches of every method, including likely error states. Most new feature contributions should include multiple unit tests. Contributions that modify or extend existing features should include a test of the new behavior.\n\nMost contributions do not require new integration tests, unless they change the Great Expectations CLI.\n\nGreat Expectations code is not tested against all SQL database types. Continuous Integration (CI) testing for SQL is limited to PostgreSQL, SQLite, MS SQL, and BigQuery.\n\n### Unit testing\n\nTo perform unit testing, run `pytest` in the `great_expectations` directory root. By default, tests are run against `pandas` and `sqlite`. You can use `pytest` flags to test additional backends like `postgresql`, `spark`, and `mssql`. For example, to run a test against PostgreSQL backend, you run `pytest --postgresql`.\n\nThe following are the supported `pytest` flags for general testing:\n\n- `--spark`: Execute tests against Spark backend.\n- `--postgresql`: Execute tests against PostgreSQL.\n- `--mysql`: Execute tests against MySql.\n- `--mssql`: Execute tests against Microsoft SQL Server.\n- `--bigquery`: Execute tests against Google BigQuery (requires additional set up).\n- `--aws`: Execute tests against AWS resources such as Amazon S3, Amazon Redshift, and Athena (requires additional setup).\n\nTo skip all local backend tests (except pandas), run `pytest --no-sqlalchemy`.\n\nTesting can generate warning messages. These warnings are often caused by dependencies such as pandas or SQLAlchemy. Run `pytest --no-sqlalchemy --disable-pytest-warnings` to suppress these warnings.\n\n### Marking tests\n\nAll tests in Great Expectations must include one marker from the `REQUIRED_MARKERS` list. To view the list of defined markers, see [tests/conftest.py](https://github.com/great-expectations/great_expectations/blob/develop/tests/conftest.py).\nTo verify each test is marked, run `invoke marker-coverage` if [invoke](https://pypi.org/project/invoke/) is installed, or run `pytest --verify-marker-coverage-and-exit`.\nWhen verification fails, a list of unmarked tests and the required markers appears.\n\n### BigQuery testing\n\n1. [Select or create a Cloud Platform project](https://console.cloud.google.com/project).\n\n2. [Setup Authentication](https://googleapis.dev/python/google-api-core/latest/auth.html).\n\n3. In your project, [create a BigQuery dataset](https://cloud.google.com/bigquery/docs/datasets) named `test_ci` and [set the dataset default table expiration](https://cloud.google.com/bigquery/docs/updating-datasets#table-expiration) to `.1` day.\n\n4. Run the following command to test your project with the `GE_TEST_BIGQUERY_PROJECT` and `GE_TEST_BIGQUERY_DATASET` environment variables:\n\n    ```bash\n    GE_TEST_BIGQUERY_PROJECT=<YOUR_GOOGLE_CLOUD_PROJECT>\n    GE_TEST_BIGQUERY_DATASET=test_ci\n    pytest tests/test_definitions/test_expectations_cfe.py --bigquery\n    ```\n\n### Unit testing Expectations\n\nOne of the most significant features of an Expectation is that it produces the same result on all supported execution environments including pandas, SQLAlchemy, and Spark. To accomplish this, Great Expectations encapsulates unit tests for Expectations as JSON files. These files are used as fixtures and executed using a specialized test runner that executes tests against all execution environments.\n\nThe following is the test fixture file structure:\n\n```json\n{\n    \"expectation_type\" : \"expect_column_max_to_be_between\",\n    \"datasets\" : [{\n        \"data\" : {...},\n        \"schemas\" : {...},\n        \"tests\" : [...]\n    }]\n}\n```\n\nBelow `datasets` are three entries: `data`, `schemas`, and `tests`.\n\n#### Data\n\nThe `data` parameter defines a DataFrame of sample data to apply Expectations against. The DataFrame is defined as a dictionary of lists, with keys containing column names and values containing lists of data entries. All lists within a dataset must have the same length. For example:\n\n```console\n\"data\" : {\n    \"w\" : [1, 2, 3, 4, 5, 5, 4, 3, 2, 1],\n    \"x\" : [2, 3, 4, 5, 6, 7, 8, 9, null, null],\n    \"y\" : [1, 1, 1, 2, 2, 2, 3, 3, 3, 4],\n    \"z\" : [\"a\", \"b\", \"c\", \"d\", \"e\", null, null, null, null, null],\n    \"zz\" : [\"1/1/2016\", \"1/2/2016\", \"2/2/2016\", \"2/2/2016\", \"3/1/2016\", \"2/1/2017\", null, null, null, null],\n    \"a\" : [null, 0, null, null, 1, null, null, 2, null, null],\n},\n```\n\n#### Schemas\n\nThe `schema` parameter defines the types to be used when instantiating tests against different execution environments, including different SQL dialects. Each schema is defined as a dictionary with column names and types as key-value pairs. If the schema isn’t specified for a given execution environment, Great Expectations introspects values and attempts to identify the schema. For example:\n\n```console\n\"schemas\": {\n    \"sqlite\": {\n        \"w\" : \"INTEGER\",\n        \"x\" : \"INTEGER\",\n        \"y\" : \"INTEGER\",\n        \"z\" : \"VARCHAR\",\n        \"zz\" : \"DATETIME\",\n        \"a\" : \"INTEGER\",\n    },\n    \"postgresql\": {\n        \"w\" : \"INTEGER\",\n        \"x\" : \"INTEGER\",\n        \"y\" : \"INTEGER\",\n        \"z\" : \"TEXT\",\n        \"zz\" : \"TIMESTAMP\",\n        \"a\" : \"INTEGER\",\n    }\n},\n```\n#### Tests\n\nThe `tests` parameter defines the tests to be executed against the DataFrame. Each item in `tests` must include `title`, `exact_match_out`, `in`, and `out`. The test runner executes the named Expectation once for each item, with the values in `in` supplied as kwargs.\n\nThe test passes if the values in the expectation Validation Result correspond with the values in `out`. If `exact_match_out` is true, then every field in the Expectation output must have a corresponding, matching field in `out`. If it’s false, then only the fields specified in `out` need to match. For most use cases, false is a better result, because it allows narrower targeting of the relevant output.\n\n`suppress_test_for` is an optional parameter to disable an Expectation for a specific list of backends. For example:\n\n```sh\n\"tests\" : [{\n    \"title\": \"Basic negative test case\",\n    \"exact_match_out\" : false,\n    \"in\": {\n        \"column\": \"w\",\n        \"result_format\": \"BASIC\",\n        \"min_value\": null,\n        \"max_value\": 4\n    },\n    \"out\": {\n        \"success\": false,\n        \"observed_value\": 5\n    },\n    \"suppress_test_for\": [\"sqlite\"]\n},\n...\n]\n\n```\n\nThe test fixture files are stored in subdirectories of `tests/test_definitions/` corresponding to the class of Expectation:\n\n- column_map_expectations\n- column_aggregate_expectations\n- column_pair_map_expectations\n- column_distributional_expectations\n- multicolumn_map_expectations\n- other_expectations\n\nBy convention, the name of the file is the name of the Expectation, with a .json suffix. Creating a new JSON file automatically adds the new Expectation tests to the test suite.\n\nIf you are implementing a new Expectation, but don’t plan to immediately implement it for all execution environments, you should add the new test to the appropriate lists in the `candidate_test_is_on_temporary_notimplemented_list_v2_api` method within `tests/test_utils.py`.\n\nYou can run just the Expectation tests with `pytest tests/test_definitions/test_expectations.py`.\n\n## Test performance\n\nTest the performance of code changes to determine they perform as expected. BigQuery is required to complete performance testing.\n\n1. Run the following command to set up the data for testing:\n\n    ```bash\n    GE_TEST_BIGQUERY_PEFORMANCE_DATASET=<YOUR_GCP_PROJECT> tests/performance/setup_bigquery_tables_for_performance_test.sh\n    ```\n\n2. Run the following command to start the performance test:\n\n    ```sh\n    pytest tests/performance/test_bigquery_benchmarks.py \\\n    --bigquery --performance-tests \\\n    -k 'test_taxi_trips_benchmark[1-True-V3]'  \\\n    --benchmark-json=tests/performance/results/`date \"+%H%M\"`_${USER}.json \\\n    -rP -vv\n    ```\n\n    Some benchmarks take significant time to complete. In the previous example, only the `test_taxi_trips_benchmark[1-True-V3]` benchmark runs. The output should appear similar to the following:\n\n    ```console\n    --------------------------------------------------- benchmark: 1 tests ------------------------------------------------------\n    Name (time in s)                         Min     Max    Mean  StdDev  Median     IQR  Outliers     OPS  Rounds  Iterations\n    -----------------------------------------------------------------------------------------------------------------------------\n    test_taxi_trips_benchmark[1-True-V3]     5.0488  5.0488  5.0488  0.0000  5.0488  0.0000       0;0  0.1981       1           1\n    -----------------------------------------------------------------------------------------------------------------------------\n    ```\n3. Run the following command to compare the test results:\n\n    ```\n    $ py.test-benchmark compare --group-by name tests/performance/results/initial_baseline.json tests/performance/results/*${USER}.json\n    ```\n\n    The output should appear similar to the following:\n\n    ```console\n    ---------------------------------------------------------------------------- benchmark 'test_taxi_trips_benchmark[1-True-V3]': 2 tests ---------------------------------------------------------------------------\n    Name (time in s)                                        Min               Max              Mean            StdDev            Median               IQR            Outliers     OPS            Rounds  Iterations\n    ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    test_taxi_trips_benchmark[1-True-V3] (initial_base)     5.0488 (1.0)      5.0488 (1.0)      5.0488 (1.0)      0.0000 (1.0)      5.0488 (1.0)      0.0000 (1.0)           0;0  0.1981 (1.0)           1           1\n    test_taxi_trips_benchmark[1-True-V3] (2114_work)        6.4675 (1.28)     6.4675 (1.28)     6.4675 (1.28)     0.0000 (1.0)      6.4675 (1.28)     0.0000 (1.0)           0;0  0.1546 (0.78)          1           1\n    ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    ```\n\n4. Optional. If your change is intended to improve performance, run the following command to generate the benchmark results that confirm the performance improvement:\n\n    ```\n    $ tests/performance/run_benchmark_multiple_times.sh minimal_multithreading\n    ```\n     The name for the tests should include the first argument provided to the script. In the previous example, this was `tests/performance/results/minimal_multithreading_*.json`.\n\n## Submit a pull request\n\n1. Push your changes to the remote fork of your repository.\n\n2. Create a pull request from your fork. See [Creating a pull request from a fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork).\n\n3. Add a meaningful title and description for your pull request (PR). Provide a detailed explanation of what you changed and why.  To help identify the type of issue you’re submitting, add one of the following identifiers to the pull request (PR) title:\n\n    - [BUGFIX] for PRs that address minor bugs without changing behavior.\n\n    - [FEATURE] for significant PRs that add a new feature likely to require being added to our feature maturity matrix.\n\n    - [MAINTENANCE] for PRs that focus on updating repository settings or related changes.\n\n    - [CONTRIB] for the contribution of Custom Expectations and supporting work into the `contrib/` directory.\n\n    - [HACKATHON] for submissions to an active Great Expectations Hackathon.\n\n4. In the section for design review, include a description of any prior discussion or coordination on the features in the PR, such as mentioning the number of the issue where discussion has taken place. For example: Closes #123”, linking to a relevant [Discourse](https://discourse.greatexpectations.io/) or Slack article, citing a team meeting, or even noting that no discussion is relevant because the issue is small.\n\n5. If this is your first Great Expectations contribution, you'll be prompted to complete the Contributor License Agreement (CLA). Complete the CLA and add `@cla-bot check` as a comment to the pull request (PR) to indicate that you’ve completed it.\n\n6. Wait for the Continuous Integration (CI) checks to complete and then correct any syntax or formatting issues.\n\n7. A Great Expectations team member reviews, approves, and merges your PR. Depending on your GitHub notification settings, you'll be notified when there are comments or when your changes are successfully merged.\n\n## Issue labels\n\nGreat Expectations uses a `stalebot` to automatically tag issues without activity as `stale`, and closes them when a response is not received within a week. To prevent `stalebot` from closing an issue, you can add the `stalebot-exempt` label.\n\nAdditionally, Great Expectations adds the following labels to indicate issue status:\n\n- `help wanted`: identifies useful issues that require help from community contributors to accelerate development\n\n- `enhacement` and `expectation-request`: identify new Great Expectations features that require additional investigation and discussion\n\n- `good first issue`: identifies issues that provide an introduction to the Great Expectations contribution process\n\nWe also have labels to indicate the level of support you can expect for each issue. They are as follows:\n- `community-supported`:  related to a part of the code-base that is not tested and actively maintained with new GX Core or GX Cloud releases; however, we actively welcome ongoing maintenance from the community\n- `not-supported`: an issue that we at GX will not be maintaining, and we will not support PRs or contributions from the community on the topic\n\nIssues without either a `community-supported` or `not-supported` label can be assumed to be **GX-supported**, which means they are related to a part of the code-base that is tested and actively maintained with new GX Core or GX Cloud releases.\n\n## Contributor license agreement (CLA)\n\n*When you contribute code, you affirm that the contribution is your original work and that you license the work to the project under the project’s open source license. Whether or not you state this explicitly, by submitting any copyrighted material via pull request, email, or other means you agree to license the material under the project’s open source license and warrant that you have the legal authority to do so.*\n\nPlease make sure you have signed our Contributor License Agreement (either [Individual Contributor License Agreement v1.0](https://docs.google.com/forms/d/e/1FAIpQLSdA-aWKQ15yBzp8wKcFPpuxIyGwohGU1Hx-6Pa4hfaEbbb3fg/viewform?usp=sf_link) or [Software Grant and Corporate Contributor License Agreement (“Agreement”) v1.0)](https://docs.google.com/forms/d/e/1FAIpQLSf3RZ_ZRWOdymT8OnTxRh5FeIadfANLWUrhaSHadg_E20zBAQ/viewform?usp=sf_link).\n\nWe are not asking you to assign copyright to us, but to give us the right to distribute your code without restriction. We ask this of all contributors in order to assure our users of the origin and continuing existence of the code. You only need to sign the CLA once.\n"
        },
        {
          "name": "CONTRIBUTING_EXPECTATIONS.md",
          "type": "blob",
          "size": 8.6435546875,
          "content": "# Contribute Custom Expectations\n\nTo submit a Custom Expectation to Great Expectations for consideration, you complete the following tasks:\n\n- [Fork and clone the Great Expectations repository](#fork-and-clone-the-great-expectations-repository)\n\n- [Generate the Expectation validation checklist](#generate-the-expectation-validation-checklist)\n\n- [Verify Expectation metadata](#verify-expectation-metadata)\n\n- [Submit a pull request](#submit-a-pull-request)\n\nTo request a documentation change, or a change that doesn't require local testing, see the [README](https://github.com/great-expectations/great_expectations/blob/develop/docs/README.md) in the `docs` repository.\n\nTo create and submit a custom package to Great Expectations for consideration, see [CONTRIBUTING_PACKAGES](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_PACKAGES.md) in the `great_expectations` repository.\n\nTo submit a code change to Great Expectations for consideration, see [CONTRIBUTING_CODE](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_CODE.md) in the `great_expectations` repository.\n\n## Prerequisites\n\n- Great Expectations installed and configured for your environment. See [Great Expectations Quickstart](https://docs.greatexpectations.io/docs/tutorials/quickstart/).\n\n- A Custom Expectation. See [Create and manage Custom Expectations](https://docs.greatexpectations.io/docs/guides/expectations/custom_expectations_lp).\n\n- A GitHub account.\n\n- A working version of Git on your computer. See [Getting Started - Installing Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n\n- A new SSH (Secure Shell Protocol) key. See [Generating a new SSH key and adding it to the ssh-agent](https://help.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).\n\n- The latest Python version installed and configured. See [Python downloads](https://www.python.org/downloads/).\n\n## Fork and clone the Great Expectations repository\n\n1. Open a browser and go to the [Great Expectations repository](https://github.com/great-expectations/great_expectations).\n\n2. Click **Fork** and then **Create Fork**.\n\n3. Click **Code** and then select the **HTTPS** or **SSH** tabs.\n\n4. Copy the URL, open a Git terminal, and then run the following command:\n\n    ```sh\n    git clone <url>\n    ```\n5. Run the following command to specify a new remote upstream repository that will be synced with the fork:\n\n    ```sh\n    git remote add upstream git@github.com:great-expectations/great_expectations.git\n    ```\n6. Run the following command to create a branch for your changes:\n\n    ```sh\n    git checkout -b <branch-name>\n    ```\n\n## Generate the Expectation validation checklist\n\nBefore you submit your Custom Expectation, you need to verify it meets the submission requirements. Great Expectations provides a checklist to help you determine if your Custom Expectation meets the minimum requirements. Your Custom Expectation must meet the first five of the listed requirements to qualify for submission.\n\nTo generate the Expectation checklist, add the `print_diagnostic_checklist()` instance method to your Custom Expectation. When the instance method runs, it returns results similar to the following:\n\n```console\n✔ Has a valid library_metadata object\n✔ Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period\n  ...\n✔ Has at least one positive and negative example case, and all test cases pass\n✔ Has core logic and passes tests on at least one Execution Engine\n  ...\n✔ Passes all linting checks\n✔ Has basic input validation and type checking\n✔ Has both statement Renderers: prescriptive and diagnostic\n✔ Has core logic that passes tests for all applicable Execution Engines and SQL dialects\n  ...\n  Has a full suite of tests, as determined by project code standards\n  Has passed a manual review by a code owner for code standards and style guides\n```\n\n## Verify Expectation metadata\n\nVerifying your Custom Expectation metadata ensures that it is accredited to you and includes an accurate description.\n\nGreat Expectations maintains a number of Custom Expectation packages, that contain thematically related Custom Expectations. These packages are located in the [Great Expectations contrib directory](https://github.com/great-expectations/great_expectations/tree/develop/contrib) and on [PyPI](https://pypi.org/).\nIf your Custom Expectation fits within one of these packages, you're encouraged to contribute your Custom Expectation directly to one of these packages.\n\nIf you're not contributing to a specific package, your Custom Expectation is automatically published in the [PyPI great-expectations-experimental package](https://pypi.org/project/great-expectations-experimental/). This package contains all Great Expectations experimental community-contributed Custom Expectations that have not been submitted to other packages.\n\nConfirm the `library_metadata` object for your Custom Expectation includes the following information:\n\n- `contributors`: Identifies the creators of the Custom Expectation.\n- `tags`: Identifies the Custom Expectation functionality and domain. For example, `statistics`, `flexible comparisons`, `geography`, and so on.\n- `requirements`: Identifies if your Custom Expectation relies on third-party packages.\n\n## Submit a pull request\n\n1. Push your changes to the remote fork of your repository.\n\n2. Create a pull request from your fork. See [Creating a pull request from a fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork).\n\n3. Add a meaningful title and description for your pull request (PR). Provide a detailed explanation of what you changed and why. To help identify the type of issue you’re submitting, add one of the following identifiers to the pull request (PR) title:\n\n    - [BUGFIX] for PRs that address minor bugs without changing behavior.\n\n    - [FEATURE] for significant PRs that add a new feature likely to require being added to our feature maturity matrix.\n\n    - [MAINTENANCE] for PRs that focus on updating repository settings or related changes.\n\n    - [CONTRIB] for the contribution of Custom Expectations and supporting work into the `contrib/` directory.\n\n    - [HACKATHON] for submissions to an active Great Expectations Hackathon.\n\n    In the section for design review, include a description of any prior discussion or coordination on the features in the PR, such as mentioning the number of the issue where discussion has taken place. For example: Closes #123”, linking to a relevant discuss or slack article, citing a team meeting, or even noting that no discussion is relevant because the issue is small.\n\n4. If this is your first Great Expectations contribution, you'll be prompted to complete the Contributor License Agreement (CLA). Complete the CLA and add `@cla-bot check` as a comment to the pull request (PR) to indicate that you’ve completed it.\n\n5. Wait for the Continuous Integration (CI) checks to complete and then correct any syntax or formatting issues.\n\n    A Great Expectations team member reviews, approves, and merges your PR. Depending on your GitHub notification settings, you'll be notified when there are comments or when your changes are successfully merged.\n\n    If your Custom Expectation doesn't meet the minimum requirements in the validation checklist, it is failing testing, or there is a functionality error, you'll be asked to resolve the issues before your Custom Expectation can move forward.\n\n    If you are submitting a production Custom Expectation, Great Expectations requires that your Custom Expectation meet or exceed Great Expectation standards for testing and coding.\n\n    When your Custom Expectation has successfully passed testing and received approval from a code owner, your contribution is complete. Your custom Expectation will be included in the next release of Great Expectations and an announcement will appear in the release notes\n\n## Issue tags\n\nGreat Expectations uses a `stalebot` to automatically tag issues without activity as `stale`, and closes them when a response is not received within a week. To prevent `stalebot` from closing an issue, you can add the `stalebot-exempt` tag.\n\nAdditionally, Great Expectations adds the following tags to indicate issue status:\n\n- The`help wanted` tag identifies useful issues that require help from community contributors to accelerate development.\n\n- The `enhacement` and `expectation-request` tags identify new Great Expectations features that require additional investigation and discussion.\n\n- The `good first issue` tag identifies issues that provide an introduction to the Great Expectations contribution process.\n"
        },
        {
          "name": "CONTRIBUTING_PACKAGES.md",
          "type": "blob",
          "size": 6.2294921875,
          "content": "# Contribute packages\n\nTo submit a custom Great Expectations package, you complete the following tasks:\n\n- [Contact the Great Expectations Developer Relations team](#contact-the-great-expectations-developer-relations-team)\n\n- [Install the Great Expectations CLI tool](#install-the-great-expectations-cli-tool)\n\n- [Initialize a project](#initialize-a-project)\n\n- [Contribute to your package](#contribute-to-your-package)\n\n- [Publish your package](#publish-your-package)\n\nTo request a documentation change, or a change that doesn't require local testing, see the [README](https://github.com/great-expectations/great_expectations/blob/develop/docs/README.md) in the `docs` repository.\n\nTo create and submit a Custom Expectation to Great Expectations for consideration, see [CONTRIBUTING_EXPECTATIONS](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_EXPECTATIONS.md) in the `great_expectations` repository.\n\nTo submit a code change to Great Expectations for consideration, see [CONTRIBUTING_CODE](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_CODE.md) in the `great_expectations` repository.\n\n## Prerequisites\n\n- Great Expectations installed and configured for your environment. See [Great Expectations Quickstart](https://docs.greatexpectations.io/docs/tutorials/quickstart/).\n\n- A Custom Expectation. See [Create and manage Custom Expectations](https://docs.greatexpectations.io/docs/guides/expectations/custom_expectations_lp).\n\n- A GitHub account.\n\n- A working version of Git on your computer. See [Getting Started - Installing Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n\n- A new SSH (Secure Shell Protocol) key. See [Generating a new SSH key and adding it to the ssh-agent](https://help.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).\n\n- The latest Python version installed and configured. See [Python downloads](https://www.python.org/downloads/).\n\n- A PyPI account. See [Create an account on PyPI](https://pypi.org/account/register/).\n\n## Contact the Great Expectations Developer Relations team\n\nBefore you develop your custom Great Expectations package, notify the Great Expectations Developer Relations team of your plans in the [Great Expectations #integrations Slack channel](https://greatexpectationstalk.slack.com/archives/C037YCYNF1Q). A member of the team will discuss your custom Great Expectations package, support your development, and help you navigate the publication and maintenance process.\n\n## Install the Great Expectations CLI tool\n\nGreat Expectations provides the `great_expectations_contrib` command-line interface (CLI) tool to simplify the package contribution process and help you adhere to code base best practices.\n\nThe `great_expectations_contrib` CLI tool performs the following functions:\n\n- Initializes your package structure\n\n- Performs a series of checks to validate your package\n\n- Publishes your package to PyPi\n\nWhile you're developing your package, you must use this tool to ensure that it meets the necessary requirements.\n\n1. Run the following command to cd to the root of the `great_expectations` codebase:\n\n    ```bash\n    cd contrib/cli\n    ```\n\n2. Run the following command to install the CLI tool:\n\n    ```bash\n    pip install -e .\n    ```\n\n3. Run the following command to verify your installation:\n\n    ```bash\n    great_expectations_contrib\n    ```\n\n## Initialize a project\n\nAfter you've enabled the `great_expectations_contrib` CLI tool, you need to initialize an empty package.\n\n1. Run the following command to initialize an empty package:\n\n    ```bash\n    great_expectations_contrib init\n    ```\n\n2. Enter the name of your package, the purpose of your package, and your GitHub and PyPI usernames.\n\n3. Run the following command to access your configured package:\n\n    ```bash\n    cd <PACKAGE_NAME>\n    tree\n    ```\n    The command returns a file structure similar to the following example:\n\n    ```bash\n    .\n    ├── LICENSE\n    ├── README.md\n    ├── assets\n    ├── package_info.yml\n    ├── requirements.txt\n    ├── setup.py\n    ├── tests\n    │   ├── __init__.py\n    │   ├── expectations\n    │   │   └── __init__.py\n    │   ├── metrics\n    │   │   └── __init__.py\n    │   └── profilers\n    │       └── __init__.py\n    └── <YOUR_PACKAGE_SOURCE_CODE>\n        ├── __init__.py\n        ├── expectations\n        │   └── __init__.py\n        ├── metrics\n        │   └── __init__.py\n        └── profilers\n            └── __init__.py\n    ```\n    To ensure consistency with other packages, maintain this general structure as you develop you package.\n\n## Contribute to your package\n\n1. Record any dependencies in your `requirements.txt` file, validate your code\nin `tests`, describe your package capabilities in `README.md`, and update publishing details in `setup.py`.\n\n2. Optional. Update package metadata and assign code owners or domain experts. See\n`package_info.yml`.\n\n3. Run the following command to run checks on your package including code formatting, annotated function signature availability, and Expectation documentation:\n\n    ```bash\n    great_expectations_contrib check\n    ```\n\n## Publish your package\n\nPublish your package after you have tested and verified its behavior and documented its capabilities.\n\n1. Run the following command to publish your package:\n\n    ```bash\n    great_expectations_contrib publish\n    ```\n2. Enter your PyPI username and password.\n\n## Issue tags\n\nGreat Expectations uses a `stalebot` to automatically tag issues without activity as `stale`, and closes them when a response is not received within a week. To prevent `stalebot` from closing an issue, you can add the `stalebot-exempt` tag.\n\nAdditionally, Great Expectations adds the following tags to indicate issue status:\n\n- The`help wanted` tag identifies useful issues that require help from community contributors to accelerate development.\n\n- The `enhacement` and `expectation-request` tags identify new Great Expectations features that require additional investigation and discussion.\n\n- The `good first issue` tag identifies issues that provide an introduction to the Great Expectations contribution process.\n"
        },
        {
          "name": "CONTRIBUTING_WORKFLOWS.md",
          "type": "blob",
          "size": 1.375,
          "content": "# Workflows\n\nThe Great Expectations code base has various places where you can contribute code to. This document describes several workflows you might want to run to get started.\n\nFirst, make sure you have cloned the repository and installed the Python dependencies. Read more on this in [Contribute a code change](CONTRIBUTING_CODE.md).\n\nThis code base provides following workflows:\n\n- [Code Linting](#code-linting)\n- [Locally deploy docs](#locally-deploy-docs)\n- [Verify links in docs](#verify-links-in-docs)\n- [Generate Glossary](#generate-glossary)\n\n## Code Linting\n\nBefore submitting a pull request, make sure that your code passes the lint check, for that run:\n\n```sh\nblack .\nruff . --fix\n```\n\n## Locally Deploy Docs\n\nYou can find more information on developing Great Expectation docs in [/docs/docusaurus/README.md](/docs/docusaurus/README.md). To get a version of the docs deployed locally, run:\n\n```sh { name=docs background=false }\ninvoke docs\n```\n\nThe website should be available at:\n\n```sh\nopen http://localhost:3000/docs\n```\n\n## Verify links in docs\n\nWe use a link checker tool to verify that links within our docs are valid, you can run it via:\n\n```sh { name=linkcheck }\npython3 docs/checks/docs_link_checker.py -p docs -r docs -s docs --skip-external\n```\n\n## Generate Glossary\n\nGenerates a glossary page in our docs:\n\n```sh { name=glossary cwd=./scripts }\npython3 ./build_glossary_page.py\n```\n"
        },
        {
          "name": "IDE_SETUP_TIPS.md",
          "type": "blob",
          "size": 1.6201171875,
          "content": "# IDE Setup Tips\n\nThis document describes useful set-up tips for contributors to this repository. Feel free to suggest more useful changes to this.\n\n## VS Code\n\nCreate a `.vscode` directory and add the following files to it:\n\n_.vscode/extension.json_\n```json\n{\n    // See https://go.microsoft.com/fwlink/?LinkId=827846 to learn about workspace recommendations.\n\t// Extension identifier format: ${publisher}.${name}. Example: vscode.csharp\n\t// List of extensions which should be recommended for users of this workspace.\n\t\"recommendations\": [\n\t\t\"stateful.runme\"\n\t],\n\t// List of extensions recommended by VS Code that should not be recommended for users of this workspace.\n\t\"unwantedRecommendations\": []\n}\n```\n\n_.vscode/launch.json_\n```json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"GX Docusarus Docs\",\n            \"type\": \"node-terminal\",\n            \"request\": \"launch\",\n            \"command\": \"invoke docs\"\n        },\n        {\n            \"name\": \"GX Start MySQL Container\",\n            \"type\": \"node-terminal\",\n            \"request\": \"launch\",\n            \"command\": \"docker-compose up -d\",\n            \"cwd\": \"${workspaceFolder}/assets/docker/mysql\"\n        },\n        {\n            \"name\": \"GX Start PostgreSQL Container\",\n            \"type\": \"node-terminal\",\n            \"request\": \"launch\",\n            \"command\": \"docker-compose up -d\",\n            \"cwd\": \"${workspaceFolder}/assets/docker/postgresql\"\n        }\n    ]\n}\n```\n\n_.vscode/settings.json_\n```json\n{\n    \"workbench.editorAssociations\": {\n        \"CONTRIBUTING_WORKFLOWS.md\": \"runme\",\n        \"CONTRIBUTING_CODE.md\": \"runme\"\n    }\n}\n```\n\n## PyCharm\n\ntbd.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.3984375,
          "content": "include *.txt\ninclude reqs/*.txt\ninclude LICENSE\ninclude great_expectations/data_context/checkpoint_template.yml\ninclude great_expectations/init_notebooks/*/*.ipynb\nrecursive-include great_expectations/render *.j2 *.md *.py\nrecursive-include great_expectations *.pyi\ngraft great_expectations/render/view/static\ngraft tests\nglobal-exclude *.py[co]\ninclude versioneer.py\ninclude great_expectations/_version.py\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.7216796875,
          "content": "[![Python Versions](https://img.shields.io/pypi/pyversions/great_expectations.svg)](https://pypi.python.org/pypi/great_expectations)\n[![PyPI](https://img.shields.io/pypi/v/great_expectations)](https://pypi.org/project/great-expectations/#history)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/great-expectations)](https://pypistats.org/packages/great-expectations)\n[![Build Status](https://img.shields.io/azure-devops/build/great-expectations/bedaf2c2-4c4a-4b37-87b0-3877190e71f5/1)](https://dev.azure.com/great-expectations/great_expectations/_build/latest?definitionId=1&branchName=develop)\n[![pre-commit.ci Status](https://results.pre-commit.ci/badge/github/great-expectations/great_expectations/develop.svg)](https://results.pre-commit.ci/latest/github/great-expectations/great_expectations/develop)\n[![codecov](https://codecov.io/gh/great-expectations/great_expectations/graph/badge.svg?token=rbHxgTxYTs)](https://codecov.io/gh/great-expectations/great_expectations)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5683574.svg)](https://doi.org/10.5281/zenodo.5683574)\n[![Twitter Follow](https://img.shields.io/twitter/follow/expectgreatdata?style=social)](https://twitter.com/expectgreatdata)\n[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://greatexpectations.io/slack)\n[![Contributors](https://img.shields.io/github/contributors/great-expectations/great_expectations)](https://github.com/great-expectations/great_expectations/graphs/contributors)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json)](https://github.com/charliermarsh/ruff)\n\n<!-- <<<Super-quickstart links go here>>> -->\n\n<img align=\"right\" src=\"./docs/docusaurus/static/img/gx-mark-160.png\">\n\n## About GX Core\n\nGX Core is the engine of the GX platform. It combines the collective wisdom of thousands of community members with a proven track record in data quality deployments worldwide, wrapped into a super-simple package for data teams.\n\nIts powerful technical tools start with Expectations: expressive and extensible unit tests for your data. Expectations foster collaboration by giving teams a common language to express data quality tests in an intuitive way. You can automatically generate documentation for each set of validation results, making it easy for everyone to stay on the same page. This not only simplifies your data quality processes, but helps preserve your organization’s institutional knowledge about its data.\n\nLearn more about how data teams are using GX Core in our featured [case studies](https://greatexpectations.io/case-studies/).\n\n## Integration support policy\n\nGX Core supports Python `3.9` through `3.12`.\nExperimental support for Python `3.13` and later can be enabled by setting a `GX_PYTHON_EXPERIMENTAL` environment variable when installing `great_expectations`.\n\nFor data sources and other integrations that GX supports, see [GX integration support policy](https://docs.greatexpectations.io/docs/application_integration_support) for additional information.\n\n## Get started\n\nGX recommends deploying GX Core within a virtual environment. For more information about getting started with GX Core, see [Introduction to GX Core](https://docs.greatexpectations.io/docs/core/introduction/).\n\n1. Run the following command in an empty base directory inside a Python virtual environment to install GX Core:\n\n\t```bash title=\"Terminal input\"\n\tpip install great_expectations\n\t```\n2. Run the following command to import the `great_expectations module` and create a Data Context:\n\n\t```python\n\timport great_expectations as gx\n\n\tcontext = gx.get_context()\n\t```\n\n## Get support from GX and the community\n\nThey are listed in the order in which GX is prioritizing the support issues:\n\n1. Issues and PRs in the [GX GitHub repository](https://github.com/great-expectations)\n2. Questions posted to the [GX Core Discourse forum](https://discourse.greatexpectations.io/c/oss-support/11)\n3. Questions posted to the [GX Slack community channel](https://greatexpectationstalk.slack.com/archives/CUTCNHN82)\n\n## Contribute\nWe deeply value the contributions of our community. We're now accepting PRs for bug fixes.\n\nTo ensure the long-term quality of the GX Core codebase, we're not yet ready to accept feature contributions to the parts of the codebase that don't have clear APIs for extensions. We're actively working to increase the surface area for contributions. Thank you being a crucial part of GX's data quality platform!\n\n### Levels of contribution readiness\n🟢 Ready. Have a clear and public API for extensions.\n\n🟡 Partially ready. Case-by-case.\n\n🔴 Not ready. Will accept contributions that fix existing bugs or workflows.\n\n| GX Component         | Readiness          | Notes |\n| -------------------- | ------------------ | ----- |\n| CredentialStore      | 🟢 Ready           |       |\n| BatchDefinition      | 🟡 Partially ready | Formerly known as splitters |\n| Action               | 🟢 Ready           |       |\n| DataSource           | 🔴 Not ready       | Includes MetricProvider and ExecutionEngine |\n| DataContext          | 🔴 Not ready       | Also known as Configuration Stores |\n| DataAsset            | 🔴 Not ready       |       |\n| Expectation          | 🔴 Not ready       |       |\n| ValidationDefinition | 🔴 Not ready       |       |\n| Checkpoint           | 🔴 Not ready       |       |\n| CustomExpectations   | 🔴 Not ready       |       |\n| Data Docs            | 🔴 Not ready       | Also known as Renderers |\n\n\n## Code of conduct\nEveryone interacting in GX Core project codebases, Discourse forums, Slack channels, and email communications is expected to adhere to the [GX Community Code of Conduct](https://discourse.greatexpectations.io/t/gx-community-code-of-conduct/1199).\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.7177734375,
          "content": "coverage:\n  status:\n    project:\n      default:\n        informational: true\n    patch:\n      default:\n        informational: true\n\nflag_management:\n  default_rules: # the rules that will be followed for any flag added, generally\n    after_n_builds: 16\n    carryforward: false\n    statuses:\n      - type: project\n        target: 80%\n        threshold: 2% # We allow the coverage to drop by this percent on a PR\n      - type: patch\n        target: 90%\n  # in general these individual flags should correspond to pytest markers\n  # individual_flags: # exceptions to the default rules above, stated flag by flag\n  # - name: some_flag\n  #   statuses:\n  #     - type: project\n  #       target: 75%\n  #     - type: patch\n  #       target: 95%\n"
        },
        {
          "name": "constraints-dev.txt",
          "type": "blob",
          "size": 0.5576171875,
          "content": "# The pip resolver became more strict starting with version 20.3\n# This constraints file is \"a way to specify global (version) limits for packages\"\n# without specifying the install of these packages.\n# See also: https://pip.pypa.io/en/stable/user_guide/#constraints-files\n# We use it to place limits on certain packages to help the resolver come to a compatible configuration more quickly\n# To install dev dependencies using the new pip resolver (recommended) please use the following syntax:\n# `python -m pip install -c constraints-dev.txt \".[test, postgresql, spark]\"`\n"
        },
        {
          "name": "contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "great_expectations",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 46.2421875,
          "content": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n# uncomment to enable pep517 after versioneer problem is fixed.\n# https://github.com/python-versioneer/python-versioneer/issues/193\n# build-backend = \"setuptools.build_meta\"\n\n[tool.mypy]\npython_version = \"3.9\"\nplugins = [\n    \"pydantic.mypy\",\n    \"sqlalchemy.ext.mypy.plugin\"\n    ]\nfiles = [\n    \"great_expectations\",\n    \"docs\",\n    \"tests\",\n    # \"contrib\" # ignore entire `contrib` package\n]\nwarn_unused_configs = true\nignore_missing_imports = false\nfollow_imports = 'normal'\nwarn_redundant_casts = true\nshow_error_codes = true\nenable_error_code = [\n    'ignore-without-code',\n    'explicit-override',\n    'possibly-undefined'\n]\n# The following list of codes are globally ignored, do not add to this list\ndisable_error_code = [\n    # annotation-unchecked are 'warning notes', not errors and won't cause mypy to fail\n    # but it does create a lot of noise in the CI mypy step.\n    # https://mypy-lang.blogspot.com/2022/11/mypy-0990-released.html\n    'annotation-unchecked',\n]\nexclude = [\n    # BEGIN ALWAYS EXCLUDE SECTION #####################################################\n    # If pattern should always be excluded add comment explaining why and put\n    # Docs should not be type checked with the rest of the library.\n    'docs/docusaurus',\n    'docs/expectation_gallery',\n    'docs/readme_assets',\n    'docs/sphinx_api_docs_source',\n    '_version\\.py',                                                      # generated by `versioneer`\n    'v012',                                                              # legacy code\n    'datasource/data_connector/configured_asset_sql_data_connector\\.py', # 37 - This is legacy code and will not be typed.\n    'core/usage_statistics/anonymizers/batch_anonymizer\\.py',            # 10 - This code will be removed in 1.0\n    'core/usage_statistics/anonymizers/batch_request_anonymizer\\.py',    # 16 - This code will be removed in 1.0\n    'core/usage_statistics/anonymizers/checkpoint_anonymizer\\.py',       # 16 - This code will be removed in 1.0\n    'core/usage_statistics/anonymizers/data_docs_anonymizer\\.py',        # 5 - This code will be removed in 1.0\n    'core/usage_statistics/anonymizers/datasource_anonymizer\\.py',       # 9 - This code will be removed in 1.0\n    'core/usage_statistics/anonymizers/expectation_anonymizer\\.py',      # 6 - This code will be removed in 1.0\n    'core/usage_statistics/anonymizers/validation_operator_anonymizer\\.py',  # 5 - This code will be removed in 1.0\n    # END ALWAYS EXCLUDE SECTION ######################################################\n    #\n    # #################################################################################\n    # TODO: complete typing for the following modules and remove from exclude list\n    # number is the current number of typing errors for the excluded pattern\n    'expectations/core/expect_column_values_to_be_of_type\\.py',                                            # 6\n    'expectations/core/expect_column_values_to_not_match_regex_list\\.py',                                  # 2\n    'expectations/core/expect_column_values_to_not_match_regex\\.py',                                       # 2\n    'expectations/core/expect_column_values_to_not_match_like_pattern_list\\.py',                           # 3\n    'expectations/core/expect_column_values_to_not_match_like_pattern\\.py',                                # 2\n    'expectations/core/expect_column_values_to_not_be_in_set\\.py',                                         # 2\n    'expectations/core/expect_column_values_to_match_strftime_format\\.py',                                 # 2\n    'expectations/core/expect_column_values_to_match_regex_list\\.py',                                      # 2\n    'expectations/core/expect_column_values_to_match_regex\\.py',                                           # 1\n    'expectations/core/expect_column_values_to_match_like_pattern_list\\.py',                               # 3\n    'expectations/core/expect_column_values_to_match_like_pattern\\.py',                                    # 2\n    'expectations/core/expect_column_values_to_match_json_schema\\.py',                                     # 1\n    'expectations/core/expect_column_values_to_be_null\\.py',                                               # 3\n    'expectations/core/expect_column_values_to_be_json_parseable\\.py',                                     # 1\n    'expectations/core/expect_column_values_to_be_increasing\\.py',                                         # 1\n    'expectations/core/expect_column_values_to_be_in_type_list\\.py',                                       # 6\n    'expectations/core/expect_column_values_to_be_decreasing\\.py',                                         # 1\n    'expectations/core/expect_column_values_to_be_dateutil_parseable\\.py',                                 # 1\n    'expectations/core/expect_column_unique_value_count_to_be_between\\.py',                                # 1\n    'expectations/core/expect_column_stdev_to_be_between\\.py',                                             # 1\n    'expectations/core/expect_column_quantile_values_to_be_between\\.py',                                   # 15\n    'expectations/core/expect_column_value_lengths_to_equal\\.py',                                          # 1\n    'expectations/core/expect_column_value_lengths_to_be_between\\.py',                                     # 5\n    'expectations/core/expect_column_proportion_of_unique_values_to_be_between\\.py',                       # 1\n    'expectations/core/expect_column_values_to_be_in_set\\.py',                                             # 2\n    'expectations/core/expect_column_values_to_be_equal\\.py',                                              # 3\n    'expectations/core/expect_column_values_a_to_be_greater_than_b\\.py',                                   # 3\n    'expectations/core/expect_column_pair_cramers_phi_value_to_be_less_than\\.py',                          # 7\n    'expectations/core/expect_column_most_common_value_to_be_in_set\\.py',                                  # 3\n    'expectations/core/expect_column_kl_divergence_to_be_less_than\\.py',                                   # 22\n    'expectations/core/expect_column_pair_values_to_be_in_set\\.py',                                        # 2\n    'expectations/core/expect_column_pair_values_to_be_equal\\.py',                                         # 6\n    'expectations/core/expect_column_pair_values_a_to_be_greater_than_b\\.py',                              # 3\n    'expectations/core/expect_column_distinct_values_to_be_in_set\\.py',                                    # 1\n    'expectations/core/expect_compound_columns_to_be_unique\\.py',                                          # 3\n    'expectations/core/expect_multicolumn_sum_to_equal\\.py',                                               # 4\n    'expectations/core/expect_multicolumn_values_to_be_unique\\.py',                                        # 3\n    'expectations/core/expect_select_column_values_to_be_unique_within_record\\.py',                        # 3\n    'expectations/core/expect_table_columns_to_match_set\\.py',                                             # 8\n    'expectations/core/expect_table_columns_to_match_ordered_list\\.py',                                    # 11\n    'expectations/core/expect_table_column_count_to_equal\\.py',                                            # 5\n    'expectations/set_based_column_map_expectation\\.py',                                                   # 3\n    'render/renderer/content_block/content_block\\.py',                                                     # 5\n    'render/renderer/content_block/exception_list_content_block\\.py',                                      # 4\n    'render/renderer/page_renderer\\.py',                                                                   # 10\n    'render/renderer/profiling_results_overview_section_renderer\\.py',                                     # 2\n    'render/renderer/site_builder\\.py',                                                                    # 3\n    'render/renderer/slack_renderer\\.py',                                                                  # 9\n    'rule_based_profiler/domain_builder/map_metric_column_domain_builder\\.py',                             # 8\n    'rule_based_profiler/estimators/bootstrap_numeric_range_estimator\\.py',                                # 8\n    'rule_based_profiler/estimators/kde_numeric_range_estimator\\.py',                                      # 7\n    'rule_based_profiler/expectation_configuration_builder',                                               # 13\n    'rule_based_profiler/helpers/util\\.py',                                                                # 46\n    'rule_based_profiler/parameter_builder/unexpected_count_statistics_multi_batch_parameter_builder\\.py', # 69\n    'rule_based_profiler/parameter_builder/mean_unexpected_map_metric_multi_batch_parameter_builder\\.py',  # 19\n    'rule_based_profiler/parameter_builder/metric_multi_batch_parameter_builder\\.py',                      # 15\n    'rule_based_profiler/parameter_builder/numeric_metric_range_multi_batch_parameter_builder\\.py',        # 27\n    'rule_based_profiler/parameter_builder/parameter_builder\\.py',                                         # 40\n    'rule_based_profiler/parameter_builder/partition_parameter_builder\\.py',                               # 9\n    'rule_based_profiler/parameter_builder/regex_pattern_string_parameter_builder\\.py',                    # 21\n    'rule_based_profiler/parameter_builder/simple_date_format_string_parameter_builder\\.py',               # 20\n    'rule_based_profiler/rule_based_profiler\\.py',                                                         # 40\n    # tests\n    'tests/actions/test_core_actions\\.py',\n    'tests/checkpoint/test_checkpoint_result_format\\.py',\n    'tests/checkpoint/test_checkpoint\\.py',\n    'tests/core/test_batch\\.py',\n    'tests/core/test_expectation_configuration\\.py',\n    'tests/core/test_expectation_suite\\.py',\n    'tests/core/test_expectation_validation_result\\.py',\n    'tests/core/test_validation_definition\\.py',\n    'tests/core/test_yaml_handler\\.py',\n    'tests/data_context/abstract_data_context/test_abstract_data_context_datasource_crud\\.py',\n    'tests/data_context/abstract_data_context/test_data_docs_config_crud\\.py',\n    'tests/data_context/cloud_data_context/test_include_rendered_content\\.py',\n    'tests/data_context/fixtures/plugins/extended_checkpoint\\.py',\n    'tests/data_context/migrator',\n    'tests/data_context/store/test_configuration_store\\.py',\n    'tests/data_context/store/test_data_asset_store\\.py',\n    'tests/data_context/store/test_datasource_store\\.py',\n    'tests/data_context/store/test_store_backends\\.py',\n    'tests/data_context/test_data_context_in_code_config\\.py',\n    'tests/data_context/test_data_context_state_management\\.py',\n    'tests/data_context/test_data_context_variables\\.py',\n    'tests/datasource/data_connector',\n    'tests/datasource/fluent/tasks\\.py',\n    'tests/datasource/test_datasource_dict\\.py',\n    'tests/datasource/test_new_datasource_with_sql_data_connector\\.py',\n    'tests/datasource/test_simple_sqlalchemy_datasource\\.py',\n    'tests/execution_engine/partition_and_sample',\n    'tests/expectations',\n    'tests/experimental/metric_repository/test_column_descriptive_metrics_metric_retriever_integration\\.py',\n    'tests/experimental/metric_repository/test_column_descriptive_metrics_metric_retriever\\.py',\n    'tests/experimental/metric_repository/test_metric_list_metric_retriever_integration\\.py',\n    'tests/experimental/metric_repository/test_metric_list_metric_retriever\\.py',\n    'tests/integration/common_workflows',\n    'tests/integration/db',\n    'tests/integration/docusaurus/connecting_to_your_data',\n    'tests/integration/docusaurus/deployment_patterns',\n    'tests/integration/docusaurus/expectations',\n    'tests/integration/docusaurus/reference',\n    'tests/integration/docusaurus/setup',\n    'tests/integration/docusaurus/tutorials',\n    'tests/integration/docusaurus/validation',\n    'tests/integration/fixtures/partition_and_sample_data',\n    'tests/integration/fixtures/yellow_tripdata_pandas_fixture',\n    'tests/integration/spark/test_spark_config\\.py',\n    'tests/integration/test_definitions',\n    'tests/integration/test_script_runner\\.py',\n    'tests/performance',\n    'tests/render',\n    'tests/rule_based_profiler',\n    'tests/test_fixtures/rule_based_profiler/plugins',\n    'tests/test_utils\\.py',\n    'tests/validator/test_metric_configuration\\.py',\n    'tests/validator/test_metrics_calculator\\.py',\n    'tests/validator/test_validation_graph\\.py',\n]\n\n[[tool.mypy.overrides]]\n# TODO: remove these overrides once we have typed the modules\nmodule = [\n    \"contrib.experimental.great_expectations_experimental.*\",\n    \"great_expectations.checkpoint.checkpoint\",\n    \"great_expectations.compatibility.docstring_parser\",\n    \"great_expectations.data_context.*\",\n    \"great_expectations.dataset.*\",\n    \"great_expectations.expectations.core.*\",\n    \"great_expectations.expectations.metrics.column_map_metrics.*\",\n    \"great_expectations.expectations.metrics.column_pair_map_metrics.*\",\n    \"great_expectations.experimental.rule_based_profiler.*\",\n    \"great_expectations.render.renderer.*\",\n    \"great_expectations.self_check.*\",\n    \"tests.rule_based_profiler.*\",\n    \"tests.test_utils\",\n]\nfollow_imports = 'silent'\n\n[[tool.mypy.overrides]]\n# need to use override because a mypy bug prevents ignoring an assignment warning inline\n# for `from azure import storage`\nmodule = [\"great_expectations.compatibility.azure\"]\ndisable_error_code = [\n    'assignment', # cannot assign NotImported to a ModuleType\n]\n\n[[tool.mypy.overrides]]\nmodule = [\n    \"altair.*\",\n    \"boto3.*\",\n    \"botocore.*\",\n    \"clickhouse_sqlalchemy.*\",\n    \"databricks.*\",\n    \"google.*\",\n    \"great_expectations.compatibility.pydantic.*\",\n    \"mistune.*\",\n    \"moto.*\",\n    \"pact.*\",\n    \"posthog.*\",\n    \"pyarrow.*\",\n    \"pyfakefs.*\",\n    \"pypd.*\",\n    \"pytest_timeout.*\",\n    \"ruamel.*\",\n    \"scipy.*\",\n    \"sempy.*\",\n    \"shapely.*\",\n    \"snowflake.*\",\n    \"sqlalchemy_bigquery.*\",\n    \"sqlalchemy_dremio.*\",\n    \"sqlalchemy_redshift.*\",\n    \"sqlalchemy.*\",  # remove once we are using sqlalchemy 2 in type-checking step\n    \"teradatasqlalchemy.*\",\n    \"trino.*\",\n]\nignore_missing_imports = true\n\n\n[tool.pydantic-mypy]\n# https://pydantic-docs.helpmanual.io/mypy_plugin/#plugin-settings\ninit_typed = true\nwarn_required_dynamic_aliases = true\nwarn_untyped_fields = true\n\n[tool.ruff]\ntarget-version = \"py39\"\nline-length = 100\nlint.preview = true  # This enables preview rules for specified rules e.g. NPY201\nlint.explicit-preview-rules = true  # https://docs.astral.sh/ruff/preview/#selecting-single-preview-rules\nlint.select = [\n    # https://beta.ruff.rs/docs/rules/#pyflakes-f\n    \"F\", # Pyflakes\n    # https://beta.ruff.rs/docs/rules/#pycodestyle-e-w\n    \"E\", # pycodestyle\n    \"W\", # Warning\n    # https://beta.ruff.rs/docs/rules/#flake8-comprehensions-c4\n    # https://beta.ruff.rs/docs/rules/#mccabe-c90\n    \"C\", # Complexity (mccabe+) & comprehensions\n    # https://beta.ruff.rs/docs/rules/#pyupgrade-up\n    \"UP\", # pyupgrade\n    # https://beta.ruff.rs/docs/rules/#isort-i\n    \"I\", # isort\n    # https://beta.ruff.rs/docs/rules/#flake8-type-checking-tch\n    \"TCH\", # flake8-type-checking-tch\n    # https://beta.ruff.rs/docs/rules/#flake8-tidy-imports-tid\n    \"TID\", # flake8-tidy-imports\n    # https://beta.ruff.rs/docs/rules/#flake8-pyi-pyi\n    \"PYI\", # flake8-pyi - type stub files\n    # https://beta.ruff.rs/docs/rules/#flake8-use-pathlib-pth\n    \"PTH\", # use-pathlib - use pathlib for os.path and glob operations\n    # https://beta.ruff.rs/docs/rules/#flake8-bugbear-b\n    \"B\", # bugbear - common python bugs & design issues\n    # https://beta.ruff.rs/docs/rules/#flake8-datetimez-dtz\n    \"DTZ\", # flake8-datetimez-dtz - prevent use of tz naive datetimes\n    # https://beta.ruff.rs/docs/rules/#pylint-pl\n    \"PL\", # pylint\n    # https://beta.ruff.rs/docs/rules/#ruff-specific-rules-ruf\n    \"RUF\", # custom ruff rules\n    #  https://docs.astral.sh/ruff/rules/#numpy-specific-rules-npy\n    \"NPY\", # NumPy-specific rules TODO: Enable after fixing or ignoring errors\n    \"NPY201\", # numpy 2.0 compatibility (see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#ruff-plugin) -- Explicitly listed to view preview rule errors\n    # https://docs.astral.sh/ruff/rules/#tryceratops-try\n    \"TRY\", # TRYceratops\n    # https://docs.astral.sh/ruff/rules/#flake8-simplify-sim\n    \"SIM\", # flake8-simplify\n]\nlint.ignore = [\n    # formatting related ignores\n    # https://docs.astral.sh/ruff/formatter/#conflicting-lint-rules\n    \"W191\", # tab-identation\n    \"E111\", # indentation-with-invalid-multiple\n    \"E114\", # indentation-with-invalid-multiple-comment\n    \"E117\", # over-idented\n    # https://beta.ruff.rs/docs/rules/#pyflakes-f\n    \"F842\", # variable annotated but unused # TODO enable\n    # https://beta.ruff.rs/docs/rules/#pycodestyle-e-w\n    \"E402\", # module level import not at top of file\n    # https://beta.ruff.rs/docs/rules/#flake8-comprehensions-c4\n    \"C400\", # TODO enable\n    \"C408\", # TODO enable\n    \"C409\", # TODO enable\n    \"C413\", # TODO enable\n    \"C414\", # TODO enable\n    \"C416\", # TODO enable\n    \"C417\", # TODO enable\n    # https://beta.ruff.rs/docs/rules/#pyupgrade-up\n    \"UP006\", # use-pep585-annotation\n    \"UP007\", # use-pep604-annotation\n    # https://beta.ruff.rs/docs/rules/#flake8-type-checking-tch\n    # minimal cost for standard lib imports; keep this disabled\n    \"TCH003\", # typing-only-standard-library-import\n    # gives false positives if we use try imports and type-checking import\n    \"TCH004\", # runtime-import-in-type-checking-block\n    \"TID252\", # Relative imports from parent modules are banned\n    # https://beta.ruff.rs/docs/rules/#flake8-use-pathlib-pth\n    \"PTH123\", # pathlib-open - this would force pathlib usage anytime open or with open was used.\n    # https://beta.ruff.rs/docs/rules/#flake8-pyi-pyi\n    \"PYI053\", # string-or-bytes-too-long - causes mypy to fail on some of our type stubs\n    \"PYI054\", # numeric-literal-too-long - causes mypy to fail on some of our type stubs\n    \"UP035\", # TODO: remove once min version of pydantic supports using collections.abc types\n    # https://beta.ruff.rs/docs/rules/#flake8-bugbear-b\n    # TODO: enable these\n    \"B904\", # raise-without-from-inside-except\n    \"B028\", # no-explicit-stacklevel - https://beta.ruff.rs/docs/rules/no-explicit-stacklevel/\n    \"B007\", # unused-loop-control-variable\n    # TODO: enable remaining ruf rules in followup PRs\n    \"RUF005\", # collection-literal-concatenation\n    \"RUF012\", # mutable-class-default - too many violations\n    \"RUF015\", # unnecessary-iterable-allocation-for-first-element - requires more careful review\n    \"TRY300\", # Consider moving this statement to an `else` block - we don't use this; this is kind of weird\n    # TODO: enable these a few at a time (ordered by number of violations)\n    \"SIM108\", # if-else-block-instead-of-if-exp # Not enabling: This is not compatible with type narrowing\n    \"SIM102\", # collapsible-if\n    \"SIM105\", # suppressible-exception\n    \"SIM117\", # multiple-with-statements\n    \"SIM910\", # dict-get-with-none-default\n    \"SIM401\", # if-else-block-instead-of-dict-get\n    \"SIM115\", # open-file-with-context-handler\n    \"SIM202\", # negate-not-equal-op\n    \"SIM210\", # if-expr-with-true-false\n    \"SIM112\", # uncapitalized-environment-variables\n\n\n\n]\nextend-exclude = [\n    \"docs/docusaurus/versioned_docs/version-0.18\",\n    \"docs/docusaurus/versioned_docs/version-0.17\",\n    \"build/*\",\n    \"versioneer*\",\n    \"examples/*\",\n    # TODO: remove the items below and fix linting issues\n    \"tests/data_asset\",                                         # 10 warnings\n    \"tests/dataset\",                                            # 10 warnings\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"ci/checks\" = [\n    \"E501\", # line too long - too many violation & not relevant for CI checks\n]\n\"assets/benchmark/benchmark.py\" = [\n    \"DTZ\", # flake8-datetimez-dtz - doesn't matter for benchmark tests\n]\n\"assets/scripts/build_gallery.py\" = [\n    \"PLR0912\", # Too many branches  - scripts are not part of the main codebase\n    \"PLR0913\", # Too many arguments - scripts are not part of the main codebase\n    \"PLR0915\", # Too many statements - scripts are not part of the main codebase\n    \"E501\", # line too long - scripts are not part of the main codebase\n]\n\"__init__.py\" = [\n    \"F401\", # unused import\n    \"F403\", # star imports\n    \"PTH207\", # use glob - __all__ needs to be list of str, not Path\n]\n\"*.pyi\" = [\n    \"TID251\", # flake8-banned-api - type stubs are not executed\n]\n\"great_expectations/_version.py\" = [\n    \"PLR\", # pylint - versioneer code\n]\n\"great_expectations/compatibility/*.py\" = [\n    \"TID251\", # flake8-banned-api\n]\n\"tasks.py\" = [\n    \"PLR0913\",\n    \"TID251\", # not part of core code\n]\n\"tests/integration/docusaurus/*.py\" = [\n    \"F841\", # local variable is assigned to but never used - ignored for code used in snippets\n    \"E501\", # may affect how code is displayed in the docs\n]\n\n[tool.ruff.lint.flake8-type-checking]\n# pydantic models use annotations at runtime\nruntime-evaluated-base-classes = [\n    # NOTE: ruff is unable to detect that these are subclasses of pydantic.BaseModel\n    \"pydantic.BaseModel\",\n    \"great_expectations.checkpoint.checkpoint.Checkpoint\",\n    \"great_expectations.datasource.fluent.fluent_base_model.FluentBaseModel\",\n    \"great_expectations.datasource.fluent.interfaces.Datasource\",\n    \"great_expectations.datasource.fluent.sql_datasource.SQLDatasource\",\n    \"great_expectations.compatibility.pydantic.BaseModel\",\n]\nruntime-evaluated-decorators = [\"pydantic.dataclasses.dataclass\"]\n\n[tool.ruff.lint.mccabe]\n# https://docs.astral.sh/ruff/rules/complex-structure/\nmax-complexity = 8\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n[tool.ruff.lint.flake8-tidy-imports.banned-api]\n\"os.environ\".msg = \"\"\"Please do not use os.environ outside of configuration files.\nIf you are working in a configuration file you may use the inline comment \\\n\"# noqa: TID251 # os.environ allowed in config files\" to ignore this error.\"\"\"\n\"sqlalchemy\".msg = \"Please do not import sqlalchemy directly, import from great_expectations.compatibility.sqlalchemy instead.\"\n\"pyspark\".msg = \"Please do not import pyspark directly, import from great_expectations.compatibility.pyspark instead.\"\n\"boto3\".msg = \"Please do not import boto3 directly, import from great_expectations.compatibility.aws instead.\"\n\"google\".msg = \"Please do not import google directly, import from great_expectations.compatibility.google instead.\"\n\"azure\".msg = \"Please do not import azure directly, import from great_expectations.compatibility.azure instead.\"\n\"trino\".msg = \"Please do not import trino directly, import from great_expectations.compatibility.trino instead.\"\n\"pyarrow\".msg = \"Please do not import pyarrow directly, import from great_expectations.compatibility.pyarrow instead.\"\n\"typing_extensions.override\".msg = \"Do not import typing_extensions.override directly, import `override` from great_expectations.compatibility.typing_extensions instead.\"\n# TODO: remove pydantic once our min version is pydantic v2\n\"pydantic\".msg = \"Please do not import pydantic directly, import from great_expectations.compatibility.pydantic instead.\"\n\"pkg_resources\".msg = \"pkg_resources module has been deprecated. Use importlib.resources or importlib.metada instead.\"\n\"great_expectations.util.convert_to_json_serializable\".msg = \"Use pydantic for serialization.\"\n\"great_expectations.util.ensure_json_serializable\".msg = \"Use pydantic for serialization.\"\n\n\n# -----------------------------------------------------------------\n\n[tool.pytest.ini_options]\n# https://docs.python.org/3/library/warnings.html#the-warnings-filter\nfilterwarnings = [\n    # Turn all warnings not explicitly filtered below into errors\n    \"error\",\n    # PendingDeprecationWarning: these relate to the use of methods that are not even deprecated yet.\n    # We want to see the warnings in the logs, but not have them cause CI to fail.\n    \"once::PendingDeprecationWarning\",\n    # Generally we shouldn't need to use a context manger with `mocker` but it can be useful\n    # if we want the patch/mock to end before the test finishes.\n    # Example: Mocks returned by pytest-mock do not need to be used as context managers.\n    # The mocker fixture automatically undoes mocking at the end of a test. This warning can be ignored if it was triggered by mocking a context manager.\n    # https://pytest-mock.readthedocs.io/en/latest/remarks.html#usage-as-context-manager\n    \"default::pytest_mock.plugin.PytestMockWarning\",\n\n    # Find and fix ruff DTZ violations\n    'once: datetime.datetime.utc.*\\(\\) is deprecated .*:DeprecationWarning',\n\n\n    # the pkg_resources module distributed with setuptools has been deprecated\n    # we should never use it but it is used in some of our dependencies\n    # https://setuptools.pypa.io/en/latest/pkg_resources.html\n    \"once:pkg_resources is deprecated as an API:DeprecationWarning\",\n    # ruamel, google, etc.\n    \"once:Deprecated call to `pkg_resources.declare_namespace:DeprecationWarning\",\n\n    # This warning is common during testing where we intentionally use a COMPLETE format even in cases that would\n    # be potentially overly resource intensive in standard operation\n    \"ignore:Setting result format to COMPLETE for a SqlAlchemyDataset:UserWarning\",\n    # This deprecation warning was fixed in moto release 1.3.15, and the filter should be removed once we migrate\n    # to that minimum version\n    \"ignore:Using or importing the ABCs:DeprecationWarning:moto.cloudformation.parsing\",\n    # This deprecation warning comes from getsentry/responses, a mocking utility for requests. It is a dependency in moto.\n    \"ignore:stream argument is deprecated. Use stream parameter in request directly:DeprecationWarning\",\n    # We likely won't be updating to `marhsmallow` 4, these errors should be filtered out\n    \"ignore::marshmallow.warnings.RemovedInMarshmallow4Warning\",\n    \"ignore::marshmallow.warnings.ChangedInMarshmallow4Warning\",\n    # pkg_resources is deprecated as an API, but third party libraries still use it\n    'ignore: Deprecated call to `pkg_resources.declare_namespace\\(.*\\)`',\n\n\n    # --------------------------------------- Great Expectations Warnings ----------------------------------\n    # This warning is for configuring the result_format parameter at the Validator-level, which will not be persisted,\n    # but is still useful for building the configuration.\n    \"ignore:`result_format` configured at the Validator-level will not be persisted:UserWarning\",\n    # This warning is for configuring the result_format parameter at the Expectation-level, which will not be persisted,\n    # but is still useful for building the configuration.\n    \"ignore:`result_format` configured at the Expectation-level will not be persisted:UserWarning\",\n    # This warning can be emitted when configuring splitters with fluent datasources\n    \"ignore:The same option name is applied for your batch regex and splitter config:UserWarning\",\n    # --------------------------------------- Great Expectations Warnings ----------------------------------\n\n    # --------------------------------------- Great Expectations Deprecation Warnings ----------------------------------\n    # Ignores in this section are for items in Great Expectations that are deprecated but not yet removed. Once the\n    # relevant code is removed, the warning ignore should also be removed.\n    # By ignoring these warnings, we will be able to turn on \"warnings are errors\" in our pipelines.\n    # Example Actual Warning: UserWarning: Your query appears to have hard-coded references to your data. By not parameterizing your query with `{batch}`, {col}, etc., you may not be validating against your intended data asset, or the expectation may fail.\n    'ignore: Your query appears to have hard-coded references to your data. By not parameterizing your query with `{batch}`, {col}, etc., you may not be validating against your intended data asset, or the expectation may fail.:UserWarning',\n    # Example Actual Warning: UserWarning: Your query appears to not be parameterized for a data asset. By not parameterizing your query with `{batch}`, you may not be validating against your intended data asset, or the expectation may fail.\n    'ignore: Your query appears to not be parameterized for a data asset. By not parameterizing your query with `{batch}`, you may not be validating against your intended data asset, or the expectation may fail.:UserWarning',\n    # Example Actual Warning: (found in test_expect_queried_column_value_frequency_to_meet_threshold_override_query_sqlite)\n    # UserWarning: `row_condition` is an experimental feature. Combining this functionality with QueryExpectations may result in unexpected behavior.\n    'ignore: `row_condition` is an experimental feature. Combining this functionality with QueryExpectations may result in unexpected behavior.:UserWarning',\n    # Example Actual Warning: (found in test_golden_path_sql_datasource_configuration)\n    # DeprecationWarning: get_batch is deprecated for the V3 Batch Request API as of v0.13.20 and will be removed in v0.16. Please use get_batch_list instead.\n    'ignore: get_batch is deprecated for the V3 Batch Request API as of v0.13.20 and will be removed in v0.16.:DeprecationWarning',\n    # Example Actual Warning: (found in test_data_context)\n    # UserWarning: Warning. An existing `great_expectations.yml` was found here\n    'ignore: Warning. An existing `great_expectations.yml` was found here:UserWarning',\n    # Example Actual Warning: (found in test_data_context)\n    # UserWarning: Warning. An existing `config_variables.yml` was found here\n    'ignore: Warning. An existing `config_variables.yml` was found here:UserWarning',\n    # --------------------------------------- Great Expectations Deprecation Warnings ----------------------------------\n\n    # --------------------------------------- TEMPORARY IGNORES --------------------------------------------------------\n    # The warnings in this section should be addressed (fixed or ignored) but are ignored here temporarily to help allow\n    # turning on \"warnings are errors\" so new warnings become errors and are addressed during PRs.\n    'ignore: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives:DeprecationWarning',\n\n    # python\n    # Example Actual Warning: RuntimeWarning: divide by zero encountered in divide, RuntimeWarning: divide by zero encountered in true_divide\n    # Found in test_atomic_prescriptive_summary_expect_column_kl_divergence_to_be_less_than, test_case_runner_v2_api\n    'ignore: divide by zero encountered:RuntimeWarning',\n    # Example Actual Warning: Found running tests/test_definitions/test_expectations_v3_api.py\n    # DeprecationWarning: NotImplemented should not be used in a boolean context\n    'ignore: NotImplemented should not be used in a boolean context:DeprecationWarning',\n\n    # python 3.10\n    # Example Actual Warning: Found in tests/datasource/fluent/data_asset/data_connector/test_s3_data_connector.py\n    # DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n    'ignore: ssl.PROTOCOL_TLS is deprecated:DeprecationWarning',\n\n    # python 3.11\n    # data_connector/util.py imports deprecated modules that will be removed in Python 3.12\n    \"ignore: module 'sre_constants' is deprecated:DeprecationWarning\",\n    \"ignore: module 'sre_parse' is deprecated:DeprecationWarning\",\n\n    # trino\n    # example actual warning: found in great_expectations/self_check/util.py:2752: in _create_trino_engine\n    # sqlalchemy.exc.SADeprecationWarning: The dbapi() classmethod on dialect classes has been renamed to import_dbapi().  Implement an import_dbapi() classmethod directly on class <class 'trino.sqlalchemy.dialect.TrinoDialect'> to remove this warning; the old .dbapi() classmethod may be maintained for backwards compatibility.\n    'ignore: The dbapi\\(\\) classmethod on dialect classes has been renamed to import_dbapi\\(\\):DeprecationWarning',\n\n    # six\n    # Example Actual Warning: Found in ImportError while loading conftest '/great_expectations/tests/conftest.py'.\n    # ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()\n    'ignore: _SixMetaPathImporter.exec_module\\(\\) not found:ImportWarning',\n\n    # distutils\n    # Example Actual Warning: Found in tests/datasource/test_batch_generators.py, test spark python 3.10\n    # DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n    'ignore: distutils Version classes are deprecated. Use packaging.version instead.:DeprecationWarning',\n\n    # pandas\n    # Example Actual Warning: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n    # Found when running pytest tests/test_definitions/test_expectations_v3_api.py\n    'ignore: In a future version of pandas, parsing datetimes with mixed time zones:FutureWarning',\n    # Example Actual Warning: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n    # Found when running pytest tests/test_definitions/test_expectations_v3_api.py\n    'ignore: The behavior of `series:FutureWarning',\n    # Example Actual Warning: UserWarning: Unknown extension is not supported and will be removed\n    # Found when running TestIO.test_read_excel\n    'ignore: Unknown extension is not supported and will be removed:UserWarning',\n    # Example Actual Warning: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n    # Found when running test_value_counts_metric_spark\n    \"ignore: The default dtype for empty Series will be 'object' instead of 'float64' in a future version.:DeprecationWarning\",\n    # Example Actual Warning: Found by running tests/expectations/metrics/test_core.py::test_value_counts_metric_spark\n    # FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n    'ignore: The default dtype for empty Series will be:FutureWarning',\n    # Example Actual Warning: Found by running tests/render/test_column_section_renderer.py::test_ProfilingResultsColumnSectionRenderer_render_bar_chart_table with Pandas 2.0. The warning is emitted through an Altair v5 codepath.\n    # FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n    # GH Issue: https://github.com/altair-viz/altair/issues/3181\n    'ignore: the convert_dtype parameter is deprecated and will be removed in a future version:FutureWarning',\n    # Example Actual Warning: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n    'ignore: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.',\n    # Example Actual Warning: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n    'ignore: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.',\n    # Example Actual Warning: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n    # Found when runing pytest tests/test_definitions/test_expectations_v3_api.py\n    'ignore: pandas only supports SQLAlchemy connectable \\(engine/connection\\) or database string URI or sqlite3 DBAPI2 connection:UserWarning',\n    # Example Actual Warning: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n    'ignore: In a future version of pandas, parsing datetimes with mixed time zones will raise an error:FutureWarning',\n    # Example Actual Warning: DeprecationWarning: Passing a BlockManager to PandasDataset is deprecated and will raise in a future version. Use public APIs instead.\n    # This is a legacy pattern that will be removed from GX\n    'ignore: Passing a BlockManager to PandasDataset is deprecated and will raise in a future version. Use public APIs instead.:DeprecationWarning',\n    # Example Actual Warning: DeprecationWarning: Passing a BlockManager to CustomPandasDataset is deprecated and will raise in a future version. Use public APIs instead.\n    'ignore: Passing a BlockManager to CustomPandasDataset is deprecated and will raise in a future version. Use public APIs instead.:DeprecationWarning',\n\n    # numpy\n    # Example Actual Warning: RuntimeWarning: Mean of empty slice.\n    # Found when running test_case_runner_v3_api[spark/column_aggregate_expectations/expect_column_median_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare]    'ignore: Mean of empty slice.:RuntimeWarning',\n    # Example Actual Warning: RuntimeWarning: invalid value encountered in double_scalars\n    # Found when running test_case_runner_v3_api[spark/column_aggregate_expectations/expect_column_median_to_be_between:test_empty_column_should_be_false_no_observed_value_with_which_to_compare]    'ignore: invalid value encountered:RuntimeWarning',\n    # spark\n    # Example Actual Warning: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n    # Found when running test_case_runner_v2_api[SparkDFDataset/column_pair_map_expectations/expect_column_pair_values_to_be_in_set:basic_positive_test_without_nulls]    'ignore: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate\\(\\) instead.:FutureWarning',\n    # Example Acutal Warning: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n    'ignore: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance\\(dtype, pd.DatetimeTZDtype\\)` instead.',\n    # Example Actual Warning:\n    # ResourceWarning: unclosed <socket.socket fd=231, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 60004), raddr=('127.0.0.1', 46627)>\n    \"ignore: unclosed <socket.socket:ResourceWarning\",\n    # pymysql\n    # Example Actual Warning: pymysql.err.Warning: (1292, \"Truncated incorrect DOUBLE value: 'cat'\")\n    # Found in tests/test_definitions/test_expectations_v2_api.py, if not found in v3 api remove this ignore directive with the v2 api code.\n    'ignore: \\(1292, \"Truncated incorrect DOUBLE value:Warning',\n    # numpy and python 3.11\n    # Found when running tests in tests/profile/test_basic_suite_builder_profiler.py\n    # Example Actual Warning: RuntimeWarning: invalid value encountered in reduce\n    \"ignore: invalid value encountered in reduce\",\n    # boto\n    # Example Actual Warning: Found when importing botocore when collecting tests with 3.10.\n    # ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n    'ignore: _SixMetaPathImporter.find_spec\\(\\) not found; falling back to find_module\\(\\):ImportWarning',\n    # ruamel\n    'ignore:\\nsafe_load will be removed:PendingDeprecationWarning',\n    'ignore:\\nload will be removed:PendingDeprecationWarning',\n    # Warning found in tests/integration/profiling/rule_based_profiler/test_profiler_basic_workflows.py, among other places.\n    'ignore:\\ndump will be removed:PendingDeprecationWarning',\n    # Warning found in tests/integration/profiling/rule_based_profiler/test_profiler_basic_workflows.py\n    'ignore:\\ndump_all will be removed:PendingDeprecationWarning',\n    # pyarrow\n    # Example Actual Warning:\n    # UserWarning: You have an incompatible version of 'pyarrow' installed (11.0.0), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n    \"ignore: You have an incompatible version of 'pyarrow' installed:UserWarning\",\n    # jupyter\n    # Example Actual Warning:\n    # DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n    # given by the platformdirs library.  To remove this warning and\n    # see the appropriate new directories, set the environment variable\n    # `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n    # The use of platformdirs will be the default in `jupyter_core` v6\n    'ignore: Jupyter is migrating its paths to use standard platformdirs:DeprecationWarning',\n    # pytest\n    # Example Actual Warning:\n    # pytest.PytestCollectionWarning: cannot collect test class 'TestConnectionError' because it has a __init__ constructor (from: tests/datasource/fluent/test_pandas_azure_blob_storage_datasource.py)\n    \"ignore: cannot collect test class 'TestConnectionError' because it has a __init__ constructor:UserWarning\",\n    # Example Actual Warning:\n    # pytest.PytestUnraisableExceptionWarning: Exception ignored in: <socket.socket fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>\n    \"ignore: Exception ignored in:UserWarning\",\n    # jsonschema (altair dependency)\n    # Example Actual Warning:\n    # DeprecationWarning: jsonschema.RefResolver is deprecated as of v4.18.0, in favor of the\n    # https://github.com/python-jsonschema/referencing library,\n    # which provides more compliant referencing behavior as well as more flexible APIs for customization.\n    # A future release will remove RefResolver. Please file a feature request (on referencing) if you are missing an API for the kind of customization you need.\n    \"ignore: jsonschema.RefResolver is deprecated as of v4.18.0:DeprecationWarning\",\n    # Example Actual Warning:\n    # DeprecationWarning: Importing ErrorTree directly from the jsonschema package is deprecated and will become an ImportError. Import it from jsonschema.exceptions instead.\n    \"ignore: Importing ErrorTree directly from the jsonschema package is deprecated and will become an ImportError. Import it from jsonschema.exceptions instead.:DeprecationWarning\",\n    # sqlalchemy\n    # Example Actual Warning:\n    # sqlalchemy.exc.RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n    'ignore: Deprecated API features detected! These feature\\(s\\) are not compatible with SQLAlchemy 2\\.0\\.',\n    # snowflake\n    # Warning is emitted when snowflake-sqlalchemy 1.6 is installed alongside sqlalchemy 1\n    \"module: The GenericFunction 'flatten' is already registered and is going to be overridden:sqlalchemy.exc.SAWarning\"\n\n\n    # --------------------------------------- TEMPORARY IGNORES --------------------------------------------------------\n]\njunit_family = \"xunit2\"\nmarkers = [\n    \"all_backends: mark tests that run against all execution engine backends\",\n    \"athena: mark a test as AWS Athena-dependent.\",\n    \"aws_creds: mark all tests that require aws credentials\",\n    \"aws_deps: mark tests that need aws dependencies like boto\",\n    \"big: mark tests that don't have external dependencies but aren't unit tests\",\n    \"bigquery: mark test as bigquery-dependent.\",\n    \"clickhouse: mark a test as Clickhouse-dependent.\",\n    \"cloud: mark test as being relevant to Great Expectations Cloud.\",\n    \"docs-basic: mark a test as a docs test, that does not require additional credentials.\",\n    \"docs-creds-needed: mark a test as a docs test that needs additional cloud credentials (these will not run on public forks).\",\n    \"databricks: mark test as requiring databricks.\",\n    \"docs: mark a test as a docs test.\",\n    \"docs-spark: temporarily mark a test as a docs test that depends on spark.\",\n    \"integration: mark test as an integration test.\",\n    \"external_sqldialect: mark test as requiring install of an external sql dialect.\",\n    \"filesystem: mark tests using the filesystem as the storage backend.\",\n    \"mssql: mark a test as mssql-dependent.\",\n    \"mysql: mark a test as mysql-dependent.\",\n    \"openpyxl: mark a test for openpyxl-dependent, which is for Excel files.\",\n    \"performance: mark a test as a performance test for BigQuery. These aren't run in our PR or release pipeline\",\n    \"postgresql: mark a test as postgresql-dependent.\",\n    \"project: mark a test that verifies properties of the gx project\",\n    \"pyarrow: mark a test as PyArrow-dependent.\",\n    \"snowflake: mark a test as snowflake-dependent.\",\n    \"sqlalchemy_version_compatibility: mark test as required for sqlalchemy version compatibility.\",\n    \"sqlite: mark test requiring sqlite\",\n    \"slow: mark tests taking longer than 1 second.\",\n    \"spark: mark a test as Spark-dependent.\",\n    \"spark_connect: mark a test as Spark Connect-dependent.\",\n    \"trino: mark a test as trino-dependent.\",\n    \"unit: mark a test as a unit test.\",\n    \"v2_api: mark test as specific to the v2 api (e.g. pre Data Connectors).\",\n]\ntestpaths = \"tests\"\n# use `pytest-mock` drop-in replacement for `unittest.mock`\n# https://pytest-mock.readthedocs.io/en/latest/configuration.html#use-standalone-mock-package\nmock_use_standalone_module = false\n# https://docs.pytest.org/en/7.1.x/how-to/logging.html#how-to-manage-logging\nlog_level = \"info\"\n\n[tool.coverage.report]\n# https://coverage.readthedocs.io/en/7.5.1/config.html\n# Regexes for lines to exclude from consideration\nexclude_also = [\n    # type-checking imports don't exist at runtime\n    \"if TYPE_CHECKING:\",\n    # Don't complain if tests don't hit NotImplementedError:\n    \"raise NotImplementedError\",\n    # Don't complain if non-runnable code isn't run:\n    \"if __name__ == .__main__.:\",\n    # Don't complain about abstract methods, they aren't run:\n    \"@(abc\\\\.)?abstractmethod\",\n    ]\n"
        },
        {
          "name": "reqs",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 0.4638671875,
          "content": "# Do not `pip install -r` this file locally or in CI.\n# It only exists to be used in the Dockerfile.tests file.\n\n\n--requirement reqs/requirements-dev-arrow.txt\n--requirement requirements.txt\n--requirement reqs/requirements-dev-test.txt\n--requirement reqs/requirements-dev-sqlalchemy.txt\n--requirement reqs/requirements-dev-azure.txt\n--requirement reqs/requirements-dev-excel.txt\n--requirement reqs/requirements-dev-pagerduty.txt\n--requirement reqs/requirements-dev-spark.txt\n"
        },
        {
          "name": "requirements-types.txt",
          "type": "blob",
          "size": 0.7431640625,
          "content": "--requirement reqs/requirements-dev-sqlalchemy2.txt\n--requirement requirements.txt\n--requirement reqs/requirements-dev-lite.txt\n--requirement reqs/requirements-dev-spark.txt\n--requirement reqs/requirements-dev-azure.txt\n--requirement reqs/requirements-dev-contrib.txt\n--requirement reqs/requirements-dev-cloud.txt\n--requirement docs/sphinx_api_docs_source/requirements-dev-api-docs.txt\npandas-stubs\n# force sqlalchemy 2 because it exports types\nsqlalchemy>2.0\n# typing stubs\ntypes-beautifulsoup4\ntypes-colorama\ntypes-decorator\ntypes-html5lib\ntypes-jsonschema\ntypes-openpyxl\ntypes-protobuf\ntypes-psutil\ntypes-psycopg2\ntypes-pycurl\ntypes-Pygments\ntypes-python-dateutil\ntypes-PyYAML\ntypes-requests\ntypes-six\ntypes-tabulate\ntypes-tqdm\ntypes-typed-ast\ntypes-tzlocal\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.6494140625,
          "content": "altair>=4.2.1,<5.0.0\ncryptography>=3.2\njinja2>=2.10\njsonschema>=2.5.1\nmarshmallow>=3.7.1,<4.0.0\nmistune>=0.8.4\nnumpy>=1.21.6; python_version == \"3.9\"\nnumpy>=1.22.4; python_version >= \"3.10\"\nnumpy>=1.26.0; python_version >= \"3.12\"\npackaging\npandas>=1.1.3,<2.2; python_version == \"3.9\"\npandas>=1.3.0,<2.2; python_version >= \"3.10\"\npandas<2.2; python_version >= \"3.12\"\n# analytics\nposthog>3,<4\n# patch version updates `typing_extensions` to the needed version\npydantic>=1.10.7\npyparsing>=2.4\npython-dateutil>=2.8.1\nrequests>=2.20\nruamel.yaml>=0.16\nscipy>=1.6.0\ntqdm>=4.59.0\ntyping-extensions>=4.1.0 # Leverage type annotations from recent Python releases\ntzlocal>=1.2\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.2646484375,
          "content": "[easy_install]\n\n[bdist_wheel]\nuniversal=0\n\n[versioneer]\nVCS = git\nstyle = pep440\nversionfile_source = great_expectations/_version.py\nversionfile_build = great_expectations/_version.py\ntag_prefix =\nparentdir_prefix = great_expectations-\n\n[options]\npython_requires = >=3.9\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 5.6904296875,
          "content": "import os\nimport re\nfrom pathlib import Path\n\n# https://setuptools.pypa.io/en/latest/pkg_resources.html\nimport pkg_resources  # noqa: TID251: TODO: switch to poetry\nfrom setuptools import find_packages, setup\n\nimport versioneer\n\nSUPPORTED_PYTHON = \">=3.9,<3.13\"\n\n\ndef get_python_requires() -> str:\n    \"\"\"\n    If the GX_PYTHON_EXPERIMENTAL environment variable is set,\n    return a version with no upper-bound.\n    \"\"\"\n    if os.getenv(\"GX_PYTHON_EXPERIMENTAL\"):\n        return \">=3.9\"\n    return SUPPORTED_PYTHON\n\n\ndef get_extras_require():\n    results = {}\n    extra_key_mapping = {\n        \"aws_secrets\": \"boto\",\n        \"azure_secrets\": \"azure\",\n        \"gcp\": \"bigquery\",\n        \"s3\": \"boto\",\n    }\n    sqla1x_only_keys = (\n        \"bigquery\",  # https://github.com/googleapis/python-bigquery-sqlalchemy/blob/main/setup.py\n        \"clickhouse\",  # https://github.com/xzkostyan/clickhouse-sqlalchemy/blob/master/setup.py\n        \"redshift\",  # https://github.com/sqlalchemy-redshift/sqlalchemy-redshift/blob/main/setup.py\n        \"teradata\",  # https://pypi.org/project/teradatasqlalchemy   https://support.teradata.com/knowledge?id=kb_article_view&sys_kb_id=a5a869149729251ced863fe3f153af27\n    )\n    sqla_keys = (\n        \"athena\",  # https://github.com/laughingman7743/PyAthena/blob/master/pyproject.toml\n        \"dremio\",  # https://github.com/narendrans/sqlalchemy_dremio/blob/master/setup.py\n        \"hive\",  # https://github.com/dropbox/PyHive/blob/master/setup.py\n        \"mssql\",  # https://github.com/mkleehammer/pyodbc/blob/master/setup.py\n        \"mysql\",  # https://github.com/PyMySQL/PyMySQL/blob/main/pyproject.toml\n        \"postgresql\",  # https://github.com/psycopg/psycopg2/blob/master/setup.py\n        \"trino\",  # https://github.com/trinodb/trino-python-client/blob/master/setup.py\n        \"vertica\",  # https://github.com/bluelabsio/sqlalchemy-vertica-python/blob/master/setup.py\n        \"databricks\",  # https://github.com/databricks/databricks-sql-python/blob/main/pyproject.toml\n        \"snowflake\",  # https://github.com/snowflakedb/snowflake-sqlalchemy/blob/main/setup.cfg\n    )\n    ignore_keys = (\n        \"sqlalchemy\",\n        \"sqlalchemy2\",\n        \"test\",\n        \"tools\",\n        \"all-contrib-expectations\",\n    )\n\n    requirements_dir = \"reqs\"\n    rx_name_part = re.compile(r\"requirements-dev-(.*).txt\")\n\n    # Use Path() from pathlib so we can make this section of the code OS agnostic.\n    # Loop through each requirement file and verify they are named\n    # correctly and are in the right location.\n    for file_path in Path().glob(f\"{requirements_dir}/*.txt\"):\n        match = rx_name_part.match(file_path.name)\n        assert (\n            match is not None\n        ), f\"The extras requirements dir ({requirements_dir}) contains files that do not adhere to the following format: requirements-dev-*.txt\"  # noqa: E501\n        key = match.group(1)\n        if key in ignore_keys:\n            continue\n        with open(file_path) as f:\n            parsed = [str(req) for req in pkg_resources.parse_requirements(f)]\n            results[key] = parsed\n\n    lite = results.pop(\"lite\")\n    contrib = results.pop(\"contrib\")\n    docs_test = results.pop(\"api-docs-test\")\n    arrow = results[\"arrow\"]\n    results[\"boto\"] = [req for req in lite if req.startswith(\"boto\")]\n    results[\"sqlalchemy2\"] = [req for req in lite if req.startswith(\"sqlalchemy\")]\n    results[\"test\"] = lite + contrib + docs_test + arrow\n\n    for new_key, existing_key in extra_key_mapping.items():\n        results[new_key] = results[existing_key]\n    for key in sqla1x_only_keys:\n        results[key] += results[\"sqlalchemy1\"]\n    for key in sqla_keys:\n        results[key] += results[\"sqlalchemy2\"]\n\n    results.pop(\"boto\")\n    results.pop(\"sqlalchemy1\")\n    results.pop(\"sqlalchemy2\")\n    # all_requirements_set = set()\n    # [all_requirements_set.update(vals) for vals in results.values()]\n    # results[\"dev\"] = sorted(all_requirements_set)\n    return results\n\n\n# Parse requirements.txt\nwith open(\"requirements.txt\") as f:\n    required = f.read().splitlines()\n\nlong_description = \"Always know what to expect from your data. (See https://github.com/great-expectations/great_expectations for full description).\"  # noqa: E501\n\nconfig = {\n    \"description\": \"Always know what to expect from your data.\",\n    \"author\": \"The Great Expectations Team\",\n    \"url\": \"https://greatexpectations.io\",\n    \"download_url\": \"https://github.com/great-expectations/great_expectations\",\n    \"author_email\": \"team@greatexpectations.io\",\n    \"version\": versioneer.get_version(),\n    \"cmdclass\": versioneer.get_cmdclass(),\n    \"install_requires\": required,\n    \"extras_require\": get_extras_require(),\n    \"packages\": find_packages(exclude=[\"contrib*\", \"docs*\", \"tests*\", \"examples*\", \"scripts*\"]),\n    \"package_data\": {\"great_expectations\": [\"**/py.typed\", \"**/*.pyi\"]},\n    \"name\": \"great_expectations\",\n    \"long_description\": long_description,\n    \"license\": \"Apache-2.0\",\n    \"keywords\": \"data science testing pipeline data quality dataquality validation datavalidation\",\n    \"include_package_data\": True,\n    \"python_requires\": get_python_requires(),\n    \"classifiers\": [\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"Intended Audience :: Other Audience\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Software Development\",\n        \"Topic :: Software Development :: Testing\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n    ],\n}\n\nsetup(**config)\n"
        },
        {
          "name": "tasks.py",
          "type": "blob",
          "size": 40.3046875,
          "content": "\"\"\"\nPyInvoke developer task file\nhttps://www.pyinvoke.org/\n\nThese tasks can be run using `invoke <NAME>` or `inv <NAME>` from the project root.\n\nTo show all available tasks `invoke --list`\n\nTo show task help page `invoke <NAME> --help`\n\"\"\"\n\nfrom __future__ import annotations\n\nimport importlib\nimport logging\nimport os\nimport pathlib\nimport pkgutil\nimport shutil\nimport sys\nfrom collections.abc import Generator, Mapping, Sequence\nfrom pprint import pformat as pf\nfrom typing import TYPE_CHECKING, Final, NamedTuple, Union\n\nimport invoke\n\nfrom docs.sphinx_api_docs_source import check_public_api_docstrings, public_api_report\nfrom docs.sphinx_api_docs_source.build_sphinx_api_docs import SphinxInvokeDocsBuilder\n\nif TYPE_CHECKING:\n    from invoke.context import Context\n    from typing_extensions import Literal\n\n\nLOGGER = logging.getLogger(__name__)\n\nGX_ROOT_DIR: Final = pathlib.Path(__file__).parent\nGX_PACKAGE_DIR: Final = GX_ROOT_DIR / \"great_expectations\"\nREQS_DIR: Final = GX_ROOT_DIR / \"reqs\"\n\n_CHECK_HELP_DESC = (\n    \"Only checks for needed changes without writing back. Exit with error code if changes needed.\"\n)\n_EXCLUDE_HELP_DESC = \"Exclude files or directories\"\n_PATH_HELP_DESC = \"Target path. (Default: .)\"\n# https://www.pyinvoke.org/faq.html?highlight=pty#why-is-my-command-behaving-differently-under-invoke-versus-being-run-by-hand\n_PTY_HELP_DESC = \"Whether or not to use a pseudo terminal\"\n\n\n@invoke.task(\n    help={\n        \"check\": _CHECK_HELP_DESC,\n        \"exclude\": _EXCLUDE_HELP_DESC,\n        \"path\": _PATH_HELP_DESC,\n        \"pty\": _PTY_HELP_DESC,\n    }\n)\ndef sort(\n    ctx: Context,\n    path: str = \".\",\n    check: bool = False,\n    exclude: str | None = None,\n    pty: bool = True,\n):\n    \"\"\"Sort module imports.\"\"\"\n    cmds = [\n        \"ruff\",\n        \"check\",\n        path,\n        \"--select I\",\n        \"--diff\" if check else \"--fix\",\n    ]\n    if exclude:\n        cmds.extend([\"--extend-exclude\", exclude])\n    ctx.run(\" \".join(cmds), echo=True, pty=pty)\n\n\n@invoke.task(\n    aliases=(\"fmt\",),\n    help={\n        \"check\": _CHECK_HELP_DESC,\n        \"exclude\": _EXCLUDE_HELP_DESC,\n        \"path\": _PATH_HELP_DESC,\n        \"sort\": \"Disable import sorting. Runs by default.\",\n        \"pty\": _PTY_HELP_DESC,\n    },\n)\ndef format(\n    ctx: Context,\n    path: str = \".\",\n    sort_: bool = True,\n    check: bool = False,\n    exclude: str | None = None,\n    pty: bool = True,\n):\n    \"\"\"\n    Run code formatter.\n    \"\"\"\n    if sort_:\n        sort(ctx, path, check=check, exclude=exclude, pty=pty)\n\n    cmds = [\"ruff\", \"format\", path]\n    if check:\n        cmds.append(\"--check\")\n    if exclude:\n        cmds.extend([\"--exclude\", exclude])\n    ctx.run(\" \".join(cmds), echo=True, pty=pty)\n\n\n@invoke.task(\n    help={\n        \"path\": _PATH_HELP_DESC,\n        \"fmt\": \"Disable formatting. Runs by default.\",\n        \"fix\": \"Attempt to automatically fix lint violations.\",\n        \"unsafe-fixes\": \"Enable potentially unsafe fixes.\",\n        \"watch\": \"Run in watch mode by re-running whenever files change.\",\n        \"pty\": _PTY_HELP_DESC,\n    }\n)\ndef lint(\n    ctx: Context,\n    path: str = \".\",\n    fmt_: bool = True,\n    fix: bool = False,\n    unsafe_fixes: bool = False,\n    output_format: Literal[\"full\", \"concise\", \"github\"] | None = None,\n    watch: bool = False,\n    pty: bool = True,\n):\n    \"\"\"Run formatter (ruff format) and linter (ruff)\"\"\"\n    if fmt_:\n        format(ctx, path, check=not fix, pty=pty)\n\n    # Run code linter (ruff)\n    cmds = [\"ruff\", \"check\", path]\n    if fix:\n        cmds.append(\"--fix\")\n    if unsafe_fixes:\n        cmds.append(\"--unsafe-fixes\")\n    if watch:\n        cmds.append(\"--watch\")\n    if output_format:\n        cmds.append(f\"--output-format={output_format}\")\n    elif os.getenv(\"GITHUB_ACTIONS\"):\n        cmds.append(\"--output-format=github\")\n    ctx.run(\" \".join(cmds), echo=True, pty=pty)\n\n\n@invoke.task(help={\"path\": _PATH_HELP_DESC, \"safe-only\": \"Only apply 'safe' fixes.\"})\ndef fix(ctx: Context, path: str = \".\", safe_only: bool = False):\n    \"\"\"\n    Automatically fix all possible code issues.\n    Applies unsafe fixes by default.\n    https://docs.astral.sh/ruff/linter/#fix-safety\n    \"\"\"\n    unsafe_fixes = not safe_only\n    lint(ctx, path=path, fmt_=False, fix=True, unsafe_fixes=unsafe_fixes)\n    format(ctx, path=path, check=False, sort_=False)\n\n\n@invoke.task(help={\"path\": _PATH_HELP_DESC})\ndef upgrade(ctx: Context, path: str = \".\"):\n    \"\"\"Run code syntax upgrades.\"\"\"\n    cmds = [\"ruff\", path, \"--select\", \"UP\", \"--fix\"]\n    ctx.run(\" \".join(cmds), echo=True, pty=True)\n\n\n@invoke.task(\n    help={\n        \"all_files\": \"Run hooks against all files, not just the current changes.\",\n        \"diff\": \"Show the diff of changes on hook failure.\",\n        \"sync\": \"Re-install the latest git hooks.\",\n    }\n)\ndef hooks(ctx: Context, all_files: bool = False, diff: bool = False, sync: bool = False):\n    \"\"\"Run and manage pre-commit hooks.\"\"\"\n    cmds = [\"pre-commit\", \"run\"]\n    if diff:\n        cmds.append(\"--show-diff-on-failure\")\n    if all_files:\n        cmds.extend([\"--all-files\"])\n    else:\n        # used in CI - runs faster and only checks files that have changed\n        cmds.extend([\"--from-ref\", \"origin/HEAD\", \"--to-ref\", \"HEAD\"])\n\n    ctx.run(\" \".join(cmds), echo=True, pty=True)\n\n    if sync:\n        print(\"  Re-installing hooks ...\")\n        ctx.run(\" \".join([\"pre-commit\", \"uninstall\"]), echo=True)\n        ctx.run(\" \".join([\"pre-commit\", \"install\"]), echo=True)\n\n\n@invoke.task(aliases=(\"docstring\",), iterable=(\"paths\",))\ndef docstrings(ctx: Context, paths: list[str] | None = None):\n    \"\"\"\n    Check public API docstrings.\n\n    Optionally pass a directory or file.\n    To pass multiple items:\n        invoke docstrings -p=great_expectations/core -p=great_expectations/util.py\n    \"\"\"\n\n    if paths:\n        select_paths = [pathlib.Path(p) for p in paths]\n    else:\n        select_paths = None\n    try:\n        check_public_api_docstrings.main(select_paths=select_paths)\n    except AssertionError as err:\n        raise invoke.Exit(\n            message=f\"{err}\\n\\nGenerated with {check_public_api_docstrings.__file__}\",\n            code=1,\n        )\n\n\n@invoke.task()\ndef marker_coverage(\n    ctx: Context,\n):\n    pytest_cmds = [\"pytest\", \"--verify-marker-coverage-and-exit\"]\n    ctx.run(\" \".join(pytest_cmds), echo=True, pty=True)\n\n\n@invoke.task(\n    aliases=[\"types\"],\n    iterable=[\"packages\"],\n    help={\n        \"packages\": \"One or more `great_expectatations` sub-packages to type-check with mypy.\",\n        \"install-types\": \"Automatically install any needed types from `typeshed`.\",\n        \"daemon\": \"Run mypy in daemon mode with faster analysis.\"\n        \" The daemon will be started and re-used for subsequent calls.\"\n        \" For detailed usage see `dmypy --help`.\",\n        \"clear-cache\": \"Clear the local mypy cache directory.\",\n        \"check-stub-sources\": \"Check the implementation `.py` files for any `.pyi`\"\n        \" stub files in `great_expectations`.\"\n        \" By default `mypy` will not check implementation files if a `.pyi` stub file exists.\"\n        \" This should be run in CI in addition to the normal type-checking step.\",\n        \"python-version\": \"Type check as if running a specific python version.\"\n        \" Default to version set in pyproject.toml\",\n    },\n)\ndef type_check(  # noqa: C901, PLR0912\n    ctx: Context,\n    packages: list[str],\n    install_types: bool = False,\n    pretty: bool = False,\n    warn_unused_ignores: bool = False,\n    disallow_untyped_decorators: bool = True,\n    daemon: bool = False,\n    clear_cache: bool = False,\n    report: bool = False,\n    check_stub_sources: bool = False,\n    ci: bool = False,\n    python_version: str = \"\",\n):\n    \"\"\"Run mypy static type-checking on select packages.\"\"\"\n    mypy_cache = pathlib.Path(\".mypy_cache\")\n\n    if ci:\n        mypy_cache.mkdir(exist_ok=True)\n        print(f\"  mypy cache {mypy_cache.absolute()}\")\n\n        type_check(\n            ctx,\n            packages,\n            install_types=True,\n            pretty=pretty,\n            warn_unused_ignores=True,\n            disallow_untyped_decorators=True,\n            daemon=daemon,\n            clear_cache=clear_cache,\n            report=report,\n            check_stub_sources=check_stub_sources,\n            ci=False,\n            python_version=python_version,\n        )\n        return  # don't run twice\n\n    if clear_cache:\n        print(f\"  Clearing {mypy_cache} ... \", end=\"\")\n        try:\n            shutil.rmtree(mypy_cache)\n            print(\"✅\")\n        except FileNotFoundError as exc:\n            print(f\"❌\\n  {exc}\")\n\n    if daemon:\n        bin = \"dmypy run --\"\n    else:\n        bin = \"mypy\"\n\n    cmds = [bin]\n\n    ge_pkgs = [f\"great_expectations.{p}\" for p in packages]\n\n    if check_stub_sources:\n        # see --help docs for explanation of this flag\n        for stub_file in GX_PACKAGE_DIR.glob(\"**/*.pyi\"):\n            source_file = stub_file.with_name(  # TODO:py3.9 .with_stem()\n                f\"{stub_file.name[:-1]}\"\n            )\n            relative_path = source_file.relative_to(GX_ROOT_DIR)\n            ge_pkgs.append(str(relative_path))\n        # following imports here can cause mutually exclusive import errors with normal type-checking  # noqa: E501\n        cmds.append(\"--follow-imports=silent\")\n\n    cmds.extend(ge_pkgs)\n\n    if install_types:\n        cmds.extend([\"--install-types\", \"--non-interactive\"])\n    if daemon:\n        # see related issue https://github.com/python/mypy/issues/9475\n        cmds.extend([\"--follow-imports=normal\"])\n    if report:\n        cmds.extend([\"--txt-report\", \"type_cov\", \"--html-report\", \"type_cov\"])\n    if pretty:\n        cmds.extend([\"--pretty\"])\n    if warn_unused_ignores:\n        cmds.extend([\"--warn-unused-ignores\"])\n    if disallow_untyped_decorators:\n        cmds.extend([\"--disallow-untyped-decorators\"])\n    if python_version:\n        cmds.extend([\"--python-version\", python_version])\n    # use pseudo-terminal for colorized output\n    ctx.run(\" \".join(cmds), echo=True, pty=True)\n\n\nUNIT_TEST_DEFAULT_TIMEOUT: float = (\n    2.0  # TODO: revert the timeout back to 1.5 or lower after resolving arc issues\n)\n\n\n@invoke.task(\n    aliases=[\"test\"],\n    help={\n        \"unit\": \"Runs tests marked with the 'unit' marker. Default behavior.\",\n        \"cloud\": \"Runs tests marked with the 'cloud' marker. Default behavior.\",\n        \"ignore-markers\": \"Don't exclude any test by not passing any markers to pytest.\",\n        \"slowest\": \"Report on the slowest n number of tests\",\n        \"ci\": \"execute tests assuming a CI environment. Publish XML reports for coverage reporting etc.\",  # noqa: E501\n        \"timeout\": f\"Fails unit-tests if calls take longer than this value. Default {UNIT_TEST_DEFAULT_TIMEOUT} seconds\",  # noqa: E501\n        \"html\": \"Create html coverage report\",\n        \"package\": \"Run tests on a specific package. Assumes there is a `tests/<PACKAGE>` directory of the same name.\",  # noqa: E501\n        \"full-cov\": \"Show coverage report on the entire `great_expectations` package regardless of `--package` param.\",  # noqa: E501\n    },\n)\ndef tests(  # noqa: C901\n    ctx: Context,\n    unit: bool = True,\n    ignore_markers: bool = False,\n    ci: bool = False,\n    html: bool = False,\n    cloud: bool = True,\n    slowest: int = 5,\n    timeout: float = UNIT_TEST_DEFAULT_TIMEOUT,\n    package: str | None = None,\n    full_cov: bool = False,\n    verbose: bool = False,\n):\n    \"\"\"\n    Run tests. Runs unit tests by default.\n\n    Use `invoke tests -p=<TARGET_PACKAGE>` to run tests on a particular package and measure coverage (or lack thereof).\n\n    See also, the newer `invoke ci-tests --help`.\n    \"\"\"  # noqa: E501\n    markers = []\n    markers += [\"unit\" if unit else \"not unit\"]\n\n    marker_text = \" and \".join(markers)\n\n    cov_param = \"--cov=great_expectations\"\n    if package and not full_cov:\n        cov_param += f\"/{package.replace('.', '/')}\"\n\n    cmds = [\n        \"pytest\",\n        f\"--durations={slowest}\",\n        cov_param,\n        \"--cov-report term\",\n        \"-rEf\",  # show extra test summary info for errors & failed tests\n    ]\n    if verbose:\n        cmds.append(\"-vv\")\n    if not ignore_markers:\n        cmds += [\"-m\", f\"'{marker_text}'\"]\n    if unit and not ignore_markers:\n        try:\n            import pytest_timeout  # noqa: F401\n\n            cmds += [f\"--timeout={timeout}\"]\n        except ImportError:\n            print(\"`pytest-timeout` is not installed, cannot use --timeout\")\n\n    if cloud:\n        cmds += [\"--cloud\"]\n    if ci:\n        cmds += [\"--cov-report\", \"xml\"]\n    if html:\n        cmds += [\"--cov-report\", \"html\"]\n    if package:\n        cmds += [f\"tests/{package.replace('.', '/')}\"]  # allow `foo.bar`` format\n    ctx.run(\" \".join(cmds), echo=True, pty=True)\n\n\nPYTHON_VERSION_DEFAULT: float = 3.8\n\n\n@invoke.task(\n    help={\n        \"name\": \"Docker image name.\",\n        \"tag\": \"Docker image tag.\",\n        \"build\": \"If True build the image, otherwise run it. Defaults to False.\",\n        \"detach\": \"Run container in background and print container ID. Defaults to False.\",\n        \"py\": f\"version of python to use. Default is {PYTHON_VERSION_DEFAULT}\",\n        \"cmd\": \"Command for docker image. Default is bash.\",\n        \"target\": \"Set the target build stage to build.\",\n    }\n)\ndef docker(\n    ctx: Context,\n    name: str = \"gx38local\",\n    tag: str = \"latest\",\n    build: bool = False,\n    detach: bool = False,\n    cmd: str = \"bash\",\n    py: float = PYTHON_VERSION_DEFAULT,\n    target: str | None = None,\n):\n    \"\"\"\n    Build or run gx docker image.\n    \"\"\"\n\n    _exit_with_error_if_not_in_repo_root(task_name=\"docker\")\n\n    filedir = os.path.realpath(\n        os.path.dirname(os.path.realpath(__file__))  # noqa: PTH120\n    )\n\n    cmds = [\"docker\"]\n\n    if build:\n        cmds.extend(\n            [\n                \"buildx\",\n                \"build\",\n                \"-f\",\n                \"docker/Dockerfile.tests\",\n                f\"--tag {name}:{tag}\",\n                *[f\"--build-arg {arg}\" for arg in [\"SOURCE=local\", f\"PYTHON_VERSION={py}\"]],\n                \".\",\n            ]\n        )\n        if target:\n            cmds.extend([\"--target\", target])\n\n    else:\n        cmds.append(\"run\")\n        if detach:\n            cmds.append(\"--detach\")\n        cmds.extend(\n            [\n                \"-it\",\n                \"--rm\",\n                \"--mount\",\n                f\"type=bind,source={filedir},target=/great_expectations\",\n                \"-w\",\n                \"/great_expectations\",\n                f\"{name}:{tag}\",\n                f\"{cmd}\",\n            ]\n        )\n\n    ctx.run(\" \".join(cmds), echo=True, pty=True)\n\n\n@invoke.task(\n    aliases=(\"schema\", \"schemas\"),\n    help={\n        \"sync\": \"Update the json schemas\",\n        \"indent\": \"Indent size for nested json objects. Default: 4\",\n        \"clean\": \"Delete all schema files and sub directories.\"\n        \" Can be combined with `--sync` to reset the /schemas dir and remove stale schemas\",\n    },\n)\ndef type_schema(  # noqa: C901 - too complex\n    ctx: Context,\n    sync: bool = False,\n    clean: bool = False,\n    indent: int = 4,\n):\n    \"\"\"\n    Show all the json schemas for Fluent Datasources & DataAssets\n\n    Generate json schema for each Datasource & DataAsset with `--sync`.\n    \"\"\"\n    import pandas\n\n    from great_expectations.datasource.fluent import (\n        _PANDAS_SCHEMA_VERSION,\n        BatchRequest,\n        DataAsset,\n        Datasource,\n    )\n    from great_expectations.datasource.fluent.sources import (\n        _iter_all_registered_types,\n    )\n    from great_expectations.expectations import core\n\n    data_source_schema_dir_root: Final[pathlib.Path] = (\n        GX_PACKAGE_DIR / \"datasource\" / \"fluent\" / \"schemas\"\n    )\n    expectation_schema_dir_root: Final[pathlib.Path] = (\n        GX_PACKAGE_DIR / \"expectations\" / \"core\" / \"schemas\"\n    )\n    if clean:\n        shutil.rmtree(data_source_schema_dir_root)\n        shutil.rmtree(expectation_schema_dir_root)\n\n    data_source_schema_dir_root.mkdir(exist_ok=True)\n    expectation_schema_dir_root.mkdir(exist_ok=True)\n\n    datasource_dir: pathlib.Path = data_source_schema_dir_root\n    expectation_dir: pathlib.Path = expectation_schema_dir_root\n\n    if not sync:\n        print(\"--------------------\\nRegistered Fluent types\\n--------------------\\n\")\n\n    name_model: list[tuple[str, type[Datasource | BatchRequest | DataAsset]]] = [\n        (\"BatchRequest\", BatchRequest),\n        (Datasource.__name__, Datasource),\n        *_iter_all_registered_types(),\n    ]\n\n    # handle data sources\n    for name, model in name_model:\n        if issubclass(model, Datasource):\n            datasource_dir = data_source_schema_dir_root.joinpath(model.__name__)\n            datasource_dir.mkdir(exist_ok=True)\n            schema_dir = data_source_schema_dir_root\n            print(\"-\" * shutil.get_terminal_size()[0])\n        else:\n            schema_dir = datasource_dir\n            print(\"  \", end=\"\")\n\n        if not sync:\n            print(f\"{name} - {model.__name__}.json\")\n            continue\n\n        if (\n            datasource_dir.name.startswith(\"Pandas\")\n            and pandas.__version__ != _PANDAS_SCHEMA_VERSION\n        ):\n            print(\n                f\"🙈  {name} - was generated with pandas\"\n                f\" {_PANDAS_SCHEMA_VERSION} but you have {pandas.__version__}; skipping\"\n            )\n            continue\n\n        try:\n            schema_path = schema_dir.joinpath(f\"{model.__name__}.json\")\n            json_str: str = model.schema_json(indent=indent) + \"\\n\"\n\n            if schema_path.exists():\n                if json_str == schema_path.read_text():\n                    print(f\"✅  {name} - {schema_path.name} unchanged\")\n                    continue\n\n            schema_path.write_text(json_str)\n            print(f\"🔃  {name} - {schema_path.name} schema updated\")\n        except TypeError as err:\n            print(f\"❌  {name} - Could not sync schema - {type(err).__name__}:{err}\")\n\n    # handle expectations\n    supported_expectations = [\n        core.ExpectColumnValuesToBeNull,\n        core.ExpectColumnValuesToNotBeNull,\n        core.ExpectColumnValuesToBeUnique,\n        core.ExpectColumnValuesToBeInSet,\n        core.ExpectColumnMaxToBeBetween,\n        core.ExpectColumnMeanToBeBetween,\n        core.ExpectColumnMedianToBeBetween,\n        core.ExpectColumnMinToBeBetween,\n        core.ExpectColumnValuesToBeInTypeList,\n        core.ExpectColumnValuesToBeOfType,\n        core.ExpectTableColumnsToMatchOrderedList,\n        core.ExpectTableRowCountToBeBetween,\n        core.ExpectTableRowCountToEqual,\n        core.ExpectColumnPairValuesToBeEqual,\n        core.ExpectMulticolumnSumToEqual,\n        core.ExpectCompoundColumnsToBeUnique,\n        core.ExpectSelectColumnValuesToBeUniqueWithinRecord,\n        core.ExpectColumnPairValuesAToBeGreaterThanB,\n        core.ExpectColumnToExist,\n        core.ExpectTableColumnCountToEqual,\n        core.ExpectTableColumnsToMatchSet,\n        core.ExpectTableColumnCountToBeBetween,\n        core.ExpectTableRowCountToEqualOtherTable,\n        core.ExpectColumnPairValuesToBeInSet,\n        core.ExpectColumnProportionOfUniqueValuesToBeBetween,\n        core.ExpectColumnUniqueValueCountToBeBetween,\n        core.ExpectColumnDistinctValuesToBeInSet,\n        core.ExpectColumnDistinctValuesToContainSet,\n        core.ExpectColumnDistinctValuesToEqualSet,\n        core.ExpectColumnMostCommonValueToBeInSet,\n        core.ExpectColumnStdevToBeBetween,\n        core.ExpectColumnSumToBeBetween,\n        core.ExpectColumnKLDivergenceToBeLessThan,\n        core.ExpectColumnQuantileValuesToBeBetween,\n        core.ExpectColumnValueLengthsToBeBetween,\n        core.ExpectColumnValueLengthsToEqual,\n        core.ExpectColumnValueZScoresToBeLessThan,\n        core.ExpectColumnValuesToBeBetween,\n        core.ExpectColumnValuesToMatchLikePattern,\n        core.ExpectColumnValuesToMatchLikePatternList,\n        core.ExpectColumnValuesToMatchRegex,\n        core.ExpectColumnValuesToMatchRegexList,\n        core.ExpectColumnValuesToNotBeInSet,\n        core.ExpectColumnValuesToNotBeNull,\n        core.ExpectColumnValuesToNotMatchLikePattern,\n        core.ExpectColumnValuesToNotMatchLikePatternList,\n        core.ExpectColumnValuesToNotMatchRegex,\n        core.ExpectColumnValuesToNotMatchRegexList,\n        core.UnexpectedRowsExpectation,\n    ]\n    for x in supported_expectations:\n        schema_path = expectation_dir.joinpath(f\"{x.__name__}.json\")\n        json_str = x.schema_json(indent=indent) + \"\\n\"  # type: ignore[attr-defined] # FIXME low priority\n        if sync:\n            schema_path.write_text(json_str)\n            print(f\"🔃  {x.__name__}.json updated\")\n\n    raise invoke.Exit(code=0)\n\n\ndef _exit_with_error_if_not_in_repo_root(task_name: str):\n    \"\"\"Exit if the command was not run from the repository root.\"\"\"\n    filedir = os.path.realpath(\n        os.path.dirname(os.path.realpath(__file__))  # noqa: PTH120\n    )\n    curdir = os.path.realpath(os.getcwd())  # noqa: PTH109\n    exit_message = f\"The {task_name} task must be invoked from the same directory as the tasks.py file at the top of the repo.\"  # noqa: E501\n    if filedir != curdir:\n        raise invoke.Exit(\n            exit_message,\n            code=1,\n        )\n\n\n@invoke.task\ndef api_docs(ctx: Context):\n    \"\"\"Build api documentation.\"\"\"\n\n    repo_root = pathlib.Path(__file__).parent\n\n    _exit_with_error_if_not_run_from_correct_dir(task_name=\"docs\", correct_dir=repo_root)\n    sphinx_api_docs_source_dir = repo_root / \"docs\" / \"sphinx_api_docs_source\"\n\n    doc_builder = SphinxInvokeDocsBuilder(\n        ctx=ctx, api_docs_source_path=sphinx_api_docs_source_dir, repo_root=repo_root\n    )\n\n    doc_builder.build_docs()\n\n\n@invoke.task(\n    name=\"docs\",\n    help={\n        \"build\": \"Build docs via yarn build instead of serve via yarn start. Default False.\",\n        \"start\": \"Only run yarn start, do not process versions. For example if you have already run invoke docs and just want to serve docs locally for editing.\",  # noqa: E501\n        \"lint\": \"Run the linter\",\n        \"clear\": \"Delete the docs' generated assets, caches, and build artifacts.\",\n    },\n)\ndef docs(\n    ctx: Context,\n    build: bool = False,\n    start: bool = False,\n    lint: bool = False,\n    version: str | None = None,\n    clear: bool = False,\n):\n    \"\"\"Build documentation site, including api documentation and earlier doc versions. Note: Internet access required to download earlier versions.\"\"\"  # noqa: E501\n    from packaging.version import parse as parse_version\n\n    from docs.docs_build import DocsBuilder\n\n    repo_root = pathlib.Path(__file__).parent\n\n    _exit_with_error_if_not_run_from_correct_dir(task_name=\"docs\", correct_dir=repo_root)\n\n    print(\"Running invoke docs from:\", repo_root)\n    old_cwd = pathlib.Path.cwd()\n    docusaurus_dir = repo_root / \"docs/docusaurus\"\n    os.chdir(docusaurus_dir)\n\n    if lint:\n        ctx.run(\" \".join([\"yarn lint\"]), echo=True)\n    elif version:\n        docs_builder = DocsBuilder(ctx, docusaurus_dir)\n        docs_builder.create_version(version=parse_version(version))\n    elif start:\n        ctx.run(\" \".join([\"yarn start\"]), echo=True)\n    elif clear:\n        ctx.run(\" \".join([\"yarn\", \"clear\"]), echo=True)\n    else:\n        docs_builder = DocsBuilder(ctx, docusaurus_dir)\n        print(\"Making sure docusaurus dependencies are installed.\")\n        ctx.run(\" \".join([\"yarn install\"]), echo=True)\n\n        if build:\n            print(\"Running build_docs from:\", docusaurus_dir)\n            docs_builder.build_docs()\n        else:\n            print(\"Running build_docs_locally from:\", docusaurus_dir)\n            docs_builder.build_docs_locally()\n\n    os.chdir(old_cwd)\n\n\n@invoke.task(\n    name=\"public-api\",\n    help={\n        \"write_to_file\": \"Write items to be addressed to public_api_report.txt, default False\",\n    },\n)\ndef public_api_task(\n    ctx: Context,\n    write_to_file: bool = False,\n):\n    \"\"\"Generate a report to determine the state of our Public API. Lists classes, methods and functions that are used in examples in our documentation, and any manual includes or excludes (see public_api_report.py). Items listed when generating this report need the @public_api decorator (and a good docstring) or to be excluded from consideration if they are not applicable to our Public API.\"\"\"  # noqa: E501\n\n    repo_root = pathlib.Path(__file__).parent\n\n    _exit_with_error_if_not_run_from_correct_dir(task_name=\"public-api\", correct_dir=repo_root)\n\n    # Docs folder is not reachable from install of Great Expectations\n    api_docs_dir = repo_root / \"docs\" / \"sphinx_api_docs_source\"\n    sys.path.append(str(api_docs_dir.resolve()))\n\n    public_api_report.generate_public_api_report(write_to_file=write_to_file)\n\n\ndef _exit_with_error_if_not_run_from_correct_dir(\n    task_name: str, correct_dir: Union[pathlib.Path, None] = None\n) -> None:\n    \"\"\"Exit if the command was not run from the correct directory.\"\"\"\n    if not correct_dir:\n        correct_dir = pathlib.Path(__file__).parent\n    curdir = pathlib.Path.cwd()\n    exit_message = (\n        f\"The {task_name} task must be invoked from the same directory as the tasks.py file.\"\n    )\n    if correct_dir != curdir:\n        raise invoke.Exit(\n            exit_message,\n            code=1,\n        )\n\n\n@invoke.task(\n    aliases=(\"automerge\",),\n)\ndef show_automerges(ctx: Context):\n    \"\"\"Show github pull requests currently in automerge state.\"\"\"\n    import requests\n\n    url = \"https://api.github.com/repos/great-expectations/great_expectations/pulls\"\n    response = requests.get(\n        url,\n        params={  # type: ignore[arg-type]\n            \"state\": \"open\",\n            \"sort\": \"updated\",\n            \"direction\": \"desc\",\n            \"per_page\": 50,\n        },\n    )\n    LOGGER.debug(f\"{response.request.method} {response.request.url} - {response}\")\n\n    if response.status_code != requests.codes.ok:\n        print(f\"Error: {response.reason}\\n{pf(response.json(), depth=2)}\")\n        response.raise_for_status()\n\n    pr_details = response.json()\n    LOGGER.debug(pf(pr_details, depth=2))\n\n    if automerge_prs := tuple(x for x in pr_details if x[\"auto_merge\"]):\n        print(f\"\\tAutomerge PRs: {len(automerge_prs)}\")\n        for i, pr in enumerate(automerge_prs, start=1):\n            print(f\"{i}. @{pr['user']['login']} {pr['title']} {pr['html_url']}\")\n    else:\n        print(\"\\tNo PRs set to automerge\")\n\n\nclass TestDependencies(NamedTuple):\n    requirement_files: tuple[str, ...]\n    services: tuple[str, ...] = tuple()\n    extra_pytest_args: tuple[  # TODO: remove this once remove the custom flagging system\n        str, ...\n    ] = tuple()\n\n\nMARKER_DEPENDENCY_MAP: Final[Mapping[str, TestDependencies]] = {\n    \"athena\": TestDependencies((\"reqs/requirements-dev-athena.txt\",)),\n    \"aws_deps\": TestDependencies((\"reqs/requirements-dev-lite.txt\",)),\n    \"bigquery\": TestDependencies((\"reqs/requirements-dev-bigquery.txt\",)),\n    \"clickhouse\": TestDependencies((\"reqs/requirements-dev-clickhouse.txt\",)),\n    \"cloud\": TestDependencies(\n        (\n            \"reqs/requirements-dev-cloud.txt\",\n            \"reqs/requirements-dev-snowflake.txt\",\n            \"reqs/requirements-dev-spark.txt\",\n        ),\n        services=(\n            \"mercury\",\n            \"spark\",\n        ),\n        extra_pytest_args=(\"--cloud\",),\n    ),\n    \"databricks\": TestDependencies(\n        requirement_files=(\"reqs/requirements-dev-databricks.txt\",),\n    ),\n    \"docs-basic\": TestDependencies(\n        # these installs are handled by the CI\n        requirement_files=(\n            \"reqs/requirements-dev-test.txt\",\n            \"reqs/requirements-dev-mssql.txt\",\n            \"reqs/requirements-dev-mysql.txt\",\n            \"reqs/requirements-dev-postgresql.txt\",\n            # \"Deprecated API features detected\" warning/error for test_docs[split_data_on_whole_table_bigquery] when pandas>=2.0  # noqa: E501\n            \"reqs/requirements-dev-sqlalchemy1.txt\",\n            \"reqs/requirements-dev-trino.txt\",\n        ),\n        services=(\"postgresql\", \"mssql\", \"mysql\", \"trino\"),\n        extra_pytest_args=(\n            \"--mssql\",\n            \"--mysql\",\n            \"--postgresql\",\n            \"--trino\",\n            \"--docs-tests\",\n        ),\n    ),\n    \"docs-creds-needed\": TestDependencies(\n        # these installs are handled by the CI\n        requirement_files=(\n            \"reqs/requirements-dev-test.txt\",\n            \"reqs/requirements-dev-azure.txt\",\n            \"reqs/requirements-dev-bigquery.txt\",\n            \"reqs/requirements-dev-redshift.txt\",\n            \"reqs/requirements-dev-snowflake.txt\",\n            # \"Deprecated API features detected\" warning/error for test_docs[split_data_on_whole_table_bigquery] when pandas>=2.0  # noqa: E501\n            \"reqs/requirements-dev-sqlalchemy1.txt\",\n        ),\n        extra_pytest_args=(\n            \"--aws\",\n            \"--azure\",\n            \"--bigquery\",\n            \"--redshift\",\n            \"--snowflake\",\n            \"--docs-tests\",\n        ),\n    ),\n    \"docs-spark\": TestDependencies(\n        requirement_files=(\n            \"reqs/requirements-dev-test.txt\",\n            \"reqs/requirements-dev-spark.txt\",\n        ),\n        services=(\"spark\",),\n        extra_pytest_args=(\"--spark\", \"--docs-tests\"),\n    ),\n    \"mssql\": TestDependencies(\n        (\"reqs/requirements-dev-mssql.txt\",),\n        services=(\"mssql\",),\n        extra_pytest_args=(\"--mssql\",),\n    ),\n    \"mysql\": TestDependencies(\n        (\"reqs/requirements-dev-mysql.txt\",),\n        services=(\"mysql\",),\n        extra_pytest_args=(\"--mysql\",),\n    ),\n    \"pyarrow\": TestDependencies((\"reqs/requirements-dev-arrow.txt\",)),\n    \"postgresql\": TestDependencies(\n        (\"reqs/requirements-dev-postgresql.txt\",),\n        services=(\"postgresql\",),\n        extra_pytest_args=(\"--postgresql\",),\n    ),\n    \"snowflake\": TestDependencies(\n        requirement_files=(\"reqs/requirements-dev-snowflake.txt\",),\n    ),\n    \"spark\": TestDependencies(\n        requirement_files=(\"reqs/requirements-dev-spark.txt\",),\n        services=(\"spark\",),\n        extra_pytest_args=(\"--spark\",),\n    ),\n    \"spark_connect\": TestDependencies(\n        requirement_files=(\n            \"reqs/requirements-dev-spark.txt\",\n            \"reqs/requirements-dev-spark-connect.txt\",\n        ),\n        services=(\"spark\",),\n        extra_pytest_args=(\"--spark_connect\",),\n    ),\n    \"trino\": TestDependencies(\n        (\"reqs/requirements-dev-trino.txt\",),\n        services=(\"trino\",),\n        extra_pytest_args=(\"--trino\",),\n    ),\n}\n\n\ndef _add_all_backends_marker(marker_string: str) -> bool:\n    # We should generalize this, possibly leveraging MARKER_DEPENDENCY_MAP, but for now\n    # right I've hardcoded all the containerized backend services we support in testing.\n    return marker_string in [\n        \"postgresql\",\n        \"mssql\",\n        \"mysql\",\n        \"spark\",\n        \"trino\",\n    ]\n\n\ndef _tokenize_marker_string(marker_string: str) -> Generator[str, None, None]:\n    \"\"\"_summary_\n\n    Args:\n        marker_string (str): _description_\n\n    Yields:\n        Generator[str, None, None]: _description_\n    \"\"\"\n    tokens = marker_string.split()\n    if len(tokens) == 1:\n        yield tokens[0]\n    elif marker_string == \"athena or openpyxl or pyarrow or project or sqlite or aws_creds\":\n        yield \"aws_creds\"\n        yield \"athena\"\n        yield \"openpyxl\"\n        yield \"pyarrow\"\n        yield \"project\"\n        yield \"sqlite\"\n    else:\n        raise ValueError(f\"Unable to tokenize marker string: {marker_string}\")  # noqa: TRY003\n\n\ndef _get_marker_dependencies(markers: str | Sequence[str]) -> list[TestDependencies]:\n    if isinstance(markers, str):\n        markers = [markers]\n    dependencies: list[TestDependencies] = []\n    for marker_string in markers:\n        for marker_token in _tokenize_marker_string(marker_string):\n            if marker_depedencies := MARKER_DEPENDENCY_MAP.get(marker_token):\n                LOGGER.debug(f\"'{marker_token}' has dependencies\")\n                dependencies.append(marker_depedencies)\n    return dependencies\n\n\n@invoke.task(\n    iterable=[\"markers\", \"requirements_dev\"],\n    help={\n        \"markers\": \"Optional marker to install dependencies for. Can be specified multiple times.\",\n        \"requirements_dev\": \"Short name of `requirements-dev-*.txt` file to install, e.g. test, spark, cloud, etc. Can be specified multiple times.\",  # noqa: E501\n        \"constraints\": \"Optional flag to install dependencies with constraints, default True\",\n        \"gx_install\": \"Install the local version of Great Expectations.\",\n        \"editable_install\": \"Install an editable local version of Great Expectations.\",\n        \"force_reinstall\": \"Force re-installation of dependencies.\",\n    },\n)\ndef deps(  # noqa: C901 - too complex\n    ctx: Context,\n    markers: list[str],\n    requirements_dev: list[str],\n    constraints: bool = True,\n    gx_install: bool = False,\n    editable_install: bool = False,\n    force_reinstall: bool = False,\n):\n    \"\"\"\n    Install dependencies for development and testing.\n\n    Specific requirement files needed for a specific test marker can be registered in `MARKER_DEPENDENCY_MAP`,\n    `invoke deps` will always check for and use these when installing dependencies.\n\n    If no `markers` or `requirements-dev` are specified, the dev-contrib and\n    core requirements are installed.\n\n    Example usage:\n    Installing the needed dependencies for running the `external_sqldialect` tests and\n    the 'requirements-dev-cloud.txt' dependencies.\n\n    $ invoke deps -m external_sqldialect -r cloud\n    \"\"\"  # noqa: E501\n    cmds = [\"pip\", \"install\"]\n    if editable_install:\n        cmds.append(\"-e .\")\n    elif gx_install:\n        cmds.append(\".\")\n\n    if force_reinstall:\n        cmds.append(\"--force-reinstall\")\n\n    req_files: list[str] = [\"requirements.txt\"]\n\n    for test_deps in _get_marker_dependencies(markers):\n        req_files.extend(test_deps.requirement_files)\n\n    for name in requirements_dev:\n        req_path: pathlib.Path = REQS_DIR / f\"requirements-dev-{name}.txt\"\n        assert req_path.exists(), f\"Requirement file {req_path} does not exist\"\n        req_files.append(str(req_path))\n\n    if not markers and not requirements_dev:\n        req_files.append(\"reqs/requirements-dev-contrib.txt\")\n\n    for req_file in req_files:\n        cmds.append(f\"-r {req_file}\")\n\n    if constraints:\n        cmds.append(\"-c constraints-dev.txt\")\n\n    ctx.run(\" \".join(cmds), echo=True, pty=True)\n\n\n@invoke.task(iterable=[\"service_names\", \"up_services\", \"verbose\"])\ndef docs_snippet_tests(\n    ctx: Context,\n    marker: str,\n    up_services: bool = False,\n    verbose: bool = False,\n    reports: bool = False,\n):\n    pytest_cmds = [\n        \"pytest\",\n        \"-rEf\",\n    ]\n    if reports:\n        pytest_cmds.extend([\"--cov=great_expectations\", \"--cov-report=xml\"])\n\n    if verbose:\n        pytest_cmds.append(\"-vv\")\n\n    for test_deps in _get_marker_dependencies(marker):\n        if up_services:\n            service(ctx, names=test_deps.services, markers=test_deps.services)\n\n        for extra_pytest_arg in test_deps.extra_pytest_args:\n            pytest_cmds.append(extra_pytest_arg)\n\n    pytest_cmds.append(\"tests/integration/test_script_runner.py\")\n    ctx.run(\" \".join(pytest_cmds), echo=True, pty=True)\n\n\n@invoke.task(\n    help={\n        \"pty\": _PTY_HELP_DESC,\n        \"reports\": \"Generate coverage & result reports to be uploaded to codecov\",\n        \"W\": \"Warnings control\",\n    },\n    iterable=[\"service_names\", \"up_services\", \"verbose\"],\n)\ndef ci_tests(  # noqa: C901 - too complex (9)\n    ctx: Context,\n    marker: str,\n    up_services: bool = False,\n    restart_services: bool = False,\n    verbose: bool = False,\n    reports: bool = False,\n    slowest: int = 5,\n    timeout: float = 0.0,  # 0 indicates no timeout\n    xdist: bool = False,\n    W: str | None = None,\n    pty: bool = True,\n):\n    \"\"\"\n    Run tests in CI.\n\n    This method looks up the pytest marker provided and runs the tests for that marker,\n    as well as looking up any required services, testing dependencies and extra CLI flags\n    that are need and starting them if `up_services` is True.\n\n    `up_services` is False by default to avoid starting services which may already be up\n    when running tests locally.\n\n    `restart_services` is False by default to avoid always restarting the services.\n\n    Defined this as a new invoke task to avoid some of the baggage of our old test setup.\n    \"\"\"\n    pytest_options = [f\"--durations={slowest}\", \"-rEf\"]\n\n    if xdist:\n        pytest_options.append(\"-n 4\")\n\n    if timeout != 0:\n        pytest_options.append(f\"--timeout={timeout}\")\n\n    if reports:\n        pytest_options.extend(\n            [\"--cov=great_expectations\", \"--cov-report=xml\", \"--junitxml=junit.xml\"]\n        )\n\n    if verbose:\n        pytest_options.append(\"-vv\")\n\n    if W:\n        # https://docs.python.org/3/library/warnings.html#describing-warning-filters\n        pytest_options.append(f\"-W={W}\")\n\n    for test_deps in _get_marker_dependencies(marker):\n        if restart_services or up_services:\n            service(\n                ctx,\n                names=test_deps.services,\n                markers=test_deps.services,\n                restart_services=restart_services,\n                pty=pty,\n            )\n\n        for extra_pytest_arg in test_deps.extra_pytest_args:\n            pytest_options.append(extra_pytest_arg)\n\n    marker_statement = (\n        f\"'all_backends or {marker}'\" if _add_all_backends_marker(marker) else f\"'{marker}'\"\n    )\n\n    pytest_cmd = [\"pytest\", \"-m\", marker_statement] + pytest_options\n    ctx.run(\" \".join(pytest_cmd), echo=True, pty=pty)\n\n\n@invoke.task(\n    aliases=(\"services\",),\n    help={\"pty\": _PTY_HELP_DESC},\n    iterable=[\"names\", \"markers\"],\n)\ndef service(\n    ctx: Context,\n    names: Sequence[str],\n    markers: Sequence[str],\n    restart_services: bool = False,\n    pty: bool = True,\n):\n    \"\"\"\n    Startup a service, by referencing its name directly or by looking up a pytest marker.\n\n    If a marker is specified, the services listed in `MARKER_DEPENDENCY_MAP` will be used.\n\n    If restart_services was passed, the containers will be stopped and re-built.\n\n    Note:\n        The main reason this is a separate task is to make it easy to start services\n        when running tests locally.\n    \"\"\"\n    service_names = set(names)\n\n    if markers:\n        for test_deps in _get_marker_dependencies(markers):\n            service_names.update(test_deps.services)\n\n    if service_names:\n        print(f\"  Starting services for {', '.join(service_names)} ...\")\n        for service_name in service_names:\n            cmds = []\n\n            if service_name == \"mercury\" and os.environ.get(\"CI\") != \"true\":\n                cmds.extend(\n                    [\n                        \"FORCE_NO_ALIAS=true\",\n                        \"assume\",\n                        \"dev\",\n                        \"--exec\",\n                        \"'aws ecr get-login-password --region us-east-1'\",\n                        \"|\",\n                        \"docker\",\n                        \"login\",\n                        \"--username\",\n                        \"AWS\",\n                        \"--password-stdin\",\n                        \"258143015559.dkr.ecr.us-east-1.amazonaws.com\",\n                        \"&&\",\n                    ]\n                )\n\n            if restart_services:\n                print(f\"  Removing existing containers and building latest for {service_name} ...\")\n                cmds.extend(\n                    [\n                        \"docker\",\n                        \"compose\",\n                        \"-f\",\n                        f\"assets/docker/{service_name}/docker-compose.yml\",\n                        \"rm\",\n                        \"-fsv\",\n                        \"&&\",\n                        \"docker\",\n                        \"compose\",\n                        \"-f\",\n                        f\"assets/docker/{service_name}/docker-compose.yml\",\n                        \"build\",\n                        \"--pull\",\n                        \"&&\",\n                    ]\n                )\n\n            cmds.extend(\n                [\n                    \"docker\",\n                    \"compose\",\n                    \"-f\",\n                    f\"assets/docker/{service_name}/docker-compose.yml\",\n                    \"up\",\n                    \"-d\",\n                    \"--quiet-pull\",\n                    \"--wait\",\n                    \"--wait-timeout 120\",\n                ]\n            )\n            ctx.run(\" \".join(cmds), echo=True, pty=pty)\n        # TODO: Add healthchecks to services that require this sleep and then remove it.\n        #       This is a temporary hack to give services enough time to come up before moving on.\n        ctx.run(\"sleep 15\")\n    else:\n        print(\"  No matching services to start\")\n\n\n@invoke.task()\ndef print_public_api(ctx: Context):\n    \"\"\"Prints to STDOUT all of our public api.\"\"\"\n    # Walk the GX package to make sure we import all submodules to ensure we\n    # retrieve all things decorated with our public api decorator.\n    import great_expectations\n\n    for module_info in pkgutil.walk_packages([\"great_expectations\"], prefix=\"great_expectations.\"):\n        importlib.import_module(module_info.name)\n    print(great_expectations._docs_decorators.public_api_introspector)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "versioneer.py",
          "type": "blob",
          "size": 66.7861328125,
          "content": "# Version: 0.18\n\n\"\"\"The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone \"update\nthe embedded version string\" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github's\n  \"tarball from tag\" feature\n* a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n  about recent \"tags\" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n\"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a 'setup.py sdist' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `['version']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project's version\n  string. The default \"pep440\" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n  below for alternative styles.\n\n* `['full-revisionid']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n\n* `['date']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `['error']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of \"unknown\".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an \"about\" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()['version']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, \"pep440\", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional \"local\nversion\" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\ntree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\nthat this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. \"0.11\".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of \"0+unknown\". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  \"master\" and \"slave\" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other languages) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n\"Entry-point scripts\" (`setup(entry_points={\"console_scripts\": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It's not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons \"Public Domain\nDedication\" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n\"\"\"\n\n\nimport configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_root():\n    \"\"\"Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    \"\"\"\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, \"setup.py\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow 'python path/to/setup.py COMMAND'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, \"setup.py\")\n        versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\n            \"Versioneer was unable to run the project root directory. \"\n            \"Versioneer requires setup.py to be executed from \"\n            \"its immediate directory (like 'python setup.py COMMAND'), \"\n            \"or in a way that lets it use sys.argv[0] to find the root \"\n            \"(like 'python path/to/setup.py COMMAND').\"\n        )\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # \"versioneer\" may be imported multiple times, and python's shared\n        # module-import table will cache the first one. So we can't use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(\n                \"Warning: build in %s is using versioneer.py from %s\"\n                % (os.path.dirname(me), versioneer_py)\n            )\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, \"setup.cfg\")\n    parser = configparser.ConfigParser()\n    with open(setup_cfg) as f:\n        parser.read_file(f)\n    VCS = parser.get(\"versioneer\", \"VCS\")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(\"versioneer\", name):\n            return parser.get(\"versioneer\", name)\n        return None\n\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, \"style\") or \"\"\n    cfg.versionfile_source = get(parser, \"versionfile_source\")\n    cfg.versionfile_build = get(parser, \"versionfile_build\")\n    cfg.tag_prefix = get(parser, \"tag_prefix\")\n    if cfg.tag_prefix in (\"''\", '\"\"'):\n        cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = get(parser, \"parentdir_prefix\")\n    cfg.verbose = get(parser, \"verbose\")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Decorator to mark a method as the handler for a particular VCS.\"\"\"\n\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except OSError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(f\"unable to find command, tried {commands}\")\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\n    \"git\"\n] = r'''\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n    git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n    git_date = \"%(DOLLAR)sFormat:%%ci%(DOLLAR)s\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"%(STYLE)s\"\n    cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n    cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n    cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Decorator to mark a method as the handler for a particular VCS.\"\"\"\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %%s\" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %%s\" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %%s (error)\" %% dispcmd)\n            print(\"stdout was %%s\" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\"version\": dirname[len(parentdir_prefix):],\n                    \"full-revisionid\": None,\n                    \"dirty\": False, \"error\": None, \"date\": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\"Tried directories %%s but none started with prefix %%s\" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"full\"] = mo.group(1)\n            if line.strip().startswith(\"git_date =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"date\"] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if not keywords:\n        raise NotThisMethod(\"no keywords at all, weird\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # git-2.2.0 added \"%%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = set([r for r in refs if re.search(r'\\d', r)])\n        if verbose:\n            print(\"discarding '%%s', no digits\" %% \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(\"picking %%s\" %% r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None,\n                    \"date\": date}\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %%s not under git control\" %% root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                          \"--always\", \"--long\",\n                                          \"--match\", \"%%s*\" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                    cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"],\n                       cwd=root)[0].strip()\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post.dev%%d\" %% pieces[\"distance\"]\n    else:\n        # exception #1\n        rendered = \"0.post.dev%%d\" %% pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%%s'\" %% style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None,\n            \"date\": pieces.get(\"date\")}\n\n\ndef get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n                \"dirty\": None,\n                \"error\": \"unable to find root of source tree\",\n                \"date\": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to compute version\", \"date\": None}\n'''\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs)\n        for line in f.readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"full\"] = mo.group(1)\n            if line.strip().startswith(\"git_date =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"date\"] = mo.group(1)\n        f.close()\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if not keywords:\n        raise NotThisMethod(\"no keywords at all, weird\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r\"\\d\", r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS,\n        [\n            \"describe\",\n            \"--tags\",\n            \"--dirty\",\n            \"--always\",\n            \"--long\",\n            \"--match\",\n            \"%s*\" % tag_prefix,\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '{}' doesn't start with prefix '{}'\".format(\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"], cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[\n        0\n    ].strip()\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    \"\"\"Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith(\".pyc\") or me.endswith(\".pyo\"):\n            me = os.path.splitext(me)[0] + \".py\"\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = \"versioneer.py\"\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open(\".gitattributes\")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if \"export-subst\" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except OSError:\n        pass\n    if not present:\n        f = open(\".gitattributes\", \"a+\")\n        f.write(\"%s export-subst\\n\" % versionfile_source)\n        f.close()\n        files.append(\".gitattributes\")\n    run_command(GITS, [\"add\", \"--\"] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            \"Tried directories %s but none started with prefix %s\"\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\nSHORT_VERSION_PY = \"\"\"\n# This file was generated by 'versioneer.py' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = '''\n%s\n'''  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n\"\"\"\n\n\ndef versions_from_file(filename):\n    \"\"\"Try to determine the version from _version.py if present.\"\"\"\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except OSError:\n        raise NotThisMethod(\"unable to read _version.py\")\n    mo = re.search(\n        r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S\n    )\n    if not mo:\n        mo = re.search(\n            r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S\n        )\n    if not mo:\n        raise NotThisMethod(\"no version_json in _version.py\")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    \"\"\"Write the given version number to the given _version.py file.\"\"\"\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True, indent=1, separators=(\",\", \": \"))\n    with open(filename, \"w\") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(\"set {} to '{}'\".format(filename, versions[\"version\"]))\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post.dev%d\" % pieces[\"distance\"]\n    else:\n        # exception #1\n        rendered = \"0.post.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\nclass VersioneerBadRootError(Exception):\n    \"\"\"The project root directory is unknown or missing key files.\"\"\"\n\n\ndef get_versions(verbose=False):\n    \"\"\"Get the project version from whatever source is available.\n\n    Returns dict with two keys: 'version' and 'full'.\n    \"\"\"\n    if \"versioneer\" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[\"versioneer\"]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert (\n        cfg.versionfile_source is not None\n    ), \"please set versioneer.versionfile_source\"\n    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. 'git\n    # describe'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by 'setup.py sdist',\n    # and for users of a tarball/zipball created by 'git archive' or github's\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(\"get_keywords\")\n    from_keywords_f = handlers.get(\"keywords\")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(\"got version from expanded keyword %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(f\"got version from file {versionfile_abs} {ver}\")\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(\"got version from VCS %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(\"got version from parentdir %s\" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(\"unable to compute version\")\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n\n\ndef get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]\n\n\ndef get_cmdclass():\n    \"\"\"Get the custom setuptools/distutils subclasses used by Versioneer.\"\"\"\n    if \"versioneer\" in sys.modules:\n        del sys.modules[\"versioneer\"]\n        # this fixes the \"python setup.py develop\" case (also 'install' and\n        # 'easy_install .'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n        # sys.modules by the time B's setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it's pre-build state, so the\n        # parent is protected against the child's \"import versioneer\". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent's versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add \"version\" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = \"report generated version string\"\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(\"Version: %s\" % vers[\"version\"])\n            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n            print(\" dirty: %s\" % vers.get(\"dirty\"))\n            print(\" date: %s\" % vers.get(\"date\"))\n            if vers[\"error\"]:\n                print(\" error: %s\" % vers[\"error\"])\n\n    cmds[\"version\"] = cmd_version\n\n    # we override \"build_py\" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn't copied too, 'git describe' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different \"build_py\" commands for both environments\n    if \"setuptools\" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n    cmds[\"build_py\"] = cmd_build_py\n\n    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        # nczeczulin reports that py2exe won't like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION\n        #   \"product_version\": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            \"DOLLAR\": \"$\",\n                            \"STYLE\": cfg.style,\n                            \"TAG_PREFIX\": cfg.tag_prefix,\n                            \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                            \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[\"build_exe\"] = cmd_build_exe\n        del cmds[\"build_py\"]\n\n    if \"py2exe\" in sys.modules:  # py2exe enabled?\n        from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            \"DOLLAR\": \"$\",\n                            \"STYLE\": cfg.style,\n                            \"TAG_PREFIX\": cfg.tag_prefix,\n                            \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                            \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[\"py2exe\"] = cmd_py2exe\n\n    # we override different \"sdist\" commands for both environments\n    if \"setuptools\" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[\"version\"]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(\n                target_versionfile, self._versioneer_generated_versions\n            )\n\n    cmds[\"sdist\"] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = \"\"\"\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n\"\"\"\n\nSAMPLE_CONFIG = \"\"\"\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n\"\"\"\n\nINIT_PY_SNIPPET = \"\"\"\nfrom ._version import get_versions\n__version__ = get_versions()['version']\ndel get_versions\n\"\"\"\n\n\ndef do_setup():\n    \"\"\"Main VCS-independent setup function for installing Versioneer.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print(\" creating %s\" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, \"w\") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(\n            LONG\n            % {\n                \"DOLLAR\": \"$\",\n                \"STYLE\": cfg.style,\n                \"TAG_PREFIX\": cfg.tag_prefix,\n                \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n            }\n        )\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), \"__init__.py\")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy) as f:\n                old = f.read()\n        except OSError:\n            old = \"\"\n        if INIT_PY_SNIPPET not in old:\n            print(\" appending to %s\" % ipy)\n            with open(ipy, \"a\") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print(\" %s unmodified\" % ipy)\n    else:\n        print(\" %s doesn't exist, ok\" % ipy)\n        ipy = None\n\n    # Make sure both the top-level \"versioneer.py\" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they'll be copied into source distributions. Pip won't be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, \"MANIFEST.in\")\n    simple_includes = set()\n    try:\n        with open(manifest_in) as f:\n            for line in f:\n                if line.startswith(\"include \"):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except OSError:\n        pass\n    # That doesn't cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant 'include'\n    # lines is safe, though.\n    if \"versioneer.py\" not in simple_includes:\n        print(\" appending 'versioneer.py' to MANIFEST.in\")\n        with open(manifest_in, \"a\") as f:\n            f.write(\"include versioneer.py\\n\")\n    else:\n        print(\" 'versioneer.py' already in MANIFEST.in\")\n    if cfg.versionfile_source not in simple_includes:\n        print(\n            \" appending versionfile_source ('%s') to MANIFEST.in\"\n            % cfg.versionfile_source\n        )\n        with open(manifest_in, \"a\") as f:\n            f.write(\"include %s\\n\" % cfg.versionfile_source)\n    else:\n        print(\" versionfile_source already in MANIFEST.in\")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    \"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n    found = set()\n    setters = False\n    errors = 0\n    with open(\"setup.py\") as f:\n        for line in f.readlines():\n            if \"import versioneer\" in line:\n                found.add(\"import\")\n            if \"versioneer.get_cmdclass()\" in line:\n                found.add(\"cmdclass\")\n            if \"versioneer.get_version()\" in line:\n                found.add(\"get_version\")\n            if \"versioneer.VCS\" in line:\n                setters = True\n            if \"versioneer.versionfile_source\" in line:\n                setters = True\n    if len(found) != 3:\n        print(\"\")\n        print(\"Your setup.py appears to be missing some important items\")\n        print(\"(but I might be wrong). Please make sure it has something\")\n        print(\"roughly like the following:\")\n        print(\"\")\n        print(\" import versioneer\")\n        print(\" setup( version=versioneer.get_version(),\")\n        print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n        print(\"\")\n        errors += 1\n    if setters:\n        print(\"You should remove lines like 'versioneer.VCS = ' and\")\n        print(\"'versioneer.versionfile_source = ' . This configuration\")\n        print(\"now lives in setup.cfg, and should be removed from setup.py\")\n        print(\"\")\n        errors += 1\n    return errors\n\n\nif __name__ == \"__main__\":\n    cmd = sys.argv[1]\n    if cmd == \"setup\":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n"
        }
      ]
    }
  ]
}