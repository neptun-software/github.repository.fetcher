{
  "metadata": {
    "timestamp": 1736561128494,
    "page": 69,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "recommenders-team/recommenders",
      "stars": 19592,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.8720703125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n*.pretrain*\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\nlicense.txt\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\njunit\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Locust files:\nlocustfile.py\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env*\n.venv*\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# Tensorflow\n*model_checkpoints\n**/outputs\n\n# Azure ML\nconfig.json\naml_config/\naml_scripts/\naml_data/\n\n# Spark\nspark-warehouse/\n\n##########################\n.DS_Store\n.~*\nUntitled*.ipynb\n*-Copy*.ipynb\n~$*\noutput.ipynb\nconda*.yaml\nreco_*.yaml\n.idea/\n*.npz\n*.data\n*.dat\n*.csv\n*.zip\n*.7z\n.vscode/\nu.item\nml-100k/\nml-10M100K/\nml-1m/\nml-20m/\n*.jar\n*.item\n*.pkl\n*.pdf\n.pretrain\n*.npy\n*.ckpt*\n*.png\n*.jpg\n*.jpeg\n*.gif\n*.model\n*.mml\nnohup.out\n*.svg\n*.html\n*.js\n*.css\n*.tff\n*.woff\n*.woff2\n*.eot\n\n#####  kdd 2020 tutorial data folder\nexamples/07_tutorials/KDD2020-tutorial/data_folder/\n\n*.vec\n*.tsv\n*.sh\n\ntests/**/resources/\nreports/\n\n### pip folders\npip-wheel*\n"
        },
        {
          "name": "AUTHORS.md",
          "type": "blob",
          "size": 6.2666015625,
          "content": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n\nContributors to Recommenders \n============================\nRecommenders is developed and maintained by a community of people interested in exploring recommendation algorithms and how best to deploy them in industry settings. The goal is to accelerate the workflow of any individual or organization working on recommender systems. Everyone is encouraged to contribute at any level to add and improve the implemented algorithms, notebooks and utilities.\n\n<p align=\"center\">\n  <img src=\"https://contributors-img.web.app/image?repo=recommenders-team/recommenders\" width = 500/>\n</p>\n\nMaintainers (sorted alphabetically)\n---------------------------------------\nMaintainers are actively supporting the project and have made substantial contributions to the repository.<br>\nThey have admin access to the repo and provide support reviewing issues and pull requests.\n\n* **[Andreas Argyriou](https://github.com/anargyri)**\n   * SAR single node improvements\n   * Reco utils metrics computations\n   * Tests for Surprise\n   * Model selection notebooks (AzureML for SVD, NNI) \n* **[Jianxun Lian](https://github.com/Leavingseason)**\n   * xDeepFM algorithm\n   * DKN algorithm\n   * Review, development and optimization of MSRA algorithms.\n* **[Jun Ki Min](https://github.com/loomlike)**\n   * ALS notebook\n   * Wide & Deep algorithm\n   * Hyperparameter tuning notebooks\n* **[Miguel González-Fierro](https://github.com/miguelfierro)**\n   * Recommendation algorithms review, development and optimization.\n   * Reco utils review, development and optimization.\n   * Continuous integration build / test setup.\n* **[Scott Graham](https://github.com/gramhagen)**\n   * Improving documentation\n   * VW notebook\n* **[Simon Zhao](https://github.com/simonyansenzhao)**\n   * SARplus algorithm upgrade\n* **[Tao Wu](https://github.com/wav8k)**\n   * Improving documentation\n\n\nContributors  (sorted alphabetically)\n-------------------------------------\n[Full List of Contributors](https://github.com/Microsoft/Recommenders/graphs/contributors)\n\nTo contributors: please add your name to the list when you submit a patch to the project.\n\n* **[Aaron He](https://github.com/AaronHeee)**\n   * Reco utils of NCF\n   * Deep dive notebook demonstrating the use of NCF\n* **[Aaron Palpallatoc](https://github.com/ubergonmx)**\n   * Corrected variable in pickle dump in `mind_utils.ipynb` notebook\n* **[Abir Chakraborty](https://github.com/aeroabir)**\n   * Self-Attentive Sequential Recommendation (SASRec)\n   * Sequential Recommendation Via Personalized Transformer (SSEPT)\n* **[Alexandros Ioannou](https://github.com/aioannou96)**\n   * Standard VAE algorithm \n   * Multinomial VAE algorithm  \n* **[Bamdev Mishra](https://github.com/bamdevm)**\n   * RLRMC algorithm\n   * GeoIMC algorithm\n* **[Beth Zeranski](https://github.com/bethz)**\n   * DevOps Pipelines used as a control plane to run existing Pytests on AzureML\n   * Automation scripts to configure AzureML environment for pipeline use \n* **[Chuyang Ke](https://github.com/ChuyangKe)**\n   * Reco utils optimization\n   * Performance tests\n* **[Dan Bianchini](https://github.com/danb27)**\n   * SAR Single Node algorithm improvements\n* **[Dan Ciborowski](https://github.com/dciborow)**\n   * ALS operationalization notebook\n   * SAR PySpark improvement\n* **[Daniel Schneider](https://github.com/danielsc)**\n   * FastAI notebook\n* **[David Davó](https://github.com/daviddavo)**\n   * Added R-Precision metric\n* **[Evgenia Chroni](https://github.com/EvgeniaChroni)**\n   * Multinomial VAE algorithm\n   * Standard VAE algorithm\n* **[Gianluca Campanella](https://github.com/gcampanella)**\n   * Spark optimization and support\n* **[Heather Spetalnick (Shapiro)](https://github.com/heatherbshapiro)**\n   * AzureML documentation and support\n* **[Jeremy Reynolds](https://github.com/jreynolds01)**\n   * Reference architecture\n* **[Jianjie Liu](https://github.com/laserprec/)**\n   * GitHub Action Migration\n   * Test Infrastructure Optimization\n* **[Kaisar Mussalim](https://github.com/kmussalim)**\n   * Multinomial VAE algorithm\n   * Standard VAE algorithm\n* **[Le Zhang](https://github.com/yueguoguo)**\n   * Reco utils\n   * Continuous integration build / test setup\n   * Quickstart, deep dive, algorithm comparison, notebooks\n* **[Markus Cozowicz](https://github.com/eisber)**\n   * SAR improvements on Spark\n* **[Max Kaznady](https://github.com/maxkazmsft)**\n   * Early SAR single node code and port from another internal codebase\n   * Early SAR on Spark-SQL implementation\n   * SAR notebooks\n   * SAR unit / integration / smoke tests\n   * Early infrastructure design based on collapsing another internal project\n* **[Mirco Milletarì](https://github.com/WessZumino)**\n   * Restricted Boltzmann Machine algorithm\n* **[Nicolas Hug](https://github.com/NicolasHug)**\n   * Jupyter notebook demonstrating the use of [Surprise](https://github.com/NicolasHug/Surprise) library for recommendations\n* **[Nikhil Joglekar](https://github.com/nikhilrj)**\n   * Improving documentation\n   * Quick start notebook\n   * Operationalization notebook\n* **[Nile Wilson](https://github.com/niwilso)**\n   * Term Frequency - Inverse Document Frequency (TF-IDF) quickstart, utils\n* **[Pradnyesh Vineet Joshi](https://github.com/pradnyeshjoshi)**\n   * GitHub workflows to trigger unit, smoke and integration tests in parallel on AzureML\n   * Scripts to configure AzureML environment\n* **[Pratik Jawanpuria](https://github.com/pratikjawanpuria)**\n   * RLRMC algorithm\n   * GeoIMC algorithm\n* **[Qi Wan](https://github.com/Qcactus)**\n   * LightGCN algorithm\n   * Deep dive notebook demonstrating the use of LightGCN\n* **[Quoc-Tuan Truong](https://github.com/tqtg)**\n   * BPR notebook using [Cornac](https://github.com/PreferredAI/cornac) framework\n   * BiVAE notebook using [Cornac](https://github.com/PreferredAI/cornac) framework\n* **[Robert Alexander](https://github.com/roalexan)**\n   * Windows test pipelines\n* **[Satyadev Ntv](https://github.com/satyadevntv)**\n   * GeoIMC algorithm\n* **[Yan Zhang](https://github.com/YanZhangADS)**\n   * Diversity metrics including coverage, novelty, diversity, and serendipity\n   * Diversity metrics evaluation sample notebook\n* **[Yassine Khelifi](https://github.com/datashinobi)**\n   * SAR notebook quickstart\n* **[Zhenhui Xu](https://github.com/motefly)**\n   * Reco utils of LightGBM\n   * LightGBM notebook quickstart\n    \n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 4.11328125,
          "content": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n\n# Recommenders Code of Conduct\n\nThis code of conduct outlines expectations for participation in the Recommenders open source community, as well as steps for reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all. People violating this code of conduct may be banned from the community. Our open source community strives to:\n\n* **Be friendly and patient**: Remember you might not be communicating in someone else's primary spoken or programming language, and others may not have your level of understanding.\n\n* **Be welcoming**: Our community welcomes and supports people of all backgrounds and identities. This includes, but is not limited to members of any race, ethnicity, culture, national origin, color, immigration status, social and economic class, educational level, sex, sexual orientation, gender identity and expression, age, size, family status, political belief, religion, and mental and physical ability.\n\n* **Be respectful**: We are a world-wide community of professionals, and we conduct ourselves professionally. Disagreement is no excuse for poor behavior and poor manners. Disrespectful and unacceptable behavior includes, but is not limited to:\n    1. Violent threats or language.\n    1. Discriminatory or derogatory jokes and language.\n    1. Posting sexually explicit or violent material.\n    1. Posting, or threatening to post, people's personally identifying information (\"doxing\").\n    1. Insults, especially those using discriminatory terms or slurs.\n    1. Behavior that could be perceived as sexual attention.\n    1. Advocating for or encouraging any of the above behaviors.\n\n* **Understand disagreements**: Disagreements, both social and technical, are useful learning opportunities. Seek to understand the other viewpoints and resolve differences constructively.\n\n* **Remember that we’re different**. The strength of our community comes from its diversity, people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn’t mean that they’re wrong. Focus on helping to resolve issues and learning from mistakes.\n\n* This code is not exhaustive or complete. It serves to capture our common understanding of a productive, collaborative environment. We expect the code to be followed in spirit as much as in the letter.\n\n## Reporting Code of Conduct Issues\n\nWe encourage all communities to resolve issues on their own whenever possible. This builds a broader and deeper understanding and ultimately a healthier interaction. In the event that an issue cannot be resolved locally, please feel free to report your concerns by contacting conduct@lfai.foundation. In your report please include:\n\n1. Your contact information.\n1. Names (real, usernames or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well.\n1. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g. a mailing list archive or a public chat log), please include a link or attachment.\n1. Any additional information that may be helpful.\n\nAll reports will be reviewed by a multi-person team and will result in a response that is deemed necessary and appropriate to the circumstances. Where additional perspectives are needed, the team may seek insight from others with relevant expertise or experience. The confidentiality of the person reporting the incident will be kept at all times. Involved parties are never part of the review team.\n\nAnyone asked to stop unacceptable behavior is expected to comply immediately. If an individual engages in unacceptable behavior, the review team may take any action they deem appropriate, including a permanent ban from the community.\n\n*This code of conduct is based on the [template](http://todogroup.org/opencodeofconduct) established by the [TODO Group](http://todogroup.org/) and used by numerous other large communities and the Scope section from the [Contributor Covenant version 1.4](http://contributor-covenant.org/version/1/4/).*\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 6.177734375,
          "content": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n\n# Contribution Guidelines\n\nContributions are welcomed! Here's a few things to know:\n\n- [Steps to Contributing](#steps-to-contributing)\n- [Ideas for Contributions](#ideas-for-contributions)\n  - [A first contribution](#a-first-contribution)\n  - [Datasets](#datasets)\n  - [Models](#models)\n  - [Metrics](#metrics)\n  - [General tips](#general-tips)\n- [Coding Guidelines](#coding-guidelines)\n- [Code of Conduct](#code-of-conduct)\n  - [Do not point fingers](#do-not-point-fingers)\n  - [Provide code feedback based on evidence](#provide-code-feedback-based-on-evidence)\n  - [Ask questions do not give answers](#ask-questions-do-not-give-answers)\n\n## Steps to Contributing\n\n**TL;DR for contributing: We use the staging branch to land all new features and fixes. To make a contribution, please create a branch from staging, make a modification in the code and create a PR to staging.** \n\nHere are the basic steps to get started with your first contribution. Please reach out with any questions.\n1. Use [open issues](https://github.com/Microsoft/Recommenders/issues) to discuss the proposed changes. Create an issue describing changes if necessary to collect feedback. Also, please use provided labels to tag issues so everyone can easily sort issues of interest.\n1. [Fork the repo](https://help.github.com/articles/fork-a-repo/) so you can make and test local changes.\n1. Create a new branch **from staging branch** for the issue (please do not create a branch from main). We suggest prefixing the branch with your username and then a descriptive title: (e.g. `gramhagen/update_contributing_docs`)\n1. Install recommenders package locally using the right optional dependency for your test and the dev option. (e.g. gpu test: `pip install -e .[gpu,dev]`)\n1. Create a test that replicates the issue.\n1. Make code changes.\n1. Ensure unit tests pass and code style / formatting is consistent (see [wiki](https://github.com/Microsoft/Recommenders/wiki/Coding-Guidelines#python-and-docstrings-style) for more details).\n1. When adding code to the repo, make sure you sign the commits, otherwise the tests will fail (see [how to sign the commits](https://github.com/recommenders-team/recommenders/wiki/How-to-sign-commits)).\n1. Create a pull request against **staging** branch.\n\nSee the wiki for more details about our [merging strategy](https://github.com/microsoft/recommenders/wiki/Strategy-to-merge-the-code-to-main-branch).\n\n## Ideas for Contributions\n\n### A first contribution\n\nFor people who are new to open source or to Recommenders, a good way to start is by contribution with documentation. You can help with any of the README files or in the notebooks.\n\nFor more advanced users, consider fixing one of the bugs listed in the issues.\n\n### Datasets\n\nTo contribute new datasets, please consider this:\n\n* Minimize dependencies, it's better to use `requests` library than a custom library.\n* Make sure that the dataset is publicly available and that the license allows for redistribution.\n\n### Models\n\nTo contribute new models, please consider this:\n\n* Please don't add models that are already implemented in the repo. An exception to this rule is if you are adding a more optimal implementation or you want to migrate a model from TensorFlow to PyTorch.\n* Prioritize the minimal code necessary instead of adding a full library. If you add code from another repository, please make sure to follow the license and give proper credit.\n* All models should be accompanied by a notebook that shows how to use the model and how to train it. The notebook should be in the [examples](examples) folder.\n* The model should be tested with unit tests, and the notebooks should be tested with functional tests.\n\n### Metrics\n\nTo contribute new metrics, please consider this:\n\n* A good way to contribute with metrics is by optimizing the code of the existing ones.\n* If you are adding a new metric, please consider adding not only a CPU version, but also a PySpark version.\n* When adding the tests, make sure you check for the limits. For example, if you add an error metric, check that the error between two identical datasets is zero.\n\n### General tips\n\n* Prioritize PyTorch over TensorFlow.\n* Minimize dependencies. Around 80% of the issues in the repo are related to dependencies.\n* Avoid adding code with GPL and other copyleft licenses. Prioritize MIT, Apache, and other permissive licenses.\n* Add the copyright statement at the beginning of the file: `Copyright (c) Recommenders contributors. Licensed under the MIT License.`\n\n## Coding Guidelines\n\nWe strive to maintain high quality code to make the utilities in the repository easy to understand, use, and extend. We also work hard to maintain a friendly and constructive environment. We've found that having clear expectations on the development process and consistent style helps to ensure everyone can contribute and collaborate effectively.\n\nPlease review the [Coding Guidelines](https://github.com/recommenders-team/recommenders/wiki/Coding-Guidelines) wiki page to see more details about the expectations for development approach and style.\n\n## Code of Conduct\n\nApart from the official [Code of Conduct](CODE_OF_CONDUCT.md), in Recommenders team we adopt the following behaviors, to ensure a great working environment:\n\n### Do not point fingers\nLet’s be constructive.\n\n<details>\n<summary><em>Click here to see some examples</em></summary>\n\n\"This method is missing docstrings\" instead of \"YOU forgot to put docstrings\".\n\n</details>\n\n### Provide code feedback based on evidence \n\nWhen making code reviews, try to support your ideas based on evidence (papers, library documentation, stackoverflow, etc) rather than your personal preferences. \n\n<details>\n<summary><em>Click here to see some examples</em></summary>\n\n\"When reviewing this code, I saw that the Python implementation of the metrics are based on classes, however, [scikit-learn](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) use functions. We should follow the standard in the industry.\"\n\n</details>\n\n### Ask questions do not give answers\nTry to be empathic. \n\n<details>\n<summary><em>Click here to see some examples</em></summary>\n\n* Would it make more sense if ...?\n* Have you considered this ... ?\n\n</details>\n\n"
        },
        {
          "name": "GLOSSARY.md",
          "type": "blob",
          "size": 7.390625,
          "content": "<!--\r\nCopyright (c) Recommenders contributors.\r\nLicensed under the MIT License.\r\n-->\r\n\r\n# Glossary\r\n\r\n* **A/B testing**: Methodology to evaluate the performance of a system in production. In the context of Recommendation Systems it is used to measure a machine learning model performance in real-time. It works by randomizing an environment response into two groups A and B, typically half of the traffic goes to the machine learning model output and the other half is left without model. By comparing the metrics from A and B branches, it is possible to evaluate whether it is beneficial the use of the model or not. A test with more than two groups it is named Multi-Variate Test.\r\n\r\n* **Click-through rate (CTR)**: Ratio of the number of users who click on a link over the total number of users that visited the page. CTR is a measure of the user engagement.\r\n\r\n* **Cold-start problem**: The cold start problem concerns the recommendations for users with no or few past history (new users). Providing recommendations to users with small past history becomes a difficult problem for collaborative filtering models because their learning and predictive ability is limited. Multiple research have been conducted in this direction using content-based filtering models. These models use auxiliary information like user or item metadata to overcome the cold start problem.\r\n\r\n* **Collaborative filtering algorithms (CF)**: CF algorithms make prediction of what is the likelihood of a user selecting an item based on the behavior of other users [1]. It assumes that if user A likes item X and Y, and user B likes item X, user B would probably like item Y. See the [list of CF examples in Recommenders repository](examples/02_model_collaborative_filtering).\r\n\r\n* **Content-based filtering algorithms (CB)**: CB algorithms make prediction of what is the likelihood of a user selecting an item based on the similarity of users and items among themselves [1]. It assumes that if user A lives in country X, has age Y and likes item Z, and user B lives in country X and has age Y, user B would probably like item Z. See the [list of CB examples in Recommenders repository](examples/02_model_content_based_filtering).\r\n\r\n* **Conversion rate**: In the context of e-commerce, the conversion rate is the ratio between the number of conversions (e.g. number of bought items) over the total number of visits. In the context of recommendation systems, conversion rate measures how efficient is an algorithm to provide recommendations that the user buys.\r\n\r\n* **Diversity metrics**: In the context of Recommendation Systems,  diversity applies to a set of items, and is related to how different the items are with respect to each other [4].\r\n\r\n* **Explicit interaction data**: When a user explicitly rate an item, typically between 1-5, the user is giving a value on the likeliness of the item. \r\n\r\n* **Implicit interaction data**: Implicit interactions are views or clicks that show a certain interest of the user about a specific items. These kind of data is more common but it doesn't define the intention of the user as clearly as the explicit data.\r\n\r\n* **Item information**: These include information about the item, some examples can be name, description, price, etc.\r\n\r\n* **Knowledge graph algorithms**: A knowledge graph algorithm is the one that uses knowledge graph data. In comparison with standard algorithms, it allows to explore graph's latent connections and improve the precision of results; the various relations in the graph can extend users' interest and increase the diversity of recommended items; also, these algorithms bring explainability to recommendation systems [5].\r\n\r\n* **Knowledge graph data**: A knowledge graph is a directed heterogeneous graph in which nodes correspond to entities (items or item attributes) and edges correspond to relations [5].\r\n\r\n* **Long tail items**: Typically, the item interaction distribution has the form of long tail, where items in the tail have a small number of interactions, corresponding to unpopular items, and items in the head have a large number of interactions [1,2]. From the algorithmic point of view, items in the tail suffer from the cold-start problem, making them hard for recommendation systems to use. However, from the business point of view, the items in the tail can be highly profitable, since these items are less popular, business can apply a higher margin to them. Recommendation systems that optimize metrics like novelty and diversity, can help to find users willing to get these long tail items. \r\n\r\n* **Multi-Variate Test (MVT)**: Methodology to evaluate the performance of a system in production. It is similar to A/B testing, with the difference that instead of having two test groups, MVT has multiples groups.\r\n\r\n* **News Information**: These include information about the news, some examples can be title, body, verticle, etc.\r\n\r\n* **Novelty metrics**: In Recommendation Systems, the novelty of a piece of information generally refers to how different it is with respect to \"what has been previously seen\" [4].\r\n\r\n* **Online metrics**: Also named business metrics. They are the metrics computed online that reflect how the Recommendation System is helping the business to improve user engagement or revenue. These metrics include CTR, conversion rate, etc.\r\n\r\n* **Offline metrics**: Metrics computed offline for measuring the performance of the machine learning model. These metrics include ranking, rating, diversity and novelty metrics.\r\n\r\n* **Ranking metrics**: These are used to evaluate how relevant recommendations are for users. They include precision at k, recall at k, nDCG and MAP. See the [list of metrics in Recommenders repository](examples/03_evaluate).\r\n\r\n* **Rating metrics**: These are used to evaluate how accurate a recommender is at predicting ratings that users give to items. They include RMSE, MAE, R squared or explained variance. See the [list of metrics in Recommenders repository](examples/03_evaluate).\r\n\r\n* **Revenue per order**: The revenue per order optimization objective is the default optimization objective for the \"Frequently bought together\" recommendation model type. This optimization objective cannot be specified for any other recommendation model type.\r\n\r\n* **User information**: These include all information that define the user, some examples can be name, address, email, demographics, etc. \r\n\r\n\r\n## References and resources\r\n\r\n[1] Aggarwal, Charu C. \"Recommender systems\". Vol. 1. Cham: Springer International Publishing, 2016.\r\n\r\n[2]. Park, Yoon-Joo, and Tuzhilin, Alexander. \"The long tail of recommender systems and how to leverage it.\" In Proceedings of the 2008 ACM conference on Recommender systems, pp. 11-18. 2008. [Link to paper](http://people.stern.nyu.edu/atuzhili/pdf/Park-Tuzhilin-RecSys08-final.pdf).\r\n\r\n[3]. Armstrong, Robert. \"The long tail: Why the future of business is selling less of more.\" Canadian Journal of Communication 33, no. 1 (2008). [Link to paper](https://www.cjc-online.ca/index.php/journal/article/view/1946/3141).\r\n\r\n[4] Castells, P., Vargas, S., and Wang, Jun. \"Novelty and diversity metrics for recommender systems: choice, discovery and relevance.\" (2011). [Link to paper](https://repositorio.uam.es/bitstream/handle/10486/666094/novelty_castells_DDR_2011.pdf?sequence=1).\r\n\r\n[5] Wang, Hongwei; Zhao, Miao; Xie, Xing; Li, Wenjie and Guo, Minyi. \"Knowledge Graph Convolutional Networks for Recommender Systems\". The World Wide Web Conference WWW'19. 2019. [Link to paper](https://arxiv.org/abs/1904.12575).\r\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.18359375,
          "content": "    MIT License\n\n    Copyright (c) 2018-present Microsoft Corporation.\n    Copyright (c) 2023-present Recommenders contributors.\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0302734375,
          "content": "include recommenders/README.md\n"
        },
        {
          "name": "NEWS.md",
          "type": "blob",
          "size": 10.0302734375,
          "content": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n\n# What's New\n\n## Update December 23, 2024\n\nWe have a new release [Recommenders 1.2.1](https://github.com/recommenders-team/recommenders/releases/tag/1.2.1)!\n\nWe fixed a lot of bugs due to dependencies, improved security, reviewed the notebooks and the libraries.\n\n## Update May 2, 2024\n\nWe have a new release [Recommenders 1.2.0](https://github.com/microsoft/recommenders/releases/tag/1.2.0)!\n\nSo many changes since our last release. We have full tests on Python 3.8 to 3.11 (around 1800 tests), upgraded performance in many algorithms, reviewed notebooks, and many more improvements.\n\n\n## Update October 10, 2023\n\nWe are pleased to announce that this repository (formerly known as Microsoft Recommenders, https://github.com/microsoft/recommenders), has joined the [Linux Foundation of AI and Data](https://lfaidata.foundation/) (LF AI & Data)! The new organization, `recommenders-team`, reflects this change.\n\nWe hope this move makes it easy for anyone to contribute! Our objective continues to be building an ecosystem and a community to sustain open source innovations and collaborations in recommendation systems. \n\nNow to access the repo, instead of going to https://github.com/microsoft/recommenders, you need to go to https://github.com/recommenders-team/recommenders. The old URL will still resolve to the new one, but we recommend that you update your bookmarks.\n\n## Update August 18, 2023\n\nWe moved to a new organization! Now to access the repo, instead of going to https://github.com/microsoft/recommenders, you need to go to https://github.com/recommenders-team/recommenders. The old URL will still resolve to the new one, but we recommend you to update your bookmarks.\n\n## Update February 7, 2023 \n\nWe reached 15,000 stars!!\n\n## Update July 20, 2022\n\nWe have a new release [Recommenders 1.1.1](https://github.com/microsoft/recommenders/releases/tag/1.1.1)! \n\nWe have introduced a new way of testing our repository using [AzureML](https://azure.microsoft.com/en-us/services/machine-learning/). With AzureML we are able to distribute our tests to different machines and run them in parallel. This allows us to test our repository on a wider range of machines and provides us with a much faster test cycle. Our total computation time went from around 9h to 35min, and we were able to reduce the costs by half. See more details [here](tests/README.md).\n\nWe also made other improvements like faster evaluation metrics and improving SAR algorithm. \n\n## Update April 1, 2022\n\nWe have a new release [Recommenders 1.1.0](https://github.com/microsoft/recommenders/releases/tag/1.1.0)! \nWe have introduced the SASRec and SSEPT algorithms that are based on transformers. \nIn addition, we now have enabled Python 3.8 and 3.9.\nWe have also made improvements on the SARPlus algorithm, including support for Azure Synapse and Spark 3.2.\nThere are also bug fixes and improvements on NCF, RBM, LightGBM, LightFM, Scikit-Surprise, the stratified splitter, dockerfile \nand upgrade to Scikit-Learn 1.0.2.\n\n## Update January 13, 2022\n\nWe have a new release [Recommenders 1.0.0](https://github.com/microsoft/recommenders/releases/tag/1.0.0)! The codebase has now migrated to TensorFlow versions 2.6 / 2.7 and to Spark version 3. In addition, there are a few changes in the dependencies and extras installed by `pip` (see [this guide](recommenders/README.md#optional-dependencies)). We have also made improvements in the code and the CI / CD pipelines.\n\n## Update September 27, 2021\n\nWe have a new release [Recommenders 0.7.0](https://github.com/microsoft/recommenders/releases/tag/0.7.0)!\n\nIn this, we have changed the names of the folders which contain the source code, so that they are more informative. This implies that you will need to change any import statements that reference the recommenders package. Specifically, the folder `reco_utils` has been renamed to `recommenders` and its subfolders have been renamed according to [issue 1390](https://github.com/microsoft/recommenders/issues/1390).  \n\nThe recommenders package now supports three types of environments: [venv](https://docs.python.org/3/library/venv.html), [virtualenv](https://virtualenv.pypa.io/en/latest/index.html#) and [conda](https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment) with Python versions 3.6 and 3.7.\n\nWe have also added new evaluation metrics: _novelty, serendipity, diversity and coverage_ (see the [evalution notebooks](examples/03_evaluate/README.md)).\n\nCode coverage reports are now generated for every PR, using [Codecov](https://about.codecov.io/).\n\n## Update June 21, 2021\n\nWe have a new release [Recommenders 0.6.0](https://github.com/microsoft/recommenders/releases/tag/0.6.0)!\n\nRecommenders is now on PyPI and can be installed using pip! In addition there are lots of bug fixes and utilities improvements.\n\nHere you can find the PyPi page: https://pypi.org/project/recommenders/\n\nHere you can find the package documentation: https://microsoft-recommenders.readthedocs.io/en/latest/\n\n## Update June 1, 2021\n\nWe have surpassed 10k stars!\n\nMicrosoft Recommenders repository has reached 10k stars and has become the most starred open-source recommender system project on GitHub.\n\nMany thanks and congratulations to all the contributors to this repository! More advanced algorithms and best practices are yet to come!\n\n## Update February 4, 2021\n\nWe have a new release [Recommenders 0.5.0](https://github.com/microsoft/recommenders/releases/tag/0.5.0)!\n\nIt comes with lots of bug fixes, optimizations and 3 new algorithms, GeoIMC, Standard VAE and Multinomial VAE. We also added tools to facilitate the use of Microsoft News dataset (MIND). In addition, we published our KDD2020 tutorial where we built a recommender of COVID papers using Microsoft Academic Graph.\n\nWe also changed the default branch from master to main. Now when you download the repo, you will get main branch.\n\n## Update October 19, 2020\n\nLeaderboard Reopen!\n\n[Microsoft News Recommendation Competition Winners Announced](https://msnews.github.io/competition.html)\n\nCongratulations to all participants and [winners](https://msnews.github.io/competition.html#winner) of the Microsoft News Recommendation Competition!  In the last two months, over 200 participants from more than 90 institutions in 19 countries and regions joined the competition and collectively advanced the state of the art of news recommendation.\n\nThe competition is based on the recently released [MIND dataset](https://msnews.github.io/), an open, large-scale English news dataset with impression logs.  Details of the dataset are available at this [ACL paper](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf).\n\nWith the competition successfully closed, the [leaderboard](https://msnews.github.io/competition.html#leaderboard) is now reopn.  Want to see if you can grab the top spot? Get familiar with the [news recommendation scenario](https://github.com/microsoft/recommenders/tree/main/scenarios/news).  Then dive into some baselines such as [DKN](examples/00_quick_start/dkn_MIND.ipynb), [LSTUR](examples/00_quick_start/lstur_MIND.ipynb), [NAML](examples/00_quick_start/naml_MIND.ipynb), [NPA](examples/00_quick_start/npa_MIND.ipynb) and [NRMS](examples/00_quick_start/nrms_MIND.ipynb) and start hacking!\n\n## Update October 5, 2020\n\n[Microsoft News Recommendation Competition Winners Announced, Leaderboard to Reopen!](https://msnews.github.io/competition.html)\n\nCongratulations to all participants and [winners](https://msnews.github.io/competition.html#winner) of the Microsoft News Recommendation Competition!  In the last two months, over 200 participants from more than 90 institutions in 19 countries and regions joined the competition and collectively advanced the state of the art of news recommendation.\n\nThe competition is based on the recently released [MIND dataset](https://msnews.github.io/), an open, large-scale English news dataset with impression logs.  Details of the dataset are available at this [ACL paper](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf).\n\nWith the competition successfully closed, the [leaderboard](https://msnews.github.io/competition.html#leaderboard) will reopen soon.  Want to see if you can grab the top spot? Get familiar with the [news recommendation scenario](https://github.com/microsoft/recommenders/tree/main/scenarios/news).  Then dive into some baselines such as [DKN](examples/00_quick_start/dkn_MIND.ipynb), [LSTUR](examples/00_quick_start/lstur_MIND.ipynb), [NAML](examples/00_quick_start/naml_MIND.ipynb), [NPA](examples/00_quick_start/npa_MIND.ipynb) and [NRMS](examples/00_quick_start/nrms_MIND.ipynb) and get ready!\n\n## Update July 20, 2020\n\nMicrosoft is hosting a News Recommendation competition based on the [MIND dataset](https://msnews.github.io/), a large-scale English news dataset with impression logs. Check out the [ACL paper](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf), get familiar with the [news recommendation scenario](https://github.com/microsoft/recommenders/tree/main/scenarios/news), and dive into the [quick start example](examples/00_quick_start/dkn_MIND.ipynb) using the DKN algorithm. Then try some other algorithms (NAML, NPA, NRMS, LSTUR) and tools in recommenders and submit your entry!\n\n## Update August 20, 2020\n\nNew release: [Recommenders 0.4.0](https://github.com/microsoft/recommenders/releases/tag/0.4.0)\n\n13 new algos and multiple fixes and new features\n\n## Update September 18, 2019\n\nNew release: [Recommenders 0.3.1](https://github.com/microsoft/recommenders/releases/tag/0.3.1)\n\n## Update September 15, 2019\n\nWe reached 5000 stars!!\n\n## Update June 3, 2019\n\nNew release: [Recommenders 0.3.0](https://github.com/microsoft/recommenders/releases/tag/0.3.0)\n\n## Update February 20, 2019\n\nNew release: [Recommenders 0.2.0](https://github.com/microsoft/recommenders/releases/tag/0.2.0)\n\n## Update February 11, 2019\n\nWe reached 1000 stars!!\n\n## Update December 12, 2018\n\nFirst release: [Recommenders 0.1.1](https://github.com/microsoft/recommenders/releases/tag/0.1.1)\n\n## Update November 12, 2018\n\nFirst pre-release: [Recommenders 0.1.0](https://github.com/microsoft/recommenders/releases/tag/0.1.0)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.4521484375,
          "content": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n<img src=\"https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg\" width=\"800\">\n\n\n[![Documentation status](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment)\n[![License](https://img.shields.io/github/license/recommenders-team/recommenders.svg)](https://github.com/recommenders-team/recommenders/blob/main/LICENSE)\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![PyPI Version](https://img.shields.io/pypi/v/recommenders.svg?logo=pypi&logoColor=white)](https://pypi.org/project/recommenders)\n[![Python Versions](https://img.shields.io/pypi/pyversions/recommenders.svg?logo=python&logoColor=white)](https://pypi.org/project/recommenders)\n\n[<img align=\"left\" width=\"300\" src=\"https://raw.githubusercontent.com/recommenders-team/artwork/main/mix/join_recommenders_slack.svg\">](https://join.slack.com/t/lfaifoundation/shared_invite/zt-2iyl7zyya-g5rOO5K518CBoevyi28W6w)\n\n<br>\n\n## What's New (Dec, 2024)\n\nWe have a new release [Recommenders 1.2.1](https://github.com/recommenders-team/recommenders/releases/tag/1.2.1)!\n\nWe fixed a lot of bugs due to dependencies, improved security, reviewed the notebooks and the libraries.\n\n## Introduction\n\nRecommenders objective is to assist researchers, developers and enthusiasts in prototyping, experimenting with and bringing to production a range of classic and state-of-the-art recommendation systems.\n\nRecommenders is a project under the [Linux Foundation of AI and Data](https://lfaidata.foundation/projects/). \n\nThis repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail our learnings on five key tasks:\n\n- [Prepare Data](examples/01_prepare_data): Preparing and loading data for each recommendation algorithm.\n- [Model](examples/00_quick_start): Building models using various classical and deep learning recommendation algorithms such as Alternating Least Squares ([ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS)) or eXtreme Deep Factorization Machines ([xDeepFM](https://arxiv.org/abs/1803.05170)).\n- [Evaluate](examples/03_evaluate): Evaluating algorithms with offline metrics.\n- [Model Select and Optimize](examples/04_model_select_and_optimize): Tuning and optimizing hyperparameters for recommendation models.\n- [Operationalize](examples/05_operationalize): Operationalizing models in a production environment on Azure.\n\nSeveral utilities are provided in [recommenders](recommenders) to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. See the [Recommenders documentation](https://readthedocs.org/projects/microsoft-recommenders/).\n\nFor a more detailed overview of the repository, please see the documents on the [wiki page](https://github.com/microsoft/recommenders/wiki/Documents-and-Presentations).\n\nFor some of the practical scenarios where recommendation systems have been applied, see [scenarios](scenarios). \n\n## Getting Started\n\nWe recommend [conda](https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment) for environment management, and [VS Code](https://code.visualstudio.com/) for development. To install the recommenders package and run an example notebook on Linux/WSL:\n\n```bash\n# 1. Install gcc if it is not installed already. On Ubuntu, this could done by using the command\n# sudo apt install gcc\n\n# 2. Create and activate a new conda environment\nconda create -n <environment_name> python=3.9\nconda activate <environment_name>\n\n# 3. Install the core recommenders package. It can run all the CPU notebooks.\npip install recommenders\n\n# 4. create a Jupyter kernel\npython -m ipykernel install --user --name <environment_name> --display-name <kernel_name>\n\n# 5. Clone this repo within VSCode or using command line:\ngit clone https://github.com/recommenders-team/recommenders.git\n\n# 6. Within VSCode:\n#   a. Open a notebook, e.g., examples/00_quick_start/sar_movielens.ipynb;  \n#   b. Select Jupyter kernel <kernel_name>;\n#   c. Run the notebook.\n```\n\nFor more information about setup on other platforms (e.g., Windows and macOS) and different configurations (e.g., GPU, Spark and experimental features), see the [Setup Guide](SETUP.md).\n\nIn addition to the core package, several extras are also provided, including:\n+ `[gpu]`: Needed for running GPU models.\n+ `[spark]`: Needed for running Spark models.\n+ `[dev]`: Needed for development for the repo.\n+ `[all]`: `[gpu]`|`[spark]`|`[dev]`\n+ `[experimental]`: Models that are not thoroughly tested and/or may require additional steps in installation.\n\n## Algorithms\n\nThe table below lists the recommendation algorithms currently available in the repository. Notebooks are linked under the Example column as Quick start, showcasing an easy to run example of the algorithm, or as Deep dive, explaining in detail the math and implementation of the algorithm.\n\n| Algorithm | Type | Description | Example |\n|-----------|------|-------------|---------|\n| Alternating Least Squares (ALS) | Collaborative Filtering | Matrix factorization algorithm for explicit or implicit feedback in large datasets, optimized for scalability and distributed computing capability. It works in the PySpark environment. | [Quick start](examples/00_quick_start/als_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/als_deep_dive.ipynb) |\n| Attentive Asynchronous Singular Value Decomposition (A2SVD)<sup>*</sup> | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Cornac/Bayesian Personalized Ranking (BPR) | Collaborative Filtering | Matrix factorization algorithm for predicting item ranking with implicit feedback. It works in the CPU environment. | [Deep dive](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) |\n| Cornac/Bilateral Variational Autoencoder (BiVAE) | Collaborative Filtering | Generative model for dyadic data (e.g., user-item interactions). It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) |\n| Convolutional Sequence Embedding Recommendation (Caser) | Collaborative Filtering | Algorithm based on convolutions that aim to capture both user’s general preferences and sequential patterns. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Deep Knowledge-Aware Network (DKN)<sup>*</sup> | Content-Based Filtering | Deep learning algorithm incorporating a knowledge graph and article embeddings for providing news or article recommendations. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/dkn_MIND.ipynb) / [Deep dive](examples/02_model_content_based_filtering/dkn_deep_dive.ipynb) |\n| Extreme Deep Factorization Machine (xDeepFM)<sup>*</sup> | Collaborative Filtering | Deep learning based algorithm for implicit and explicit feedback with user/item features. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/xdeepfm_criteo.ipynb) |\n| FastAI Embedding Dot Bias (FAST) | Collaborative Filtering | General purpose algorithm with embeddings and biases for users and items. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/fastai_movielens.ipynb) |\n| LightFM/Factorization Machine | Collaborative Filtering | Factorization Machine algorithm for both implicit and explicit feedbacks. It works in the CPU environment. | [Quick start](examples/02_model_collaborative_filtering/lightfm_deep_dive.ipynb) |\n| LightGBM/Gradient Boosting Tree<sup>*</sup> | Content-Based Filtering | Gradient Boosting Tree algorithm for fast training and low memory usage in content-based problems. It works in the CPU/GPU/PySpark environments. | [Quick start in CPU](examples/00_quick_start/lightgbm_tinycriteo.ipynb) / [Deep dive in PySpark](examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb) |\n| LightGCN | Collaborative Filtering | Deep learning algorithm which simplifies the design of GCN for predicting implicit feedback. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) |\n| GeoIMC<sup>*</sup> | Collaborative Filtering | Matrix completion algorithm that takes into account user and item features using Riemannian conjugate gradient optimization and follows a geometric approach. It works in the CPU environment. | [Quick start](examples/00_quick_start/geoimc_movielens.ipynb) |\n| GRU | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using recurrent neural networks. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Multinomial VAE | Collaborative Filtering | Generative model for predicting user/item interactions. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb) |\n| Neural Recommendation with Long- and Short-term User Representations (LSTUR)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/lstur_MIND.ipynb) |\n| Neural Recommendation with Attentive Multi-View Learning (NAML)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with attentive multi-view learning. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/naml_MIND.ipynb) |\n| Neural Collaborative Filtering (NCF) | Collaborative Filtering | Deep learning algorithm with enhanced performance for user/item implicit feedback. It works in the CPU/GPU environment.| [Quick start](examples/00_quick_start/ncf_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) |\n| Neural Recommendation with Personalized Attention (NPA)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with personalized attention network. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/npa_MIND.ipynb) |\n| Neural Recommendation with Multi-Head Self-Attention (NRMS)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with multi-head self-attention. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/nrms_MIND.ipynb) |\n| Next Item Recommendation (NextItNet) | Collaborative Filtering | Algorithm based on dilated convolutions and residual network that aims to capture sequential patterns. It considers both user/item interactions and features.  It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Restricted Boltzmann Machines (RBM) | Collaborative Filtering | Neural network based algorithm for learning the underlying probability distribution for explicit or implicit user/item feedback. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/rbm_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb) |\n| Riemannian Low-rank Matrix Completion (RLRMC)<sup>*</sup> | Collaborative Filtering | Matrix factorization algorithm using Riemannian conjugate gradients optimization with small memory consumption to predict user/item interactions. It works in the CPU environment. | [Quick start](examples/00_quick_start/rlrmc_movielens.ipynb) |\n| Simple Algorithm for Recommendation (SAR)<sup>*</sup> | Collaborative Filtering | Similarity-based algorithm for implicit user/item feedback.  It works in the CPU environment. | [Quick start](examples/00_quick_start/sar_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/sar_deep_dive.ipynb) |\n| Self-Attentive Sequential Recommendation (SASRec) | Collaborative Filtering | Transformer based algorithm for sequential recommendation. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sasrec_amazon.ipynb) |\n| Short-term and Long-term Preference Integrated Recommender (SLi-Rec)<sup>*</sup> | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism, a time-aware controller and a content-aware controller. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Multi-Interest-Aware Sequential User Modeling (SUM)<sup>*</sup> | Collaborative Filtering | An enhanced memory network-based sequential user model which aims to capture users' multiple interests. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Sequential Recommendation Via Personalized Transformer (SSEPT) | Collaborative Filtering | Transformer based algorithm for sequential recommendation with User embedding. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sasrec_amazon.ipynb) |\n| Standard VAE | Collaborative Filtering | Generative Model for predicting user/item interactions.  It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) |\n| Surprise/Singular Value Decomposition (SVD) | Collaborative Filtering | Matrix factorization algorithm for predicting explicit rating feedback in small datasets. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) |\n| Term Frequency - Inverse Document Frequency (TF-IDF) | Content-Based Filtering | Simple similarity-based algorithm for content-based recommendations with text datasets. It works in the CPU environment. | [Quick  start](examples/00_quick_start/tfidf_covid.ipynb) |\n| Vowpal Wabbit (VW)<sup>*</sup> | Content-Based Filtering | Fast online learning algorithms, great for scenarios where user features / context are constantly changing. It uses the CPU for online learning. | [Deep dive](examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb) |\n| Wide and Deep | Collaborative Filtering | Deep learning algorithm that can memorize feature interactions and generalize user features. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/wide_deep_movielens.ipynb) |\n| xLearn/Factorization Machine (FM) & Field-Aware FM (FFM) | Collaborative Filtering | Quick and memory efficient algorithm to predict labels with user/item features. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/fm_deep_dive.ipynb) |\n\n**NOTE**: <sup>*</sup> indicates algorithms invented/contributed by Microsoft.\n\nIndependent or incubating algorithms and utilities are candidates for the [contrib](contrib) folder. This will house contributions which may not easily fit into the core repository or need time to refactor or mature the code and add necessary tests.\n\n| Algorithm | Type | Description | Example |\n|-----------|------|-------------|---------|\n| SARplus <sup>*</sup> | Collaborative Filtering | Optimized implementation of SAR for Spark |  [Quick start](contrib/sarplus/README.md) |\n\n### Algorithm Comparison\n\nWe provide a [benchmark notebook](examples/06_benchmarks/movielens.ipynb) to illustrate how different algorithms could be evaluated and compared. In this notebook, the MovieLens dataset is split into training/test sets at a 75/25 ratio using a stratified split. A recommendation model is trained using each of the collaborative filtering algorithms below. We utilize empirical parameter values reported in literature [here](http://mymedialite.net/examples/datasets.html). For ranking metrics we use `k=10` (top 10 recommended items). We run the comparison on a Standard NC6s_v2 [Azure DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/) (6 vCPUs, 112 GB memory and 1 P100 GPU). Spark ALS is run in local standalone mode. In this table we show the results on Movielens 100k, running the algorithms for 15 epochs.\n\n| Algo | MAP | nDCG@k | Precision@k | Recall@k | RMSE | MAE | R<sup>2</sup> | Explained Variance |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| [ALS](examples/00_quick_start/als_movielens.ipynb) | 0.004732 |\t0.044239 |\t0.048462 |\t0.017796 | 0.965038 |\t0.753001 |\t0.255647 |\t0.251648 |\n| [BiVAE](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) | 0.146126\t| 0.475077 |\t0.411771 |\t0.219145 | N/A |\tN/A |\tN/A |\tN/A |\n| [BPR](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) | 0.132478\t| 0.441997 |\t0.388229 |\t0.212522 | N/A |\tN/A |\tN/A |\tN/A |\n| [FastAI](examples/00_quick_start/fastai_movielens.ipynb) | 0.025503 |\t0.147866 |\t0.130329 |\t0.053824 | 0.943084 |\t0.744337 |\t0.285308 |\t0.287671 |\n| [LightGCN](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) | 0.088526 | 0.419846 | 0.379626 | 0.144336 | N/A | N/A | N/A | N/A |\n| [NCF](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) | 0.107720\t| 0.396118 |\t0.347296 |\t0.180775 | N/A | N/A | N/A | N/A |\n| [SAR](examples/00_quick_start/sar_movielens.ipynb) | 0.110591 |\t0.382461 | \t0.330753 | 0.176385 | 1.253805 | 1.048484 |\t-0.569363 |\t0.030474 |\n| [SVD](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) | 0.012873\t| 0.095930 |\t0.091198 |\t0.032783 | 0.938681 | 0.742690 | 0.291967 | 0.291971 |\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Before contributing, please see our [contribution guidelines](CONTRIBUTING.md).\n\nThis project adheres to this [Code of Conduct](CODE_OF_CONDUCT.md) in order to foster a welcoming and inspiring community for all.\n\n## Build Status\n\nThese tests are the nightly builds, which compute the asynchronous tests. `main` is our principal branch and `staging` is our development branch. We use [pytest](https://docs.pytest.org/) for testing python utilities in [recommenders](recommenders) and the Recommenders [notebook executor](recommenders/utils/notebook_utils.py) for the [notebooks](examples). \n\nFor more information about the testing pipelines, please see the [test documentation](tests/README.md).\n\n### AzureML Nightly Build Status\n\nThe nightly build tests are run daily on AzureML.\n\n| Build Type | Branch | Status |  | Branch | Status |\n| --- | --- | --- | --- | --- | --- |\n| **Linux CPU** | main | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Amain) | | staging | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Astaging) |\n| **Linux GPU** | main | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Amain) | | staging | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Astaging) |\n| **Linux Spark** | main | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Amain) | | staging | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Astaging) |\n\n## References\n\n- **FREE COURSE**: M. González-Fierro, \"Recommendation Systems: A Practical Introduction\", LinkedIn Learning, 2024. [Available on this link](https://www.linkedin.com/learning/recommendation-systems-a-practical-introduction).\n- D. Li, J. Lian, L. Zhang, K. Ren, D. Lu, T. Wu, X. Xie, \"Recommender Systems: Frontiers and Practices\", Springer, Beijing, 2024. [Available on this link](https://www.amazon.com/Recommender-Systems-Frontiers-Practices-Dongsheng/dp/9819989639/).\n- A. Argyriou, M. González-Fierro, and L. Zhang, \"Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems\", *WWW 2020: International World Wide Web Conference Taipei*, 2020. Available online: https://dl.acm.org/doi/abs/10.1145/3366424.3382692\n- S. Graham,  J.K. Min, T. Wu, \"Microsoft recommenders: tools to accelerate developing recommender systems\", *RecSys '19: Proceedings of the 13th ACM Conference on Recommender Systems*, 2019. Available online: https://dl.acm.org/doi/10.1145/3298689.3346967\n- L. Zhang, T. Wu, X. Xie, A. Argyriou, M. González-Fierro and J. Lian, \"Building Production-Ready Recommendation System at Scale\", *ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2019 (KDD 2019)*, 2019.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.9072265625,
          "content": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n\n# Security Policy\n\n## Reporting a Vulnerability\nIf you think you have found a security vulnerability, please send a report to recommenders-security@lists.lfaidata.foundation. \n\nWe don't currently have a PGP key, unfortunately.\n\nA Recommenders committer will send you a response indicating the next steps in handling your report. After the initial reply to your report, the committer will keep you informed of the progress towards a fix and full announcement, and may ask for additional information or guidance.\n\nImportant: Please don't disclose the vulnerability before it has been fixed and announced, to protect our users.\n\n## Security announcements\nPlease subscribe to the [announcements mailing list](https://lists.lfaidata.foundation/g/recommenders-announce), where we post notifications and remediation details for security vulnerabilities."
        },
        {
          "name": "SETUP.md",
          "type": "blob",
          "size": 12.703125,
          "content": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n\n# Setup Guide\n\nThe repo, including this guide, is tested on Linux. Where applicable, we document differences in [Windows](#windows-specific-instructions) and [MacOS](#macos-specific-instructions) although \nsuch documentation may not always be up to date.   \n\n## Extras\n\nIn addition to the pip installable package, several extras are provided, including:\n+ `[gpu]`: Needed for running GPU models.  \n+ `[spark]`: Needed for running Spark models.\n+ `[dev]`: Needed for development.\n+ `[all]`: `[gpu]`|`[spark]`|`[dev]`\n+ `[experimental]`: Models that are not thoroughly tested and/or may require additional steps in installation).\n\n## Setup for Core Package\n\nFollow the [Getting Started](./README.md#Getting-Started) section in the [README](./README.md) to install the package and run the examples.\n\n## Setup for GPU\n\n```bash\n# 1. Make sure CUDA is installed.\n\n# 2. Follow Steps 1-5 in the Getting Started section in README.md to install the package and Jupyter kernel, adding the gpu extra to the pip install command:\npip install recommenders[gpu]\n\n# 3. Within VSCode:\n#   a. Open a notebook with a GPU model, e.g., examples/00_quick_start/wide_deep_movielens.ipynb;\n#   b. Select Jupyter kernel <kernel_name>;\n#   c. Run the notebook.\n```\n\n## Setup for Spark \n\n```bash\n# 1. Make sure JDK is installed.  For example, OpenJDK 11 can be installed using the command\n# sudo apt-get install openjdk-11-jdk\n\n# 2. Follow Steps 1-5 in the Getting Started section in README.md to install the package and Jupyter kernel, adding the spark extra to the pip install command:\npip install recommenders[spark]\n\n# 3. Within VSCode:\n#   a. Open a notebook with a Spark model, e.g., examples/00_quick_start/als_movielens.ipynb;  \n#   b. Select Jupyter kernel <kernel_name>;\n#   c. Run the notebook.\n```\n\n## Setup for Databricks\n\nThe following instructions were tested on Databricks Runtime 15.4 LTS (Apache Spark version 3.5.0), 14.3 LTS (Apache Spark version 3.5.0), 13.3 LTS (Apache Spark version 3.4.1), and 12.2 LTS (Apache Spark version 3.3.2). We have tested the runtime on python 3.9,3.10 and 3.11. \n\nAfter an Databricks cluster is provisioned:\n```bash\n# 1. Go to the \"Compute\" tab on the left of the page, click on the provisioned cluster and then click on \"Libraries\". \n# 2. Click the \"Install new\" button.  \n# 3. In the popup window, select \"PyPI\" as the library source. Enter \"recommenders[examples]\" as the package name. Click \"Install\" to install the package.\n# 4. Now, repeat the step 3 for below packages:\n#   a. numpy<2.0.0\n#   b. pandera<=0.18.3\n#   c. scipy<=1.13.1\n```\n\n### Prepare Azure Databricks for Operationalization\n<!-- TO DO: This is to be verified/updated 23/04/16 -->\nThis repository includes an end-to-end example notebook that uses Azure Databricks to estimate a recommendation model using matrix factorization with Alternating Least Squares, writes pre-computed recommendations to Azure Cosmos DB, and then creates a real-time scoring service that retrieves the recommendations from Cosmos DB. In order to execute that [notebook](examples/05_operationalize/als_movie_o16n.ipynb), you must install the Recommenders repository as a library (as described above), **AND** you must also install some additional dependencies. With the *Quick install* method, you just need to pass an additional option to the [installation script](tools/databricks_install.py).\n\n<details>\n<summary><strong><em>Quick install</em></strong></summary>\n\nThis option utilizes the installation script to do the setup. Just run the installation script\nwith an additional option. If you have already run the script once to upload and install the `Recommenders.egg` library, you can also add an `--overwrite` option:\n\n```{shell}\npython tools/databricks_install.py --overwrite --prepare-o16n <CLUSTER_ID>\n```\n\nThis script does all of the steps described in the *Manual setup* section below.\n\n</details>\n\n<details>\n<summary><strong><em>Manual setup</em></strong></summary>\n\nYou must install three packages as libraries from PyPI:\n\n* `azure-cli==2.0.56`\n* `azureml-sdk[databricks]==1.0.8`\n* `pydocumentdb==2.3.3`\n\nYou can follow instructions [here](https://docs.azuredatabricks.net/user-guide/libraries.html#install-a-library-on-a-cluster) for details on how to install packages from PyPI.\n\nAdditionally, you must install the [spark-cosmosdb connector](https://docs.databricks.com/spark/latest/data-sources/azure/cosmosdb-connector.html) on the cluster. The easiest way to manually do that is to:\n\n\n1. Download the [appropriate jar](https://search.maven.org/remotecontent?filepath=com/azure/cosmos/spark/azure-cosmos-spark_3-1_2-12/4.3.1/azure-cosmos-spark_3-1_2-12-4.3.1.jar) from MAVEN. **NOTE** This is the appropriate jar for spark versions `3.1.X`, and is the appropriate version for the recommended Azure Databricks run-time detailed above. See the [Databricks installation script](https://github.com/microsoft/recommenders/blob/main/tools/databricks_install.py#L45) for other Databricks runtimes.\n2. Upload and install the jar by:\n   1. Log into your `Azure Databricks` workspace\n   2. Select the `Clusters` button on the left.\n   3. Select the cluster on which you want to import the library.\n   4. Select the `Upload` and `Jar` options, and click in the box that has the text `Drop JAR here` in it.\n   5. Navigate to the downloaded `.jar` file, select it, and click `Open`.\n   6. Click on `Install`.\n   7. Restart the cluster.\n\n</details>\n\n\n## Setup for Experimental \n<!-- FIXME FIXME 23/04/01 move to experimental. Have not tested -->\nThe `xlearn` package has dependency on `cmake`. If one uses the `xlearn` related notebooks or scripts, make sure `cmake` is installed in the system. The easiest way to install on Linux is with apt-get: `sudo apt-get install -y build-essential cmake`. Detailed instructions for installing `cmake` from source can be found [here](https://cmake.org/install/). \n\n## Windows-Specific Instructions\n\nFor Spark features to work, make sure Java and Spark are installed and respective environment varialbes such as `JAVA_HOME`, `SPARK_HOME` and `HADOOP_HOME` are set properly. Also make sure environment variables `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` are set to the the same python executable.\n\n## MacOS-Specific Instructions\n\nWe recommend using [Homebrew](https://brew.sh/) to install the dependencies on macOS, including conda (please remember to add conda's path to `$PATH`). One may also need to install lightgbm using Homebrew before pip install the package.\n\nIf zsh is used, one will need to use `pip install 'recommenders[<extras>]'` to install \\<extras\\>.\n\nFor Spark features to work, make sure Java and Spark are installed first. Also make sure environment variables `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` are set to the the same python executable.\n<!-- TO DO: Pytorch m1 mac GPU suppoort -->\n\n## Setup for Developers\n\nIf you want to contribute to Recommenders, please first read the [Contributing Guide](./CONTRIBUTING.md). You will notice that our development branch is `staging`.\n\nTo start developing, you need to install the latest `staging` branch in local, the `dev` package, and any other package you want. For example, for starting developing with GPU models, you can use the following command:\n\n```bash\ngit checkout staging\npip install -e .[dev,gpu]\n```\n\nYou can decide which packages you want to install, if you want to install all of them, you can use the following command:\n\n```bash\ngit checkout staging\npip install -e .[all]\n```\n\nWe also provide a [devcontainer.json](./.devcontainer/devcontainer.json)\nand [Dockerfile](./tools/docker/Dockerfile) for developers to\nfacilitate the development on\n[Dev Containers with VS Code](https://code.visualstudio.com/docs/devcontainers/containers)\nand [GitHub Codespaces](https://github.com/features/codespaces).\n\n<details>\n<summary><strong><em>VS Code Dev Containers</em></strong></summary>\n\nThe typical scenario using Docker containers for development is as\nfollows.  Say, we want to develop applications for a specific\nenvironment, so\n1. we create a contaienr with the dependencies required, \n1. and mount the folder containing the code to the container,\n1. then code parsing, debugging and testing are all performed against\n   the container.\nThis workflow seperates the development environment from your local\nenvironment, so that your local environment won't be affected.  The\ncontainer used here for this end is called Dev Container in the\nVS Code Dev Containers extension.  And the extension eases this\ndevelopment workflow with Docker containers automatically without\npain.\n\nTo use VS Code Dev Containers, your local machine must have the\nfollowing applicatioins installed:\n* [Docker](https://docs.docker.com/get-started/get-docker/)\n* [VS Code Remote Development Extension Pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack)\n\nThen\n* When you open your local Recommenders folder in VS Code, it will\n  detect [devcontainer.json](./.devcontainer/devcontainer.json), and\n  prompt you to **Reopen in Container**.  If you'd like to reopen,\n  it will create a container with the required environment described\n  in `devcontainer.json`, install a VS Code server in the container,\n  and mount the folder into the container.\n  + If you don't see the prompt, you can use the command\n    **Dev Containers: Reopen in Container**\n* If you don't have a local clone of Recommenders, you can also use\n  the command **Dev Containers: Clone Repository in Container Volume**,\n  and type in a branch/PR URL of Recommenders you'd like to develop\n  on, such as https://github.com/recommenders-team/recommenders,\n  https://github.com/recommenders-team/recommenders/tree/staging, or\n  https://github.com/recommenders-team/recommenders/pull/2098.  VS\n  Code will create a container with the environment described in\n  `devcontainer.json`, and clone the specified branch of Recommenders\n  into the container.\n\nOnce everything is set up, VS Code will act as a client to the server\nin the container, and all subsequent operations on VS Code will be\nperformed against the container.\n\n</details>\n\n<details>\n<summary><strong><em>GitHub Codespaces</em></strong></summary>\n\nGitHub Codespaces also uses `devcontainer.json` and Dockerfile in the\nrepo to create the environment on a VM for you to develop on the Web\nVS Code.  To use the GitHub Codespaces on Recommenders, you can go to\n[Recommenders](https://github.com/recommenders-team/recommenders)\n$\\to$ switch to the branch of interest $\\to$ Code $\\to$ Codespaces\n$\\to$ Create codespaces on the branch.\n\n</details>\n\n<details>\n<summary><strong><em>devcontainer.json & Dockerfile</em></strong></summary>\n\n[devcontainer.json](./.devcontainer/devcontainer.json) describes:\n* the Dockerfile to use with configurable build arguments, such as\n  `COMPUTE` and `PYTHON_VERSION`.\n* settings on VS Code server, such as Python interpreter path in the\n  container, Python formatter.\n* extensions on VS Code server, such as black-formatter, pylint.\n* how to create the Conda environment for Recommenders in \n  `postCreateCommand`\n\n[Dockerfile](./tools/docker/Dockerfile) is used in 3 places:\n* Dev containers on VS Code and GitHub Codespaces\n* [Testing workflows on AzureML](./tests/README.md)\n* [Jupyter notebook examples on Docker](./tools/docker/README.md)\n\n</details>\n\n\n## Test Environments\n\nDepending on the type of recommender system and the notebook that needs to be run, there are different computational requirements.\n\nCurrently, tests are done on **Python CPU** (the base environment), **Python GPU** (corresponding to `[gpu]` extra above) and **PySpark** (corresponding to `[spark]` extra above).\n\nAnother way is to build a docker image and use the functions inside a [docker container](#setup-guide-for-docker).\n\n## Setup for Making a Release\n\nThe process of making a new release and publishing it to [PyPI](https://pypi.org/project/recommenders/) is as follows:\n\nFirst make sure that the tag that you want to add, e.g. `0.6.0`, is added in [`recommenders.py/__init__.py`](recommenders.py/__init__.py). Follow the [contribution guideline](CONTRIBUTING.md) to add the change.\n\n1. Make sure that the code in main passes all the tests (unit and nightly tests).\n1. Create a tag with the version number: e.g. `git tag -a 0.6.0 -m \"Recommenders 0.6.0\"`.\n1. Push the tag to the remote server: `git push origin 0.6.0`.\n1. When the new tag is pushed, a release pipeline is executed. This pipeline runs all the tests again (PR gate and nightly builds), generates a wheel and a tar.gz which are uploaded to a [GitHub draft release](https://github.com/microsoft/recommenders/releases).\n1. Fill up the draft release with all the recent changes in the code.\n1. Download the wheel and tar.gz locally, these files shouldn't have any bug, since they passed all the tests.\n1. Install twine: `pip install twine`\n1. Publish the wheel and tar.gz to PyPI: `twine upload recommenders*`\n\n"
        },
        {
          "name": "contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.4404296875,
          "content": "[build-system]\nrequires = [\n    \"setuptools>=52\",\n    \"wheel>=0.36\",\n    \"numpy>=1.15,<2\",\n]\ndependencies = [\n    \"setuptools>=52\",\n    \"wheel>=0.36\",\n    \"numpy>=1.15,<2\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.pytest.ini_options]\nmarkers = [\n    \"experimental: tests that will not be executed and may need extra dependencies\",\n    \"gpu: tests running on GPU\",\n    \"notebooks: tests for notebooks\",\n    \"spark: tests that requires Spark\",\n]"
        },
        {
          "name": "recommenders",
          "type": "tree",
          "content": null
        },
        {
          "name": "scenarios",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.859375,
          "content": "# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n\nfrom os import environ\nfrom pathlib import Path\nfrom setuptools import setup, find_packages\nimport site\nimport sys\nimport time\n\n# Workaround for enabling editable user pip installs\nsite.ENABLE_USER_SITE = \"--user\" in sys.argv[1:]\n\n# Version\nhere = Path(__file__).absolute().parent\nversion_data = {}\nwith open(here.joinpath(\"recommenders\", \"__init__.py\"), \"r\") as f:\n    exec(f.read(), version_data)\nversion = version_data.get(\"__version__\", \"0.0\")\n\n# Get the long description from the README file\nwith open(here.joinpath(\"recommenders\", \"README.md\"), encoding=\"utf-8\") as f:\n    LONG_DESCRIPTION = f.read()\n\nHASH = environ.get(\"HASH\", None)\nif HASH is not None:\n    version += \".post\" + str(int(time.time()))\n\ninstall_requires = [\n    \"category-encoders>=2.6.0,<3\",  # requires packaging\n    \"cornac>=1.15.2,<3\",  # requires packaging, tqdm\n    \"hyperopt>=0.2.7,<1\",\n    \"lightgbm>=4.0.0,<5\",\n    \"locust>=2.12.2,<3\",  # requires jinja2\n    \"memory-profiler>=0.61.0,<1\",\n    \"nltk>=3.8.1,<4\",  # requires tqdm\n    \"notebook>=6.5.5,<8\",  # requires ipykernel, jinja2, jupyter, nbconvert, nbformat, packaging, requests\n    \"numba>=0.57.0,<1\",\n    \"pandas>2.0.0,<3.0.0\",  # requires numpy\n    \"pandera[strategies]>=0.6.5,<0.18;python_version<='3.8'\",  # For generating fake datasets\n    \"pandera[strategies]>=0.15.0;python_version>='3.9'\",\n    \"retrying>=1.3.4,<2\",\n    \"scikit-learn>=1.2.0,<2\",  # requires scipy, and introduce breaking change affects feature_extraction.text.TfidfVectorizer.min_df\n    \"scikit-surprise>=1.1.3\",\n    \"seaborn>=0.13.0,<1\",  # requires matplotlib, packaging\n    \"statsmodels<=0.14.1;python_version<='3.8'\",\n    \"statsmodels>=0.14.4;python_version>='3.9'\",\n    \"transformers>=4.27.0,<5\",  # requires packaging, pyyaml, requests, tqdm\n]\n\n# shared dependencies\nextras_require = {\n    \"gpu\": [\n        \"fastai>=2.7.11,<3\",\n        \"numpy<1.25.0;python_version<='3.8'\",\n        \"nvidia-ml-py>=11.525.84\",\n        \"spacy<=3.7.5;python_version<='3.8'\",\n        \"tensorflow>=2.8.4,!=2.9.0.*,!=2.9.1,!=2.9.2,!=2.10.0.*,<2.16\",  # Fixed TF due to constant security problems and breaking changes #2073\n        \"tf-slim>=1.1.0\",  # No python_requires in its setup.py\n        \"torch>=2.0.1,<3\",\n    ],\n    \"spark\": [\n        \"pyarrow>=10.0.1\",\n        \"pyspark>=3.3.0,<=4\",\n    ],\n    \"dev\": [\n        \"black>=23.3.0\",\n        \"pytest>=7.2.1\",\n        \"pytest-cov>=4.1.0\",\n        \"pytest-mock>=3.10.0\",  # for access to mock fixtures in pytest\n    ],\n}\n# For the brave of heart\nextras_require[\"all\"] = list(set(sum([*extras_require.values()], [])))\n\n# The following dependencies need additional testing\nextras_require[\"experimental\"] = [\n    # xlearn requires cmake to be pre-installed\n    \"xlearn==0.40a1\",\n    # VW C++ binary needs to be installed manually for some code to work\n    \"vowpalwabbit>=8.9.0,<9\",\n    # nni needs to be upgraded\n    \"nni==1.5\",\n    \"pymanopt>=0.2.5\",\n    \"lightfm>=1.17,<2\",\n]\n\n# The following dependency can be installed as below, however PyPI does not allow direct URLs.\n# Temporary fix for pymanopt, only this commit works with TF2\n# \"pymanopt@https://github.com/pymanopt/pymanopt/archive/fb36a272cdeecb21992cfd9271eb82baafeb316d.zip\",\n\nsetup(\n    name=\"recommenders\",\n    version=version,\n    description=\"Recommenders - Python utilities for building recommendation systems\",\n    long_description=LONG_DESCRIPTION,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/recommenders-team/recommenders\",\n    project_urls={\n        \"Documentation\": \"https://recommenders-team.github.io/recommenders/intro.html\",\n        \"Wiki\": \"https://github.com/recommenders-team/recommenders/wiki\",\n    },\n    author=\"Recommenders contributors\",\n    author_email=\"recommenders-technical-discuss@lists.lfaidata.foundation\",\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Information Technology\",\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Operating System :: POSIX :: Linux\",\n    ],\n    extras_require=extras_require,\n    keywords=\"recommendations recommendation recommenders recommender system engine \"\n    \"machine learning python spark gpu\",\n    install_requires=install_requires,\n    package_dir={\"recommenders\": \"recommenders\"},\n    python_requires=\">=3.6\",\n    packages=find_packages(\n        where=\".\",\n        exclude=[\"contrib\", \"docs\", \"examples\", \"scenarios\", \"tests\", \"tools\"],\n    ),\n    setup_requires=[\"numpy>=1.19\"],\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}