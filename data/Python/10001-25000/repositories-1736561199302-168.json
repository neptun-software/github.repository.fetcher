{
  "metadata": {
    "timestamp": 1736561199302,
    "page": 168,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "QwenLM/Qwen",
      "stars": 15269,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.1162109375,
          "content": "__pycache__\n*.so\nbuild\n.coverage_*\n*.egg-info\n*~\n.vscode/\n.idea/\n.git/\n.github/\n.DS_Store\n\n/private/\n/README-docker.md\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.083984375,
          "content": "__pycache__\n*.so\nbuild\n.coverage_*\n*.egg-info\n*~\n.vscode/\n.idea/\n.DS_Store\n\n/private/\n"
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 3.541015625,
          "content": "# FAQ\n\n## Installation & Environment\n\n#### Failure in installing flash attention\n\nFlash attention is an option for accelerating training and inference. Only NVIDIA GPUs of Turing, Ampere, Ada, and Hopper architecture, e.g., H100, A100, RTX 3090, T4, RTX 2080, can support flash attention. **You can use our models without installing it.**\n\n#### Which version of transformers should I use?\n\n4.32.0 is preferred.\n\n#### I downloaded the codes and checkpoints but I can't load the model locally. What should I do?\n\nPlease check if you have updated the code to the latest, and correctly downloaded all the sharded checkpoint files.\n\n#### `qwen.tiktoken` is not found. What is it?\n\nThis is the merge file of the tokenizer. You have to download it. Note that if you just git clone the repo without [git-lfs](https://git-lfs.com), you cannot download this file.\n\n#### transformers_stream_generator/tiktoken/accelerate not found\n\nRun the command `pip install -r requirements.txt`. You can find the file at [https://github.com/QwenLM/Qwen-7B/blob/main/requirements.txt](https://github.com/QwenLM/Qwen/blob/main/requirements.txt).\n<br><br>\n\n\n\n## Demo & Inference\n\n#### Is there any demo? CLI demo and Web UI demo?\n\nYes, see `web_demo.py` for web demo and `cli_demo.py` for CLI demo. See README for more information.\n\n\n#### Can I use CPU only?\n\nYes, run `python  cli_demo.py --cpu-only` will load the model and inference on CPU only.\n\n#### Can Qwen support streaming?\n\nYes. See the function `chat_stream` in `modeling_qwen.py`.\n\n#### Gibberish in result when using chat_stream().\n\nThis is because tokens represent bytes and a single token may be a meaningless string. We have updated the default setting of our tokenizer to avoid such decoding results. Please update the code to the latest version.\n\n#### It seems that the generation is not related to the instruction...\n\nPlease check if you are loading Qwen-Chat instead of Qwen. Qwen is the base model without alignment, which behaves differently from the SFT/Chat model.\n\n#### Is quantization supported?\n\nYes, the quantization is supported by AutoGPTQ. \n\n\n#### Slow when processing long sequences\n\nUpdating the code to the latest version can help.\n\n#### Unsatisfactory performance in processing long sequences\n\nPlease ensure that NTK is applied. `use_dynamc_ntk` and `use_logn_attn` in `config.json` should be set to `true` (`true` by default).\n<br><br>\n\n\n\n## Finetuning\n\n#### Can Qwen support SFT or even RLHF?\n\nYes, we now support SFT, including full-parameter finetuning, LoRA, and Q-LoRA. Also you can check other projects like [FastChat](**[https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat)), [Firefly]([https://github.com/yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly)), [**LLaMA Efficient Tuning**]([https://github.com/hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)), etc.\n\nHowever, temporarily we do not support RLHF. We will provide the code in the near future.\n<br><br>\n\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id not found\n\nIn our training, we only use `<|endoftext|>` as the separator and padding token. You can set bos_id, eos_id, and pad_id to tokenizer.eod_id. Learn more about our tokenizer from our documents about the tokenizer.\n\n\n\n## Docker\n\n#### Download official docker image is very slow\n\nWhen downloading our official docker image, you may have a slow download speed due to some network issues. You can refer to [Alibaba Cloud Container Image Service](https://help.aliyun.com/zh/acr/user-guide/accelerate-the-pulls-of-docker-official-images) to accelerate the download of official images.\n"
        },
        {
          "name": "FAQ_ja.md",
          "type": "blob",
          "size": 4.322265625,
          "content": "# FAQ\n\n## インストールと環境\n\n#### Flash attention 導入の失敗例\n\nFlash attention は、トレーニングと推論を加速するオプションです。H100、A100、RTX 3090、T4、RTX 2080 などの Turing、Ampere、Ada、および Hopper アーキテクチャの NVIDIA GPU だけが、flash attention をサポートできます。それをインストールせずに私たちのモデルを使用することができます。\n\n#### transformers のバージョンは？\n\n4.32.0 が望ましいです。\n\n#### コードとチェックポイントをダウンロードしましたが、モデルをローカルにロードできません。どうすればよいでしょうか？\n\nコードを最新のものに更新し、すべてのシャードされたチェックポイントファイルを正しくダウンロードしたかどうか確認してください。\n\n#### `qwen.tiktoken` が見つかりません。これは何ですか？\n\nこれはトークナイザーのマージファイルです。ダウンロードする必要があります。[git-lfs](https://git-lfs.com) を使わずにリポジトリを git clone しただけでは、このファイルをダウンロードできないことに注意してください。\n\n#### transformers_stream_generator/tiktoken/accelerate が見つかりません。\n\nコマンド `pip install -r requirements.txt` を実行してください。このファイルは [https://github.com/QwenLM/Qwen/blob/main/requirements.txt](https://github.com/QwenLM/Qwen/blob/main/requirements.txt) にあります。\n<br><br>\n\n\n\n## デモと推論\n\n#### デモはありますか？CLI と Web UI のデモはありますか？\n\nはい、Web デモは `web_demo.py` を、CLI デモは `cli_demo.py` を参照してください。詳しくは README を参照してください。\n\n\n\n#### CPU のみを使うことはできますか？\n\nはい、`python cli_demo.py --cpu-only` を実行すると、CPU のみでモデルと推論をロードします。\n\n#### Qwen はストリーミングに対応していますか？\n\n`modeling_qwen.py` の `chat_stream` 関数を参照してください。\n\n#### chat_stream() を使用すると、結果に文字化けが発生します。\n\nこれは、トークンがバイトを表し、単一のトークンが無意味な文字列である可能性があるためです。このようなデコード結果を避けるため、トークナイザのデフォルト設定を更新しました。コードを最新版に更新してください。\n\n#### インストラクションとは関係ないようですが...\n\nQwen ではなく Qwen-Chat を読み込んでいないか確認してください。Qwen はアライメントなしのベースモデルで、SFT/Chat モデルとは挙動が異なります。\n\n#### 量子化はサポートされていますか？\n\nはい、量子化は AutoGPTQ でサポートされています。\n\n\n#### 長いシーケンスの処理に時間がかかる\n\nコードを最新版に更新することで解決します。\n\n#### 長いシーケンスの処理で不満足なパフォーマンス\n\nNTK が適用されていることを確認してください。`config.json` の `use_dynamc_ntk` と `use_logn_attn` を `true` に設定する必要があります（デフォルトでは `true`）。\n<br><br>\n\n\n\n## ファインチューニング\n\n#### Qwen は SFT、あるいは RLHF に対応できますか？\n\nSFTのコードは提供します。[FastChat](**[https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat))、[Firefly]([https://github.com/yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly))、[**LLaMA Efficient Tuning**]([https://github.com/hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning))など、いくつかのプロジェクトではファインチューニングをサポートしています。近日中に関連コードを更新する予定です。\n<br><br>\n\n\n\n## トークナイザー\n\n#### bos_id/eos_id/pad_id が見つかりません。\n\n私たちのトレーニングでは、セパレータとパディングトークンとして `<|endoftext|>` のみを使用しています。bos_id、eos_id、pad_id は tokenizer.eod_id に設定できます。私たちのトークナイザーについて詳しくは、トークナイザーについてのドキュメントをご覧ください。\n\n"
        },
        {
          "name": "FAQ_zh.md",
          "type": "blob",
          "size": 4.072265625,
          "content": "# FAQ\n\n## 安装&环境\n\n#### flash attention 安装失败\n\nflash attention是一个用于加速模型训练推理的可选项，且仅适用于Turing、Ampere、Ada、Hopper架构的Nvidia GPU显卡（如H100、A100、RTX 3090、T4、RTX 2080），您可以在不安装flash attention的情况下正常使用模型进行推理。\n\n#### 我应该用哪个transformers版本？\n\n建议使用4.32.0。\n\n#### 我把模型和代码下到本地，按照教程无法使用，该怎么办？\n\n答：别着急，先检查你的代码是不是更新到最新版本，然后确认你是否完整地将模型checkpoint下到本地。\n\n#### `qwen.tiktoken`这个文件找不到，怎么办？\n\n这个是我们的tokenizer的merge文件，你必须下载它才能使用我们的tokenizer。注意，如果你使用git clone却没有使用git-lfs，这个文件不会被下载。如果你不了解git-lfs，可点击[官网](https://git-lfs.com/)了解。\n\n#### transformers_stream_generator/tiktoken/accelerate，这几个库提示找不到，怎么办？\n\n运行如下命令：`pip install -r requirements.txt`。相关依赖库在[https://github.com/QwenLM/Qwen-7B/blob/main/requirements.txt](https://github.com/QwenLM/Qwen/blob/main/requirements.txt) 可以找到。\n<br><br>\n\n\n## Demo & 推理\n\n#### 是否提供Demo？CLI Demo及Web UI Demo？\n\n`web_demo.py`和`cli_demo.py`分别提供了Web UI以及CLI的Demo。请查看README相关内容了解更多。\n\n#### 我没有GPU，只用CPU运行CLI demo可以吗？\n\n可以的，运行`python  cli_demo.py --cpu-only`命令即可将模型读取到CPU并使用CPU进行推理。\n\n#### Qwen支持流式推理吗？\n\nQwen当前支持流式推理。见位于`modeling_qwen.py`的`chat_stream`函数。\n\n#### 使用`chat_stream()`生成混乱的内容及乱码，为什么？\n\n这是由于模型生成过程中输出的部分token需要与后续token一起解码才能输出正常文本，单个token解码结果是无意义字符串，我们已经更新了tokenizer解码时的默认设置，避免这些字符串在生成结果中出现，如果仍有类似问题请更新模型至最新版本。\n\n#### 模型的输出看起来与输入无关/没有遵循指令/看起来呆呆的\n\n请检查是否加载的是Qwen-Chat模型进行推理，Qwen模型是未经align的预训练基模型，不期望具备响应用户指令的能力。我们在模型最新版本已经对`chat`及`chat_stream`接口内进行了检查，避免您误将预训练模型作为SFT/Chat模型使用。\n\n#### 是否有量化版本模型\n\n目前Qwen支持基于AutoGPTQ的4-bit的量化推理。\n\n#### 生成序列较长后速度显著变慢\n\n请更新到最新代码。\n\n#### 处理长序列时效果有问题\n\n请确认是否开启ntk。若要启用这些技巧，请将`config.json`里的`use_dynamc_ntk`和`use_logn_attn`设置为`true`。最新代码默认为`true`。\n<br><br>\n\n\n## 微调\n\n#### 当前是否支持SFT和RLHF？\n\n我们目前提供了SFT的代码，支持全参数微调、LoRA和Q-LoRA。此外，当前有多个外部项目也已实现支持，如[FastChat](**[https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat))、[Firefly]([https://github.com/yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly))、[**LLaMA Efficient Tuning**]([https://github.com/hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning))等。我们会尽快更新这部分代码和说明。\n\n我们还没提供对RLHF训练的支持，敬请期待。\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id，这些token id不存在，为什么？\n\n在训练过程中，我们仅使用<|endoftext|>这一token作为sample/document之间的分隔符及padding位置占位符，你可以将bos_id, eos_id, pad_id均指向tokenizer.eod_id。请阅读我们关于tokenizer的文档，了解如何设置这些id。\n\n\n## Docker\n\n#### 下载官方Docker镜像速度很慢\n\n在下载官方镜像时，您可能由于某些网络原因导致下载速度变慢。可以参考[阿里云容器镜像服务](https://help.aliyun.com/zh/acr/user-guide/accelerate-the-pulls-of-docker-official-images)加速官方镜像的下载。"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.076171875,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Alibaba Cloud\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 14.9521484375,
          "content": "------------- LICENSE FOR NVIDIA Megatron-LM code  --------------\n\nCopyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n  * Neither the name of NVIDIA CORPORATION nor the names of its\n    contributors may be used to endorse or promote products derived\n    from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n------------- LICENSE FOR OpenAI tiktoken code  --------------\n\nMIT License\n\nCopyright (c) 2022 OpenAI, Shantanu Jain\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n------------- LICENSE FOR stanford_alpaca code  --------------\n\n                                Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n   \n------------- LICENSE FOR PanQiWei AutoGPTQ code  --------------\n\nMIT License\n\nCopyright (c) 2023 潘其威(William)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "QWEN_TECHNICAL_REPORT.pdf",
          "type": "blob",
          "size": 1569.6767578125,
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 74.2578125,
          "content": "<p align=\"left\">\n    <a href=\"README_CN.md\">中文</a>&nbsp ｜ &nbspEnglish&nbsp ｜ &nbsp<a href=\"README_JA.md\">日本語</a> ｜ &nbsp<a href=\"README_FR.md\">Français</a> ｜ &nbsp<a href=\"README_ES.md\">Español</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        🤗 <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ｜ &nbsp&nbsp🖥️ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp ｜  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br><br>\n\n> [!Important]\n> Qwen2 is here! You are welcome to follow [QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) and share your experience there.\n>\n> This repo ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) is no longer actively maintained, due to substantial codebase differences.\n\n<br>\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">🤗</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">🤗</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">🤗</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">🤗</a> |\n\n\n\nWe opensource our **Qwen** series, now including **Qwen**, the base language models, namely **Qwen-1.8B**, **Qwen-7B**, **Qwen-14B**, and **Qwen-72B**, as well as **Qwen-Chat**, the chat models, namely **Qwen-1.8B-Chat**, **Qwen-7B-Chat**, **Qwen-14B-Chat**, and **Qwen-72B-Chat**. Links are on the above table. Click them and check the model cards. Also, we release the **[technical report](https://arxiv.org/abs/2309.16609)**. Please click the paper link and check it out!\n\nIn brief, we have strong base language models, which have been stably pretrained for up to 3 trillion tokens of multilingual data with a wide coverage of domains, languages (with a focus on Chinese and English), etc. They are able to achieve competitive performance on benchmark datasets. Additionally, we have chat models that are aligned with human preference based on SFT and RLHF (not released yet), which are able to chat, create content, extract information, summarize, translate, code, solve math problems, and so on, and are able to use tools, play as agents, or even play as code interpreters, etc.\n\n| Model     | Release Date | Max Length | System Prompt Enhancement | # of Pretrained Tokens | Minimum GPU Memory Usage of Finetuning (Q-Lora) | Minimum GPU Usage of Generating 2048 Tokens (Int4) | Tool Usage |\n|:----------|:------------:|:----------:|:-------------------------:|:----------------------:|:-----------------------------------------------:|:--------------------------------------------------:|:----------:|\n| Qwen-1.8B |   23.11.30   |    32K     |             ✅             |          2.2T          |                      5.8GB                      |                       2.9GB                        |     ✅      |  \n| Qwen-7B   |   23.08.03   |    32K     |             ❎             |          2.4T          |                     11.5GB                      |                       8.2GB                        |     ✅      |   \n| Qwen-14B  |   23.09.25   |     8K     |             ❎             |          3.0T          |                     18.7GB                      |                       13.0GB                       |     ✅      |\n| Qwen-72B  |   23.11.30   |    32K     |             ✅             |          3.0T          |                     61.4GB                      |                       48.9GB                       |     ✅      |   \n\nIn this repo, you can figure out:\n\n* Quickstart with Qwen, and enjoy the simple inference.\n* Details about the quantization models, including GPTQ and KV cache quantization.\n* Statistics of inference performance, including speed and memory.\n* Tutorials on finetuning, including full-parameter tuning, LoRA, and Q-LoRA.\n* Instructions on deployment, with the example of vLLM and FastChat.\n* Instructions on building demos, including WebUI, CLI demo, etc.\n* Introduction to DashScope API service, as well as the instructions on building an OpenAI-style API for your model.\n* Information about Qwen for tool use, agent, and code interpreter\n* Statistics of long-context understanding evaluation\n* License agreement\n* ...\n\nAlso, if you meet problems, turn to [FAQ](FAQ.md) for help first. Still feeling struggled? Feel free to shoot us issues (better in English so that more people can understand you)! If you would like to help us, send us pull requests with no hesitation! We are always excited about PR! \n\nWould like to chat with us or date us coffee time? Welcome to our Discord or WeChat! \n<br><br>\n\n## News and Updates\n* 2023.11.30 🔥 We release **Qwen-72B** and **Qwen-72B-Chat**, which are trained on 3T tokens and support 32k context, along with **Qwen-1.8B**, and **Qwen-1.8B-Chat**, on ModelScope and Hugging Face. We have also strengthened the System Prompt capabilities of the Qwen-72B-Chat and Qwen-1.8B-Chat, see [example documentation](examples/system_prompt.md). Additionally, support the inference on **Ascend 910** and **Hygon DCU**. Check `ascend-support` and `dcu-support` for more details.\n* 2023.10.17 We release the Int8 quantized model **Qwen-7B-Chat-Int8** and **Qwen-14B-Chat-Int8**. \n* 2023.9.25 🔥 We release **Qwen-14B** and **Qwen-14B-Chat** on ModelScope and Hugging Face, along with [qwen.cpp](https://github.com/QwenLM/qwen.cpp) and [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). Codes and checkpoints of **Qwen-7B** and **Qwen-7B-Chat** are also updated. **PLEASE PULL THE LATEST VERSION!**\n    - Compared to **Qwen-7B** (original), **Qwen-7B** uses more training tokens, increasing from 2.2T tokens to 2.4T tokens, while the context length extends from 2048 to 8192. The Chinese knowledge and coding ability of **Qwen-7B** have been further improved.\n* 2023.9.12 We now support finetuning on the Qwen-7B models, including full-parameter finetuning, LoRA and Q-LoRA.\n* 2023.8.21 We release the Int4 quantized model for Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, which requires low memory costs but achieves improved inference speed. Besides, there is no significant performance degradation on the benchmark evaluation.\n* 2023.8.3 We release both **Qwen-7B** and **Qwen-7B-Chat** on ModelScope and Hugging Face. We also provide a technical memo for more details about the model, including training details and model performance.\n<br>\n\n## Performance\nQwen models outperform the baseline models of similar model sizes on a series of benchmark datasets, e.g., MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., which evaluate the models’ capabilities on natural language understanding, mathematic problem solving, coding, etc. Qwen-72B achieves better performance than LLaMA2-70B on all tasks and outperforms GPT-3.5 on 7 out of 10 tasks. \n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=600px/>\n<p>\n      \n\n<br>\n\n| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |\n| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\nFor all compared models, we report the best scores between their official reported results and [OpenCompass](https://opencompass.org.cn/leaderboard-llm). \n\nFor more experimental results (detailed model performance on more benchmark datasets) and details, please refer to our technical report by clicking [here](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).\n<br><br>\n\n## Requirements\n\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* transformers 4.32 and above\n* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n<br>\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen-Chat with 🤖 ModelScope and 🤗 Transformers.\n\nYou can use our pre-built docker images to skip most of the environment setup steps, see Section [\"Using Pre-built Docker Images\"](#-docker) for more details. \n\nIf not using docker, please make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.\n\n```bash\npip install -r requirements.txt\n```\n\nIf your device supports fp16 or bf16, we recommend installing [flash-attention](https://github.com/Dao-AILab/flash-attention) (**we support flash attention 2 now.**) for higher efficiency and lower memory usage. (**flash-attention is optional and the project can run normally without installing it**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# Below are optional. Installing them might be slow.\n# pip install csrc/layer_norm\n# If the version of flash-attn is higher than 2.1.1, the following is not needed.\n# pip install csrc/rotary\n```\n\nNow you can start with ModelScope or Transformers.\n\n### 🤗 Transformers\n\nTo use Qwen-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. Remember to pass in the correct model names or paths, such as \"Qwen/Qwen-7B-Chat\" and \"Qwen/Qwen-14B-Chat\". However, **please make sure that you are using the latest code.**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\n# 你好！很高兴为你提供帮助。\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\nprint(response)\n# 这是一个关于一个年轻人奋斗创业最终取得成功的故事。\n# 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。\n# 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。\n# 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。\n# 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。\n# 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。\n\n# 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"给这个故事起一个标题\", history=history)\nprint(response)\n# 《奋斗创业：一个年轻人的成功之路》\n```\n\nRunning Qwen, the base language model, is also simple.\n\n<details>\n  <summary>Running Qwen</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\" \ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# 蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）...\n```\n\n</details>\n\n<p id=\"DownloadModel\">\nIn the event of a network issue while attempting to download model checkpoints and codes from HuggingFace, an alternative approach is to initially fetch the checkpoint from ModelScope and then load it from the local directory as outlined below:\n</p>\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n# model_dir = snapshot_download('qwen/Qwen-14B')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### 🤖 ModelScope\n\nModelScope is an open-source platform for Model-as-a-Service (MaaS), which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# Model names: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"浙江的省会在哪里？\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"它有什么好玩的景点\", history=history)\nprint(response)\n```\n\n### Batch Inference\nQwen supports batch inference. With flash attention enabled, using batch inference can bring a 40% speedup. The example code is shown below:\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\n# To generate attention masks automatically, it is necessary to assign distinct\n# token_ids to pad_token and eos_token, and set pad_token_id in the generation_config.\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"我想听你说爱我。\", \"今天我想吃点啥，甜甜的，推荐下\", \"我马上迟到了，怎么做才能不迟到\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"我想听你说爱我。\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"今天我想吃点啥，甜甜的，推荐下\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"我马上迟到了，怎么做才能不迟到\", history=None)\nprint(response)\n```\n\n### CPU\n\nTo deploy our models on CPU, we strongly advise you to use [qwen.cpp](https://github.com/QwenLM/qwen.cpp), which is a pure C++ implementation of Qwen and tiktoken. Check the repo for more details!\n\nAlso, it is also simple to directly run the model on CPU, which requires your specification of device:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nHowever, it is likely that you suffer from extremely low inference efficiency.\n\n### Multiple GPUs\n\nIf you suffer from lack of GPU memory and you would like to run the model on more than 1 GPU, you can directly use the default loading method, which is now supported by Transformers. The previous method based on `utils.py` is deprecated.\n\nHowever, though this method is simple, the efficiency of the native pipeline parallelism is low. We advise you to use vLLM with FastChat and please read the section for deployment.\n\n### x86 Platforms\nWhen deploy on Core™/Xeon® Scalable Processors or with Arc™ GPU, [OpenVINO™ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html) is recommended. You can install and run this [example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot). For related issues, you are welcome to file an issue at [OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues). \n\n### DashScope\nThe most simple way to use Qwen through APIs is DashScope API service through Alibaba Cloud. We give an introduction to the usage. Additionally, we provide a script for you to deploy an OpenAI-style API on your own servers.\n\nDashScope is the large language model API service provided by Alibaba Cloud, which now supports Qwen. Note that the models behind DashScope are in-house versions temporarily without details provided. The services include `qwen-turbo` and `qwen-plus`, where the former one runs faster and the latter achieves better performance. For more information, visit the documentation [here](https://dashscope.aliyun.com).\n\nPlease head to the official website [link](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) to create a DashScope account and obtain the API key (AK). We recommend setting the AK with an environment variable:\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nThen please install the packages and click [here](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) for the documentation. If you use Python, you can install DashScope with pip:\n```bash\npip install dashscope\n```\nIf you use JAVA SDK, you can install it in this way:\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nThe simplest way to use DashScope is the usage with messages, which is similar to OpenAI API. The example is demonstrated below:\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': '如何做西红柿鸡蛋？'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\nFor more usages, please visit the official website for more details.\n<br><br>\n\n## Quantization\n\n### GPTQ\n\nWe provide a solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release the Int4 and Int8 quantized models, which achieve nearly lossless model effects but improved performance on both memory costs and inference speed.\n\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\n\n```bash\npip install auto-gptq optimum\n```\n\nIf you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a wheel.\n\n> Note: The pre-compiled `auto-gptq` packages strongly depend on the version of `torch` and its CUDA version. Moreover, due to recent update, \n> you may also encounter unsupported version errors from `transformers`, `optimum`, or `peft`.\n> We recommend using the latest versions meeting the following requirements:\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0\n\nThen you can load the quantized model easily and run inference as same as usual:\n\n```python\n# Model names: \"Qwen/Qwen-7B-Chat-Int4\", \"Qwen/Qwen-14B-Chat-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\nWe illustrate the model performance of both BF16, Int8 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n\n### Quantization of KV cache\n\n> NOTE: Please be aware that due to the internal mechanism of Hugging Face, the support files for this functionality \n> (i.e., `cache_autogptq_cuda_256.cpp` and `cache_autogptq_cuda_kernel_256.cu`) may be missing. Please manually download\n> them from the Hugging Face Hub and place them into the same folder as the other module files.\n\nThe attention KV cache can be quantized and compressed for storage, to get a higher sample throughput. The arguments `use_cache_quantization` and `use_cache_kernel` in `config.json` are provided to enable KV cache quantization. The specific use method is as follows:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\nAttention: Currently, KV cache quantization and flash attention cannot be used at the same time.\nIf you enable KV cache quantization and flash attention at the same time (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` is disabled by default (`use_flash_attn=false`).\n\nWe have verified that the use of the quantized Int8-KV-Cache model does not suffer from significant performance degradation in downstream evaluation. In the following, we focus on profiling its memory footprint in different conditions. \nThe profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4. \nWe use BF16 models to generate 1024 tokens by default, and \"OOM\" indicates out-of-memory error.\n\nWith KV cache quantization, the model can infer with a larger batch size (bs).\n\n| USE KV Cache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|--------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| No           | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |\n| Yes          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\nWith KV cache quantization the model can save more memory when generating longer sequence (`sl`, sequence length, referring to the number of tokens generated) at the stage of inference.\n\n| USE KV Cache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|--------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| No           | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| Yes          |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\nThe model with KV cache quantization will convert the format of `layer_past` from float to int8, and meanwhile the quantized `layer-past` will also store the quantization parameters.\n\nSpecific steps are as follows:\n\n1. Quantize key/value\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n2. Store into layer_past\n\nThe following is the format of quantized `layer_past`:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\n\nThe original format of `layer_past` is shown below:\n```\n    layer_past=(key,value)\n```\n\nIf you want to use the attention KV which is quantized, you can use the dequantization operation to convert the Int8 key/value back to the float format as follows:\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n\n## Inference Performance\n\nThis section provides the statistics of speed and memory of models in different precisions. The speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py). \n\nWe measured the average inference speed (tokens/s) and GPU memory usage of generating 2048 with the models in BF16, Int8, and Int4. \n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nThe profiling runs on a single A100-SXM4-80G GPU (except 2xA100 is mentioned) with PyTorch 2.0.1, CUDA 11.8, and Flash-Attention 2. (72B + vLLM uses PyTorch 2.1.0 and Cuda 11.8.) The inference speed is averaged over the encoded and generated tokens.\n\nNote: The generation speed of the Int4/Int8 models mentioned above is provided by the autogptq library. The current speed of the model loaded using ``AutoModelForCausalLM.from_pretrained`` will be approximately 20% slower. We have reported this issue to the HuggingFace team and will update it promptly if a solution is available.\n\nWe also measure the inference speed and GPU memory usage with different settings of context and generation lengths, Flash-Attention version. You can find the results in the according modelcards on Hugging Face or ModelScope.\n\n## Finetuning\n\n### Usage\nNow we provide the official training script, `finetune.py`, for users to finetune the pretrained model for downstream applications in a simple fashion. Additionally, we provide shell scripts to launch finetuning with no worries. This script supports the training with [DeepSpeed](https://github.com/microsoft/DeepSpeed) and [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). The shell scripts that we provide use DeepSpeed (Note: this may have conflicts with the latest version of pydantic and you should use make sure `pydantic<2.0`) and Peft. You can install them by:\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\nTo prepare your training data, you need to put all the samples into a list and save it to a json file. Each sample is a dictionary consisting of an id and a list for conversation. Below is a simple example list with 1 sample:\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是一个语言模型，我叫通义千问。\"\n      }\n    ]\n  }\n]\n```\n\nAfter data preparation, you can use the provided shell scripts to run finetuning. Remember to specify the path to the data file, `$DATA`.\n\nThe finetuning scripts allow you to perform:\n- Full-parameter finetuning\n- LoRA\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```bash\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\nbash finetune/finetune_ds.sh\n```\n\nRemember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument `--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use `--bf16 True` or `--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.\n\nSimilarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed `peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.\n\n```bash\n# Single GPU training\nbash finetune/finetune_lora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_lora_ds.sh\n```\n\nIn comparison with full-parameter finetuning, LoRA ([paper](https://arxiv.org/abs/2106.09685)) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs. \n\nNote that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information. \n\nIf you still suffer from insufficient memory, you can consider Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs. \n\nNote: to run single-GPU Q-LoRA training, you may need to install `mpi4py` through `pip` or `conda`.\n\nTo run Q-LoRA, directly run the following script:\n\n```bash\n# Single GPU training\nbash finetune/finetune_qlora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_qlora_ds.sh\n```\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You **SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision training due to our observation of errors caused by torch amp. Besides, for Q-LoRA, the troubles with the special tokens in LoRA still exist. However, as we only provide the Int4 models for chat models, which means the language model has learned the special tokens of ChatML format, you have no worry about the layers. Note that the layers of the Int4 model should not be trainable, and thus if you introduce special tokens in your training, Q-LoRA might not work.\n\n> NOTE: Please be aware that due to the internal mechanisms of Hugging Face, certain non-Python files (e.g., `*.cpp` and `*.cu`) \n> may be missing from the saved checkpoint. You may need to manually copy them to the directory containing other files.\n\nDifferent from full-parameter finetuning, the training of both LoRA and Q-LoRA only saves the adapter parameters. Suppose your training starts from Qwen-7B, you can load the finetuned model for inference as shown below:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n> NOTE: If `peft>=0.8.0`, it will try to load the tokenizer as well, however, initialized without `trust_remote_code=True`, leading to `ValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.` Currently, you could downgrade `peft<0.8.0` or move tokenizer files elsewhere to workaround this issue.\n\nIf you want to merge the adapters and save the finetuned model as a standalone model (you can only do this with LoRA, and you CANNOT merge the parameters from Q-LoRA), you can run the following codes:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\nThe `new_model_directory` directory will contain the merged model weights and module files. Please note that `*.cu` and `*.cpp` files may be missing in the saved files. If you wish to use the KV cache functionality, please manually copy them. Besides, the tokenizer files are not saved in the new directory in this step. You can copy the tokenizer files or use the following code\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    path_to_adapter, # path to the output directory\n    trust_remote_code=True\n)\n\ntokenizer.save_pretrained(new_model_directory)\n```\n\n\nNote: For multi-GPU training, you need to specify the proper hyperparameters for distributed training based on your machine. Besides, we advise you to specify your maximum sequence length with the argument `--model_max_length`, based on your consideration of data, memory footprint, and training speed.\n\n### Quantize Fine-tuned Models\n\nThis section applies to full-parameter/LoRA fine-tuned models. (Note: You do not need to quantize the Q-LoRA fine-tuned model because it is already quantized.)\nIf you use LoRA, please follow the above instructions to merge your model before quantization. \n\nWe recommend using [auto_gptq](https://github.com/PanQiWei/AutoGPTQ) to quantize the finetuned model. \n\n```bash\npip install auto-gptq optimum\n```\n\nNote: Currently AutoGPTQ has a bug referred in [this issue](https://github.com/PanQiWei/AutoGPTQ/issues/370). Here is a [workaround PR](https://github.com/PanQiWei/AutoGPTQ/pull/495), and you can pull this branch and install from the source.\n\nFirst, prepare the calibration data. You can reuse the fine-tuning data, or use other data following the same format.\n\nSecond, run the following script:\n\n```bash\npython run_gptq.py \\\n    --model_name_or_path $YOUR_LORA_MODEL_PATH \\\n    --data_path $DATA \\\n    --out_path $OUTPUT_PATH \\\n    --bits 4 # 4 for int4; 8 for int8\n```\n\nThis step requires GPUs and may costs a few hours according to your data size and model size.\n\nThen, copy all `*.py`, `*.cu`, `*.cpp` files and `generation_config.json` to the output path. And we recommend you to overwrite `config.json` by copying the file from the coresponding official quantized model\n(for example, if you are fine-tuning `Qwen-7B-Chat` and use `--bits 4`, you can find the `config.json` from [Qwen-7B-Chat-Int4](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4/blob/main/config.json)).\nYou should also rename the ``gptq.safetensors`` into ``model.safetensors``.\n\nFinally, test the model by the same method to load the official quantized model. For example,\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"/path/to/your/model\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/path/to/your/model\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\n```\n\n### Multinode Finetuning\n\nOur provided scripts support multinode finetuning. You can refer to the comments in [script](./finetune/finetune_lora_ds.sh) to correctly set corresponding arguments and launch the script on each node. For more information about multinode distributed training, please refer to [torchrun](https://pytorch.org/docs/stable/elastic/run.html).\n\nNote: DeepSpeed ZeRO 3 requires much greater inter-node communication rate than ZeRO 2, which will significantly reduce the training speed in the case of multinode finetuning. Therefore, we do not recommend using DeepSpeed ZeRO 3 configurations in multinode finetuning scripts.\n\n### Profiling of Memory and Speed\nWe profile the GPU memory and training speed of both LoRA (LoRA (emb) refers to training the embedding and output layer, while LoRA has no trainable embedding and output layer) and Q-LoRA in the setup of single-GPU training. In this test, we experiment on a single A100-SXM4-80G GPU, and we use CUDA 11.8 and Pytorch 2.0. Flash attention 2 is applied. We uniformly use a batch size of 1 and gradient accumulation of 8. We profile the memory (GB) and speed (s/iter) of inputs of different lengths, namely 256, 512, 1024, 2048, 4096, and 8192. We also report the statistics of full-parameter finetuning with Qwen-7B on 2 A100 GPUs. We only report the statistics of 256, 512, and 1024 tokens due to the limitation of GPU memory. \n\nFor Qwen-7B, we also test the performance of multinode finetuning. We experiment using two servers, each containing two A100-SXM4-80G GPUs, and the rest of configurations are the same as other Qwen-7B experiments. The results of multinode finetuning are marked as LoRA (multinode) in the table.\n\nFor Qwen-72B, we experiment in two ways: 1) Lora fintuning + DeepSpeed ZeRO 3 on 4 A100-SXM4-80G GPUs and 2) QLora (int4) fine-tuning on a single A100-SXM4-80G GPU. Note that OOM occurs on 4 A100-SXM4-80G GPUs both with LoRA (emb) fine-tuning and LoRA fine-tuning without Deepspeed ZeRO 3 (you can pass `--deepspeed finetune/ds_config_zero3.json` to [`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh) to enable DeepSpeed ZeRO 3).\n\nThe statistics are listed below:\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th rowspan=\"2\">#Nodes</th><th rowspan=\"2\">#GPUs per node</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"5\">7B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n<td>1</td><td>2</td>\n<td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>LoRA (multinode)</td>\n        <td>2</td><td>2</td>\n        <td align=\"center\">74.7G / 2.09s/it</td><td align=\"center\">77.6G / 3.16s/it</td><td align=\"center\">84.9G / 5.17s/it</td><td align=\"center\">95.1G / 9.25s/it</td><td align=\"center\">121.1G / 18.1s/it</td><td align=\"center\">155.5G / 37.4s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"2\">72B</th>\n        <td>LoRA + Deepspeed Zero3</td>\n        <td>1</td><td>4</td>\n        <td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n\n<br>\n\n## Deployment\n\n### vLLM \n\nFor deployment and fast inference, we suggest using vLLM. \n\nIf you use **CUDA 12.1 and PyTorch 2.1**, you can directly use the following command to install vLLM.\n\n```bash\npip install vllm\n```\n\nOtherwise, please refer to the official vLLM [Installation Instructions](https://docs.vllm.ai/en/latest/getting_started/installation.html).\n\n#### vLLM + Transformer-like Wrapper\n\nYou can download the [wrapper codes](examples/vllm_wrapper.py) and execute the following commands for multiple rounds of dialogue interaction. (Note: It currently only supports the ``model.chat()`` method.)\n\n```python\nfrom vllm_wrapper import vLLMWrapper\n\nmodel = vLLMWrapper('Qwen/Qwen-7B-Chat', tensor_parallel_size=1)\n# model = vLLMWrapper('Qwen/Qwen-7B-Chat-Int4', tensor_parallel_size=1, dtype=\"float16\")\n\nresponse, history = model.chat(query=\"你好\", history=None)\nprint(response)\nresponse, history = model.chat(query=\"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\nprint(response)\nresponse, history = model.chat(query=\"给这个故事起一个标题\", history=history)\nprint(response)\n```\n\n#### vLLM + Web Demo / OpenAI-like API\n\nYou can use FastChat to lauch a web demo or an OpenAI API server. First, install FastChat:\n\n```bash\npip install \"fschat[model_worker,webui]\"\n```\n\nTo run Qwen with vLLM and FastChat, you need launch a controller by:\n```bash\npython -m fastchat.serve.controller\n```\n\nThen you can launch the model worker, which means loading your model for inference. For single GPU inference, you can directly run:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype float16 # run int4 model\n```\nHowever, if you hope to run the model on multiple GPUs for faster inference or larger memory, you can use tensor parallelism supported by vLLM. Suppose you run the model on 4 GPUs, the command is shown below:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype float16 # run int4 model\n```\n\nAfter launching your model worker, you can launch a:\n\n* Web UI Demo\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* OpenAI API\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\nHowever, if you find it difficult to use vLLM and FastChat, you can try our provided simplest methods to deploy a web demo, CLI demo, and API.\n\n\n### Web UI\n\nWe provide code for users to build a web UI demo (thanks to @wysaid). Before you start, make sure you install the following packages:\n\n```\npip install -r requirements_web_demo.txt\n```\n\nThen run the command below and click on the generated link:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n### CLI Demo\n\nWe provide a CLI demo example in `cli_demo.py`, which supports streaming output for the generation. Users can interact with Qwen-7B-Chat by inputting prompts, and the model returns model outputs in the streaming mode. Run the command below:\n\n```bash\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\nWe provide methods to deploy local API based on OpenAI API (thanks to @hanpenggit). Before you start, install the required packages:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nThen run the command to deploy your API:\n\n```bash\npython openai_api.py\n```\n\nYou can change your arguments, e.g., `-c` for checkpoint name or path, `--cpu-only` for CPU deployment, etc. If you meet problems launching your API deployment, updating the packages to the latest version can probably solve them.\n\nUsing the API is also simple. See the example below:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# create a request activating streaming response\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=True \n    # Specifying stop words in streaming output format is not yet supported and is under development.\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# create a request not activating streaming response\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=False,\n    stop=[] # You can add custom stop words here, e.g., stop=[\"Observation:\"] for ReAct prompting.\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n**Function calling** is also supported (but only when `stream=False` for the moment). See the [example usage](examples/function_call_examples.py) here.\n<br><br>\n\n## 🐳 Docker\n\nTo simplify the deployment process, we provide docker images with pre-built environments: [qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen). You only need to install the driver and download model files to launch demos, deploy OpenAI API, and finetune the model.\n\n### Preparation\n\n1. Install the correct version of Nvidia driver depending on the image to use:\n  - `qwenllm/qwen:cu117` (**recommend**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: same as `qwenllm/qwen:cu117`\n\n2. Install and configure [docker](https://docs.docker.com/engine/install/) and [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html):\n\n```bash\n# configure docker\nsudo systemctl start docker\n# test if docker is correctly installed\nsudo docker run hello-world\n\n# configure nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. Download model checkpoints and codes to your environment (see [here](#DownloadModel)).\n\n### Deployment\n\nHere we use Qwen-7B-Chat as an example. Before launching a web demo or API, you can setup the configuration as shown below:\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes\n```\nThe following scripts can help you build:\n\n* OpenAI API\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Web UI\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* CLI Demo\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\nThe commands above will automatically download the required image and launch a Web UI demo in background (the service will auto-restart). You can open `http://localhost:${PORT}` on the host to use the demo.\n\nThe demo is successfully launched if you see the following output:\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nIf you want to check the status of the demo, you can use `docker logs qwen` to display outputs.\n\nYou can use `docker rm -f qwen` to stop the service and remove the container.\n\n\n### Finetuning\n\nThe method of finetuning using the pre-built Docker image is basically the same as [the above chapter](#Finetuning) (we have already installed dependencies in the image):\n\nThe following is an example of single-GPU LoRA:\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs\n\n# Use all host devices by default\nDEVICE=all\n# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# Single-GPU LoRA finetuning\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\nTo make a change to single-GPU Q-LoRA for example, you just need to modify the bash command inside `docker run`:\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## 🔥 System Prompt\nQwen-1.8-Chat and Qwen-72B-Chat have been fully trained on diverse system prompts with multiple rounds of complex interactions, so that they can follow a variety of system prompts and realize model customization in context, further improving the scalability of Qwen-chat.\n\nWith System Prompt, Qwen-Chat can realize **roly playing**, **language style transfer**, **task setting**, and **behavior setting**.\n\n![](assets/system_prompt_language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\nFor more information, please refer to the [example documentation](examples/system_prompt.md).\n\n## Tool Usage\n\nQwen-Chat has been optimized for tool usage and function calling capabilities. Users can develop agents, LangChain applications, and even augment Qwen with a Python Code Interpreter.\n\nWe provide documentation on how to implement tool calls based on the principle of ReAct Prompting, please refer to [the ReAct example](examples/react_prompt.md). Based on this principle, we provide support for function calling in [openai_api.py](openai_api.py).\n\nWe have tested the model's tool calling capabilities on our open-source Chinese evaluation benchmark and found that Qwen-Chat consistently performs well:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.↑)</th><th align=\"center\">Tool Input (Rouge-L↑)</th><th align=\"center\">False Positive Error↓</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\nTo assess Qwen's ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this [link](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nWe have observed that Qwen performs well in terms of code executability and result accuracy when generating code:\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Math↑</th><th align=\"center\">Visualization-Hard↑</th><th align=\"center\">Visualization-Easy↑</th><th align=\"center\">General↑</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## Long-Context Understanding\n\nTo extend the context length and break the bottleneck of training sequence length, we introduce several techniques, including NTK-aware interpolation, window attention, and LogN attention scaling, to extend the context length of Qwen-14B from 2K to over 8K tokens, and Qwen-1.8B/7B from 8K to 32K tokens. \n\nFor Qwen-72B, we adapt RoPE to longer contexts with a larger rotary base. Qwen-72B supports the max context length of 32K tokens.\n\nWe conduct language modeling experiments on the arXiv dataset with the PPL evaluation and find that Qwen can reach outstanding performance in the scenario of long context. Results are demonstrated below:\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.56</td><td align=\"center\">4.62</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.49</td><td align=\"center\">4.32</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n    <tr>\n        <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n    </tr>\n</table>\n\nFurthermore, to verify the ability of Qwen-72B-Chat on long text understanding, we tested it on [L-Eval](https://arxiv.org/abs/2307.11088) (closed-ended tasks). The results are as follows:\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\nWe conducted the \"needle in a haystack\" experiment (the idea came from [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)) to test whether the model can retrieve information at different positions in the inputs of different lengths, the result is as follows:\n\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\nThe above results show that Qwen-72B-Chat can accurately retrieve information placed in various positions within an input length of 32k, proving its excellent long text understanding capabilities.\n\n## Tokenizer\n\nOur tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](tokenization_note.md).\n<br><br>\n\n## Reproduction\n\nFor your reproduction of the model performance on benchmark datasets, we provide scripts for you to reproduce the results. Check [eval/EVALUATION.md](eval/EVALUATION.md) for more information. Note that the reproduction may lead to slight differences from our reported results.\n<br><br>\n\n## FAQ\n\nIf you meet problems, please refer to [FAQ](FAQ.md) and the issues first to search a solution before you launch a new issue.\n<br><br>\n\n## Citation\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## License Agreement\n\nThe source code provided at <https://github.com/QwenLM/Qwen> is licensed under the [Apache 2.0 License](./LICENSE) that can be found at the root directory.\n\nResearchers and developers are free to use the codes and model weights of both Qwen and Qwen-Chat. For their commercial use, please check the License Agreement accompanying each model.\n\n- Qwen-72B, Qwen-14B, and Qwen-7B are licensed under the [Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT) that can be found at the corresponding HuggingFace and ModelScope repository. For commercial use, please fill out the form ([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), and [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen)) to apply.\n\n- Qwen-1.8B is licensed under the [Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT) that can be found at the corresponding HuggingFace and ModelScope repository. For commercial use, please contact us.\n<br><br>\n\n## Contact Us\n\nIf you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to qianwen_opensource@alibabacloud.com.\n\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 71.09375,
          "content": "<p align=\"left\">\n    中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>&nbsp ｜ &nbsp<a href=\"README_JA.md\">日本語</a> ｜ &nbsp<a href=\"README_FR.md\">Français</a> ｜ &nbsp<a href=\"README_ES.md\">Español</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        🤗 <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ｜ &nbsp&nbsp🖥️ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ｜  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://qianwen.aliyun.com\">Web</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://apps.apple.com/cn/app/%E9%80%9A%E4%B9%89%E5%8D%83%E9%97%AE/id6466733523\">APP</a>\n</p>\n<br><br>\n\n> [!Important]\n> Qwen2已开，欢迎关注！看这里：[QwenLM/Qwen2](https://github.com/QwenLM/Qwen2)\n>\n> Qwen2模型代码和用法相比此前版本有较大不同，因此我们使用新的repo进行维护。此repo ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) 已停止主要更新维护。\n\n> [!Warning]\n> 请勿混用[Qwen](https://github.com/QwenLM/Qwen)和[Qwen2](https://github.com/QwenLM/Qwen2)代码，两者并不兼容。\n<br>\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">🤗</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">🤗</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">🤗</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">🤗</a> |\n\n\n  \n我们开源了**Qwen**（通义千问）系列工作，当前开源模型的参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。本次开源包括基础模型**Qwen**，即**Qwen-1.8B**、**Qwen-7B**、**Qwen-14B**、**Qwen-72B**，以及对话模型**Qwen-Chat**，即**Qwen-1.8B-Chat**、**Qwen-7B-Chat**、**Qwen-14B-Chat**和**Qwen-72B-Chat**。模型链接在表格中，请点击了解详情。同时，我们公开了我们的<b><a href=\"https://arxiv.org/abs/2309.16609\">技术报告</a></b>，请点击上方论文链接查看。\n\n当前基础模型已经稳定训练了大规模高质量且多样化的数据，覆盖多语言（当前以中文和英文为主），总量高达3万亿token。在相关基准评测中，Qwen系列模型拿出非常有竞争力的表现，显著超出同规模模型并紧追一系列最强的闭源模型。此外，我们利用SFT和RLHF技术实现对齐，从基座模型训练得到对话模型。Qwen-Chat具备聊天、文字创作、摘要、信息抽取、翻译等能力，同时还具备一定的代码生成和简单数学推理的能力。在此基础上，我们针对LLM对接外部系统等方面针对性地做了优化，当前具备较强的工具调用能力，以及最近备受关注的Code Interpreter的能力和扮演Agent的能力。我们将各个大小模型的特点列到了下表。\n\n| 模型        |   开源日期   | 最大上下文长度 | System Prompt强化 | 预训练token数 | 微调（Q-Lora）最小GPU用量 | 生成2048个token的最小显存占用（Int4） | 工具调用 |\n|:----------|:--------:|:-------:|:---------------:|:---------:|:-----------------:|:-------------------:|:----:|\n| Qwen-1.8B | 23.11.30 |   32K   |        ✅        |   2.2T    |       5.8GB       |        2.9GB        |  ✅   |  \n| Qwen-7B   | 23.08.03 |   32K   |        ❎        |   2.4T    |      11.5GB       |        8.2GB        |  ✅   |   \n| Qwen-14B  | 23.09.25 |   8K    |        ❎        |   3.0T    |      18.7GB       |       13.0GB        |  ✅   |\n| Qwen-72B  | 23.11.30 |   32K   |        ✅        |   3.0T    |      61.4GB       |       48.9GB        |  ✅   |   \n\n  \n在这个项目中，你可以了解到以下内容\n\n* 快速上手Qwen-Chat教程，玩转大模型推理\n* 量化模型相关细节，包括GPTQ和KV cache量化\n* 推理性能数据，包括推理速度和显存占用\n* 微调的教程，帮你实现全参数微调、LoRA以及Q-LoRA\n* 部署教程，以vLLM和FastChat为例\n* 搭建Demo的方法，包括WebUI和CLI Demo\n* 搭建API的方法，我们提供的示例为OpenAI风格的API\n* 更多关于Qwen在工具调用、Code Interpreter、Agent方面的内容\n* 长序列理解能力及评测\n* 使用协议\n* ...\n\n如果遇到问题，请优先考虑查询[FAQ](FAQ.md)。如仍未解决，随时提出issue（但建议使用英语或提供翻译，有助于帮助更多用户）。如果想帮助我们提升，欢迎提交Pull Requests！\n\n想和我们一起讨论和聊天的话，赶紧加入我们的微信群和Discord server（入口见文档开头部分）！\n<br><br>\n\n## 新闻\n\n* 2023.11.30 🔥 我们推出 **Qwen-72B** 和 **Qwen-72B-Chat**，它们在 3T tokens上进行训练，并支持 32k 上下文。同时也发布了 **Qwen-1.8B** 和 **Qwen-1.8B-Chat**。我们还增强了 Qwen-72B-Chat 和 Qwen-1.8B-Chat 的系统指令（System Prompt）功能，请参阅[示例文档](examples/system_prompt.md)。此外，我们还对**昇腾910**以及**海光DCU**实现了推理的支持，详情请查看`ascend-support`及`dcu-support`文件夹。\n* 2023年10月17日 我们推出了Int8量化模型**Qwen-7B-Chat-Int8**和**Qwen-14B-Chat-Int8**。\n* 2023年9月25日 在魔搭社区（ModelScope）和Hugging Face推出**Qwen-14B**和**Qwen-14B-Chat**模型，并开源 [qwen.cpp](https://github.com/QwenLM/qwen.cpp) 和 [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)。**Qwen-7B**和**Qwen-7B-Chat**的代码和模型也同步得到更新。**请使用最新的代码和模型！**\n    - 相比原版Qwen-7B，新版用了更多训练数据（从2.2T增加到2.4T tokens），序列长度从2048扩展至8192。整体中文能力以及代码能力均有所提升。\n* 2023年9月12日 支持Qwen-7B和Qwen-7B-Chat的微调，其中包括全参数微调、LoRA以及Q-LoRA。\n* 2023年8月21日 发布Qwen-7B-Chat的Int4量化模型，Qwen-7B-Chat-Int4。该模型显存占用低，推理速度相比半精度模型显著提升，在基准评测上效果损失较小。\n* 2023年8月3日 在魔搭社区（ModelScope）和Hugging Face同步推出Qwen-7B和Qwen-7B-Chat模型。同时，我们发布了技术备忘录，介绍了相关的训练细节和模型表现。\n<br>\n\n## 评测表现\n\nQwen系列模型相比同规模模型均实现了效果的显著提升。我们评测的数据集包括MMLU、C-Eval、 GSM8K、 MATH、HumanEval、MBPP、BBH等数据集，考察的能力包括自然语言理解、知识、数学计算和推理、代码生成、逻辑推理等。Qwen-72B在所有任务上均超越了LLaMA2-70B的性能，同时在10项任务中的7项任务中超越GPT-3.5.\n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=\"600\"/>\n<p>\n<br>\n\n| Model              |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:-------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                    |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B          |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B         |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B         |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B        |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B        |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B       |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B       |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B      |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |   -      |   26.3    |   -      |  -       |   -      |\n| **Qwen-1.8B**      |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**        |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**       |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**       | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\n\n对于以上所有对比模型，我们列出了其官方汇报结果与[OpenCompass](https://opencompass.org.cn/leaderboard-llm)结果之间的最佳分数。\n\n更多的实验结果和细节请查看我们的技术备忘录。点击[这里](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf)。\n<br><br>\n\n## 要求\n\n* python 3.8及以上版本\n* pytorch 1.12及以上版本，推荐2.0及以上版本\n* transformers 4.32及以上版本\n* 建议使用CUDA 11.4及以上（GPU用户、flash-attention用户等需考虑此选项）\n<br>\n\n## 快速使用\n\n我们提供简单的示例来说明如何利用🤖 ModelScope和🤗 Transformers快速使用Qwen-7B和Qwen-7B-Chat。\n\n你可以使用我们预构建好的Docker镜像，省去大部分配置环境的操作，详情见[“使用预构建的docker镜像”](#-使用预构建的docker镜像)一节。\n\n如不使用Docker，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n\n```bash\npip install -r requirements.txt\n```\n\n如果你的显卡支持fp16或bf16精度，我们还推荐安装[flash-attention](https://github.com/Dao-AILab/flash-attention)（**当前已支持flash attention 2**）来提高你的运行效率以及降低显存占用。(**flash-attention只是可选项，不安装也可正常运行该项目**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# 下方安装可选，安装可能比较缓慢。\n# pip install csrc/layer_norm\n# 如果flash-attn版本高于2.1.1，下方无需安装。\n# pip install csrc/rotary\n```\n\n接下来你可以开始使用Transformers或者ModelScope来使用我们的模型。\n\n### 🤗 Transformers\n\n如希望使用Qwen-chat进行推理，所需要写的只是如下所示的数行代码。**请确保你使用的是最新代码，并指定正确的模型名称和路径，如`Qwen/Qwen-7B-Chat`和`Qwen/Qwen-14B-Chat`**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# 可选的模型包括: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 打开bf16精度，A100、H100、RTX3060、RTX3070等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# 打开fp16精度，V100、P100、T4等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# 使用CPU进行推理，需要约32GB内存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# 默认使用自动模式，根据设备自动选择精度\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# 可指定不同的生成长度、top_p等相关超参\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 第一轮对话\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\n# 你好！很高兴为你提供帮助。\n\n# 第二轮对话\nresponse, history = model.chat(tokenizer, \"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\nprint(response)\n# 这是一个关于一个年轻人奋斗创业最终取得成功的故事。\n# 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。\n# 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。\n# 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。\n# 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。\n# 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。\n\n# 第三轮对话\nresponse, history = model.chat(tokenizer, \"给这个故事起一个标题\", history=history)\nprint(response)\n# 《奋斗创业：一个年轻人的成功之路》\n```\n\n运行Qwen同样非常简单。\n\n<details>\n  <summary>运行Qwen</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# 可选的模型包括: \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\n# 打开bf16精度，A100、H100、RTX3060、RTX3070等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# 打开fp16精度，V100、P100、T4等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# 使用CPU进行推理，需要约32GB内存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# 默认使用自动模式，根据设备自动选择精度\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# 可指定不同的生成长度、top_p等相关超参\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# 蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）...\n```\n\n</details>\n\n<p id=\"DownloadModel\">\n若在使用上述代码时由于各种原因无法从 HuggingFace 拉取模型和代码，可以先从 ModelScope 下载模型及代码至本地，再从本地加载模型：\n</p>\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n# model_dir = snapshot_download('qwen/Qwen-14B')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### 🤖 ModelScope\n\n魔搭（ModelScope）是开源的模型即服务共享平台，为泛AI开发者提供灵活、易用、低成本的一站式模型服务产品。使用ModelScope同样非常简单，代码如下所示：\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# 可选的模型包括: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"浙江的省会在哪里？\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"它有什么好玩的景点\", history=history)\nprint(response)\n```\n\n### Batch推理\n千问支持batch批量推理。在开启flash-attention的状态下，使用batch推理可以约40%的提速。示例代码如下所示：\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"我想听你说爱我。\", \"今天我想吃点啥，甜甜的，推荐下\", \"我马上迟到了，怎么做才能不迟到\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"我想听你说爱我。\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"今天我想吃点啥，甜甜的，推荐下\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"我马上迟到了，怎么做才能不迟到\", history=None)\nprint(response)\n```\n\n### CPU\n\n我们推荐你使用 [qwen.cpp](https://github.com/QwenLM/qwen.cpp) 来实现CPU部署和推理。qwen.cpp是Qwen和tiktoken的C++实现。你可以点击链接进入repo了解详情。\n\n当然，直接在CPU上运行模型也是可以的，示例如下：\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\n但是，这样的推理效率大概率会非常低。\n\n### 多GPU\n\n如果你遇到显存不足的问题而希望使用多张GPU进行推理，可以使用上述的默认的使用方法读取模型。此前提供的脚本`utils.py`已停止维护。\n\n尽管这个方法很简单，但它的效率相对较低。我们建议使用vLLM和FastChat并请阅读部署章节。\n\n### x86 平台\n在 酷睿™/至强® 可扩展处理器或 Arc™ GPU 上部署量化模型时，建议使用 [OpenVINO™ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html) 以充分利用硬件，实现更好的推理性能。您可以安装并运行此[example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)。相关问题，您可在 [OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues)中提交。\n\n\n### 阿里云灵积（DashScope）API服务\n最简单的使用Qwen模型API服务的方法就是通过DashScope（阿里云灵积API模型服务）。我们提供了简单介绍说明使用方法。同时，我们还提供了自己部署OpenAI格式的API的方法。\n\nDashScope是阿里云提供的大语言模型的API服务，目前支持Qwen。但请注意，目前提供服务的Qwen模型为内部模型，暂无更多具体细节对外透露。模型服务包括`qwen-turbo`、`qwen-plus`和`qwen-max`，`qwen-turbo`速度更快，`qwen-plus`效果更优，`qwen-max`是最新发布的千亿级通义千问2.0模型。详情请查看[文档](https://dashscope.aliyun.com)。\n\n请首先前往[官网](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn)开通DashScope，获得API Key（AK）。建议通过环境变量设置AK：\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\n随后安装相关代码包，点击[此处](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk)查看安装文档。如使用python，则直接通过pip安装：\n```bash\npip install dashscope\n```\n如安装JAVA SDK，则通过如下命令安装：\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\n最简单的使用方法就是通过messages调用，用法类似OpenAI API。示例如下：\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': '如何做西红柿鸡蛋？'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\n更多用法请查看官方文档了解详情。\n<br><br>\n\n\n## 量化\n\n### GPTQ\n\n我们提供了基于[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)的量化方案，并开源了Int4和Int8量化模型。量化模型的效果损失很小，但能显著降低显存占用并提升推理速度。\n\n以下我们提供示例说明如何使用Int4量化模型。在开始使用前，请先保证满足要求（如torch 2.0及以上，transformers版本为4.32.0及以上，等等），并安装所需安装包：\n\n```bash\npip install auto-gptq optimum\n```\n\n如安装`auto-gptq`遇到问题，我们建议您到官方[repo](https://github.com/PanQiWei/AutoGPTQ)搜索合适的wheel。\n\n> 注意：预编译的`auto-gptq`版本对`torch`版本及其CUDA版本要求严格。同时，由于\n> 其近期更新，你可能会遇到`transformers`、`optimum`或`peft`抛出的版本错误。\n> 我们建议使用符合以下要求的最新版本：\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0\n\n随后即可使用和上述一致的用法调用量化模型：\n\n```python\n# 可选模型包括：\"Qwen/Qwen-7B-Chat-Int4\", \"Qwen/Qwen-14B-Chat-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\n我们对BF16，Int8和Int4模型在基准评测上做了测试，发现量化模型效果损失较小，结果如下所示：\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n<br>\n\n\n### KV cache量化\n\n> 注意：由于Hugging Face的内部实现，本功能的支持文件`cache_autogptq_cuda_256.cpp`与`cache_autogptq_cuda_kernel_256.cu`可能没被下载。如需开启使用，请手动从相关位置下载，并放置到相应文件中。\n\n在模型推理时，我们可以将中间结果key以及value的值量化后压缩存储，这样便可以在相同的卡上存储更多的key以及value，增加样本吞吐。\n\n我们在`config.json`里提供了`use_cache_quantization`和`use_cache_kernel`两个参数来控制是否启用KV cache量化，具体使用方法如下：\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\n注意：当前该功能不支持与flash attention同时开启，如果你开了KV cache量化的同时又开了flash attention（`use_flash_attn=True`， `use_cache_quantization=True`, `use_cache_kernel=True`），程序默认将关闭`use_flash_attn`。\n\n效果方面，我们验证过Int8 KV Cache的使用对模型整体的精度指标基本无损。我们做了针对显存占用的性能测试。评测运行于单张A100-SXM4-80G GPU，模型默认使用BF16格式，默认生成1024个token，其中OOM表示内存不足。\n\n开启了KV cache量化之后，模型在推理的时候可以开启更大的batch size (bs)。\n\n| USE KV Cache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|--------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| No           | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  oom   |  oom   |\n| Yes          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\n\n开启了KV cache量化之后，模型在推理时可在生成更长的序列（sl，生成的token数）时，节约更多的显存。\n\n| USE KV Cache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|--------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| no           | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| yes          |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\n\n开启KV cache量化后，模型在推理时会将原始存进`layer-past`的float格式的key/value转换成int8格式，同时存储量化部分的参数。\n\n具体操作如下：\n\n1. 将key/value进行量化操作\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n2. 存入`layer_past`中:\n\n量化格式的`layer-past`:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\n原始格式的`layer-past`:\n```\n    layer_past=(key,value)\n```\n如果需要将`layer-past`中存好的key，value直接取出使用，可以使用反量化操作将Int8格式的key/value转回float格式：\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n### 推理性能\n这一部分将介绍模型推理的速度和显存占用的相关数据。下文的性能测算使用 [此脚本](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py) 完成。\n\n我们测算了BF16、Int8和Int4模型在生成2048个token时的平均推理速度（tokens/s）和显存使用。结果如下所示：\n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\n评测运行于单张A100-SXM4-80G GPU（除非提到使用2xA100），使用PyTorch 2.0.1、CUDA 11.8和Flash-Attention2。(72B + vLLM 使用 PyTorch 2.1.0和Cuda 11.8.)推理速度是生成2048个token的速度均值。\n\n注意：以上Int4/Int8模型生成速度使用autogptq库给出，当前``AutoModelForCausalLM.from_pretrained``载入的模型生成速度会慢大约20%。我们已经将该问题汇报给HuggingFace团队，若有解决方案将即时更新。\n\n我们还测量了不同上下文长度、生成长度、Flash-Attention版本的推理速度和 GPU 内存使用情况。可以在 Hugging Face 或 ModelScope 上的相应的模型介绍页面找到结果。\n\n## 微调\n\n### 使用方法\n我们提供了`finetune.py`这个脚本供用户实现在自己的数据上进行微调的功能，以接入下游任务。此外，我们还提供了shell脚本减少用户的工作量。这个脚本支持 [DeepSpeed](https://github.com/microsoft/DeepSpeed) 和 [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) 。我们提供的shell脚本使用了DeepSpeed，因此建议您确保已经安装DeepSpeed和Peft（注意：DeepSpeed可能不兼容最新的pydantic版本，请确保`pydantic<2.0`）。你可以使用如下命令安装：\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\n首先，你需要准备你的训练数据。你需要将所有样本放到一个列表中并存入json文件中。每个样本对应一个字典，包含id和conversation，其中后者为一个列表。示例如下所示：\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是一个语言模型，我叫通义千问。\"\n      }\n    ]\n  }\n]\n```\n\n准备好数据后，你可以使用我们提供的shell脚本实现微调。注意，你需要在脚本中指定你的数据的路径。\n\n微调脚本能够帮你实现：\n- 全参数微调\n- LoRA\n- Q-LoRA\n\n全参数微调在训练过程中更新所有参数。你可以运行这个脚本开始训练：\n\n```bash\n# 分布式训练。由于显存限制将导致单卡训练失败，我们不提供单卡训练脚本。\nbash finetune/finetune_ds.sh\n```\n\n尤其注意，你需要在脚本中指定正确的模型名称或路径、数据路径、以及模型输出的文件夹路径。在这个脚本中我们使用了DeepSpeed ZeRO 3。如果你想修改这个配置，可以删除掉`--deepspeed`这个输入或者自行根据需求修改DeepSpeed配置json文件。此外，我们支持混合精度训练，因此你可以设置`--bf16 True`或者`--fp16 True`。在使用fp16时，请使用DeepSpeed支持混合精度训练。经验上，如果你的机器支持bf16，我们建议使用bf16，这样可以和我们的预训练和对齐训练保持一致，这也是为什么我们把默认配置设为它的原因。\n\n运行LoRA的方法类似全参数微调。但在开始前，请确保已经安装`peft`代码库。另外，记住要设置正确的模型、数据和输出路径。我们建议你为模型路径使用绝对路径。这是因为LoRA仅存储adapter部分参数，而adapter配置json文件记录了预训练模型的路径，用于读取预训练模型权重。同样，你可以设置bf16或者fp16。\n\n```bash\n# 单卡训练\nbash finetune/finetune_lora_single_gpu.sh\n# 分布式训练\nbash finetune/finetune_lora_ds.sh\n```\n\n与全参数微调不同，LoRA ([论文](https://arxiv.org/abs/2106.09685)) 只更新adapter层的参数而无需更新原有语言模型的参数。这种方法允许用户用更低的显存开销来训练模型，也意味着更小的计算开销。\n\n注意，如果你使用预训练模型进行LoRA微调，而非chat模型，模型的embedding和输出层的参数将被设为可训练的参数。这是因为预训练模型没有学习过ChatML格式中的特殊token，因此需要将这部分参数设为可训练才能让模型学会理解和预测这些token。这也意味着，假如你的训练引入新的特殊token，你需要通过代码中的`modules_to_save`将这些参数设为可训练的参数。此外，这部分训练参数的引入会影响ZeRO 3的使用，因此我们默认推荐使用ZeRO 2。当然，如果你不需要引入这部分训练参数，你可以通过替换DeepSpeed的配置文件来使用ZeRO 3。如果你想节省显存占用，可以考虑使用chat模型进行LoRA微调，显存占用将大幅度降低。下文的显存占用和训练速度的记录将详细介绍这部分细节。\n\n如果你依然遇到显存不足的问题，可以考虑使用Q-LoRA ([论文](https://arxiv.org/abs/2305.14314)) 。该方法使用4比特量化模型以及paged attention等技术实现更小的显存开销。\n\n注意：如你使用单卡Q-LoRA，你可能需要安装`mpi4py`。你可以通过`pip`或者`conda`来安装。\n\n运行Q-LoRA你只需运行如下脚本：\n\n```bash\n# 单卡训练\nbash finetune/finetune_qlora_single_gpu.sh\n# 分布式训练\nbash finetune/finetune_qlora_ds.sh\n```\n\n我们建议你使用我们提供的Int4量化模型进行训练，即Qwen-7B-Chat-Int4。请**不要使用**非量化模型！与全参数微调以及LoRA不同，Q-LoRA仅支持fp16。注意，由于我们发现torch amp支持的fp16混合精度训练存在问题，因此当前的单卡训练Q-LoRA必须使用DeepSpeed。此外，上述LoRA关于特殊token的问题在Q-LoRA依然存在。并且，Int4模型的参数无法被设为可训练的参数。所幸的是，我们只提供了Chat模型的Int4模型，因此你不用担心这个问题。但是，如果你执意要在Q-LoRA中引入新的特殊token，很抱歉，我们无法保证你能成功训练。\n\n> 注意：由于Hugging Face的内部实现，模型在保存时，一些非Python文件未保存（例如`*.cpp`与`*.cu`），如需要支持相关功能，请手动复制有关文件。\n\n与全参数微调不同，LoRA和Q-LoRA的训练只需存储adapter部分的参数。假如你需要使用LoRA训练后的模型，你需要使用如下方法。假设你使用Qwen-7B训练模型，你可以用如下代码读取模型：\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n> 注意: 如果`peft>=0.8.0`，加载模型同时会尝试加载tokenizer，但peft内部未相应设置`trust_remote_code=True`，导致`ValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.`要避过这一问题，你可以降级`peft<0.8.0`或将tokenizer相关文件移到其它文件夹。\n\n\n如果你觉得这样一步到位的方式让你很不安心或者影响你接入下游应用，你可以选择先合并并存储模型（LoRA支持合并，Q-LoRA不支持），再用常规方式读取你的新模型，示例如下：\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\n`new_model_directory`目录将包含合并后的模型参数与相关模型代码。请注意`*.cu`和`*.cpp`文件可能没被保存，请手动复制。另外，`merge_and_unload`仅保存模型，并未保存tokenizer，如有需要，请复制相关文件或使用以以下代码保存\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    path_to_adapter, # path to the output directory\n    trust_remote_code=True\n)\ntokenizer.save_pretrained(new_model_directory)\n```\n\n\n注意：分布式训练需要根据你的需求和机器指定正确的分布式训练超参数。此外，你需要根据你的数据、显存情况和训练速度预期，使用`--model_max_length`设定你的数据长度。\n\n### 量化微调后模型\n\n这一小节用于量化全参/LoRA微调后的模型。（注意：你不需要量化Q-LoRA模型因为它本身就是量化过的。）\n如果你需要量化LoRA微调后的模型，请先根据上方说明去合并你的模型权重。\n\n我们推荐使用[auto_gptq](https://github.com/PanQiWei/AutoGPTQ)去量化你的模型。\n\n```bash\npip install auto-gptq optimum\n```\n\n注意: 当前AutoGPTQ有个bug，可以在该[issue](https://github.com/PanQiWei/AutoGPTQ/issues/370)查看。这里有个[修改PR](https://github.com/PanQiWei/AutoGPTQ/pull/495)，你可以使用该分支从代码进行安装。\n\n首先，准备校准集。你可以重用微调你的数据，或者按照微调相同的方式准备其他数据。\n\n第二步，运行以下命令：\n\n```bash\npython run_gptq.py \\\n    --model_name_or_path $YOUR_LORA_MODEL_PATH \\\n    --data_path $DATA \\\n    --out_path $OUTPUT_PATH \\\n    --bits 4 # 4 for int4; 8 for int8\n```\n\n这一步需要使用GPU，根据你的校准集大小和模型大小，可能会消耗数个小时。\n\n接下来, 将原模型中所有 `*.py`, `*.cu`, `*.cpp` 文件和 `generation_config.json` 文件复制到输出模型目录下。同时，使用官方对应版本的量化模型的 `config.json` 文件覆盖输出模型目录下的文件\n(例如, 如果你微调了 `Qwen-7B-Chat`和`--bits 4`, 那么你可以从 [Qwen-7B-Chat-Int4](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4/blob/main/config.json) 仓库中找到对应的`config.json` )。\n并且，你需要将 ``gptq.safetensors`` 重命名为 ``model.safetensors``。\n\n最后，像官方量化模型一样测试你的模型。例如：\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"/path/to/your/model\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/path/to/your/model\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\n```\n\n### 多机微调\n\n我们提供的脚本支持多机微调，可以参考[脚本](./finetune/finetune_lora_ds.sh)中的注释，在每个节点上正确设置相应的参数并启动训练脚本。关于多机分布式训练的更多信息，请参考[torchrun](https://pytorch.org/docs/stable/elastic/run.html)。\n\n注意： DeepSpeed ZeRO 3 对节点间通信速率的要求远大于 ZeRO 2，在多机微调的情况下会大幅降低训练速度。因此，我们不建议在多机微调的情况下使用 DeepSpeed ZeRO 3 配置。\n\n### 显存占用及训练速度\n\n下面记录7B和14B模型在单GPU使用LoRA（LoRA (emb)指的是embedding和输出层参与训练，而LoRA则不优化这部分参数）和QLoRA时处理不同长度输入的显存占用和训练速度的情况。本次评测运行于单张A100-SXM4-80G GPU，使用CUDA 11.8和Pytorch 2.0，并使用了flash attention 2。我们统一使用batch size为1，gradient accumulation为8的训练配置，记录输入长度分别为256、512、1024、2048、4096和8192的显存占用（GB）和训练速度（s/iter）。我们还使用2张A100测了Qwen-7B的全参数微调。受限于显存大小，我们仅测试了256、512和1024token的性能。\n\n对于 Qwen-7B，我们额外测试了多机微调的性能。我们在两台服务器上运行评测，每台服务器包含两张A100-SXM4-80G GPU，其余配置与Qwen-7B的其他评测相同。多机微调的结果在表中以 LoRA (multinode) 标示。\n\n对于 Qwen-72B，我们测试了两种方案：1）使用4个 A100-SXM4-80G GPUs，通过 Lora + DeepSpeed ZeRO 3 微调和2）使用单张A100-SXM4-80G GPU，通过 QLora (int4) 微调。请注意，使用 LoRA (emb) 微调和不带 DeepSpeed ZeRO 3 的 LoRA 微调在4个A100-SXM4-80G GPUs 上都会出现OOM（你可以通过将`--deepspeed finetune/ds_config_zero3.json`参数传给[`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh)来打开 DeepSpeed ZeRO 3 配置）。\n\n具体数值如下所示：\n\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th rowspan=\"2\">#Nodes</th><th rowspan=\"2\">#GPUs per node</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"5\">7B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n<td>1</td><td>2</td>\n<td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>LoRA (multinode)</td>\n        <td>2</td><td>2</td>\n        <td align=\"center\">74.7G / 2.09s/it</td><td align=\"center\">77.6G / 3.16s/it</td><td align=\"center\">84.9G / 5.17s/it</td><td align=\"center\">95.1G / 9.25s/it</td><td align=\"center\">121.1G / 18.1s/it</td><td align=\"center\">155.5G / 37.4s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"2\">72B</th>\n        <td>LoRA + Deepspeed Zero3</td>\n        <td>1</td><td>4</td>\n        <td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n\n<br>\n\n## 部署\n\n### vLLM\n如希望部署及加速推理，我们建议你使用vLLM。\n\n如果你使用**CUDA 12.1和PyTorch 2.1**，可以直接使用以下命令安装vLLM。\n\n```bash\npip install vllm\n```\n\n否则请参考vLLM官方的[安装说明](https://docs.vllm.ai/en/latest/getting_started/installation.html)。\n\n#### vLLM + 类Transformer接口\n\n请下载[接口封装代码](examples/vllm_wrapper.py)到当前文件夹，并执行以下命令进行多轮对话交互。（注意：该方法当前只支持``model.chat()``接口。）\n\n```python\nfrom vllm_wrapper import vLLMWrapper\n\nmodel = vLLMWrapper('Qwen/Qwen-7B-Chat', tensor_parallel_size=1)\n\nresponse, history = model.chat(query=\"你好\", history=None)\nprint(response)\nresponse, history = model.chat(query=\"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\nprint(response)\nresponse, history = model.chat(query=\"给这个故事起一个标题\", history=history)\nprint(response)\n```\n\n#### vLLM + 网页Demo / 类OpenAI API\n\n你可以使用FastChat去搭建一个网页Demo或类OpenAI API服务器。首先，请安装FastChat：\n\n```bash\npip install \"fschat[model_worker,webui]\"\n```\n\n使用vLLM和FastChat运行Qwen之前，首先启动一个controller：\n```bash\npython -m fastchat.serve.controller\n```\n\n然后启动model worker读取模型。如使用单卡推理，运行如下命令：\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype float16 # 运行int4模型\n```\n然而，如果你希望使用多GPU加速推理或者增大显存，你可以使用vLLM支持的模型并行机制。假设你需要在4张GPU上运行你的模型，命令如下所示：\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype float16 # 运行int4模型\n```\n\n启动model worker后，你可以启动一个：\n\n* Web UI Demo\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* OpenAI API\n\n使用OpenAI API前，请阅读我们的API章节配置好环境，然后运行如下命令：\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\n然而，如果你觉得使用vLLM和FastChat比较困难，你也可以尝试以下我们提供的最简单的方式部署Web Demo、CLI Demo和OpenAI API。\n<br>\n\n\n### Web UI\n\n我们提供了Web UI的demo供用户使用 (感谢 @wysaid 支持)。在开始前，确保已经安装如下代码库：\n\n```bash\npip install -r requirements_web_demo.txt\n```\n\n随后运行如下命令，并点击生成链接：\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n### 交互式Demo\n\n我们提供了一个简单的交互式Demo示例，请查看`cli_demo.py`。当前模型已经支持流式输出，用户可通过输入文字的方式和Qwen-7B-Chat交互，模型将流式输出返回结果。运行如下命令：\n\n```bash\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\n我们提供了OpenAI API格式的本地API部署方法（感谢@hanpenggit）。在开始之前先安装必要的代码库：\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\n随后即可运行以下命令部署你的本地API：\n\n```bash\npython openai_api.py\n```\n\n你也可以修改参数，比如`-c`来修改模型名称或路径, `--cpu-only`改为CPU部署等等。如果部署出现问题，更新上述代码库往往可以解决大多数问题。\n\n使用API同样非常简单，示例如下：\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# 使用流式回复的请求\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=True\n    # 流式输出的自定义stopwords功能尚未支持，正在开发中\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# 不使用流式回复的请求\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=False,\n    stop=[] # 在此处添加自定义的stop words 例如ReAct prompting时需要增加： stop=[\"Observation:\"]。\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n该接口也支持函数调用（**Function Calling**），但暂时仅限 `stream=False` 时能生效。用法见[函数调用示例](examples/function_call_examples.py)。\n<br><br>\n\n## 🐳 使用预构建的Docker镜像\n\n为简化部署流程，我们提供了预配置好相应环境的Docker镜像：[qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen)，只需安装驱动、下载模型文件即可启动Demo、部署OpenAI API以及进行微调。\n\n### 准备操作\n\n1. 根据需要使用的镜像版本，安装相应版本的Nvidia驱动：\n  - `qwenllm/qwen:cu117`（**推荐**）：`>= 515.48.07`\n  - `qwenllm/qwen:cu114`（不支持flash-attention）：`>= 470.82.01`\n  - `qwenllm/qwen:cu121`：`>= 530.30.02`\n  - `qwenllm/qwen:latest`：与`qwenllm/qwen:cu117`相同\n\n2. 安装并配置[docker](https://docs.docker.com/engine/install/)和[nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)：\n\n```bash\n# 配置docker\nsudo systemctl start docker\n# 测试docker是否安装正确\nsudo docker run hello-world\n\n# 配置nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# 测试nvidia-container-toolkit是否安装正确\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. 下载模型及代码至本地（参考[此处说明](#DownloadModel)）\n\n### 部署\n\n下面我们以Qwen-7B-Chat为例。在启动Web Demo或者部署API前，请先参照下方代码完成配置工作：\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # 下载到本地的模型及代码路径\n```\n\n如下脚本可以帮你部署:\n\n* OpenAI API\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Web UI\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* 交互式Demo\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\n这些命令将自动下载所需镜像以及后台启动Web UI Demo。你可以打开`http://localhost:${PORT}` 来使用该Demo。\n\n如果输出如下内容，则说明Demo启动成功：\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\n如果你想查看Demo的状态，你可以使用这个命令来展示输出结果：`docker logs qwen`。\n\n你可以使用这个命令`docker rm -f qwen`来停止服务并删除容器。\n\n### 微调\n\n使用预配置好的Docker镜像进行微调的方法与[上一章](#微调)基本一致（我们已经在镜像中安装了相关依赖）：\n\n以下是一个单卡LoRA微调的示例：\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # 下载的模型和代码路径\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # 下载的模型和代码路径 (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # 准备微调数据放在 ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # 微调输出路径\n\n# 默认使用主机所有GPU\nDEVICE=all\n# 如果需要指定用于训练的GPU，按照以下方式设置device（注意：内层的引号不可省略）\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# 单卡LoRA微调\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\n如需修改为单卡Q-LoRA微调示例，只要修改`docker run`中的bash命令：\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## 🔥 系统指令 (System Prompt)\nQwen-1.8-Chat 和 Qwen-72B-Chat 通义千问在多样且存在多轮复杂交互的系统指令上进行了充分训练，使模型可以跟随多样的系统指令，实现上下文(in-context)中的模型定制化，进一步提升了通义千问的可扩展性。\n\n通过系统指令，Qwen-Chat能够实现**角色扮演**，**语言风格迁移**，**任务设定**，和**行为设定**等能力。\n\n![](assets/system_prompt_language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\n更多关于系统指令的介绍信息可以参考[示例文档](examples/system_prompt.md).\n\n\n## 工具调用\n\nQwen-Chat针对工具使用、函数调用能力进行了优化。用户可以开发基于Qwen的Agent、LangChain应用、甚至Code Interpreter。\n\n我们提供了文档说明如何根据ReAct Prompting的原理实现工具调用，请参见[ReAct示例](examples/react_prompt.md)。基于该原理，我们在 [openai_api.py](openai_api.py) 里提供了函数调用（Function Calling）的支持。\n我们在已开源的中文[评测数据集](eval/EVALUATION.md)上测试模型的工具调用能力，并发现Qwen-Chat能够取得稳定的表现：\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">中文工具调用评测基准（版本 20231206）</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.↑)</th><th align=\"center\">Tool Input (Rouge-L↑)</th><th align=\"center\">False Positive Error↓</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\n为了考察Qwen使用Python Code Interpreter完成数学解题、数据可视化、及文件处理与爬虫等任务的能力，我们专门建设并开源了一个评测这方面能力的[评测基准](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)。\n我们发现Qwen在生成代码的可执行率、结果正确性上均表现较好：\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">代码执行结果正确性 (%)</th>\n        <th colspan=\"1\" align=\"center\">生成代码的可执行率 (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Math↑</th><th align=\"center\">Visualization-Hard↑</th><th align=\"center\">Visualization-Easy↑</th><th align=\"center\">General↑</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## 长文本理解\n\n我们引入了NTK插值、窗口注意力、LogN注意力缩放等技术来提升模型的上下文长度并突破训练序列长度的限制，原生长度为2K的Qwen-14B可以扩展到8K的序列长度，而原生长度8K的Qwen-1.8B/7B能够在32K长序列的设置下取得不错的表现。\n\n对于Qwen-72B，我们基于RoPE采用更大的旋转Base来适应更长的上下文。Qwen-72B支持32K的上下文长度。\n\n通过arXiv数据集上的语言模型实验，发现 Qwen 在长上下文场景下可以达到出色的性能。结果如下：\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n            <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\"><b>3.23</b></td><td align=\"center\">3.33</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n</table>\n\n进一步，我们为了验证Qwen-72B-Chat在长文本任务上的能力，在[L-Eval](https://arxiv.org/abs/2307.11088)客观题上进行了测试，评分结果如下：\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\n\n我们进一步进行了“大海捞针”实验（想法来自于[@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)），测试模型在不同长度的输入下，是否能检索到文章不同位置的信息，结果如下：\n\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\n以上结果说明，Qwen-72B-Chat可以能准确检索到32K以内的输入长度中放在各种位置的信息，证明了其具有优秀的长文本处理能力。\n\n## Tokenizer\n\n> 注：作为术语的“tokenizer”在中文中尚无共识的概念对应，本文档采用英文表达以利说明。\n\n基于tiktoken的tokenizer有别于其他分词器，比如sentencepiece tokenizer。尤其在微调阶段，需要特别注意特殊token的使用。关于tokenizer的更多信息，以及微调时涉及的相关使用，请参阅[文档](tokenization_note_zh.md)。\n<br><br>\n\n## 复现\n\n我们提供了评测脚本以供复现我们的实验结果。注意，由于内部代码和开源代码存在少许差异，评测结果可能与汇报结果存在细微的结果不一致。请阅读[eval/EVALUATION.md](eval/EVALUATION.md)了解更多信息。\n<br><br>\n\n## FAQ\n\n如遇到问题，敬请查阅[FAQ](FAQ_zh.md)以及issue区，如仍无法解决再提交issue。\n<br><br>\n\n## 引用\n如果你觉得我们的工作对你有帮助，欢迎引用！\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## 使用协议\n\n<https://github.com/QwenLM/Qwen>中的源代码采用[Apache 2.0协议](./LICENSE)授权，您可在该仓库根目录找到协议全文。\n\n研究人员与开发者可使用Qwen和Qwen-Chat或进行二次开发。对于商业使用，请查看模型各自的LICENSE。\n\n- Qwen-72B、Qwen-14B和Qwen-7B采用[Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT)授权，您可在相应模型的HuggingFace或ModelScope仓库找到协议原文。如需商用，您只需遵循使用协议进行商用即可，我们欢迎您填写问卷([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat)、[14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat)、[7B](https://dashscope.console.aliyun.com/openModelApply/qianwen))。\n\n- Qwen-1.8B采用[Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT)授权，您可在相应模型的HuggingFace或ModelScope仓库找到协议原文。如需商用，请联系我们。\n\n<br><br>\n\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，欢迎加入我们的微信群和Discord server。当然也可以通过邮件（qianwen_opensource@alibabacloud.com）联系我们。\n\n"
        },
        {
          "name": "README_ES.md",
          "type": "blob",
          "size": 71.8203125,
          "content": "<p align=\"left\">\n    <a href=\"README_CN.md\">中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>&nbsp ｜ &nbsp<a href=\"README_JA.md\">日本語</a> ｜ &nbsp<a href=\"README_FR.md\">Français</a> ｜ &nbspEspañol\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        🤗 <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ｜ &nbsp&nbsp🖥️ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ｜  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br><br>\n\n> [!Important]\n> ¡Qwen2 está aquí! Estás invitado a seguir [QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) y compartir tu experiencia allí.\n>\n> Este repositorio ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) ya no se mantiene activamente, debido a diferencias sustanciales en la base de código.\n<br>\n\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">🤗</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">🤗</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">🤗</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">🤗</a> |\n\n\n\nAbrimos nuestra serie **Qwen**, que ahora incluye **Qwen**, los modelos de lenguaje, es decir **Qwen-7B** y **Qwen-14B**, así como **Qwen-Chat**, los modelos de chat, es decir **Qwen-7B-Chat** y **Qwen-14B-Chat**. Los enlaces se encuentran en la tabla anterior. Haz clic en ellos y comprueba las fichas de los modelos. Además, publicamos el **[informe técnico](https://arxiv.org/abs/2309.16609)**. Haz clic en el enlace y compruébalo.\n\nEn resumen, disponemos de modelos lingüísticos sólidos, que han sido preentrenados de forma estable para hasta 3 billones de tokens de datos multilingües con una amplia cobertura de dominios, idiomas (con especial atención al chino y al inglés), etc. Son capaces de lograr un rendimiento competitivo en conjuntos de datos de referencia. Además, disponemos de modelos de chat alineados con las preferencias humanas basados en SFT y RLHF (aún no publicados), que son capaces de chatear, crear contenidos, extraer información, resumir, traducir, codificar, resolver problemas matemáticos, etc., y son capaces de utilizar herramientas, jugar como agentes o incluso jugar como intérpretes de código, etc.\n\n| Modelo    | Fecha de Publicación | Longitud Máx. | Mejora del Sistema de Avisos | # de Fichas Preentrenadas | Uso Mínimo de Memoria GPU de Finetuning (Q-Lora) | Uso Mínimo de la GPU para Generar 2048 Tokens (Int4) | Uso de Herramientas |\n|:----------|:--------------------:|:-------------:|:----------------------------:|:-------------------------:|:------------------------------------------------:|:----------------------------------------------------:|:-------------------:|\n| Qwen-1.8B |       23.11.30       |      32K      |              ✅               |           2.2T            |                      5.8GB                       |                        2.9GB                         |          ✅          |  \n| Qwen-7B   |       23.08.03       |      32K      |              ❎               |           2.4T            |                      11.5GB                      |                        8.2GB                         |          ✅          |   \n| Qwen-14B  |       23.09.25       |      8K       |              ❎               |           3.0T            |                      18.7GB                      |                        13.0GB                        |          ✅          |\n| Qwen-72B  |       23.11.30       |      32K      |              ✅               |           3.0T            |                      61.4GB                      |                        48.9GB                        |          ✅          |   \n\nEn este repo, usted puede averiguar:\n\n* Inicio rápido con Qwen, y disfrute de la simple inferencia.\n* Detalles sobre los modelos de cuantificación, incluyendo GPTQ y cuantización de caché KV.\n* Estadísticas de rendimiento de la inferencia, incluyendo velocidad y memoria.\n* Tutoriales sobre ajuste fino, incluyendo ajuste de parámetros completos, LoRA y Q-LoRA.\n* Instrucciones de despliegue, con el ejemplo de vLLM y FastChat.\n* Instrucciones para construir demos, incluyendo WebUI, CLI demo, etc.\n* Introducción al servicio API de DashScope, así como instrucciones para crear una API de estilo OpenAI para tu modelo.\n* Información sobre Qwen para el uso de herramientas, agente e intérprete de código.\n* Estadísticas de la evaluación de la comprensión del contexto largo\n* Acuerdo de licencia\n* ...\n\nAdemás, si tienes problemas, consulta primero [FAQ](FAQ.md) para obtener ayuda. ¿Sigues teniendo problemas? No dudes en plantearnos tus problemas (mejor en inglés para que te entienda más gente). Si quieres ayudarnos, ¡envíanos pull requests sin dudarlo! ¡Siempre nos entusiasman los PR!\n\n¿Quieres charlar con nosotros o quedar para tomar un café? ¡Bienvenido a nuestro Discord o WeChat!\n<br><br>\n\n## Noticias y Actualizaciones\n\n* 2023.11.30 🔥 Lanzamos **Qwen-72B** y **Qwen-72B-Chat**, que están entrenados en tokens 3T y soportan 32k contextos, junto con **Qwen-1.8B**, y **Qwen-1.8B-Chat**, en ModelScope y Hugging Face. También hemos reforzado las capacidades de System Prompt de Qwen-72B-Chat y Qwen-1.8B-Chat, ver [documentación de ejemplo](examples/system_prompt.md). Adicionalmente, soporta la inferencia en **Ascend 910** y **Hygon DCU**. Consulta `ascend-support` y `dcu-support` para más detalles.\n* 2023.10.17 Publicamos el modelo cuantizado Int8 **Qwen-7B-Chat-Int8** y **Qwen-14B-Chat-Int8**.\n* 2023.9.25 Publicamos **Qwen-14B** y **Qwen-14B-Chat** en ModelScope y Hugging Face, junto con [qwen.cpp](https://github.com/QwenLM/qwen.cpp) y [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). También se actualizan los códigos y pesos de **Qwen-7B** y **Qwen-7B-Chat**. **POR FAVOR, DESCARGA LA ÚLTIMA VERSIÓN!**\n    - En comparación con **Qwen-7B** (original), **Qwen-7B** utiliza más tokens de entrenamiento, pasando de 2,2T tokens a 2,4T tokens, mientras que la longitud del contexto se amplía de 2048 a 8192. El conocimiento del chino y la capacidad de codificación de **Qwen-7B** se han mejorado aún más.\n* 2023.9.12 Ahora es posible el ajuste fino de los modelos Qwen-7B, incluido el ajuste fino de parámetros completos, LoRA y Q-LoRA.\n* 2023.8.21 Publicamos el modelo cuantizado Int4 para Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, que requiere bajos costes de memoria pero consigue mejorar la velocidad de inferencia. Además, no se produce una degradación significativa del rendimiento en la evaluación comparativa.\n* 2023.8.3 Publicamos **Qwen-7B** y **Qwen-7B-Chat** en ModelScope y Hugging Face. También proporcionamos una nota técnica para más detalles sobre el modelo, incluidos los detalles de entrenamiento y el rendimiento del modelo.\n<br>\n\n## Rendimiento\n\nLos modelos Qwen superan a los modelos de referencia de tamaños de modelo similares en una serie de conjuntos de datos de referencia, como MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., que evalúan las capacidades de los modelos en comprensión del lenguaje natural, resolución de problemas matemáticos, codificación, etc. Qwen-72B obtiene mejores resultados que LLaMA2-70B en todas las tareas y supera a GPT-3.5 en 7 de cada 10 tareas.\n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=600px/>\n<p>\n<br>\n\n| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |\n| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\nPara todos los modelos comparados, presentamos las mejores puntuaciones entre sus resultados oficiales y [OpenCompass](https://opencompass.org.cn/leaderboard-llm).\n\nPara más resultados experimentales (rendimiento detallado del modelo en más conjuntos de datos de referencia) y detalles, consulte nuestro informe técnico haciendo clic [aquí](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).\n<br><br>\n\n## Requisitos\n\n* python 3.8 y superior\n* pytorch 1.12 y superior, se recomienda 2.0 y superior\n* transformers 4.32 y superiores\n* Se recomienda CUDA 11.4 y superior (esto es para usuarios de GPU, usuarios de flash-attention, etc.)\n<br>\n\n## Inicio rápido\n\nA continuación, proporcionamos ejemplos sencillos para mostrar cómo utilizar Qwen-Chat con 🤖 ModelScope y 🤗 Transformers.\n\nPuedes usar nuestras imágenes docker pre-construidas para saltarte la mayoría de los pasos de configuración del entorno, mira la Sección [\"Usando Imágenes Docker Pre-construidas\"](#-docker) para más detalles. \n\nSi no utiliza Docker, asegúrese de haber configurado el entorno e instalado los paquetes necesarios. Asegúrese de que cumple los requisitos anteriores y, a continuación, instale las bibliotecas dependientes.\n\n```bash\npip install -r requirements.txt\n```\n\nSi tu dispositivo soporta fp16 o bf16, te recomendamos instalar [flash-attention](https://github.com/Dao-AILab/flash-attention) (**ahora soportamos flash attention 2.**) para una mayor eficiencia y un menor uso de memoria. (**flash-attention es opcional y el proyecto puede ejecutarse normalmente sin instalarlo**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# Below are optional. Installing them might be slow.\n# pip install csrc/layer_norm\n# pip install csrc/rotary\n```\n\nAhora puedes empezar con ModelScope o Transformers.\n\n### 🤗 Transformers\n\nPara utilizar Qwen-Chat para la inferencia, todo lo que tienes que hacer es introducir unas pocas líneas de código como se demuestra a continuación. Recuerda introducir los nombres o rutas correctos de los modelos, como \"Qwen/Qwen-7B-Chat\" y \"Qwen/Qwen-14B-Chat\". Sin embargo, **por favor, asegúrese de que está utilizando el código más reciente.**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\n# 你好！很高兴为你提供帮助。\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\nprint(response)\n# 这是一个关于一个年轻人奋斗创业最终取得成功的故事。\n# 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。\n# 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。\n# 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。\n# 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。\n# 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。\n\n# 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"给这个故事起一个标题\", history=history)\nprint(response)\n# 《奋斗创业：一个年轻人的成功之路》\n```\n\nEjecutar Qwen, el modelo lingüístico base, también es sencillo.\n\n<details>\n  <summary>Ejecutar Qwen</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\" \ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# 蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）...\n```\n\n</details>\n\nEn caso de que se produzca un problema de red al intentar descargar puntos de control y códigos de modelos desde Hugging Face, un método alternativo consiste en obtener inicialmente el punto de control desde ModelScope y luego cargarlo desde el directorio local como se indica a continuación:\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B', revision='v1.1.4')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat', revision='v1.1.4')\n# model_dir = snapshot_download('qwen/Qwen-14B', revision='v1.0.4')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat', revision='v1.0.4')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### 🤖 ModelScope\n\nModelScope es una plataforma de código abierto para Model-as-a-Service (MaaS), que proporciona un servicio de modelos flexible y rentable a los desarrolladores de IA. Del mismo modo, puede ejecutar los modelos con ModelScope como se muestra a continuación:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# Model names: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", revision='v1.0.5', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", revision='v1.0.5', device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", revision='v1.0.5', trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"浙江的省会在哪里？\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"它有什么好玩的景点\", history=history)\nprint(response)\n```\n\n### Inferencia por lotes\nQwen admite la inferencia por lotes. Con la atención flash activada, el uso de la inferencia por lotes puede suponer un aumento de velocidad del 40%. El código de ejemplo se muestra a continuación:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"我想听你说爱我。\", \"今天我想吃点啥，甜甜的，推荐下\", \"我马上迟到了，怎么做才能不迟到\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"我想听你说爱我。\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"今天我想吃点啥，甜甜的，推荐下\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"我马上迟到了，怎么做才能不迟到\", history=None)\nprint(response)\n```\n\n### CPU\n\nPara desplegar nuestros modelos en la CPU, le recomendamos encarecidamente que utilice [qwen.cpp](https://github.com/QwenLM/qwen.cpp), que es una implementación C++ pura de Qwen y tiktoken. Comprueba el repositorio para más detalles.\n\nAdemás, también es sencillo ejecutar directamente el modelo en la CPU, lo que requiere que especifiques el dispositivo:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nPero es probable que sufra una eficacia de inferencia extremadamente baja.\n\n### Múltiples GPU\n\nSi sufres de falta de memoria en la GPU y quieres ejecutar el modelo en más de 1 GPU, puedes utilizar directamente el método de carga por defecto, que ahora es soportado por Transformers. El método anterior basado en `utils.py` está obsoleto.\n\nSin embargo, aunque este método es sencillo, la eficiencia del paralelismo del pipeline nativo es baja. Le aconsejamos que utilice vLLM con FastChat y por favor lea la sección para el despliegue.\n\n### DashScope\n\nLa forma más sencilla de utilizar Qwen a través de APIs es el servicio DashScope API a través de Alibaba Cloud. Damos una introducción al uso. Además, proporcionamos un script para que despliegues una API estilo OpenAI en tus propios servidores.\n\nDashScope es el gran servicio de API de modelos lingüísticos proporcionado por Alibaba Cloud, que ahora es compatible con Qwen. Tenga en cuenta que los modelos detrás de DashScope son versiones internas temporalmente sin detalles proporcionados. Los servicios incluyen `qwen-turbo` y `qwen-plus`, donde el primero se ejecuta más rápido y el segundo consigue un mejor rendimiento. Para más información, visita la documentación [aquí](https://dashscope.aliyun.com).\n\nDirígete al sitio web oficial [enlace](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) para crear una cuenta DashScope y obtener la clave API (AK). Recomendamos configurar la AK con una variable de entorno:\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nA continuación, instala los paquetes y haz clic [aquí](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) para consultar la documentación. Si utilizas Python, puedes instalar DashScope con pip:\n```bash\npip install dashscope\n```\nSi utiliza JAVA SDK, puede instalarlo de esta forma:\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nLa forma más sencilla de utilizar DashScope es el uso con mensajes, que es similar a la API OpenAI. El ejemplo se muestra a continuación:\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': '如何做西红柿鸡蛋？'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\nPara más usos, visite el sitio web oficial.\n<br><br>\n\n## Cuantización\n\n### GPTQ\n\nProporcionamos una solución basada en [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), y liberamos los modelos cuantificados Int4 e Int8, que consiguen efectos de modelo casi sin pérdidas pero un rendimiento mejorado tanto en costes de memoria como en velocidad de inferencia.\n\nAquí demostramos cómo utilizar los modelos cuantizados que proporcionamos para la inferencia. Antes de empezar, asegúrese de que cumple los requisitos de auto-gptq (por ejemplo, torch 2.0 y superior, transformers 4.32.0 y superior, etc.) e instale los paquetes necesarios:\n\n```bash\npip install auto-gptq optimum\n```\n\nSi tiene problemas para instalar `auto-gptq`, le aconsejamos que consulte el [repo] oficial (https://github.com/PanQiWei/AutoGPTQ) para encontrar una rueda.\n\n> Nota: Los paquetes `auto-gptq` precompilados dependen en gran medida de la versión de `torch` y de su versión CUDA. Además, debido a la reciente actualización \n> también puede encontrar errores de versión no soportada de `transformers`, `optimum`, o `peft`.\n> Recomendamos utilizar las últimas versiones que cumplan los siguientes requisitos:\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - antorcha>=2.0,<2.1 auto-gptq<0.5.0 transformadores<4.35.0 óptimo<1.14.0 peft>=0.5.0,<0.6.0\n\nA continuación, puede cargar el modelo cuantizado fácilmente y ejecutar la inferencia como de costumbre:\n\n```python\n# Model names: \"Qwen/Qwen-7B-Chat-Int4\", \"Qwen/Qwen-14B-Chat-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\nIlustramos el rendimiento de los modelos BF16, Int8 e Int4 en la prueba de referencia, y observamos que el modelo cuantizado no sufre una degradación significativa del rendimiento. Los resultados se muestran a continuación:\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n\n### Cuantización de la caché KV\n\n> NOTA: Por favor, ten en cuenta que debido al mecanismo interno de Hugging Face, los archivos de soporte para esta funcionalidad\n> (es decir, `cache_autogptq_cuda_256.cpp` y `cache_autogptq_cuda_kernel_256.cu`). \n> Por favor, descárguelos manualmente del Hugging Face Hub y colóquelos en la misma carpeta que los demás archivos del módulo.\n\nLa caché KV de atención puede cuantificarse y comprimirse para su almacenamiento, con el fin de obtener un mayor rendimiento de la muestra. Los argumentos `use_cache_quantization` y `use_cache_kernel` en `config.json` se proporcionan para habilitar la cuantización de la caché KV. \nEl método de uso específico es el siguiente:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\nAtención: Actualmente, la cuantización de caché KV y flash attention no se pueden utilizar al mismo tiempo.\nSi habilita la cuantización de caché KV y flash attention al mismo tiempo (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` está deshabilitado por defecto (`use_flash_attn=false`).\n\nHemos comprobado que el uso del modelo int8-kvcache cuantizado no sufre una degradación significativa del rendimiento en la evaluación posterior. A continuación, nos centraremos en el análisis de su huella de memoria en diferentes condiciones. \nEl perfil se ejecuta en una única GPU A100-SXM4-80G con PyTorch 2.0.1 y CUDA 11.4. \nUtilizamos modelos BF16 para generar 1024 tokens por defecto, y \"OOM\" indica error de memoria insuficiente.\n\nCon la cuantización de la caché KV, el modelo puede inferir con un tamaño de lote (bs) mayor.\n\n| Utilizar la caché KV |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|----------------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| No                   | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |\n| Yes                  | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\nCon la cuantización kv-cache activada, el modelo puede ahorrar más memoria cuando genera seq-length más largos (sl, número de tokens generados) en infer.\n\n| Utilizar la caché KV | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|----------------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| No                   | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| Yes                  |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\nEl modelo con cuantificación de caché KV convertirá el formato de `layer_past` de float a int8, y mientras tanto el `layer-past` cuantificado también almacenará los parámetros de cuantificación.\n\nLos pasos específicos son los siguientes\n\n1. Cuantificar clave/valor\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n2. Almacenar en layer_past\n\nA continuación se muestra el formato de `layer_past` cuantificado:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\nA continuación se muestra el formato original de `layer_past`:\n```\n    layer_past=(key,value)\n```\nSi desea utilizar la atención KV que se cuantiza, \npuede utilizar la operación de decuantización para convertir la clave/valor int8 de nuevo al formato float de la siguiente manera:\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n\n## Rendimiento de Inferencia\n\nEsta sección proporciona las estadísticas de velocidad y memoria de los modelos en diferentes precisiones. Los perfiles de velocidad y memoria se realizan utilizando [este script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).\n\nMedimos la velocidad media de inferencia (tokens/s) y el uso de memoria de la GPU al generar 2048 con los modelos en BF16, Int8 e Int4.\n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nEl perfil se ejecuta en una única GPU A100-SXM4-80G (salvo que se mencione 2xA100) con PyTorch 2.0.1, CUDA 11.8 y Flash-Attention 2. (72B + vLLM utiliza PyTorch 2.1.0 y Cuda 11.8.) La velocidad de inferencia se promedia sobre los tokens codificados y generados.\n\nNota: La velocidad de generación de los modelos Int4/Int8 mencionados anteriormente es proporcionada por la librería autogptq. La velocidad actual del modelo cargado utilizando ``AutoModelForCausalLM.from_pretrained`` será aproximadamente un 20% más lenta. Hemos informado de este problema al equipo de HuggingFace y lo actualizaremos rápidamente si se encuentra una solución.\n\nTambién medimos la velocidad de inferencia y el uso de memoria de la GPU con diferentes configuraciones de contexto y longitudes de generación, versión Flash-Attention. Puedes encontrar los resultados en las modelcards correspondientes en Hugging Face o ModelScope.\n\n\n\n## Finetuning\n\n### Utilización\nAhora proporcionamos el script de entrenamiento oficial, `finetune.py`, para que los usuarios puedan ajustar el modelo preentrenado para aplicaciones posteriores de forma sencilla. Además, proporcionamos scripts de shell para lanzar el ajuste fino sin preocupaciones. Este script soporta el entrenamiento con [DeepSpeed](https://github.com/microsoft/DeepSpeed) y [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). Los shell scripts que proporcionamos utilizan DeepSpeed (Nota: esto puede tener conflictos con la última versión de pydantic y debe utilizar make sure `pydantic<2.0`) y Peft. Puede instalarlos de la siguiente manera:\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\nPara preparar tus datos de entrenamiento, necesitas poner todas las muestras en una lista y guardarla en un archivo json. Cada muestra es un diccionario que consiste en un id y una lista para la conversación. A continuación se muestra una lista de ejemplo simple con 1 muestra:\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是一个语言模型，我叫通义千问。\"\n      }\n    ]\n  }\n]\n```\n\nUna vez preparados los datos, puede utilizar los scripts de shell suministrados para ejecutar el ajuste fino. Recuerde especificar la ruta al archivo de datos, `$DATA`.\n\nLos guiones de finetuning permiten realizar:\n- Finetuning de todos los parámetros\n- LoRA\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```bash\n# Entrenamiento distribuido. No proporcionamos un script de entrenamiento para una sola GPU, ya que la insuficiente memoria de la GPU interrumpiría el entrenamiento.\nbash finetune/finetune_ds.sh\n```\n\nRecuerde especificar el nombre correcto del modelo o ruta, la ruta de datos, así como el directorio de salida en los scripts de shell. Otra cosa a notar es que usamos DeepSpeed ZeRO 3 en este script. Si desea realizar cambios, basta con eliminar el argumento `--deepspeed` o realizar cambios en el archivo json de configuración de DeepSpeed en función de sus necesidades. Además, este script soporta entrenamiento de precisión mixta, por lo que puedes usar `--bf16 True` o `--fp16 True`. Recuerde utilizar DeepSpeed cuando utilice fp16 debido al entrenamiento de precisión mixta. \nEmpíricamente le aconsejamos que utilice bf16 para que su entrenamiento sea coherente con nuestro preentrenamiento y alineación si su máquina soporta bf16, y por lo tanto lo utilizamos por defecto.\n\nPara ejecutar LoRA, utilice otro script para ejecutar como se muestra a continuación. Antes de empezar, asegúrese de que ha instalado `peft`. Además, es necesario especificar las rutas a su modelo, los datos y la salida. Le aconsejamos que utilice la ruta absoluta para su modelo pre-entrenado. Esto se debe a que LoRA sólo guarda el adaptador y la ruta absoluta en el archivo json de configuración del adaptador se utiliza para encontrar el modelo preentrenado para cargar. Además, este script soporta tanto bf16 como fp16.\n\n```bash\n# Single GPU training\nbash finetune/finetune_lora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_lora_ds.sh\n```\n\nEn comparación con el ajuste fino de parámetros completos, LoRA ([artículo](https://arxiv.org/abs/2106.09685)) sólo actualiza los parámetros de las capas adaptadoras, pero mantiene congeladas las grandes capas originales del modelo de lenguaje. Esto permite muchos menos costes de memoria y, por tanto, de computación.\n\nTenga en cuenta que si utiliza LoRA para ajustar el modelo de lenguaje base, por ejemplo, Qwen-7B, en lugar de los modelos de chat, por ejemplo, Qwen-7B-Chat, el script cambia automáticamente la incrustación y la capa de salida como parámetros entrenables. Esto se debe a que el modelo de lenguaje base no tiene conocimiento de los tokens especiales que aporta el formato ChatML. Por lo tanto, estas capas deben actualizarse para que el modelo comprenda y prediga los tokens. O en otras palabras, si tu entrenamiento trae tokens especiales en LoRA, deberías poner las capas como parámetros entrenables poniendo `modules_to_save` dentro del código. Además, si tenemos estos parámetros entrenables, no está disponible para usar ZeRO 3, y es por esto que usamos ZeRO 2 en el script por defecto. Si no tenemos nuevos parámetros entrenables, podemos cambiar a ZeRO 3 cambiando el fichero de configuración de DeepSpeed. Además, encontramos que hay una brecha significativa entre la huella de memoria de LoRA con y sin estos parámetros entrenables. Por lo tanto, si usted tiene problemas con la memoria, le aconsejamos LoRA finetune los modelos de chat. Compruebe el perfil de abajo para obtener más información.\n\nSi sigue sufriendo de memoria insuficiente, puede considerar Q-LoRA ([artículo](https://arxiv.org/abs/2305.14314)), que utiliza el modelo de lenguaje cuantizado de gran tamaño y otras técnicas como la atención paginada para permitir incluso menos costes de memoria.\n\nNota: para ejecutar el entrenamiento Q-LoRA con una sola GPU, puede que necesites instalar `mpi4py` a través de `pip` o `conda`.\n\nPara ejecutar Q-LoRA, ejecute directamente el siguiente script:\n\n```bash\n# Entrenamiento con una sola GPU\nbash finetune/finetune_qlora_single_gpu.sh\n# Entrenamiento distribuida\nbash finetune/finetune_qlora_ds.sh\n```\n\nPara Q-LoRA, le aconsejamos que cargue nuestro modelo cuantizado proporcionado, por ejemplo, Qwen-7B-Chat-Int4. **NO DEBE** utilizar los modelos bf16. A diferencia del finetuning de parámetros completos y LoRA, sólo fp16 es compatible con Q-LoRA. Para el entrenamiento con una sola GPU, tenemos que utilizar DeepSpeed para el entrenamiento de precisión mixta debido a nuestra observación de errores causados por el amplificador de antorcha. Además, para Q-LoRA, los problemas con los tokens especiales en LoRA siguen existiendo. Sin embargo, como sólo proporcionamos los modelos Int4 para los modelos de chat, lo que significa que el modelo lingüístico ha aprendido los tokens especiales del formato ChatML, no hay que preocuparse por las capas. Ten en cuenta que las capas del modelo Int4 no deben ser entrenables, por lo que si introduces tokens especiales en tu entrenamiento, Q-LoRA podría no funcionar.\n\n> NOTA: Tenga en cuenta que debido a los mecanismos internos de Hugging Face, ciertos archivos que no son de Python (por ejemplo, `*.cpp` y `*.cu`) pueden faltar en el punto de control guardado. \n> pueden faltar en el punto de control guardado. Es posible que tenga que copiarlos manualmente en el directorio que contiene otros archivos.\n\nA diferencia del finetuning de parámetros completo, el entrenamiento de LoRA y Q-LoRA sólo guarda los parámetros del adaptador. Supongamos que su entrenamiento comienza desde Qwen-7B, puede cargar el modelo ajustado para la inferencia como se muestra a continuación:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nSi quieres fusionar los adaptadores y guardar el modelo ajustado como un modelo independiente (sólo puedes hacer esto con LoRA, y NO puedes fusionar los parámetros desde Q-LoRA), puedes ejecutar los siguientes códigos:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\nNota: Para el entrenamiento multi-GPU, es necesario especificar los hiperparámetros adecuados para el entrenamiento distribuido basado en su máquina. Además, le aconsejamos que especifique la longitud máxima de la secuencia con el argumento `--model_max_length`, en función de los datos, el espacio de memoria y la velocidad de entrenamiento.\n\n\n### Perfiles de Memoria y Velocidad\nPerfilamos la memoria de la GPU y la velocidad de entrenamiento tanto de LoRA (LoRA (emb) se refiere al entrenamiento de la capa de incrustación y salida, mientras que LoRA no tiene capa de incrustación y salida entrenables) como de Q-LoRA en la configuración de entrenamiento en una sola GPU. En esta prueba, experimentamos con una única GPU A100-SXM4-80G, y utilizamos CUDA 11.8 y Pytorch 2.0. Se aplica Flash attention 2. Utilizamos uniformemente un tamaño de lote de 1 y una acumulación de gradiente de 8. Perfilamos la memoria (GB) y la velocidad (s/iter) de entradas de distintas longitudes, a saber, 256, 512, 1024, 2048, 4096 y 8192. También presentamos las estadísticas del ajuste fino de todos los parámetros con Qwen-7B en 2 GPU A100. Sólo se presentan las estadísticas de 256, 512 y 1024 tokens debido a la limitación de memoria de la GPU. \n\nPara Qwen-72B, experimentamos de dos formas: 1) Ajuste fino de Lora + DeepSpeed ZeRO 3 en 4 GPUs A100-SXM4-80G y 2) Ajuste fino de QLora (int4) en una sola GPU A100-SXM4-80G. Ten en cuenta que la OOM se produce en 4 GPUs A100-SXM4-80G tanto con ajuste fino LoRA (emb) como con ajuste fino LoRA sin Deepspeed ZeRO 3 (puedes pasar `--deepspeed finetune/ds_config_zero3.json` a [`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh) para activar DeepSpeed ZeRO 3).\n\nLas estadísticas se enumeran a continuación:\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    </tr>\n    </tr>\n\t\t<tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td><td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">7B</th><td>LoRA</td><td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th><td>LoRA</td><td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n\t<tr>\n        <th rowspan=\"2\">72B</th><td>LoRA + Deepspeed Zero3</td><td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n<br>\n\n## Despliegue\n\n### vLLM \nPara el despliegue y la inferencia rápida, sugerimos utilizar vLLM con FastChat. Instale primero los paquetes:\n```bash\npip install vllm fastchat\n```\nO puede instalarlos desde el código fuente mediante `git clone` y `pip install -e .`. Le aconsejamos que lea sus documentos si encuentra problemas en la instalación.\n\nPara ejecutar Qwen con vLLM y FastChat, primero necesitas lanzar un controlador por:\n```bash\npython -m fastchat.serve.controller\n```\n\nA continuación, puede iniciar el model worker, lo que significa cargar su modelo para la inferencia. Para la inferencia de una sola GPU, puede ejecutar directamente:\n\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code\n```\nSin embargo, si desea ejecutar el modelo en varias GPU para acelerar la inferencia o disponer de más memoria, puede utilizar el paralelismo tensorial soportado por vLLM. Supongamos que ejecutas el modelo en 4 GPUs, el comando se muestra a continuación:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4\n```\n\nDespués de lanzar tu model worker, puedes lanzar:\n\n* Web UI Demo\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* API OpenAI\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\n### Interfaz Web\n\nProporcionamos código para que los usuarios construyan una web UI demo (gracias a @wysaid). Antes de empezar, asegúrate de instalar los siguientes paquetes:\n```\npip install -r requirements_web_demo.txt\n```\n\nA continuación, ejecute el siguiente comando y haga clic en el enlace generado:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\nSin embargo, si le resulta difícil utilizar vLLM y FastChat, puede probar los métodos más sencillos que le proporcionamos para desplegar una demo web, una demo CLI y una API.\n\n### Demo CLI\n\nProporcionamos un ejemplo de demostración CLI en `cli_demo.py`, que soporta la salida de streaming para la generación. Los usuarios pueden interactuar con Qwen-7B-Chat introduciendo mensajes, y el modelo devuelve los resultados del modelo en modo streaming. Ejecute el siguiente comando:\n\n```bash\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\nProporcionamos métodos para desplegar la API local basada en la API de OpenAI (gracias a @hanpenggit). Antes de empezar, instala los paquetes necesarios:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nA continuación, ejecute el comando para desplegar su API:\n\n```bash\npython openai_api.py\n```\n\nPuede cambiar sus argumentos, por ejemplo, `-c` para el nombre o la ruta del punto de control, `--cpu-only` para el despliegue en CPU, etc. Si tienes problemas al iniciar el despliegue de tu API, probablemente puedas solucionarlos actualizando los paquetes a la última versión.\n\nUtilizar la API también es sencillo. Vea el siguiente ejemplo:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# create a request activating streaming response\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=True \n    # Specifying stop words in streaming output format is not yet supported and is under development.\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# create a request not activating streaming response\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=False,\n    stop=[] # You can add custom stop words here, e.g., stop=[\"Observation:\"] for ReAct prompting.\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n**Function calling** también está soportada (pero sólo cuando `stream=False` por el momento). Ver el [ejemplo de uso](examples/function_call_examples.py) aquí.\n<br><br>\n\n## 🐳 Docker\n\nPara simplificar el proceso de despliegue, proporcionamos imágenes Docker con entornos preconstruidos: [qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen). Solo tienes que instalar el controlador y descargar los archivos del modelo para lanzar demos, desplegar la API de OpenAI y ajustar el modelo.\n\n### Preparación\n\n1. Instale la versión correcta del controlador Nvidia en función de la imagen que vaya a utilizar:\n  - `qwenllm/qwen:cu117` (**recomendado**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: igual que `qwenllm/qwen:cu117`\n\n2. Instale y configure [docker](https://docs.docker.com/engine/install/) y [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html):\n\n```bash\n# configure docker\nsudo systemctl start docker\n# test if docker is correctly installed\nsudo docker run hello-world\n\n# configure nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. Descargue los checkpoints y los códigos del modelo a su entorno (véase [aquí](#DownloadModel)).\n\n### Despliegue\n\nAquí usamos Qwen-7B-Chat como ejemplo. Antes de lanzar una demo web o API, puede establecer la configuración como se muestra a continuación:\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes\n```\nLos siguientes scripts pueden ayudarte a construir:\n\n* API OpenAI\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Interfaz Web\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Demo CLI\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\nLos comandos anteriores descargarán automáticamente la imagen requerida y lanzarán una demo Web UI en segundo plano (el servicio se reiniciará automáticamente). Puede abrir `http://localhost:${PORT}` en el host para utilizar la demo.\n\nLa demostración se ha iniciado correctamente si ve la siguiente salida:\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nSi quieres comprobar el estado de la demo, puedes usar `docker logs qwen` para mostrar los resultados.\n\nPuede utilizar `docker rm -f qwen` para detener el servicio y eliminar el contenedor.\n\n\n### Finetuning\n\nEl método de finetuning utilizando la imagen Docker pre-construida es básicamente el mismo que [el capítulo anterior](#Finetuning) (ya hemos instalado dependencias en la imagen):\n\nA continuación se muestra un ejemplo de LoRA de GPU única:\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs\n\n# Use all host devices by default\nDEVICE=all\n# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# Single-GPU LoRA finetuning\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\nPara realizar un cambio a Q-LoRA de una sola GPU, por ejemplo, basta con modificar el comando bash dentro de `docker run`:\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## 🔥 Indicaciones del sistema\nQwen-1.8-Chat y Qwen-72B-Chat han sido completamente entrenados en diversas indicaciones del sistema con múltiples rondas de interacciones complejas, para que puedan seguir una variedad de indicaciones del sistema y realizar la personalización del modelo en contexto, mejorando aún más la escalabilidad de Qwen-chat.\n\nGracias a las instrucciones del sistema, Qwen-Chat puede realizar **juegos de rol**, **transferencia de estilos de lenguaje**, **configuración de tareas** y **configuración de comportamientos**.\n\n![](assets/system_prompt_language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\nPara más información, consulta la [documentación de ejemplo](examples/system_prompt.md).\n\n\n## Uso de Herramientas\n\nQwen-Chat ha sido optimizado para el uso de herramientas y capacidades de llamada a funciones. Los usuarios pueden desarrollar agentes, aplicaciones LangChain e incluso aumentar Qwen con un intérprete de código Python.\n\nProporcionamos documentación sobre cómo implementar llamadas a herramientas basadas en el principio de ReAct Prompting, por favor consulte [the ReAct example](examples/react_prompt.md). Basándonos en este principio, proporcionamos soporte para llamadas a funciones en [openai_api.py](openai_api.py).\n\nHemos probado las capacidades de llamada de la herramienta del modelo en nuestro punto de referencia de evaluación chino de código abierto y hemos descubierto que Qwen-Chat obtiene siempre buenos resultados:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.↑)</th><th align=\"center\">Tool Input (Rouge-L↑)</th><th align=\"center\">False Positive Error↓</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\nPara evaluar la capacidad de Qwen para utilizar el intérprete de código Python en tareas como la resolución de problemas matemáticos, la visualización de datos y otras tareas de propósito general como el manejo de archivos y el web scraping, hemos creado y puesto a disposición del público un benchmark específicamente diseñado para evaluar estas capacidades. Puede encontrar el punto de referencia en este [enlace](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nHemos observado que Qwen funciona bien en términos de ejecutabilidad del código y precisión de los resultados al generar código:\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Math↑</th><th align=\"center\">Visualization-Hard↑</th><th align=\"center\">Visualization-Easy↑</th><th align=\"center\">General↑</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## Comprensión del Contexto Largo\n\nPara ampliar la longitud del contexto y romper el cuello de botella de la longitud de la secuencia de entrenamiento, introducimos varias técnicas, como la interpolación NTK, la atención de ventana y el escalado de atención LogN, para ampliar la longitud del contexto de Qwen-14B de 2K a más de 8K tokens, y Qwen-1.8B/7B de 8K a 32K tokens. \n\nPara Qwen-72B, adaptamos RoPE a contextos más largos con una base rotatoria mayor. Qwen-72B admite una longitud máxima de contexto de 32K tokens.\n\nRealizamos experimentos de modelado lingüístico en el conjunto de datos arXiv con la evaluación PPL y descubrimos que Qwen puede alcanzar un rendimiento sobresaliente en el escenario de contextos largos. Los resultados se muestran a continuación:\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.56</td><td align=\"center\">4.62</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.49</td><td align=\"center\">4.32</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n    <tr>\n        <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n    </tr>\n</table>\n\nFurthermore, to verify the ability of Qwen-72B-Chat on long text understanding, we tested it on [L-Eval](https://arxiv.org/abs/2307.11088) (closed-ended tasks). The results are as follows:\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\nHemos realizado el experimento de la \"aguja en el pajar\" (la idea procede de [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)) para comprobar si el modelo puede recuperar información en distintas posiciones de las entradas de distintas longitudes, el resultado es el siguiente:\n\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\nLos resultados anteriores muestran que Qwen-72B-Chat puede recuperar con precisión información situada en varias posiciones dentro de una longitud de entrada de 32K, lo que demuestra su excelente capacidad de comprensión de textos largos.\n\n\n## Tokenizador\n\nNuestro tokenizador basado en tiktoken es diferente de otros tokenizadores, por ejemplo, el tokenizador sentencepiece. Es necesario prestar atención a los tokens especiales, especialmente en el finetuning. Para obtener información más detallada sobre el tokenizador y su uso en el ajuste fino, consulte la [documentación](tokenization_note.md).\n<br><br>\n\n## Reproducción\n\nPara que pueda reproducir el rendimiento del modelo en conjuntos de datos de referencia, le proporcionamos secuencias de comandos para que reproduzca los resultados. Consulte [eval/EVALUATION.md](eval/EVALUATION.md) para obtener más información. Tenga en cuenta que la reproducción puede dar lugar a ligeras diferencias con respecto a nuestros resultados.\n<br><br>\n\n## FAQ\n\nSi tiene problemas, consulte primero [FAQ](FAQ.md) y las incidencias para buscar una solución antes de lanzar una nueva incidencia.\n<br><br>\n\n## Cita\nSi nuestro trabajo le resulta útil, no dude en citarnos.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## Acuerdo de Licencia\n\nEl código fuente proporcionado en <https://github.com/QwenLM/Qwen> está licenciado bajo la [Licencia Apache 2.0](./LICENSE) que puede encontrarse en el directorio raíz.\n\nLos investigadores y desarrolladores son libres de utilizar los códigos y los pesos de los modelos tanto de Qwen como de Qwen-Chat. Para su uso comercial, consulte el Acuerdo de Licencia que acompaña a cada modelo.\n\n- Qwen-72B, Qwen-14B, y Qwen-7B están licenciados bajo el [Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT) que se puede encontrar en el repositorio correspondiente de HuggingFace y ModelScope. Para uso comercial, rellene el formulario ([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), y [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen)) para solicitarlo.\n\n- Qwen-1.8B está licenciado bajo el [Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT) que puede encontrarse en el repositorio correspondiente de HuggingFace y ModelScope. Para uso comercial, póngase en contacto con nosotros.\n<br><br>\n\n## Contacte con Nosotros\n\nSi estás interesado en dejar un mensaje a nuestro equipo de investigación o de producto, únete a nuestros grupos de Discord o WeChat. También puedes enviar un correo electrónico a qianwen_opensource@alibabacloud.com.\n\n"
        },
        {
          "name": "README_FR.md",
          "type": "blob",
          "size": 73.9296875,
          "content": "<p align=\"left\">\n    <a href=\"README_CN.md\">中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>&nbsp ｜ &nbsp<a href=\"README_JA.md\">日本語</a>&nbsp ｜ &nbspFrançais ｜ &nbsp<a href=\"README_ES.md\">Español</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        🤗 <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ｜ &nbsp&nbsp🖥️ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ｜  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br><br>\n\n> [!Important]\n> Qwen2 est là ! Vous êtes invité à suivre [QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) et à partager vos expériences là-bas.\n>\n> Ce repo ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) n'est plus activement maintenu, en raison de différences substantielles dans le code source.\n<br>\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">🤗</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">🤗</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">🤗</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">🤗</a> |\n\n\n\nNous ouvrons notre série **Qwen**, qui comprend désormais **Qwen**, les modèles de langue de base, à savoir **Qwen-7B** et **Qwen-14B**, ainsi que **Qwen-Chat**, les modèles de chat, à savoir **Qwen-7B-Chat** et **Qwen-14B-Chat**. Les liens se trouvent dans le tableau ci-dessus. Cliquez dessus et consultez les fiches des modèles. Nous publions également le **[rapport technique](https://arxiv.org/abs/2309.16609)**. Cliquez sur le lien du document et consultez-le !\n\nEn bref, nous disposons de modèles linguistiques solides, qui ont été pré-entraîné de manière stable pour 3 000 milliards de tokens de données multilingues avec une large couverture de domaines, de langues (en particulier le chinois et l'anglais), etc. Ils sont capables d'atteindre des performances compétitives sur des ensembles de données de référence. En outre, nous disposons de modèles de chat alignés sur les préférences humaines basées sur SFT et RLHF (pas encore publiés), qui sont capables de chatter, de créer du contenu, d'extraire des informations, de résumer, de traduire, de coder, de résoudre des problèmes mathématiques, etc. et d'utiliser des outils, de jouer le rôle d'agents ou même code interpreter, etc.\n\n| Modèle    | Date de sortie | Longueur maximale | Amélioration de l'invite du système | # de tokens pré-formés | Utilisation minimale de la mémoire du GPU pour Finetuning (Q-Lora) | Utilisation minimale du GPU pour générer 2048 jetons (Int4) | Utilisation des outils |\n|:----------|:--------------:|:-----------------:|:-----------------------------------:|:----------------------:|:------------------------------------------------------------------:|:-----------------------------------------------------------:|:----------------------:|\n| Qwen-1.8B |    23.11.30    |        32K        |                  ✅                  |          2.2T          |                               5.8GB                                |                            2.9GB                            |           ✅            |  \n| Qwen-7B   |    23.08.03    |        32K        |                  ❎                  |          2.4T          |                               11.5GB                               |                            8.2GB                            |           ✅            |   \n| Qwen-14B  |    23.09.25    |        8K         |                  ❎                  |          3.0T          |                               18.7GB                               |                           13.0GB                            |           ✅            |\n| Qwen-72B  |    23.11.30    |        32K        |                  ✅                  |          3.0T          |                               61.4GB                               |                           48.9GB                            |           ✅            |   \n\n\nDans la repo, vous pouvez trouver:\n\n* Comment utiliser Qwen, et profiter de l'inférence simple.\n* Détails sur les modèles de quantization, y compris GPTQ et la quantization de KV cache.\n* Statistiques sur les performances de l'inférence, y compris la vitesse et la mémoire.\n* Tutoriels sur le finetuning, y compris le finetuning de paramètres complets, LoRA, et Q-LoRA.\n* Instructions de déploiement, avec l'exemple de vLLM et FastChat.\n* Instructions sur la création de démos, y compris WebUI, démo CLI, etc.\n* Introduction au service API de DashScope, ainsi que les instructions pour construire une API de type OpenAI pour votre modèle.\n* Informations sur Qwen pour l'utilisation d'outils, d'agents et code interpreter.\n* Statistiques de l'évaluation de la compréhension du contexte long.\n* Contrat de licence.\n* ...\n\nEn outre, si vous rencontrez des problèmes, consultez d'abord la [FAQ](FAQ.md) pour obtenir de l'aide. Vous vous sentez toujours en difficulté ? N'hésitez pas à nous envoyer des questions (de préférence en anglais pour que plus de gens puissent vous comprendre) ! Si vous souhaitez nous aider, envoyez-nous des demandes d'extension sans hésitation ! Nous sommes toujours enthousiastes à propos des relations publiques ! \n\nVous voulez discuter avec nous ou prendre un café avec nous ? Bienvenue sur notre Discord ou WeChat !\n<br><br>\n\n## Nouvelles et mises à jour\n\n* 2023.11.30 🔥 Nous publions **Qwen-72B** et **Qwen-72B-Chat**, qui sont entraînés sur des tokens 3T et prennent en charge 32k contextes, ainsi que **Qwen-1.8B** et **Qwen-1.8B-Chat**, sur ModelScope et Hugging Face. Nous avons également renforcé les capacités de l'invite système du Qwen-72B-Chat et du Qwen-1.8B-Chat, voir la [documentation d'exemple](examples/system_prompt.md). De plus, nous supportons l'inférence sur **Ascend 910** et **Hygon DCU**. Consultez `ascend-support` et `dcu-support` pour plus de détails.\n* 2023.10.17 Nous publions le modèle quantifié Int8 **Qwen-7B-Chat-Int8** et **Qwen-14B-Chat-Int8**.\n* 2023.9.25 🔥 Nous publions **Qwen-14B** et **Qwen-14B-Chat** sur ModelScope et Hugging Face, ainsi que [qwen.cpp](https://github.com/QwenLM/qwen.cpp) et [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). Les codes et les poids de **Qwen-7B** et **Qwen-7B-Chat** ont également été mis à jour. **S'IL VOUS PLAÎT, TIREZ LA DERNIÈRE VERSION!**\n    - Par rapport à **Qwen-7B** (original), **Qwen-7B** utilise davantage de jetons d'entraînement, passant de 2,2 à 2,4T de jetons, tandis que la longueur du contexte passe de 2048 à 8192. La connaissance du chinois et la capacité de codage de **Qwen-7B** ont été encore améliorées.\n* 2023.9.12 Nous prenons désormais en charge le finetuning sur les modèles Qwen-7B, y compris le finetuning de tous les paramètres, LoRA et Q-LoRA.\n* 2023.8.21 Nous publions le modèle quantifié Int4 pour Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, qui nécessite de faibles coûts de mémoire mais permet d'améliorer la vitesse d'inférence. En outre, il n'y a pas de dégradation significative des performances lors de l'évaluation de référence.\n* 2023.8.3 Nous publions **Qwen-7B** et **Qwen-7B-Chat** sur ModelScope et Hugging Face. Nous fournissons également un mémo technique pour plus de détails sur le modèle, y compris les détails de l'entraînement et les performances du modèle.\n<br>\n\n## Performance\n\nLes modèles Qwen surpassent les modèles de base de taille similaire sur une série de données de référence, par exemple MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., qui évaluent les capacités des modèles sur la compréhension du langage naturel, la résolution de problèmes mathématiques, le codage, etc. Qwen-72B obtient de meilleures performances que LLaMA2-70B dans toutes les tâches et surpasse GPT-3.5 dans 7 tâches sur 10.\n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=600px/>\n<p>\n<br>\n\n| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |\n| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\nPour tous les modèles comparés, nous indiquons les meilleurs scores entre leurs résultats officiels et [OpenCompass] (https://opencompass.org.cn/leaderboard-llm). \n\nPour plus de résultats expérimentaux (performances détaillées des modèles sur d'autres ensembles de données de référence) et de détails, veuillez vous référer à notre rapport technique en cliquant [ici](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).\n<br><br>\n\n## Besoins\n\n* python 3.8 et plus\n* pytorch 1.12 et plus, 2.0 et plus sont recommandés\n* transformers 4.32 et plus\n* CUDA 11.4 et plus sont recommandés (pour les utilisateurs de GPU, les utilisateurs de flash, etc.)\n<br>\n\n## Démarrage Rapide\n\nCi-dessous, nous fournissons des exemples simples pour montrer comment utiliser Qwen-Chat avec 🤖 ModelScope et 🤗 Transformers.\n\nVous pouvez utiliser nos images docker pré-construites pour sauter la plupart des étapes de configuration de l'environnement, voir la section [\"Utiliser des images docker pré-construites\"](#-docker) pour plus de détails. \n\nSi vous n'utilisez pas Docker, assurez-vous d'avoir configuré l'environnement et installé les paquets requis. Assurez-vous de répondre aux exigences ci-dessus, puis installez les bibliothèques dépendantes.\n\n```bash\npip install -r requirements.txt\n```\n\nSi votre appareil supporte fp16 ou bf16, nous vous recommandons d'installer [flash-attention](https://github.com/Dao-AILab/flash-attention) (**nous supportons flash-attention 2 maintenant.**) pour une meilleure efficacité et une moindre utilisation de la mémoire. (**flash-attention est optionnel et le projet peut fonctionner normalement sans l'installer**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# Below are optional. Installing them might be slow.\n# pip install csrc/layer_norm\n# pip install csrc/rotary\n```\n\nVous pouvez maintenant commencer avec ModelScope ou Transformers.\n\n### 🤗 Transformers\n\nPour utiliser Qwen-Chat pour l'inférence, il vous suffit de saisir quelques lignes de code, comme indiqué ci-dessous. N'oubliez pas de transmettre les noms de modèles ou les chemins corrects, tels que \"Qwen/Qwen-7B-Chat\" et \"Qwen/Qwen-14B-Chat\". Cependant, **veuillez vous assurer que vous utilisez le code le plus récent**.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\n# 你好！很高兴为你提供帮助。\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\nprint(response)\n# 这是一个关于一个年轻人奋斗创业最终取得成功的故事。\n# 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。\n# 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。\n# 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。\n# 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。\n# 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。\n\n# 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"给这个故事起一个标题\", history=history)\nprint(response)\n# 《奋斗创业：一个年轻人的成功之路》\n```\n\nL'exécution du modèle pré-entraîné de Qwen est également simple.\n\n<details>\n  <summary>Running Qwen</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\" \ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# 蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）...\n```\n\n</details>\n\nEn cas de problème de réseau lors de la tentative de téléchargement des poids et des codes du modèle à partir de HuggingFace, une autre approche consiste à récupérer le point de contrôle à partir de ModelScope, puis à le charger à partir du répertoire local, comme indiqué ci-dessous:\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n# model_dir = snapshot_download('qwen/Qwen-14B')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### 🤖 ModelScope\n\nModelScope est une plateforme opensource pour Model-as-a-Service (MaaS), qui fournit un service de modèle flexible et rentable aux développeurs d'IA. De même, vous pouvez exécuter les modèles avec ModelScope comme indiqué ci-dessous:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# Model names: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"浙江的省会在哪里？\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"它有什么好玩的景点\", history=history)\nprint(response)\n```\n\n### Inférence par lots\nQwen prend en charge l'inférence par lots. Lorsque flash attention est activée, l'utilisation de l'inférence par lots peut entraîner une accélération de 40 %. Le code d'exemple est présenté ci-dessous:\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"我想听你说爱我。\", \"今天我想吃点啥，甜甜的，推荐下\", \"我马上迟到了，怎么做才能不迟到\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"我想听你说爱我。\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"今天我想吃点啥，甜甜的，推荐下\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"我马上迟到了，怎么做才能不迟到\", history=None)\nprint(response)\n```\n\n### CPU\n\nPour déployer nos modèles sur CPU, nous vous conseillons vivement d'utiliser [qwen.cpp](https://github.com/QwenLM/qwen.cpp), qui est une implémentation purement C++ de Qwen et de tiktoken. Consultez le repo pour plus de détails!\n\nIl est simple d'exécuter directement le modèle sur le CPU, ce qui nécessite la spécification de votre appareil:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nCependant, il est probable que vous souffriez d'une efficacité d'inférence extrêmement faible.\n\n### Plusieurs GPU\n\nSi vous souffrez d'un manque de mémoire GPU et que vous souhaitez exécuter le modèle sur plus d'un GPU, vous pouvez utiliser directement la méthode de chargement par défaut, qui est maintenant supportée par Transformers. La méthode précédente basée sur `utils.py` est obsolète.\n\nCependant, bien que cette méthode soit simple, l'efficacité du parallélisme natif du pipeline est faible. Nous vous conseillons d'utiliser vLLM avec FastChat et de lire la section relative au déploiement.\n\n\n### DashScope\n\nLe moyen le plus simple d'utiliser Qwen via les API est le service API DashScope via Alibaba Cloud. Nous présentons une introduction à l'utilisation. De plus, nous fournissons un script pour vous permettre de déployer une API de type OpenAI sur vos propres serveurs.\n\nDashScope est le service API de grands modèles linguistiques fourni par Alibaba Cloud, qui prend désormais en charge Qwen. Notez que les modèles derrière DashScope sont des versions internes temporairement sans détails fournis. Les services comprennent `qwen-turbo` et `qwen-plus`, le premier fonctionnant plus rapidement et le second atteignant de meilleures performances. Pour plus d'informations, consultez la documentation [ici] (https://dashscope.aliyun.com).\n\nVeuillez vous rendre sur le site officiel [lien](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) pour créer un compte DashScope et obtenir la clé API (AK). Nous recommandons de définir l'AK à l'aide d'une variable d'environnement:\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nInstallez ensuite les paquets et cliquez sur [ici](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) pour obtenir la documentation. Si vous utilisez Python, vous pouvez installer DashScope avec pip:\n```bash\npip install dashscope\n```\nSi vous utilisez JAVA SDK, vous pouvez l'installer de cette manière:\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nLa manière la plus simple d'utiliser DashScope est l'utilisation de messages, qui est similaire à l'API OpenAI. L'exemple est présenté ci-dessous:\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': '如何做西红柿鸡蛋？'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\nPour d'autres utilisations, veuillez consulter le site web officiel pour plus de détails.\n<br><br>\n\n## Quantization\n\n### GPTQ\n\nNous proposons une solution basée sur [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), et publions les modèles quantifiés Int4 et Int8, qui permettent d'obtenir des effets de modèle presque sans perte mais des performances améliorées en termes de coûts de mémoire et de vitesse d'inférence.\n\nNous démontrons ici comment utiliser les modèles quantifiés que nous fournissons pour l'inférence. Avant de commencer, assurez-vous que vous répondez aux exigences d'auto-gptq (par exemple, torch 2.0 et plus, transformers 4.32.0 et plus, etc.) et installez les paquets requis:\n\n```bash\npip install auto-gptq optimum\n```\n\nSi vous rencontrez des problèmes pour installer `auto-gptq`, nous vous conseillons de consulter le [repo](https://github.com/PanQiWei/AutoGPTQ) officiel pour trouver une roue.\n\n> Note : Les paquets `auto-gptq` précompilés dépendent fortement de la version de `torch` et de sa version CUDA. De plus, en raison d'une récente mise à jour,\n> vous pouvez aussi rencontrer des erreurs de version non supportée avec `transformers`, `optimum`, ou `peft`.\n> Nous recommandons d'utiliser les dernières versions répondant aux exigences suivantes :\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0\n\nVous pouvez ensuite charger facilement le modèle quantifié et lancer l'inférence comme d'habitude:\n\n```python\n# Model names: \"Qwen/Qwen-7B-Chat-Int4\", \"Qwen/Qwen-14B-Chat-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\nNous illustrons les performances des modèles BF16, Int8 et Int4 sur le benchmark, et nous constatons que le modèle quantifié ne souffre pas d'une dégradation significative des performances. Les résultats sont présentés ci-dessous:\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n\n### Quantization du cache KV\n\n> NOTE : Veuillez noter qu'en raison du mécanisme interne de Hugging Face, les fichiers de support pour cette fonctionnalité \n> (i.e., `cache_autogptq_cuda_256.cpp` et `cache_autogptq_cuda_kernel_256.cu`) peuvent être manquants. \n> Veuillez les télécharger manuellement manuellement depuis le Hugging Face Hub et placez-les dans le même dossier que les autres fichiers du module.\n\nLe cache KV de l'attention peut être quantifié et compressé pour le stockage, afin d'obtenir un débit d'échantillonnage plus élevé. Les arguments `use_cache_quantization` et `use_cache_kernel` dans `config.json` sont fournis pour activer la quantification du cache KV. \nLa méthode d'utilisation spécifique est la suivante:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\nAttention : Actuellement, la quantification du cache KV et flash attention ne peuvent pas être utilisées en même temps.\nSi vous activez la quantification du cache KV et flash attention en même temps (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` est désactivé par défaut (`use_flash_attn=false`).\n\nNous avons vérifié que l'utilisation du modèle int8-kvcache quantifié ne souffre pas d'une dégradation significative des performances dans l'évaluation en aval. Dans ce qui suit, nous nous concentrons sur le profilage de son empreinte mémoire dans différentes conditions. \nLe profilage s'exécute sur un seul GPU A100-SXM4-80G avec PyTorch 2.0.1 et CUDA 11.4. \nNous utilisons des modèles BF16 pour générer 1024 jetons par défaut, et \"OOM\" indique une erreur de mémoire insuffisante.\n\nAvec la quantification du cache KV, le modèle peut inférer avec une taille de lot (bs) plus grande.\n\n| Utilisation du cache KV |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|--------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| Non          | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |\n| Oui          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\nAvec la quantification du cache KV, le modèle peut économiser plus de mémoire lorsqu'il génère des séquences plus longues (`sl`, se référant au nombre de jetons générés) à l'étape de l'inférence.\n\n| Utilisation du cache KV | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|-------------------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| Non                     | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| Oui                     | 15.0GB | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\nLe modèle avec quantification du cache KV convertira le format de `layer_past` de float à int8, et pendant ce temps le `layer-past` quantifié stockera également les paramètres de quantification.\n\nLes étapes spécifiques sont les suivantes:\n\n1. Quantifier clé/valeur\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n2. Stocker dans `layer_past`\n\nVoici le format de `layer_past` quantifié:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\n\nLe format original de `layer_past` est illustré ci-dessous:\n```\n    layer_past=(key,value)\n```\n\nSi vous souhaitez utiliser l'attention KV qui est quantifiée, vous pouvez utiliser l'opération de déquantification pour reconvertir la clé/valeur int8 au format float comme suit \nvous pouvez utiliser l'opération de déquantification pour reconvertir la clé/valeur int8 au format float comme suit:\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n\n## Performance de l'inférence\n\nCette section fournit les statistiques de vitesse et de mémoire des modèles dans différentes précisions. Le profilage de la vitesse et de la mémoire est effectué à l'aide de [ce script] (https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).\n\nNous avons mesuré la vitesse moyenne d'inférence (tokens/s) et l'utilisation de la mémoire GPU pour générer 2048 avec les modèles en BF16, Int8 et Int4.\n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nLe profilage s'exécute sur un seul GPU A100-SXM4-80G (sauf si 2xA100 est mentionné) avec PyTorch 2.0.1, CUDA 11.8, et Flash-Attention 2. (72B + vLLM utilise PyTorch 2.1.0 et Cuda 11.8.) La vitesse d'inférence est calculée en moyenne sur les tokens encodés et générés.\n\nNote : La vitesse de génération des modèles Int4/Int8 mentionnés ci-dessus est fournie par la bibliothèque autogptq. La vitesse actuelle du modèle chargé en utilisant ``AutoModelForCausalLM.from_pretrained`` sera environ 20% plus lente. Nous avons signalé ce problème à l'équipe HuggingFace et nous le mettrons à jour rapidement si une solution est disponible.\n\nNous mesurons également la vitesse d'inférence et l'utilisation de la mémoire du GPU avec différents paramètres de contexte et de longueur de génération, version Flash-Attention. Vous pouvez trouver les résultats dans les cartes modèles correspondantes sur Hugging Face ou ModelScope.\n\n\n## Finetuning\n\n### Utilisation\nNous fournissons maintenant le script d'entraînement officiel, `finetune.py`, pour que les utilisateurs puissent ajuster le modèle pré-entraîné pour les applications en aval de manière simple. De plus, nous fournissons des scripts shell pour lancer le finetune sans soucis. Ce script prend en charge l'entraînement avec [DeepSpeed](https://github.com/microsoft/DeepSpeed) et [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). Les scripts que nous fournissons utilisent DeepSpeed (Note : il peut y avoir des conflits avec la dernière version de pydantic et vous devriez utiliser make sure `pydantic<2.0`) et Peft. Vous pouvez les installer en procédant comme suit :\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\nPour préparer vos données d'entraînement, vous devez rassembler tous les échantillons dans une liste et l'enregistrer dans un fichier json. Chaque échantillon est un dictionnaire composé d'un identifiant et d'une liste de conversation. Voici un exemple simple de liste avec 1 échantillon :\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是一个语言模型，我叫通义千问。\"\n      }\n    ]\n  }\n]\n```\n\nAprès la préparation des données, vous pouvez utiliser les scripts shell fournis pour lancer le finetuning. N'oubliez pas de spécifier le chemin d'accès au fichier de données, `$DATA`.\n\nLes scripts de finetuning vous permettent d'effectuer les opérations suivantes\n- Finetuning de tous les paramètres\n- LoRA\n- Q-LoRA\n\nLe finetuning de tous les paramètres nécessite la mise à jour de tous les paramètres au cours de l'ensemble du processus de formation. Pour lancer votre formation, exécutez le script suivant:\n\n```bash\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\nbash finetune/finetune_ds.sh\n```\n\nN'oubliez pas de spécifier le nom ou le chemin d'accès au modèle, le chemin d'accès aux données, ainsi que le répertoire de sortie dans les scripts shell. Une autre chose à noter est que nous utilisons DeepSpeed ZeRO 3 dans ce script. Si vous voulez faire des changements, il suffit de supprimer l'argument `--deepspeed` ou de faire des changements dans le fichier json de configuration de DeepSpeed en fonction de vos besoins. De plus, ce script supporte l'entraînement en précision mixte, et donc vous pouvez utiliser `--bf16 True` ou `--fp16 True`. N'oubliez pas d'utiliser DeepSpeed lorsque vous utilisez fp16 en raison de l'entraînement de précision mixte. Empiriquement, nous vous conseillons d'utiliser bf16 pour rendre votre apprentissage cohérent avec notre pré-entraînement et notre alignement si votre machine supporte bf16, et nous l'utilisons donc par défaut.\n\nPour exécuter LoRA, utilisez un autre script à exécuter comme indiqué ci-dessous. Avant de commencer, assurez-vous que vous avez installé `peft`. Vous devez spécifier les chemins d'accès à votre modèle, à vos données et à vos résultats. Nous vous conseillons d'utiliser des chemins absolus pour votre modèle pré-entraîné. En effet, LoRA ne sauvegarde que l'adaptateur et le chemin absolu dans le fichier json de configuration de l'adaptateur est utilisé pour trouver le modèle pré-entraîné à charger. De plus, ce script supporte à la fois bf16 et fp16.\n\n```bash\n# Single GPU training\nbash finetune/finetune_lora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_lora_ds.sh\n```\n\nPar rapport au finetuning de tous les paramètres, LoRA ([paper](https://arxiv.org/abs/2106.09685)) ne met à jour que les paramètres des couches d'adaptateurs, tout en gelant les couches originales du grand modèle de langage. Cela permet de réduire considérablement les coûts de mémoire et donc les coûts de calcul.\n\nNotez que si vous utilisez LoRA pour affiner le modèle linguistique de base, par exemple Qwen-7B, au lieu des modèles de chat, par exemple Qwen-7B-Chat, le script change automatiquement l'intégration et la couche de sortie en tant que paramètres entraînables. En effet, le modèle linguistique de base n'a aucune connaissance des jetons spéciaux apportés par le format ChatML. Ces couches doivent donc être mises à jour pour que le modèle comprenne et prédise les jetons. En d'autres termes, si votre formation apporte des tokens spéciaux dans LoRA, vous devez définir les couches comme des paramètres entraînables en définissant `modules_to_save` à l'intérieur du code. De plus, si ces paramètres sont entraînables, il n'est pas possible d'utiliser ZeRO 3, et c'est pourquoi nous utilisons ZeRO 2 par défaut dans le script. Si vous n'avez pas de nouveaux paramètres entraînables, vous pouvez passer à ZeRO 3 en modifiant le fichier de configuration de DeepSpeed. En outre, nous constatons qu'il existe un écart important entre l'empreinte mémoire de LoRA avec et sans ces paramètres d'entraînement. Par conséquent, si vous avez des problèmes de mémoire, nous vous conseillons d'affiner les modèles de chat de LoRA. Consultez le profil ci-dessous pour plus d'informations.\n\nSi vous souffrez toujours d'un manque de mémoire, vous pouvez envisager Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), qui utilise le modèle de langage quantifié et d'autres techniques telles que l'attention paginée pour réduire encore les coûts de mémoire.\n\nNote : pour exécuter l'entraînement Q-LoRA sur un seul GPU, vous pouvez avoir besoin d'installer `mpi4py` via `pip` ou `conda`.\n\nPour lancer Q-LoRA, exécutez directement le script suivant :\n\n```bash\n# Single GPU training\nbash finetune/finetune_qlora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_qlora_ds.sh\n```\n\nPour Q-LoRA, nous vous conseillons de charger le modèle quantifié que nous fournissons, par exemple Qwen-7B-Chat-Int4. Vous **NE DEVRIEZ PAS** utiliser les modèles bf16. Contrairement au finetuning de tous les paramètres et à la LoRA, seul le modèle fp16 est pris en charge pour la Q-LoRA. Pour l'entraînement sur un seul GPU, nous devons utiliser DeepSpeed pour l'entraînement en précision mixte en raison de notre observation des erreurs causées par torch amp. En outre, pour Q-LoRA, les problèmes avec les jetons spéciaux dans LoRA existent toujours. Cependant, comme nous ne fournissons que les modèles Int4 pour les modèles de chat, ce qui signifie que le modèle de langage a appris les tokens spéciaux du format ChatML, vous n'avez pas à vous soucier des couches. Notez que les couches du modèle Int4 ne doivent pas être entraînables, et donc si vous introduisez des tokens spéciaux dans votre entraînement, Q-LoRA risque de ne pas fonctionner.\n\n> NOTE : Veuillez noter qu'en raison des mécanismes internes de Hugging Face, certains fichiers non-Python (par exemple, `*.cpp` et `*.cu`) \n> peuvent être absents du point de contrôle sauvegardé. Vous devrez peut-être les copier manuellement dans le répertoire contenant les autres fichiers.\n\nContrairement au finetuning des paramètres complets, l'entraînement de LoRA et de Q-LoRA n'enregistre que les paramètres de l'adaptateur. Supposons que votre entraînement commence à partir de Qwen-7B, vous pouvez charger le modèle finalisé pour l'inférence comme indiqué ci-dessous:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nSi vous souhaitez fusionner les adaptateurs et enregistrer le modèle affiné en tant que modèle autonome (vous ne pouvez le faire qu'avec LoRA, et vous **NE POUVEZ PAS** fusionner les paramètres de Q-LoRA), vous pouvez exécuter les codes suivants :\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\nNote : Pour l'entraînement multi-GPU, vous devez spécifier les hyperparamètres appropriés pour l'entraînement distribué en fonction de votre machine. De plus, nous vous conseillons de spécifier votre longueur maximale de séquence avec l'argument `--model_max_length`, en fonction de votre considération des données, de l'empreinte mémoire, et de la vitesse d'apprentissage.\n\n\n### Profilage de la mémoire et de la vitesse\nNous profilons la mémoire du GPU et la vitesse d'apprentissage de LoRA (LoRA (emb) se réfère à l'apprentissage de la couche d'intégration et de sortie, tandis que LoRA n'a pas de couche d'intégration et de sortie pouvant être entraînée) et de Q-LoRA dans la configuration de l'apprentissage sur un seul GPU. Dans ce test, nous expérimentons sur un seul GPU A100-SXM4-80G, et nous utilisons CUDA 11.8 et Pytorch 2.0. Flash attention 2 est appliqué. Nous utilisons uniformément une taille de lot de 1 et une accumulation de gradient de 8. Nous profilons la mémoire (GB) et la vitesse (s/iter) des entrées de différentes longueurs, à savoir 256, 512, 1024, 2048, 4096, et 8192. Nous présentons également les statistiques du réglage fin de tous les paramètres avec Qwen-7B sur 2 GPU A100. Nous ne présentons que les statistiques de 256, 512 et 1024 jetons en raison de la limitation de la mémoire du GPU. \n\nPour Qwen-72B, nous expérimentons de deux manières : 1) Lora fintuning + DeepSpeed ZeRO 3 sur 4 GPU A100-SXM4-80G et 2) QLora (int4) fintuning sur un seul GPU A100-SXM4-80G. Notez que l'OOM se produit sur 4 GPUs A100-SXM4-80G à la fois avec le réglage fin LoRA (emb) et le réglage fin LoRA sans Deepspeed ZeRO 3 (vous pouvez passer `--deepspeed finetune/ds_config_zero3.json` à [`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh) afin d'activer DeepSpeed ZeRO 3).\n\nLes statistiques sont listées ci-dessous :\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    </tr>\n    </tr>\n\t\t<tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td><td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">7B</th><td>LoRA</td><td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th><td>LoRA</td><td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n\t<tr>\n        <th rowspan=\"2\">72B</th><td>LoRA + Deepspeed Zero3</td><td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n<br>\n\n## Déploiement\n\n### vLLM \nPour le déploiement et l'inférence rapide, nous suggérons d'utiliser vLLM avec FastChat. Installez d'abord les paquets:\n```bash\npip install vllm\npip install \"fschat[model_worker,webui]\"\n```\nOu vous pouvez les installer à partir des sources par `git clone` et `pip install -e .`. Nous vous conseillons de lire leurs documents si vous rencontrez des problèmes lors de l'installation.\n\nPour faire fonctionner Qwen avec vLLM et FastChat, vous devez d'abord lancer un contrôleur par:\n```bash\npython -m fastchat.serve.controller\n```\n\nEnsuite, vous pouvez lancer le travailleur de modèle, ce qui signifie charger votre modèle pour l'inférence. Pour l'inférence sur un seul GPU, vous pouvez directement lancer:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code\n```\nCependant, si vous souhaitez exécuter le modèle sur plusieurs GPU pour une inférence plus rapide ou une mémoire plus importante, vous pouvez utiliser le parallélisme tensoriel pris en charge par vLLM. Supposons que vous exécutiez le modèle sur 4 GPU, la commande est présentée ci-dessous:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4\n```\n\nAprès avoir lancé votre model worker, vous pouvez lancer :\n\n* Démonstration de l'interface web\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* API OpenAI\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\nCependant, si vous avez des difficultés à utiliser vLLM et FastChat, vous pouvez essayer nos méthodes les plus simples pour déployer une démo web, une démo CLI et une API.\n\n### Interface Web\n\nNous fournissons du code pour que les utilisateurs puissent construire une démo d'interface web (merci à @wysaid). Avant de commencer, assurez-vous d'installer les paquets suivants:\n\n```\npip install -r requirements_web_demo.txt\n```\n\nExécutez ensuite la commande ci-dessous et cliquez sur le lien généré:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n### Démo CLI\n\nNous fournissons un exemple de démonstration CLI dans `cli_demo.py`, qui prend en charge la sortie en continu pour la génération. Les utilisateurs peuvent interagir avec Qwen-7B-Chat en saisissant des invites, et le modèle renvoie les sorties du modèle en mode streaming. Exécutez la commande ci-dessous:\n\n```bash\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\nNous fournissons des méthodes pour déployer une API locale basée sur l'API OpenAI (merci à @hanpenggit). Avant de commencer, installez les paquets nécessaires:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nExécutez ensuite la commande pour déployer votre API:\n\n```bash\npython openai_api.py\n```\n\nVous pouvez modifier vos arguments, par exemple, `-c` pour le nom ou le chemin du poids, `--cpu-only` pour le déploiement CPU, etc. Si vous rencontrez des problèmes lors du lancement du déploiement de l'API, la mise à jour des paquets vers la dernière version peut probablement les résoudre.\n\nL'utilisation de l'API est simple. Voir l'exemple ci-dessous:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# create a request activating streaming response\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=True \n    # Specifying stop words in streaming output format is not yet supported and is under development.\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# create a request not activating streaming response\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=False,\n    stop=[] # You can add custom stop words here, e.g., stop=[\"Observation:\"] for ReAct prompting.\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n**Function calling** est aussi supporté (mais seulement quand `stream=False` pour le moment). Voir [l'exemple d'utilisation](examples/function_call_examples.py) ici.\n<br><br>\n\n\n## 🐳 Docker\n\nPour simplifier le processus de déploiement, nous fournissons des images docker avec des environnements préconstruits : [qwenllm/qwen] (https://hub.docker.com/r/qwenllm/qwen). Il vous suffit d'installer le pilote et de télécharger les fichiers de modèle pour lancer les démonstrations, déployer l'API OpenAI et affiner le modèle.\n\n### Préparation\n\n1. Installez la version correcte du pilote Nvidia en fonction de l'image à utiliser :\n  - `qwenllm/qwen:cu117` (**recommandé**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: même que `qwenllm/qwen:cu117`\n\n2. Installer et configurer [docker](https://docs.docker.com/engine/install/) et [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) :\n\n```bash\n# configure docker\nsudo systemctl start docker\n# test if docker is correctly installed\nsudo docker run hello-world\n\n# configure nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. Téléchargez les checkpoints et les codes du modèle dans votre environnement (voir [ici](#DownloadModel)).\n\n### Déploiement\n\nNous utilisons ici Qwen-7B-Chat comme exemple. Avant de lancer une démo web ou une API, vous pouvez établir la configuration comme indiqué ci-dessous :\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes\n```\nLes scripts suivants peuvent vous aider à construire :\n\n* API OpenAI\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Interface Web\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Démo CLI\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\nLes commandes ci-dessus téléchargeront automatiquement l'image requise et lanceront une démo d'interface Web en arrière-plan (le service redémarrera automatiquement). Vous pouvez ouvrir `http://localhost:${PORT}` sur l'hôte pour utiliser la démo.\n\nLa démo est lancée avec succès si vous obtenez le résultat suivant :\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nSi vous voulez vérifier le statut de la démo, vous pouvez utiliser `docker logs qwen` pour afficher les résultats.\n\nVous pouvez utiliser `docker rm -f qwen` pour arrêter le service et supprimer le conteneur.\n\n\n### Finetuning\n\nLa méthode de finetuning utilisant l'image Docker préconstruite est fondamentalement la même que [le chapitre ci-dessus](#Finetuning) (nous avons déjà installé les dépendances dans l'image) :\n\nVoici un exemple de LoRA à une seule GPU :\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs\n\n# Use all host devices by default\nDEVICE=all\n# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# Single-GPU LoRA finetuning\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\nPour faire un changement vers Q-LoRA à GPU unique par exemple, il suffit de modifier la commande bash à l'intérieur de `docker run` :\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## 🔥 Invite du système\nQwen-1.8-Chat et Qwen-72B-Chat ont été entièrement formés à diverses invites de système avec plusieurs séries d'interactions complexes, de sorte qu'ils peuvent suivre une variété d'invites de système et réaliser la personnalisation du modèle dans le contexte, améliorant ainsi l'évolutivité de Qwen-chat.\n\nGrâce aux messages-guides du système, Qwen-Chat peut **jouer avec enthousiasme**, **transférer le style de langage**, **fixer des tâches** et **fixer des comportements**.\n\n![](assets/system_prompt_language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\nPour plus d'informations, veuillez vous référer à la [documentation d'exemple](examples/system_prompt.md).\n\n\n## Utilisation des outils\n\nQwen-Chat a été optimisé pour l'utilisation d'outils et les capacités d'appel de fonctions. Les utilisateurs peuvent développer des agents, des applications LangChain, et même augmenter Qwen avec un Code Interpreter.\n\nNous fournissons une documentation sur la manière d'implémenter les appels d'outils basés sur le principe de ReAct Prompting, veuillez vous référer à [l'exemple ReAct](examples/react_prompt.md). Sur la base de ce principe, nous fournissons un support pour function calling dans [openai_api.py](openai_api.py).\n\nNous avons testé les capacités d'appel d'outil du modèle sur notre benchmark d'évaluation chinois à source ouverte et nous avons constaté que Qwen-Chat obtient systématiquement de bons résultats:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.↑)</th><th align=\"center\">Tool Input (Rouge-L↑)</th><th align=\"center\">False Positive Error↓</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\nPour évaluer la capacité de Qwen à utiliser l'interpréteur de code Python pour des tâches telles que la résolution de problèmes mathématiques, la visualisation de données et d'autres tâches générales telles que la manipulation de fichiers et l'exploration du Web, nous avons créé et mis en libre accès un test de référence spécialement conçu pour évaluer ces capacités. Vous pouvez trouver le benchmark sur ce [lien](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nNous avons observé que Qwen est performant en termes d'exécutabilité du code et de précision des résultats lors de la génération du code:\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Math↑</th><th align=\"center\">Visualization-Hard↑</th><th align=\"center\">Visualization-Easy↑</th><th align=\"center\">General↑</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## Compréhension du Contexte Long\n\nPour augmenter la longueur du contexte et éliminer le goulot d'étranglement que constitue la longueur de la séquence d'entraînement, nous introduisons plusieurs techniques, notamment l'interpolation tenant compte des NTK, l'attention par fenêtre et la mise à l'échelle de l'attention LogN, afin d'augmenter la longueur du contexte de Qwen-14B de 2K à plus de 8K tokens, et de Qwen-1.8B/7B de 8K à 32K tokens. \n\nPour Qwen-72B, nous adaptons RoPE à des contextes plus longs avec une base rotative plus importante. Qwen-72B prend en charge la longueur de contexte maximale de 32K tokens.\n\nNous menons des expériences de modélisation du langage sur l'ensemble de données arXiv avec l'évaluation PPL et nous constatons que Qwen peut atteindre des performances exceptionnelles dans le scénario d'un contexte long. Les résultats sont présentés ci-dessous :\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.56</td><td align=\"center\">4.62</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.49</td><td align=\"center\">4.32</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n    <tr>\n        <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n    </tr>\n</table>\n\nEn outre, pour vérifier la capacité de Qwen-72B-Chat à comprendre des textes longs, nous l'avons testé sur [L-Eval] (https://arxiv.org/abs/2307.11088) (tâches fermées). Les résultats sont les suivants :\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\nNous avons réalisé l'expérience de \"l'aiguille dans une botte de foin\" (l'idée vient de [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)) pour tester si le modèle peut récupérer des informations à différentes positions dans les entrées de différentes longueurs, le résultat est le suivant :\n\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\nLes résultats ci-dessus montrent que Qwen-72B-Chat peut récupérer avec précision des informations placées dans différentes positions dans une longueur d'entrée de 32K, ce qui prouve ses excellentes capacités de compréhension de textes longs.\n\n\n## Tokenizer\n\nNotre tokenizer basé sur tiktoken est différent des autres tokenizers, par exemple le tokenizer sentencepiece. Vous devez faire attention aux tokens spéciaux, en particulier lors de la mise au point. Pour des informations plus détaillées sur le tokenizer et son utilisation dans le cadre du finetuning, veuillez vous référer à la [documentation](tokenization_note.md).\n<br><br>\n\n## Reproduction\n\nPour reproduire les performances du modèle sur des ensembles de données de référence, nous fournissons des scripts permettant de reproduire les résultats. Consultez [eval/EVALUATION.md](eval/EVALUATION.md) pour plus d'informations. Notez que la reproduction peut entraîner de légères différences par rapport à nos résultats.\n<br><br>\n\n## FAQ\n\nSi vous rencontrez des problèmes, veuillez vous référer à la [FAQ](FAQ.md) et aux problèmes pour trouver une solution avant de lancer un nouveau problème.\n<br><br>\n\n## Citation\nSi vous trouvez notre travail utile, n'hésitez pas à nous citer.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## Accord de Licence\n\nLe code source fourni à l'adresse <https://github.com/QwenLM/Qwen> est soumis à la licence [Apache 2.0 License](./LICENSE) qui se trouve dans le répertoire racine.\n\nLes chercheurs et les développeurs sont libres d'utiliser les codes et les poids des modèles de Qwen et de Qwen-Chat. Pour leur utilisation commerciale, veuillez consulter l'accord de licence accompagnant chaque modèle.\n\n- Qwen-72B, Qwen-14B et Qwen-7B sont sous licence [Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT) que l'on peut trouver dans les dépôts HuggingFace et ModelScope correspondants. Pour une utilisation commerciale, veuillez remplir le formulaire ([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), et [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen)) pour en faire la demande.\n\n- Qwen-1.8B est sous licence [Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT) qui peut être trouvé dans les dépôts HuggingFace et ModelScope correspondants. Pour une utilisation commerciale, veuillez nous contacter.\n<br><br>\n\n## Contactez-nous\n\nSi vous souhaitez laisser un message à notre équipe de recherche ou à notre équipe produit, rejoignez nos groupes Discord ou WeChat! N'hésitez pas non plus à envoyer un courriel à qianwen_opensource@alibabacloud.com.\n\n"
        },
        {
          "name": "README_JA.md",
          "type": "blob",
          "size": 77.9443359375,
          "content": "<p align=\"left\">\n    <a href=\"README_CN.md\">中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>&nbsp ｜ &nbsp日本語 ｜ &nbsp<a href=\"README_FR.md\">Français</a> ｜ &nbsp<a href=\"README_ES.md\">Español</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        🤗 <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ｜ &nbsp&nbsp🖥️ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ｜  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br><br>\n\n> [!Important]\n> Qwen2 が登場しました！[QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) のフォローと、そこであなたの体験を共有することをお待ちしております。\n>\n> このリポジトリ ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) は、コードベースに大幅な違いがあるため、今後アクティブにメンテナンスされなくなります。\n<br>\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">🤗</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">🤗</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">🤗</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">🤗</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">🤗</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">🤗</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">🤖</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">🤗</a> |\n\n\n\n**Qwen-1.8B**、**Qwen-7B**、**Qwen-14B**、**Qwen-72B**の基本言語モデルである**Qwen**と、**Qwen-1.8B-Chat**、**Qwen-7B-Chat**、**Qwen-14B-Chat**、**Qwen-72B-Chat**のチャットモデルである**Qwen-Chat**をオープンソース化します。上の表にリンクがあります。リンクをクリックして、モデルカードをご確認ください。また、**[テクニカルレポート](https://arxiv.org/abs/2309.16609)**も公開しています。論文リンクをクリックしてご覧ください！\n\n簡単に説明すると、私たちは、ドメインや言語（中国語と英語を中心に）などを幅広くカバーする最大3兆トークンの多言語データに対して安定的に事前学習された強力なベース言語モデルを持っています。これらのモデルは、ベンチマークデータセットにおいて競争力のあるパフォーマンスを達成することができます。さらに、SFTとRLHFに基づく人間の嗜好に沿ったチャットモデル（まだリリースされていません）があり、チャット、コンテンツ作成、情報抽出、要約、翻訳、コーディング、数学の問題を解くなどが可能で、ツールを使ったり、エージェントとして遊んだり、コードインタプリタとして遊んだりすることもできます。\n\n\n| モデル       |   発行日    | コンテキストの最大長 | システムプロンプトの強化 | 预训练されたトークンの数 | Finetuning（Q-Lora）の最小GPUメモリ使用量 | 2048トークン生成時の最小GPUメモリ使用量（Int4） | ツールの使用能力 |\n|:----------|:--------:|:----------:|:------------:|:------------:|:------------------------------:|:-----------------------------:|:--------:|\n| Qwen-1.8B | 23.11.30 |    32K     |      ✅       |     2.2T     |             5.8GB              |             2.9GB             |    ✅     |  \n| Qwen-7B   | 23.08.03 |    32K     |      ❎       |     2.4T     |             11.5GB             |             8.2GB             |    ✅     |   \n| Qwen-14B  | 23.09.25 |     8K     |      ❎       |     3.0T     |             18.7GB             |            13.0GB             |    ✅     |\n| Qwen-72B  | 23.11.30 |    32K     |      ✅       |     3.0T     |             61.4GB             |            48.9GB             |    ✅     |   \n\n\nこのレポでは、それを把握することができる：\n\n* Qwenのクイックスタート。\n* 量子化モデルの詳細（使用量、メモリ、推論速度など）。比較のために、BF16モデルの統計も提供します。\n* フルパラメーターチューニング、LoRA、Q-LoRAを含む、微調整に関するチュートリアル。\n* vLLMとFastChatを例に、デプロイメントについて説明します。\n* WebUI、CLIデモなど、デモの構築に関する説明。\n* あなたのモデルのためのOpenAIスタイルのAPIを構築する手順。\n* ツール使用、エージェント、コードインタプリタの Qwen の詳細。\n* ロングコンテクスト理解評価の統計\n* ライセンス契約\n* ...\n\nまた、困ったことがあれば、まずは[FAQ](FAQ.md)を参照してほしい。まだ悩んでいますか？遠慮なく私たちに問題を送ってください（より多くの人が理解できるように、英語で）！私たちを助けたいなら、遠慮なくプルリクエストを送ってください！\n\n私たちとチャットしたいですか？私たちのDiscordまたはWeChatグループへようこそ！\n<br><br>\n\n## ニュースとアップデート\n\n* 2023.11.30 🔥 3T トークンで学習し、32k コンテキストをサポートする **Qwen-72B** と **Qwen-72B-Chat** を、 **Qwen-1.8B** と **Qwen-1.8B-Chat** とともに、ModelScope と Hugging Face 上でリリースしました。また、Qwen-72B-ChatとQwen-1.8B-Chatのシステム・プロンプト機能を強化しました。[サンプル・ドキュメント](examples/system_prompt.md)を参照してください。さらに、**Ascend 910** と **Hygon DCU** での推論をサポートしました。詳細は `ascend-support` と `dcu-support` を参照してください。\n* 2023.10.17 Int8量子化モデル**Qwen-7B-Chat-Int8**と**Qwen-14B-Chat-Int8**をリリースしました。\n* 2023.9.25 🔥 Qwen-14BとQwen-14B-ChatをModelScopeとHugging Faceでリリースしました。[qwen.cpp](https://github.com/QwenLM/qwen.cpp) と [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) もリリースされました。同時に、Qwen-7B と Qwen-7B-Chat も更新しました。Qwen-7B（オリジナル）と比較して、Qwen-7Bはより多くの学習トークンを使用し、2.2Tトークンから2.4Tトークンに増加し、コンテキスト長は2048から8192に拡張された。Qwen-7Bの中国語知識とコーディング能力はさらに向上しています。最新のコードとチェックポイントをお使いください！\n* 2023.9.12 Qwen-7Bモデルにおいて、フルパラメーター・ファインチューニング、LoRA、Q-LoRAを含むファインチューニングをサポートしました。\n* 2023.8.21 Qwen-7B-Chat 用 Int4 量子化モデル **Qwen-7B-Chat-Int4** をリリースしました。また、ベンチマーク評価においても大きな性能低下は見られませんでした。\n* 2023.8.3 ModelScope と Hugging Face 上で **Qwen-7B** と **Qwen-7B-Chat** をリリースしました。また、トレーニングの詳細やモデルの性能など、モデルの詳細については技術メモを提供しています。\n<br>\n\n## 性能\n\nQwenモデルは、MMLU、C-Eval、GSM8K、MATH、HumanEval、MBPP、BBHなど、自然言語理解、数学的問題解決、コーディングなどに関するモデルの能力を評価する一連のベンチマークデータセットにおいて、同様のモデルサイズを持つベースラインモデルを上回る性能を発揮する。Qwen-72Bは全てのタスクでLLaMA2-70Bを上回り、10タスク中7タスクでGPT-3.5を上回った。\n\n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=600px/>\n<p>\n<br>\n\n| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |\n| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\n\n比較されたすべてのモデルについて、公式に報告された結果と[OpenCompass](https://opencompass.org.cn/leaderboard-llm) の間の最高スコアを報告します。\n\nより詳細な実験結果（より多くのベンチマークデータセットでの詳細なモデル性能）や詳細については、[こちら](TODO)をクリックして技術メモを参照してください。\n<br><br>\n\n## 必要条件\n\n* python 3.8 以上\n* pytorch 1.12 以上、2.0 以上を推奨\n* transformers 4.32 以上\n* CUDA 11.4 以上を推奨（GPU ユーザー、フラッシュアテンションユーザー向けなど）\n<br>\n\n## クイックスタート\n\n以下では、Qwen-Chat と 🤖 ModelScope と 🤗 Transformers の簡単な使用例を示します。\n\n詳しくはセクション[\"ビルド済みDockerイメージの使用\"](#-docker)を参照してください。\n\nDockerを使用しない場合は、環境のセットアップと必要なパッケージのインストールが済んでいることを確認してください。上記の要件を満たしていることを確認してから、依存するライブラリをインストールしてください。\n\n```bash\npip install -r requirements.txt\n```\n\nお使いのデバイスが fp16 または bf16 をサポートしている場合、[flash-attention](https://github.com/Dao-AILab/flash-attention) （flash attention 2に対応しました）をインストールすることで、より高い効率とメモリ使用量を抑えることができます。(**flash-attention はオプションであり、インストールしなくてもプロジェクトは正常に実行できます**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# 以下はオプションです。インストールに時間がかかる場合があります。\n# pip install csrc/layer_norm\n# flash-attn のバージョンが 2.1.1 以降の場合、以下は必要ありません。\n# pip install csrc/rotary\n```\n\nこれで ModelScope か Transformers で始めることができます。\n\n### 🤗 Transformers\n\nQwen-Chat を推論に使用するには、以下のように数行のコードを入力するだけです。Qwen/Qwen-7B-Chat \"や \"Qwen/Qwen-14B-Chat \"のように、正しいモデル名やパスを渡すことを忘れないでください。**最新のコードを使用していることを確認してください。**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names：\"Qwen/Qwen-7B-Chat\"、\"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# bf16 を使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 を使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# CPU のみ使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# オートモードを使用すると、デバイスに応じて自動的に精度が選択されます。\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# 生成のためのハイパーパラメータを指定。ただし、4.32.0 以上のトTransformerを使用している場合は、これを行う必要はありません。\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 第一回対話ターン\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\n# 你好！很高兴为你提供帮助。\n\n# 第二回対話ターン\nresponse, history = model.chat(tokenizer, \"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\nprint(response)\n# 这是一个关于一个年轻人奋斗创业最终取得成功的故事。\n# 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。\n# 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。\n# 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。\n# 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。\n# 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。\n\n# 第三轮对话 第三回対話ターン\nresponse, history = model.chat(tokenizer, \"给这个故事起一个标题\", history=history)\nprint(response)\n# 《奋斗创业：一个年轻人的成功之路》\n```\n\nQwen の学習済みベースモデルの実行も簡単です。\n\n<details>\n  <summary>Qwen の実行</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names：\"Qwen/Qwen-7B\"、\"Qwen/Qwen-14B\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n# bf16 を使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 を使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# CPU のみ使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# オートモードを使用すると、デバイスに応じて自動的に精度が選択されます。\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# 生成のためのハイパーパラメータを指定。ただし、4.32.0 以上のトTransformerを使用している場合は、これを行う必要はありません。\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\ninputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# 蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）...\n```\n\n</details>\n\n<p id=\"DownloadModel\">\nHuggingFaceからモデルのチェックポイントとコードをダウンロードする際にネットワークの問題が発生した場合、ModelScopeからチェックポイントをダウンロードする方法はこちらでございます。\n</p>\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n# model_dir = snapshot_download('qwen/Qwen-14B')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### 🤖 ModelScope\n\nModelScope は、MaaS（Model-as-a-Service） のためのオープンソースプラットフォームであり、AI 開発者に柔軟で費用対効果の高いモデルサービスを提供します。同様に、以下のように ModelScope でモデルを実行することができます:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# Model names：\"Qwen/Qwen-7B-Chat\"、\"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n\nresponse, history = model.chat(tokenizer, \"你好\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"浙江的省会在哪里？\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"它有什么好玩的景点\", history=history)\nprint(response)\n```\n\n### バッチ推論\nQwenはバッチ推論をサポートしている。フラッシュ・アテンションを有効にした場合、バッチ推論を使用することで40%のスピードアップが期待できる。以下にコード例を示す：\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"我想听你说爱我。\", \"今天我想吃点啥，甜甜的，推荐下\", \"我马上迟到了，怎么做才能不迟到\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"我想听你说爱我。\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"今天我想吃点啥，甜甜的，推荐下\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"我马上迟到了，怎么做才能不迟到\", history=None)\nprint(response)\n```\n\n### CPU\n\nQwenとtiktokenの純粋なC++実装である [qwen.cpp](https://github.com/QwenLM/qwen.cpp) を使用することを強くお勧めします。詳細はレポを確認してください！\n\nまた、CPU上でモデルを直接実行することも簡単ですが、その場合はデバイスの指定が必要です：\n\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nただし、推論効率が極端に低下する可能性があります。\n\n### 複数のGPU\n\nGPUメモリ不足に悩まされ、1つ以上のGPUでモデルを実行したい場合、Transformersでサポートされるようになったデフォルトのロード方法を直接使うことができます。以前の `utils.py` に基づく方法は非推奨です。\n\nしかし、この方法は簡単ですが、ネイティブ・パイプライン並列の効率は低いです。FastChatでvLLMを使用することをお勧めします。\n\n### DashScope\n\nAPIを通じてQwenを利用する最も簡単な方法は、Alibaba Cloudを通じたDashScope APIサービスです。その使い方を紹介します。さらに、OpenAIスタイルのAPIをご自身のサーバーにデプロイするためのスクリプトも提供しています。\n\nDashScopeはAlibaba Cloudが提供する大規模言語モデルAPIサービスで、今回Qwenに対応した。DashScopeの背後にあるモデルは、詳細が提供されていない一時的な社内バージョンであることに注意してください。サービスには `qwen-turbo` と `qwen-plus` があり、前者はより高速に動作し、後者はより優れたパフォーマンスを実現している。詳細はドキュメント [こちら](https://dashscope.aliyun.com) を参照。\n\n公式サイト [link](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) で DashScope アカウントを作成し、API キー (AK) を取得してください。AK は環境変数で設定することをお勧めします：\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nその後、パッケージをインストールし、ドキュメントは [こちら](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) をクリックしてください。Python をお使いの場合は、pip で DashScope をインストールできます：\n```bash\npip install dashscope\n```\nJAVA SDKを使用する場合は、この方法でインストールできます：\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nDashScope を使用する最も簡単な方法は、OpenAI API と同様のメッセージを使用する方法です。以下にその例を示す：\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': '如何做西红柿鸡蛋？'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\n詳しい使い方は公式サイトをご覧ください。\n<br><br>\n\n\n## 量子化\n\n### GPTQ\n\n我々は、[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)に基づいた解決策を提供し、Int4とInt8の量子化モデルをリリースすることで、ほぼ無損失なモデル効果を達成しつつ、メモリコストと推論速度の両方で性能を向上させた。\n\nここでは、量子化されたモデルを推論に使用する方法を説明する。始める前に、auto-gptqの要件を満たしていることを確認し（例：torch 2.0以上、transformers 4.32.0以上など）、必要なパッケージをインストールしてください：\n\n```bash\npip install auto-gptq optimum\n```\n\nauto-gptq`のインストールに問題がある場合は、公式の[repo](https://github.com/PanQiWei/AutoGPTQ)をチェックして、ホイールを見つけることをお勧めする。\n\n> 注意：コンパイル済みの `auto-gptq` パッケージは `torch` のバージョンと CUDA バージョンに強く依存しています。さらに、最近のアップデートにより \n> さらに、最近のアップデートにより、`transformers`、`optimum`、`peft` でサポートされていないバージョンのエラーが発生する可能性があります。\n> 以下の要件を満たす最新バージョンの使用をお勧めします：\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1 > - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0, <2.1 auto-gptq<0.5.0 transformers<4.35.0  optimum<1.14.0 peft>=0.5.0,<0.6.0\n\nそうすれば、量子化されたモデルを簡単にロードすることができ、いつもと同じように推論を実行することができる：\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\nベンチマークにおける BF16 モデルと Int8、Int4 モデルの性能について説明します。その結果は以下に示します：\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n\n### KVキャッシュ量子化\n\n> 注意: Hugging Faceの内部メカニズムにより、この機能のサポートファイル \n> (すなわち、`cache_autogptq_cuda_256.cpp`と`cache_autogptq_cuda_kernel_256.cu`)が欠落している可能性があります。以下を手動でダウンロードしてください。\n> Hugging Face Hubから手動でダウンロードし、他のモジュールファイルと同じフォルダに入れてください。\n\nアテンション KV キャッシュを量子化して圧縮して保存すると、サンプルのスループットが向上する。この機能を有効にするには、`config.json` に `use_cache_quantization` と `use_cache_kernel` という引数を指定する。\n具体的な使用方法は以下の通りである：\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\n\n注意： 現在、KVキャッシュの量子化とフラッシュ・アテンションを同時に使用することはできない。\nKV キャッシュの量子化とフラッシュ・アテンションを同時に有効にした場合（`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`）、デフォルトでは `use_flash_attn` は無効になる（`use_flash_attn=false`）。\n\n量子化されたint8-kvcacheモデルを使用しても、下流の評価で大幅な性能低下がないことを確認しました。以下では、さまざまな条件下でのメモリフットプリントのプロファイリングに焦点を当てます。\nプロファイリングは、PyTorch 2.0.1とCUDA 11.4を搭載したシングルA100-SXM4-80G GPUで実行しました。\nデフォルトで1024トークンを生成するためにBF16モデルを使用し、\"OOM \"はメモリ不足エラーを示します。\n\nKVキャッシュの量子化により、モデルはより大きなバッチサイズ（bs）で推論することができる。\n\n| USE KV Cache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|--------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| No           | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |\n| Yes          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\nKVキャッシュ量子化により、推論段階でより長いシーケンス（`sl`, シーケンス長、生成されるトークン数を指す）を生成する際、モデルはより多くのメモリを節約することができる。\n\n| USE KV Cache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|--------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| No           | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| Yes          |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\nKVキャッシュ量子化モデルでは、layer-pastのフォーマットをfloatからint8に変換し、量子化された `layer-past` には量子化パラメータも格納される。\n\n具体的な手順は以下の通り：\n\n1. key/valueの量子化を行います。\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n\n2. `layer_past`に保存します。\n\n量子化されたの`layer-past`は:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\n`layer_past`の元のフォーマットは以下の通りである：\n```\n    layer_past=(key,value)\n```\n量子化されたアテンションKVを使用したい場合、 \nInt8のkey/valueをfloatフォーマットに戻すには、以下のように逆量子化操作を使用します：\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n## 推論パフォーマンス\n\nこのセクションでは、さまざまな精度のモデルのスピードとメモリの統計情報を提供する。スピードとメモリーのプロファイリングは[このスクリプト](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)を使用しています。\n\nBF16、Int8、および Int4 のモデルを使用して 2048 を生成する際の平均推論速度 (トークン/秒) と GPU メモリ使用量を測定しました。\n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nプロファイリングは、PyTorch 2.0.1、CUDA 11.8、および Flash-Attendant 2 を備えた単一の A100-SXM4-80G GPU (2xA100 について言及されている場合を除く) で実行されます。(72B + vLLM は PyTorch 2.1.0 および Cuda 11.8 を使用します。) 推論速度 は、エンコードされ生成されたトークンの平均である。\n\n注意：上記のInt4/Int8モデルの推論速度は、autogptqを使用しています。現在、``AutoModelForCausalLM.from_pretrained``で読み込まれるモデルの推論速度は約20%遅くなります。この問題はHuggingFaceチームに報告済みであり、解決策があれば即座に更新されます。\n\nまた、コンテキストと生成の長さ、Flash Attention バージョンのさまざまな設定で推論速度と GPU メモリ使用量も測定します。 結果は、Hugging Face または ModelScope の対応するモデルカードで確認できます。\n\n## ファインチューニング\n\n### 使用方法\n現在、公式のトレーニングスクリプト `finetune.py` を提供しています。さらに、finetune.pyのシェルスクリプトを提供し、finetune.pyを実行することで、finetune.pyを起動することができる。さらに、安心してファインチューニングを開始するためのシェルスクリプトも提供しています。このスクリプトは、[DeepSpeed](https://github.com/microsoft/DeepSpeed) (注意：これはpydanticの最新バージョンとコンフリクトする可能性があるので、`pydantic<2.0`にする必要があります) および [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) を使用したトレーニングをサポートします。弊社が提供するシェル・スクリプトは DeepSpeed と Peft を使用するため、事前に DeepSpeed と Peft をインストールすることをお勧めします：\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\n学習データを準備するには、すべてのサンプルをリストにまとめ、jsonファイルに保存する必要があります。各サンプルはidと会話リストで構成される辞書です。以下は1つのサンプルを含む単純なリストの例です：\n\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是一个语言模型，我叫通义千问。\"\n      }\n    ]\n  }\n]\n```\n\nデータ準備の後、提供されているシェルスクリプトを使って微調整を実行することができる。データファイルのパス `$DATA` を忘れずに指定してください。\n\nファインチューニングのスクリプトを使用することで、以下のことが可能になる：\n- フルパラメーター・ファインチューニング\n- LoRA\n- Q-LoRA\n\nフルパラメータパラメータのファインチューニングを行うには、トレーニングプロセス全体ですべてのパラメータを更新する必要があります。トレーニングを開始するには、以下のスクリプトを実行します：\n\n```bash\n# 分散トレーニング。GPUメモリが不足するとトレーニングが破綻するため、シングルGPUのトレーニングスクリプトは提供していません。\nbash finetune/finetune_ds.sh\n```\n\nシェルスクリプトでは、正しいモデル名またはパス、データパス、出力ディレクトリを指定することを忘れないでください。このスクリプトでは DeepSpeed ZeRO 3 を使用しています。変更したい場合は、引数 `--deepspeed` を削除するか、要件に基づいて DeepSpeed 設定 json ファイルを変更してください。さらに、このスクリプトは混合精度のトレーニングに対応しており、`--bf16 True` または `--fp16 True` を使用することができます。fp16を使用する場合は、混合精度のトレーニングのため、DeepSpeedを使用することを忘れないこと。経験的に、あなたのマシンがbf16をサポートしている場合、私たちのプリトレーニングとアライメントを整合させるためにbf16を使用することをお勧めします。\n\n同様に、LoRAを実行するには、以下のように別のスクリプトを使って実行する。始める前に、`peft`がインストールされていることを確認してください。また、モデル、データ、出力へのパスを指定する必要があります。学習済みモデルには絶対パスを使用することをお勧めします。なぜなら、LoRAはアダプタのみを保存し、アダプタ設定jsonファイルの絶対パスは、ロードする事前学習済みモデルを見つけるために使用されるからです。また、このスクリプトはbf16とfp16の両方をサポートしている。\n\n```bash\n# シングルGPUトレーニング\nbash finetune/finetune_lora_single_gpu.sh\n# 分散トレーニング\nbash finetune/finetune_lora_ds.sh\n```\n\nLoRA ([論文](https://arxiv.org/abs/2106.09685)) は、フルパラメーターによるファインチューニングと比較して、adapterのパラメーターを更新するだけで、元の大きな言語モデル層は凍結されたままである。そのため、メモリコストが大幅に削減でき、計算コストも削減できる。\n\nなお、チャットモデル（Qwen-7B-Chatなど）ではなく、ベース言語モデル（Qwen-7Bなど）の微調整にLoRAを使用した場合、スクリプトは自動的に学習可能なパラメータとして埋め込み層と出力層を切り替えます。これは、ベースとなる言語モデルには、ChatMLフォーマットによってもたらされる特殊なトークンに関する知識がないためです。したがって、これらのレイヤーは、モデルがトークンを理解し予測するために更新される必要があります。別の言い方をすれば、もしLoRAで特殊なトークンを学習するのであれば、コード内で `modules_to_save` を設定することで、レイヤーを学習可能なパラメータに設定する必要があります。また、これらのパラメータが学習可能な場合、ZeRO 3 を使用することはできません。新しいトレーニング可能なパラメータがない場合は、DeepSpeed 設定ファイルを変更することで ZeRO 3 に切り替えることができます。さらに、LoRAのメモリフットプリントは、このような学習可能なパラメータがある場合とない場合で、大きな開きがあることがわかります。そのため、メモリに問題がある場合は、LoRAのChatモデルを微調整することをお勧めします。詳細は以下のプロファイルを参照してください。\n\nしかし、それでもメモリ不足に悩む場合は、Q-LoRA（[論文](https://arxiv.org/abs/2305.14314)）を検討することができます。これは、量子化されたラージ言語モデルと、ページド・アテンションなどの他のテクニックを使用し、さらに少ないメモリコストで実行することができます。\n\n注：シングル GPU Q-LoRA トレーニングを実行するには、`pip` または `conda` を使って `mpi4py` をインストールする必要があるかもしれない。\n\nQ-LoRAを実行するには、以下のスクリプトを直接実行してください：\n\n```bash\n# シングルGPUトレーニング\nbash finetune/finetune_qlora_single_gpu.sh\n# 分散トレーニング\nbash finetune/finetune_qlora_ds.sh\n```\n\nQ-LoRAについては、弊社が提供する量子化モデル、例えばQwen-7B-Chat-Int4をロードすることをお勧めします。BF16モデルは使用し**ない**でください！フルパラメータ・ファインチューニングやLoRAとは異なり、Q-LoRAではfp16のみがサポートされる。シングルGPUのトレーニングでは、トーチアンプによるエラーが観測されたため、混合精度のトレーニングにはDeepSpeedを使用する必要がある。また、Q-LoRAの場合、LoRAの特殊トークンの問題が残っています。しかし、Q-LoRAではチャットモデルとしてInt4モデルのみを提供しており、言語モデルはChatML形式の特殊トークンを学習しているため、レイヤーの心配はありません。なお、Int4モデルのレイヤーは学習できないはずなので、学習で特殊なトークンを導入すると、Q-LoRAが動作しなくなる可能性があります。\n\nLoRAとQ-LoRAの学習は、フルパラメータによるファインチューニングとは異なり、アダプターパラメータのみを保存する。仮にQwen-7Bから学習を開始したとすると、以下のようにファインチューニングされたモデルを読み込んで推論を行うことができる：\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nアダプターをマージし、微調整したモデルをスタンドアロンモデルとして保存したい場合は（これは LoRA でのみ可能で、Q-LoRA からパラメータをマージすることはできません）、以下のコードを実行します：\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\n`new_model_directory` ディレクトリには、マージされたモデルの重みとモジュール ファイルが含まれます。 保存されたファイルに `*.cu` および `*.cpp` ファイルが存在しない可能性があることに注意してください。 KVキャッシュ機能を使用したい場合は、手動でコピーしてください。 また、このステップではトークナイザー ファイルは新しいディレクトリに保存されません。 トークナイザー ファイルをコピーするか、次のコードを使用できます。\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    path_to_adapter, # path to the output directory\n    trust_remote_code=True\n)\n\ntokenizer.save_pretrained(new_model_directory)\n```\n\n注意：マルチGPUトレーニングの場合、分散トレーニング用の適切なハイパーパラメータをマシンに応じて指定する必要があります。また、データ、メモリフットプリント、トレーニング速度を考慮して、引数 `--model_max_length` で最大シーケンス長を指定することをお勧めします。\n\n### メモリと速度のプロファイリング\nシングルGPUトレーニングのセットアップにおいて、LoRA (LoRA(emb)はembeddingと出力層を学習させるが、LoRAはembeddingと出力層を学習させない) とQ-LoRAのGPUメモリとトレーニング速度をプロファイリングする。このテストでは、シングルA100-SXM4-80G GPUで実験し、CUDA 11.8とPytorch 2.0を使用します。Flash attention 2を使用します。256、512、1024、2048、4096、8192という異なる長さの入力のメモリ（GB）と速度（s/iter）をプロファイリングします。また、2台のA100 GPUを用いたQwen-7Bによるフルパラメータ・ファインチューニングの統計量も報告する。GPUメモリの制限のため、256、512、1024トークンの統計のみを報告する。\n\n\nQwen-72B については、2 つの方法で実験します。1) 4 つの A100-SXM4-80G GPU での Lora 微調整 + DeepSpeed ZeRO 3、および 2) 1 つの A100-SXM4-80G GPU での QLora (int4) 微調整。 OOM は、LoRA (emb) 微調整と Deepspeed ZeRO 3 を使用しない LoRA 微調整の両方で 4 つの A100-SXM4-80G GPU で発生することに注意してください (`--deepspeedfinetune/ds_config_zero3.json` を [`finetune/finetune_lora_ds に渡すことができます) .sh`](finetune/finetune_lora_ds.sh) を使用して DeepSpeed ZeRO 3 を有効にします)。\n\n統計量を以下に示す：\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    </tr>\n    </tr>\n\t\t<tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td><td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">7B</th><td>LoRA</td><td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th><td>LoRA</td><td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n\t<tr>\n        <th rowspan=\"2\">72B</th><td>LoRA + Deepspeed Zero3</td><td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n<br>\n\n## デプロイ\n\n### vLLM \nデプロイメントと高速推論のためには、vLLMを使用することをお勧めします。\n\n**CUDA 12.1** および **PyTorch 2.1** を使用している場合は、次のコマンドを直接使用して vLLM をインストールできます。\n```bash\npip install vllm\n```\n\nそれ以外の場合は、公式 vLLM [インストール手順](https://docs.vllm.ai/en/latest/getting_started/installation.html) を参照してください。\n\n#### vLLM + Transformer Wrapper\n\n[ラッパー コード](examples/vllm_wrapper.py) をダウンロードし、複数ラウンドの対話対話のために次のコマンドを実行できます。 (注: 現在は ``model.chat()`` メソッドのみをサポートしています。)\n\n```python\nfrom vllm_wrapper import vLLMWrapper\n\nmodel = vLLMWrapper('Qwen/Qwen-7B-Chat', tensor_parallel_size=1)\n# model = vLLMWrapper('Qwen/Qwen-7B-Chat-Int4', tensor_parallel_size=1, dtype=\"float16\")\n\nresponse, history = model.chat(query=\"你好\", history=None)\nprint(response)\nresponse, history = model.chat(query=\"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\nprint(response)\nresponse, history = model.chat(query=\"给这个故事起一个标题\", history=history)\nprint(response)\n```\n#### vLLM + Web デモ / OpenAI API\nFastChat を使用して、Web デモまたは OpenAI API サーバーを起動できます。 まず、FastChat をインストールします。\n```\npip install \"fschat[model_worker,webui]\"\n```\n\nvLLM および FastChat で Qwen を実行するには、次の方法でコントローラーを起動する必要があります。\n```bash\npython -m fastchat.serve.controller\n```\n\nそれからmodel workerを起動し、推論のためにモデルをロードします。シングルGPU推論の場合は、直接実行できます：\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype float16 # INT4モデルを実行します\n```\nしかし、より高速な推論や大容量メモリーのために複数のGPUでモデルを実行したい場合は、vLLMがサポートするテンソル並列を使用することができます。モデルを4GPUで実行するとすると、コマンドは以下のようになります：\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype float16 # run int4 model # INT4モデルを実行します\n```\n\nモデルワーカーを起動した後、起動することができます：\n\n* Web UI Demo\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* OpenAI API\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\nただし、vLLM と FastChat の使用が難しい場合は、Web デモ、CLI デモ、および API をデプロイするために提供されている最も簡単な方法を試すことができます。\n\n\n### ウェブ UI\n\nウェブ UI デモを構築するためのコードを提供します（@wysaid に感謝）。これを始める前に、以下のパッケージがインストールされていることを確認してください:\n\n```bash\npip install -r requirements_web_demo.txt\n```\n\nそして、以下のコマンドを実行し、生成されたリンクをクリックします:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n### CLI デモ\n\n`cli_demo.py` に CLI のデモ例を用意しています。ユーザはプロンプトを入力することで Qwen-7B-Chat と対話することができ、モデルはストリーミングモードでモデルの出力を返します。以下のコマンドを実行する:\n\n```\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\nOpenAI API をベースにローカルAPIをデプロイする方法を提供する（@hanpenggit に感謝）。始める前に、必要なパッケージをインストールしてください:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nそれから、API をデプロイするコマンドを実行します:\n\n```bash\npython openai_api.py\n```\n\nチェックポイント名やパスには `-c`、CPU デプロイメントには `--cpu-only` など、引数を変更できます。API デプロイメントを起動する際に問題が発生した場合は、パッケージを最新バージョンに更新することで解決できる可能性があります。\n\nAPI の使い方も簡単です。以下の例をご覧ください:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# ストリーミングレスポンスを有効化するリクエストを作成する\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=True\n    # ストリーミング出力形式でのストップワードの指定はまだサポートされておらず、開発中です。\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# ストリーミングレスポンスを有効化しないリクエストを作成する\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好\"}\n    ],\n    stream=False,\n    stop=[] # 例えば、stop=[\"Observation:\"] (ReAct プロンプトの場合)。\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n**Function Calling** もサポートされています(ただし、今のところ `stream=False` の場合のみ)。使用例](examples/function_call_examples.py) を参照してください。\n<br><br>\n\n## 🐳 Docker\n\nデプロイプロセスを簡素化するために、あらかじめ環境を構築した docker イメージを提供しています： [qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen)。ドライバを導入し、モデルファイルをダウンロードするだけで、デモを起動し、OpenAI APIをデプロイし、モデルを微調整することができます。\n\n### 準備\n\n1. 使用するイメージに応じて、正しいバージョンのNvidiaドライバをインストールしてください：\n  - `qwenllm/qwen:cu117` (**recommend**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: same as `qwenllm/qwen:cu117`\n\n2. [Docker](https://docs.docker.com/engine/install/) と [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) をインストールして設定します：\n\n```bash\n# configure docker\nsudo systemctl start docker\n# test if docker is correctly installed\nsudo docker run hello-world\n\n# configure nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. モデルのチェックポイントとコードを環境にダウンロードします（[こちら](#DownloadModel)を参照）。\n\n### デプロイ\n\nここでは例として Qwen-7B-Chat を使用する。ウェブ・デモや API を起動する前に、以下のように設定を行います：\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes\n```\n以下のスクリプトがビルドに役立つ：\n\n* OpenAI API\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Web UI\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* CLI Demo\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\n上記のコマンドは自動的に必要なイメージをダウンロードし、バックグラウンドでWeb UIデモを起動します（サービスは自動で再起動します）。デモを使用するには、ホスト上で `http://localhost:${PORT}` を開いてください。\n\n以下の出力が表示されれば、デモは正常に起動しています：\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nデモの状態を確認したい場合は、`docker logs qwen` を使って出力を表示できる。\n\ndocker rm -f qwen` でサービスを停止し、コンテナを削除できる。\n\n\n### ファインチューニング\n\nビルド済みのDockerイメージを利用したファインチューニングの方法は、基本的に[前章](#Finetuning)と同じです(すでにイメージに依存関係がインストールされています)：\n\n以下はシングルGPUのLoRAの例です：\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs\n\n# Use all host devices by default\nDEVICE=all\n# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# Single-GPU LoRA finetuning\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\n例えばシングルGPUのQ-LoRAに変更するには、`docker run`内のbashコマンドを変更するだけでいい：\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## 🔥 システムプロンプト\nQwen-1.8-Chat と Qwen-72B-Chat は、複数回の複雑な対話を伴う多様なシステム プロンプトで完全にトレーニングされているため、さまざまなシステム プロンプトに従い、コンテキストに応じたモデルのカスタマイズを実現し、Qwen-Chat のスケーラビリティをさらに向上させることができます。\n\nシステム プロンプトを使用すると、Qwen-Chat は **ローリー プレイ**、**言語スタイルの転送**、**タスク設定**、**動作設定**を実現できます。\n\n![](assets/system_prompt_ language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\n詳細については、[サンプルドキュメント](examples/system_prompt.md)を参照してください。\n\n## ツールの使用\n\nQwen-Chat は、ツールの使用法と関数呼び出し機能に合わせて最適化されています。 ユーザーはエージェント、LangChain アプリケーションを開発し、Python コード インタープリターで Qwen を拡張することもできます。\n\nReAct プロンプトの原則に基づいてツール呼び出しを実装する方法に関するドキュメントを提供しています。[ReAct の例](examples/react_prompt.md) を参照してください。 この原則に基づいて、[openai_api.py](openai_api.py) で関数呼び出しのサポートを提供します。\n\nオープンソースの中国語評価ベンチマークでモデルのツール呼び出し機能をテストしたところ、Qwen-Chat が一貫して良好なパフォーマンスを発揮することがわかりました。\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.↑)</th><th align=\"center\">Tool Input (Rouge-L↑)</th><th align=\"center\">False Positive Error↓</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\n数学的問題解決、データ視覚化、ファイル処理や Web スクレイピングなどのその他の汎用タスクに Python コード インタープリターを使用する Qwen の能力を評価するために、これらの能力を評価するために特別に設計されたベンチマークを作成し、オープンソース化しました。 。 ベンチマークはこの [リンク](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark) で見つけることができます。\n\nQwen は、コード生成時のコードの実行可能性と結果の精度の点で優れたパフォーマンスを発揮することがわかりました。\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Math↑</th><th align=\"center\">Visualization-Hard↑</th><th align=\"center\">Visualization-Easy↑</th><th align=\"center\">General↑</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## 長い文脈の理解\n\nコンテキスト長を拡張し、トレーニング シーケンス長のボトルネックを解消するために、NTK 対応補間、ウィンドウ アテンション、LogN アテンション スケーリングなどのいくつかの技術を導入し、Qwen-14B のコンテキスト長を 2K から 8K 以上に拡張します。 トークン、および Qwen-1.8B/7B は 8K から 32K トークンまで。\n\nQwen-72B では、より大きな回転ベースを備えたより長いコンテキストに RoPE を適応させます。 Qwen-72B は、32K トークンの最大コンテキスト長をサポートします。\n\n私たちは、PPL 評価を使用して arXiv データセットで言語モデリング実験を実施し、Qwen が長いコンテキストのシナリオで優れたパフォーマンスを達成できることを発見しました。 結果を以下に示します。\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.56</td><td align=\"center\">4.62</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.49</td><td align=\"center\">4.32</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n    <tr>\n        <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n    </tr>\n</table>\n\nさらに、Qwen-72B-Chat の長文理解能力を検証するために、[L-Eval](https://arxiv.org/abs/2307.11088) (クローズドエンド タスク) でテストしました。 結果は次のとおりです。\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\n私たちは、モデルが入力内のさまざまな位置で情報を取得できるかどうかをテストするために、「干し草の山の中の針」実験 (このアイデアは [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393) から来ました) を実施しました。 異なる長さの場合、結果は次のようになります。\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\n上記の結果は、Qwen-72B-Chat が 32K の入力長内でさまざまな位置に配置された情報を正確に取得できることを示しており、その優れた長文理解能力を証明しています。\n\n## トークナイザー\n\ntiktoken に基づくトークナイザーは、他のトークナイザー、例えばセンテンスピーストークナイザーとは異なります。特にファインチューニングの際には、特殊なトークンに注意を払う必要があります。トークナイザに関する詳細な情報や、ファインチューニングにおける使用方法については、[ドキュメント](tokenization_note_ja.md)を参照してください。\n<br><br>\n\n## 再現\n\nベンチマークデータセットでのモデル性能の再現のために、結果を再現するスクリプトを提供しています。詳しくは [eval/EVALUATION.md](eval/EVALUATION.md) を確認してください。なお、再現の結果、我々の報告結果と若干異なる場合があります。\n<br><br>\n\n## FAQ\n\n問題が発生した場合は、まずは [FAQ](FAQ_ja.md) や issue を参照し、新しい issue を立ち上げる前に解決策を探してください。\n<br><br>\n\n## 引用\n私たちの仕事が役に立ったと思ったら、遠慮なく引用してください。\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## ライセンス契約\n\n<https://github.com/QwenLM/Qwen>で提供されるソースコードは、ルートディレクトリにある[Apache 2.0 License](./LICENSE)の下でライセンスされています。\n\n研究者や開発者は、QwenとQwen-Chatのコードとモデルウェイトを自由に使用することができます。商用利用については、各モデルに添付されている使用許諾契約書をご確認ください。\n\n- Qwen-72B、Qwen-14B、Qwen-7Bは、対応するHuggingFaceとModelScopeのリポジトリにある[Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT)に基づいてライセンスされています。商用利用の場合は、フォーム([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen))に記入して申請してください。\n\n- Qwen-1.8Bは、対応するHuggingFaceとModelScopeのリポジトリにある[Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT)に基づいてライセンスされています。商用利用については、私たちにご連絡ください。\n<br><br>\n\n## お問い合わせ\n\n研究チームまたは製品チームへのメッセージは、qianwen_opensource@alibabacloud.com までお気軽にお送りください。\n\n"
        },
        {
          "name": "Tongyi Qianwen LICENSE AGREEMENT",
          "type": "blob",
          "size": 6.734375,
          "content": "Tongyi Qianwen LICENSE AGREEMENT\n\nTongyi Qianwen Release Date: August 3, 2023\n\nBy clicking to agree or by using or distributing any portion or element of the Tongyi Qianwen Materials, you will be deemed to have recognized and accepted the content of this Agreement, which is effective immediately.\n\n1. Definitions\n    a. This Tongyi Qianwen LICENSE AGREEMENT (this \"Agreement\") shall mean the terms and conditions for use, reproduction, distribution and modification of the Materials as defined by this Agreement.\n    b. \"We\"(or \"Us\") shall mean Alibaba Cloud.\n    c. \"You\" (or \"Your\") shall mean a natural person or legal entity exercising the rights granted by this Agreement and/or using the Materials for any purpose and in any field of use.\n    d. \"Third Parties\" shall mean individuals or legal entities that are not under common control with Us or You.\n    e. \"Tongyi Qianwen\" shall mean the large language models (including Qwen model and Qwen-Chat model), and software and algorithms, consisting of trained model weights, parameters (including optimizer states), machine-learning model code, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Us.\n    f. \"Materials\" shall mean, collectively, Alibaba Cloud's proprietary Tongyi Qianwen and Documentation (and any portion thereof) made available under this Agreement.\n    g. \"Source\" form shall mean the preferred form for making modifications, including but not limited to model source code, documentation source, and configuration files.\n    h. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation,\n and conversions to other media types.\n\n2. Grant of Rights\nYou are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Alibaba Cloud's intellectual property or other rights owned by Us embodied in the Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Materials.\n\n3. Redistribution\nYou may reproduce and distribute copies of the Materials or derivative works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n    a. You shall give any other recipients of the Materials or derivative works a copy of this Agreement;\n    b. You shall cause any modified files to carry prominent notices stating that You changed the files;\n    c. You shall retain in all copies of the Materials that You distribute the following attribution notices within a \"Notice\" text file distributed as a part of such copies: \"Tongyi Qianwen is licensed under the Tongyi Qianwen LICENSE AGREEMENT, Copyright (c) Alibaba Cloud. All Rights Reserved.\"; and\n    d. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such derivative works as a whole, provided Your use, reproduction, and distribution of the work otherwise complies with the terms and conditions of this Agreement.\n\n4. Restrictions\nIf you are commercially using the Materials, and your product or service has more than 100 million monthly active users, You shall request a license from Us. You cannot exercise your rights under this Agreement without our express authorization.\n\n5. Rules of use\n    a. The Materials may be subject to export controls or restrictions in China, the United States or other countries or regions. You shall comply with applicable laws and regulations in your use of the Materials.\n    b. You can not use the Materials or any output therefrom to improve any other large language model (excluding Tongyi Qianwen or derivative works thereof).\n\n6. Intellectual Property\n    a. We retain ownership of all intellectual property rights in and to the Materials and derivatives made by or for Us. Conditioned upon compliance with the terms and conditions of this Agreement, with respect to any derivative works and modifications of the Materials that are made by you, you are and will be the owner of such derivative works and modifications.\n    b. No trademark license is granted to use the trade names, trademarks, service marks, or product names of Us, except as required to fulfill notice requirements under this Agreement or as required for reasonable and customary use in describing and redistributing the Materials.\n    c. If you commence a lawsuit or other proceedings (including a cross-claim or counterclaim in a lawsuit) against Us or any entity alleging that the Materials or any output therefrom, or any part of the foregoing, infringe any intellectual property or other right owned or licensable by you, then all licences granted to you under this Agreement shall terminate as of the date such lawsuit or other proceeding is commenced or brought.\n\n7. Disclaimer of Warranty and Limitation of Liability\n\n    a. We are not obligated to support, update, provide training for, or develop any further version of the Tongyi Qianwen Materials or to grant any license thereto.\n    b. THE MATERIALS ARE PROVIDED \"AS IS\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, OR FITNESS FOR A PARTICULAR PURPOSE. WE MAKE NO WARRANTY AND ASSUME NO RESPONSIBILITY FOR THE SAFETY OR STABILITY OF THE MATERIALS AND ANY OUTPUT THEREFROM.\n    c. IN NO EVENT SHALL WE BE LIABLE TO YOU FOR ANY DAMAGES, INCLUDING, BUT NOT LIMITED TO ANY DIRECT, OR INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES ARISING FROM YOUR USE OR INABILITY TO USE THE MATERIALS OR ANY OUTPUT OF IT, NO MATTER HOW IT’S CAUSED.\n    d. You will defend, indemnify and hold harmless Us from and against any claim by any third party arising out of or related to your use or distribution of the Materials.\n\n8. Survival and Termination.\n    a. The term of this Agreement shall commence upon your acceptance of this Agreement or access to the Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein.\n    b. We may terminate this Agreement if you breach any of the terms or conditions of this Agreement. Upon termination of this Agreement, you must delete and cease use of the Materials. Sections 7 and 9 shall survive the termination of this Agreement.\n\n9. Governing Law and Jurisdiction.\n    a. This Agreement and any dispute arising out of or relating to it will be governed by the laws of China, without regard to conflict of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement.\n    b. The People's Courts in Hangzhou City shall have exclusive jurisdiction over any dispute arising out of this Agreement."
        },
        {
          "name": "Tongyi Qianwen RESEARCH LICENSE AGREEMENT",
          "type": "blob",
          "size": 7.111328125,
          "content": "Tongyi Qianwen RESEARCH LICENSE AGREEMENT\n\nTongyi Qianwen Release Date: November 30, 2023\n\nBy clicking to agree or by using or distributing any portion or element of the Tongyi Qianwen Materials, you will be deemed to have recognized and accepted the content of this Agreement, which is effective immediately.\n\n1. Definitions\n    a. This Tongyi Qianwen RESEARCH LICENSE AGREEMENT (this \"Agreement\") shall mean the terms and conditions for use, reproduction, distribution and modification of the Materials as defined by this Agreement.\n    b. \"We\"(or \"Us\") shall mean Alibaba Cloud.\n    c. \"You\" (or \"Your\") shall mean a natural person or legal entity exercising the rights granted by this Agreement and/or using the Materials for any purpose and in any field of use.\n    d. \"Third Parties\" shall mean individuals or legal entities that are not under common control with Us or You.\n    e. \"Tongyi Qianwen\" shall mean the large language models, and software and algorithms, consisting of trained model weights, parameters (including optimizer states), machine-learning model code, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Us.\n    f. \"Materials\" shall mean, collectively, Alibaba Cloud's proprietary Tongyi Qianwen and Documentation (and any portion thereof) made available under this Agreement.\n    g. \"Source\" form shall mean the preferred form for making modifications, including but not limited to model source code, documentation source, and configuration files.\n    h. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation,\n and conversions to other media types.\n    i. \"Non-Commercial\" shall mean for research or evaluation purposes only.\n\n2. Grant of Rights\n    a. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Alibaba Cloud's intellectual property or other rights owned by Us embodied in the Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Materials FOR NON-COMMERCIAL PURPOSES ONLY.\n    b. If you are commercially using the Materials, You shall request a license from Us.\n\n3. Redistribution\nYou may reproduce and distribute copies of the Materials or derivative works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n    a. You shall give any other recipients of the Materials or derivative works a copy of this Agreement;\n    b. You shall cause any modified files to carry prominent notices stating that You changed the files;\n    c. You shall retain in all copies of the Materials that You distribute the following attribution notices within a \"Notice\" text file distributed as a part of such copies: \"Tongyi Qianwen is licensed under the Tongyi Qianwen RESEARCH LICENSE AGREEMENT, Copyright (c) Alibaba Cloud. All Rights Reserved.\"; and\n    d. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such derivative works as a whole, provided Your use, reproduction, and distribution of the work otherwise complies with the terms and conditions of this Agreement.\n\n4. Rules of use\n    a. The Materials may be subject to export controls or restrictions in China, the United States or other countries or regions. You shall comply with applicable laws and regulations in your use of the Materials.\n    b. You can not use the Materials or any output therefrom to improve any other large language model (excluding Tongyi Qianwen or derivative works thereof).\n\n5. Intellectual Property\n    a. We retain ownership of all intellectual property rights in and to the Materials and derivatives made by or for Us. Conditioned upon compliance with the terms and conditions of this Agreement, with respect to any derivative works and modifications of the Materials that are made by you, you are and will be the owner of such derivative works and modifications.\n    b. No trademark license is granted to use the trade names, trademarks, service marks, or product names of Us, except as required to fulfill notice requirements under this Agreement or as required for reasonable and customary use in describing and redistributing the Materials.\n    c. If you commence a lawsuit or other proceedings (including a cross-claim or counterclaim in a lawsuit) against Us or any entity alleging that the Materials or any output therefrom, or any part of the foregoing, infringe any intellectual property or other right owned or licensable by you, then all licences granted to you under this Agreement shall terminate as of the date such lawsuit or other proceeding is commenced or brought.\n\n6. Disclaimer of Warranty and Limitation of Liability\n    a. We are not obligated to support, update, provide training for, or develop any further version of the Tongyi Qianwen Materials or to grant any license thereto.\n    b. THE MATERIALS ARE PROVIDED \"AS IS\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, OR FITNESS FOR A PARTICULAR PURPOSE. WE MAKE NO WARRANTY AND ASSUME NO RESPONSIBILITY FOR THE SAFETY OR STABILITY OF THE MATERIALS AND ANY OUTPUT THEREFROM.\n    c. IN NO EVENT SHALL WE BE LIABLE TO YOU FOR ANY DAMAGES, INCLUDING, BUT NOT LIMITED TO ANY DIRECT, OR INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES ARISING FROM YOUR USE OR INABILITY TO USE THE MATERIALS OR ANY OUTPUT OF IT, NO MATTER HOW IT’S CAUSED.\n    d. You will defend, indemnify and hold harmless Us from and against any claim by any third party arising out of or related to your use or distribution of the Materials.\n\n7. Survival and Termination.\n    a. The term of this Agreement shall commence upon your acceptance of this Agreement or access to the Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein.\n    b. We may terminate this Agreement if you breach any of the terms or conditions of this Agreement. Upon termination of this Agreement, you must delete and cease use of the Materials. Sections 6 and 8 shall survive the termination of this Agreement.\n\n8. Governing Law and Jurisdiction.\n    a. This Agreement and any dispute arising out of or relating to it will be governed by the laws of China, without regard to conflict of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement.\n    b. The People's Courts in Hangzhou City shall have exclusive jurisdiction over any dispute arising out of this Agreement.\n\n9. Other Terms and Conditions.\n    a. Any arrangements, understandings, or agreements regarding the Material not stated herein are separate from and independent of the terms and conditions of this Agreement. You shall request a seperate license from Us, if You use the Materials in ways not expressly agreed to in this Agreement.\n    b. We shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\n"
        },
        {
          "name": "ascend-support",
          "type": "tree",
          "content": null
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cli_demo.py",
          "type": "blob",
          "size": 7.3759765625,
          "content": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"A simple command-line interactive chat demo.\"\"\"\n\nimport argparse\nimport os\nimport platform\nimport shutil\nfrom copy import deepcopy\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nfrom transformers.trainer_utils import set_seed\n\nDEFAULT_CKPT_PATH = 'Qwen/Qwen-7B-Chat'\n\n_WELCOME_MSG = '''\\\nWelcome to use Qwen-Chat model, type text to start chat, type :h to show command help.\n(欢迎使用 Qwen-Chat 模型，输入内容即可进行对话，:h 显示命令帮助。)\n\nNote: This demo is governed by the original license of Qwen.\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, including hate speech, violence, pornography, deception, etc.\n(注：本演示受Qwen的许可协议限制。我们强烈建议，用户不应传播及不应允许他人传播以下内容，包括但不限于仇恨言论、暴力、色情、欺诈相关的有害信息。)\n'''\n_HELP_MSG = '''\\\nCommands:\n    :help / :h          Show this help message              显示帮助信息\n    :exit / :quit / :q  Exit the demo                       退出Demo\n    :clear / :cl        Clear screen                        清屏\n    :clear-his / :clh   Clear history                       清除对话历史\n    :history / :his     Show history                        显示对话历史\n    :seed               Show current random seed            显示当前随机种子\n    :seed <N>           Set random seed to <N>              设置随机种子\n    :conf               Show current generation config      显示生成配置\n    :conf <key>=<value> Change generation config            修改生成配置\n    :reset-conf         Reset generation config             重置生成配置\n'''\n\n\ndef _load_model_tokenizer(args):\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"auto\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    ).eval()\n\n    config = GenerationConfig.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True,\n    )\n\n    return model, tokenizer, config\n\n\ndef _gc():\n    import gc\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef _clear_screen():\n    if platform.system() == \"Windows\":\n        os.system(\"cls\")\n    else:\n        os.system(\"clear\")\n\n\ndef _print_history(history):\n    terminal_width = shutil.get_terminal_size()[0]\n    print(f'History ({len(history)})'.center(terminal_width, '='))\n    for index, (query, response) in enumerate(history):\n        print(f'User[{index}]: {query}')\n        print(f'QWen[{index}]: {response}')\n    print('=' * terminal_width)\n\n\ndef _get_input() -> str:\n    while True:\n        try:\n            message = input('User> ').strip()\n        except UnicodeDecodeError:\n            print('[ERROR] Encoding error in input')\n            continue\n        except KeyboardInterrupt:\n            exit(1)\n        if message:\n            return message\n        print('[ERROR] Query is empty')\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='QWen-Chat command-line interactive chat demo.')\n    parser.add_argument(\"-c\", \"--checkpoint-path\", type=str, default=DEFAULT_CKPT_PATH,\n                        help=\"Checkpoint name or path, default to %(default)r\")\n    parser.add_argument(\"-s\", \"--seed\", type=int, default=1234, help=\"Random seed\")\n    parser.add_argument(\"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\")\n    args = parser.parse_args()\n\n    history, response = [], ''\n\n    model, tokenizer, config = _load_model_tokenizer(args)\n    orig_gen_config = deepcopy(model.generation_config)\n\n    _clear_screen()\n    print(_WELCOME_MSG)\n\n    seed = args.seed\n\n    while True:\n        query = _get_input()\n\n        # Process commands.\n        if query.startswith(':'):\n            command_words = query[1:].strip().split()\n            if not command_words:\n                command = ''\n            else:\n                command = command_words[0]\n\n            if command in ['exit', 'quit', 'q']:\n                break\n            elif command in ['clear', 'cl']:\n                _clear_screen()\n                print(_WELCOME_MSG)\n                _gc()\n                continue\n            elif command in ['clear-his', 'clh']:\n                print(f'[INFO] All {len(history)} history cleared')\n                history.clear()\n                _gc()\n                continue\n            elif command in ['help', 'h']:\n                print(_HELP_MSG)\n                continue\n            elif command in ['history', 'his']:\n                _print_history(history)\n                continue\n            elif command in ['seed']:\n                if len(command_words) == 1:\n                    print(f'[INFO] Current random seed: {seed}')\n                    continue\n                else:\n                    new_seed_s = command_words[1]\n                    try:\n                        new_seed = int(new_seed_s)\n                    except ValueError:\n                        print(f'[WARNING] Fail to change random seed: {new_seed_s!r} is not a valid number')\n                    else:\n                        print(f'[INFO] Random seed changed to {new_seed}')\n                        seed = new_seed\n                    continue\n            elif command in ['conf']:\n                if len(command_words) == 1:\n                    print(model.generation_config)\n                else:\n                    for key_value_pairs_str in command_words[1:]:\n                        eq_idx = key_value_pairs_str.find('=')\n                        if eq_idx == -1:\n                            print('[WARNING] format: <key>=<value>')\n                            continue\n                        conf_key, conf_value_str = key_value_pairs_str[:eq_idx], key_value_pairs_str[eq_idx + 1:]\n                        try:\n                            conf_value = eval(conf_value_str)\n                        except Exception as e:\n                            print(e)\n                            continue\n                        else:\n                            print(f'[INFO] Change config: model.generation_config.{conf_key} = {conf_value}')\n                            setattr(model.generation_config, conf_key, conf_value)\n                continue\n            elif command in ['reset-conf']:\n                print('[INFO] Reset generation config')\n                model.generation_config = deepcopy(orig_gen_config)\n                print(model.generation_config)\n                continue\n            else:\n                # As normal query.\n                pass\n\n        # Run chat.\n        set_seed(seed)\n        try:\n            for response in model.chat_stream(tokenizer, query, history=history, generation_config=config):\n                _clear_screen()\n                print(f\"\\nUser: {query}\")\n                print(f\"\\nQwen-Chat: {response}\")\n        except KeyboardInterrupt:\n            print('[WARNING] Generation interrupted')\n            continue\n\n        history.append((query, response))\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "dcu-support",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune.py",
          "type": "blob",
          "size": 12.1982421875,
          "content": "# This code is based on the revised code from fastchat based on tatsu-lab/stanford_alpaca.\n\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport logging\nimport os\nfrom typing import Dict, Optional, List\nimport torch\nfrom torch.utils.data import Dataset\nfrom deepspeed import zero\nfrom deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\nimport transformers\nfrom transformers import Trainer, GPTQConfig, deepspeed\nfrom transformers.trainer_pt_utils import LabelSmoother\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom accelerate.utils import DistributedType\n\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"Qwen/Qwen-7B\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    eval_data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=8192,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n    use_lora: bool = False\n\n\n@dataclass\nclass LoraArguments:\n    lora_r: int = 64\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_target_modules: List[str] = field(\n        default_factory=lambda: [\"c_attn\", \"c_proj\", \"w1\", \"w2\"]\n    )\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n    q_lora: bool = False\n\n\ndef maybe_zero_3(param):\n    if hasattr(param, \"ds_id\"):\n        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\n# Borrowed from peft.utils.get_peft_model_state_dict\ndef get_peft_state_maybe_zero_3(named_params, bias):\n    if bias == \"none\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n    elif bias == \"all\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n    elif bias == \"lora_only\":\n        to_return = {}\n        maybe_lora_bias = {}\n        lora_bias_names = set()\n        for k, t in named_params:\n            if \"lora_\" in k:\n                to_return[k] = t\n                bias_name = k.split(\"lora_\")[0] + \"bias\"\n                lora_bias_names.add(bias_name)\n            elif \"bias\" in k:\n                maybe_lora_bias[k] = t\n        for k, t in maybe_lora_bias:\n            if bias_name in lora_bias_names:\n                to_return[bias_name] = t\n    else:\n        raise NotImplementedError\n    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n    return to_return\n\n\nlocal_rank = None\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str, bias=\"none\"):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    # check if zero3 mode enabled\n    if deepspeed.is_deepspeed_zero3_enabled():\n        state_dict = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    else:\n        if trainer.args.use_lora:\n            state_dict = get_peft_state_maybe_zero_3(\n                trainer.model.named_parameters(), bias\n            )\n        else:\n            state_dict = trainer.model.state_dict()\n    if trainer.args.should_save and trainer.args.local_rank == 0:\n        trainer._save(output_dir, state_dict=state_dict)\n\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n    max_len: int,\n    system_message: str = \"You are a helpful assistant.\"\n) -> Dict:\n    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n\n    im_start = tokenizer.im_start_id\n    im_end = tokenizer.im_end_id\n    nl_tokens = tokenizer('\\n').input_ids\n    _system = tokenizer('system').input_ids + nl_tokens\n    _user = tokenizer('user').input_ids + nl_tokens\n    _assistant = tokenizer('assistant').input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != roles[\"user\"]:\n            source = source[1:]\n\n        input_id, target = [], []\n        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n        input_id += system\n        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n        assert len(input_id) == len(target)\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n                tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n            input_id += _input_id\n            if role == '<|im_start|>user':\n                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n            elif role == '<|im_start|>assistant':\n                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n            else:\n                raise NotImplementedError\n            target += _target\n        assert len(input_id) == len(target)\n        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n        input_ids.append(input_id[:max_len])\n        targets.append(target[:max_len])\n    input_ids = torch.tensor(input_ids, dtype=torch.int)\n    targets = torch.tensor(targets, dtype=torch.int)\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in raw_data]\n        data_dict = preprocess(sources, tokenizer, max_len)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer, self.max_len)\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args, max_len,\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n\n    train_json = json.load(open(data_args.data_path, \"r\"))\n    train_dataset = dataset_cls(train_json, tokenizer=tokenizer, max_len=max_len)\n\n    if data_args.eval_data_path:\n        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer, max_len=max_len)\n    else:\n        eval_dataset = None\n\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n    )\n    (\n        model_args,\n        data_args,\n        training_args,\n        lora_args,\n    ) = parser.parse_args_into_dataclasses()\n\n    # This serves for single-gpu qlora.\n    if getattr(training_args, 'deepspeed', None) and int(os.environ.get(\"WORLD_SIZE\", 1))==1:\n        training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED\n\n    local_rank = training_args.local_rank\n\n    device_map = None\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if lora_args.q_lora:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else \"auto\"\n        if len(training_args.fsdp) > 0 or deepspeed.is_deepspeed_zero3_enabled():\n            logging.warning(\n                \"FSDP or ZeRO3 are incompatible with QLoRA.\"\n            )\n\n    is_chat_model = 'chat' in model_args.model_name_or_path.lower()\n    if (\n            training_args.use_lora\n            and not lora_args.q_lora\n            and deepspeed.is_deepspeed_zero3_enabled()\n            and not is_chat_model\n    ):\n        raise RuntimeError(\"ZeRO3 is incompatible with LoRA when finetuning on base model.\")\n\n    model_load_kwargs = {\n        'low_cpu_mem_usage': not deepspeed.is_deepspeed_zero3_enabled(),\n    }\n\n    # Set RoPE scaling factor\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=True,\n    )\n    config.use_cache = False\n\n    # Load model and tokenizer\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=training_args.cache_dir,\n        device_map=device_map,\n        trust_remote_code=True,\n        quantization_config=GPTQConfig(\n            bits=4, disable_exllama=True\n        )\n        if training_args.use_lora and lora_args.q_lora\n        else None,\n        **model_load_kwargs,\n    )\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n        trust_remote_code=True,\n    )\n    tokenizer.pad_token_id = tokenizer.eod_id\n\n    if training_args.use_lora:\n        if lora_args.q_lora or is_chat_model:\n            modules_to_save = None\n        else:\n            modules_to_save = [\"wte\", \"lm_head\"]\n        lora_config = LoraConfig(\n            r=lora_args.lora_r,\n            lora_alpha=lora_args.lora_alpha,\n            target_modules=lora_args.lora_target_modules,\n            lora_dropout=lora_args.lora_dropout,\n            bias=lora_args.lora_bias,\n            task_type=\"CAUSAL_LM\",\n            modules_to_save=modules_to_save  # This argument serves for adding new tokens.\n        )\n        if lora_args.q_lora:\n            model = prepare_model_for_kbit_training(\n                model, use_gradient_checkpointing=training_args.gradient_checkpointing\n            )\n\n        model = get_peft_model(model, lora_config)\n\n        # Print peft trainable params\n        model.print_trainable_parameters()\n\n        if training_args.gradient_checkpointing:\n            model.enable_input_require_grads()\n\n    # Load data\n    data_module = make_supervised_data_module(\n        tokenizer=tokenizer, data_args=data_args, max_len=training_args.model_max_length\n    )\n\n    # Start trainner\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    trainer.train()\n    trainer.save_state()\n\n    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir, bias=lora_args.lora_bias)\n\n\nif __name__ == \"__main__\":\n    train()\n"
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_api.py",
          "type": "blob",
          "size": 20.6025390625,
          "content": "# Requirement:\n#   pip install \"openai<1.0\"\n# Usage:\n#   python openai_api.py\n# Visit http://localhost:8000/docs for documents.\n\nimport base64\nimport copy\nimport json\nimport time\nfrom argparse import ArgumentParser\nfrom contextlib import asynccontextmanager\nfrom pprint import pprint\nfrom typing import Dict, List, Literal, Optional, Union\n\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom sse_starlette.sse import EventSourceResponse\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n\nclass BasicAuthMiddleware(BaseHTTPMiddleware):\n\n    def __init__(self, app, username: str, password: str):\n        super().__init__(app)\n        self.required_credentials = base64.b64encode(\n            f'{username}:{password}'.encode()).decode()\n\n    async def dispatch(self, request: Request, call_next):\n        authorization: str = request.headers.get('Authorization')\n        if authorization:\n            try:\n                schema, credentials = authorization.split()\n                if credentials == self.required_credentials:\n                    return await call_next(request)\n            except ValueError:\n                pass\n\n        headers = {'WWW-Authenticate': 'Basic'}\n        return Response(status_code=401, headers=headers)\n\n\ndef _gc(forced: bool = False):\n    global args\n    if args.disable_gc and not forced:\n        return\n\n    import gc\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):  # collects GPU memory\n    yield\n    _gc(forced=True)\n\n\napp = FastAPI(lifespan=lifespan)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'],\n    allow_headers=['*'],\n)\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = 'model'\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = 'owner'\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None\n\n\nclass ModelList(BaseModel):\n    object: str = 'list'\n    data: List[ModelCard] = []\n\n\nclass ChatMessage(BaseModel):\n    role: Literal['user', 'assistant', 'system', 'function']\n    content: Optional[str]\n    function_call: Optional[Dict] = None\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal['user', 'assistant', 'system']] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessage]\n    functions: Optional[List[Dict]] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    top_k: Optional[int] = None\n    max_length: Optional[int] = None\n    stream: Optional[bool] = False\n    stop: Optional[List[str]] = None\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: Union[ChatMessage]\n    finish_reason: Literal['stop', 'length', 'function_call']\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal['stop', 'length']]\n\n\nclass ChatCompletionResponse(BaseModel):\n    model: str\n    object: Literal['chat.completion', 'chat.completion.chunk']\n    choices: List[Union[ChatCompletionResponseChoice,\n                        ChatCompletionResponseStreamChoice]]\n    created: Optional[int] = Field(default_factory=lambda: int(time.time()))\n\n\n@app.get('/v1/models', response_model=ModelList)\nasync def list_models():\n    global model_args\n    model_card = ModelCard(id='gpt-3.5-turbo')\n    return ModelList(data=[model_card])\n\n\n# To work around that unpleasant leading-\\n tokenization issue!\ndef add_extra_stop_words(stop_words):\n    if stop_words:\n        _stop_words = []\n        _stop_words.extend(stop_words)\n        for x in stop_words:\n            s = x.lstrip('\\n')\n            if s and (s not in _stop_words):\n                _stop_words.append(s)\n        return _stop_words\n    return stop_words\n\n\ndef trim_stop_words(response, stop_words):\n    if stop_words:\n        for stop in stop_words:\n            idx = response.find(stop)\n            if idx != -1:\n                response = response[:idx]\n    return response\n\n\nTOOL_DESC = (\n    '{name_for_model}: Call this tool to interact with the {name_for_human} API.'\n    ' What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}'\n)\n\nREACT_INSTRUCTION = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n\n{tools_text}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tools_name_text}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\"\"\"\n\n_TEXT_COMPLETION_CMD = object()\n\n\ndef parse_messages(messages, functions):\n    if all(m.role != 'user' for m in messages):\n        raise HTTPException(\n            status_code=400,\n            detail='Invalid request: Expecting at least one user message.',\n        )\n\n    messages = copy.deepcopy(messages)\n    if messages[0].role == 'system':\n        system = messages.pop(0).content.lstrip('\\n').rstrip()\n    else:\n        system = 'You are a helpful assistant.'\n\n    if functions:\n        tools_text = []\n        tools_name_text = []\n        for func_info in functions:\n            name = func_info.get('name', '')\n            name_m = func_info.get('name_for_model', name)\n            name_h = func_info.get('name_for_human', name)\n            desc = func_info.get('description', '')\n            desc_m = func_info.get('description_for_model', desc)\n            tool = TOOL_DESC.format(\n                name_for_model=name_m,\n                name_for_human=name_h,\n                # Hint: You can add the following format requirements in description:\n                #   \"Format the arguments as a JSON object.\"\n                #   \"Enclose the code within triple backticks (`) at the beginning and end of the code.\"\n                description_for_model=desc_m,\n                parameters=json.dumps(func_info['parameters'],\n                                      ensure_ascii=False),\n            )\n            tools_text.append(tool)\n            tools_name_text.append(name_m)\n        tools_text = '\\n\\n'.join(tools_text)\n        tools_name_text = ', '.join(tools_name_text)\n        instruction = (REACT_INSTRUCTION.format(\n            tools_text=tools_text,\n            tools_name_text=tools_name_text,\n        ).lstrip('\\n').rstrip())\n    else:\n        instruction = ''\n\n    messages_with_fncall = messages\n    messages = []\n    for m_idx, m in enumerate(messages_with_fncall):\n        role, content, func_call = m.role, m.content, m.function_call\n        content = content or ''\n        content = content.lstrip('\\n').rstrip()\n        if role == 'function':\n            if (len(messages) == 0) or (messages[-1].role != 'assistant'):\n                raise HTTPException(\n                    status_code=400,\n                    detail=\n                    'Invalid request: Expecting role assistant before role function.',\n                )\n            messages[-1].content += f'\\nObservation: {content}'\n            if m_idx == len(messages_with_fncall) - 1:\n                # add a prefix for text completion\n                messages[-1].content += '\\nThought:'\n        elif role == 'assistant':\n            if len(messages) == 0:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\n                    'Invalid request: Expecting role user before role assistant.',\n                )\n            if func_call is None:\n                if functions:\n                    content = f'Thought: I now know the final answer.\\nFinal Answer: {content}'\n            else:\n                f_name, f_args = func_call['name'], func_call['arguments']\n                if not content.startswith('Thought:'):\n                    content = f'Thought: {content}'\n                content = f'{content}\\nAction: {f_name}\\nAction Input: {f_args}'\n            if messages[-1].role == 'user':\n                messages.append(\n                    ChatMessage(role='assistant',\n                                content=content.lstrip('\\n').rstrip()))\n            else:\n                messages[-1].content += '\\n' + content\n        elif role == 'user':\n            messages.append(\n                ChatMessage(role='user',\n                            content=content.lstrip('\\n').rstrip()))\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=f'Invalid request: Incorrect role {role}.')\n\n    query = _TEXT_COMPLETION_CMD\n    if messages[-1].role == 'user':\n        query = messages[-1].content\n        messages = messages[:-1]\n\n    if len(messages) % 2 != 0:\n        raise HTTPException(status_code=400, detail='Invalid request')\n\n    history = []  # [(Q1, A1), (Q2, A2), ..., (Q_last_turn, A_last_turn)]\n    for i in range(0, len(messages), 2):\n        if messages[i].role == 'user' and messages[i + 1].role == 'assistant':\n            usr_msg = messages[i].content.lstrip('\\n').rstrip()\n            bot_msg = messages[i + 1].content.lstrip('\\n').rstrip()\n            if instruction and (i == len(messages) - 2):\n                usr_msg = f'{instruction}\\n\\nQuestion: {usr_msg}'\n                instruction = ''\n            history.append([usr_msg, bot_msg])\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=\n                'Invalid request: Expecting exactly one user (or function) role before every assistant role.',\n            )\n    if instruction:\n        assert query is not _TEXT_COMPLETION_CMD\n        query = f'{instruction}\\n\\nQuestion: {query}'\n    return query, history, system\n\n\ndef parse_response(response):\n    func_name, func_args = '', ''\n    i = response.find('\\nAction:')\n    j = response.find('\\nAction Input:')\n    k = response.find('\\nObservation:')\n    if 0 <= i < j:  # If the text has `Action` and `Action input`,\n        if k < j:  # but does not contain `Observation`,\n            # then it is likely that `Observation` is omitted by the LLM,\n            # because the output text may have discarded the stop word.\n            response = response.rstrip() + '\\nObservation:'  # Add it back.\n        k = response.find('\\nObservation:')\n        func_name = response[i + len('\\nAction:'):j].strip()\n        func_args = response[j + len('\\nAction Input:'):k].strip()\n\n    if func_name:\n        response = response[:i]\n        t = response.find('Thought: ')\n        if t >= 0:\n            response = response[t + len('Thought: '):]\n        response = response.strip()\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(\n                role='assistant',\n                content=response,\n                function_call={\n                    'name': func_name,\n                    'arguments': func_args\n                },\n            ),\n            finish_reason='function_call',\n        )\n        return choice_data\n\n    z = response.rfind('\\nFinal Answer: ')\n    if z >= 0:\n        response = response[z + len('\\nFinal Answer: '):]\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=ChatMessage(role='assistant', content=response),\n        finish_reason='stop',\n    )\n    return choice_data\n\n\n# completion mode, not chat mode\ndef text_complete_last_message(history, stop_words_ids, gen_kwargs, system):\n    im_start = '<|im_start|>'\n    im_end = '<|im_end|>'\n    prompt = f'{im_start}system\\n{system}{im_end}'\n    for i, (query, response) in enumerate(history):\n        query = query.lstrip('\\n').rstrip()\n        response = response.lstrip('\\n').rstrip()\n        prompt += f'\\n{im_start}user\\n{query}{im_end}'\n        prompt += f'\\n{im_start}assistant\\n{response}{im_end}'\n    prompt = prompt[:-len(im_end)]\n\n    _stop_words_ids = [tokenizer.encode(im_end)]\n    if stop_words_ids:\n        for s in stop_words_ids:\n            _stop_words_ids.append(s)\n    stop_words_ids = _stop_words_ids\n\n    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(model.device)\n    output = model.generate(input_ids,\n                            stop_words_ids=stop_words_ids,\n                            **gen_kwargs).tolist()[0]\n    output = tokenizer.decode(output, errors='ignore')\n    assert output.startswith(prompt)\n    output = output[len(prompt):]\n    output = trim_stop_words(output, ['<|endoftext|>', im_end])\n    print(f'<completion>\\n{prompt}\\n<!-- *** -->\\n{output}\\n</completion>')\n    return output\n\n\n@app.post('/v1/chat/completions', response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    global model, tokenizer\n\n    gen_kwargs = {}\n    if request.top_k is not None:\n        gen_kwargs['top_k'] = request.top_k\n    if request.temperature is not None:\n        if request.temperature < 0.01:\n            gen_kwargs['top_k'] = 1  # greedy decoding\n        else:\n            # Not recommended. Please tune top_p instead.\n            gen_kwargs['temperature'] = request.temperature\n    if request.top_p is not None:\n        gen_kwargs['top_p'] = request.top_p\n\n    stop_words = add_extra_stop_words(request.stop)\n    if request.functions:\n        stop_words = stop_words or []\n        if 'Observation:' not in stop_words:\n            stop_words.append('Observation:')\n\n    query, history, system = parse_messages(request.messages,\n                                            request.functions)\n\n    if request.stream:\n        if request.functions:\n            raise HTTPException(\n                status_code=400,\n                detail=\n                'Invalid request: Function calling is not yet implemented for stream mode.',\n            )\n        generate = predict(query,\n                           history,\n                           request.model,\n                           stop_words,\n                           gen_kwargs,\n                           system=system)\n        return EventSourceResponse(generate, media_type='text/event-stream')\n\n    stop_words_ids = [tokenizer.encode(s)\n                      for s in stop_words] if stop_words else None\n    if query is _TEXT_COMPLETION_CMD:\n        response = text_complete_last_message(history,\n                                              stop_words_ids=stop_words_ids,\n                                              gen_kwargs=gen_kwargs,\n                                              system=system)\n    else:\n        response, _ = model.chat(\n            tokenizer,\n            query,\n            history=history,\n            system=system,\n            stop_words_ids=stop_words_ids,\n            **gen_kwargs,\n        )\n        print('<chat>')\n        pprint(history, indent=2)\n        print(f'{query}\\n<!-- *** -->\\n{response}\\n</chat>')\n    _gc()\n\n    response = trim_stop_words(response, stop_words)\n    if request.functions:\n        choice_data = parse_response(response)\n    else:\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(role='assistant', content=response),\n            finish_reason='stop',\n        )\n    return ChatCompletionResponse(model=request.model,\n                                  choices=[choice_data],\n                                  object='chat.completion')\n\n\ndef _dump_json(data: BaseModel, *args, **kwargs) -> str:\n    try:\n        return data.model_dump_json(*args, **kwargs)\n    except AttributeError:  # pydantic<2.0.0\n        return data.json(*args, **kwargs)  # noqa\n\n\nasync def predict(\n    query: str,\n    history: List[List[str]],\n    model_id: str,\n    stop_words: List[str],\n    gen_kwargs: Dict,\n    system: str,\n):\n    global model, tokenizer\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(role='assistant'), finish_reason=None)\n    chunk = ChatCompletionResponse(model=model_id,\n                                   choices=[choice_data],\n                                   object='chat.completion.chunk')\n    yield '{}'.format(_dump_json(chunk, exclude_unset=True))\n\n    current_length = 0\n    stop_words_ids = [tokenizer.encode(s)\n                      for s in stop_words] if stop_words else None\n\n    delay_token_num = max([len(x) for x in stop_words]) if stop_words_ids else 0\n    response_generator = model.chat_stream(tokenizer,\n                                           query,\n                                           history=history,\n                                           stop_words_ids=stop_words_ids,\n                                           system=system,\n                                           **gen_kwargs)\n    for _new_response in response_generator:\n        if len(_new_response) <= delay_token_num:\n            continue\n        new_response = _new_response[:-delay_token_num] if delay_token_num else _new_response\n\n        if len(new_response) == current_length:\n            continue\n\n        new_text = new_response[current_length:]\n        current_length = len(new_response)\n\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0, delta=DeltaMessage(content=new_text), finish_reason=None)\n        chunk = ChatCompletionResponse(model=model_id,\n                                       choices=[choice_data],\n                                       object='chat.completion.chunk')\n        yield '{}'.format(_dump_json(chunk, exclude_unset=True))\n    \n    if current_length != len(_new_response):\n        # Determine whether to print the delay tokens\n        delayed_text = _new_response[current_length:]\n        new_text = trim_stop_words(delayed_text, stop_words)\n        if len(new_text) > 0:\n            choice_data = ChatCompletionResponseStreamChoice(\n                index=0, delta=DeltaMessage(content=new_text), finish_reason=None)\n            chunk = ChatCompletionResponse(model=model_id,\n                                        choices=[choice_data],\n                                        object='chat.completion.chunk')\n            yield '{}'.format(_dump_json(chunk, exclude_unset=True))\n\n    choice_data = ChatCompletionResponseStreamChoice(index=0,\n                                                     delta=DeltaMessage(),\n                                                     finish_reason='stop')\n    chunk = ChatCompletionResponse(model=model_id,\n                                   choices=[choice_data],\n                                   object='chat.completion.chunk')\n    yield '{}'.format(_dump_json(chunk, exclude_unset=True))\n    yield '[DONE]'\n\n    _gc()\n\n\ndef _get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\n        '-c',\n        '--checkpoint-path',\n        type=str,\n        default='Qwen/Qwen-7B-Chat',\n        help='Checkpoint name or path, default to %(default)r',\n    )\n    parser.add_argument('--api-auth', help='API authentication credentials')\n    parser.add_argument('--cpu-only',\n                        action='store_true',\n                        help='Run demo with CPU only')\n    parser.add_argument('--server-port',\n                        type=int,\n                        default=8000,\n                        help='Demo server port.')\n    parser.add_argument(\n        '--server-name',\n        type=str,\n        default='127.0.0.1',\n        help=\n        'Demo server name. Default: 127.0.0.1, which is only visible from the local computer.'\n        ' If you want other computers to access your server, use 0.0.0.0 instead.',\n    )\n    parser.add_argument(\n        '--disable-gc',\n        action='store_true',\n        help='Disable GC after each response generated.',\n    )\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == '__main__':\n    args = _get_args()\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    if args.api_auth:\n        app.add_middleware(BasicAuthMiddleware,\n                           username=args.api_auth.split(':')[0],\n                           password=args.api_auth.split(':')[1])\n\n    if args.cpu_only:\n        device_map = 'cpu'\n    else:\n        device_map = 'auto'\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    ).eval()\n\n    model.generation_config = GenerationConfig.from_pretrained(\n        args.checkpoint_path,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    uvicorn.run(app, host=args.server_name, port=args.server_port, workers=1)\n"
        },
        {
          "name": "recipes",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0966796875,
          "content": "transformers>=4.32.0,<4.38.0\naccelerate\ntiktoken\neinops\ntransformers_stream_generator==0.0.4\nscipy\n"
        },
        {
          "name": "requirements_web_demo.txt",
          "type": "blob",
          "size": 0.0224609375,
          "content": "gradio<3.42\nmdtex2html\n"
        },
        {
          "name": "run_gptq.py",
          "type": "blob",
          "size": 4.1630859375,
          "content": "import argparse\r\nimport json\r\nfrom typing import Dict\r\nimport logging\r\n\r\nimport torch\r\nimport transformers\r\nfrom transformers import AutoTokenizer\r\nfrom transformers.trainer_pt_utils import LabelSmoother\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\r\n\r\ndef preprocess(\r\n    sources,\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n    max_len: int,\r\n    system_message: str = \"You are a helpful assistant.\"\r\n) -> Dict:\r\n    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\r\n\r\n    im_start = tokenizer.im_start_id\r\n    im_end = tokenizer.im_end_id\r\n    nl_tokens = tokenizer('\\n').input_ids\r\n    _system = tokenizer('system').input_ids + nl_tokens\r\n    _user = tokenizer('user').input_ids + nl_tokens\r\n    _assistant = tokenizer('assistant').input_ids + nl_tokens\r\n\r\n    # Apply prompt templates\r\n    data = []\r\n    # input_ids, targets = [], []\r\n    for i, source in enumerate(sources):\r\n        source = source[\"conversations\"]\r\n        if roles[source[0][\"from\"]] != roles[\"user\"]:\r\n            source = source[1:]\r\n\r\n        input_id, target = [], []\r\n        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\r\n        input_id += system\r\n        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\r\n        assert len(input_id) == len(target)\r\n        for j, sentence in enumerate(source):\r\n            role = roles[sentence[\"from\"]]\r\n            _input_id = tokenizer(role).input_ids + nl_tokens + \\\r\n                tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\r\n            input_id += _input_id\r\n            if role == '<|im_start|>user':\r\n                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\r\n            elif role == '<|im_start|>assistant':\r\n                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\r\n                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\r\n            else:\r\n                raise NotImplementedError\r\n            target += _target\r\n        assert len(input_id) == len(target)\r\n        input_id = torch.tensor(input_id[:max_len], dtype=torch.int)\r\n        target = torch.tensor(target[:max_len], dtype=torch.int)\r\n        data.append(dict(input_ids=input_id, attention_mask=input_id.ne(tokenizer.pad_token_id)))\r\n\r\n    return data\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser(\"Model Quantization using AutoGPTQ\")\r\n    parser.add_argument(\"--model_name_or_path\", type=str, help=\"model path\")\r\n    parser.add_argument(\"--data_path\", type=str, help=\"calibration data path\")\r\n    parser.add_argument(\"--out_path\", type=str, help=\"output path of the quantized model\")\r\n    parser.add_argument(\"--max_len\", type=int, default=8192, help=\"max length of calibration data\")\r\n    parser.add_argument(\"--bits\", type=int, default=4, help=\"the bits of quantized model. 4 indicates int4 models.\")\r\n    parser.add_argument(\"--group-size\", type=int, default=128, help=\"the group size of quantized model\")\r\n    args = parser.parse_args()\r\n    \r\n    quantize_config = BaseQuantizeConfig(\r\n        bits=args.bits,\r\n        group_size=args.group_size,\r\n        damp_percent=0.01,\r\n        desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\r\n        static_groups=False,\r\n        sym=True,\r\n        true_sequential=True,\r\n        model_name_or_path=None,\r\n        model_file_base_name=\"model\"\r\n    )\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)\r\n    tokenizer.pad_token_id = tokenizer.eod_id\r\n    data = preprocess(json.load(open(args.data_path)), tokenizer, args.max_len)\r\n\r\n    model = AutoGPTQForCausalLM.from_pretrained(args.model_name_or_path, quantize_config, device_map=\"auto\", trust_remote_code=True)\r\n\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\r\n    )\r\n    model.quantize(data, cache_examples_on_gpu=False)\r\n\r\n    model.save_quantized(args.out_path, use_safetensors=True)\r\n    tokenizer.save_pretrained(args.out_path)\r\n"
        },
        {
          "name": "tech_memo.md",
          "type": "blob",
          "size": 21.1982421875,
          "content": "# Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts)\n\nLarge language models have recently attracted an extremely large amount of\nattention.\nThe boom of [ChatGPT](https://openai.com/blog/chatgpt) rocketed the development of artificial general intelligence and indicates that large language models compress world knowledge into neural networks, and the alignment to human cognition can lead to powerful conversational agents that can provide assistance by interacting with human users.\nNow, the latest version of ChatGPT based on [GPT-4](https://arxiv.org/abs/2303.08774) demonstrates tremendously exciting performance across unlimited capabilities, say, language understanding, logical reasoning, planning, etc., and its incorporation with external tools, including tools and models, releases the power of an agent capable of understanding instructions, executing code, using tools, and so on, to reach the objectives set up by human users.\n\nThese significant progresses indicate the importance of large language models as _the foundation of AI services_.\n\nWe are happy to release the 7B-parameter models of our large pretrained model series Qwen (abbr. Tongyi Qianwen), Qwen-7B.\nThis release includes model weights and codes for pretrained and human-aligned language models of 7B parameters:\n\n- `Qwen-7B` is the pretrained language model, and `Qwen-7B-Chat` is fine-tuned to align with human intent.\n- `Qwen-7B` is pretrained on over 2.2 trillion tokens with a context length of 2048. On the series of benchmarks we tested, Qwen-7B generally performs better than existing open models of similar scales and appears to be on par with some of the larger models.\n- `Qwen-7B-Chat` is fine-tuned on curated data, including not only task-oriented data but also specific security- and service-oriented data, which seems insufficient in existing open models.\n- Example codes for fine-tuning, evaluation, and inference are included. There are also guides on long-context and tool use in inference.\n\n**Goal of release**:\nWe believe that while the recent waves of releases of LLMs may have deepened our understanding of model behaviors under standard regimes, it is yet to be revealed how the accompanied techniques of nowadays LLMs, such as 1) quantization and fine-tuning after quantization, 2) training-free long-context inference, and 3) fine-tuning with service-oriented data, including search and tool uses, affect the models as a whole.\nThe open release of Qwen-7B marks our first step towards fully understanding the real-world application of such techniques.\nIt is our hope that it will enable the community to analyze and continue to improve the safety of those models, striving to establish responsible development and deployment of LLMs.\n\n> **Disclaimer**:\n> We must note that even though the weights and codes are released in an open manner and commercial use is not prohibited, similar to other pretrained language models, Qwen-7B comes with potential risks influenced by complex factors, including but not limited to over-diversified, inaccurate, or misleading generation.\n> Developers and stakeholders should perform their own red teaming and provide related security measures before deployment, and they must abide by and comply with local governance and regulations.\n> In no event shall the authors be held liable for any claim, damages, or other liability arising from the use of the released weights or codes.\n\nThe remainder of this document describes our pretraining and fine-tuning methodology.\n\n## Pretraining\n\nQwen-7B is a transformer-based decoder-only language model with an architecture similar to the [LLaMA](https://github.com/facebookresearch/llama) series of models.\nIt is pretrained on over 2.2 trillion tokens with 2048 context length from publicly available data, covering general and professional fields with a focus on the English and Chinese languages.\n\n### Data\n\n**Pretraining data**:\nOur training data includes a mix of data from publicly available sources, consisting mainly of web documents and code files.\nBesides, the data are multilingual, with most of them in English and Chinese.\nWe made an effort and employed an ensemble of models to exclude data of low quality or deemed unfit for pretraining, such as NSFW content.\nFor math reasoning, we include RFT data from [gsm8k-ScRel](https://github.com/OFA-Sys/gsm8k-ScRel).\nThe final data underwent global fuzzy deduplication.\nThe mix of pretraining corpora has been optimized through numerous ablation experiments.\n\n**Tokenization**:\nCompared to the current mainstream open models based on Chinese and English vocabularies, we use a vocabulary of 151,851 tokens.\nIt first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary.\nIt segments numbers by single digits and calls the [tiktoken](https://github.com/openai/tiktoken) tokenizer library for efficient tokenization.\nAfter tokenization, the data amounts to over 2.2 trillion tokens.\n\n<figure>\n    <img src=\"assets/tokenizer.png\"\n         alt=\"Tokenization efficiency\"\n         width=\"1200px\">\n    <figcaption>We randomly selected 1 million document corpora of each language to test and compare the encoding compression rates of different models (with XLM-R, which supports 100 languages, as the base value 1, not shown in the figure). As can be seen, while ensuring the efficient decoding of Chinese, English, and code, Qwen-7B also achieves a high compression rate for many other languages (such as th, he, ar, ko, vi, ja, tr, id, pl, ru, nl, pt, it, de, es, fr etc.), equipping the model with strong scalability as well as high training and inference efficiency in these languages.</figcaption>\n</figure>\n\n### Model\n\n**Model architecture**:\nQwen-7B is built with architecture similar to LLaMA.\nThe following are the main differences from the standard transformer: 1) using untied embedding, 2) using rotary positional embedding, 3) no biases except for QKV in attention, 4) RMSNorm instead of LayerNorm, 5) SwiGLU instead of ReLU, and 6) adopting flash attention to accelerate training.\nThe model has 32 layers, the embedding dimension is 4096, and the number of attention heads is 32.\n\n**Training details**:\nThe model is trained using the AdamW optimizer, with $\\beta_1=0.9, \\beta_2=0.95, \\epsilon=10^{-6}$.\nThe sequence length is 2048, and the batch size is 2048, which means each optimization step accumulates over 4 million tokens.\nWe use a cosine learning rate schedule, with a warm-up of 2000 steps, a peak learning rate of $3 \\times 10^{-4}$, and a minimum learning rate of 10% of the peak learning rate.\nWe use a weight decay of 0.1 and gradient clipping of 1.0.\nThe training adopts mixed precision training with `bfloat16`.\n\n\n### Evaluation\n\nWe report results of Qwen-7B on standard benchmarks.\n\n#### World knowledge\n\n[C-Eval](https://arxiv.org/abs/2305.08322) is a common evaluation benchmark for testing the common-sense capability of pretrained models in Chinese. It covers 52 subjects in four major directions: humanities, social sciences, STEM, and other specialties. According to standard practice, we use the development set samples as the source of few-shot prompts to evaluate the 5-shot validation set and test set accuracy of the Qwen-7B pretrained model.\n\nThe accuracy comparison of the Qwen-7B model and other models on the C-Eval validation set is as follows:\n\n| Model       |  Average |\n| :---------- | -------: |\n| Alpaca-7B   |     28.9 |\n| Vicuna-7B   |     31.2 |\n| ChatGLM-6B  |     37.1 |\n| Baichuan-7B |     42.7 |\n| ChatGLM2-6B |     50.9 |\n| InternLM-7B |     53.4 |\n| ChatGPT     |     53.5 |\n| Claude-v1.3 |     55.5 |\n| **Qwen-7B** | **60.8** |\n\nThe performance comparison of the Qwen-7B pretrained model and other models on the C-Eval test set is shown in the following table:\n\n| Model                   | Avg.     | Avg. (Hard) | STEM | Social Sciences | Humanities | Others |\n| :---------------------- | -------- | ----------: | ---: | --------------: | ---------: | -----: |\n| ChatGLM-6B              | 38.9     |        29.2 | 33.3 |            48.3 |       41.3 |   38.0 |\n| Chinese-Alpaca-Plus-13B | 41.5     |        30.5 | 36.6 |            49.7 |       43.1 |   41.2 |\n| Baichuan-7B             | 42.8     |        31.5 | 38.2 |            52.0 |       46.2 |   39.3 |\n| WestlakeLM-19B          | 44.6     |        34.9 | 41.6 |            51.0 |       44.3 |   44.5 |\n| AndesLM-13B             | 46.0     |        29.7 | 38.1 |            61.0 |       51.0 |   41.9 |\n| BatGPT-15B-sirius       | 47.0     |        31.9 | 42.7 |            57.5 |       48.6 |   43.6 |\n| ChatGLM2-6B             | 51.7     |        37.1 | 48.6 |            60.5 |       51.3 |   49.8 |\n| InternLM-7B             | 52.8     |        37.1 | 48.0 |            67.4 |       55.4 |   45.8 |\n| Baichuan-13B            | 53.6     |        36.7 | 47.0 |            66.8 |       57.3 |   49.8 |\n| Claude-v1.3             | 54.2     |        39.0 | 51.9 |            61.7 |       52.1 |   53.7 |\n| ChatGPT                 | 54.4     |        41.4 | 52.9 |            61.8 |       50.9 |   53.6 |\n| **Qwen-7B**             | **59.6** |        41.0 | 52.8 |            74.1 |       63.1 |   55.2 |\n\nAs can be seen, Qwen-7B achieves the best performance out of all existing models of similar scale and even surpasses larger-scale models.\n\nMMLU is currently one of the most recognized benchmarks for evaluating English comprehension abilities, covering 57 subtasks across different academic fields and difficulty levels. The MMLU 5-shot accuracy performance of the Qwen-7B is shown in the following table:\n\n| Model        |  Average | STEM | Social Sciences | Humanities | Others |\n| :----------- | -------: | ---: | --------------: | ---------: | -----: |\n| LLaMA-7B     |     35.1 | 30.5 |            38.3 |       34.0 |   38.1 |\n| Baichuan-7B  |     42.3 | 35.6 |            48.9 |       38.4 |   48.1 |\n| LLaMA2-7B    |     45.3 | 36.4 |            51.2 |       42.9 |   52.2 |\n| LLaMA-13B    |     46.9 | 35.8 |            53.8 |       45.0 |   53.3 |\n| ChatGLM2-6B  |     47.9 | 41.2 |            54.4 |       43.7 |   54.5 |\n| InternLM-7B  |     51.0 |    - |               - |          - |      - |\n| Baichuan-13B |     51.6 | 41.6 |            60.9 |       47.4 |   58.5 |\n| LLaMA2-13B   |     54.8 | 44.1 |            62.6 |       52.8 |   61.1 |\n| ChatGLM2-12B |     56.2 | 48.2 |            65.1 |       52.6 |   60.9 |\n| **Qwen-7B**  | **56.7** | 47.6 |            65.9 |       51.5 |   64.7 |\n\nIn terms of English, Qwen-7B also surpasses other similar open pretrained models, and is competitive when compared to larger versions of other models.\n\n#### Coding\n\nWe compared the code capabilities of pretrained models on [HumanEval](https://github.com/openai/human-eval), and the results are as follows:\n\n| Model        |   Pass@1 |\n| :----------- | -------: |\n| Baichuan-7B  |      9.2 |\n| ChatGLM2-6B  |      9.2 |\n| InternLM-7B  |     10.4 |\n| LLaMA-7B     |     10.5 |\n| LLaMA2-7B    |     12.8 |\n| Baichuan-13B |     12.8 |\n| LLaMA-13B    |     15.8 |\n| MPT-7B       |     18.3 |\n| LLaMA2-13B   |     18.3 |\n| **Qwen-7B**  | **24.4** |\n\n#### Math\n\nWe compared the math capabilities of pretrained models on [GSM8K](https://github.com/openai/grade-school-math) (8-shot), and the results are as follows:\n\n| Model        | Accuracy |\n| :----------- | -------: |\n| MPT-7B       |      6.8 |\n| Falcon-7B    |      6.8 |\n| Baichuan-7B  |      9.7 |\n| LLaMA-7B     |     11.0 |\n| LLaMA2-7B    |     14.6 |\n| LLaMA-13B    |     17.8 |\n| Baichuan-13B |     26.6 |\n| LLaMA2-13B   |     28.7 |\n| InternLM-7B  |     31.2 |\n| ChatGLM2-6B  |     32.4 |\n| ChatGLM2-12B |     40.9 |\n| **Qwen-7B**  | **51.6** |\n\n#### Natural language processing\n\nWe compared the translation capabilities of pre-trained models on WMT22 zh-en and en-zh (5-shot BLEU), and the results are as follows:\n\n| Model       |  Average |    zh-en |    en-zh |\n| :---------- | -------: | -------: | -------: |\n| InternLM-7B |     11.8 |      9.0 |     14.5 |\n| LLaMA-7B    |     12.7 |     16.7 |      8.7 |\n| LLaMA-13B   |     15.8 |     19.5 |     12.0 |\n| LLaMA2-7B   |     19.9 |     21.9 |     17.9 |\n| Bloom-7B    |     20.3 |     19.1 |     21.4 |\n| LLaMA2-13B  |     23.3 |     22.4 |     24.2 |\n| PolyLM-13B  |     23.6 |     20.2 |     27.0 |\n| Baichuan-7B |     24.6 |     22.6 |     26.6 |\n| **Qwen-7B** | **27.5** | **24.3** | **30.6** |\n\n#### Long-context inference\n\nWe include support for training-free long-context inference based on ntk-aware interpolation, LogN attention scaling, and local window attention.\nThe context can be expanded from 2048 to over 8192.\nThe following are the test results on arXiv in terms of perplexity (PPL).\n\n<table>\n\t<tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"5\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"right\"><b>4.23</b></td><td align=\"right\"><b>3.78</b></td><td align=\"right\">39.35</td><td align=\"right\">469.81</td><td align=\"right\">2645.09</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"right\"><b>4.23</b></td><td align=\"right\"><b>3.78</b></td><td align=\"right\">3.59</td><td align=\"right\">3.66</td><td align=\"right\">5.71</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"right\"><b>4.23</b></td><td align=\"right\"><b>3.78</b></td><td align=\"right\"><b>3.58</b></td><td align=\"right\">3.56</td><td align=\"right\">4.62</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + local_attn</td><td align=\"right\"><b>4.23</b></td><td align=\"right\"><b>3.78</b></td><td align=\"right\"><b>3.58</b></td><td align=\"right\"><b>3.49</b></td><td align=\"right\"><b>4.32</b></td>\n    </tr>\n</table>\n\n## Fine-tuning\n\n`Qwen-7B-Chat` embodies our practice in alignment with human intents, ensuring internalized safety, and building intelligent agents for services.\n\n### Data\n\n**Alignment data**:\nThe data includes common instruction-style conversations, and security- and service-oriented data, which involves substantial annotation efforts.\nInstruction data covers broad abilities, such as writing, question answering, brainstorming and planning, content understanding, summarization, natural language processing, and coding.\nSecurity data tries to prevent the model from generating harmful and inappropriate content.\nService data tries to enhance the model with specific conversation patterns that can be parsed to invoke and incorporate external systems.\n\n**Data formatting**:\nSince the data consists of conversation turns, we arrange them into texts using the [ChatML](https://github.com/openai/openai-python/blob/main/chatml.md) format, which is a meta language that can describe both the metadata (e.g., roles) and the content of a turn.\nCurrently, existing roles include system, user, and assistant.\n\n### Model\n\n**Training details**:\nThe causal language modeling objective is used to fine-tune the model, except for the tokens in the content of user's turns.\nThe model is trained using the AdamW optimizer, with $\\beta_1=0.9, \\beta_2=0.95, \\epsilon=10^{-6}$.\nThe sequence length is limited to 2048, and the batch size is 128.\nThe model is trained for 4000 steps, and over the first 1430 steps, the learning rate is warmed up to $1 \\times 10^{-5}$.\nWe use weight decay of 0.1, dropout of 0.1, and gradient clipping of 1.0.\n\n### Evaluation\n\nEvaluation of human-aligned models is non-trivial and often non-standardized, since such models often target specific applications.\nWe evaluate Qwen-7B-Chat from multiple perspectives.\n\n#### World knowledge\n\nAs fine-tuning uses a much smaller dataset than pretraining and humans' understanding of world knowledge may be limited, we also evaluate the world knowledge of Qwen-7B-Chat using C-Eval and MMLU in a zero-shot and generative manner.\n\nWe demonstrate the zero-shot accuracy of Qwen-7B-Chat on the C-Eval validation set.\n\n| Model                   | Avg. Acc. |\n| :---------------------- | --------: |\n| LLaMA2-7B-Chat          |      31.9 |\n| LLaMA2-13B-Chat         |      40.6 |\n| Chinese-Alpaca-2-7B     |      41.3 |\n| Chinese-Alpaca-Plus-13B |      43.3 |\n| Baichuan-13B-Chat       |      50.4 |\n| ChatGLM2-6B-Chat        |      50.7 |\n| InternLM-7B-Chat        |      53.2 |\n| **Qwen-7B-Chat**        |  **54.2** |\n\nThe zero-shot accuracy of Qwen-7B-Chat on C-Eval testing set is provided below\n\n| Model                   |     Avg. | STEM | Social Sciences | Humanities | Others |\n| :---------------------- | -------: | ---: | --------------: | ---------: | -----: |\n| Chinese-Alpaca-Plus-13B |     41.5 | 36.6 |            49.7 |       43.1 |   41.2 |\n| Chinese-Alpaca-2-7B     |     40.3 |    - |               - |          - |      - |\n| ChatGLM2-6B-Chat        |     50.1 | 46.4 |            60.4 |       50.6 |   46.9 |\n| Baichuan-13B-Chat       |     51.5 | 43.7 |            64.6 |       56.2 |   49.2 |\n| **Qwen-7B-Chat**        | **54.6** | 47.8 |            67.6 |       59.3 |   50.6 |\n\nCompared with other models with comparable model sizes, the human-aligned Qwen-7B-Chat performs well in C-Eval accuracy.\n\nThe zero-shot accuracy of Qwen-7B-Chat on MMLU is provided below.\nThe performance of Qwen-7B-Chat is still on top among other human-aligned models with comparable size.\n\n| Model             | Avg. Acc. |\n| :---------------- | --------: |\n| ChatGLM2-6B-Chat  |      45.5 |\n| LLaMA2-7B-Chat    |      47.0 |\n| InternLM-7B-Chat  |      50.8 |\n| Baichuan-13B-Chat |      52.1 |\n| ChatGLM2-12B-Chat |      52.1 |\n| **Qwen-7B-Chat**  |  **53.9** |\n\n#### Coding\n\nThe zero-shot Pass@1 of Qwen-7B-Chat on [HumanEval](https://github.com/openai/human-eval) is demonstrated below\n\n| Model             |   Pass@1 |\n| :---------------- | -------: |\n| LLaMA2-7B-Chat    |     12.2 |\n| InternLM-7B-Chat  |     14.0 |\n| Baichuan-13B-Chat |     16.5 |\n| LLaMA2-13B-Chat   |     18.9 |\n| **Qwen-7B-Chat**  | **24.4** |\n\n#### Math\n\nThe accuracy of Qwen-7B-Chat on GSM8K is shown below\n\n| Model             | Zero-shot Acc. | 4-shot Acc. |\n| :---------------- | -------------: | ----------: |\n| ChatGLM2-6B-Chat  |              - |        28.0 |\n| LLaMA2-7B-Chat    |           20.4 |        28.2 |\n| LLaMA2-13B-Chat   |           29.4 |        36.7 |\n| InternLM-7B-Chat  |           32.6 |        34.5 |\n| Baichuan-13B-Chat |              - |        36.3 |\n| ChatGLM2-12B-Chat |              - |        38.1 |\n| **Qwen-7B-Chat**  |       **41.1** |    **43.5** |\n\n#### Service\n\nLLMs have shown capability in coordinating multiple external systems to achieve the given instructions, which creates new opportunities in traditional online services, the most notable being web search.\n\nQwen supports calling plugins/tools/APIs through [ReAct Prompting](https://arxiv.org/abs/2210.03629).\nReAct is also one of the main approaches used by the [LangChain](https://python.langchain.com/) framework.\nFor how to write and use prompts for ReAct Prompting, please refer to [the ReAct examples](examples/react_prompt.md).\nIn our evaluation [benchmark](eval/EVALUATION.md) for assessing tool usage capabilities, Qwen's performance is as follows:\n\n| Model       | Tool Selection (Acc.↑)      | Tool Input (Rouge-L↑)      | False Positive Error↓      |\n| :---------- | --------------------------: | -------------------------: | -------------------------: |\n| GPT-4       |                         95% |                   **0.90** |                      15.0% |\n| GPT-3.5     |                         85% |                       0.88 |                      75.0% |\n| **Qwen-7B** |                     **99%** |                       0.89 |                   **9.7%** |\n\n> The plugins that appear in the evaluation set do not appear in the training set of Qwen.\n> This benchmark evaluates the accuracy of the model in selecting the correct plugin from multiple candidate plugins, the rationality of the parameters passed into the plugin, and the false positive rate.\n> False Positive: Incorrectly invoking a plugin when it should not have been called when responding to a query.\n\nQwen also has the capability to be used as a [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents).\nIts performance on the benchmark provided by HuggingFace is as follows:\n\n| Model           | Tool Selection↑      | Tool Used↑      | Code↑      |\n| :-------------- | -------------------: | --------------: | ---------: |\n| GPT-4           |           **100.00** |      **100.00** |  **97.41** |\n| GPT-3.5         |                95.37 |           96.30 |      87.04 |\n| StarCoder-15.5B |                87.04 |           87.96 |      68.89 |\n| **Qwen-7B**     |                90.74 |           92.59 |      74.07 |\n\n## Conclusion\n\nIn this document, we describe Qwen-7B, including a pretrained model and a human-aligned model.\nThese models have demonstrated exciting performance compared to existing open models of similar or even larger scales.\nAs part of our ongoing commitment to the concept of Model as a Service, the release also includes practical pieces such as long context inference and external system integration, which we hope would facilitate developers realizing their own ideas and concepts.\nWe believe that the open release of Qwen-7B models would further our understanding of variables and techniques introduced in realistic settings and help to drive progress in this important area together with the community.\n"
        },
        {
          "name": "tokenization_note.md",
          "type": "blob",
          "size": 12.3779296875,
          "content": "# Tokenization\r\n\r\nQwen-7B uses BPE tokenization on UTF-8 bytes using the `tiktoken` package.\r\nThere are two types of tokens in Qwen-7B, i.e., the regular tokens (of type `bytes`) in BPE and the special/control tokens (of type `str`).\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True)\r\n```\r\n\r\n## Regular tokens\r\n\r\nThe regular tokens are BPE tokens learned from byte sequences of texts encoded using the UTF-8 encoding.\r\nWhile this allows tokenization of all texts and no unknown token exists, it may fall back to using single bytes when tokenizing uncommon texts.\r\nYou may encounter UTF-8 decoding errors and as the errors are default to `replace`, thus the replacement character (�) in incomplete generation.\r\nYou can change this behavior by passing `errors=\"ignore\"` to the `decode` function for once or to the `from_pretrained` function forever.\r\nFor more options of `errors`, please refer to [the Python documentation](https://docs.python.org/3/library/stdtypes.html#bytes.decode).\r\n\r\n```python\r\n>>> tokenizer.decode([51461])\r\n' �'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461])\r\n[b' \\xe6\\xa0']\r\n\r\n>>> b' \\xe6\\xa0'.decode(\"utf-8\", errors='replace')\r\n' �'\r\n\r\n>>> tokenizer.decode([51461, 117])\r\n' 根'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461, 117])\r\n[b' \\xe6\\xa0', b'\\xb9']\r\n\r\n>>> b' \\xe6\\xa0\\xb9'.decode(\"utf-8\", errors='replace')\r\n' 根'\r\n```\r\n\r\nThe mapping from regular tokens (in `bytes`) to its ID can be retrieved from `tokenizer.get_vocab()`.\r\nWe do not support or recommended adding regular tokens to the vocabulary.\r\n\r\n## Special tokens\r\n\r\nThe special tokens signify special functions to the model, e.g., reaching the end of a document.\r\nIn theory, they do not exist in the input texts and only appear after the input texts are processed.\r\nTheir surface forms, e.g., `<|endoftext|>` for the end of a document, are only meant for ease of reference.\r\nCurrently, used special tokens are `<|endoftext|>` in Qwen-7B, and `<|endoftext|>`, `<|im_start|>`, and `<|im_end|>` in Qwen-7B-Chat, which means they have determined meanings to the corresponding model, and should not be used otherwise.\r\nFor other purposes, we keep extra special tokens from `<|extra_0|>` to `<|extra_204|>`, and you can use them as you wish.\r\nThe mapping from surface forms of the special tokens (in `str`) to its ID can be retrieved from `tokenizer.special_tokens`.\r\n\r\nThe concepts of `bos`, `eos`, `unk`, `pad`, `mask`, `sep` and such are not appliable to our pretrained models (Qwen-7B and Qwen-7B-Chat).\r\nThe `pad` token, however, is a different story, as in theory, the model never sees or computes this token, so you may use any known token.\r\nBut to be safe, we limit the value of special tokens specified in the initialization of the tokenizer to the known special tokens.\r\nYou may specify special tokens in fine-tuning or in any other frameworks that necessitate them like this\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True, pad_token='<|endoftext|>')\r\n```\r\n\r\n> WARNING: For our pretrained models, setting `bos`, `eos`, `unk`, and such makes no sense.\r\n> Unknown behavior may be introduced if you set them without fine-tuning that designates their meanings to the model.\r\n> Especially, you should not use `<|endoftext|>` as `eos`, unless you are sure that the end of a sentence and the end of a document, which may contain many sentences, are the same in your scenario.\r\n\r\n## Injection attack prevention\r\n\r\nAs special tokens are different from regular tokens, what will happen if the surface forms of a control token appear in the input texts?\r\nFor example, note that a piece of text like this\r\n\r\n```\r\nprint(\"<|endoftext|>\")\r\n```\r\n\r\nshould be tokenized as\r\n\r\n```\r\nids:[1350, 9639, 91, 8691, 723, 427, 91, 82598]\r\ntokens: [b'print', b'(\"<', b'|', b'endo', b'ft', b'ext', b'|', b'>\")']\r\n```\r\n\r\nnot\r\n\r\n```\r\nids: [1350, 445, 151643, 899]\r\ntokens: [b'print', b'(\"', '<|endoftext|>', b'\")']\r\n```\r\n\r\nOur default used to be the correct one, that is, treating the surface forms of special tokens just like regular texts, and special tokens should be taken cared of by developers after tokenization of the texts.\r\nHowever, this conflicts with (albeit unsafe) practice in the community, and adds another step for developers to reuse their wheels.\r\n\r\nThe default behavior has been changed to parse the surface forms of all the known special tokens as special tokens.\r\nTo enable injection prevention, pass `allowed_special=set()` to the calls of the tokenizer:\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=set())\r\n{'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\nYou can control the behavior in a fine-grained manner by passing a set of `str` as `allowed_special`\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'})\r\n{'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\nYou can also make the tokenizer raise errors if the surface forms of certain special tokens are encountered in the input texts by passing a collection of `str` as `disallowed_special`\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'}, disallowed_special=('<|extra_0|>', ))\r\n...\r\nValueError: Encountered text corresponding to disallowed special token '<|extra_0|>'.\r\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|extra_0|>', ...}`.\r\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|extra_0|>'})`.\r\nTo disable this check for all special tokens, pass `disallowed_special=()`.\r\n```\r\n\r\nFor more information on `allowed_special` and `disallowed_special`, please refer to [the `tiktoken` documentation](https://github.com/openai/tiktoken/blob/095924e02c85617df6889698d94515f91666c7ea/tiktoken/core.py#L75).\r\n\r\nThe new default is the same as\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=\"all\", disallowed_special=())\r\n{'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\r\n```\r\n\r\n## Vocabulary Expansion\r\n\r\n> WARNING: Read carefully, be aware of what you are doing, and use at your own risk. \r\n> There are certain caveats regarding how your vocabulary is produced.\r\n\r\nThe tokenizer of Qwen models are based on BPE and you cannot directly expand the vocabulary by adding words to the vocabulary. \r\nThe intermediate merges are needed for tokenization.\r\nPlease follow the steps to obtain such information.\r\n\r\n1. Prepare a plain text file `qwen_extra_vocab.txt`, where each line contains a token and its frequency separated by `\\t`. \r\n\r\n   An example is given below:\r\n   ```\r\n   我是一只猫\t20\r\n   你是一只猫\t10\r\n   他是一只猫\t5\r\n   一只\t200\r\n   一只猫\t100\r\n   夸张的 比喻手法\t20\r\n   ```\r\n   The frequencies are needed to compute the BPE.\r\n\r\n   \r\n\r\n2. Prepare the base vocabulary file, e.g., `qwen.tiktoken`, and determine the start index for new tokens.\r\n   \r\n   There are 151,643 regular tokens and 208 control tokens in the vocabulary for Qwen models. \r\n   For simplicity, the start index can be set as 151,851, which is the default value. \r\n   You can, of course, override the many inactive control tokens, but you will need to modify the tokenizer code. \r\n\r\n3. Run the following command:\r\n   ```\r\n   python add_merges.py qwen.tiktoken qwen_extra.tiktoken qwen_extra_vocab.txt\r\n   ```\r\n   `add_merges.py` can be found [here](examples/add_merges.py).\r\n   It will learn the new merges based on the provided `qwen_extra_vocab.txt`. \r\n   The new tokens and their indices will be stored in `qwen_extra.tiktoken`. \r\n   Modify the paths as you wish.\r\n\r\n   It is a pure Python implementation, so please expect it to be slow if you are adding a lot of words.\r\n\r\n   Please note that not all words can be added due to pre-tokenization. \r\n   You will get warnings if you try to add such word:\r\n   ```\r\n   WARNING - 夸张的 比喻手法 would be pre-tokenized to ['夸张的', ' 比喻手法'], and thus cannot be added to vocabulary\r\n   WARNING - word 一只 is already a token b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', skipping\r\n   INFO - number of existing merges: 151643\r\n   INFO - number of words for expanding: 4\r\n   DEBUG - (b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', b'\\xe7\\x8c\\xab') (一只猫) is selected as the next merge with freq 100\r\n   DEBUG - (b'\\xe5\\x8f\\xaa', b'\\xe7\\x8c\\xab') (只猫) is selected as the next merge with freq 35\r\n   DEBUG - (b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80', b'\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (是一只猫) is selected as the next merge with freq 35\r\n   DEBUG - (b'\\xe6\\x88\\x91', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (我是一只猫) is selected as the next merge with freq 20\r\n   DEBUG - (b'\\xe4\\xbd\\xa0', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (你是一只猫) is selected as the next merge with freq 10\r\n   DEBUG - (b'\\xe4\\xbb\\x96', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (他是一只猫) is selected as the next merge with freq 5\r\n   INFO - number of newly learned merges: 6\r\n   ```\r\n\r\nThe `qwen_extra.tiktoken` will contain the following lines:\r\n```\r\n5LiA5Y+q54yr 151851\r\n5Y+q54yr 151852\r\n5piv5LiA5Y+q54yr 151853\r\n5oiR5piv5LiA5Y+q54yr 151854\r\n5L2g5piv5LiA5Y+q54yr 151855\r\n5LuW5piv5LiA5Y+q54yr 151856\r\n```\r\n\r\nYou may use the file as follows in your code:\r\n``` python\r\nfrom transformers import AutoTokenizer\r\n\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True, extra_vocab_file=\"qwen_extra.tiktoken\")\r\n\r\n>>> len(tokenizer)\r\n151857\r\n\r\n>>> tokenizer(\"我是一只猫\")\r\n{'input_ids': [151854], 'token_type_ids': [0], 'attention_mask': [1]}\r\n```\r\nNote: You need the latest tokenizer code, i.e., after 2023-10-08, to use the `extra_vocab_file` argument.\r\nOtherwise, you need to manually append `qwen.tiktoken` (of which path varies with your configuration) with the content from `qwen_extra.tiktoken`.\r\n\r\nCertainly, you will need to finetune the model for the new tokens to work.\r\n\r\n\r\n### Caveats\r\n\r\n\r\nThe tokenizer of Qwen operates directly on UTF-8 byte sequences, unlike others, e.g., SentencePiece that operates on Unicode codepoints/characters and falls back to UTF-8 byte sequences for the unknown (IIRC). \r\nThe thing is if the frequencies are computed on limited data, the Unicode codepoint boundary may not be correctly recognized.\r\nIn theory, it could be a problem for fine-tuned models using the expanded vocabulary with limited data.\r\n\r\nFor example, it could happen that `b'\\x80\\xe5'` might be merged first for the UTF-8 byte sequence `b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa'` of the string `一只`, across the Unicode codepoint of `一` (`b'\\xe4\\xb8\\x80'`) and `只` (`b'\\xe5\\x8f\\xaa'`).\r\nNormally, this would work just fine for known tokens, but for actually unknown words, unusual merges may happen, which may not be well understood for the pre-trained model.\r\n\r\nOur advice is that to be safe, you should gather the Unicode codepoints from all the words you need to add, and also add them to the file with frequencies higher than the sum of the frequencies of the corresponding words.\r\nBut since Qwen has most of the Chinese words, it could be okay to just add the Chinese words alone.\r\n\r\nFor curious minds, you will also notice that in the given example, `一只` is a token and `只猫` is also learned as a new token. \r\nThe reason is that `是一` is also a token in Qwen and has higher merging priority than `一只`, such that the merging path for `是|一|只|猫` is `是一|只|猫 -> 是一|只猫 -> 是一只猫` (omitting the UTF-8 byte merges).\r\n\r\nThis is the characteristic for plain BPE: it is based solely on distribution, meaning it does not have knowledge of which bytes can form a valid Unicode codepoint, character, or meaningful word.\r\n\r\nThe byproduct is that text may be sub-tokenized differently in different contexts, even for words containing only ASCII characters.\r\n```python\r\n>>> tokenizer.tokenize(\"Panda\")\r\n[b'P', b'anda']\r\n\r\n>>> tokenizer.tokenize(\" Panda\")\r\n[b' Panda']\r\n\r\n>>> tokenizer.tokenize(\"Pandas\")\r\n[b'P', b'andas']\r\n\r\n>>> tokenizer.tokenize(\" Pandas\")\r\n[b' Pand', b'as']\r\n```\r\nThis simply suggests that those combinations occur more frequently in the data.\r\nIf you have vast amount of training data, it should not be a problem."
        },
        {
          "name": "tokenization_note_ja.md",
          "type": "blob",
          "size": 7.2763671875,
          "content": "# トークン化\r\n\r\nQwen-7B は `tiktoken` パッケージを使用して、UTF-8 バイトを BPE トークン化します。\r\nQwen-7B には 2 種類のトークンがあります。BPE の通常のトークン (`bytes` 型) と特殊/制御トークン (`str` 型) です。\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True)\r\n```\r\n\r\n## 通常のトークン\r\n\r\n通常のトークンは、UTF-8 エンコーディングでエンコードされたテキストのバイト列から学習した BPE トークンです。\r\nこれによってすべてのテキストをトークン化することができ、未知のトークンは存在しませんが、一般的でないテキストをトークン化するときにシングルバイトを使用するようにフォールバックすることがあります。\r\nUTF-8 のデコードエラーに遭遇することがあり、そのエラーのデフォルトは `replace` であるため、不完全な生成では置換文字 (�) が使用されます。\r\nこの動作は `errors=\"ignore\"` を `decode` 関数に渡すことで変更することができる。\r\n`errors` のオプションについては、[Python ドキュメント](https://docs.python.org/3/library/stdtypes.html#bytes.decode) を参照してください。\r\n\r\n```python\r\n>>> tokenizer.decode([51461])\r\n' �'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461])\r\n[b' \\xe6\\xa0']\r\n\r\n>>> b' \\xe6\\xa0'.decode(\"utf-8\", errors='replace')\r\n' �'\r\n\r\n>>> tokenizer.decode([51461, 117])\r\n' 根'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461, 117])\r\n[b' \\xe6\\xa0', b'\\xb9']\r\n\r\n>>> b' \\xe6\\xa0\\xb9'.decode(\"utf-8\", errors='replace')\r\n' 根'\r\n```\r\n\r\n通常のトークン (`bytes` 単位) からその ID へのマッピングは `tokenizer.get_vocab()` から取得できます。\r\n通常のトークンを語彙に追加することはサポートしていませんし、推奨もしていません。\r\n\r\n## 特別なトークン\r\n\r\n特別なトークンは、例えば文書の最後に到達するなど、モデルにとって特別な機能を意味します。\r\n理論的には、これらは入力テキストには存在せず、入力テキストが処理された後にのみ現れます。\r\n例えば、文書の終わりを表す `<|endoftext|>` のような表面的な形は、参照を容易にするためだけのものである。\r\n現在、Qwen-7B では `<|endoftext|>` が、Qwen-7B-Chat では `<|endoftext|>`, `<|im_start|>`, `<|im_end|>` が特殊トークンとして使われています。\r\n他の目的のために、`<|extra_0|>` から `<|extra_204|>` までの特別なトークンを保持しています。\r\n特殊トークンの表面形式 (`str` 内) から ID へのマッピングは `tokenizer.special_tokens` から取得できます。\r\n\r\n`bos`、`eos`、`unk`、`pad`、`mask`、`sep` などの概念は学習済みモデル（Qwen-7B と Qwen-7B-Chat）には適用できません。\r\nしかし、`pad` トークンは話が別です。理論的には、モデルがこのトークンを見たり計算したりすることはないので、既知のトークンを使用することができます。\r\nしかし、安全のために、トークナイザーの初期化で指定する特別なトークンの値は、既知の特別なトークンに限定します。\r\n微調整やその他のフレームワークで特別なトークンを必要とする場合は、次のように指定できます\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True, pad_token='<|endoftext|>')\r\n```\r\n\r\n> 警告: 私たちが事前に学習したモデルでは、`bos`, `eos`, `unk` などを設定しても意味がありません。\r\n> 特に、`<<endoftext|>` を `eos` のように使ってはいけません。\r\n> 特に `<|endoftext|>` を `eos` として使用することは、文末と文末が同じであると確信できる場合を除き、避けるべきです。\r\n\r\n## インジェクション攻撃の防止\r\n\r\n特殊トークンは通常のトークンとは異なるため、コントロールトークンの表面形が入力テキストに現れるとどうなるでしょうか？\r\n例えば、次のようなテキストがあるとします\r\n\r\n```\r\nprint(\"<|endoftext|>\")\r\n```\r\n\r\nこれは次のようにしてトークン化する必要があります\r\n\r\n```\r\nids:[1350, 9639, 91, 8691, 723, 427, 91, 82598]\r\ntokens: [b'print', b'(\"<', b'|', b'endo', b'ft', b'ext', b'|', b'>\")']\r\n```\r\n\r\nこちらではありません\r\n\r\n```\r\nids: [1350, 445, 151643, 899]\r\ntokens: [b'print', b'(\"', '<|endoftext|>', b'\")']\r\n```\r\n\r\nつまり、特殊トークンの表面形は通常のテキストと同じように扱い、特殊トークンはテキストのトークン化後に開発者が処理するというものです。\r\nしかし、これはコミュニティにおける（安全ではないとはいえ）慣習に抵触し、開発者が車輪を再利用するための新たなステップを追加することになります。\r\n\r\nデフォルトの動作は、すべての既知の特殊トークンの表面形を特殊トークンとして解析するように変更されました。\r\nインジェクション防止を有効にするには、トークナイザーの呼び出しに `allowed_special=set()` を渡します:\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=set())\r\n{'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\n`str` のセットを `allowed_special` として渡すことで、きめ細かく動作を制御することができます\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'})\r\n{'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\n`str` のコレクションを `disallowed_special` として渡すことで、特定の特殊なトークンの表形式が入力テキストで遭遇した場合にトークナイザーがエラーを発生するようにすることもできます\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'}, disallowed_special=('<|extra_0|>', ))\r\n...\r\nValueError: Encountered text corresponding to disallowed special token '<|extra_0|>'.\r\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|extra_0|>', ...}`.\r\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|extra_0|>'})`.\r\nTo disable this check for all special tokens, pass `disallowed_special=()`.\r\n```\r\n\r\n`allowed_special` と `disallowed_special` の詳細については、[`tiktoken` ドキュメント](https://github.com/openai/tiktoken/blob/095924e02c85617df6889698d94515f91666c7ea/tiktoken/core.py#L75)を参照してください。\r\n\r\n新しいデフォルトは以下の通り\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=\"all\", disallowed_special=())\r\n{'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\r\n```\r\n\r\n"
        },
        {
          "name": "tokenization_note_zh.md",
          "type": "blob",
          "size": 12.689453125,
          "content": "# Tokenization\r\n\r\n> 注：作为术语的“tokenization”在中文中尚无共识的概念对应，本文档采用英文表达以利说明。\r\n\r\nQwen-7B采用UTF-8字节级别的BPE tokenization方式，并依赖`tiktoken`这一高效的软件包执行分词。\r\nQwen-7B中有两类token，即源于BPE、`bytes`类型的普通token和特殊指定、`str`类型的特殊token。\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True)\r\n```\r\n\r\n## 普通token\r\n\r\n普通token源于BPE，是在UTF-8编码的文本字节序列上学习得到的。\r\n尽管基于字节序列的方式保证了所有文本均可被tokenize且没有未登录token问题，但处理罕见文本时有可能回退到字节级别的编码。\r\n由于从字节序列解码为文本时，`errors`参数设为`replace`，处理不完整的token序列可能会遇到UTF-8解码错误，表象是生成中包含“替换字符”(�)。\r\n这一行为可以通过将`errors`参数设为`ignore`来规避。\r\n一次性修改可以传入tokenizer的`decode`函数，持久性修改可以传入tokenizer的初始化函数，请注意`decode`的配置优先级更高。\r\n`errors`的可选值，请参阅[Python文档](https://docs.python.org/3/library/stdtypes.html#bytes.decode).\r\n\r\n```python\r\n>>> tokenizer.decode([51461])\r\n' �'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461])\r\n[b' \\xe6\\xa0']\r\n\r\n>>> b' \\xe6\\xa0'.decode(\"utf-8\", errors='replace')\r\n' �'\r\n\r\n>>> tokenizer.decode([51461, 117])\r\n' 根'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461, 117])\r\n[b' \\xe6\\xa0', b'\\xb9']\r\n\r\n>>> b' \\xe6\\xa0\\xb9'.decode(\"utf-8\", errors='replace')\r\n' 根'\r\n```\r\n\r\n`bytes`类型的普通token到id的映射可以通过`tokenizer.get_vocab()`获取。\r\n尚不支持也不推荐向tokenizer增加普通token。\r\n\r\n## 特殊token\r\n\r\n特殊token用以给模型传递特殊信号，如到达文本末尾。\r\n理论上，输入文本中不包含特殊token，它们仅在tokenization后由开发者手动加入。\r\n特殊token的字面表达，如表示文本结束的`<|endoftext|>`，仅便于指代特殊token，不意味着它们在输入文本空间中。\r\n目前，训练中使用的、已经有固定含义的、不应做它用的特殊token，Qwen-7B中有`<|endoftext|>`，Qwen-7B-Chat中有`<|endoftext|>`、`<|im_start|>`以及`<|im_end|>`。\r\n但词表中也留有供扩展的特殊token位，可用`<|extra_0|>`到`<|extra_204|>`来指代。\r\n`str`类型的特殊token字面表达到id的映射，可以通过`tokenizer.special_tokens`获取。\r\n\r\n对于提供的模型参数(Qwen-7B和Qwen-7B-Chat)而言，诸如`bos`、`eos`、`unk`、`pad`、`mask`、`sep`等的特殊token的概念并不适用。\r\n特例是`pad`，由于这个token理论上并不参与模型计算，所以可以使用任意token表达这一概念。\r\n但保险起见，目前可在tokenizer初始化时设定的特殊token，仅可使用已知的特殊token字面表达，即`<|endoftext|>`、`<|im_start|>`、`<|im_end|>`和`<|extra_0|>`到`<|extra_204|>`。\r\n对于微调或者其它需要这些token才能运行的框架，可以如下配置\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True, pad_token='<|endoftext|>')\r\n```\r\n\r\n> 注意: 对于提供的训练好的模型，设置诸如`bos`、`eos`、`unk`之类的没有意义，即模型不需要这些概念。\r\n> 如果设置了这些token，但没有相应的微调这些token以让模型理解其含义，未知行为可能被触发。\r\n> 特别时，不应混淆`<|endoftext|>`和`eos`的概念，除非应用场景中它们的实际含义是一致的，即句子末尾等价于文本末尾。\r\n\r\n**注入攻击防御**\r\n\r\n由于特殊token和普通token概念上的差异，如果输入文本中含有特殊token的字面表达该如何处理？\r\n以下面文本为例\r\n\r\n```\r\nprint(\"<|endoftext|>\")\r\n```\r\n\r\n其正确的tokenization为\r\n\r\n```\r\nids:[1350, 9639, 91, 8691, 723, 427, 91, 82598]\r\ntokens: [b'print', b'(\"<', b'|', b'endo', b'ft', b'ext', b'|', b'>\")']\r\n```\r\n\r\n不是\r\n\r\n```\r\nids: [1350, 445, 151643, 899]\r\ntokens: [b'print', b'(\"', '<|endoftext|>', b'\")']\r\n```\r\n\r\n默认行为曾是正确的，即输入文本中任何字符一律按普通token处理，特殊token应由开发者在tokenization人工处理。\r\n然后，这与社区中的实践似有差异，为开发者复用代码增加了额外适配步骤。\r\n\r\n默认行为已被调整为从输入文本中解析特殊token的字面表达。\r\n如需启用注入攻击防御，请传入参数`allowed_special=set()`：\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=set())\r\n{'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\n这一行为可以更精细的调控，将`allowed_special`设计为`str`的集合即可：\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'})\r\n{'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\n如果希望输入中遇到特殊token的字面表达时，获得更直接的提醒，通过配置`disallowed_special`可以让tokenizer直接触发异常：\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'}, disallowed_special=('<|extra_0|>', ))\r\n...\r\nValueError: Encountered text corresponding to disallowed special token '<|extra_0|>'.\r\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|extra_0|>', ...}`.\r\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|extra_0|>'})`.\r\nTo disable this check for all special tokens, pass `disallowed_special=()`.\r\n```\r\n\r\n更多关于`allowed_special`和`disallowed_special`的信息, 请参阅[`tiktoken`代码](https://github.com/openai/tiktoken/blob/095924e02c85617df6889698d94515f91666c7ea/tiktoken/core.py#L75).\r\n\r\n新的默认行为与以下设定等价\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=\"all\", disallowed_special=())\r\n{'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\r\n```\r\n\r\n## 词表扩展\r\n\r\n> 特别提醒：请仔细阅读本部分的说明，理解每一步操作，并承担可能的后果。\r\n> 由于词表扩展部分由您提供，产出方式的差异可能导致特定的不兼容情况，请审慎操作。\r\n\r\nQwen系列模型的tokenizer基于BPE方案提取文本中的token。\r\n从UTF-8编码的字节开始（每个字节都可以是一个token），两两token合并成为新token，直至不能再合并出新的token为止。\r\n由于词表同时还记录了token的合并方式，直接向词表中添加词可能对Qwen的tokenizer并不适用，即通过已有的token可能合并不出来您添加词。\r\n\r\n因而，请参照以下步骤获得合并信息：\r\n\r\n1. 准备一个纯文本文件，例如名为`qwen_extra_vocab.txt`，每行一个待添加的词和它的频率，中间用制表符`\\t`分隔。\r\n\r\n   以下是一个文件的例子：\r\n   ```\r\n   我是一只猫\t20\r\n   你是一只猫\t10\r\n   他是一只猫\t5\r\n   一只\t200\r\n   一只猫\t100\r\n   夸张的 比喻手法\t20  \r\n   ```\r\n   频率是必需的，用来计算合并的优先级。\r\n\r\n2. 准备基础的词表文件，例如`qwen.tiktoken`，并确认新加入token的起始索引。\r\n\r\n   Qwen模型词表中有151,643个普通token，有208个特殊token。\r\n   简单起见，起始索引可以设置为151,851（默认值）。\r\n   您可以覆写不起效的特殊token，但您需要相应的修改tokenizer代码。\r\n\r\n3. 运行以下命令：\r\n   ```\r\n   python add_merges.py qwen.tiktoken qwen_extra.tiktoken qwen_extra_vocab.txt\r\n   ```\r\n   `add_merges.py`代码在[GitHub存储库](examples/add_merges.py)中。\r\n   基于提供的`qwen_extra_vocab.txt`，该脚本将学习新的token合并方式。\r\n   新token及其索引将存储在`qwen_extra.tiktoken`文件中。\r\n   您可以视情况修改有关路径。\r\n\r\n   由于是纯Python实现，如果您添加了非常多的词，预期会花费较多时间。\r\n\r\n   请注意，由于预切分，有些词是无法作为token加入的。\r\n   如果您添加了这些词，您会收到警告：\r\n   ```\r\n   WARNING - 夸张的 比喻手法 would be pre-tokenized to ['夸张的', ' 比喻手法'], and thus cannot be added to vocabulary\r\n   WARNING - word 一只 is already a token b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', skipping\r\n   INFO - number of existing merges: 151643\r\n   INFO - number of words for expanding: 4\r\n   DEBUG - (b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', b'\\xe7\\x8c\\xab') (一只猫) is selected as the next merge with freq 100\r\n   DEBUG - (b'\\xe5\\x8f\\xaa', b'\\xe7\\x8c\\xab') (只猫) is selected as the next merge with freq 35\r\n   DEBUG - (b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80', b'\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (是一只猫) is selected as the next merge with freq 35\r\n   DEBUG - (b'\\xe6\\x88\\x91', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (我是一只猫) is selected as the next merge with freq 20\r\n   DEBUG - (b'\\xe4\\xbd\\xa0', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (你是一只猫) is selected as the next merge with freq 10\r\n   DEBUG - (b'\\xe4\\xbb\\x96', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (他是一只猫) is selected as the next merge with freq 5\r\n   INFO - number of newly learned merges: 6\r\n   ```\r\n\r\n`qwen_extra.tiktoken`会包含以下内容：\r\n```\r\n5LiA5Y+q54yr 151851\r\n5Y+q54yr 151852\r\n5piv5LiA5Y+q54yr 151853\r\n5oiR5piv5LiA5Y+q54yr 151854\r\n5L2g5piv5LiA5Y+q54yr 151855\r\n5LuW5piv5LiA5Y+q54yr 151856\r\n```\r\n\r\n您可以按如下方式使用扩展后的词表：\r\n``` python\r\nfrom transformers import AutoTokenizer\r\n\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True, extra_vocab_file=\"qwen_extra.tiktoken\")\r\n\r\n>>> len(tokenizer)\r\n151857\r\n\r\n>>> tokenizer(\"我是一只猫\")\r\n{'input_ids': [151854], 'token_type_ids': [0], 'attention_mask': [1]}\r\n```\r\n\r\n注意：您需要使用2023年10月8日后的tokenizer代码才能传递`extra_vocab_file`参数。如是其它情况，您可以将`qwen_extra.tiktoken`内容复制粘贴到`qwen.tiktoken`内容后面。\r\n\r\n您需要微调模型才能使新的token发挥作用。\r\n\r\n### 注意事项\r\n\r\nQwen的tokenizer是直接从UTF-8编码的字节序列开始处理的，这与其它tokenizer比如SentencePiece是很不一样的。SentencePiece是从Unicode码位（可以理解为一个字符）开始处理，遇到未登录的再用UTF-8编码成字节。\r\n从字节开始的一个潜在问题是如果频率信息不够准确，比如频率信息是在很少数据上统计得到的，Unicode码位按UTF-8编码成字节后的边界可能会出现差错。\r\n理论上，如果模型微调数据量不足，使用扩展后的词表也可能出现意外问题。\r\n\r\n举个例子（非实际情况），对于`一只`的UTF-8字节序列`b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa'`，中间两个字节`b'\\x80\\xe5'`可能会先合并为一个token，跨越了`一`(`b'\\xe4\\xb8\\x80'`)和`只`(`b'\\xe5\\x8f\\xaa'`)的码位边界。\r\n这对于已登录token不会有什么影响（最后总会合并为`一只`），但对于未登录的，可能会产生一些不同寻常的合并/token。\r\n这些token序列可能对于预训练模型是陌生的。\r\n\r\n我们的建议是保险起见，您最好先收集待添加词中的所有Unicode码位，然后单独指定它们的频率大于其所构成词的频率之和。\r\n不过由于Qwen的tokenizer已包含了大多数中文字，对于中文词的话，不添加中文字的频率，大部分情况下是可行的。\r\n\r\n您可能已经发现了，在提供的例子中，`一只`已经是登录过的token了，但`只猫`还是学习成为了一个新token，出现了“交叉”。\r\n原因是在Qwen中`是一`也是一个已知token，且其频率/优先级比`一只`要高，因而对于`是|一|只|猫`这个片段，合并的次序是`是一|只|猫 -> 是一|只猫 -> 是一只猫`（省略UTF-8字节级别的合并）。\r\n\r\n这是常规BPE的特性，其完全基于分布，并不知道哪些字节可以构成合法的Unicode码位、合法的字符或是词。\r\n\r\n副产物是一段文本在不同的上下文下可能会有不同的tokenize结果，对于仅包含ASCII字符的文本同样如此。\r\n```python\r\n>>> tokenizer.tokenize(\"Panda\")\r\n[b'P', b'anda']\r\n\r\n>>> tokenizer.tokenize(\" Panda\")\r\n[b' Panda']\r\n\r\n>>> tokenizer.tokenize(\"Pandas\")\r\n[b'P', b'andas']\r\n\r\n>>> tokenizer.tokenize(\" Pandas\")\r\n[b' Pand', b'as']\r\n```\r\n这仅说明在用于学习BPE的数据中，这样的组合是更高频的。\r\n如果您有海量的训练语料，这并不会是个问题。"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 1.3525390625,
          "content": "import torch\nfrom transformers import AutoModelForCausalLM\nfrom accelerate import dispatch_model\n\n\ndef _device_map(num_gpus, num_layers):\n    per_gpu_layers = (num_layers + 2) / num_gpus\n\n    device_map = {\n        'transformer.wte': 0,\n        'transformer.ln_f': 0,\n        'lm_head': num_gpus-1\n    }\n\n    used = 1\n    gpu_target = 0\n    for i in range(num_layers):\n        if used >= per_gpu_layers:\n            gpu_target += 1\n            used = 0 if gpu_target < num_gpus-1 else 1\n        assert gpu_target < num_gpus\n        device_map[f'transformer.h.{i}'] = gpu_target\n        used += 1\n\n    return device_map\n\n\ndef load_model_on_gpus(model_name_or_path, num_gpus: int = 2):\n    num_devices = torch.cuda.device_count()\n\n    if num_gpus == 1:\n        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='auto',\n                                                     trust_remote_code=True).eval()\n    elif 1 < num_gpus <= num_devices:\n        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu',\n                                                     trust_remote_code=True).eval()\n        num_layers = model.config.num_hidden_layers\n        device_map = _device_map(num_gpus, num_layers)\n        print(device_map)\n        model = dispatch_model(model, device_map=device_map)\n    else:\n        raise KeyError\n\n    return model\n"
        },
        {
          "name": "web_demo.py",
          "type": "blob",
          "size": 7.337890625,
          "content": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"A simple web interactive chat demo based on gradio.\"\"\"\nimport os\nfrom argparse import ArgumentParser\n\nimport gradio as gr\nimport mdtex2html\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n\nDEFAULT_CKPT_PATH = 'Qwen/Qwen-7B-Chat'\n\n\ndef _get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\"-c\", \"--checkpoint-path\", type=str, default=DEFAULT_CKPT_PATH,\n                        help=\"Checkpoint name or path, default to %(default)r\")\n    parser.add_argument(\"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\")\n\n    parser.add_argument(\"--share\", action=\"store_true\", default=False,\n                        help=\"Create a publicly shareable link for the interface.\")\n    parser.add_argument(\"--inbrowser\", action=\"store_true\", default=False,\n                        help=\"Automatically launch the interface in a new tab on the default browser.\")\n    parser.add_argument(\"--server-port\", type=int, default=8000,\n                        help=\"Demo server port.\")\n    parser.add_argument(\"--server-name\", type=str, default=\"127.0.0.1\",\n                        help=\"Demo server name.\")\n\n    args = parser.parse_args()\n    return args\n\n\ndef _load_model_tokenizer(args):\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"auto\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    ).eval()\n\n    config = GenerationConfig.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True,\n    )\n\n    return model, tokenizer, config\n\n\ndef postprocess(self, y):\n    if y is None:\n        return []\n    for i, (message, response) in enumerate(y):\n        y[i] = (\n            None if message is None else mdtex2html.convert(message),\n            None if response is None else mdtex2html.convert(response),\n        )\n    return y\n\n\ngr.Chatbot.postprocess = postprocess\n\n\ndef _parse_text(text):\n    lines = text.split(\"\\n\")\n    lines = [line for line in lines if line != \"\"]\n    count = 0\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            count += 1\n            items = line.split(\"`\")\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = f\"<br></code></pre>\"\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace(\"`\", r\"\\`\")\n                    line = line.replace(\"<\", \"&lt;\")\n                    line = line.replace(\">\", \"&gt;\")\n                    line = line.replace(\" \", \"&nbsp;\")\n                    line = line.replace(\"*\", \"&ast;\")\n                    line = line.replace(\"_\", \"&lowbar;\")\n                    line = line.replace(\"-\", \"&#45;\")\n                    line = line.replace(\".\", \"&#46;\")\n                    line = line.replace(\"!\", \"&#33;\")\n                    line = line.replace(\"(\", \"&#40;\")\n                    line = line.replace(\")\", \"&#41;\")\n                    line = line.replace(\"$\", \"&#36;\")\n                lines[i] = \"<br>\" + line\n    text = \"\".join(lines)\n    return text\n\n\ndef _gc():\n    import gc\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef _launch_demo(args, model, tokenizer, config):\n\n    def predict(_query, _chatbot, _task_history):\n        print(f\"User: {_parse_text(_query)}\")\n        _chatbot.append((_parse_text(_query), \"\"))\n        full_response = \"\"\n\n        for response in model.chat_stream(tokenizer, _query, history=_task_history, generation_config=config):\n            _chatbot[-1] = (_parse_text(_query), _parse_text(response))\n\n            yield _chatbot\n            full_response = _parse_text(response)\n\n        print(f\"History: {_task_history}\")\n        _task_history.append((_query, full_response))\n        print(f\"Qwen-Chat: {_parse_text(full_response)}\")\n\n    def regenerate(_chatbot, _task_history):\n        if not _task_history:\n            yield _chatbot\n            return\n        item = _task_history.pop(-1)\n        _chatbot.pop(-1)\n        yield from predict(item[0], _chatbot, _task_history)\n\n    def reset_user_input():\n        return gr.update(value=\"\")\n\n    def reset_state(_chatbot, _task_history):\n        _task_history.clear()\n        _chatbot.clear()\n        _gc()\n        return _chatbot\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\"\"\"\\\n<p align=\"center\"><img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" style=\"height: 80px\"/><p>\"\"\")\n        gr.Markdown(\"\"\"<center><font size=8>Qwen-Chat Bot</center>\"\"\")\n        gr.Markdown(\n            \"\"\"\\\n<center><font size=3>This WebUI is based on Qwen-Chat, developed by Alibaba Cloud. \\\n(本WebUI基于Qwen-Chat打造，实现聊天机器人功能。)</center>\"\"\")\n        gr.Markdown(\"\"\"\\\n<center><font size=4>\nQwen-7B <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">🤖 </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-7B\">🤗</a>&nbsp ｜ \nQwen-7B-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">🤖 </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">🤗</a>&nbsp ｜ \nQwen-14B <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">🤖 </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-14B\">🤗</a>&nbsp ｜ \nQwen-14B-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">🤖 </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">🤗</a>&nbsp ｜ \n&nbsp<a href=\"https://github.com/QwenLM/Qwen\">Github</a></center>\"\"\")\n\n        chatbot = gr.Chatbot(label='Qwen-Chat', elem_classes=\"control-height\")\n        query = gr.Textbox(lines=2, label='Input')\n        task_history = gr.State([])\n\n        with gr.Row():\n            empty_btn = gr.Button(\"🧹 Clear History (清除历史)\")\n            submit_btn = gr.Button(\"🚀 Submit (发送)\")\n            regen_btn = gr.Button(\"🤔️ Regenerate (重试)\")\n\n        submit_btn.click(predict, [query, chatbot, task_history], [chatbot], show_progress=True)\n        submit_btn.click(reset_user_input, [], [query])\n        empty_btn.click(reset_state, [chatbot, task_history], outputs=[chatbot], show_progress=True)\n        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)\n\n        gr.Markdown(\"\"\"\\\n<font size=2>Note: This demo is governed by the original license of Qwen. \\\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \\\nincluding hate speech, violence, pornography, deception, etc. \\\n(注：本演示受Qwen的许可协议限制。我们强烈建议，用户不应传播及不应允许他人传播以下内容，\\\n包括但不限于仇恨言论、暴力、色情、欺诈相关的有害信息。)\"\"\")\n\n    demo.queue().launch(\n        share=args.share,\n        inbrowser=args.inbrowser,\n        server_port=args.server_port,\n        server_name=args.server_name,\n    )\n\n\ndef main():\n    args = _get_args()\n\n    model, tokenizer, config = _load_model_tokenizer(args)\n\n    _launch_demo(args, model, tokenizer, config)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    }
  ]
}