{
  "metadata": {
    "timestamp": 1736561199302,
    "page": 168,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "QwenLM/Qwen",
      "stars": 15269,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.1162109375,
          "content": "__pycache__\n*.so\nbuild\n.coverage_*\n*.egg-info\n*~\n.vscode/\n.idea/\n.git/\n.github/\n.DS_Store\n\n/private/\n/README-docker.md\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.083984375,
          "content": "__pycache__\n*.so\nbuild\n.coverage_*\n*.egg-info\n*~\n.vscode/\n.idea/\n.DS_Store\n\n/private/\n"
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 3.541015625,
          "content": "# FAQ\n\n## Installation & Environment\n\n#### Failure in installing flash attention\n\nFlash attention is an option for accelerating training and inference. Only NVIDIA GPUs of Turing, Ampere, Ada, and Hopper architecture, e.g., H100, A100, RTX 3090, T4, RTX 2080, can support flash attention. **You can use our models without installing it.**\n\n#### Which version of transformers should I use?\n\n4.32.0 is preferred.\n\n#### I downloaded the codes and checkpoints but I can't load the model locally. What should I do?\n\nPlease check if you have updated the code to the latest, and correctly downloaded all the sharded checkpoint files.\n\n#### `qwen.tiktoken` is not found. What is it?\n\nThis is the merge file of the tokenizer. You have to download it. Note that if you just git clone the repo without [git-lfs](https://git-lfs.com), you cannot download this file.\n\n#### transformers_stream_generator/tiktoken/accelerate not found\n\nRun the command `pip install -r requirements.txt`. You can find the file at [https://github.com/QwenLM/Qwen-7B/blob/main/requirements.txt](https://github.com/QwenLM/Qwen/blob/main/requirements.txt).\n<br><br>\n\n\n\n## Demo & Inference\n\n#### Is there any demo? CLI demo and Web UI demo?\n\nYes, see `web_demo.py` for web demo and `cli_demo.py` for CLI demo. See README for more information.\n\n\n#### Can I use CPU only?\n\nYes, run `python  cli_demo.py --cpu-only` will load the model and inference on CPU only.\n\n#### Can Qwen support streaming?\n\nYes. See the function `chat_stream` in `modeling_qwen.py`.\n\n#### Gibberish in result when using chat_stream().\n\nThis is because tokens represent bytes and a single token may be a meaningless string. We have updated the default setting of our tokenizer to avoid such decoding results. Please update the code to the latest version.\n\n#### It seems that the generation is not related to the instruction...\n\nPlease check if you are loading Qwen-Chat instead of Qwen. Qwen is the base model without alignment, which behaves differently from the SFT/Chat model.\n\n#### Is quantization supported?\n\nYes, the quantization is supported by AutoGPTQ. \n\n\n#### Slow when processing long sequences\n\nUpdating the code to the latest version can help.\n\n#### Unsatisfactory performance in processing long sequences\n\nPlease ensure that NTK is applied. `use_dynamc_ntk` and `use_logn_attn` in `config.json` should be set to `true` (`true` by default).\n<br><br>\n\n\n\n## Finetuning\n\n#### Can Qwen support SFT or even RLHF?\n\nYes, we now support SFT, including full-parameter finetuning, LoRA, and Q-LoRA. Also you can check other projects like [FastChat](**[https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat)), [Firefly]([https://github.com/yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly)), [**LLaMA Efficient Tuning**]([https://github.com/hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)), etc.\n\nHowever, temporarily we do not support RLHF. We will provide the code in the near future.\n<br><br>\n\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id not found\n\nIn our training, we only use `<|endoftext|>` as the separator and padding token. You can set bos_id, eos_id, and pad_id to tokenizer.eod_id. Learn more about our tokenizer from our documents about the tokenizer.\n\n\n\n## Docker\n\n#### Download official docker image is very slow\n\nWhen downloading our official docker image, you may have a slow download speed due to some network issues. You can refer to [Alibaba Cloud Container Image Service](https://help.aliyun.com/zh/acr/user-guide/accelerate-the-pulls-of-docker-official-images) to accelerate the download of official images.\n"
        },
        {
          "name": "FAQ_ja.md",
          "type": "blob",
          "size": 4.322265625,
          "content": "# FAQ\n\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ç’°å¢ƒ\n\n#### Flash attention å°å…¥ã®å¤±æ•—ä¾‹\n\nFlash attention ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚’åŠ é€Ÿã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚H100ã€A100ã€RTX 3090ã€T4ã€RTX 2080 ãªã©ã® Turingã€Ampereã€Adaã€ãŠã‚ˆã³ Hopper ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã® NVIDIA GPU ã ã‘ãŒã€flash attention ã‚’ã‚µãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚ãã‚Œã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã›ãšã«ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\n#### transformers ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ï¼Ÿ\n\n4.32.0 ãŒæœ›ã¾ã—ã„ã§ã™ã€‚\n\n#### ã‚³ãƒ¼ãƒ‰ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸãŒã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚ã©ã†ã™ã‚Œã°ã‚ˆã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n\nã‚³ãƒ¼ãƒ‰ã‚’æœ€æ–°ã®ã‚‚ã®ã«æ›´æ–°ã—ã€ã™ã¹ã¦ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‹ã©ã†ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n#### `qwen.tiktoken` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã“ã‚Œã¯ä½•ã§ã™ã‹ï¼Ÿ\n\nã“ã‚Œã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚[git-lfs](https://git-lfs.com) ã‚’ä½¿ã‚ãšã«ãƒªãƒã‚¸ãƒˆãƒªã‚’ git clone ã—ãŸã ã‘ã§ã¯ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n\n#### transformers_stream_generator/tiktoken/accelerate ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\nã‚³ãƒãƒ³ãƒ‰ `pip install -r requirements.txt` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ [https://github.com/QwenLM/Qwen/blob/main/requirements.txt](https://github.com/QwenLM/Qwen/blob/main/requirements.txt) ã«ã‚ã‚Šã¾ã™ã€‚\n<br><br>\n\n\n\n## ãƒ‡ãƒ¢ã¨æ¨è«–\n\n#### ãƒ‡ãƒ¢ã¯ã‚ã‚Šã¾ã™ã‹ï¼ŸCLI ã¨ Web UI ã®ãƒ‡ãƒ¢ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\n\nã¯ã„ã€Web ãƒ‡ãƒ¢ã¯ `web_demo.py` ã‚’ã€CLI ãƒ‡ãƒ¢ã¯ `cli_demo.py` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚è©³ã—ãã¯ README ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n\n\n#### CPU ã®ã¿ã‚’ä½¿ã†ã“ã¨ã¯ã§ãã¾ã™ã‹ï¼Ÿ\n\nã¯ã„ã€`python cli_demo.py --cpu-only` ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€CPU ã®ã¿ã§ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n\n#### Qwen ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\n\n`modeling_qwen.py` ã® `chat_stream` é–¢æ•°ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n#### chat_stream() ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€çµæœã«æ–‡å­—åŒ–ã‘ãŒç™ºç”Ÿã—ã¾ã™ã€‚\n\nã“ã‚Œã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒã‚¤ãƒˆã‚’è¡¨ã—ã€å˜ä¸€ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒç„¡æ„å‘³ãªæ–‡å­—åˆ—ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã§ã™ã€‚ã“ã®ã‚ˆã†ãªãƒ‡ã‚³ãƒ¼ãƒ‰çµæœã‚’é¿ã‘ã‚‹ãŸã‚ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚ã‚³ãƒ¼ãƒ‰ã‚’æœ€æ–°ç‰ˆã«æ›´æ–°ã—ã¦ãã ã•ã„ã€‚\n\n#### ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã¨ã¯é–¢ä¿‚ãªã„ã‚ˆã†ã§ã™ãŒ...\n\nQwen ã§ã¯ãªã Qwen-Chat ã‚’èª­ã¿è¾¼ã‚“ã§ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚Qwen ã¯ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãªã—ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã€SFT/Chat ãƒ¢ãƒ‡ãƒ«ã¨ã¯æŒ™å‹•ãŒç•°ãªã‚Šã¾ã™ã€‚\n\n#### é‡å­åŒ–ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\n\nã¯ã„ã€é‡å­åŒ–ã¯ AutoGPTQ ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n\n#### é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹\n\nã‚³ãƒ¼ãƒ‰ã‚’æœ€æ–°ç‰ˆã«æ›´æ–°ã™ã‚‹ã“ã¨ã§è§£æ±ºã—ã¾ã™ã€‚\n\n#### é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡¦ç†ã§ä¸æº€è¶³ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n\nNTK ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚`config.json` ã® `use_dynamc_ntk` ã¨ `use_logn_attn` ã‚’ `true` ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ `true`ï¼‰ã€‚\n<br><br>\n\n\n\n## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n\n#### Qwen ã¯ SFTã€ã‚ã‚‹ã„ã¯ RLHF ã«å¯¾å¿œã§ãã¾ã™ã‹ï¼Ÿ\n\nSFTã®ã‚³ãƒ¼ãƒ‰ã¯æä¾›ã—ã¾ã™ã€‚[FastChat](**[https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat))ã€[Firefly]([https://github.com/yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly))ã€[**LLaMA Efficient Tuning**]([https://github.com/hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning))ãªã©ã€ã„ãã¤ã‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚è¿‘æ—¥ä¸­ã«é–¢é€£ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ã™ã‚‹äºˆå®šã§ã™ã€‚\n<br><br>\n\n\n\n## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n\n#### bos_id/eos_id/pad_id ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\nç§ãŸã¡ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ã€ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã¨ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ `<|endoftext|>` ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚bos_idã€eos_idã€pad_id ã¯ tokenizer.eod_id ã«è¨­å®šã§ãã¾ã™ã€‚ç§ãŸã¡ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã¤ã„ã¦è©³ã—ãã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã¤ã„ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚\n\n"
        },
        {
          "name": "FAQ_zh.md",
          "type": "blob",
          "size": 4.072265625,
          "content": "# FAQ\n\n## å®‰è£…&ç¯å¢ƒ\n\n#### flash attention å®‰è£…å¤±è´¥\n\nflash attentionæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿæ¨¡å‹è®­ç»ƒæ¨ç†çš„å¯é€‰é¡¹ï¼Œä¸”ä»…é€‚ç”¨äºTuringã€Ampereã€Adaã€Hopperæ¶æ„çš„Nvidia GPUæ˜¾å¡ï¼ˆå¦‚H100ã€A100ã€RTX 3090ã€T4ã€RTX 2080ï¼‰ï¼Œæ‚¨å¯ä»¥åœ¨ä¸å®‰è£…flash attentionçš„æƒ…å†µä¸‹æ­£å¸¸ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚\n\n#### æˆ‘åº”è¯¥ç”¨å“ªä¸ªtransformersç‰ˆæœ¬ï¼Ÿ\n\nå»ºè®®ä½¿ç”¨4.32.0ã€‚\n\n#### æˆ‘æŠŠæ¨¡å‹å’Œä»£ç ä¸‹åˆ°æœ¬åœ°ï¼ŒæŒ‰ç…§æ•™ç¨‹æ— æ³•ä½¿ç”¨ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ\n\nç­”ï¼šåˆ«ç€æ€¥ï¼Œå…ˆæ£€æŸ¥ä½ çš„ä»£ç æ˜¯ä¸æ˜¯æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ï¼Œç„¶åç¡®è®¤ä½ æ˜¯å¦å®Œæ•´åœ°å°†æ¨¡å‹checkpointä¸‹åˆ°æœ¬åœ°ã€‚\n\n#### `qwen.tiktoken`è¿™ä¸ªæ–‡ä»¶æ‰¾ä¸åˆ°ï¼Œæ€ä¹ˆåŠï¼Ÿ\n\nè¿™ä¸ªæ˜¯æˆ‘ä»¬çš„tokenizerçš„mergeæ–‡ä»¶ï¼Œä½ å¿…é¡»ä¸‹è½½å®ƒæ‰èƒ½ä½¿ç”¨æˆ‘ä»¬çš„tokenizerã€‚æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨git cloneå´æ²¡æœ‰ä½¿ç”¨git-lfsï¼Œè¿™ä¸ªæ–‡ä»¶ä¸ä¼šè¢«ä¸‹è½½ã€‚å¦‚æœä½ ä¸äº†è§£git-lfsï¼Œå¯ç‚¹å‡»[å®˜ç½‘](https://git-lfs.com/)äº†è§£ã€‚\n\n#### transformers_stream_generator/tiktoken/accelerateï¼Œè¿™å‡ ä¸ªåº“æç¤ºæ‰¾ä¸åˆ°ï¼Œæ€ä¹ˆåŠï¼Ÿ\n\nè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š`pip install -r requirements.txt`ã€‚ç›¸å…³ä¾èµ–åº“åœ¨[https://github.com/QwenLM/Qwen-7B/blob/main/requirements.txt](https://github.com/QwenLM/Qwen/blob/main/requirements.txt) å¯ä»¥æ‰¾åˆ°ã€‚\n<br><br>\n\n\n## Demo & æ¨ç†\n\n#### æ˜¯å¦æä¾›Demoï¼ŸCLI DemoåŠWeb UI Demoï¼Ÿ\n\n`web_demo.py`å’Œ`cli_demo.py`åˆ†åˆ«æä¾›äº†Web UIä»¥åŠCLIçš„Demoã€‚è¯·æŸ¥çœ‹READMEç›¸å…³å†…å®¹äº†è§£æ›´å¤šã€‚\n\n#### æˆ‘æ²¡æœ‰GPUï¼Œåªç”¨CPUè¿è¡ŒCLI demoå¯ä»¥å—ï¼Ÿ\n\nå¯ä»¥çš„ï¼Œè¿è¡Œ`python  cli_demo.py --cpu-only`å‘½ä»¤å³å¯å°†æ¨¡å‹è¯»å–åˆ°CPUå¹¶ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ã€‚\n\n#### Qwenæ”¯æŒæµå¼æ¨ç†å—ï¼Ÿ\n\nQwenå½“å‰æ”¯æŒæµå¼æ¨ç†ã€‚è§ä½äº`modeling_qwen.py`çš„`chat_stream`å‡½æ•°ã€‚\n\n#### ä½¿ç”¨`chat_stream()`ç”Ÿæˆæ··ä¹±çš„å†…å®¹åŠä¹±ç ï¼Œä¸ºä»€ä¹ˆï¼Ÿ\n\nè¿™æ˜¯ç”±äºæ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­è¾“å‡ºçš„éƒ¨åˆ†tokenéœ€è¦ä¸åç»­tokenä¸€èµ·è§£ç æ‰èƒ½è¾“å‡ºæ­£å¸¸æ–‡æœ¬ï¼Œå•ä¸ªtokenè§£ç ç»“æœæ˜¯æ— æ„ä¹‰å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬å·²ç»æ›´æ–°äº†tokenizerè§£ç æ—¶çš„é»˜è®¤è®¾ç½®ï¼Œé¿å…è¿™äº›å­—ç¬¦ä¸²åœ¨ç”Ÿæˆç»“æœä¸­å‡ºç°ï¼Œå¦‚æœä»æœ‰ç±»ä¼¼é—®é¢˜è¯·æ›´æ–°æ¨¡å‹è‡³æœ€æ–°ç‰ˆæœ¬ã€‚\n\n#### æ¨¡å‹çš„è¾“å‡ºçœ‹èµ·æ¥ä¸è¾“å…¥æ— å…³/æ²¡æœ‰éµå¾ªæŒ‡ä»¤/çœ‹èµ·æ¥å‘†å‘†çš„\n\nè¯·æ£€æŸ¥æ˜¯å¦åŠ è½½çš„æ˜¯Qwen-Chatæ¨¡å‹è¿›è¡Œæ¨ç†ï¼ŒQwenæ¨¡å‹æ˜¯æœªç»alignçš„é¢„è®­ç»ƒåŸºæ¨¡å‹ï¼Œä¸æœŸæœ›å…·å¤‡å“åº”ç”¨æˆ·æŒ‡ä»¤çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨æ¨¡å‹æœ€æ–°ç‰ˆæœ¬å·²ç»å¯¹`chat`åŠ`chat_stream`æ¥å£å†…è¿›è¡Œäº†æ£€æŸ¥ï¼Œé¿å…æ‚¨è¯¯å°†é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºSFT/Chatæ¨¡å‹ä½¿ç”¨ã€‚\n\n#### æ˜¯å¦æœ‰é‡åŒ–ç‰ˆæœ¬æ¨¡å‹\n\nç›®å‰Qwenæ”¯æŒåŸºäºAutoGPTQçš„4-bitçš„é‡åŒ–æ¨ç†ã€‚\n\n#### ç”Ÿæˆåºåˆ—è¾ƒé•¿åé€Ÿåº¦æ˜¾è‘—å˜æ…¢\n\nè¯·æ›´æ–°åˆ°æœ€æ–°ä»£ç ã€‚\n\n#### å¤„ç†é•¿åºåˆ—æ—¶æ•ˆæœæœ‰é—®é¢˜\n\nè¯·ç¡®è®¤æ˜¯å¦å¼€å¯ntkã€‚è‹¥è¦å¯ç”¨è¿™äº›æŠ€å·§ï¼Œè¯·å°†`config.json`é‡Œçš„`use_dynamc_ntk`å’Œ`use_logn_attn`è®¾ç½®ä¸º`true`ã€‚æœ€æ–°ä»£ç é»˜è®¤ä¸º`true`ã€‚\n<br><br>\n\n\n## å¾®è°ƒ\n\n#### å½“å‰æ˜¯å¦æ”¯æŒSFTå’ŒRLHFï¼Ÿ\n\næˆ‘ä»¬ç›®å‰æä¾›äº†SFTçš„ä»£ç ï¼Œæ”¯æŒå…¨å‚æ•°å¾®è°ƒã€LoRAå’ŒQ-LoRAã€‚æ­¤å¤–ï¼Œå½“å‰æœ‰å¤šä¸ªå¤–éƒ¨é¡¹ç›®ä¹Ÿå·²å®ç°æ”¯æŒï¼Œå¦‚[FastChat](**[https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat))ã€[Firefly]([https://github.com/yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly))ã€[**LLaMA Efficient Tuning**]([https://github.com/hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning))ç­‰ã€‚æˆ‘ä»¬ä¼šå°½å¿«æ›´æ–°è¿™éƒ¨åˆ†ä»£ç å’Œè¯´æ˜ã€‚\n\næˆ‘ä»¬è¿˜æ²¡æä¾›å¯¹RLHFè®­ç»ƒçš„æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ã€‚\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_idï¼Œè¿™äº›token idä¸å­˜åœ¨ï¼Œä¸ºä»€ä¹ˆï¼Ÿ\n\nåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨<|endoftext|>è¿™ä¸€tokenä½œä¸ºsample/documentä¹‹é—´çš„åˆ†éš”ç¬¦åŠpaddingä½ç½®å ä½ç¬¦ï¼Œä½ å¯ä»¥å°†bos_id, eos_id, pad_idå‡æŒ‡å‘tokenizer.eod_idã€‚è¯·é˜…è¯»æˆ‘ä»¬å…³äºtokenizerçš„æ–‡æ¡£ï¼Œäº†è§£å¦‚ä½•è®¾ç½®è¿™äº›idã€‚\n\n\n## Docker\n\n#### ä¸‹è½½å®˜æ–¹Dockeré•œåƒé€Ÿåº¦å¾ˆæ…¢\n\nåœ¨ä¸‹è½½å®˜æ–¹é•œåƒæ—¶ï¼Œæ‚¨å¯èƒ½ç”±äºæŸäº›ç½‘ç»œåŸå› å¯¼è‡´ä¸‹è½½é€Ÿåº¦å˜æ…¢ã€‚å¯ä»¥å‚è€ƒ[é˜¿é‡Œäº‘å®¹å™¨é•œåƒæœåŠ¡](https://help.aliyun.com/zh/acr/user-guide/accelerate-the-pulls-of-docker-official-images)åŠ é€Ÿå®˜æ–¹é•œåƒçš„ä¸‹è½½ã€‚"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.076171875,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Alibaba Cloud\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 14.9521484375,
          "content": "------------- LICENSE FOR NVIDIA Megatron-LM code  --------------\n\nCopyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n  * Neither the name of NVIDIA CORPORATION nor the names of its\n    contributors may be used to endorse or promote products derived\n    from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n------------- LICENSE FOR OpenAI tiktoken code  --------------\n\nMIT License\n\nCopyright (c) 2022 OpenAI, Shantanu Jain\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n------------- LICENSE FOR stanford_alpaca code  --------------\n\n                                Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n   \n------------- LICENSE FOR PanQiWei AutoGPTQ code  --------------\n\nMIT License\n\nCopyright (c) 2023 æ½˜å…¶å¨(William)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "QWEN_TECHNICAL_REPORT.pdf",
          "type": "blob",
          "size": 1569.6767578125,
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 74.2578125,
          "content": "<p align=\"left\">\n    <a href=\"README_CN.md\">ä¸­æ–‡</a>&nbsp ï½œ &nbspEnglish&nbsp ï½œ &nbsp<a href=\"README_JA.md\">æ—¥æœ¬èª</a> ï½œ &nbsp<a href=\"README_FR.md\">FranÃ§ais</a> ï½œ &nbsp<a href=\"README_ES.md\">EspaÃ±ol</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        ğŸ¤— <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br><br>\n\n> [!Important]\n> Qwen2 is here! You are welcome to follow [QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) and share your experience there.\n>\n> This repo ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) is no longer actively maintained, due to substantial codebase differences.\n\n<br>\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">ğŸ¤—</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">ğŸ¤—</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">ğŸ¤—</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">ğŸ¤—</a> |\n\n\n\nWe opensource our **Qwen** series, now including **Qwen**, the base language models, namely **Qwen-1.8B**, **Qwen-7B**, **Qwen-14B**, and **Qwen-72B**, as well as **Qwen-Chat**, the chat models, namely **Qwen-1.8B-Chat**, **Qwen-7B-Chat**, **Qwen-14B-Chat**, and **Qwen-72B-Chat**. Links are on the above table. Click them and check the model cards. Also, we release the **[technical report](https://arxiv.org/abs/2309.16609)**. Please click the paper link and check it out!\n\nIn brief, we have strong base language models, which have been stably pretrained for up to 3 trillion tokens of multilingual data with a wide coverage of domains, languages (with a focus on Chinese and English), etc. They are able to achieve competitive performance on benchmark datasets. Additionally, we have chat models that are aligned with human preference based on SFT and RLHF (not released yet), which are able to chat, create content, extract information, summarize, translate, code, solve math problems, and so on, and are able to use tools, play as agents, or even play as code interpreters, etc.\n\n| Model     | Release Date | Max Length | System Prompt Enhancement | # of Pretrained Tokens | Minimum GPU Memory Usage of Finetuning (Q-Lora) | Minimum GPU Usage of Generating 2048 Tokens (Int4) | Tool Usage |\n|:----------|:------------:|:----------:|:-------------------------:|:----------------------:|:-----------------------------------------------:|:--------------------------------------------------:|:----------:|\n| Qwen-1.8B |   23.11.30   |    32K     |             âœ…             |          2.2T          |                      5.8GB                      |                       2.9GB                        |     âœ…      |  \n| Qwen-7B   |   23.08.03   |    32K     |             â             |          2.4T          |                     11.5GB                      |                       8.2GB                        |     âœ…      |   \n| Qwen-14B  |   23.09.25   |     8K     |             â             |          3.0T          |                     18.7GB                      |                       13.0GB                       |     âœ…      |\n| Qwen-72B  |   23.11.30   |    32K     |             âœ…             |          3.0T          |                     61.4GB                      |                       48.9GB                       |     âœ…      |   \n\nIn this repo, you can figure out:\n\n* Quickstart with Qwen, and enjoy the simple inference.\n* Details about the quantization models, including GPTQ and KV cache quantization.\n* Statistics of inference performance, including speed and memory.\n* Tutorials on finetuning, including full-parameter tuning, LoRA, and Q-LoRA.\n* Instructions on deployment, with the example of vLLM and FastChat.\n* Instructions on building demos, including WebUI, CLI demo, etc.\n* Introduction to DashScope API service, as well as the instructions on building an OpenAI-style API for your model.\n* Information about Qwen for tool use, agent, and code interpreter\n* Statistics of long-context understanding evaluation\n* License agreement\n* ...\n\nAlso, if you meet problems, turn to [FAQ](FAQ.md) for help first. Still feeling struggled? Feel free to shoot us issues (better in English so that more people can understand you)! If you would like to help us, send us pull requests with no hesitation! We are always excited about PR! \n\nWould like to chat with us or date us coffee time? Welcome to our Discord or WeChat! \n<br><br>\n\n## News and Updates\n* 2023.11.30 ğŸ”¥ We release **Qwen-72B** and **Qwen-72B-Chat**, which are trained on 3T tokens and support 32k context, along with **Qwen-1.8B**, and **Qwen-1.8B-Chat**, on ModelScope and Hugging Face. We have also strengthened the System Prompt capabilities of the Qwen-72B-Chat and Qwen-1.8B-Chat, see [example documentation](examples/system_prompt.md). Additionally, support the inference on **Ascend 910** and **Hygon DCU**. Check `ascend-support` and `dcu-support` for more details.\n* 2023.10.17 We release the Int8 quantized model **Qwen-7B-Chat-Int8** and **Qwen-14B-Chat-Int8**. \n* 2023.9.25 ğŸ”¥ We release **Qwen-14B** and **Qwen-14B-Chat** on ModelScope and Hugging Face, along with [qwen.cpp](https://github.com/QwenLM/qwen.cpp) and [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). Codes and checkpoints of **Qwen-7B** and **Qwen-7B-Chat** are also updated. **PLEASE PULL THE LATEST VERSION!**\n    - Compared to **Qwen-7B** (original), **Qwen-7B** uses more training tokens, increasing from 2.2T tokens to 2.4T tokens, while the context length extends from 2048 to 8192. The Chinese knowledge and coding ability of **Qwen-7B** have been further improved.\n* 2023.9.12 We now support finetuning on the Qwen-7B models, including full-parameter finetuning, LoRA and Q-LoRA.\n* 2023.8.21 We release the Int4 quantized model for Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, which requires low memory costs but achieves improved inference speed. Besides, there is no significant performance degradation on the benchmark evaluation.\n* 2023.8.3 We release both **Qwen-7B** and **Qwen-7B-Chat** on ModelScope and Hugging Face. We also provide a technical memo for more details about the model, including training details and model performance.\n<br>\n\n## Performance\nQwen models outperform the baseline models of similar model sizes on a series of benchmark datasets, e.g., MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., which evaluate the modelsâ€™ capabilities on natural language understanding, mathematic problem solving, coding, etc. Qwen-72B achieves better performance than LLaMA2-70B on all tasks and outperforms GPT-3.5 on 7 out of 10 tasks. \n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=600px/>\n<p>\n      \n\n<br>\n\n| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |\n| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\nFor all compared models, we report the best scores between their official reported results and [OpenCompass](https://opencompass.org.cn/leaderboard-llm). \n\nFor more experimental results (detailed model performance on more benchmark datasets) and details, please refer to our technical report by clicking [here](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).\n<br><br>\n\n## Requirements\n\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* transformers 4.32 and above\n* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n<br>\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen-Chat with ğŸ¤– ModelScope and ğŸ¤— Transformers.\n\nYou can use our pre-built docker images to skip most of the environment setup steps, see Section [\"Using Pre-built Docker Images\"](#-docker) for more details. \n\nIf not using docker, please make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.\n\n```bash\npip install -r requirements.txt\n```\n\nIf your device supports fp16 or bf16, we recommend installing [flash-attention](https://github.com/Dao-AILab/flash-attention) (**we support flash attention 2 now.**) for higher efficiency and lower memory usage. (**flash-attention is optional and the project can run normally without installing it**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# Below are optional. Installing them might be slow.\n# pip install csrc/layer_norm\n# If the version of flash-attn is higher than 2.1.1, the following is not needed.\n# pip install csrc/rotary\n```\n\nNow you can start with ModelScope or Transformers.\n\n### ğŸ¤— Transformers\n\nTo use Qwen-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. Remember to pass in the correct model names or paths, such as \"Qwen/Qwen-7B-Chat\" and \"Qwen/Qwen-14B-Chat\". However, **please make sure that you are using the latest code.**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\n# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\n# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚\n# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚\n# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚\n# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚\n# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚\n\n# 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹\n```\n\nRunning Qwen, the base language model, is also simple.\n\n<details>\n  <summary>Running Qwen</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\" \ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...\n```\n\n</details>\n\n<p id=\"DownloadModel\">\nIn the event of a network issue while attempting to download model checkpoints and codes from HuggingFace, an alternative approach is to initially fetch the checkpoint from ModelScope and then load it from the local directory as outlined below:\n</p>\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n# model_dir = snapshot_download('qwen/Qwen-14B')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### ğŸ¤– ModelScope\n\nModelScope is an open-source platform for Model-as-a-Service (MaaS), which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# Model names: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\n\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹\", history=history)\nprint(response)\n```\n\n### Batch Inference\nQwen supports batch inference. With flash attention enabled, using batch inference can bring a 40% speedup. The example code is shown below:\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\n# To generate attention masks automatically, it is necessary to assign distinct\n# token_ids to pad_token and eos_token, and set pad_token_id in the generation_config.\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\", history=None)\nprint(response)\n```\n\n### CPU\n\nTo deploy our models on CPU, we strongly advise you to use [qwen.cpp](https://github.com/QwenLM/qwen.cpp), which is a pure C++ implementation of Qwen and tiktoken. Check the repo for more details!\n\nAlso, it is also simple to directly run the model on CPU, which requires your specification of device:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nHowever, it is likely that you suffer from extremely low inference efficiency.\n\n### Multiple GPUs\n\nIf you suffer from lack of GPU memory and you would like to run the model on more than 1 GPU, you can directly use the default loading method, which is now supported by Transformers. The previous method based on `utils.py` is deprecated.\n\nHowever, though this method is simple, the efficiency of the native pipeline parallelism is low. We advise you to use vLLM with FastChat and please read the section for deployment.\n\n### x86 Platforms\nWhen deploy on Coreâ„¢/XeonÂ® Scalable Processors or with Arcâ„¢ GPU, [OpenVINOâ„¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html) is recommended. You can install and run this [example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot). For related issues, you are welcome to file an issue at [OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues). \n\n### DashScope\nThe most simple way to use Qwen through APIs is DashScope API service through Alibaba Cloud. We give an introduction to the usage. Additionally, we provide a script for you to deploy an OpenAI-style API on your own servers.\n\nDashScope is the large language model API service provided by Alibaba Cloud, which now supports Qwen. Note that the models behind DashScope are in-house versions temporarily without details provided. The services include `qwen-turbo` and `qwen-plus`, where the former one runs faster and the latter achieves better performance. For more information, visit the documentation [here](https://dashscope.aliyun.com).\n\nPlease head to the official website [link](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) to create a DashScope account and obtain the API key (AK). We recommend setting the AK with an environment variable:\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nThen please install the packages and click [here](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) for the documentation. If you use Python, you can install DashScope with pip:\n```bash\npip install dashscope\n```\nIf you use JAVA SDK, you can install it in this way:\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nThe simplest way to use DashScope is the usage with messages, which is similar to OpenAI API. The example is demonstrated below:\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\nFor more usages, please visit the official website for more details.\n<br><br>\n\n## Quantization\n\n### GPTQ\n\nWe provide a solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release the Int4 and Int8 quantized models, which achieve nearly lossless model effects but improved performance on both memory costs and inference speed.\n\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\n\n```bash\npip install auto-gptq optimum\n```\n\nIf you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a wheel.\n\n> Note: The pre-compiled `auto-gptq` packages strongly depend on the version of `torch` and its CUDA version. Moreover, due to recent update, \n> you may also encounter unsupported version errors from `transformers`, `optimum`, or `peft`.\n> We recommend using the latest versions meeting the following requirements:\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0\n\nThen you can load the quantized model easily and run inference as same as usual:\n\n```python\n# Model names: \"Qwen/Qwen-7B-Chat-Int4\", \"Qwen/Qwen-14B-Chat-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\nWe illustrate the model performance of both BF16, Int8 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n\n### Quantization of KV cache\n\n> NOTE: Please be aware that due to the internal mechanism of Hugging Face, the support files for this functionality \n> (i.e., `cache_autogptq_cuda_256.cpp` and `cache_autogptq_cuda_kernel_256.cu`) may be missing. Please manually download\n> them from the Hugging Face Hub and place them into the same folder as the other module files.\n\nThe attention KV cache can be quantized and compressed for storage, to get a higher sample throughput. The arguments `use_cache_quantization` and `use_cache_kernel` in `config.json` are provided to enable KV cache quantization. The specific use method is as follows:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\nAttention: Currently, KV cache quantization and flash attention cannot be used at the same time.\nIf you enable KV cache quantization and flash attention at the same time (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` is disabled by default (`use_flash_attn=false`).\n\nWe have verified that the use of the quantized Int8-KV-Cache model does not suffer from significant performance degradation in downstream evaluation. In the following, we focus on profiling its memory footprint in different conditions. \nThe profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4. \nWe use BF16 models to generate 1024 tokens by default, and \"OOM\" indicates out-of-memory error.\n\nWith KV cache quantization, the model can infer with a larger batch size (bs).\n\n| USE KV Cache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|--------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| No           | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |\n| Yes          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\nWith KV cache quantization the model can save more memory when generating longer sequence (`sl`, sequence length, referring to the number of tokens generated) at the stage of inference.\n\n| USE KV Cache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|--------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| No           | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| Yes          |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\nThe model with KV cache quantization will convert the format of `layer_past` from float to int8, and meanwhile the quantized `layer-past` will also store the quantization parameters.\n\nSpecific steps are as follows:\n\n1. Quantize key/value\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n2. Store into layer_past\n\nThe following is the format of quantized `layer_past`:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\n\nThe original format of `layer_past` is shown below:\n```\n    layer_past=(key,value)\n```\n\nIf you want to use the attention KV which is quantized, you can use the dequantization operation to convert the Int8 key/value back to the float format as follows:\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n\n## Inference Performance\n\nThis section provides the statistics of speed and memory of models in different precisions. The speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py). \n\nWe measured the average inference speed (tokens/s) and GPU memory usage of generating 2048 with the models in BF16, Int8, and Int4. \n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nThe profiling runs on a single A100-SXM4-80G GPU (except 2xA100 is mentioned) with PyTorch 2.0.1, CUDA 11.8, and Flash-Attention 2. (72B + vLLM uses PyTorch 2.1.0 and Cuda 11.8.) The inference speed is averaged over the encoded and generated tokens.\n\nNote: The generation speed of the Int4/Int8 models mentioned above is provided by the autogptq library. The current speed of the model loaded using ``AutoModelForCausalLM.from_pretrained`` will be approximately 20% slower. We have reported this issue to the HuggingFace team and will update it promptly if a solution is available.\n\nWe also measure the inference speed and GPU memory usage with different settings of context and generation lengths, Flash-Attention version. You can find the results in the according modelcards on Hugging Face or ModelScope.\n\n## Finetuning\n\n### Usage\nNow we provide the official training script, `finetune.py`, for users to finetune the pretrained model for downstream applications in a simple fashion. Additionally, we provide shell scripts to launch finetuning with no worries. This script supports the training with [DeepSpeed](https://github.com/microsoft/DeepSpeed) and [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). The shell scripts that we provide use DeepSpeed (Note: this may have conflicts with the latest version of pydantic and you should use make sure `pydantic<2.0`) and Peft. You can install them by:\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\nTo prepare your training data, you need to put all the samples into a list and save it to a json file. Each sample is a dictionary consisting of an id and a list for conversation. Below is a simple example list with 1 sample:\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚\"\n      }\n    ]\n  }\n]\n```\n\nAfter data preparation, you can use the provided shell scripts to run finetuning. Remember to specify the path to the data file, `$DATA`.\n\nThe finetuning scripts allow you to perform:\n- Full-parameter finetuning\n- LoRA\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```bash\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\nbash finetune/finetune_ds.sh\n```\n\nRemember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument `--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use `--bf16 True` or `--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.\n\nSimilarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed `peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.\n\n```bash\n# Single GPU training\nbash finetune/finetune_lora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_lora_ds.sh\n```\n\nIn comparison with full-parameter finetuning, LoRA ([paper](https://arxiv.org/abs/2106.09685)) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs. \n\nNote that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information. \n\nIf you still suffer from insufficient memory, you can consider Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs. \n\nNote: to run single-GPU Q-LoRA training, you may need to install `mpi4py` through `pip` or `conda`.\n\nTo run Q-LoRA, directly run the following script:\n\n```bash\n# Single GPU training\nbash finetune/finetune_qlora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_qlora_ds.sh\n```\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You **SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision training due to our observation of errors caused by torch amp. Besides, for Q-LoRA, the troubles with the special tokens in LoRA still exist. However, as we only provide the Int4 models for chat models, which means the language model has learned the special tokens of ChatML format, you have no worry about the layers. Note that the layers of the Int4 model should not be trainable, and thus if you introduce special tokens in your training, Q-LoRA might not work.\n\n> NOTE: Please be aware that due to the internal mechanisms of Hugging Face, certain non-Python files (e.g., `*.cpp` and `*.cu`) \n> may be missing from the saved checkpoint. You may need to manually copy them to the directory containing other files.\n\nDifferent from full-parameter finetuning, the training of both LoRA and Q-LoRA only saves the adapter parameters. Suppose your training starts from Qwen-7B, you can load the finetuned model for inference as shown below:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n> NOTE: If `peft>=0.8.0`, it will try to load the tokenizer as well, however, initialized without `trust_remote_code=True`, leading to `ValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.` Currently, you could downgrade `peft<0.8.0` or move tokenizer files elsewhere to workaround this issue.\n\nIf you want to merge the adapters and save the finetuned model as a standalone model (you can only do this with LoRA, and you CANNOT merge the parameters from Q-LoRA), you can run the following codes:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\nThe `new_model_directory` directory will contain the merged model weights and module files. Please note that `*.cu` and `*.cpp` files may be missing in the saved files. If you wish to use the KV cache functionality, please manually copy them. Besides, the tokenizer files are not saved in the new directory in this step. You can copy the tokenizer files or use the following code\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    path_to_adapter, # path to the output directory\n    trust_remote_code=True\n)\n\ntokenizer.save_pretrained(new_model_directory)\n```\n\n\nNote: For multi-GPU training, you need to specify the proper hyperparameters for distributed training based on your machine. Besides, we advise you to specify your maximum sequence length with the argument `--model_max_length`, based on your consideration of data, memory footprint, and training speed.\n\n### Quantize Fine-tuned Models\n\nThis section applies to full-parameter/LoRA fine-tuned models. (Note: You do not need to quantize the Q-LoRA fine-tuned model because it is already quantized.)\nIf you use LoRA, please follow the above instructions to merge your model before quantization. \n\nWe recommend using [auto_gptq](https://github.com/PanQiWei/AutoGPTQ) to quantize the finetuned model. \n\n```bash\npip install auto-gptq optimum\n```\n\nNote: Currently AutoGPTQ has a bug referred in [this issue](https://github.com/PanQiWei/AutoGPTQ/issues/370). Here is a [workaround PR](https://github.com/PanQiWei/AutoGPTQ/pull/495), and you can pull this branch and install from the source.\n\nFirst, prepare the calibration data. You can reuse the fine-tuning data, or use other data following the same format.\n\nSecond, run the following script:\n\n```bash\npython run_gptq.py \\\n    --model_name_or_path $YOUR_LORA_MODEL_PATH \\\n    --data_path $DATA \\\n    --out_path $OUTPUT_PATH \\\n    --bits 4 # 4 for int4; 8 for int8\n```\n\nThis step requires GPUs and may costs a few hours according to your data size and model size.\n\nThen, copy all `*.py`, `*.cu`, `*.cpp` files and `generation_config.json` to the output path. And we recommend you to overwrite `config.json` by copying the file from the coresponding official quantized model\n(for example, if you are fine-tuning `Qwen-7B-Chat` and use `--bits 4`, you can find the `config.json` from [Qwen-7B-Chat-Int4](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4/blob/main/config.json)).\nYou should also rename the ``gptq.safetensors`` into ``model.safetensors``.\n\nFinally, test the model by the same method to load the official quantized model. For example,\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"/path/to/your/model\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/path/to/your/model\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n```\n\n### Multinode Finetuning\n\nOur provided scripts support multinode finetuning. You can refer to the comments in [script](./finetune/finetune_lora_ds.sh) to correctly set corresponding arguments and launch the script on each node. For more information about multinode distributed training, please refer to [torchrun](https://pytorch.org/docs/stable/elastic/run.html).\n\nNote: DeepSpeed ZeRO 3 requires much greater inter-node communication rate than ZeRO 2, which will significantly reduce the training speed in the case of multinode finetuning. Therefore, we do not recommend using DeepSpeed ZeRO 3 configurations in multinode finetuning scripts.\n\n### Profiling of Memory and Speed\nWe profile the GPU memory and training speed of both LoRA (LoRA (emb) refers to training the embedding and output layer, while LoRA has no trainable embedding and output layer) and Q-LoRA in the setup of single-GPU training. In this test, we experiment on a single A100-SXM4-80G GPU, and we use CUDA 11.8 and Pytorch 2.0. Flash attention 2 is applied. We uniformly use a batch size of 1 and gradient accumulation of 8. We profile the memory (GB) and speed (s/iter) of inputs of different lengths, namely 256, 512, 1024, 2048, 4096, and 8192. We also report the statistics of full-parameter finetuning with Qwen-7B on 2 A100 GPUs. We only report the statistics of 256, 512, and 1024 tokens due to the limitation of GPU memory. \n\nFor Qwen-7B, we also test the performance of multinode finetuning. We experiment using two servers, each containing two A100-SXM4-80G GPUs, and the rest of configurations are the same as other Qwen-7B experiments. The results of multinode finetuning are marked as LoRA (multinode) in the table.\n\nFor Qwen-72B, we experiment in two ways: 1) Lora fintuning + DeepSpeed ZeRO 3 on 4 A100-SXM4-80G GPUs and 2) QLora (int4) fine-tuning on a single A100-SXM4-80G GPU. Note that OOM occurs on 4 A100-SXM4-80G GPUs both with LoRA (emb) fine-tuning and LoRA fine-tuning without Deepspeed ZeRO 3 (you can pass `--deepspeed finetune/ds_config_zero3.json` to [`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh) to enable DeepSpeed ZeRO 3).\n\nThe statistics are listed below:\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th rowspan=\"2\">#Nodes</th><th rowspan=\"2\">#GPUs per node</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"5\">7B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n<td>1</td><td>2</td>\n<td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>LoRA (multinode)</td>\n        <td>2</td><td>2</td>\n        <td align=\"center\">74.7G / 2.09s/it</td><td align=\"center\">77.6G / 3.16s/it</td><td align=\"center\">84.9G / 5.17s/it</td><td align=\"center\">95.1G / 9.25s/it</td><td align=\"center\">121.1G / 18.1s/it</td><td align=\"center\">155.5G / 37.4s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"2\">72B</th>\n        <td>LoRA + Deepspeed Zero3</td>\n        <td>1</td><td>4</td>\n        <td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n\n<br>\n\n## Deployment\n\n### vLLM \n\nFor deployment and fast inference, we suggest using vLLM. \n\nIf you use **CUDA 12.1 and PyTorch 2.1**, you can directly use the following command to install vLLM.\n\n```bash\npip install vllm\n```\n\nOtherwise, please refer to the official vLLM [Installation Instructions](https://docs.vllm.ai/en/latest/getting_started/installation.html).\n\n#### vLLM + Transformer-like Wrapper\n\nYou can download the [wrapper codes](examples/vllm_wrapper.py) and execute the following commands for multiple rounds of dialogue interaction. (Note: It currently only supports the ``model.chat()`` method.)\n\n```python\nfrom vllm_wrapper import vLLMWrapper\n\nmodel = vLLMWrapper('Qwen/Qwen-7B-Chat', tensor_parallel_size=1)\n# model = vLLMWrapper('Qwen/Qwen-7B-Chat-Int4', tensor_parallel_size=1, dtype=\"float16\")\n\nresponse, history = model.chat(query=\"ä½ å¥½\", history=None)\nprint(response)\nresponse, history = model.chat(query=\"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\nresponse, history = model.chat(query=\"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n```\n\n#### vLLM + Web Demo / OpenAI-like API\n\nYou can use FastChat to lauch a web demo or an OpenAI API server. First, install FastChat:\n\n```bash\npip install \"fschat[model_worker,webui]\"\n```\n\nTo run Qwen with vLLM and FastChat, you need launch a controller by:\n```bash\npython -m fastchat.serve.controller\n```\n\nThen you can launch the model worker, which means loading your model for inference. For single GPU inference, you can directly run:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype float16 # run int4 model\n```\nHowever, if you hope to run the model on multiple GPUs for faster inference or larger memory, you can use tensor parallelism supported by vLLM. Suppose you run the model on 4 GPUs, the command is shown below:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype float16 # run int4 model\n```\n\nAfter launching your model worker, you can launch a:\n\n* Web UI Demo\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* OpenAI API\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\nHowever, if you find it difficult to use vLLM and FastChat, you can try our provided simplest methods to deploy a web demo, CLI demo, and API.\n\n\n### Web UI\n\nWe provide code for users to build a web UI demo (thanks to @wysaid). Before you start, make sure you install the following packages:\n\n```\npip install -r requirements_web_demo.txt\n```\n\nThen run the command below and click on the generated link:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n### CLI Demo\n\nWe provide a CLI demo example in `cli_demo.py`, which supports streaming output for the generation. Users can interact with Qwen-7B-Chat by inputting prompts, and the model returns model outputs in the streaming mode. Run the command below:\n\n```bash\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\nWe provide methods to deploy local API based on OpenAI API (thanks to @hanpenggit). Before you start, install the required packages:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nThen run the command to deploy your API:\n\n```bash\npython openai_api.py\n```\n\nYou can change your arguments, e.g., `-c` for checkpoint name or path, `--cpu-only` for CPU deployment, etc. If you meet problems launching your API deployment, updating the packages to the latest version can probably solve them.\n\nUsing the API is also simple. See the example below:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# create a request activating streaming response\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=True \n    # Specifying stop words in streaming output format is not yet supported and is under development.\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# create a request not activating streaming response\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=False,\n    stop=[] # You can add custom stop words here, e.g., stop=[\"Observation:\"] for ReAct prompting.\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n**Function calling** is also supported (but only when `stream=False` for the moment). See the [example usage](examples/function_call_examples.py) here.\n<br><br>\n\n## ğŸ³ Docker\n\nTo simplify the deployment process, we provide docker images with pre-built environments: [qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen). You only need to install the driver and download model files to launch demos, deploy OpenAI API, and finetune the model.\n\n### Preparation\n\n1. Install the correct version of Nvidia driver depending on the image to use:\n  - `qwenllm/qwen:cu117` (**recommend**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: same as `qwenllm/qwen:cu117`\n\n2. Install and configure [docker](https://docs.docker.com/engine/install/) and [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html):\n\n```bash\n# configure docker\nsudo systemctl start docker\n# test if docker is correctly installed\nsudo docker run hello-world\n\n# configure nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. Download model checkpoints and codes to your environment (see [here](#DownloadModel)).\n\n### Deployment\n\nHere we use Qwen-7B-Chat as an example. Before launching a web demo or API, you can setup the configuration as shown below:\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes\n```\nThe following scripts can help you build:\n\n* OpenAI API\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Web UI\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* CLI Demo\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\nThe commands above will automatically download the required image and launch a Web UI demo in background (the service will auto-restart). You can open `http://localhost:${PORT}` on the host to use the demo.\n\nThe demo is successfully launched if you see the following output:\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nIf you want to check the status of the demo, you can use `docker logs qwen` to display outputs.\n\nYou can use `docker rm -f qwen` to stop the service and remove the container.\n\n\n### Finetuning\n\nThe method of finetuning using the pre-built Docker image is basically the same as [the above chapter](#Finetuning) (we have already installed dependencies in the image):\n\nThe following is an example of single-GPU LoRA:\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs\n\n# Use all host devices by default\nDEVICE=all\n# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# Single-GPU LoRA finetuning\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\nTo make a change to single-GPU Q-LoRA for example, you just need to modify the bash command inside `docker run`:\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## ğŸ”¥ System Prompt\nQwen-1.8-Chat and Qwen-72B-Chat have been fully trained on diverse system prompts with multiple rounds of complex interactions, so that they can follow a variety of system prompts and realize model customization in context, further improving the scalability of Qwen-chat.\n\nWith System Prompt, Qwen-Chat can realize **roly playing**, **language style transfer**, **task setting**, and **behavior setting**.\n\n![](assets/system_prompt_language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\nFor more information, please refer to the [example documentation](examples/system_prompt.md).\n\n## Tool Usage\n\nQwen-Chat has been optimized for tool usage and function calling capabilities. Users can develop agents, LangChain applications, and even augment Qwen with a Python Code Interpreter.\n\nWe provide documentation on how to implement tool calls based on the principle of ReAct Prompting, please refer to [the ReAct example](examples/react_prompt.md). Based on this principle, we provide support for function calling in [openai_api.py](openai_api.py).\n\nWe have tested the model's tool calling capabilities on our open-source Chinese evaluation benchmark and found that Qwen-Chat consistently performs well:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.â†‘)</th><th align=\"center\">Tool Input (Rouge-Lâ†‘)</th><th align=\"center\">False Positive Errorâ†“</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\nTo assess Qwen's ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this [link](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nWe have observed that Qwen performs well in terms of code executability and result accuracy when generating code:\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Mathâ†‘</th><th align=\"center\">Visualization-Hardâ†‘</th><th align=\"center\">Visualization-Easyâ†‘</th><th align=\"center\">Generalâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## Long-Context Understanding\n\nTo extend the context length and break the bottleneck of training sequence length, we introduce several techniques, including NTK-aware interpolation, window attention, and LogN attention scaling, to extend the context length of Qwen-14B from 2K to over 8K tokens, and Qwen-1.8B/7B from 8K to 32K tokens. \n\nFor Qwen-72B, we adapt RoPE to longer contexts with a larger rotary base. Qwen-72B supports the max context length of 32K tokens.\n\nWe conduct language modeling experiments on the arXiv dataset with the PPL evaluation and find that Qwen can reach outstanding performance in the scenario of long context. Results are demonstrated below:\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.56</td><td align=\"center\">4.62</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.49</td><td align=\"center\">4.32</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n    <tr>\n        <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n    </tr>\n</table>\n\nFurthermore, to verify the ability of Qwen-72B-Chat on long text understanding, we tested it on [L-Eval](https://arxiv.org/abs/2307.11088) (closed-ended tasks). The results are as follows:\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\nWe conducted the \"needle in a haystack\" experiment (the idea came from [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)) to test whether the model can retrieve information at different positions in the inputs of different lengths, the result is as follows:\n\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\nThe above results show that Qwen-72B-Chat can accurately retrieve information placed in various positions within an input length of 32k, proving its excellent long text understanding capabilities.\n\n## Tokenizer\n\nOur tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](tokenization_note.md).\n<br><br>\n\n## Reproduction\n\nFor your reproduction of the model performance on benchmark datasets, we provide scripts for you to reproduce the results. Check [eval/EVALUATION.md](eval/EVALUATION.md) for more information. Note that the reproduction may lead to slight differences from our reported results.\n<br><br>\n\n## FAQ\n\nIf you meet problems, please refer to [FAQ](FAQ.md) and the issues first to search a solution before you launch a new issue.\n<br><br>\n\n## Citation\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## License Agreement\n\nThe source code provided at <https://github.com/QwenLM/Qwen> is licensed under the [Apache 2.0 License](./LICENSE) that can be found at the root directory.\n\nResearchers and developers are free to use the codes and model weights of both Qwen and Qwen-Chat. For their commercial use, please check the License Agreement accompanying each model.\n\n- Qwen-72B, Qwen-14B, and Qwen-7B are licensed under the [Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT) that can be found at the corresponding HuggingFace and ModelScope repository. For commercial use, please fill out the form ([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), and [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen)) to apply.\n\n- Qwen-1.8B is licensed under the [Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT) that can be found at the corresponding HuggingFace and ModelScope repository. For commercial use, please contact us.\n<br><br>\n\n## Contact Us\n\nIf you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to qianwen_opensource@alibabacloud.com.\n\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 71.09375,
          "content": "<p align=\"left\">\n    ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href=\"README.md\">English</a>&nbsp ï½œ &nbsp<a href=\"README_JA.md\">æ—¥æœ¬èª</a> ï½œ &nbsp<a href=\"README_FR.md\">FranÃ§ais</a> ï½œ &nbsp<a href=\"README_ES.md\">EspaÃ±ol</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        ğŸ¤— <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://qianwen.aliyun.com\">Web</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://apps.apple.com/cn/app/%E9%80%9A%E4%B9%89%E5%8D%83%E9%97%AE/id6466733523\">APP</a>\n</p>\n<br><br>\n\n> [!Important]\n> Qwen2å·²å¼€ï¼Œæ¬¢è¿å…³æ³¨ï¼çœ‹è¿™é‡Œï¼š[QwenLM/Qwen2](https://github.com/QwenLM/Qwen2)\n>\n> Qwen2æ¨¡å‹ä»£ç å’Œç”¨æ³•ç›¸æ¯”æ­¤å‰ç‰ˆæœ¬æœ‰è¾ƒå¤§ä¸åŒï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨æ–°çš„repoè¿›è¡Œç»´æŠ¤ã€‚æ­¤repo ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) å·²åœæ­¢ä¸»è¦æ›´æ–°ç»´æŠ¤ã€‚\n\n> [!Warning]\n> è¯·å‹¿æ··ç”¨[Qwen](https://github.com/QwenLM/Qwen)å’Œ[Qwen2](https://github.com/QwenLM/Qwen2)ä»£ç ï¼Œä¸¤è€…å¹¶ä¸å…¼å®¹ã€‚\n<br>\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">ğŸ¤—</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">ğŸ¤—</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">ğŸ¤—</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">ğŸ¤—</a> |\n\n\n  \næˆ‘ä»¬å¼€æºäº†**Qwen**ï¼ˆé€šä¹‰åƒé—®ï¼‰ç³»åˆ—å·¥ä½œï¼Œå½“å‰å¼€æºæ¨¡å‹çš„å‚æ•°è§„æ¨¡ä¸º18äº¿ï¼ˆ1.8Bï¼‰ã€70äº¿ï¼ˆ7Bï¼‰ã€140äº¿ï¼ˆ14Bï¼‰å’Œ720äº¿ï¼ˆ72Bï¼‰ã€‚æœ¬æ¬¡å¼€æºåŒ…æ‹¬åŸºç¡€æ¨¡å‹**Qwen**ï¼Œå³**Qwen-1.8B**ã€**Qwen-7B**ã€**Qwen-14B**ã€**Qwen-72B**ï¼Œä»¥åŠå¯¹è¯æ¨¡å‹**Qwen-Chat**ï¼Œå³**Qwen-1.8B-Chat**ã€**Qwen-7B-Chat**ã€**Qwen-14B-Chat**å’Œ**Qwen-72B-Chat**ã€‚æ¨¡å‹é“¾æ¥åœ¨è¡¨æ ¼ä¸­ï¼Œè¯·ç‚¹å‡»äº†è§£è¯¦æƒ…ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬çš„<b><a href=\"https://arxiv.org/abs/2309.16609\">æŠ€æœ¯æŠ¥å‘Š</a></b>ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹è®ºæ–‡é“¾æ¥æŸ¥çœ‹ã€‚\n\nå½“å‰åŸºç¡€æ¨¡å‹å·²ç»ç¨³å®šè®­ç»ƒäº†å¤§è§„æ¨¡é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„æ•°æ®ï¼Œè¦†ç›–å¤šè¯­è¨€ï¼ˆå½“å‰ä»¥ä¸­æ–‡å’Œè‹±æ–‡ä¸ºä¸»ï¼‰ï¼Œæ€»é‡é«˜è¾¾3ä¸‡äº¿tokenã€‚åœ¨ç›¸å…³åŸºå‡†è¯„æµ‹ä¸­ï¼ŒQwenç³»åˆ—æ¨¡å‹æ‹¿å‡ºéå¸¸æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæ˜¾è‘—è¶…å‡ºåŒè§„æ¨¡æ¨¡å‹å¹¶ç´§è¿½ä¸€ç³»åˆ—æœ€å¼ºçš„é—­æºæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨SFTå’ŒRLHFæŠ€æœ¯å®ç°å¯¹é½ï¼Œä»åŸºåº§æ¨¡å‹è®­ç»ƒå¾—åˆ°å¯¹è¯æ¨¡å‹ã€‚Qwen-Chatå…·å¤‡èŠå¤©ã€æ–‡å­—åˆ›ä½œã€æ‘˜è¦ã€ä¿¡æ¯æŠ½å–ã€ç¿»è¯‘ç­‰èƒ½åŠ›ï¼ŒåŒæ—¶è¿˜å…·å¤‡ä¸€å®šçš„ä»£ç ç”Ÿæˆå’Œç®€å•æ•°å­¦æ¨ç†çš„èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é’ˆå¯¹LLMå¯¹æ¥å¤–éƒ¨ç³»ç»Ÿç­‰æ–¹é¢é’ˆå¯¹æ€§åœ°åšäº†ä¼˜åŒ–ï¼Œå½“å‰å…·å¤‡è¾ƒå¼ºçš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œä»¥åŠæœ€è¿‘å¤‡å—å…³æ³¨çš„Code Interpreterçš„èƒ½åŠ›å’Œæ‰®æ¼”Agentçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å„ä¸ªå¤§å°æ¨¡å‹çš„ç‰¹ç‚¹åˆ—åˆ°äº†ä¸‹è¡¨ã€‚\n\n| æ¨¡å‹        |   å¼€æºæ—¥æœŸ   | æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ | System Promptå¼ºåŒ– | é¢„è®­ç»ƒtokenæ•° | å¾®è°ƒï¼ˆQ-Loraï¼‰æœ€å°GPUç”¨é‡ | ç”Ÿæˆ2048ä¸ªtokençš„æœ€å°æ˜¾å­˜å ç”¨ï¼ˆInt4ï¼‰ | å·¥å…·è°ƒç”¨ |\n|:----------|:--------:|:-------:|:---------------:|:---------:|:-----------------:|:-------------------:|:----:|\n| Qwen-1.8B | 23.11.30 |   32K   |        âœ…        |   2.2T    |       5.8GB       |        2.9GB        |  âœ…   |  \n| Qwen-7B   | 23.08.03 |   32K   |        â        |   2.4T    |      11.5GB       |        8.2GB        |  âœ…   |   \n| Qwen-14B  | 23.09.25 |   8K    |        â        |   3.0T    |      18.7GB       |       13.0GB        |  âœ…   |\n| Qwen-72B  | 23.11.30 |   32K   |        âœ…        |   3.0T    |      61.4GB       |       48.9GB        |  âœ…   |   \n\n  \nåœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œä½ å¯ä»¥äº†è§£åˆ°ä»¥ä¸‹å†…å®¹\n\n* å¿«é€Ÿä¸Šæ‰‹Qwen-Chatæ•™ç¨‹ï¼Œç©è½¬å¤§æ¨¡å‹æ¨ç†\n* é‡åŒ–æ¨¡å‹ç›¸å…³ç»†èŠ‚ï¼ŒåŒ…æ‹¬GPTQå’ŒKV cacheé‡åŒ–\n* æ¨ç†æ€§èƒ½æ•°æ®ï¼ŒåŒ…æ‹¬æ¨ç†é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨\n* å¾®è°ƒçš„æ•™ç¨‹ï¼Œå¸®ä½ å®ç°å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRA\n* éƒ¨ç½²æ•™ç¨‹ï¼Œä»¥vLLMå’ŒFastChatä¸ºä¾‹\n* æ­å»ºDemoçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬WebUIå’ŒCLI Demo\n* æ­å»ºAPIçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æä¾›çš„ç¤ºä¾‹ä¸ºOpenAIé£æ ¼çš„API\n* æ›´å¤šå…³äºQwenåœ¨å·¥å…·è°ƒç”¨ã€Code Interpreterã€Agentæ–¹é¢çš„å†…å®¹\n* é•¿åºåˆ—ç†è§£èƒ½åŠ›åŠè¯„æµ‹\n* ä½¿ç”¨åè®®\n* ...\n\nå¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ä¼˜å…ˆè€ƒè™‘æŸ¥è¯¢[FAQ](FAQ.md)ã€‚å¦‚ä»æœªè§£å†³ï¼Œéšæ—¶æå‡ºissueï¼ˆä½†å»ºè®®ä½¿ç”¨è‹±è¯­æˆ–æä¾›ç¿»è¯‘ï¼Œæœ‰åŠ©äºå¸®åŠ©æ›´å¤šç”¨æˆ·ï¼‰ã€‚å¦‚æœæƒ³å¸®åŠ©æˆ‘ä»¬æå‡ï¼Œæ¬¢è¿æäº¤Pull Requestsï¼\n\næƒ³å’Œæˆ‘ä»¬ä¸€èµ·è®¨è®ºå’ŒèŠå¤©çš„è¯ï¼Œèµ¶ç´§åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å’ŒDiscord serverï¼ˆå…¥å£è§æ–‡æ¡£å¼€å¤´éƒ¨åˆ†ï¼‰ï¼\n<br><br>\n\n## æ–°é—»\n\n* 2023.11.30 ğŸ”¥ æˆ‘ä»¬æ¨å‡º **Qwen-72B** å’Œ **Qwen-72B-Chat**ï¼Œå®ƒä»¬åœ¨ 3T tokensä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶æ”¯æŒ 32k ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ä¹Ÿå‘å¸ƒäº† **Qwen-1.8B** å’Œ **Qwen-1.8B-Chat**ã€‚æˆ‘ä»¬è¿˜å¢å¼ºäº† Qwen-72B-Chat å’Œ Qwen-1.8B-Chat çš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆSystem Promptï¼‰åŠŸèƒ½ï¼Œè¯·å‚é˜…[ç¤ºä¾‹æ–‡æ¡£](examples/system_prompt.md)ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹**æ˜‡è…¾910**ä»¥åŠ**æµ·å…‰DCU**å®ç°äº†æ¨ç†çš„æ”¯æŒï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹`ascend-support`åŠ`dcu-support`æ–‡ä»¶å¤¹ã€‚\n* 2023å¹´10æœˆ17æ—¥ æˆ‘ä»¬æ¨å‡ºäº†Int8é‡åŒ–æ¨¡å‹**Qwen-7B-Chat-Int8**å’Œ**Qwen-14B-Chat-Int8**ã€‚\n* 2023å¹´9æœˆ25æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging Faceæ¨å‡º**Qwen-14B**å’Œ**Qwen-14B-Chat**æ¨¡å‹ï¼Œå¹¶å¼€æº [qwen.cpp](https://github.com/QwenLM/qwen.cpp) å’Œ [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)ã€‚**Qwen-7B**å’Œ**Qwen-7B-Chat**çš„ä»£ç å’Œæ¨¡å‹ä¹ŸåŒæ­¥å¾—åˆ°æ›´æ–°ã€‚**è¯·ä½¿ç”¨æœ€æ–°çš„ä»£ç å’Œæ¨¡å‹ï¼**\n    - ç›¸æ¯”åŸç‰ˆQwen-7Bï¼Œæ–°ç‰ˆç”¨äº†æ›´å¤šè®­ç»ƒæ•°æ®ï¼ˆä»2.2Tå¢åŠ åˆ°2.4T tokensï¼‰ï¼Œåºåˆ—é•¿åº¦ä»2048æ‰©å±•è‡³8192ã€‚æ•´ä½“ä¸­æ–‡èƒ½åŠ›ä»¥åŠä»£ç èƒ½åŠ›å‡æœ‰æ‰€æå‡ã€‚\n* 2023å¹´9æœˆ12æ—¥ æ”¯æŒQwen-7Bå’ŒQwen-7B-Chatçš„å¾®è°ƒï¼Œå…¶ä¸­åŒ…æ‹¬å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRAã€‚\n* 2023å¹´8æœˆ21æ—¥ å‘å¸ƒQwen-7B-Chatçš„Int4é‡åŒ–æ¨¡å‹ï¼ŒQwen-7B-Chat-Int4ã€‚è¯¥æ¨¡å‹æ˜¾å­˜å ç”¨ä½ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åŠç²¾åº¦æ¨¡å‹æ˜¾è‘—æå‡ï¼Œåœ¨åŸºå‡†è¯„æµ‹ä¸Šæ•ˆæœæŸå¤±è¾ƒå°ã€‚\n* 2023å¹´8æœˆ3æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging FaceåŒæ­¥æ¨å‡ºQwen-7Bå’ŒQwen-7B-Chatæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æŠ€æœ¯å¤‡å¿˜å½•ï¼Œä»‹ç»äº†ç›¸å…³çš„è®­ç»ƒç»†èŠ‚å’Œæ¨¡å‹è¡¨ç°ã€‚\n<br>\n\n## è¯„æµ‹è¡¨ç°\n\nQwenç³»åˆ—æ¨¡å‹ç›¸æ¯”åŒè§„æ¨¡æ¨¡å‹å‡å®ç°äº†æ•ˆæœçš„æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬è¯„æµ‹çš„æ•°æ®é›†åŒ…æ‹¬MMLUã€C-Evalã€ GSM8Kã€ MATHã€HumanEvalã€MBPPã€BBHç­‰æ•°æ®é›†ï¼Œè€ƒå¯Ÿçš„èƒ½åŠ›åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ã€çŸ¥è¯†ã€æ•°å­¦è®¡ç®—å’Œæ¨ç†ã€ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰ã€‚Qwen-72Båœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†LLaMA2-70Bçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨10é¡¹ä»»åŠ¡ä¸­çš„7é¡¹ä»»åŠ¡ä¸­è¶…è¶ŠGPT-3.5.\n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=\"600\"/>\n<p>\n<br>\n\n| Model              |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:-------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                    |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B          |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B         |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B         |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B        |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B        |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B       |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B       |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B      |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |   -      |   26.3    |   -      |  -       |   -      |\n| **Qwen-1.8B**      |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**        |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**       |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**       | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\n\nå¯¹äºä»¥ä¸Šæ‰€æœ‰å¯¹æ¯”æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†å…¶å®˜æ–¹æ±‡æŠ¥ç»“æœä¸[OpenCompass](https://opencompass.org.cn/leaderboard-llm)ç»“æœä¹‹é—´çš„æœ€ä½³åˆ†æ•°ã€‚\n\næ›´å¤šçš„å®éªŒç»“æœå’Œç»†èŠ‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚ç‚¹å‡»[è¿™é‡Œ](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf)ã€‚\n<br><br>\n\n## è¦æ±‚\n\n* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬\n* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬\n* transformers 4.32åŠä»¥ä¸Šç‰ˆæœ¬\n* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰\n<br>\n\n## å¿«é€Ÿä½¿ç”¨\n\næˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ğŸ¤– ModelScopeå’ŒğŸ¤— Transformerså¿«é€Ÿä½¿ç”¨Qwen-7Bå’ŒQwen-7B-Chatã€‚\n\nä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬é¢„æ„å»ºå¥½çš„Dockeré•œåƒï¼Œçœå»å¤§éƒ¨åˆ†é…ç½®ç¯å¢ƒçš„æ“ä½œï¼Œè¯¦æƒ…è§[â€œä½¿ç”¨é¢„æ„å»ºçš„dockeré•œåƒâ€](#-ä½¿ç”¨é¢„æ„å»ºçš„dockeré•œåƒ)ä¸€èŠ‚ã€‚\n\nå¦‚ä¸ä½¿ç”¨Dockerï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚\n\n```bash\npip install -r requirements.txt\n```\n\nå¦‚æœä½ çš„æ˜¾å¡æ”¯æŒfp16æˆ–bf16ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜æ¨èå®‰è£…[flash-attention](https://github.com/Dao-AILab/flash-attention)ï¼ˆ**å½“å‰å·²æ”¯æŒflash attention 2**ï¼‰æ¥æé«˜ä½ çš„è¿è¡Œæ•ˆç‡ä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚(**flash-attentionåªæ˜¯å¯é€‰é¡¹ï¼Œä¸å®‰è£…ä¹Ÿå¯æ­£å¸¸è¿è¡Œè¯¥é¡¹ç›®**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚\n# pip install csrc/layer_norm\n# å¦‚æœflash-attnç‰ˆæœ¬é«˜äº2.1.1ï¼Œä¸‹æ–¹æ— éœ€å®‰è£…ã€‚\n# pip install csrc/rotary\n```\n\næ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚\n\n### ğŸ¤— Transformers\n\nå¦‚å¸Œæœ›ä½¿ç”¨Qwen-chatè¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ï¼Œå¹¶æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°å’Œè·¯å¾„ï¼Œå¦‚`Qwen/Qwen-7B-Chat`å’Œ`Qwen/Qwen-14B-Chat`**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# ç¬¬ä¸€è½®å¯¹è¯\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚\n\n# ç¬¬äºŒè½®å¯¹è¯\nresponse, history = model.chat(tokenizer, \"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\n# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\n# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚\n# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚\n# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚\n# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚\n# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚\n\n# ç¬¬ä¸‰è½®å¯¹è¯\nresponse, history = model.chat(tokenizer, \"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹\n```\n\nè¿è¡ŒQwenåŒæ ·éå¸¸ç®€å•ã€‚\n\n<details>\n  <summary>è¿è¡ŒQwen</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\n# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...\n```\n\n</details>\n\n<p id=\"DownloadModel\">\nè‹¥åœ¨ä½¿ç”¨ä¸Šè¿°ä»£ç æ—¶ç”±äºå„ç§åŸå› æ— æ³•ä» HuggingFace æ‹‰å–æ¨¡å‹å’Œä»£ç ï¼Œå¯ä»¥å…ˆä» ModelScope ä¸‹è½½æ¨¡å‹åŠä»£ç è‡³æœ¬åœ°ï¼Œå†ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼š\n</p>\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n# model_dir = snapshot_download('qwen/Qwen-14B')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### ğŸ¤– ModelScope\n\né­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\n\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹\", history=history)\nprint(response)\n```\n\n### Batchæ¨ç†\nåƒé—®æ”¯æŒbatchæ‰¹é‡æ¨ç†ã€‚åœ¨å¼€å¯flash-attentionçš„çŠ¶æ€ä¸‹ï¼Œä½¿ç”¨batchæ¨ç†å¯ä»¥çº¦40%çš„æé€Ÿã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\", history=None)\nprint(response)\n```\n\n### CPU\n\næˆ‘ä»¬æ¨èä½ ä½¿ç”¨ [qwen.cpp](https://github.com/QwenLM/qwen.cpp) æ¥å®ç°CPUéƒ¨ç½²å’Œæ¨ç†ã€‚qwen.cppæ˜¯Qwenå’Œtiktokençš„C++å®ç°ã€‚ä½ å¯ä»¥ç‚¹å‡»é“¾æ¥è¿›å…¥repoäº†è§£è¯¦æƒ…ã€‚\n\nå½“ç„¶ï¼Œç›´æ¥åœ¨CPUä¸Šè¿è¡Œæ¨¡å‹ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nä½†æ˜¯ï¼Œè¿™æ ·çš„æ¨ç†æ•ˆç‡å¤§æ¦‚ç‡ä¼šéå¸¸ä½ã€‚\n\n### å¤šGPU\n\nå¦‚æœä½ é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜è€Œå¸Œæœ›ä½¿ç”¨å¤šå¼ GPUè¿›è¡Œæ¨ç†ï¼Œå¯ä»¥ä½¿ç”¨ä¸Šè¿°çš„é»˜è®¤çš„ä½¿ç”¨æ–¹æ³•è¯»å–æ¨¡å‹ã€‚æ­¤å‰æä¾›çš„è„šæœ¬`utils.py`å·²åœæ­¢ç»´æŠ¤ã€‚\n\nå°½ç®¡è¿™ä¸ªæ–¹æ³•å¾ˆç®€å•ï¼Œä½†å®ƒçš„æ•ˆç‡ç›¸å¯¹è¾ƒä½ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨vLLMå’ŒFastChatå¹¶è¯·é˜…è¯»éƒ¨ç½²ç« èŠ‚ã€‚\n\n### x86 å¹³å°\nåœ¨ é…·ç¿â„¢/è‡³å¼ºÂ® å¯æ‰©å±•å¤„ç†å™¨æˆ– Arcâ„¢ GPU ä¸Šéƒ¨ç½²é‡åŒ–æ¨¡å‹æ—¶ï¼Œå»ºè®®ä½¿ç”¨ [OpenVINOâ„¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html) ä»¥å……åˆ†åˆ©ç”¨ç¡¬ä»¶ï¼Œå®ç°æ›´å¥½çš„æ¨ç†æ€§èƒ½ã€‚æ‚¨å¯ä»¥å®‰è£…å¹¶è¿è¡Œæ­¤[example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)ã€‚ç›¸å…³é—®é¢˜ï¼Œæ‚¨å¯åœ¨ [OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues)ä¸­æäº¤ã€‚\n\n\n### é˜¿é‡Œäº‘çµç§¯ï¼ˆDashScopeï¼‰APIæœåŠ¡\næœ€ç®€å•çš„ä½¿ç”¨Qwenæ¨¡å‹APIæœåŠ¡çš„æ–¹æ³•å°±æ˜¯é€šè¿‡DashScopeï¼ˆé˜¿é‡Œäº‘çµç§¯APIæ¨¡å‹æœåŠ¡ï¼‰ã€‚æˆ‘ä»¬æä¾›äº†ç®€å•ä»‹ç»è¯´æ˜ä½¿ç”¨æ–¹æ³•ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†è‡ªå·±éƒ¨ç½²OpenAIæ ¼å¼çš„APIçš„æ–¹æ³•ã€‚\n\nDashScopeæ˜¯é˜¿é‡Œäº‘æä¾›çš„å¤§è¯­è¨€æ¨¡å‹çš„APIæœåŠ¡ï¼Œç›®å‰æ”¯æŒQwenã€‚ä½†è¯·æ³¨æ„ï¼Œç›®å‰æä¾›æœåŠ¡çš„Qwenæ¨¡å‹ä¸ºå†…éƒ¨æ¨¡å‹ï¼Œæš‚æ— æ›´å¤šå…·ä½“ç»†èŠ‚å¯¹å¤–é€éœ²ã€‚æ¨¡å‹æœåŠ¡åŒ…æ‹¬`qwen-turbo`ã€`qwen-plus`å’Œ`qwen-max`ï¼Œ`qwen-turbo`é€Ÿåº¦æ›´å¿«ï¼Œ`qwen-plus`æ•ˆæœæ›´ä¼˜ï¼Œ`qwen-max`æ˜¯æœ€æ–°å‘å¸ƒçš„åƒäº¿çº§é€šä¹‰åƒé—®2.0æ¨¡å‹ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹[æ–‡æ¡£](https://dashscope.aliyun.com)ã€‚\n\nè¯·é¦–å…ˆå‰å¾€[å®˜ç½‘](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn)å¼€é€šDashScopeï¼Œè·å¾—API Keyï¼ˆAKï¼‰ã€‚å»ºè®®é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®AKï¼š\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\néšåå®‰è£…ç›¸å…³ä»£ç åŒ…ï¼Œç‚¹å‡»[æ­¤å¤„](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk)æŸ¥çœ‹å®‰è£…æ–‡æ¡£ã€‚å¦‚ä½¿ç”¨pythonï¼Œåˆ™ç›´æ¥é€šè¿‡pipå®‰è£…ï¼š\n```bash\npip install dashscope\n```\nå¦‚å®‰è£…JAVA SDKï¼Œåˆ™é€šè¿‡å¦‚ä¸‹å‘½ä»¤å®‰è£…ï¼š\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\næœ€ç®€å•çš„ä½¿ç”¨æ–¹æ³•å°±æ˜¯é€šè¿‡messagesè°ƒç”¨ï¼Œç”¨æ³•ç±»ä¼¼OpenAI APIã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\næ›´å¤šç”¨æ³•è¯·æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£äº†è§£è¯¦æƒ…ã€‚\n<br><br>\n\n\n## é‡åŒ–\n\n### GPTQ\n\næˆ‘ä»¬æä¾›äº†åŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶å¼€æºäº†Int4å’ŒInt8é‡åŒ–æ¨¡å‹ã€‚é‡åŒ–æ¨¡å‹çš„æ•ˆæœæŸå¤±å¾ˆå°ï¼Œä½†èƒ½æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å¹¶æå‡æ¨ç†é€Ÿåº¦ã€‚\n\nä»¥ä¸‹æˆ‘ä»¬æä¾›ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨Int4é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆä¿è¯æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch 2.0åŠä»¥ä¸Šï¼Œtransformersç‰ˆæœ¬ä¸º4.32.0åŠä»¥ä¸Šï¼Œç­‰ç­‰ï¼‰ï¼Œå¹¶å®‰è£…æ‰€éœ€å®‰è£…åŒ…ï¼š\n\n```bash\npip install auto-gptq optimum\n```\n\nå¦‚å®‰è£…`auto-gptq`é‡åˆ°é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åˆ°å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ)æœç´¢åˆé€‚çš„wheelã€‚\n\n> æ³¨æ„ï¼šé¢„ç¼–è¯‘çš„`auto-gptq`ç‰ˆæœ¬å¯¹`torch`ç‰ˆæœ¬åŠå…¶CUDAç‰ˆæœ¬è¦æ±‚ä¸¥æ ¼ã€‚åŒæ—¶ï¼Œç”±äº\n> å…¶è¿‘æœŸæ›´æ–°ï¼Œä½ å¯èƒ½ä¼šé‡åˆ°`transformers`ã€`optimum`æˆ–`peft`æŠ›å‡ºçš„ç‰ˆæœ¬é”™è¯¯ã€‚\n> æˆ‘ä»¬å»ºè®®ä½¿ç”¨ç¬¦åˆä»¥ä¸‹è¦æ±‚çš„æœ€æ–°ç‰ˆæœ¬ï¼š\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0\n\néšåå³å¯ä½¿ç”¨å’Œä¸Šè¿°ä¸€è‡´çš„ç”¨æ³•è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š\n\n```python\n# å¯é€‰æ¨¡å‹åŒ…æ‹¬ï¼š\"Qwen/Qwen-7B-Chat-Int4\", \"Qwen/Qwen-14B-Chat-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\næˆ‘ä»¬å¯¹BF16ï¼ŒInt8å’ŒInt4æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹ä¸Šåšäº†æµ‹è¯•ï¼Œå‘ç°é‡åŒ–æ¨¡å‹æ•ˆæœæŸå¤±è¾ƒå°ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n<br>\n\n\n### KV cacheé‡åŒ–\n\n> æ³¨æ„ï¼šç”±äºHugging Faceçš„å†…éƒ¨å®ç°ï¼Œæœ¬åŠŸèƒ½çš„æ”¯æŒæ–‡ä»¶`cache_autogptq_cuda_256.cpp`ä¸`cache_autogptq_cuda_kernel_256.cu`å¯èƒ½æ²¡è¢«ä¸‹è½½ã€‚å¦‚éœ€å¼€å¯ä½¿ç”¨ï¼Œè¯·æ‰‹åŠ¨ä»ç›¸å…³ä½ç½®ä¸‹è½½ï¼Œå¹¶æ”¾ç½®åˆ°ç›¸åº”æ–‡ä»¶ä¸­ã€‚\n\nåœ¨æ¨¡å‹æ¨ç†æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸­é—´ç»“æœkeyä»¥åŠvalueçš„å€¼é‡åŒ–åå‹ç¼©å­˜å‚¨ï¼Œè¿™æ ·ä¾¿å¯ä»¥åœ¨ç›¸åŒçš„å¡ä¸Šå­˜å‚¨æ›´å¤šçš„keyä»¥åŠvalueï¼Œå¢åŠ æ ·æœ¬ååã€‚\n\næˆ‘ä»¬åœ¨`config.json`é‡Œæä¾›äº†`use_cache_quantization`å’Œ`use_cache_kernel`ä¸¤ä¸ªå‚æ•°æ¥æ§åˆ¶æ˜¯å¦å¯ç”¨KV cacheé‡åŒ–ï¼Œå…·ä½“ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\næ³¨æ„ï¼šå½“å‰è¯¥åŠŸèƒ½ä¸æ”¯æŒä¸flash attentionåŒæ—¶å¼€å¯ï¼Œå¦‚æœä½ å¼€äº†KV cacheé‡åŒ–çš„åŒæ—¶åˆå¼€äº†flash attentionï¼ˆ`use_flash_attn=True`ï¼Œ `use_cache_quantization=True`, `use_cache_kernel=True`ï¼‰ï¼Œç¨‹åºé»˜è®¤å°†å…³é—­`use_flash_attn`ã€‚\n\næ•ˆæœæ–¹é¢ï¼Œæˆ‘ä»¬éªŒè¯è¿‡Int8 KV Cacheçš„ä½¿ç”¨å¯¹æ¨¡å‹æ•´ä½“çš„ç²¾åº¦æŒ‡æ ‡åŸºæœ¬æ— æŸã€‚æˆ‘ä»¬åšäº†é’ˆå¯¹æ˜¾å­˜å ç”¨çš„æ€§èƒ½æµ‹è¯•ã€‚è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œæ¨¡å‹é»˜è®¤ä½¿ç”¨BF16æ ¼å¼ï¼Œé»˜è®¤ç”Ÿæˆ1024ä¸ªtokenï¼Œå…¶ä¸­OOMè¡¨ç¤ºå†…å­˜ä¸è¶³ã€‚\n\nå¼€å¯äº†KV cacheé‡åŒ–ä¹‹åï¼Œæ¨¡å‹åœ¨æ¨ç†çš„æ—¶å€™å¯ä»¥å¼€å¯æ›´å¤§çš„batch size (bs)ã€‚\n\n| USE KV Cache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|--------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| No           | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  oom   |  oom   |\n| Yes          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\n\nå¼€å¯äº†KV cacheé‡åŒ–ä¹‹åï¼Œæ¨¡å‹åœ¨æ¨ç†æ—¶å¯åœ¨ç”Ÿæˆæ›´é•¿çš„åºåˆ—ï¼ˆslï¼Œç”Ÿæˆçš„tokenæ•°ï¼‰æ—¶ï¼ŒèŠ‚çº¦æ›´å¤šçš„æ˜¾å­˜ã€‚\n\n| USE KV Cache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|--------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| no           | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| yes          |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\n\nå¼€å¯KV cacheé‡åŒ–åï¼Œæ¨¡å‹åœ¨æ¨ç†æ—¶ä¼šå°†åŸå§‹å­˜è¿›`layer-past`çš„floatæ ¼å¼çš„key/valueè½¬æ¢æˆint8æ ¼å¼ï¼ŒåŒæ—¶å­˜å‚¨é‡åŒ–éƒ¨åˆ†çš„å‚æ•°ã€‚\n\nå…·ä½“æ“ä½œå¦‚ä¸‹ï¼š\n\n1. å°†key/valueè¿›è¡Œé‡åŒ–æ“ä½œ\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n2. å­˜å…¥`layer_past`ä¸­:\n\né‡åŒ–æ ¼å¼çš„`layer-past`:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\nåŸå§‹æ ¼å¼çš„`layer-past`:\n```\n    layer_past=(key,value)\n```\nå¦‚æœéœ€è¦å°†`layer-past`ä¸­å­˜å¥½çš„keyï¼Œvalueç›´æ¥å–å‡ºä½¿ç”¨ï¼Œå¯ä»¥ä½¿ç”¨åé‡åŒ–æ“ä½œå°†Int8æ ¼å¼çš„key/valueè½¬å›floatæ ¼å¼ï¼š\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n### æ¨ç†æ€§èƒ½\nè¿™ä¸€éƒ¨åˆ†å°†ä»‹ç»æ¨¡å‹æ¨ç†çš„é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨çš„ç›¸å…³æ•°æ®ã€‚ä¸‹æ–‡çš„æ€§èƒ½æµ‹ç®—ä½¿ç”¨ [æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py) å®Œæˆã€‚\n\næˆ‘ä»¬æµ‹ç®—äº†BF16ã€Int8å’ŒInt4æ¨¡å‹åœ¨ç”Ÿæˆ2048ä¸ªtokenæ—¶çš„å¹³å‡æ¨ç†é€Ÿåº¦ï¼ˆtokens/sï¼‰å’Œæ˜¾å­˜ä½¿ç”¨ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nè¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼ˆé™¤éæåˆ°ä½¿ç”¨2xA100ï¼‰ï¼Œä½¿ç”¨PyTorch 2.0.1ã€CUDA 11.8å’ŒFlash-Attention2ã€‚(72B + vLLM ä½¿ç”¨ PyTorch 2.1.0å’ŒCuda 11.8.)æ¨ç†é€Ÿåº¦æ˜¯ç”Ÿæˆ2048ä¸ªtokençš„é€Ÿåº¦å‡å€¼ã€‚\n\næ³¨æ„ï¼šä»¥ä¸ŠInt4/Int8æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä½¿ç”¨autogptqåº“ç»™å‡ºï¼Œå½“å‰``AutoModelForCausalLM.from_pretrained``è½½å…¥çš„æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä¼šæ…¢å¤§çº¦20%ã€‚æˆ‘ä»¬å·²ç»å°†è¯¥é—®é¢˜æ±‡æŠ¥ç»™HuggingFaceå›¢é˜Ÿï¼Œè‹¥æœ‰è§£å†³æ–¹æ¡ˆå°†å³æ—¶æ›´æ–°ã€‚\n\næˆ‘ä»¬è¿˜æµ‹é‡äº†ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦ã€ç”Ÿæˆé•¿åº¦ã€Flash-Attentionç‰ˆæœ¬çš„æ¨ç†é€Ÿåº¦å’Œ GPU å†…å­˜ä½¿ç”¨æƒ…å†µã€‚å¯ä»¥åœ¨ Hugging Face æˆ– ModelScope ä¸Šçš„ç›¸åº”çš„æ¨¡å‹ä»‹ç»é¡µé¢æ‰¾åˆ°ç»“æœã€‚\n\n## å¾®è°ƒ\n\n### ä½¿ç”¨æ–¹æ³•\næˆ‘ä»¬æä¾›äº†`finetune.py`è¿™ä¸ªè„šæœ¬ä¾›ç”¨æˆ·å®ç°åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„åŠŸèƒ½ï¼Œä»¥æ¥å…¥ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†shellè„šæœ¬å‡å°‘ç”¨æˆ·çš„å·¥ä½œé‡ã€‚è¿™ä¸ªè„šæœ¬æ”¯æŒ [DeepSpeed](https://github.com/microsoft/DeepSpeed) å’Œ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) ã€‚æˆ‘ä»¬æä¾›çš„shellè„šæœ¬ä½¿ç”¨äº†DeepSpeedï¼Œå› æ­¤å»ºè®®æ‚¨ç¡®ä¿å·²ç»å®‰è£…DeepSpeedå’ŒPeftï¼ˆæ³¨æ„ï¼šDeepSpeedå¯èƒ½ä¸å…¼å®¹æœ€æ–°çš„pydanticç‰ˆæœ¬ï¼Œè¯·ç¡®ä¿`pydantic<2.0`ï¼‰ã€‚ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ï¼š\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\né¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡ä½ çš„è®­ç»ƒæ•°æ®ã€‚ä½ éœ€è¦å°†æ‰€æœ‰æ ·æœ¬æ”¾åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­å¹¶å­˜å…¥jsonæ–‡ä»¶ä¸­ã€‚æ¯ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«idå’Œconversationï¼Œå…¶ä¸­åè€…ä¸ºä¸€ä¸ªåˆ—è¡¨ã€‚ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚\"\n      }\n    ]\n  }\n]\n```\n\nå‡†å¤‡å¥½æ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„shellè„šæœ¬å®ç°å¾®è°ƒã€‚æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šä½ çš„æ•°æ®çš„è·¯å¾„ã€‚\n\nå¾®è°ƒè„šæœ¬èƒ½å¤Ÿå¸®ä½ å®ç°ï¼š\n- å…¨å‚æ•°å¾®è°ƒ\n- LoRA\n- Q-LoRA\n\nå…¨å‚æ•°å¾®è°ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªè„šæœ¬å¼€å§‹è®­ç»ƒï¼š\n\n```bash\n# åˆ†å¸ƒå¼è®­ç»ƒã€‚ç”±äºæ˜¾å­˜é™åˆ¶å°†å¯¼è‡´å•å¡è®­ç»ƒå¤±è´¥ï¼Œæˆ‘ä»¬ä¸æä¾›å•å¡è®­ç»ƒè„šæœ¬ã€‚\nbash finetune/finetune_ds.sh\n```\n\nå°¤å…¶æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°æˆ–è·¯å¾„ã€æ•°æ®è·¯å¾„ã€ä»¥åŠæ¨¡å‹è¾“å‡ºçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚åœ¨è¿™ä¸ªè„šæœ¬ä¸­æˆ‘ä»¬ä½¿ç”¨äº†DeepSpeed ZeRO 3ã€‚å¦‚æœä½ æƒ³ä¿®æ”¹è¿™ä¸ªé…ç½®ï¼Œå¯ä»¥åˆ é™¤æ‰`--deepspeed`è¿™ä¸ªè¾“å…¥æˆ–è€…è‡ªè¡Œæ ¹æ®éœ€æ±‚ä¿®æ”¹DeepSpeedé…ç½®jsonæ–‡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå› æ­¤ä½ å¯ä»¥è®¾ç½®`--bf16 True`æˆ–è€…`--fp16 True`ã€‚åœ¨ä½¿ç”¨fp16æ—¶ï¼Œè¯·ä½¿ç”¨DeepSpeedæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€‚ç»éªŒä¸Šï¼Œå¦‚æœä½ çš„æœºå™¨æ”¯æŒbf16ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨bf16ï¼Œè¿™æ ·å¯ä»¥å’Œæˆ‘ä»¬çš„é¢„è®­ç»ƒå’Œå¯¹é½è®­ç»ƒä¿æŒä¸€è‡´ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æŠŠé»˜è®¤é…ç½®è®¾ä¸ºå®ƒçš„åŸå› ã€‚\n\nè¿è¡ŒLoRAçš„æ–¹æ³•ç±»ä¼¼å…¨å‚æ•°å¾®è°ƒã€‚ä½†åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…`peft`ä»£ç åº“ã€‚å¦å¤–ï¼Œè®°ä½è¦è®¾ç½®æ­£ç¡®çš„æ¨¡å‹ã€æ•°æ®å’Œè¾“å‡ºè·¯å¾„ã€‚æˆ‘ä»¬å»ºè®®ä½ ä¸ºæ¨¡å‹è·¯å¾„ä½¿ç”¨ç»å¯¹è·¯å¾„ã€‚è¿™æ˜¯å› ä¸ºLoRAä»…å­˜å‚¨adapteréƒ¨åˆ†å‚æ•°ï¼Œè€Œadapteré…ç½®jsonæ–‡ä»¶è®°å½•äº†é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œç”¨äºè¯»å–é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚åŒæ ·ï¼Œä½ å¯ä»¥è®¾ç½®bf16æˆ–è€…fp16ã€‚\n\n```bash\n# å•å¡è®­ç»ƒ\nbash finetune/finetune_lora_single_gpu.sh\n# åˆ†å¸ƒå¼è®­ç»ƒ\nbash finetune/finetune_lora_ds.sh\n```\n\nä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRA ([è®ºæ–‡](https://arxiv.org/abs/2106.09685)) åªæ›´æ–°adapterå±‚çš„å‚æ•°è€Œæ— éœ€æ›´æ–°åŸæœ‰è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•å…è®¸ç”¨æˆ·ç”¨æ›´ä½çš„æ˜¾å­˜å¼€é”€æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ„å‘³ç€æ›´å°çš„è®¡ç®—å¼€é”€ã€‚\n\næ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œè€Œéchatæ¨¡å‹ï¼Œæ¨¡å‹çš„embeddingå’Œè¾“å‡ºå±‚çš„å‚æ•°å°†è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰å­¦ä¹ è¿‡ChatMLæ ¼å¼ä¸­çš„ç‰¹æ®Štokenï¼Œå› æ­¤éœ€è¦å°†è¿™éƒ¨åˆ†å‚æ•°è®¾ä¸ºå¯è®­ç»ƒæ‰èƒ½è®©æ¨¡å‹å­¦ä¼šç†è§£å’Œé¢„æµ‹è¿™äº›tokenã€‚è¿™ä¹Ÿæ„å‘³ç€ï¼Œå‡å¦‚ä½ çš„è®­ç»ƒå¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œä½ éœ€è¦é€šè¿‡ä»£ç ä¸­çš„`modules_to_save`å°†è¿™äº›å‚æ•°è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚æ­¤å¤–ï¼Œè¿™éƒ¨åˆ†è®­ç»ƒå‚æ•°çš„å¼•å…¥ä¼šå½±å“ZeRO 3çš„ä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬é»˜è®¤æ¨èä½¿ç”¨ZeRO 2ã€‚å½“ç„¶ï¼Œå¦‚æœä½ ä¸éœ€è¦å¼•å…¥è¿™éƒ¨åˆ†è®­ç»ƒå‚æ•°ï¼Œä½ å¯ä»¥é€šè¿‡æ›¿æ¢DeepSpeedçš„é…ç½®æ–‡ä»¶æ¥ä½¿ç”¨ZeRO 3ã€‚å¦‚æœä½ æƒ³èŠ‚çœæ˜¾å­˜å ç”¨ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨chatæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œæ˜¾å­˜å ç”¨å°†å¤§å¹…åº¦é™ä½ã€‚ä¸‹æ–‡çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„è®°å½•å°†è¯¦ç»†ä»‹ç»è¿™éƒ¨åˆ†ç»†èŠ‚ã€‚\n\nå¦‚æœä½ ä¾ç„¶é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨Q-LoRA ([è®ºæ–‡](https://arxiv.org/abs/2305.14314)) ã€‚è¯¥æ–¹æ³•ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹ä»¥åŠpaged attentionç­‰æŠ€æœ¯å®ç°æ›´å°çš„æ˜¾å­˜å¼€é”€ã€‚\n\næ³¨æ„ï¼šå¦‚ä½ ä½¿ç”¨å•å¡Q-LoRAï¼Œä½ å¯èƒ½éœ€è¦å®‰è£…`mpi4py`ã€‚ä½ å¯ä»¥é€šè¿‡`pip`æˆ–è€…`conda`æ¥å®‰è£…ã€‚\n\nè¿è¡ŒQ-LoRAä½ åªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬ï¼š\n\n```bash\n# å•å¡è®­ç»ƒ\nbash finetune/finetune_qlora_single_gpu.sh\n# åˆ†å¸ƒå¼è®­ç»ƒ\nbash finetune/finetune_qlora_ds.sh\n```\n\næˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨æˆ‘ä»¬æä¾›çš„Int4é‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå³Qwen-7B-Chat-Int4ã€‚è¯·**ä¸è¦ä½¿ç”¨**éé‡åŒ–æ¨¡å‹ï¼ä¸å…¨å‚æ•°å¾®è°ƒä»¥åŠLoRAä¸åŒï¼ŒQ-LoRAä»…æ”¯æŒfp16ã€‚æ³¨æ„ï¼Œç”±äºæˆ‘ä»¬å‘ç°torch ampæ”¯æŒçš„fp16æ··åˆç²¾åº¦è®­ç»ƒå­˜åœ¨é—®é¢˜ï¼Œå› æ­¤å½“å‰çš„å•å¡è®­ç»ƒQ-LoRAå¿…é¡»ä½¿ç”¨DeepSpeedã€‚æ­¤å¤–ï¼Œä¸Šè¿°LoRAå…³äºç‰¹æ®Štokençš„é—®é¢˜åœ¨Q-LoRAä¾ç„¶å­˜åœ¨ã€‚å¹¶ä¸”ï¼ŒInt4æ¨¡å‹çš„å‚æ•°æ— æ³•è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚æ‰€å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬åªæä¾›äº†Chatæ¨¡å‹çš„Int4æ¨¡å‹ï¼Œå› æ­¤ä½ ä¸ç”¨æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ æ‰§æ„è¦åœ¨Q-LoRAä¸­å¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œå¾ˆæŠ±æ­‰ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯ä½ èƒ½æˆåŠŸè®­ç»ƒã€‚\n\n> æ³¨æ„ï¼šç”±äºHugging Faceçš„å†…éƒ¨å®ç°ï¼Œæ¨¡å‹åœ¨ä¿å­˜æ—¶ï¼Œä¸€äº›éPythonæ–‡ä»¶æœªä¿å­˜ï¼ˆä¾‹å¦‚`*.cpp`ä¸`*.cu`ï¼‰ï¼Œå¦‚éœ€è¦æ”¯æŒç›¸å…³åŠŸèƒ½ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶æœ‰å…³æ–‡ä»¶ã€‚\n\nä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRAå’ŒQ-LoRAçš„è®­ç»ƒåªéœ€å­˜å‚¨adapteréƒ¨åˆ†çš„å‚æ•°ã€‚å‡å¦‚ä½ éœ€è¦ä½¿ç”¨LoRAè®­ç»ƒåçš„æ¨¡å‹ï¼Œä½ éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ã€‚å‡è®¾ä½ ä½¿ç”¨Qwen-7Bè®­ç»ƒæ¨¡å‹ï¼Œä½ å¯ä»¥ç”¨å¦‚ä¸‹ä»£ç è¯»å–æ¨¡å‹ï¼š\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n> æ³¨æ„: å¦‚æœ`peft>=0.8.0`ï¼ŒåŠ è½½æ¨¡å‹åŒæ—¶ä¼šå°è¯•åŠ è½½tokenizerï¼Œä½†peftå†…éƒ¨æœªç›¸åº”è®¾ç½®`trust_remote_code=True`ï¼Œå¯¼è‡´`ValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.`è¦é¿è¿‡è¿™ä¸€é—®é¢˜ï¼Œä½ å¯ä»¥é™çº§`peft<0.8.0`æˆ–å°†tokenizerç›¸å…³æ–‡ä»¶ç§»åˆ°å…¶å®ƒæ–‡ä»¶å¤¹ã€‚\n\n\nå¦‚æœä½ è§‰å¾—è¿™æ ·ä¸€æ­¥åˆ°ä½çš„æ–¹å¼è®©ä½ å¾ˆä¸å®‰å¿ƒæˆ–è€…å½±å“ä½ æ¥å…¥ä¸‹æ¸¸åº”ç”¨ï¼Œä½ å¯ä»¥é€‰æ‹©å…ˆåˆå¹¶å¹¶å­˜å‚¨æ¨¡å‹ï¼ˆLoRAæ”¯æŒåˆå¹¶ï¼ŒQ-LoRAä¸æ”¯æŒï¼‰ï¼Œå†ç”¨å¸¸è§„æ–¹å¼è¯»å–ä½ çš„æ–°æ¨¡å‹ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\n`new_model_directory`ç›®å½•å°†åŒ…å«åˆå¹¶åçš„æ¨¡å‹å‚æ•°ä¸ç›¸å…³æ¨¡å‹ä»£ç ã€‚è¯·æ³¨æ„`*.cu`å’Œ`*.cpp`æ–‡ä»¶å¯èƒ½æ²¡è¢«ä¿å­˜ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶ã€‚å¦å¤–ï¼Œ`merge_and_unload`ä»…ä¿å­˜æ¨¡å‹ï¼Œå¹¶æœªä¿å­˜tokenizerï¼Œå¦‚æœ‰éœ€è¦ï¼Œè¯·å¤åˆ¶ç›¸å…³æ–‡ä»¶æˆ–ä½¿ç”¨ä»¥ä»¥ä¸‹ä»£ç ä¿å­˜\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    path_to_adapter, # path to the output directory\n    trust_remote_code=True\n)\ntokenizer.save_pretrained(new_model_directory)\n```\n\n\næ³¨æ„ï¼šåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦æ ¹æ®ä½ çš„éœ€æ±‚å’Œæœºå™¨æŒ‡å®šæ­£ç¡®çš„åˆ†å¸ƒå¼è®­ç»ƒè¶…å‚æ•°ã€‚æ­¤å¤–ï¼Œä½ éœ€è¦æ ¹æ®ä½ çš„æ•°æ®ã€æ˜¾å­˜æƒ…å†µå’Œè®­ç»ƒé€Ÿåº¦é¢„æœŸï¼Œä½¿ç”¨`--model_max_length`è®¾å®šä½ çš„æ•°æ®é•¿åº¦ã€‚\n\n### é‡åŒ–å¾®è°ƒåæ¨¡å‹\n\nè¿™ä¸€å°èŠ‚ç”¨äºé‡åŒ–å…¨å‚/LoRAå¾®è°ƒåçš„æ¨¡å‹ã€‚ï¼ˆæ³¨æ„ï¼šä½ ä¸éœ€è¦é‡åŒ–Q-LoRAæ¨¡å‹å› ä¸ºå®ƒæœ¬èº«å°±æ˜¯é‡åŒ–è¿‡çš„ã€‚ï¼‰\nå¦‚æœä½ éœ€è¦é‡åŒ–LoRAå¾®è°ƒåçš„æ¨¡å‹ï¼Œè¯·å…ˆæ ¹æ®ä¸Šæ–¹è¯´æ˜å»åˆå¹¶ä½ çš„æ¨¡å‹æƒé‡ã€‚\n\næˆ‘ä»¬æ¨èä½¿ç”¨[auto_gptq](https://github.com/PanQiWei/AutoGPTQ)å»é‡åŒ–ä½ çš„æ¨¡å‹ã€‚\n\n```bash\npip install auto-gptq optimum\n```\n\næ³¨æ„: å½“å‰AutoGPTQæœ‰ä¸ªbugï¼Œå¯ä»¥åœ¨è¯¥[issue](https://github.com/PanQiWei/AutoGPTQ/issues/370)æŸ¥çœ‹ã€‚è¿™é‡Œæœ‰ä¸ª[ä¿®æ”¹PR](https://github.com/PanQiWei/AutoGPTQ/pull/495)ï¼Œä½ å¯ä»¥ä½¿ç”¨è¯¥åˆ†æ”¯ä»ä»£ç è¿›è¡Œå®‰è£…ã€‚\n\né¦–å…ˆï¼Œå‡†å¤‡æ ¡å‡†é›†ã€‚ä½ å¯ä»¥é‡ç”¨å¾®è°ƒä½ çš„æ•°æ®ï¼Œæˆ–è€…æŒ‰ç…§å¾®è°ƒç›¸åŒçš„æ–¹å¼å‡†å¤‡å…¶ä»–æ•°æ®ã€‚\n\nç¬¬äºŒæ­¥ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n\n```bash\npython run_gptq.py \\\n    --model_name_or_path $YOUR_LORA_MODEL_PATH \\\n    --data_path $DATA \\\n    --out_path $OUTPUT_PATH \\\n    --bits 4 # 4 for int4; 8 for int8\n```\n\nè¿™ä¸€æ­¥éœ€è¦ä½¿ç”¨GPUï¼Œæ ¹æ®ä½ çš„æ ¡å‡†é›†å¤§å°å’Œæ¨¡å‹å¤§å°ï¼Œå¯èƒ½ä¼šæ¶ˆè€—æ•°ä¸ªå°æ—¶ã€‚\n\næ¥ä¸‹æ¥, å°†åŸæ¨¡å‹ä¸­æ‰€æœ‰ `*.py`, `*.cu`, `*.cpp` æ–‡ä»¶å’Œ `generation_config.json` æ–‡ä»¶å¤åˆ¶åˆ°è¾“å‡ºæ¨¡å‹ç›®å½•ä¸‹ã€‚åŒæ—¶ï¼Œä½¿ç”¨å®˜æ–¹å¯¹åº”ç‰ˆæœ¬çš„é‡åŒ–æ¨¡å‹çš„ `config.json` æ–‡ä»¶è¦†ç›–è¾“å‡ºæ¨¡å‹ç›®å½•ä¸‹çš„æ–‡ä»¶\n(ä¾‹å¦‚, å¦‚æœä½ å¾®è°ƒäº† `Qwen-7B-Chat`å’Œ`--bits 4`, é‚£ä¹ˆä½ å¯ä»¥ä» [Qwen-7B-Chat-Int4](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4/blob/main/config.json) ä»“åº“ä¸­æ‰¾åˆ°å¯¹åº”çš„`config.json` )ã€‚\nå¹¶ä¸”ï¼Œä½ éœ€è¦å°† ``gptq.safetensors`` é‡å‘½åä¸º ``model.safetensors``ã€‚\n\næœ€åï¼Œåƒå®˜æ–¹é‡åŒ–æ¨¡å‹ä¸€æ ·æµ‹è¯•ä½ çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼š\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"/path/to/your/model\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/path/to/your/model\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n```\n\n### å¤šæœºå¾®è°ƒ\n\næˆ‘ä»¬æä¾›çš„è„šæœ¬æ”¯æŒå¤šæœºå¾®è°ƒï¼Œå¯ä»¥å‚è€ƒ[è„šæœ¬](./finetune/finetune_lora_ds.sh)ä¸­çš„æ³¨é‡Šï¼Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ­£ç¡®è®¾ç½®ç›¸åº”çš„å‚æ•°å¹¶å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚å…³äºå¤šæœºåˆ†å¸ƒå¼è®­ç»ƒçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[torchrun](https://pytorch.org/docs/stable/elastic/run.html)ã€‚\n\næ³¨æ„ï¼š DeepSpeed ZeRO 3 å¯¹èŠ‚ç‚¹é—´é€šä¿¡é€Ÿç‡çš„è¦æ±‚è¿œå¤§äº ZeRO 2ï¼Œåœ¨å¤šæœºå¾®è°ƒçš„æƒ…å†µä¸‹ä¼šå¤§å¹…é™ä½è®­ç»ƒé€Ÿåº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸å»ºè®®åœ¨å¤šæœºå¾®è°ƒçš„æƒ…å†µä¸‹ä½¿ç”¨ DeepSpeed ZeRO 3 é…ç½®ã€‚\n\n### æ˜¾å­˜å ç”¨åŠè®­ç»ƒé€Ÿåº¦\n\nä¸‹é¢è®°å½•7Bå’Œ14Bæ¨¡å‹åœ¨å•GPUä½¿ç”¨LoRAï¼ˆLoRA (emb)æŒ‡çš„æ˜¯embeddingå’Œè¾“å‡ºå±‚å‚ä¸è®­ç»ƒï¼Œè€ŒLoRAåˆ™ä¸ä¼˜åŒ–è¿™éƒ¨åˆ†å‚æ•°ï¼‰å’ŒQLoRAæ—¶å¤„ç†ä¸åŒé•¿åº¦è¾“å…¥çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æƒ…å†µã€‚æœ¬æ¬¡è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨CUDA 11.8å’ŒPytorch 2.0ï¼Œå¹¶ä½¿ç”¨äº†flash attention 2ã€‚æˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨batch sizeä¸º1ï¼Œgradient accumulationä¸º8çš„è®­ç»ƒé…ç½®ï¼Œè®°å½•è¾“å…¥é•¿åº¦åˆ†åˆ«ä¸º256ã€512ã€1024ã€2048ã€4096å’Œ8192çš„æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰å’Œè®­ç»ƒé€Ÿåº¦ï¼ˆs/iterï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨2å¼ A100æµ‹äº†Qwen-7Bçš„å…¨å‚æ•°å¾®è°ƒã€‚å—é™äºæ˜¾å­˜å¤§å°ï¼Œæˆ‘ä»¬ä»…æµ‹è¯•äº†256ã€512å’Œ1024tokençš„æ€§èƒ½ã€‚\n\nå¯¹äº Qwen-7Bï¼Œæˆ‘ä»¬é¢å¤–æµ‹è¯•äº†å¤šæœºå¾®è°ƒçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸¤å°æœåŠ¡å™¨ä¸Šè¿è¡Œè¯„æµ‹ï¼Œæ¯å°æœåŠ¡å™¨åŒ…å«ä¸¤å¼ A100-SXM4-80G GPUï¼Œå…¶ä½™é…ç½®ä¸Qwen-7Bçš„å…¶ä»–è¯„æµ‹ç›¸åŒã€‚å¤šæœºå¾®è°ƒçš„ç»“æœåœ¨è¡¨ä¸­ä»¥ LoRA (multinode) æ ‡ç¤ºã€‚\n\nå¯¹äº Qwen-72Bï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä¸¤ç§æ–¹æ¡ˆï¼š1ï¼‰ä½¿ç”¨4ä¸ª A100-SXM4-80G GPUsï¼Œé€šè¿‡ Lora + DeepSpeed ZeRO 3 å¾®è°ƒå’Œ2ï¼‰ä½¿ç”¨å•å¼ A100-SXM4-80G GPUï¼Œé€šè¿‡ QLora (int4) å¾®è°ƒã€‚è¯·æ³¨æ„ï¼Œä½¿ç”¨ LoRA (emb) å¾®è°ƒå’Œä¸å¸¦ DeepSpeed ZeRO 3 çš„ LoRA å¾®è°ƒåœ¨4ä¸ªA100-SXM4-80G GPUs ä¸Šéƒ½ä¼šå‡ºç°OOMï¼ˆä½ å¯ä»¥é€šè¿‡å°†`--deepspeed finetune/ds_config_zero3.json`å‚æ•°ä¼ ç»™[`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh)æ¥æ‰“å¼€ DeepSpeed ZeRO 3 é…ç½®ï¼‰ã€‚\n\nå…·ä½“æ•°å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th rowspan=\"2\">#Nodes</th><th rowspan=\"2\">#GPUs per node</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"5\">7B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n<td>1</td><td>2</td>\n<td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>LoRA (multinode)</td>\n        <td>2</td><td>2</td>\n        <td align=\"center\">74.7G / 2.09s/it</td><td align=\"center\">77.6G / 3.16s/it</td><td align=\"center\">84.9G / 5.17s/it</td><td align=\"center\">95.1G / 9.25s/it</td><td align=\"center\">121.1G / 18.1s/it</td><td align=\"center\">155.5G / 37.4s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"2\">72B</th>\n        <td>LoRA + Deepspeed Zero3</td>\n        <td>1</td><td>4</td>\n        <td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n\n<br>\n\n## éƒ¨ç½²\n\n### vLLM\nå¦‚å¸Œæœ›éƒ¨ç½²åŠåŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨vLLMã€‚\n\nå¦‚æœä½ ä½¿ç”¨**CUDA 12.1å’ŒPyTorch 2.1**ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…vLLMã€‚\n\n```bash\npip install vllm\n```\n\nå¦åˆ™è¯·å‚è€ƒvLLMå®˜æ–¹çš„[å®‰è£…è¯´æ˜](https://docs.vllm.ai/en/latest/getting_started/installation.html)ã€‚\n\n#### vLLM + ç±»Transformeræ¥å£\n\nè¯·ä¸‹è½½[æ¥å£å°è£…ä»£ç ](examples/vllm_wrapper.py)åˆ°å½“å‰æ–‡ä»¶å¤¹ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œå¤šè½®å¯¹è¯äº¤äº’ã€‚ï¼ˆæ³¨æ„ï¼šè¯¥æ–¹æ³•å½“å‰åªæ”¯æŒ``model.chat()``æ¥å£ã€‚ï¼‰\n\n```python\nfrom vllm_wrapper import vLLMWrapper\n\nmodel = vLLMWrapper('Qwen/Qwen-7B-Chat', tensor_parallel_size=1)\n\nresponse, history = model.chat(query=\"ä½ å¥½\", history=None)\nprint(response)\nresponse, history = model.chat(query=\"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\nresponse, history = model.chat(query=\"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n```\n\n#### vLLM + ç½‘é¡µDemo / ç±»OpenAI API\n\nä½ å¯ä»¥ä½¿ç”¨FastChatå»æ­å»ºä¸€ä¸ªç½‘é¡µDemoæˆ–ç±»OpenAI APIæœåŠ¡å™¨ã€‚é¦–å…ˆï¼Œè¯·å®‰è£…FastChatï¼š\n\n```bash\npip install \"fschat[model_worker,webui]\"\n```\n\nä½¿ç”¨vLLMå’ŒFastChatè¿è¡ŒQwenä¹‹å‰ï¼Œé¦–å…ˆå¯åŠ¨ä¸€ä¸ªcontrollerï¼š\n```bash\npython -m fastchat.serve.controller\n```\n\nç„¶åå¯åŠ¨model workerè¯»å–æ¨¡å‹ã€‚å¦‚ä½¿ç”¨å•å¡æ¨ç†ï¼Œè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype float16 # è¿è¡Œint4æ¨¡å‹\n```\nç„¶è€Œï¼Œå¦‚æœä½ å¸Œæœ›ä½¿ç”¨å¤šGPUåŠ é€Ÿæ¨ç†æˆ–è€…å¢å¤§æ˜¾å­˜ï¼Œä½ å¯ä»¥ä½¿ç”¨vLLMæ”¯æŒçš„æ¨¡å‹å¹¶è¡Œæœºåˆ¶ã€‚å‡è®¾ä½ éœ€è¦åœ¨4å¼ GPUä¸Šè¿è¡Œä½ çš„æ¨¡å‹ï¼Œå‘½ä»¤å¦‚ä¸‹æ‰€ç¤ºï¼š\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype float16 # è¿è¡Œint4æ¨¡å‹\n```\n\nå¯åŠ¨model workeråï¼Œä½ å¯ä»¥å¯åŠ¨ä¸€ä¸ªï¼š\n\n* Web UI Demo\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* OpenAI API\n\nä½¿ç”¨OpenAI APIå‰ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„APIç« èŠ‚é…ç½®å¥½ç¯å¢ƒï¼Œç„¶åè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\nç„¶è€Œï¼Œå¦‚æœä½ è§‰å¾—ä½¿ç”¨vLLMå’ŒFastChatæ¯”è¾ƒå›°éš¾ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•ä»¥ä¸‹æˆ‘ä»¬æä¾›çš„æœ€ç®€å•çš„æ–¹å¼éƒ¨ç½²Web Demoã€CLI Demoå’ŒOpenAI APIã€‚\n<br>\n\n\n### Web UI\n\næˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ (æ„Ÿè°¢ @wysaid æ”¯æŒ)ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š\n\n```bash\npip install -r requirements_web_demo.txt\n```\n\néšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n### äº¤äº’å¼Demo\n\næˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„äº¤äº’å¼Demoç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹`cli_demo.py`ã€‚å½“å‰æ¨¡å‹å·²ç»æ”¯æŒæµå¼è¾“å‡ºï¼Œç”¨æˆ·å¯é€šè¿‡è¾“å…¥æ–‡å­—çš„æ–¹å¼å’ŒQwen-7B-Chatäº¤äº’ï¼Œæ¨¡å‹å°†æµå¼è¾“å‡ºè¿”å›ç»“æœã€‚è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š\n\n```bash\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\næˆ‘ä»¬æä¾›äº†OpenAI APIæ ¼å¼çš„æœ¬åœ°APIéƒ¨ç½²æ–¹æ³•ï¼ˆæ„Ÿè°¢@hanpenggitï¼‰ã€‚åœ¨å¼€å§‹ä¹‹å‰å…ˆå®‰è£…å¿…è¦çš„ä»£ç åº“ï¼š\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\néšåå³å¯è¿è¡Œä»¥ä¸‹å‘½ä»¤éƒ¨ç½²ä½ çš„æœ¬åœ°APIï¼š\n\n```bash\npython openai_api.py\n```\n\nä½ ä¹Ÿå¯ä»¥ä¿®æ”¹å‚æ•°ï¼Œæ¯”å¦‚`-c`æ¥ä¿®æ”¹æ¨¡å‹åç§°æˆ–è·¯å¾„, `--cpu-only`æ”¹ä¸ºCPUéƒ¨ç½²ç­‰ç­‰ã€‚å¦‚æœéƒ¨ç½²å‡ºç°é—®é¢˜ï¼Œæ›´æ–°ä¸Šè¿°ä»£ç åº“å¾€å¾€å¯ä»¥è§£å†³å¤§å¤šæ•°é—®é¢˜ã€‚\n\nä½¿ç”¨APIåŒæ ·éå¸¸ç®€å•ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=True\n    # æµå¼è¾“å‡ºçš„è‡ªå®šä¹‰stopwordsåŠŸèƒ½å°šæœªæ”¯æŒï¼Œæ­£åœ¨å¼€å‘ä¸­\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# ä¸ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=False,\n    stop=[] # åœ¨æ­¤å¤„æ·»åŠ è‡ªå®šä¹‰çš„stop words ä¾‹å¦‚ReAct promptingæ—¶éœ€è¦å¢åŠ ï¼š stop=[\"Observation:\"]ã€‚\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\nè¯¥æ¥å£ä¹Ÿæ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆ**Function Calling**ï¼‰ï¼Œä½†æš‚æ—¶ä»…é™ `stream=False` æ—¶èƒ½ç”Ÿæ•ˆã€‚ç”¨æ³•è§[å‡½æ•°è°ƒç”¨ç¤ºä¾‹](examples/function_call_examples.py)ã€‚\n<br><br>\n\n## ğŸ³ ä½¿ç”¨é¢„æ„å»ºçš„Dockeré•œåƒ\n\nä¸ºç®€åŒ–éƒ¨ç½²æµç¨‹ï¼Œæˆ‘ä»¬æä¾›äº†é¢„é…ç½®å¥½ç›¸åº”ç¯å¢ƒçš„Dockeré•œåƒï¼š[qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen)ï¼Œåªéœ€å®‰è£…é©±åŠ¨ã€ä¸‹è½½æ¨¡å‹æ–‡ä»¶å³å¯å¯åŠ¨Demoã€éƒ¨ç½²OpenAI APIä»¥åŠè¿›è¡Œå¾®è°ƒã€‚\n\n### å‡†å¤‡æ“ä½œ\n\n1. æ ¹æ®éœ€è¦ä½¿ç”¨çš„é•œåƒç‰ˆæœ¬ï¼Œå®‰è£…ç›¸åº”ç‰ˆæœ¬çš„Nvidiaé©±åŠ¨ï¼š\n  - `qwenllm/qwen:cu117`ï¼ˆ**æ¨è**ï¼‰ï¼š`>= 515.48.07`\n  - `qwenllm/qwen:cu114`ï¼ˆä¸æ”¯æŒflash-attentionï¼‰ï¼š`>= 470.82.01`\n  - `qwenllm/qwen:cu121`ï¼š`>= 530.30.02`\n  - `qwenllm/qwen:latest`ï¼šä¸`qwenllm/qwen:cu117`ç›¸åŒ\n\n2. å®‰è£…å¹¶é…ç½®[docker](https://docs.docker.com/engine/install/)å’Œ[nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)ï¼š\n\n```bash\n# é…ç½®docker\nsudo systemctl start docker\n# æµ‹è¯•dockeræ˜¯å¦å®‰è£…æ­£ç¡®\nsudo docker run hello-world\n\n# é…ç½®nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# æµ‹è¯•nvidia-container-toolkitæ˜¯å¦å®‰è£…æ­£ç¡®\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. ä¸‹è½½æ¨¡å‹åŠä»£ç è‡³æœ¬åœ°ï¼ˆå‚è€ƒ[æ­¤å¤„è¯´æ˜](#DownloadModel)ï¼‰\n\n### éƒ¨ç½²\n\nä¸‹é¢æˆ‘ä»¬ä»¥Qwen-7B-Chatä¸ºä¾‹ã€‚åœ¨å¯åŠ¨Web Demoæˆ–è€…éƒ¨ç½²APIå‰ï¼Œè¯·å…ˆå‚ç…§ä¸‹æ–¹ä»£ç å®Œæˆé…ç½®å·¥ä½œï¼š\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # ä¸‹è½½åˆ°æœ¬åœ°çš„æ¨¡å‹åŠä»£ç è·¯å¾„\n```\n\nå¦‚ä¸‹è„šæœ¬å¯ä»¥å¸®ä½ éƒ¨ç½²:\n\n* OpenAI API\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Web UI\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* äº¤äº’å¼Demo\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\nè¿™äº›å‘½ä»¤å°†è‡ªåŠ¨ä¸‹è½½æ‰€éœ€é•œåƒä»¥åŠåå°å¯åŠ¨Web UI Demoã€‚ä½ å¯ä»¥æ‰“å¼€`http://localhost:${PORT}` æ¥ä½¿ç”¨è¯¥Demoã€‚\n\nå¦‚æœè¾“å‡ºå¦‚ä¸‹å†…å®¹ï¼Œåˆ™è¯´æ˜Demoå¯åŠ¨æˆåŠŸï¼š\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nå¦‚æœä½ æƒ³æŸ¥çœ‹Demoçš„çŠ¶æ€ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤æ¥å±•ç¤ºè¾“å‡ºç»“æœï¼š`docker logs qwen`ã€‚\n\nä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤`docker rm -f qwen`æ¥åœæ­¢æœåŠ¡å¹¶åˆ é™¤å®¹å™¨ã€‚\n\n### å¾®è°ƒ\n\nä½¿ç”¨é¢„é…ç½®å¥½çš„Dockeré•œåƒè¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ä¸[ä¸Šä¸€ç« ](#å¾®è°ƒ)åŸºæœ¬ä¸€è‡´ï¼ˆæˆ‘ä»¬å·²ç»åœ¨é•œåƒä¸­å®‰è£…äº†ç›¸å…³ä¾èµ–ï¼‰ï¼š\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªå•å¡LoRAå¾®è°ƒçš„ç¤ºä¾‹ï¼š\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # ä¸‹è½½çš„æ¨¡å‹å’Œä»£ç è·¯å¾„\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # ä¸‹è½½çš„æ¨¡å‹å’Œä»£ç è·¯å¾„ (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # å‡†å¤‡å¾®è°ƒæ•°æ®æ”¾åœ¨ ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # å¾®è°ƒè¾“å‡ºè·¯å¾„\n\n# é»˜è®¤ä½¿ç”¨ä¸»æœºæ‰€æœ‰GPU\nDEVICE=all\n# å¦‚æœéœ€è¦æŒ‡å®šç”¨äºè®­ç»ƒçš„GPUï¼ŒæŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¾ç½®deviceï¼ˆæ³¨æ„ï¼šå†…å±‚çš„å¼•å·ä¸å¯çœç•¥ï¼‰\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# å•å¡LoRAå¾®è°ƒ\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\nå¦‚éœ€ä¿®æ”¹ä¸ºå•å¡Q-LoRAå¾®è°ƒç¤ºä¾‹ï¼Œåªè¦ä¿®æ”¹`docker run`ä¸­çš„bashå‘½ä»¤ï¼š\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## ğŸ”¥ ç³»ç»ŸæŒ‡ä»¤ (System Prompt)\nQwen-1.8-Chat å’Œ Qwen-72B-Chat é€šä¹‰åƒé—®åœ¨å¤šæ ·ä¸”å­˜åœ¨å¤šè½®å¤æ‚äº¤äº’çš„ç³»ç»ŸæŒ‡ä»¤ä¸Šè¿›è¡Œäº†å……åˆ†è®­ç»ƒï¼Œä½¿æ¨¡å‹å¯ä»¥è·Ÿéšå¤šæ ·çš„ç³»ç»ŸæŒ‡ä»¤ï¼Œå®ç°ä¸Šä¸‹æ–‡(in-context)ä¸­çš„æ¨¡å‹å®šåˆ¶åŒ–ï¼Œè¿›ä¸€æ­¥æå‡äº†é€šä¹‰åƒé—®çš„å¯æ‰©å±•æ€§ã€‚\n\né€šè¿‡ç³»ç»ŸæŒ‡ä»¤ï¼ŒQwen-Chatèƒ½å¤Ÿå®ç°**è§’è‰²æ‰®æ¼”**ï¼Œ**è¯­è¨€é£æ ¼è¿ç§»**ï¼Œ**ä»»åŠ¡è®¾å®š**ï¼Œå’Œ**è¡Œä¸ºè®¾å®š**ç­‰èƒ½åŠ›ã€‚\n\n![](assets/system_prompt_language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\næ›´å¤šå…³äºç³»ç»ŸæŒ‡ä»¤çš„ä»‹ç»ä¿¡æ¯å¯ä»¥å‚è€ƒ[ç¤ºä¾‹æ–‡æ¡£](examples/system_prompt.md).\n\n\n## å·¥å…·è°ƒç”¨\n\nQwen-Chaté’ˆå¯¹å·¥å…·ä½¿ç”¨ã€å‡½æ•°è°ƒç”¨èƒ½åŠ›è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç”¨æˆ·å¯ä»¥å¼€å‘åŸºäºQwençš„Agentã€LangChainåº”ç”¨ã€ç”šè‡³Code Interpreterã€‚\n\næˆ‘ä»¬æä¾›äº†æ–‡æ¡£è¯´æ˜å¦‚ä½•æ ¹æ®ReAct Promptingçš„åŸç†å®ç°å·¥å…·è°ƒç”¨ï¼Œè¯·å‚è§[ReActç¤ºä¾‹](examples/react_prompt.md)ã€‚åŸºäºè¯¥åŸç†ï¼Œæˆ‘ä»¬åœ¨ [openai_api.py](openai_api.py) é‡Œæä¾›äº†å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰çš„æ”¯æŒã€‚\næˆ‘ä»¬åœ¨å·²å¼€æºçš„ä¸­æ–‡[è¯„æµ‹æ•°æ®é›†](eval/EVALUATION.md)ä¸Šæµ‹è¯•æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¹¶å‘ç°Qwen-Chatèƒ½å¤Ÿå–å¾—ç¨³å®šçš„è¡¨ç°ï¼š\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">ä¸­æ–‡å·¥å…·è°ƒç”¨è¯„æµ‹åŸºå‡†ï¼ˆç‰ˆæœ¬ 20231206ï¼‰</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.â†‘)</th><th align=\"center\">Tool Input (Rouge-Lâ†‘)</th><th align=\"center\">False Positive Errorâ†“</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\nä¸ºäº†è€ƒå¯ŸQwenä½¿ç”¨Python Code Interpreterå®Œæˆæ•°å­¦è§£é¢˜ã€æ•°æ®å¯è§†åŒ–ã€åŠæ–‡ä»¶å¤„ç†ä¸çˆ¬è™«ç­‰ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸“é—¨å»ºè®¾å¹¶å¼€æºäº†ä¸€ä¸ªè¯„æµ‹è¿™æ–¹é¢èƒ½åŠ›çš„[è¯„æµ‹åŸºå‡†](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)ã€‚\næˆ‘ä»¬å‘ç°Qwenåœ¨ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ã€ç»“æœæ­£ç¡®æ€§ä¸Šå‡è¡¨ç°è¾ƒå¥½ï¼š\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">ä»£ç æ‰§è¡Œç»“æœæ­£ç¡®æ€§ (%)</th>\n        <th colspan=\"1\" align=\"center\">ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Mathâ†‘</th><th align=\"center\">Visualization-Hardâ†‘</th><th align=\"center\">Visualization-Easyâ†‘</th><th align=\"center\">Generalâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## é•¿æ–‡æœ¬ç†è§£\n\næˆ‘ä»¬å¼•å…¥äº†NTKæ’å€¼ã€çª—å£æ³¨æ„åŠ›ã€LogNæ³¨æ„åŠ›ç¼©æ”¾ç­‰æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦å¹¶çªç ´è®­ç»ƒåºåˆ—é•¿åº¦çš„é™åˆ¶ï¼ŒåŸç”Ÿé•¿åº¦ä¸º2Kçš„Qwen-14Bå¯ä»¥æ‰©å±•åˆ°8Kçš„åºåˆ—é•¿åº¦ï¼Œè€ŒåŸç”Ÿé•¿åº¦8Kçš„Qwen-1.8B/7Bèƒ½å¤Ÿåœ¨32Ké•¿åºåˆ—çš„è®¾ç½®ä¸‹å–å¾—ä¸é”™çš„è¡¨ç°ã€‚\n\nå¯¹äºQwen-72Bï¼Œæˆ‘ä»¬åŸºäºRoPEé‡‡ç”¨æ›´å¤§çš„æ—‹è½¬Baseæ¥é€‚åº”æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€‚Qwen-72Bæ”¯æŒ32Kçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚\n\né€šè¿‡arXivæ•°æ®é›†ä¸Šçš„è¯­è¨€æ¨¡å‹å®éªŒï¼Œå‘ç° Qwen åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹å¯ä»¥è¾¾åˆ°å‡ºè‰²çš„æ€§èƒ½ã€‚ç»“æœå¦‚ä¸‹ï¼š\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n            <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\"><b>3.23</b></td><td align=\"center\">3.33</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n</table>\n\nè¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬ä¸ºäº†éªŒè¯Qwen-72B-Chatåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œåœ¨[L-Eval](https://arxiv.org/abs/2307.11088)å®¢è§‚é¢˜ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯„åˆ†ç»“æœå¦‚ä¸‹ï¼š\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\n\næˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œäº†â€œå¤§æµ·æé’ˆâ€å®éªŒï¼ˆæƒ³æ³•æ¥è‡ªäº[@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)ï¼‰ï¼Œæµ‹è¯•æ¨¡å‹åœ¨ä¸åŒé•¿åº¦çš„è¾“å…¥ä¸‹ï¼Œæ˜¯å¦èƒ½æ£€ç´¢åˆ°æ–‡ç« ä¸åŒä½ç½®çš„ä¿¡æ¯ï¼Œç»“æœå¦‚ä¸‹ï¼š\n\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\nä»¥ä¸Šç»“æœè¯´æ˜ï¼ŒQwen-72B-Chatå¯ä»¥èƒ½å‡†ç¡®æ£€ç´¢åˆ°32Kä»¥å†…çš„è¾“å…¥é•¿åº¦ä¸­æ”¾åœ¨å„ç§ä½ç½®çš„ä¿¡æ¯ï¼Œè¯æ˜äº†å…¶å…·æœ‰ä¼˜ç§€çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚\n\n## Tokenizer\n\n> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizerâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚\n\nåŸºäºtiktokençš„tokenizeræœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepiece tokenizerã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](tokenization_note_zh.md)ã€‚\n<br><br>\n\n## å¤ç°\n\næˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚æ³¨æ„ï¼Œç”±äºå†…éƒ¨ä»£ç å’Œå¼€æºä»£ç å­˜åœ¨å°‘è®¸å·®å¼‚ï¼Œè¯„æµ‹ç»“æœå¯èƒ½ä¸æ±‡æŠ¥ç»“æœå­˜åœ¨ç»†å¾®çš„ç»“æœä¸ä¸€è‡´ã€‚è¯·é˜…è¯»[eval/EVALUATION.md](eval/EVALUATION.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n<br><br>\n\n## FAQ\n\nå¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚\n<br><br>\n\n## å¼•ç”¨\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## ä½¿ç”¨åè®®\n\n<https://github.com/QwenLM/Qwen>ä¸­çš„æºä»£ç é‡‡ç”¨[Apache 2.0åè®®](./LICENSE)æˆæƒï¼Œæ‚¨å¯åœ¨è¯¥ä»“åº“æ ¹ç›®å½•æ‰¾åˆ°åè®®å…¨æ–‡ã€‚\n\nç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwenå’ŒQwen-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚å¯¹äºå•†ä¸šä½¿ç”¨ï¼Œè¯·æŸ¥çœ‹æ¨¡å‹å„è‡ªçš„LICENSEã€‚\n\n- Qwen-72Bã€Qwen-14Bå’ŒQwen-7Bé‡‡ç”¨[Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT)æˆæƒï¼Œæ‚¨å¯åœ¨ç›¸åº”æ¨¡å‹çš„HuggingFaceæˆ–ModelScopeä»“åº“æ‰¾åˆ°åè®®åŸæ–‡ã€‚å¦‚éœ€å•†ç”¨ï¼Œæ‚¨åªéœ€éµå¾ªä½¿ç”¨åè®®è¿›è¡Œå•†ç”¨å³å¯ï¼Œæˆ‘ä»¬æ¬¢è¿æ‚¨å¡«å†™é—®å·([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat)ã€[14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat)ã€[7B](https://dashscope.console.aliyun.com/openModelApply/qianwen))ã€‚\n\n- Qwen-1.8Bé‡‡ç”¨[Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT)æˆæƒï¼Œæ‚¨å¯åœ¨ç›¸åº”æ¨¡å‹çš„HuggingFaceæˆ–ModelScopeä»“åº“æ‰¾åˆ°åè®®åŸæ–‡ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚\n\n<br><br>\n\n## è”ç³»æˆ‘ä»¬\n\nå¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å’ŒDiscord serverã€‚å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚\n\n"
        },
        {
          "name": "README_ES.md",
          "type": "blob",
          "size": 71.8203125,
          "content": "<p align=\"left\">\n    <a href=\"README_CN.md\">ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href=\"README.md\">English</a>&nbsp ï½œ &nbsp<a href=\"README_JA.md\">æ—¥æœ¬èª</a> ï½œ &nbsp<a href=\"README_FR.md\">FranÃ§ais</a> ï½œ &nbspEspaÃ±ol\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        ğŸ¤— <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br><br>\n\n> [!Important]\n> Â¡Qwen2 estÃ¡ aquÃ­! EstÃ¡s invitado a seguir [QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) y compartir tu experiencia allÃ­.\n>\n> Este repositorio ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) ya no se mantiene activamente, debido a diferencias sustanciales en la base de cÃ³digo.\n<br>\n\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">ğŸ¤—</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">ğŸ¤—</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">ğŸ¤—</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">ğŸ¤—</a> |\n\n\n\nAbrimos nuestra serie **Qwen**, que ahora incluye **Qwen**, los modelos de lenguaje, es decir **Qwen-7B** y **Qwen-14B**, asÃ­ como **Qwen-Chat**, los modelos de chat, es decir **Qwen-7B-Chat** y **Qwen-14B-Chat**. Los enlaces se encuentran en la tabla anterior. Haz clic en ellos y comprueba las fichas de los modelos. AdemÃ¡s, publicamos el **[informe tÃ©cnico](https://arxiv.org/abs/2309.16609)**. Haz clic en el enlace y compruÃ©balo.\n\nEn resumen, disponemos de modelos lingÃ¼Ã­sticos sÃ³lidos, que han sido preentrenados de forma estable para hasta 3 billones de tokens de datos multilingÃ¼es con una amplia cobertura de dominios, idiomas (con especial atenciÃ³n al chino y al inglÃ©s), etc. Son capaces de lograr un rendimiento competitivo en conjuntos de datos de referencia. AdemÃ¡s, disponemos de modelos de chat alineados con las preferencias humanas basados en SFT y RLHF (aÃºn no publicados), que son capaces de chatear, crear contenidos, extraer informaciÃ³n, resumir, traducir, codificar, resolver problemas matemÃ¡ticos, etc., y son capaces de utilizar herramientas, jugar como agentes o incluso jugar como intÃ©rpretes de cÃ³digo, etc.\n\n| Modelo    | Fecha de PublicaciÃ³n | Longitud MÃ¡x. | Mejora del Sistema de Avisos | # de Fichas Preentrenadas | Uso MÃ­nimo de Memoria GPU de Finetuning (Q-Lora) | Uso MÃ­nimo de la GPU para Generar 2048 Tokens (Int4) | Uso de Herramientas |\n|:----------|:--------------------:|:-------------:|:----------------------------:|:-------------------------:|:------------------------------------------------:|:----------------------------------------------------:|:-------------------:|\n| Qwen-1.8B |       23.11.30       |      32K      |              âœ…               |           2.2T            |                      5.8GB                       |                        2.9GB                         |          âœ…          |  \n| Qwen-7B   |       23.08.03       |      32K      |              â               |           2.4T            |                      11.5GB                      |                        8.2GB                         |          âœ…          |   \n| Qwen-14B  |       23.09.25       |      8K       |              â               |           3.0T            |                      18.7GB                      |                        13.0GB                        |          âœ…          |\n| Qwen-72B  |       23.11.30       |      32K      |              âœ…               |           3.0T            |                      61.4GB                      |                        48.9GB                        |          âœ…          |   \n\nEn este repo, usted puede averiguar:\n\n* Inicio rÃ¡pido con Qwen, y disfrute de la simple inferencia.\n* Detalles sobre los modelos de cuantificaciÃ³n, incluyendo GPTQ y cuantizaciÃ³n de cachÃ© KV.\n* EstadÃ­sticas de rendimiento de la inferencia, incluyendo velocidad y memoria.\n* Tutoriales sobre ajuste fino, incluyendo ajuste de parÃ¡metros completos, LoRA y Q-LoRA.\n* Instrucciones de despliegue, con el ejemplo de vLLM y FastChat.\n* Instrucciones para construir demos, incluyendo WebUI, CLI demo, etc.\n* IntroducciÃ³n al servicio API de DashScope, asÃ­ como instrucciones para crear una API de estilo OpenAI para tu modelo.\n* InformaciÃ³n sobre Qwen para el uso de herramientas, agente e intÃ©rprete de cÃ³digo.\n* EstadÃ­sticas de la evaluaciÃ³n de la comprensiÃ³n del contexto largo\n* Acuerdo de licencia\n* ...\n\nAdemÃ¡s, si tienes problemas, consulta primero [FAQ](FAQ.md) para obtener ayuda. Â¿Sigues teniendo problemas? No dudes en plantearnos tus problemas (mejor en inglÃ©s para que te entienda mÃ¡s gente). Si quieres ayudarnos, Â¡envÃ­anos pull requests sin dudarlo! Â¡Siempre nos entusiasman los PR!\n\nÂ¿Quieres charlar con nosotros o quedar para tomar un cafÃ©? Â¡Bienvenido a nuestro Discord o WeChat!\n<br><br>\n\n## Noticias y Actualizaciones\n\n* 2023.11.30 ğŸ”¥ Lanzamos **Qwen-72B** y **Qwen-72B-Chat**, que estÃ¡n entrenados en tokens 3T y soportan 32k contextos, junto con **Qwen-1.8B**, y **Qwen-1.8B-Chat**, en ModelScope y Hugging Face. TambiÃ©n hemos reforzado las capacidades de System Prompt de Qwen-72B-Chat y Qwen-1.8B-Chat, ver [documentaciÃ³n de ejemplo](examples/system_prompt.md). Adicionalmente, soporta la inferencia en **Ascend 910** y **Hygon DCU**. Consulta `ascend-support` y `dcu-support` para mÃ¡s detalles.\n* 2023.10.17 Publicamos el modelo cuantizado Int8 **Qwen-7B-Chat-Int8** y **Qwen-14B-Chat-Int8**.\n* 2023.9.25 Publicamos **Qwen-14B** y **Qwen-14B-Chat** en ModelScope y Hugging Face, junto con [qwen.cpp](https://github.com/QwenLM/qwen.cpp) y [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). TambiÃ©n se actualizan los cÃ³digos y pesos de **Qwen-7B** y **Qwen-7B-Chat**. **POR FAVOR, DESCARGA LA ÃšLTIMA VERSIÃ“N!**\n    - En comparaciÃ³n con **Qwen-7B** (original), **Qwen-7B** utiliza mÃ¡s tokens de entrenamiento, pasando de 2,2T tokens a 2,4T tokens, mientras que la longitud del contexto se amplÃ­a de 2048 a 8192. El conocimiento del chino y la capacidad de codificaciÃ³n de **Qwen-7B** se han mejorado aÃºn mÃ¡s.\n* 2023.9.12 Ahora es posible el ajuste fino de los modelos Qwen-7B, incluido el ajuste fino de parÃ¡metros completos, LoRA y Q-LoRA.\n* 2023.8.21 Publicamos el modelo cuantizado Int4 para Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, que requiere bajos costes de memoria pero consigue mejorar la velocidad de inferencia. AdemÃ¡s, no se produce una degradaciÃ³n significativa del rendimiento en la evaluaciÃ³n comparativa.\n* 2023.8.3 Publicamos **Qwen-7B** y **Qwen-7B-Chat** en ModelScope y Hugging Face. TambiÃ©n proporcionamos una nota tÃ©cnica para mÃ¡s detalles sobre el modelo, incluidos los detalles de entrenamiento y el rendimiento del modelo.\n<br>\n\n## Rendimiento\n\nLos modelos Qwen superan a los modelos de referencia de tamaÃ±os de modelo similares en una serie de conjuntos de datos de referencia, como MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., que evalÃºan las capacidades de los modelos en comprensiÃ³n del lenguaje natural, resoluciÃ³n de problemas matemÃ¡ticos, codificaciÃ³n, etc. Qwen-72B obtiene mejores resultados que LLaMA2-70B en todas las tareas y supera a GPT-3.5 en 7 de cada 10 tareas.\n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=600px/>\n<p>\n<br>\n\n| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |\n| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\nPara todos los modelos comparados, presentamos las mejores puntuaciones entre sus resultados oficiales y [OpenCompass](https://opencompass.org.cn/leaderboard-llm).\n\nPara mÃ¡s resultados experimentales (rendimiento detallado del modelo en mÃ¡s conjuntos de datos de referencia) y detalles, consulte nuestro informe tÃ©cnico haciendo clic [aquÃ­](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).\n<br><br>\n\n## Requisitos\n\n* python 3.8 y superior\n* pytorch 1.12 y superior, se recomienda 2.0 y superior\n* transformers 4.32 y superiores\n* Se recomienda CUDA 11.4 y superior (esto es para usuarios de GPU, usuarios de flash-attention, etc.)\n<br>\n\n## Inicio rÃ¡pido\n\nA continuaciÃ³n, proporcionamos ejemplos sencillos para mostrar cÃ³mo utilizar Qwen-Chat con ğŸ¤– ModelScope y ğŸ¤— Transformers.\n\nPuedes usar nuestras imÃ¡genes docker pre-construidas para saltarte la mayorÃ­a de los pasos de configuraciÃ³n del entorno, mira la SecciÃ³n [\"Usando ImÃ¡genes Docker Pre-construidas\"](#-docker) para mÃ¡s detalles. \n\nSi no utiliza Docker, asegÃºrese de haber configurado el entorno e instalado los paquetes necesarios. AsegÃºrese de que cumple los requisitos anteriores y, a continuaciÃ³n, instale las bibliotecas dependientes.\n\n```bash\npip install -r requirements.txt\n```\n\nSi tu dispositivo soporta fp16 o bf16, te recomendamos instalar [flash-attention](https://github.com/Dao-AILab/flash-attention) (**ahora soportamos flash attention 2.**) para una mayor eficiencia y un menor uso de memoria. (**flash-attention es opcional y el proyecto puede ejecutarse normalmente sin instalarlo**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# Below are optional. Installing them might be slow.\n# pip install csrc/layer_norm\n# pip install csrc/rotary\n```\n\nAhora puedes empezar con ModelScope o Transformers.\n\n### ğŸ¤— Transformers\n\nPara utilizar Qwen-Chat para la inferencia, todo lo que tienes que hacer es introducir unas pocas lÃ­neas de cÃ³digo como se demuestra a continuaciÃ³n. Recuerda introducir los nombres o rutas correctos de los modelos, como \"Qwen/Qwen-7B-Chat\" y \"Qwen/Qwen-14B-Chat\". Sin embargo, **por favor, asegÃºrese de que estÃ¡ utilizando el cÃ³digo mÃ¡s reciente.**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\n# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\n# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚\n# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚\n# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚\n# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚\n# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚\n\n# 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹\n```\n\nEjecutar Qwen, el modelo lingÃ¼Ã­stico base, tambiÃ©n es sencillo.\n\n<details>\n  <summary>Ejecutar Qwen</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\" \ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...\n```\n\n</details>\n\nEn caso de que se produzca un problema de red al intentar descargar puntos de control y cÃ³digos de modelos desde Hugging Face, un mÃ©todo alternativo consiste en obtener inicialmente el punto de control desde ModelScope y luego cargarlo desde el directorio local como se indica a continuaciÃ³n:\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B', revision='v1.1.4')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat', revision='v1.1.4')\n# model_dir = snapshot_download('qwen/Qwen-14B', revision='v1.0.4')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat', revision='v1.0.4')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### ğŸ¤– ModelScope\n\nModelScope es una plataforma de cÃ³digo abierto para Model-as-a-Service (MaaS), que proporciona un servicio de modelos flexible y rentable a los desarrolladores de IA. Del mismo modo, puede ejecutar los modelos con ModelScope como se muestra a continuaciÃ³n:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# Model names: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", revision='v1.0.5', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", revision='v1.0.5', device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", revision='v1.0.5', trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\n\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹\", history=history)\nprint(response)\n```\n\n### Inferencia por lotes\nQwen admite la inferencia por lotes. Con la atenciÃ³n flash activada, el uso de la inferencia por lotes puede suponer un aumento de velocidad del 40%. El cÃ³digo de ejemplo se muestra a continuaciÃ³n:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\", history=None)\nprint(response)\n```\n\n### CPU\n\nPara desplegar nuestros modelos en la CPU, le recomendamos encarecidamente que utilice [qwen.cpp](https://github.com/QwenLM/qwen.cpp), que es una implementaciÃ³n C++ pura de Qwen y tiktoken. Comprueba el repositorio para mÃ¡s detalles.\n\nAdemÃ¡s, tambiÃ©n es sencillo ejecutar directamente el modelo en la CPU, lo que requiere que especifiques el dispositivo:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nPero es probable que sufra una eficacia de inferencia extremadamente baja.\n\n### MÃºltiples GPU\n\nSi sufres de falta de memoria en la GPU y quieres ejecutar el modelo en mÃ¡s de 1 GPU, puedes utilizar directamente el mÃ©todo de carga por defecto, que ahora es soportado por Transformers. El mÃ©todo anterior basado en `utils.py` estÃ¡ obsoleto.\n\nSin embargo, aunque este mÃ©todo es sencillo, la eficiencia del paralelismo del pipeline nativo es baja. Le aconsejamos que utilice vLLM con FastChat y por favor lea la secciÃ³n para el despliegue.\n\n### DashScope\n\nLa forma mÃ¡s sencilla de utilizar Qwen a travÃ©s de APIs es el servicio DashScope API a travÃ©s de Alibaba Cloud. Damos una introducciÃ³n al uso. AdemÃ¡s, proporcionamos un script para que despliegues una API estilo OpenAI en tus propios servidores.\n\nDashScope es el gran servicio de API de modelos lingÃ¼Ã­sticos proporcionado por Alibaba Cloud, que ahora es compatible con Qwen. Tenga en cuenta que los modelos detrÃ¡s de DashScope son versiones internas temporalmente sin detalles proporcionados. Los servicios incluyen `qwen-turbo` y `qwen-plus`, donde el primero se ejecuta mÃ¡s rÃ¡pido y el segundo consigue un mejor rendimiento. Para mÃ¡s informaciÃ³n, visita la documentaciÃ³n [aquÃ­](https://dashscope.aliyun.com).\n\nDirÃ­gete al sitio web oficial [enlace](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) para crear una cuenta DashScope y obtener la clave API (AK). Recomendamos configurar la AK con una variable de entorno:\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nA continuaciÃ³n, instala los paquetes y haz clic [aquÃ­](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) para consultar la documentaciÃ³n. Si utilizas Python, puedes instalar DashScope con pip:\n```bash\npip install dashscope\n```\nSi utiliza JAVA SDK, puede instalarlo de esta forma:\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nLa forma mÃ¡s sencilla de utilizar DashScope es el uso con mensajes, que es similar a la API OpenAI. El ejemplo se muestra a continuaciÃ³n:\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\nPara mÃ¡s usos, visite el sitio web oficial.\n<br><br>\n\n## CuantizaciÃ³n\n\n### GPTQ\n\nProporcionamos una soluciÃ³n basada en [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), y liberamos los modelos cuantificados Int4 e Int8, que consiguen efectos de modelo casi sin pÃ©rdidas pero un rendimiento mejorado tanto en costes de memoria como en velocidad de inferencia.\n\nAquÃ­ demostramos cÃ³mo utilizar los modelos cuantizados que proporcionamos para la inferencia. Antes de empezar, asegÃºrese de que cumple los requisitos de auto-gptq (por ejemplo, torch 2.0 y superior, transformers 4.32.0 y superior, etc.) e instale los paquetes necesarios:\n\n```bash\npip install auto-gptq optimum\n```\n\nSi tiene problemas para instalar `auto-gptq`, le aconsejamos que consulte el [repo] oficial (https://github.com/PanQiWei/AutoGPTQ) para encontrar una rueda.\n\n> Nota: Los paquetes `auto-gptq` precompilados dependen en gran medida de la versiÃ³n de `torch` y de su versiÃ³n CUDA. AdemÃ¡s, debido a la reciente actualizaciÃ³n \n> tambiÃ©n puede encontrar errores de versiÃ³n no soportada de `transformers`, `optimum`, o `peft`.\n> Recomendamos utilizar las Ãºltimas versiones que cumplan los siguientes requisitos:\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - antorcha>=2.0,<2.1 auto-gptq<0.5.0 transformadores<4.35.0 Ã³ptimo<1.14.0 peft>=0.5.0,<0.6.0\n\nA continuaciÃ³n, puede cargar el modelo cuantizado fÃ¡cilmente y ejecutar la inferencia como de costumbre:\n\n```python\n# Model names: \"Qwen/Qwen-7B-Chat-Int4\", \"Qwen/Qwen-14B-Chat-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\nIlustramos el rendimiento de los modelos BF16, Int8 e Int4 en la prueba de referencia, y observamos que el modelo cuantizado no sufre una degradaciÃ³n significativa del rendimiento. Los resultados se muestran a continuaciÃ³n:\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n\n### CuantizaciÃ³n de la cachÃ© KV\n\n> NOTA: Por favor, ten en cuenta que debido al mecanismo interno de Hugging Face, los archivos de soporte para esta funcionalidad\n> (es decir, `cache_autogptq_cuda_256.cpp` y `cache_autogptq_cuda_kernel_256.cu`). \n> Por favor, descÃ¡rguelos manualmente del Hugging Face Hub y colÃ³quelos en la misma carpeta que los demÃ¡s archivos del mÃ³dulo.\n\nLa cachÃ© KV de atenciÃ³n puede cuantificarse y comprimirse para su almacenamiento, con el fin de obtener un mayor rendimiento de la muestra. Los argumentos `use_cache_quantization` y `use_cache_kernel` en `config.json` se proporcionan para habilitar la cuantizaciÃ³n de la cachÃ© KV. \nEl mÃ©todo de uso especÃ­fico es el siguiente:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\nAtenciÃ³n: Actualmente, la cuantizaciÃ³n de cachÃ© KV y flash attention no se pueden utilizar al mismo tiempo.\nSi habilita la cuantizaciÃ³n de cachÃ© KV y flash attention al mismo tiempo (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` estÃ¡ deshabilitado por defecto (`use_flash_attn=false`).\n\nHemos comprobado que el uso del modelo int8-kvcache cuantizado no sufre una degradaciÃ³n significativa del rendimiento en la evaluaciÃ³n posterior. A continuaciÃ³n, nos centraremos en el anÃ¡lisis de su huella de memoria en diferentes condiciones. \nEl perfil se ejecuta en una Ãºnica GPU A100-SXM4-80G con PyTorch 2.0.1 y CUDA 11.4. \nUtilizamos modelos BF16 para generar 1024 tokens por defecto, y \"OOM\" indica error de memoria insuficiente.\n\nCon la cuantizaciÃ³n de la cachÃ© KV, el modelo puede inferir con un tamaÃ±o de lote (bs) mayor.\n\n| Utilizar la cachÃ© KV |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|----------------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| No                   | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |\n| Yes                  | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\nCon la cuantizaciÃ³n kv-cache activada, el modelo puede ahorrar mÃ¡s memoria cuando genera seq-length mÃ¡s largos (sl, nÃºmero de tokens generados) en infer.\n\n| Utilizar la cachÃ© KV | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|----------------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| No                   | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| Yes                  |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\nEl modelo con cuantificaciÃ³n de cachÃ© KV convertirÃ¡ el formato de `layer_past` de float a int8, y mientras tanto el `layer-past` cuantificado tambiÃ©n almacenarÃ¡ los parÃ¡metros de cuantificaciÃ³n.\n\nLos pasos especÃ­ficos son los siguientes\n\n1. Cuantificar clave/valor\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n2. Almacenar en layer_past\n\nA continuaciÃ³n se muestra el formato de `layer_past` cuantificado:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\nA continuaciÃ³n se muestra el formato original de `layer_past`:\n```\n    layer_past=(key,value)\n```\nSi desea utilizar la atenciÃ³n KV que se cuantiza, \npuede utilizar la operaciÃ³n de decuantizaciÃ³n para convertir la clave/valor int8 de nuevo al formato float de la siguiente manera:\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n\n## Rendimiento de Inferencia\n\nEsta secciÃ³n proporciona las estadÃ­sticas de velocidad y memoria de los modelos en diferentes precisiones. Los perfiles de velocidad y memoria se realizan utilizando [este script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).\n\nMedimos la velocidad media de inferencia (tokens/s) y el uso de memoria de la GPU al generar 2048 con los modelos en BF16, Int8 e Int4.\n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nEl perfil se ejecuta en una Ãºnica GPU A100-SXM4-80G (salvo que se mencione 2xA100) con PyTorch 2.0.1, CUDA 11.8 y Flash-Attention 2. (72B + vLLM utiliza PyTorch 2.1.0 y Cuda 11.8.) La velocidad de inferencia se promedia sobre los tokens codificados y generados.\n\nNota: La velocidad de generaciÃ³n de los modelos Int4/Int8 mencionados anteriormente es proporcionada por la librerÃ­a autogptq. La velocidad actual del modelo cargado utilizando ``AutoModelForCausalLM.from_pretrained`` serÃ¡ aproximadamente un 20% mÃ¡s lenta. Hemos informado de este problema al equipo de HuggingFace y lo actualizaremos rÃ¡pidamente si se encuentra una soluciÃ³n.\n\nTambiÃ©n medimos la velocidad de inferencia y el uso de memoria de la GPU con diferentes configuraciones de contexto y longitudes de generaciÃ³n, versiÃ³n Flash-Attention. Puedes encontrar los resultados en las modelcards correspondientes en Hugging Face o ModelScope.\n\n\n\n## Finetuning\n\n### UtilizaciÃ³n\nAhora proporcionamos el script de entrenamiento oficial, `finetune.py`, para que los usuarios puedan ajustar el modelo preentrenado para aplicaciones posteriores de forma sencilla. AdemÃ¡s, proporcionamos scripts de shell para lanzar el ajuste fino sin preocupaciones. Este script soporta el entrenamiento con [DeepSpeed](https://github.com/microsoft/DeepSpeed) y [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). Los shell scripts que proporcionamos utilizan DeepSpeed (Nota: esto puede tener conflictos con la Ãºltima versiÃ³n de pydantic y debe utilizar make sure `pydantic<2.0`) y Peft. Puede instalarlos de la siguiente manera:\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\nPara preparar tus datos de entrenamiento, necesitas poner todas las muestras en una lista y guardarla en un archivo json. Cada muestra es un diccionario que consiste en un id y una lista para la conversaciÃ³n. A continuaciÃ³n se muestra una lista de ejemplo simple con 1 muestra:\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚\"\n      }\n    ]\n  }\n]\n```\n\nUna vez preparados los datos, puede utilizar los scripts de shell suministrados para ejecutar el ajuste fino. Recuerde especificar la ruta al archivo de datos, `$DATA`.\n\nLos guiones de finetuning permiten realizar:\n- Finetuning de todos los parÃ¡metros\n- LoRA\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```bash\n# Entrenamiento distribuido. No proporcionamos un script de entrenamiento para una sola GPU, ya que la insuficiente memoria de la GPU interrumpirÃ­a el entrenamiento.\nbash finetune/finetune_ds.sh\n```\n\nRecuerde especificar el nombre correcto del modelo o ruta, la ruta de datos, asÃ­ como el directorio de salida en los scripts de shell. Otra cosa a notar es que usamos DeepSpeed ZeRO 3 en este script. Si desea realizar cambios, basta con eliminar el argumento `--deepspeed` o realizar cambios en el archivo json de configuraciÃ³n de DeepSpeed en funciÃ³n de sus necesidades. AdemÃ¡s, este script soporta entrenamiento de precisiÃ³n mixta, por lo que puedes usar `--bf16 True` o `--fp16 True`. Recuerde utilizar DeepSpeed cuando utilice fp16 debido al entrenamiento de precisiÃ³n mixta. \nEmpÃ­ricamente le aconsejamos que utilice bf16 para que su entrenamiento sea coherente con nuestro preentrenamiento y alineaciÃ³n si su mÃ¡quina soporta bf16, y por lo tanto lo utilizamos por defecto.\n\nPara ejecutar LoRA, utilice otro script para ejecutar como se muestra a continuaciÃ³n. Antes de empezar, asegÃºrese de que ha instalado `peft`. AdemÃ¡s, es necesario especificar las rutas a su modelo, los datos y la salida. Le aconsejamos que utilice la ruta absoluta para su modelo pre-entrenado. Esto se debe a que LoRA sÃ³lo guarda el adaptador y la ruta absoluta en el archivo json de configuraciÃ³n del adaptador se utiliza para encontrar el modelo preentrenado para cargar. AdemÃ¡s, este script soporta tanto bf16 como fp16.\n\n```bash\n# Single GPU training\nbash finetune/finetune_lora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_lora_ds.sh\n```\n\nEn comparaciÃ³n con el ajuste fino de parÃ¡metros completos, LoRA ([artÃ­culo](https://arxiv.org/abs/2106.09685)) sÃ³lo actualiza los parÃ¡metros de las capas adaptadoras, pero mantiene congeladas las grandes capas originales del modelo de lenguaje. Esto permite muchos menos costes de memoria y, por tanto, de computaciÃ³n.\n\nTenga en cuenta que si utiliza LoRA para ajustar el modelo de lenguaje base, por ejemplo, Qwen-7B, en lugar de los modelos de chat, por ejemplo, Qwen-7B-Chat, el script cambia automÃ¡ticamente la incrustaciÃ³n y la capa de salida como parÃ¡metros entrenables. Esto se debe a que el modelo de lenguaje base no tiene conocimiento de los tokens especiales que aporta el formato ChatML. Por lo tanto, estas capas deben actualizarse para que el modelo comprenda y prediga los tokens. O en otras palabras, si tu entrenamiento trae tokens especiales en LoRA, deberÃ­as poner las capas como parÃ¡metros entrenables poniendo `modules_to_save` dentro del cÃ³digo. AdemÃ¡s, si tenemos estos parÃ¡metros entrenables, no estÃ¡ disponible para usar ZeRO 3, y es por esto que usamos ZeRO 2 en el script por defecto. Si no tenemos nuevos parÃ¡metros entrenables, podemos cambiar a ZeRO 3 cambiando el fichero de configuraciÃ³n de DeepSpeed. AdemÃ¡s, encontramos que hay una brecha significativa entre la huella de memoria de LoRA con y sin estos parÃ¡metros entrenables. Por lo tanto, si usted tiene problemas con la memoria, le aconsejamos LoRA finetune los modelos de chat. Compruebe el perfil de abajo para obtener mÃ¡s informaciÃ³n.\n\nSi sigue sufriendo de memoria insuficiente, puede considerar Q-LoRA ([artÃ­culo](https://arxiv.org/abs/2305.14314)), que utiliza el modelo de lenguaje cuantizado de gran tamaÃ±o y otras tÃ©cnicas como la atenciÃ³n paginada para permitir incluso menos costes de memoria.\n\nNota: para ejecutar el entrenamiento Q-LoRA con una sola GPU, puede que necesites instalar `mpi4py` a travÃ©s de `pip` o `conda`.\n\nPara ejecutar Q-LoRA, ejecute directamente el siguiente script:\n\n```bash\n# Entrenamiento con una sola GPU\nbash finetune/finetune_qlora_single_gpu.sh\n# Entrenamiento distribuida\nbash finetune/finetune_qlora_ds.sh\n```\n\nPara Q-LoRA, le aconsejamos que cargue nuestro modelo cuantizado proporcionado, por ejemplo, Qwen-7B-Chat-Int4. **NO DEBE** utilizar los modelos bf16. A diferencia del finetuning de parÃ¡metros completos y LoRA, sÃ³lo fp16 es compatible con Q-LoRA. Para el entrenamiento con una sola GPU, tenemos que utilizar DeepSpeed para el entrenamiento de precisiÃ³n mixta debido a nuestra observaciÃ³n de errores causados por el amplificador de antorcha. AdemÃ¡s, para Q-LoRA, los problemas con los tokens especiales en LoRA siguen existiendo. Sin embargo, como sÃ³lo proporcionamos los modelos Int4 para los modelos de chat, lo que significa que el modelo lingÃ¼Ã­stico ha aprendido los tokens especiales del formato ChatML, no hay que preocuparse por las capas. Ten en cuenta que las capas del modelo Int4 no deben ser entrenables, por lo que si introduces tokens especiales en tu entrenamiento, Q-LoRA podrÃ­a no funcionar.\n\n> NOTA: Tenga en cuenta que debido a los mecanismos internos de Hugging Face, ciertos archivos que no son de Python (por ejemplo, `*.cpp` y `*.cu`) pueden faltar en el punto de control guardado. \n> pueden faltar en el punto de control guardado. Es posible que tenga que copiarlos manualmente en el directorio que contiene otros archivos.\n\nA diferencia del finetuning de parÃ¡metros completo, el entrenamiento de LoRA y Q-LoRA sÃ³lo guarda los parÃ¡metros del adaptador. Supongamos que su entrenamiento comienza desde Qwen-7B, puede cargar el modelo ajustado para la inferencia como se muestra a continuaciÃ³n:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nSi quieres fusionar los adaptadores y guardar el modelo ajustado como un modelo independiente (sÃ³lo puedes hacer esto con LoRA, y NO puedes fusionar los parÃ¡metros desde Q-LoRA), puedes ejecutar los siguientes cÃ³digos:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\nNota: Para el entrenamiento multi-GPU, es necesario especificar los hiperparÃ¡metros adecuados para el entrenamiento distribuido basado en su mÃ¡quina. AdemÃ¡s, le aconsejamos que especifique la longitud mÃ¡xima de la secuencia con el argumento `--model_max_length`, en funciÃ³n de los datos, el espacio de memoria y la velocidad de entrenamiento.\n\n\n### Perfiles de Memoria y Velocidad\nPerfilamos la memoria de la GPU y la velocidad de entrenamiento tanto de LoRA (LoRA (emb) se refiere al entrenamiento de la capa de incrustaciÃ³n y salida, mientras que LoRA no tiene capa de incrustaciÃ³n y salida entrenables) como de Q-LoRA en la configuraciÃ³n de entrenamiento en una sola GPU. En esta prueba, experimentamos con una Ãºnica GPU A100-SXM4-80G, y utilizamos CUDA 11.8 y Pytorch 2.0. Se aplica Flash attention 2. Utilizamos uniformemente un tamaÃ±o de lote de 1 y una acumulaciÃ³n de gradiente de 8. Perfilamos la memoria (GB) y la velocidad (s/iter) de entradas de distintas longitudes, a saber, 256, 512, 1024, 2048, 4096 y 8192. TambiÃ©n presentamos las estadÃ­sticas del ajuste fino de todos los parÃ¡metros con Qwen-7B en 2 GPU A100. SÃ³lo se presentan las estadÃ­sticas de 256, 512 y 1024 tokens debido a la limitaciÃ³n de memoria de la GPU. \n\nPara Qwen-72B, experimentamos de dos formas: 1) Ajuste fino de Lora + DeepSpeed ZeRO 3 en 4 GPUs A100-SXM4-80G y 2) Ajuste fino de QLora (int4) en una sola GPU A100-SXM4-80G. Ten en cuenta que la OOM se produce en 4 GPUs A100-SXM4-80G tanto con ajuste fino LoRA (emb) como con ajuste fino LoRA sin Deepspeed ZeRO 3 (puedes pasar `--deepspeed finetune/ds_config_zero3.json` a [`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh) para activar DeepSpeed ZeRO 3).\n\nLas estadÃ­sticas se enumeran a continuaciÃ³n:\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    </tr>\n    </tr>\n\t\t<tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td><td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">7B</th><td>LoRA</td><td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th><td>LoRA</td><td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n\t<tr>\n        <th rowspan=\"2\">72B</th><td>LoRA + Deepspeed Zero3</td><td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n<br>\n\n## Despliegue\n\n### vLLM \nPara el despliegue y la inferencia rÃ¡pida, sugerimos utilizar vLLM con FastChat. Instale primero los paquetes:\n```bash\npip install vllm fastchat\n```\nO puede instalarlos desde el cÃ³digo fuente mediante `git clone` y `pip install -e .`. Le aconsejamos que lea sus documentos si encuentra problemas en la instalaciÃ³n.\n\nPara ejecutar Qwen con vLLM y FastChat, primero necesitas lanzar un controlador por:\n```bash\npython -m fastchat.serve.controller\n```\n\nA continuaciÃ³n, puede iniciar el model worker, lo que significa cargar su modelo para la inferencia. Para la inferencia de una sola GPU, puede ejecutar directamente:\n\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code\n```\nSin embargo, si desea ejecutar el modelo en varias GPU para acelerar la inferencia o disponer de mÃ¡s memoria, puede utilizar el paralelismo tensorial soportado por vLLM. Supongamos que ejecutas el modelo en 4 GPUs, el comando se muestra a continuaciÃ³n:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4\n```\n\nDespuÃ©s de lanzar tu model worker, puedes lanzar:\n\n* Web UI Demo\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* API OpenAI\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\n### Interfaz Web\n\nProporcionamos cÃ³digo para que los usuarios construyan una web UI demo (gracias a @wysaid). Antes de empezar, asegÃºrate de instalar los siguientes paquetes:\n```\npip install -r requirements_web_demo.txt\n```\n\nA continuaciÃ³n, ejecute el siguiente comando y haga clic en el enlace generado:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\nSin embargo, si le resulta difÃ­cil utilizar vLLM y FastChat, puede probar los mÃ©todos mÃ¡s sencillos que le proporcionamos para desplegar una demo web, una demo CLI y una API.\n\n### Demo CLI\n\nProporcionamos un ejemplo de demostraciÃ³n CLI en `cli_demo.py`, que soporta la salida de streaming para la generaciÃ³n. Los usuarios pueden interactuar con Qwen-7B-Chat introduciendo mensajes, y el modelo devuelve los resultados del modelo en modo streaming. Ejecute el siguiente comando:\n\n```bash\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\nProporcionamos mÃ©todos para desplegar la API local basada en la API de OpenAI (gracias a @hanpenggit). Antes de empezar, instala los paquetes necesarios:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nA continuaciÃ³n, ejecute el comando para desplegar su API:\n\n```bash\npython openai_api.py\n```\n\nPuede cambiar sus argumentos, por ejemplo, `-c` para el nombre o la ruta del punto de control, `--cpu-only` para el despliegue en CPU, etc. Si tienes problemas al iniciar el despliegue de tu API, probablemente puedas solucionarlos actualizando los paquetes a la Ãºltima versiÃ³n.\n\nUtilizar la API tambiÃ©n es sencillo. Vea el siguiente ejemplo:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# create a request activating streaming response\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=True \n    # Specifying stop words in streaming output format is not yet supported and is under development.\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# create a request not activating streaming response\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=False,\n    stop=[] # You can add custom stop words here, e.g., stop=[\"Observation:\"] for ReAct prompting.\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n**Function calling** tambiÃ©n estÃ¡ soportada (pero sÃ³lo cuando `stream=False` por el momento). Ver el [ejemplo de uso](examples/function_call_examples.py) aquÃ­.\n<br><br>\n\n## ğŸ³ Docker\n\nPara simplificar el proceso de despliegue, proporcionamos imÃ¡genes Docker con entornos preconstruidos: [qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen). Solo tienes que instalar el controlador y descargar los archivos del modelo para lanzar demos, desplegar la API de OpenAI y ajustar el modelo.\n\n### PreparaciÃ³n\n\n1. Instale la versiÃ³n correcta del controlador Nvidia en funciÃ³n de la imagen que vaya a utilizar:\n  - `qwenllm/qwen:cu117` (**recomendado**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: igual que `qwenllm/qwen:cu117`\n\n2. Instale y configure [docker](https://docs.docker.com/engine/install/) y [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html):\n\n```bash\n# configure docker\nsudo systemctl start docker\n# test if docker is correctly installed\nsudo docker run hello-world\n\n# configure nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. Descargue los checkpoints y los cÃ³digos del modelo a su entorno (vÃ©ase [aquÃ­](#DownloadModel)).\n\n### Despliegue\n\nAquÃ­ usamos Qwen-7B-Chat como ejemplo. Antes de lanzar una demo web o API, puede establecer la configuraciÃ³n como se muestra a continuaciÃ³n:\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes\n```\nLos siguientes scripts pueden ayudarte a construir:\n\n* API OpenAI\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Interfaz Web\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Demo CLI\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\nLos comandos anteriores descargarÃ¡n automÃ¡ticamente la imagen requerida y lanzarÃ¡n una demo Web UI en segundo plano (el servicio se reiniciarÃ¡ automÃ¡ticamente). Puede abrir `http://localhost:${PORT}` en el host para utilizar la demo.\n\nLa demostraciÃ³n se ha iniciado correctamente si ve la siguiente salida:\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nSi quieres comprobar el estado de la demo, puedes usar `docker logs qwen` para mostrar los resultados.\n\nPuede utilizar `docker rm -f qwen` para detener el servicio y eliminar el contenedor.\n\n\n### Finetuning\n\nEl mÃ©todo de finetuning utilizando la imagen Docker pre-construida es bÃ¡sicamente el mismo que [el capÃ­tulo anterior](#Finetuning) (ya hemos instalado dependencias en la imagen):\n\nA continuaciÃ³n se muestra un ejemplo de LoRA de GPU Ãºnica:\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs\n\n# Use all host devices by default\nDEVICE=all\n# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# Single-GPU LoRA finetuning\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\nPara realizar un cambio a Q-LoRA de una sola GPU, por ejemplo, basta con modificar el comando bash dentro de `docker run`:\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## ğŸ”¥ Indicaciones del sistema\nQwen-1.8-Chat y Qwen-72B-Chat han sido completamente entrenados en diversas indicaciones del sistema con mÃºltiples rondas de interacciones complejas, para que puedan seguir una variedad de indicaciones del sistema y realizar la personalizaciÃ³n del modelo en contexto, mejorando aÃºn mÃ¡s la escalabilidad de Qwen-chat.\n\nGracias a las instrucciones del sistema, Qwen-Chat puede realizar **juegos de rol**, **transferencia de estilos de lenguaje**, **configuraciÃ³n de tareas** y **configuraciÃ³n de comportamientos**.\n\n![](assets/system_prompt_language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\nPara mÃ¡s informaciÃ³n, consulta la [documentaciÃ³n de ejemplo](examples/system_prompt.md).\n\n\n## Uso de Herramientas\n\nQwen-Chat ha sido optimizado para el uso de herramientas y capacidades de llamada a funciones. Los usuarios pueden desarrollar agentes, aplicaciones LangChain e incluso aumentar Qwen con un intÃ©rprete de cÃ³digo Python.\n\nProporcionamos documentaciÃ³n sobre cÃ³mo implementar llamadas a herramientas basadas en el principio de ReAct Prompting, por favor consulte [the ReAct example](examples/react_prompt.md). BasÃ¡ndonos en este principio, proporcionamos soporte para llamadas a funciones en [openai_api.py](openai_api.py).\n\nHemos probado las capacidades de llamada de la herramienta del modelo en nuestro punto de referencia de evaluaciÃ³n chino de cÃ³digo abierto y hemos descubierto que Qwen-Chat obtiene siempre buenos resultados:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.â†‘)</th><th align=\"center\">Tool Input (Rouge-Lâ†‘)</th><th align=\"center\">False Positive Errorâ†“</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\nPara evaluar la capacidad de Qwen para utilizar el intÃ©rprete de cÃ³digo Python en tareas como la resoluciÃ³n de problemas matemÃ¡ticos, la visualizaciÃ³n de datos y otras tareas de propÃ³sito general como el manejo de archivos y el web scraping, hemos creado y puesto a disposiciÃ³n del pÃºblico un benchmark especÃ­ficamente diseÃ±ado para evaluar estas capacidades. Puede encontrar el punto de referencia en este [enlace](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nHemos observado que Qwen funciona bien en tÃ©rminos de ejecutabilidad del cÃ³digo y precisiÃ³n de los resultados al generar cÃ³digo:\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Mathâ†‘</th><th align=\"center\">Visualization-Hardâ†‘</th><th align=\"center\">Visualization-Easyâ†‘</th><th align=\"center\">Generalâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## ComprensiÃ³n del Contexto Largo\n\nPara ampliar la longitud del contexto y romper el cuello de botella de la longitud de la secuencia de entrenamiento, introducimos varias tÃ©cnicas, como la interpolaciÃ³n NTK, la atenciÃ³n de ventana y el escalado de atenciÃ³n LogN, para ampliar la longitud del contexto de Qwen-14B de 2K a mÃ¡s de 8K tokens, y Qwen-1.8B/7B de 8K a 32K tokens. \n\nPara Qwen-72B, adaptamos RoPE a contextos mÃ¡s largos con una base rotatoria mayor. Qwen-72B admite una longitud mÃ¡xima de contexto de 32K tokens.\n\nRealizamos experimentos de modelado lingÃ¼Ã­stico en el conjunto de datos arXiv con la evaluaciÃ³n PPL y descubrimos que Qwen puede alcanzar un rendimiento sobresaliente en el escenario de contextos largos. Los resultados se muestran a continuaciÃ³n:\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.56</td><td align=\"center\">4.62</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.49</td><td align=\"center\">4.32</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n    <tr>\n        <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n    </tr>\n</table>\n\nFurthermore, to verify the ability of Qwen-72B-Chat on long text understanding, we tested it on [L-Eval](https://arxiv.org/abs/2307.11088) (closed-ended tasks). The results are as follows:\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\nHemos realizado el experimento de la \"aguja en el pajar\" (la idea procede de [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)) para comprobar si el modelo puede recuperar informaciÃ³n en distintas posiciones de las entradas de distintas longitudes, el resultado es el siguiente:\n\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\nLos resultados anteriores muestran que Qwen-72B-Chat puede recuperar con precisiÃ³n informaciÃ³n situada en varias posiciones dentro de una longitud de entrada de 32K, lo que demuestra su excelente capacidad de comprensiÃ³n de textos largos.\n\n\n## Tokenizador\n\nNuestro tokenizador basado en tiktoken es diferente de otros tokenizadores, por ejemplo, el tokenizador sentencepiece. Es necesario prestar atenciÃ³n a los tokens especiales, especialmente en el finetuning. Para obtener informaciÃ³n mÃ¡s detallada sobre el tokenizador y su uso en el ajuste fino, consulte la [documentaciÃ³n](tokenization_note.md).\n<br><br>\n\n## ReproducciÃ³n\n\nPara que pueda reproducir el rendimiento del modelo en conjuntos de datos de referencia, le proporcionamos secuencias de comandos para que reproduzca los resultados. Consulte [eval/EVALUATION.md](eval/EVALUATION.md) para obtener mÃ¡s informaciÃ³n. Tenga en cuenta que la reproducciÃ³n puede dar lugar a ligeras diferencias con respecto a nuestros resultados.\n<br><br>\n\n## FAQ\n\nSi tiene problemas, consulte primero [FAQ](FAQ.md) y las incidencias para buscar una soluciÃ³n antes de lanzar una nueva incidencia.\n<br><br>\n\n## Cita\nSi nuestro trabajo le resulta Ãºtil, no dude en citarnos.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## Acuerdo de Licencia\n\nEl cÃ³digo fuente proporcionado en <https://github.com/QwenLM/Qwen> estÃ¡ licenciado bajo la [Licencia Apache 2.0](./LICENSE) que puede encontrarse en el directorio raÃ­z.\n\nLos investigadores y desarrolladores son libres de utilizar los cÃ³digos y los pesos de los modelos tanto de Qwen como de Qwen-Chat. Para su uso comercial, consulte el Acuerdo de Licencia que acompaÃ±a a cada modelo.\n\n- Qwen-72B, Qwen-14B, y Qwen-7B estÃ¡n licenciados bajo el [Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT) que se puede encontrar en el repositorio correspondiente de HuggingFace y ModelScope. Para uso comercial, rellene el formulario ([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), y [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen)) para solicitarlo.\n\n- Qwen-1.8B estÃ¡ licenciado bajo el [Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT) que puede encontrarse en el repositorio correspondiente de HuggingFace y ModelScope. Para uso comercial, pÃ³ngase en contacto con nosotros.\n<br><br>\n\n## Contacte con Nosotros\n\nSi estÃ¡s interesado en dejar un mensaje a nuestro equipo de investigaciÃ³n o de producto, Ãºnete a nuestros grupos de Discord o WeChat. TambiÃ©n puedes enviar un correo electrÃ³nico a qianwen_opensource@alibabacloud.com.\n\n"
        },
        {
          "name": "README_FR.md",
          "type": "blob",
          "size": 73.9296875,
          "content": "<p align=\"left\">\n    <a href=\"README_CN.md\">ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href=\"README.md\">English</a>&nbsp ï½œ &nbsp<a href=\"README_JA.md\">æ—¥æœ¬èª</a>&nbsp ï½œ &nbspFranÃ§ais ï½œ &nbsp<a href=\"README_ES.md\">EspaÃ±ol</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        ğŸ¤— <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br><br>\n\n> [!Important]\n> Qwen2 est lÃ  ! Vous Ãªtes invitÃ© Ã  suivre [QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) et Ã  partager vos expÃ©riences lÃ -bas.\n>\n> Ce repo ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) n'est plus activement maintenu, en raison de diffÃ©rences substantielles dans le code source.\n<br>\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">ğŸ¤—</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">ğŸ¤—</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">ğŸ¤—</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">ğŸ¤—</a> |\n\n\n\nNous ouvrons notre sÃ©rie **Qwen**, qui comprend dÃ©sormais **Qwen**, les modÃ¨les de langue de base, Ã  savoir **Qwen-7B** et **Qwen-14B**, ainsi que **Qwen-Chat**, les modÃ¨les de chat, Ã  savoir **Qwen-7B-Chat** et **Qwen-14B-Chat**. Les liens se trouvent dans le tableau ci-dessus. Cliquez dessus et consultez les fiches des modÃ¨les. Nous publions Ã©galement le **[rapport technique](https://arxiv.org/abs/2309.16609)**. Cliquez sur le lien du document et consultez-le !\n\nEn bref, nous disposons de modÃ¨les linguistiques solides, qui ont Ã©tÃ© prÃ©-entraÃ®nÃ© de maniÃ¨re stable pour 3 000 milliards de tokens de donnÃ©es multilingues avec une large couverture de domaines, de langues (en particulier le chinois et l'anglais), etc. Ils sont capables d'atteindre des performances compÃ©titives sur des ensembles de donnÃ©es de rÃ©fÃ©rence. En outre, nous disposons de modÃ¨les de chat alignÃ©s sur les prÃ©fÃ©rences humaines basÃ©es sur SFT et RLHF (pas encore publiÃ©s), qui sont capables de chatter, de crÃ©er du contenu, d'extraire des informations, de rÃ©sumer, de traduire, de coder, de rÃ©soudre des problÃ¨mes mathÃ©matiques, etc. et d'utiliser des outils, de jouer le rÃ´le d'agents ou mÃªme code interpreter, etc.\n\n| ModÃ¨le    | Date de sortie | Longueur maximale | AmÃ©lioration de l'invite du systÃ¨me | # de tokens prÃ©-formÃ©s | Utilisation minimale de la mÃ©moire du GPU pour Finetuning (Q-Lora) | Utilisation minimale du GPU pour gÃ©nÃ©rer 2048 jetons (Int4) | Utilisation des outils |\n|:----------|:--------------:|:-----------------:|:-----------------------------------:|:----------------------:|:------------------------------------------------------------------:|:-----------------------------------------------------------:|:----------------------:|\n| Qwen-1.8B |    23.11.30    |        32K        |                  âœ…                  |          2.2T          |                               5.8GB                                |                            2.9GB                            |           âœ…            |  \n| Qwen-7B   |    23.08.03    |        32K        |                  â                  |          2.4T          |                               11.5GB                               |                            8.2GB                            |           âœ…            |   \n| Qwen-14B  |    23.09.25    |        8K         |                  â                  |          3.0T          |                               18.7GB                               |                           13.0GB                            |           âœ…            |\n| Qwen-72B  |    23.11.30    |        32K        |                  âœ…                  |          3.0T          |                               61.4GB                               |                           48.9GB                            |           âœ…            |   \n\n\nDans la repo, vous pouvez trouver:\n\n* Comment utiliser Qwen, et profiter de l'infÃ©rence simple.\n* DÃ©tails sur les modÃ¨les de quantization, y compris GPTQ et la quantization de KV cache.\n* Statistiques sur les performances de l'infÃ©rence, y compris la vitesse et la mÃ©moire.\n* Tutoriels sur le finetuning, y compris le finetuning de paramÃ¨tres complets, LoRA, et Q-LoRA.\n* Instructions de dÃ©ploiement, avec l'exemple de vLLM et FastChat.\n* Instructions sur la crÃ©ation de dÃ©mos, y compris WebUI, dÃ©mo CLI, etc.\n* Introduction au service API de DashScope, ainsi que les instructions pour construire une API de type OpenAI pour votre modÃ¨le.\n* Informations sur Qwen pour l'utilisation d'outils, d'agents et code interpreter.\n* Statistiques de l'Ã©valuation de la comprÃ©hension du contexte long.\n* Contrat de licence.\n* ...\n\nEn outre, si vous rencontrez des problÃ¨mes, consultez d'abord la [FAQ](FAQ.md) pour obtenir de l'aide. Vous vous sentez toujours en difficultÃ© ? N'hÃ©sitez pas Ã  nous envoyer des questions (de prÃ©fÃ©rence en anglais pour que plus de gens puissent vous comprendre) ! Si vous souhaitez nous aider, envoyez-nous des demandes d'extension sans hÃ©sitation ! Nous sommes toujours enthousiastes Ã  propos des relations publiques ! \n\nVous voulez discuter avec nous ou prendre un cafÃ© avec nous ? Bienvenue sur notre Discord ou WeChat !\n<br><br>\n\n## Nouvelles et mises Ã  jour\n\n* 2023.11.30 ğŸ”¥ Nous publions **Qwen-72B** et **Qwen-72B-Chat**, qui sont entraÃ®nÃ©s sur des tokens 3T et prennent en charge 32k contextes, ainsi que **Qwen-1.8B** et **Qwen-1.8B-Chat**, sur ModelScope et Hugging Face. Nous avons Ã©galement renforcÃ© les capacitÃ©s de l'invite systÃ¨me du Qwen-72B-Chat et du Qwen-1.8B-Chat, voir la [documentation d'exemple](examples/system_prompt.md). De plus, nous supportons l'infÃ©rence sur **Ascend 910** et **Hygon DCU**. Consultez `ascend-support` et `dcu-support` pour plus de dÃ©tails.\n* 2023.10.17 Nous publions le modÃ¨le quantifiÃ© Int8 **Qwen-7B-Chat-Int8** et **Qwen-14B-Chat-Int8**.\n* 2023.9.25 ğŸ”¥ Nous publions **Qwen-14B** et **Qwen-14B-Chat** sur ModelScope et Hugging Face, ainsi que [qwen.cpp](https://github.com/QwenLM/qwen.cpp) et [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). Les codes et les poids de **Qwen-7B** et **Qwen-7B-Chat** ont Ã©galement Ã©tÃ© mis Ã  jour. **S'IL VOUS PLAÃT, TIREZ LA DERNIÃˆRE VERSION!**\n    - Par rapport Ã  **Qwen-7B** (original), **Qwen-7B** utilise davantage de jetons d'entraÃ®nement, passant de 2,2 Ã  2,4T de jetons, tandis que la longueur du contexte passe de 2048 Ã  8192. La connaissance du chinois et la capacitÃ© de codage de **Qwen-7B** ont Ã©tÃ© encore amÃ©liorÃ©es.\n* 2023.9.12 Nous prenons dÃ©sormais en charge le finetuning sur les modÃ¨les Qwen-7B, y compris le finetuning de tous les paramÃ¨tres, LoRA et Q-LoRA.\n* 2023.8.21 Nous publions le modÃ¨le quantifiÃ© Int4 pour Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, qui nÃ©cessite de faibles coÃ»ts de mÃ©moire mais permet d'amÃ©liorer la vitesse d'infÃ©rence. En outre, il n'y a pas de dÃ©gradation significative des performances lors de l'Ã©valuation de rÃ©fÃ©rence.\n* 2023.8.3 Nous publions **Qwen-7B** et **Qwen-7B-Chat** sur ModelScope et Hugging Face. Nous fournissons Ã©galement un mÃ©mo technique pour plus de dÃ©tails sur le modÃ¨le, y compris les dÃ©tails de l'entraÃ®nement et les performances du modÃ¨le.\n<br>\n\n## Performance\n\nLes modÃ¨les Qwen surpassent les modÃ¨les de base de taille similaire sur une sÃ©rie de donnÃ©es de rÃ©fÃ©rence, par exemple MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., qui Ã©valuent les capacitÃ©s des modÃ¨les sur la comprÃ©hension du langage naturel, la rÃ©solution de problÃ¨mes mathÃ©matiques, le codage, etc. Qwen-72B obtient de meilleures performances que LLaMA2-70B dans toutes les tÃ¢ches et surpasse GPT-3.5 dans 7 tÃ¢ches sur 10.\n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=600px/>\n<p>\n<br>\n\n| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |\n| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\nPour tous les modÃ¨les comparÃ©s, nous indiquons les meilleurs scores entre leurs rÃ©sultats officiels et [OpenCompass] (https://opencompass.org.cn/leaderboard-llm). \n\nPour plus de rÃ©sultats expÃ©rimentaux (performances dÃ©taillÃ©es des modÃ¨les sur d'autres ensembles de donnÃ©es de rÃ©fÃ©rence) et de dÃ©tails, veuillez vous rÃ©fÃ©rer Ã  notre rapport technique en cliquant [ici](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).\n<br><br>\n\n## Besoins\n\n* python 3.8 et plus\n* pytorch 1.12 et plus, 2.0 et plus sont recommandÃ©s\n* transformers 4.32 et plus\n* CUDA 11.4 et plus sont recommandÃ©s (pour les utilisateurs de GPU, les utilisateurs de flash, etc.)\n<br>\n\n## DÃ©marrage Rapide\n\nCi-dessous, nous fournissons des exemples simples pour montrer comment utiliser Qwen-Chat avec ğŸ¤– ModelScope et ğŸ¤— Transformers.\n\nVous pouvez utiliser nos images docker prÃ©-construites pour sauter la plupart des Ã©tapes de configuration de l'environnement, voir la section [\"Utiliser des images docker prÃ©-construites\"](#-docker) pour plus de dÃ©tails. \n\nSi vous n'utilisez pas Docker, assurez-vous d'avoir configurÃ© l'environnement et installÃ© les paquets requis. Assurez-vous de rÃ©pondre aux exigences ci-dessus, puis installez les bibliothÃ¨ques dÃ©pendantes.\n\n```bash\npip install -r requirements.txt\n```\n\nSi votre appareil supporte fp16 ou bf16, nous vous recommandons d'installer [flash-attention](https://github.com/Dao-AILab/flash-attention) (**nous supportons flash-attention 2 maintenant.**) pour une meilleure efficacitÃ© et une moindre utilisation de la mÃ©moire. (**flash-attention est optionnel et le projet peut fonctionner normalement sans l'installer**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# Below are optional. Installing them might be slow.\n# pip install csrc/layer_norm\n# pip install csrc/rotary\n```\n\nVous pouvez maintenant commencer avec ModelScope ou Transformers.\n\n### ğŸ¤— Transformers\n\nPour utiliser Qwen-Chat pour l'infÃ©rence, il vous suffit de saisir quelques lignes de code, comme indiquÃ© ci-dessous. N'oubliez pas de transmettre les noms de modÃ¨les ou les chemins corrects, tels que \"Qwen/Qwen-7B-Chat\" et \"Qwen/Qwen-14B-Chat\". Cependant, **veuillez vous assurer que vous utilisez le code le plus rÃ©cent**.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\n# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\n# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚\n# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚\n# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚\n# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚\n# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚\n\n# 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹\n```\n\nL'exÃ©cution du modÃ¨le prÃ©-entraÃ®nÃ© de Qwen est Ã©galement simple.\n\n<details>\n  <summary>Running Qwen</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model names: \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\" \ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...\n```\n\n</details>\n\nEn cas de problÃ¨me de rÃ©seau lors de la tentative de tÃ©lÃ©chargement des poids et des codes du modÃ¨le Ã  partir de HuggingFace, une autre approche consiste Ã  rÃ©cupÃ©rer le point de contrÃ´le Ã  partir de ModelScope, puis Ã  le charger Ã  partir du rÃ©pertoire local, comme indiquÃ© ci-dessous:\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n# model_dir = snapshot_download('qwen/Qwen-14B')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### ğŸ¤– ModelScope\n\nModelScope est une plateforme opensource pour Model-as-a-Service (MaaS), qui fournit un service de modÃ¨le flexible et rentable aux dÃ©veloppeurs d'IA. De mÃªme, vous pouvez exÃ©cuter les modÃ¨les avec ModelScope comme indiquÃ© ci-dessous:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# Model names: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\n\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹\", history=history)\nprint(response)\n```\n\n### InfÃ©rence par lots\nQwen prend en charge l'infÃ©rence par lots. Lorsque flash attention est activÃ©e, l'utilisation de l'infÃ©rence par lots peut entraÃ®ner une accÃ©lÃ©ration de 40 %. Le code d'exemple est prÃ©sentÃ© ci-dessous:\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\", history=None)\nprint(response)\n```\n\n### CPU\n\nPour dÃ©ployer nos modÃ¨les sur CPU, nous vous conseillons vivement d'utiliser [qwen.cpp](https://github.com/QwenLM/qwen.cpp), qui est une implÃ©mentation purement C++ de Qwen et de tiktoken. Consultez le repo pour plus de dÃ©tails!\n\nIl est simple d'exÃ©cuter directement le modÃ¨le sur le CPU, ce qui nÃ©cessite la spÃ©cification de votre appareil:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nCependant, il est probable que vous souffriez d'une efficacitÃ© d'infÃ©rence extrÃªmement faible.\n\n### Plusieurs GPU\n\nSi vous souffrez d'un manque de mÃ©moire GPU et que vous souhaitez exÃ©cuter le modÃ¨le sur plus d'un GPU, vous pouvez utiliser directement la mÃ©thode de chargement par dÃ©faut, qui est maintenant supportÃ©e par Transformers. La mÃ©thode prÃ©cÃ©dente basÃ©e sur `utils.py` est obsolÃ¨te.\n\nCependant, bien que cette mÃ©thode soit simple, l'efficacitÃ© du parallÃ©lisme natif du pipeline est faible. Nous vous conseillons d'utiliser vLLM avec FastChat et de lire la section relative au dÃ©ploiement.\n\n\n### DashScope\n\nLe moyen le plus simple d'utiliser Qwen via les API est le service API DashScope via Alibaba Cloud. Nous prÃ©sentons une introduction Ã  l'utilisation. De plus, nous fournissons un script pour vous permettre de dÃ©ployer une API de type OpenAI sur vos propres serveurs.\n\nDashScope est le service API de grands modÃ¨les linguistiques fourni par Alibaba Cloud, qui prend dÃ©sormais en charge Qwen. Notez que les modÃ¨les derriÃ¨re DashScope sont des versions internes temporairement sans dÃ©tails fournis. Les services comprennent `qwen-turbo` et `qwen-plus`, le premier fonctionnant plus rapidement et le second atteignant de meilleures performances. Pour plus d'informations, consultez la documentation [ici] (https://dashscope.aliyun.com).\n\nVeuillez vous rendre sur le site officiel [lien](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) pour crÃ©er un compte DashScope et obtenir la clÃ© API (AK). Nous recommandons de dÃ©finir l'AK Ã  l'aide d'une variable d'environnement:\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nInstallez ensuite les paquets et cliquez sur [ici](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) pour obtenir la documentation. Si vous utilisez Python, vous pouvez installer DashScope avec pip:\n```bash\npip install dashscope\n```\nSi vous utilisez JAVA SDK, vous pouvez l'installer de cette maniÃ¨re:\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nLa maniÃ¨re la plus simple d'utiliser DashScope est l'utilisation de messages, qui est similaire Ã  l'API OpenAI. L'exemple est prÃ©sentÃ© ci-dessous:\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\nPour d'autres utilisations, veuillez consulter le site web officiel pour plus de dÃ©tails.\n<br><br>\n\n## Quantization\n\n### GPTQ\n\nNous proposons une solution basÃ©e sur [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), et publions les modÃ¨les quantifiÃ©s Int4 et Int8, qui permettent d'obtenir des effets de modÃ¨le presque sans perte mais des performances amÃ©liorÃ©es en termes de coÃ»ts de mÃ©moire et de vitesse d'infÃ©rence.\n\nNous dÃ©montrons ici comment utiliser les modÃ¨les quantifiÃ©s que nous fournissons pour l'infÃ©rence. Avant de commencer, assurez-vous que vous rÃ©pondez aux exigences d'auto-gptq (par exemple, torch 2.0 et plus, transformers 4.32.0 et plus, etc.) et installez les paquets requis:\n\n```bash\npip install auto-gptq optimum\n```\n\nSi vous rencontrez des problÃ¨mes pour installer `auto-gptq`, nous vous conseillons de consulter le [repo](https://github.com/PanQiWei/AutoGPTQ) officiel pour trouver une roue.\n\n> Note : Les paquets `auto-gptq` prÃ©compilÃ©s dÃ©pendent fortement de la version de `torch` et de sa version CUDA. De plus, en raison d'une rÃ©cente mise Ã  jour,\n> vous pouvez aussi rencontrer des erreurs de version non supportÃ©e avec `transformers`, `optimum`, ou `peft`.\n> Nous recommandons d'utiliser les derniÃ¨res versions rÃ©pondant aux exigences suivantes :\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0\n\nVous pouvez ensuite charger facilement le modÃ¨le quantifiÃ© et lancer l'infÃ©rence comme d'habitude:\n\n```python\n# Model names: \"Qwen/Qwen-7B-Chat-Int4\", \"Qwen/Qwen-14B-Chat-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\nNous illustrons les performances des modÃ¨les BF16, Int8 et Int4 sur le benchmark, et nous constatons que le modÃ¨le quantifiÃ© ne souffre pas d'une dÃ©gradation significative des performances. Les rÃ©sultats sont prÃ©sentÃ©s ci-dessous:\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n\n### Quantization du cache KV\n\n> NOTE : Veuillez noter qu'en raison du mÃ©canisme interne de Hugging Face, les fichiers de support pour cette fonctionnalitÃ© \n> (i.e., `cache_autogptq_cuda_256.cpp` et `cache_autogptq_cuda_kernel_256.cu`) peuvent Ãªtre manquants. \n> Veuillez les tÃ©lÃ©charger manuellement manuellement depuis le Hugging Face Hub et placez-les dans le mÃªme dossier que les autres fichiers du module.\n\nLe cache KV de l'attention peut Ãªtre quantifiÃ© et compressÃ© pour le stockage, afin d'obtenir un dÃ©bit d'Ã©chantillonnage plus Ã©levÃ©. Les arguments `use_cache_quantization` et `use_cache_kernel` dans `config.json` sont fournis pour activer la quantification du cache KV. \nLa mÃ©thode d'utilisation spÃ©cifique est la suivante:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\nAttention : Actuellement, la quantification du cache KV et flash attention ne peuvent pas Ãªtre utilisÃ©es en mÃªme temps.\nSi vous activez la quantification du cache KV et flash attention en mÃªme temps (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` est dÃ©sactivÃ© par dÃ©faut (`use_flash_attn=false`).\n\nNous avons vÃ©rifiÃ© que l'utilisation du modÃ¨le int8-kvcache quantifiÃ© ne souffre pas d'une dÃ©gradation significative des performances dans l'Ã©valuation en aval. Dans ce qui suit, nous nous concentrons sur le profilage de son empreinte mÃ©moire dans diffÃ©rentes conditions. \nLe profilage s'exÃ©cute sur un seul GPU A100-SXM4-80G avec PyTorch 2.0.1 et CUDA 11.4. \nNous utilisons des modÃ¨les BF16 pour gÃ©nÃ©rer 1024 jetons par dÃ©faut, et \"OOM\" indique une erreur de mÃ©moire insuffisante.\n\nAvec la quantification du cache KV, le modÃ¨le peut infÃ©rer avec une taille de lot (bs) plus grande.\n\n| Utilisation du cache KV |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|--------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| Non          | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |\n| Oui          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\nAvec la quantification du cache KV, le modÃ¨le peut Ã©conomiser plus de mÃ©moire lorsqu'il gÃ©nÃ¨re des sÃ©quences plus longues (`sl`, se rÃ©fÃ©rant au nombre de jetons gÃ©nÃ©rÃ©s) Ã  l'Ã©tape de l'infÃ©rence.\n\n| Utilisation du cache KV | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|-------------------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| Non                     | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| Oui                     | 15.0GB | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\nLe modÃ¨le avec quantification du cache KV convertira le format de `layer_past` de float Ã  int8, et pendant ce temps le `layer-past` quantifiÃ© stockera Ã©galement les paramÃ¨tres de quantification.\n\nLes Ã©tapes spÃ©cifiques sont les suivantes:\n\n1. Quantifier clÃ©/valeur\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n2. Stocker dans `layer_past`\n\nVoici le format de `layer_past` quantifiÃ©:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\n\nLe format original de `layer_past` est illustrÃ© ci-dessous:\n```\n    layer_past=(key,value)\n```\n\nSi vous souhaitez utiliser l'attention KV qui est quantifiÃ©e, vous pouvez utiliser l'opÃ©ration de dÃ©quantification pour reconvertir la clÃ©/valeur int8 au format float comme suit \nvous pouvez utiliser l'opÃ©ration de dÃ©quantification pour reconvertir la clÃ©/valeur int8 au format float comme suit:\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n\n## Performance de l'infÃ©rence\n\nCette section fournit les statistiques de vitesse et de mÃ©moire des modÃ¨les dans diffÃ©rentes prÃ©cisions. Le profilage de la vitesse et de la mÃ©moire est effectuÃ© Ã  l'aide de [ce script] (https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).\n\nNous avons mesurÃ© la vitesse moyenne d'infÃ©rence (tokens/s) et l'utilisation de la mÃ©moire GPU pour gÃ©nÃ©rer 2048 avec les modÃ¨les en BF16, Int8 et Int4.\n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nLe profilage s'exÃ©cute sur un seul GPU A100-SXM4-80G (sauf si 2xA100 est mentionnÃ©) avec PyTorch 2.0.1, CUDA 11.8, et Flash-Attention 2. (72B + vLLM utilise PyTorch 2.1.0 et Cuda 11.8.) La vitesse d'infÃ©rence est calculÃ©e en moyenne sur les tokens encodÃ©s et gÃ©nÃ©rÃ©s.\n\nNote : La vitesse de gÃ©nÃ©ration des modÃ¨les Int4/Int8 mentionnÃ©s ci-dessus est fournie par la bibliothÃ¨que autogptq. La vitesse actuelle du modÃ¨le chargÃ© en utilisant ``AutoModelForCausalLM.from_pretrained`` sera environ 20% plus lente. Nous avons signalÃ© ce problÃ¨me Ã  l'Ã©quipe HuggingFace et nous le mettrons Ã  jour rapidement si une solution est disponible.\n\nNous mesurons Ã©galement la vitesse d'infÃ©rence et l'utilisation de la mÃ©moire du GPU avec diffÃ©rents paramÃ¨tres de contexte et de longueur de gÃ©nÃ©ration, version Flash-Attention. Vous pouvez trouver les rÃ©sultats dans les cartes modÃ¨les correspondantes sur Hugging Face ou ModelScope.\n\n\n## Finetuning\n\n### Utilisation\nNous fournissons maintenant le script d'entraÃ®nement officiel, `finetune.py`, pour que les utilisateurs puissent ajuster le modÃ¨le prÃ©-entraÃ®nÃ© pour les applications en aval de maniÃ¨re simple. De plus, nous fournissons des scripts shell pour lancer le finetune sans soucis. Ce script prend en charge l'entraÃ®nement avec [DeepSpeed](https://github.com/microsoft/DeepSpeed) et [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). Les scripts que nous fournissons utilisent DeepSpeed (Note : il peut y avoir des conflits avec la derniÃ¨re version de pydantic et vous devriez utiliser make sure `pydantic<2.0`) et Peft. Vous pouvez les installer en procÃ©dant comme suit :\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\nPour prÃ©parer vos donnÃ©es d'entraÃ®nement, vous devez rassembler tous les Ã©chantillons dans une liste et l'enregistrer dans un fichier json. Chaque Ã©chantillon est un dictionnaire composÃ© d'un identifiant et d'une liste de conversation. Voici un exemple simple de liste avec 1 Ã©chantillon :\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚\"\n      }\n    ]\n  }\n]\n```\n\nAprÃ¨s la prÃ©paration des donnÃ©es, vous pouvez utiliser les scripts shell fournis pour lancer le finetuning. N'oubliez pas de spÃ©cifier le chemin d'accÃ¨s au fichier de donnÃ©es, `$DATA`.\n\nLes scripts de finetuning vous permettent d'effectuer les opÃ©rations suivantes\n- Finetuning de tous les paramÃ¨tres\n- LoRA\n- Q-LoRA\n\nLe finetuning de tous les paramÃ¨tres nÃ©cessite la mise Ã  jour de tous les paramÃ¨tres au cours de l'ensemble du processus de formation. Pour lancer votre formation, exÃ©cutez le script suivant:\n\n```bash\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\nbash finetune/finetune_ds.sh\n```\n\nN'oubliez pas de spÃ©cifier le nom ou le chemin d'accÃ¨s au modÃ¨le, le chemin d'accÃ¨s aux donnÃ©es, ainsi que le rÃ©pertoire de sortie dans les scripts shell. Une autre chose Ã  noter est que nous utilisons DeepSpeed ZeRO 3 dans ce script. Si vous voulez faire des changements, il suffit de supprimer l'argument `--deepspeed` ou de faire des changements dans le fichier json de configuration de DeepSpeed en fonction de vos besoins. De plus, ce script supporte l'entraÃ®nement en prÃ©cision mixte, et donc vous pouvez utiliser `--bf16 True` ou `--fp16 True`. N'oubliez pas d'utiliser DeepSpeed lorsque vous utilisez fp16 en raison de l'entraÃ®nement de prÃ©cision mixte. Empiriquement, nous vous conseillons d'utiliser bf16 pour rendre votre apprentissage cohÃ©rent avec notre prÃ©-entraÃ®nement et notre alignement si votre machine supporte bf16, et nous l'utilisons donc par dÃ©faut.\n\nPour exÃ©cuter LoRA, utilisez un autre script Ã  exÃ©cuter comme indiquÃ© ci-dessous. Avant de commencer, assurez-vous que vous avez installÃ© `peft`. Vous devez spÃ©cifier les chemins d'accÃ¨s Ã  votre modÃ¨le, Ã  vos donnÃ©es et Ã  vos rÃ©sultats. Nous vous conseillons d'utiliser des chemins absolus pour votre modÃ¨le prÃ©-entraÃ®nÃ©. En effet, LoRA ne sauvegarde que l'adaptateur et le chemin absolu dans le fichier json de configuration de l'adaptateur est utilisÃ© pour trouver le modÃ¨le prÃ©-entraÃ®nÃ© Ã  charger. De plus, ce script supporte Ã  la fois bf16 et fp16.\n\n```bash\n# Single GPU training\nbash finetune/finetune_lora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_lora_ds.sh\n```\n\nPar rapport au finetuning de tous les paramÃ¨tres, LoRA ([paper](https://arxiv.org/abs/2106.09685)) ne met Ã  jour que les paramÃ¨tres des couches d'adaptateurs, tout en gelant les couches originales du grand modÃ¨le de langage. Cela permet de rÃ©duire considÃ©rablement les coÃ»ts de mÃ©moire et donc les coÃ»ts de calcul.\n\nNotez que si vous utilisez LoRA pour affiner le modÃ¨le linguistique de base, par exemple Qwen-7B, au lieu des modÃ¨les de chat, par exemple Qwen-7B-Chat, le script change automatiquement l'intÃ©gration et la couche de sortie en tant que paramÃ¨tres entraÃ®nables. En effet, le modÃ¨le linguistique de base n'a aucune connaissance des jetons spÃ©ciaux apportÃ©s par le format ChatML. Ces couches doivent donc Ãªtre mises Ã  jour pour que le modÃ¨le comprenne et prÃ©dise les jetons. En d'autres termes, si votre formation apporte des tokens spÃ©ciaux dans LoRA, vous devez dÃ©finir les couches comme des paramÃ¨tres entraÃ®nables en dÃ©finissant `modules_to_save` Ã  l'intÃ©rieur du code. De plus, si ces paramÃ¨tres sont entraÃ®nables, il n'est pas possible d'utiliser ZeRO 3, et c'est pourquoi nous utilisons ZeRO 2 par dÃ©faut dans le script. Si vous n'avez pas de nouveaux paramÃ¨tres entraÃ®nables, vous pouvez passer Ã  ZeRO 3 en modifiant le fichier de configuration de DeepSpeed. En outre, nous constatons qu'il existe un Ã©cart important entre l'empreinte mÃ©moire de LoRA avec et sans ces paramÃ¨tres d'entraÃ®nement. Par consÃ©quent, si vous avez des problÃ¨mes de mÃ©moire, nous vous conseillons d'affiner les modÃ¨les de chat de LoRA. Consultez le profil ci-dessous pour plus d'informations.\n\nSi vous souffrez toujours d'un manque de mÃ©moire, vous pouvez envisager Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), qui utilise le modÃ¨le de langage quantifiÃ© et d'autres techniques telles que l'attention paginÃ©e pour rÃ©duire encore les coÃ»ts de mÃ©moire.\n\nNote : pour exÃ©cuter l'entraÃ®nement Q-LoRA sur un seul GPU, vous pouvez avoir besoin d'installer `mpi4py` via `pip` ou `conda`.\n\nPour lancer Q-LoRA, exÃ©cutez directement le script suivant :\n\n```bash\n# Single GPU training\nbash finetune/finetune_qlora_single_gpu.sh\n# Distributed training\nbash finetune/finetune_qlora_ds.sh\n```\n\nPour Q-LoRA, nous vous conseillons de charger le modÃ¨le quantifiÃ© que nous fournissons, par exemple Qwen-7B-Chat-Int4. Vous **NE DEVRIEZ PAS** utiliser les modÃ¨les bf16. Contrairement au finetuning de tous les paramÃ¨tres et Ã  la LoRA, seul le modÃ¨le fp16 est pris en charge pour la Q-LoRA. Pour l'entraÃ®nement sur un seul GPU, nous devons utiliser DeepSpeed pour l'entraÃ®nement en prÃ©cision mixte en raison de notre observation des erreurs causÃ©es par torch amp. En outre, pour Q-LoRA, les problÃ¨mes avec les jetons spÃ©ciaux dans LoRA existent toujours. Cependant, comme nous ne fournissons que les modÃ¨les Int4 pour les modÃ¨les de chat, ce qui signifie que le modÃ¨le de langage a appris les tokens spÃ©ciaux du format ChatML, vous n'avez pas Ã  vous soucier des couches. Notez que les couches du modÃ¨le Int4 ne doivent pas Ãªtre entraÃ®nables, et donc si vous introduisez des tokens spÃ©ciaux dans votre entraÃ®nement, Q-LoRA risque de ne pas fonctionner.\n\n> NOTE : Veuillez noter qu'en raison des mÃ©canismes internes de Hugging Face, certains fichiers non-Python (par exemple, `*.cpp` et `*.cu`) \n> peuvent Ãªtre absents du point de contrÃ´le sauvegardÃ©. Vous devrez peut-Ãªtre les copier manuellement dans le rÃ©pertoire contenant les autres fichiers.\n\nContrairement au finetuning des paramÃ¨tres complets, l'entraÃ®nement de LoRA et de Q-LoRA n'enregistre que les paramÃ¨tres de l'adaptateur. Supposons que votre entraÃ®nement commence Ã  partir de Qwen-7B, vous pouvez charger le modÃ¨le finalisÃ© pour l'infÃ©rence comme indiquÃ© ci-dessous:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nSi vous souhaitez fusionner les adaptateurs et enregistrer le modÃ¨le affinÃ© en tant que modÃ¨le autonome (vous ne pouvez le faire qu'avec LoRA, et vous **NE POUVEZ PAS** fusionner les paramÃ¨tres de Q-LoRA), vous pouvez exÃ©cuter les codes suivants :\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\nNote : Pour l'entraÃ®nement multi-GPU, vous devez spÃ©cifier les hyperparamÃ¨tres appropriÃ©s pour l'entraÃ®nement distribuÃ© en fonction de votre machine. De plus, nous vous conseillons de spÃ©cifier votre longueur maximale de sÃ©quence avec l'argument `--model_max_length`, en fonction de votre considÃ©ration des donnÃ©es, de l'empreinte mÃ©moire, et de la vitesse d'apprentissage.\n\n\n### Profilage de la mÃ©moire et de la vitesse\nNous profilons la mÃ©moire du GPU et la vitesse d'apprentissage de LoRA (LoRA (emb) se rÃ©fÃ¨re Ã  l'apprentissage de la couche d'intÃ©gration et de sortie, tandis que LoRA n'a pas de couche d'intÃ©gration et de sortie pouvant Ãªtre entraÃ®nÃ©e) et de Q-LoRA dans la configuration de l'apprentissage sur un seul GPU. Dans ce test, nous expÃ©rimentons sur un seul GPU A100-SXM4-80G, et nous utilisons CUDA 11.8 et Pytorch 2.0. Flash attention 2 est appliquÃ©. Nous utilisons uniformÃ©ment une taille de lot de 1 et une accumulation de gradient de 8. Nous profilons la mÃ©moire (GB) et la vitesse (s/iter) des entrÃ©es de diffÃ©rentes longueurs, Ã  savoir 256, 512, 1024, 2048, 4096, et 8192. Nous prÃ©sentons Ã©galement les statistiques du rÃ©glage fin de tous les paramÃ¨tres avec Qwen-7B sur 2 GPU A100. Nous ne prÃ©sentons que les statistiques de 256, 512 et 1024 jetons en raison de la limitation de la mÃ©moire du GPU. \n\nPour Qwen-72B, nous expÃ©rimentons de deux maniÃ¨res : 1) Lora fintuning + DeepSpeed ZeRO 3 sur 4 GPU A100-SXM4-80G et 2) QLora (int4) fintuning sur un seul GPU A100-SXM4-80G. Notez que l'OOM se produit sur 4 GPUs A100-SXM4-80G Ã  la fois avec le rÃ©glage fin LoRA (emb) et le rÃ©glage fin LoRA sans Deepspeed ZeRO 3 (vous pouvez passer `--deepspeed finetune/ds_config_zero3.json` Ã  [`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh) afin d'activer DeepSpeed ZeRO 3).\n\nLes statistiques sont listÃ©es ci-dessous :\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    </tr>\n    </tr>\n\t\t<tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td><td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">7B</th><td>LoRA</td><td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th><td>LoRA</td><td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n\t<tr>\n        <th rowspan=\"2\">72B</th><td>LoRA + Deepspeed Zero3</td><td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n<br>\n\n## DÃ©ploiement\n\n### vLLM \nPour le dÃ©ploiement et l'infÃ©rence rapide, nous suggÃ©rons d'utiliser vLLM avec FastChat. Installez d'abord les paquets:\n```bash\npip install vllm\npip install \"fschat[model_worker,webui]\"\n```\nOu vous pouvez les installer Ã  partir des sources par `git clone` et `pip install -e .`. Nous vous conseillons de lire leurs documents si vous rencontrez des problÃ¨mes lors de l'installation.\n\nPour faire fonctionner Qwen avec vLLM et FastChat, vous devez d'abord lancer un contrÃ´leur par:\n```bash\npython -m fastchat.serve.controller\n```\n\nEnsuite, vous pouvez lancer le travailleur de modÃ¨le, ce qui signifie charger votre modÃ¨le pour l'infÃ©rence. Pour l'infÃ©rence sur un seul GPU, vous pouvez directement lancer:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code\n```\nCependant, si vous souhaitez exÃ©cuter le modÃ¨le sur plusieurs GPU pour une infÃ©rence plus rapide ou une mÃ©moire plus importante, vous pouvez utiliser le parallÃ©lisme tensoriel pris en charge par vLLM. Supposons que vous exÃ©cutiez le modÃ¨le sur 4 GPU, la commande est prÃ©sentÃ©e ci-dessous:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4\n```\n\nAprÃ¨s avoir lancÃ© votre model worker, vous pouvez lancer :\n\n* DÃ©monstration de l'interface web\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* API OpenAI\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\nCependant, si vous avez des difficultÃ©s Ã  utiliser vLLM et FastChat, vous pouvez essayer nos mÃ©thodes les plus simples pour dÃ©ployer une dÃ©mo web, une dÃ©mo CLI et une API.\n\n### Interface Web\n\nNous fournissons du code pour que les utilisateurs puissent construire une dÃ©mo d'interface web (merci Ã  @wysaid). Avant de commencer, assurez-vous d'installer les paquets suivants:\n\n```\npip install -r requirements_web_demo.txt\n```\n\nExÃ©cutez ensuite la commande ci-dessous et cliquez sur le lien gÃ©nÃ©rÃ©:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n### DÃ©mo CLI\n\nNous fournissons un exemple de dÃ©monstration CLI dans `cli_demo.py`, qui prend en charge la sortie en continu pour la gÃ©nÃ©ration. Les utilisateurs peuvent interagir avec Qwen-7B-Chat en saisissant des invites, et le modÃ¨le renvoie les sorties du modÃ¨le en mode streaming. ExÃ©cutez la commande ci-dessous:\n\n```bash\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\nNous fournissons des mÃ©thodes pour dÃ©ployer une API locale basÃ©e sur l'API OpenAI (merci Ã  @hanpenggit). Avant de commencer, installez les paquets nÃ©cessaires:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nExÃ©cutez ensuite la commande pour dÃ©ployer votre API:\n\n```bash\npython openai_api.py\n```\n\nVous pouvez modifier vos arguments, par exemple, `-c` pour le nom ou le chemin du poids, `--cpu-only` pour le dÃ©ploiement CPU, etc. Si vous rencontrez des problÃ¨mes lors du lancement du dÃ©ploiement de l'API, la mise Ã  jour des paquets vers la derniÃ¨re version peut probablement les rÃ©soudre.\n\nL'utilisation de l'API est simple. Voir l'exemple ci-dessous:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# create a request activating streaming response\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=True \n    # Specifying stop words in streaming output format is not yet supported and is under development.\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# create a request not activating streaming response\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=False,\n    stop=[] # You can add custom stop words here, e.g., stop=[\"Observation:\"] for ReAct prompting.\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n**Function calling** est aussi supportÃ© (mais seulement quand `stream=False` pour le moment). Voir [l'exemple d'utilisation](examples/function_call_examples.py) ici.\n<br><br>\n\n\n## ğŸ³ Docker\n\nPour simplifier le processus de dÃ©ploiement, nous fournissons des images docker avec des environnements prÃ©construits : [qwenllm/qwen] (https://hub.docker.com/r/qwenllm/qwen). Il vous suffit d'installer le pilote et de tÃ©lÃ©charger les fichiers de modÃ¨le pour lancer les dÃ©monstrations, dÃ©ployer l'API OpenAI et affiner le modÃ¨le.\n\n### PrÃ©paration\n\n1. Installez la version correcte du pilote Nvidia en fonction de l'image Ã  utiliser :\n  - `qwenllm/qwen:cu117` (**recommandÃ©**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: mÃªme que `qwenllm/qwen:cu117`\n\n2. Installer et configurer [docker](https://docs.docker.com/engine/install/) et [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) :\n\n```bash\n# configure docker\nsudo systemctl start docker\n# test if docker is correctly installed\nsudo docker run hello-world\n\n# configure nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. TÃ©lÃ©chargez les checkpoints et les codes du modÃ¨le dans votre environnement (voir [ici](#DownloadModel)).\n\n### DÃ©ploiement\n\nNous utilisons ici Qwen-7B-Chat comme exemple. Avant de lancer une dÃ©mo web ou une API, vous pouvez Ã©tablir la configuration comme indiquÃ© ci-dessous :\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes\n```\nLes scripts suivants peuvent vous aider Ã  construire :\n\n* API OpenAI\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Interface Web\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* DÃ©mo CLI\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\nLes commandes ci-dessus tÃ©lÃ©chargeront automatiquement l'image requise et lanceront une dÃ©mo d'interface Web en arriÃ¨re-plan (le service redÃ©marrera automatiquement). Vous pouvez ouvrir `http://localhost:${PORT}` sur l'hÃ´te pour utiliser la dÃ©mo.\n\nLa dÃ©mo est lancÃ©e avec succÃ¨s si vous obtenez le rÃ©sultat suivant :\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nSi vous voulez vÃ©rifier le statut de la dÃ©mo, vous pouvez utiliser `docker logs qwen` pour afficher les rÃ©sultats.\n\nVous pouvez utiliser `docker rm -f qwen` pour arrÃªter le service et supprimer le conteneur.\n\n\n### Finetuning\n\nLa mÃ©thode de finetuning utilisant l'image Docker prÃ©construite est fondamentalement la mÃªme que [le chapitre ci-dessus](#Finetuning) (nous avons dÃ©jÃ  installÃ© les dÃ©pendances dans l'image) :\n\nVoici un exemple de LoRA Ã  une seule GPU :\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs\n\n# Use all host devices by default\nDEVICE=all\n# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# Single-GPU LoRA finetuning\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\nPour faire un changement vers Q-LoRA Ã  GPU unique par exemple, il suffit de modifier la commande bash Ã  l'intÃ©rieur de `docker run` :\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## ğŸ”¥ Invite du systÃ¨me\nQwen-1.8-Chat et Qwen-72B-Chat ont Ã©tÃ© entiÃ¨rement formÃ©s Ã  diverses invites de systÃ¨me avec plusieurs sÃ©ries d'interactions complexes, de sorte qu'ils peuvent suivre une variÃ©tÃ© d'invites de systÃ¨me et rÃ©aliser la personnalisation du modÃ¨le dans le contexte, amÃ©liorant ainsi l'Ã©volutivitÃ© de Qwen-chat.\n\nGrÃ¢ce aux messages-guides du systÃ¨me, Qwen-Chat peut **jouer avec enthousiasme**, **transfÃ©rer le style de langage**, **fixer des tÃ¢ches** et **fixer des comportements**.\n\n![](assets/system_prompt_language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\nPour plus d'informations, veuillez vous rÃ©fÃ©rer Ã  la [documentation d'exemple](examples/system_prompt.md).\n\n\n## Utilisation des outils\n\nQwen-Chat a Ã©tÃ© optimisÃ© pour l'utilisation d'outils et les capacitÃ©s d'appel de fonctions. Les utilisateurs peuvent dÃ©velopper des agents, des applications LangChain, et mÃªme augmenter Qwen avec un Code Interpreter.\n\nNous fournissons une documentation sur la maniÃ¨re d'implÃ©menter les appels d'outils basÃ©s sur le principe de ReAct Prompting, veuillez vous rÃ©fÃ©rer Ã  [l'exemple ReAct](examples/react_prompt.md). Sur la base de ce principe, nous fournissons un support pour function calling dans [openai_api.py](openai_api.py).\n\nNous avons testÃ© les capacitÃ©s d'appel d'outil du modÃ¨le sur notre benchmark d'Ã©valuation chinois Ã  source ouverte et nous avons constatÃ© que Qwen-Chat obtient systÃ©matiquement de bons rÃ©sultats:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.â†‘)</th><th align=\"center\">Tool Input (Rouge-Lâ†‘)</th><th align=\"center\">False Positive Errorâ†“</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\nPour Ã©valuer la capacitÃ© de Qwen Ã  utiliser l'interprÃ©teur de code Python pour des tÃ¢ches telles que la rÃ©solution de problÃ¨mes mathÃ©matiques, la visualisation de donnÃ©es et d'autres tÃ¢ches gÃ©nÃ©rales telles que la manipulation de fichiers et l'exploration du Web, nous avons crÃ©Ã© et mis en libre accÃ¨s un test de rÃ©fÃ©rence spÃ©cialement conÃ§u pour Ã©valuer ces capacitÃ©s. Vous pouvez trouver le benchmark sur ce [lien](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nNous avons observÃ© que Qwen est performant en termes d'exÃ©cutabilitÃ© du code et de prÃ©cision des rÃ©sultats lors de la gÃ©nÃ©ration du code:\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Mathâ†‘</th><th align=\"center\">Visualization-Hardâ†‘</th><th align=\"center\">Visualization-Easyâ†‘</th><th align=\"center\">Generalâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## ComprÃ©hension du Contexte Long\n\nPour augmenter la longueur du contexte et Ã©liminer le goulot d'Ã©tranglement que constitue la longueur de la sÃ©quence d'entraÃ®nement, nous introduisons plusieurs techniques, notamment l'interpolation tenant compte des NTK, l'attention par fenÃªtre et la mise Ã  l'Ã©chelle de l'attention LogN, afin d'augmenter la longueur du contexte de Qwen-14B de 2K Ã  plus de 8K tokens, et de Qwen-1.8B/7B de 8K Ã  32K tokens. \n\nPour Qwen-72B, nous adaptons RoPE Ã  des contextes plus longs avec une base rotative plus importante. Qwen-72B prend en charge la longueur de contexte maximale de 32K tokens.\n\nNous menons des expÃ©riences de modÃ©lisation du langage sur l'ensemble de donnÃ©es arXiv avec l'Ã©valuation PPL et nous constatons que Qwen peut atteindre des performances exceptionnelles dans le scÃ©nario d'un contexte long. Les rÃ©sultats sont prÃ©sentÃ©s ci-dessous :\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.56</td><td align=\"center\">4.62</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.49</td><td align=\"center\">4.32</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n    <tr>\n        <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n    </tr>\n</table>\n\nEn outre, pour vÃ©rifier la capacitÃ© de Qwen-72B-Chat Ã  comprendre des textes longs, nous l'avons testÃ© sur [L-Eval] (https://arxiv.org/abs/2307.11088) (tÃ¢ches fermÃ©es). Les rÃ©sultats sont les suivants :\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\nNous avons rÃ©alisÃ© l'expÃ©rience de \"l'aiguille dans une botte de foin\" (l'idÃ©e vient de [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)) pour tester si le modÃ¨le peut rÃ©cupÃ©rer des informations Ã  diffÃ©rentes positions dans les entrÃ©es de diffÃ©rentes longueurs, le rÃ©sultat est le suivant :\n\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\nLes rÃ©sultats ci-dessus montrent que Qwen-72B-Chat peut rÃ©cupÃ©rer avec prÃ©cision des informations placÃ©es dans diffÃ©rentes positions dans une longueur d'entrÃ©e de 32K, ce qui prouve ses excellentes capacitÃ©s de comprÃ©hension de textes longs.\n\n\n## Tokenizer\n\nNotre tokenizer basÃ© sur tiktoken est diffÃ©rent des autres tokenizers, par exemple le tokenizer sentencepiece. Vous devez faire attention aux tokens spÃ©ciaux, en particulier lors de la mise au point. Pour des informations plus dÃ©taillÃ©es sur le tokenizer et son utilisation dans le cadre du finetuning, veuillez vous rÃ©fÃ©rer Ã  la [documentation](tokenization_note.md).\n<br><br>\n\n## Reproduction\n\nPour reproduire les performances du modÃ¨le sur des ensembles de donnÃ©es de rÃ©fÃ©rence, nous fournissons des scripts permettant de reproduire les rÃ©sultats. Consultez [eval/EVALUATION.md](eval/EVALUATION.md) pour plus d'informations. Notez que la reproduction peut entraÃ®ner de lÃ©gÃ¨res diffÃ©rences par rapport Ã  nos rÃ©sultats.\n<br><br>\n\n## FAQ\n\nSi vous rencontrez des problÃ¨mes, veuillez vous rÃ©fÃ©rer Ã  la [FAQ](FAQ.md) et aux problÃ¨mes pour trouver une solution avant de lancer un nouveau problÃ¨me.\n<br><br>\n\n## Citation\nSi vous trouvez notre travail utile, n'hÃ©sitez pas Ã  nous citer.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## Accord de Licence\n\nLe code source fourni Ã  l'adresse <https://github.com/QwenLM/Qwen> est soumis Ã  la licence [Apache 2.0 License](./LICENSE) qui se trouve dans le rÃ©pertoire racine.\n\nLes chercheurs et les dÃ©veloppeurs sont libres d'utiliser les codes et les poids des modÃ¨les de Qwen et de Qwen-Chat. Pour leur utilisation commerciale, veuillez consulter l'accord de licence accompagnant chaque modÃ¨le.\n\n- Qwen-72B, Qwen-14B et Qwen-7B sont sous licence [Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT) que l'on peut trouver dans les dÃ©pÃ´ts HuggingFace et ModelScope correspondants. Pour une utilisation commerciale, veuillez remplir le formulaire ([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), et [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen)) pour en faire la demande.\n\n- Qwen-1.8B est sous licence [Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT) qui peut Ãªtre trouvÃ© dans les dÃ©pÃ´ts HuggingFace et ModelScope correspondants. Pour une utilisation commerciale, veuillez nous contacter.\n<br><br>\n\n## Contactez-nous\n\nSi vous souhaitez laisser un message Ã  notre Ã©quipe de recherche ou Ã  notre Ã©quipe produit, rejoignez nos groupes Discord ou WeChat! N'hÃ©sitez pas non plus Ã  envoyer un courriel Ã  qianwen_opensource@alibabacloud.com.\n\n"
        },
        {
          "name": "README_JA.md",
          "type": "blob",
          "size": 77.9443359375,
          "content": "<p align=\"left\">\n    <a href=\"README_CN.md\">ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href=\"README.md\">English</a>&nbsp ï½œ &nbspæ—¥æœ¬èª ï½œ &nbsp<a href=\"README_FR.md\">FranÃ§ais</a> ï½œ &nbsp<a href=\"README_ES.md\">EspaÃ±ol</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        ğŸ¤— <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br><br>\n\n> [!Important]\n> Qwen2 ãŒç™»å ´ã—ã¾ã—ãŸï¼[QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) ã®ãƒ•ã‚©ãƒ­ãƒ¼ã¨ã€ãã“ã§ã‚ãªãŸã®ä½“é¨“ã‚’å…±æœ‰ã™ã‚‹ã“ã¨ã‚’ãŠå¾…ã¡ã—ã¦ãŠã‚Šã¾ã™ã€‚\n>\n> ã“ã®ãƒªãƒã‚¸ãƒˆãƒª ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) ã¯ã€ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã«å¤§å¹…ãªé•ã„ãŒã‚ã‚‹ãŸã‚ã€ä»Šå¾Œã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã•ã‚Œãªããªã‚Šã¾ã™ã€‚\n<br>\n\n|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |\n|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|\n| 1.8B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-1_8B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-1_8B\">ğŸ¤—</a>  |\n| 7B  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int4\">ğŸ¤—</a>  | <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat-Int8\">ğŸ¤—</a>  |  <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-7B\">ğŸ¤—</a>  |\n| 14B | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-14B\">ğŸ¤—</a> |\n| 72B | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int4\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B-Chat-Int8\">ğŸ¤—</a> | <a href=\"https://modelscope.cn/models/qwen/Qwen-72B/summary\">ğŸ¤–</a>  <a href=\"https://huggingface.co/Qwen/Qwen-72B\">ğŸ¤—</a> |\n\n\n\n**Qwen-1.8B**ã€**Qwen-7B**ã€**Qwen-14B**ã€**Qwen-72B**ã®åŸºæœ¬è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹**Qwen**ã¨ã€**Qwen-1.8B-Chat**ã€**Qwen-7B-Chat**ã€**Qwen-14B-Chat**ã€**Qwen-72B-Chat**ã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹**Qwen-Chat**ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¾ã™ã€‚ä¸Šã®è¡¨ã«ãƒªãƒ³ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ã¾ãŸã€**[ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ](https://arxiv.org/abs/2309.16609)**ã‚‚å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚è«–æ–‡ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã”è¦§ãã ã•ã„ï¼\n\nç°¡å˜ã«èª¬æ˜ã™ã‚‹ã¨ã€ç§ãŸã¡ã¯ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚„è¨€èªï¼ˆä¸­å›½èªã¨è‹±èªã‚’ä¸­å¿ƒã«ï¼‰ãªã©ã‚’å¹…åºƒãã‚«ãƒãƒ¼ã™ã‚‹æœ€å¤§3å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®‰å®šçš„ã«äº‹å‰å­¦ç¿’ã•ã‚ŒãŸå¼·åŠ›ãªãƒ™ãƒ¼ã‚¹è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€SFTã¨RLHFã«åŸºã¥ãäººé–“ã®å—œå¥½ã«æ²¿ã£ãŸãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆã¾ã ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰ãŒã‚ã‚Šã€ãƒãƒ£ãƒƒãƒˆã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã€æƒ…å ±æŠ½å‡ºã€è¦ç´„ã€ç¿»è¨³ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ•°å­¦ã®å•é¡Œã‚’è§£ããªã©ãŒå¯èƒ½ã§ã€ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ãŸã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã—ã¦éŠã‚“ã ã‚Šã€ã‚³ãƒ¼ãƒ‰ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã¨ã—ã¦éŠã‚“ã ã‚Šã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n\n\n| ãƒ¢ãƒ‡ãƒ«       |   ç™ºè¡Œæ—¥    | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§é•· | ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼·åŒ– | é¢„è®­ç»ƒã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®æ•° | Finetuningï¼ˆQ-Loraï¼‰ã®æœ€å°GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 2048ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆæ™‚ã®æœ€å°GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆInt4ï¼‰ | ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨èƒ½åŠ› |\n|:----------|:--------:|:----------:|:------------:|:------------:|:------------------------------:|:-----------------------------:|:--------:|\n| Qwen-1.8B | 23.11.30 |    32K     |      âœ…       |     2.2T     |             5.8GB              |             2.9GB             |    âœ…     |  \n| Qwen-7B   | 23.08.03 |    32K     |      â       |     2.4T     |             11.5GB             |             8.2GB             |    âœ…     |   \n| Qwen-14B  | 23.09.25 |     8K     |      â       |     3.0T     |             18.7GB             |            13.0GB             |    âœ…     |\n| Qwen-72B  | 23.11.30 |    32K     |      âœ…       |     3.0T     |             61.4GB             |            48.9GB             |    âœ…     |   \n\n\nã“ã®ãƒ¬ãƒã§ã¯ã€ãã‚Œã‚’æŠŠæ¡ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š\n\n* Qwenã®ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã€‚\n* é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ï¼ˆä½¿ç”¨é‡ã€ãƒ¡ãƒ¢ãƒªã€æ¨è«–é€Ÿåº¦ãªã©ï¼‰ã€‚æ¯”è¼ƒã®ãŸã‚ã«ã€BF16ãƒ¢ãƒ‡ãƒ«ã®çµ±è¨ˆã‚‚æä¾›ã—ã¾ã™ã€‚\n* ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€LoRAã€Q-LoRAã‚’å«ã‚€ã€å¾®èª¿æ•´ã«é–¢ã™ã‚‹ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€‚\n* vLLMã¨FastChatã‚’ä¾‹ã«ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚\n* WebUIã€CLIãƒ‡ãƒ¢ãªã©ã€ãƒ‡ãƒ¢ã®æ§‹ç¯‰ã«é–¢ã™ã‚‹èª¬æ˜ã€‚\n* ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®OpenAIã‚¹ã‚¿ã‚¤ãƒ«ã®APIã‚’æ§‹ç¯‰ã™ã‚‹æ‰‹é †ã€‚\n* ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã‚³ãƒ¼ãƒ‰ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã® Qwen ã®è©³ç´°ã€‚\n* ãƒ­ãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆç†è§£è©•ä¾¡ã®çµ±è¨ˆ\n* ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å¥‘ç´„\n* ...\n\nã¾ãŸã€å›°ã£ãŸã“ã¨ãŒã‚ã‚Œã°ã€ã¾ãšã¯[FAQ](FAQ.md)ã‚’å‚ç…§ã—ã¦ã»ã—ã„ã€‚ã¾ã æ‚©ã‚“ã§ã„ã¾ã™ã‹ï¼Ÿé æ…®ãªãç§ãŸã¡ã«å•é¡Œã‚’é€ã£ã¦ãã ã•ã„ï¼ˆã‚ˆã‚Šå¤šãã®äººãŒç†è§£ã§ãã‚‹ã‚ˆã†ã«ã€è‹±èªã§ï¼‰ï¼ç§ãŸã¡ã‚’åŠ©ã‘ãŸã„ãªã‚‰ã€é æ…®ãªããƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ã£ã¦ãã ã•ã„ï¼\n\nç§ãŸã¡ã¨ãƒãƒ£ãƒƒãƒˆã—ãŸã„ã§ã™ã‹ï¼Ÿç§ãŸã¡ã®Discordã¾ãŸã¯WeChatã‚°ãƒ«ãƒ¼ãƒ—ã¸ã‚ˆã†ã“ãï¼\n<br><br>\n\n## ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ\n\n* 2023.11.30 ğŸ”¥ 3T ãƒˆãƒ¼ã‚¯ãƒ³ã§å­¦ç¿’ã—ã€32k ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ **Qwen-72B** ã¨ **Qwen-72B-Chat** ã‚’ã€ **Qwen-1.8B** ã¨ **Qwen-1.8B-Chat** ã¨ã¨ã‚‚ã«ã€ModelScope ã¨ Hugging Face ä¸Šã§ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚ã¾ãŸã€Qwen-72B-Chatã¨Qwen-1.8B-Chatã®ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ©Ÿèƒ½ã‚’å¼·åŒ–ã—ã¾ã—ãŸã€‚[ã‚µãƒ³ãƒ—ãƒ«ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](examples/system_prompt.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€**Ascend 910** ã¨ **Hygon DCU** ã§ã®æ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚è©³ç´°ã¯ `ascend-support` ã¨ `dcu-support` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n* 2023.10.17 Int8é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«**Qwen-7B-Chat-Int8**ã¨**Qwen-14B-Chat-Int8**ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚\n* 2023.9.25 ğŸ”¥ Qwen-14Bã¨Qwen-14B-Chatã‚’ModelScopeã¨Hugging Faceã§ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚[qwen.cpp](https://github.com/QwenLM/qwen.cpp) ã¨ [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) ã‚‚ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸã€‚åŒæ™‚ã«ã€Qwen-7B ã¨ Qwen-7B-Chat ã‚‚æ›´æ–°ã—ã¾ã—ãŸã€‚Qwen-7Bï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ï¼‰ã¨æ¯”è¼ƒã—ã¦ã€Qwen-7Bã¯ã‚ˆã‚Šå¤šãã®å­¦ç¿’ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã€2.2Tãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰2.4Tãƒˆãƒ¼ã‚¯ãƒ³ã«å¢—åŠ ã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã¯2048ã‹ã‚‰8192ã«æ‹¡å¼µã•ã‚ŒãŸã€‚Qwen-7Bã®ä¸­å›½èªçŸ¥è­˜ã¨ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°èƒ½åŠ›ã¯ã•ã‚‰ã«å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚æœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãŠä½¿ã„ãã ã•ã„ï¼\n* 2023.9.12 Qwen-7Bãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€LoRAã€Q-LoRAã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚\n* 2023.8.21 Qwen-7B-Chat ç”¨ Int4 é‡å­åŒ–ãƒ¢ãƒ‡ãƒ« **Qwen-7B-Chat-Int4** ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚ã¾ãŸã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã«ãŠã„ã¦ã‚‚å¤§ããªæ€§èƒ½ä½ä¸‹ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n* 2023.8.3 ModelScope ã¨ Hugging Face ä¸Šã§ **Qwen-7B** ã¨ **Qwen-7B-Chat** ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚ã¾ãŸã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©³ç´°ã‚„ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãªã©ã€ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯æŠ€è¡“ãƒ¡ãƒ¢ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\n<br>\n\n## æ€§èƒ½\n\nQwenãƒ¢ãƒ‡ãƒ«ã¯ã€MMLUã€C-Evalã€GSM8Kã€MATHã€HumanEvalã€MBPPã€BBHãªã©ã€è‡ªç„¶è¨€èªç†è§£ã€æ•°å­¦çš„å•é¡Œè§£æ±ºã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã©ã«é–¢ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ä¸€é€£ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€åŒæ§˜ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’æŒã¤ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã€‚Qwen-72Bã¯å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ã§LLaMA2-70Bã‚’ä¸Šå›ã‚Šã€10ã‚¿ã‚¹ã‚¯ä¸­7ã‚¿ã‚¹ã‚¯ã§GPT-3.5ã‚’ä¸Šå›ã£ãŸã€‚\n\n\n<p align=\"left\">\n    <img src=\"assets/radar_72b.jpg\" width=600px/>\n<p>\n<br>\n\n| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |\n|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|\n|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |\n| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |\n| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |\n| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |\n| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |\n| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |\n| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |\n| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |\n| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |\n| Yi-34B      \t  \t  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |\n| XVERSE-65B      \t |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |\n| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |\n| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |\n| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |\n| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |\n\n\næ¯”è¼ƒã•ã‚ŒãŸã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã€å…¬å¼ã«å ±å‘Šã•ã‚ŒãŸçµæœã¨[OpenCompass](https://opencompass.org.cn/leaderboard-llm) ã®é–“ã®æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’å ±å‘Šã—ã¾ã™ã€‚\n\nã‚ˆã‚Šè©³ç´°ãªå®Ÿé¨“çµæœï¼ˆã‚ˆã‚Šå¤šãã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è©³ç´°ãªãƒ¢ãƒ‡ãƒ«æ€§èƒ½ï¼‰ã‚„è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰](TODO)ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æŠ€è¡“ãƒ¡ãƒ¢ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n<br><br>\n\n## å¿…è¦æ¡ä»¶\n\n* python 3.8 ä»¥ä¸Š\n* pytorch 1.12 ä»¥ä¸Šã€2.0 ä»¥ä¸Šã‚’æ¨å¥¨\n* transformers 4.32 ä»¥ä¸Š\n* CUDA 11.4 ä»¥ä¸Šã‚’æ¨å¥¨ï¼ˆGPU ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ãªã©ï¼‰\n<br>\n\n## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ\n\nä»¥ä¸‹ã§ã¯ã€Qwen-Chat ã¨ ğŸ¤– ModelScope ã¨ ğŸ¤— Transformers ã®ç°¡å˜ãªä½¿ç”¨ä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚\n\nè©³ã—ãã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³[\"ãƒ“ãƒ«ãƒ‰æ¸ˆã¿Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ä½¿ç”¨\"](#-docker)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\nDockerã‚’ä½¿ç”¨ã—ãªã„å ´åˆã¯ã€ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒæ¸ˆã‚“ã§ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ä¸Šè¨˜ã®è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã€ä¾å­˜ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚\n\n```bash\npip install -r requirements.txt\n```\n\nãŠä½¿ã„ã®ãƒ‡ãƒã‚¤ã‚¹ãŒ fp16 ã¾ãŸã¯ bf16 ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã€[flash-attention](https://github.com/Dao-AILab/flash-attention) ï¼ˆflash attention 2ã«å¯¾å¿œã—ã¾ã—ãŸï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé«˜ã„åŠ¹ç‡ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚(**flash-attention ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãªãã¦ã‚‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æ­£å¸¸ã«å®Ÿè¡Œã§ãã¾ã™**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# ä»¥ä¸‹ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\n# pip install csrc/layer_norm\n# flash-attn ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒ 2.1.1 ä»¥é™ã®å ´åˆã€ä»¥ä¸‹ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ã€‚\n# pip install csrc/rotary\n```\n\nã“ã‚Œã§ ModelScope ã‹ Transformers ã§å§‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\n### ğŸ¤— Transformers\n\nQwen-Chat ã‚’æ¨è«–ã«ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã ã‘ã§ã™ã€‚Qwen/Qwen-7B-Chat \"ã‚„ \"Qwen/Qwen-14B-Chat \"ã®ã‚ˆã†ã«ã€æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«åã‚„ãƒ‘ã‚¹ã‚’æ¸¡ã™ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚**æœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model namesï¼š\"Qwen/Qwen-7B-Chat\"ã€\"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# bf16 ã‚’ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 ã‚’ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# CPU ã®ã¿ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# ã‚ªãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ‡ãƒã‚¤ã‚¹ã«å¿œã˜ã¦è‡ªå‹•çš„ã«ç²¾åº¦ãŒé¸æŠã•ã‚Œã¾ã™ã€‚\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# ç”Ÿæˆã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šã€‚ãŸã ã—ã€4.32.0 ä»¥ä¸Šã®ãƒˆTransformerã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€ã“ã‚Œã‚’è¡Œã†å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# ç¬¬ä¸€å›å¯¾è©±ã‚¿ãƒ¼ãƒ³\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚\n\n# ç¬¬äºŒå›å¯¾è©±ã‚¿ãƒ¼ãƒ³\nresponse, history = model.chat(tokenizer, \"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\n# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\n# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚\n# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚\n# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚\n# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚\n# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚\n\n# ç¬¬ä¸‰è½®å¯¹è¯ ç¬¬ä¸‰å›å¯¾è©±ã‚¿ãƒ¼ãƒ³\nresponse, history = model.chat(tokenizer, \"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹\n```\n\nQwen ã®å­¦ç¿’æ¸ˆã¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã‚‚ç°¡å˜ã§ã™ã€‚\n\n<details>\n  <summary>Qwen ã®å®Ÿè¡Œ</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Model namesï¼š\"Qwen/Qwen-7B\"ã€\"Qwen/Qwen-14B\"\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n# bf16 ã‚’ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 ã‚’ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# CPU ã®ã¿ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# ã‚ªãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ‡ãƒã‚¤ã‚¹ã«å¿œã˜ã¦è‡ªå‹•çš„ã«ç²¾åº¦ãŒé¸æŠã•ã‚Œã¾ã™ã€‚\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# ç”Ÿæˆã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šã€‚ãŸã ã—ã€4.32.0 ä»¥ä¸Šã®ãƒˆTransformerã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€ã“ã‚Œã‚’è¡Œã†å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\ninputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...\n```\n\n</details>\n\n<p id=\"DownloadModel\">\nHuggingFaceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã‚³ãƒ¼ãƒ‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã€ModelScopeã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã¯ã“ã¡ã‚‰ã§ã”ã–ã„ã¾ã™ã€‚\n</p>\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-7B')\n# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n# model_dir = snapshot_download('qwen/Qwen-14B')\nmodel_dir = snapshot_download('qwen/Qwen-14B-Chat')\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n### ğŸ¤– ModelScope\n\nModelScope ã¯ã€MaaSï¼ˆModel-as-a-Serviceï¼‰ ã®ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚ã‚Šã€AI é–‹ç™ºè€…ã«æŸ”è»Ÿã§è²»ç”¨å¯¾åŠ¹æœã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚åŒæ§˜ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ã« ModelScope ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nfrom modelscope import GenerationConfig\n\n# Model namesï¼š\"Qwen/Qwen-7B-Chat\"ã€\"Qwen/Qwen-14B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\n\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\nresponse, history = model.chat(tokenizer, \"æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ\", history=history) \nprint(response)\nresponse, history = model.chat(tokenizer, \"å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹\", history=history)\nprint(response)\n```\n\n### ãƒãƒƒãƒæ¨è«–\nQwenã¯ãƒãƒƒãƒæ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹ã«ã—ãŸå ´åˆã€ãƒãƒƒãƒæ¨è«–ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§40%ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ãŒæœŸå¾…ã§ãã‚‹ã€‚ä»¥ä¸‹ã«ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ç¤ºã™ï¼š\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    './',\n    pad_token='<|extra_0|>',\n    eos_token='<|endoftext|>',\n    padding_side='left',\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    './',\n    pad_token_id=tokenizer.pad_token_id,\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nmodel.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)\n\nall_raw_text = [\"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\"]\nbatch_raw_text = []\nfor q in all_raw_text:\n    raw_text, _ = make_context(\n        tokenizer,\n        q,\n        system=\"You are a helpful assistant.\",\n        max_window_size=model.generation_config.max_window_size,\n        chat_format=model.generation_config.chat_format,\n    )\n    batch_raw_text.append(raw_text)\n\nbatch_input_ids = tokenizer(batch_raw_text, padding='longest')\nbatch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)\nbatch_out_ids = model.generate(\n    batch_input_ids,\n    return_dict_in_generate=False,\n    generation_config=model.generation_config\n)\npadding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n\nbatch_response = [\n    decode_tokens(\n        batch_out_ids[i][padding_lens[i]:],\n        tokenizer,\n        raw_text_len=len(batch_raw_text[i]),\n        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),\n        chat_format=\"chatml\",\n        verbose=False,\n        errors='replace'\n    ) for i in range(len(all_raw_text))\n]\nprint(batch_response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹\", history=None)\nprint(response)\n\nresponse, _ = model.chat(tokenizer, \"æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°\", history=None)\nprint(response)\n```\n\n### CPU\n\nQwenã¨tiktokenã®ç´”ç²‹ãªC++å®Ÿè£…ã§ã‚ã‚‹ [qwen.cpp](https://github.com/QwenLM/qwen.cpp) ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚è©³ç´°ã¯ãƒ¬ãƒã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼\n\nã¾ãŸã€CPUä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚ç°¡å˜ã§ã™ãŒã€ãã®å ´åˆã¯ãƒ‡ãƒã‚¤ã‚¹ã®æŒ‡å®šãŒå¿…è¦ã§ã™ï¼š\n\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n```\n\nãŸã ã—ã€æ¨è«–åŠ¹ç‡ãŒæ¥µç«¯ã«ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\n### è¤‡æ•°ã®GPU\n\nGPUãƒ¡ãƒ¢ãƒªä¸è¶³ã«æ‚©ã¾ã•ã‚Œã€1ã¤ä»¥ä¸Šã®GPUã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ãŸã„å ´åˆã€Transformersã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ­ãƒ¼ãƒ‰æ–¹æ³•ã‚’ç›´æ¥ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ã€‚ä»¥å‰ã® `utils.py` ã«åŸºã¥ãæ–¹æ³•ã¯éæ¨å¥¨ã§ã™ã€‚\n\nã—ã‹ã—ã€ã“ã®æ–¹æ³•ã¯ç°¡å˜ã§ã™ãŒã€ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ã®åŠ¹ç‡ã¯ä½ã„ã§ã™ã€‚FastChatã§vLLMã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n\n### DashScope\n\nAPIã‚’é€šã˜ã¦Qwenã‚’åˆ©ç”¨ã™ã‚‹æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€Alibaba Cloudã‚’é€šã˜ãŸDashScope APIã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚ãã®ä½¿ã„æ–¹ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€OpenAIã‚¹ã‚¿ã‚¤ãƒ«ã®APIã‚’ã”è‡ªèº«ã®ã‚µãƒ¼ãƒãƒ¼ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚\n\nDashScopeã¯Alibaba CloudãŒæä¾›ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«APIã‚µãƒ¼ãƒ“ã‚¹ã§ã€ä»Šå›Qwenã«å¯¾å¿œã—ãŸã€‚DashScopeã®èƒŒå¾Œã«ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€è©³ç´°ãŒæä¾›ã•ã‚Œã¦ã„ãªã„ä¸€æ™‚çš„ãªç¤¾å†…ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã‚µãƒ¼ãƒ“ã‚¹ã«ã¯ `qwen-turbo` ã¨ `qwen-plus` ãŒã‚ã‚Šã€å‰è€…ã¯ã‚ˆã‚Šé«˜é€Ÿã«å‹•ä½œã—ã€å¾Œè€…ã¯ã‚ˆã‚Šå„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚è©³ç´°ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ [ã“ã¡ã‚‰](https://dashscope.aliyun.com) ã‚’å‚ç…§ã€‚\n\nå…¬å¼ã‚µã‚¤ãƒˆ [link](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) ã§ DashScope ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã—ã€API ã‚­ãƒ¼ (AK) ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚AK ã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼š\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nãã®å¾Œã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ [ã“ã¡ã‚‰](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚Python ã‚’ãŠä½¿ã„ã®å ´åˆã¯ã€pip ã§ DashScope ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™ï¼š\n```bash\npip install dashscope\n```\nJAVA SDKã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ã“ã®æ–¹æ³•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™ï¼š\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nDashScope ã‚’ä½¿ç”¨ã™ã‚‹æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€OpenAI API ã¨åŒæ§˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã§ã™ã€‚ä»¥ä¸‹ã«ãã®ä¾‹ã‚’ç¤ºã™ï¼š\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n        result_format='message',  # set the result to be \"message\" format.\n    )\n    return response\n\n\nif __name__ == '__main__':\n    response = call_with_messages()\n    if response.status_code == HTTPStatus.OK:\n        print(response)\n    else:\n        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n            response.request_id, response.status_code,\n            response.code, response.message\n        ))\n```\nè©³ã—ã„ä½¿ã„æ–¹ã¯å…¬å¼ã‚µã‚¤ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚\n<br><br>\n\n\n## é‡å­åŒ–\n\n### GPTQ\n\næˆ‘ã€…ã¯ã€[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)ã«åŸºã¥ã„ãŸè§£æ±ºç­–ã‚’æä¾›ã—ã€Int4ã¨Int8ã®é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒªãƒ¼ã‚¹ã™ã‚‹ã“ã¨ã§ã€ã»ã¼ç„¡æå¤±ãªãƒ¢ãƒ‡ãƒ«åŠ¹æœã‚’é”æˆã—ã¤ã¤ã€ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆã¨æ¨è«–é€Ÿåº¦ã®ä¸¡æ–¹ã§æ€§èƒ½ã‚’å‘ä¸Šã•ã›ãŸã€‚\n\nã“ã“ã§ã¯ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã™ã‚‹ã€‚å§‹ã‚ã‚‹å‰ã«ã€auto-gptqã®è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ï¼ˆä¾‹ï¼štorch 2.0ä»¥ä¸Šã€transformers 4.32.0ä»¥ä¸Šãªã©ï¼‰ã€å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š\n\n```bash\npip install auto-gptq optimum\n```\n\nauto-gptq`ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€å…¬å¼ã®[repo](https://github.com/PanQiWei/AutoGPTQ)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€ãƒ›ã‚¤ãƒ¼ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã™ã‚‹ã€‚\n\n> æ³¨æ„ï¼šã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã® `auto-gptq` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ `torch` ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¼·ãä¾å­˜ã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€æœ€è¿‘ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã«ã‚ˆã‚Š \n> ã•ã‚‰ã«ã€æœ€è¿‘ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã«ã‚ˆã‚Šã€`transformers`ã€`optimum`ã€`peft` ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n> ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä½¿ç”¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼š\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1 > - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0, <2.1 auto-gptq<0.5.0 transformers<4.35.0  optimum<1.14.0 peft>=0.5.0,<0.6.0\n\nãã†ã™ã‚Œã°ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã€ã„ã¤ã‚‚ã¨åŒã˜ã‚ˆã†ã«æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"Hi\", history=None)\n```\n\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ãŠã‘ã‚‹ BF16 ãƒ¢ãƒ‡ãƒ«ã¨ Int8ã€Int4 ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚ãã®çµæœã¯ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š\n\n| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |\n|----------------------|:----:|:-----------:|:-----:|:---------:|\n| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |\n| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |\n| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |\n| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |\n| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |\n| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |\n| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |\n| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |\n| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |\n| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |\n| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |\n| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |\n\n### KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ–\n\n> æ³¨æ„: Hugging Faceã®å†…éƒ¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã‚Šã€ã“ã®æ©Ÿèƒ½ã®ã‚µãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« \n> (ã™ãªã‚ã¡ã€`cache_autogptq_cuda_256.cpp`ã¨`cache_autogptq_cuda_kernel_256.cu`)ãŒæ¬ è½ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã‚’æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n> Hugging Face Hubã‹ã‚‰æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä»–ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«å…¥ã‚Œã¦ãã ã•ã„ã€‚\n\nã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ KV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é‡å­åŒ–ã—ã¦åœ§ç¸®ã—ã¦ä¿å­˜ã™ã‚‹ã¨ã€ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒå‘ä¸Šã™ã‚‹ã€‚ã“ã®æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€`config.json` ã« `use_cache_quantization` ã¨ `use_cache_kernel` ã¨ã„ã†å¼•æ•°ã‚’æŒ‡å®šã™ã‚‹ã€‚\nå…·ä½“çš„ãªä½¿ç”¨æ–¹æ³•ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ï¼š\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat\",\n     device_map=\"auto\",\n     trust_remote_code=True,\n     use_cache_quantization=True,\n     use_cache_kernel=True,\n     use_flash_attn=False\n)\n```\n\næ³¨æ„ï¼š ç¾åœ¨ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®é‡å­åŒ–ã¨ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’åŒæ™‚ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯ã§ããªã„ã€‚\nKV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®é‡å­åŒ–ã¨ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’åŒæ™‚ã«æœ‰åŠ¹ã«ã—ãŸå ´åˆï¼ˆ`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`ï¼‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ `use_flash_attn` ã¯ç„¡åŠ¹ã«ãªã‚‹ï¼ˆ`use_flash_attn=false`ï¼‰ã€‚\n\né‡å­åŒ–ã•ã‚ŒãŸint8-kvcacheãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚‚ã€ä¸‹æµã®è©•ä¾¡ã§å¤§å¹…ãªæ€§èƒ½ä½ä¸‹ãŒãªã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã§ã¯ã€ã•ã¾ã–ã¾ãªæ¡ä»¶ä¸‹ã§ã®ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚\nãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯ã€PyTorch 2.0.1ã¨CUDA 11.4ã‚’æ­è¼‰ã—ãŸã‚·ãƒ³ã‚°ãƒ«A100-SXM4-80G GPUã§å®Ÿè¡Œã—ã¾ã—ãŸã€‚\nãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§1024ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«BF16ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã€\"OOM \"ã¯ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ã‚’ç¤ºã—ã¾ã™ã€‚\n\nKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®é‡å­åŒ–ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆbsï¼‰ã§æ¨è«–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n\n| USE KV Cache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |\n|--------------|:------:|:------:|:------:|:------:|:------:|:------:|\n| No           | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |\n| Yes          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |\n\nKVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ–ã«ã‚ˆã‚Šã€æ¨è«–æ®µéšã§ã‚ˆã‚Šé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆ`sl`, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã€ç”Ÿæˆã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡ã™ï¼‰ã‚’ç”Ÿæˆã™ã‚‹éš›ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚ˆã‚Šå¤šãã®ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n\n| USE KV Cache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |\n|--------------|:------:|:-------:|:-------:|:-------:|:-------:|\n| No           | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |\n| Yes          |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |\n\nKVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€layer-pastã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’floatã‹ã‚‰int8ã«å¤‰æ›ã—ã€é‡å­åŒ–ã•ã‚ŒãŸ `layer-past` ã«ã¯é‡å­åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚æ ¼ç´ã•ã‚Œã‚‹ã€‚\n\nå…·ä½“çš„ãªæ‰‹é †ã¯ä»¥ä¸‹ã®é€šã‚Šï¼š\n\n1. key/valueã®é‡å­åŒ–ã‚’è¡Œã„ã¾ã™ã€‚\n```\n    qv,scale,zero_point=quantize_cache_v(v)\n```\n\n2. `layer_past`ã«ä¿å­˜ã—ã¾ã™ã€‚\n\né‡å­åŒ–ã•ã‚ŒãŸã®`layer-past`ã¯:\n```\n    layer_past=((q_key,key_scale,key_zero_point),\n                (q_value,value_scale,value_zero_point))\n```\n`layer_past`ã®å…ƒã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ï¼š\n```\n    layer_past=(key,value)\n```\né‡å­åŒ–ã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³KVã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã€ \nInt8ã®key/valueã‚’floatãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æˆ»ã™ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«é€†é‡å­åŒ–æ“ä½œã‚’ä½¿ç”¨ã—ã¾ã™ï¼š\n```\n    v=dequantize_cache_torch(qv,scale,zero_point)\n```\n<br>\n\n## æ¨è«–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n\nã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã•ã¾ã–ã¾ãªç²¾åº¦ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã¨ãƒ¡ãƒ¢ãƒªã®çµ±è¨ˆæƒ…å ±ã‚’æä¾›ã™ã‚‹ã€‚ã‚¹ãƒ”ãƒ¼ãƒ‰ã¨ãƒ¡ãƒ¢ãƒªãƒ¼ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯[ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n\nBF16ã€Int8ã€ãŠã‚ˆã³ Int4 ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ 2048 ã‚’ç”Ÿæˆã™ã‚‹éš›ã®å¹³å‡æ¨è«–é€Ÿåº¦ (ãƒˆãƒ¼ã‚¯ãƒ³/ç§’) ã¨ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®šã—ã¾ã—ãŸã€‚\n\n<table>\n    <tr>\n        <td>Model Size</td>\n        <td>Quantization</td>\n        <td>Speed (Tokens/s)</td>\n        <td>GPU Memory Usage</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">1.8B</td>\n        <td>BF16</td>\n        <td>54.09</td>\n        <td>4.23GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>55.56</td>\n        <td>3.48GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>71.07</td>\n        <td>2.91GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">7B</td>\n        <td>BF16</td>\n        <td>40.93</td>\n        <td>16.99GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>37.47</td>\n        <td>11.20GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>50.09</td>\n        <td>8.21GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">14B</td>\n        <td>BF16</td>\n        <td>32.22</td>\n        <td>30.15GB</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>29.28</td>\n        <td>18.81GB</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>38.72</td>\n        <td>13.01GB</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">72B</td>\n        <td>BF16</td>\n        <td>8.48</td>\n        <td>144.69GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int8</td>\n        <td>9.05</td>\n        <td>81.27GB (2xA100)</td>\n    </tr>\n    <tr>\n        <td>Int4</td>\n        <td>11.32</td>\n        <td>48.86GB</td>\n    </tr>\n    <tr>\n        <td>72B + vLLM</td>\n        <td>BF16</td>\n        <td>17.60</td>\n        <td>2xA100</td>\n    </tr>\n</table>\n\nãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯ã€PyTorch 2.0.1ã€CUDA 11.8ã€ãŠã‚ˆã³ Flash-Attendant 2 ã‚’å‚™ãˆãŸå˜ä¸€ã® A100-SXM4-80G GPU (2xA100 ã«ã¤ã„ã¦è¨€åŠã•ã‚Œã¦ã„ã‚‹å ´åˆã‚’é™¤ã) ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚(72B + vLLM ã¯ PyTorch 2.1.0 ãŠã‚ˆã³ Cuda 11.8 ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚) æ¨è«–é€Ÿåº¦ ã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®å¹³å‡ã§ã‚ã‚‹ã€‚\n\næ³¨æ„ï¼šä¸Šè¨˜ã®Int4/Int8ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–é€Ÿåº¦ã¯ã€autogptqã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã€``AutoModelForCausalLM.from_pretrained``ã§èª­ã¿è¾¼ã¾ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–é€Ÿåº¦ã¯ç´„20%é…ããªã‚Šã¾ã™ã€‚ã“ã®å•é¡Œã¯HuggingFaceãƒãƒ¼ãƒ ã«å ±å‘Šæ¸ˆã¿ã§ã‚ã‚Šã€è§£æ±ºç­–ãŒã‚ã‚Œã°å³åº§ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚\n\nã¾ãŸã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ç”Ÿæˆã®é•·ã•ã€Flash Attention ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã•ã¾ã–ã¾ãªè¨­å®šã§æ¨è«–é€Ÿåº¦ã¨ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚‚æ¸¬å®šã—ã¾ã™ã€‚ çµæœã¯ã€Hugging Face ã¾ãŸã¯ ModelScope ã®å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã§ç¢ºèªã§ãã¾ã™ã€‚\n\n## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n\n### ä½¿ç”¨æ–¹æ³•\nç¾åœ¨ã€å…¬å¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ `finetune.py` ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€finetune.pyã®ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã—ã€finetune.pyã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€finetune.pyã‚’èµ·å‹•ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ã•ã‚‰ã«ã€å®‰å¿ƒã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã®ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€[DeepSpeed](https://github.com/microsoft/DeepSpeed) (æ³¨æ„ï¼šã“ã‚Œã¯pydanticã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€`pydantic<2.0`ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™) ãŠã‚ˆã³ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) ã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚å¼Šç¤¾ãŒæä¾›ã™ã‚‹ã‚·ã‚§ãƒ«ãƒ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ DeepSpeed ã¨ Peft ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€äº‹å‰ã« DeepSpeed ã¨ Peft ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼š\n```bash\npip install \"peft<0.8.0\" deepspeed\n```\n\nå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ã«ã¯ã€ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã€jsonãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å„ã‚µãƒ³ãƒ—ãƒ«ã¯idã¨ä¼šè©±ãƒªã‚¹ãƒˆã§æ§‹æˆã•ã‚Œã‚‹è¾æ›¸ã§ã™ã€‚ä»¥ä¸‹ã¯1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å«ã‚€å˜ç´”ãªãƒªã‚¹ãƒˆã®ä¾‹ã§ã™ï¼š\n\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚\"\n      }\n    ]\n  }\n]\n```\n\nãƒ‡ãƒ¼ã‚¿æº–å‚™ã®å¾Œã€æä¾›ã•ã‚Œã¦ã„ã‚‹ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ã£ã¦å¾®èª¿æ•´ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ `$DATA` ã‚’å¿˜ã‚Œãšã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n\nãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®ã“ã¨ãŒå¯èƒ½ã«ãªã‚‹ï¼š\n- ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n- LoRA\n- Q-LoRA\n\nãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ï¼š\n\n```bash\n# åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚GPUãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã™ã‚‹ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒç ´ç¶»ã™ã‚‹ãŸã‚ã€ã‚·ãƒ³ã‚°ãƒ«GPUã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æä¾›ã—ã¦ã„ã¾ã›ã‚“ã€‚\nbash finetune/finetune_ds.sh\n```\n\nã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹ã€ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã€å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ DeepSpeed ZeRO 3 ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€å¼•æ•° `--deepspeed` ã‚’å‰Šé™¤ã™ã‚‹ã‹ã€è¦ä»¶ã«åŸºã¥ã„ã¦ DeepSpeed è¨­å®š json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æ··åˆç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¯¾å¿œã—ã¦ãŠã‚Šã€`--bf16 True` ã¾ãŸã¯ `--fp16 True` ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚fp16ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€æ··åˆç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã€DeepSpeedã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã“ã¨ã€‚çµŒé¨“çš„ã«ã€ã‚ãªãŸã®ãƒã‚·ãƒ³ãŒbf16ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã€ç§ãŸã¡ã®ãƒ—ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’æ•´åˆã•ã›ã‚‹ãŸã‚ã«bf16ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n\nåŒæ§˜ã«ã€LoRAã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ¥ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ã£ã¦å®Ÿè¡Œã™ã‚‹ã€‚å§‹ã‚ã‚‹å‰ã«ã€`peft`ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã€å‡ºåŠ›ã¸ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã¯çµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãªãœãªã‚‰ã€LoRAã¯ã‚¢ãƒ€ãƒ—ã‚¿ã®ã¿ã‚’ä¿å­˜ã—ã€ã‚¢ãƒ€ãƒ—ã‚¿è¨­å®šjsonãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ã¯ã€ãƒ­ãƒ¼ãƒ‰ã™ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‹ã‚‰ã§ã™ã€‚ã¾ãŸã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯bf16ã¨fp16ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚\n\n```bash\n# ã‚·ãƒ³ã‚°ãƒ«GPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\nbash finetune/finetune_lora_single_gpu.sh\n# åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\nbash finetune/finetune_lora_ds.sh\n```\n\nLoRA ([è«–æ–‡](https://arxiv.org/abs/2106.09685)) ã¯ã€ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ã€adapterã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ›´æ–°ã™ã‚‹ã ã‘ã§ã€å…ƒã®å¤§ããªè¨€èªãƒ¢ãƒ‡ãƒ«å±¤ã¯å‡çµã•ã‚ŒãŸã¾ã¾ã§ã‚ã‚‹ã€‚ãã®ãŸã‚ã€ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆãŒå¤§å¹…ã«å‰Šæ¸›ã§ãã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚‚å‰Šæ¸›ã§ãã‚‹ã€‚\n\nãªãŠã€ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆQwen-7B-Chatãªã©ï¼‰ã§ã¯ãªãã€ãƒ™ãƒ¼ã‚¹è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen-7Bãªã©ï¼‰ã®å¾®èª¿æ•´ã«LoRAã‚’ä½¿ç”¨ã—ãŸå ´åˆã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯è‡ªå‹•çš„ã«å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦åŸ‹ã‚è¾¼ã¿å±¤ã¨å‡ºåŠ›å±¤ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã¯ã€ChatMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã£ã¦ã‚‚ãŸã‚‰ã•ã‚Œã‚‹ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã«é–¢ã™ã‚‹çŸ¥è­˜ãŒãªã„ãŸã‚ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€ã“ã‚Œã‚‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç†è§£ã—äºˆæ¸¬ã™ã‚‹ãŸã‚ã«æ›´æ–°ã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚åˆ¥ã®è¨€ã„æ–¹ã‚’ã™ã‚Œã°ã€ã‚‚ã—LoRAã§ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã®ã§ã‚ã‚Œã°ã€ã‚³ãƒ¼ãƒ‰å†…ã§ `modules_to_save` ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå­¦ç¿’å¯èƒ½ãªå ´åˆã€ZeRO 3 ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€DeepSpeed è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§ ZeRO 3 ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€LoRAã®ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã¯ã€ã“ã®ã‚ˆã†ãªå­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã§ã€å¤§ããªé–‹ããŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€ãƒ¡ãƒ¢ãƒªã«å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€LoRAã®Chatãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚è©³ç´°ã¯ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\nã—ã‹ã—ã€ãã‚Œã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã«æ‚©ã‚€å ´åˆã¯ã€Q-LoRAï¼ˆ[è«–æ–‡](https://arxiv.org/abs/2305.14314)ï¼‰ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ©ãƒ¼ã‚¸è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã€ãƒšãƒ¼ã‚¸ãƒ‰ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãªã©ã®ä»–ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã€ã•ã‚‰ã«å°‘ãªã„ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆã§å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\næ³¨ï¼šã‚·ãƒ³ã‚°ãƒ« GPU Q-LoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€`pip` ã¾ãŸã¯ `conda` ã‚’ä½¿ã£ã¦ `mpi4py` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚\n\nQ-LoRAã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç›´æ¥å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š\n\n```bash\n# ã‚·ãƒ³ã‚°ãƒ«GPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\nbash finetune/finetune_qlora_single_gpu.sh\n# åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\nbash finetune/finetune_qlora_ds.sh\n```\n\nQ-LoRAã«ã¤ã„ã¦ã¯ã€å¼Šç¤¾ãŒæä¾›ã™ã‚‹é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã€ä¾‹ãˆã°Qwen-7B-Chat-Int4ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚BF16ãƒ¢ãƒ‡ãƒ«ã¯ä½¿ç”¨ã—**ãªã„**ã§ãã ã•ã„ï¼ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„LoRAã¨ã¯ç•°ãªã‚Šã€Q-LoRAã§ã¯fp16ã®ã¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã€‚ã‚·ãƒ³ã‚°ãƒ«GPUã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ã€ãƒˆãƒ¼ãƒã‚¢ãƒ³ãƒ—ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼ãŒè¦³æ¸¬ã•ã‚ŒãŸãŸã‚ã€æ··åˆç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯DeepSpeedã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ã¾ãŸã€Q-LoRAã®å ´åˆã€LoRAã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®å•é¡ŒãŒæ®‹ã£ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€Q-LoRAã§ã¯ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦Int4ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’æä¾›ã—ã¦ãŠã‚Šã€è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ChatMLå½¢å¼ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å¿ƒé…ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãªãŠã€Int4ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯å­¦ç¿’ã§ããªã„ã¯ãšãªã®ã§ã€å­¦ç¿’ã§ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’å°å…¥ã™ã‚‹ã¨ã€Q-LoRAãŒå‹•ä½œã—ãªããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\nLoRAã¨Q-LoRAã®å­¦ç¿’ã¯ã€ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯ç•°ãªã‚Šã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿å­˜ã™ã‚‹ã€‚ä»®ã«Qwen-7Bã‹ã‚‰å­¦ç¿’ã‚’é–‹å§‹ã—ãŸã¨ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ¨è«–ã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ï¼š\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒãƒ¼ã‚¸ã—ã€å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜ã—ãŸã„å ´åˆã¯ï¼ˆã“ã‚Œã¯ LoRA ã§ã®ã¿å¯èƒ½ã§ã€Q-LoRA ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ï¼‰ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\n`new_model_directory` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯ã€ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¾ã™ã€‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã« `*.cu` ãŠã‚ˆã³ `*.cpp` ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ã€æ‰‹å‹•ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚ ã¾ãŸã€ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã€‚ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ã‹ã€æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    path_to_adapter, # path to the output directory\n    trust_remote_code=True\n)\n\ntokenizer.save_pretrained(new_model_directory)\n```\n\næ³¨æ„ï¼šãƒãƒ«ãƒGPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆã€åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚·ãƒ³ã«å¿œã˜ã¦æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿ã€ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’è€ƒæ…®ã—ã¦ã€å¼•æ•° `--model_max_length` ã§æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n\n### ãƒ¡ãƒ¢ãƒªã¨é€Ÿåº¦ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°\nã‚·ãƒ³ã‚°ãƒ«GPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ãŠã„ã¦ã€LoRA (LoRA(emb)ã¯embeddingã¨å‡ºåŠ›å±¤ã‚’å­¦ç¿’ã•ã›ã‚‹ãŒã€LoRAã¯embeddingã¨å‡ºåŠ›å±¤ã‚’å­¦ç¿’ã•ã›ãªã„) ã¨Q-LoRAã®GPUãƒ¡ãƒ¢ãƒªã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹ã€‚ã“ã®ãƒ†ã‚¹ãƒˆã§ã¯ã€ã‚·ãƒ³ã‚°ãƒ«A100-SXM4-80G GPUã§å®Ÿé¨“ã—ã€CUDA 11.8ã¨Pytorch 2.0ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚Flash attention 2ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚256ã€512ã€1024ã€2048ã€4096ã€8192ã¨ã„ã†ç•°ãªã‚‹é•·ã•ã®å…¥åŠ›ã®ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰ã¨é€Ÿåº¦ï¼ˆs/iterï¼‰ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚ã¾ãŸã€2å°ã®A100 GPUã‚’ç”¨ã„ãŸQwen-7Bã«ã‚ˆã‚‹ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµ±è¨ˆé‡ã‚‚å ±å‘Šã™ã‚‹ã€‚GPUãƒ¡ãƒ¢ãƒªã®åˆ¶é™ã®ãŸã‚ã€256ã€512ã€1024ãƒˆãƒ¼ã‚¯ãƒ³ã®çµ±è¨ˆã®ã¿ã‚’å ±å‘Šã™ã‚‹ã€‚\n\n\nQwen-72B ã«ã¤ã„ã¦ã¯ã€2 ã¤ã®æ–¹æ³•ã§å®Ÿé¨“ã—ã¾ã™ã€‚1) 4 ã¤ã® A100-SXM4-80G GPU ã§ã® Lora å¾®èª¿æ•´ + DeepSpeed ZeRO 3ã€ãŠã‚ˆã³ 2) 1 ã¤ã® A100-SXM4-80G GPU ã§ã® QLora (int4) å¾®èª¿æ•´ã€‚ OOM ã¯ã€LoRA (emb) å¾®èª¿æ•´ã¨ Deepspeed ZeRO 3 ã‚’ä½¿ç”¨ã—ãªã„ LoRA å¾®èª¿æ•´ã®ä¸¡æ–¹ã§ 4 ã¤ã® A100-SXM4-80G GPU ã§ç™ºç”Ÿã™ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ (`--deepspeedfinetune/ds_config_zero3.json` ã‚’ [`finetune/finetune_lora_ds ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™) .sh`](finetune/finetune_lora_ds.sh) ã‚’ä½¿ç”¨ã—ã¦ DeepSpeed ZeRO 3 ã‚’æœ‰åŠ¹ã«ã—ã¾ã™)ã€‚\n\nçµ±è¨ˆé‡ã‚’ä»¥ä¸‹ã«ç¤ºã™ï¼š\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    </tr>\n    </tr>\n\t\t<tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td><td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">7B</th><td>LoRA</td><td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td><td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th><td>LoRA</td><td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td><td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n\t<tr>\n        <th rowspan=\"2\">72B</th><td>LoRA + Deepspeed Zero3</td><td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n<br>\n\n## ãƒ‡ãƒ—ãƒ­ã‚¤\n\n### vLLM \nãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨é«˜é€Ÿæ¨è«–ã®ãŸã‚ã«ã¯ã€vLLMã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n\n**CUDA 12.1** ãŠã‚ˆã³ **PyTorch 2.1** ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ vLLM ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™ã€‚\n```bash\npip install vllm\n```\n\nãã‚Œä»¥å¤–ã®å ´åˆã¯ã€å…¬å¼ vLLM [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †](https://docs.vllm.ai/en/latest/getting_started/installation.html) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n#### vLLM + Transformer Wrapper\n\n[ãƒ©ãƒƒãƒ‘ãƒ¼ ã‚³ãƒ¼ãƒ‰](examples/vllm_wrapper.py) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®å¯¾è©±å¯¾è©±ã®ãŸã‚ã«æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚ (æ³¨: ç¾åœ¨ã¯ ``model.chat()`` ãƒ¡ã‚½ãƒƒãƒ‰ã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚)\n\n```python\nfrom vllm_wrapper import vLLMWrapper\n\nmodel = vLLMWrapper('Qwen/Qwen-7B-Chat', tensor_parallel_size=1)\n# model = vLLMWrapper('Qwen/Qwen-7B-Chat-Int4', tensor_parallel_size=1, dtype=\"float16\")\n\nresponse, history = model.chat(query=\"ä½ å¥½\", history=None)\nprint(response)\nresponse, history = model.chat(query=\"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\nresponse, history = model.chat(query=\"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n```\n#### vLLM + Web ãƒ‡ãƒ¢ / OpenAI API\nFastChat ã‚’ä½¿ç”¨ã—ã¦ã€Web ãƒ‡ãƒ¢ã¾ãŸã¯ OpenAI API ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã§ãã¾ã™ã€‚ ã¾ãšã€FastChat ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚\n```\npip install \"fschat[model_worker,webui]\"\n```\n\nvLLM ãŠã‚ˆã³ FastChat ã§ Qwen ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€æ¬¡ã®æ–¹æ³•ã§ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’èµ·å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n```bash\npython -m fastchat.serve.controller\n```\n\nãã‚Œã‹ã‚‰model workerã‚’èµ·å‹•ã—ã€æ¨è«–ã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã‚·ãƒ³ã‚°ãƒ«GPUæ¨è«–ã®å ´åˆã¯ã€ç›´æ¥å®Ÿè¡Œã§ãã¾ã™ï¼š\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype float16 # INT4ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™\n```\nã—ã‹ã—ã€ã‚ˆã‚Šé«˜é€Ÿãªæ¨è«–ã‚„å¤§å®¹é‡ãƒ¡ãƒ¢ãƒªãƒ¼ã®ãŸã‚ã«è¤‡æ•°ã®GPUã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ãŸã„å ´åˆã¯ã€vLLMãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’4GPUã§å®Ÿè¡Œã™ã‚‹ã¨ã™ã‚‹ã¨ã€ã‚³ãƒãƒ³ãƒ‰ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype bfloat16\n# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype float16 # run int4 model # INT4ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™\n```\n\nãƒ¢ãƒ‡ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•ã—ãŸå¾Œã€èµ·å‹•ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š\n\n* Web UI Demo\n```bash\npython -m fastchat.serve.gradio_web_server\n```\n\n* OpenAI API\n```bash\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\n```\n\nãŸã ã—ã€vLLM ã¨ FastChat ã®ä½¿ç”¨ãŒé›£ã—ã„å ´åˆã¯ã€Web ãƒ‡ãƒ¢ã€CLI ãƒ‡ãƒ¢ã€ãŠã‚ˆã³ API ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã«æä¾›ã•ã‚Œã¦ã„ã‚‹æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n\n\n### ã‚¦ã‚§ãƒ– UI\n\nã‚¦ã‚§ãƒ– UI ãƒ‡ãƒ¢ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™ï¼ˆ@wysaid ã«æ„Ÿè¬ï¼‰ã€‚ã“ã‚Œã‚’å§‹ã‚ã‚‹å‰ã«ã€ä»¥ä¸‹ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„:\n\n```bash\npip install -r requirements_web_demo.txt\n```\n\nãã—ã¦ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€ç”Ÿæˆã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n### CLI ãƒ‡ãƒ¢\n\n`cli_demo.py` ã« CLI ã®ãƒ‡ãƒ¢ä¾‹ã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ã§ Qwen-7B-Chat ã¨å¯¾è©±ã™ã‚‹ã“ã¨ãŒã§ãã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’è¿”ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹:\n\n```\npython cli_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/cli_demo.gif\" width=\"600\" />\n    <br>\n<p>\n<br>\n\n### API\n\nOpenAI API ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ã‚«ãƒ«APIã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹æ–¹æ³•ã‚’æä¾›ã™ã‚‹ï¼ˆ@hanpenggit ã«æ„Ÿè¬ï¼‰ã€‚å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nãã‚Œã‹ã‚‰ã€API ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™:\n\n```bash\npython openai_api.py\n```\n\nãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåã‚„ãƒ‘ã‚¹ã«ã¯ `-c`ã€CPU ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã«ã¯ `--cpu-only` ãªã©ã€å¼•æ•°ã‚’å¤‰æ›´ã§ãã¾ã™ã€‚API ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’èµ·å‹•ã™ã‚‹éš›ã«å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«æ›´æ–°ã™ã‚‹ã“ã¨ã§è§£æ±ºã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\nAPI ã®ä½¿ã„æ–¹ã‚‚ç°¡å˜ã§ã™ã€‚ä»¥ä¸‹ã®ä¾‹ã‚’ã”è¦§ãã ã•ã„:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹\nfor chunk in openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=True\n    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›å½¢å¼ã§ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®æŒ‡å®šã¯ã¾ã ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãŠã‚‰ãšã€é–‹ç™ºä¸­ã§ã™ã€‚\n):\n    if hasattr(chunk.choices[0].delta, \"content\"):\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æœ‰åŠ¹åŒ–ã—ãªã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹\nresponse = openai.ChatCompletion.create(\n    model=\"Qwen\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n    ],\n    stream=False,\n    stop=[] # ä¾‹ãˆã°ã€stop=[\"Observation:\"] (ReAct ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å ´åˆ)ã€‚\n)\nprint(response.choices[0].message.content)\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/openai_api.gif\" width=\"600\" />\n    <br>\n<p>\n\n**Function Calling** ã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™(ãŸã ã—ã€ä»Šã®ã¨ã“ã‚ `stream=False` ã®å ´åˆã®ã¿)ã€‚ä½¿ç”¨ä¾‹](examples/function_call_examples.py) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n<br><br>\n\n## ğŸ³ Docker\n\nãƒ‡ãƒ—ãƒ­ã‚¤ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç°¡ç´ åŒ–ã™ã‚‹ãŸã‚ã«ã€ã‚ã‚‰ã‹ã˜ã‚ç’°å¢ƒã‚’æ§‹ç¯‰ã—ãŸ docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æä¾›ã—ã¦ã„ã¾ã™ï¼š [qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen)ã€‚ãƒ‰ãƒ©ã‚¤ãƒã‚’å°å…¥ã—ã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ã€ãƒ‡ãƒ¢ã‚’èµ·å‹•ã—ã€OpenAI APIã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\n### æº–å‚™\n\n1. ä½¿ç”¨ã™ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã«å¿œã˜ã¦ã€æ­£ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®Nvidiaãƒ‰ãƒ©ã‚¤ãƒã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š\n  - `qwenllm/qwen:cu117` (**recommend**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: same as `qwenllm/qwen:cu117`\n\n2. [Docker](https://docs.docker.com/engine/install/) ã¨ [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦è¨­å®šã—ã¾ã™ï¼š\n\n```bash\n# configure docker\nsudo systemctl start docker\n# test if docker is correctly installed\nsudo docker run hello-world\n\n# configure nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n# test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã‚³ãƒ¼ãƒ‰ã‚’ç’°å¢ƒã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼ˆ[ã“ã¡ã‚‰](#DownloadModel)ã‚’å‚ç…§ï¼‰ã€‚\n\n### ãƒ‡ãƒ—ãƒ­ã‚¤\n\nã“ã“ã§ã¯ä¾‹ã¨ã—ã¦ Qwen-7B-Chat ã‚’ä½¿ç”¨ã™ã‚‹ã€‚ã‚¦ã‚§ãƒ–ãƒ»ãƒ‡ãƒ¢ã‚„ API ã‚’èµ·å‹•ã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes\n```\nä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒãƒ“ãƒ«ãƒ‰ã«å½¹ç«‹ã¤ï¼š\n\n* OpenAI API\n```bash\nbash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* Web UI\n```bash\nbash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}\n```\n\n* CLI Demo\n```bash\nbash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}\n```\n\nä¸Šè¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã¯è‡ªå‹•çš„ã«å¿…è¦ãªã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§Web UIãƒ‡ãƒ¢ã‚’èµ·å‹•ã—ã¾ã™ï¼ˆã‚µãƒ¼ãƒ“ã‚¹ã¯è‡ªå‹•ã§å†èµ·å‹•ã—ã¾ã™ï¼‰ã€‚ãƒ‡ãƒ¢ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒ›ã‚¹ãƒˆä¸Šã§ `http://localhost:${PORT}` ã‚’é–‹ã„ã¦ãã ã•ã„ã€‚\n\nä»¥ä¸‹ã®å‡ºåŠ›ãŒè¡¨ç¤ºã•ã‚Œã‚Œã°ã€ãƒ‡ãƒ¢ã¯æ­£å¸¸ã«èµ·å‹•ã—ã¦ã„ã¾ã™ï¼š\n\n```text\nSuccessfully started web demo. Open '...' to try!\nRun `docker logs ...` to check demo status.\nRun `docker rm -f ...` to stop and remove the demo.\n```\n\nãƒ‡ãƒ¢ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ãŸã„å ´åˆã¯ã€`docker logs qwen` ã‚’ä½¿ã£ã¦å‡ºåŠ›ã‚’è¡¨ç¤ºã§ãã‚‹ã€‚\n\ndocker rm -f qwen` ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢ã—ã€ã‚³ãƒ³ãƒ†ãƒŠã‚’å‰Šé™¤ã§ãã‚‹ã€‚\n\n\n### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n\nãƒ“ãƒ«ãƒ‰æ¸ˆã¿ã®Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’åˆ©ç”¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ–¹æ³•ã¯ã€åŸºæœ¬çš„ã«[å‰ç« ](#Finetuning)ã¨åŒã˜ã§ã™(ã™ã§ã«ã‚¤ãƒ¡ãƒ¼ã‚¸ã«ä¾å­˜é–¢ä¿‚ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™)ï¼š\n\nä»¥ä¸‹ã¯ã‚·ãƒ³ã‚°ãƒ«GPUã®LoRAã®ä¾‹ã§ã™ï¼š\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes\n#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)\nDATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json\nOUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs\n\n# Use all host devices by default\nDEVICE=all\n# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)\n#DEVICE='\"device=0,1,2,3\"'\n\nmkdir -p ${OUTPUT_PATH}\n\n# Single-GPU LoRA finetuning\ndocker run --gpus ${DEVICE} --rm --name qwen \\\n    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \\\n    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \\\n    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \\\n    --shm-size=2gb \\\n    -it ${IMAGE_NAME} \\\n    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json\n```\n\nä¾‹ãˆã°ã‚·ãƒ³ã‚°ãƒ«GPUã®Q-LoRAã«å¤‰æ›´ã™ã‚‹ã«ã¯ã€`docker run`å†…ã®bashã‚³ãƒãƒ³ãƒ‰ã‚’å¤‰æ›´ã™ã‚‹ã ã‘ã§ã„ã„ï¼š\n```bash\nbash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json\n```\n<br>\n\n## ğŸ”¥ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\nQwen-1.8-Chat ã¨ Qwen-72B-Chat ã¯ã€è¤‡æ•°å›ã®è¤‡é›‘ãªå¯¾è©±ã‚’ä¼´ã†å¤šæ§˜ãªã‚·ã‚¹ãƒ†ãƒ  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Œå…¨ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã•ã¾ã–ã¾ãªã‚·ã‚¹ãƒ†ãƒ  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã„ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ãŸãƒ¢ãƒ‡ãƒ«ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã‚’å®Ÿç¾ã—ã€Qwen-Chat ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\nã‚·ã‚¹ãƒ†ãƒ  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€Qwen-Chat ã¯ **ãƒ­ãƒ¼ãƒªãƒ¼ ãƒ—ãƒ¬ã‚¤**ã€**è¨€èªã‚¹ã‚¿ã‚¤ãƒ«ã®è»¢é€**ã€**ã‚¿ã‚¹ã‚¯è¨­å®š**ã€**å‹•ä½œè¨­å®š**ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚\n\n![](assets/system_prompt_ language_style.png)\n\n![](assets/system_prompt_role_play_en.png)\n\nè©³ç´°ã«ã¤ã„ã¦ã¯ã€[ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](examples/system_prompt.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n## ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨\n\nQwen-Chat ã¯ã€ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨æ³•ã¨é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã«åˆã‚ã›ã¦æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€LangChain ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã—ã€Python ã‚³ãƒ¼ãƒ‰ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿ãƒ¼ã§ Qwen ã‚’æ‹¡å¼µã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n\nReAct ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŸå‰‡ã«åŸºã¥ã„ã¦ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å®Ÿè£…ã™ã‚‹æ–¹æ³•ã«é–¢ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚[ReAct ã®ä¾‹](examples/react_prompt.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ ã“ã®åŸå‰‡ã«åŸºã¥ã„ã¦ã€[openai_api.py](openai_api.py) ã§é–¢æ•°å‘¼ã³å‡ºã—ã®ã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚\n\nã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ä¸­å›½èªè©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ãƒ¢ãƒ‡ãƒ«ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ãŸã¨ã“ã‚ã€Qwen-Chat ãŒä¸€è²«ã—ã¦è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.â†‘)</th><th align=\"center\">Tool Input (Rouge-Lâ†‘)</th><th align=\"center\">False Positive Errorâ†“</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\næ•°å­¦çš„å•é¡Œè§£æ±ºã€ãƒ‡ãƒ¼ã‚¿è¦–è¦šåŒ–ã€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚„ Web ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãªã©ã®ãã®ä»–ã®æ±ç”¨ã‚¿ã‚¹ã‚¯ã« Python ã‚³ãƒ¼ãƒ‰ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ Qwen ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€ã“ã‚Œã‚‰ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ä½œæˆã—ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¾ã—ãŸã€‚ ã€‚ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã“ã® [ãƒªãƒ³ã‚¯](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark) ã§è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\nQwen ã¯ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆæ™‚ã®ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œå¯èƒ½æ€§ã¨çµæœã®ç²¾åº¦ã®ç‚¹ã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Mathâ†‘</th><th align=\"center\">Visualization-Hardâ†‘</th><th align=\"center\">Visualization-Easyâ†‘</th><th align=\"center\">Generalâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n## é•·ã„æ–‡è„ˆã®ç†è§£\n\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ‹¡å¼µã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã«ã€NTK å¯¾å¿œè£œé–“ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€LogN ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã©ã®ã„ãã¤ã‹ã®æŠ€è¡“ã‚’å°å…¥ã—ã€Qwen-14B ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’ 2K ã‹ã‚‰ 8K ä»¥ä¸Šã«æ‹¡å¼µã—ã¾ã™ã€‚ ãƒˆãƒ¼ã‚¯ãƒ³ã€ãŠã‚ˆã³ Qwen-1.8B/7B ã¯ 8K ã‹ã‚‰ 32K ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ã€‚\n\nQwen-72B ã§ã¯ã€ã‚ˆã‚Šå¤§ããªå›è»¢ãƒ™ãƒ¼ã‚¹ã‚’å‚™ãˆãŸã‚ˆã‚Šé•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã« RoPE ã‚’é©å¿œã•ã›ã¾ã™ã€‚ Qwen-72B ã¯ã€32K ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n\nç§ãŸã¡ã¯ã€PPL è©•ä¾¡ã‚’ä½¿ç”¨ã—ã¦ arXiv ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°å®Ÿé¨“ã‚’å®Ÿæ–½ã—ã€Qwen ãŒé•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚·ãƒŠãƒªã‚ªã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚ çµæœã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚\n\n<table>\n    <tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th><th align=\"center\">32768</th>\n    </tr>\n     <tr>\n        <td>Qwen-7B (original)</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">39.35</td><td align=\"center\">469.81</td><td align=\"center\">2645.09</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.59</td><td align=\"center\">3.66</td><td align=\"center\">5.71</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.56</td><td align=\"center\">4.62</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\">4.23</td><td align=\"center\">3.78</td><td align=\"center\">3.58</td><td align=\"center\">3.49</td><td align=\"center\">4.32</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n    <tr>\n        <td>Qwen-1.8B</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.13</b></td><td align=\"center\"><b>3.89</b></td><td align=\"center\">17.42</td><td align=\"center\">433.85</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>5.00</b></td><td align=\"center\"><b>4.48</b></td><td align=\"center\"><b>4.14</b></td><td align=\"center\"><b>3.93</b></td><td align=\"center\"><b>3.82</b></td><td align=\"center\"><b>3.83</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.31</b></td><td align=\"center\">7.27</td><td align=\"center\">181.49</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>4.23</b></td><td align=\"center\"><b>3.81</b></td><td align=\"center\"><b>3.52</b></td><td align=\"center\"><b>3.33</b></td><td align=\"center\"><b>3.22</b></td><td align=\"center\"><b>3.17</b></td>\n    </tr>\n    <tr>\n        <td>Qwen-14B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\">22.79</td><td align=\"center\">334.65</td><td align=\"center\">3168.35</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + window_attn</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>3.46</b></td><td align=\"center\"><b>3.29</b></td><td align=\"center\"><b>3.18</b></td><td align=\"center\">3.42</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B</td><td align=\"center\"><b>-</b></td><td align=\"center\"><b>-</b></td><td align=\"center\">-</td><td align=\"center\"><b>2.83</b></td><td align=\"center\"><b>2.73</b></td><td align=\"center\"><b>2.72</b></td>\n    </tr>\n    </tr>\n</table>\n\nã•ã‚‰ã«ã€Qwen-72B-Chat ã®é•·æ–‡ç†è§£èƒ½åŠ›ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã«ã€[L-Eval](https://arxiv.org/abs/2307.11088) (ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ã‚¨ãƒ³ãƒ‰ ã‚¿ã‚¹ã‚¯) ã§ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸã€‚ çµæœã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚\n\n| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |\n|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |\n| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |\n\nç§ãŸã¡ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒå…¥åŠ›å†…ã®ã•ã¾ã–ã¾ãªä½ç½®ã§æƒ…å ±ã‚’å–å¾—ã§ãã‚‹ã‹ã©ã†ã‹ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã«ã€ã€Œå¹²ã—è‰ã®å±±ã®ä¸­ã®é‡ã€å®Ÿé¨“ (ã“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393) ã‹ã‚‰æ¥ã¾ã—ãŸ) ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚ ç•°ãªã‚‹é•·ã•ã®å ´åˆã€çµæœã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n![](assets/qwen_72b_needle_in_a_haystack.png)\n\nä¸Šè¨˜ã®çµæœã¯ã€Qwen-72B-Chat ãŒ 32K ã®å…¥åŠ›é•·å†…ã§ã•ã¾ã–ã¾ãªä½ç½®ã«é…ç½®ã•ã‚ŒãŸæƒ…å ±ã‚’æ­£ç¢ºã«å–å¾—ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ãŠã‚Šã€ãã®å„ªã‚ŒãŸé•·æ–‡ç†è§£èƒ½åŠ›ã‚’è¨¼æ˜ã—ã¦ã„ã¾ã™ã€‚\n\n## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n\ntiktoken ã«åŸºã¥ããƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ã€ä»–ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ä¾‹ãˆã°ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒ”ãƒ¼ã‚¹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚ç‰¹ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®éš›ã«ã¯ã€ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨æ„ã‚’æ‰•ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã«é–¢ã™ã‚‹è©³ç´°ãªæƒ…å ±ã‚„ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](tokenization_note_ja.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n<br><br>\n\n## å†ç¾\n\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®å†ç¾ã®ãŸã‚ã«ã€çµæœã‚’å†ç¾ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚è©³ã—ãã¯ [eval/EVALUATION.md](eval/EVALUATION.md) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãªãŠã€å†ç¾ã®çµæœã€æˆ‘ã€…ã®å ±å‘Šçµæœã¨è‹¥å¹²ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\n<br><br>\n\n## FAQ\n\nå•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ã¾ãšã¯ [FAQ](FAQ_ja.md) ã‚„ issue ã‚’å‚ç…§ã—ã€æ–°ã—ã„ issue ã‚’ç«‹ã¡ä¸Šã’ã‚‹å‰ã«è§£æ±ºç­–ã‚’æ¢ã—ã¦ãã ã•ã„ã€‚\n<br><br>\n\n## å¼•ç”¨\nç§ãŸã¡ã®ä»•äº‹ãŒå½¹ã«ç«‹ã£ãŸã¨æ€ã£ãŸã‚‰ã€é æ…®ãªãå¼•ç”¨ã—ã¦ãã ã•ã„ã€‚\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å¥‘ç´„\n\n<https://github.com/QwenLM/Qwen>ã§æä¾›ã•ã‚Œã‚‹ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯ã€ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹[Apache 2.0 License](./LICENSE)ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nç ”ç©¶è€…ã‚„é–‹ç™ºè€…ã¯ã€Qwenã¨Qwen-Chatã®ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚’è‡ªç”±ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å•†ç”¨åˆ©ç”¨ã«ã¤ã„ã¦ã¯ã€å„ãƒ¢ãƒ‡ãƒ«ã«æ·»ä»˜ã•ã‚Œã¦ã„ã‚‹ä½¿ç”¨è¨±è«¾å¥‘ç´„æ›¸ã‚’ã”ç¢ºèªãã ã•ã„ã€‚\n\n- Qwen-72Bã€Qwen-14Bã€Qwen-7Bã¯ã€å¯¾å¿œã™ã‚‹HuggingFaceã¨ModelScopeã®ãƒªãƒã‚¸ãƒˆãƒªã«ã‚ã‚‹[Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT)ã«åŸºã¥ã„ã¦ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚å•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ ([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen))ã«è¨˜å…¥ã—ã¦ç”³è«‹ã—ã¦ãã ã•ã„ã€‚\n\n- Qwen-1.8Bã¯ã€å¯¾å¿œã™ã‚‹HuggingFaceã¨ModelScopeã®ãƒªãƒã‚¸ãƒˆãƒªã«ã‚ã‚‹[Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT)ã«åŸºã¥ã„ã¦ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚å•†ç”¨åˆ©ç”¨ã«ã¤ã„ã¦ã¯ã€ç§ãŸã¡ã«ã”é€£çµ¡ãã ã•ã„ã€‚\n<br><br>\n\n## ãŠå•ã„åˆã‚ã›\n\nç ”ç©¶ãƒãƒ¼ãƒ ã¾ãŸã¯è£½å“ãƒãƒ¼ãƒ ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã€qianwen_opensource@alibabacloud.com ã¾ã§ãŠæ°—è»½ã«ãŠé€ã‚Šãã ã•ã„ã€‚\n\n"
        },
        {
          "name": "Tongyi Qianwen LICENSE AGREEMENT",
          "type": "blob",
          "size": 6.734375,
          "content": "Tongyi Qianwen LICENSE AGREEMENT\n\nTongyi Qianwen Release Date: August 3, 2023\n\nBy clicking to agree or by using or distributing any portion or element of the Tongyi Qianwen Materials, you will be deemed to have recognized and accepted the content of this Agreement, which is effective immediately.\n\n1. Definitions\n    a. This Tongyi Qianwen LICENSE AGREEMENT (this \"Agreement\") shall mean the terms and conditions for use, reproduction, distribution and modification of the Materials as defined by this Agreement.\n    b. \"We\"(or \"Us\") shall mean Alibaba Cloud.\n    c. \"You\" (or \"Your\") shall mean a natural person or legal entity exercising the rights granted by this Agreement and/or using the Materials for any purpose and in any field of use.\n    d. \"Third Parties\" shall mean individuals or legal entities that are not under common control with Us or You.\n    e. \"Tongyi Qianwen\" shall mean the large language models (including Qwen model and Qwen-Chat model), and software and algorithms, consisting of trained model weights, parameters (including optimizer states), machine-learning model code, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Us.\n    f. \"Materials\" shall mean, collectively, Alibaba Cloud's proprietary Tongyi Qianwen and Documentation (and any portion thereof) made available under this Agreement.\n    g. \"Source\" form shall mean the preferred form for making modifications, including but not limited to model source code, documentation source, and configuration files.\n    h. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation,\n and conversions to other media types.\n\n2. Grant of Rights\nYou are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Alibaba Cloud's intellectual property or other rights owned by Us embodied in the Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Materials.\n\n3. Redistribution\nYou may reproduce and distribute copies of the Materials or derivative works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n    a. You shall give any other recipients of the Materials or derivative works a copy of this Agreement;\n    b. You shall cause any modified files to carry prominent notices stating that You changed the files;\n    c. You shall retain in all copies of the Materials that You distribute the following attribution notices within a \"Notice\" text file distributed as a part of such copies: \"Tongyi Qianwen is licensed under the Tongyi Qianwen LICENSE AGREEMENT, Copyright (c) Alibaba Cloud. All Rights Reserved.\"; and\n    d. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such derivative works as a whole, provided Your use, reproduction, and distribution of the work otherwise complies with the terms and conditions of this Agreement.\n\n4. Restrictions\nIf you are commercially using the Materials, and your product or service has more than 100 million monthly active users, You shall request a license from Us. You cannot exercise your rights under this Agreement without our express authorization.\n\n5. Rules of use\n    a. The Materials may be subject to export controls or restrictions in China, the United States or other countries or regions. You shall comply with applicable laws and regulations in your use of the Materials.\n    b. You can not use the Materials or any output therefrom to improve any other large language model (excluding Tongyi Qianwen or derivative works thereof).\n\n6. Intellectual Property\n    a. We retain ownership of all intellectual property rights in and to the Materials and derivatives made by or for Us. Conditioned upon compliance with the terms and conditions of this Agreement, with respect to any derivative works and modifications of the Materials that are made by you, you are and will be the owner of such derivative works and modifications.\n    b. No trademark license is granted to use the trade names, trademarks, service marks, or product names of Us, except as required to fulfill notice requirements under this Agreement or as required for reasonable and customary use in describing and redistributing the Materials.\n    c. If you commence a lawsuit or other proceedings (including a cross-claim or counterclaim in a lawsuit) against Us or any entity alleging that the Materials or any output therefrom, or any part of the foregoing, infringe any intellectual property or other right owned or licensable by you, then all licences granted to you under this Agreement shall terminate as of the date such lawsuit or other proceeding is commenced or brought.\n\n7. Disclaimer of Warranty and Limitation of Liability\n\n    a. We are not obligated to support, update, provide training for, or develop any further version of the Tongyi Qianwen Materials or to grant any license thereto.\n    b. THE MATERIALS ARE PROVIDED \"AS IS\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, OR FITNESS FOR A PARTICULAR PURPOSE. WE MAKE NO WARRANTY AND ASSUME NO RESPONSIBILITY FOR THE SAFETY OR STABILITY OF THE MATERIALS AND ANY OUTPUT THEREFROM.\n    c. IN NO EVENT SHALL WE BE LIABLE TO YOU FOR ANY DAMAGES, INCLUDING, BUT NOT LIMITED TO ANY DIRECT, OR INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES ARISING FROM YOUR USE OR INABILITY TO USE THE MATERIALS OR ANY OUTPUT OF IT, NO MATTER HOW ITâ€™S CAUSED.\n    d. You will defend, indemnify and hold harmless Us from and against any claim by any third party arising out of or related to your use or distribution of the Materials.\n\n8. Survival and Termination.\n    a. The term of this Agreement shall commence upon your acceptance of this Agreement or access to the Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein.\n    b. We may terminate this Agreement if you breach any of the terms or conditions of this Agreement. Upon termination of this Agreement, you must delete and cease use of the Materials. Sections 7 and 9 shall survive the termination of this Agreement.\n\n9. Governing Law and Jurisdiction.\n    a. This Agreement and any dispute arising out of or relating to it will be governed by the laws of China, without regard to conflict of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement.\n    b. The People's Courts in Hangzhou City shall have exclusive jurisdiction over any dispute arising out of this Agreement."
        },
        {
          "name": "Tongyi Qianwen RESEARCH LICENSE AGREEMENT",
          "type": "blob",
          "size": 7.111328125,
          "content": "Tongyi Qianwen RESEARCH LICENSE AGREEMENT\n\nTongyi Qianwen Release Date: November 30, 2023\n\nBy clicking to agree or by using or distributing any portion or element of the Tongyi Qianwen Materials, you will be deemed to have recognized and accepted the content of this Agreement, which is effective immediately.\n\n1. Definitions\n    a. This Tongyi Qianwen RESEARCH LICENSE AGREEMENT (this \"Agreement\") shall mean the terms and conditions for use, reproduction, distribution and modification of the Materials as defined by this Agreement.\n    b. \"We\"(or \"Us\") shall mean Alibaba Cloud.\n    c. \"You\" (or \"Your\") shall mean a natural person or legal entity exercising the rights granted by this Agreement and/or using the Materials for any purpose and in any field of use.\n    d. \"Third Parties\" shall mean individuals or legal entities that are not under common control with Us or You.\n    e. \"Tongyi Qianwen\" shall mean the large language models, and software and algorithms, consisting of trained model weights, parameters (including optimizer states), machine-learning model code, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Us.\n    f. \"Materials\" shall mean, collectively, Alibaba Cloud's proprietary Tongyi Qianwen and Documentation (and any portion thereof) made available under this Agreement.\n    g. \"Source\" form shall mean the preferred form for making modifications, including but not limited to model source code, documentation source, and configuration files.\n    h. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation,\n and conversions to other media types.\n    i. \"Non-Commercial\" shall mean for research or evaluation purposes only.\n\n2. Grant of Rights\n    a. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Alibaba Cloud's intellectual property or other rights owned by Us embodied in the Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Materials FOR NON-COMMERCIAL PURPOSES ONLY.\n    b. If you are commercially using the Materials, You shall request a license from Us.\n\n3. Redistribution\nYou may reproduce and distribute copies of the Materials or derivative works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n    a. You shall give any other recipients of the Materials or derivative works a copy of this Agreement;\n    b. You shall cause any modified files to carry prominent notices stating that You changed the files;\n    c. You shall retain in all copies of the Materials that You distribute the following attribution notices within a \"Notice\" text file distributed as a part of such copies: \"Tongyi Qianwen is licensed under the Tongyi Qianwen RESEARCH LICENSE AGREEMENT, Copyright (c) Alibaba Cloud. All Rights Reserved.\"; and\n    d. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such derivative works as a whole, provided Your use, reproduction, and distribution of the work otherwise complies with the terms and conditions of this Agreement.\n\n4. Rules of use\n    a. The Materials may be subject to export controls or restrictions in China, the United States or other countries or regions. You shall comply with applicable laws and regulations in your use of the Materials.\n    b. You can not use the Materials or any output therefrom to improve any other large language model (excluding Tongyi Qianwen or derivative works thereof).\n\n5. Intellectual Property\n    a. We retain ownership of all intellectual property rights in and to the Materials and derivatives made by or for Us. Conditioned upon compliance with the terms and conditions of this Agreement, with respect to any derivative works and modifications of the Materials that are made by you, you are and will be the owner of such derivative works and modifications.\n    b. No trademark license is granted to use the trade names, trademarks, service marks, or product names of Us, except as required to fulfill notice requirements under this Agreement or as required for reasonable and customary use in describing and redistributing the Materials.\n    c. If you commence a lawsuit or other proceedings (including a cross-claim or counterclaim in a lawsuit) against Us or any entity alleging that the Materials or any output therefrom, or any part of the foregoing, infringe any intellectual property or other right owned or licensable by you, then all licences granted to you under this Agreement shall terminate as of the date such lawsuit or other proceeding is commenced or brought.\n\n6. Disclaimer of Warranty and Limitation of Liability\n    a. We are not obligated to support, update, provide training for, or develop any further version of the Tongyi Qianwen Materials or to grant any license thereto.\n    b. THE MATERIALS ARE PROVIDED \"AS IS\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, OR FITNESS FOR A PARTICULAR PURPOSE. WE MAKE NO WARRANTY AND ASSUME NO RESPONSIBILITY FOR THE SAFETY OR STABILITY OF THE MATERIALS AND ANY OUTPUT THEREFROM.\n    c. IN NO EVENT SHALL WE BE LIABLE TO YOU FOR ANY DAMAGES, INCLUDING, BUT NOT LIMITED TO ANY DIRECT, OR INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES ARISING FROM YOUR USE OR INABILITY TO USE THE MATERIALS OR ANY OUTPUT OF IT, NO MATTER HOW ITâ€™S CAUSED.\n    d. You will defend, indemnify and hold harmless Us from and against any claim by any third party arising out of or related to your use or distribution of the Materials.\n\n7. Survival and Termination.\n    a. The term of this Agreement shall commence upon your acceptance of this Agreement or access to the Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein.\n    b. We may terminate this Agreement if you breach any of the terms or conditions of this Agreement. Upon termination of this Agreement, you must delete and cease use of the Materials. Sections 6 and 8 shall survive the termination of this Agreement.\n\n8. Governing Law and Jurisdiction.\n    a. This Agreement and any dispute arising out of or relating to it will be governed by the laws of China, without regard to conflict of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement.\n    b. The People's Courts in Hangzhou City shall have exclusive jurisdiction over any dispute arising out of this Agreement.\n\n9. Other Terms and Conditions.\n    a. Any arrangements, understandings, or agreements regarding the Material not stated herein are separate from and independent of the terms and conditions of this Agreement. You shall request a seperate license from Us, if You use the Materials in ways not expressly agreed to in this Agreement.\n    b. We shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\n"
        },
        {
          "name": "ascend-support",
          "type": "tree",
          "content": null
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cli_demo.py",
          "type": "blob",
          "size": 7.3759765625,
          "content": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"A simple command-line interactive chat demo.\"\"\"\n\nimport argparse\nimport os\nimport platform\nimport shutil\nfrom copy import deepcopy\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nfrom transformers.trainer_utils import set_seed\n\nDEFAULT_CKPT_PATH = 'Qwen/Qwen-7B-Chat'\n\n_WELCOME_MSG = '''\\\nWelcome to use Qwen-Chat model, type text to start chat, type :h to show command help.\n(æ¬¢è¿ä½¿ç”¨ Qwen-Chat æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œ:h æ˜¾ç¤ºå‘½ä»¤å¸®åŠ©ã€‚)\n\nNote: This demo is governed by the original license of Qwen.\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, including hate speech, violence, pornography, deception, etc.\n(æ³¨ï¼šæœ¬æ¼”ç¤ºå—Qwençš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)\n'''\n_HELP_MSG = '''\\\nCommands:\n    :help / :h          Show this help message              æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯\n    :exit / :quit / :q  Exit the demo                       é€€å‡ºDemo\n    :clear / :cl        Clear screen                        æ¸…å±\n    :clear-his / :clh   Clear history                       æ¸…é™¤å¯¹è¯å†å²\n    :history / :his     Show history                        æ˜¾ç¤ºå¯¹è¯å†å²\n    :seed               Show current random seed            æ˜¾ç¤ºå½“å‰éšæœºç§å­\n    :seed <N>           Set random seed to <N>              è®¾ç½®éšæœºç§å­\n    :conf               Show current generation config      æ˜¾ç¤ºç”Ÿæˆé…ç½®\n    :conf <key>=<value> Change generation config            ä¿®æ”¹ç”Ÿæˆé…ç½®\n    :reset-conf         Reset generation config             é‡ç½®ç”Ÿæˆé…ç½®\n'''\n\n\ndef _load_model_tokenizer(args):\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"auto\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    ).eval()\n\n    config = GenerationConfig.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True,\n    )\n\n    return model, tokenizer, config\n\n\ndef _gc():\n    import gc\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef _clear_screen():\n    if platform.system() == \"Windows\":\n        os.system(\"cls\")\n    else:\n        os.system(\"clear\")\n\n\ndef _print_history(history):\n    terminal_width = shutil.get_terminal_size()[0]\n    print(f'History ({len(history)})'.center(terminal_width, '='))\n    for index, (query, response) in enumerate(history):\n        print(f'User[{index}]: {query}')\n        print(f'QWen[{index}]: {response}')\n    print('=' * terminal_width)\n\n\ndef _get_input() -> str:\n    while True:\n        try:\n            message = input('User> ').strip()\n        except UnicodeDecodeError:\n            print('[ERROR] Encoding error in input')\n            continue\n        except KeyboardInterrupt:\n            exit(1)\n        if message:\n            return message\n        print('[ERROR] Query is empty')\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='QWen-Chat command-line interactive chat demo.')\n    parser.add_argument(\"-c\", \"--checkpoint-path\", type=str, default=DEFAULT_CKPT_PATH,\n                        help=\"Checkpoint name or path, default to %(default)r\")\n    parser.add_argument(\"-s\", \"--seed\", type=int, default=1234, help=\"Random seed\")\n    parser.add_argument(\"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\")\n    args = parser.parse_args()\n\n    history, response = [], ''\n\n    model, tokenizer, config = _load_model_tokenizer(args)\n    orig_gen_config = deepcopy(model.generation_config)\n\n    _clear_screen()\n    print(_WELCOME_MSG)\n\n    seed = args.seed\n\n    while True:\n        query = _get_input()\n\n        # Process commands.\n        if query.startswith(':'):\n            command_words = query[1:].strip().split()\n            if not command_words:\n                command = ''\n            else:\n                command = command_words[0]\n\n            if command in ['exit', 'quit', 'q']:\n                break\n            elif command in ['clear', 'cl']:\n                _clear_screen()\n                print(_WELCOME_MSG)\n                _gc()\n                continue\n            elif command in ['clear-his', 'clh']:\n                print(f'[INFO] All {len(history)} history cleared')\n                history.clear()\n                _gc()\n                continue\n            elif command in ['help', 'h']:\n                print(_HELP_MSG)\n                continue\n            elif command in ['history', 'his']:\n                _print_history(history)\n                continue\n            elif command in ['seed']:\n                if len(command_words) == 1:\n                    print(f'[INFO] Current random seed: {seed}')\n                    continue\n                else:\n                    new_seed_s = command_words[1]\n                    try:\n                        new_seed = int(new_seed_s)\n                    except ValueError:\n                        print(f'[WARNING] Fail to change random seed: {new_seed_s!r} is not a valid number')\n                    else:\n                        print(f'[INFO] Random seed changed to {new_seed}')\n                        seed = new_seed\n                    continue\n            elif command in ['conf']:\n                if len(command_words) == 1:\n                    print(model.generation_config)\n                else:\n                    for key_value_pairs_str in command_words[1:]:\n                        eq_idx = key_value_pairs_str.find('=')\n                        if eq_idx == -1:\n                            print('[WARNING] format: <key>=<value>')\n                            continue\n                        conf_key, conf_value_str = key_value_pairs_str[:eq_idx], key_value_pairs_str[eq_idx + 1:]\n                        try:\n                            conf_value = eval(conf_value_str)\n                        except Exception as e:\n                            print(e)\n                            continue\n                        else:\n                            print(f'[INFO] Change config: model.generation_config.{conf_key} = {conf_value}')\n                            setattr(model.generation_config, conf_key, conf_value)\n                continue\n            elif command in ['reset-conf']:\n                print('[INFO] Reset generation config')\n                model.generation_config = deepcopy(orig_gen_config)\n                print(model.generation_config)\n                continue\n            else:\n                # As normal query.\n                pass\n\n        # Run chat.\n        set_seed(seed)\n        try:\n            for response in model.chat_stream(tokenizer, query, history=history, generation_config=config):\n                _clear_screen()\n                print(f\"\\nUser: {query}\")\n                print(f\"\\nQwen-Chat: {response}\")\n        except KeyboardInterrupt:\n            print('[WARNING] Generation interrupted')\n            continue\n\n        history.append((query, response))\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "dcu-support",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune.py",
          "type": "blob",
          "size": 12.1982421875,
          "content": "# This code is based on the revised code from fastchat based on tatsu-lab/stanford_alpaca.\n\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport logging\nimport os\nfrom typing import Dict, Optional, List\nimport torch\nfrom torch.utils.data import Dataset\nfrom deepspeed import zero\nfrom deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\nimport transformers\nfrom transformers import Trainer, GPTQConfig, deepspeed\nfrom transformers.trainer_pt_utils import LabelSmoother\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom accelerate.utils import DistributedType\n\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"Qwen/Qwen-7B\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    eval_data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=8192,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n    use_lora: bool = False\n\n\n@dataclass\nclass LoraArguments:\n    lora_r: int = 64\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_target_modules: List[str] = field(\n        default_factory=lambda: [\"c_attn\", \"c_proj\", \"w1\", \"w2\"]\n    )\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n    q_lora: bool = False\n\n\ndef maybe_zero_3(param):\n    if hasattr(param, \"ds_id\"):\n        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\n# Borrowed from peft.utils.get_peft_model_state_dict\ndef get_peft_state_maybe_zero_3(named_params, bias):\n    if bias == \"none\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n    elif bias == \"all\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n    elif bias == \"lora_only\":\n        to_return = {}\n        maybe_lora_bias = {}\n        lora_bias_names = set()\n        for k, t in named_params:\n            if \"lora_\" in k:\n                to_return[k] = t\n                bias_name = k.split(\"lora_\")[0] + \"bias\"\n                lora_bias_names.add(bias_name)\n            elif \"bias\" in k:\n                maybe_lora_bias[k] = t\n        for k, t in maybe_lora_bias:\n            if bias_name in lora_bias_names:\n                to_return[bias_name] = t\n    else:\n        raise NotImplementedError\n    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n    return to_return\n\n\nlocal_rank = None\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str, bias=\"none\"):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    # check if zero3 mode enabled\n    if deepspeed.is_deepspeed_zero3_enabled():\n        state_dict = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    else:\n        if trainer.args.use_lora:\n            state_dict = get_peft_state_maybe_zero_3(\n                trainer.model.named_parameters(), bias\n            )\n        else:\n            state_dict = trainer.model.state_dict()\n    if trainer.args.should_save and trainer.args.local_rank == 0:\n        trainer._save(output_dir, state_dict=state_dict)\n\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n    max_len: int,\n    system_message: str = \"You are a helpful assistant.\"\n) -> Dict:\n    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n\n    im_start = tokenizer.im_start_id\n    im_end = tokenizer.im_end_id\n    nl_tokens = tokenizer('\\n').input_ids\n    _system = tokenizer('system').input_ids + nl_tokens\n    _user = tokenizer('user').input_ids + nl_tokens\n    _assistant = tokenizer('assistant').input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != roles[\"user\"]:\n            source = source[1:]\n\n        input_id, target = [], []\n        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n        input_id += system\n        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n        assert len(input_id) == len(target)\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n                tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n            input_id += _input_id\n            if role == '<|im_start|>user':\n                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n            elif role == '<|im_start|>assistant':\n                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n            else:\n                raise NotImplementedError\n            target += _target\n        assert len(input_id) == len(target)\n        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n        input_ids.append(input_id[:max_len])\n        targets.append(target[:max_len])\n    input_ids = torch.tensor(input_ids, dtype=torch.int)\n    targets = torch.tensor(targets, dtype=torch.int)\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in raw_data]\n        data_dict = preprocess(sources, tokenizer, max_len)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer, self.max_len)\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args, max_len,\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n\n    train_json = json.load(open(data_args.data_path, \"r\"))\n    train_dataset = dataset_cls(train_json, tokenizer=tokenizer, max_len=max_len)\n\n    if data_args.eval_data_path:\n        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer, max_len=max_len)\n    else:\n        eval_dataset = None\n\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n    )\n    (\n        model_args,\n        data_args,\n        training_args,\n        lora_args,\n    ) = parser.parse_args_into_dataclasses()\n\n    # This serves for single-gpu qlora.\n    if getattr(training_args, 'deepspeed', None) and int(os.environ.get(\"WORLD_SIZE\", 1))==1:\n        training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED\n\n    local_rank = training_args.local_rank\n\n    device_map = None\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if lora_args.q_lora:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else \"auto\"\n        if len(training_args.fsdp) > 0 or deepspeed.is_deepspeed_zero3_enabled():\n            logging.warning(\n                \"FSDP or ZeRO3 are incompatible with QLoRA.\"\n            )\n\n    is_chat_model = 'chat' in model_args.model_name_or_path.lower()\n    if (\n            training_args.use_lora\n            and not lora_args.q_lora\n            and deepspeed.is_deepspeed_zero3_enabled()\n            and not is_chat_model\n    ):\n        raise RuntimeError(\"ZeRO3 is incompatible with LoRA when finetuning on base model.\")\n\n    model_load_kwargs = {\n        'low_cpu_mem_usage': not deepspeed.is_deepspeed_zero3_enabled(),\n    }\n\n    # Set RoPE scaling factor\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=True,\n    )\n    config.use_cache = False\n\n    # Load model and tokenizer\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=training_args.cache_dir,\n        device_map=device_map,\n        trust_remote_code=True,\n        quantization_config=GPTQConfig(\n            bits=4, disable_exllama=True\n        )\n        if training_args.use_lora and lora_args.q_lora\n        else None,\n        **model_load_kwargs,\n    )\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n        trust_remote_code=True,\n    )\n    tokenizer.pad_token_id = tokenizer.eod_id\n\n    if training_args.use_lora:\n        if lora_args.q_lora or is_chat_model:\n            modules_to_save = None\n        else:\n            modules_to_save = [\"wte\", \"lm_head\"]\n        lora_config = LoraConfig(\n            r=lora_args.lora_r,\n            lora_alpha=lora_args.lora_alpha,\n            target_modules=lora_args.lora_target_modules,\n            lora_dropout=lora_args.lora_dropout,\n            bias=lora_args.lora_bias,\n            task_type=\"CAUSAL_LM\",\n            modules_to_save=modules_to_save  # This argument serves for adding new tokens.\n        )\n        if lora_args.q_lora:\n            model = prepare_model_for_kbit_training(\n                model, use_gradient_checkpointing=training_args.gradient_checkpointing\n            )\n\n        model = get_peft_model(model, lora_config)\n\n        # Print peft trainable params\n        model.print_trainable_parameters()\n\n        if training_args.gradient_checkpointing:\n            model.enable_input_require_grads()\n\n    # Load data\n    data_module = make_supervised_data_module(\n        tokenizer=tokenizer, data_args=data_args, max_len=training_args.model_max_length\n    )\n\n    # Start trainner\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    trainer.train()\n    trainer.save_state()\n\n    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir, bias=lora_args.lora_bias)\n\n\nif __name__ == \"__main__\":\n    train()\n"
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_api.py",
          "type": "blob",
          "size": 20.6025390625,
          "content": "# Requirement:\n#   pip install \"openai<1.0\"\n# Usage:\n#   python openai_api.py\n# Visit http://localhost:8000/docs for documents.\n\nimport base64\nimport copy\nimport json\nimport time\nfrom argparse import ArgumentParser\nfrom contextlib import asynccontextmanager\nfrom pprint import pprint\nfrom typing import Dict, List, Literal, Optional, Union\n\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom sse_starlette.sse import EventSourceResponse\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n\nclass BasicAuthMiddleware(BaseHTTPMiddleware):\n\n    def __init__(self, app, username: str, password: str):\n        super().__init__(app)\n        self.required_credentials = base64.b64encode(\n            f'{username}:{password}'.encode()).decode()\n\n    async def dispatch(self, request: Request, call_next):\n        authorization: str = request.headers.get('Authorization')\n        if authorization:\n            try:\n                schema, credentials = authorization.split()\n                if credentials == self.required_credentials:\n                    return await call_next(request)\n            except ValueError:\n                pass\n\n        headers = {'WWW-Authenticate': 'Basic'}\n        return Response(status_code=401, headers=headers)\n\n\ndef _gc(forced: bool = False):\n    global args\n    if args.disable_gc and not forced:\n        return\n\n    import gc\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):  # collects GPU memory\n    yield\n    _gc(forced=True)\n\n\napp = FastAPI(lifespan=lifespan)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'],\n    allow_headers=['*'],\n)\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = 'model'\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = 'owner'\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None\n\n\nclass ModelList(BaseModel):\n    object: str = 'list'\n    data: List[ModelCard] = []\n\n\nclass ChatMessage(BaseModel):\n    role: Literal['user', 'assistant', 'system', 'function']\n    content: Optional[str]\n    function_call: Optional[Dict] = None\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal['user', 'assistant', 'system']] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessage]\n    functions: Optional[List[Dict]] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    top_k: Optional[int] = None\n    max_length: Optional[int] = None\n    stream: Optional[bool] = False\n    stop: Optional[List[str]] = None\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: Union[ChatMessage]\n    finish_reason: Literal['stop', 'length', 'function_call']\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal['stop', 'length']]\n\n\nclass ChatCompletionResponse(BaseModel):\n    model: str\n    object: Literal['chat.completion', 'chat.completion.chunk']\n    choices: List[Union[ChatCompletionResponseChoice,\n                        ChatCompletionResponseStreamChoice]]\n    created: Optional[int] = Field(default_factory=lambda: int(time.time()))\n\n\n@app.get('/v1/models', response_model=ModelList)\nasync def list_models():\n    global model_args\n    model_card = ModelCard(id='gpt-3.5-turbo')\n    return ModelList(data=[model_card])\n\n\n# To work around that unpleasant leading-\\n tokenization issue!\ndef add_extra_stop_words(stop_words):\n    if stop_words:\n        _stop_words = []\n        _stop_words.extend(stop_words)\n        for x in stop_words:\n            s = x.lstrip('\\n')\n            if s and (s not in _stop_words):\n                _stop_words.append(s)\n        return _stop_words\n    return stop_words\n\n\ndef trim_stop_words(response, stop_words):\n    if stop_words:\n        for stop in stop_words:\n            idx = response.find(stop)\n            if idx != -1:\n                response = response[:idx]\n    return response\n\n\nTOOL_DESC = (\n    '{name_for_model}: Call this tool to interact with the {name_for_human} API.'\n    ' What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}'\n)\n\nREACT_INSTRUCTION = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n\n{tools_text}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tools_name_text}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\"\"\"\n\n_TEXT_COMPLETION_CMD = object()\n\n\ndef parse_messages(messages, functions):\n    if all(m.role != 'user' for m in messages):\n        raise HTTPException(\n            status_code=400,\n            detail='Invalid request: Expecting at least one user message.',\n        )\n\n    messages = copy.deepcopy(messages)\n    if messages[0].role == 'system':\n        system = messages.pop(0).content.lstrip('\\n').rstrip()\n    else:\n        system = 'You are a helpful assistant.'\n\n    if functions:\n        tools_text = []\n        tools_name_text = []\n        for func_info in functions:\n            name = func_info.get('name', '')\n            name_m = func_info.get('name_for_model', name)\n            name_h = func_info.get('name_for_human', name)\n            desc = func_info.get('description', '')\n            desc_m = func_info.get('description_for_model', desc)\n            tool = TOOL_DESC.format(\n                name_for_model=name_m,\n                name_for_human=name_h,\n                # Hint: You can add the following format requirements in description:\n                #   \"Format the arguments as a JSON object.\"\n                #   \"Enclose the code within triple backticks (`) at the beginning and end of the code.\"\n                description_for_model=desc_m,\n                parameters=json.dumps(func_info['parameters'],\n                                      ensure_ascii=False),\n            )\n            tools_text.append(tool)\n            tools_name_text.append(name_m)\n        tools_text = '\\n\\n'.join(tools_text)\n        tools_name_text = ', '.join(tools_name_text)\n        instruction = (REACT_INSTRUCTION.format(\n            tools_text=tools_text,\n            tools_name_text=tools_name_text,\n        ).lstrip('\\n').rstrip())\n    else:\n        instruction = ''\n\n    messages_with_fncall = messages\n    messages = []\n    for m_idx, m in enumerate(messages_with_fncall):\n        role, content, func_call = m.role, m.content, m.function_call\n        content = content or ''\n        content = content.lstrip('\\n').rstrip()\n        if role == 'function':\n            if (len(messages) == 0) or (messages[-1].role != 'assistant'):\n                raise HTTPException(\n                    status_code=400,\n                    detail=\n                    'Invalid request: Expecting role assistant before role function.',\n                )\n            messages[-1].content += f'\\nObservation: {content}'\n            if m_idx == len(messages_with_fncall) - 1:\n                # add a prefix for text completion\n                messages[-1].content += '\\nThought:'\n        elif role == 'assistant':\n            if len(messages) == 0:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\n                    'Invalid request: Expecting role user before role assistant.',\n                )\n            if func_call is None:\n                if functions:\n                    content = f'Thought: I now know the final answer.\\nFinal Answer: {content}'\n            else:\n                f_name, f_args = func_call['name'], func_call['arguments']\n                if not content.startswith('Thought:'):\n                    content = f'Thought: {content}'\n                content = f'{content}\\nAction: {f_name}\\nAction Input: {f_args}'\n            if messages[-1].role == 'user':\n                messages.append(\n                    ChatMessage(role='assistant',\n                                content=content.lstrip('\\n').rstrip()))\n            else:\n                messages[-1].content += '\\n' + content\n        elif role == 'user':\n            messages.append(\n                ChatMessage(role='user',\n                            content=content.lstrip('\\n').rstrip()))\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=f'Invalid request: Incorrect role {role}.')\n\n    query = _TEXT_COMPLETION_CMD\n    if messages[-1].role == 'user':\n        query = messages[-1].content\n        messages = messages[:-1]\n\n    if len(messages) % 2 != 0:\n        raise HTTPException(status_code=400, detail='Invalid request')\n\n    history = []  # [(Q1, A1), (Q2, A2), ..., (Q_last_turn, A_last_turn)]\n    for i in range(0, len(messages), 2):\n        if messages[i].role == 'user' and messages[i + 1].role == 'assistant':\n            usr_msg = messages[i].content.lstrip('\\n').rstrip()\n            bot_msg = messages[i + 1].content.lstrip('\\n').rstrip()\n            if instruction and (i == len(messages) - 2):\n                usr_msg = f'{instruction}\\n\\nQuestion: {usr_msg}'\n                instruction = ''\n            history.append([usr_msg, bot_msg])\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=\n                'Invalid request: Expecting exactly one user (or function) role before every assistant role.',\n            )\n    if instruction:\n        assert query is not _TEXT_COMPLETION_CMD\n        query = f'{instruction}\\n\\nQuestion: {query}'\n    return query, history, system\n\n\ndef parse_response(response):\n    func_name, func_args = '', ''\n    i = response.find('\\nAction:')\n    j = response.find('\\nAction Input:')\n    k = response.find('\\nObservation:')\n    if 0 <= i < j:  # If the text has `Action` and `Action input`,\n        if k < j:  # but does not contain `Observation`,\n            # then it is likely that `Observation` is omitted by the LLM,\n            # because the output text may have discarded the stop word.\n            response = response.rstrip() + '\\nObservation:'  # Add it back.\n        k = response.find('\\nObservation:')\n        func_name = response[i + len('\\nAction:'):j].strip()\n        func_args = response[j + len('\\nAction Input:'):k].strip()\n\n    if func_name:\n        response = response[:i]\n        t = response.find('Thought: ')\n        if t >= 0:\n            response = response[t + len('Thought: '):]\n        response = response.strip()\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(\n                role='assistant',\n                content=response,\n                function_call={\n                    'name': func_name,\n                    'arguments': func_args\n                },\n            ),\n            finish_reason='function_call',\n        )\n        return choice_data\n\n    z = response.rfind('\\nFinal Answer: ')\n    if z >= 0:\n        response = response[z + len('\\nFinal Answer: '):]\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=ChatMessage(role='assistant', content=response),\n        finish_reason='stop',\n    )\n    return choice_data\n\n\n# completion mode, not chat mode\ndef text_complete_last_message(history, stop_words_ids, gen_kwargs, system):\n    im_start = '<|im_start|>'\n    im_end = '<|im_end|>'\n    prompt = f'{im_start}system\\n{system}{im_end}'\n    for i, (query, response) in enumerate(history):\n        query = query.lstrip('\\n').rstrip()\n        response = response.lstrip('\\n').rstrip()\n        prompt += f'\\n{im_start}user\\n{query}{im_end}'\n        prompt += f'\\n{im_start}assistant\\n{response}{im_end}'\n    prompt = prompt[:-len(im_end)]\n\n    _stop_words_ids = [tokenizer.encode(im_end)]\n    if stop_words_ids:\n        for s in stop_words_ids:\n            _stop_words_ids.append(s)\n    stop_words_ids = _stop_words_ids\n\n    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(model.device)\n    output = model.generate(input_ids,\n                            stop_words_ids=stop_words_ids,\n                            **gen_kwargs).tolist()[0]\n    output = tokenizer.decode(output, errors='ignore')\n    assert output.startswith(prompt)\n    output = output[len(prompt):]\n    output = trim_stop_words(output, ['<|endoftext|>', im_end])\n    print(f'<completion>\\n{prompt}\\n<!-- *** -->\\n{output}\\n</completion>')\n    return output\n\n\n@app.post('/v1/chat/completions', response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    global model, tokenizer\n\n    gen_kwargs = {}\n    if request.top_k is not None:\n        gen_kwargs['top_k'] = request.top_k\n    if request.temperature is not None:\n        if request.temperature < 0.01:\n            gen_kwargs['top_k'] = 1  # greedy decoding\n        else:\n            # Not recommended. Please tune top_p instead.\n            gen_kwargs['temperature'] = request.temperature\n    if request.top_p is not None:\n        gen_kwargs['top_p'] = request.top_p\n\n    stop_words = add_extra_stop_words(request.stop)\n    if request.functions:\n        stop_words = stop_words or []\n        if 'Observation:' not in stop_words:\n            stop_words.append('Observation:')\n\n    query, history, system = parse_messages(request.messages,\n                                            request.functions)\n\n    if request.stream:\n        if request.functions:\n            raise HTTPException(\n                status_code=400,\n                detail=\n                'Invalid request: Function calling is not yet implemented for stream mode.',\n            )\n        generate = predict(query,\n                           history,\n                           request.model,\n                           stop_words,\n                           gen_kwargs,\n                           system=system)\n        return EventSourceResponse(generate, media_type='text/event-stream')\n\n    stop_words_ids = [tokenizer.encode(s)\n                      for s in stop_words] if stop_words else None\n    if query is _TEXT_COMPLETION_CMD:\n        response = text_complete_last_message(history,\n                                              stop_words_ids=stop_words_ids,\n                                              gen_kwargs=gen_kwargs,\n                                              system=system)\n    else:\n        response, _ = model.chat(\n            tokenizer,\n            query,\n            history=history,\n            system=system,\n            stop_words_ids=stop_words_ids,\n            **gen_kwargs,\n        )\n        print('<chat>')\n        pprint(history, indent=2)\n        print(f'{query}\\n<!-- *** -->\\n{response}\\n</chat>')\n    _gc()\n\n    response = trim_stop_words(response, stop_words)\n    if request.functions:\n        choice_data = parse_response(response)\n    else:\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(role='assistant', content=response),\n            finish_reason='stop',\n        )\n    return ChatCompletionResponse(model=request.model,\n                                  choices=[choice_data],\n                                  object='chat.completion')\n\n\ndef _dump_json(data: BaseModel, *args, **kwargs) -> str:\n    try:\n        return data.model_dump_json(*args, **kwargs)\n    except AttributeError:  # pydantic<2.0.0\n        return data.json(*args, **kwargs)  # noqa\n\n\nasync def predict(\n    query: str,\n    history: List[List[str]],\n    model_id: str,\n    stop_words: List[str],\n    gen_kwargs: Dict,\n    system: str,\n):\n    global model, tokenizer\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(role='assistant'), finish_reason=None)\n    chunk = ChatCompletionResponse(model=model_id,\n                                   choices=[choice_data],\n                                   object='chat.completion.chunk')\n    yield '{}'.format(_dump_json(chunk, exclude_unset=True))\n\n    current_length = 0\n    stop_words_ids = [tokenizer.encode(s)\n                      for s in stop_words] if stop_words else None\n\n    delay_token_num = max([len(x) for x in stop_words]) if stop_words_ids else 0\n    response_generator = model.chat_stream(tokenizer,\n                                           query,\n                                           history=history,\n                                           stop_words_ids=stop_words_ids,\n                                           system=system,\n                                           **gen_kwargs)\n    for _new_response in response_generator:\n        if len(_new_response) <= delay_token_num:\n            continue\n        new_response = _new_response[:-delay_token_num] if delay_token_num else _new_response\n\n        if len(new_response) == current_length:\n            continue\n\n        new_text = new_response[current_length:]\n        current_length = len(new_response)\n\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0, delta=DeltaMessage(content=new_text), finish_reason=None)\n        chunk = ChatCompletionResponse(model=model_id,\n                                       choices=[choice_data],\n                                       object='chat.completion.chunk')\n        yield '{}'.format(_dump_json(chunk, exclude_unset=True))\n    \n    if current_length != len(_new_response):\n        # Determine whether to print the delay tokens\n        delayed_text = _new_response[current_length:]\n        new_text = trim_stop_words(delayed_text, stop_words)\n        if len(new_text) > 0:\n            choice_data = ChatCompletionResponseStreamChoice(\n                index=0, delta=DeltaMessage(content=new_text), finish_reason=None)\n            chunk = ChatCompletionResponse(model=model_id,\n                                        choices=[choice_data],\n                                        object='chat.completion.chunk')\n            yield '{}'.format(_dump_json(chunk, exclude_unset=True))\n\n    choice_data = ChatCompletionResponseStreamChoice(index=0,\n                                                     delta=DeltaMessage(),\n                                                     finish_reason='stop')\n    chunk = ChatCompletionResponse(model=model_id,\n                                   choices=[choice_data],\n                                   object='chat.completion.chunk')\n    yield '{}'.format(_dump_json(chunk, exclude_unset=True))\n    yield '[DONE]'\n\n    _gc()\n\n\ndef _get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\n        '-c',\n        '--checkpoint-path',\n        type=str,\n        default='Qwen/Qwen-7B-Chat',\n        help='Checkpoint name or path, default to %(default)r',\n    )\n    parser.add_argument('--api-auth', help='API authentication credentials')\n    parser.add_argument('--cpu-only',\n                        action='store_true',\n                        help='Run demo with CPU only')\n    parser.add_argument('--server-port',\n                        type=int,\n                        default=8000,\n                        help='Demo server port.')\n    parser.add_argument(\n        '--server-name',\n        type=str,\n        default='127.0.0.1',\n        help=\n        'Demo server name. Default: 127.0.0.1, which is only visible from the local computer.'\n        ' If you want other computers to access your server, use 0.0.0.0 instead.',\n    )\n    parser.add_argument(\n        '--disable-gc',\n        action='store_true',\n        help='Disable GC after each response generated.',\n    )\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == '__main__':\n    args = _get_args()\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    if args.api_auth:\n        app.add_middleware(BasicAuthMiddleware,\n                           username=args.api_auth.split(':')[0],\n                           password=args.api_auth.split(':')[1])\n\n    if args.cpu_only:\n        device_map = 'cpu'\n    else:\n        device_map = 'auto'\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    ).eval()\n\n    model.generation_config = GenerationConfig.from_pretrained(\n        args.checkpoint_path,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    uvicorn.run(app, host=args.server_name, port=args.server_port, workers=1)\n"
        },
        {
          "name": "recipes",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0966796875,
          "content": "transformers>=4.32.0,<4.38.0\naccelerate\ntiktoken\neinops\ntransformers_stream_generator==0.0.4\nscipy\n"
        },
        {
          "name": "requirements_web_demo.txt",
          "type": "blob",
          "size": 0.0224609375,
          "content": "gradio<3.42\nmdtex2html\n"
        },
        {
          "name": "run_gptq.py",
          "type": "blob",
          "size": 4.1630859375,
          "content": "import argparse\r\nimport json\r\nfrom typing import Dict\r\nimport logging\r\n\r\nimport torch\r\nimport transformers\r\nfrom transformers import AutoTokenizer\r\nfrom transformers.trainer_pt_utils import LabelSmoother\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\r\n\r\ndef preprocess(\r\n    sources,\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n    max_len: int,\r\n    system_message: str = \"You are a helpful assistant.\"\r\n) -> Dict:\r\n    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\r\n\r\n    im_start = tokenizer.im_start_id\r\n    im_end = tokenizer.im_end_id\r\n    nl_tokens = tokenizer('\\n').input_ids\r\n    _system = tokenizer('system').input_ids + nl_tokens\r\n    _user = tokenizer('user').input_ids + nl_tokens\r\n    _assistant = tokenizer('assistant').input_ids + nl_tokens\r\n\r\n    # Apply prompt templates\r\n    data = []\r\n    # input_ids, targets = [], []\r\n    for i, source in enumerate(sources):\r\n        source = source[\"conversations\"]\r\n        if roles[source[0][\"from\"]] != roles[\"user\"]:\r\n            source = source[1:]\r\n\r\n        input_id, target = [], []\r\n        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\r\n        input_id += system\r\n        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\r\n        assert len(input_id) == len(target)\r\n        for j, sentence in enumerate(source):\r\n            role = roles[sentence[\"from\"]]\r\n            _input_id = tokenizer(role).input_ids + nl_tokens + \\\r\n                tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\r\n            input_id += _input_id\r\n            if role == '<|im_start|>user':\r\n                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\r\n            elif role == '<|im_start|>assistant':\r\n                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\r\n                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\r\n            else:\r\n                raise NotImplementedError\r\n            target += _target\r\n        assert len(input_id) == len(target)\r\n        input_id = torch.tensor(input_id[:max_len], dtype=torch.int)\r\n        target = torch.tensor(target[:max_len], dtype=torch.int)\r\n        data.append(dict(input_ids=input_id, attention_mask=input_id.ne(tokenizer.pad_token_id)))\r\n\r\n    return data\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser(\"Model Quantization using AutoGPTQ\")\r\n    parser.add_argument(\"--model_name_or_path\", type=str, help=\"model path\")\r\n    parser.add_argument(\"--data_path\", type=str, help=\"calibration data path\")\r\n    parser.add_argument(\"--out_path\", type=str, help=\"output path of the quantized model\")\r\n    parser.add_argument(\"--max_len\", type=int, default=8192, help=\"max length of calibration data\")\r\n    parser.add_argument(\"--bits\", type=int, default=4, help=\"the bits of quantized model. 4 indicates int4 models.\")\r\n    parser.add_argument(\"--group-size\", type=int, default=128, help=\"the group size of quantized model\")\r\n    args = parser.parse_args()\r\n    \r\n    quantize_config = BaseQuantizeConfig(\r\n        bits=args.bits,\r\n        group_size=args.group_size,\r\n        damp_percent=0.01,\r\n        desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\r\n        static_groups=False,\r\n        sym=True,\r\n        true_sequential=True,\r\n        model_name_or_path=None,\r\n        model_file_base_name=\"model\"\r\n    )\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)\r\n    tokenizer.pad_token_id = tokenizer.eod_id\r\n    data = preprocess(json.load(open(args.data_path)), tokenizer, args.max_len)\r\n\r\n    model = AutoGPTQForCausalLM.from_pretrained(args.model_name_or_path, quantize_config, device_map=\"auto\", trust_remote_code=True)\r\n\r\n    logging.basicConfig(\r\n        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\r\n    )\r\n    model.quantize(data, cache_examples_on_gpu=False)\r\n\r\n    model.save_quantized(args.out_path, use_safetensors=True)\r\n    tokenizer.save_pretrained(args.out_path)\r\n"
        },
        {
          "name": "tech_memo.md",
          "type": "blob",
          "size": 21.1982421875,
          "content": "# Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts)\n\nLarge language models have recently attracted an extremely large amount of\nattention.\nThe boom of [ChatGPT](https://openai.com/blog/chatgpt) rocketed the development of artificial general intelligence and indicates that large language models compress world knowledge into neural networks, and the alignment to human cognition can lead to powerful conversational agents that can provide assistance by interacting with human users.\nNow, the latest version of ChatGPT based on [GPT-4](https://arxiv.org/abs/2303.08774) demonstrates tremendously exciting performance across unlimited capabilities, say, language understanding, logical reasoning, planning, etc., and its incorporation with external tools, including tools and models, releases the power of an agent capable of understanding instructions, executing code, using tools, and so on, to reach the objectives set up by human users.\n\nThese significant progresses indicate the importance of large language models as _the foundation of AI services_.\n\nWe are happy to release the 7B-parameter models of our large pretrained model series Qwen (abbr. Tongyi Qianwen), Qwen-7B.\nThis release includes model weights and codes for pretrained and human-aligned language models of 7B parameters:\n\n- `Qwen-7B` is the pretrained language model, and `Qwen-7B-Chat` is fine-tuned to align with human intent.\n- `Qwen-7B` is pretrained on over 2.2 trillion tokens with a context length of 2048. On the series of benchmarks we tested, Qwen-7B generally performs better than existing open models of similar scales and appears to be on par with some of the larger models.\n- `Qwen-7B-Chat` is fine-tuned on curated data, including not only task-oriented data but also specific security- and service-oriented data, which seems insufficient in existing open models.\n- Example codes for fine-tuning, evaluation, and inference are included. There are also guides on long-context and tool use in inference.\n\n**Goal of release**:\nWe believe that while the recent waves of releases of LLMs may have deepened our understanding of model behaviors under standard regimes, it is yet to be revealed how the accompanied techniques of nowadays LLMs, such as 1) quantization and fine-tuning after quantization, 2) training-free long-context inference, and 3) fine-tuning with service-oriented data, including search and tool uses, affect the models as a whole.\nThe open release of Qwen-7B marks our first step towards fully understanding the real-world application of such techniques.\nIt is our hope that it will enable the community to analyze and continue to improve the safety of those models, striving to establish responsible development and deployment of LLMs.\n\n> **Disclaimer**:\n> We must note that even though the weights and codes are released in an open manner and commercial use is not prohibited, similar to other pretrained language models, Qwen-7B comes with potential risks influenced by complex factors, including but not limited to over-diversified, inaccurate, or misleading generation.\n> Developers and stakeholders should perform their own red teaming and provide related security measures before deployment, and they must abide by and comply with local governance and regulations.\n> In no event shall the authors be held liable for any claim, damages, or other liability arising from the use of the released weights or codes.\n\nThe remainder of this document describes our pretraining and fine-tuning methodology.\n\n## Pretraining\n\nQwen-7B is a transformer-based decoder-only language model with an architecture similar to the [LLaMA](https://github.com/facebookresearch/llama) series of models.\nIt is pretrained on over 2.2 trillion tokens with 2048 context length from publicly available data, covering general and professional fields with a focus on the English and Chinese languages.\n\n### Data\n\n**Pretraining data**:\nOur training data includes a mix of data from publicly available sources, consisting mainly of web documents and code files.\nBesides, the data are multilingual, with most of them in English and Chinese.\nWe made an effort and employed an ensemble of models to exclude data of low quality or deemed unfit for pretraining, such as NSFW content.\nFor math reasoning, we include RFT data from [gsm8k-ScRel](https://github.com/OFA-Sys/gsm8k-ScRel).\nThe final data underwent global fuzzy deduplication.\nThe mix of pretraining corpora has been optimized through numerous ablation experiments.\n\n**Tokenization**:\nCompared to the current mainstream open models based on Chinese and English vocabularies, we use a vocabulary of 151,851 tokens.\nIt first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary.\nIt segments numbers by single digits and calls the [tiktoken](https://github.com/openai/tiktoken) tokenizer library for efficient tokenization.\nAfter tokenization, the data amounts to over 2.2 trillion tokens.\n\n<figure>\n    <img src=\"assets/tokenizer.png\"\n         alt=\"Tokenization efficiency\"\n         width=\"1200px\">\n    <figcaption>We randomly selected 1 million document corpora of each language to test and compare the encoding compression rates of different models (with XLM-R, which supports 100 languages, as the base value 1, not shown in the figure). As can be seen, while ensuring the efficient decoding of Chinese, English, and code, Qwen-7B also achieves a high compression rate for many other languages (such as th, he, ar, ko, vi, ja, tr, id, pl, ru, nl, pt, it, de, es, fr etc.), equipping the model with strong scalability as well as high training and inference efficiency in these languages.</figcaption>\n</figure>\n\n### Model\n\n**Model architecture**:\nQwen-7B is built with architecture similar to LLaMA.\nThe following are the main differences from the standard transformer: 1) using untied embedding, 2) using rotary positional embedding, 3) no biases except for QKV in attention, 4) RMSNorm instead of LayerNorm, 5) SwiGLU instead of ReLU, and 6) adopting flash attention to accelerate training.\nThe model has 32 layers, the embedding dimension is 4096, and the number of attention heads is 32.\n\n**Training details**:\nThe model is trained using the AdamW optimizer, with $\\beta_1=0.9, \\beta_2=0.95, \\epsilon=10^{-6}$.\nThe sequence length is 2048, and the batch size is 2048, which means each optimization step accumulates over 4 million tokens.\nWe use a cosine learning rate schedule, with a warm-up of 2000 steps, a peak learning rate of $3 \\times 10^{-4}$, and a minimum learning rate of 10% of the peak learning rate.\nWe use a weight decay of 0.1 and gradient clipping of 1.0.\nThe training adopts mixed precision training with `bfloat16`.\n\n\n### Evaluation\n\nWe report results of Qwen-7B on standard benchmarks.\n\n#### World knowledge\n\n[C-Eval](https://arxiv.org/abs/2305.08322) is a common evaluation benchmark for testing the common-sense capability of pretrained models in Chinese. It covers 52 subjects in four major directions: humanities, social sciences, STEM, and other specialties. According to standard practice, we use the development set samples as the source of few-shot prompts to evaluate the 5-shot validation set and test set accuracy of the Qwen-7B pretrained model.\n\nThe accuracy comparison of the Qwen-7B model and other models on the C-Eval validation set is as follows:\n\n| Model       |  Average |\n| :---------- | -------: |\n| Alpaca-7B   |     28.9 |\n| Vicuna-7B   |     31.2 |\n| ChatGLM-6B  |     37.1 |\n| Baichuan-7B |     42.7 |\n| ChatGLM2-6B |     50.9 |\n| InternLM-7B |     53.4 |\n| ChatGPT     |     53.5 |\n| Claude-v1.3 |     55.5 |\n| **Qwen-7B** | **60.8** |\n\nThe performance comparison of the Qwen-7B pretrained model and other models on the C-Eval test set is shown in the following table:\n\n| Model                   | Avg.     | Avg. (Hard) | STEM | Social Sciences | Humanities | Others |\n| :---------------------- | -------- | ----------: | ---: | --------------: | ---------: | -----: |\n| ChatGLM-6B              | 38.9     |        29.2 | 33.3 |            48.3 |       41.3 |   38.0 |\n| Chinese-Alpaca-Plus-13B | 41.5     |        30.5 | 36.6 |            49.7 |       43.1 |   41.2 |\n| Baichuan-7B             | 42.8     |        31.5 | 38.2 |            52.0 |       46.2 |   39.3 |\n| WestlakeLM-19B          | 44.6     |        34.9 | 41.6 |            51.0 |       44.3 |   44.5 |\n| AndesLM-13B             | 46.0     |        29.7 | 38.1 |            61.0 |       51.0 |   41.9 |\n| BatGPT-15B-sirius       | 47.0     |        31.9 | 42.7 |            57.5 |       48.6 |   43.6 |\n| ChatGLM2-6B             | 51.7     |        37.1 | 48.6 |            60.5 |       51.3 |   49.8 |\n| InternLM-7B             | 52.8     |        37.1 | 48.0 |            67.4 |       55.4 |   45.8 |\n| Baichuan-13B            | 53.6     |        36.7 | 47.0 |            66.8 |       57.3 |   49.8 |\n| Claude-v1.3             | 54.2     |        39.0 | 51.9 |            61.7 |       52.1 |   53.7 |\n| ChatGPT                 | 54.4     |        41.4 | 52.9 |            61.8 |       50.9 |   53.6 |\n| **Qwen-7B**             | **59.6** |        41.0 | 52.8 |            74.1 |       63.1 |   55.2 |\n\nAs can be seen, Qwen-7B achieves the best performance out of all existing models of similar scale and even surpasses larger-scale models.\n\nMMLU is currently one of the most recognized benchmarks for evaluating English comprehension abilities, covering 57 subtasks across different academic fields and difficulty levels. The MMLU 5-shot accuracy performance of the Qwen-7B is shown in the following table:\n\n| Model        |  Average | STEM | Social Sciences | Humanities | Others |\n| :----------- | -------: | ---: | --------------: | ---------: | -----: |\n| LLaMA-7B     |     35.1 | 30.5 |            38.3 |       34.0 |   38.1 |\n| Baichuan-7B  |     42.3 | 35.6 |            48.9 |       38.4 |   48.1 |\n| LLaMA2-7B    |     45.3 | 36.4 |            51.2 |       42.9 |   52.2 |\n| LLaMA-13B    |     46.9 | 35.8 |            53.8 |       45.0 |   53.3 |\n| ChatGLM2-6B  |     47.9 | 41.2 |            54.4 |       43.7 |   54.5 |\n| InternLM-7B  |     51.0 |    - |               - |          - |      - |\n| Baichuan-13B |     51.6 | 41.6 |            60.9 |       47.4 |   58.5 |\n| LLaMA2-13B   |     54.8 | 44.1 |            62.6 |       52.8 |   61.1 |\n| ChatGLM2-12B |     56.2 | 48.2 |            65.1 |       52.6 |   60.9 |\n| **Qwen-7B**  | **56.7** | 47.6 |            65.9 |       51.5 |   64.7 |\n\nIn terms of English, Qwen-7B also surpasses other similar open pretrained models, and is competitive when compared to larger versions of other models.\n\n#### Coding\n\nWe compared the code capabilities of pretrained models on [HumanEval](https://github.com/openai/human-eval), and the results are as follows:\n\n| Model        |   Pass@1 |\n| :----------- | -------: |\n| Baichuan-7B  |      9.2 |\n| ChatGLM2-6B  |      9.2 |\n| InternLM-7B  |     10.4 |\n| LLaMA-7B     |     10.5 |\n| LLaMA2-7B    |     12.8 |\n| Baichuan-13B |     12.8 |\n| LLaMA-13B    |     15.8 |\n| MPT-7B       |     18.3 |\n| LLaMA2-13B   |     18.3 |\n| **Qwen-7B**  | **24.4** |\n\n#### Math\n\nWe compared the math capabilities of pretrained models on [GSM8K](https://github.com/openai/grade-school-math) (8-shot), and the results are as follows:\n\n| Model        | Accuracy |\n| :----------- | -------: |\n| MPT-7B       |      6.8 |\n| Falcon-7B    |      6.8 |\n| Baichuan-7B  |      9.7 |\n| LLaMA-7B     |     11.0 |\n| LLaMA2-7B    |     14.6 |\n| LLaMA-13B    |     17.8 |\n| Baichuan-13B |     26.6 |\n| LLaMA2-13B   |     28.7 |\n| InternLM-7B  |     31.2 |\n| ChatGLM2-6B  |     32.4 |\n| ChatGLM2-12B |     40.9 |\n| **Qwen-7B**  | **51.6** |\n\n#### Natural language processing\n\nWe compared the translation capabilities of pre-trained models on WMT22 zh-en and en-zh (5-shot BLEU), and the results are as follows:\n\n| Model       |  Average |    zh-en |    en-zh |\n| :---------- | -------: | -------: | -------: |\n| InternLM-7B |     11.8 |      9.0 |     14.5 |\n| LLaMA-7B    |     12.7 |     16.7 |      8.7 |\n| LLaMA-13B   |     15.8 |     19.5 |     12.0 |\n| LLaMA2-7B   |     19.9 |     21.9 |     17.9 |\n| Bloom-7B    |     20.3 |     19.1 |     21.4 |\n| LLaMA2-13B  |     23.3 |     22.4 |     24.2 |\n| PolyLM-13B  |     23.6 |     20.2 |     27.0 |\n| Baichuan-7B |     24.6 |     22.6 |     26.6 |\n| **Qwen-7B** | **27.5** | **24.3** | **30.6** |\n\n#### Long-context inference\n\nWe include support for training-free long-context inference based on ntk-aware interpolation, LogN attention scaling, and local window attention.\nThe context can be expanded from 2048 to over 8192.\nThe following are the test results on arXiv in terms of perplexity (PPL).\n\n<table>\n\t<tr>\n        <th rowspan=\"2\">Model</th><th colspan=\"5\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th><th align=\"center\">16384</th>\n    </tr>\n    <tr>\n        <td>Qwen-7B</td><td align=\"right\"><b>4.23</b></td><td align=\"right\"><b>3.78</b></td><td align=\"right\">39.35</td><td align=\"right\">469.81</td><td align=\"right\">2645.09</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk</td><td align=\"right\"><b>4.23</b></td><td align=\"right\"><b>3.78</b></td><td align=\"right\">3.59</td><td align=\"right\">3.66</td><td align=\"right\">5.71</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn</td><td align=\"right\"><b>4.23</b></td><td align=\"right\"><b>3.78</b></td><td align=\"right\"><b>3.58</b></td><td align=\"right\">3.56</td><td align=\"right\">4.62</td>\n    </tr>\n    <tr>\n        <td>+ dynamic_ntk + logn + local_attn</td><td align=\"right\"><b>4.23</b></td><td align=\"right\"><b>3.78</b></td><td align=\"right\"><b>3.58</b></td><td align=\"right\"><b>3.49</b></td><td align=\"right\"><b>4.32</b></td>\n    </tr>\n</table>\n\n## Fine-tuning\n\n`Qwen-7B-Chat` embodies our practice in alignment with human intents, ensuring internalized safety, and building intelligent agents for services.\n\n### Data\n\n**Alignment data**:\nThe data includes common instruction-style conversations, and security- and service-oriented data, which involves substantial annotation efforts.\nInstruction data covers broad abilities, such as writing, question answering, brainstorming and planning, content understanding, summarization, natural language processing, and coding.\nSecurity data tries to prevent the model from generating harmful and inappropriate content.\nService data tries to enhance the model with specific conversation patterns that can be parsed to invoke and incorporate external systems.\n\n**Data formatting**:\nSince the data consists of conversation turns, we arrange them into texts using the [ChatML](https://github.com/openai/openai-python/blob/main/chatml.md) format, which is a meta language that can describe both the metadata (e.g., roles) and the content of a turn.\nCurrently, existing roles include system, user, and assistant.\n\n### Model\n\n**Training details**:\nThe causal language modeling objective is used to fine-tune the model, except for the tokens in the content of user's turns.\nThe model is trained using the AdamW optimizer, with $\\beta_1=0.9, \\beta_2=0.95, \\epsilon=10^{-6}$.\nThe sequence length is limited to 2048, and the batch size is 128.\nThe model is trained for 4000 steps, and over the first 1430 steps, the learning rate is warmed up to $1 \\times 10^{-5}$.\nWe use weight decay of 0.1, dropout of 0.1, and gradient clipping of 1.0.\n\n### Evaluation\n\nEvaluation of human-aligned models is non-trivial and often non-standardized, since such models often target specific applications.\nWe evaluate Qwen-7B-Chat from multiple perspectives.\n\n#### World knowledge\n\nAs fine-tuning uses a much smaller dataset than pretraining and humans' understanding of world knowledge may be limited, we also evaluate the world knowledge of Qwen-7B-Chat using C-Eval and MMLU in a zero-shot and generative manner.\n\nWe demonstrate the zero-shot accuracy of Qwen-7B-Chat on the C-Eval validation set.\n\n| Model                   | Avg. Acc. |\n| :---------------------- | --------: |\n| LLaMA2-7B-Chat          |      31.9 |\n| LLaMA2-13B-Chat         |      40.6 |\n| Chinese-Alpaca-2-7B     |      41.3 |\n| Chinese-Alpaca-Plus-13B |      43.3 |\n| Baichuan-13B-Chat       |      50.4 |\n| ChatGLM2-6B-Chat        |      50.7 |\n| InternLM-7B-Chat        |      53.2 |\n| **Qwen-7B-Chat**        |  **54.2** |\n\nThe zero-shot accuracy of Qwen-7B-Chat on C-Eval testing set is provided below\n\n| Model                   |     Avg. | STEM | Social Sciences | Humanities | Others |\n| :---------------------- | -------: | ---: | --------------: | ---------: | -----: |\n| Chinese-Alpaca-Plus-13B |     41.5 | 36.6 |            49.7 |       43.1 |   41.2 |\n| Chinese-Alpaca-2-7B     |     40.3 |    - |               - |          - |      - |\n| ChatGLM2-6B-Chat        |     50.1 | 46.4 |            60.4 |       50.6 |   46.9 |\n| Baichuan-13B-Chat       |     51.5 | 43.7 |            64.6 |       56.2 |   49.2 |\n| **Qwen-7B-Chat**        | **54.6** | 47.8 |            67.6 |       59.3 |   50.6 |\n\nCompared with other models with comparable model sizes, the human-aligned Qwen-7B-Chat performs well in C-Eval accuracy.\n\nThe zero-shot accuracy of Qwen-7B-Chat on MMLU is provided below.\nThe performance of Qwen-7B-Chat is still on top among other human-aligned models with comparable size.\n\n| Model             | Avg. Acc. |\n| :---------------- | --------: |\n| ChatGLM2-6B-Chat  |      45.5 |\n| LLaMA2-7B-Chat    |      47.0 |\n| InternLM-7B-Chat  |      50.8 |\n| Baichuan-13B-Chat |      52.1 |\n| ChatGLM2-12B-Chat |      52.1 |\n| **Qwen-7B-Chat**  |  **53.9** |\n\n#### Coding\n\nThe zero-shot Pass@1 of Qwen-7B-Chat on [HumanEval](https://github.com/openai/human-eval) is demonstrated below\n\n| Model             |   Pass@1 |\n| :---------------- | -------: |\n| LLaMA2-7B-Chat    |     12.2 |\n| InternLM-7B-Chat  |     14.0 |\n| Baichuan-13B-Chat |     16.5 |\n| LLaMA2-13B-Chat   |     18.9 |\n| **Qwen-7B-Chat**  | **24.4** |\n\n#### Math\n\nThe accuracy of Qwen-7B-Chat on GSM8K is shown below\n\n| Model             | Zero-shot Acc. | 4-shot Acc. |\n| :---------------- | -------------: | ----------: |\n| ChatGLM2-6B-Chat  |              - |        28.0 |\n| LLaMA2-7B-Chat    |           20.4 |        28.2 |\n| LLaMA2-13B-Chat   |           29.4 |        36.7 |\n| InternLM-7B-Chat  |           32.6 |        34.5 |\n| Baichuan-13B-Chat |              - |        36.3 |\n| ChatGLM2-12B-Chat |              - |        38.1 |\n| **Qwen-7B-Chat**  |       **41.1** |    **43.5** |\n\n#### Service\n\nLLMs have shown capability in coordinating multiple external systems to achieve the given instructions, which creates new opportunities in traditional online services, the most notable being web search.\n\nQwen supports calling plugins/tools/APIs through [ReAct Prompting](https://arxiv.org/abs/2210.03629).\nReAct is also one of the main approaches used by the [LangChain](https://python.langchain.com/) framework.\nFor how to write and use prompts for ReAct Prompting, please refer to [the ReAct examples](examples/react_prompt.md).\nIn our evaluation [benchmark](eval/EVALUATION.md) for assessing tool usage capabilities, Qwen's performance is as follows:\n\n| Model       | Tool Selection (Acc.â†‘)      | Tool Input (Rouge-Lâ†‘)      | False Positive Errorâ†“      |\n| :---------- | --------------------------: | -------------------------: | -------------------------: |\n| GPT-4       |                         95% |                   **0.90** |                      15.0% |\n| GPT-3.5     |                         85% |                       0.88 |                      75.0% |\n| **Qwen-7B** |                     **99%** |                       0.89 |                   **9.7%** |\n\n> The plugins that appear in the evaluation set do not appear in the training set of Qwen.\n> This benchmark evaluates the accuracy of the model in selecting the correct plugin from multiple candidate plugins, the rationality of the parameters passed into the plugin, and the false positive rate.\n> False Positive: Incorrectly invoking a plugin when it should not have been called when responding to a query.\n\nQwen also has the capability to be used as a [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents).\nIts performance on the benchmark provided by HuggingFace is as follows:\n\n| Model           | Tool Selectionâ†‘      | Tool Usedâ†‘      | Codeâ†‘      |\n| :-------------- | -------------------: | --------------: | ---------: |\n| GPT-4           |           **100.00** |      **100.00** |  **97.41** |\n| GPT-3.5         |                95.37 |           96.30 |      87.04 |\n| StarCoder-15.5B |                87.04 |           87.96 |      68.89 |\n| **Qwen-7B**     |                90.74 |           92.59 |      74.07 |\n\n## Conclusion\n\nIn this document, we describe Qwen-7B, including a pretrained model and a human-aligned model.\nThese models have demonstrated exciting performance compared to existing open models of similar or even larger scales.\nAs part of our ongoing commitment to the concept of Model as a Service, the release also includes practical pieces such as long context inference and external system integration, which we hope would facilitate developers realizing their own ideas and concepts.\nWe believe that the open release of Qwen-7B models would further our understanding of variables and techniques introduced in realistic settings and help to drive progress in this important area together with the community.\n"
        },
        {
          "name": "tokenization_note.md",
          "type": "blob",
          "size": 12.3779296875,
          "content": "# Tokenization\r\n\r\nQwen-7B uses BPE tokenization on UTF-8 bytes using the `tiktoken` package.\r\nThere are two types of tokens in Qwen-7B, i.e., the regular tokens (of type `bytes`) in BPE and the special/control tokens (of type `str`).\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True)\r\n```\r\n\r\n## Regular tokens\r\n\r\nThe regular tokens are BPE tokens learned from byte sequences of texts encoded using the UTF-8 encoding.\r\nWhile this allows tokenization of all texts and no unknown token exists, it may fall back to using single bytes when tokenizing uncommon texts.\r\nYou may encounter UTF-8 decoding errors and as the errors are default to `replace`, thus the replacement character (ï¿½) in incomplete generation.\r\nYou can change this behavior by passing `errors=\"ignore\"` to the `decode` function for once or to the `from_pretrained` function forever.\r\nFor more options of `errors`, please refer to [the Python documentation](https://docs.python.org/3/library/stdtypes.html#bytes.decode).\r\n\r\n```python\r\n>>> tokenizer.decode([51461])\r\n' ï¿½'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461])\r\n[b' \\xe6\\xa0']\r\n\r\n>>> b' \\xe6\\xa0'.decode(\"utf-8\", errors='replace')\r\n' ï¿½'\r\n\r\n>>> tokenizer.decode([51461, 117])\r\n' æ ¹'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461, 117])\r\n[b' \\xe6\\xa0', b'\\xb9']\r\n\r\n>>> b' \\xe6\\xa0\\xb9'.decode(\"utf-8\", errors='replace')\r\n' æ ¹'\r\n```\r\n\r\nThe mapping from regular tokens (in `bytes`) to its ID can be retrieved from `tokenizer.get_vocab()`.\r\nWe do not support or recommended adding regular tokens to the vocabulary.\r\n\r\n## Special tokens\r\n\r\nThe special tokens signify special functions to the model, e.g., reaching the end of a document.\r\nIn theory, they do not exist in the input texts and only appear after the input texts are processed.\r\nTheir surface forms, e.g., `<|endoftext|>` for the end of a document, are only meant for ease of reference.\r\nCurrently, used special tokens are `<|endoftext|>` in Qwen-7B, and `<|endoftext|>`, `<|im_start|>`, and `<|im_end|>` in Qwen-7B-Chat, which means they have determined meanings to the corresponding model, and should not be used otherwise.\r\nFor other purposes, we keep extra special tokens from `<|extra_0|>` to `<|extra_204|>`, and you can use them as you wish.\r\nThe mapping from surface forms of the special tokens (in `str`) to its ID can be retrieved from `tokenizer.special_tokens`.\r\n\r\nThe concepts of `bos`, `eos`, `unk`, `pad`, `mask`, `sep` and such are not appliable to our pretrained models (Qwen-7B and Qwen-7B-Chat).\r\nThe `pad` token, however, is a different story, as in theory, the model never sees or computes this token, so you may use any known token.\r\nBut to be safe, we limit the value of special tokens specified in the initialization of the tokenizer to the known special tokens.\r\nYou may specify special tokens in fine-tuning or in any other frameworks that necessitate them like this\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True, pad_token='<|endoftext|>')\r\n```\r\n\r\n> WARNING: For our pretrained models, setting `bos`, `eos`, `unk`, and such makes no sense.\r\n> Unknown behavior may be introduced if you set them without fine-tuning that designates their meanings to the model.\r\n> Especially, you should not use `<|endoftext|>` as `eos`, unless you are sure that the end of a sentence and the end of a document, which may contain many sentences, are the same in your scenario.\r\n\r\n## Injection attack prevention\r\n\r\nAs special tokens are different from regular tokens, what will happen if the surface forms of a control token appear in the input texts?\r\nFor example, note that a piece of text like this\r\n\r\n```\r\nprint(\"<|endoftext|>\")\r\n```\r\n\r\nshould be tokenized as\r\n\r\n```\r\nids:[1350, 9639, 91, 8691, 723, 427, 91, 82598]\r\ntokens: [b'print', b'(\"<', b'|', b'endo', b'ft', b'ext', b'|', b'>\")']\r\n```\r\n\r\nnot\r\n\r\n```\r\nids: [1350, 445, 151643, 899]\r\ntokens: [b'print', b'(\"', '<|endoftext|>', b'\")']\r\n```\r\n\r\nOur default used to be the correct one, that is, treating the surface forms of special tokens just like regular texts, and special tokens should be taken cared of by developers after tokenization of the texts.\r\nHowever, this conflicts with (albeit unsafe) practice in the community, and adds another step for developers to reuse their wheels.\r\n\r\nThe default behavior has been changed to parse the surface forms of all the known special tokens as special tokens.\r\nTo enable injection prevention, pass `allowed_special=set()` to the calls of the tokenizer:\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=set())\r\n{'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\nYou can control the behavior in a fine-grained manner by passing a set of `str` as `allowed_special`\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'})\r\n{'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\nYou can also make the tokenizer raise errors if the surface forms of certain special tokens are encountered in the input texts by passing a collection of `str` as `disallowed_special`\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'}, disallowed_special=('<|extra_0|>', ))\r\n...\r\nValueError: Encountered text corresponding to disallowed special token '<|extra_0|>'.\r\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|extra_0|>', ...}`.\r\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|extra_0|>'})`.\r\nTo disable this check for all special tokens, pass `disallowed_special=()`.\r\n```\r\n\r\nFor more information on `allowed_special` and `disallowed_special`, please refer to [the `tiktoken` documentation](https://github.com/openai/tiktoken/blob/095924e02c85617df6889698d94515f91666c7ea/tiktoken/core.py#L75).\r\n\r\nThe new default is the same as\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=\"all\", disallowed_special=())\r\n{'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\r\n```\r\n\r\n## Vocabulary Expansion\r\n\r\n> WARNING: Read carefully, be aware of what you are doing, and use at your own risk. \r\n> There are certain caveats regarding how your vocabulary is produced.\r\n\r\nThe tokenizer of Qwen models are based on BPE and you cannot directly expand the vocabulary by adding words to the vocabulary. \r\nThe intermediate merges are needed for tokenization.\r\nPlease follow the steps to obtain such information.\r\n\r\n1. Prepare a plain text file `qwen_extra_vocab.txt`, where each line contains a token and its frequency separated by `\\t`. \r\n\r\n   An example is given below:\r\n   ```\r\n   æˆ‘æ˜¯ä¸€åªçŒ«\t20\r\n   ä½ æ˜¯ä¸€åªçŒ«\t10\r\n   ä»–æ˜¯ä¸€åªçŒ«\t5\r\n   ä¸€åª\t200\r\n   ä¸€åªçŒ«\t100\r\n   å¤¸å¼ çš„ æ¯”å–»æ‰‹æ³•\t20\r\n   ```\r\n   The frequencies are needed to compute the BPE.\r\n\r\n   \r\n\r\n2. Prepare the base vocabulary file, e.g., `qwen.tiktoken`, and determine the start index for new tokens.\r\n   \r\n   There are 151,643 regular tokens and 208 control tokens in the vocabulary for Qwen models. \r\n   For simplicity, the start index can be set as 151,851, which is the default value. \r\n   You can, of course, override the many inactive control tokens, but you will need to modify the tokenizer code. \r\n\r\n3. Run the following command:\r\n   ```\r\n   python add_merges.py qwen.tiktoken qwen_extra.tiktoken qwen_extra_vocab.txt\r\n   ```\r\n   `add_merges.py` can be found [here](examples/add_merges.py).\r\n   It will learn the new merges based on the provided `qwen_extra_vocab.txt`. \r\n   The new tokens and their indices will be stored in `qwen_extra.tiktoken`. \r\n   Modify the paths as you wish.\r\n\r\n   It is a pure Python implementation, so please expect it to be slow if you are adding a lot of words.\r\n\r\n   Please note that not all words can be added due to pre-tokenization. \r\n   You will get warnings if you try to add such word:\r\n   ```\r\n   WARNING - å¤¸å¼ çš„ æ¯”å–»æ‰‹æ³• would be pre-tokenized to ['å¤¸å¼ çš„', ' æ¯”å–»æ‰‹æ³•'], and thus cannot be added to vocabulary\r\n   WARNING - word ä¸€åª is already a token b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', skipping\r\n   INFO - number of existing merges: 151643\r\n   INFO - number of words for expanding: 4\r\n   DEBUG - (b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', b'\\xe7\\x8c\\xab') (ä¸€åªçŒ«) is selected as the next merge with freq 100\r\n   DEBUG - (b'\\xe5\\x8f\\xaa', b'\\xe7\\x8c\\xab') (åªçŒ«) is selected as the next merge with freq 35\r\n   DEBUG - (b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80', b'\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (æ˜¯ä¸€åªçŒ«) is selected as the next merge with freq 35\r\n   DEBUG - (b'\\xe6\\x88\\x91', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (æˆ‘æ˜¯ä¸€åªçŒ«) is selected as the next merge with freq 20\r\n   DEBUG - (b'\\xe4\\xbd\\xa0', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (ä½ æ˜¯ä¸€åªçŒ«) is selected as the next merge with freq 10\r\n   DEBUG - (b'\\xe4\\xbb\\x96', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (ä»–æ˜¯ä¸€åªçŒ«) is selected as the next merge with freq 5\r\n   INFO - number of newly learned merges: 6\r\n   ```\r\n\r\nThe `qwen_extra.tiktoken` will contain the following lines:\r\n```\r\n5LiA5Y+q54yr 151851\r\n5Y+q54yr 151852\r\n5piv5LiA5Y+q54yr 151853\r\n5oiR5piv5LiA5Y+q54yr 151854\r\n5L2g5piv5LiA5Y+q54yr 151855\r\n5LuW5piv5LiA5Y+q54yr 151856\r\n```\r\n\r\nYou may use the file as follows in your code:\r\n``` python\r\nfrom transformers import AutoTokenizer\r\n\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True, extra_vocab_file=\"qwen_extra.tiktoken\")\r\n\r\n>>> len(tokenizer)\r\n151857\r\n\r\n>>> tokenizer(\"æˆ‘æ˜¯ä¸€åªçŒ«\")\r\n{'input_ids': [151854], 'token_type_ids': [0], 'attention_mask': [1]}\r\n```\r\nNote: You need the latest tokenizer code, i.e., after 2023-10-08, to use the `extra_vocab_file` argument.\r\nOtherwise, you need to manually append `qwen.tiktoken` (of which path varies with your configuration) with the content from `qwen_extra.tiktoken`.\r\n\r\nCertainly, you will need to finetune the model for the new tokens to work.\r\n\r\n\r\n### Caveats\r\n\r\n\r\nThe tokenizer of Qwen operates directly on UTF-8 byte sequences, unlike others, e.g., SentencePiece that operates on Unicode codepoints/characters and falls back to UTF-8 byte sequences for the unknown (IIRC). \r\nThe thing is if the frequencies are computed on limited data, the Unicode codepoint boundary may not be correctly recognized.\r\nIn theory, it could be a problem for fine-tuned models using the expanded vocabulary with limited data.\r\n\r\nFor example, it could happen that `b'\\x80\\xe5'` might be merged first for the UTF-8 byte sequence `b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa'` of the string `ä¸€åª`, across the Unicode codepoint of `ä¸€` (`b'\\xe4\\xb8\\x80'`) and `åª` (`b'\\xe5\\x8f\\xaa'`).\r\nNormally, this would work just fine for known tokens, but for actually unknown words, unusual merges may happen, which may not be well understood for the pre-trained model.\r\n\r\nOur advice is that to be safe, you should gather the Unicode codepoints from all the words you need to add, and also add them to the file with frequencies higher than the sum of the frequencies of the corresponding words.\r\nBut since Qwen has most of the Chinese words, it could be okay to just add the Chinese words alone.\r\n\r\nFor curious minds, you will also notice that in the given example, `ä¸€åª` is a token and `åªçŒ«` is also learned as a new token. \r\nThe reason is that `æ˜¯ä¸€` is also a token in Qwen and has higher merging priority than `ä¸€åª`, such that the merging path for `æ˜¯|ä¸€|åª|çŒ«` is `æ˜¯ä¸€|åª|çŒ« -> æ˜¯ä¸€|åªçŒ« -> æ˜¯ä¸€åªçŒ«` (omitting the UTF-8 byte merges).\r\n\r\nThis is the characteristic for plain BPE: it is based solely on distribution, meaning it does not have knowledge of which bytes can form a valid Unicode codepoint, character, or meaningful word.\r\n\r\nThe byproduct is that text may be sub-tokenized differently in different contexts, even for words containing only ASCII characters.\r\n```python\r\n>>> tokenizer.tokenize(\"Panda\")\r\n[b'P', b'anda']\r\n\r\n>>> tokenizer.tokenize(\" Panda\")\r\n[b' Panda']\r\n\r\n>>> tokenizer.tokenize(\"Pandas\")\r\n[b'P', b'andas']\r\n\r\n>>> tokenizer.tokenize(\" Pandas\")\r\n[b' Pand', b'as']\r\n```\r\nThis simply suggests that those combinations occur more frequently in the data.\r\nIf you have vast amount of training data, it should not be a problem."
        },
        {
          "name": "tokenization_note_ja.md",
          "type": "blob",
          "size": 7.2763671875,
          "content": "# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\r\n\r\nQwen-7B ã¯ `tiktoken` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½¿ç”¨ã—ã¦ã€UTF-8 ãƒã‚¤ãƒˆã‚’ BPE ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚\r\nQwen-7B ã«ã¯ 2 ç¨®é¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚BPE ã®é€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³ (`bytes` å‹) ã¨ç‰¹æ®Š/åˆ¶å¾¡ãƒˆãƒ¼ã‚¯ãƒ³ (`str` å‹) ã§ã™ã€‚\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True)\r\n```\r\n\r\n## é€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³\r\n\r\né€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€UTF-8 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚¤ãƒˆåˆ—ã‹ã‚‰å­¦ç¿’ã—ãŸ BPE ãƒˆãƒ¼ã‚¯ãƒ³ã§ã™ã€‚\r\nã“ã‚Œã«ã‚ˆã£ã¦ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã€æœªçŸ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯å­˜åœ¨ã—ã¾ã›ã‚“ãŒã€ä¸€èˆ¬çš„ã§ãªã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã¨ãã«ã‚·ãƒ³ã‚°ãƒ«ãƒã‚¤ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\r\nUTF-8 ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã«é­é‡ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã€ãã®ã‚¨ãƒ©ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ `replace` ã§ã‚ã‚‹ãŸã‚ã€ä¸å®Œå…¨ãªç”Ÿæˆã§ã¯ç½®æ›æ–‡å­— (ï¿½) ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\r\nã“ã®å‹•ä½œã¯ `errors=\"ignore\"` ã‚’ `decode` é–¢æ•°ã«æ¸¡ã™ã“ã¨ã§å¤‰æ›´ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\r\n`errors` ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã¯ã€[Python ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.python.org/3/library/stdtypes.html#bytes.decode) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\r\n\r\n```python\r\n>>> tokenizer.decode([51461])\r\n' ï¿½'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461])\r\n[b' \\xe6\\xa0']\r\n\r\n>>> b' \\xe6\\xa0'.decode(\"utf-8\", errors='replace')\r\n' ï¿½'\r\n\r\n>>> tokenizer.decode([51461, 117])\r\n' æ ¹'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461, 117])\r\n[b' \\xe6\\xa0', b'\\xb9']\r\n\r\n>>> b' \\xe6\\xa0\\xb9'.decode(\"utf-8\", errors='replace')\r\n' æ ¹'\r\n```\r\n\r\né€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³ (`bytes` å˜ä½) ã‹ã‚‰ãã® ID ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã¯ `tokenizer.get_vocab()` ã‹ã‚‰å–å¾—ã§ãã¾ã™ã€‚\r\né€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èªå½™ã«è¿½åŠ ã™ã‚‹ã“ã¨ã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã—ã€æ¨å¥¨ã‚‚ã—ã¦ã„ã¾ã›ã‚“ã€‚\r\n\r\n## ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³\r\n\r\nç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€ä¾‹ãˆã°æ–‡æ›¸ã®æœ€å¾Œã«åˆ°é”ã™ã‚‹ãªã©ã€ãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦ç‰¹åˆ¥ãªæ©Ÿèƒ½ã‚’æ„å‘³ã—ã¾ã™ã€‚\r\nç†è«–çš„ã«ã¯ã€ã“ã‚Œã‚‰ã¯å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã«ã¯å­˜åœ¨ã›ãšã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒå‡¦ç†ã•ã‚ŒãŸå¾Œã«ã®ã¿ç¾ã‚Œã¾ã™ã€‚\r\nä¾‹ãˆã°ã€æ–‡æ›¸ã®çµ‚ã‚ã‚Šã‚’è¡¨ã™ `<|endoftext|>` ã®ã‚ˆã†ãªè¡¨é¢çš„ãªå½¢ã¯ã€å‚ç…§ã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã ã‘ã®ã‚‚ã®ã§ã‚ã‚‹ã€‚\r\nç¾åœ¨ã€Qwen-7B ã§ã¯ `<|endoftext|>` ãŒã€Qwen-7B-Chat ã§ã¯ `<|endoftext|>`, `<|im_start|>`, `<|im_end|>` ãŒç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚\r\nä»–ã®ç›®çš„ã®ãŸã‚ã«ã€`<|extra_0|>` ã‹ã‚‰ `<|extra_204|>` ã¾ã§ã®ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿æŒã—ã¦ã„ã¾ã™ã€‚\r\nç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨é¢å½¢å¼ (`str` å†…) ã‹ã‚‰ ID ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã¯ `tokenizer.special_tokens` ã‹ã‚‰å–å¾—ã§ãã¾ã™ã€‚\r\n\r\n`bos`ã€`eos`ã€`unk`ã€`pad`ã€`mask`ã€`sep` ãªã©ã®æ¦‚å¿µã¯å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆQwen-7B ã¨ Qwen-7B-Chatï¼‰ã«ã¯é©ç”¨ã§ãã¾ã›ã‚“ã€‚\r\nã—ã‹ã—ã€`pad` ãƒˆãƒ¼ã‚¯ãƒ³ã¯è©±ãŒåˆ¥ã§ã™ã€‚ç†è«–çš„ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã“ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãŸã‚Šè¨ˆç®—ã—ãŸã‚Šã™ã‚‹ã“ã¨ã¯ãªã„ã®ã§ã€æ—¢çŸ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\r\nã—ã‹ã—ã€å®‰å…¨ã®ãŸã‚ã«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–ã§æŒ‡å®šã™ã‚‹ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã®å€¤ã¯ã€æ—¢çŸ¥ã®ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã«é™å®šã—ã¾ã™ã€‚\r\nå¾®èª¿æ•´ã‚„ãã®ä»–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¿…è¦ã¨ã™ã‚‹å ´åˆã¯ã€æ¬¡ã®ã‚ˆã†ã«æŒ‡å®šã§ãã¾ã™\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True, pad_token='<|endoftext|>')\r\n```\r\n\r\n> è­¦å‘Š: ç§ãŸã¡ãŒäº‹å‰ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã¯ã€`bos`, `eos`, `unk` ãªã©ã‚’è¨­å®šã—ã¦ã‚‚æ„å‘³ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\r\n> ç‰¹ã«ã€`<<endoftext|>` ã‚’ `eos` ã®ã‚ˆã†ã«ä½¿ã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\r\n> ç‰¹ã« `<|endoftext|>` ã‚’ `eos` ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯ã€æ–‡æœ«ã¨æ–‡æœ«ãŒåŒã˜ã§ã‚ã‚‹ã¨ç¢ºä¿¡ã§ãã‚‹å ´åˆã‚’é™¤ãã€é¿ã‘ã‚‹ã¹ãã§ã™ã€‚\r\n\r\n## ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒã®é˜²æ­¢\r\n\r\nç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯é€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã¯ç•°ãªã‚‹ãŸã‚ã€ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨é¢å½¢ãŒå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã«ç¾ã‚Œã‚‹ã¨ã©ã†ãªã‚‹ã§ã—ã‚‡ã†ã‹ï¼Ÿ\r\nä¾‹ãˆã°ã€æ¬¡ã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹ã¨ã—ã¾ã™\r\n\r\n```\r\nprint(\"<|endoftext|>\")\r\n```\r\n\r\nã“ã‚Œã¯æ¬¡ã®ã‚ˆã†ã«ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\r\n\r\n```\r\nids:[1350, 9639, 91, 8691, 723, 427, 91, 82598]\r\ntokens: [b'print', b'(\"<', b'|', b'endo', b'ft', b'ext', b'|', b'>\")']\r\n```\r\n\r\nã“ã¡ã‚‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“\r\n\r\n```\r\nids: [1350, 445, 151643, 899]\r\ntokens: [b'print', b'(\"', '<|endoftext|>', b'\")']\r\n```\r\n\r\nã¤ã¾ã‚Šã€ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨é¢å½¢ã¯é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¨åŒã˜ã‚ˆã†ã«æ‰±ã„ã€ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å¾Œã«é–‹ç™ºè€…ãŒå‡¦ç†ã™ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚\r\nã—ã‹ã—ã€ã“ã‚Œã¯ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ãŠã‘ã‚‹ï¼ˆå®‰å…¨ã§ã¯ãªã„ã¨ã¯ã„ãˆï¼‰æ…£ç¿’ã«æŠµè§¦ã—ã€é–‹ç™ºè€…ãŒè»Šè¼ªã‚’å†åˆ©ç”¨ã™ã‚‹ãŸã‚ã®æ–°ãŸãªã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\r\n\r\nãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‹•ä½œã¯ã€ã™ã¹ã¦ã®æ—¢çŸ¥ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨é¢å½¢ã‚’ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è§£æã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚\r\nã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³é˜²æ­¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‘¼ã³å‡ºã—ã« `allowed_special=set()` ã‚’æ¸¡ã—ã¾ã™:\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=set())\r\n{'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\n`str` ã®ã‚»ãƒƒãƒˆã‚’ `allowed_special` ã¨ã—ã¦æ¸¡ã™ã“ã¨ã§ã€ãã‚ç´°ã‹ãå‹•ä½œã‚’åˆ¶å¾¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'})\r\n{'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\n`str` ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ `disallowed_special` ã¨ã—ã¦æ¸¡ã™ã“ã¨ã§ã€ç‰¹å®šã®ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨å½¢å¼ãŒå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã§é­é‡ã—ãŸå ´åˆã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'}, disallowed_special=('<|extra_0|>', ))\r\n...\r\nValueError: Encountered text corresponding to disallowed special token '<|extra_0|>'.\r\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|extra_0|>', ...}`.\r\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|extra_0|>'})`.\r\nTo disable this check for all special tokens, pass `disallowed_special=()`.\r\n```\r\n\r\n`allowed_special` ã¨ `disallowed_special` ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[`tiktoken` ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/openai/tiktoken/blob/095924e02c85617df6889698d94515f91666c7ea/tiktoken/core.py#L75)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\r\n\r\næ–°ã—ã„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä»¥ä¸‹ã®é€šã‚Š\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=\"all\", disallowed_special=())\r\n{'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\r\n```\r\n\r\n"
        },
        {
          "name": "tokenization_note_zh.md",
          "type": "blob",
          "size": 12.689453125,
          "content": "# Tokenization\r\n\r\n> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚\r\n\r\nQwen-7Bé‡‡ç”¨UTF-8å­—èŠ‚çº§åˆ«çš„BPE tokenizationæ–¹å¼ï¼Œå¹¶ä¾èµ–`tiktoken`è¿™ä¸€é«˜æ•ˆçš„è½¯ä»¶åŒ…æ‰§è¡Œåˆ†è¯ã€‚\r\nQwen-7Bä¸­æœ‰ä¸¤ç±»tokenï¼Œå³æºäºBPEã€`bytes`ç±»å‹çš„æ™®é€štokenå’Œç‰¹æ®ŠæŒ‡å®šã€`str`ç±»å‹çš„ç‰¹æ®Štokenã€‚\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True)\r\n```\r\n\r\n## æ™®é€štoken\r\n\r\næ™®é€štokenæºäºBPEï¼Œæ˜¯åœ¨UTF-8ç¼–ç çš„æ–‡æœ¬å­—èŠ‚åºåˆ—ä¸Šå­¦ä¹ å¾—åˆ°çš„ã€‚\r\nå°½ç®¡åŸºäºå­—èŠ‚åºåˆ—çš„æ–¹å¼ä¿è¯äº†æ‰€æœ‰æ–‡æœ¬å‡å¯è¢«tokenizeä¸”æ²¡æœ‰æœªç™»å½•tokené—®é¢˜ï¼Œä½†å¤„ç†ç½•è§æ–‡æœ¬æ—¶æœ‰å¯èƒ½å›é€€åˆ°å­—èŠ‚çº§åˆ«çš„ç¼–ç ã€‚\r\nç”±äºä»å­—èŠ‚åºåˆ—è§£ç ä¸ºæ–‡æœ¬æ—¶ï¼Œ`errors`å‚æ•°è®¾ä¸º`replace`ï¼Œå¤„ç†ä¸å®Œæ•´çš„tokenåºåˆ—å¯èƒ½ä¼šé‡åˆ°UTF-8è§£ç é”™è¯¯ï¼Œè¡¨è±¡æ˜¯ç”Ÿæˆä¸­åŒ…å«â€œæ›¿æ¢å­—ç¬¦â€(ï¿½)ã€‚\r\nè¿™ä¸€è¡Œä¸ºå¯ä»¥é€šè¿‡å°†`errors`å‚æ•°è®¾ä¸º`ignore`æ¥è§„é¿ã€‚\r\nä¸€æ¬¡æ€§ä¿®æ”¹å¯ä»¥ä¼ å…¥tokenizerçš„`decode`å‡½æ•°ï¼ŒæŒä¹…æ€§ä¿®æ”¹å¯ä»¥ä¼ å…¥tokenizerçš„åˆå§‹åŒ–å‡½æ•°ï¼Œè¯·æ³¨æ„`decode`çš„é…ç½®ä¼˜å…ˆçº§æ›´é«˜ã€‚\r\n`errors`çš„å¯é€‰å€¼ï¼Œè¯·å‚é˜…[Pythonæ–‡æ¡£](https://docs.python.org/3/library/stdtypes.html#bytes.decode).\r\n\r\n```python\r\n>>> tokenizer.decode([51461])\r\n' ï¿½'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461])\r\n[b' \\xe6\\xa0']\r\n\r\n>>> b' \\xe6\\xa0'.decode(\"utf-8\", errors='replace')\r\n' ï¿½'\r\n\r\n>>> tokenizer.decode([51461, 117])\r\n' æ ¹'\r\n\r\n>>> tokenizer.convert_ids_to_tokens([51461, 117])\r\n[b' \\xe6\\xa0', b'\\xb9']\r\n\r\n>>> b' \\xe6\\xa0\\xb9'.decode(\"utf-8\", errors='replace')\r\n' æ ¹'\r\n```\r\n\r\n`bytes`ç±»å‹çš„æ™®é€štokenåˆ°idçš„æ˜ å°„å¯ä»¥é€šè¿‡`tokenizer.get_vocab()`è·å–ã€‚\r\nå°šä¸æ”¯æŒä¹Ÿä¸æ¨èå‘tokenizerå¢åŠ æ™®é€štokenã€‚\r\n\r\n## ç‰¹æ®Štoken\r\n\r\nç‰¹æ®Štokenç”¨ä»¥ç»™æ¨¡å‹ä¼ é€’ç‰¹æ®Šä¿¡å·ï¼Œå¦‚åˆ°è¾¾æ–‡æœ¬æœ«å°¾ã€‚\r\nç†è®ºä¸Šï¼Œè¾“å…¥æ–‡æœ¬ä¸­ä¸åŒ…å«ç‰¹æ®Štokenï¼Œå®ƒä»¬ä»…åœ¨tokenizationåç”±å¼€å‘è€…æ‰‹åŠ¨åŠ å…¥ã€‚\r\nç‰¹æ®Štokençš„å­—é¢è¡¨è¾¾ï¼Œå¦‚è¡¨ç¤ºæ–‡æœ¬ç»“æŸçš„`<|endoftext|>`ï¼Œä»…ä¾¿äºæŒ‡ä»£ç‰¹æ®Štokenï¼Œä¸æ„å‘³ç€å®ƒä»¬åœ¨è¾“å…¥æ–‡æœ¬ç©ºé—´ä¸­ã€‚\r\nç›®å‰ï¼Œè®­ç»ƒä¸­ä½¿ç”¨çš„ã€å·²ç»æœ‰å›ºå®šå«ä¹‰çš„ã€ä¸åº”åšå®ƒç”¨çš„ç‰¹æ®Štokenï¼ŒQwen-7Bä¸­æœ‰`<|endoftext|>`ï¼ŒQwen-7B-Chatä¸­æœ‰`<|endoftext|>`ã€`<|im_start|>`ä»¥åŠ`<|im_end|>`ã€‚\r\nä½†è¯è¡¨ä¸­ä¹Ÿç•™æœ‰ä¾›æ‰©å±•çš„ç‰¹æ®Štokenä½ï¼Œå¯ç”¨`<|extra_0|>`åˆ°`<|extra_204|>`æ¥æŒ‡ä»£ã€‚\r\n`str`ç±»å‹çš„ç‰¹æ®Štokenå­—é¢è¡¨è¾¾åˆ°idçš„æ˜ å°„ï¼Œå¯ä»¥é€šè¿‡`tokenizer.special_tokens`è·å–ã€‚\r\n\r\nå¯¹äºæä¾›çš„æ¨¡å‹å‚æ•°(Qwen-7Bå’ŒQwen-7B-Chat)è€Œè¨€ï¼Œè¯¸å¦‚`bos`ã€`eos`ã€`unk`ã€`pad`ã€`mask`ã€`sep`ç­‰çš„ç‰¹æ®Štokençš„æ¦‚å¿µå¹¶ä¸é€‚ç”¨ã€‚\r\nç‰¹ä¾‹æ˜¯`pad`ï¼Œç”±äºè¿™ä¸ªtokenç†è®ºä¸Šå¹¶ä¸å‚ä¸æ¨¡å‹è®¡ç®—ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ä»»æ„tokenè¡¨è¾¾è¿™ä¸€æ¦‚å¿µã€‚\r\nä½†ä¿é™©èµ·è§ï¼Œç›®å‰å¯åœ¨tokenizeråˆå§‹åŒ–æ—¶è®¾å®šçš„ç‰¹æ®Štokenï¼Œä»…å¯ä½¿ç”¨å·²çŸ¥çš„ç‰¹æ®Štokenå­—é¢è¡¨è¾¾ï¼Œå³`<|endoftext|>`ã€`<|im_start|>`ã€`<|im_end|>`å’Œ`<|extra_0|>`åˆ°`<|extra_204|>`ã€‚\r\nå¯¹äºå¾®è°ƒæˆ–è€…å…¶å®ƒéœ€è¦è¿™äº›tokenæ‰èƒ½è¿è¡Œçš„æ¡†æ¶ï¼Œå¯ä»¥å¦‚ä¸‹é…ç½®\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True, pad_token='<|endoftext|>')\r\n```\r\n\r\n> æ³¨æ„: å¯¹äºæä¾›çš„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè®¾ç½®è¯¸å¦‚`bos`ã€`eos`ã€`unk`ä¹‹ç±»çš„æ²¡æœ‰æ„ä¹‰ï¼Œå³æ¨¡å‹ä¸éœ€è¦è¿™äº›æ¦‚å¿µã€‚\r\n> å¦‚æœè®¾ç½®äº†è¿™äº›tokenï¼Œä½†æ²¡æœ‰ç›¸åº”çš„å¾®è°ƒè¿™äº›tokenä»¥è®©æ¨¡å‹ç†è§£å…¶å«ä¹‰ï¼ŒæœªçŸ¥è¡Œä¸ºå¯èƒ½è¢«è§¦å‘ã€‚\r\n> ç‰¹åˆ«æ—¶ï¼Œä¸åº”æ··æ·†`<|endoftext|>`å’Œ`eos`çš„æ¦‚å¿µï¼Œé™¤éåº”ç”¨åœºæ™¯ä¸­å®ƒä»¬çš„å®é™…å«ä¹‰æ˜¯ä¸€è‡´çš„ï¼Œå³å¥å­æœ«å°¾ç­‰ä»·äºæ–‡æœ¬æœ«å°¾ã€‚\r\n\r\n**æ³¨å…¥æ”»å‡»é˜²å¾¡**\r\n\r\nç”±äºç‰¹æ®Štokenå’Œæ™®é€štokenæ¦‚å¿µä¸Šçš„å·®å¼‚ï¼Œå¦‚æœè¾“å…¥æ–‡æœ¬ä¸­å«æœ‰ç‰¹æ®Štokençš„å­—é¢è¡¨è¾¾è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ\r\nä»¥ä¸‹é¢æ–‡æœ¬ä¸ºä¾‹\r\n\r\n```\r\nprint(\"<|endoftext|>\")\r\n```\r\n\r\nå…¶æ­£ç¡®çš„tokenizationä¸º\r\n\r\n```\r\nids:[1350, 9639, 91, 8691, 723, 427, 91, 82598]\r\ntokens: [b'print', b'(\"<', b'|', b'endo', b'ft', b'ext', b'|', b'>\")']\r\n```\r\n\r\nä¸æ˜¯\r\n\r\n```\r\nids: [1350, 445, 151643, 899]\r\ntokens: [b'print', b'(\"', '<|endoftext|>', b'\")']\r\n```\r\n\r\né»˜è®¤è¡Œä¸ºæ›¾æ˜¯æ­£ç¡®çš„ï¼Œå³è¾“å…¥æ–‡æœ¬ä¸­ä»»ä½•å­—ç¬¦ä¸€å¾‹æŒ‰æ™®é€štokenå¤„ç†ï¼Œç‰¹æ®Štokenåº”ç”±å¼€å‘è€…åœ¨tokenizationäººå·¥å¤„ç†ã€‚\r\nç„¶åï¼Œè¿™ä¸ç¤¾åŒºä¸­çš„å®è·µä¼¼æœ‰å·®å¼‚ï¼Œä¸ºå¼€å‘è€…å¤ç”¨ä»£ç å¢åŠ äº†é¢å¤–é€‚é…æ­¥éª¤ã€‚\r\n\r\né»˜è®¤è¡Œä¸ºå·²è¢«è°ƒæ•´ä¸ºä»è¾“å…¥æ–‡æœ¬ä¸­è§£æç‰¹æ®Štokençš„å­—é¢è¡¨è¾¾ã€‚\r\nå¦‚éœ€å¯ç”¨æ³¨å…¥æ”»å‡»é˜²å¾¡ï¼Œè¯·ä¼ å…¥å‚æ•°`allowed_special=set()`ï¼š\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=set())\r\n{'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\nè¿™ä¸€è¡Œä¸ºå¯ä»¥æ›´ç²¾ç»†çš„è°ƒæ§ï¼Œå°†`allowed_special`è®¾è®¡ä¸º`str`çš„é›†åˆå³å¯ï¼š\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'})\r\n{'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\r\n```\r\n\r\nå¦‚æœå¸Œæœ›è¾“å…¥ä¸­é‡åˆ°ç‰¹æ®Štokençš„å­—é¢è¡¨è¾¾æ—¶ï¼Œè·å¾—æ›´ç›´æ¥çš„æé†’ï¼Œé€šè¿‡é…ç½®`disallowed_special`å¯ä»¥è®©tokenizerç›´æ¥è§¦å‘å¼‚å¸¸ï¼š\r\n\r\n```python\r\n>>> tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'}, disallowed_special=('<|extra_0|>', ))\r\n...\r\nValueError: Encountered text corresponding to disallowed special token '<|extra_0|>'.\r\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|extra_0|>', ...}`.\r\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|extra_0|>'})`.\r\nTo disable this check for all special tokens, pass `disallowed_special=()`.\r\n```\r\n\r\næ›´å¤šå…³äº`allowed_special`å’Œ`disallowed_special`çš„ä¿¡æ¯, è¯·å‚é˜…[`tiktoken`ä»£ç ](https://github.com/openai/tiktoken/blob/095924e02c85617df6889698d94515f91666c7ea/tiktoken/core.py#L75).\r\n\r\næ–°çš„é»˜è®¤è¡Œä¸ºä¸ä»¥ä¸‹è®¾å®šç­‰ä»·\r\n\r\n```python\r\n>>> tokenizer('print(\"<|endoftext|>\")', allowed_special=\"all\", disallowed_special=())\r\n{'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\r\n```\r\n\r\n## è¯è¡¨æ‰©å±•\r\n\r\n> ç‰¹åˆ«æé†’ï¼šè¯·ä»”ç»†é˜…è¯»æœ¬éƒ¨åˆ†çš„è¯´æ˜ï¼Œç†è§£æ¯ä¸€æ­¥æ“ä½œï¼Œå¹¶æ‰¿æ‹…å¯èƒ½çš„åæœã€‚\r\n> ç”±äºè¯è¡¨æ‰©å±•éƒ¨åˆ†ç”±æ‚¨æä¾›ï¼Œäº§å‡ºæ–¹å¼çš„å·®å¼‚å¯èƒ½å¯¼è‡´ç‰¹å®šçš„ä¸å…¼å®¹æƒ…å†µï¼Œè¯·å®¡æ…æ“ä½œã€‚\r\n\r\nQwenç³»åˆ—æ¨¡å‹çš„tokenizeråŸºäºBPEæ–¹æ¡ˆæå–æ–‡æœ¬ä¸­çš„tokenã€‚\r\nä»UTF-8ç¼–ç çš„å­—èŠ‚å¼€å§‹ï¼ˆæ¯ä¸ªå­—èŠ‚éƒ½å¯ä»¥æ˜¯ä¸€ä¸ªtokenï¼‰ï¼Œä¸¤ä¸¤tokenåˆå¹¶æˆä¸ºæ–°tokenï¼Œç›´è‡³ä¸èƒ½å†åˆå¹¶å‡ºæ–°çš„tokenä¸ºæ­¢ã€‚\r\nç”±äºè¯è¡¨åŒæ—¶è¿˜è®°å½•äº†tokençš„åˆå¹¶æ–¹å¼ï¼Œç›´æ¥å‘è¯è¡¨ä¸­æ·»åŠ è¯å¯èƒ½å¯¹Qwençš„tokenizerå¹¶ä¸é€‚ç”¨ï¼Œå³é€šè¿‡å·²æœ‰çš„tokenå¯èƒ½åˆå¹¶ä¸å‡ºæ¥æ‚¨æ·»åŠ è¯ã€‚\r\n\r\nå› è€Œï¼Œè¯·å‚ç…§ä»¥ä¸‹æ­¥éª¤è·å¾—åˆå¹¶ä¿¡æ¯ï¼š\r\n\r\n1. å‡†å¤‡ä¸€ä¸ªçº¯æ–‡æœ¬æ–‡ä»¶ï¼Œä¾‹å¦‚åä¸º`qwen_extra_vocab.txt`ï¼Œæ¯è¡Œä¸€ä¸ªå¾…æ·»åŠ çš„è¯å’Œå®ƒçš„é¢‘ç‡ï¼Œä¸­é—´ç”¨åˆ¶è¡¨ç¬¦`\\t`åˆ†éš”ã€‚\r\n\r\n   ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ–‡ä»¶çš„ä¾‹å­ï¼š\r\n   ```\r\n   æˆ‘æ˜¯ä¸€åªçŒ«\t20\r\n   ä½ æ˜¯ä¸€åªçŒ«\t10\r\n   ä»–æ˜¯ä¸€åªçŒ«\t5\r\n   ä¸€åª\t200\r\n   ä¸€åªçŒ«\t100\r\n   å¤¸å¼ çš„ æ¯”å–»æ‰‹æ³•\t20  \r\n   ```\r\n   é¢‘ç‡æ˜¯å¿…éœ€çš„ï¼Œç”¨æ¥è®¡ç®—åˆå¹¶çš„ä¼˜å…ˆçº§ã€‚\r\n\r\n2. å‡†å¤‡åŸºç¡€çš„è¯è¡¨æ–‡ä»¶ï¼Œä¾‹å¦‚`qwen.tiktoken`ï¼Œå¹¶ç¡®è®¤æ–°åŠ å…¥tokençš„èµ·å§‹ç´¢å¼•ã€‚\r\n\r\n   Qwenæ¨¡å‹è¯è¡¨ä¸­æœ‰151,643ä¸ªæ™®é€štokenï¼Œæœ‰208ä¸ªç‰¹æ®Štokenã€‚\r\n   ç®€å•èµ·è§ï¼Œèµ·å§‹ç´¢å¼•å¯ä»¥è®¾ç½®ä¸º151,851ï¼ˆé»˜è®¤å€¼ï¼‰ã€‚\r\n   æ‚¨å¯ä»¥è¦†å†™ä¸èµ·æ•ˆçš„ç‰¹æ®Štokenï¼Œä½†æ‚¨éœ€è¦ç›¸åº”çš„ä¿®æ”¹tokenizerä»£ç ã€‚\r\n\r\n3. è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\r\n   ```\r\n   python add_merges.py qwen.tiktoken qwen_extra.tiktoken qwen_extra_vocab.txt\r\n   ```\r\n   `add_merges.py`ä»£ç åœ¨[GitHubå­˜å‚¨åº“](examples/add_merges.py)ä¸­ã€‚\r\n   åŸºäºæä¾›çš„`qwen_extra_vocab.txt`ï¼Œè¯¥è„šæœ¬å°†å­¦ä¹ æ–°çš„tokenåˆå¹¶æ–¹å¼ã€‚\r\n   æ–°tokenåŠå…¶ç´¢å¼•å°†å­˜å‚¨åœ¨`qwen_extra.tiktoken`æ–‡ä»¶ä¸­ã€‚\r\n   æ‚¨å¯ä»¥è§†æƒ…å†µä¿®æ”¹æœ‰å…³è·¯å¾„ã€‚\r\n\r\n   ç”±äºæ˜¯çº¯Pythonå®ç°ï¼Œå¦‚æœæ‚¨æ·»åŠ äº†éå¸¸å¤šçš„è¯ï¼Œé¢„æœŸä¼šèŠ±è´¹è¾ƒå¤šæ—¶é—´ã€‚\r\n\r\n   è¯·æ³¨æ„ï¼Œç”±äºé¢„åˆ‡åˆ†ï¼Œæœ‰äº›è¯æ˜¯æ— æ³•ä½œä¸ºtokenåŠ å…¥çš„ã€‚\r\n   å¦‚æœæ‚¨æ·»åŠ äº†è¿™äº›è¯ï¼Œæ‚¨ä¼šæ”¶åˆ°è­¦å‘Šï¼š\r\n   ```\r\n   WARNING - å¤¸å¼ çš„ æ¯”å–»æ‰‹æ³• would be pre-tokenized to ['å¤¸å¼ çš„', ' æ¯”å–»æ‰‹æ³•'], and thus cannot be added to vocabulary\r\n   WARNING - word ä¸€åª is already a token b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', skipping\r\n   INFO - number of existing merges: 151643\r\n   INFO - number of words for expanding: 4\r\n   DEBUG - (b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', b'\\xe7\\x8c\\xab') (ä¸€åªçŒ«) is selected as the next merge with freq 100\r\n   DEBUG - (b'\\xe5\\x8f\\xaa', b'\\xe7\\x8c\\xab') (åªçŒ«) is selected as the next merge with freq 35\r\n   DEBUG - (b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80', b'\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (æ˜¯ä¸€åªçŒ«) is selected as the next merge with freq 35\r\n   DEBUG - (b'\\xe6\\x88\\x91', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (æˆ‘æ˜¯ä¸€åªçŒ«) is selected as the next merge with freq 20\r\n   DEBUG - (b'\\xe4\\xbd\\xa0', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (ä½ æ˜¯ä¸€åªçŒ«) is selected as the next merge with freq 10\r\n   DEBUG - (b'\\xe4\\xbb\\x96', b'\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe5\\x8f\\xaa\\xe7\\x8c\\xab') (ä»–æ˜¯ä¸€åªçŒ«) is selected as the next merge with freq 5\r\n   INFO - number of newly learned merges: 6\r\n   ```\r\n\r\n`qwen_extra.tiktoken`ä¼šåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š\r\n```\r\n5LiA5Y+q54yr 151851\r\n5Y+q54yr 151852\r\n5piv5LiA5Y+q54yr 151853\r\n5oiR5piv5LiA5Y+q54yr 151854\r\n5L2g5piv5LiA5Y+q54yr 151855\r\n5LuW5piv5LiA5Y+q54yr 151856\r\n```\r\n\r\næ‚¨å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼ä½¿ç”¨æ‰©å±•åçš„è¯è¡¨ï¼š\r\n``` python\r\nfrom transformers import AutoTokenizer\r\n\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True, extra_vocab_file=\"qwen_extra.tiktoken\")\r\n\r\n>>> len(tokenizer)\r\n151857\r\n\r\n>>> tokenizer(\"æˆ‘æ˜¯ä¸€åªçŒ«\")\r\n{'input_ids': [151854], 'token_type_ids': [0], 'attention_mask': [1]}\r\n```\r\n\r\næ³¨æ„ï¼šæ‚¨éœ€è¦ä½¿ç”¨2023å¹´10æœˆ8æ—¥åçš„tokenizerä»£ç æ‰èƒ½ä¼ é€’`extra_vocab_file`å‚æ•°ã€‚å¦‚æ˜¯å…¶å®ƒæƒ…å†µï¼Œæ‚¨å¯ä»¥å°†`qwen_extra.tiktoken`å†…å®¹å¤åˆ¶ç²˜è´´åˆ°`qwen.tiktoken`å†…å®¹åé¢ã€‚\r\n\r\næ‚¨éœ€è¦å¾®è°ƒæ¨¡å‹æ‰èƒ½ä½¿æ–°çš„tokenå‘æŒ¥ä½œç”¨ã€‚\r\n\r\n### æ³¨æ„äº‹é¡¹\r\n\r\nQwençš„tokenizeræ˜¯ç›´æ¥ä»UTF-8ç¼–ç çš„å­—èŠ‚åºåˆ—å¼€å§‹å¤„ç†çš„ï¼Œè¿™ä¸å…¶å®ƒtokenizeræ¯”å¦‚SentencePieceæ˜¯å¾ˆä¸ä¸€æ ·çš„ã€‚SentencePieceæ˜¯ä»Unicodeç ä½ï¼ˆå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªå­—ç¬¦ï¼‰å¼€å§‹å¤„ç†ï¼Œé‡åˆ°æœªç™»å½•çš„å†ç”¨UTF-8ç¼–ç æˆå­—èŠ‚ã€‚\r\nä»å­—èŠ‚å¼€å§‹çš„ä¸€ä¸ªæ½œåœ¨é—®é¢˜æ˜¯å¦‚æœé¢‘ç‡ä¿¡æ¯ä¸å¤Ÿå‡†ç¡®ï¼Œæ¯”å¦‚é¢‘ç‡ä¿¡æ¯æ˜¯åœ¨å¾ˆå°‘æ•°æ®ä¸Šç»Ÿè®¡å¾—åˆ°çš„ï¼ŒUnicodeç ä½æŒ‰UTF-8ç¼–ç æˆå­—èŠ‚åçš„è¾¹ç•Œå¯èƒ½ä¼šå‡ºç°å·®é”™ã€‚\r\nç†è®ºä¸Šï¼Œå¦‚æœæ¨¡å‹å¾®è°ƒæ•°æ®é‡ä¸è¶³ï¼Œä½¿ç”¨æ‰©å±•åçš„è¯è¡¨ä¹Ÿå¯èƒ½å‡ºç°æ„å¤–é—®é¢˜ã€‚\r\n\r\nä¸¾ä¸ªä¾‹å­ï¼ˆéå®é™…æƒ…å†µï¼‰ï¼Œå¯¹äº`ä¸€åª`çš„UTF-8å­—èŠ‚åºåˆ—`b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa'`ï¼Œä¸­é—´ä¸¤ä¸ªå­—èŠ‚`b'\\x80\\xe5'`å¯èƒ½ä¼šå…ˆåˆå¹¶ä¸ºä¸€ä¸ªtokenï¼Œè·¨è¶Šäº†`ä¸€`(`b'\\xe4\\xb8\\x80'`)å’Œ`åª`(`b'\\xe5\\x8f\\xaa'`)çš„ç ä½è¾¹ç•Œã€‚\r\nè¿™å¯¹äºå·²ç™»å½•tokenä¸ä¼šæœ‰ä»€ä¹ˆå½±å“ï¼ˆæœ€åæ€»ä¼šåˆå¹¶ä¸º`ä¸€åª`ï¼‰ï¼Œä½†å¯¹äºæœªç™»å½•çš„ï¼Œå¯èƒ½ä¼šäº§ç”Ÿä¸€äº›ä¸åŒå¯»å¸¸çš„åˆå¹¶/tokenã€‚\r\nè¿™äº›tokenåºåˆ—å¯èƒ½å¯¹äºé¢„è®­ç»ƒæ¨¡å‹æ˜¯é™Œç”Ÿçš„ã€‚\r\n\r\næˆ‘ä»¬çš„å»ºè®®æ˜¯ä¿é™©èµ·è§ï¼Œæ‚¨æœ€å¥½å…ˆæ”¶é›†å¾…æ·»åŠ è¯ä¸­çš„æ‰€æœ‰Unicodeç ä½ï¼Œç„¶åå•ç‹¬æŒ‡å®šå®ƒä»¬çš„é¢‘ç‡å¤§äºå…¶æ‰€æ„æˆè¯çš„é¢‘ç‡ä¹‹å’Œã€‚\r\nä¸è¿‡ç”±äºQwençš„tokenizerå·²åŒ…å«äº†å¤§å¤šæ•°ä¸­æ–‡å­—ï¼Œå¯¹äºä¸­æ–‡è¯çš„è¯ï¼Œä¸æ·»åŠ ä¸­æ–‡å­—çš„é¢‘ç‡ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹æ˜¯å¯è¡Œçš„ã€‚\r\n\r\næ‚¨å¯èƒ½å·²ç»å‘ç°äº†ï¼Œåœ¨æä¾›çš„ä¾‹å­ä¸­ï¼Œ`ä¸€åª`å·²ç»æ˜¯ç™»å½•è¿‡çš„tokenäº†ï¼Œä½†`åªçŒ«`è¿˜æ˜¯å­¦ä¹ æˆä¸ºäº†ä¸€ä¸ªæ–°tokenï¼Œå‡ºç°äº†â€œäº¤å‰â€ã€‚\r\nåŸå› æ˜¯åœ¨Qwenä¸­`æ˜¯ä¸€`ä¹Ÿæ˜¯ä¸€ä¸ªå·²çŸ¥tokenï¼Œä¸”å…¶é¢‘ç‡/ä¼˜å…ˆçº§æ¯”`ä¸€åª`è¦é«˜ï¼Œå› è€Œå¯¹äº`æ˜¯|ä¸€|åª|çŒ«`è¿™ä¸ªç‰‡æ®µï¼Œåˆå¹¶çš„æ¬¡åºæ˜¯`æ˜¯ä¸€|åª|çŒ« -> æ˜¯ä¸€|åªçŒ« -> æ˜¯ä¸€åªçŒ«`ï¼ˆçœç•¥UTF-8å­—èŠ‚çº§åˆ«çš„åˆå¹¶ï¼‰ã€‚\r\n\r\nè¿™æ˜¯å¸¸è§„BPEçš„ç‰¹æ€§ï¼Œå…¶å®Œå…¨åŸºäºåˆ†å¸ƒï¼Œå¹¶ä¸çŸ¥é“å“ªäº›å­—èŠ‚å¯ä»¥æ„æˆåˆæ³•çš„Unicodeç ä½ã€åˆæ³•çš„å­—ç¬¦æˆ–æ˜¯è¯ã€‚\r\n\r\nå‰¯äº§ç‰©æ˜¯ä¸€æ®µæ–‡æœ¬åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸‹å¯èƒ½ä¼šæœ‰ä¸åŒçš„tokenizeç»“æœï¼Œå¯¹äºä»…åŒ…å«ASCIIå­—ç¬¦çš„æ–‡æœ¬åŒæ ·å¦‚æ­¤ã€‚\r\n```python\r\n>>> tokenizer.tokenize(\"Panda\")\r\n[b'P', b'anda']\r\n\r\n>>> tokenizer.tokenize(\" Panda\")\r\n[b' Panda']\r\n\r\n>>> tokenizer.tokenize(\"Pandas\")\r\n[b'P', b'andas']\r\n\r\n>>> tokenizer.tokenize(\" Pandas\")\r\n[b' Pand', b'as']\r\n```\r\nè¿™ä»…è¯´æ˜åœ¨ç”¨äºå­¦ä¹ BPEçš„æ•°æ®ä¸­ï¼Œè¿™æ ·çš„ç»„åˆæ˜¯æ›´é«˜é¢‘çš„ã€‚\r\nå¦‚æœæ‚¨æœ‰æµ·é‡çš„è®­ç»ƒè¯­æ–™ï¼Œè¿™å¹¶ä¸ä¼šæ˜¯ä¸ªé—®é¢˜ã€‚"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 1.3525390625,
          "content": "import torch\nfrom transformers import AutoModelForCausalLM\nfrom accelerate import dispatch_model\n\n\ndef _device_map(num_gpus, num_layers):\n    per_gpu_layers = (num_layers + 2) / num_gpus\n\n    device_map = {\n        'transformer.wte': 0,\n        'transformer.ln_f': 0,\n        'lm_head': num_gpus-1\n    }\n\n    used = 1\n    gpu_target = 0\n    for i in range(num_layers):\n        if used >= per_gpu_layers:\n            gpu_target += 1\n            used = 0 if gpu_target < num_gpus-1 else 1\n        assert gpu_target < num_gpus\n        device_map[f'transformer.h.{i}'] = gpu_target\n        used += 1\n\n    return device_map\n\n\ndef load_model_on_gpus(model_name_or_path, num_gpus: int = 2):\n    num_devices = torch.cuda.device_count()\n\n    if num_gpus == 1:\n        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='auto',\n                                                     trust_remote_code=True).eval()\n    elif 1 < num_gpus <= num_devices:\n        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu',\n                                                     trust_remote_code=True).eval()\n        num_layers = model.config.num_hidden_layers\n        device_map = _device_map(num_gpus, num_layers)\n        print(device_map)\n        model = dispatch_model(model, device_map=device_map)\n    else:\n        raise KeyError\n\n    return model\n"
        },
        {
          "name": "web_demo.py",
          "type": "blob",
          "size": 7.337890625,
          "content": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"A simple web interactive chat demo based on gradio.\"\"\"\nimport os\nfrom argparse import ArgumentParser\n\nimport gradio as gr\nimport mdtex2html\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n\nDEFAULT_CKPT_PATH = 'Qwen/Qwen-7B-Chat'\n\n\ndef _get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\"-c\", \"--checkpoint-path\", type=str, default=DEFAULT_CKPT_PATH,\n                        help=\"Checkpoint name or path, default to %(default)r\")\n    parser.add_argument(\"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\")\n\n    parser.add_argument(\"--share\", action=\"store_true\", default=False,\n                        help=\"Create a publicly shareable link for the interface.\")\n    parser.add_argument(\"--inbrowser\", action=\"store_true\", default=False,\n                        help=\"Automatically launch the interface in a new tab on the default browser.\")\n    parser.add_argument(\"--server-port\", type=int, default=8000,\n                        help=\"Demo server port.\")\n    parser.add_argument(\"--server-name\", type=str, default=\"127.0.0.1\",\n                        help=\"Demo server name.\")\n\n    args = parser.parse_args()\n    return args\n\n\ndef _load_model_tokenizer(args):\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"auto\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    ).eval()\n\n    config = GenerationConfig.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True,\n    )\n\n    return model, tokenizer, config\n\n\ndef postprocess(self, y):\n    if y is None:\n        return []\n    for i, (message, response) in enumerate(y):\n        y[i] = (\n            None if message is None else mdtex2html.convert(message),\n            None if response is None else mdtex2html.convert(response),\n        )\n    return y\n\n\ngr.Chatbot.postprocess = postprocess\n\n\ndef _parse_text(text):\n    lines = text.split(\"\\n\")\n    lines = [line for line in lines if line != \"\"]\n    count = 0\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            count += 1\n            items = line.split(\"`\")\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = f\"<br></code></pre>\"\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace(\"`\", r\"\\`\")\n                    line = line.replace(\"<\", \"&lt;\")\n                    line = line.replace(\">\", \"&gt;\")\n                    line = line.replace(\" \", \"&nbsp;\")\n                    line = line.replace(\"*\", \"&ast;\")\n                    line = line.replace(\"_\", \"&lowbar;\")\n                    line = line.replace(\"-\", \"&#45;\")\n                    line = line.replace(\".\", \"&#46;\")\n                    line = line.replace(\"!\", \"&#33;\")\n                    line = line.replace(\"(\", \"&#40;\")\n                    line = line.replace(\")\", \"&#41;\")\n                    line = line.replace(\"$\", \"&#36;\")\n                lines[i] = \"<br>\" + line\n    text = \"\".join(lines)\n    return text\n\n\ndef _gc():\n    import gc\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef _launch_demo(args, model, tokenizer, config):\n\n    def predict(_query, _chatbot, _task_history):\n        print(f\"User: {_parse_text(_query)}\")\n        _chatbot.append((_parse_text(_query), \"\"))\n        full_response = \"\"\n\n        for response in model.chat_stream(tokenizer, _query, history=_task_history, generation_config=config):\n            _chatbot[-1] = (_parse_text(_query), _parse_text(response))\n\n            yield _chatbot\n            full_response = _parse_text(response)\n\n        print(f\"History: {_task_history}\")\n        _task_history.append((_query, full_response))\n        print(f\"Qwen-Chat: {_parse_text(full_response)}\")\n\n    def regenerate(_chatbot, _task_history):\n        if not _task_history:\n            yield _chatbot\n            return\n        item = _task_history.pop(-1)\n        _chatbot.pop(-1)\n        yield from predict(item[0], _chatbot, _task_history)\n\n    def reset_user_input():\n        return gr.update(value=\"\")\n\n    def reset_state(_chatbot, _task_history):\n        _task_history.clear()\n        _chatbot.clear()\n        _gc()\n        return _chatbot\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\"\"\"\\\n<p align=\"center\"><img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" style=\"height: 80px\"/><p>\"\"\")\n        gr.Markdown(\"\"\"<center><font size=8>Qwen-Chat Bot</center>\"\"\")\n        gr.Markdown(\n            \"\"\"\\\n<center><font size=3>This WebUI is based on Qwen-Chat, developed by Alibaba Cloud. \\\n(æœ¬WebUIåŸºäºQwen-Chatæ‰“é€ ï¼Œå®ç°èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚)</center>\"\"\")\n        gr.Markdown(\"\"\"\\\n<center><font size=4>\nQwen-7B <a href=\"https://modelscope.cn/models/qwen/Qwen-7B/summary\">ğŸ¤– </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-7B\">ğŸ¤—</a>&nbsp ï½œ \nQwen-7B-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary\">ğŸ¤– </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-7B-Chat\">ğŸ¤—</a>&nbsp ï½œ \nQwen-14B <a href=\"https://modelscope.cn/models/qwen/Qwen-14B/summary\">ğŸ¤– </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-14B\">ğŸ¤—</a>&nbsp ï½œ \nQwen-14B-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary\">ğŸ¤– </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\">ğŸ¤—</a>&nbsp ï½œ \n&nbsp<a href=\"https://github.com/QwenLM/Qwen\">Github</a></center>\"\"\")\n\n        chatbot = gr.Chatbot(label='Qwen-Chat', elem_classes=\"control-height\")\n        query = gr.Textbox(lines=2, label='Input')\n        task_history = gr.State([])\n\n        with gr.Row():\n            empty_btn = gr.Button(\"ğŸ§¹ Clear History (æ¸…é™¤å†å²)\")\n            submit_btn = gr.Button(\"ğŸš€ Submit (å‘é€)\")\n            regen_btn = gr.Button(\"ğŸ¤”ï¸ Regenerate (é‡è¯•)\")\n\n        submit_btn.click(predict, [query, chatbot, task_history], [chatbot], show_progress=True)\n        submit_btn.click(reset_user_input, [], [query])\n        empty_btn.click(reset_state, [chatbot, task_history], outputs=[chatbot], show_progress=True)\n        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)\n\n        gr.Markdown(\"\"\"\\\n<font size=2>Note: This demo is governed by the original license of Qwen. \\\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \\\nincluding hate speech, violence, pornography, deception, etc. \\\n(æ³¨ï¼šæœ¬æ¼”ç¤ºå—Qwençš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼Œ\\\nåŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)\"\"\")\n\n    demo.queue().launch(\n        share=args.share,\n        inbrowser=args.inbrowser,\n        server_port=args.server_port,\n        server_name=args.server_name,\n    )\n\n\ndef main():\n    args = _get_args()\n\n    model, tokenizer, config = _load_model_tokenizer(args)\n\n    _launch_demo(args, model, tokenizer, config)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    }
  ]
}