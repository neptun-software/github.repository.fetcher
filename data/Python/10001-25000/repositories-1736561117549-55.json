{
  "metadata": {
    "timestamp": 1736561117549,
    "page": 55,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "karpathy/minGPT",
      "stars": 20774,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.052734375,
          "content": ".ipynb_checkpoints/\n__pycache__/\n*.swp\n.env\n.pylintrc\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0556640625,
          "content": "The MIT License (MIT) Copyright (c) 2020 Andrej Karpathy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.6376953125,
          "content": "\n# minGPT\n\n![mingpt](mingpt.jpg)\n\nA PyTorch re-implementation of [GPT](https://github.com/openai/gpt-2), both training and inference. minGPT tries to be small, clean, interpretable and educational, as most of the currently available GPT model implementations can a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code (see [mingpt/model.py](mingpt/model.py)). All that's going on is that a sequence of indices feeds into a [Transformer](https://arxiv.org/abs/1706.03762), and a probability distribution over the next index in the sequence comes out. The majority of the complexity is just being clever with batching (both across examples and over sequence length) for efficiency.\n\n**note (Jan 2023)**: though I may continue to accept and change some details, minGPT is in a semi-archived state. For more recent developments see my rewrite [nanoGPT](https://github.com/karpathy/nanoGPT). Basically, minGPT became referenced across a wide variety of places (notebooks, blogs, courses, books, etc.) which made me less willing to make the bigger changes I wanted to make to move the code forward. I also wanted to change the direction a bit, from a sole focus on education to something that is still simple and hackable but has teeth (reproduces medium-sized industry benchmarks, accepts some tradeoffs to gain runtime efficiency, etc).\n\nThe minGPT library is three files: [mingpt/model.py](mingpt/model.py) contains the actual Transformer model definition, [mingpt/bpe.py](mingpt/bpe.py) contains a mildly refactored Byte Pair Encoder that translates between text and sequences of integers exactly like OpenAI did in GPT, [mingpt/trainer.py](mingpt/trainer.py) is (GPT-independent) PyTorch boilerplate code that trains the model. Then there are a number of demos and projects that use the library in the `projects` folder:\n\n- `projects/adder` trains a GPT from scratch to add numbers (inspired by the addition section in the GPT-3 paper)\n- `projects/chargpt` trains a GPT to be a character-level language model on some input text file\n- `demo.ipynb` shows a minimal usage of the `GPT` and `Trainer` in a notebook format on a simple sorting example\n- `generate.ipynb` shows how one can load a pretrained GPT2 and generate text given some prompt\n\n### Library Installation\n\nIf you want to `import mingpt` into your project:\n\n```\ngit clone https://github.com/karpathy/minGPT.git\ncd minGPT\npip install -e .\n```\n\n### Usage\n\nHere's how you'd instantiate a GPT-2 (124M param version):\n\n```python\nfrom mingpt.model import GPT\nmodel_config = GPT.get_default_config()\nmodel_config.model_type = 'gpt2'\nmodel_config.vocab_size = 50257 # openai's model vocabulary\nmodel_config.block_size = 1024  # openai's model block_size (i.e. input context length)\nmodel = GPT(model_config)\n```\n\nAnd here's how you'd train it:\n\n```python\n# your subclass of torch.utils.data.Dataset that emits example\n# torch LongTensor of lengths up to 1024, with integers from [0,50257)\ntrain_dataset = YourDataset()\n\nfrom mingpt.trainer import Trainer\ntrain_config = Trainer.get_default_config()\ntrain_config.learning_rate = 5e-4 # many possible options, see the file\ntrain_config.max_iters = 1000\ntrain_config.batch_size = 32\ntrainer = Trainer(train_config, model, train_dataset)\ntrainer.run()\n```\n\nSee `demo.ipynb` for a more concrete example.\n\n### Unit tests\n\nCoverage is not super amazing just yet but:\n\n```\npython -m unittest discover tests\n```\n\n### todos\n\n- add gpt-2 finetuning demo on arbitrary given text file\n- add dialog agent demo\n- better docs of outcomes for existing projects (adder, chargpt)\n- add mixed precision and related training scaling goodies\n- distributed training support\n- reproduce some benchmarks in projects/, e.g. text8 or other language modeling\n- proper logging instead of print statement amateur hour haha\n- i probably should have a requirements.txt file...\n- it should be possible to load in many other model weights other than just gpt2-\\*\n\n### References\n\nCode:\n\n- [openai/gpt-2](https://github.com/openai/gpt-2) has the model definition in TensorFlow, but not the training code\n- [openai/image-gpt](https://github.com/openai/image-gpt) has some more modern gpt-3 like modification in its code, good reference as well\n- [huggingface/transformers](https://github.com/huggingface/transformers) has a [language-modeling example](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling). It is full-featured but as a result also somewhat challenging to trace. E.g. some large functions have as much as 90% unused code behind various branching statements that is unused in the default setting of simple language modeling\n\nPapers + some implementation notes:\n\n#### Improving Language Understanding by Generative Pre-Training (GPT-1)\n\n- Our model largely follows the original transformer work\n- We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.\n- Adam max learning rate of 2.5e-4. (later GPT-3 for this model size uses 6e-4)\n- LR decay: increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule\n- We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\n- Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient\n- bytepair encoding (BPE) vocabulary with 40,000 merges\n- residual, embedding, and attention dropouts with a rate of 0.1 for regularization.\n- modified version of L2 regularization proposed in (37), with w = 0.01 on all non bias or gain weights\n- For the activation function, we used the Gaussian Error Linear Unit (GELU).\n- We used learned position embeddings instead of the sinusoidal version proposed in the original work\n- For finetuning: We add dropout to the classifier with a rate of 0.1. learning rate of 6.25e-5 and a batchsize of 32. 3 epochs. We use a linear learning rate decay schedule with warmup over 0.2% of training. λ was set to 0.5.\n- GPT-1 model is 12 layers and d_model 768, ~117M params\n\n#### Language Models are Unsupervised Multitask Learners (GPT-2)\n\n- LayerNorm was moved to the input of each sub-block, similar to a pre-activation residual network\n- an additional layer normalization was added after the final self-attention block.\n- modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/√N where N is the number of residual layers. (weird because in their released code i can only find a simple use of the old 0.02... in their release of image-gpt I found it used for c_proj, and even then only for attn, not for mlp. huh. https://github.com/openai/image-gpt/blob/master/src/model.py)\n- the vocabulary is expanded to 50,257\n- increase the context size from 512 to 1024 tokens\n- larger batchsize of 512 is used\n- GPT-2 used 48 layers and d_model 1600 (vs. original 12 layers and d_model 768). ~1.542B params\n\n#### Language Models are Few-Shot Learners (GPT-3)\n\n- GPT-3: 96 layers, 96 heads, with d_model of 12,288 (175B parameters).\n- GPT-1-like: 12 layers, 12 heads, d_model 768 (125M)\n- We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein\n- we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer\n- we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 ∗ dmodel\n- all models use a context window of nctx = 2048 tokens.\n- Adam with β1 = 0.9, β2 = 0.95, and eps = 10−8\n- All models use weight decay of 0.1 to provide a small amount of regularization. (NOTE: GPT-1 used 0.01 I believe, see above)\n- clip the global norm of the gradient at 1.0\n- Linear LR warmup over the first 375 million tokens. Then use cosine decay for learning rate down to 10% of its value, over 260 billion tokens.\n- gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size.\n- full 2048-sized time context window is always used, with a special END OF DOCUMENT token delimiter\n\n#### Generative Pretraining from Pixels (Image GPT)\n\n- When working with images, we pick the identity permutation πi = i for 1 ≤ i ≤ n, also known as raster order.\n- we create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512.\n- Our largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters.\n- Our next largest model, iGPT-L, is essentially identical to GPT-2 with L = 48 layers, but contains a slightly smaller embedding size of d = 1536 (vs 1600) for a total of 1.4B parameters.\n- We use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in Sparse Transformer (Child et al., 2019) and zero-initialize all projections producing logits.\n- We also train iGPT-M, a 455M parameter model with L = 36 and d = 1024\n- iGPT-S, a 76M parameter model with L = 24 and d = 512 (okay, and how many heads? looks like the Github code claims 8)\n- When pre-training iGPT-XL, we use a batch size of 64 and train for 2M iterations, and for all other models we use a batch size of 128 and train for 1M iterations.\n- Adam with β1 = 0.9 and β2 = 0.95\n- The learning rate is warmed up for one epoch, and then decays to 0\n- We did not use weight decay because applying a small weight decay of 0.01 did not change representation quality.\n- iGPT-S lr 0.003\n- No dropout is used.\n\n### License\n\nMIT\n"
        },
        {
          "name": "demo.ipynb",
          "type": "blob",
          "size": 11.001953125,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"A cute little demo showing the simplest usage of minGPT. Configured to run fine on Macbook Air in like a minute.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import torch\\n\",\n    \"from torch.utils.data import Dataset\\n\",\n    \"from torch.utils.data.dataloader import DataLoader\\n\",\n    \"from mingpt.utils import set_seed\\n\",\n    \"set_seed(3407)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import pickle\\n\",\n    \"\\n\",\n    \"class SortDataset(Dataset):\\n\",\n    \"    \\\"\\\"\\\" \\n\",\n    \"    Dataset for the Sort problem. E.g. for problem length 6:\\n\",\n    \"    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\\n\",\n    \"    Which will feed into the transformer concatenated as:\\n\",\n    \"    input:  0 0 2 1 0 1 0 0 0 1 1\\n\",\n    \"    output: I I I I I 0 0 0 1 1 2\\n\",\n    \"    where I is \\\"ignore\\\", as the transformer is reading the input sequence\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"    def __init__(self, split, length=6, num_digits=3):\\n\",\n    \"        assert split in {'train', 'test'}\\n\",\n    \"        self.split = split\\n\",\n    \"        self.length = length\\n\",\n    \"        self.num_digits = num_digits\\n\",\n    \"    \\n\",\n    \"    def __len__(self):\\n\",\n    \"        return 10000 # ...\\n\",\n    \"    \\n\",\n    \"    def get_vocab_size(self):\\n\",\n    \"        return self.num_digits\\n\",\n    \"    \\n\",\n    \"    def get_block_size(self):\\n\",\n    \"        # the length of the sequence that will feed into transformer, \\n\",\n    \"        # containing concatenated input and the output, but -1 because\\n\",\n    \"        # the transformer starts making predictions at the last input element\\n\",\n    \"        return self.length * 2 - 1\\n\",\n    \"\\n\",\n    \"    def __getitem__(self, idx):\\n\",\n    \"        \\n\",\n    \"        # use rejection sampling to generate an input example from the desired split\\n\",\n    \"        while True:\\n\",\n    \"            # generate some random integers\\n\",\n    \"            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\\n\",\n    \"            # half of the time let's try to boost the number of examples that \\n\",\n    \"            # have a large number of repeats, as this is what the model seems to struggle\\n\",\n    \"            # with later in training, and they are kind of rate\\n\",\n    \"            if torch.rand(1).item() < 0.5:\\n\",\n    \"                if inp.unique().nelement() > self.length // 2:\\n\",\n    \"                    # too many unqiue digits, re-sample\\n\",\n    \"                    continue\\n\",\n    \"            # figure out if this generated example is train or test based on its hash\\n\",\n    \"            h = hash(pickle.dumps(inp.tolist()))\\n\",\n    \"            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\\n\",\n    \"            if inp_split == self.split:\\n\",\n    \"                break # ok\\n\",\n    \"        \\n\",\n    \"        # solve the task: i.e. sort\\n\",\n    \"        sol = torch.sort(inp)[0]\\n\",\n    \"\\n\",\n    \"        # concatenate the problem specification and the solution\\n\",\n    \"        cat = torch.cat((inp, sol), dim=0)\\n\",\n    \"\\n\",\n    \"        # the inputs to the transformer will be the offset sequence\\n\",\n    \"        x = cat[:-1].clone()\\n\",\n    \"        y = cat[1:].clone()\\n\",\n    \"        # we only want to predict at output locations, mask out the loss at the input locations\\n\",\n    \"        y[:self.length-1] = -1\\n\",\n    \"        return x, y\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"1 -1\\n\",\n      \"0 -1\\n\",\n      \"1 -1\\n\",\n      \"0 -1\\n\",\n      \"0 -1\\n\",\n      \"0 0\\n\",\n      \"0 0\\n\",\n      \"0 0\\n\",\n      \"0 0\\n\",\n      \"0 1\\n\",\n      \"1 1\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# print an example instance of the dataset\\n\",\n    \"train_dataset = SortDataset('train')\\n\",\n    \"test_dataset = SortDataset('test')\\n\",\n    \"x, y = train_dataset[0]\\n\",\n    \"for a, b in zip(x,y):\\n\",\n    \"    print(int(a),int(b))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"number of parameters: 0.09M\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# create a GPT instance\\n\",\n    \"from mingpt.model import GPT\\n\",\n    \"\\n\",\n    \"model_config = GPT.get_default_config()\\n\",\n    \"model_config.model_type = 'gpt-nano'\\n\",\n    \"model_config.vocab_size = train_dataset.get_vocab_size()\\n\",\n    \"model_config.block_size = train_dataset.get_block_size()\\n\",\n    \"model = GPT(model_config)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"running on device cuda\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# create a Trainer object\\n\",\n    \"from mingpt.trainer import Trainer\\n\",\n    \"\\n\",\n    \"train_config = Trainer.get_default_config()\\n\",\n    \"train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\\n\",\n    \"train_config.max_iters = 2000\\n\",\n    \"train_config.num_workers = 0\\n\",\n    \"trainer = Trainer(train_config, model, train_dataset)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"iter_dt 0.00ms; iter 0: train loss 1.06407\\n\",\n      \"iter_dt 18.17ms; iter 100: train loss 0.14712\\n\",\n      \"iter_dt 18.70ms; iter 200: train loss 0.05315\\n\",\n      \"iter_dt 19.65ms; iter 300: train loss 0.04404\\n\",\n      \"iter_dt 31.64ms; iter 400: train loss 0.04724\\n\",\n      \"iter_dt 18.43ms; iter 500: train loss 0.02521\\n\",\n      \"iter_dt 19.83ms; iter 600: train loss 0.03352\\n\",\n      \"iter_dt 19.58ms; iter 700: train loss 0.00539\\n\",\n      \"iter_dt 18.72ms; iter 800: train loss 0.02057\\n\",\n      \"iter_dt 18.26ms; iter 900: train loss 0.00360\\n\",\n      \"iter_dt 18.50ms; iter 1000: train loss 0.00788\\n\",\n      \"iter_dt 20.64ms; iter 1100: train loss 0.01162\\n\",\n      \"iter_dt 18.63ms; iter 1200: train loss 0.00963\\n\",\n      \"iter_dt 18.32ms; iter 1300: train loss 0.02066\\n\",\n      \"iter_dt 18.40ms; iter 1400: train loss 0.01739\\n\",\n      \"iter_dt 18.37ms; iter 1500: train loss 0.00376\\n\",\n      \"iter_dt 18.67ms; iter 1600: train loss 0.00133\\n\",\n      \"iter_dt 18.38ms; iter 1700: train loss 0.00179\\n\",\n      \"iter_dt 18.66ms; iter 1800: train loss 0.00079\\n\",\n      \"iter_dt 18.48ms; iter 1900: train loss 0.00042\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"def batch_end_callback(trainer):\\n\",\n    \"    if trainer.iter_num % 100 == 0:\\n\",\n    \"        print(f\\\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\\\")\\n\",\n    \"trainer.set_callback('on_batch_end', batch_end_callback)\\n\",\n    \"\\n\",\n    \"trainer.run()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# now let's perform some evaluation\\n\",\n    \"model.eval();\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"train final score: 5000/5000 = 100.00% correct\\n\",\n      \"test final score: 5000/5000 = 100.00% correct\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"def eval_split(trainer, split, max_batches):\\n\",\n    \"    dataset = {'train':train_dataset, 'test':test_dataset}[split]\\n\",\n    \"    n = train_dataset.length # naugy direct access shrug\\n\",\n    \"    results = []\\n\",\n    \"    mistakes_printed_already = 0\\n\",\n    \"    loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)\\n\",\n    \"    for b, (x, y) in enumerate(loader):\\n\",\n    \"        x = x.to(trainer.device)\\n\",\n    \"        y = y.to(trainer.device)\\n\",\n    \"        # isolate the input pattern alone\\n\",\n    \"        inp = x[:, :n]\\n\",\n    \"        sol = y[:, -n:]\\n\",\n    \"        # let the model sample the rest of the sequence\\n\",\n    \"        cat = model.generate(inp, n, do_sample=False) # using greedy argmax, not sampling\\n\",\n    \"        sol_candidate = cat[:, n:] # isolate the filled in sequence\\n\",\n    \"        # compare the predicted sequence to the true sequence\\n\",\n    \"        correct = (sol == sol_candidate).all(1).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line haha\\n\",\n    \"        for i in range(x.size(0)):\\n\",\n    \"            results.append(int(correct[i]))\\n\",\n    \"            if not correct[i] and mistakes_printed_already < 3: # only print up to 5 mistakes to get a sense\\n\",\n    \"                mistakes_printed_already += 1\\n\",\n    \"                print(\\\"GPT claims that %s sorted is %s but gt is %s\\\" % (inp[i].tolist(), sol_candidate[i].tolist(), sol[i].tolist()))\\n\",\n    \"        if max_batches is not None and b+1 >= max_batches:\\n\",\n    \"            break\\n\",\n    \"    rt = torch.tensor(results, dtype=torch.float)\\n\",\n    \"    print(\\\"%s final score: %d/%d = %.2f%% correct\\\" % (split, rt.sum(), len(results), 100*rt.mean()))\\n\",\n    \"    return rt.sum()\\n\",\n    \"\\n\",\n    \"# run a lot of examples from both train and test through the model and verify the output correctness\\n\",\n    \"with torch.no_grad():\\n\",\n    \"    train_score = eval_split(trainer, 'train', max_batches=50)\\n\",\n    \"    test_score  = eval_split(trainer, 'test',  max_batches=50)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 9,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"input sequence  : [[0, 0, 2, 1, 0, 1]]\\n\",\n      \"predicted sorted: [[0, 0, 0, 1, 1, 2]]\\n\",\n      \"gt sort         : [0, 0, 0, 1, 1, 2]\\n\",\n      \"matches         : True\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# let's run a random given sequence through the model as well\\n\",\n    \"n = train_dataset.length # naugy direct access shrug\\n\",\n    \"inp = torch.tensor([[0, 0, 2, 1, 0, 1]], dtype=torch.long).to(trainer.device)\\n\",\n    \"assert inp[0].nelement() == n\\n\",\n    \"with torch.no_grad():\\n\",\n    \"    cat = model.generate(inp, n, do_sample=False)\\n\",\n    \"sol = torch.sort(inp[0])[0]\\n\",\n    \"sol_candidate = cat[:, n:]\\n\",\n    \"print('input sequence  :', inp.tolist())\\n\",\n    \"print('predicted sorted:', sol_candidate.tolist())\\n\",\n    \"print('gt sort         :', sol.tolist())\\n\",\n    \"print('matches         :', bool((sol == sol_candidate).all()))\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3.10.4 64-bit\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.4\"\n  },\n  \"orig_nbformat\": 4,\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        },
        {
          "name": "generate.ipynb",
          "type": "blob",
          "size": 6.216796875,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Shows how one can generate text given a prompt and some hyperparameters, using either minGPT or huggingface/transformers\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import torch\\n\",\n    \"from transformers import GPT2Tokenizer, GPT2LMHeadModel\\n\",\n    \"from mingpt.model import GPT\\n\",\n    \"from mingpt.utils import set_seed\\n\",\n    \"from mingpt.bpe import BPETokenizer\\n\",\n    \"set_seed(3407)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"use_mingpt = True # use minGPT or huggingface/transformers model?\\n\",\n    \"model_type = 'gpt2-xl'\\n\",\n    \"device = 'cuda'\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"number of parameters: 1557.61M\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"if use_mingpt:\\n\",\n    \"    model = GPT.from_pretrained(model_type)\\n\",\n    \"else:\\n\",\n    \"    model = GPT2LMHeadModel.from_pretrained(model_type)\\n\",\n    \"    model.config.pad_token_id = model.config.eos_token_id # suppress a warning\\n\",\n    \"\\n\",\n    \"# ship model to device and set to eval mode\\n\",\n    \"model.to(device)\\n\",\n    \"model.eval();\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"\\n\",\n    \"def generate(prompt='', num_samples=10, steps=20, do_sample=True):\\n\",\n    \"        \\n\",\n    \"    # tokenize the input prompt into integer input sequence\\n\",\n    \"    if use_mingpt:\\n\",\n    \"        tokenizer = BPETokenizer()\\n\",\n    \"        if prompt == '':\\n\",\n    \"            # to create unconditional samples...\\n\",\n    \"            # manually create a tensor with only the special <|endoftext|> token\\n\",\n    \"            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\\n\",\n    \"            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\\n\",\n    \"        else:\\n\",\n    \"            x = tokenizer(prompt).to(device)\\n\",\n    \"    else:\\n\",\n    \"        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\\n\",\n    \"        if prompt == '': \\n\",\n    \"            # to create unconditional samples...\\n\",\n    \"            # huggingface/transformers tokenizer special cases these strings\\n\",\n    \"            prompt = '<|endoftext|>'\\n\",\n    \"        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\\n\",\n    \"        x = encoded_input['input_ids']\\n\",\n    \"    \\n\",\n    \"    # we'll process all desired num_samples in a batch, so expand out the batch dim\\n\",\n    \"    x = x.expand(num_samples, -1)\\n\",\n    \"\\n\",\n    \"    # forward the model `steps` times to get samples, in a batch\\n\",\n    \"    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\\n\",\n    \"    \\n\",\n    \"    for i in range(num_samples):\\n\",\n    \"        out = tokenizer.decode(y[i].cpu().squeeze())\\n\",\n    \"        print('-'*80)\\n\",\n    \"        print(out)\\n\",\n    \"        \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the chief of the criminal investigation department, said during a news conference, \\\"We still have a lot of\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the man whom most of America believes is the architect of the current financial crisis. He runs the National Council\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the head of the Department for Regional Reform of Bulgaria and an MP in the centre-right GERB party\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the former head of the World Bank's IMF department, who worked closely with the IMF. The IMF had\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the vice president for innovation and research at Citi who oversaw the team's work to make sense of the\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the CEO of OOAK Research, said that the latest poll indicates that it won't take much to\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the former prime minister of Estonia was at the helm of a three-party coalition when parliament met earlier this\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the director of the Institute of Economic and Social Research, said if the rate of return is only 5 per\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the minister of commerce for Latvia's western neighbour: \\\"The deal means that our two countries have reached more\\n\",\n      \"--------------------------------------------------------------------------------\\n\",\n      \"Andrej Karpathy, the state's environmental protection commissioner. \\\"That's why we have to keep these systems in place.\\\"\\n\",\n      \"\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"generate(prompt='Andrej Karpathy, the', num_samples=10, steps=20)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3.10.4 64-bit\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.4\"\n  },\n  \"orig_nbformat\": 4,\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        },
        {
          "name": "mingpt.jpg",
          "type": "blob",
          "size": 115.50390625,
          "content": null
        },
        {
          "name": "mingpt",
          "type": "tree",
          "content": null
        },
        {
          "name": "projects",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.2607421875,
          "content": "from setuptools import setup\n\nsetup(name='minGPT',\n      version='0.0.1',\n      author='Andrej Karpathy',\n      packages=['mingpt'],\n      description='A PyTorch re-implementation of GPT',\n      license='MIT',\n      install_requires=[\n            'torch',\n      ],\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}