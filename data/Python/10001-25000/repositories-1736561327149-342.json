{
  "metadata": {
    "timestamp": 1736561327149,
    "page": 342,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lucidrains/DALLE2-pytorch",
      "stars": 11204,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9169921875,
          "content": "# default experiment tracker data\n.tracker-data/\n\n# Configuration Files\nconfigs/*\n!configs/*.example\n!configs/*_defaults.py\n!configs/README.md\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n.tracker_data\n*.pth\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2022 Phil Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0380859375,
          "content": "recursive-include dalle2_pytorch *.txt\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.150390625,
          "content": "install:\n\tpip install -U pip\n\tpip install -e .\n\ntest:\n\tCUDA_VISIBLE_DEVICES= python train_decoder.py --config_file configs/train_decoder_config.test.json\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 43.6923828125,
          "content": "<img src=\"./dalle2.png\" width=\"450px\"></img>\n\n## DALL-E 2 - Pytorch\n\nImplementation of <a href=\"https://openai.com/dall-e-2/\">DALL-E 2</a>, OpenAI's updated text-to-image synthesis neural network, in Pytorch.\n\n<a href=\"https://youtu.be/RJwPN4qNi_Y?t=555\">Yannic Kilcher summary</a> | <a href=\"https://www.youtube.com/watch?v=F1X4fHzF4mQ\">AssemblyAI explainer</a>\n\nThe main novelty seems to be an extra layer of indirection with the prior network (whether it is an autoregressive transformer or a diffusion network), which predicts an image embedding based on the text embedding from CLIP. Specifically, this repository will only build out the diffusion prior network, as it is the best performing variant (but which incidentally involves a causal transformer as the denoising network üòÇ)\n\nThis model is SOTA for text-to-image for now.\n\nPlease join <a href=\"https://discord.gg/xBPBXfcFHd\"><img alt=\"Join us on Discord\" src=\"https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white\"></a> if you are interested in helping out with the replication with the <a href=\"https://laion.ai/\">LAION</a> community | <a href=\"https://www.youtube.com/watch?v=AIOE1l1W0Tw\">Yannic Interview</a>\n\nAs of 5/23/22, it is no longer SOTA. SOTA will be <a href=\"https://github.com/lucidrains/imagen-pytorch\">here</a>. Jax versions as well as text-to-video project will be shifted towards the Imagen architecture, as it is way simpler.\n\n## Status\n\n- A research group has used the code in this repository to train a functional diffusion prior for their CLIP generations. Will share their work once they release their preprint. This, and <a href=\"https://github.com/crowsonkb\">Katherine's</a> own experiments, validate OpenAI's finding that the extra prior increases variety of generations.\n\n- Decoder is now verified working for unconditional generation on my experimental setup for Oxford flowers. 2 researchers have also confirmed Decoder is working for them.\n\n<img src=\"./samples/oxford.png\" width=\"450px\" />\n\n*ongoing at 21k steps*\n\n- <a href=\"https://twitter.com/Buntworthy/status/1529475416775434240?t=0GEge3Kr9I36cjcUVCQUTg\">Justin Pinkney</a> successfully trained the diffusion prior in the repository for his CLIP to Stylegan2 text-to-image application\n\n- <a href=\"https://github.com/rom1504\">Romain</a> has scaled up training to 800 GPUs with the available scripts without any issues\n\n## Pre-Trained Models\n\n- LAION is training prior models. Checkpoints are available on <a href=\"https://huggingface.co/zenglishuci/conditioned-prior\">ü§óhuggingface</a> and the training statistics are available on <a href=\"https://wandb.ai/nousr_laion/conditioned-prior/reports/LAION-DALLE2-PyTorch-Prior--VmlldzoyMDI2OTIx\">üêùWANDB</a>.\n- Decoder - <a href=\"https://wandb.ai/veldrovive/dalle2_train_decoder/runs/jkrtg0so?workspace=user-veldrovive\">In-progress test run</a> üöß\n- Decoder - <a href=\"https://wandb.ai/veldrovive/dalle2_train_decoder/runs/3d5rytsa?workspace=\">Another test run with sparse attention</a>\n- DALL-E 2 üöß - <a href=\"https://github.com/LAION-AI/dalle2-laion\">DALL-E 2 Laion repository</a>\n\n## Appreciation\n\nThis library would not have gotten to this working state without the help of\n\n- <a href=\"https://github.com/nousr\">Zion</a> for the distributed training code for the diffusion prior\n- <a href=\"https://github.com/Veldrovive\">Aidan</a> for the distributed training code for the decoder as well as the dataloaders\n- <a href=\"https://github.com/krish240574\">Kumar</a> for working on the initial diffusion training script\n- <a href=\"https://github.com/rom1504\">Romain</a> for the pull request reviews and project management\n- <a href=\"https://github.com/Ciaohe\">He Cao</a> and <a href=\"https://github.com/xiankgx\">xiankgx</a> for the Q&A and for identifying of critical bugs\n- <a href=\"https://github.com/marunine\">Marunine</a> for identifying issues with resizing of the low resolution conditioner, when training the upsampler, in addition to various other bug fixes\n- <a href=\"https://github.com/malumadev\">MalumaDev</a> for proposing the use of pixel shuffle upsampler for fixing checkboard artifacts\n- <a href=\"https://github.com/crowsonkb\">Katherine</a> for her advice\n- <a href=\"https://stability.ai/\">Stability AI</a> for the generous sponsorship\n- <a href=\"https://huggingface.co\">ü§ó Huggingface</a> and in particular <a href=\"https://github.com/sgugger\">Sylvain</a> for the <a href=\"https://github.com/huggingface/accelerate\">Accelerate</a> library\n- <a href=\"https://github.com/arogozhnikov\">Alex</a> for <a href=\"https://github.com/arogozhnikov/einops\">einops</a>, indispensable tool for tensor manipulation\n\n... and many others. Thank you! üôè\n\n## Install\n\n```bash\n$ pip install dalle2-pytorch\n```\n\n## Usage\n\nTo train DALLE-2 is a 3 step process, with the training of CLIP being the most important\n\nTo train CLIP, you can either use <a href=\"https://github.com/lucidrains/x-clip\">x-clip</a> package, or join the LAION discord, where a lot of replication efforts are already <a href=\"https://github.com/mlfoundations/open_clip\">underway</a>.\n\nThis repository will demonstrate integration with `x-clip` for starters\n\n```python\nimport torch\nfrom dalle2_pytorch import CLIP\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 1,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 1,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8,\n    use_all_token_embeds = True,            # whether to use fine-grained contrastive learning (FILIP)\n    decoupled_contrastive_learning = True,  # use decoupled contrastive learning (DCL) objective function, removing positive pairs from the denominator of the InfoNCE loss (CLOOB + DCL)\n    extra_latent_projection = True,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)\n    use_visual_ssl = True,                  # whether to do self supervised learning on images\n    visual_ssl_type = 'simclr',             # can be either 'simclr' or 'simsiam', depending on using DeCLIP or SLIP\n    use_mlm = False,                        # use masked language learning (MLM) on text (DeCLIP)\n    text_ssl_loss_weight = 0.05,            # weight for text MLM loss\n    image_ssl_loss_weight = 0.05            # weight for image self-supervised learning loss\n).cuda()\n\n# mock data\n\ntext = torch.randint(0, 49408, (4, 256)).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# train\n\nloss = clip(\n    text,\n    images,\n    return_loss = True              # needs to be set to True to return contrastive loss\n)\n\nloss.backward()\n\n# do the above with as many texts and images as possible in a loop\n```\n\nThen, you will need to train the decoder, which learns to generate images based on the image embedding coming from the trained CLIP above\n\n```python\nimport torch\nfrom dalle2_pytorch import Unet, Decoder, CLIP\n\n# trained clip from step 1\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 1,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 1,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8\n).cuda()\n\n# unet for the decoder\n\nunet = Unet(\n    dim = 128,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults=(1, 2, 4, 8)\n).cuda()\n\n# decoder, which contains the unet and clip\n\ndecoder = Decoder(\n    unet = unet,\n    clip = clip,\n    timesteps = 100,\n    image_cond_drop_prob = 0.1,\n    text_cond_drop_prob = 0.5\n).cuda()\n\n# mock images (get a lot of this)\n\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# feed images into decoder\n\nloss = decoder(images)\nloss.backward()\n\n# do the above for many many many many steps\n# then it will learn to generate images based on the CLIP image embeddings\n```\n\nFinally, the main contribution of the paper. The repository offers the diffusion prior network. It takes the CLIP text embeddings and tries to generate the CLIP image embeddings. Again, you will need the trained CLIP from the first step\n\n```python\nimport torch\nfrom dalle2_pytorch import DiffusionPriorNetwork, DiffusionPrior, CLIP\n\n# get trained CLIP from step one\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 6,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 6,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8,\n).cuda()\n\n# setup prior network, which contains an autoregressive transformer\n\nprior_network = DiffusionPriorNetwork(\n    dim = 512,\n    depth = 6,\n    dim_head = 64,\n    heads = 8\n).cuda()\n\n# diffusion prior network, which contains the CLIP and network (with transformer) above\n\ndiffusion_prior = DiffusionPrior(\n    net = prior_network,\n    clip = clip,\n    timesteps = 100,\n    cond_drop_prob = 0.2\n).cuda()\n\n# mock data\n\ntext = torch.randint(0, 49408, (4, 256)).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# feed text and images into diffusion prior network\n\nloss = diffusion_prior(text, images)\nloss.backward()\n\n# do the above for many many many steps\n# now the diffusion prior can generate image embeddings from the text embeddings\n```\n\nIn the paper, they actually used a <a href=\"https://cascaded-diffusion.github.io/\">recently discovered technique</a>, from <a href=\"http://www.jonathanho.me/\">Jonathan Ho</a> himself (original author of DDPMs, the core technique used in DALL-E v2) for high resolution image synthesis.\n\nThis can easily be used within this framework as so\n\n```python\nimport torch\nfrom dalle2_pytorch import Unet, Decoder, CLIP\n\n# trained clip from step 1\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 6,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 6,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8\n).cuda()\n\n# 2 unets for the decoder (a la cascading DDPM)\n\nunet1 = Unet(\n    dim = 32,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults = (1, 2, 4, 8)\n).cuda()\n\nunet2 = Unet(\n    dim = 32,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults = (1, 2, 4, 8, 16)\n).cuda()\n\n# decoder, which contains the unet(s) and clip\n\ndecoder = Decoder(\n    clip = clip,\n    unet = (unet1, unet2),            # insert both unets in order of low resolution to highest resolution (you can have as many stages as you want here)\n    image_sizes = (256, 512),         # resolutions, 256 for first unet, 512 for second. these must be unique and in ascending order (matches with the unets passed in)\n    timesteps = 1000,\n    image_cond_drop_prob = 0.1,\n    text_cond_drop_prob = 0.5\n).cuda()\n\n# mock images (get a lot of this)\n\nimages = torch.randn(4, 3, 512, 512).cuda()\n\n# feed images into decoder, specifying which unet you want to train\n# each unet can be trained separately, which is one of the benefits of the cascading DDPM scheme\n\nloss = decoder(images, unet_number = 1)\nloss.backward()\n\nloss = decoder(images, unet_number = 2)\nloss.backward()\n\n# do the above for many steps for both unets\n```\n\nFinally, to generate the DALL-E2 images from text. Insert the trained `DiffusionPrior` as well as the `Decoder` (which wraps `CLIP`, the causal transformer, and unet(s))\n\n```python\nfrom dalle2_pytorch import DALLE2\n\ndalle2 = DALLE2(\n    prior = diffusion_prior,\n    decoder = decoder\n)\n\n# send the text as a string if you want to use the simple tokenizer from DALLE v1\n# or you can do it as token ids, if you have your own tokenizer\n\ntexts = ['glistening morning dew on a flower petal']\nimages = dalle2(texts) # (1, 3, 256, 256)\n```\n\nThat's it!\n\nLet's see the whole script below\n\n```python\nimport torch\nfrom dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder, CLIP\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 6,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 6,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8\n).cuda()\n\n# mock data\n\ntext = torch.randint(0, 49408, (4, 256)).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# train\n\nloss = clip(\n    text,\n    images,\n    return_loss = True\n)\n\nloss.backward()\n\n# do above for many steps ...\n\n# prior networks (with transformer)\n\nprior_network = DiffusionPriorNetwork(\n    dim = 512,\n    depth = 6,\n    dim_head = 64,\n    heads = 8\n).cuda()\n\ndiffusion_prior = DiffusionPrior(\n    net = prior_network,\n    clip = clip,\n    timesteps = 1000,\n    sample_timesteps = 64,\n    cond_drop_prob = 0.2\n).cuda()\n\nloss = diffusion_prior(text, images)\nloss.backward()\n\n# do above for many steps ...\n\n# decoder (with unet)\n\nunet1 = Unet(\n    dim = 128,\n    image_embed_dim = 512,\n    text_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults=(1, 2, 4, 8),\n    cond_on_text_encodings = True    # set to True for any unets that need to be conditioned on text encodings\n).cuda()\n\nunet2 = Unet(\n    dim = 16,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults = (1, 2, 4, 8, 16)\n).cuda()\n\ndecoder = Decoder(\n    unet = (unet1, unet2),\n    image_sizes = (128, 256),\n    clip = clip,\n    timesteps = 100,\n    image_cond_drop_prob = 0.1,\n    text_cond_drop_prob = 0.5\n).cuda()\n\nfor unet_number in (1, 2):\n    loss = decoder(images, text = text, unet_number = unet_number) # this can optionally be decoder(images, text) if you wish to condition on the text encodings as well, though it was hinted in the paper it didn't do much\n    loss.backward()\n\n# do above for many steps\n\ndalle2 = DALLE2(\n    prior = diffusion_prior,\n    decoder = decoder\n)\n\nimages = dalle2(\n    ['cute puppy chasing after a squirrel'],\n    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n)\n\n# save your image (in this example, of size 256x256)\n```\n\nEverything in this readme should run without error\n\nYou can also train the decoder on images of greater than the size (say 512x512) at which CLIP was trained (256x256). The images will be resized to CLIP image resolution for the image embeddings\n\nFor the layperson, no worries, training will all be automated into a CLI tool, at least for small scale training.\n\n## Training on Preprocessed CLIP Embeddings\n\nIt is likely, when scaling up, that you would first preprocess your images and text into corresponding embeddings before training the prior network. You can do so easily by simply passing in `image_embed`, `text_embed`, and optionally `text_encodings`\n\nWorking example below\n\n```python\nimport torch\nfrom dalle2_pytorch import DiffusionPriorNetwork, DiffusionPrior, CLIP\n\n# get trained CLIP from step one\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 6,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 6,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8,\n).cuda()\n\n# setup prior network, which contains an autoregressive transformer\n\nprior_network = DiffusionPriorNetwork(\n    dim = 512,\n    depth = 6,\n    dim_head = 64,\n    heads = 8\n).cuda()\n\n# diffusion prior network, which contains the CLIP and network (with transformer) above\n\ndiffusion_prior = DiffusionPrior(\n    net = prior_network,\n    clip = clip,\n    timesteps = 100,\n    cond_drop_prob = 0.2,\n    condition_on_text_encodings = False  # this probably should be true, but just to get Laion started\n).cuda()\n\n# mock data\n\ntext = torch.randint(0, 49408, (4, 256)).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# precompute the text and image embeddings\n# here using the diffusion prior class, but could be done with CLIP alone\n\nclip_image_embeds = diffusion_prior.clip.embed_image(images).image_embed\nclip_text_embeds = diffusion_prior.clip.embed_text(text).text_embed\n\n# feed text and images into diffusion prior network\n\nloss = diffusion_prior(\n    text_embed = clip_text_embeds,\n    image_embed = clip_image_embeds\n)\n\nloss.backward()\n\n# do the above for many many many steps\n# now the diffusion prior can generate image embeddings from the text embeddings\n```\n\nYou can also completely go `CLIP`-less, in which case you will need to pass in the `image_embed_dim` into the `DiffusionPrior` on initialization\n\n```python\nimport torch\nfrom dalle2_pytorch import DiffusionPriorNetwork, DiffusionPrior\n\n# setup prior network, which contains an autoregressive transformer\n\nprior_network = DiffusionPriorNetwork(\n    dim = 512,\n    depth = 6,\n    dim_head = 64,\n    heads = 8\n).cuda()\n\n# diffusion prior network, which contains the CLIP and network (with transformer) above\n\ndiffusion_prior = DiffusionPrior(\n    net = prior_network,\n    image_embed_dim = 512,               # this needs to be set\n    timesteps = 100,\n    cond_drop_prob = 0.2,\n    condition_on_text_encodings = False  # this probably should be true, but just to get Laion started\n).cuda()\n\n# mock data\n\ntext = torch.randint(0, 49408, (4, 256)).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# precompute the text and image embeddings\n# here using the diffusion prior class, but could be done with CLIP alone\n\nclip_image_embeds = torch.randn(4, 512).cuda()\nclip_text_embeds = torch.randn(4, 512).cuda()\n\n# feed text and images into diffusion prior network\n\nloss = diffusion_prior(\n    text_embed = clip_text_embeds,\n    image_embed = clip_image_embeds\n)\n\nloss.backward()\n\n# do the above for many many many steps\n# now the diffusion prior can generate image embeddings from the text embeddings\n```\n\n## OpenAI CLIP\n\nAlthough there is the possibility they are using an unreleased, more powerful CLIP, you can use one of the released ones, if you do not wish to train your own CLIP from scratch. This will also allow the community to more quickly validate the conclusions of the paper.\n\nTo use a pretrained OpenAI CLIP, simply import `OpenAIClipAdapter` and pass it into the `DiffusionPrior` or `Decoder` like so\n\n```python\nimport torch\nfrom dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder, OpenAIClipAdapter\n\n# openai pretrained clip - defaults to ViT-B/32\n\nclip = OpenAIClipAdapter()\n\n# mock data\n\ntext = torch.randint(0, 49408, (4, 256)).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# prior networks (with transformer)\n\nprior_network = DiffusionPriorNetwork(\n    dim = 512,\n    depth = 6,\n    dim_head = 64,\n    heads = 8\n).cuda()\n\ndiffusion_prior = DiffusionPrior(\n    net = prior_network,\n    clip = clip,\n    timesteps = 100,\n    cond_drop_prob = 0.2\n).cuda()\n\nloss = diffusion_prior(text, images)\nloss.backward()\n\n# do above for many steps ...\n\n# decoder (with unet)\n\nunet1 = Unet(\n    dim = 128,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults=(1, 2, 4, 8),\n    text_embed_dim = 512,\n    cond_on_text_encodings = True  # set to True for any unets that need to be conditioned on text encodings (ex. first unet in cascade)\n).cuda()\n\nunet2 = Unet(\n    dim = 16,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults = (1, 2, 4, 8, 16)\n).cuda()\n\ndecoder = Decoder(\n    unet = (unet1, unet2),\n    image_sizes = (128, 256),\n    clip = clip,\n    timesteps = 1000,\n    sample_timesteps = (250, 27),\n    image_cond_drop_prob = 0.1,\n    text_cond_drop_prob = 0.5\n).cuda()\n\nfor unet_number in (1, 2):\n    loss = decoder(images, text = text, unet_number = unet_number) # this can optionally be decoder(images, text) if you wish to condition on the text encodings as well, though it was hinted in the paper it didn't do much\n    loss.backward()\n\n# do above for many steps\n\ndalle2 = DALLE2(\n    prior = diffusion_prior,\n    decoder = decoder\n)\n\nimages = dalle2(\n    ['a butterfly trying to escape a tornado'],\n    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n)\n\n# save your image (in this example, of size 256x256)\n```\n\nAlternatively, you can also use <a href=\"https://github.com/mlfoundations/open_clip\">Open Clip</a>\n\n```bash\n$ pip install open-clip-torch\n```\n\nEx. using the <a href=\"https://laion.ai/blog/large-openclip/\">SOTA Open Clip</a> model trained by <a href=\"https://github.com/rom1504\">Romain</a>\n\n```python\nfrom dalle2_pytorch import OpenClipAdapter\n\nclip = OpenClipAdapter('ViT-H/14')\n```\n\nNow you'll just have to worry about training the Prior and the Decoder!\n\n## Inpainting\n\nInpainting is also built into the `Decoder`. You simply have to pass in the `inpaint_image` and `inpaint_mask` (boolean tensor where `True` indicates which regions of the inpaint image to keep)\n\nThis repository uses the formulation put forth by <a href=\"https://arxiv.org/abs/2201.09865\">Lugmayr et al. in Repaint</a>\n\n```python\nimport torch\nfrom dalle2_pytorch import Unet, Decoder, CLIP\n\n# trained clip from step 1\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 6,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 6,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8\n).cuda()\n\n# 2 unets for the decoder (a la cascading DDPM)\n\nunet = Unet(\n    dim = 16,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults = (1, 1, 1, 1)\n).cuda()\n\n\n# decoder, which contains the unet(s) and clip\n\ndecoder = Decoder(\n    clip = clip,\n    unet = (unet,),               # insert both unets in order of low resolution to highest resolution (you can have as many stages as you want here)\n    image_sizes = (256,),         # resolutions, 256 for first unet, 512 for second. these must be unique and in ascending order (matches with the unets passed in)\n    timesteps = 1000,\n    image_cond_drop_prob = 0.1,\n    text_cond_drop_prob = 0.5\n).cuda()\n\n# mock images (get a lot of this)\n\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# feed images into decoder, specifying which unet you want to train\n# each unet can be trained separately, which is one of the benefits of the cascading DDPM scheme\n\nloss = decoder(images, unet_number = 1)\nloss.backward()\n\n# do the above for many steps for both unets\n\nmock_image_embed = torch.randn(1, 512).cuda()\n\n# then to do inpainting\n\ninpaint_image = torch.randn(1, 3, 256, 256).cuda()      # (batch, channels, height, width)\ninpaint_mask = torch.ones(1, 256, 256).bool().cuda()    # (batch, height, width)\n\ninpainted_images = decoder.sample(\n    image_embed = mock_image_embed,\n    inpaint_image = inpaint_image,    # just pass in the inpaint image\n    inpaint_mask = inpaint_mask       # and the mask\n)\n\ninpainted_images.shape # (1, 3, 256, 256)\n```\n\n## Experimental\n\n### DALL-E2 with Latent Diffusion\n\nThis repository decides to take the next step and offer DALL-E v2 combined with <a href=\"https://huggingface.co/spaces/multimodalart/latentdiffusion\">latent diffusion</a>, from Rombach et al.\n\nYou can use it as follows. Latent diffusion can be limited to just the first U-Net in the cascade, or to any number you wish.\n\nThe repository also comes equipped with all the necessary settings to recreate `ViT-VQGan` from the <a href=\"https://arxiv.org/abs/2110.04627\">Improved VQGans</a> paper. Furthermore, the <a href=\"https://github.com/lucidrains/vector-quantize-pytorch\">vector quantization</a> library also comes equipped to do <a href=\"https://arxiv.org/abs/2203.01941\">residual or multi-headed quantization</a>, which I believe will give an even further boost in performance to the autoencoder.\n\n```python\nimport torch\nfrom dalle2_pytorch import Unet, Decoder, CLIP, VQGanVAE\n\n# trained clip from step 1\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 1,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 1,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8\n)\n\n# 3 unets for the decoder (a la cascading DDPM)\n\n# first two unets are doing latent diffusion\n# vqgan-vae must be trained beforehand\n\nvae1 = VQGanVAE(\n    dim = 32,\n    image_size = 256,\n    layers = 3,\n    layer_mults = (1, 2, 4)\n)\n\nvae2 = VQGanVAE(\n    dim = 32,\n    image_size = 512,\n    layers = 3,\n    layer_mults = (1, 2, 4)\n)\n\nunet1 = Unet(\n    dim = 32,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    sparse_attn = True,\n    sparse_attn_window = 2,\n    dim_mults = (1, 2, 4, 8)\n)\n\nunet2 = Unet(\n    dim = 32,\n    image_embed_dim = 512,\n    channels = 3,\n    dim_mults = (1, 2, 4, 8, 16),\n    cond_on_image_embeds = True,\n    cond_on_text_encodings = False\n)\n\nunet3 = Unet(\n    dim = 32,\n    image_embed_dim = 512,\n    channels = 3,\n    dim_mults = (1, 2, 4, 8, 16),\n    cond_on_image_embeds = True,\n    cond_on_text_encodings = False,\n    attend_at_middle = False\n)\n\n# decoder, which contains the unet(s) and clip\n\ndecoder = Decoder(\n    clip = clip,\n    vae = (vae1, vae2),                # latent diffusion for unet1 (vae1) and unet2 (vae2), but not for the last unet3\n    unet = (unet1, unet2, unet3),      # insert unets in order of low resolution to highest resolution (you can have as many stages as you want here)\n    image_sizes = (256, 512, 1024),    # resolutions, 256 for first unet, 512 for second, 1024 for third\n    timesteps = 100,\n    image_cond_drop_prob = 0.1,\n    text_cond_drop_prob = 0.5\n).cuda()\n\n# mock images (get a lot of this)\n\nimages = torch.randn(1, 3, 1024, 1024).cuda()\n\n# feed images into decoder, specifying which unet you want to train\n# each unet can be trained separately, which is one of the benefits of the cascading DDPM scheme\n\nwith decoder.one_unet_in_gpu(1):\n    loss = decoder(images, unet_number = 1)\n    loss.backward()\n\nwith decoder.one_unet_in_gpu(2):\n    loss = decoder(images, unet_number = 2)\n    loss.backward()\n\nwith decoder.one_unet_in_gpu(3):\n    loss = decoder(images, unet_number = 3)\n    loss.backward()\n\n# do the above for many steps for both unets\n\n# then it will learn to generate images based on the CLIP image embeddings\n\n# chaining the unets from lowest resolution to highest resolution (thus cascading)\n\nmock_image_embed = torch.randn(1, 512).cuda()\nimages = decoder.sample(mock_image_embed) # (1, 3, 1024, 1024)\n```\n\n## Training wrapper\n\n### Decoder Training\n\nTraining the `Decoder` may be confusing, as one needs to keep track of an optimizer for each of the `Unet`(s) separately. Each `Unet` will also need its own corresponding exponential moving average. The `DecoderTrainer` hopes to make this simple, as shown below\n\n```python\nimport torch\nfrom dalle2_pytorch import DALLE2, Unet, Decoder, CLIP, DecoderTrainer\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 6,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 6,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8\n).cuda()\n\n# mock data\n\ntext = torch.randint(0, 49408, (32, 256)).cuda()\nimages = torch.randn(32, 3, 256, 256).cuda()\n\n# decoder (with unet)\n\nunet1 = Unet(\n    dim = 128,\n    image_embed_dim = 512,\n    text_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults=(1, 2, 4, 8),\n    cond_on_text_encodings = True,\n).cuda()\n\nunet2 = Unet(\n    dim = 16,\n    image_embed_dim = 512,\n    cond_dim = 128,\n    channels = 3,\n    dim_mults = (1, 2, 4, 8, 16),\n).cuda()\n\ndecoder = Decoder(\n    unet = (unet1, unet2),\n    image_sizes = (128, 256),\n    clip = clip,\n    timesteps = 1000\n).cuda()\n\ndecoder_trainer = DecoderTrainer(\n    decoder,\n    lr = 3e-4,\n    wd = 1e-2,\n    ema_beta = 0.99,\n    ema_update_after_step = 1000,\n    ema_update_every = 10,\n)\n\nfor unet_number in (1, 2):\n    loss = decoder_trainer(\n        images,\n        text = text,\n        unet_number = unet_number, # which unet to train on\n        max_batch_size = 4         # gradient accumulation - this sets the maximum batch size in which to do forward and backwards pass - for this example 32 / 4 == 8 times\n    )\n\n    decoder_trainer.update(unet_number) # update the specific unet as well as its exponential moving average\n\n# after much training\n# you can sample from the exponentially moving averaged unets as so\n\nmock_image_embed = torch.randn(32, 512).cuda()\nimages = decoder_trainer.sample(image_embed = mock_image_embed, text = text) # (4, 3, 256, 256)\n```\n\n### Diffusion Prior Training\n\nSimilarly, one can use the `DiffusionPriorTrainer` to automatically instantiate and keep track of an exponential moving averaged prior.\n\n```python\nimport torch\nfrom dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, DiffusionPriorTrainer, Unet, Decoder, CLIP\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 49408,\n    text_enc_depth = 6,\n    text_seq_len = 256,\n    text_heads = 8,\n    visual_enc_depth = 6,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8\n).cuda()\n\n# mock data\n\ntext = torch.randint(0, 49408, (512, 256)).cuda()\nimages = torch.randn(512, 3, 256, 256).cuda()\n\n# prior networks (with transformer)\n\nprior_network = DiffusionPriorNetwork(\n    dim = 512,\n    depth = 6,\n    dim_head = 64,\n    heads = 8\n).cuda()\n\ndiffusion_prior = DiffusionPrior(\n    net = prior_network,\n    clip = clip,\n    timesteps = 100,\n    cond_drop_prob = 0.2\n).cuda()\n\ndiffusion_prior_trainer = DiffusionPriorTrainer(\n    diffusion_prior,\n    lr = 3e-4,\n    wd = 1e-2,\n    ema_beta = 0.99,\n    ema_update_after_step = 1000,\n    ema_update_every = 10,\n)\n\nloss = diffusion_prior_trainer(text, images, max_batch_size = 4)\ndiffusion_prior_trainer.update()  # this will update the optimizer as well as the exponential moving averaged diffusion prior\n\n# after much of the above three lines in a loop\n# you can sample from the exponential moving average of the diffusion prior identically to how you do so for DiffusionPrior\n\nimage_embeds = diffusion_prior_trainer.sample(text, max_batch_size = 4) # (512, 512) - exponential moving averaged image embeddings\n```\n\n## Bonus\n\n### Unconditional Training\n\nThe repository also contains the means to train unconditional DDPM model, or even cascading DDPMs. You simply have to set `unconditional = True` in the `Decoder`\n\nex.\n\n```python\nimport torch\nfrom dalle2_pytorch import Unet, Decoder, DecoderTrainer\n\n# unet for the cascading ddpm\n\nunet1 = Unet(\n    dim = 128,\n    dim_mults=(1, 2, 4, 8)\n).cuda()\n\nunet2 = Unet(\n    dim = 32,\n    dim_mults = (1, 2, 4, 8, 16)\n).cuda()\n\n# decoder, which contains the unets\n\ndecoder = Decoder(\n    unet = (unet1, unet2),\n    image_sizes = (256, 512),  # first unet up to 256px, then second to 512px\n    timesteps = 1000,\n    unconditional = True\n).cuda()\n\n# decoder trainer\n\ndecoder_trainer = DecoderTrainer(decoder)\n\n# images (get a lot of this)\n\nimages = torch.randn(1, 3, 512, 512).cuda()\n\n# feed images into decoder\n\nfor i in (1, 2):\n    loss = decoder_trainer(images, unet_number = i)\n    decoder_trainer.update(unet_number = i)\n\n# do the above for many many many many images\n# then it will learn to generate images\n\nimages = decoder_trainer.sample(batch_size = 36, max_batch_size = 4) # (36, 3, 512, 512)\n```\n\n## Dataloaders\n\n### Decoder Dataloaders\n\nIn order to make loading data simple and efficient, we include some general dataloaders that can be used to train portions of the network.\n\n#### Decoder: Image Embedding Dataset\n\nWhen training the decoder (and up samplers if training together) in isolation, you will need to load images and corresponding image embeddings. This dataset can read two similar types of datasets. First, it can read a [webdataset](https://github.com/webdataset/webdataset) that contains `.jpg` and `.npy` files in the `.tar`s that contain the images and associated image embeddings respectively. Alternatively, you can also specify a source for the embeddings outside of the webdataset. In this case, the path to the embeddings should contain `.npy` files with the same shard numbers as the webdataset and there should be a correspondence between the filename of the `.jpg` and the index of the embedding in the `.npy`. So, for example, `0001.tar` from the webdataset with image `00010509.jpg` (the first 4 digits are the shard number and the last 4 are the index) in it should be paralleled by a `img_emb_0001.npy` which contains a NumPy array with the embedding at index 509.\n\nGenerating a dataset of this type: \n1. Use [img2dataset](https://github.com/rom1504/img2dataset) to generate a webdataset.\n2. Use [clip-retrieval](https://github.com/rom1504/clip-retrieval) to convert the images to embeddings.\n3. Use [embedding-dataset-reordering](https://github.com/Veldrovive/embedding-dataset-reordering) to reorder the embeddings into the expected format.\n\nUsage:\n\n```python\nfrom dalle2_pytorch.dataloaders import ImageEmbeddingDataset, create_image_embedding_dataloader\n\n# Create a dataloader directly.\ndataloader = create_image_embedding_dataloader(\n    tar_url=\"/path/or/url/to/webdataset/{0000..9999}.tar\", # Uses bracket expanding notation. This specifies to read all tars from 0000.tar to 9999.tar\n    embeddings_url=\"path/or/url/to/embeddings/folder\",     # Included if .npy files are not in webdataset. Left out or set to None otherwise\n    num_workers=4,\n    batch_size=32,\n    shard_width=4,                                         # If a file in the webdataset shard 3 is named 0003039.jpg, we know the shard width is 4 and the last three digits are the index\n    shuffle_num=200,                                       # Does a shuffle of the data with a buffer size of 200\n    shuffle_shards=True,                                   # Shuffle the order the shards are read in\n    resample_shards=False,                                 # Sample shards with replacement. If true, an epoch will be infinite unless stopped manually\n)\nfor img, emb in dataloader:\n    print(img.shape)  # torch.Size([32, 3, 256, 256])\n    print(emb[\"img\"].shape)  # torch.Size([32, 512])\n    # Train decoder only as shown above\n\n# Or create a dataset without a loader so you can configure it manually\ndataset = ImageEmbeddingDataset(\n    urls=\"/path/or/url/to/webdataset/{0000..9999}.tar\",\n    embedding_folder_url=\"path/or/url/to/embeddings/folder\",\n    shard_width=4,\n    shuffle_shards=True,\n    resample=False\n)\n```\n\n### Scripts\n\n#### `train_diffusion_prior.py`\n\nFor detailed information on training the diffusion prior, please refer to the [dedicated readme](prior.md)\n\n## Todo\n\n- [x] finish off gaussian diffusion class for latent embedding - allow for prediction of epsilon\n- [x] add what was proposed in the paper, where DDPM objective for image latent embedding predicts x0 directly (reread vq-diffusion paper and get caught up on that line of work)\n- [x] make sure it works end to end to produce an output tensor, taking a single gradient step\n- [x] augment unet so that it can also be conditioned on text encodings (although in paper they hinted this didn't make much a difference)\n- [x] figure out all the current bag of tricks needed to make DDPMs great (starting with the blur trick mentioned in paper)\n- [x] build the cascading ddpm by having Decoder class manage multiple unets at different resolutions\n- [x] add efficient attention in unet\n- [x] be able to finely customize what to condition on (text, image embed) for specific unet in the cascade (super resolution ddpms near the end may not need too much conditioning)\n- [x] offload unets not being trained on to CPU for memory efficiency (for training each resolution unets separately)\n- [x] build out latent diffusion architecture, with the vq-reg variant (vqgan-vae), make it completely optional and compatible with cascading ddpms\n- [x] for decoder, allow ability to customize objective (predict epsilon vs x0), in case latent diffusion does better with prediction of x0\n- [x] use attention-based upsampling https://arxiv.org/abs/2112.11435\n- [x] use inheritance just this once for sharing logic between decoder and prior network ddpms\n- [x] bring in vit-vqgan https://arxiv.org/abs/2110.04627 for the latent diffusion\n- [x] abstract interface for CLIP adapter class, so other CLIPs can be brought in\n- [x] take care of mixed precision as well as gradient accumulation within decoder trainer\n- [x] just take care of the training for the decoder in a wrapper class, as each unet in the cascade will need its own optimizer\n- [x] bring in tools to train vqgan-vae\n- [x] add convnext backbone for vqgan-vae (in addition to vit [vit-vqgan] + resnet)\n- [x] make sure DDPMs can be run with traditional resnet blocks (but leave convnext as an option for experimentation)\n- [x] make sure for the latter unets in the cascade, one can train on crops for learning super resolution (constrain the unet to be only convolutions in that case, or allow conv-like attention with rel pos bias)\n- [x] offer setting in diffusion prior to split time and image embeddings into multiple tokens, configurable, for more surface area during attention\n- [x] make sure resnet hyperparameters can be configurable across unet depth (groups and expansion factor)\n- [x] pull logic for training diffusion prior into a class DiffusionPriorTrainer, for eventual script based + CLI based training\n- [x] make sure the cascading ddpm in the repository can be trained unconditionally, offer a one-line CLI tool for training on a folder of images\n- [x] bring in cross-scale embedding from iclr paper https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/crossformer.py#L14\n- [x] cross embed layers for downsampling, as an option\n- [x] use an experimental tracker agnostic setup, as done <a href=\"https://github.com/lucidrains/tf-bind-transformer#simple-trainer-class-for-fine-tuning\">here</a>\n- [x] use pydantic for config drive training\n- [x] for both diffusion prior and decoder, all exponential moving averaged models needs to be saved and restored as well (as well as the step number)\n- [x] offer save / load methods on the trainer classes to automatically take care of state dicts for scalers / optimizers / saving versions and checking for breaking changes\n- [x] allow for creation of diffusion prior model off pydantic config classes - consider the same for tracker configs\n- [x] bring in skip-layer excitations (from lightweight gan paper) to see if it helps for either decoder of unet or vqgan-vae training (doesnt work well)\n- [x] test out grid attention in cascading ddpm locally, decide whether to keep or remove https://arxiv.org/abs/2204.01697 (keeping, seems to be fine)\n- [x] allow for unet to be able to condition non-cross attention style as well\n- [x] speed up inference, read up on papers (ddim)\n- [x] add inpainting ability using resampler from repaint paper https://arxiv.org/abs/2201.09865\n- [x] add the final combination of upsample feature maps, used in unet squared, seems to have an effect in local experiments\n- [ ] consider elucidated dalle2 https://arxiv.org/abs/2206.00364\n- [ ] add simple outpainting, text-guided 2x size the image for starters\n- [ ] interface out the vqgan-vae so a pretrained one can be pulled off the shelf to validate latent diffusion + DALL-E2\n\n## Citations\n\n```bibtex\n@misc{ramesh2022,\n    title   = {Hierarchical Text-Conditional Image Generation with CLIP Latents}, \n    author  = {Aditya Ramesh et al},\n    year    = {2022}\n}\n```\n\n```bibtex\n@misc{crowson2022,\n    author  = {Katherine Crowson},\n    url     = {https://twitter.com/rivershavewings}\n}\n```\n\n```bibtex\n@misc{rombach2021highresolution,\n    title   = {High-Resolution Image Synthesis with Latent Diffusion Models}, \n    author  = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer},\n    year    = {2021},\n    eprint  = {2112.10752},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@article{shen2019efficient,\n    author  = {Zhuoran Shen and Mingyuan Zhang and Haiyu Zhao and Shuai Yi and Hongsheng Li},\n    title   = {Efficient Attention: Attention with Linear Complexities},\n    journal = {CoRR},\n    year    = {2018},\n    url     = {http://arxiv.org/abs/1812.01243},\n}\n```\n\n```bibtex\n@article{Yu2021VectorquantizedIM,\n    title   = {Vector-quantized Image Modeling with Improved VQGAN},\n    author  = {Jiahui Yu and Xin Li and Jing Yu Koh and Han Zhang and Ruoming Pang and James Qin and Alexander Ku and Yuanzhong Xu and Jason Baldridge and Yonghui Wu},\n    journal = {ArXiv},\n    year    = {2021},\n    volume  = {abs/2110.04627}\n}\n```\n\n```bibtex\n@article{Shleifer2021NormFormerIT,\n    title   = {NormFormer: Improved Transformer Pretraining with Extra Normalization},\n    author  = {Sam Shleifer and Jason Weston and Myle Ott},\n    journal = {ArXiv},\n    year    = {2021},\n    volume  = {abs/2110.09456}\n}\n```\n\n```bibtex\n@article{Yu2022CoCaCC,\n    title   = {CoCa: Contrastive Captioners are Image-Text Foundation Models},\n    author  = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2205.01917}\n}\n```\n\n```bibtex\n@misc{wang2021crossformer,\n    title   = {CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention},\n    author  = {Wenxiao Wang and Lu Yao and Long Chen and Binbin Lin and Deng Cai and Xiaofei He and Wei Liu},\n    year    = {2021},\n    eprint  = {2108.00154},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@article{ho2021cascaded,\n    title   = {Cascaded Diffusion Models for High Fidelity Image Generation},\n    author  = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},\n    journal = {arXiv preprint arXiv:2106.15282},\n    year    = {2021}\n}\n```\n\n```bibtex\n@misc{Saharia2022,\n    title   = {Imagen: unprecedented photorealism √ó deep level of language understanding},\n    author  = {Chitwan Saharia*, William Chan*, Saurabh Saxena‚Ä†, Lala Li‚Ä†, Jay Whang‚Ä†, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho‚Ä†, David Fleet‚Ä†, Mohammad Norouzi*},\n    year    = {2022}\n}\n```\n\n```bibtex\n@article{Choi2022PerceptionPT,\n    title   = {Perception Prioritized Training of Diffusion Models},\n    author  = {Jooyoung Choi and Jungbeom Lee and Chaehun Shin and Sungwon Kim and Hyunwoo J. Kim and Sung-Hoon Yoon},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2204.00227}\n}\n```\n\n```bibtex\n@article{Saharia2021PaletteID,\n    title   = {Palette: Image-to-Image Diffusion Models},\n    author  = {Chitwan Saharia and William Chan and Huiwen Chang and Chris A. Lee and Jonathan Ho and Tim Salimans and David J. Fleet and Mohammad Norouzi},\n    journal = {ArXiv},\n    year    = {2021},\n    volume  = {abs/2111.05826}\n}\n```\n\n```bibtex\n@article{Lugmayr2022RePaintIU,\n    title   = {RePaint: Inpainting using Denoising Diffusion Probabilistic Models},\n    author  = {Andreas Lugmayr and Martin Danelljan and Andr{\\'e}s Romero and Fisher Yu and Radu Timofte and Luc Van Gool},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2201.09865}\n}\n```\n\n```bibtex\n@misc{chen2022analog,\n    title   = {Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning},\n    author  = {Ting Chen and Ruixiang Zhang and Geoffrey Hinton},\n    year    = {2022},\n    eprint  = {2208.04202},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@article{Qiao2019WeightS,\n    title   = {Weight Standardization},\n    author  = {Siyuan Qiao and Huiyu Wang and Chenxi Liu and Wei Shen and Alan Loddon Yuille},\n    journal = {ArXiv},\n    year    = {2019},\n    volume  = {abs/1903.10520}\n}\n```\n\n```bibtex\n@inproceedings{rogozhnikov2022einops,\n    title   = {Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},\n    author  = {Alex Rogozhnikov},\n    booktitle = {International Conference on Learning Representations},\n    year    = {2022},\n    url     = {https://openreview.net/forum?id=oapKSVM2bcj}\n}\n```\n\n```bibtex\n@article{Sunkara2022NoMS,\n    title   = {No More Strided Convolutions or Pooling: A New CNN Building Block for Low-Resolution Images and Small Objects},\n    author  = {Raja Sunkara and Tie Luo},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2208.03641}\n}\n```\n\n```bibtex\n@article{Salimans2022ProgressiveDF,\n    title   = {Progressive Distillation for Fast Sampling of Diffusion Models},\n    author  = {Tim Salimans and Jonathan Ho},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2202.00512}\n}\n```\n\n*Creating noise from data is easy; creating data from noise is generative modeling.* - <a href=\"https://arxiv.org/abs/2011.13456\">Yang Song's paper</a>\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dalle2.png",
          "type": "blob",
          "size": 403.7431640625,
          "content": null
        },
        {
          "name": "dalle2_pytorch",
          "type": "tree",
          "content": null
        },
        {
          "name": "prior.md",
          "type": "blob",
          "size": 13.6435546875,
          "content": "# Diffusion Prior\nThis readme serves as an introduction to the diffusion prior.\n\n## Intro\n\nA properly trained prior will allow you to translate between two embedding spaces. If you know *a priori* that two embeddings are connected some way‚Äîthen ability the translate between them could extremely helpful.\n\n### Motivation\n\nBefore we dive into the model, let‚Äôs look at a quick example of where the model may be helpful.\n\nFor demonstration purposes we will imagine that we wish to generate images from text using CLIP and a Decoder.\n\n> [CLIP](https://openai.com/blog/clip/) is a contrastive model that learns to maximize the cosine similarity between a given image and caption, however, there is no guarantee that these embeddings are in the same space. While the embeddings generated are ***close*** the image and text embeddings occupy two disjoint sets.\n\n```python\n# Load Models\nclip_model = clip.load(\"ViT-L/14\")\ndecoder = Decoder(checkpoint=\"best.pth\") # A decoder trained on CLIP Image embeddings\n\n# Retrieve prompt from user and encode with CLIP\nprompt = \"A corgi wearing sunglasses\"\ntokenized_text = tokenize(prompt)\ntext_embedding = clip_model.encode_text(tokenized_text)\n\n# Now, pass the text embedding to the decoder\npredicted_image = decoder.sample(text_embedding)\n```\n\n> **Question**: *Can you spot the issue here?*\n>\n> **Answer**: *We‚Äôre trying to generate an image from a text embedding!*\n\nUnfortunately, we run into the issue previously mentioned--the image embeddings and the text embeddings are not interchangeable! Now let's look at a better solution\n\n```python\n# Load Models\nprior= Prior(checkpoint=\"prior.pth\") # A decoder trained to go from: text-> clip text emb -> clip img emb\ndecoder = Decoder(checkpoint=\"decoder.pth\") # A decoder trained on CLIP Image embeddings\n\n# Retrieve prompt from user and encode with a prior\nprompt = \"A corgi wearing sunglasses\"\ntokenized_text = tokenize(prompt)\ntext_embedding = prior.sample(tokenized_text) # <-- now we get an embedding in the same space as images!\n\n# Now, pass the predicted image embedding to the decoder\npredicted_image = decoder.sample(text_embedding)\n```\n\nWith the prior we are able to successfully generate embeddings *within* CLIP's image space! For this reason, the decoder will perform much better as it receives input that is much closer to its training data.\n\n> **You may be asking yourself the following question:**\n>\n> *\"Why don't you just train the decoder on clip text embeddings instead of image embeddings?\"*\n>\n> OpenAI covers this topic in their [DALLE-2 paper](https://arxiv.org/abs/2204.06125). The TL;DR is *\"it doesn't work as well as decoders trained on image embeddings\"*...also...its just an example :smile:\n\n## Usage\n\nTo utilize a pre-trained prior, it‚Äôs quite simple.\n\n### Loading Checkpoints\n```python\nimport torch\nfrom dalle2_pytorch import DiffusionPrior, DiffusionPriorNetwork, OpenAIClipAdapter\nfrom dalle2_pytorch.trainer import DiffusionPriorTrainer\n\ndef load_diffusion_model(dprior_path):\n\n    prior_network = DiffusionPriorNetwork(\n        dim=768,\n        depth=24,\n        dim_head=64,\n        heads=32,\n        normformer=True,\n        attn_dropout=5e-2,\n        ff_dropout=5e-2,\n        num_time_embeds=1,\n        num_image_embeds=1,\n        num_text_embeds=1,\n        num_timesteps=1000,\n        ff_mult=4\n    )\n\n    diffusion_prior = DiffusionPrior(\n        net=prior_network,\n        clip=OpenAIClipAdapter(\"ViT-L/14\"),\n        image_embed_dim=768,\n        timesteps=1000,\n        cond_drop_prob=0.1,\n        loss_type=\"l2\",\n        condition_on_text_encodings=True,\n\n    )\n\n    trainer = DiffusionPriorTrainer(\n        diffusion_prior=diffusion_prior,\n        lr=1.1e-4,\n        wd=6.02e-2,\n        max_grad_norm=0.5,\n        amp=False,\n        group_wd_params=True,\n        use_ema=True,\n        device=device,\n        accelerator=None,\n    )\n\n    trainer.load(dprior_path)\n\n    return trainer\n```\n\n Here we instantiate a model matches the configuration it was trained with, and then load the weights (*just like any other PyTorch model!*)\n\n### Sampling\nOnce we have a pre-trained model, generating embeddings is quite simple!\n```python\n# tokenize the text\ntokenized_text = clip.tokenize(\"<your amazing prompt>\")\n# predict an embedding\npredicted_embedding = prior.sample(tokenized_text, n_samples_per_batch=2, cond_scale=1.0)\n```\n\nThe resulting tensor returned from `.sample()` is of the same shape as your training data along the non-batch dimension(s). For example, a prior trained on `ViT-L/14` embeddings will predict an embedding of shape (1, 768).\n\n> For CLIP priors, this is quite handy as it means that you can use prior.sample(tokenizer_text) as a drop in replacement for clip.encode_text().\n\n**Some things to note:**\n* It is possible to specify the number of embeddings to sample from (the default suggested by OpenAI is `n=2`). Put simply, the idea here is that you avoid getting unlucky with a bad embedding generation by creating two; and selecting the one with the higher cosine similarity with the prompt.\n* You may specify a higher conditioning scale than the default (`1.0`). It is unclear whether OpenAI uses a higher value for the prior specifically, or only on the decoder. Local testing has shown poor results with anything higher than `1.0` but *ymmv*.\n\n---\n\n## Training\n\n### Overview\n\nTraining the prior is a relatively straightforward process thanks to the Trainer base class. The major step that is required of you is preparing a dataset in the format that EmbeddingReader expects. Having pre-computed embeddings massively increases training efficiency and is generally recommended as you will likely benefit from having them on hand for other tasks as well. Once you have a dataset, you are ready to move onto configuration\n\n## Dataset\n\nTo train the prior, it is highly recommended to use precomputed embeddings for the images. To obtain these for a custom dataset, you can leverage [img2datset](https://github.com/rom1504/img2dataset) to pull images from a list of URLs and [clip_retrieval](https://github.com/rom1504/clip-retrieval#clip-inference) for generating the actual embeddings that can be used in the prior's dataloader.\n\n## Configuration\n\nThe configuration file allows for you to easily track and reproduce experiments. It is a simple JSON file that will specify the architecture, dataset, and training parameters. For more information and specifics please see the configuration README.\n\n## Distributed Training\n\nIf you would like to train in a distributed manner we have opted to leverage huggingface‚Äô new Accelerate library. HFA makes it extremely simple to distribute work across multiple GPU‚Äôs and nodes. All that is required of you is to follow the simple CLI configuration tool [more information here](https://huggingface.co/docs/accelerate/accelerator).\n\n## Evaluation\n\nThere are a variety of metrics available to you when training the prior. You can read a brief description of each in the table below:\n| Metric                              | Description                                                                                                                                                                                                                                                  | Comments                                                                                                                                                                                                                                                                                                                                                |\n| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Online Model Validation             | The validation loss associated with your online model.                                                                                                                                                                                                       | Ideally validation loss will be as low as possible. Using L2 loss, values as low as `0.1` and lower are possible after around 1 Billion samples seen.                                                                                                                                                                                                |\n| EMA Validation                      | This metric measures the validation loss associated with your EMA model.                                                                                                                                                                                     | This will likely lag behind your \"online\" model's validation loss, but should outperform in the long-term.                                                                                                                                                                                                                                              |\n| Baseline Similarity                 | Baseline similarity refers to the similarity between your dataset's prompts and associated image embeddings. This will serve as a guide for your prior's performance in cosine similarity.                                                                    | Generally `0.3` is considered a good cosine similarity for caption similarity.                                                                                                                                                                                                                                                                         |\n| Similarity With Original Image      | This metric will measure the cosine similarity between your prior's predicted image embedding and the actual image that the caption was associated with. This is useful for determining wether your prior is generating images with the right contents.      | Values around `0.75`+ are obtainable. This metric should improve rapidly in the early stages of training and plateau with diminishing increases over time. If it takes hundreds of millions of samples to reach above `0.5`/`0.6` similarity--then you likely are suffering from some kind of training error or inefficiency (i.e. not using EMA) |\n| Difference From Baseline Similarity | Sometimes its useful to visualize a metric in another light. This metric will show you how your prior's predicted image embeddings match up with the baseline similarity measured in your dataset.                                                           | This value should float around `0.0` with some room for variation. After a billion samples seen, values are within `0.01`+/- of `0.0`. If this climbs to high, (~>`0.02`) then this may be a sign that your model is overfitting somehow.                                                                                                       |\n| Similarity With Text                | This metric is your bread and butter cosine similarity between the predicted image embedding and the original caption given to the prior. Monitoring this metric will be on of your main focuses and is probably the second most important behind your loss. | As mentioned, this value should be close to baseline similarity. We have observed early rapid increase with diminishing returns as the prior learns to generate valid image embeddings. If this value increases too far beyond the baseline similarity--it could be an indication that your model is overfitting.                                       |\n| Similarity With Unrelated Caption   | This metric will attempt to exposed an overfit prior by feeding it arbitrary prompts (from your dataset) and then measure the similarity of this predicted embedding with some other image.                                                                   | Early on we found that a poorly trained/modeled prior could effectively fool CLIP into believing that the cosine similarity between two images were high (when in fact the caption and image were completely unrelated). With this in mind--a low value is ideal, anything below `0.1` is probably safe.                                              |\n\n## Launching the script\n\nNow that you‚Äôve done all the prep it‚Äôs time for the easy part! üöÄ\n\nTo actually launch the script, you will either use `accelerate launch train_diffusion_prior.py --config_path <path to your config>` to launch with distributed training & huggingface accelerate or `python train_diffusion_prior.py` if you would like to train on your gpu/cpu without huggingface accelerate.\n\n## Checkpointing\n\nCheckpoints will be saved to the directory specified in your configuration file.\n\nAdditionally, a final checkpoint is saved before running the test split. This file will be saved to the same directory and titled ‚Äúlatest.pth‚Äù. This is to avoid problems where your `save_every` configuration does not overlap with the number of steps required to do a complete pass through the data.\n\n## Things To Keep In Mind\n\nThe prior has not been trained for tasks other than the traditional CLIP embedding translation‚Ä¶at least yet.\n\nAs we finalize the replication of unCLIP, there will almost assuredly be experiments attempting to apply the prior network to other tasks.\n\nWith that in mind, you are more or less a pioneer in embedding-translation if you are reading this and attempting something you don‚Äôt see documentation for!\n"
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.46484375,
          "content": "from setuptools import setup, find_packages\nexec(open('dalle2_pytorch/version.py').read())\n\nsetup(\n  name = 'dalle2-pytorch',\n  packages = find_packages(exclude=[]),\n  include_package_data = True,\n  entry_points={\n    'console_scripts': [\n      'dalle2_pytorch = dalle2_pytorch.cli:main',\n      'dream = dalle2_pytorch.cli:dream'\n    ],\n  },\n  version = __version__,\n  license='MIT',\n  description = 'DALL-E 2',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/dalle2-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'text to image'\n  ],\n  install_requires=[\n    'accelerate',\n    'click',\n    'open-clip-torch>=2.0.0,<3.0.0',\n    'clip-anytorch>=2.5.2',\n    'coca-pytorch>=0.0.5',\n    'ema-pytorch>=0.0.7',\n    'einops>=0.7.0',\n    'embedding-reader',\n    'kornia>=0.5.4',\n    'numpy',\n    'packaging',\n    'pillow',\n    'pydantic>=2',\n    'pytorch-warmup',\n    'resize-right>=0.0.2',\n    'rotary-embedding-torch',\n    'torch>=1.10',\n    'torchvision',\n    'tqdm',\n    'vector-quantize-pytorch',\n    'x-clip>=0.4.4',\n    'webdataset>=0.2.5',\n    'fsspec>=2022.1.0',\n    'torchmetrics[image]>=0.8.0'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)\n"
        },
        {
          "name": "test_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "train_decoder.py",
          "type": "blob",
          "size": 33.078125,
          "content": "from pathlib import Path\nfrom typing import List\nfrom datetime import timedelta\n\nfrom dalle2_pytorch.trainer import DecoderTrainer\nfrom dalle2_pytorch.dataloaders import create_image_embedding_dataloader\nfrom dalle2_pytorch.trackers import Tracker\nfrom dalle2_pytorch.train_configs import DecoderConfig, TrainDecoderConfig\nfrom dalle2_pytorch.utils import Timer, print_ribbon\nfrom dalle2_pytorch.dalle2_pytorch import Decoder, resize_image_to\nfrom clip import tokenize\n\nimport torchvision\nimport torch\nfrom torch import nn\nfrom torchmetrics.image.fid import FrechetInceptionDistance\nfrom torchmetrics.image.inception import InceptionScore\nfrom torchmetrics.image.kid import KernelInceptionDistance\nfrom torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\nfrom accelerate import Accelerator, DistributedDataParallelKwargs, InitProcessGroupKwargs\nfrom accelerate.utils import dataclasses as accelerate_dataclasses\nimport webdataset as wds\nimport click\n\n# constants\n\nTRAIN_CALC_LOSS_EVERY_ITERS = 10\nVALID_CALC_LOSS_EVERY_ITERS = 10\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\n# main functions\n\ndef create_dataloaders(\n    available_shards,\n    webdataset_base_url,\n    img_embeddings_url=None,\n    text_embeddings_url=None,\n    shard_width=6,\n    num_workers=4,\n    batch_size=32,\n    n_sample_images=6,\n    shuffle_train=True,\n    resample_train=False,\n    img_preproc = None,\n    index_width=4,\n    train_prop = 0.75,\n    val_prop = 0.15,\n    test_prop = 0.10,\n    seed = 0,\n    **kwargs\n):\n    \"\"\"\n    Randomly splits the available shards into train, val, and test sets and returns a dataloader for each\n    \"\"\"\n    assert train_prop + test_prop + val_prop == 1\n    num_train = round(train_prop*len(available_shards))\n    num_test = round(test_prop*len(available_shards))\n    num_val = len(available_shards) - num_train - num_test\n    assert num_train + num_test + num_val == len(available_shards), f\"{num_train} + {num_test} + {num_val} = {num_train + num_test + num_val} != {len(available_shards)}\"\n    train_split, test_split, val_split = torch.utils.data.random_split(available_shards, [num_train, num_test, num_val], generator=torch.Generator().manual_seed(seed))\n\n    # The shard number in the webdataset file names has a fixed width. We zero pad the shard numbers so they correspond to a filename.\n    train_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in train_split]\n    test_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in test_split]\n    val_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in val_split]\n    \n    create_dataloader = lambda tar_urls, shuffle=False, resample=False, for_sampling=False: create_image_embedding_dataloader(\n        tar_url=tar_urls,\n        num_workers=num_workers,\n        batch_size=batch_size if not for_sampling else n_sample_images,\n        img_embeddings_url=img_embeddings_url,\n        text_embeddings_url=text_embeddings_url,\n        index_width=index_width,\n        shuffle_num = None,\n        extra_keys= [\"txt\"],\n        shuffle_shards = shuffle,\n        resample_shards = resample, \n        img_preproc=img_preproc,\n        handler=wds.handlers.warn_and_continue\n    )\n\n    train_dataloader = create_dataloader(train_urls, shuffle=shuffle_train, resample=resample_train)\n    train_sampling_dataloader = create_dataloader(train_urls, shuffle=False, for_sampling=True)\n    val_dataloader = create_dataloader(val_urls, shuffle=False)\n    test_dataloader = create_dataloader(test_urls, shuffle=False)\n    test_sampling_dataloader = create_dataloader(test_urls, shuffle=False, for_sampling=True)\n    return {\n        \"train\": train_dataloader,\n        \"train_sampling\": train_sampling_dataloader,\n        \"val\": val_dataloader,\n        \"test\": test_dataloader,\n        \"test_sampling\": test_sampling_dataloader\n    }\n\ndef get_dataset_keys(dataloader):\n    \"\"\"\n    It is sometimes neccesary to get the keys the dataloader is returning. Since the dataset is burried in the dataloader, we need to do a process to recover it.\n    \"\"\"\n    # If the dataloader is actually a WebLoader, we need to extract the real dataloader\n    if isinstance(dataloader, wds.WebLoader):\n        dataloader = dataloader.pipeline[0]\n    return dataloader.dataset.key_map\n\ndef get_example_data(dataloader, device, n=5):\n    \"\"\"\n    Samples the dataloader and returns a zipped list of examples\n    \"\"\"\n    images = []\n    img_embeddings = []\n    text_embeddings = []\n    captions = []\n    for img, emb, txt in dataloader:\n        img_emb, text_emb = emb.get('img'), emb.get('text')\n        if img_emb is not None:\n            img_emb = img_emb.to(device=device, dtype=torch.float)\n            img_embeddings.extend(list(img_emb))\n        else:\n            # Then we add None img.shape[0] times\n            img_embeddings.extend([None]*img.shape[0])\n        if text_emb is not None:\n            text_emb = text_emb.to(device=device, dtype=torch.float)\n            text_embeddings.extend(list(text_emb))\n        else:\n            # Then we add None img.shape[0] times\n            text_embeddings.extend([None]*img.shape[0])\n        img = img.to(device=device, dtype=torch.float)\n        images.extend(list(img))\n        captions.extend(list(txt))\n        if len(images) >= n:\n            break\n    return list(zip(images[:n], img_embeddings[:n], text_embeddings[:n], captions[:n]))\n\ndef generate_samples(trainer, example_data, clip=None, start_unet=1, end_unet=None, condition_on_text_encodings=False, cond_scale=1.0, device=None, text_prepend=\"\", match_image_size=True):\n    \"\"\"\n    Takes example data and generates images from the embeddings\n    Returns three lists: real images, generated images, and captions\n    \"\"\"\n    real_images, img_embeddings, text_embeddings, txts = zip(*example_data)\n    sample_params = {}\n    if img_embeddings[0] is None:\n        # Generate image embeddings from clip\n        imgs_tensor = torch.stack(real_images)\n        assert clip is not None, \"clip is None, but img_embeddings is None\"\n        imgs_tensor.to(device=device)\n        img_embeddings, img_encoding = clip.embed_image(imgs_tensor)\n        sample_params[\"image_embed\"] = img_embeddings\n    else:\n        # Then we are using precomputed image embeddings\n        img_embeddings = torch.stack(img_embeddings)\n        sample_params[\"image_embed\"] = img_embeddings\n    if condition_on_text_encodings:\n        if text_embeddings[0] is None:\n            # Generate text embeddings from text\n            assert clip is not None, \"clip is None, but text_embeddings is None\"\n            tokenized_texts = tokenize(txts, truncate=True).to(device=device)\n            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n            sample_params[\"text_encodings\"] = text_encodings\n        else:\n            # Then we are using precomputed text embeddings\n            text_embeddings = torch.stack(text_embeddings)\n            sample_params[\"text_encodings\"] = text_embeddings\n    sample_params[\"start_at_unet_number\"] = start_unet\n    sample_params[\"stop_at_unet_number\"] = end_unet\n    if start_unet > 1:\n        # If we are only training upsamplers\n        sample_params[\"image\"] = torch.stack(real_images)\n    if device is not None:\n        sample_params[\"_device\"] = device\n    samples = trainer.sample(**sample_params, _cast_deepspeed_precision=False)  # At sampling time we don't want to cast to FP16\n    generated_images = list(samples)\n    captions = [text_prepend + txt for txt in txts]\n    if match_image_size:\n        generated_image_size = generated_images[0].shape[-1]\n        real_images = [resize_image_to(image, generated_image_size, clamp_range=(0, 1)) for image in real_images]\n    return real_images, generated_images, captions\n\ndef generate_grid_samples(trainer, examples, clip=None, start_unet=1, end_unet=None, condition_on_text_encodings=False, cond_scale=1.0, device=None, text_prepend=\"\"):\n    \"\"\"\n    Generates samples and uses torchvision to put them in a side by side grid for easy viewing\n    \"\"\"\n    real_images, generated_images, captions = generate_samples(trainer, examples, clip, start_unet, end_unet, condition_on_text_encodings, cond_scale, device, text_prepend)\n    grid_images = [torchvision.utils.make_grid([original_image, generated_image]) for original_image, generated_image in zip(real_images, generated_images)]\n    return grid_images, captions\n                    \ndef evaluate_trainer(trainer, dataloader, device, start_unet, end_unet, clip=None, condition_on_text_encodings=False, cond_scale=1.0, inference_device=None, n_evaluation_samples=1000, FID=None, IS=None, KID=None, LPIPS=None):\n    \"\"\"\n    Computes evaluation metrics for the decoder\n    \"\"\"\n    metrics = {}\n    # Prepare the data\n    examples = get_example_data(dataloader, device, n_evaluation_samples)\n    if len(examples) == 0:\n        print(\"No data to evaluate. Check that your dataloader has shards.\")\n        return metrics\n    real_images, generated_images, captions = generate_samples(trainer, examples, clip, start_unet, end_unet, condition_on_text_encodings, cond_scale, inference_device)\n    real_images = torch.stack(real_images).to(device=device, dtype=torch.float)\n    generated_images = torch.stack(generated_images).to(device=device, dtype=torch.float)\n    # Convert from [0, 1] to [0, 255] and from torch.float to torch.uint8\n    int_real_images = real_images.mul(255).add(0.5).clamp(0, 255).type(torch.uint8)\n    int_generated_images = generated_images.mul(255).add(0.5).clamp(0, 255).type(torch.uint8)\n\n    def null_sync(t, *args, **kwargs):\n        return [t]\n\n    if exists(FID):\n        fid = FrechetInceptionDistance(**FID, dist_sync_fn=null_sync)\n        fid.to(device=device)\n        fid.update(int_real_images, real=True)\n        fid.update(int_generated_images, real=False)\n        metrics[\"FID\"] = fid.compute().item()\n    if exists(IS):\n        inception = InceptionScore(**IS, dist_sync_fn=null_sync)\n        inception.to(device=device)\n        inception.update(int_real_images)\n        is_mean, is_std = inception.compute()\n        metrics[\"IS_mean\"] = is_mean.item()\n        metrics[\"IS_std\"] = is_std.item()\n    if exists(KID):\n        kernel_inception = KernelInceptionDistance(**KID, dist_sync_fn=null_sync)\n        kernel_inception.to(device=device)\n        kernel_inception.update(int_real_images, real=True)\n        kernel_inception.update(int_generated_images, real=False)\n        kid_mean, kid_std = kernel_inception.compute()\n        metrics[\"KID_mean\"] = kid_mean.item()\n        metrics[\"KID_std\"] = kid_std.item()\n    if exists(LPIPS):\n        # Convert from [0, 1] to [-1, 1]\n        renorm_real_images = real_images.mul(2).sub(1).clamp(-1,1)\n        renorm_generated_images = generated_images.mul(2).sub(1).clamp(-1,1)\n        lpips = LearnedPerceptualImagePatchSimilarity(**LPIPS, dist_sync_fn=null_sync)\n        lpips.to(device=device)\n        lpips.update(renorm_real_images, renorm_generated_images)\n        metrics[\"LPIPS\"] = lpips.compute().item()\n\n    if trainer.accelerator.num_processes > 1:\n        # Then we should sync the metrics\n        metrics_order = sorted(metrics.keys())\n        metrics_tensor = torch.zeros(1, len(metrics), device=device, dtype=torch.float)\n        for i, metric_name in enumerate(metrics_order):\n            metrics_tensor[0, i] = metrics[metric_name]\n        metrics_tensor = trainer.accelerator.gather(metrics_tensor)\n        metrics_tensor = metrics_tensor.mean(dim=0)\n        for i, metric_name in enumerate(metrics_order):\n            metrics[metric_name] = metrics_tensor[i].item()\n    return metrics\n\ndef save_trainer(tracker: Tracker, trainer: DecoderTrainer, epoch: int, sample: int, next_task: str, validation_losses: List[float], samples_seen: int, is_latest=True, is_best=False):\n    \"\"\"\n    Logs the model with an appropriate method depending on the tracker\n    \"\"\"\n    tracker.save(trainer, is_best=is_best, is_latest=is_latest, epoch=epoch, sample=sample, next_task=next_task, validation_losses=validation_losses, samples_seen=samples_seen)\n    \ndef recall_trainer(tracker: Tracker, trainer: DecoderTrainer):\n    \"\"\"\n    Loads the model with an appropriate method depending on the tracker\n    \"\"\"\n    trainer.accelerator.print(print_ribbon(f\"Loading model from {type(tracker.loader).__name__}\"))\n    state_dict = tracker.recall()\n    trainer.load_state_dict(state_dict, only_model=False, strict=True)\n    return state_dict.get(\"epoch\", 0), state_dict.get(\"validation_losses\", []), state_dict.get(\"next_task\", \"train\"), state_dict.get(\"sample\", 0), state_dict.get(\"samples_seen\", 0)\n\ndef train(\n    dataloaders,\n    decoder: Decoder,\n    accelerator: Accelerator,\n    tracker: Tracker,\n    inference_device,\n    clip=None,\n    evaluate_config=None,\n    epoch_samples = None,  # If the training dataset is resampling, we have to manually stop an epoch\n    validation_samples = None,\n    save_immediately=False,\n    epochs = 20,\n    n_sample_images = 5,\n    save_every_n_samples = 100000,\n    unet_training_mask=None,\n    condition_on_text_encodings=False,\n    cond_scale=1.0,\n    **kwargs\n):\n    \"\"\"\n    Trains a decoder on a dataset.\n    \"\"\"\n    is_master = accelerator.process_index == 0\n\n    if not exists(unet_training_mask):\n        # Then the unet mask should be true for all unets in the decoder\n        unet_training_mask = [True] * len(decoder.unets)\n    assert len(unet_training_mask) == len(decoder.unets), f\"The unet training mask should be the same length as the number of unets in the decoder. Got {len(unet_training_mask)} and {trainer.num_unets}\"\n    trainable_unet_numbers = [i+1 for i, trainable in enumerate(unet_training_mask) if trainable]\n    first_trainable_unet = trainable_unet_numbers[0]\n    last_trainable_unet = trainable_unet_numbers[-1]\n    def move_unets(unet_training_mask):\n        for i in range(len(decoder.unets)):\n            if not unet_training_mask[i]:\n                # Replace the unet from the module list with a nn.Identity(). This training script never uses unets that aren't being trained so this is fine.\n                decoder.unets[i] = nn.Identity().to(inference_device)\n    # Remove non-trainable unets\n    move_unets(unet_training_mask)\n\n    trainer = DecoderTrainer(\n        decoder=decoder,\n        accelerator=accelerator,\n        dataloaders=dataloaders,\n        **kwargs\n    )\n\n    # Set up starting model and parameters based on a recalled state dict\n    start_epoch = 0\n    validation_losses = []\n    next_task = 'train'\n    sample = 0\n    samples_seen = 0\n    val_sample = 0\n    step = lambda: int(trainer.num_steps_taken(unet_number=first_trainable_unet))\n\n    if tracker.can_recall:\n        start_epoch, validation_losses, next_task, recalled_sample, samples_seen = recall_trainer(tracker, trainer)\n        if next_task == 'train':\n            sample = recalled_sample\n        if next_task == 'val':\n            val_sample = recalled_sample\n        accelerator.print(f\"Loaded model from {type(tracker.loader).__name__} on epoch {start_epoch} having seen {samples_seen} samples with minimum validation loss {min(validation_losses) if len(validation_losses) > 0 else 'N/A'}\")\n        accelerator.print(f\"Starting training from task {next_task} at sample {sample} and validation sample {val_sample}\")\n    trainer.to(device=inference_device)\n\n    accelerator.print(print_ribbon(\"Generating Example Data\", repeat=40))\n    accelerator.print(\"This can take a while to load the shard lists...\")\n    if is_master:\n        train_example_data = get_example_data(dataloaders[\"train_sampling\"], inference_device, n_sample_images)\n        accelerator.print(\"Generated training examples\")\n        test_example_data = get_example_data(dataloaders[\"test_sampling\"], inference_device, n_sample_images)\n        accelerator.print(\"Generated testing examples\")\n    \n    send_to_device = lambda arr: [x.to(device=inference_device, dtype=torch.float) for x in arr]\n\n    sample_length_tensor = torch.zeros(1, dtype=torch.int, device=inference_device)\n    unet_losses_tensor = torch.zeros(TRAIN_CALC_LOSS_EVERY_ITERS, trainer.num_unets, dtype=torch.float, device=inference_device)\n    for epoch in range(start_epoch, epochs):\n        accelerator.print(print_ribbon(f\"Starting epoch {epoch}\", repeat=40))\n\n        timer = Timer()\n        last_sample = sample\n        last_snapshot = sample\n\n        if next_task == 'train':\n            for i, (img, emb, txt) in enumerate(dataloaders[\"train\"]):\n                # We want to count the total number of samples across all processes\n                sample_length_tensor[0] = len(img)\n                all_samples = accelerator.gather(sample_length_tensor)  # TODO: accelerator.reduce is broken when this was written. If it is fixed replace this.\n                total_samples = all_samples.sum().item()\n                sample += total_samples\n                samples_seen += total_samples\n                img_emb = emb.get('img')\n                has_img_embedding = img_emb is not None\n                if has_img_embedding:\n                    img_emb, = send_to_device((img_emb,))\n                text_emb = emb.get('text')\n                has_text_embedding = text_emb is not None\n                if has_text_embedding:\n                    text_emb, = send_to_device((text_emb,))\n                img, = send_to_device((img,))\n\n                trainer.train()\n                for unet in range(1, trainer.num_unets+1):\n                    # Check if this is a unet we are training\n                    if not unet_training_mask[unet-1]: # Unet index is the unet number - 1\n                        continue\n\n                    forward_params = {}\n                    if has_img_embedding:\n                        forward_params['image_embed'] = img_emb\n                    else:\n                        # Forward pass automatically generates embedding\n                        assert clip is not None\n                        img_embed, img_encoding = clip.embed_image(img)\n                        forward_params['image_embed'] = img_embed\n                    if condition_on_text_encodings:\n                        if has_text_embedding:\n                            forward_params['text_encodings'] = text_emb\n                        else:\n                            # Then we need to pass the text instead\n                            assert clip is not None\n                            tokenized_texts = tokenize(txt, truncate=True).to(inference_device)\n                            assert tokenized_texts.shape[0] == len(img), f\"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})\"\n                            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n                            forward_params['text_encodings'] = text_encodings\n                    loss = trainer.forward(img, **forward_params, unet_number=unet, _device=inference_device)\n                    trainer.update(unet_number=unet)\n                    unet_losses_tensor[i % TRAIN_CALC_LOSS_EVERY_ITERS, unet-1] = loss\n                \n                samples_per_sec = (sample - last_sample) / timer.elapsed()\n                timer.reset()\n                last_sample = sample\n\n                if i % TRAIN_CALC_LOSS_EVERY_ITERS == 0:\n                    # We want to average losses across all processes\n                    unet_all_losses = accelerator.gather(unet_losses_tensor)\n                    mask = unet_all_losses != 0\n                    unet_average_loss = (unet_all_losses * mask).sum(dim=0) / mask.sum(dim=0)\n                    loss_map = { f\"Unet {index} Training Loss\": loss.item() for index, loss in enumerate(unet_average_loss) if unet_training_mask[index] }\n\n                    # gather decay rate on each UNet\n                    ema_decay_list = {f\"Unet {index} EMA Decay\": ema_unet.get_current_decay() for index, ema_unet in enumerate(trainer.ema_unets) if unet_training_mask[index]}\n\n                    log_data = {\n                        \"Epoch\": epoch,\n                        \"Sample\": sample,\n                        \"Step\": i,\n                        \"Samples per second\": samples_per_sec,\n                        \"Samples Seen\": samples_seen,\n                        **ema_decay_list,\n                        **loss_map\n                    }\n\n                    if is_master:\n                        tracker.log(log_data, step=step())\n\n                if is_master and (last_snapshot + save_every_n_samples < sample or (save_immediately and i == 0)):  # This will miss by some amount every time, but it's not a big deal... I hope\n                    # It is difficult to gather this kind of info on the accelerator, so we have to do it on the master\n                    print(\"Saving snapshot\")\n                    last_snapshot = sample\n                    # We need to know where the model should be saved\n                    save_trainer(tracker, trainer, epoch, sample, next_task, validation_losses, samples_seen)\n                    if exists(n_sample_images) and n_sample_images > 0:\n                        trainer.eval()\n                        train_images, train_captions = generate_grid_samples(trainer, train_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Train: \")\n                        tracker.log_images(train_images, captions=train_captions, image_section=\"Train Samples\", step=step())\n                \n                if epoch_samples is not None and sample >= epoch_samples:\n                    break\n            next_task = 'val'\n            sample = 0\n\n        all_average_val_losses = None\n        if next_task == 'val':\n            trainer.eval()\n            accelerator.print(print_ribbon(f\"Starting Validation {epoch}\", repeat=40))\n            last_val_sample = val_sample\n            val_sample_length_tensor = torch.zeros(1, dtype=torch.int, device=inference_device)\n            average_val_loss_tensor = torch.zeros(1, trainer.num_unets, dtype=torch.float, device=inference_device)\n            timer = Timer()\n            accelerator.wait_for_everyone()\n            i = 0\n            for i, (img, emb, txt) in enumerate(dataloaders['val']):  # Use the accelerate prepared loader\n                val_sample_length_tensor[0] = len(img)\n                all_samples = accelerator.gather(val_sample_length_tensor)\n                total_samples = all_samples.sum().item()\n                val_sample += total_samples\n                img_emb = emb.get('img')\n                has_img_embedding = img_emb is not None\n                if has_img_embedding:\n                    img_emb, = send_to_device((img_emb,))\n                text_emb = emb.get('text')\n                has_text_embedding = text_emb is not None\n                if has_text_embedding:\n                    text_emb, = send_to_device((text_emb,))\n                img, = send_to_device((img,))\n\n                for unet in range(1, len(decoder.unets)+1):\n                    if not unet_training_mask[unet-1]: # Unet index is the unet number - 1\n                        # No need to evaluate an unchanging unet\n                        continue\n                        \n                    forward_params = {}\n                    if has_img_embedding:\n                        forward_params['image_embed'] = img_emb.float()\n                    else:\n                        # Forward pass automatically generates embedding\n                        assert clip is not None\n                        img_embed, img_encoding = clip.embed_image(img)\n                        forward_params['image_embed'] = img_embed\n                    if condition_on_text_encodings:\n                        if has_text_embedding:\n                            forward_params['text_encodings'] = text_emb.float()\n                        else:\n                            # Then we need to pass the text instead\n                            assert clip is not None\n                            tokenized_texts = tokenize(txt, truncate=True).to(device=inference_device)\n                            assert tokenized_texts.shape[0] == len(img), f\"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})\"\n                            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n                            forward_params['text_encodings'] = text_encodings\n                    loss = trainer.forward(img.float(), **forward_params, unet_number=unet, _device=inference_device)\n                    average_val_loss_tensor[0, unet-1] += loss\n\n                if i % VALID_CALC_LOSS_EVERY_ITERS == 0:\n                    samples_per_sec = (val_sample - last_val_sample) / timer.elapsed()\n                    timer.reset()\n                    last_val_sample = val_sample\n                    accelerator.print(f\"Epoch {epoch}/{epochs} Val Step {i} -  Sample {val_sample} - {samples_per_sec:.2f} samples/sec\")\n                    accelerator.print(f\"Loss: {(average_val_loss_tensor / (i+1))}\")\n                    accelerator.print(\"\")\n                \n                if validation_samples is not None and val_sample >= validation_samples:\n                    break\n            print(f\"Rank {accelerator.state.process_index} finished validation after {i} steps\")\n            accelerator.wait_for_everyone()\n            average_val_loss_tensor /= i+1\n            # Gather all the average loss tensors\n            all_average_val_losses = accelerator.gather(average_val_loss_tensor)\n            if is_master:\n                unet_average_val_loss = all_average_val_losses.mean(dim=0)\n                val_loss_map = { f\"Unet {index} Validation Loss\": loss.item() for index, loss in enumerate(unet_average_val_loss) if loss != 0 }\n                tracker.log(val_loss_map, step=step())\n            next_task = 'eval'\n\n        if next_task == 'eval':\n            if exists(evaluate_config):\n                accelerator.print(print_ribbon(f\"Starting Evaluation {epoch}\", repeat=40))\n                evaluation = evaluate_trainer(trainer, dataloaders[\"val\"], inference_device, first_trainable_unet, last_trainable_unet, clip=clip, inference_device=inference_device, **evaluate_config.model_dump(), condition_on_text_encodings=condition_on_text_encodings, cond_scale=cond_scale)\n                if is_master:\n                    tracker.log(evaluation, step=step())\n            next_task = 'sample'\n            val_sample = 0\n\n        if next_task == 'sample':\n            if is_master:\n                # Generate examples and save the model if we are the master\n                # Generate sample images\n                print(print_ribbon(f\"Sampling Set {epoch}\", repeat=40))\n                test_images, test_captions = generate_grid_samples(trainer, test_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Test: \")\n                train_images, train_captions = generate_grid_samples(trainer, train_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Train: \")\n                tracker.log_images(test_images, captions=test_captions, image_section=\"Test Samples\", step=step())\n                tracker.log_images(train_images, captions=train_captions, image_section=\"Train Samples\", step=step())\n\n                print(print_ribbon(f\"Starting Saving {epoch}\", repeat=40))\n                is_best = False\n                if all_average_val_losses is not None:\n                    average_loss = all_average_val_losses.mean(dim=0).sum() / sum(unet_training_mask)\n                    if len(validation_losses) == 0 or average_loss < min(validation_losses):\n                        is_best = True\n                    validation_losses.append(average_loss)\n                save_trainer(tracker, trainer, epoch, sample, next_task, validation_losses, samples_seen, is_best=is_best)\n            next_task = 'train'\n\ndef create_tracker(accelerator: Accelerator, config: TrainDecoderConfig, config_path: str, dummy: bool = False) -> Tracker:\n    tracker_config = config.tracker\n    accelerator_config = {\n        \"Distributed\": accelerator.distributed_type != accelerate_dataclasses.DistributedType.NO,\n        \"DistributedType\": accelerator.distributed_type,\n        \"NumProcesses\": accelerator.num_processes,\n        \"MixedPrecision\": accelerator.mixed_precision\n    }\n    accelerator.wait_for_everyone()  # If nodes arrive at this point at different times they might try to autoresume the current run which makes no sense and will cause errors\n    tracker: Tracker = tracker_config.create(config, accelerator_config, dummy_mode=dummy)\n    tracker.save_config(config_path, config_name='decoder_config.json')\n    tracker.add_save_metadata(state_dict_key='config', metadata=config.model_dump())\n    return tracker\n    \ndef initialize_training(config: TrainDecoderConfig, config_path):\n    # Make sure if we are not loading, distributed models are initialized to the same values\n    torch.manual_seed(config.seed)\n\n    # Set up accelerator for configurable distributed training\n    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=config.train.find_unused_parameters, static_graph=config.train.static_graph)\n    init_kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=60*60))\n    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs, init_kwargs])\n\n    if accelerator.num_processes > 1:\n        # We are using distributed training and want to immediately ensure all can connect\n        accelerator.print(\"Waiting for all processes to connect...\")\n        accelerator.wait_for_everyone()\n        accelerator.print(\"All processes online and connected\")\n\n    # If we are in deepspeed fp16 mode, we must ensure learned variance is off\n    if accelerator.mixed_precision == \"fp16\" and accelerator.distributed_type == accelerate_dataclasses.DistributedType.DEEPSPEED and config.decoder.learned_variance:\n        raise ValueError(\"DeepSpeed fp16 mode does not support learned variance\")\n    \n    # Set up data\n    all_shards = list(range(config.data.start_shard, config.data.end_shard + 1))\n    world_size = accelerator.num_processes\n    rank = accelerator.process_index\n    shards_per_process = len(all_shards) // world_size\n    assert shards_per_process > 0, \"Not enough shards to split evenly\"\n    my_shards = all_shards[rank * shards_per_process: (rank + 1) * shards_per_process]\n\n    dataloaders = create_dataloaders (\n        available_shards=my_shards,\n        img_preproc = config.data.img_preproc,\n        train_prop = config.data.splits.train,\n        val_prop = config.data.splits.val,\n        test_prop = config.data.splits.test,\n        n_sample_images=config.train.n_sample_images,\n        **config.data.model_dump(),\n        rank = rank,\n        seed = config.seed,\n    )\n\n    # If clip is in the model, we need to remove it for compatibility with deepspeed\n    clip = None\n    if config.decoder.clip is not None:\n        clip = config.decoder.clip.create()  # Of course we keep it to use it during training, just not in the decoder as that causes issues\n        config.decoder.clip = None\n    # Create the decoder model and print basic info\n    decoder = config.decoder.create()\n    get_num_parameters = lambda model, only_training=False: sum(p.numel() for p in model.parameters() if (p.requires_grad or not only_training))\n\n    # Create and initialize the tracker if we are the master\n    tracker = create_tracker(accelerator, config, config_path, dummy = rank!=0)\n\n    has_img_embeddings = config.data.img_embeddings_url is not None\n    has_text_embeddings = config.data.text_embeddings_url is not None\n    conditioning_on_text = any([unet.cond_on_text_encodings for unet in config.decoder.unets])\n\n    has_clip_model = clip is not None\n    data_source_string = \"\"\n\n    if has_img_embeddings:\n        data_source_string += \"precomputed image embeddings\"\n    elif has_clip_model:\n        data_source_string += \"clip image embeddings generation\"\n    else:\n        raise ValueError(\"No image embeddings source specified\")\n    if conditioning_on_text:\n        if has_text_embeddings:\n            data_source_string += \" and precomputed text embeddings\"\n        elif has_clip_model:\n            data_source_string += \" and clip text encoding generation\"\n        else:\n            raise ValueError(\"No text embeddings source specified\")\n\n    accelerator.print(print_ribbon(\"Loaded Config\", repeat=40))\n    accelerator.print(f\"Running training with {accelerator.num_processes} processes and {accelerator.distributed_type} distributed training\")\n    accelerator.print(f\"Training using {data_source_string}. {'conditioned on text' if conditioning_on_text else 'not conditioned on text'}\")\n    accelerator.print(f\"Number of parameters: {get_num_parameters(decoder)} total; {get_num_parameters(decoder, only_training=True)} training\")\n    for i, unet in enumerate(decoder.unets):\n        accelerator.print(f\"Unet {i} has {get_num_parameters(unet)} total; {get_num_parameters(unet, only_training=True)} training\")\n\n    train(dataloaders, decoder, accelerator,\n        clip=clip,\n        tracker=tracker,\n        inference_device=accelerator.device,\n        evaluate_config=config.evaluate,\n        condition_on_text_encodings=conditioning_on_text,\n        **config.train.model_dump(),\n    )\n    \n# Create a simple click command line interface to load the config and start the training\n@click.command()\n@click.option(\"--config_file\", default=\"./train_decoder_config.json\", help=\"Path to config file\")\ndef main(config_file):\n    config_file_path = Path(config_file)\n    config = TrainDecoderConfig.from_json_path(str(config_file_path))\n    initialize_training(config, config_path=config_file_path)\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "train_diffusion_prior.py",
          "type": "blob",
          "size": 22.5166015625,
          "content": "import click\nimport torch\n\nfrom torch import nn\nfrom typing import List\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom torch.utils.data import DataLoader\nfrom embedding_reader import EmbeddingReader\nfrom accelerate.utils import dataclasses as accelerate_dataclasses\n\nfrom dalle2_pytorch.utils import Timer\nfrom dalle2_pytorch.trackers import Tracker\nfrom dalle2_pytorch import DiffusionPriorTrainer\nfrom dalle2_pytorch.dataloaders import get_reader, make_splits\nfrom dalle2_pytorch.train_configs import (\n    DiffusionPriorConfig,\n    DiffusionPriorTrainConfig,\n    TrainDiffusionPriorConfig,\n)\n\n\n# helpers\n\n\ncos = nn.CosineSimilarity(dim=1, eps=1e-6)\n\n\ndef exists(val):\n    return val is not None\n\n\ndef all_between(values: list, lower_bound, upper_bound):\n    for value in values:\n        if value < lower_bound or value > upper_bound:\n            return False\n\n    return True\n\n\ndef make_model(\n    prior_config: DiffusionPriorConfig,\n    train_config: DiffusionPriorTrainConfig,\n    device: str = None,\n    accelerator: Accelerator = None,\n):\n    # create model from config\n    diffusion_prior = prior_config.create()\n\n    # instantiate the trainer\n    trainer = DiffusionPriorTrainer(\n        diffusion_prior=diffusion_prior,\n        lr=train_config.lr,\n        wd=train_config.wd,\n        max_grad_norm=train_config.max_grad_norm,\n        amp=train_config.amp,\n        use_ema=train_config.use_ema,\n        device=device,\n        accelerator=accelerator,\n        warmup_steps=train_config.warmup_steps,\n    )\n\n    return trainer\n\n\ndef create_tracker(\n    accelerator: Accelerator,\n    config: TrainDiffusionPriorConfig,\n    config_path: str,\n    dummy: bool = False,\n) -> Tracker:\n    tracker_config = config.tracker\n\n    accelerator_config = {\n        \"Distributed\": accelerator.distributed_type\n        != accelerate_dataclasses.DistributedType.NO,\n        \"DistributedType\": accelerator.distributed_type,\n        \"NumProcesses\": accelerator.num_processes,\n        \"MixedPrecision\": accelerator.mixed_precision,\n    }\n\n    tracker: Tracker = tracker_config.create(\n        config, accelerator_config, dummy_mode=dummy\n    )\n\n    tracker.save_config(config_path, config_name=\"prior_config.json\")\n\n    return tracker\n\n\ndef pad_gather_reduce(trainer: DiffusionPriorTrainer, x, method=\"mean\"):\n    \"\"\"\n    pad a value or tensor across all processes and gather\n\n    params:\n        - trainer: a trainer that carries an accelerator object\n        - x: a number or torch tensor to reduce\n        - method: \"mean\", \"sum\", \"max\", \"min\"\n\n    return:\n        - the average tensor after maskin out 0's\n        - None if the gather resulted in an empty tensor\n    \"\"\"\n\n    assert method in [\n        \"mean\",\n        \"sum\",\n        \"max\",\n        \"min\",\n    ], \"This function has limited capabilities [sum, mean, max, min]\"\n    assert type(x) is not None, \"Cannot reduce a None type object\"\n\n    # wait for everyone to arrive here before gathering\n\n    if type(x) is not torch.Tensor:\n        x = torch.tensor([x])\n\n    # verify that the tensor is on the proper device\n    x = x.to(trainer.device)\n\n    # pad across processes\n    padded_x = trainer.accelerator.pad_across_processes(x, dim=0)\n\n    # gather across all procesess\n    gathered_x = trainer.accelerator.gather(padded_x)\n\n    # mask out zeros\n    masked_x = gathered_x[gathered_x != 0]\n\n    # if the tensor is empty, warn and return None\n    if len(masked_x) == 0:\n        click.secho(\n            f\"The call to this method resulted in an empty tensor after masking out zeros. The gathered tensor was this: {gathered_x} and the original value passed was: {x}.\",\n            fg=\"red\",\n        )\n        return None\n\n    if method == \"mean\":\n        return torch.mean(masked_x)\n    elif method == \"sum\":\n        return torch.sum(masked_x)\n    elif method == \"max\":\n        return torch.max(masked_x)\n    elif method == \"min\":\n        return torch.min(masked_x)\n\n\ndef save_trainer(\n    tracker: Tracker,\n    trainer: DiffusionPriorTrainer,\n    is_latest: bool,\n    is_best: bool,\n    epoch: int,\n    samples_seen: int,\n    best_validation_loss: float,\n):\n    \"\"\"\n    Logs the model with an appropriate method depending on the tracker\n    \"\"\"\n    trainer.accelerator.wait_for_everyone()\n\n    if trainer.accelerator.is_main_process:\n        click.secho(\n            f\"RANK:{trainer.accelerator.process_index} | Saving Model | Best={is_best} | Latest={is_latest}\",\n            fg=\"magenta\",\n        )\n\n    tracker.save(\n        trainer=trainer,\n        is_best=is_best,\n        is_latest=is_latest,\n        epoch=int(epoch),\n        samples_seen=int(samples_seen),\n        best_validation_loss=best_validation_loss,\n    )\n\n\ndef recall_trainer(tracker: Tracker, trainer: DiffusionPriorTrainer):\n    \"\"\"\n    Loads the model with an appropriate method depending on the tracker\n    \"\"\"\n\n    if trainer.accelerator.is_main_process:\n        click.secho(f\"Loading model from {type(tracker.loader).__name__}\", fg=\"yellow\")\n\n    state_dict = tracker.recall()\n\n    trainer.load(state_dict, strict=True)\n\n    return (\n        int(state_dict.get(\"epoch\", 0)),\n        state_dict.get(\"best_validation_loss\", 0),\n        int(state_dict.get(\"samples_seen\", 0)),\n    )\n\n\n# eval functions\n\n\ndef report_validation_loss(\n    trainer: DiffusionPriorTrainer,\n    dataloader: DataLoader,\n    text_conditioned: bool,\n    use_ema: bool,\n    tracker: Tracker,\n    split: str,\n    tracker_folder: str,\n    loss_type: str,\n):\n    \"\"\"\n    Compute the validation loss on a given subset of data.\n    \"\"\"\n\n    if trainer.accelerator.is_main_process:\n        click.secho(\n            f\"Measuring performance on {use_ema}-{split} split\",\n            fg=\"green\",\n            blink=True,\n        )\n\n    total_loss = torch.zeros(1, dtype=torch.float, device=trainer.device)\n\n    for image_embeddings, text_data in dataloader:\n        image_embeddings = image_embeddings.to(trainer.device)\n        text_data = text_data.to(trainer.device)\n\n        input_args = dict(image_embed=image_embeddings)\n\n        if text_conditioned:\n            input_args = dict(**input_args, text=text_data)\n        else:\n            input_args = dict(**input_args, text_embed=text_data)\n\n        if use_ema:\n            loss = trainer.ema_diffusion_prior(**input_args)\n        else:\n            loss = trainer(**input_args)\n\n        total_loss += loss\n\n    # compute the average loss across all processes\n\n    avg_loss = pad_gather_reduce(trainer, total_loss, method=\"mean\")\n    stats = {f\"{tracker_folder}/{loss_type}-loss\": avg_loss}\n\n    # print and log results on main process\n    tracker.log(stats, step=trainer.step.item() + 1)\n\n    return avg_loss\n\n\ndef report_cosine_sims(\n    trainer: DiffusionPriorTrainer,\n    dataloader: DataLoader,\n    text_conditioned: bool,\n    tracker: Tracker,\n    split: str,\n    timesteps: int,\n    tracker_folder: str,\n):\n    trainer.eval()\n    if trainer.accelerator.is_main_process:\n        click.secho(\n            f\"Measuring Cosine-Similarity on {split} split with {timesteps} timesteps\",\n            fg=\"green\",\n            blink=True,\n        )\n\n    for test_image_embeddings, text_data in dataloader:\n        test_image_embeddings = test_image_embeddings.to(trainer.device)\n        text_data = text_data.to(trainer.device)\n\n        # we are text conditioned, we produce an embedding from the tokenized text\n        if text_conditioned:\n            text_embedding, text_encodings = trainer.embed_text(text_data)\n            text_cond = dict(text_embed=text_embedding, text_encodings=text_encodings)\n        else:\n            text_embedding = text_data\n            text_cond = dict(text_embed=text_embedding)\n\n        # make a copy of the text embeddings for shuffling\n        text_embed_shuffled = text_embedding.clone()\n\n        # roll the text to simulate \"unrelated\" captions\n        rolled_idx = torch.roll(torch.arange(text_embedding.shape[0]), 1)\n        text_embed_shuffled = text_embed_shuffled[rolled_idx]\n        text_embed_shuffled = text_embed_shuffled / text_embed_shuffled.norm(\n            dim=1, keepdim=True\n        )\n\n        if text_conditioned:\n            text_encodings_shuffled = text_encodings[rolled_idx]\n        else:\n            text_encodings_shuffled = None\n\n        text_cond_shuffled = dict(\n            text_embed=text_embed_shuffled, text_encodings=text_encodings_shuffled\n        )\n\n        # prepare the text embedding\n        text_embed = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n\n        # prepare image embeddings\n        test_image_embeddings = test_image_embeddings / test_image_embeddings.norm(\n            dim=1, keepdim=True\n        )\n\n        # predict on the unshuffled text embeddings\n        predicted_image_embeddings = trainer.p_sample_loop(\n            test_image_embeddings.shape,\n            text_cond,\n            timesteps=timesteps,\n        )\n\n        predicted_image_embeddings = (\n            predicted_image_embeddings\n            / predicted_image_embeddings.norm(dim=1, keepdim=True)\n        )\n\n        # predict on the shuffled embeddings\n        predicted_unrelated_embeddings = trainer.p_sample_loop(\n            test_image_embeddings.shape,\n            text_cond_shuffled,\n            timesteps=timesteps,\n        )\n\n        predicted_unrelated_embeddings = (\n            predicted_unrelated_embeddings\n            / predicted_unrelated_embeddings.norm(dim=1, keepdim=True)\n        )\n\n        # calculate similarities\n        orig_sim = pad_gather_reduce(\n            trainer, cos(text_embed, test_image_embeddings), method=\"mean\"\n        )\n        pred_sim = pad_gather_reduce(\n            trainer, cos(text_embed, predicted_image_embeddings), method=\"mean\"\n        )\n        unrel_sim = pad_gather_reduce(\n            trainer, cos(text_embed, predicted_unrelated_embeddings), method=\"mean\"\n        )\n        pred_img_sim = pad_gather_reduce(\n            trainer,\n            cos(test_image_embeddings, predicted_image_embeddings),\n            method=\"mean\",\n        )\n\n        stats = {\n            f\"{tracker_folder}/baseline similarity [steps={timesteps}]\": orig_sim,\n            f\"{tracker_folder}/similarity with text [steps={timesteps}]\": pred_sim,\n            f\"{tracker_folder}/similarity with original image [steps={timesteps}]\": pred_img_sim,\n            f\"{tracker_folder}/similarity with unrelated caption [steps={timesteps}]\": unrel_sim,\n            f\"{tracker_folder}/difference from baseline similarity [steps={timesteps}]\": pred_sim\n            - orig_sim,\n        }\n\n        tracker.log(stats, step=trainer.step.item() + 1)\n\n\ndef eval_model(\n    trainer: DiffusionPriorTrainer,\n    dataloader: DataLoader,\n    text_conditioned: bool,\n    split: str,\n    tracker: Tracker,\n    use_ema: bool,\n    report_cosine: bool,\n    report_loss: bool,\n    timesteps: List[int],\n    loss_type: str = None,\n):\n    \"\"\"\n    Run evaluation on a model and track metrics\n\n    returns: loss if requested\n    \"\"\"\n    trainer.eval()\n\n    use_ema = \"ema\" if use_ema else \"online\"\n    tracker_folder = f\"metrics/{use_ema}-{split}\"\n\n    # detemine if valid timesteps are passed\n\n    min_timesteps = trainer.accelerator.unwrap_model(\n        trainer.diffusion_prior\n    ).sample_timesteps\n    max_timesteps = trainer.accelerator.unwrap_model(\n        trainer.diffusion_prior\n    ).noise_scheduler.num_timesteps\n\n    assert all_between(\n        timesteps, lower_bound=min_timesteps, upper_bound=max_timesteps\n    ), f\"all timesteps values must be between {min_timesteps} and {max_timesteps}: got {timesteps}\"\n\n    # measure cosine metrics across various eta and timesteps\n\n    if report_cosine:\n        for timestep in timesteps:\n            report_cosine_sims(\n                trainer,\n                dataloader=dataloader,\n                text_conditioned=text_conditioned,\n                tracker=tracker,\n                split=split,\n                timesteps=timestep,\n                tracker_folder=tracker_folder,\n            )\n\n    # measure loss on a seperate split of data\n\n    if report_loss:\n        loss = report_validation_loss(\n            trainer=trainer,\n            dataloader=dataloader,\n            text_conditioned=text_conditioned,\n            use_ema=use_ema,\n            tracker=tracker,\n            split=split,\n            tracker_folder=tracker_folder,\n            loss_type=loss_type,\n        )\n\n        return loss\n\n\n# training script\n\n\ndef train(\n    trainer: DiffusionPriorTrainer,\n    tracker: Tracker,\n    train_loader: DataLoader,\n    eval_loader: DataLoader,\n    test_loader: DataLoader,\n    config: DiffusionPriorTrainConfig,\n):\n    # init timers\n    save_timer = Timer()  # when to save\n    samples_timer = Timer()  # samples/sec\n    validation_profiler = Timer()  # how long is validation taking\n    validation_countdown = Timer()  # when to perform evalutation\n\n    # keep track of best validation loss\n\n    best_validation_loss = config.train.best_validation_loss\n    samples_seen = config.train.num_samples_seen\n\n    # do training\n\n    start_epoch = config.train.current_epoch\n\n    for epoch in range(start_epoch, config.train.epochs):\n        # if we finished out an old epoch, reset the distribution to be a full epoch\n        tracker.log({\"tracking/epoch\": epoch}, step=trainer.step.item())\n\n        if train_loader.dataset.get_start() > 0 and epoch == start_epoch+1:\n            if trainer.accelerator.is_main_process:\n                click.secho(f\"Finished resumed epoch...resetting dataloader.\")\n            train_loader.dataset.set_start(0)\n\n        for img, txt in train_loader:\n            # setup things every step\n\n            trainer.train()\n            current_step = trainer.step.item()\n            samples_timer.reset()\n\n            # place data on device\n\n            img = img.to(trainer.device)\n            txt = txt.to(trainer.device)\n\n            # pass to model\n\n            loss = trainer(text=txt, image_embed=img)\n\n            # perform backprop & apply EMA updates\n\n            trainer.update()\n\n            # gather info about training step\n\n            all_loss = pad_gather_reduce(trainer, loss, method=\"mean\")\n            num_samples = pad_gather_reduce(trainer, len(txt), method=\"sum\")\n            samples_per_sec = num_samples / samples_timer.elapsed()\n            samples_seen += num_samples\n            ema_decay = trainer.ema_diffusion_prior.get_current_decay()\n\n            # log\n\n            tracker.log(\n                {\n                    \"tracking/samples-sec\": samples_per_sec,\n                    \"tracking/samples-seen\": samples_seen,\n                    \"tracking/ema-decay\": ema_decay,\n                    f\"tracking/training-{config.prior.loss_type}\": all_loss,\n                },\n                step=current_step,\n            )\n\n            # Metric Tracking @ Timed Intervals\n\n            eval_delta = pad_gather_reduce(\n                trainer, validation_countdown.elapsed(), method=\"min\"\n            )\n\n            if eval_delta != None and eval_delta > config.data.eval_every_seconds:\n                # begin timing how long this takes\n\n                validation_profiler.reset()\n\n                # package kwargs for evaluation\n\n                eval_kwargs = {\n                    \"trainer\": trainer,\n                    \"tracker\": tracker,\n                    \"text_conditioned\": config.prior.condition_on_text_encodings,\n                    \"timesteps\": config.train.eval_timesteps,\n                }\n\n                # ONLINE MODEL : COSINE : LOSS : VALIDATION SPLIT\n\n                eval_model(\n                    dataloader=eval_loader,\n                    loss_type=config.prior.loss_type,\n                    split=\"validation\",\n                    use_ema=False,\n                    report_cosine=False,\n                    report_loss=True,\n                    **eval_kwargs,\n                )\n\n                # EMA MODEL : COSINE : LOSS : VALIDATION DATA\n\n                ema_val_loss = eval_model(\n                    dataloader=eval_loader,\n                    loss_type=config.prior.loss_type,\n                    split=\"validation\",\n                    use_ema=True,\n                    report_cosine=True,\n                    report_loss=True,\n                    **eval_kwargs,\n                )\n\n                tracker.log(\n                    {\n                        \"tracking/validation length (minutes)\": validation_profiler.elapsed()\n                        / 60\n                    }\n                )\n\n                # check if the ema validation is the lowest seen yet\n\n                if ema_val_loss < best_validation_loss:\n                    best_validation_loss = ema_val_loss\n\n                    #  go save the model as best\n\n                    save_trainer(\n                        trainer=trainer,\n                        tracker=tracker,\n                        is_best=True,\n                        is_latest=False,\n                        samples_seen=samples_seen,\n                        epoch=epoch,\n                        best_validation_loss=best_validation_loss,\n                    )\n\n                # reset timer for validaiton\n\n                validation_countdown.reset()\n\n            elif eval_delta is None:\n                click.secho(\n                    f\"Error occured reading the eval time on rank: {trainer.device}\",\n                    fg=\"yellow\",\n                )\n\n            # save as latest model on schedule\n\n            save_delta = pad_gather_reduce(trainer, save_timer.elapsed(), method=\"min\")\n\n            if save_delta != None and save_delta >= config.train.save_every_seconds:\n                save_trainer(\n                    trainer=trainer,\n                    tracker=tracker,\n                    is_best=False,\n                    is_latest=True,\n                    samples_seen=samples_seen,\n                    epoch=epoch,\n                    best_validation_loss=best_validation_loss,\n                )\n\n                save_timer.reset()\n\n            elif save_delta is None:\n                click.secho(\n                    f\"Error occured reading the save time on rank: {trainer.device}\",\n                    fg=\"yellow\",\n                )\n\n    # evaluate on test data\n\n    if trainer.accelerator.is_main_process:\n        click.secho(f\"Starting Test\", fg=\"red\")\n\n    # save one last time as latest before beginning validation\n\n    save_trainer(\n        tracker=tracker,\n        trainer=trainer,\n        is_best=False,\n        is_latest=True,\n        samples_seen=samples_seen,\n        epoch=epoch,\n        best_validation_loss=best_validation_loss,\n    )\n\n    test_loss = eval_model(\n        trainer=trainer,\n        dataloader=test_loader,\n        text_conditioned=config.prior.condition_on_text_encodings,\n        split=\"test\",\n        tracker=tracker,\n        use_ema=True,\n        report_cosine=False,\n        report_loss=True,\n        timesteps=config.train.eval_timesteps,\n        loss_type=config.prior.loss_type,\n    )\n\n    if test_loss < best_validation_loss:\n        best_validation_loss = test_loss\n\n        #  go save the model as best\n\n        save_trainer(\n            trainer=trainer,\n            tracker=tracker,\n            is_best=True,\n            is_latest=False,\n            samples_seen=samples_seen,\n            epoch=epoch,\n            best_validation_loss=test_loss,\n        )\n\n\ndef initialize_training(config_file, accelerator):\n    \"\"\"\n    Parse the configuration file, and prepare everything necessary for training\n    \"\"\"\n    # load the configuration file\n    if accelerator.is_main_process:\n        click.secho(f\"Loading configuration from {config_file}\", fg=\"green\")\n\n    config = TrainDiffusionPriorConfig.from_json_path(config_file)\n\n    # seed\n\n    set_seed(config.train.random_seed)\n\n    # get a device\n\n    device = accelerator.device\n\n    # make the trainer (will automatically distribute if possible & configured)\n\n    trainer: DiffusionPriorTrainer = make_model(\n        config.prior, config.train, device, accelerator\n    ).to(device)\n\n    # create a tracker\n\n    tracker = create_tracker(\n        accelerator, config, config_file, dummy=accelerator.process_index != 0\n    )\n\n    # reload from chcekpoint\n\n    if tracker.can_recall:\n        current_epoch, best_validation_loss, samples_seen = recall_trainer(\n            tracker=tracker, trainer=trainer\n        )\n\n        # display best values\n        if trainer.accelerator.is_main_process:\n            click.secho(f\"Current Epoch: {current_epoch} | Best Val Loss: {best_validation_loss} | Samples Seen: {samples_seen}\", fg=\"yellow\")\n\n        # update config to reflect recalled values\n        config.train.num_samples_seen = samples_seen\n        config.train.current_epoch = current_epoch\n        config.train.best_validation_loss = best_validation_loss\n\n    # fetch and prepare data\n\n    if trainer.accelerator.is_main_process:\n        click.secho(\"Grabbing data...\", fg=\"blue\", blink=True)\n\n    trainer.accelerator.wait_for_everyone()\n    img_reader = get_reader(\n        text_conditioned=trainer.text_conditioned,\n        img_url=config.data.image_url,\n        meta_url=config.data.meta_url,\n    )\n\n    # calculate start point within epoch\n\n    trainer.accelerator.wait_for_everyone()\n\n    train_loader, eval_loader, test_loader = make_splits(\n        text_conditioned=trainer.text_conditioned,\n        batch_size=config.data.batch_size,\n        num_data_points=config.data.num_data_points,\n        train_split=config.data.splits.train,\n        eval_split=config.data.splits.val,\n        image_reader=img_reader,\n        rank=accelerator.state.process_index,\n        world_size=accelerator.state.num_processes,\n        start=0,\n    )\n\n    # update the start point to finish out the epoch on a resumed run\n\n    if tracker.can_recall:\n        samples_seen = config.train.num_samples_seen\n        length = (\n            config.data.num_data_points\n            if samples_seen <= img_reader.count\n            else img_reader.count\n        )\n        scaled_samples = length * config.train.current_epoch\n        start_point = (\n            scaled_samples - samples_seen if scaled_samples > samples_seen else samples_seen\n        )\n\n        if trainer.accelerator.is_main_process:\n            click.secho(f\"Resuming at sample: {start_point}\", fg=\"yellow\")\n\n        train_loader.dataset.set_start(start_point)\n\n    # start training\n\n    if trainer.accelerator.is_main_process:\n        click.secho(\n            f\"Beginning Prior Training : Distributed={accelerator.state.distributed_type != accelerate_dataclasses.DistributedType.NO}\",\n            fg=\"yellow\",\n        )\n\n    train(\n        trainer=trainer,\n        tracker=tracker,\n        train_loader=train_loader,\n        eval_loader=eval_loader,\n        test_loader=test_loader,\n        config=config,\n    )\n\n\n@click.command()\n@click.option(\"--config_file\", default=\"configs/train_prior_config.example.json\")\ndef main(config_file):\n    # start HFA\n    accelerator = Accelerator()\n\n    # setup training\n    initialize_training(config_file, accelerator)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}