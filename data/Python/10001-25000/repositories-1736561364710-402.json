{
  "metadata": {
    "timestamp": 1736561364710,
    "page": 402,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/CogVideo",
      "stars": 10227,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.255859375,
          "content": "*__pycache__/\nsamples*/\nruns/\ncheckpoints/\nmaster_ip\nlogs/\n*.DS_Store\n.idea\noutput*\ntest*\nvenv\n**/.swp\n**/*.log\n**/*.debug\n**/.vscode\n\n**/*debug*\n**/.gitignore\n**/finetune/*-lora-*\n**/finetune/Disney-*\n**/wandb\n**/results\n**/*.mp4\n**/validation_set\nCogVideo-1.0\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.09375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2024 CogVideo Model Team @ Zhipu AI\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 5.5693359375,
          "content": "The CogVideoX License\n\n1. Definitions\n\n“Licensor” means the CogVideoX Model Team that distributes its Software.\n\n“Software” means the CogVideoX model parameters made available under this license.\n\n2. License Grant\n\nUnder the terms and conditions of this license, the licensor hereby grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license. The intellectual property rights of the generated content belong to the user to the extent permitted by applicable local laws.\nThis license allows you to freely use all open-source models in this repository for academic research. Users who wish to use the models for commercial purposes must register and obtain a basic commercial license in https://open.bigmodel.cn/mla/form .\nUsers who have registered and obtained the basic commercial license can use the models for commercial activities for free, but must comply with all terms and conditions of this license. Additionally, the number of service users (visits) for your commercial activities must not exceed 1 million visits per month.\nIf the number of service users (visits) for your commercial activities exceeds 1 million visits per month, you need to contact our business team to obtain more commercial licenses.\nThe above copyright statement and this license statement should be included in all copies or significant portions of this software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of People’s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us at license@zhipuai.cn.\n\n1. 定义\n\n“许可方”是指分发其软件的 CogVideoX 模型团队。\n\n“软件”是指根据本许可提供的 CogVideoX 模型参数。\n\n2. 许可授予\n\n根据本许可的条款和条件，许可方特此授予您非排他性、全球性、不可转让、不可再许可、可撤销、免版税的版权许可。生成内容的知识产权所属，可根据适用当地法律的规定，在法律允许的范围内由用户享有生成内容的知识产权或其他权利。\n本许可允许您免费使用本仓库中的所有开源模型进行学术研究。对于希望将模型用于商业目的的用户，需在 https://open.bigmodel.cn/mla/form 完成登记并获得基础商用授权。\n\n经过登记并获得基础商用授权的用户可以免费使用本模型进行商业活动，但必须遵守本许可的所有条款和条件。\n在本许可证下，您的商业活动的服务用户数量（访问量）不得超过100万人次访问 / 每月。如果超过，您需要与我们的商业团队联系以获得更多的商业许可。\n上述版权声明和本许可声明应包含在本软件的所有副本或重要部分中。\n\n3.限制\n\n您不得出于任何军事或非法目的使用、复制、修改、合并、发布、分发、复制或创建本软件的全部或部分衍生作品。\n\n您不得利用本软件从事任何危害国家安全和国家统一、危害社会公共利益、侵犯人身权益的行为。\n\n4.免责声明\n\n本软件“按原样”提供，不提供任何明示或暗示的保证，包括但不限于对适销性、特定用途的适用性和非侵权性的保证。\n在任何情况下，作者或版权持有人均不对任何索赔、损害或其他责任负责，无论是在合同诉讼、侵权行为还是其他方面，由软件或软件的使用或其他交易引起、由软件引起或与之相关 软件。\n\n5. 责任限制\n\n除适用法律禁止的范围外，在任何情况下且根据任何法律理论，无论是基于侵权行为、疏忽、合同、责任或其他原因，任何许可方均不对您承担任何直接、间接、特殊、偶然、示范性、 或间接损害，或任何其他商业损失，即使许可人已被告知此类损害的可能性。\n\n6.争议解决\n\n本许可受中华人民共和国法律管辖并按其解释。 因本许可引起的或与本许可有关的任何争议应提交北京市海淀区人民法院。\n\n请注意，许可证可能会更新到更全面的版本。 有关许可和版权的任何问题，请通过 license@zhipuai.cn 与我们联系。"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 26.8330078125,
          "content": "# CogVideo & CogVideoX\n\n[中文阅读](./README_zh.md)\n\n[日本語で読む](./README_ja.md)\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"50%\"/>\n</div>\n<p align=\"center\">\nExperience the CogVideoX-5B model online at <a href=\"https://huggingface.co/spaces/THUDM/CogVideoX-5B\" target=\"_blank\"> 🤗 Huggingface Space</a> or <a href=\"https://modelscope.cn/studios/ZhipuAI/CogVideoX-5b-demo\" target=\"_blank\"> 🤖 ModelScope Space</a>\n</p>\n<p align=\"center\">\n📚 View the <a href=\"https://arxiv.org/abs/2408.06072\" target=\"_blank\">paper</a> and <a href=\"https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh\" target=\"_blank\">user guide</a>\n</p>\n<p align=\"center\">\n    👋 Join our <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/dCGfUsagrD\" target=\"_blank\">Discord</a> \n</p>\n<p align=\"center\">\n📍 Visit <a href=\"https://chatglm.cn/video?lang=en?fr=osm_cogvideo\">QingYing</a> and <a href=\"https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9\">API Platform</a> to experience larger-scale commercial video generation models.\n</p>\n\n## Project Updates\n\n- 🔥🔥 **News**: ```2025/01/08```: We have updated the code for `Lora` fine-tuning based on the `diffusers` version model, which uses less GPU memory. For more details, please see [here](finetune/README.md).\n- 🔥 **News**: ```2024/11/15```: We released the `CogVideoX1.5` model in the diffusers version. Only minor parameter adjustments are needed to continue using previous code.\n- 🔥 **News**: ```2024/11/08```: We have released the CogVideoX1.5 model. CogVideoX1.5 is an upgraded version of the open-source model CogVideoX.\nThe CogVideoX1.5-5B series supports 10-second videos with higher resolution, and CogVideoX1.5-5B-I2V supports video generation at any resolution. \nThe SAT code has already been updated, while the diffusers version is still under adaptation. Download the SAT version code [here](https://huggingface.co/THUDM/CogVideoX1.5-5B-SAT).\n- 🔥 **News**: ```2024/10/13```: A more cost-effective fine-tuning framework for `CogVideoX-5B` that works with a single\n  4090 GPU, [cogvideox-factory](https://github.com/a-r-r-o-w/cogvideox-factory), has been released. It supports\n  fine-tuning with multiple resolutions. Feel free to use it!\n- 🔥 **News**: ```2024/10/10```: We have updated our technical report. Please\n  click [here](https://arxiv.org/pdf/2408.06072) to view it. More training details and a demo have been added. To see\n  the demo, click [here](https://yzy-thu.github.io/CogVideoX-demo/).- 🔥 **News**: ```2024/10/09```: We have publicly\n  released the [technical documentation](https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh) for CogVideoX\n  fine-tuning on Feishu, further increasing distribution flexibility. All examples in the public documentation can be\n  fully reproduced.\n- 🔥 **News**: ```2024/9/19```: We have open-sourced the CogVideoX series image-to-video model **CogVideoX-5B-I2V**.\n  This model can take an image as a background input and generate a video combined with prompt words, offering greater\n  controllability. With this, the CogVideoX series models now support three tasks: text-to-video generation, video\n  continuation, and image-to-video generation. Welcome to try it online\n  at [Experience](https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space).\n- 🔥 ```2024/9/19```: The Caption\n  model [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption), used in the training process of\n  CogVideoX to convert video data into text descriptions, has been open-sourced. Welcome to download and use it.\n- 🔥 ```2024/8/27```: We have open-sourced a larger model in the CogVideoX series, **CogVideoX-5B**. We have\n  significantly optimized the model's inference performance, greatly lowering the inference threshold. \n  You can run **CogVideoX-2B** on older GPUs like `GTX 1080TI`, and **CogVideoX-5B** on desktop GPUs like `RTX 3060`. Please strictly\n  follow the [requirements](requirements.txt) to update and install dependencies, and refer\n  to [cli_demo](inference/cli_demo.py) for inference code. Additionally, the open-source license for \n  the **CogVideoX-2B** model has been changed to the **Apache 2.0 License**.\n- 🔥 ```2024/8/6```: We have open-sourced **3D Causal VAE**, used for **CogVideoX-2B**, which can reconstruct videos with\n  almost no loss.\n- 🔥 ```2024/8/6```: We have open-sourced the first model of the CogVideoX series video generation models, **CogVideoX-2B\n  **.\n- 🌱 **Source**: ```2022/5/19```: We have open-sourced the CogVideo video generation model (now you can see it in\n  the `CogVideo` branch). This is the first open-source large Transformer-based text-to-video generation model. You can\n  access the [ICLR'23 paper](https://arxiv.org/abs/2205.15868) for technical details.\n\n## Table of Contents\n\nJump to a specific section:\n\n- [Quick Start](#quick-start)\n  - [Prompt Optimization](#prompt-optimization)\n  - [SAT](#sat)\n  - [Diffusers](#diffusers)\n- [Gallery](#gallery)\n  - [CogVideoX-5B](#cogvideox-5b)\n  - [CogVideoX-2B](#cogvideox-2b)\n- [Model Introduction](#model-introduction)\n- [Friendly Links](#friendly-links)\n- [Project Structure](#project-structure)\n  - [Quick Start with Colab](#quick-start-with-colab)\n  - [Inference](#inference)\n  - [finetune](#finetune)\n  - [sat](#sat-1)\n  - [Tools](#tools)\n- [CogVideo(ICLR'23)](#cogvideoiclr23)\n- [Citation](#citation)\n- [Model-License](#model-license)\n\n## Quick Start\n\n### Prompt Optimization\n\nBefore running the model, please refer to [this guide](inference/convert_demo.py) to see how we use large models like\nGLM-4 (or other comparable products, such as GPT-4) to optimize the model. This is crucial because the model is trained\nwith long prompts, and a good prompt directly impacts the quality of the video generation.\n\n### SAT\n\n**Please make sure your Python version is between 3.10 and 3.12, inclusive of both 3.10 and 3.12.**\n\nFollow instructions in [sat_demo](sat/README.md): Contains the inference code and fine-tuning code of SAT weights. It is\nrecommended to improve based on the CogVideoX model structure. Innovative researchers use this code to better perform\nrapid stacking and development.\n\n### Diffusers\n\n**Please make sure your Python version is between 3.10 and 3.12, inclusive of both 3.10 and 3.12.**\n\n```\npip install -r requirements.txt\n```\n\nThen follow [diffusers_demo](inference/cli_demo.py): A more detailed explanation of the inference code, mentioning the\nsignificance of common parameters.\n\nFor more details on quantized inference, please refer\nto [diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao/). With Diffusers and TorchAO, quantized inference\nis also possible leading to memory-efficient inference as well as speedup in some cases when compiled. A full list of\nmemory and time benchmarks with various settings on A100 and H100 has been published\nat [diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao).\n\n## Gallery\n\n### CogVideoX-5B\n\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/cf5953ea-96d3-48fd-9907-c4708752c714\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/fe0a78e6-b669-4800-8cf0-b5f9b5145b52\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/c182f606-8f8c-421d-b414-8487070fcfcb\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/7db2bbce-194d-434d-a605-350254b6c298\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/62b01046-8cab-44cc-bd45-4d965bb615ec\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/d78e552a-4b3f-4b81-ac3f-3898079554f6\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/30894f12-c741-44a2-9e6e-ddcacc231e5b\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/926575ca-7150-435b-a0ff-4900a963297b\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n</table>\n\n### CogVideoX-2B\n\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/ea3af39a-3160-4999-90ec-2f7863c5b0e9\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/9de41efd-d4d1-4095-aeda-246dd834e91d\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/941d6661-6a8d-4a1b-b912-59606f0b2841\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/938529c4-91ae-4f60-b96b-3c3947fa63cb\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n</table>\n\nTo view the corresponding prompt words for the gallery, please click [here](resources/galary_prompt.md)\n\n## Model Introduction\n\nCogVideoX is an open-source version of the video generation model originating\nfrom [QingYing](https://chatglm.cn/video?lang=en?fr=osm_cogvideo). The table below displays the list of video generation\nmodels we currently offer, along with their foundational information.\n\n<table style=\"border-collapse: collapse; width: 100%;\">\n  <tr>\n    <th style=\"text-align: center;\">Model Name</th>\n    <th style=\"text-align: center;\">CogVideoX1.5-5B (Latest)</th>\n    <th style=\"text-align: center;\">CogVideoX1.5-5B-I2V (Latest)</th>\n    <th style=\"text-align: center;\">CogVideoX-2B</th>\n    <th style=\"text-align: center;\">CogVideoX-5B</th>\n    <th style=\"text-align: center;\">CogVideoX-5B-I2V</th>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Release Date</td>\n    <th style=\"text-align: center;\">November 8, 2024</th>\n    <th style=\"text-align: center;\">November 8, 2024</th>\n    <th style=\"text-align: center;\">August 6, 2024</th>\n    <th style=\"text-align: center;\">August 27, 2024</th>\n    <th style=\"text-align: center;\">September 19, 2024</th>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Video Resolution</td>\n    <td colspan=\"1\" style=\"text-align: center;\">1360 * 768</td>\n    <td colspan=\"1\" style=\"text-align: center;\"> Min(W, H) = 768 <br> 768 ≤ Max(W, H) ≤ 1360 <br> Max(W, H) % 16 = 0 </td>\n    <td colspan=\"3\" style=\"text-align: center;\">720 * 480</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Inference Precision</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16 (Recommended)</b>, FP16, FP32, FP8*, INT8, Not supported: INT4</td>\n    <td style=\"text-align: center;\"><b>FP16*(Recommended)</b>, BF16, FP32, FP8*, INT8, Not supported: INT4</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16 (Recommended)</b>, FP16, FP32, FP8*, INT8, Not supported: INT4</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Single GPU Memory Usage<br></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> BF16: 76GB <br><b>diffusers BF16: from 10GB*</b><br><b>diffusers INT8(torchao): from 7GB*</b></td>\n    <td style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> FP16: 18GB <br><b>diffusers FP16: 4GB minimum* </b><br><b>diffusers INT8 (torchao): 3.6GB minimum*</b></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> BF16: 26GB <br><b>diffusers BF16 : 5GB minimum* </b><br><b>diffusers INT8 (torchao): 4.4GB minimum* </b></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Multi-GPU Memory Usage</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16: 24GB* using diffusers</b><br></td>\n    <td style=\"text-align: center;\"><b>FP16: 10GB* using diffusers</b><br></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16: 15GB* using diffusers</b><br></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Inference Speed<br>(Step = 50, FP/BF16)</td>\n    <td colspan=\"2\" style=\"text-align: center;\">Single A100: ~1000 seconds (5-second video)<br>Single H100: ~550 seconds (5-second video)</td>\n    <td style=\"text-align: center;\">Single A100: ~90 seconds<br>Single H100: ~45 seconds</td>\n    <td colspan=\"2\" style=\"text-align: center;\">Single A100: ~180 seconds<br>Single H100: ~90 seconds</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Prompt Language</td>\n    <td colspan=\"5\" style=\"text-align: center;\">English*</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Prompt Token Limit</td>\n    <td colspan=\"2\" style=\"text-align: center;\">224 Tokens</td>\n    <td colspan=\"3\" style=\"text-align: center;\">226 Tokens</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Video Length</td>\n    <td colspan=\"2\" style=\"text-align: center;\">5 seconds or 10 seconds</td>\n    <td colspan=\"3\" style=\"text-align: center;\">6 seconds</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Frame Rate</td>\n    <td colspan=\"2\" style=\"text-align: center;\">16 frames / second </td>\n    <td colspan=\"3\" style=\"text-align: center;\">8 frames / second </td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Position Encoding</td>\n    <td colspan=\"2\" style=\"text-align: center;\">3d_rope_pos_embed</td>\n    <td style=\"text-align: center;\">3d_sincos_pos_embed</td> \n    <td style=\"text-align: center;\">3d_rope_pos_embed</td>\n    <td style=\"text-align: center;\">3d_rope_pos_embed + learnable_pos_embed</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Download Link (Diffusers)</td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5B\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5B\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5B\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5B-I2V\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5B-I2V\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5B-I2V\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-2b\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-2b\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-2b\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-5b\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-5b\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-5b\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-5b-I2V\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-5b-I2V\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-5b-I2V\">🟣 WiseModel</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">Download Link (SAT)</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5b-SAT\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5b-SAT\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5b-SAT\">🟣 WiseModel</a></td>\n    <td colspan=\"3\" style=\"text-align: center;\"><a href=\"./sat/README_zh.md\">SAT</a></td>\n  </tr>\n</table>\n\n**Data Explanation**\n\n+ While testing using the diffusers library, all optimizations included in the diffusers library were enabled. This\n  scheme has not been tested for actual memory usage on devices outside of **NVIDIA A100 / H100** architectures.\n  Generally, this scheme can be adapted to all **NVIDIA Ampere architecture** and above devices. If optimizations are\n  disabled, memory consumption will multiply, with peak memory usage being about 3 times the value in the table.\n  However, speed will increase by about 3-4 times. You can selectively disable some optimizations, including:\n\n```\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\n```\n\n+ For multi-GPU inference, the `enable_sequential_cpu_offload()` optimization needs to be disabled.\n+ Using INT8 models will slow down inference, which is done to accommodate lower-memory GPUs while maintaining minimal\n  video quality loss, though inference speed will significantly decrease.\n+ The CogVideoX-2B model was trained in `FP16` precision, and all CogVideoX-5B models were trained in `BF16` precision.\n  We recommend using the precision in which the model was trained for inference.\n+ [PytorchAO](https://github.com/pytorch/ao) and [Optimum-quanto](https://github.com/huggingface/optimum-quanto/) can be\n  used to quantize the text encoder, transformer, and VAE modules to reduce the memory requirements of CogVideoX. This\n  allows the model to run on free T4 Colabs or GPUs with smaller memory! Also, note that TorchAO quantization is fully\n  compatible with `torch.compile`, which can significantly improve inference speed. FP8 precision must be used on\n  devices with NVIDIA H100 and above, requiring source installation of `torch`, `torchao` Python packages. CUDA 12.4 is recommended.\n+ The inference speed tests also used the above memory optimization scheme. Without memory optimization, inference speed\n  increases by about 10%. Only the `diffusers` version of the model supports quantization.\n+ The model only supports English input; other languages can be translated into English for use via large model\n  refinement.\n\n\n## Friendly Links\n\nWe highly welcome contributions from the community and actively contribute to the open-source community. The following\nworks have already been adapted for CogVideoX, and we invite everyone to use them:\n\n+ [CogVideoX-Fun](https://github.com/aigc-apps/CogVideoX-Fun): CogVideoX-Fun is a modified pipeline based on the\n  CogVideoX architecture, supporting flexible resolutions and multiple launch methods.\n+ [CogStudio](https://github.com/pinokiofactory/cogstudio): A separate repository for CogVideo's Gradio Web UI, which\n  supports more functional Web UIs.\n+ [Xorbits Inference](https://github.com/xorbitsai/inference): A powerful and comprehensive distributed inference\n  framework, allowing you to easily deploy your own models or the latest cutting-edge open-source models with just one\n  click.\n+ [ComfyUI-CogVideoXWrapper](https://github.com/kijai/ComfyUI-CogVideoXWrapper) Use the ComfyUI framework to integrate\n  CogVideoX into your workflow.\n+ [VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys): VideoSys provides a user-friendly, high-performance\n  infrastructure for video generation, with full pipeline support and continuous integration of the latest models and\n  techniques.\n+ [AutoDL Space](https://www.codewithgpu.com/i/THUDM/CogVideo/CogVideoX-5b-demo): A one-click deployment Huggingface\n  Space image provided by community members.\n+ [Interior Design Fine-Tuning Model](https://huggingface.co/collections/bertjiazheng/koolcogvideox-66e4762f53287b7f39f8f3ba):\n  is a fine-tuned model based on CogVideoX, specifically designed for interior design.\n+ [xDiT](https://github.com/xdit-project/xDiT): xDiT is a scalable inference engine for Diffusion Transformers (DiTs)\n  on multiple GPU Clusters. xDiT supports real-time image and video generations services.\n  [cogvideox-factory](https://github.com/a-r-r-o-w/cogvideox-factory): A cost-effective\n  fine-tuning framework for CogVideoX, compatible with the `diffusers` version model. Supports more resolutions, and\n  fine-tuning CogVideoX-5B can be done with a single 4090 GPU.\n+ [CogVideoX-Interpolation](https://github.com/feizc/CogvideX-Interpolation): A pipeline based on the modified CogVideoX\n  structure, aimed at providing greater flexibility for keyframe interpolation generation.\n+ [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): DiffSynth Studio is a diffusion engine. It has\n  restructured the architecture, including text encoders, UNet, VAE, etc., enhancing computational performance while\n  maintaining compatibility with open-source community models. The framework has been adapted for CogVideoX.\n+ [CogVideoX-Controlnet](https://github.com/TheDenk/cogvideox-controlnet): A simple ControlNet module code that includes the CogVideoX model.\n+ [VideoTuna](https://github.com/VideoVerses/VideoTuna): VideoTuna is the first repo that integrates multiple AI video generation models for text-to-video, image-to-video, text-to-image generation.\n+ [ConsisID](https://github.com/PKU-YuanGroup/ConsisID): An identity-preserving text-to-video generation model, bases on CogVideoX-5B, which keep the face consistent in the generated video by frequency decomposition.\n+ [A Step by Step Tutorial](https://www.youtube.com/watch?v=5UCkMzP2VLE&ab_channel=SECourses): A step-by-step guide on installing and optimizing the CogVideoX1.5-5B-I2V model in Windows and cloud environments. Special thanks to the [FurkanGozukara](https://github.com/FurkanGozukara) for his effort and support!\n\n## Project Structure\n\nThis open-source repository will guide developers to quickly get started with the basic usage and fine-tuning examples\nof the **CogVideoX** open-source model.\n\n### Quick Start with Colab\n\nHere provide three projects that can be run directly on free Colab T4 instances:\n\n+ [CogVideoX-5B-T2V-Colab.ipynb](https://colab.research.google.com/drive/1pCe5s0bC_xuXbBlpvIH1z0kfdTLQPzCS?usp=sharing):\n  CogVideoX-5B Text-to-Video Colab code.\n+ [CogVideoX-5B-T2V-Int8-Colab.ipynb](https://colab.research.google.com/drive/1DUffhcjrU-uz7_cpuJO3E_D4BaJT7OPa?usp=sharing):\n  CogVideoX-5B Quantized Text-to-Video Inference Colab code, which takes about 30 minutes per run.\n+ [CogVideoX-5B-I2V-Colab.ipynb](https://colab.research.google.com/drive/17CqYCqSwz39nZAX2YyonDxosVKUZGzcX?usp=sharing):\n  CogVideoX-5B Image-to-Video Colab code.\n+ [CogVideoX-5B-V2V-Colab.ipynb](https://colab.research.google.com/drive/1comfGAUJnChl5NwPuO8Ox5_6WCy4kbNN?usp=sharing):\n  CogVideoX-5B Video-to-Video Colab code.\n\n### Inference\n\n+ [dcli_demo](inference/cli_demo.py): A more detailed inference code explanation, including the significance of\n  common parameters. All of this is covered here.\n+ [cli_demo_quantization](inference/cli_demo_quantization.py):\n  Quantized model inference code that can run on devices with lower memory. You can also modify this code to support\n  running CogVideoX models in FP8 precision.\n+ [diffusers_vae_demo](inference/cli_vae_demo.py): Code for running VAE inference separately.\n+ [space demo](inference/gradio_composite_demo): The same GUI code as used in the Huggingface Space, with frame\n  interpolation and super-resolution tools integrated.\n\n<div style=\"text-align: center;\">\n    <img src=\"resources/web_demo.png\" style=\"width: 100%; height: auto;\" />\n</div>\n\n+ [convert_demo](inference/convert_demo.py): How to convert user input into long-form input suitable for CogVideoX.\n  Since CogVideoX is trained on long texts, we need to transform the input text distribution to match the training data\n  using an LLM. The script defaults to using GLM-4, but it can be replaced with GPT, Gemini, or any other large language\n  model.\n+ [gradio_web_demo](inference/gradio_composite_demo): A simple Gradio web application demonstrating how to use the\n  CogVideoX-2B / 5B model to generate videos. Similar to our Huggingface Space, you can use this script to run a simple\n  web application for video generation.\n\n### finetune\n\n+ [finetune_demo](finetune/README.md): Fine-tuning scheme and details of the diffusers version of the CogVideoX model.\n\n### sat\n\n+ [sat_demo](sat/README.md): Contains the inference code and fine-tuning code of SAT weights. It is recommended to\n  improve based on the CogVideoX model structure. Innovative researchers use this code to better perform rapid stacking\n  and development.\n\n### Tools\n\nThis folder contains some tools for model conversion / caption generation, etc.\n\n+ [convert_weight_sat2hf](tools/convert_weight_sat2hf.py): Converts SAT model weights to Huggingface model weights.\n+ [caption_demo](tools/caption/README.md): Caption tool, a model that understands videos and outputs descriptions in\n  text.\n+ [export_sat_lora_weight](tools/export_sat_lora_weight.py): SAT fine-tuning model export tool, exports the SAT Lora\n  Adapter in diffusers format.\n+ [load_cogvideox_lora](tools/load_cogvideox_lora.py): Tool code for loading the diffusers version of fine-tuned Lora\n  Adapter.\n+ [llm_flux_cogvideox](tools/llm_flux_cogvideox/llm_flux_cogvideox.py): Automatically generate videos using an\n  open-source local large language model + Flux + CogVideoX.\n+ [parallel_inference_xdit](tools/parallel_inference/parallel_inference_xdit.py):\n  Supported by [xDiT](https://github.com/xdit-project/xDiT), parallelize the\n  video generation process on multiple GPUs.\n\n## CogVideo(ICLR'23)\n\nThe official repo for the\npaper: [CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers](https://arxiv.org/abs/2205.15868)\nis on the [CogVideo branch](https://github.com/THUDM/CogVideo/tree/CogVideo)\n\n**CogVideo is able to generate relatively high-frame-rate videos.**\nA 4-second clip of 32 frames is shown below.\n\n![High-frame-rate sample](https://raw.githubusercontent.com/THUDM/CogVideo/CogVideo/assets/appendix-sample-highframerate.png)\n\n![Intro images](https://raw.githubusercontent.com/THUDM/CogVideo/CogVideo/assets/intro-image.png)\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/2fa19651-e925-4a2a-b8d6-b3f216d490ba\" width=\"80%\" controls autoplay></video>\n</div>\n\n\nThe demo for CogVideo is at [https://models.aminer.cn/cogvideo](https://models.aminer.cn/cogvideo/), where you can get\nhands-on practice on text-to-video generation. *The original input is in Chinese.*\n\n## Citation\n\n🌟 If you find our work helpful, please leave us a star and cite our paper.\n\n```\n@article{yang2024cogvideox,\n  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},\n  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},\n  journal={arXiv preprint arXiv:2408.06072},\n  year={2024}\n}\n@article{hong2022cogvideo,\n  title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},\n  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},\n  journal={arXiv preprint arXiv:2205.15868},\n  year={2022}\n}\n```\n\nWe welcome your contributions! You can click [here](resources/contribute.md) for more information.\n\n## Model-License\n\nThe code in this repository is released under the [Apache 2.0 License](LICENSE).\n\nThe CogVideoX-2B model (including its corresponding Transformers module and VAE module) is released under\nthe [Apache 2.0 License](LICENSE).\n\nThe CogVideoX-5B model (Transformers module, include I2V and T2V) is released under\nthe [CogVideoX LICENSE](https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE).\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 30.72265625,
          "content": "# CogVideo & CogVideoX\n\n[Read this in English](./README.md)\n\n[中文阅读](./README_zh.md)\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"50%\"/>\n</div>\n<p align=\"center\">\n<a href=\"https://huggingface.co/spaces/THUDM/CogVideoX-5B\" target=\"_blank\"> 🤗 Huggingface Space</a> または <a href=\"https://modelscope.cn/studios/ZhipuAI/CogVideoX-5b-demo\" target=\"_blank\"> 🤖 ModelScope Space</a> で CogVideoX-5B モデルをオンラインで体験してください\n</p>\n<p align=\"center\">\n📚 <a href=\"https://arxiv.org/abs/2408.06072\" target=\"_blank\">論文</a>と<a href=\"https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh\" target=\"_blank\">使用ドキュメント</a>を表示します。\n</p>\n<p align=\"center\">\n    👋 <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a> と <a href=\"https://discord.gg/dCGfUsagrD\" target=\"_blank\">Discord</a> に参加\n</p>\n<p align=\"center\">\n📍 <a href=\"https://chatglm.cn/video?lang=en?fr=osm_cogvideo\">清影</a> と <a href=\"https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9\">APIプラットフォーム</a> を訪問して、より大規模な商用ビデオ生成モデルを体験.\n</p>\n\n## 更新とニュース\n\n- 🔥🔥 **ニュース**: ```2025/01/08```: 私たちは`diffusers`バージョンのモデルをベースにした`Lora`微調整用のコードを更新しました。より少ないVRAM（ビデオメモリ）で動作します。詳細については[こちら](finetune/README_ja.md)をご覧ください。\n- 🔥 **ニュース**: ```2024/11/15```: `CogVideoX1.5`モデルのdiffusersバージョンをリリースしました。わずかなパラメータ調整で以前のコードをそのまま利用可能です。\n- 🔥 **ニュース**: ```2024/11/08```: `CogVideoX1.5` モデルをリリースしました。CogVideoX1.5 は CogVideoX オープンソースモデルのアップグレードバージョンです。\nCogVideoX1.5-5B シリーズモデルは、10秒 長の動画とより高い解像度をサポートしており、`CogVideoX1.5-5B-I2V` は任意の解像度での動画生成に対応しています。\nSAT コードはすでに更新されており、`diffusers` バージョンは現在適応中です。\nSAT バージョンのコードは [こちら](https://huggingface.co/THUDM/CogVideoX1.5-5B-SAT) からダウンロードできます。\n- 🔥 **ニュース**: ```2024/10/13```: コスト削減のため、単一の4090 GPUで`CogVideoX-5B`\n  を微調整できるフレームワーク [cogvideox-factory](https://github.com/a-r-r-o-w/cogvideox-factory)\n  がリリースされました。複数の解像度での微調整に対応しています。ぜひご利用ください！\n- 🔥**ニュース**: ```2024/10/10```:\n  技術報告書を更新し、より詳細なトレーニング情報とデモを追加しました。\n- 🔥 **ニュース**: ```2024/10/10```: 技術報告書を更新しました。[こちら](https://arxiv.org/pdf/2408.06072)\n  をクリックしてご覧ください。さらにトレーニングの詳細とデモを追加しました。デモを見るには[こちら](https://yzy-thu.github.io/CogVideoX-demo/)\n  をクリックしてください。\n- 🔥**ニュース**: ```2024/10/09```: 飛書の[技術ドキュメント](https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh)\n  でCogVideoXの微調整ガイドを公開しています。分配の自由度をさらに高めるため、公開されているドキュメント内のすべての例が完全に再現可能です。\n- 🔥**ニュース**: ```2024/9/19```: CogVideoXシリーズの画像生成ビデオモデル **CogVideoX-5B-I2V**\n  をオープンソース化しました。このモデルは、画像を背景入力として使用し、プロンプトワードと組み合わせてビデオを生成することができ、より高い制御性を提供します。これにより、CogVideoXシリーズのモデルは、テキストからビデオ生成、ビデオの継続、画像からビデオ生成の3つのタスクをサポートするようになりました。オンラインでの[体験](https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space)\n  をお楽しみください。\n- 🔥 **ニュース**: ```2024/9/19```:\n  CogVideoXのトレーニングプロセスでビデオデータをテキスト記述に変換するために使用されるキャプションモデル [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption)\n  をオープンソース化しました。ダウンロードしてご利用ください。\n- 🔥 ```2024/8/27```: CogVideoXシリーズのより大きなモデル **CogVideoX-5B**\n  をオープンソース化しました。モデルの推論性能を大幅に最適化し、推論のハードルを大幅に下げました。`GTX 1080TI` などの旧型GPUで\n  **CogVideoX-2B** を、`RTX 3060` などのデスクトップGPUで **CogVideoX-5B**\n  モデルを実行できます。依存関係を更新・インストールするために、[要件](requirements.txt)\n  を厳守し、推論コードは [cli_demo](inference/cli_demo.py) を参照してください。さらに、**CogVideoX-2B** モデルのオープンソースライセンスが\n  **Apache 2.0 ライセンス** に変更されました。\n- 🔥 ```2024/8/6```: **CogVideoX-2B** 用の **3D Causal VAE** をオープンソース化しました。これにより、ビデオをほぼ無損失で再構築することができます。\n- 🔥 ```2024/8/6```: CogVideoXシリーズのビデオ生成モデルの最初のモデル、**CogVideoX-2B** をオープンソース化しました。\n- 🌱 **ソース**: ```2022/5/19```: CogVideoビデオ生成モデルをオープンソース化しました（現在、`CogVideo`\n  ブランチで確認できます）。これは、トランスフォーマーに基づく初のオープンソース大規模テキスト生成ビデオモデルです。技術的な詳細については、[ICLR'23論文](https://arxiv.org/abs/2205.15868)\n  をご覧ください。\n\n**より強力なモデルが、より大きなパラメータサイズで登場予定です。お楽しみに！**\n\n## 目次\n\n特定のセクションにジャンプ：\n\n- [クイックスタート](#クイックスタート)\n  - [プロンプトの最適化](#プロンプトの最適化)\n  - [SAT](#sat)\n  - [Diffusers](#diffusers)\n- [Gallery](#gallery)\n  - [CogVideoX-5B](#cogvideox-5b)\n  - [CogVideoX-2B](#cogvideox-2b)\n- [モデル紹介](#モデル紹介)\n- [友好的リンク](#友好的リンク)\n- [プロジェクト構造](#プロジェクト構造)\n  - [Colabでのクイックスタート](#colabでのクイックスタート)\n  - [Inference](#inference)\n  - [finetune](#finetune)\n  - [sat](#sat-1)\n  - [ツール](#ツール)\n- [CogVideo(ICLR'23)](#cogvideoiclr23)\n- [引用](#引用)\n- [ライセンス契約](#ライセンス契約)\n\n## クイックスタート\n\n### プロンプトの最適化\n\nモデルを実行する前に、[こちら](inference/convert_demo.py)\nを参考にして、GLM-4（または同等の製品、例えばGPT-4）の大規模モデルを使用してどのようにモデルを最適化するかをご確認ください。これは非常に重要です。モデルは長いプロンプトでトレーニングされているため、良いプロンプトがビデオ生成の品質に直接影響を与えます。\n\n### SAT\n\n[sat_demo](sat/README.md) の指示に従ってください:\nSATウェイトの推論コードと微調整コードが含まれています。CogVideoXモデル構造に基づいて改善することをお勧めします。革新的な研究者は、このコードを使用して迅速なスタッキングと開発を行うことができます。\n\n### Diffusers\n\n```\npip install -r requirements.txt\n```\n\n次に [diffusers_demo](inference/cli_demo.py) を参照してください: 推論コードの詳細な説明が含まれており、一般的なパラメータの意味についても言及しています。\n\n量子化推論の詳細については、[diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao/) を参照してください。Diffusers\nと TorchAO を使用することで、量子化推論も可能となり、メモリ効率の良い推論や、コンパイル時に場合によっては速度の向上が期待できます。A100\nおよび H100\n上でのさまざまな設定におけるメモリおよび時間のベンチマークの完全なリストは、[diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao)\nに公開されています。\n\n## Gallery\n\n### CogVideoX-5B\n\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/cf5953ea-96d3-48fd-9907-c4708752c714\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/fe0a78e6-b669-4800-8cf0-b5f9b5145b52\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/c182f606-8f8c-421d-b414-8487070fcfcb\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/7db2bbce-194d-434d-a605-350254b6c298\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/62b01046-8cab-44cc-bd45-4d965bb615ec\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/d78e552a-4b3f-4b81-ac3f-3898079554f6\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/30894f12-c741-44a2-9e6e-ddcacc231e5b\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/926575ca-7150-435b-a0ff-4900a963297b\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n</table>\n\n### CogVideoX-2B\n\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/ea3af39a-3160-4999-90ec-2f7863c5b0e9\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/9de41efd-d4d1-4095-aeda-246dd834e91d\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/941d6661-6a8d-4a1b-b912-59606f0b2841\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/938529c4-91ae-4f60-b96b-3c3947fa63cb\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n</table>\n\nギャラリーの対応するプロンプトワードを表示するには、[こちら](resources/galary_prompt.md)をクリックしてください\n\n## モデル紹介\n\nCogVideoXは、[清影](https://chatglm.cn/video?fr=osm_cogvideox) と同源のオープンソース版ビデオ生成モデルです。\n以下の表に、提供しているビデオ生成モデルの基本情報を示します:\n\n<table style=\"border-collapse: collapse; width: 100%;\">\n  <tr>\n    <th style=\"text-align: center;\">モデル名</th>\n    <th style=\"text-align: center;\">CogVideoX1.5-5B (最新)</th>\n    <th style=\"text-align: center;\">CogVideoX1.5-5B-I2V (最新)</th>\n    <th style=\"text-align: center;\">CogVideoX-2B</th>\n    <th style=\"text-align: center;\">CogVideoX-5B</th>\n    <th style=\"text-align: center;\">CogVideoX-5B-I2V</th>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">公開日</td>\n    <th style=\"text-align: center;\">2024年11月8日</th>\n    <th style=\"text-align: center;\">2024年11月8日</th>\n    <th style=\"text-align: center;\">2024年8月6日</th>\n    <th style=\"text-align: center;\">2024年8月27日</th>\n    <th style=\"text-align: center;\">2024年9月19日</th>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">ビデオ解像度</td>\n    <td colspan=\"1\" style=\"text-align: center;\">1360 * 768</td>\n    <td colspan=\"1\" style=\"text-align: center;\"> Min(W, H) = 768 <br> 768 ≤ Max(W, H) ≤ 1360 <br> Max(W, H) % 16 = 0 </td>\n    <td colspan=\"3\" style=\"text-align: center;\">720 * 480</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">推論精度</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16(推奨)</b>, FP16, FP32，FP8*，INT8，INT4非対応</td>\n    <td style=\"text-align: center;\"><b>FP16*(推奨)</b>, BF16, FP32，FP8*，INT8，INT4非対応</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16(推奨)</b>, FP16, FP32，FP8*，INT8，INT4非対応</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">単一GPUメモリ消費量<br></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> BF16: 76GB <br><b>diffusers BF16：10GBから*</b><br><b>diffusers INT8(torchao)：7GBから*</b></td>\n    <td style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> FP16: 18GB <br><b>diffusers FP16: 4GB以上* </b><br><b>diffusers INT8(torchao): 3.6GB以上*</b></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> BF16: 26GB <br><b>diffusers BF16 : 5GB以上* </b><br><b>diffusers INT8(torchao): 4.4GB以上* </b></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">複数GPU推論メモリ消費量</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16: 24GB* using diffusers</b><br></td>\n    <td style=\"text-align: center;\"><b>FP16: 10GB* diffusers使用</b><br></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16: 15GB* diffusers使用</b><br></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">推論速度<br>(Step = 50, FP/BF16)</td>\n    <td colspan=\"2\" style=\"text-align: center;\">シングルA100: ~1000秒(5秒ビデオ)<br>シングルH100: ~550秒(5秒ビデオ)</td>\n    <td style=\"text-align: center;\">シングルA100: ~90秒<br>シングルH100: ~45秒</td>\n    <td colspan=\"2\" style=\"text-align: center;\">シングルA100: ~180秒<br>シングルH100: ~90秒</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">プロンプト言語</td>\n    <td colspan=\"5\" style=\"text-align: center;\">英語*</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">プロンプト長さの上限</td>\n    <td colspan=\"2\" style=\"text-align: center;\">224トークン</td>\n    <td colspan=\"3\" style=\"text-align: center;\">226トークン</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">ビデオ長さ</td>\n    <td colspan=\"2\" style=\"text-align: center;\">5秒または10秒</td>\n    <td colspan=\"3\" style=\"text-align: center;\">6秒</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">フレームレート</td>\n    <td colspan=\"2\" style=\"text-align: center;\">16フレーム/秒</td>\n    <td colspan=\"3\" style=\"text-align: center;\">8フレーム/秒</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">位置エンコーディング</td>\n    <td colspan=\"2\" style=\"text-align: center;\">3d_rope_pos_embed</td>\n    <td style=\"text-align: center;\">3d_sincos_pos_embed</td> \n    <td style=\"text-align: center;\">3d_rope_pos_embed</td>\n    <td style=\"text-align: center;\">3d_rope_pos_embed + learnable_pos_embed</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">ダウンロードリンク (Diffusers)</td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5B\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5B\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5B\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5B-I2V\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5B-I2V\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5B-I2V\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-2b\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-2b\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-2b\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-5b\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-5b\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-5b\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-5b-I2V\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-5b-I2V\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-5b-I2V\">🟣 WiseModel</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">ダウンロードリンク (SAT)</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5b-SAT\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5b-SAT\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5b-SAT\">🟣 WiseModel</a></td>\n    <td colspan=\"3\" style=\"text-align: center;\"><a href=\"./sat/README_zh.md\">SAT</a></td>\n  </tr>\n</table>\n\n**データ解説**\n\n+ diffusersライブラリを使用してテストする際には、`diffusers`ライブラリが提供する全ての最適化が有効になっています。この方法は\n  **NVIDIA A100 / H100**以外のデバイスでのメモリ/メモリ消費のテストは行っていません。通常、この方法は**NVIDIA\n  Ampereアーキテクチャ**\n  以上の全てのデバイスに適応できます。最適化を無効にすると、メモリ消費は倍増し、ピークメモリ使用量は表の3倍になりますが、速度は約3〜4倍向上します。以下の最適化を部分的に無効にすることが可能です:\n\n```\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\n```\n\n+ マルチGPUで推論する場合、`enable_sequential_cpu_offload()`最適化を無効にする必要があります。\n+ INT8モデルを使用すると推論速度が低下しますが、これはメモリの少ないGPUで正常に推論を行い、ビデオ品質の損失を最小限に抑えるための措置です。推論速度は大幅に低下します。\n+ CogVideoX-2Bモデルは`FP16`精度でトレーニングされており、CogVideoX-5Bモデルは`BF16`\n  精度でトレーニングされています。推論時にはモデルがトレーニングされた精度を使用することをお勧めします。\n+ [PytorchAO](https://github.com/pytorch/ao)および[Optimum-quanto](https://github.com/huggingface/optimum-quanto/)\n  は、CogVideoXのメモリ要件を削減するためにテキストエンコーダ、トランスフォーマ、およびVAEモジュールを量子化するために使用できます。これにより、無料のT4\n  Colabやより少ないメモリのGPUでモデルを実行することが可能になります。同様に重要なのは、TorchAOの量子化は`torch.compile`\n  と完全に互換性があり、推論速度を大幅に向上させることができる点です。`NVIDIA H100`およびそれ以上のデバイスでは`FP8`\n  精度を使用する必要があります。これには、`torch`、`torchao` Pythonパッケージのソースコードからのインストールが必要です。`CUDA 12.4`の使用をお勧めします。\n+ 推論速度テストも同様に、上記のメモリ最適化方法を使用しています。メモリ最適化を使用しない場合、推論速度は約10％向上します。\n  `diffusers`バージョンのモデルのみが量子化をサポートしています。\n+ モデルは英語入力のみをサポートしており、他の言語は大規模モデルの改善を通じて英語に翻訳できます。\n\n\n## 友好的リンク\n\nコミュニティからの貢献を大歓迎し、私たちもオープンソースコミュニティに積極的に貢献しています。以下の作品はすでにCogVideoXに対応しており、ぜひご利用ください：\n\n+ [CogVideoX-Fun](https://github.com/aigc-apps/CogVideoX-Fun):\n  CogVideoX-Funは、CogVideoXアーキテクチャを基にした改良パイプラインで、自由な解像度と複数の起動方法をサポートしています。\n+ [CogStudio](https://github.com/pinokiofactory/cogstudio): CogVideo の Gradio Web UI の別のリポジトリ。より高機能な Web\n  UI をサポートします。\n+ [Xorbits Inference](https://github.com/xorbitsai/inference):\n  強力で包括的な分散推論フレームワークであり、ワンクリックで独自のモデルや最新のオープンソースモデルを簡単にデプロイできます。\n+ [ComfyUI-CogVideoXWrapper](https://github.com/kijai/ComfyUI-CogVideoXWrapper)\n  ComfyUIフレームワークを使用して、CogVideoXをワークフローに統合します。\n+ [VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys): VideoSysは、使いやすく高性能なビデオ生成インフラを提供し、最新のモデルや技術を継続的に統合しています。\n+ [AutoDLイメージ](https://www.codewithgpu.com/i/THUDM/CogVideo/CogVideoX-5b-demo): コミュニティメンバーが提供するHuggingface\n  Spaceイメージのワンクリックデプロイメント。\n+ [インテリアデザイン微調整モデル](https://huggingface.co/collections/bertjiazheng/koolcogvideox-66e4762f53287b7f39f8f3ba):\n  は、CogVideoXを基盤にした微調整モデルで、インテリアデザイン専用に設計されています。\n+ [xDiT](https://github.com/xdit-project/xDiT):\n  xDiTは、複数のGPUクラスター上でDiTsを並列推論するためのエンジンです。xDiTはリアルタイムの画像およびビデオ生成サービスをサポートしています。\n+ [CogVideoX-Interpolation](https://github.com/feizc/CogvideX-Interpolation):\n  キーフレーム補間生成において、より大きな柔軟性を提供することを目的とした、CogVideoX構造を基にした修正版のパイプライン。\n+ [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): DiffSynth\n  Studioは、拡散エンジンです。テキストエンコーダー、UNet、VAEなどを含むアーキテクチャを再構築し、オープンソースコミュニティモデルとの互換性を維持しつつ、計算性能を向上させました。このフレームワークはCogVideoXに適応しています。\n+ [CogVideoX-Controlnet](https://github.com/TheDenk/cogvideox-controlnet): CogVideoXモデルを含むシンプルなControlNetモジュールのコード。\n+ [VideoTuna](https://github.com/VideoVerses/VideoTuna): VideoTuna は、テキストからビデオ、画像からビデオ、テキストから画像生成のための複数のAIビデオ生成モデルを統合した最初のリポジトリです。\n+ [ConsisID](https://github.com/PKU-YuanGroup/ConsisID): 一貫性のある顔を保持するために、周波数分解を使用するCogVideoX-5Bに基づいたアイデンティティ保持型テキストから動画生成モデル。\n+ [ステップバイステップチュートリアル](https://www.youtube.com/watch?v=5UCkMzP2VLE&ab_channel=SECourses): WindowsおよびクラウドでのCogVideoX1.5-5B-I2Vモデルのインストールと最適化に関するステップバイステップガイド。[FurkanGozukara](https://github.com/FurkanGozukara)氏の尽力とサポートに感謝いたします！\n\n## プロジェクト構造\n\nこのオープンソースリポジトリは、**CogVideoX** オープンソースモデルの基本的な使用方法と微調整の例を迅速に開始するためのガイドです。\n\n### Colabでのクイックスタート\n\n無料のColab T4上で直接実行できる3つのプロジェクトを提供しています。\n\n+ [CogVideoX-5B-T2V-Colab.ipynb](https://colab.research.google.com/drive/1pCe5s0bC_xuXbBlpvIH1z0kfdTLQPzCS?usp=sharing):\n  CogVideoX-5B テキストからビデオへの生成用Colabコード。\n+ [CogVideoX-5B-T2V-Int8-Colab.ipynb](https://colab.research.google.com/drive/1DUffhcjrU-uz7_cpuJO3E_D4BaJT7OPa?usp=sharing):\n  CogVideoX-5B テキストからビデオへの量子化推論用Colabコード。1回の実行に約30分かかります。\n+ [CogVideoX-5B-I2V-Colab.ipynb](https://colab.research.google.com/drive/17CqYCqSwz39nZAX2YyonDxosVKUZGzcX?usp=sharing):\n  CogVideoX-5B 画像からビデオへの生成用Colabコード。\n+ [CogVideoX-5B-V2V-Colab.ipynb](https://colab.research.google.com/drive/1comfGAUJnChl5NwPuO8Ox5_6WCy4kbNN?usp=sharing):\n  CogVideoX-5B ビデオからビデオへの生成用Colabコード。\n\n### Inference\n\n+ [cli_demo](inference/cli_demo.py): 推論コードの詳細な説明が含まれており、一般的なパラメータの意味についても言及しています。\n+ [cli_demo_quantization](inference/cli_demo_quantization.py):\n  量子化モデル推論コードで、低メモリのデバイスでも実行可能です。また、このコードを変更して、FP8 精度の CogVideoX\n  モデルの実行をサポートすることもできます。\n+ [diffusers_vae_demo](inference/cli_vae_demo.py): VAE推論コードの実行には現在71GBのメモリが必要ですが、将来的には最適化される予定です。\n+ [space demo](inference/gradio_composite_demo): Huggingface Spaceと同じGUIコードで、フレーム補間や超解像ツールが組み込まれています。\n\n<div style=\"text-align: center;\">\n    <img src=\"resources/web_demo.png\" style=\"width: 100%; height: auto;\" />\n</div>\n\n+ [convert_demo](inference/convert_demo.py):\n  ユーザー入力をCogVideoXに適した形式に変換する方法。CogVideoXは長いキャプションでトレーニングされているため、入力テキストをLLMを使用してトレーニング分布と一致させる必要があります。デフォルトではGLM-4を使用しますが、GPT、Geminiなどの他のLLMに置き換えることもできます。\n+ [gradio_web_demo](inference/gradio_web_demo.py): CogVideoX-2B / 5B モデルを使用して動画を生成する方法を示す、シンプルな\n  Gradio Web UI デモです。私たちの Huggingface Space と同様に、このスクリプトを使用して Web デモを起動することができます。\n\n### finetune\n\n+ [train_cogvideox_lora](finetune/README_ja.md): CogVideoX diffusers 微調整方法の詳細な説明が含まれています。このコードを使用して、自分のデータセットで\n  CogVideoX を微調整することができます。\n\n### sat\n\n+ [sat_demo](sat/README.md):\n  SATウェイトの推論コードと微調整コードが含まれています。CogVideoXモデル構造に基づいて改善することをお勧めします。革新的な研究者は、このコードを使用して迅速なスタッキングと開発を行うことができます。\n\n### ツール\n\nこのフォルダには、モデル変換/キャプション生成などのツールが含まれています。\n\n+ [convert_weight_sat2hf](tools/convert_weight_sat2hf.py): SAT モデルの重みを Huggingface モデルの重みに変換します。\n+ [caption_demo](tools/caption/README_ja.md): Caption ツール、ビデオを理解してテキストで出力するモデル。\n+ [export_sat_lora_weight](tools/export_sat_lora_weight.py): SAT ファインチューニングモデルのエクスポートツール、SAT Lora\n  Adapter を diffusers 形式でエクスポートします。\n+ [load_cogvideox_lora](tools/load_cogvideox_lora.py): diffusers 版のファインチューニングされた Lora Adapter\n  をロードするためのツールコード。\n+ [llm_flux_cogvideox](tools/llm_flux_cogvideox/llm_flux_cogvideox.py): オープンソースのローカル大規模言語モデル +\n  Flux + CogVideoX を使用して自動的に動画を生成します。\n+ [parallel_inference_xdit](tools/parallel_inference/parallel_inference_xdit.py)：\n  [xDiT](https://github.com/xdit-project/xDiT)\n  によってサポートされ、ビデオ生成プロセスを複数の GPU で並列化します。\n+ [cogvideox-factory](https://github.com/a-r-r-o-w/cogvideox-factory): CogVideoXの低コスト微調整フレームワークで、\n  `diffusers`バージョンのモデルに適応しています。より多くの解像度に対応し、単一の4090 GPUでCogVideoX-5Bの微調整が可能です。\n\n## CogVideo(ICLR'23)\n\n論文の公式リポジトリ: [CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers](https://arxiv.org/abs/2205.15868)\nは [CogVideo branch](https://github.com/THUDM/CogVideo/tree/CogVideo) にあります。\n\n**CogVideoは比較的高フレームレートのビデオを生成することができます。**\n32フレームの4秒間のクリップが以下に示されています。\n\n![High-frame-rate sample](https://raw.githubusercontent.com/THUDM/CogVideo/CogVideo/assets/appendix-sample-highframerate.png)\n\n![Intro images](https://raw.githubusercontent.com/THUDM/CogVideo/CogVideo/assets/intro-image.png)\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/2fa19651-e925-4a2a-b8d6-b3f216d490ba\" width=\"80%\" controls autoplay></video>\n</div>\n\n\nCogVideoのデモは [https://models.aminer.cn/cogvideo](https://models.aminer.cn/cogvideo/) で体験できます。\n*元の入力は中国語です。*\n\n## 引用\n\n🌟 私たちの仕事が役立つと思われた場合、ぜひスターを付けていただき、論文を引用してください。\n\n```\n@article{yang2024cogvideox,\n  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},\n  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},\n  journal={arXiv preprint arXiv:2408.06072},\n  year={2024}\n}\n@article{hong2022cogvideo,\n  title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},\n  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},\n  journal={arXiv preprint arXiv:2205.15868},\n  year={2022}\n}\n```\n\nあなたの貢献をお待ちしています！詳細は[こちら](resources/contribute_ja.md)をクリックしてください。\n\n## ライセンス契約\n\nこのリポジトリのコードは [Apache 2.0 License](LICENSE) の下で公開されています。\n\nCogVideoX-2B モデル (対応するTransformersモジュールやVAEモジュールを含む) は\n[Apache 2.0 License](LICENSE) の下で公開されています。\n\nCogVideoX-5B モデル（Transformers モジュール、画像生成ビデオとテキスト生成ビデオのバージョンを含む） は\n[CogVideoX LICENSE](https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE) の下で公開されています。\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 24.5771484375,
          "content": "# CogVideo & CogVideoX\n\n[Read this in English](./README.md)\n\n[日本語で読む](./README_ja.md)\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"50%\"/>\n</div>\n<p align=\"center\">\n在 <a href=\"https://huggingface.co/spaces/THUDM/CogVideoX-5B\" target=\"_blank\"> 🤗 Huggingface Space</a> 或 <a href=\"https://modelscope.cn/studios/ZhipuAI/CogVideoX-5b-demo\" target=\"_blank\"> 🤖 ModelScope Space</a> 在线体验 CogVideoX-5B 模型\n</p>\n<p align=\"center\">\n📚 查看 <a href=\"https://arxiv.org/abs/2408.06072\" target=\"_blank\">论文</a> 和 <a href=\"https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh\" target=\"_blank\">使用文档</a>\n</p>\n<p align=\"center\">\n    👋 加入我们的 <a href=\"resources/WECHAT.md\" target=\"_blank\">微信</a> 和  <a href=\"https://discord.gg/dCGfUsagrD\" target=\"_blank\">Discord</a> \n</p>\n<p align=\"center\">\n📍 前往<a href=\"https://chatglm.cn/video?fr=osm_cogvideox\"> 清影</a> 和 <a href=\"https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9\"> API平台</a> 体验更大规模的商业版视频生成模型。\n</p>\n\n## 项目更新\n\n- 🔥🔥 **News**: ```2025/01/08```: 我们更新了基于`diffusers`版本模型的`Lora`微调代码，占用显存更低，详情请见[这里](finetune/README_zh.md)。\n- 🔥 **News**: ```2024/11/15```: 我们发布 `CogVideoX1.5` 模型的diffusers版本，仅需调整部分参数仅可沿用之前的代码。\n- 🔥 **News**: ```2024/11/08```: 我们发布 `CogVideoX1.5` 模型。CogVideoX1.5 是 CogVideoX 开源模型的升级版本。 \nCogVideoX1.5-5B 系列模型支持 **10秒** 长度的视频和更高的分辨率，其中 `CogVideoX1.5-5B-I2V` 支持 **任意分辨率** 的视频生成，SAT代码已经更新。`diffusers`版本还在适配中。SAT版本代码前往 [这里](https://huggingface.co/THUDM/CogVideoX1.5-5B-SAT) 下载。\n- 🔥**News**: ```2024/10/13```: 成本更低，单卡4090可微调 `CogVideoX-5B`\n  的微调框架[cogvideox-factory](https://github.com/a-r-r-o-w/cogvideox-factory)已经推出，多种分辨率微调，欢迎使用。\n- 🔥 **News**: ```2024/10/10```: 我们更新了我们的技术报告,请点击 [这里](https://arxiv.org/pdf/2408.06072)\n  查看，附上了更多的训练细节和demo，关于demo，点击[这里](https://yzy-thu.github.io/CogVideoX-demo/) 查看。\n- 🔥 **News**: ```2024/10/09```: 我们在飞书[技术文档](https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh\")\n  公开CogVideoX微调指导，以进一步增加分发自由度，公开文档中所有示例可以完全复现\n- 🔥 **News**: ```2024/9/19```: 我们开源 CogVideoX 系列图生视频模型 **CogVideoX-5B-I2V**\n  。该模型可以将一张图像作为背景输入，结合提示词一起生成视频，具有更强的可控性。\n  至此，CogVideoX系列模型已经支持文本生成视频，视频续写，图片生成视频三种任务。欢迎前往在线[体验](https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space)。\n- 🔥 **News**: ```2024/9/19```: CogVideoX 训练过程中用于将视频数据转换为文本描述的 Caption\n  模型 [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption)\n  已经开源。欢迎前往下载并使用。\n- 🔥 ```2024/8/27```:  我们开源 CogVideoX 系列更大的模型 **CogVideoX-5B**\n  。我们大幅度优化了模型的推理性能，推理门槛大幅降低，您可以在 `GTX 1080TI` 等早期显卡运行 **CogVideoX-2B**，在 `RTX 3060`\n  等桌面端甜品卡运行 **CogVideoX-5B** 模型。 请严格按照[要求](requirements.txt)\n  更新安装依赖，推理代码请查看 [cli_demo](inference/cli_demo.py)。同时，**CogVideoX-2B** 模型开源协议已经修改为**Apache 2.0 协议**。\n- 🔥 ```2024/8/6```: 我们开源 **3D Causal VAE**，用于 **CogVideoX-2B**，可以几乎无损地重构视频。\n- 🔥 ```2024/8/6```: 我们开源 CogVideoX 系列视频生成模型的第一个模型, **CogVideoX-2B**。\n- 🌱 **Source**: ```2022/5/19```: 我们开源了 CogVideo 视频生成模型（现在你可以在 `CogVideo` 分支中看到），这是首个开源的基于\n  Transformer 的大型文本生成视频模型，您可以访问 [ICLR'23 论文](https://arxiv.org/abs/2205.15868) 查看技术细节。\n\n## 目录\n\n跳转到指定部分：\n\n- [快速开始](#快速开始)\n  - [提示词优化](#提示词优化)\n  - [SAT](#sat)\n  - [Diffusers](#diffusers)\n- [视频作品](#视频作品)\n  - [CogVideoX-5B](#cogvideox-5b)\n  - [CogVideoX-2B](#cogvideox-2b)\n- [模型介绍](#模型介绍)\n- [友情链接](#友情链接)\n- [完整项目代码结构](#完整项目代码结构)\n  - [Colab 快速使用](#colab-快速使用)\n  - [inference](#inference)\n  - [finetune](#finetune)\n  - [sat](#sat-1)\n  - [tools](#tools)\n- [CogVideo(ICLR'23)](#cogvideoiclr23)\n- [引用](#引用)\n- [模型协议](#模型协议)\n\n## 快速开始\n\n### 提示词优化\n\n在开始运行模型之前，请参考 [这里](inference/convert_demo.py) 查看我们是怎么使用GLM-4(或者同级别的其他产品，例如GPT-4)\n大模型对模型进行优化的，这很重要，\n由于模型是在长提示词下训练的，一个好的提示词直接影响了视频生成的质量。\n\n### SAT\n\n查看sat文件夹下的 [sat_demo](sat/README.md)：包含了 SAT 权重的推理代码和微调代码，推荐基于此代码进行 CogVideoX\n模型结构的改进，研究者使用该代码可以更好的进行快速的迭代和开发。\n\n### Diffusers\n\n```\npip install -r requirements.txt\n```\n\n查看[diffusers_demo](inference/cli_demo.py)：包含对推理代码更详细的解释，包括各种关键的参数。\n\n欲了解更多关于量化推理的细节，请参考 [diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao/)。使用 Diffusers\n和 TorchAO，量化推理也是可能的，这可以实现内存高效的推理，并且在某些情况下编译后速度有所提升。有关在 A100 和 H100\n上使用各种设置的内存和时间基准测试的完整列表，已发布在 [diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao)\n上。\n\n## 视频作品\n\n### CogVideoX-5B\n\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/cf5953ea-96d3-48fd-9907-c4708752c714\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/fe0a78e6-b669-4800-8cf0-b5f9b5145b52\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/c182f606-8f8c-421d-b414-8487070fcfcb\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/7db2bbce-194d-434d-a605-350254b6c298\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/62b01046-8cab-44cc-bd45-4d965bb615ec\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/d78e552a-4b3f-4b81-ac3f-3898079554f6\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/30894f12-c741-44a2-9e6e-ddcacc231e5b\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/926575ca-7150-435b-a0ff-4900a963297b\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n</table>\n\n### CogVideoX-2B\n\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\n  <tr>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/ea3af39a-3160-4999-90ec-2f7863c5b0e9\" width=\"100%\" controls autoplay loop></video>\n      </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/9de41efd-d4d1-4095-aeda-246dd834e91d\" width=\"100%\" controls autoplay loop></video>\n      </td>\n       <td>\n          <video src=\"https://github.com/user-attachments/assets/941d6661-6a8d-4a1b-b912-59606f0b2841\" width=\"100%\" controls autoplay loop></video>\n     </td>\n      <td>\n          <video src=\"https://github.com/user-attachments/assets/938529c4-91ae-4f60-b96b-3c3947fa63cb\" width=\"100%\" controls autoplay loop></video>\n     </td>\n  </tr>\n</table>\n\n\n查看画廊的对应提示词，请点击[这里](resources/galary_prompt.md)\n\n## 模型介绍\n\nCogVideoX是 [清影](https://chatglm.cn/video?fr=osm_cogvideox) 同源的开源版本视频生成模型。\n下表展示我们提供的视频生成模型相关基础信息:\n\n<table  style=\"border-collapse: collapse; width: 100%;\">\n  <tr>\n    <th style=\"text-align: center;\">模型名</th>\n    <th style=\"text-align: center;\">CogVideoX1.5-5B (最新)</th>\n    <th style=\"text-align: center;\">CogVideoX1.5-5B-I2V (最新)</th>\n    <th style=\"text-align: center;\">CogVideoX-2B</th>\n    <th style=\"text-align: center;\">CogVideoX-5B</th>\n    <th style=\"text-align: center;\">CogVideoX-5B-I2V </th>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">发布时间</td>\n    <th style=\"text-align: center;\">2024年11月8日</th>\n    <th style=\"text-align: center;\">2024年11月8日</th>\n    <th style=\"text-align: center;\">2024年8月6日</th>\n    <th style=\"text-align: center;\">2024年8月27日</th>\n    <th style=\"text-align: center;\">2024年9月19日</th>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">视频分辨率</td>\n    <td colspan=\"1\" style=\"text-align: center;\">1360 * 768</td>\n    <td colspan=\"1\" style=\"text-align: center;\"> Min(W, H) = 768 <br> 768 ≤ Max(W, H) ≤ 1360 <br> Max(W, H) % 16 = 0 </td>\n    <td colspan=\"3\" style=\"text-align: center;\">720 * 480</td>\n    </tr>\n  <tr>\n    <td style=\"text-align: center;\">推理精度</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16(推荐)</b>, FP16, FP32，FP8*，INT8，不支持INT4</td>\n    <td style=\"text-align: center;\"><b>FP16*(推荐)</b>, BF16, FP32，FP8*，INT8，不支持INT4</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16(推荐)</b>, FP16, FP32，FP8*，INT8，不支持INT4</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">单GPU显存消耗<br></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> BF16: 76GB <br><b>diffusers BF16 : 10GB起* </b><br><b>diffusers INT8(torchao): 7G起* </b></td>\n    <td style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> FP16: 18GB <br><b>diffusers FP16: 4GB起* </b><br><b>diffusers INT8(torchao): 3.6G起*</b></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://github.com/THUDM/SwissArmyTransformer\">SAT</a> BF16: 26GB <br><b>diffusers BF16 : 5GB起* </b><br><b>diffusers INT8(torchao): 4.4G起* </b></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">多GPU推理显存消耗</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16: 24GB* using diffusers</b><br></td>\n    <td style=\"text-align: center;\"><b>FP16: 10GB* using diffusers</b><br></td>\n    <td colspan=\"2\" style=\"text-align: center;\"><b>BF16: 15GB* using diffusers</b><br></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">推理速度<br>(Step = 50, FP/BF16)</td>\n    <td colspan=\"2\" style=\"text-align: center;\">单卡A100: ~1000秒(5秒视频)<br>单卡H100: ~550秒(5秒视频)</td>\n    <td style=\"text-align: center;\">单卡A100: ~90秒<br>单卡H100: ~45秒</td>\n    <td colspan=\"2\" style=\"text-align: center;\">单卡A100: ~180秒<br>单卡H100: ~90秒</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">提示词语言</td>\n    <td colspan=\"5\" style=\"text-align: center;\">English*</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">提示词长度上限</td>\n    <td colspan=\"2\" style=\"text-align: center;\">224 Tokens</td>\n    <td colspan=\"3\" style=\"text-align: center;\">226 Tokens</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">视频长度</td>\n    <td colspan=\"2\" style=\"text-align: center;\">5 秒 或 10 秒</td>\n    <td colspan=\"3\" style=\"text-align: center;\">6 秒</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">帧率</td>\n    <td colspan=\"2\" style=\"text-align: center;\">16 帧 / 秒 </td>\n    <td colspan=\"3\" style=\"text-align: center;\">8 帧 / 秒 </td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">位置编码</td>\n    <td colspan=\"2\" style=\"text-align: center;\">3d_rope_pos_embed</td>\n    <td style=\"text-align: center;\">3d_sincos_pos_embed</td> \n    <td style=\"text-align: center;\">3d_rope_pos_embed</td>\n    <td style=\"text-align: center;\">3d_rope_pos_embed + learnable_pos_embed</td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">下载链接 (Diffusers)</td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5B\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5B\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5B\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5B-I2V\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5B-I2V\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5B-I2V\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-2b\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-2b\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-2b\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-5b\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-5b\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-5b\">🟣 WiseModel</a></td>\n    <td style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX-5b-I2V\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX-5b-I2V\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX-5b-I2V\">🟣 WiseModel</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center;\">下载链接 (SAT)</td>\n    <td colspan=\"2\" style=\"text-align: center;\"><a href=\"https://huggingface.co/THUDM/CogVideoX1.5-5b-SAT\">🤗 HuggingFace</a><br><a href=\"https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5b-SAT\">🤖 ModelScope</a><br><a href=\"https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5b-SAT\">🟣 WiseModel</a></td>\n    <td colspan=\"3\" style=\"text-align: center;\"><a href=\"./sat/README_zh.md\">SAT</a></td>\n  </tr>\n</table>\n\n**数据解释**\n\n+ 使用 diffusers 库进行测试时，启用了全部`diffusers`库自带的优化，该方案未测试在非**NVIDIA A100 / H100**\n  外的设备上的实际显存 / 内存占用。通常，该方案可以适配于所有 **NVIDIA 安培架构**\n  以上的设备。若关闭优化，显存占用会成倍增加，峰值显存约为表格的3倍。但速度提升3-4倍左右。你可以选择性的关闭部分优化，这些优化包括:\n\n```\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\n```\n\n+ 多GPU推理时，需要关闭 `enable_sequential_cpu_offload()` 优化。\n+ 使用 INT8 模型会导致推理速度降低，此举是为了满足显存较低的显卡能正常推理并保持较少的视频质量损失，推理速度大幅降低。\n+ CogVideoX-2B 模型采用 `FP16` 精度训练， 搜有 CogVideoX-5B 模型采用 `BF16` 精度训练。我们推荐使用模型训练的精度进行推理。\n+ [PytorchAO](https://github.com/pytorch/ao) 和 [Optimum-quanto](https://github.com/huggingface/optimum-quanto/)\n  可以用于量化文本编码器、Transformer 和 VAE 模块，以降低 CogVideoX 的内存需求。这使得在免费的 T4 Colab 或更小显存的 GPU\n  上运行模型成为可能！同样值得注意的是，TorchAO 量化完全兼容 `torch.compile`，这可以显著提高推理速度。在 `NVIDIA H100`\n  及以上设备上必须使用 `FP8` 精度，这需要源码安装 `torch`、`torchao` Python 包。建议使用 `CUDA 12.4`。\n+ 推理速度测试同样采用了上述显存优化方案，不采用显存优化的情况下，推理速度提升约10%。 只有`diffusers`版本模型支持量化。\n+ 模型仅支持英语输入，其他语言可以通过大模型润色时翻译为英语。\n\n## 友情链接\n\n我们非常欢迎来自社区的贡献，并积极的贡献开源社区。以下作品已经对CogVideoX进行了适配，欢迎大家使用:\n\n+ [CogVideoX-Fun](https://github.com/aigc-apps/CogVideoX-Fun):\n  CogVideoX-Fun是一个基于CogVideoX结构修改后的的pipeline，支持自由的分辨率，多种启动方式。\n+ [CogStudio](https://github.com/pinokiofactory/cogstudio): CogVideo 的 Gradio Web UI单独实现仓库，支持更多功能的 Web UI。\n+ [Xorbits Inference](https://github.com/xorbitsai/inference): 性能强大且功能全面的分布式推理框架，轻松一键部署你自己的模型或内置的前沿开源模型。\n+ [ComfyUI-CogVideoXWrapper](https://github.com/kijai/ComfyUI-CogVideoXWrapper) 使用ComfyUI框架，将CogVideoX加入到你的工作流中。\n+ [VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys): VideoSys 提供了易用且高性能的视频生成基础设施，支持完整的管道，并持续集成最新的模型和技术。\n+ [AutoDL镜像](https://www.codewithgpu.com/i/THUDM/CogVideo/CogVideoX-5b-demo): 由社区成员提供的一键部署Huggingface\n  Space镜像。\n+ [室内设计微调模型](https://huggingface.co/collections/bertjiazheng/koolcogvideox-66e4762f53287b7f39f8f3ba) 基于\n  CogVideoX的微调模型，它专为室内设计而设计\n+ [xDiT](https://github.com/xdit-project/xDiT): xDiT是一个用于在多GPU集群上对DiTs并行推理的引擎。xDiT支持实时图像和视频生成服务。\n+ [CogVideoX-Interpolation](https://github.com/feizc/CogvideX-Interpolation): 基于 CogVideoX 结构修改的管道，旨在为关键帧插值生成提供更大的灵活性。\n+ [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): DiffSynth 工作室是一款扩散引擎。重构了架构，包括文本编码器、UNet、VAE\n  等，在保持与开源社区模型兼容性的同时，提升了计算性能。该框架已经适配 CogVideoX。\n+ [CogVideoX-Controlnet](https://github.com/TheDenk/cogvideox-controlnet): 一个包含 CogvideoX 模型的简单 Controlnet 模块的代码。\n+ [VideoTuna](https://github.com/VideoVerses/VideoTuna)：VideoTuna 是首个集成多种 AI 视频生成模型的仓库，支持文本转视频、图像转视频、文本转图像生成。\n+ [ConsisID](https://github.com/PKU-YuanGroup/ConsisID): 一种身份保持的文本到视频生成模型，基于 CogVideoX-5B，通过频率分解在生成的视频中保持面部一致性。\n+ [教程](https://www.youtube.com/watch?v=5UCkMzP2VLE&ab_channel=SECourses): 一个关于在Windows和云环境中安装和优化CogVideoX1.5-5B-I2V模型的分步指南。特别感谢[FurkanGozukara](https://github.com/FurkanGozukara)的努力和支持！\n\n\n## 完整项目代码结构\n\n本开源仓库将带领开发者快速上手 **CogVideoX** 开源模型的基础调用方式、微调示例。\n\n### Colab 快速使用\n\n这里提供了三个能直接在免费的 Colab T4上 运行的项目\n\n+ [CogVideoX-5B-T2V-Colab.ipynb](https://colab.research.google.com/drive/1pCe5s0bC_xuXbBlpvIH1z0kfdTLQPzCS?usp=sharing):\n  CogVideoX-5B 文字生成视频 Colab 代码。\n+ [CogVideoX-5B-T2V-Int8-Colab.ipynb](https://colab.research.google.com/drive/1DUffhcjrU-uz7_cpuJO3E_D4BaJT7OPa?usp=sharing):\n  CogVideoX-5B 文字生成视频量化推理 Colab 代码，运行一次大约需要30分钟。\n+ [CogVideoX-5B-I2V-Colab.ipynb](https://colab.research.google.com/drive/17CqYCqSwz39nZAX2YyonDxosVKUZGzcX?usp=sharing):\n  CogVideoX-5B 图片生成视频 Colab 代码。\n+ [CogVideoX-5B-V2V-Colab.ipynb](https://colab.research.google.com/drive/1comfGAUJnChl5NwPuO8Ox5_6WCy4kbNN?usp=sharing):\n  CogVideoX-5B 视频生成视频 Colab 代码。\n\n### inference\n\n+ [cli_demo](inference/cli_demo.py): 更详细的推理代码讲解，常见参数的意义，在这里都会提及。\n+ [cli_demo_quantization](inference/cli_demo_quantization.py):\n  量化模型推理代码，可以在显存较低的设备上运行，也可以基于此代码修改，以支持运行FP8等精度的CogVideoX模型。请注意，FP8\n  仅测试通过，且必须将 `torch-nightly`,`torchao`源代码安装，不建议在生产环境中使用。\n+ [diffusers_vae_demo](inference/cli_vae_demo.py): 单独执行VAE的推理代码。\n+ [space demo](inference/gradio_composite_demo): Huggingface Space同款的 GUI 代码，植入了插帧，超分工具。\n\n<div style=\"text-align: center;\">\n    <img src=\"resources/web_demo.png\" style=\"width: 100%; height: auto;\" />\n</div>\n\n+ [convert_demo](inference/convert_demo.py): 如何将用户的输入转换成适合\n  CogVideoX的长输入。因为CogVideoX是在长文本上训练的，所以我们需要把输入文本的分布通过LLM转换为和训练一致的长文本。脚本中默认使用GLM-4，也可以替换为GPT、Gemini等任意大语言模型。\n+ [gradio_web_demo](inference/gradio_composite_demo/app.py): 与 Huggingface Space 完全相同的代码实现，快速部署 CogVideoX\n  GUI体验。\n\n### finetune\n\n+ [train_cogvideox_lora](finetune/README_zh.md): diffusers版本 CogVideoX 模型微调方案和细节。\n\n### sat\n\n+ [sat_demo](sat/README_zh.md): 包含了 SAT 权重的推理代码和微调代码，推荐基于 CogVideoX\n  模型结构进行改进，创新的研究者使用改代码以更好的进行快速的堆叠和开发。\n\n### tools\n\n本文件夹包含了一些工具，用于模型的转换 / Caption 等工作。\n\n+ [convert_weight_sat2hf](tools/convert_weight_sat2hf.py): 将 SAT 模型权重转换为 Huggingface 模型权重。\n+ [caption_demo](tools/caption/README_zh.md):  Caption 工具，对视频理解并用文字输出的模型。\n+ [export_sat_lora_weight](tools/export_sat_lora_weight.py):  SAT微调模型导出工具，将\n  SAT Lora Adapter 导出为 diffusers 格式。\n+ [load_cogvideox_lora](tools/load_cogvideox_lora.py): 载入diffusers版微调Lora Adapter的工具代码。\n+ [llm_flux_cogvideox](tools/llm_flux_cogvideox/llm_flux_cogvideox.py): 使用开源本地大语言模型 + Flux +\n  CogVideoX实现自动化生成视频。\n+ [parallel_inference_xdit](tools/parallel_inference/parallel_inference_xdit.py):\n  在多个 GPU 上并行化视频生成过程，\n  由[xDiT](https://github.com/xdit-project/xDiT)提供支持。\n+ [cogvideox-factory](https://github.com/a-r-r-o-w/cogvideox-factory): CogVideoX低成文微调框架，适配`diffusers`\n  版本模型。支持更多分辨率，单卡4090即可微调 CogVideoX-5B 。\n\n## CogVideo(ICLR'23)\n\n[CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers](https://arxiv.org/abs/2205.15868)\n的官方repo位于[CogVideo branch](https://github.com/THUDM/CogVideo/tree/CogVideo)。\n\n**CogVideo可以生成高帧率视频，下面展示了一个32帧的4秒视频。**\n\n![High-frame-rate sample](https://raw.githubusercontent.com/THUDM/CogVideo/CogVideo/assets/appendix-sample-highframerate.png)\n\n![Intro images](https://raw.githubusercontent.com/THUDM/CogVideo/CogVideo/assets/intro-image.png)\n\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/ea3af39a-3160-4999-90ec-2f7863c5b0e9\" width=\"80%\" controls autoplay></video>\n</div>\n\nCogVideo的demo网站在[https://models.aminer.cn/cogvideo](https://models.aminer.cn/cogvideo/)。您可以在这里体验文本到视频生成。\n*原始输入为中文。*\n\n## 引用\n\n🌟 如果您发现我们的工作有所帮助，欢迎引用我们的文章，留下宝贵的stars\n\n```\n@article{yang2024cogvideox,\n  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},\n  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},\n  journal={arXiv preprint arXiv:2408.06072},\n  year={2024}\n}\n@article{hong2022cogvideo,\n  title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},\n  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},\n  journal={arXiv preprint arXiv:2205.15868},\n  year={2022}\n}\n```\n\n我们欢迎您的贡献，您可以点击[这里](resources/contribute_zh.md)查看更多信息。\n\n## 模型协议\n\n本仓库代码使用 [Apache 2.0 协议](LICENSE) 发布。\n\nCogVideoX-2B 模型 (包括其对应的Transformers模块，VAE模块) 根据 [Apache 2.0 协议](LICENSE) 许可证发布。\n\nCogVideoX-5B 模型 (Transformers 模块，包括图生视频，文生视频版本)\n根据 [CogVideoX LICENSE](https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE)\n许可证发布。\n"
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.6826171875,
          "content": "[tool.ruff]\nline-length = 119\n\n[tool.ruff.lint]\n# Never enforce `E501` (line length violations).\nignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\"]\nselect = [\"C\", \"E\", \"F\", \"I\", \"W\"]\n\n# Ignore import violations in all `__init__.py` files.\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"E402\", \"F401\", \"F403\", \"F811\"]\n\n[tool.ruff.lint.isort]\nlines-after-imports = 2\n\n[tool.ruff.format]\n# Like Black, use double quotes for strings.\nquote-style = \"double\"\n\n# Like Black, indent with spaces, rather than tabs.\nindent-style = \"space\"\n\n# Like Black, respect magic trailing commas.\nskip-magic-trailing-comma = false\n\n# Like Black, automatically detect the appropriate line ending.\nline-ending = \"auto\"\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.25,
          "content": "diffusers>=0.31.0\naccelerate>=1.1.1\ntransformers>=4.46.2\nnumpy==1.26.0\ntorch>=2.5.0\ntorchvision>=0.20.0\nsentencepiece>=0.2.0\nSwissArmyTransformer>=0.4.12\ngradio>=5.5.0\nimageio>=2.35.1\nimageio-ffmpeg>=0.5.1\nopenai>=1.54.0\nmoviepy>=1.0.3\nscikit-video>=1.1.11"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "sat",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}