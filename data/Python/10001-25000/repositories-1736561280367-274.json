{
  "metadata": {
    "timestamp": 1736561280367,
    "page": 274,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ydataai/ydata-profiling",
      "stars": 12635,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.01953125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\n.venv\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\n# PyBuilder\ntarget/\n\n# gedit backup files\n*~\n\n# remove pycharm boilerplate files\n.idea\n\n# ipynb example stuff\n.ipynb_checkpoints/\nipynb_tmp/\n*.parquet\n\n# mypy\n.mypy_cache/\n\n# playground\n/data/\nplayground/\n.devcontainer/\n\n# build data\nexamples/*/*.html\nexamples/*/*.csv\nstatic/docs\ndocsrc/_build/\ndocsrc/source/pages/api/_autosummary/\n\n# User created\nVERSION\nversion.py\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.5068359375,
          "content": "default_stages: [commit, push, manual]\n\nrepos:\n-   repo: https://github.com/commitizen-tools/commitizen\n    rev: v2.37.0\n    hooks:\n    - id: commitizen\n      stages: [commit-msg]\n-   repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n    - id: black\n      language_version: python3\n-   repo: https://github.com/nbQA-dev/nbQA\n    rev: 1.2.2\n    hooks:\n    - id: nbqa-black\n    - id: nbqa-isort\n      args: [ --profile=black, --project=ydata_profiling ]\n    - id: nbqa-pyupgrade\n      args: [ --py36-plus ]\n-   repo: https://github.com/asottile/pyupgrade\n    rev: v2.31.0\n    hooks:\n    -   id: pyupgrade\n        args: ['--py37-plus','--exit-zero-even-if-changed']\n-   repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        files: '.*'\n        args: [ --profile=black, --project=ydata_profiling ]\n-   repo: https://github.com/mgedmin/check-manifest\n    rev: \"0.47\"\n    hooks:\n    -   id: check-manifest\n        args: [ \"--ignore=src/ydata_profiling/version.py\" ]\n        stages: [manual]\n-   repo: https://github.com/PyCQA/flake8\n    rev: \"4.0.1\"\n    hooks:\n    -   id: flake8\n        args: [ \"--ignore=E203,E501,W291,W503,SFS301,SIM106\" ]\n        additional_dependencies:\n          - flake8-comprehensions\n          - flake8-sfs\n          - flake8-simplify\n          - flake8-eradicate\n          - flake8-print\n-   repo: https://github.com/PyCQA/flake8\n    rev: \"4.0.1\"\n    hooks:\n    -   id: flake8\n        name: flake8-annotations\n        args: [ \"--select=ANN001,ANN201,ANN202,ANN205,ANN206,ANN301,C4\" ]\n        additional_dependencies:\n          - flake8-annotations\n          - flake8-comprehensions\n        exclude: |\n          (?x)(\n            ^tests/|\n            ^docsrc/|\n            ^src/ydata_profiling/utils/common.py|\n            ^src/ydata_profiling/utils/imghdr_patch.py\n          )\n-   repo: https://github.com/asottile/blacken-docs\n    rev: v1.12.1\n    hooks:\n    -   id: blacken-docs\n-   repo: https://github.com/pre-commit/mirrors-mypy\n    rev: 'v0.982'\n    hooks:\n    -   id: mypy\n        additional_dependencies:\n          - types-requests\n          - types-python-dateutil\n          - types-PyYAML\n          - types-setuptools\n-   repo: https://github.com/sbrugman/tryceratops\n    rev: v0.6.0\n    hooks:\n      - id: tryceratops\n        args: [\"-i\", \"TC003\",\"--autofix\"]\n-   repo: https://github.com/kynan/nbstripout\n    rev: 0.5.0\n    hooks:\n      - id: nbstripout\n\nci:\n  autoupdate_commit_msg: 'ci: pre-commit-config update'\n  autofix_prs: false\n  autofix_commit_msg: 'ci: pre-commit auto fixes'\n"
        },
        {
          "name": ".releaserc.json",
          "type": "blob",
          "size": 0.2529296875,
          "content": "{\n  \"plugins\": [\n      [\"@semantic-release/commit-analyzer\", {\n        \"preset\": \"angular\",\n        \"releaseRules\": [\n          {\"type\": \"chore\", \"scope\": \"deps\", \"release\": \"patch\"}\n        ]\n      }],\n      \"@semantic-release/release-notes-generator\"\n  ]\n}\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 6.0634765625,
          "content": "## How to contribute to YData-Profiling\n\nYData-profiling aims to ease exploratory data analysis for structured datasets, including time-series. \nOur focus is to provide users with useful and robust statistics for such datasets encountered in industry, academia and elsewhere.\nYData-profiling is open-source and stimulates contributions from passionate community users.\n\n\n#### Themes to contribute\nIn line with our aim, we identify the following themes:\n\n- **Exploratory data analysis**: \n  The core of the package is a dataset summarization by its main characteristics, which is complemented with warnings on data issues and visualisations.\n\n  _Suggestions for contribution_: \n  Extend the support of more data types (think of paths, location or GPS coordinates and ordinal data types),\n  text data (e.g. encoding, vocabulary size, spelling errors, language detection), \n  time series analysis, \n  or even images (e.g. dimensions, EXIF).\n  \n  _Related_: [#7][i7], [#129][i129], [#190][i190], [#204][i204] or [create one](https://github.com/ydataai/ydata-profiling/issues/new/choose).\n\n- **Stability, Performance and Restricted environment compatibility:** \n  Data exploration takes place in all kinds of conditions, on the latest machine learning platforms with enormous dataset to managed environments in large corporations.\n  `ydata-profiling` helps analysts, researchers and engineers alike in these cases.\n  We do this by fixing bugs, improving performance on big datasets and adding environment compatibility.\n  \n  _Suggestions for contribution (Performance)_: \n  Perform concurrency analysis or profile execution times and leverage the gained insights for improved performance (e.g. multiprocessing, cython, numba) or test the performance of `ydata-profiling` with [big data sets](https://www.stats.govt.nz/large-datasets/csv-files-for-download/) and corresponding commonly used data formats (such as parquet). \n  \n  _Suggestions for contribution (Stability)_: \n  Either review the code and add tests or watch the [issues page](https://github.com/ydataai/ydata-profiling/issues) and [Stackoverflow tag](https://stackoverflow.com/questions/tagged/ydata-profiling) to find current issues.\n     \n  _Related_: [#98][i98], [#122][i122] or [create one](https://github.com/ydataai/ydata-profiling/issues/new/choose).\n\n- **Interaction, presentation and user experience**: \n  As `ydata-profiling` eases exploratory data analysis, working with the package should reflect that.\n  Interaction and user experience plays a central role in working with the package.\n  Working on interactive and static features is possible through the modular nature of the package: the user can configure which features to use.\n\n  _Suggestions for contribution (interactivity)_:\n  Interactivity allows for more user friendly applications, including but not limited to on demand analysis (don't compute what you don't want to see) and interactive histograms and correlations. \n  This is ideal for smaller datasets, where we can compute this on-the-fly. \n  `ipywidgets` would be a great place to start (e.g. [widget based view](https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html)).\n\n  _Suggestions for contribution (presentation)_:\n  Other forms of distribution than HTML (for example PDF or packaged as an GUI application via [PyQt](https://riverbankcomputing.com/software/pyqt/intro))\n  Users should be able to share reports (improve size of labels in graph, add explanations to correlation matrices and allow for styling/branding).\n\n  _Related_: [#161][i161], [#175][i175], [#191][i191] or [create one](https://github.com/ydataai/ydata-profiling/issues/new/choose).\n\n- **Community**: \n  The success of this package demonstrates the power of sharing and working together.\n  You are welcome as part of this community.\n  \n  _Suggestions for contribution_:\n  Share with us if this package is of value to you, let us know [in our community](https://discord.com/invite/mw7xjJ7b7s).\n  We are interested in how you use `ydata-profiling` in your work.\n  \n  _Related_: [#87][i87] or [create one](https://github.com/ydataai/ydata-profiling/issues/new/choose).\n\n- **Machine learning:** \n  `ydata-profiling` is not a machine learning package, even though many of our users use EDA as a step prior to developing their models.\n  Our focus lies in the exploratory data analysis.\n  Any functionality that enables machine learning applications by more effective data profiling, is welcome.\n\n  _Related_: [#124][i124], [#173][i173], [#198][i198] or [create one](https://github.com/ydataai/ydata-profiling/issues/new/choose).\n\n#### **Did you find a bug?**\n\n* **Ensure the bug was not already reported** by searching on Github under [Issues](https://github.com/ydataai/ydata-profiling/issues).\n\n* If you're unable to find an open issue addressing the problem, [open a new one](https://github.com/ydataai/ydata-profiling/issues/new/choose). \nIf possible, use the relevant bug report templates to create the issue. \n\n#### **Did you write a patch that fixes a bug?**\n\n* Open a new Github pull request with the patch.\n\n* Ensure the PR description clearly describes the problem and solution. \nInclude the relevant issue number if applicable.\n\n\n#### Acknowledgements\n\nWe would like to thank everyone who has helped getting us to where we are now.\n\nSee the [Contributor Graph](https://github.com/ydataai/ydata-profiling/graphs/contributors)\n\n[i7]: https://github.com/ydataai/ydata-profiling/issues/7\n[i129]: https://github.com/ydataai/ydata-profiling/issues/129\n[i190]: https://github.com/ydataai/ydata-profiling/issues/190\n[i204]: https://github.com/ydataai/ydata-profiling/issues/204\n[i98]: https://github.com/ydataai/ydata-profiling/issues/98\n[i122]: https://github.com/ydataai/ydata-profiling/issues/122\n[i124]: https://github.com/ydataai/ydata-profiling/issues/24\n[i173]: https://github.com/ydataai/ydata-profiling/issues/173\n[i198]: https://github.com/ydataai/ydata-profiling/issues/198\n[i87]: https://github.com/ydataai/ydata-profiling/issues/87\n[i161]: https://github.com/ydataai/ydata-profiling/issues/161\n[i175]: https://github.com/ydataai/ydata-profiling/issues/175\n[i191]: https://github.com/ydataai/ydata-profiling/issues/191\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1064453125,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2016 Jos Polfliet, 2019-2021 Simon Brugman, 2022-Present YData Labs Inc\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.728515625,
          "content": "# Requirements\ninclude requirements*.txt\n\n# Include license, Readme, etc.\ninclude LICENSE\ninclude *.md\ninclude mypy.ini\ninclude src/ydata_profiling/py.typed\n\n# Templates and static resources\nrecursive-include src/ydata_profiling/report/presentation/flavours/html/templates *.html *.js *.css\n\n# Configuration\ninclude src/ydata_profiling/*.yaml\n\n# Spark Dev venv\nrecursive-include venv *.yml\n\n# Exclude development, docs, testing and example code\nexclude .pre-commit-config.yaml\nexclude commitlint.config.js\nexclude .releaserc.json\ninclude Makefile make.bat\nexclude docs examples tests docsrc .devcontainer\nrecursive-exclude docs *\nrecursive-exclude docsrc *\nrecursive-exclude examples *\nrecursive-exclude tests *\nrecursive-exclude .devcontainer *\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.3916015625,
          "content": ".PHONY: docs examples\n\ndocs:\n\tmkdocs build\n\ntest:\n\tpytest tests/unit/\n\tpytest tests/issues/\n\tpytest --nbval tests/notebooks/\n\tydata_profiling -h\n\ntest_spark:\n\tpytest --spark_home=${SPARK_HOME} tests/backends/spark_backend/\n\tydata_profiling -h\n\ntest_cov:\n\tpytest --cov=. tests/unit/\n\tpytest --cov=. --cov-append tests/issues/\n\tpytest --cov=. --cov-append --nbval tests/notebooks/\n\tydata_profiling -h\n\nexamples:\n\tfind ./examples -maxdepth 2 -type f -name \"*.py\" -execdir python {} \\;\n\npackage:\n\trm -rf build dist\n\techo \"$(version)\" > VERSION\n\tpython setup.py sdist bdist_wheel\n\ttwine check dist/*\n\ninstall:\n\tpip install -e .[notebook]\n\ninstall-docs: install ### Installs regular and docs dependencies\n\tpip install -r requirements-docs.txt\n\ninstall-spark-ci:\n\tsudo apt-get update\n\tsudo apt-get -y install openjdk-8-jdk\n\tcurl https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n\t--output ${SPARK_DIRECTORY}/spark.tgz\n\tcd ${SPARK_DIRECTORY} && tar -xvzf spark.tgz && mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark\n\npublish-docs: examples ### Publishes the documentation\n\tmkdir docs/examples\n\trsync -R examples/*/*.html docs\n\tmike deploy --push --update-aliases $(version) latest\n\nlint:\n\tpre-commit run --all-files\n\nclean:\n\tgit rm --cached `git ls-files -i --exclude-from=.gitignore`\n\nall:\n\tmake lint\n\tmake install\n\tmake examples\n\tmake docs\n\tmake test\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.244140625,
          "content": "# ydata-profiling\n\n[![Build Status](https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml/badge.svg?branch=master)](https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml)\n[![PyPI download month](https://img.shields.io/pypi/dm/ydata-profiling.svg)](https://pypi.python.org/pypi/ydata-profiling/)\n[![](https://pepy.tech/badge/pandas-profiling)](https://pypi.org/project/ydata-profiling/)\n[![Code Coverage](https://codecov.io/gh/ydataai/pandas-profiling/branch/master/graph/badge.svg?token=gMptB4YUnF)](https://codecov.io/gh/ydataai/pandas-profiling)\n[![Release Version](https://img.shields.io/github/release/ydataai/pandas-profiling.svg)](https://github.com/ydataai/pandas-profiling/releases)\n[![Python Version](https://img.shields.io/pypi/pyversions/ydata-profiling)](https://pypi.org/project/ydata-profiling/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=cb7e69df-af81-4352-809a-d4251756affc\" />\n\n<p align=\"center\"><img width=\"300\" src=\"https://assets.ydata.ai/oss/ydata-profiling_black.png\" alt=\"YData Profiling Logo\"></p>\n\n<p align=\"center\">\n  <a href=\"https://ydata-profiling.ydata.ai/docs/master/\">Documentation</a>\n  |\n  <a href=\"https://tiny.ydata.ai/dcai-ydata-profiling\">Discord</a>\n  | \n  <a href=\"https://stackoverflow.com/questions/tagged/pandas-profiling+or+ydata-profiling\">Stack Overflow</a>\n  |\n  <a href=\"https://ydata-profiling.ydata.ai/docs/master/pages/reference/changelog.html#changelog\">Latest changelog</a>\n\n</p>\n\n<p align=\"center\">\n  Do you like this project? Show us your love and <a href=\"https://engage.ydata.ai\">give feedback!</a>\n</p>\n\n`ydata-profiling` primary goal is to provide a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution. Like pandas `df.describe()` function, that is so handy, ydata-profiling delivers an extended analysis of a DataFrame while allowing the data analysis to be exported in different formats such as **html** and **json**.\n\nThe package outputs a simple and digested analysis of a dataset, including **time-series** and **text**.\n\n> **Looking for a scalable solution that can fully integrate with your database systems?**<br>\n> Leverage YData Fabric Data Catalog to connect to different databases and storages (Oracle, snowflake, PostGreSQL, GCS, S3, etc.) and leverage an interactive and guided profiling experience in Fabric. Check out the [Community Version](http://ydata.ai/register?utm_source=ydata-profiling&utm_medium=documentation&utm_campaign=YData%20Fabric%20Community).\n\n## ▶️ Quickstart\n\n### Install\n```cmd\npip install ydata-profiling\n```\nor\n```cmd\nconda install -c conda-forge ydata-profiling\n```\n### Start profiling\n\nStart by loading your pandas `DataFrame` as you normally would, e.g. by using:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\ndf = pd.DataFrame(np.random.rand(100, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n```\n\nTo generate the standard profiling report, merely run:\n\n```python\nprofile = ProfileReport(df, title=\"Profiling Report\")\n```\n\n## 📊 Key features\n\n- **Type inference**: automatic detection of columns' data types (*Categorical*, *Numerical*, *Date*, etc.)\n- **Warnings**: A summary of the problems/challenges in the data that you might need to work on (*missing data*, *inaccuracies*, *skewness*, etc.)\n- **Univariate analysis**: including descriptive statistics (mean, median, mode, etc) and informative visualizations such as distribution histograms\n- **Multivariate analysis**: including correlations, a detailed analysis of missing data, duplicate rows, and visual support for variables pairwise interaction\n- **Time-Series**: including different statistical information relative to time dependent data such as auto-correlation and seasonality, along ACF and PACF plots.\n- **Text analysis**: most common categories (uppercase, lowercase, separator), scripts (Latin, Cyrillic) and blocks (ASCII, Cyrilic)\n- **File and Image analysis**: file sizes, creation dates, dimensions, indication of truncated images and existence of EXIF metadata\n- **Compare datasets**: one-line solution to enable a fast and complete report on the comparison of datasets\n- **Flexible output formats**: all analysis can be exported to an HTML report that can be easily shared with different parties, as JSON for an easy integration in automated systems and as a widget in a Jupyter Notebook.\n\nThe report contains three additional sections:\n\n- **Overview**: mostly global details about the dataset (number of records, number of variables, overall missigness and duplicates, memory footprint)\n- **Alerts**: a comprehensive and automatic list of potential data quality issues (high correlation, skewness, uniformity, zeros, missing values, constant values, between others)\n- **Reproduction**: technical details about the analysis (time, version and configuration)\n\n### 🎁 Latest features\n\n- Want to scale? Check the latest release with ⭐⚡[Spark support](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/pypspark.html)! \n- Looking for how you can do an EDA for Time-Series 🕛 ? Check [this blogpost](https://towardsdatascience.com/how-to-do-an-eda-for-time-series-cbb92b3b1913).\n- You want to compare 2 datasets and get a report? Check [this blogpost](https://medium.com/towards-artificial-intelligence/how-to-compare-2-dataset-with-pandas-profiling-2ae3a9d7695e)\n\n### ✨ Spark\n\nSpark support has been released, but we are always looking for an extra pair of hands 👐.\n[Check current work in progress!](https://github.com/ydataai/ydata-profiling/projects/3).\n\n## 📝 Use cases\nYData-profiling can be used to deliver a variety of different use-case. The documentation includes guides, tips and tricks for tackling them:\n\n| Use case | Description                                                                                 |\n|----------|---------------------------------------------------------------------------------------------|\n| [Comparing datasets](https://docs.profiling.ydata.ai/latest/features/comparing_datasets)                        | Comparing multiple version of the same dataset                                              |\n| [Profiling a Time-Series dataset](https://docs.profiling.ydata.ai/latest/features/time_series_datasets)               | Generating a report for a time-series dataset with a single line of code                    |\n|[Profiling large datasets](https://docs.profiling.ydata.ai/latest/features/big_data)                            | Tips on how to prepare data and configure `ydata-profiling` for working with large datasets |\n| [Handling sensitive data](https://docs.profiling.ydata.ai/latest/features/sensitive_data)                       | Generating reports which are mindful about sensitive data in the input dataset              |\n| [Dataset metadata and data dictionaries](https://docs.profiling.ydata.ai/latest/features/metadata)               | Complementing the report with dataset details and column-specific data dictionaries         |\n| [Customizing the report's appearance](https://docs.profiling.ydata.ai/latest/features/custom_reports) | Changing the appearance of the report's page and of the contained visualizations            |\n| [Profiling Databases](https://docs.profiling.ydata.ai/latest/features/collaborative_data_profiling) | For a seamless profiling experience in your organization's databases, check [Fabric Data Catalog](https://ydata.ai/products/data_catalog), which allows to consume data from different types of storages such as RDBMs (Azure SQL, PostGreSQL, Oracle, etc.) and object storages (Google Cloud Storage, AWS S3, Snowflake, etc.), among others. |\n### Using inside Jupyter Notebooks\n\nThere are two interfaces to consume the report inside a Jupyter notebook: through widgets and through an embedded HTML report.\n\n<img alt=\"Notebook Widgets\" src=\"https://ydata-profiling.ydata.ai/docs/master/assets/widgets.gif\" width=\"800\" />\n\nThe above is achieved by simply displaying the report as a set of widgets. In a Jupyter Notebook, run:\n\n```python\nprofile.to_widgets()\n```\n\nThe HTML report can be directly embedded in a cell in a similar fashion:\n\n```python\nprofile.to_notebook_iframe()\n```\n\n<img alt=\"HTML\" src=\"https://ydata-profiling.ydata.ai/docs/master/assets/iframe.gif\" width=\"800\" />\n\n### Exporting the report to a file\n\nTo generate a HTML report file, save the `ProfileReport` to an object and use the `to_file()` function:\n\n```python\nprofile.to_file(\"your_report.html\")\n```\n\nAlternatively, the report's data can be obtained as a JSON file:\n\n```python\n# As a JSON string\njson_data = profile.to_json()\n\n# As a file\nprofile.to_file(\"your_report.json\")\n```\n\n### Using in the command line\n\nFor standard formatted CSV files (which can be read directly by pandas without additional settings), the `ydata_profiling` executable can be used in the command line. The example below generates a report named *Example Profiling Report*, using a configuration file called `default.yaml`, in the file `report.html` by processing a `data.csv` dataset.\n\n```sh\nydata_profiling --title \"Example Profiling Report\" --config_file default.yaml data.csv report.html\n```\n\nAdditional details on the CLI are available [on the documentation](https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html#command-line-usage).\n\n## 👀 Examples\n\nThe following example reports showcase the potentialities of the package across a wide range of dataset and data types:\n\n* [Census Income](https://ydata-profiling.ydata.ai/examples/master/census/census_report.html) (US Adult Census data relating income with other demographic properties)\n* [NASA Meteorites](https://ydata-profiling.ydata.ai/examples/master/meteorites/meteorites_report.html) (comprehensive set of meteorite landing - object properties and locations) [![Open In Colab](https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/ydataai/pandas-profiling/blob/master/examples/meteorites/meteorites_cloud.ipynb) [![Binder](https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667)](https://mybinder.org/v2/gh/ydataai/pandas-profiling/master?filepath=examples%2Fmeteorites%2Fmeteorites%5Fcloud.ipynb)\n* [Titanic](https://ydata-profiling.ydata.ai/examples/master/titanic/titanic_report.html) (the \"Wonderwall\" of datasets) [![Open In Colab](https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/ydataai/pandas-profiling/blob/master/examples/titanic/titanic_cloud.ipynb) [![Binder](https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667)](https://mybinder.org/v2/gh/ydataai/pandas-profiling/master?filepath=examples%2Ftitanic%2Ftitanic%5Fcloud.ipynb)\n* [NZA](https://ydata-profiling.ydata.ai/examples/master/nza/nza_report.html) (open data from the Dutch Healthcare Authority)\n* [Stata Auto](https://ydata-profiling.ydata.ai/examples/master/stata_auto/stata_auto_report.html) (1978 Automobile data)\n* [Colors](https://ydata-profiling.ydata.ai/examples/master/colors/colors_report.html) (a simple colors dataset)\n* [Vektis](https://ydata-profiling.ydata.ai/examples/master/vektis/vektis_report.html) (Vektis Dutch Healthcare data)\n* [UCI Bank Dataset](https://ydata-profiling.ydata.ai/examples/master/bank_marketing_data/uci_bank_marketing_report.html) (marketing dataset from a bank)\n* [Russian Vocabulary](https://ydata-profiling.ydata.ai/examples/master/features/russian_vocabulary.html) (100 most common Russian words, showcasing unicode text analysis)\n* [Website Inaccessibility](https://ydata-profiling.ydata.ai/examples/master/features/website_inaccessibility_report.html) (website accessibility analysis, showcasing support for URL data)\n* [Orange prices](https://ydata-profiling.ydata.ai/examples/master/features/united_report.html) and \n* [Coal prices](https://ydata-profiling.ydata.ai/examples/master/features/flatly_report.html) (simple pricing evolution datasets, showcasing the theming options)\n* [USA Air Quality](https://github.com/ydataai/pandas-profiling/tree/master/examples/usaairquality) (Time-series air quality dataset EDA example)\n* [HCC](https://github.com/ydataai/pandas-profiling/tree/master/examples/hcc) (Open dataset from healthcare, showcasing compare between two sets of data, before and after preprocessing)\n\n## 🛠️ Installation\nAdditional details, including information about widget support, are available [on the documentation](https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/installation.html).\n\n### Using pip\n[![PyPi Downloads](https://pepy.tech/badge/ydata-profiling)](https://pepy.tech/project/ydata-profiling)\n[![PyPi Monthly Downloads](https://pepy.tech/badge/pandas-profiling/month)](https://pepy.tech/project/ydata-profiling/month)\n[![PyPi Version](https://badge.fury.io/py/ydata-profiling.svg)](https://pypi.org/project/ydata-profiling/)\n\nYou can install using the `pip` package manager by running:\n\n```sh\npip install -U ydata-profiling\n```\n\n#### Extras\n\nThe package declares \"extras\", sets of additional dependencies.\n\n* `[notebook]`: support for rendering the report in Jupyter notebook widgets.\n* `[unicode]`: support for more detailed Unicode analysis, at the expense of additional disk space.\n* `[pyspark]`: support for pyspark for big dataset analysis\n\nInstall these with e.g.\n\n```sh\npip install -U ydata-profiling[notebook,unicode,pyspark]\n```\n\n\n### Using conda\n[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas-profiling.svg)](https://anaconda.org/conda-forge/pandas-profiling)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/pandas-profiling.svg)](https://anaconda.org/conda-forge/pandas-profiling) \n\n\nYou can install using the `conda` package manager by running:\n\n```sh\nconda install -c conda-forge ydata-profiling\n```\n\n### From source (development)\n\nDownload the source code by cloning the repository or click on [Download ZIP](https://github.com/ydataai/pandas-profiling/archive/master.zip) to download the latest stable version.\n\nInstall it by navigating to the proper directory and running:\n\n```sh\npip install -e .\n```\n\nThe profiling report is written in HTML and CSS, which means a modern browser is required. \n\nYou need [Python 3](https://python3statement.github.io/) to run the package. Other dependencies can be found in the requirements files:\n\n| Filename | Requirements|\n|----------|-------------|\n| [requirements.txt](https://github.com/ydataai/pandas-profiling/blob/master/requirements.txt) | Package requirements|\n| [requirements-dev.txt](https://github.com/ydataai/pandas-profiling/blob/master/requirements-dev.txt)  |  Requirements for development|\n| [requirements-test.txt](https://github.com/ydataai/pandas-profiling/blob/master/requirements-test.txt) | Requirements for testing|\n| [setup.py](https://github.com/ydataai/pandas-profiling/blob/master/setup.py) | Requirements for widgets etc. |\n\n## 🔗 Integrations\n\nTo maximize its usefulness in real world contexts, `ydata-profiling` has a set of implicit and explicit integrations with a variety of other actors in the Data Science ecosystem: \n\n| Integration type | Description |\n|---|---|\n| [Other DataFrame libraries](https://docs.profiling.ydata.ai/latest/integrations/other_dataframe_libraries) | How to compute the profiling of data stored in libraries other than pandas |\n| [Great Expectations](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/great_expectations.html) | Generating [Great Expectations](https://greatexpectations.io) expectations suites directly from a profiling report |\n| [Interactive applications](https://docs.profiling.ydata.ai/latest/integrations/interactive_applications) | Embedding profiling reports in [Streamlit](http://streamlit.io), [Dash](http://dash.plotly.com) or [Panel](https://panel.holoviz.org) applications |\n| [Pipelines](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/pipelines.html) | Integration with DAG workflow execution tools like [Airflow](https://airflow.apache.org) or [Kedro](https://kedro.org) |\n| [Cloud services](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/cloud_services.html) | Using `ydata-profiling` in hosted computation services like [Lambda](https://lambdalabs.com), [Google Cloud](https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/blob/master/retail/propensity-model/bqml/bqml_kfp_retail_propensity_to_purchase.ipynb) or [Kaggle](https://www.kaggle.com/code) |\n| [IDEs](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/ides.html) | Using `ydata-profiling` directly from integrated development environments such as [PyCharm](https://www.jetbrains.com/pycharm/) |\n\n## 🙋 Support\nNeed help? Want to share a perspective? Report a bug? Ideas for collaborations? Reach out via the following channels:\n\n- [Stack Overflow](https://stackoverflow.com/questions/tagged/pandas-profiling+or+ydata-profiling): ideal for asking questions on how to use the package\n- [GitHub Issues](https://github.com/ydataai/ydata-profiling/issues): bugs, proposals for changes, feature requests\n- [Discord](https://tiny.ydata.ai/dcai-ydata-profiling): ideal for projects discussions, ask questions, collaborations, general chat\n\n> **Need Help?**<br>\n> Get your questions answered with a product owner by [booking a Pawsome chat](https://meetings.hubspot.com/fabiana-clemente)! 🐼\n\n> ❗ Before reporting an issue on GitHub, check out [Common Issues](https://docs.profiling.ydata.ai/latest/support-contribution/common_issues).\n\n## 🤝🏽 Contributing\nLearn how to get involved in the [Contribution Guide](https://ydata-profiling.ydata.ai/docs/master/pages/support_contrib/contribution_guidelines.html).\n\nA low-threshold place to ask questions or start contributing is the [Data Centric AI Community's Discord](https://tiny.ydata.ai/dcai-ydata-profiling).\n\n\nA big thank you to all our amazing contributors! \n\n<a href=\"https://github.com/ydataai/ydata-profiling/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=ydataai/ydata-profiling\" />\n</a>\n\nContributors wall made with [contrib.rocks](https://contrib.rocks).\n"
        },
        {
          "name": "commitlint.config.js",
          "type": "blob",
          "size": 0.2939453125,
          "content": "module.exports = {\r\n    extends: ['@commitlint/config-conventional'],\r\n    ignores: [(message) => message.includes('Merge branch') || message.includes('[skip ci]')],\r\n    rules: {\r\n        'body-max-line-length': [2, 'always', 120],\r\n        'footer-max-line-length': [2, 'always', 120],\r\n    },\r\n};\r\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "install.bat",
          "type": "blob",
          "size": 0.046875,
          "content": "@echo off\npip install -r requirements.txt\npause\n"
        },
        {
          "name": "make.bat",
          "type": "blob",
          "size": 0.955078125,
          "content": "@echo off\nsetlocal enabledelayedexpansion\n\nIF \"%1%\" == \"docs\" (\n    mkdir docs/\n    :: sphinx\n    cd docsrc/ && make github\n    ECHO \"Docs updated!\"\n    GOTO end\n)\n\nIF \"%1\" == \"test\" (\n    pytest tests/unit/\n    pytest tests/issues/\n    pytest --nbval tests/notebooks/\n    ECHO \"Tests completed!\"\n    GOTO end\n)\n\nIF \"%1\" == \"examples\" (\n    FOR /R /D %%d in (\"examples\\*\") do (\n        SET B=%%d\n        FOR /R %%f in (\"!B:%CD%\\=!\\*.py\") DO (\n            SET C=%%f\n            ECHO \"Running !C:%%d\\=! (in %%d)\"\n            CD %%d && python \"!C:%%d\\=!\"\n            CD %CD%\n        )\n    )\n\n    ECHO \"Example runs completed!\"\n    GOTO end\n)\n\nIF \"%1\" == \"lint\" (\n    pre-commit run --all-files\n    GOTO end\n)\n\nIF \"%1\" == \"install\" (\n\tpip install -e .[notebook]\n\tGOTO end\n)\n\nIF \"%1%\" == \"clean\" (\n\tECHO \"Not implemented yet\"\n\tGOTO end\n)\n\nIF \"%1%\" == \"all\" (\n    make lint\n    make install\n    make examples\n    make docs\n    make test\n    GOTO end\n)\n\nECHO \"No command matched\"\n:end\n"
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 4.5771484375,
          "content": "site_name: \"YData Profiling\"\nrepo_url: https://github.com/ydataai/ydata-profiling\nrepo_name: ydataai/ydata-profiling\ndev_addr: 0.0.0.0:1235\nsite_dir: static/docs\nnav:\n  - Welcome:\n    - 'index.md'\n  - Getting Started:\n    - Overview: 'getting-started/concepts.md'\n    - Installation: 'getting-started/installation.md'\n    - Quickstart: 'getting-started/quickstart.md'\n    - Examples: 'getting-started/examples.md'\n  - Features:\n    - Dataset metadata: 'features/metadata.md'\n    - Datasets catalog **: 'features/collaborative_data_profiling.md'\n    - Sensitive data: 'features/sensitive_data.md'\n    - Automated PII classification & management **: 'features/pii_identification_management.md'\n    - Time-series: 'features/time_series_datasets.md'\n    - Comparing datasets: 'features/comparing_datasets.md'\n    - Big data: 'features/big_data.md'\n    - Customize reports: 'features/custom_reports.md'\n    - Accessing profile values: 'features/profile_values.md'\n  - Advanced settings:\n    -  General settings: 'advanced_settings/available_settings.md'\n    -  Changing settings: 'advanced_settings/changing_settings.md'\n    -  Caching: 'advanced_settings/caching.md'\n    -  Analytics: 'advanced_settings/analytics.md'\n  - Integrations:\n    -  Other dataframes: 'integrations/other_dataframe_libraries.md'\n    -  Pyspark: 'integrations/pyspark.md'\n    -  Interactive applications: 'integrations/interactive_applications.md'\n    -  Pipelines: 'integrations/pipelines.md'\n    -  IDEs: 'integrations/ides.md'\n    -  Great Expectations: 'integrations/great_expectations.md'\n    -  Bytewax: 'integrations/bytewax.md'\n  - Support & Contributions:\n    - Contribution guidelines: 'support-contribution/contribution_guidelines.md'\n    - Supports & Troubleshoot: 'support-contribution/help_troubleshoot.md'\n    - Common issues guide: 'support-contribution/common_issues.md'\n  - Reference:\n    - 'reference/resources.md'\n    - History & community: 'reference/history.md'\n\ntheme:\n  name: material\n  language: en\n  palette:\n    - scheme: ydata\n      media: \"(prefers-color-scheme: light)\"\n      primary: custom\n  logo: 'https://assets.ydata.ai/oss/ydata-profiling_red.png'\n  features:\n    - content.code.annotate\n    - content.code.copy\n    - content.tabs.link\n    - navigation.instant\n    - navigation.tracking\n    - navigation.top\n    - navigation.footer\n    - header.autohide\n    - navigation.tabs\n    - navigation.sections\n    - navigation.indexes\n    - toc.follow\n    - toc.integrate\n  icon:\n    admonition:\n      note: octicons/tag-16\n      abstract: octicons/checklist-16\n      info: octicons/info-16\n      tip: octicons/squirrel-16\n      success: octicons/check-16\n      question: octicons/question-16\n      warning: octicons/alert-16\n      failure: octicons/x-circle-16\n      danger: octicons/zap-16\n      bug: octicons/bug-16\n      example: octicons/beaker-16\n      quote: octicons/quote-16\nmarkdown_extensions:\n  - def_list\n  - meta\n  - footnotes\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - admonition\n  - pymdownx.details\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.critic\n  - pymdownx.caret\n  - pymdownx.keys\n  - pymdownx.mark\n  - pymdownx.tilde\n  - pymdownx.betterem\n  - attr_list\n  - md_in_html\n  - pymdownx.emoji:\n      emoji_index: !!python/name:materialx.emoji.twemoji\n      emoji_generator: !!python/name:materialx.emoji.to_svg\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n  - pymdownx.tabbed:\n      alternate_style: true\n  - pymdownx.arithmatex:\n      generic: true\n  - tables\nextra_css:\n  - stylesheets/extra.css\nextra_javascript:\n  - https://cdnjs.cloudflare.com/ajax/libs/tablesort/5.2.1/tablesort.min.js\n  - https://polyfill.io/v3/polyfill.min.js?features=es6\n  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\nextra:\n  version:\n    provider: mike\n  generator: false\n  social:\n    - icon: fontawesome/solid/globe\n      link: https://www.ydata.ai\n    - icon: fontawesome/brands/linkedin\n      link: https://www.linkedin.com/company/ydataai\n\nwatch:\n  - src\nplugins:\n  - table-reader\n  - badges\n  - search:\n        lang: en\n  - autorefs\n  - mkdocstrings:\n      default_handler: python\n      handlers:\n        python:\n          selection:\n            filters: [\"!^_\", \"!^__\"]\n          import:\n          - https://docs.python.org/3/objects.inv\n          - http://pandas.pydata.org/pandas-docs/stable/objects.inv\n          setup_commands:\n            - import sys\n            - sys.path.append('../src')\n          merge_init_into_class: yes\n          show_submodules: no\n"
        },
        {
          "name": "renovate.json",
          "type": "blob",
          "size": 0.1611328125,
          "content": "{\n  \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",\n  \"extends\": [\n    \"github>ydataai/renovate-config\"\n  ],\n  \"baseBranches\": [\n    \"develop\"\n  ]\n}\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 0.193359375,
          "content": "black>=20.8b1\nisort>=5.0.7\npre-commit>=2.8.2\nvirtualenv>=20.0.33\ntwine\nwheel\nmyst-parser>=0.18.1\nsphinx_rtd_theme>=0.4.3\nsphinx-autodoc-typehints>=1.10.3\nsphinx-multiversion>=0.2.3\nautodoc_pydantic\n"
        },
        {
          "name": "requirements-docs.txt",
          "type": "blob",
          "size": 0.1923828125,
          "content": "mkdocs>=1.6.0,<1.7.0\nmkdocs-material>=9.0.12,<10.0.0\nmkdocs-material-extensions>=1.1.1,<2.0.0\nmkdocs-table-reader-plugin<=2.2.0\nmike>=2.1.1,<2.2.0\nmkdocstrings[python]>=0.20.0,<1.0.0\nmkdocs-badges\n"
        },
        {
          "name": "requirements-spark.txt",
          "type": "blob",
          "size": 0.3837890625,
          "content": "# this provides the recommended pyspark and pyarrow versions for spark to work on pandas-profiling\n# note that if you are using pyspark 2.3 or 2.4 and pyarrow >= 0.15, you might need to\n# set ARROW_PRE_0_15_IPC_FORMAT=1 in your conf/spark-env.sh for toPandas functions to work properly\npyspark>=2.3.0\npyarrow>=2.0.0\npandas>1.1, <2, !=1.4.0\nnumpy>=1.16.0,<1.24\nvisions[type_image_path]==0.7.5\n\n"
        },
        {
          "name": "requirements-test.txt",
          "type": "blob",
          "size": 0.087890625,
          "content": "pytest\ncoverage>=6.5, <8\ncodecov\npytest-cov\npytest-spark\nnbval\npyarrow\ntwine>=3.1.1\nkaggle"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.5009765625,
          "content": "scipy>=1.4.1, <1.14\npandas>1.1, <3, !=1.4.0\nmatplotlib>=3.5, <3.10\npydantic>=2\nPyYAML>=5.0.0, <6.1\njinja2>=2.11.1, <3.2\nvisions[type_image_path]>=0.7.5, <0.7.7\nnumpy>=1.16.0, <2.2\n# Could be optional\n# Related to HTML report\nhtmlmin==0.1.12\n# Correlations\nphik>=0.11.1,<0.13\n# Examples\nrequests>=2.24.0, <3\n# Progress bar\ntqdm>=4.48.2, <5\nseaborn>=0.10.1, <0.14\nmultimethod>=1.4, <2\n# metrics\nstatsmodels>=0.13.2, <1\n# type checking\ntypeguard>=3, <5\nimagehash==4.3.1\nwordcloud>=1.9.3\ndacite>=1.8\nnumba>=0.56.0, <1"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.5615234375,
          "content": "from pathlib import Path\n\nfrom setuptools import find_packages, setup\n\n# Read the contents of README file\nsource_root = Path(\".\")\nwith (source_root / \"README.md\").open(encoding=\"utf-8\") as f:\n    long_description = f.read()\n\n# Read the requirements\nwith (source_root / \"requirements.txt\").open(encoding=\"utf8\") as f:\n    requirements = f.readlines()\n\ntry:\n    version = (source_root / \"VERSION\").read_text().rstrip(\"\\n\")\nexcept FileNotFoundError:\n    version = \"0.0.dev0\"\n\nwith open(source_root / \"src/ydata_profiling/version.py\", \"w\") as version_file:\n    version_file.write(f\"__version__ = '{version}'\")\n\nsetup(\n    name=\"ydata-profiling\",\n    version=version,\n    author=\"YData Labs Inc\",\n    author_email=\"opensource@ydata.ai\",\n    packages=find_packages(\"src\"),\n    package_dir={\"\": \"src\"},\n    url=\"https://github.com/ydataai/ydata-profiling\",\n    license=\"MIT\",\n    description=\"Generate profile report for pandas DataFrame\",\n    python_requires=\">=3.7, <3.13\",\n    install_requires=requirements,\n    extras_require={\n        \"notebook\": [\n            \"jupyter>=1.0.0\",\n            \"ipywidgets>=7.5.1\",\n        ],\n        \"unicode\": [\n            \"tangled-up-in-unicode==0.2.0\",\n        ],\n    },\n    package_data={\n        \"ydata_profiling\": [\"py.typed\"],\n    },\n    include_package_data=True,\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Topic :: Software Development :: Build Tools\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Environment :: Console\",\n        \"Operating System :: OS Independent\",\n        \"Intended Audience :: Science/Research\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Financial and Insurance Industry\",\n        \"Intended Audience :: Healthcare Industry\",\n        \"Topic :: Scientific/Engineering\",\n        \"Framework :: IPython\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n    ],\n    keywords=\"pandas data-science data-analysis python jupyter ipython\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    entry_points={\n        \"console_scripts\": [\n            \"ydata_profiling = ydata_profiling.controller.console:main\",\n            \"pandas_profiling = ydata_profiling.controller.console:main\",\n        ]\n    },\n    options={\"bdist_wheel\": {\"universal\": True}},\n)\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "venv",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}