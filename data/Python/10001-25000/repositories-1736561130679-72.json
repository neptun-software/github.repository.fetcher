{
  "metadata": {
    "timestamp": 1736561130679,
    "page": 72,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "stanford-oval/storm",
      "stars": 19444,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2451171875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# mac\n.DS_Store\n\n# Other\n.vscode\n*.tsv\n*.pt\ngpt*.txt\n*.env\nlocal/\nlocal_*\nbuild/\n*.egg-info/\n.idea\n.venv\n\n# Project-specific\nsecrets.toml\n*.log\n*/assertion.log\n*results/\n.venv/"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.23828125,
          "content": "repos:\n  - repo: https://github.com/psf/black\n    rev: 24.8.0\n    hooks:\n      - id: black\n        name: Format Python code with black\n        entry: black\n        args: [\"knowledge_storm/\"]\n        language: python\n        pass_filenames: true"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.3388671875,
          "content": "# Contributing\n\nThank you for your interest in contributing to STORM! \n\nContributions aren't just about code. Currently (last edit: 7/22/2024), we are accepting the following forms of contribution:\n- Pull requests for additional language model support to `knowledge_storm/lm.py`.\n- Pull requests for additional retrieval model/search engine support to `knowledge_storm/rm.py`.\n- Pull requests for new features to `frontend/demo_light` to assist other developers.\n- Identification and reporting of issues or bugs.\n- Helping each other by responding to issues.\n\nPlease note that we are not accepting code refactoring PRs at this time to avoid conflicts with our team's efforts.\n\n## Development\nThis section contains technical instructions & hints for contributors.\n\n### Setting up\n1. Fork this repository and clone your forked repository.\n2. Install the required packages:\n    ```\n    conda create -n storm python=3.11\n    conda activate storm\n    pip install -r requirements.txt\n    ```\n3. If you want to contribute to `frontend/demo_light`, follow its [Setup guide](https://github.com/stanford-oval/storm/tree/main/frontend/demo_light#setup) to install additional packages.\n\n### PR suggestions\n\nFollowing the suggested format can lead to a faster review process.\n\n**Title:**\n\n[New LM/New RM/Demo Enhancement] xxx\n\n**Description:**\n- For new language model support, (1) describe how to use the new LM class, (2) create an example script following the style of existing example scripts under `examples/`, (3) attach an input-output example of the example script.\n- For new retrieval model/search engine support, (1) describe how to use the new RM class and (2) attach input-output examples of the RM class.\n- For demo light enhancements, (1) describe what's new and (2) attach screenshots to demonstrate the UI change.\n- Please clearly describe the required API keys and provide instructions on how to get them (if applicable). This project manages API key with `secrets.toml`.\n\n**Code Format:**\n\nWe adopt [`black`](https://github.com/psf/black) for arranging and formatting Python code. To streamline the contribution process, we set up a [pre-commit hook](https://pre-commit.com/) to format the code under `knowledge_storm/` before committing. To install the pre-commit hook, run:\n```\npip install pre-commit\npre-commit install\n```\nThe hook will automatically format the code before each commit.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0654296875,
          "content": "MIT License\n\nCopyright (c) 2024 Stanford Open Virtual Assistant Lab\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.6162109375,
          "content": "<p align=\"center\">\n  <img src=\"assets/logo.svg\" style=\"width: 25%; height: auto;\">\n</p>\n\n# STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n\n<p align=\"center\">\n| <a href=\"http://storm.genie.stanford.edu\"><b>Research preview</b></a> | <a href=\"https://arxiv.org/abs/2402.14207\"><b>STORM Paper</b></a>| <a href=\"https://www.arxiv.org/abs/2408.15232\"><b>Co-STORM Paper</b></a>  | <a href=\"https://storm-project.stanford.edu/\"><b>Website</b></a> |\n</p>\n\n**Latest News** üî•\n\n- [2024/09] Co-STORM codebase is now released and integrated into `knowledge-storm` python package v1.0.0. Run `pip install knowledge-storm --upgrade` to check it out.\n\n- [2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! [Co-STORM Paper](https://www.arxiv.org/abs/2408.15232) has been accepted to EMNLP 2024 main conference.\n\n- [2024/07] You can now install our package with `pip install knowledge-storm`!\n- [2024/07] We add `VectorRM` to support grounding on user-provided documents, complementing existing support of search engines (`YouRM`, `BingSearch`). (check out [#58](https://github.com/stanford-oval/storm/pull/58))\n- [2024/07] We release demo light for developers a minimal user interface built with streamlit framework in Python, handy for local development and demo hosting (checkout [#54](https://github.com/stanford-oval/storm/pull/54))\n- [2024/06] We will present STORM at NAACL 2024! Find us at Poster Session 2 on June 17 or check our [presentation material](assets/storm_naacl2024_slides.pdf). \n- [2024/05] We add Bing Search support in [rm.py](knowledge_storm/rm.py). Test STORM with `GPT-4o` - we now configure the article generation part in our demo using `GPT-4o` model.\n- [2024/04] We release refactored version of STORM codebase! We define [interface](knowledge_storm/interface.py) for STORM pipeline and reimplement STORM-wiki (check out [`src/storm_wiki`](knowledge_storm/storm_wiki)) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.\n\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n## Overview [(Try STORM now!)](https://storm.genie.stanford.edu/)\n\n<p align=\"center\">\n  <img src=\"assets/overview.svg\" style=\"width: 90%; height: auto;\">\n</p>\nSTORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. Co-STORM further enhanced its feature by enabling human to collaborative LLM system to support more aligned and preferred information seeking and knowledge curation.\n\nWhile the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.\n\n**More than 70,000 people have tried our [live research preview](https://storm.genie.stanford.edu/). Try it out to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!**\n\n\n\n## How STORM & Co-STORM works\n\n### STORM\n\nSTORM breaks down generating long articles with citations into two steps:\n\n1. **Pre-writing stage**: The system conducts Internet-based research to collect references and generates an outline.\n2. **Writing stage**: The system uses the outline and references to generate the full-length article with citations.\n<p align=\"center\">\n  <img src=\"assets/two_stages.jpg\" style=\"width: 60%; height: auto;\">\n</p>\n\nSTORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:\n1. **Perspective-Guided Question Asking**: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.\n2. **Simulated Conversation**: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.\n\n### CO-STORM\n\nCo-STORM proposes **a collaborative discourse protocol** which implements a turn management policy to support smooth collaboration among \n\n- **Co-STORM LLM experts**: This type of agent generates answers grounded on external knowledge sources and/or raises follow-up questions based on the discourse history.\n- **Moderator**: This agent generates thought-provoking questions inspired by information discovered by the retriever but not directly used in previous turns. Question generation can also be grounded!\n- **Human user**: The human user will take the initiative to either (1) observe the discourse to gain deeper understanding of the topic, or (2) actively engage in the conversation by injecting utterances to steer the discussion focus.\n\n<p align=\"center\">\n  <img src=\"assets/co-storm-workflow.jpg\" style=\"width: 60%; height: auto;\">\n</p>\n\nCo-STORM also maintains a dynamic updated **mind map**, which organize collected information into a hierarchical concept structure, aiming to **build a shared conceptual space between the human user and the system**. The mind map has been proven to help reduce the mental load when the discourse goes long and in-depth. \n\nBoth STORM and Co-STORM are implemented in a highly modular way using [dspy](https://github.com/stanfordnlp/dspy).\n\n## Installation\n\n\nTo install the knowledge storm library, use `pip install knowledge-storm`. \n\nYou could also install the source code which allows you to modify the behavior of STORM engine directly.\n1. Clone the git repository.\n    ```shell\n    git clone https://github.com/stanford-oval/storm.git\n    cd storm\n    ```\n   \n2. Install the required packages.\n   ```shell\n   conda create -n storm python=3.11\n   conda activate storm\n   pip install -r requirements.txt\n   ```\n   \n\n## API\n\nCurrently, our package support:\n\n- `OpenAIModel`, `AzureOpenAIModel`, `ClaudeModel`, `VLLMClient`, `TGIClient`, `TogetherClient`, `OllamaClient`, `GoogleModel`, `DeepSeekModel`, `GroqModel` as language model components\n- `YouRM`, `BingSearch`, `VectorRM`, `SerperRM`, `BraveRM`, `SearXNG`, `DuckDuckGoSearchRM`, `TavilySearchRM`, `GoogleSearch`, and `AzureAISearch` as retrieval module components\n\n:star2: **PRs for integrating more language models into [knowledge_storm/lm.py](knowledge_storm/lm.py) and search engines/retrievers into [knowledge_storm/rm.py](knowledge_storm/rm.py) are highly appreciated!**\n\nBoth STORM and Co-STORM are working in the information curation layer, you need to set up the information retrieval module and language model module to create their `Runner` classes respectively.\n\n### STORM\n\nThe STORM knowledge curation engine is defined as a simple Python `STORMWikiRunner` class. Here is an example of using You.com search engine and OpenAI models.\n\n```python\nimport os\nfrom knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs\nfrom knowledge_storm.lm import OpenAIModel\nfrom knowledge_storm.rm import YouRM\n\nlm_configs = STORMWikiLMConfigs()\nopenai_kwargs = {\n    'api_key': os.getenv(\"OPENAI_API_KEY\"),\n    'temperature': 1.0,\n    'top_p': 0.9,\n}\n# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.\n# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.\n# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.\ngpt_35 = OpenAIModel(model='gpt-3.5-turbo', max_tokens=500, **openai_kwargs)\ngpt_4 = OpenAIModel(model='gpt-4o', max_tokens=3000, **openai_kwargs)\nlm_configs.set_conv_simulator_lm(gpt_35)\nlm_configs.set_question_asker_lm(gpt_35)\nlm_configs.set_outline_gen_lm(gpt_4)\nlm_configs.set_article_gen_lm(gpt_4)\nlm_configs.set_article_polish_lm(gpt_4)\n# Check out the STORMWikiRunnerArguments class for more configurations.\nengine_args = STORMWikiRunnerArguments(...)\nrm = YouRM(ydc_api_key=os.getenv('YDC_API_KEY'), k=engine_args.search_top_k)\nrunner = STORMWikiRunner(engine_args, lm_configs, rm)\n```\n\nThe `STORMWikiRunner` instance can be evoked with the simple `run` method:\n```python\ntopic = input('Topic: ')\nrunner.run(\n    topic=topic,\n    do_research=True,\n    do_generate_outline=True,\n    do_generate_article=True,\n    do_polish_article=True,\n)\nrunner.post_run()\nrunner.summary()\n```\n- `do_research`: if True, simulate conversations with difference perspectives to collect information about the topic; otherwise, load the results.\n- `do_generate_outline`: if True, generate an outline for the topic; otherwise, load the results.\n- `do_generate_article`: if True, generate an article for the topic based on the outline and the collected information; otherwise, load the results.\n- `do_polish_article`: if True, polish the article by adding a summarization section and (optionally) removing duplicate content; otherwise, load the results.\n\n### Co-STORM\n\nThe Co-STORM knowledge curation engine is defined as a simple Python `CoStormRunner` class. Here is an example of using Bing search engine and OpenAI models.\n\n```python\nfrom knowledge_storm.collaborative_storm.engine import CollaborativeStormLMConfigs, RunnerArgument, CoStormRunner\nfrom knowledge_storm.lm import OpenAIModel\nfrom knowledge_storm.logging_wrapper import LoggingWrapper\nfrom knowledge_storm.rm import BingSearch\n\n# Co-STORM adopts the same multi LM system paradigm as STORM \nlm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()\nopenai_kwargs = {\n    \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    \"api_provider\": \"openai\",\n    \"temperature\": 1.0,\n    \"top_p\": 0.9,\n    \"api_base\": None,\n} \nquestion_answering_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)\ndiscourse_manage_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)\nutterance_polishing_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=2000, **openai_kwargs)\nwarmstart_outline_gen_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)\nquestion_asking_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=300, **openai_kwargs)\nknowledge_base_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)\n\nlm_config.set_question_answering_lm(question_answering_lm)\nlm_config.set_discourse_manage_lm(discourse_manage_lm)\nlm_config.set_utterance_polishing_lm(utterance_polishing_lm)\nlm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)\nlm_config.set_question_asking_lm(question_asking_lm)\nlm_config.set_knowledge_base_lm(knowledge_base_lm)\n\n# Check out the Co-STORM's RunnerArguments class for more configurations.\ntopic = input('Topic: ')\nrunner_argument = RunnerArgument(topic=topic, ...)\nlogging_wrapper = LoggingWrapper(lm_config)\nbing_rm = BingSearch(bing_search_api_key=os.environ.get(\"BING_SEARCH_API_KEY\"),\n                     k=runner_argument.retrieve_top_k)\ncostorm_runner = CoStormRunner(lm_config=lm_config,\n                               runner_argument=runner_argument,\n                               logging_wrapper=logging_wrapper,\n                               rm=bing_rm)\n```\n\nThe `CoStormRunner` instance can be evoked with the `warmstart()` and `step(...)` methods.\n\n```python\n# Warm start the system to build shared conceptual space between Co-STORM and users\ncostorm_runner.warm_start()\n\n# Step through the collaborative discourse \n# Run either of the code snippets below in any order, as many times as you'd like\n# To observe the conversation:\nconv_turn = costorm_runner.step()\n# To inject your utterance to actively steer the conversation:\ncostorm_runner.step(user_utterance=\"YOUR UTTERANCE HERE\")\n\n# Generate report based on the collaborative discourse\ncostorm_runner.knowledge_base.reorganize()\narticle = costorm_runner.generate_report()\nprint(article)\n```\n\n\n\n## Quick Start with Example Scripts\n\nWe provide scripts in our [examples folder](examples) as a quick start to run STORM and Co-STORM with different configurations.\n\nWe suggest using `secrets.toml` to set up the API keys. Create a file `secrets.toml` under the root directory and add the following content:\n\n```shell\n# Set up OpenAI API key.\nOPENAI_API_KEY=\"your_openai_api_key\"\n# If you are using the API service provided by OpenAI, include the following line:\nOPENAI_API_TYPE=\"openai\"\n# If you are using the API service provided by Microsoft Azure, include the following lines:\nOPENAI_API_TYPE=\"azure\"\nAZURE_API_BASE=\"your_azure_api_base_url\"\nAZURE_API_VERSION=\"your_azure_api_version\"\n# Set up You.com search API key.\nYDC_API_KEY=\"your_youcom_api_key\"\n```\n\nfor **Co-STORM**, please also add following\n```\n# if use openai encoder\nENCODER_API_TYPE=\"openai\"\n# or ENCODER_API_TYPE=\"azure\" if use azure openai encoder\n```\n\n### STORM examples\n\n**To run STORM with `gpt` family models with default configurations:**\n\nRun the following command.\n```bash\npython examples/storm_examples/run_storm_wiki_gpt.py \\\n    --output-dir $OUTPUT_DIR \\\n    --retriever you \\\n    --do-research \\\n    --do-generate-outline \\\n    --do-generate-article \\\n    --do-polish-article\n```\n\n**To run STORM using your favorite language models or grounding on your own corpus:** Check out [examples/storm_examples/README.md](examples/storm_examples/README.md).\n\n### Co-STORM examples\n\nTo run Co-STORM with `gpt` family models with default configurations,\n\n1. Add `BING_SEARCH_API_KEY=\"xxx\"` and `ENCODER_API_TYPE=\"xxx\"` to `secrets.toml`\n2. Run the following command\n\n```bash\npython examples/costorm_examples/run_costorm_gpt.py \\\n    --output-dir $OUTPUT_DIR \\\n    --retriever bing\n```\n\n\n## Customization of the Pipeline\n\n### STORM\n\nIf you have installed the source code, you can customize STORM based on your own use case. STORM engine consists of 4 modules:\n\n1. Knowledge Curation Module: Collects a broad coverage of information about the given topic.\n2. Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.\n3. Article Generation Module: Populates the generated outline with the collected information.\n4. Article Polishing Module: Refines and enhances the written article for better presentation.\n\nThe interface for each module is defined in `knowledge_storm/interface.py`, while their implementations are instantiated in `knowledge_storm/storm_wiki/modules/*`. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).\n\n### Co-STORM\n\nIf you have installed the source code, you can customize Co-STORM based on your own use case\n\n1. Co-STORM introduces multiple LLM agent types (i.e. Co-STORM experts and Moderator). LLM agent interface is defined in `knowledge_storm/interface.py` , while its implementation is instantiated in `knowledge_storm/collaborative_storm/modules/co_storm_agents.py`. Different LLM agent policies can be customized.\n2. Co-STORM introduces a collaborative discourse protocol, with its core function centered on turn policy management. We provide an example implementation of turn policy management through `DiscourseManager` in `knowledge_storm/collaborative_storm/engine.py`. It can be customized and further improved.\n\n## Datasets\nTo facilitate the study of automatic knowledge curation and complex information seeking, our project releases the following datasets:\n\n### FreshWiki\nThe FreshWiki Dataset is a collection of 100 high-quality Wikipedia articles focusing on the most-edited pages from February 2022 to September 2023. See Section 2.1 in [STORM paper](https://arxiv.org/abs/2402.14207) for more details.\n\nYou can download the dataset from [huggingface](https://huggingface.co/datasets/EchoShao8899/FreshWiki) directly. To ease the data contamination issue, we archive the [source code](https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup/FreshWiki) for the data construction pipeline that can be repeated at future dates.\n\n### WildSeek\nTo study users‚Äô interests in complex information seeking tasks in the wild, we utilized data collected from the web research preview to create the WildSeek dataset. We downsampled the data to ensure the diversity of the topics and the quality of the data. Each data point is a pair comprising a topic and the user‚Äôs goal for conducting deep search on the topic.  For more details, please refer to Section 2.2 and Appendix A of [Co-STORM paper](https://www.arxiv.org/abs/2408.15232).\n\nThe WildSeek dataset is available [here](https://huggingface.co/datasets/YuchengJiang/WildSeek).\n\n## Replicate STORM & Co-STORM paper result\n\nFor STORM paper experiments, please switch to the branch `NAACL-2024-code-backup` [here](https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup).\n\nFor Co-STORM paper experiments, please switch to the branch `EMNLP-2024-code-backup` (placeholder for now, will be updated soon).\n\n## Roadmap & Contributions\nOur team is actively working on:\n1. Human-in-the-Loop Functionalities: Supporting user participation in the knowledge curation process.\n2. Information Abstraction: Developing abstractions for curated information to support presentation formats beyond the Wikipedia-style report.\n\nIf you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!\n\nContact person: [Yijia Shao](mailto:shaoyj@stanford.edu) and [Yucheng Jiang](mailto:yuchengj@stanford.edu)\n\n## Acknowledgement\nWe would like to thank Wikipedia for its excellent open-source content. The FreshWiki dataset is sourced from Wikipedia, licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.\n\nWe are very grateful to [Michelle Lam](https://michelle123lam.github.io/) for designing the logo for this project and [Dekun Ma](https://dekun.me) for leading the UI development.\n\n## Citation\nPlease cite our paper if you use this code or part of it in your work:\n```bibtex\n@misc{jiang2024unknownunknowns,\n      title={Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations}, \n      author={Yucheng Jiang and Yijia Shao and Dekun Ma and Sina J. Semnani and Monica S. Lam},\n      year={2024},\n      eprint={2408.15232},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2408.15232}, \n}\n\n@inproceedings{shao2024assisting,\n      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, \n      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},\n      year={2024},\n      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}\n}\n```\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "frontend",
          "type": "tree",
          "content": null
        },
        {
          "name": "knowledge_storm",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1591796875,
          "content": "dspy_ai==2.4.9\nwikipedia==1.4.0\nsentence-transformers\ntoml\nlangchain-text-splitters\ntrafilatura\nlangchain-huggingface\nqdrant-client\nlangchain-qdrant\nnumpy==1.26.4\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.2373046875,
          "content": "import re\n\nfrom setuptools import setup, find_packages\n\n# Read the content of the README file\nwith open(\"README.md\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n    # Remove p tags.\n    pattern = re.compile(r\"<p.*?>.*?</p>\", re.DOTALL)\n    long_description = re.sub(pattern, \"\", long_description)\n\n# Read the content of the requirements.txt file\nwith open(\"requirements.txt\", encoding=\"utf-8\") as f:\n    requirements = f.read().splitlines()\n\n\nsetup(\n    name=\"knowledge-storm\",\n    version=\"1.0.1\",\n    author=\"Yijia Shao, Yucheng Jiang\",\n    author_email=\"shaoyj@stanford.edu, yuchengj@stanford.edu\",\n    description=\"STORM: A language model-powered knowledge curation engine.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/stanford-oval/storm\",\n    license=\"MIT License\",\n    packages=find_packages(),\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n    ],\n    python_requires=\">=3.10\",\n    install_requires=requirements,\n)\n"
        }
      ]
    }
  ]
}