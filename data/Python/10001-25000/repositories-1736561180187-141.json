{
  "metadata": {
    "timestamp": 1736561180187,
    "page": 141,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "meta-llama/codellama",
      "stars": 16142,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.0048828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.453125,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@meta.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.322265625,
          "content": "# Contributing to Code Llama\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests to address bugs or inconsistencies but\nare not planning to add new features to the existing implementation.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Meta's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nMeta has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## License\nBy contributing to Code Llama, you agree that your contributions will be\nlicensed under the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.0830078125,
          "content": "Please refer to license: https://github.com/facebookresearch/llama/blob/main/LICENSE\n"
        },
        {
          "name": "MODEL_CARD.md",
          "type": "blob",
          "size": 4.2216796875,
          "content": "# Code Llama\n\n## **Model Details**\n\n**Model Developers** Meta AI\n\n**Variations** Code Llama comes in four model sizes, and three variants:\n1) Code Llama: our base models are designed for general code synthesis and understanding\n2) Code Llama - Python: designed specifically for Python\n3) Code Llama - Instruct: for instruction following and safer deployment\n\nAll variants are available in sizes of 7B, 13B, 34B and 70B parameters.\n\n**Input** Models input text only.\n\n**Output** Models output text only.\n\n**Model Architecture** Code Llama and its variants are autoregressive language models using optimized transformer architectures. Code Llama 7B, 13B and 70B additionally support infilling text generation. All models but Code Llama - Python 70B and Code Llama - Instruct 70B were fine-tuned with up to 16K tokens, and support up to 100K tokens at inference time.\n\n**Model Dates** Code Llama and its variants have been trained between January 2023 and January 2024.\n\n**Status** This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released  as we improve model safety with community feedback.\n\n**Licence** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n\n**Research Paper** More information can be found in the paper \"[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\".\n\n**Where to send comments** Instructions on how to provide feedback or comments on the model can be found in the model [README](README.md), or by opening an issue in the GitHub repository ([https://github.com/facebookresearch/codellama/](https://github.com/facebookresearch/codellama/)).\n\n## **Intended Use**\n**Intended Use Cases** Code Llama and its variants are intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistance and generation applications.\n\n**Out-of-Scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\n\n## **Hardware and Software**\n**Training Factors**\nWe used custom training libraries. The training and fine-tuning of the released models have been performed by Meta’s Research Super Cluster.\n\n**Carbon Footprint** In aggregate, training all 12 Code Llama models required 1400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 228.55 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n**Training data**\nAll experiments reported here and the released models have been trained and fine-tuned using the same data as Llama 2 with different weights (see Section 2 and Table 1 in the [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) for details).\nCode Llama - Instruct uses additional instruction fine-tuning data.\n\n**Evaluation Results**\nSee evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.\n\n## **Ethical Considerations and Limitations**\nCode Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llama’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available available at [https://ai.meta.com/llama/responsible-user-guide](https://ai.meta.com/llama/responsible-user-guide).\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.6298828125,
          "content": "# Introducing Code Llama\n\nCode Llama is a family of large language models for code based on [Llama 2](https://github.com/facebookresearch/llama) providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model training, architecture and parameters, evaluations, responsible AI and safety refer to  our [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/). Output generated by code generation features of the Llama Materials, including Code Llama, may be subject to third party licenses, including, without limitation, open source licenses.\n\nWe are unlocking the power of large language models and our latest version of Code Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. This release includes model weights and starting code for pretrained and fine-tuned Llama language models — ranging from 7B to 34B parameters.\n\nThis repository is intended as a minimal example to load [Code Llama](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) models and run inference.\n\n\n[comment]: <> (Code Llama models are compatible with the scripts in llama-recipes)\n\n\n## Download\n\nIn order to download the model weights and tokenizers, please visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License.\n\nOnce your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, **do not use the 'Copy link address' option** when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way.\n\nPre-requisites: make sure you have `wget` and `md5sum` installed. Then to run the script: `bash download.sh`.\n\nKeep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as `403: Forbidden`, you can always re-request a link.\n\n### Model sizes\n\n| Model | Size     |\n|-------|----------|\n| 7B    | ~12.55GB |\n| 13B   | 24GB     |\n| 34B   | 63GB     |\n| 70B   | 131GB    |\n\n[comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n\n## Setup\n\nIn a conda environment with PyTorch / CUDA available, clone the repo and run in the top-level directory:\n\n```\npip install -e .\n```\n\n## Inference\n\nDifferent models require different model-parallel (MP) values:\n\n| Model | MP |\n|-------|----|\n| 7B    | 1  |\n| 13B   | 2  |\n| 34B   | 4  |\n| 70B   | 8  |\n\nAll models, except the 70B python and instruct versions, support sequence lengths up to 100,000 tokens, but we pre-allocate the cache according to `max_seq_len` and `max_batch_size` values. So set those according to your hardware and use-case.\n\n### Pretrained Code Models\n\nThe Code Llama and Code Llama - Python models are not fine-tuned to follow instructions. They should be prompted so that the expected answer is the natural continuation of the prompt.\n\n\nSee `example_completion.py` for some examples. To illustrate, see command below to run it with the `CodeLlama-7b` model (`nproc_per_node` needs to be set to the `MP` value):\n\n```\ntorchrun --nproc_per_node 1 example_completion.py \\\n    --ckpt_dir CodeLlama-7b/ \\\n    --tokenizer_path CodeLlama-7b/tokenizer.model \\\n    --max_seq_len 128 --max_batch_size 4\n```\n\nPretrained code models are: the Code Llama models `CodeLlama-7b`, `CodeLlama-13b`, `CodeLlama-34b`, `CodeLlama-70b` and the Code Llama - Python models\n`CodeLlama-7b-Python`, `CodeLlama-13b-Python`, `CodeLlama-34b-Python`, `CodeLlama-70b-Python`.\n\n### Code Infilling\n\nCode Llama and Code Llama - Instruct 7B and 13B models are capable of filling in code given the surrounding context.\n\n\nSee `example_infilling.py` for some examples. The `CodeLlama-7b` model can be run for infilling with the command below (`nproc_per_node` needs to be set to the `MP` value):\n```\ntorchrun --nproc_per_node 1 example_infilling.py \\\n    --ckpt_dir CodeLlama-7b/ \\\n    --tokenizer_path CodeLlama-7b/tokenizer.model \\\n    --max_seq_len 192 --max_batch_size 4\n```\n\nPretrained infilling models are: the Code Llama models `CodeLlama-7b` and `CodeLlama-13b` and the Code Llama - Instruct models `CodeLlama-7b-Instruct`, `CodeLlama-13b-Instruct`.\n\n### Fine-tuned Instruction Models\n\nCode Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for the 7B, 13B and 34B variants, a specific formatting defined in [`chat_completion()`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L319-L361)\nneeds to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and linebreaks in between (we recommend calling `strip()` on inputs to avoid double-spaces).\n`CodeLlama-70b-Instruct` requires a separate turn-based prompt format defined in [`dialog_prompt_tokens()`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L506-L548).\nYou can use `chat_completion()` directly to generate answers with all instruct models; it will automatically perform the required formatting.\n\nYou can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/src/llama_recipes/inference/safety_utils.py) of how to add a safety checker to the inputs and outputs of your inference code.\n\nExamples using `CodeLlama-7b-Instruct`:\n\n```\ntorchrun --nproc_per_node 1 example_instructions.py \\\n    --ckpt_dir CodeLlama-7b-Instruct/ \\\n    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 4\n```\n\nFine-tuned instruction-following models are: the Code Llama - Instruct models `CodeLlama-7b-Instruct`, `CodeLlama-13b-Instruct`, `CodeLlama-34b-Instruct`, `CodeLlama-70b-Instruct`.\n\nCode Llama is a new technology that carries potential risks with use. Testing conducted to date has not — and could not — cover all scenarios.\nIn order to help developers address these risks, we have created the [Responsible Use Guide](https://github.com/facebookresearch/llama/blob/main/Responsible-Use-Guide.pdf). More details can be found in our research papers as well.\n\n## Issues\nPlease report any software “bug”, or other problems with the models through one of the following means:\n- Reporting issues with the model: [github.com/facebookresearch/codellama](http://github.com/facebookresearch/codellama)\n- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Model Card\nSee [MODEL_CARD.md](MODEL_CARD.md) for the model card of Code Llama.\n\n## License\n\nOur model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.\n\nSee the [LICENSE](https://github.com/facebookresearch/llama/blob/main/LICENSE) file, as well as our accompanying [Acceptable Use Policy](https://github.com/facebookresearch/llama/blob/main/USE_POLICY.md)\n\n## References\n\n1. [Code Llama Research Paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\n2. [Code Llama Blog Post](https://ai.meta.com/blog/code-llama-large-language-model-coding/)\n"
        },
        {
          "name": "USE_POLICY.md",
          "type": "blob",
          "size": 0.1025390625,
          "content": "Please refer to acceptable use policy: https://github.com/facebookresearch/llama/blob/main/USE_POLICY.md\n"
        },
        {
          "name": "dev-requirements.txt",
          "type": "blob",
          "size": 0.0498046875,
          "content": "black==24.3.0\nusort==1.0.5\nufmt==2.0.1\nmypy==1.3.0\n"
        },
        {
          "name": "download.sh",
          "type": "blob",
          "size": 2.109375,
          "content": "#!/bin/bash\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nread -p \"Enter the URL from email: \" PRESIGNED_URL\necho \"\"\nALL_MODELS=\"7b,13b,34b,70b,7b-Python,13b-Python,34b-Python,70b-Python,7b-Instruct,13b-Instruct,34b-Instruct,70b-Instruct\"\nread -p \"Enter the list of models to download without spaces ($ALL_MODELS), or press Enter for all: \" MODEL_SIZE\nTARGET_FOLDER=\".\"             # where all files should end up\nmkdir -p ${TARGET_FOLDER}\n\nif [[ $MODEL_SIZE == \"\" ]]; then\n    MODEL_SIZE=$ALL_MODELS\nfi\n\necho \"Downloading LICENSE and Acceptable Usage Policy\"\nwget --continue ${PRESIGNED_URL/'*'/\"LICENSE\"} -O ${TARGET_FOLDER}\"/LICENSE\"\nwget --continue ${PRESIGNED_URL/'*'/\"USE_POLICY.md\"} -O ${TARGET_FOLDER}\"/USE_POLICY.md\"\n\nfor m in ${MODEL_SIZE//,/ }\ndo\n    case $m in\n      7b)\n        SHARD=0 ;;\n      13b)\n        SHARD=1 ;;\n      34b)\n        SHARD=3 ;;\n      70b)\n        SHARD=7 ;;\n      7b-Python)\n        SHARD=0 ;;\n      13b-Python)\n        SHARD=1 ;;\n      34b-Python)\n        SHARD=3 ;;\n      70b-Python)\n        SHARD=7 ;;\n      7b-Instruct)\n        SHARD=0 ;;\n      13b-Instruct)\n        SHARD=1 ;;\n      34b-Instruct)\n        SHARD=3 ;;\n      70b-Instruct)\n        SHARD=7 ;;\n      *)\n        echo \"Unknown model: $m\"\n        exit 1\n    esac\n\n    MODEL_PATH=\"CodeLlama-$m\"\n    echo \"Downloading ${MODEL_PATH}\"\n    mkdir -p ${TARGET_FOLDER}\"/${MODEL_PATH}\"\n\n    for s in $(seq -f \"0%g\" 0 ${SHARD})\n    do\n        wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n    done\n\n    wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/params.json\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/params.json\"\n    wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/tokenizer.model\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/tokenizer.model\"\n    wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/checklist.chk\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/checklist.chk\"\n    echo \"Checking checksums\"\n    (cd ${TARGET_FOLDER}\"/${MODEL_PATH}\" && md5sum -c checklist.chk)\ndone\n"
        },
        {
          "name": "example_completion.py",
          "type": "blob",
          "size": 1.2333984375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nfrom typing import Optional\n\nimport fire\n\nfrom llama import Llama\n\n\ndef main(\n    ckpt_dir: str,\n    tokenizer_path: str,\n    temperature: float = 0.2,\n    top_p: float = 0.9,\n    max_seq_len: int = 256,\n    max_batch_size: int = 4,\n    max_gen_len: Optional[int] = None,\n):\n    generator = Llama.build(\n        ckpt_dir=ckpt_dir,\n        tokenizer_path=tokenizer_path,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_batch_size,\n    )\n\n    prompts = [\n        # For these prompts, the expected answer is the natural continuation of the prompt\n        \"\"\"\\\ndef fizzbuzz(n: int):\"\"\",\n        \"\"\"\\\nimport argparse\n\ndef main(string: str):\n    print(string)\n    print(string[::-1])\n\nif __name__ == \"__main__\":\"\"\"\n    ]\n    results = generator.text_completion(\n        prompts,\n        max_gen_len=max_gen_len,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    for prompt, result in zip(prompts, results):\n        print(prompt)\n        print(f\"> {result['generation']}\")\n        print(\"\\n==================================\\n\")\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n\n"
        },
        {
          "name": "example_infilling.py",
          "type": "blob",
          "size": 2.052734375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nimport fire\n\nfrom llama import Llama\n\n\ndef main(\n    ckpt_dir: str,\n    tokenizer_path: str,\n    temperature: float = 0.0,\n    top_p: float = 0.9,\n    max_seq_len: int = 192,\n    max_gen_len: int = 128,\n    max_batch_size: int = 4,\n):\n    generator = Llama.build(\n        ckpt_dir=ckpt_dir,\n        tokenizer_path=tokenizer_path,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_batch_size,\n    )\n\n    prompts = [\n        '''def remove_non_ascii(s: str) -> str:\n    \"\"\" <FILL>\n    return result\n''',\n        \"\"\"# Installation instructions:\n    ```bash\n<FILL>\n    ```\nThis downloads the LLaMA inference code and installs the repository as a local pip package.\n\"\"\",\n        \"\"\"class InterfaceManagerFactory(AbstractManagerFactory):\n    def __init__(<FILL>\ndef main():\n    factory = InterfaceManagerFactory(start=datetime.now())\n    managers = []\n    for i in range(10):\n        managers.append(factory.build(id=i))\n\"\"\",\n        \"\"\"/-- A quasi-prefunctoid is 1-connected iff all its etalisations are 1-connected. -/\ntheorem connected_iff_etalisation [C D : precategoroid] (P : quasi_prefunctoid C D) :\n  π₁ P = 0 ↔ <FILL> = 0 :=\nbegin\n  split,\n  { intros h f,\n    rw pi_1_etalisation at h,\n    simp [h],\n    refl\n  },\n  { intro h,\n    have := @quasi_adjoint C D P,\n    simp [←pi_1_etalisation, this, h],\n    refl\n  }\nend\n\"\"\",\n    ]\n    prefixes = [p.split(\"<FILL>\")[0] for p in prompts]\n    suffixes = [p.split(\"<FILL>\")[1] for p in prompts]\n    results = generator.text_infilling(\n        prefixes=prefixes,\n        suffixes=suffixes,\n        max_gen_len=max_gen_len,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    for prompt, result in zip(prompts, results):\n        print(\"\\n================= Prompt text =================\\n\")\n        print(prompt)\n        print(\"\\n================= Filled text =================\\n\")\n        print(result[\"full_text\"])\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"
        },
        {
          "name": "example_instructions.py",
          "type": "blob",
          "size": 1.9248046875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nfrom typing import Optional\n\nimport fire\n\nfrom llama import Llama\n\n\ndef main(\n    ckpt_dir: str,\n    tokenizer_path: str,\n    temperature: float = 0.2,\n    top_p: float = 0.95,\n    max_seq_len: int = 512,\n    max_batch_size: int = 8,\n    max_gen_len: Optional[int] = None,\n):\n    generator = Llama.build(\n        ckpt_dir=ckpt_dir,\n        tokenizer_path=tokenizer_path,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_batch_size,\n    )\n\n    instructions = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month?\",\n            }\n        ],\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What is the difference between inorder and preorder traversal? Give an example in Python.\",\n            }\n        ],\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"Provide answers in JavaScript\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Write a function that computes the set of sums of all contiguous sublists of a given list.\",\n            }\n        ],\n    ]\n    results = generator.chat_completion(\n        instructions,  # type: ignore\n        max_gen_len=max_gen_len,\n        temperature=temperature,\n        top_p=top_p,\n    )\n\n    for instruction, result in zip(instructions, results):\n        for msg in instruction:\n            print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n        print(\n            f\"> {result['generation']['role'].capitalize()}: {result['generation']['content']}\"\n        )\n        print(\"\\n==================================\\n\")\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"
        },
        {
          "name": "llama",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0341796875,
          "content": "torch\nfairscale\nfire\nsentencepiece\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.419921875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nfrom setuptools import find_packages, setup\n\n\ndef get_requirements(path: str):\n    return [l.strip() for l in open(path)]\n\n\nsetup(\n    name=\"codellama\",\n    version=\"0.0.1\",\n    packages=find_packages(),\n    install_requires=get_requirements(\"requirements.txt\"),\n)\n"
        }
      ]
    }
  ]
}