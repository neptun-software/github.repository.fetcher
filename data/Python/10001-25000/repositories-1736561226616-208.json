{
  "metadata": {
    "timestamp": 1736561226616,
    "page": 208,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ivy-llc/ivy",
      "stars": 14020,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.12109375,
          "content": "# ivy/compiler/_cache/*.pkl filter=lfs diff=lfs merge=lfs -text\n# ivy/compiler/_cache/* filter=lfs diff=lfs merge=lfs -text\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.5625,
          "content": "*.egg-info/\n**/.idea/workspace.xml\n**/.idea/tasks.xml\n**/.idea/usage.statistics.xml\n**/.idea/shelf/\n**/.idea/markdown-navigator*\n.run\n__pycache__/\n.pytest_cache/\nbuild/\ndist/\nautogenerated_source/\nlog/\nchkpt/\nsaved_model/\nruntime_analysis/\nwith_time_logs/\n*.csv\n*.csv#\n*.ods\n*.jpg\n*.jpeg\n*.gif\n*.so\n*.pyd\n.hypothesis\n.array_api_tests_k_flag*\ninternal_automation_tools/\n.vscode/*\n.idea/*\n.ivy/*\n.DS_Store*\nfn_path_cache\nivy/engines/XLA/rust_api/target/\nivy/engines/XLA/rust_api/xla_extension/\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\nivy_dev/\n*venv/\nvenv*\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.2470703125,
          "content": "\n[submodule \"ivy_tests/array_api_testing/test_array_api\"]\n\tpath = ivy_tests/array_api_testing/test_array_api\n\turl = https://github.com/data-apis/array-api-tests.git\n[submodule \"docs/demos\"]\n\tpath = docs/demos\n\turl = https://github.com/ivy-llc/demos.git\n"
        },
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.0576171875,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n      - id: check-yaml\n      - id: trailing-whitespace\n      - id: check-toml\n      - id: end-of-file-fixer\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.3.5\n    hooks:\n      # Run the linter.\n      - id: ruff\n        args: [ --fix ]\n  - repo: https://github.com/psf/black-pre-commit-mirror\n    rev: 24.3.0\n    hooks:\n      - id: black\n        language_version: python3\n        args:\n          - \"--preview\"\n        exclude: >\n          (?x)\n          (\n            ivy/functional/frontends/(?!.*(?:config\\.py|__init__\\.py)$).* |\n            ivy_tests/test_ivy/(?!.*(?:__init__\\.py|conftest\\.py|helpers/.*|test_frontends/config/.*$)).*\n          )\n  - repo: https://github.com/PyCQA/autoflake\n    rev: v2.3.1\n    hooks:\n      - id: autoflake\n  - repo: https://github.com/PyCQA/docformatter\n    rev: v1.7.5\n    hooks:\n      - id: docformatter\n  - repo: https://github.com/ivy-llc/lint-hook\n    rev: a72ffb17562d919311653d7f593cb537d1245c19\n    hooks:\n      - id: ivy-lint\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 1.966796875,
          "content": "cff-version: 1.2.0\ntitle: >-\n  Ivy: Templated deep learning for inter-framework\n  portability\nmessage: >-\n  If you are using Ivy, we would really appreciate it if you\n  cite it in your work!\nauthors:\n  - given-names: Daniel\n    family-names: Lenton\n  - given-names: Fabio\n    family-names: Pardo\n  - given-names: Fabian\n    family-names: Falck\n  - given-names: Stephen\n    family-names: James\n  - given-names: Ronald\n    family-names: Clark\nidentifiers:\n  - type: doi\n    value: 10.48550/arXiv.2102.02886\n    description: 'arXiv preprint '\nrepository-code: 'https://github.com/ivy-llc/ivy'\nurl: 'https://unify.ai/'\nrepository: 'https://github.com/unifyai/'\nabstract: 'We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks. Ivy unifies the core functions of these frameworks to exhibit consistent call signatures, syntax and input-output behaviour. New high-level framework-agnostic functions and classes, which are usable alongside framework-specific code, can then be implemented as compositions of the unified low-level Ivy functions. Ivy currently supports TensorFlow, PyTorch, MXNet, Jax and NumPy. We also release four pure-Ivy libraries for mechanics, 3D vision, robotics, and differentiable environments. Through our evaluations, we show that Ivy can significantly reduce lines of code with a runtime overhead of less than 1% in most cases. We welcome developers to join the Ivy community by writing their own functions, layers and libraries in Ivy, maximizing their audience and helping to accelerate DL research through inter-framework codebases.'\nlicense: Apache-2.0\npreferred-citation:\n  type: article\n  authors:\n  - given-names: Daniel\n    family-names: Lenton\n  - given-names: Fabio\n    family-names: Pardo\n  - given-names: Fabian\n    family-names: Falck\n  - given-names: Stephen\n    family-names: James\n  - given-names: Ronald\n    family-names: Clark\n  doi: 10.48550/arXiv.2102.02886\n  title: \"Ivy: Templated deep learning for inter-framework portability\"\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.0068359375,
          "content": "# How to Contribute\n\nYou can pick an open issue to contribute from our [ToDo list issues](https://github.com/unifyai/ivy/issues?q=is%3Aopen+is%3Aissue+label%3AToDo), which is the placeholder of our subtasks.\n\nPlease, follow the next process when you work on your subtask:\n\n## Steps\n\n1. **Choosing a Task:**\n\n   - Choose a task to work on which:\n     - is not marked as completed with a tick.\n     - does not have an issue created.\n     - is not mentioned in the comments.\n\n   Currently, there are three open tasks:\n   - [Transpiler](https://github.com/ivy-llc/ivy/issues?q=is%3Aopen+is%3Aissue+label%3ATranspiler)\n   - [Frontend APIs](https://docs.ivy.dev/overview/contributing/open_tasks.html#frontend-apis)\n   - [Ivy Experimental API](https://docs.ivy.dev/overview/contributing/open_tasks.html#ivy-experimental-api)\n\n2. **Create Issue:**\n\n   - Create a new issue with the title being just the name of the sub-task you would like to work on.\n\n3. **Comment on the ToDo List:**\n\n   - Comment on the ToDo list issue with a reference to your new issue like so: `- [ ] #Issue_number`. For example, if your issue number is 12345, then the text of your comment should be `- [ ] #12345`. You could also use just the issue number (`#12345`), or a link to the issue itself (`https://github.com/ivy-llc/ivy/issues/12345`).\n   - At some point after your comment is made, your issue will automatically be added to the ToDo list and the comment will be deleted. No need to wait for this to happen before progressing to the next stage. Donâ€™t comment anything else on these ToDo issues.\n\n4. **Start Working:**\n\n   - When you have finished PR or need help open the PR make sure to follow our PR template.\n\n5. **Review Process:**\n   - Wait for us to review your PR. \n   - Every time you respond to our requested changes you must re-request a review in order for us to re-engage with the PR.\n   - Once the PR is in good shape, we will merge into main, and then you become an Ivy contributor!\n\n\nFor questions, please reach out on [discord](https://discord.gg/uYRmyPxMQq)!\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.4287109375,
          "content": "Copyright 2024 Transpile AI Ltd.  All rights reserved.\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   10. The software in this directory and its subdirectories is licensed \n       under the Apache License 2.0, except for the software contained \n       within the ivy/compiler directory, which is subject to the license \n       set forth in the LICENSE file located within that directory.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.2509765625,
          "content": "include requirements/requirements.txt\ninclude ivy/compiler/utils/*.so\ninclude ivy/compiler/utils/*.pyd\ninclude ivy/compiler/*.so\ninclude ivy/compiler/*.pyd\ninclude ivy/compiler/*.py\ninclude binaries.json\ninclude available_configs.json\ninclude wrappers.json\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 36.1513671875,
          "content": "<div style=\"display: block;\" align=\"center\">\r\n    <a href=\"https://ivy.dev/\">\r\n        <img class=\"dark-light\" width=\"50%\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/ivy-long.svg\"/>\r\n    </a>\r\n</div>\r\n\r\n------------------------------------------------------------------------\r\n\r\n<table align=\"center\">\r\n  <tr>\r\n    <td align=\"center\">\r\n      <a href=\"https://ivy.dev/\">\r\n          <img class=\"dark-light\" width=\"75\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/website.svg\" alt=\"Website\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://ivy.dev/\" style=\"text-decoration: none;\">Website</a>\r\n    </td>\r\n    <td align=\"center\">\r\n      <a href=\"https://docs.ivy.dev/\">\r\n          <img class=\"dark-light\" width=\"70\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/docs.svg\" alt=\"Docs\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://docs.ivy.dev/\" style=\"text-decoration: none;\">Docs</a>\r\n    </td>\r\n    <td align=\"center\">\r\n      <a href=\"https://www.docs.ivy.dev/demos/examples_and_demos.html\">\r\n          <img class=\"dark-light\" width=\"75\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/demos.svg\" alt=\"Demos\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://www.docs.ivy.dev/demos/examples_and_demos.html\" style=\"text-decoration: none;\">Demos</a>\r\n    </td>\r\n    <td align=\"center\">\r\n      <a href=\"https://docs.ivy.dev/overview/design.html\">\r\n          <img class=\"dark-light\" width=\"75\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/design.svg\" alt=\"Design\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://docs.ivy.dev/overview/design.html\" style=\"text-decoration: none;\">Design</a>\r\n    </td>\r\n    <td align=\"center\">\r\n      <a href=\"https://docs.ivy.dev/overview/faq.html\">\r\n          <img class=\"dark-light\" width=\"75\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/faq.svg\" alt=\"FAQ\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://docs.ivy.dev/overview/faq.html\" style=\"text-decoration: none;\">FAQ</a>\r\n    </td>\r\n  </tr>\r\n</table>\r\n<br>\r\n\r\n<div style=\"margin-top: 10px; margin-bottom: 10px; display: block;\" align=\"center\">\r\n    <a href=\"https://github.com/ivy-llc/ivy/issues\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/github/issues/ivy-llc/ivy\">\r\n    </a>\r\n    <a href=\"https://github.com/ivy-llc/ivy/network/members\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/github/forks/ivy-llc/ivy\">\r\n    </a>\r\n    <a href=\"https://github.com/ivy-llc/ivy/stargazers\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/github/stars/ivy-llc/ivy\">\r\n    </a>\r\n    <a href=\"https://github.com/ivy-llc/ivy/pulls\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg\">\r\n    </a>\r\n    <a href=\"https://pypi.org/project/ivy\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://badge.fury.io/py/ivy.svg\">\r\n    </a>\r\n    <a href=\"https://github.com/ivy-llc/ivy/actions?query=workflow%3Adocs\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://github.com/ivy-llc/ivy/actions/workflows/docs.yml/badge.svg\">\r\n    </a>\r\n    <a href=\"https://discord.gg/uYRmyPxMQq\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/discord/1220325004013604945?color=blue&label=%20&logo=discord&logoColor=white\">\r\n    </a>\r\n</div>\r\n<br clear=\"all\" />\r\n\r\n\r\n# Convert Machine Learning Code Between Frameworks\r\n\r\nIvy enables you to:\r\n\r\n- Convert ML models, tools and libraries between frameworks while maintaining complete functionality using `ivy.transpile`\r\n- Create optimized graph-based models and functions in any native framework (PyTorch, TensorFlow, etc..) with `ivy.trace_graph`\r\n\r\n<div style=\"display: block;\" align=\"center\">\r\n    <div>\r\n    <a href=\"https://jax.readthedocs.io\">\r\n        <img class=\"dark-light\" width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/jax.svg\">\r\n    </a>\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <a href=\"https://www.tensorflow.org\">\r\n        <img class=\"dark-light\" width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/tensorflow.svg\">\r\n    </a>\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <a href=\"https://pytorch.org\">\r\n        <img class=\"dark-light\" width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/pytorch.svg\">\r\n    </a>\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <a href=\"https://numpy.org\">\r\n        <img class=\"dark-light\" width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/numpy.svg\">\r\n    </a>\r\n    </div>\r\n</div>\r\n\r\n<br clear=\"all\" />\r\n\r\n# Installing ivy\r\n\r\nThe easiest way to set up Ivy is to install it using **pip**:\r\n\r\n``` bash\r\npip install ivy\r\n```\r\n\r\n<details>\r\n<summary><b>Docker Image</b></summary>\r\n\r\nYou can pull the Docker image for Ivy from:\r\n\r\n``` bash\r\ndocker pull ivyllc/ivy:latest\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary><b>From Source</b></summary>\r\n\r\nYou can also install Ivy from source if you want to take advantage of\r\nthe latest changes, but we can\\'t ensure everything will work as\r\nexpected ðŸ˜…\r\n\r\n``` bash\r\ngit clone https://github.com/ivy-llc/ivy.git\r\ncd ivy\r\npip install --user -e .\r\n```\r\n\r\nIf you want to set up testing and various frameworks it\\'s probably best\r\nto check out the [Setting Up](https://docs.ivy.dev/overview/contributing/setting_up.html)\r\npage, where OS-specific and IDE-specific instructions are available!\r\n\r\n</details>\r\n\r\n<br>\r\n\r\n# Supported Frameworks\r\n\r\nThese are the frameworks that `ivy.transpile` currently supports conversions from and to.\r\nWe're working hard on adding support for more frameworks, let us know on [Discord](https://discord.gg/uYRmyPxMQq) if there are source/target frameworks that would be useful for you!\r\n\r\n| Framework  | Source | Target |\r\n|------------|:------:|:------:|\r\n| PyTorch    |   âœ…   |   ðŸš§   |\r\n| TensorFlow |   ðŸš§   |   âœ…   |\r\n| JAX        |   ðŸš§   |   âœ…   |\r\n| NumPy      |   ðŸš§   |   âœ…   |\r\n\r\n<br>\r\n\r\n# Getting started\r\n\r\n- [Docs](https://docs.ivy.dev/)\r\n- [Demos](https://www.docs.ivy.dev/demos/examples_and_demos.html)\r\n- [FAQ](https://docs.ivy.dev/overview/faq.html)\r\n\r\n[Ivy's transpiler](https://docs.ivy.dev/overview/design/ivy_as_a_transpiler.html) allows you convert code between different ML frameworks. Have a look at our [Quickstart](https://docs.ivy.dev/demos/quickstart.html) notebook to get a brief idea of the features!\r\n\r\nBeyond that, based on the frameworks you want to convert code between, there are a few more [examples](#using-ivy) further down this page ðŸ‘‡ which contain a number of models and libraries transpiled between PyTorch, JAX, TensorFlow and NumPy.\r\n\r\n<br>\r\n\r\n# Using ivy\r\n\r\nHere's some examples, to help you get started using Ivy! The [examples page](https://www.docs.ivy.dev/demos/examples_and_demos.html) also features a wide range of\r\ndemos and tutorials showcasing some more use cases for Ivy.\r\n\r\n  <details>\r\n   <summary><b>Transpiling any code from one framework to another</b></summary>\r\n\r\n   ``` python\r\n   import ivy\r\n   import torch\r\n   import tensorflow as tf\r\n\r\n   def torch_fn(x):\r\n       a = torch.mul(x, x)\r\n       b = torch.mean(x)\r\n       return x * a + b\r\n\r\n   tf_fn = ivy.transpile(torch_fn, source=\"torch\", target=\"tensorflow\")\r\n\r\n   tf_x = tf.convert_to_tensor([1., 2., 3.])\r\n   ret = tf_fn(tf_x)\r\n   ```\r\n\r\n   </details>\r\n\r\n  <details>\r\n   <summary><b>Tracing a computational graph of any code</b></summary>\r\n\r\n   ``` python\r\n   import ivy\r\n   import torch\r\n\r\n   def torch_fn(x):\r\n       a = torch.mul(x, x)\r\n       b = torch.mean(x)\r\n       return x * a + b\r\n\r\n   torch_x = torch.tensor([1., 2., 3.])\r\n   graph = ivy.trace_graph(jax_fn, to=\"torch\", args=(torch_x,))\r\n   ret = graph(torch_x)\r\n   ```\r\n\r\n   </details>\r\n\r\n<!-- <details>\r\n<summary><b>I'm using PyTorch&ensp;<img class=\"dark-light\" src=\"https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/torch_small_logo.png\"></b></summary>\r\n   <blockquote>You can use Ivy to get PyTorch code from:\r\n      <details>\r\n         <summary>Any model</summary>\r\n         <blockquote>\r\n            <details>\r\n               <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport tensorflow as tf\r\n\r\n# Get a pretrained keras model\r\neff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\r\n    include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)\r\n)\r\n\r\n# Transpile it into a torch.nn.Module with the corresponding parameters\r\nnoise = tf.random.normal(shape=(1, 224, 224, 3))\r\ntorch_eff_encoder = ivy.transpile(eff_encoder, source=\"tensorflow\", to=\"torch\", args=(noise,))\r\n\r\n# Build a classifier using the transpiled encoder\r\nclass Classifier(torch.nn.Module):\r\n    def __init__(self, num_classes=20):\r\n        super().__init__()\r\n        self.encoder = torch_eff_encoder\r\n        self.fc = torch.nn.Linear(1280, num_classes)\r\n\r\n    def forward(self, x):\r\n        x = self.encoder(x)\r\n        return self.fc(x)\r\n\r\n# Initialize a trainable, customizable, torch.nn.Module\r\nclassifier = Classifier()\r\nret = classifier(torch.rand((1, 244, 244, 3)))\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nimport torch\r\n\r\n# Get a pretrained haiku model\r\n# https://github.com/unifyai/demos/blob/15c235f/scripts/deepmind_perceiver_io.py\r\nfrom deepmind_perceiver_io import key, perceiver_backbone\r\n\r\n# Transpile it into a torch.nn.Module with the corresponding parameters\r\ndummy_input = jax.random.uniform(key, shape=(1, 3, 224, 224))\r\nparams = perceiver_backbone.init(rng=key, images=dummy_input)\r\nivy.set_backend(\"jax\")\r\nbackbone = ivy.transpile(\r\n    perceiver_backbone, source=\"jax\", to=\"torch\", params_v=params, kwargs={\"images\": dummy_input}\r\n)\r\n\r\n# Build a classifier using the transpiled backbone\r\nclass PerceiverIOClassifier(torch.nn.Module):\r\n    def __init__(self, num_classes=20):\r\n        super().__init__()\r\n        self.backbone = backbone\r\n        self.max_pool = torch.nn.MaxPool2d((512, 1))\r\n        self.flatten = torch.nn.Flatten()\r\n        self.fc = torch.nn.Linear(1024, num_classes)\r\n\r\n    def forward(self, x):\r\n        x = self.backbone(images=x)\r\n        x = self.flatten(self.max_pool(x))\r\n        return self.fc(x)\r\n\r\n# Initialize a trainable, customizable, torch.nn.Module\r\nclassifier = PerceiverIOClassifier()\r\nret = classifier(torch.rand((1, 3, 224, 224)))\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any library</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From Tensorflow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport os\r\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\r\nimport segmentation_models as sm\r\n\r\n# transpile sm from tensorflow to torch\r\ntorch_sm = ivy.transpile(sm, source=\"tensorflow\", to=\"torch\")\r\n\r\n# get some image-like arrays\r\noutput = torch.rand((1, 3, 512, 512))\r\ntarget = torch.rand((1, 3, 512, 512))\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = torch_sm.metrics.iou_score(output, target)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport rax\r\nimport torch\r\n\r\n# transpile rax from jax to torch\r\ntorch_rax = ivy.transpile(rax, source=\"jax\", to=\"torch\")\r\n\r\n# get some arrays\r\nscores = torch.tensor([2.2, 1.3, 5.4])\r\nlabels = torch.tensor([1.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = torch_rax.poly1_softmax_loss(scores, labels)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport madmom\r\n\r\n# transpile madmon from numpy to torch\r\ntorch_madmom = ivy.transpile(madmom, source=\"numpy\", to=\"torch\")\r\n\r\n# get some arrays\r\nfreqs = torch.arange(20) * 10\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = torch_madmom.audio.filters.hz2midi(freqs)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any function</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From Tensorflow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport tensorflow as tf\r\nimport torch\r\n\r\ndef loss(predictions, targets):\r\n    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))\r\n\r\n# transpile any function from tf to torch\r\ntorch_loss = ivy.transpile(loss, source=\"tensorflow\", to=\"torch\")\r\n\r\n# get some arrays\r\np = torch.tensor([3.0, 2.0, 1.0])\r\nt = torch.tensor([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = torch_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax.numpy as jnp\r\nimport torch\r\n\r\ndef loss(predictions, targets):\r\n    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from jax to torch\r\ntorch_loss = ivy.transpile(loss, source=\"jax\", to=\"torch\")\r\n\r\n# get some arrays\r\np = torch.tensor([3.0, 2.0, 1.0])\r\nt = torch.tensor([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = torch_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport numpy as np\r\nimport torch\r\n\r\ndef loss(predictions, targets):\r\n    return np.sqrt(np.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from numpy to torch\r\ntorch_loss = ivy.transpile(loss, source=\"numpy\", to=\"torch\")\r\n\r\n# get some arrays\r\np = torch.tensor([3.0, 2.0, 1.0])\r\nt = torch.tensor([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = torch_loss(p, t)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary><b>I'm using TensorFlow&ensp;<img class=\"dark-light\" src=\"https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/tf_small_logo.png\"></b></summary>\r\n<blockquote>You can use Ivy to get TensorFlow code from:\r\n<details>\r\n<summary>Any model</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport timm\r\nimport tensorflow as tf\r\n\r\n# Get a pretrained pytorch model\r\nmlp_encoder = timm.create_model(\"mixer_b16_224\", pretrained=True, num_classes=0)\r\n\r\n# Transpile it into a keras.Model with the corresponding parameters\r\nnoise = torch.randn(1, 3, 224, 224)\r\nmlp_encoder = ivy.transpile(mlp_encoder, to=\"tensorflow\", args=(noise,))\r\n\r\n# Build a classifier using the transpiled encoder\r\nclass Classifier(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = mlp_encoder\r\n        self.output_dense = tf.keras.layers.Dense(units=1000, activation=\"softmax\")\r\n\r\n    def call(self, x):\r\n        x = self.encoder(x)\r\n        return self.output_dense(x)\r\n\r\n# Transform the classifier and use it as a standard keras.Model\r\nx = tf.random.normal(shape=(1, 3, 224, 224))\r\nmodel = Classifier()\r\nret = model(x)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nimport tensorflow as tf\r\n\r\n# Get a pretrained haiku model\r\n# https://ivy.dev/demos/scripts/deepmind_perceiver_io.py\r\nfrom deepmind_perceiver_io import key, perceiver_backbone\r\n\r\n# Transpile it into a tf.keras.Model with the corresponding parameters\r\ndummy_input = jax.random.uniform(key, shape=(1, 3, 224, 224))\r\nparams = perceiver_backbone.init(rng=key, images=dummy_input)\r\nbackbone = ivy.transpile(\r\n    perceiver_backbone, to=\"tensorflow\", params_v=params, args=(dummy_input,)\r\n)\r\n\r\n# Build a classifier using the transpiled backbone\r\nclass PerceiverIOClassifier(tf.keras.Model):\r\n    def __init__(self, num_classes=20):\r\n        super().__init__()\r\n        self.backbone = backbone\r\n        self.max_pool = tf.keras.layers.MaxPooling1D(pool_size=512)\r\n        self.flatten = tf.keras.layers.Flatten()\r\n        self.fc = tf.keras.layers.Dense(num_classes)\r\n\r\n    def call(self, x):\r\n        x = self.backbone(x)\r\n        x = self.flatten(self.max_pool(x))\r\n        return self.fc(x)\r\n\r\n# Initialize a trainable, customizable, tf.keras.Model\r\nx = tf.random.normal(shape=(1, 3, 224, 224))\r\nclassifier = PerceiverIOClassifier()\r\nret = classifier(x)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any library</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport kornia\r\nimport requests\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\n\r\n# transpile kornia from torch to tensorflow\r\ntf_kornia = ivy.transpile(kornia, source=\"torch\", to=\"tensorflow\")\r\n\r\n# get an image\r\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\r\nraw_img = Image.open(requests.get(url, stream=True).raw)\r\n\r\n# convert it to the format expected by kornia\r\nimg = np.array(raw_img)\r\nimg = tf.transpose(tf.constant(img), (2, 0, 1))\r\nimg = tf.expand_dims(img, 0) / 255\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = tf_kornia.enhance.sharpness(img, 5)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport rax\r\nimport tensorflow as tf\r\n\r\n# transpile rax from jax to tensorflow\r\ntf_rax = ivy.transpile(rax, source=\"jax\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\nscores = tf.constant([2.2, 1.3, 5.4])\r\nlabels = tf.constant([1.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = tf_rax.poly1_softmax_loss(scores, labels)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport madmom\r\nimport tensorflow as tf\r\n\r\n# transpile madmom from numpy to tensorflow\r\ntf_madmom = ivy.transpile(madmom, source=\"numpy\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\nfreqs = tf.range(20) * 10\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = tf_madmom.audio.filters.hz2midi(freqs)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any function</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport tensorflow as tf\r\n\r\ndef loss(predictions, targets):\r\n    return torch.sqrt(torch.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from torch to tensorflow\r\ntf_loss = ivy.transpile(loss, source=\"torch\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\np = tf.constant([3.0, 2.0, 1.0])\r\nt = tf.constant([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = tf_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax.numpy as jnp\r\nimport tensorflow as tf\r\n\r\ndef loss(predictions, targets):\r\n    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from jax to tensorflow\r\ntf_loss = ivy.transpile(loss, source=\"jax\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\np = tf.constant([3.0, 2.0, 1.0])\r\nt = tf.constant([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = tf_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef loss(predictions, targets):\r\n    return np.sqrt(np.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from numpy to tensorflow\r\ntf_loss = ivy.transpile(loss, source=\"numpy\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\np = tf.constant([3.0, 2.0, 1.0])\r\nt = tf.constant([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = tf_loss(p, t)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary><b>I'm using Jax&ensp;<img class=\"dark-light\" src=\"https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/jax_small_logo.png\"></b></summary>\r\n<blockquote>You can use Ivy to get JAX code from:\r\n<details>\r\n<summary>Any model</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport timm\r\nimport torch\r\nimport jax\r\nimport haiku as hk\r\n\r\n# Get a pretrained pytorch model\r\nmlp_encoder = timm.create_model(\"mixer_b16_224\", pretrained=True, num_classes=0)\r\n\r\n# Transpile it into a hk.Module with the corresponding parameters\r\nnoise = torch.randn(1, 3, 224, 224)\r\nmlp_encoder = ivy.transpile(mlp_encoder, source=\"torch\", to=\"haiku\", args=(noise,))\r\n\r\n# Build a classifier using the transpiled encoder\r\nclass Classifier(hk.Module):\r\n    def __init__(self, num_classes=1000):\r\n        super().__init__()\r\n        self.encoder = mlp_encoder()\r\n        self.fc = hk.Linear(output_size=num_classes, with_bias=True)\r\n\r\n    def __call__(self, x):\r\n        x = self.encoder(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\ndef _forward_classifier(x):\r\n    module = Classifier()\r\n    return module(x)\r\n\r\n# Transform the classifier and use it as a standard hk.Module\r\nrng_key = jax.random.PRNGKey(42)\r\nx = jax.random.uniform(key=rng_key, shape=(1, 3, 224, 224), dtype=jax.numpy.float32)\r\nforward_classifier = hk.transform(_forward_classifier)\r\nparams = forward_classifier.init(rng=rng_key, x=x)\r\n\r\nret = forward_classifier.apply(params, None, x)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nimport haiku as hk\r\nimport tensorflow as tf\r\njax.config.update(\"jax_enable_x64\", True)\r\n\r\n# Get a pretrained keras model\r\neff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\r\n    include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)\r\n)\r\n\r\n# Transpile it into a hk.Module with the corresponding parameters\r\nnoise = tf.random.normal(shape=(1, 224, 224, 3))\r\nhk_eff_encoder = ivy.transpile(eff_encoder, source=\"tensorflow\", to=\"haiku\", args=(noise,))\r\n\r\n# Build a classifier using the transpiled encoder\r\nclass Classifier(hk.Module):\r\n    def __init__(self, num_classes=1000):\r\n        super().__init__()\r\n        self.encoder = hk_eff_encoder()\r\n        self.fc = hk.Linear(output_size=num_classes, with_bias=True)\r\n\r\n    def __call__(self, x):\r\n        x = self.encoder(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\ndef _forward_classifier(x):\r\n    module = Classifier()\r\n    return module(x)\r\n\r\n# Transform the classifier and use it as a standard hk.Module\r\nrng_key = jax.random.PRNGKey(42)\r\ndummy_x = jax.random.uniform(key=rng_key, shape=(1, 224, 224, 3))\r\nforward_classifier = hk.transform(_forward_classifier)\r\nparams = forward_classifier.init(rng=rng_key, x=dummy_x)\r\n\r\nret = forward_classifier.apply(params, None, dummy_x)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any library</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport kornia\r\nimport requests\r\nimport jax.numpy as jnp\r\nfrom PIL import Image\r\njax.config.update(\"jax_enable_x64\", True)\r\n\r\n# transpile kornia from torch to jax\r\njax_kornia = ivy.transpile(kornia, source=\"torch\", to=\"jax\")\r\n\r\n# get an image\r\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\r\nraw_img = Image.open(requests.get(url, stream=True).raw)\r\n\r\n# convert it to the format expected by kornia\r\nimg = jnp.transpose(jnp.array(raw_img), (2, 0, 1))\r\nimg = jnp.expand_dims(img, 0) / 255\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = jax_kornia.enhance.sharpness(img, 5)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nimport os\r\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\r\nimport segmentation_models as sm\r\n\r\n# transpile sm from tensorflow to jax\r\njax_sm = ivy.transpile(sm, source=\"tensorflow\", to=\"jax\")\r\n\r\n# get some image-like arrays\r\nkey = jax.random.PRNGKey(23)\r\nkey1, key2 = jax.random.split(key)\r\noutput = jax.random.uniform(key1, (1, 3, 512, 512))\r\ntarget = jax.random.uniform(key2, (1, 3, 512, 512))\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = jax_sm.metrics.iou_score(output, target)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport madmom\r\nimport jax.numpy as jnp\r\n\r\n# transpile madmon from numpy to jax\r\njax_madmom = ivy.transpile(madmom, source=\"numpy\", to=\"jax\")\r\n\r\n# get some arrays\r\nfreqs = jnp.arange(20) * 10\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = jax_madmom.audio.filters.hz2midi(freqs)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any function</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport jax.numpy as jnp\r\n\r\ndef loss(predictions, targets):\r\n    return torch.sqrt(torch.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from torch to jax\r\njax_loss = ivy.transpile(loss, source=\"torch\", to=\"jax\")\r\n\r\n# get some arrays\r\np = jnp.array([3.0, 2.0, 1.0])\r\nt = jnp.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = jax_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport tensorflow as tf\r\nimport jax.numpy as jnp\r\n\r\ndef loss(predictions, targets):\r\n    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))\r\n\r\n# transpile any function from tf to jax\r\njax_loss = ivy.transpile(loss, source=\"tensorflow\", to=\"jax\")\r\n\r\n# get some arrays\r\np = jnp.array([3.0, 2.0, 1.0])\r\nt = jnp.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = jax_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport numpy as np\r\nimport jax\r\nimport jax.numpy as jnp\r\njax.config.update('jax_enable_x64', True)\r\n\r\ndef loss(predictions, targets):\r\n    return np.sqrt(np.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from numpy to jax\r\njax_loss = ivy.transpile(loss, source=\"numpy\", to=\"jax\")\r\n\r\n# get some arrays\r\np = jnp.array([3.0, 2.0, 1.0])\r\nt = jnp.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = jax_loss(p, t)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary><b>I'm using NumPy&ensp;<img class=\"dark-light\" src=\"https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/numpy_small_logo.png\"></b></summary>\r\n<blockquote>You can use Ivy to get NumPy code from:\r\n<details>\r\n<summary>Any library</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport kornia\r\nimport requests\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\n# transpile kornia from torch to np\r\nnp_kornia = ivy.transpile(kornia, source=\"torch\", to=\"numpy\")\r\n\r\n# get an image\r\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\r\nraw_img = Image.open(requests.get(url, stream=True).raw)\r\n\r\n# convert it to the format expected by kornia\r\nimg = np.transpose(np.array(raw_img), (2, 0, 1))\r\nimg = np.expand_dims(img, 0) / 255\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = np_kornia.enhance.sharpness(img, 5)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport numpy as np\r\nimport os\r\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\r\nimport segmentation_models as sm\r\n\r\n# transpile sm from tensorflow to numpy\r\nnp_sm = ivy.transpile(sm, source=\"tensorflow\", to=\"numpy\")\r\n\r\n# get some image-like arrays\r\noutput = np.random.rand(1, 3, 512, 512).astype(dtype=np.float32)\r\ntarget = np.random.rand(1, 3, 512, 512).astype(dtype=np.float32)\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = np_sm.metrics.iou_score(output, target)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From Jax</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport rax\r\nimport numpy as np\r\n\r\n# transpile rax from jax to numpy\r\nnp_rax = ivy.transpile(rax, source=\"jax\", to=\"numpy\")\r\n\r\n# get some arrays\r\nscores = np.array([2.2, 1.3, 5.4])\r\nlabels = np.array([1.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = np_rax.poly1_softmax_loss(scores, labels)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any function</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport numpy as np\r\n\r\ndef loss(predictions, targets):\r\n    return torch.sqrt(torch.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from torch to numpy\r\nnp_loss = ivy.transpile(loss, source=\"torch\", to=\"numpy\")\r\n\r\n# get some arrays\r\np = np.array([3.0, 2.0, 1.0])\r\nt = np.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = np_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef loss(predictions, targets):\r\n    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))\r\n\r\n# transpile any function from tf to numpy\r\nnp_loss = ivy.transpile(loss, source=\"tensorflow\", to=\"numpy\")\r\n\r\n# get some arrays\r\np = np.array([3.0, 2.0, 1.0])\r\nt = np.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = np_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax.numpy as jnp\r\nimport numpy as np\r\n\r\ndef loss(predictions, targets):\r\n    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from jax to numpy\r\nnp_loss = ivy.transpile(loss, source=\"jax\", to=\"numpy\")\r\n\r\n# get some arrays\r\np = np.array([3.0, 2.0, 1.0])\r\nt = np.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = np_loss(p, t)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n</blockquote>\r\n</details> -->\r\n\r\n<br>\r\n\r\n# How ivy works?\r\n\r\nLet's take a look at how Ivy works as a transpiler in more detail to get an idea of why and where to use it.\r\n\r\n<blockquote>\r\n<details>\r\n<summary>When is Ivy's transpiler useful?</summary>\r\n\r\nIf you want to use building blocks published in other frameworks (neural\r\nnetworks, layers, array computing libraries, training pipelines\\...),\r\nyou want to integrate code developed in various frameworks, or maybe\r\nstraight up migrate code from one framework to another or even between versions of the same framework, the transpiler is\r\ndefinitely the tool for the job! You can use the converted code just\r\nas if it was code originally developed in that framework, applying\r\nframework-specific optimizations or tools, instantly exposing your\r\nproject to all of the unique perks of a different framework.\r\n</details>\r\n</blockquote>\r\n\r\n\\\r\nIvy\\'s transpiler allows you to use code from any other framework (or\r\nfrom any other version of the same framework!) in your own code, by just\r\nadding one line of code.\r\n\r\nThis way, Ivy makes all ML-related projects available for you,\r\nindependently of the framework you want to use to research, develop, or\r\ndeploy systems. Feel free to head over to the docs for the full API\r\nreference, but the functions you\\'d most likely want to use are:\r\n\r\n``` python\r\n# Converts framework-specific code to a target framework of choice. See usage in the documentation\r\nivy.transpile()\r\n\r\n# Traces an efficient fully-functional graph from a function, removing all wrapping and redundant code. See usage in the documentation\r\nivy.trace_graph()\r\n```\r\n\r\n#### `ivy.transpile` will eagerly transpile if a class or function is provided\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport tensorflow as tf\r\n\r\ndef torch_fn(x):\r\n    x = torch.abs(x)\r\n    return torch.sum(x)\r\n\r\nx1 = torch.tensor([1., 2.])\r\nx1 = tf.convert_to_tensor([1., 2.])\r\n\r\n# Transpilation happens eagerly\r\ntf_fn = ivy.transpile(test_fn, source=\"torch\", target=\"tensorflow\")\r\n\r\n# tf_fn is now tensorflow code and runs efficiently\r\nret = tf_fn(x1)\r\n```\r\n\r\n#### `ivy.transpile` will lazily transpile if a module (library) is provided\r\n\r\n``` python\r\nimport kornia\r\n\r\nx2 = torch.rand(5, 3, 4, 4)\r\n\r\n# Module is provided -> transpilation happens lazily\r\ntf_kornia = ivy.transpile(kornia, source=\"torch\", target=\"tensorflow\")\r\n\r\n# The transpilation is initialized here, and this function is converted to tensorflwo\r\nret = tf_kornia.color.rgb_to_grayscale(x2)\r\n\r\n# Transpilation has already occurred, the tensorflow function runs efficiently\r\nret = tf_kornia.color.rgb_to_grayscale(x2)\r\n```\r\n\r\n#### `ivy.trace_graph` can be used eagerly or lazily\r\nIf you pass the necessary arguments for function tracing, the graph tracing step will\r\nhappen instantly (eagerly). Otherwise, the graph tracing\r\nwill happen only when the returned function is first invoked.\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nivy.set_backend(\"jax\")\r\n\r\n# Simple JAX function to transpile\r\ndef test_fn(x):\r\n    return jax.numpy.sum(x)\r\n\r\nx1 = ivy.array([1., 2.])\r\n```\r\n\r\n``` python\r\n# Arguments are available -> tracing happens eagerly\r\neager_graph = ivy.trace_graph(test_fn, to=\"jax\", args=(x1,))\r\n\r\n# eager_graph now runs efficiently\r\nret = eager_graph(x1)\r\n```\r\n\r\n``` python\r\n# Arguments are not available -> tracing happens lazily\r\nlazy_graph = ivy.trace_graph(test_fn, to=\"jax\")\r\n\r\n# The traced graph is initialized, tracing will happen here\r\nret = lazy_graph(x1)\r\n\r\n# Tracing has already happend, traced graph runs efficiently\r\nret = lazy_graph(x1)\r\n```\r\n\r\nIf you want to learn more, you can find more information in the [Ivy as\r\na transpiler section of the\r\ndocs!](https://docs.ivy.dev/overview/design/ivy_as_a_transpiler.html)\r\n\r\n\r\n<br>\r\n\r\n# Documentation\r\n\r\nYou can find Ivy's documentation on the [Docs page](https://docs.ivy.dev/), which includes:\r\n- [Motivation](https://docs.ivy.dev/overview/motivation.html): This contextualizes the problem Ivy is trying to solve by going over\r\n    - The current [ML Explosion](https://docs.ivy.dev/overview/motivation/ml_explosion.html#ml-explosion).\r\n    - Explaining why it is important [to solve this problem](https://www.docs.ivy.dev/overview/motivation/why_transpile.html#why-transpile).\r\n- [Related Work](https://docs.ivy.dev/overview/related_work.html): Which paints a picture of the role Ivy plays in the ML stack, comparing it to other existing solutions in terms of functionalities and abstraction level.\r\n- [Design](https://docs.ivy.dev/overview/design.html): A user-focused guide about the design decision behind the architecture and the main building blocks of Ivy.\r\n- [Deep Dive](https://docs.ivy.dev/overview/deep_dive.html): Which delves deeper into the implementation details of Ivy and is oriented towards potential contributors to the code base.\r\n\r\n\r\n<br>\r\n\r\n# Contributing\r\n\r\nWe believe that everyone can contribute and make a difference. Whether\r\nit\\'s writing code, fixing bugs, or simply sharing feedback,\r\nyour contributions are definitely welcome and appreciated ðŸ™Œ\r\n\r\nCheck out all of our [Open Tasks](https://docs.ivy.dev/overview/contributing/open_tasks.html),\r\nand find out more info in our [Contributing guide](https://docs.ivy.dev/overview/contributing.html)\r\nin the docs! Or to immediately dive into a useful task, look for any failing tests on our [Test Dashboard](https://github.com/ivy-llc/ivy-tests-dashboard/blob/main/DASHBOARD.md)!\r\n\r\n\r\n<br>\r\n\r\n# Community\r\n\r\n<a href=\"https://github.com/ivy-llc/ivy/graphs/contributors\">\r\n  <img class=\"dark-light\" src=\"https://contrib.rocks/image?repo=ivy-llc/ivy&anon=0&columns=20&max=100&r=true\" />\r\n</a>\r\n\r\n<br>\r\n<br>\r\n\r\nJoin our growing community on a mission to make conversions between frameworks simple and accessible to all!\r\nWhether you are a seasoned developer or just starting out, you\\'ll find a place here! Join the Ivy community on\r\nour [Discord](https://discord.gg/uYRmyPxMQq) ðŸ‘¾ server, which is the\r\nperfect place to ask questions, share ideas, and get help from both\r\nfellow developers and the Ivy Team directly.\r\n\r\n<b> See you there! </b>\r\n\r\n\r\n<br>\r\n\r\n# Citation\r\n\r\nIf you use Ivy for your work, please don\\'t forget to give proper credit\r\nby including the accompanying [paper](https://arxiv.org/abs/2102.02886)\r\nðŸ“„ in your references. It\\'s a small way to show appreciation and help\r\nto continue to support this and other open source projects ðŸ™Œ\r\n\r\n\r\n    @article{lenton2021ivy,\r\n      title={Ivy: Templated deep learning for inter-framework portability},\r\n      author={Lenton, Daniel and Pardo, Fabio and Falck, Fabian and James, Stephen and Clark, Ronald},\r\n      journal={arXiv preprint arXiv:2102.02886},\r\n      year={2021}\r\n    }\r\n"
        },
        {
          "name": "automation_tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "available_configs.json",
          "type": "blob",
          "size": 0.6357421875,
          "content": "{\n    \"compiler\": [\n        \"cp38-cp38-manylinux_2_17_x86_64\",\n        \"cp38-cp38-win_amd64\",\n        \"cp39-cp39-manylinux_2_17_x86_64\",\n        \"cp39-cp39-win_amd64\",\n        \"cp310-cp310-manylinux_2_17_x86_64\",\n        \"cp310-cp310-win_amd64\",\n        \"cp310-cp310-macosx_12_0_arm64\",\n        \"cp311-cp311-manylinux_2_17_x86_64\",\n        \"cp311-cp311-win_amd64\",\n        \"cp311-cp311-macosx_12_0_arm64\",\n        \"cp312-cp312-manylinux_2_17_x86_64\",\n        \"cp312-cp312-win_amd64\",\n        \"cp312-cp312-macosx_12_0_arm64\",\n        \"cp313-cp313-manylinux_2_17_x86_64\",\n        \"cp313-cp313-win_amd64\",\n        \"cp313-cp313-macosx_12_0_arm64\"\n    ]\n}\n"
        },
        {
          "name": "badges",
          "type": "tree",
          "content": null
        },
        {
          "name": "binaries.json",
          "type": "blob",
          "size": 4.04296875,
          "content": "{\n    \"ivy\": {\n        \"compiler\": [\n            \"_compiler\",\n            {\n                \"utils\": [\n                    \"C\",\n                    \"CD\",\n                    \"CI\",\n                    \"CL\",\n                    \"CV\",\n                    \"CX\",\n                    \"D\",\n                    \"DC\",\n                    \"DD\",\n                    \"DI\",\n                    \"DL\",\n                    \"DM\",\n                    \"DV\",\n                    \"DX\",\n                    \"I\",\n                    \"IC\",\n                    \"ICC\",\n                    \"ICD\",\n                    \"ICI\",\n                    \"ICL\",\n                    \"ICM\",\n                    \"ICV\",\n                    \"ICX\",\n                    \"ID\",\n                    \"IDC\",\n                    \"IDD\",\n                    \"IDI\",\n                    \"IDL\",\n                    \"IDM\",\n                    \"IDV\",\n                    \"IDX\",\n                    \"II\",\n                    \"IIC\",\n                    \"IID\",\n                    \"III\",\n                    \"IIL\",\n                    \"IIM\",\n                    \"IIX\",\n                    \"IL\",\n                    \"ILC\",\n                    \"ILD\",\n                    \"ILI\",\n                    \"ILL\",\n                    \"ILM\",\n                    \"ILV\",\n                    \"ILX\",\n                    \"IM\",\n                    \"IMC\",\n                    \"IMD\",\n                    \"IMI\",\n                    \"IML\",\n                    \"IMM\",\n                    \"IMV\",\n                    \"IMX\",\n                    \"IV\",\n                    \"IVD\",\n                    \"IVI\",\n                    \"IVL\",\n                    \"IVM\",\n                    \"IVV\",\n                    \"IVX\",\n                    \"IX\",\n                    \"IXC\",\n                    \"IXD\",\n                    \"IXI\",\n                    \"IXL\",\n                    \"IXM\",\n                    \"IXV\",\n                    \"IXX\",\n                    \"L\",\n                    \"LC\",\n                    \"LD\",\n                    \"LI\",\n                    \"LL\",\n                    \"LM\",\n                    \"LV\",\n                    \"LX\",\n                    \"M\",\n                    \"MC\",\n                    \"MD\",\n                    \"MI\",\n                    \"ML\",\n                    \"MM\",\n                    \"MV\",\n                    \"MX\",\n                    \"V\",\n                    \"VC\",\n                    \"VCC\",\n                    \"VCD\",\n                    \"VCI\",\n                    \"VCL\",\n                    \"VCM\",\n                    \"VCV\",\n                    \"VCX\",\n                    \"VD\",\n                    \"VDC\",\n                    \"VDD\",\n                    \"VDI\",\n                    \"VDL\",\n                    \"VDM\",\n                    \"VDV\",\n                    \"VDX\",\n                    \"VI\",\n                    \"VIC\",\n                    \"VID\",\n                    \"VII\",\n                    \"VIL\",\n                    \"VIM\",\n                    \"VIV\",\n                    \"VIX\",\n                    \"VL\",\n                    \"VLC\",\n                    \"VLD\",\n                    \"VLI\",\n                    \"VLL\",\n                    \"VLM\",\n                    \"VLV\",\n                    \"VLX\",\n                    \"VM\",\n                    \"VMC\",\n                    \"VMD\",\n                    \"VMI\",\n                    \"VML\",\n                    \"VMM\",\n                    \"VMV\",\n                    \"VMX\",\n                    \"VV\",\n                    \"VVC\",\n                    \"VVD\",\n                    \"VVI\",\n                    \"VVL\",\n                    \"VVM\",\n                    \"VVV\",\n                    \"VVX\",\n                    \"VX\",\n                    \"VXC\",\n                    \"VXD\",\n                    \"VXI\",\n                    \"VXL\",\n                    \"VXM\",\n                    \"VXV\",\n                    \"VXX\",\n                    \"X\",\n                    \"XC\",\n                    \"XD\",\n                    \"XI\",\n                    \"XL\",\n                    \"XM\",\n                    \"XV\",\n                    \"XX\"\n                ]\n            }\n        ]\n    }\n}"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "install_dependencies.sh",
          "type": "blob",
          "size": 0.451171875,
          "content": "# This shell script is required by the doc-builder. Moving it might break\n# the doc-building pipeline\npip install -e .\npip install -r requirements/requirements.txt\nif [[ $(arch) == 'arm64' ]]; then\n      brew install pandoc\n      pip install -r requirements/optional_apple_silicon_1.txt\n      pip install -r requirements/optional_apple_silicon_2.txt\nelse\n    sudo apt-get update\n    sudo apt-get install pandoc -y\n    pip install -r requirements/optional.txt\nfi\n"
        },
        {
          "name": "ivy",
          "type": "tree",
          "content": null
        },
        {
          "name": "ivy_tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.7412109375,
          "content": "[build-system]\nrequires = [\n    \"setuptools>=42\",\n    \"wheel\",\n    \"pip\"\n]\nbuild-backend = \"setuptools.build_meta\"\n\n\n[tool.docformatter]\nwrap-summaries = 88\npre-summary-newline = true\n\n\n[tool.autoflake]\nin-place = true\nremove-all-unused-imports = true\nignore-init-module-imports = true\nremove-duplicate-keys = true\nremove-unused-variables = true\nquiet = true\nignore-pass-after-docstring = true\nexclude = [\"__init__.py\"]\n\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py38\"\n\n[tool.ruff.lint]\nselect = [\n    # pyflakes\n    \"F\",\n    # pycodestyle\n    \"E\", \"W\",\n    # pydocstyle\n    \"D\",\n    \"I002\",     # Missing required import.\n    \"UP008\",    # Checks for super calls that pass redundant arguments.\n    \"G010\",     # deprecated-log-warn.\n    \"PLR1722\",  # Use sys.exit() instead of exit() and quit().\n    \"TRY004\",   # Prefer TypeError exception for invalid type.\n    \"PT014\",    # pytest-duplicate-parametrize-test-cases.\n    \"PT006\",    # Checks for the type of parameter names passed to pytest.mark.parametrize.\n    \"PT007\",    # Checks for the type of parameter values passed to pytest.mark.parametrize.\n    \"PT018\",    # Checks for assertions that combine multiple independent conditions.\n]\n\n\nignore = [\n    \"E203\",\t# Whitespace-before-punctuation.\n    \"E402\", # Module-import-not-at-top-of-file.\n    \"E731\", # Do not assign a lambda expression, use a def.\n    \"D100\", # Missing docstring in public module.\n    \"D101\", # Missing docstring in public class.\n    \"D102\", # Missing docstring in public method.\n    \"D103\", # Missing docstring in public function.\n    \"D104\", # Missing docstring in public package.\n    \"D105\", # Missing docstring in magic method.\n    \"D106\", # Missing docstring in public nested class.\n    \"D107\", # Missing docstring in `__init__`.\n    \"D203\", # 1 blank line required before class docstring.\n    \"D205\", # 1 blank line required between summary line and description.\n    \"D212\", # Multi-line docstring summary should start at the first line.\n    \"D213\", # Multi-line docstring summary should start at the second line.\n    \"D209\", # Multi-line docstring closing quotes should be on a separate line.\n    \"D400\", # First line should end with a period.\n    \"D413\", # Missing blank line after last section of docstrings.\n    \"D401\", # First line of docstring should be in imperative mood.\n    \"D415\", # First line should end with a period, question mark, or exclamation point.\n    \"D416\", # Section name should end with a colon (\"Attributes\").\n    \"D417\", # Missing argument description in the docstring for argument \"X\".\n]\n\n[tool.ruff.lint.per-file-ignores]\n'ivy/functional/(frontends|backends)/(?!.*/func_wrapper\\.py$).*(?!__init__\\.py$)' = [\"D\"]\n\"**/__init__.py\" = [\"F401\",\"F403\",\"F405\",\"F811\",\"F821\", \"E501\"]\n\"ivy/functional/frontends/paddle/**\" = [\"F401\", \"F403\", \"F405\"]\n"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.35546875,
          "content": "# lint as: python3\n# Copyright 2021 The Ivy Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License..\n# ==============================================================================\n__version__ = None\n\nimport setuptools\nfrom setuptools import setup\nfrom pathlib import Path\nfrom urllib import request\nimport os\nimport json\nimport re\n\n\ndef _get_paths_from_binaries(binaries, root_dir=\"\"):\n    \"\"\"Get all the paths from the binaries.json into a list.\"\"\"\n    paths = []\n    ext = \"pyd\" if os.name == \"nt\" else \"so\"\n    if isinstance(binaries, str):\n        return [os.path.join(root_dir, binaries + \".\" + ext)]\n    elif isinstance(binaries, dict):\n        for k, v in binaries.items():\n            paths += _get_paths_from_binaries(v, os.path.join(root_dir, k))\n    else:\n        for i in binaries:\n            paths += _get_paths_from_binaries(i, root_dir)\n    return paths\n\n\ndef _strip(line):\n    return line.split(\" \")[0].split(\"#\")[0].split(\",\")[0]\n\n\n# Download all relevant binaries in binaries.json\nbinaries_dict = json.load(open(\"binaries.json\"))\navailable_configs = json.load(open(\"available_configs.json\"))\nbinaries_paths = _get_paths_from_binaries(binaries_dict)\nversion = os.environ.get(\"VERSION\", \"main\")\nfixed_tag = os.environ.get(\"TAG\", None)\nclean = os.environ.get(\"CLEAN\", None)\nterminate = False\nall_tags, python_tag, plat_name, options = None, None, None, None\nif fixed_tag:\n    python_tag, _, plat_name = str(fixed_tag).split(\"-\")\n    options = {\"bdist_wheel\": {\"python_tag\": python_tag, \"plat_name\": plat_name}}\n    all_tags = [fixed_tag]\nelse:\n    from pip._vendor.packaging import tags\n\n    all_tags = list(tags.sys_tags())\n\n# download binaries for the tag with highest precedence\nfor tag in all_tags:\n    if terminate:\n        break\n    for path in binaries_paths:\n        module = path.split(os.sep)[1]\n        if (os.path.exists(path) and not clean) or str(tag) not in available_configs[\n            module\n        ]:\n            continue\n        folders = path.split(os.sep)\n        folder_path, file_path = os.sep.join(folders[:-1]), folders[-1]\n        ext = \"pyd\" if os.name == \"nt\" else \"so\"\n        file_name = f\"{file_path[:-(len(ext)+1)]}_{tag}.{ext}\"\n        search_path = f\"{module}/{file_name}\"\n        try:\n            response = request.urlopen(\n                f\"https://github.com/ivy-llc/binaries/raw/{version}/{search_path}\",\n                timeout=40,\n            )\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, \"wb\") as f:\n                f.write(response.read())\n            terminate = path == binaries_paths[-1]\n        except request.HTTPError:\n            break\n\n\nthis_directory = Path(__file__).parent\nlong_description = (this_directory / \"README.md\").read_text(encoding=\"utf-8\")\n\n# Remove img tags that have class \"only-dark\"\nlong_description = re.sub(\n    r\"<img [^>]*class=\\\"only-dark\\\"[^>]*>\",\n    \"\",\n    long_description,\n    flags=re.MULTILINE,\n)\n\n# Remove a tags that have class \"only-dark\"\nlong_description = re.sub(\n    r\"<a [^>]*class=\\\"only-dark\\\"[^>]*>((?:(?!<\\/a>).)|\\s)*<\\/a>\\n\",\n    \"\",\n    long_description,\n    flags=re.MULTILINE,\n)\n\n# Apply version\nwith open(\"ivy/_version.py\") as f:\n    exec(f.read(), __version__)\n\nsetup(\n    name=\"ivy\",\n    version=__version__,\n    author=\"Transpile AI\",\n    author_email=\"contact@ivy.dev\",\n    description=\"Convert Machine Learning Code Between Frameworks\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://ivy.dev\",\n    project_urls={\n        \"Docs\": \"https://docs.ivy.dev/\",\n        \"Source\": \"https://github.com/ivy-llc/ivy\",\n    },\n    include_package_data=True,\n    packages=setuptools.find_packages(),\n    install_requires=[\n        _strip(line)\n        for line in open(\"requirements/requirements.txt\", \"r\", encoding=\"utf-8\")\n    ],\n    classifiers=[],\n    license=\"End-User License Agreement for Ivy\",\n    options=options,\n)\n"
        },
        {
          "name": "wrappers.json",
          "type": "blob",
          "size": 4.4482421875,
          "content": "{\n    \"ivy\": {\n        \"functional\": [\"negative.so\",\n        \"bitwise_xor.so\",\n        \"vander.so\",\n        \"std.so\",\n        \"atanh.so\",\n        \"argmin.so\",\n        \"asinh.so\",\n        \"squeeze.so\",\n        \"square.so\",\n        \"matrix_norm.so\",\n        \"not_equal.so\",\n        \"log.so\",\n        \"expand_dims.so\",\n        \"divide.so\",\n        \"min.so\",\n        \"unique_counts.so\",\n        \"vector_norm.so\",\n        \"matrix_rank.so\",\n        \"equal.so\",\n        \"expm1.so\",\n        \"sigmoid.so\",\n        \"adam_update.so\",\n        \"cumsum.so\",\n        \"lars_update.so\",\n        \"isinf.so\",\n        \"pinv.so\",\n        \"deg2rad.so\",\n        \"var.so\",\n        \"pow.so\",\n        \"random_uniform.so\",\n        \"trapz.so\",\n        \"adam_step.so\",\n        \"tile.so\",\n        \"tan.so\",\n        \"sparse_cross_entropy.so\",\n        \"det.so\",\n        \"round.so\",\n        \"acos.so\",\n        \"matrix_power.so\",\n        \"while_loop.so\",\n        \"cross.so\",\n        \"trunc.so\",\n        \"jac.so\",\n        \"sqrt.so\",\n        \"bitwise_left_shift.so\",\n        \"atan.so\",\n        \"clip.so\",\n        \"conv2d_transpose.so\",\n        \"exp2.so\",\n        \"less.so\",\n        \"conv2d.so\",\n        \"einsum.so\",\n        \"searchsorted.so\",\n        \"floor.so\",\n        \"cross_entropy.so\",\n        \"seed.so\",\n        \"scaled_dot_product_attention.so\",\n        \"bitwise_and.so\",\n        \"logaddexp2.so\",\n        \"optimizer_update.so\",\n        \"mish.so\",\n        \"mean.so\",\n        \"argsort.so\",\n        \"eigh.so\",\n        \"svd.so\",\n        \"cumprod.so\",\n        \"eigvalsh.so\",\n        \"asin.so\",\n        \"random_normal.so\",\n        \"try_except.so\",\n        \"split.so\",\n        \"log_softmax.so\",\n        \"nan_to_num.so\",\n        \"cmp_isnot.so\",\n        \"matrix_transpose.so\",\n        \"diag.so\",\n        \"remainder.so\",\n        \"sinh.so\",\n        \"bitwise_or.so\",\n        \"softplus.so\",\n        \"flip.so\",\n        \"conv_general_transpose.so\",\n        \"shuffle.so\",\n        \"roi_align.so\",\n        \"log1p.so\",\n        \"tensordot.so\",\n        \"zero_pad.so\",\n        \"logical_xor.so\",\n        \"inv.so\",\n        \"softmax.so\",\n        \"greater.so\",\n        \"logical_not.so\",\n        \"conv1d.so\",\n        \"vecdot.so\",\n        \"multi_head_attention.so\",\n        \"diagonal.so\",\n        \"isnan.so\",\n        \"inner.so\",\n        \"bitwise_invert.so\",\n        \"slogdet.so\",\n        \"tensorsolve.so\",\n        \"value_and_grad.so\",\n        \"depthwise_conv2d.so\",\n        \"trunc_divide.so\",\n        \"erf.so\",\n        \"svdvals.so\",\n        \"reshape.so\",\n        \"constant_pad.so\",\n        \"unique_all.so\",\n        \"qr.so\",\n        \"isfinite.so\",\n        \"logical_and.so\",\n        \"if_else.so\",\n        \"nonzero.so\",\n        \"tanh.so\",\n        \"conv.so\",\n        \"add.so\",\n        \"subtract.so\",\n        \"argmax.so\",\n        \"maximum.so\",\n        \"real.so\",\n        \"msort.so\",\n        \"fmin.so\",\n        \"abs.so\",\n        \"lstm_update.so\",\n        \"permute_dims.so\",\n        \"lamb_update.so\",\n        \"swapaxes.so\",\n        \"cosh.so\",\n        \"log10.so\",\n        \"bitwise_right_shift.so\",\n        \"for_loop.so\",\n        \"imag.so\",\n        \"dropout.so\",\n        \"where.so\",\n        \"roll.so\",\n        \"leaky_relu.so\",\n        \"fmod.so\",\n        \"randint.so\",\n        \"logical_or.so\",\n        \"relu.so\",\n        \"binary_cross_entropy.so\",\n        \"unique_values.so\",\n        \"linear.so\",\n        \"sin.so\",\n        \"vector_to_skew_symmetric_matrix.so\",\n        \"closest_valid_dtype.so\",\n        \"atan2.so\",\n        \"stack.so\",\n        \"max.so\",\n        \"sign.so\",\n        \"exp.so\",\n        \"cholesky.so\",\n        \"ceil.so\",\n        \"cmp_is.so\",\n        \"repeat.so\",\n        \"gelu.so\",\n        \"reciprocal.so\",\n        \"unstack.so\",\n        \"conv1d_transpose.so\",\n        \"less_equal.so\",\n        \"stop_gradient.so\",\n        \"angle.so\",\n        \"matmul.so\",\n        \"cos.so\",\n        \"execute_with_gradients.so\",\n        \"gradient_descent_update.so\",\n        \"softsign.so\",\n        \"unique_inverse.so\",\n        \"solve.so\",\n        \"sum.so\",\n        \"argwhere.so\",\n        \"greater_equal.so\",\n        \"outer.so\",\n        \"rad2deg.so\",\n        \"floor_divide.so\",\n        \"conv_general_dilated.so\",\n        \"logaddexp.so\",\n        \"concat.so\",\n        \"positive.so\",\n        \"minimum.so\",\n        \"log2.so\",\n        \"lcm.so\",\n        \"acosh.so\",\n        \"conv3d_transpose.so\",\n        \"multinomial.so\",\n        \"lu_factor.so\",\n        \"layer_norm.so\",\n        \"eig.so\",\n        \"conv3d.so\",\n        \"sort.so\",\n        \"isreal.so\",\n        \"multiply.so\",\n        \"gcd.so\",\n        \"grad.so\",\n        \"prod.so\"]\n    }\n}\n"
        }
      ]
    }
  ]
}