{
  "metadata": {
    "timestamp": 1736561246178,
    "page": 234,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "KwaiVGI/LivePortrait",
      "stars": 13579,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.6474609375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n**/__pycache__/\n*.py[cod]\n**/*.py[cod]\n*$py.class\n\n# Model weights\n**/*.pth\n**/*.onnx\n\npretrained_weights/*.md\npretrained_weights/docs\npretrained_weights/liveportrait\npretrained_weights/liveportrait_animals\n\n# Ipython notebook\n*.ipynb\n\n# Temporary files or benchmark resources\nanimations/*\ntmp/*\n.vscode/launch.json\n**/*.DS_Store\ngradio_temp/**\n\n# Windows dependencies\nffmpeg/\nLivePortrait_env/\n\n# XPose build files\nsrc/utils/dependencies/XPose/models/UniPose/ops/build\nsrc/utils/dependencies/XPose/models/UniPose/ops/dist\nsrc/utils/dependencies/XPose/models/UniPose/ops/MultiScaleDeformableAttention.egg-info\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.3837890625,
          "content": "MIT License\n\nCopyright (c) 2024 Kuaishou Visual Generation and Interaction Center\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n---\n\nThe code of InsightFace is released under the MIT License.\nThe models of InsightFace are for non-commercial research purposes only.\n\nIf you want to use the LivePortrait project for commercial purposes, you \nshould remove and replace InsightFace‚Äôs detection models to fully comply with \nthe MIT license.\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 24.9580078125,
          "content": "# coding: utf-8\n\n\"\"\"\nThe entrance of the gradio for human\n\"\"\"\n\nimport os\nimport tyro\nimport subprocess\nimport gradio as gr\nimport os.path as osp\nfrom src.utils.helper import load_description\nfrom src.gradio_pipeline import GradioPipeline\nfrom src.config.crop_config import CropConfig\nfrom src.config.argument_config import ArgumentConfig\nfrom src.config.inference_config import InferenceConfig\n\n\ndef partial_fields(target_class, kwargs):\n    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n\n\ndef fast_check_ffmpeg():\n    try:\n        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n        return True\n    except:\n        return False\n\n\n# set tyro theme\ntyro.extras.set_accent_color(\"bright_cyan\")\nargs = tyro.cli(ArgumentConfig)\n\nffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\nif osp.exists(ffmpeg_dir):\n    os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n\nif not fast_check_ffmpeg():\n    raise ImportError(\n        \"FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html\"\n    )\n# specify configs for inference\ninference_cfg = partial_fields(InferenceConfig, args.__dict__)  # use attribute of args to initial InferenceConfig\ncrop_cfg = partial_fields(CropConfig, args.__dict__)  # use attribute of args to initial CropConfig\n# global_tab_selection = None\n\ngradio_pipeline = GradioPipeline(\n    inference_cfg=inference_cfg,\n    crop_cfg=crop_cfg,\n    args=args\n)\n\nif args.gradio_temp_dir not in (None, ''):\n    os.environ[\"GRADIO_TEMP_DIR\"] = args.gradio_temp_dir\n    os.makedirs(args.gradio_temp_dir, exist_ok=True)\n\n\ndef gpu_wrapped_execute_video(*args, **kwargs):\n    return gradio_pipeline.execute_video(*args, **kwargs)\n\n\ndef gpu_wrapped_execute_image_retargeting(*args, **kwargs):\n    return gradio_pipeline.execute_image_retargeting(*args, **kwargs)\n\n\ndef gpu_wrapped_execute_video_retargeting(*args, **kwargs):\n    return gradio_pipeline.execute_video_retargeting(*args, **kwargs)\n\n\ndef reset_sliders(*args, **kwargs):\n    return 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.5, True, True\n\n\n# assets\ntitle_md = \"assets/gradio/gradio_title.md\"\nexample_portrait_dir = \"assets/examples/source\"\nexample_video_dir = \"assets/examples/driving\"\ndata_examples_i2v = [\n    [osp.join(example_portrait_dir, \"s9.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n    [osp.join(example_portrait_dir, \"s6.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n    [osp.join(example_portrait_dir, \"s10.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n    [osp.join(example_portrait_dir, \"s5.jpg\"), osp.join(example_video_dir, \"d18.mp4\"), True, True, True, False],\n    [osp.join(example_portrait_dir, \"s7.jpg\"), osp.join(example_video_dir, \"d19.mp4\"), True, True, True, False],\n    [osp.join(example_portrait_dir, \"s2.jpg\"), osp.join(example_video_dir, \"d13.mp4\"), True, True, True, True],\n]\ndata_examples_v2v = [\n    [osp.join(example_portrait_dir, \"s13.mp4\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False, 3e-7],\n    # [osp.join(example_portrait_dir, \"s14.mp4\"), osp.join(example_video_dir, \"d18.mp4\"), True, True, True, False, False, 3e-7],\n    # [osp.join(example_portrait_dir, \"s15.mp4\"), osp.join(example_video_dir, \"d19.mp4\"), True, True, True, False, False, 3e-7],\n    [osp.join(example_portrait_dir, \"s18.mp4\"), osp.join(example_video_dir, \"d6.mp4\"), True, True, True, False, 3e-7],\n    # [osp.join(example_portrait_dir, \"s19.mp4\"), osp.join(example_video_dir, \"d6.mp4\"), True, True, True, False, False, 3e-7],\n    [osp.join(example_portrait_dir, \"s20.mp4\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False, 3e-7],\n]\n#################### interface logic ####################\n\n# Define components first\nretargeting_source_scale = gr.Number(minimum=1.8, maximum=3.2, value=2.5, step=0.05, label=\"crop scale\")\nvideo_retargeting_source_scale = gr.Number(minimum=1.8, maximum=3.2, value=2.3, step=0.05, label=\"crop scale\")\ndriving_smooth_observation_variance_retargeting = gr.Number(value=3e-6, label=\"motion smooth strength\", minimum=1e-11, maximum=1e-2, step=1e-8)\nvideo_retargeting_silence = gr.Checkbox(value=False, label=\"keeping the lip silent\")\neye_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target eyes-open ratio\")\nlip_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target lip-open ratio\")\nvideo_lip_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target lip-open ratio\")\nhead_pitch_slider = gr.Slider(minimum=-15.0, maximum=15.0, value=0, step=1, label=\"relative pitch\")\nhead_yaw_slider = gr.Slider(minimum=-25, maximum=25, value=0, step=1, label=\"relative yaw\")\nhead_roll_slider = gr.Slider(minimum=-15.0, maximum=15.0, value=0, step=1, label=\"relative roll\")\nmov_x = gr.Slider(minimum=-0.19, maximum=0.19, value=0.0, step=0.01, label=\"x-axis movement\")\nmov_y = gr.Slider(minimum=-0.19, maximum=0.19, value=0.0, step=0.01, label=\"y-axis movement\")\nmov_z = gr.Slider(minimum=0.9, maximum=1.2, value=1.0, step=0.01, label=\"z-axis movement\")\nlip_variation_zero = gr.Slider(minimum=-0.09, maximum=0.09, value=0, step=0.01, label=\"pouting\")\nlip_variation_one = gr.Slider(minimum=-20.0, maximum=15.0, value=0, step=0.01, label=\"pursing üòê\")\nlip_variation_two = gr.Slider(minimum=0.0, maximum=15.0, value=0, step=0.01, label=\"grin üòÅ\")\nlip_variation_three = gr.Slider(minimum=-90.0, maximum=120.0, value=0, step=1.0, label=\"lip close <-> open\")\nsmile = gr.Slider(minimum=-0.3, maximum=1.3, value=0, step=0.01, label=\"smile üòÑ\")\nwink = gr.Slider(minimum=0, maximum=39, value=0, step=0.01, label=\"wink üòâ\")\neyebrow = gr.Slider(minimum=-30, maximum=30, value=0, step=0.01, label=\"eyebrow ü§®\")\neyeball_direction_x = gr.Slider(minimum=-30.0, maximum=30.0, value=0, step=0.01, label=\"eye gaze (horizontal) üëÄ\")\neyeball_direction_y = gr.Slider(minimum=-63.0, maximum=63.0, value=0, step=0.01, label=\"eye gaze (vertical) üôÑ\")\nretargeting_input_image = gr.Image(type=\"filepath\")\nretargeting_input_video = gr.Video()\noutput_image = gr.Image(type=\"numpy\")\noutput_image_paste_back = gr.Image(type=\"numpy\")\nretargeting_output_image = gr.Image(type=\"numpy\")\nretargeting_output_image_paste_back = gr.Image(type=\"numpy\")\noutput_video = gr.Video(autoplay=False)\noutput_video_paste_back = gr.Video(autoplay=False)\n\nwith gr.Blocks(theme=gr.themes.Soft(font=[gr.themes.GoogleFont(\"Plus Jakarta Sans\")])) as demo:\n    gr.HTML(load_description(title_md))\n\n    gr.Markdown(load_description(\"assets/gradio/gradio_description_upload.md\"))\n    with gr.Row():\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"üñºÔ∏è Source Image\") as tab_image:\n                    with gr.Accordion(open=True, label=\"Source Image\"):\n                        source_image_input = gr.Image(type=\"filepath\")\n                        gr.Examples(\n                            examples=[\n                                [osp.join(example_portrait_dir, \"s9.jpg\")],\n                                [osp.join(example_portrait_dir, \"s6.jpg\")],\n                                [osp.join(example_portrait_dir, \"s10.jpg\")],\n                                [osp.join(example_portrait_dir, \"s5.jpg\")],\n                                [osp.join(example_portrait_dir, \"s7.jpg\")],\n                                [osp.join(example_portrait_dir, \"s12.jpg\")],\n                                [osp.join(example_portrait_dir, \"s22.jpg\")],\n                                [osp.join(example_portrait_dir, \"s23.jpg\")],\n                            ],\n                            inputs=[source_image_input],\n                            cache_examples=False,\n                        )\n\n                with gr.TabItem(\"üéûÔ∏è Source Video\") as tab_video:\n                    with gr.Accordion(open=True, label=\"Source Video\"):\n                        source_video_input = gr.Video()\n                        gr.Examples(\n                            examples=[\n                                [osp.join(example_portrait_dir, \"s13.mp4\")],\n                                # [osp.join(example_portrait_dir, \"s14.mp4\")],\n                                # [osp.join(example_portrait_dir, \"s15.mp4\")],\n                                [osp.join(example_portrait_dir, \"s18.mp4\")],\n                                # [osp.join(example_portrait_dir, \"s19.mp4\")],\n                                [osp.join(example_portrait_dir, \"s20.mp4\")],\n                            ],\n                            inputs=[source_video_input],\n                            cache_examples=False,\n                        )\n\n                tab_selection = gr.Textbox(visible=False)\n                tab_image.select(lambda: \"Image\", None, tab_selection)\n                tab_video.select(lambda: \"Video\", None, tab_selection)\n            with gr.Accordion(open=True, label=\"Cropping Options for Source Image or Video\"):\n                with gr.Row():\n                    flag_do_crop_input = gr.Checkbox(value=True, label=\"do crop (source)\")\n                    scale = gr.Number(value=2.3, label=\"source crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n                    vx_ratio = gr.Number(value=0.0, label=\"source crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n                    vy_ratio = gr.Number(value=-0.125, label=\"source crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"üéûÔ∏è Driving Video\") as v_tab_video:\n                    with gr.Accordion(open=True, label=\"Driving Video\"):\n                        driving_video_input = gr.Video()\n                        gr.Examples(\n                            examples=[\n                                [osp.join(example_video_dir, \"d0.mp4\")],\n                                [osp.join(example_video_dir, \"d18.mp4\")],\n                                [osp.join(example_video_dir, \"d19.mp4\")],\n                                [osp.join(example_video_dir, \"d14.mp4\")],\n                                [osp.join(example_video_dir, \"d6.mp4\")],\n                                [osp.join(example_video_dir, \"d20.mp4\")],\n                            ],\n                            inputs=[driving_video_input],\n                            cache_examples=False,\n                        )\n                with gr.TabItem(\"üñºÔ∏è Driving Image\") as v_tab_image:\n                    with gr.Accordion(open=True, label=\"Driving Image\"):\n                        driving_image_input = gr.Image(type=\"filepath\")\n                        gr.Examples(\n                            examples=[\n                                [osp.join(example_video_dir, \"d30.jpg\")],\n                                [osp.join(example_video_dir, \"d9.jpg\")],\n                                [osp.join(example_video_dir, \"d19.jpg\")],\n                                [osp.join(example_video_dir, \"d8.jpg\")],\n                                [osp.join(example_video_dir, \"d12.jpg\")],\n                                [osp.join(example_video_dir, \"d38.jpg\")],\n                            ],\n                            inputs=[driving_image_input],\n                            cache_examples=False,\n                        )\n\n                with gr.TabItem(\"üìÅ Driving Pickle\") as v_tab_pickle:\n                    with gr.Accordion(open=True, label=\"Driving Pickle\"):\n                        driving_video_pickle_input = gr.File(type=\"filepath\", file_types=[\".pkl\"])\n                        gr.Examples(\n                            examples=[\n                                [osp.join(example_video_dir, \"d1.pkl\")],\n                                [osp.join(example_video_dir, \"d2.pkl\")],\n                                [osp.join(example_video_dir, \"d5.pkl\")],\n                                [osp.join(example_video_dir, \"d7.pkl\")],\n                                [osp.join(example_video_dir, \"d8.pkl\")],\n                            ],\n                            inputs=[driving_video_pickle_input],\n                            cache_examples=False,\n                        )\n\n                v_tab_selection = gr.Textbox(visible=False)\n                v_tab_video.select(lambda: \"Video\", None, v_tab_selection)\n                v_tab_image.select(lambda: \"Image\", None, v_tab_selection)\n                v_tab_pickle.select(lambda: \"Pickle\", None, v_tab_selection)\n            # with gr.Accordion(open=False, label=\"Animation Instructions\"):\n                # gr.Markdown(load_description(\"assets/gradio/gradio_description_animation.md\"))\n            with gr.Accordion(open=True, label=\"Cropping Options for Driving Video\"):\n                with gr.Row():\n                    flag_crop_driving_video_input = gr.Checkbox(value=False, label=\"do crop (driving)\")\n                    scale_crop_driving_video = gr.Number(value=2.2, label=\"driving crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n                    vx_ratio_crop_driving_video = gr.Number(value=0.0, label=\"driving crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n                    vy_ratio_crop_driving_video = gr.Number(value=-0.1, label=\"driving crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n\n    with gr.Row():\n        with gr.Accordion(open=True, label=\"Animation Options\"):\n            with gr.Row():\n                flag_normalize_lip = gr.Checkbox(value=False, label=\"normalize lip\")\n                flag_relative_input = gr.Checkbox(value=True, label=\"relative motion\")\n                flag_remap_input = gr.Checkbox(value=True, label=\"paste-back\")\n                flag_stitching_input = gr.Checkbox(value=True, label=\"stitching\")\n                animation_region = gr.Radio([\"exp\", \"pose\", \"lip\", \"eyes\", \"all\"], value=\"all\", label=\"animation region\")\n                driving_option_input = gr.Radio(['expression-friendly', 'pose-friendly'], value=\"expression-friendly\", label=\"driving option (i2v)\")\n                driving_multiplier = gr.Number(value=1.0, label=\"driving multiplier (i2v)\", minimum=0.0, maximum=2.0, step=0.02)\n                driving_smooth_observation_variance = gr.Number(value=3e-7, label=\"motion smooth strength (v2v)\", minimum=1e-11, maximum=1e-2, step=1e-8)\n\n    gr.Markdown(load_description(\"assets/gradio/gradio_description_animate_clear.md\"))\n    with gr.Row():\n        process_button_animation = gr.Button(\"üöÄ Animate\", variant=\"primary\")\n    with gr.Row():\n        with gr.Column():\n            output_video_i2v = gr.Video(autoplay=False, label=\"The animated video in the original image space\")\n        with gr.Column():\n            output_video_concat_i2v = gr.Video(autoplay=False, label=\"The animated video\")\n    with gr.Row():\n        with gr.Column():\n            output_image_i2i = gr.Image(type=\"numpy\", label=\"The animated image in the original image space\", visible=False)\n        with gr.Column():\n            output_image_concat_i2i = gr.Image(type=\"numpy\", label=\"The animated image\", visible=False)\n    with gr.Row():\n        process_button_reset = gr.ClearButton([source_image_input, source_video_input, driving_video_pickle_input, driving_video_input, driving_image_input, output_video_i2v, output_video_concat_i2v, output_image_i2i, output_image_concat_i2i], value=\"üßπ Clear\")\n\n    with gr.Row():\n        # Examples\n        gr.Markdown(\"## You could also choose the examples below by one click ‚¨áÔ∏è\")\n    with gr.Row():\n        with gr.Tabs():\n            with gr.TabItem(\"üñºÔ∏è Portrait Animation\"):\n                gr.Examples(\n                    examples=data_examples_i2v,\n                    fn=gpu_wrapped_execute_video,\n                    inputs=[\n                        source_image_input,\n                        driving_video_input,\n                        flag_relative_input,\n                        flag_do_crop_input,\n                        flag_remap_input,\n                        flag_crop_driving_video_input,\n                    ],\n                    outputs=[output_image, output_image_paste_back],\n                    examples_per_page=len(data_examples_i2v),\n                    cache_examples=False,\n                )\n            with gr.TabItem(\"üéûÔ∏è Portrait Video Editing\"):\n                gr.Examples(\n                    examples=data_examples_v2v,\n                    fn=gpu_wrapped_execute_video,\n                    inputs=[\n                        source_video_input,\n                        driving_video_input,\n                        flag_relative_input,\n                        flag_do_crop_input,\n                        flag_remap_input,\n                        flag_crop_driving_video_input,\n                        driving_smooth_observation_variance,\n                    ],\n                    outputs=[output_image, output_image_paste_back],\n                    examples_per_page=len(data_examples_v2v),\n                    cache_examples=False,\n                )\n\n    # Retargeting Image\n    gr.Markdown(load_description(\"assets/gradio/gradio_description_retargeting.md\"), visible=True)\n    with gr.Row(visible=True):\n        flag_do_crop_input_retargeting_image = gr.Checkbox(value=True, label=\"do crop (source)\")\n        flag_stitching_retargeting_input = gr.Checkbox(value=True, label=\"stitching\")\n        retargeting_source_scale.render()\n        eye_retargeting_slider.render()\n        lip_retargeting_slider.render()\n    with gr.Row(visible=True):\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"Facial movement sliders\"):\n                with gr.Row(visible=True):\n                    head_pitch_slider.render()\n                    head_yaw_slider.render()\n                    head_roll_slider.render()\n                with gr.Row(visible=True):\n                    mov_x.render()\n                    mov_y.render()\n                    mov_z.render()\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"Facial expression sliders\"):\n                with gr.Row(visible=True):\n                    lip_variation_zero.render()\n                    lip_variation_one.render()\n                    lip_variation_two.render()\n                with gr.Row(visible=True):\n                    lip_variation_three.render()\n                    smile.render()\n                    wink.render()\n                with gr.Row(visible=True):\n                    eyebrow.render()\n                    eyeball_direction_x.render()\n                    eyeball_direction_y.render()\n    with gr.Row(visible=True):\n        reset_button = gr.Button(\"üîÑ Reset\")\n        reset_button.click(\n            fn=reset_sliders,\n            inputs=None,\n            outputs=[\n                head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z,\n                lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y,\n                retargeting_source_scale, flag_stitching_retargeting_input, flag_do_crop_input_retargeting_image\n            ]\n        )\n    with gr.Row(visible=True):\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"Retargeting Image Input\"):\n                retargeting_input_image.render()\n                gr.Examples(\n                    examples=[\n                        [osp.join(example_portrait_dir, \"s9.jpg\")],\n                        [osp.join(example_portrait_dir, \"s6.jpg\")],\n                        [osp.join(example_portrait_dir, \"s10.jpg\")],\n                        [osp.join(example_portrait_dir, \"s5.jpg\")],\n                        [osp.join(example_portrait_dir, \"s7.jpg\")],\n                        [osp.join(example_portrait_dir, \"s12.jpg\")],\n                        [osp.join(example_portrait_dir, \"s22.jpg\")],\n                        # [osp.join(example_portrait_dir, \"s23.jpg\")],\n                        [osp.join(example_portrait_dir, \"s42.jpg\")],\n                    ],\n                    inputs=[retargeting_input_image],\n                    cache_examples=False,\n                )\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"Retargeting Result\"):\n                retargeting_output_image.render()\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"Paste-back Result\"):\n                retargeting_output_image_paste_back.render()\n    with gr.Row(visible=True):\n        process_button_reset_retargeting = gr.ClearButton(\n            [\n                retargeting_input_image,\n                retargeting_output_image,\n                retargeting_output_image_paste_back,\n            ],\n            value=\"üßπ Clear\"\n        )\n\n    # Retargeting Video\n    gr.Markdown(load_description(\"assets/gradio/gradio_description_retargeting_video.md\"), visible=True)\n    with gr.Row(visible=True):\n        flag_do_crop_input_retargeting_video = gr.Checkbox(value=True, label=\"do crop (source)\")\n        video_retargeting_source_scale.render()\n        video_lip_retargeting_slider.render()\n        driving_smooth_observation_variance_retargeting.render()\n        video_retargeting_silence.render()\n    with gr.Row(visible=True):\n        process_button_retargeting_video = gr.Button(\"üöó Retargeting Video\", variant=\"primary\")\n    with gr.Row(visible=True):\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"Retargeting Video Input\"):\n                retargeting_input_video.render()\n                gr.Examples(\n                    examples=[\n                        [osp.join(example_portrait_dir, \"s13.mp4\")],\n                        # [osp.join(example_portrait_dir, \"s18.mp4\")],\n                        # [osp.join(example_portrait_dir, \"s20.mp4\")],\n                        [osp.join(example_portrait_dir, \"s29.mp4\")],\n                        [osp.join(example_portrait_dir, \"s32.mp4\")],\n                        [osp.join(example_video_dir, \"d3.mp4\")],\n                    ],\n                    inputs=[retargeting_input_video],\n                    cache_examples=False,\n                )\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"Retargeting Result\"):\n                output_video.render()\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"Paste-back Result\"):\n                output_video_paste_back.render()\n    with gr.Row(visible=True):\n        process_button_reset_retargeting = gr.ClearButton(\n            [\n                video_lip_retargeting_slider,\n                retargeting_input_video,\n                output_video,\n                output_video_paste_back\n            ],\n            value=\"üßπ Clear\"\n        )\n\n    # binding functions for buttons\n    process_button_animation.click(\n        fn=gpu_wrapped_execute_video,\n        inputs=[\n            source_image_input,\n            source_video_input,\n            driving_video_input,\n            driving_image_input,\n            driving_video_pickle_input,\n            flag_normalize_lip,\n            flag_relative_input,\n            flag_do_crop_input,\n            flag_remap_input,\n            flag_stitching_input,\n            animation_region,\n            driving_option_input,\n            driving_multiplier,\n            flag_crop_driving_video_input,\n            scale,\n            vx_ratio,\n            vy_ratio,\n            scale_crop_driving_video,\n            vx_ratio_crop_driving_video,\n            vy_ratio_crop_driving_video,\n            driving_smooth_observation_variance,\n            tab_selection,\n            v_tab_selection,\n        ],\n        outputs=[output_video_i2v, output_video_i2v, output_video_concat_i2v, output_video_concat_i2v, output_image_i2i, output_image_i2i, output_image_concat_i2i, output_image_concat_i2i],\n        show_progress=True\n    )\n\n\n    retargeting_input_image.change(\n        fn=gradio_pipeline.init_retargeting_image,\n        inputs=[retargeting_source_scale, eye_retargeting_slider, lip_retargeting_slider, retargeting_input_image],\n        outputs=[eye_retargeting_slider, lip_retargeting_slider]\n    )\n\n    sliders = [eye_retargeting_slider, lip_retargeting_slider, head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z, lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y]\n    for slider in sliders:\n        # NOTE: gradio >= 4.0.0 may cause slow response\n        slider.change(\n            fn=gpu_wrapped_execute_image_retargeting,\n            inputs=[\n                eye_retargeting_slider, lip_retargeting_slider, head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z,\n                lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y,\n                retargeting_input_image, retargeting_source_scale, flag_stitching_retargeting_input, flag_do_crop_input_retargeting_image\n            ],\n            outputs=[retargeting_output_image, retargeting_output_image_paste_back],\n        )\n\n    process_button_retargeting_video.click(\n        fn=gpu_wrapped_execute_video_retargeting,\n        inputs=[video_lip_retargeting_slider, retargeting_input_video, video_retargeting_source_scale, driving_smooth_observation_variance_retargeting, video_retargeting_silence, flag_do_crop_input_retargeting_video],\n        outputs=[output_video, output_video_paste_back],\n        show_progress=True\n    )\n\ndemo.launch(\n    server_port=args.server_port,\n    share=args.share,\n    server_name=args.server_name\n)\n"
        },
        {
          "name": "app_animals.py",
          "type": "blob",
          "size": 11.1181640625,
          "content": "# coding: utf-8\n\n\"\"\"\nThe entrance of the gradio for animal\n\"\"\"\n\nimport os\nimport tyro\nimport subprocess\nimport gradio as gr\nimport os.path as osp\nfrom src.utils.helper import load_description\nfrom src.gradio_pipeline import GradioPipelineAnimal\nfrom src.config.crop_config import CropConfig\nfrom src.config.argument_config import ArgumentConfig\nfrom src.config.inference_config import InferenceConfig\n\n\ndef partial_fields(target_class, kwargs):\n    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n\n\ndef fast_check_ffmpeg():\n    try:\n        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n        return True\n    except:\n        return False\n\n\n# set tyro theme\ntyro.extras.set_accent_color(\"bright_cyan\")\nargs = tyro.cli(ArgumentConfig)\n\nffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\nif osp.exists(ffmpeg_dir):\n    os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n\nif not fast_check_ffmpeg():\n    raise ImportError(\n        \"FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html\"\n    )\n# specify configs for inference\ninference_cfg = partial_fields(InferenceConfig, args.__dict__)  # use attribute of args to initial InferenceConfig\ncrop_cfg = partial_fields(CropConfig, args.__dict__)  # use attribute of args to initial CropConfig\n\ngradio_pipeline_animal: GradioPipelineAnimal = GradioPipelineAnimal(\n    inference_cfg=inference_cfg,\n    crop_cfg=crop_cfg,\n    args=args\n)\n\nif args.gradio_temp_dir not in (None, ''):\n    os.environ[\"GRADIO_TEMP_DIR\"] = args.gradio_temp_dir\n    os.makedirs(args.gradio_temp_dir, exist_ok=True)\n\ndef gpu_wrapped_execute_video(*args, **kwargs):\n    return gradio_pipeline_animal.execute_video(*args, **kwargs)\n\n\n# assets\ntitle_md = \"assets/gradio/gradio_title.md\"\nexample_portrait_dir = \"assets/examples/source\"\nexample_video_dir = \"assets/examples/driving\"\ndata_examples_i2v = [\n    [osp.join(example_portrait_dir, \"s41.jpg\"), osp.join(example_video_dir, \"d3.mp4\"), True, False, False, False],\n    [osp.join(example_portrait_dir, \"s40.jpg\"), osp.join(example_video_dir, \"d6.mp4\"), True, False, False, False],\n    [osp.join(example_portrait_dir, \"s25.jpg\"), osp.join(example_video_dir, \"d19.mp4\"), True, False, False, False],\n]\ndata_examples_i2v_pickle = [\n    [osp.join(example_portrait_dir, \"s25.jpg\"), osp.join(example_video_dir, \"wink.pkl\"), True, False, False, False],\n    [osp.join(example_portrait_dir, \"s40.jpg\"), osp.join(example_video_dir, \"talking.pkl\"), True, False, False, False],\n    [osp.join(example_portrait_dir, \"s41.jpg\"), osp.join(example_video_dir, \"aggrieved.pkl\"), True, False, False, False],\n]\n#################### interface logic ####################\n\n# Define components first\noutput_image = gr.Image(type=\"numpy\")\noutput_image_paste_back = gr.Image(type=\"numpy\")\noutput_video_i2v = gr.Video(autoplay=False)\noutput_video_concat_i2v = gr.Video(autoplay=False)\noutput_video_i2v_gif = gr.Image(type=\"numpy\")\n\n\nwith gr.Blocks(theme=gr.themes.Soft(font=[gr.themes.GoogleFont(\"Plus Jakarta Sans\")])) as demo:\n    gr.HTML(load_description(title_md))\n\n    gr.Markdown(load_description(\"assets/gradio/gradio_description_upload_animal.md\"))\n    with gr.Row():\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"üê± Source Animal Image\"):\n                source_image_input = gr.Image(type=\"filepath\")\n                gr.Examples(\n                    examples=[\n                        [osp.join(example_portrait_dir, \"s25.jpg\")],\n                        [osp.join(example_portrait_dir, \"s30.jpg\")],\n                        [osp.join(example_portrait_dir, \"s31.jpg\")],\n                        [osp.join(example_portrait_dir, \"s32.jpg\")],\n                        [osp.join(example_portrait_dir, \"s33.jpg\")],\n                        [osp.join(example_portrait_dir, \"s39.jpg\")],\n                        [osp.join(example_portrait_dir, \"s40.jpg\")],\n                        [osp.join(example_portrait_dir, \"s41.jpg\")],\n                        [osp.join(example_portrait_dir, \"s38.jpg\")],\n                        [osp.join(example_portrait_dir, \"s36.jpg\")],\n                    ],\n                    inputs=[source_image_input],\n                    cache_examples=False,\n                )\n\n            with gr.Accordion(open=True, label=\"Cropping Options for Source Image\"):\n                with gr.Row():\n                    flag_do_crop_input = gr.Checkbox(value=True, label=\"do crop (source)\")\n                    scale = gr.Number(value=2.3, label=\"source crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n                    vx_ratio = gr.Number(value=0.0, label=\"source crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n                    vy_ratio = gr.Number(value=-0.125, label=\"source crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"üìÅ Driving Pickle\") as tab_pickle:\n                    with gr.Accordion(open=True, label=\"Driving Pickle\"):\n                        driving_video_pickle_input = gr.File()\n                        gr.Examples(\n                            examples=[\n                                [osp.join(example_video_dir, \"wink.pkl\")],\n                                [osp.join(example_video_dir, \"shy.pkl\")],\n                                [osp.join(example_video_dir, \"aggrieved.pkl\")],\n                                [osp.join(example_video_dir, \"open_lip.pkl\")],\n                                [osp.join(example_video_dir, \"laugh.pkl\")],\n                                [osp.join(example_video_dir, \"talking.pkl\")],\n                                [osp.join(example_video_dir, \"shake_face.pkl\")],\n                            ],\n                            inputs=[driving_video_pickle_input],\n                            cache_examples=False,\n                        )\n                with gr.TabItem(\"üéûÔ∏è Driving Video\") as tab_video:\n                    with gr.Accordion(open=True, label=\"Driving Video\"):\n                        driving_video_input = gr.Video()\n                        gr.Examples(\n                            examples=[\n                                # [osp.join(example_video_dir, \"d0.mp4\")],\n                                # [osp.join(example_video_dir, \"d18.mp4\")],\n                                [osp.join(example_video_dir, \"d19.mp4\")],\n                                [osp.join(example_video_dir, \"d14.mp4\")],\n                                [osp.join(example_video_dir, \"d6.mp4\")],\n                                [osp.join(example_video_dir, \"d3.mp4\")],\n                            ],\n                            inputs=[driving_video_input],\n                            cache_examples=False,\n                        )\n\n                    tab_selection = gr.Textbox(visible=False)\n                    tab_pickle.select(lambda: \"Pickle\", None, tab_selection)\n                    tab_video.select(lambda: \"Video\", None, tab_selection)\n            with gr.Accordion(open=True, label=\"Cropping Options for Driving Video\"):\n                with gr.Row():\n                    flag_crop_driving_video_input = gr.Checkbox(value=False, label=\"do crop (driving)\")\n                    scale_crop_driving_video = gr.Number(value=2.2, label=\"driving crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n                    vx_ratio_crop_driving_video = gr.Number(value=0.0, label=\"driving crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n                    vy_ratio_crop_driving_video = gr.Number(value=-0.1, label=\"driving crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n\n    with gr.Row():\n        with gr.Accordion(open=False, label=\"Animation Options\"):\n            with gr.Row():\n                flag_stitching = gr.Checkbox(value=False, label=\"stitching (not recommended)\")\n                flag_remap_input = gr.Checkbox(value=False, label=\"paste-back (not recommended)\")\n                driving_multiplier = gr.Number(value=1.0, label=\"driving multiplier\", minimum=0.0, maximum=2.0, step=0.02)\n\n    gr.Markdown(load_description(\"assets/gradio/gradio_description_animate_clear.md\"))\n    with gr.Row():\n        process_button_animation = gr.Button(\"üöÄ Animate\", variant=\"primary\")\n    with gr.Row():\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"The animated video in the cropped image space\"):\n                output_video_i2v.render()\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"The animated gif in the cropped image space\"):\n                output_video_i2v_gif.render()\n        with gr.Column():\n            with gr.Accordion(open=True, label=\"The animated video\"):\n                output_video_concat_i2v.render()\n    with gr.Row():\n        process_button_reset = gr.ClearButton([source_image_input, driving_video_input, output_video_i2v, output_video_concat_i2v, output_video_i2v_gif], value=\"üßπ Clear\")\n\n    with gr.Row():\n        # Examples\n        gr.Markdown(\"## You could also choose the examples below by one click ‚¨áÔ∏è\")\n    with gr.Row():\n        with gr.Tabs():\n            with gr.TabItem(\"üìÅ Driving Pickle\") as tab_video:\n                gr.Examples(\n                    examples=data_examples_i2v_pickle,\n                    fn=gpu_wrapped_execute_video,\n                    inputs=[\n                        source_image_input,\n                        driving_video_pickle_input,\n                        flag_do_crop_input,\n                        flag_stitching,\n                        flag_remap_input,\n                        flag_crop_driving_video_input,\n                    ],\n                    outputs=[output_image, output_image_paste_back, output_video_i2v_gif],\n                    examples_per_page=len(data_examples_i2v_pickle),\n                    cache_examples=False,\n                )\n            with gr.TabItem(\"üéûÔ∏è Driving Video\") as tab_video:\n                gr.Examples(\n                    examples=data_examples_i2v,\n                    fn=gpu_wrapped_execute_video,\n                    inputs=[\n                        source_image_input,\n                        driving_video_input,\n                        flag_do_crop_input,\n                        flag_stitching,\n                        flag_remap_input,\n                        flag_crop_driving_video_input,\n                    ],\n                    outputs=[output_image, output_image_paste_back, output_video_i2v_gif],\n                    examples_per_page=len(data_examples_i2v),\n                    cache_examples=False,\n                )\n\n    process_button_animation.click(\n        fn=gpu_wrapped_execute_video,\n        inputs=[\n            source_image_input,\n            driving_video_input,\n            driving_video_pickle_input,\n            flag_do_crop_input,\n            flag_remap_input,\n            driving_multiplier,\n            flag_stitching,\n            flag_crop_driving_video_input,\n            scale,\n            vx_ratio,\n            vy_ratio,\n            scale_crop_driving_video,\n            vx_ratio_crop_driving_video,\n            vy_ratio_crop_driving_video,\n            tab_selection,\n        ],\n        outputs=[output_video_i2v, output_video_concat_i2v, output_video_i2v_gif],\n        show_progress=True\n    )\n\ndemo.launch(\n    server_port=args.server_port,\n    share=args.share,\n    server_name=args.server_name\n)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 1.7373046875,
          "content": "# coding: utf-8\n\n\"\"\"\nThe entrance of humans\n\"\"\"\n\nimport os\nimport os.path as osp\nimport tyro\nimport subprocess\nfrom src.config.argument_config import ArgumentConfig\nfrom src.config.inference_config import InferenceConfig\nfrom src.config.crop_config import CropConfig\nfrom src.live_portrait_pipeline import LivePortraitPipeline\n\n\ndef partial_fields(target_class, kwargs):\n    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n\n\ndef fast_check_ffmpeg():\n    try:\n        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n        return True\n    except:\n        return False\n\n\ndef fast_check_args(args: ArgumentConfig):\n    if not osp.exists(args.source):\n        raise FileNotFoundError(f\"source info not found: {args.source}\")\n    if not osp.exists(args.driving):\n        raise FileNotFoundError(f\"driving info not found: {args.driving}\")\n\n\ndef main():\n    # set tyro theme\n    tyro.extras.set_accent_color(\"bright_cyan\")\n    args = tyro.cli(ArgumentConfig)\n\n    ffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\n    if osp.exists(ffmpeg_dir):\n        os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n\n    if not fast_check_ffmpeg():\n        raise ImportError(\n            \"FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html\"\n        )\n\n    fast_check_args(args)\n\n    # specify configs for inference\n    inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n    crop_cfg = partial_fields(CropConfig, args.__dict__)\n\n    live_portrait_pipeline = LivePortraitPipeline(\n        inference_cfg=inference_cfg,\n        crop_cfg=crop_cfg\n    )\n\n    # run\n    live_portrait_pipeline.execute(args)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "inference_animals.py",
          "type": "blob",
          "size": 1.76953125,
          "content": "# coding: utf-8\n\n\"\"\"\nThe entrance of animal\n\"\"\"\n\nimport os\nimport os.path as osp\nimport tyro\nimport subprocess\nfrom src.config.argument_config import ArgumentConfig\nfrom src.config.inference_config import InferenceConfig\nfrom src.config.crop_config import CropConfig\nfrom src.live_portrait_pipeline_animal import LivePortraitPipelineAnimal\n\n\ndef partial_fields(target_class, kwargs):\n    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n\n\ndef fast_check_ffmpeg():\n    try:\n        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n        return True\n    except:\n        return False\n\n\ndef fast_check_args(args: ArgumentConfig):\n    if not osp.exists(args.source):\n        raise FileNotFoundError(f\"source info not found: {args.source}\")\n    if not osp.exists(args.driving):\n        raise FileNotFoundError(f\"driving info not found: {args.driving}\")\n\n\ndef main():\n    # set tyro theme\n    tyro.extras.set_accent_color(\"bright_cyan\")\n    args = tyro.cli(ArgumentConfig)\n\n    ffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\n    if osp.exists(ffmpeg_dir):\n        os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n\n    if not fast_check_ffmpeg():\n        raise ImportError(\n            \"FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html\"\n        )\n\n    fast_check_args(args)\n\n    # specify configs for inference\n    inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n    crop_cfg = partial_fields(CropConfig, args.__dict__)\n\n    live_portrait_pipeline_animal = LivePortraitPipelineAnimal(\n        inference_cfg=inference_cfg,\n        crop_cfg=crop_cfg\n    )\n\n    # run\n    live_portrait_pipeline_animal.execute(args)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "pretrained_weights",
          "type": "tree",
          "content": null
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 18.0400390625,
          "content": "<h1 align=\"center\">LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</h1>\n\n<div align='center'>\n    <a href='https://github.com/cleardusk' target='_blank'><strong>Jianzhu Guo</strong></a><sup> 1‚Ä†</sup>&emsp;\n    <a href='https://github.com/Mystery099' target='_blank'><strong>Dingyun Zhang</strong></a><sup> 1,2</sup>&emsp;\n    <a href='https://github.com/KwaiVGI' target='_blank'><strong>Xiaoqiang Liu</strong></a><sup> 1</sup>&emsp;\n    <a href='https://github.com/zzzweakman' target='_blank'><strong>Zhizhou Zhong</strong></a><sup> 1,3</sup>&emsp;\n    <a href='https://scholar.google.com.hk/citations?user=_8k1ubAAAAAJ' target='_blank'><strong>Yuan Zhang</strong></a><sup> 1</sup>&emsp;\n</div>\n\n<div align='center'>\n    <a href='https://scholar.google.com/citations?user=P6MraaYAAAAJ' target='_blank'><strong>Pengfei Wan</strong></a><sup> 1</sup>&emsp;\n    <a href='https://openreview.net/profile?id=~Di_ZHANG3' target='_blank'><strong>Di Zhang</strong></a><sup> 1</sup>&emsp;\n</div>\n\n<div align='center'>\n    <sup>1 </sup>Kuaishou Technology&emsp; <sup>2 </sup>University of Science and Technology of China&emsp; <sup>3 </sup>Fudan University&emsp;\n</div>\n<div align='center'>\n    <small><sup>‚Ä†</sup> Corresponding author</small>\n</div>\n<br>\n\n<div align=\"center\">\n  <!-- <a href='LICENSE'><img src='https://img.shields.io/badge/license-MIT-yellow'></a> -->\n  <a href='https://arxiv.org/pdf/2407.03168'><img src='https://img.shields.io/badge/arXiv-LivePortrait-red'></a>\n  <a href='https://liveportrait.github.io'><img src='https://img.shields.io/badge/Project-LivePortrait-green'></a>\n  <a href='https://huggingface.co/spaces/KwaiVGI/liveportrait'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a>\n  <a href=\"https://github.com/KwaiVGI/LivePortrait\"><img src=\"https://img.shields.io/github/stars/KwaiVGI/LivePortrait\"></a>\n  <br>\n  <strong>English</strong> | <a href=\"./readme_zh_cn.md\"><strong>ÁÆÄ‰Ωì‰∏≠Êñá</strong></a>\n</div>\n<br>\n\n\n<p align=\"center\">\n  <img src=\"./assets/docs/showcase2.gif\" alt=\"showcase\">\n  <br>\n  üî• For more results, visit our <a href=\"https://liveportrait.github.io/\"><strong>homepage</strong></a> üî•\n</p>\n\n\n## üî• Updates\n- **`2025/01/01`**: üê∂ We updated a new version of the Animals model with more data, see [**here**](./assets/docs/changelog/2025-01-01.md).\n- **`2024/10/18`**: ‚ùó We have updated the versions of the `transformers` and `gradio` libraries to avoid security vulnerabilities. Details [here](https://github.com/KwaiVGI/LivePortrait/pull/421/files).\n- **`2024/08/29`**: üì¶ We update the Windows [one-click installer](https://huggingface.co/cleardusk/LivePortrait-Windows/blob/main/LivePortrait-Windows-v20240829.zip) and support auto-updates, see [changelog](https://huggingface.co/cleardusk/LivePortrait-Windows#20240829).\n- **`2024/08/19`**: üñºÔ∏è We support **image driven mode** and **regional control**. For details, see [**here**](./assets/docs/changelog/2024-08-19.md).\n- **`2024/08/06`**: üé® We support **precise portrait editing** in the Gradio interface, inspired by [ComfyUI-AdvancedLivePortrait](https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait). See [**here**](./assets/docs/changelog/2024-08-06.md).\n- **`2024/08/05`**: üì¶ Windows users can now download the [one-click installer](https://huggingface.co/cleardusk/LivePortrait-Windows/blob/main/LivePortrait-Windows-v20240806.zip) for Humans mode and **Animals mode** now! For details, see [**here**](./assets/docs/changelog/2024-08-05.md).\n- **`2024/08/02`**: üò∏ We released a version of the **Animals model**, along with several other updates and improvements. Check out the details [**here**](./assets/docs/changelog/2024-08-02.md)!\n- **`2024/07/25`**: üì¶ Windows users can now download the package from [HuggingFace](https://huggingface.co/cleardusk/LivePortrait-Windows/tree/main). Simply unzip and double-click `run_windows.bat` to enjoy!\n- **`2024/07/24`**: üé® We support pose editing for source portraits in the Gradio interface. We‚Äôve also lowered the default detection threshold to increase recall. [Have fun](assets/docs/changelog/2024-07-24.md)!\n- **`2024/07/19`**: ‚ú® We support üéûÔ∏è **portrait video editing (aka v2v)**! More to see [here](assets/docs/changelog/2024-07-19.md).\n- **`2024/07/17`**: üçé We support macOS with Apple Silicon, modified from [jeethu](https://github.com/jeethu)'s PR [#143](https://github.com/KwaiVGI/LivePortrait/pull/143).\n- **`2024/07/10`**: üí™ We support audio and video concatenating, driving video auto-cropping, and template making to protect privacy. More to see [here](assets/docs/changelog/2024-07-10.md).\n- **`2024/07/09`**: ü§ó We released the [HuggingFace Space](https://huggingface.co/spaces/KwaiVGI/liveportrait), thanks to the HF team and [Gradio](https://github.com/gradio-app/gradio)!\n- **`2024/07/04`**: üòä We released the initial version of the inference code and models. Continuous updates, stay tuned!\n- **`2024/07/04`**: üî• We released the [homepage](https://liveportrait.github.io) and technical report on [arXiv](https://arxiv.org/pdf/2407.03168).\n\n\n\n## Introduction üìñ\nThis repo, named **LivePortrait**, contains the official PyTorch implementation of our paper [LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control](https://arxiv.org/pdf/2407.03168).\nWe are actively updating and improving this repository. If you find any bugs or have suggestions, welcome to raise issues or submit pull requests (PR) üíñ.\n\n## Getting Started üèÅ\n### 1. Clone the code and prepare the environment üõ†Ô∏è\n\n> [!Note]\n> Make sure your system has [`git`](https://git-scm.com/), [`conda`](https://anaconda.org/anaconda/conda), and [`FFmpeg`](https://ffmpeg.org/download.html) installed. For details on FFmpeg installation, see [**how to install FFmpeg**](assets/docs/how-to-install-ffmpeg.md).\n\n```bash\ngit clone https://github.com/KwaiVGI/LivePortrait\ncd LivePortrait\n\n# create env using conda\nconda create -n LivePortrait python=3.10\nconda activate LivePortrait\n```\n\n#### For Linux or Windows Users\n[X-Pose](https://github.com/IDEA-Research/X-Pose) requires your `torch` version to be compatible with the CUDA version.\n\nFirstly, check your current CUDA version by:\n```bash\nnvcc -V # example versions: 11.1, 11.8, 12.1, etc.\n```\n\nThen, install the corresponding torch version. Here are examples for different CUDA versions. Visit the [PyTorch Official Website](https://pytorch.org/get-started/previous-versions) for installation commands if your CUDA version is not listed:\n```bash\n# for CUDA 11.1\npip install torch==1.10.1+cu111 torchvision==0.11.2 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n# for CUDA 11.8\npip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118\n# for CUDA 12.1\npip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n# ...\n```\n\n**Note**: On Windows systems, some higher versions of CUDA (such as 12.4, 12.6, etc.) may lead to unknown issues. You may consider downgrading CUDA to version 11.8 for stability. See the [downgrade guide](https://github.com/dimitribarbot/sd-webui-live-portrait/blob/main/assets/docs/how-to-install-xpose.md#cuda-toolkit-118) by [@dimitribarbot](https://github.com/dimitribarbot).\n\nFinally, install the remaining dependencies:\n```bash\npip install -r requirements.txt\n```\n\n#### For macOS with Apple Silicon Users\nThe [X-Pose](https://github.com/IDEA-Research/X-Pose) dependency does not support macOS, so you can skip its installation. While Humans mode works as usual, Animals mode is not supported. Use the provided requirements file for macOS with Apple Silicon:\n```bash\n# for macOS with Apple Silicon users\npip install -r requirements_macOS.txt\n```\n\n### 2. Download pretrained weights üì•\n\nThe easiest way to download the pretrained weights is from HuggingFace:\n```bash\n# !pip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude \"*.git*\" \"README.md\" \"docs\"\n```\n\nIf you cannot access to Huggingface, you can use [hf-mirror](https://hf-mirror.com/) to download:\n```bash\n# !pip install -U \"huggingface_hub[cli]\"\nexport HF_ENDPOINT=https://hf-mirror.com\nhuggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude \"*.git*\" \"README.md\" \"docs\"\n```\n\nAlternatively, you can download all pretrained weights from [Google Drive](https://drive.google.com/drive/folders/1UtKgzKjFAOmZkhNK-OYT0caJ_w2XAnib) or [Baidu Yun](https://pan.baidu.com/s/1MGctWmNla_vZxDbEp2Dtzw?pwd=z5cn). Unzip and place them in `./pretrained_weights`.\n\nEnsuring the directory structure is as or contains [**this**](assets/docs/directory-structure.md).\n\n### 3. Inference üöÄ\n\n#### Fast hands-on (humans) üë§\n```bash\n# For Linux and Windows users\npython inference.py\n\n# For macOS users with Apple Silicon (Intel is not tested). NOTE: this maybe 20x slower than RTX 4090\nPYTORCH_ENABLE_MPS_FALLBACK=1 python inference.py\n```\n\nIf the script runs successfully, you will get an output mp4 file named `animations/s6--d0_concat.mp4`. This file includes the following results: driving video, input image or video, and generated result.\n\n<p align=\"center\">\n  <img src=\"./assets/docs/inference.gif\" alt=\"image\">\n</p>\n\nOr, you can change the input by specifying the `-s` and `-d` arguments:\n\n```bash\n# source input is an image\npython inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d0.mp4\n\n# source input is a video ‚ú®\npython inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d0.mp4\n\n# more options to see\npython inference.py -h\n```\n\n#### Fast hands-on (animals) üê±üê∂\nAnimals mode is ONLY tested on Linux and Windows with NVIDIA GPU.\n\nYou need to build an OP named `MultiScaleDeformableAttention` first, which is used by [X-Pose](https://github.com/IDEA-Research/X-Pose), a general keypoint detection framework.\n```bash\ncd src/utils/dependencies/XPose/models/UniPose/ops\npython setup.py build install\ncd - # equal to cd ../../../../../../../\n```\n\nThen\n```bash\npython inference_animals.py -s assets/examples/source/s39.jpg -d assets/examples/driving/wink.pkl --driving_multiplier 1.75 --no_flag_stitching\n```\nIf the script runs successfully, you will get an output mp4 file named `animations/s39--wink_concat.mp4`.\n<p align=\"center\">\n  <img src=\"./assets/docs/inference-animals.gif\" alt=\"image\">\n</p>\n\n#### Driving video auto-cropping üì¢üì¢üì¢\n> [!IMPORTANT]\n> To use your own driving video, we **recommend**: ‚¨áÔ∏è\n> - Crop it to a **1:1** aspect ratio (e.g., 512x512 or 256x256 pixels), or enable auto-cropping by `--flag_crop_driving_video`.\n> - Focus on the head area, similar to the example videos.\n> - Minimize shoulder movement.\n> - Make sure the first frame of driving video is a frontal face with **neutral expression**.\n\nBelow is an auto-cropping case by `--flag_crop_driving_video`:\n```bash\npython inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d13.mp4 --flag_crop_driving_video\n```\n\nIf you find the results of auto-cropping is not well, you can modify the `--scale_crop_driving_video`, `--vy_ratio_crop_driving_video` options to adjust the scale and offset, or do it manually.\n\n#### Motion template making\nYou can also use the auto-generated motion template files ending with `.pkl` to speed up inference, and **protect privacy**, such as:\n```bash\npython inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d5.pkl # portrait animation\npython inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d5.pkl # portrait video editing\n```\n\n### 4. Gradio interface ü§ó\n\nWe also provide a Gradio <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a> interface for a better experience, just run by:\n\n```bash\n# For Linux and Windows users (and macOS with Intel??)\npython app.py # humans mode\n\n# For macOS with Apple Silicon users, Intel not supported, this maybe 20x slower than RTX 4090\nPYTORCH_ENABLE_MPS_FALLBACK=1 python app.py # humans mode\n```\n\nWe also provide a Gradio interface of animals mode, which is only tested on Linux with NVIDIA GPU:\n```bash\npython app_animals.py # animals mode üê±üê∂\n```\n\nYou can specify the `--server_port`, `--share`, `--server_name` arguments to satisfy your needs!\n\nüöÄ We also provide an acceleration option `--flag_do_torch_compile`. The first-time inference triggers an optimization process (about one minute), making subsequent inferences 20-30% faster. Performance gains may vary with different CUDA versions.\n```bash\n# enable torch.compile for faster inference\npython app.py --flag_do_torch_compile\n```\n**Note**: This method is not supported on Windows and macOS.\n\n**Or, try it out effortlessly on [HuggingFace](https://huggingface.co/spaces/KwaiVGI/LivePortrait) ü§ó**\n\n### 5. Inference speed evaluation üöÄüöÄüöÄ\nWe have also provided a script to evaluate the inference speed of each module:\n\n```bash\n# For NVIDIA GPU\npython speed.py\n```\n\nThe results are [**here**](./assets/docs/speed.md).\n\n## Community Resources ü§ó\n\nDiscover the invaluable resources contributed by our community to enhance your LivePortrait experience.\n\n\n### Community-developed Projects\n\n| Repo (sorted by created timestamp) | Description | Author |\n|------|------|--------|\n| [**AdvancedLivePortrait-WebUI**](https://github.com/jhj0517/AdvancedLivePortrait-WebUI) | Dedicated gradio based WebUI started from [ComfyUI-AdvancedLivePortrait](https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait) | [@jhj0517](https://github.com/jhj0517) |\n| [**FacePoke**](https://github.com/jbilcke-hf/FacePoke) | A real-time head transformation app, controlled by your mouse! | [@jbilcke-hf](https://github.com/jbilcke-hf) |\n| [**FaceFusion**](https://github.com/facefusion/facefusion) | FaceFusion 3.0 integregates LivePortrait as `expression_restorer` and `face_editor` processors. | [@henryruhs](https://github.com/henryruhs) |\n| [**sd-webui-live-portrait**](https://github.com/dimitribarbot/sd-webui-live-portrait) | WebUI extension of LivePortrait, adding atab to the original Stable Diffusion WebUI to benefit from LivePortrait features. | [@dimitribarbot](https://github.com/dimitribarbot) |\n| [**ComfyUI-LivePortraitKJ**](https://github.com/kijai/ComfyUI-LivePortraitKJ) | A ComfyUI node to use LivePortrait, with MediaPipe as as an alternative to Insightface. | [@kijai](https://github.com/kijai) |\n| [**ComfyUI-AdvancedLivePortrait**](https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait) | A faster ComfyUI node with real-time preview that has inspired many other community-developed tools and projects. | [@PowerHouseMan](https://github.com/PowerHouseMan) |\n| [**comfyui-liveportrait**](https://github.com/shadowcz007/comfyui-liveportrait) | A ComfyUI node to use LivePortrait, supporting multi-faces, expression interpolation etc, with a [tutorial](https://www.bilibili.com/video/BV1JW421R7sP). | [@shadowcz007](https://github.com/shadowcz007) |\n\n### Playgrounds, ü§ó HuggingFace Spaces and Others\n- [FacePoke Space](https://huggingface.co/spaces/jbilcke-hf/FacePoke)\n- [Expression Editor Space](https://huggingface.co/spaces/fffiloni/expression-editor)\n- [Expression Editor Replicate](https://replicate.com/fofr/expression-editor)\n- [Face Control Realtime Demo](https://fal.ai/demos/face-control) on FAL\n- [Replicate Playground](https://replicate.com/fofr/live-portrait)\n- Nuke can use LivePortrait through CompyUI node, details [here](https://x.com/bilawalsidhu/status/1837349806475276338)\n- LivePortrait lives on [Poe](https://poe.com/LivePortrait)\n\n### Video Tutorials\n- [Workflow of LivePortrait Video to Video](https://youtu.be/xfzK_6cTs58?si=aYjgypeJBkhc46VL) by [@curiousrefuge](https://www.youtube.com/@curiousrefuge)\n- [Google Colab tutorial](https://youtu.be/59Y9ePAXTp0?si=KzEWhklBlporW7D8) by [@Planet Ai](https://www.youtube.com/@planetai217)\n- [Paper reading](https://youtu.be/fD0P6UWSu8I?si=Vn5wxUa8qSu1jv4l) by [@TwoMinutePapers](https://www.youtube.com/@TwoMinutePapers)\n- [ComfyUI Advanced LivePortrait](https://youtu.be/q0Vf-ZZsbzI?si=nbs3npleH-dVCt28) by [TutoView](https://www.youtube.com/@TutoView)\n- [LivePortarit exploration](https://www.youtube.com/watch?v=vsvlbTEqgXQ) and [A deep dive into LivePortrait](https://youtu.be/cucaEEDYmsw?si=AtPaDWc5G-a4E8dD) by [TheoreticallyMedia](https://www.youtube.com/@TheoreticallyMedia)\n- [LivePortrait hands-on tutorial](https://www.youtube.com/watch?v=uyjSTAOY7yI) by [@AI Search](https://www.youtube.com/@theAIsearch)\n- [ComfyUI tutorial](https://www.youtube.com/watch?v=8-IcDDmiUMM) by [@Sebastian Kamph](https://www.youtube.com/@sebastiankamph)\n- A [tutorial](https://www.bilibili.com/video/BV1cf421i7Ly) on BiliBili\n\nAnd so MANY amazing contributions from our community, too many to list them all üíñ\n\n## Acknowledgements üíê\nWe would like to thank the contributors of [FOMM](https://github.com/AliaksandrSiarohin/first-order-model), [Open Facevid2vid](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis), [SPADE](https://github.com/NVlabs/SPADE), [InsightFace](https://github.com/deepinsight/insightface) and [X-Pose](https://github.com/IDEA-Research/X-Pose) repositories, for their open research and contributions.\n\n## Ethics Considerations üõ°Ô∏è\nPortrait animation technologies come with social risks, particularly the potential for misuse in creating deepfakes. To mitigate these risks, it‚Äôs crucial to follow ethical guidelines and adopt responsible usage practices. At present, the synthesized results contain visual artifacts that may help in detecting deepfakes. Please note that we do not assume any legal responsibility for the use of the results generated by this project.\n\n## Citation üíñ\nIf you find LivePortrait useful for your research, welcome to üåü this repo and cite our work using the following BibTeX:\n```bibtex\n@article{guo2024liveportrait,\n  title   = {LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control},\n  author  = {Guo, Jianzhu and Zhang, Dingyun and Liu, Xiaoqiang and Zhong, Zhizhou and Zhang, Yuan and Wan, Pengfei and Zhang, Di},\n  journal = {arXiv preprint arXiv:2407.03168},\n  year    = {2024}\n}\n```\n\n## Contact üìß\n[**Jianzhu Guo (ÈÉ≠Âª∫Áè†)**](https://guojianzhu.com); **guojianzhu1994@gmail.com**\n"
        },
        {
          "name": "readme_zh_cn.md",
          "type": "blob",
          "size": 17.5380859375,
          "content": "<h1 align=\"center\">LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</h1>\n\n<div align='center'>\n    <a href='https://github.com/cleardusk' target='_blank'><strong>ÈÉ≠Âª∫Áè†</strong></a><sup> 1‚Ä†</sup>&emsp;\n    <a href='https://github.com/Mystery099' target='_blank'><strong>Âº†‰∏ÅËä∏</strong></a><sup> 1,2</sup>&emsp;\n    <a href='https://github.com/KwaiVGI' target='_blank'><strong>ÂàòÊôìÂº∫</strong></a><sup> 1</sup>&emsp;\n    <a href='https://github.com/zzzweakman' target='_blank'><strong>ÈíüÊô∫Ëàü</strong></a><sup> 1,3</sup>&emsp;\n    <a href='https://scholar.google.com.hk/citations?user=_8k1ubAAAAAJ' target='_blank'><strong>Âº†Ê∏ä</strong></a><sup> 1</sup>&emsp;\n    <a href='https://scholar.google.com/citations?user=P6MraaYAAAAJ' target='_blank'><strong>‰∏áÈπèÈ£û</strong></a><sup> 1</sup>&emsp;\n    <a href='https://openreview.net/profile?id=~Di_ZHANG3' target='_blank'><strong>Âº†Ëø™</strong></a><sup> 1</sup>&emsp;\n</div>\n\n<div align='center'>\n    <sup>1 </sup>Âø´ÊâãÁßëÊäÄ&emsp; <sup>2 </sup>‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶&emsp; <sup>3 </sup>Â§çÊó¶Â§ßÂ≠¶&emsp;\n</div>\n<div align='center'>\n    <small><sup>‚Ä†</sup> ÈÄöËÆØ‰ΩúËÄÖÔºàProject LeadÔºâ</small>\n</div>\n\n<br>\n<div align=\"center\">\n  <!-- <a href='LICENSE'><img src='https://img.shields.io/badge/license-MIT-yellow'></a> -->\n  <a href='https://arxiv.org/pdf/2407.03168'><img src='https://img.shields.io/badge/arXiv-LivePortrait-red'></a>\n  <a href='https://liveportrait.github.io'><img src='https://img.shields.io/badge/Project-LivePortrait-green'></a>\n  <a href='https://huggingface.co/spaces/KwaiVGI/liveportrait'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a>\n  <a href=\"https://github.com/KwaiVGI/LivePortrait\"><img src=\"https://img.shields.io/github/stars/KwaiVGI/LivePortrait\"></a>\n  <br>\n  <a href=\"./readme.md\"><strong>English</strong></a> | <strong>ÁÆÄ‰Ωì‰∏≠Êñá</strong>\n</div>\n\n<br>\n\n\n<p align=\"center\">\n  <img src=\"./assets/docs/showcase2.gif\" alt=\"showcase\">\n  <br>\n  üî• Êõ¥Â§öÊïàÊûúÔºåËØ∑Êü•ÁúãÊàë‰ª¨ÁöÑ <a href=\"https://liveportrait.github.io/\"><strong>‰∏ªÈ°µ</strong></a> üî•\n</p>\n\n\n\n## üî• Êõ¥Êñ∞Êó•Âøó\n- **`2024/10/18`**Ôºö‚ùó Êàë‰ª¨Êõ¥Êñ∞‰∫Ü`transformers`Ôºå`gradio`Â∫ìÁöÑÁâàÊú¨ÈÅøÂÖçÂÆâÂÖ®ÊºèÊ¥ûÔºåÂÖ∑‰ΩìÊü•Áúã[ËøôÈáå](https://github.com/KwaiVGI/LivePortrait/pull/421/files).\n- **`2024/08/29`**Ôºöüì¶ Êàë‰ª¨Êõ¥Êñ∞‰∫ÜWindows[‰∏ÄÈîÆÂÆâË£ÖÁ®ãÂ∫è](https://huggingface.co/cleardusk/LivePortrait-Windows/blob/main/LivePortrait-Windows-v20240829.zip)Âπ∂ÊîØÊåÅËá™Âä®Êõ¥Êñ∞, ËØ¶ÊÉÖÂª∫[ËøôÈáå](https://huggingface.co/cleardusk/LivePortrait-Windows#20240829)„ÄÇ\n- **`2024/08/19`**ÔºöüñºÔ∏è Êàë‰ª¨ÊîØÊåÅ‰∫Ü**ÂõæÂÉèÈ©±Âä®Ê®°Âºè**Âíå**Âå∫ÂüüÊéßÂà∂**„ÄÇËØ¶ÊÉÖËØ∑ËßÅ[**ËøôÈáå**](./assets/docs/changelog/2024-08-19.md)„ÄÇ\n- **`2024/08/06`**Ôºöüé® Êàë‰ª¨Âú®GradioÁïåÈù¢ÊîØÊåÅ**Á≤æÁ°ÆÁöÑ‰∫∫ÂÉèÁºñËæë**, ÂèóÂà∞[ComfyUI-AdvancedLivePortrait](https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait)ÂêØÂèë„ÄÇËØ¶ËßÅ[**ËøôÈáå**](./assets/docs/changelog/2024-08-06.md)„ÄÇ\n- **`2024/08/05`**Ôºöüì¶ WindowsÁî®Êà∑Áé∞Âú®ÂèØ‰ª•‰∏ãËΩΩ[‰∏ÄÈîÆÂÆâË£ÖÁ®ãÂ∫è](https://huggingface.co/cleardusk/LivePortrait-Windows/blob/main/LivePortrait-Windows-v20240806.zip)ÔºåÊîØÊåÅ**‰∫∫Á±ªÊ®°Âºè**Âíå**Âä®Áâ©Ê®°Âºè**ÔºÅËØ¶ÊÉÖËßÅ[**ËøôÈáå**](./assets/docs/changelog/2024-08-05.md)„ÄÇ\n- **`2024/08/02`**Ôºöüò∏ Êàë‰ª¨ÂèëÂ∏É‰∫Ü**Âä®Áâ©Ê®°Âûã**ÁâàÊú¨Ôºå‰ª•ÂèäÂÖ∂‰ªñ‰∏Ä‰∫õÊõ¥Êñ∞ÂíåÊîπËøõ„ÄÇÊü•ÁúãËØ¶ÊÉÖ[**ËøôÈáå**](./assets/docs/changelog/2024-08-02.md)ÔºÅ\n- **`2024/07/25`**Ôºöüì¶ WindowsÁî®Êà∑Áé∞Âú®ÂèØ‰ª•‰ªé [HuggingFace](https://huggingface.co/cleardusk/LivePortrait-Windows/tree/main) Êàñ [ÁôæÂ∫¶‰∫ë](https://pan.baidu.com/s/1FWsWqKe0eNfXrwjEhhCqlw?pwd=86q2) ‰∏ãËΩΩËΩØ‰ª∂ÂåÖ„ÄÇËß£ÂéãÂπ∂ÂèåÂáª`run_windows.bat`Âç≥ÂèØ‰∫´ÂèóÔºÅ\n- **`2024/07/24`**Ôºöüé® Êàë‰ª¨Âú®GradioÁïåÈù¢ÊîØÊåÅÊ∫ê‰∫∫ÂÉèÁöÑÂßøÂäøÁºñËæë„ÄÇÊàë‰ª¨ËøòÈôç‰Ωé‰∫ÜÈªòËÆ§Ê£ÄÊµãÈòàÂÄº‰ª•Â¢ûÂä†Âè¨ÂõûÁéá„ÄÇ[Áé©ÂæóÂºÄÂøÉ](assets/docs/changelog/2024-07-24.md)ÔºÅ\n- **`2024/07/19`**Ôºö‚ú® Êàë‰ª¨ÊîØÊåÅüéûÔ∏è **‰∫∫ÂÉèËßÜÈ¢ëÁºñËæëÔºàaka v2vÔºâ**ÔºÅÊõ¥Â§ö‰ø°ÊÅØËßÅ[**ËøôÈáå**](assets/docs/changelog/2024-07-19.md)„ÄÇ\n- **`2024/07/17`**Ôºöüçé Êàë‰ª¨ÊîØÊåÅmacOSÊê≠ËΩΩApple SiliconÔºå‰øÆÊîπÊù•Ëá™ [jeethu](https://github.com/jeethu) ÁöÑPR [#143](https://github.com/KwaiVGI/LivePortrait/pull/143) „ÄÇ\n- **`2024/07/10`**Ôºöüí™Êàë‰ª¨ÊîØÊåÅÈü≥È¢ëÂíåËßÜÈ¢ëÊãºÊé•„ÄÅÈ©±Âä®ËßÜÈ¢ëËá™Âä®Ë£ÅÂâ™‰ª•ÂèäÂà∂‰ΩúÊ®°Êùø‰ª•‰øùÊä§ÈöêÁßÅ„ÄÇÊõ¥Â§ö‰ø°ÊÅØËßÅ[ËøôÈáå](assets/docs/changelog/2024-07-10.md)„ÄÇ\n- **`2024/07/09`**Ôºöü§ó Êàë‰ª¨ÂèëÂ∏É‰∫Ü[HuggingFace Space](https://huggingface.co/spaces/KwaiVGI/liveportrait)ÔºåÊÑüË∞¢HFÂõ¢ÈòüÂíå[Gradio](https://github.com/gradio-app/gradio)ÔºÅ\n- **`2024/07/04`**Ôºöüòä Êàë‰ª¨ÂèëÂ∏É‰∫ÜÂàùÂßãÁâàÊú¨ÁöÑÊé®ÁêÜ‰ª£Á†ÅÂíåÊ®°Âûã„ÄÇÊåÅÁª≠Êõ¥Êñ∞ÔºåÊï¨ËØ∑ÂÖ≥Ê≥®ÔºÅ\n- **`2024/07/04`**Ôºöüî• Êàë‰ª¨ÂèëÂ∏É‰∫Ü[‰∏ªÈ°µ](https://liveportrait.github.io)ÂíåÂú®[arXiv](https://arxiv.org/pdf/2407.03168)‰∏äÁöÑÊäÄÊúØÊä•Âëä„ÄÇ\n\n\n\n## ‰ªãÁªç üìñ\nÊ≠§‰ªìÂ∫ìÂêç‰∏∫**LivePortrait**ÔºåÂåÖÂê´Êàë‰ª¨ËÆ∫ÊñáÔºà[LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control](https://arxiv.org/pdf/2407.03168)ÔºâÁöÑÂÆòÊñπPyTorchÂÆûÁé∞„ÄÇ Êàë‰ª¨Ê≠£Âú®ÁßØÊûÅÊõ¥Êñ∞ÂíåÊîπËøõÊ≠§‰ªìÂ∫ì„ÄÇÂ¶ÇÊûúÊÇ®ÂèëÁé∞‰ªª‰ΩïÈîôËØØÊàñÊúâÂª∫ËÆÆÔºåÊ¨¢ËøéÊèêÂá∫ÈóÆÈ¢òÊàñÊèê‰∫§ÂêàÂπ∂ËØ∑Ê±Çüíñ„ÄÇ\n\n## ‰∏äÊâãÊåáÂçó üèÅ\n### 1. ÂÖãÈöÜ‰ª£Á†ÅÂíåÂÆâË£ÖËøêË°åÁéØÂ¢É üõ†Ô∏è\n\n> [!Note]\n> Á°Æ‰øùÊÇ®ÁöÑÁ≥ªÁªüÂ∑≤ÂÆâË£Ö[`git`](https://git-scm.com/)„ÄÅ[`conda`](https://anaconda.org/anaconda/conda)Âíå[`FFmpeg`](https://ffmpeg.org/download.html)„ÄÇÊúâÂÖ≥FFmpegÂÆâË£ÖÁöÑËØ¶ÁªÜ‰ø°ÊÅØÔºåËßÅ[**Â¶Ç‰ΩïÂÆâË£ÖFFmpeg**](assets/docs/how-to-install-ffmpeg.md)„ÄÇ\n\n```bash\ngit clone https://github.com/KwaiVGI/LivePortrait\ncd LivePortrait\n\n# ‰ΩøÁî®condaÂàõÂª∫ÁéØÂ¢É\nconda create -n LivePortrait python=3.10\nconda activate LivePortrait\n```\n\n#### ÂØπ‰∫éLinuxÊàñWindowsÁî®Êà∑\n\n[X-Pose](https://github.com/IDEA-Research/X-Pose)ÈúÄË¶ÅÊÇ®ÁöÑ`torch`ÁâàÊú¨‰∏éCUDAÁâàÊú¨ÂÖºÂÆπ„ÄÇ\n\nÈ¶ñÂÖàÔºåÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§Ê£ÄÊü•ÊÇ®ÂΩìÂâçÁöÑCUDAÁâàÊú¨Ôºö\n\n```bash\nnvcc -V # example versions: 11.1, 11.8, 12.1, etc.\n```\n\nÁÑ∂ÂêéÔºåÂÆâË£ÖÁõ∏Â∫îÁâàÊú¨ÁöÑtorch„ÄÇ‰ª•‰∏ãÊòØ‰∏çÂêåCUDAÁâàÊú¨ÁöÑÁ§∫‰æã„ÄÇÂ¶ÇÊûúÊÇ®ÁöÑCUDAÁâàÊú¨Êú™ÂàóÂá∫ÔºåËØ∑ËÆøÈóÆ[PyTorchÂÆòÊñπÁΩëÁ´ô](https://pytorch.org/get-started/previous-versions)Ëé∑ÂèñÂÆâË£ÖÂëΩ‰ª§Ôºö\n```bash\n# for CUDA 11.1\npip install torch==1.10.1+cu111 torchvision==0.11.2 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n# for CUDA 11.8\npip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118\n# for CUDA 12.1\npip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n# ...\n```\n\n**Ê≥®ÊÑè**ÔºöÂú®WindowsÁ≥ªÁªü‰∏äÔºå‰∏Ä‰∫õËøáÈ´òÁâàÊú¨ÁöÑCUDA(12.4„ÄÅ12.6Á≠â)ÂèØËÉΩ‰ºöÂØºËá¥Êú™Áü•ÁöÑÈóÆÈ¢òÔºåÊÇ®ÂèØ‰ª•ËÄÉËôëÈôç‰ΩéÊÇ®ÁöÑCUDAÁâàÊú¨Âà∞11.8ÔºåËøôÊòØÊàë‰ª¨ÊµãËØïÁöÑ‰∏Ä‰∏™ËæÉ‰∏∫Á®≥ÂÆöÁöÑÁâàÊú¨„ÄÇÈôçÁ∫ßÊñπÊ≥ïÂèØ‰ª•ÂèÇËÄÉ [@dimitribarbot](https://github.com/dimitribarbot)\nÊèê‰æõÁöÑ[ÊñáÊ°£](https://github.com/dimitribarbot/sd-webui-live-portrait/blob/main/assets/docs/how-to-install-xpose.md#cuda-toolkit-118).\n\nÊúÄÂêéÔºåÂÆâË£ÖÂÖ∂‰Ωô‰æùËµñÈ°πÔºö\n\n```bash\npip install -r requirements.txt\n```\n\n#### ÂØπ‰∫éÊê≠ËΩΩApple SiliconÁöÑmacOSÁî®Êà∑\n\n[X-Pose](https://github.com/IDEA-Research/X-Pose)‰æùËµñÈ°π‰∏çÊîØÊåÅmacOSÔºåÂõ†Ê≠§ÊÇ®ÂèØ‰ª•Ë∑≥ËøáÂÖ∂ÂÆâË£Ö„ÄÇ‰∫∫Á±ªÊ®°ÂºèÁÖßÂ∏∏Â∑•‰ΩúÔºå‰ΩÜ‰∏çÊîØÊåÅÂä®Áâ©Ê®°Âºè„ÄÇ‰ΩøÁî®‰∏∫Êê≠ËΩΩApple SiliconÁöÑmacOSÊèê‰æõÁöÑrequirementsÊñá‰ª∂Ôºö\n\n```bash\n# ÂØπ‰∫éÊê≠ËΩΩApple SiliconÁöÑmacOSÁî®Êà∑\npip install -r requirements_macOS.txt\n```\n\n### 2. ‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáç(Pretrained weights) üì•\n\n‰ªéHuggingFace‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÁöÑÊúÄÁÆÄÂçïÊñπÊ≥ïÊòØÔºö\n```bash\n# !pip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude \"*.git*\" \"README.md\" \"docs\"\n```\n\nËã•ÊÇ®‰∏çËÉΩËÆøÈóÆHuggingFaceÂπ≥Âè∞Ôºå‰Ω†ÂèØ‰ª•ËÆøÈóÆÂÖ∂ÈïúÂÉèÁΩëÁ´ô[hf-mirror](https://hf-mirror.com/)ËøõË°å‰∏ãËΩΩÊìç‰ΩúÔºö\n\n```bash\n# !pip install -U \"huggingface_hub[cli]\"\nexport HF_ENDPOINT=https://hf-mirror.com\nhuggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude \"*.git*\" \"README.md\" \"docs\"\n```\n\nÊàñËÄÖÔºåÊÇ®ÂèØ‰ª•‰ªé[Google Drive](https://drive.google.com/drive/folders/1UtKgzKjFAOmZkhNK-OYT0caJ_w2XAnib)Êàñ[ÁôæÂ∫¶‰∫ë](https://pan.baidu.com/s/1MGctWmNla_vZxDbEp2Dtzw?pwd=z5cn)ÔºàËøõË°å‰∏≠Ôºâ‰∏ãËΩΩÊâÄÊúâÈ¢ÑËÆ≠ÁªÉÊùÉÈáç„ÄÇËß£ÂéãÂπ∂Â∞ÜÂÆÉ‰ª¨ÊîæÁΩÆÂú®`./pretrained_weights`ÁõÆÂΩï‰∏ã„ÄÇ\n\nÁ°Æ‰øùÁõÆÂΩïÁªìÊûÑÂ¶ÇÊâÄÁ§∫ÂåÖÂê´[**Êú¨‰ªìÂ∫ìËØ•Ë∑ØÂæÑ**](assets/docs/directory-structure.md)ÂÖ∂‰∏≠Â±ïÁ§∫ÁöÑÂÜÖÂÆπ„ÄÇ\n\n### 3. Êé®ÁêÜ üöÄ\n\n#### Âø´ÈÄü‰∏äÊâãÔºà‰∫∫Á±ªÊ®°ÂûãÔºâüë§\n\n```bash\n# ÂØπ‰∫éLinuxÂíåWindowsÁî®Êà∑\npython inference.py\n\n# ÂØπ‰∫éÊê≠ËΩΩApple SiliconÁöÑmacOSÁî®Êà∑ÔºàIntelÊú™ÊµãËØïÔºâ„ÄÇÊ≥®ÊÑèÔºöËøôÂèØËÉΩÊØîRTX 4090ÊÖ¢20ÂÄç\nPYTORCH_ENABLE_MPS_FALLBACK=1 python inference.py\n```\n\nÂ¶ÇÊûúËÑöÊú¨ÊàêÂäüËøêË°åÔºåÊÇ®Â∞ÜÂæóÂà∞‰∏Ä‰∏™Âêç‰∏∫`animations/s6--d0_concat.mp4`ÁöÑËæìÂá∫mp4Êñá‰ª∂„ÄÇÊ≠§Êñá‰ª∂ÂåÖÂê´‰ª•‰∏ãÁªìÊûúÔºöÈ©±Âä®ËßÜÈ¢ë„ÄÅËæìÂÖ•ÂõæÂÉèÊàñËßÜÈ¢ë‰ª•ÂèäÁîüÊàêÁªìÊûú„ÄÇ\n\n<p align=\"center\">\n  <img src=\"./assets/docs/inference.gif\" alt=\"image\">\n</p>\nÊàñËÄÖÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáÊåáÂÆö`-s`Âíå`-d`ÂèÇÊï∞Êù•Êõ¥ÊîπËæìÂÖ•Ôºö\n\n```bash\n# Ê∫êËæìÂÖ•ÊòØÂõæÂÉè\npython inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d0.mp4\n\n# Ê∫êËæìÂÖ•ÊòØËßÜÈ¢ë ‚ú®\npython inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d0.mp4\n\n# Êõ¥Â§öÈÄâÈ°πËØ∑ËßÅ\npython inference.py -h\n```\n\n#### Âø´ÈÄü‰∏äÊâãÔºàÂä®Áâ©Ê®°ÂûãÔºâ üê±üê∂\n\nÂä®Áâ©Ê®°Âºè‰ªÖÂú®LinuxÂíåWindows‰∏äÁªèËøáÊµãËØïÔºåÂπ∂‰∏îÈúÄË¶ÅNVIDIA GPU„ÄÇ\n\nÊÇ®ÈúÄË¶ÅÈ¶ñÂÖàÊûÑÂª∫‰∏Ä‰∏™Âêç‰∏∫`MultiScaleDeformableAttention`ÁöÑOPÔºåËØ•OPÁî±[X-Pose](https://github.com/IDEA-Research/X-Pose)‰ΩøÁî®ÔºåËøôÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÊ°ÜÊû∂„ÄÇ\n\n```bash\ncd src/utils/dependencies/XPose/models/UniPose/ops\npython setup.py build install\ncd - # Á≠âÂêå‰∫é cd ../../../../../../../\n```\n\nÁÑ∂ÂêéÊâßË°å\n```bash\npython inference_animals.py -s assets/examples/source/s39.jpg -d assets/examples/driving/wink.pkl --driving_multiplier 1.75 --no_flag_stitching\n```\nÂ¶ÇÊûúËÑöÊú¨ÊàêÂäüËøêË°åÔºåÊÇ®Â∞ÜÂæóÂà∞‰∏Ä‰∏™Âêç‰∏∫`animations/s39--wink_concat.mp4`ÁöÑËæìÂá∫mp4Êñá‰ª∂„ÄÇ\n<p align=\"center\">\n  <img src=\"./assets/docs/inference-animals.gif\" alt=\"image\">\n</p>\n\n#### È©±Âä®ËßÜÈ¢ëËá™Âä®Ë£ÅÂâ™ üì¢üì¢üì¢\n\n> [!IMPORTANT]\n> ‰ΩøÁî®ÊÇ®Ëá™Â∑±ÁöÑÈ©±Âä®ËßÜÈ¢ëÊó∂ÔºåÊàë‰ª¨**Êé®Ëçê**Ôºö ‚¨áÔ∏è\n>\n> - Â∞ÜÂÖ∂Ë£ÅÂâ™‰∏∫**1:1**ÁöÑÂÆΩÈ´òÊØîÔºà‰æãÂ¶ÇÔºå512x512Êàñ256x256ÂÉèÁ¥†ÔºâÔºåÊàñÈÄöËøá`--flag_crop_driving_video`ÂêØÁî®Ëá™Âä®Ë£ÅÂâ™„ÄÇ\n> - ‰∏ìÊ≥®‰∫éÂ§¥ÈÉ®Âå∫ÂüüÔºåÁ±ª‰ºº‰∫éÁ§∫‰æãËßÜÈ¢ë„ÄÇ\n> - ÊúÄÂ∞èÂåñËÇ©ÈÉ®ËøêÂä®„ÄÇ\n> - Á°Æ‰øùÈ©±Âä®ËßÜÈ¢ëÁöÑÁ¨¨‰∏ÄÂ∏ßÊòØÂÖ∑Êúâ**‰∏≠ÊÄßË°®ÊÉÖ**ÁöÑÊ≠£Èù¢Èù¢ÈÉ®„ÄÇ\n\n‰ª•‰∏ãÊòØÈÄöËøá`--flag_crop_driving_video`Ëá™Âä®Ë£ÅÂâ™ÁöÑÁ§∫‰æãÔºö\n\n```bash\npython inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d13.mp4 --flag_crop_driving_video\n```\n\nÂ¶ÇÊûúËá™Âä®Ë£ÅÂâ™ÁöÑÁªìÊûú‰∏çÁêÜÊÉ≥ÔºåÊÇ®ÂèØ‰ª•‰øÆÊîπ`--scale_crop_driving_video`„ÄÅ`--vy_ratio_crop_driving_video`ÈÄâÈ°πÊù•Ë∞ÉÊï¥ÊØî‰æãÂíåÂÅèÁßªÔºåÊàñËÄÖÊâãÂä®ËøõË°åË∞ÉÊï¥„ÄÇ\n\n#### Âä®‰ΩúÊ®°ÊùøÂà∂‰Ωú\n\nÊÇ®‰πüÂèØ‰ª•‰ΩøÁî®‰ª•`.pkl`ÁªìÂ∞æÁöÑËá™Âä®ÁîüÊàêÁöÑÂä®‰ΩúÊ®°ÊùøÊñá‰ª∂Êù•Âä†Âø´Êé®ÁêÜÈÄüÂ∫¶ÔºåÂπ∂**‰øùÊä§ÈöêÁßÅ**Ôºå‰æãÂ¶ÇÔºö\n```bash\npython inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d5.pkl # ‰∫∫ÂÉèÂä®Áîª\npython inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d5.pkl # ‰∫∫ÂÉèËßÜÈ¢ëÁºñËæë\n```\n\n### 4. Gradio ÁïåÈù¢ ü§ó\n\nÊàë‰ª¨ËøòÊèê‰æõ‰∫ÜGradioÁïåÈù¢ <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>Ôºå‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑ‰ΩìÈ™åÔºåÂè™ÈúÄËøêË°åÔºö\n\n```bash\n# ÂØπ‰∫éLinuxÂíåWindowsÁî®Êà∑Ôºà‰ª•ÂèäÊê≠ËΩΩIntelÁöÑmacOSÔºüÔºüÔºâ\npython app.py # ‰∫∫Á±ªÊ®°ÂûãÊ®°Âºè\n\n# ÂØπ‰∫éÊê≠ËΩΩApple SiliconÁöÑmacOSÁî®Êà∑Ôºå‰∏çÊîØÊåÅIntelÔºåËøôÂèØËÉΩÊØîRTX 4090ÊÖ¢20ÂÄç\nPYTORCH_ENABLE_MPS_FALLBACK=1 python app.py # ‰∫∫Á±ªÊ®°ÂûãÊ®°Âºè\n```\n\nÊàë‰ª¨Ëøò‰∏∫Âä®Áâ©Ê®°ÂºèÊèê‰æõ‰∫ÜGradioÁïåÈù¢ÔºåËøô‰ªÖÂú®Linux‰∏äÁªèËøáNVIDIA GPUÊµãËØïÔºö\n```bash\npython app_animals.py # animals mode üê±üê∂\n```\n\nÊÇ®ÂèØ‰ª•ÊåáÂÆö`--server_port`„ÄÅ`--share`„ÄÅ`--server_name`ÂèÇÊï∞‰ª•Êª°Ë∂≥ÊÇ®ÁöÑÈúÄÊ±ÇÔºÅ\n\nüöÄÊàë‰ª¨ËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™Âä†ÈÄüÈÄâÈ°π`--flag_do_torch_compile`„ÄÇÁ¨¨‰∏ÄÊ¨°Êé®ÁêÜËß¶Âèë‰ºòÂåñËøáÁ®ãÔºàÁ∫¶‰∏ÄÂàÜÈíüÔºâÔºå‰ΩøÂêéÁª≠Êé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò20-30%„ÄÇ‰∏çÂêåCUDAÁâàÊú¨ÁöÑÊÄßËÉΩÊèêÂçáÂèØËÉΩÊúâÊâÄ‰∏çÂêå„ÄÇ\n\n```bash\n# ÂêØÁî®torch.compile‰ª•ËøõË°åÊõ¥Âø´ÁöÑÊé®ÁêÜ\npython app.py --flag_do_torch_compile\n```\n**Ê≥®ÊÑè**ÔºöÊ≠§ÊñπÊ≥ïÂú®WindowsÂíåmacOS‰∏ä‰∏çÂèóÊîØÊåÅ„ÄÇ\n\n**ÊàñËÄÖÔºåÂú®[HuggingFace](https://huggingface.co/spaces/KwaiVGI/LivePortrait)‰∏äËΩªÊùæÂ∞ùËØï**ü§ó„ÄÇ\n\n### 5. Êé®ÁêÜÈÄüÂ∫¶È¢Ñ‰º∞ üöÄüöÄüöÄ\nÊàë‰ª¨ËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™ËÑöÊú¨Êù•ËØÑ‰º∞ÊØè‰∏™Ê®°ÂùóÁöÑÊé®ÁêÜÈÄüÂ∫¶Ôºö\n\n```bash\n# ÂØπ‰∫éNVIDIA GPU\npython speed.py\n```\n\nÁªìÊûúÂú®[**Êú¨‰ªìÂ∫ìËØ•Êñá‰ª∂Â±ïÁ§∫**](./assets/docs/speed.md).\n\n## Á§æÂå∫ËµÑÊ∫ê ü§ó\n\n### Á§æÂå∫È°πÁõÆ\n\n| ‰ªìÂ∫ì (ÊåâÊó∂Èó¥ÊéíÂ∫è) | ÊèèËø∞ | ‰ΩúËÄÖ |\n|------|------|--------|\n| [**AdvancedLivePortrait-WebUI**](https://github.com/jhj0517/AdvancedLivePortrait-WebUI) | Dedicated gradio based WebUI started from [ComfyUI-AdvancedLivePortrait](https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait) | [@jhj0517](https://github.com/jhj0517) |\n| [**FacePoke**](https://github.com/jbilcke-hf/FacePoke) | ‰∏Ä‰∏™ÂÆûÊó∂ÁöÑÂ§¥ÈÉ®ÂßøÊÄÅË°®ÊÉÖÊéßÂà∂Â∫îÁî®ÔºåÈÄöËøáÈº†Ê†áÊéßÂà∂ÔºÅ | [@jbilcke-hf](https://github.com/jbilcke-hf) |\n| [**FaceFusion**](https://github.com/facefusion/facefusion) | FaceFusion 3.0 ÈõÜÊàê‰∫Ü LivePortrait ‰Ωú‰∏∫ `expression_restorer` Âíå `face_editor` Â§ÑÁêÜÂô®„ÄÇ | [@henryruhs](https://github.com/henryruhs) |\n| [**sd-webui-live-portrait**](https://github.com/dimitribarbot/sd-webui-live-portrait) | LivePortrait ÁöÑ WebUI Êâ©Â±ïÔºåÂú®ÂéüÁâà Stable Diffusion WebUI ‰∏≠Ê∑ªÂä†‰∫Ü‰∏Ä‰∏™Ê†áÁ≠æ‰ª•‰ΩøÁî® LivePortrait ÁöÑÂäüËÉΩ„ÄÇ | [@dimitribarbot](https://github.com/dimitribarbot) |\n| [**ComfyUI-LivePortraitKJ**](https://github.com/kijai/ComfyUI-LivePortraitKJ) | ‰∏Ä‰∏™Áî®‰∫é LivePortrait ÁöÑ ComfyUI ËäÇÁÇπÔºå‰ΩøÁî® MediaPipe ‰Ωú‰∏∫ Insightface ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ | [@kijai](https://github.com/kijai) |\n| [**ComfyUI-AdvancedLivePortrait**](https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait) | ‰∏Ä‰∏™Êõ¥Âø´ÁöÑ ComfyUI ËäÇÁÇπÔºåÂÖ∑ÊúâÂÆûÊó∂È¢ÑËßàÂäüËÉΩÔºåÂêØÂèë‰∫ÜËÆ∏Â§öÁ§æÂå∫ÂºÄÂèëÁöÑÂ∑•ÂÖ∑ÂíåÈ°πÁõÆ„ÄÇ | [@PowerHouseMan](https://github.com/PowerHouseMan) |\n| [**comfyui-liveportrait**](https://github.com/shadowcz007/comfyui-liveportrait) | ‰∏Ä‰∏™Áî®‰∫é LivePortrait ÁöÑ ComfyUI ËäÇÁÇπÔºåÊîØÊåÅÂ§öÈù¢ÈÉ®„ÄÅË°®ÊÉÖÊèíÂÄºÁ≠âÂäüËÉΩÔºåÂπ∂Êúâ[ÊïôÁ®ã](https://www.bilibili.com/video/BV1JW421R7sP)„ÄÇ | [@shadowcz007](https://github.com/shadowcz007) |\n\n### Playgrounds, ü§ó HuggingFace Spaces ‰ª•ÂèäÂÖ∂ÂÆÉ\n- [FacePoke Space](https://huggingface.co/spaces/jbilcke-hf/FacePoke)\n- [Expression Editor Space](https://huggingface.co/spaces/fffiloni/expression-editor)\n- [Expression Editor Replicate](https://replicate.com/fofr/expression-editor)\n- [Face Control Realtime Demo](https://fal.ai/demos/face-control) on FAL\n- [Replicate Playground](https://replicate.com/fofr/live-portrait)\n- Nuke ÂèØ‰ª•ÈÄöËøá CompyUI ËäÇÁÇπ‰ΩøÁî® LivePortraitÔºåËØ¶ÊÉÖËßÅ[ËøôÈáå](https://x.com/bilawalsidhu/status/1837349806475276338)\n- LivePortrait Âú® [Poe](https://poe.com/LivePortrait) ‰∏äËøêË°å\n\n### ËßÜÈ¢ëÊïôÁ®ã\n- [LivePortrait ËßÜÈ¢ëËΩ¨ËßÜÈ¢ëÁöÑÂ∑•‰ΩúÊµÅÁ®ã](https://youtu.be/xfzK_6cTs58?si=aYjgypeJBkhc46VL) Áî± [@curiousrefuge](https://www.youtube.com/@curiousrefuge) Âà∂‰Ωú\n- [Google Colab ÊïôÁ®ã](https://youtu.be/59Y9ePAXTp0?si=KzEWhklBlporW7D8) Áî± [@Planet Ai](https://www.youtube.com/@planetai217) Âà∂‰Ωú\n- [ËÆ∫ÊñáËß£ËØª](https://youtu.be/fD0P6UWSu8I?si=Vn5wxUa8qSu1jv4l) Áî± [@TwoMinutePapers](https://www.youtube.com/@TwoMinutePapers) Âà∂‰Ωú\n- [ComfyUI È´òÁ∫ß LivePortrait ÊïôÁ®ã](https://youtu.be/q0Vf-ZZsbzI?si=nbs3npleH-dVCt28) Áî± [TutoView](https://www.youtube.com/@TutoView) Âà∂‰Ωú\n- [LivePortrait Êé¢Á¥¢](https://www.youtube.com/watch?v=vsvlbTEqgXQ) Âíå [LivePortrait Ê∑±ÂÖ•Êé¢ËÆ®](https://youtu.be/cucaEEDYmsw?si=AtPaDWc5G-a4E8dD) Áî± [TheoreticallyMedia](https://www.youtube.com/@TheoreticallyMedia) Âà∂‰Ωú\n- [LivePortrait ÂÆûÊàòÊïôÁ®ã](https://www.youtube.com/watch?v=uyjSTAOY7yI) Áî± [@AI Search](https://www.youtube.com/@theAIsearch) Âà∂‰Ωú\n- [ComfyUI ÊïôÁ®ã](https://www.youtube.com/watch?v=8-IcDDmiUMM) Áî± [@Sebastian Kamph](https://www.youtube.com/@sebastiankamph) Âà∂‰Ωú\n- B Á´ô‰∏äÁöÑ[ÊïôÁ®ã](https://www.bilibili.com/video/BV1cf421i7Ly)\n\nËøòÊúâÊù•Ëá™Á§æÂå∫ÁöÑÊó†Êï∞‰ª§‰∫∫ÊÉäÂèπÁöÑË¥°ÁåÆÔºåÊú™ËÉΩ‰∏Ä‰∏ÄÂàó‰∏æ üíñ\n\n## Ëá¥Ë∞¢ üíê\n\nÊàë‰ª¨Ë¶ÅÊÑüË∞¢[FOMM](https://github.com/AliaksandrSiarohin/first-order-model)„ÄÅ[Open Facevid2vid](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis)„ÄÅ[SPADE](https://github.com/NVlabs/SPADE)„ÄÅ[InsightFace](https://github.com/deepinsight/insightface)Âíå[X-Pose](https://github.com/IDEA-Research/X-Pose)‰ªìÂ∫ìÁöÑÁöÑË¥°ÁåÆËÄÖÔºåÊÑüË∞¢‰ªñ‰ª¨ÁöÑÂºÄÊîæÁ†îÁ©∂ÂíåË¥°ÁåÆ„ÄÇ\n\n## ÈÅìÂæ∑ËÄÉÈáè üõ°Ô∏è\nËÇñÂÉèÂä®ÁîªÊäÄÊúØ‰º¥ÈöèÁùÄÁ§æ‰ºöÈ£éÈô©ÔºåÁâπÂà´ÊòØÂú®ÂàõÂª∫Ê∑±Â∫¶‰º™ÈÄ†ÔºàdeepfakesÔºâÊó∂ÂèØËÉΩË¢´Êª•Áî®„ÄÇ‰∏∫‰∫ÜÂáèËΩªËøô‰∫õÈ£éÈô©ÔºåÈÅµÂæ™ÈÅìÂæ∑ÊåáÂçóÂπ∂ÈááÂèñË¥üË¥£‰ªªÁöÑ‰ΩøÁî®ÂÆûË∑µËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁõÆÂâçÔºåÁîüÊàêÁöÑÁªìÊûúÂåÖÂê´‰∏Ä‰∫õËßÜËßâ‰º™ÂΩ±ÔºåËøô‰∫õ‰º™ÂΩ±ÂèØËÉΩÊúâÂä©‰∫éÊ£ÄÊµãÊ∑±Â∫¶‰º™ÈÄ†„ÄÇËØ∑Ê≥®ÊÑèÔºåÊàë‰ª¨‰∏çÂØπÊú¨È°πÁõÆÁîüÊàêÁöÑÁªìÊûúÁöÑ‰ΩøÁî®ÊâøÊãÖ‰ªª‰ΩïÊ≥ïÂæãË¥£‰ªª„ÄÇ\n\n## ÂºïÁî® üíñ\n\nÂ¶ÇÊûúÊÇ®ÂèëÁé∞LivePortraitÂØπÊÇ®ÁöÑÁ†îÁ©∂ÊúâÁî®ÔºåÊ¨¢ËøéÂºïÁî®Êàë‰ª¨ÁöÑÂ∑•‰ΩúÔºå‰ΩøÁî®‰ª•‰∏ãBibTeXÔºö\n\n```bibtex\n@article{guo2024liveportrait,\n  title   = {LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control},\n  author  = {Guo, Jianzhu and Zhang, Dingyun and Liu, Xiaoqiang and Zhong, Zhizhou and Zhang, Yuan and Wan, Pengfei and Zhang, Di},\n  journal = {arXiv preprint arXiv:2407.03168},\n  year    = {2024}\n}\n```\n\n## ËÅîÁ≥ªÊñπÂºè üìß\n\n[**Jianzhu Guo (ÈÉ≠Âª∫Áè†)**](https://guojianzhu.com); **guojianzhu1994@gmail.com**Ôºõ\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0693359375,
          "content": "-r requirements_base.txt\n\nonnxruntime-gpu==1.18.0\ntransformers==4.38.0\n"
        },
        {
          "name": "requirements_base.txt",
          "type": "blob",
          "size": 0.2880859375,
          "content": "numpy==1.26.4\npyyaml==6.0.1\nopencv-python==4.10.0.84\nscipy==1.13.1\nimageio==2.34.2\nlmdb==1.4.1\ntqdm==4.66.4\nrich==13.7.1\nffmpeg-python==0.2.0\nonnx==1.16.1\nscikit-image==0.24.0\nalbumentations==1.4.10\nmatplotlib==3.9.0\nimageio-ffmpeg==0.5.1\ntyro==0.8.5\ngradio==5.1.0\npykalman==0.9.7\npillow>=10.2.0"
        },
        {
          "name": "requirements_macOS.txt",
          "type": "blob",
          "size": 0.15625,
          "content": "-r requirements_base.txt\n\n--extra-index-url https://download.pytorch.org/whl/cpu\ntorch==2.3.0\ntorchvision==0.18.0\ntorchaudio==2.3.0\nonnxruntime-silicon==1.16.3\n"
        },
        {
          "name": "speed.py",
          "type": "blob",
          "size": 7.626953125,
          "content": "# coding: utf-8\n\n\"\"\"\nBenchmark the inference speed of each module in LivePortrait.\n\nTODO: heavy GPT style, need to refactor\n\"\"\"\n\nimport torch\ntorch._dynamo.config.suppress_errors = True  # Suppress errors and fall back to eager execution\n\nimport yaml\nimport time\nimport numpy as np\n\nfrom src.utils.helper import load_model, concat_feat\nfrom src.config.inference_config import InferenceConfig\n\n\ndef initialize_inputs(batch_size=1, device_id=0):\n    \"\"\"\n    Generate random input tensors and move them to GPU\n    \"\"\"\n    feature_3d = torch.randn(batch_size, 32, 16, 64, 64).to(device_id).half()\n    kp_source = torch.randn(batch_size, 21, 3).to(device_id).half()\n    kp_driving = torch.randn(batch_size, 21, 3).to(device_id).half()\n    source_image = torch.randn(batch_size, 3, 256, 256).to(device_id).half()\n    generator_input = torch.randn(batch_size, 256, 64, 64).to(device_id).half()\n    eye_close_ratio = torch.randn(batch_size, 3).to(device_id).half()\n    lip_close_ratio = torch.randn(batch_size, 2).to(device_id).half()\n    feat_stitching = concat_feat(kp_source, kp_driving).half()\n    feat_eye = concat_feat(kp_source, eye_close_ratio).half()\n    feat_lip = concat_feat(kp_source, lip_close_ratio).half()\n\n    inputs = {\n        'feature_3d': feature_3d,\n        'kp_source': kp_source,\n        'kp_driving': kp_driving,\n        'source_image': source_image,\n        'generator_input': generator_input,\n        'feat_stitching': feat_stitching,\n        'feat_eye': feat_eye,\n        'feat_lip': feat_lip\n    }\n\n    return inputs\n\n\ndef load_and_compile_models(cfg, model_config):\n    \"\"\"\n    Load and compile models for inference\n    \"\"\"\n    appearance_feature_extractor = load_model(cfg.checkpoint_F, model_config, cfg.device_id, 'appearance_feature_extractor')\n    motion_extractor = load_model(cfg.checkpoint_M, model_config, cfg.device_id, 'motion_extractor')\n    warping_module = load_model(cfg.checkpoint_W, model_config, cfg.device_id, 'warping_module')\n    spade_generator = load_model(cfg.checkpoint_G, model_config, cfg.device_id, 'spade_generator')\n    stitching_retargeting_module = load_model(cfg.checkpoint_S, model_config, cfg.device_id, 'stitching_retargeting_module')\n\n    models_with_params = [\n        ('Appearance Feature Extractor', appearance_feature_extractor),\n        ('Motion Extractor', motion_extractor),\n        ('Warping Network', warping_module),\n        ('SPADE Decoder', spade_generator)\n    ]\n\n    compiled_models = {}\n    for name, model in models_with_params:\n        model = model.half()\n        model = torch.compile(model, mode='max-autotune')  # Optimize for inference\n        model.eval()  # Switch to evaluation mode\n        compiled_models[name] = model\n\n    retargeting_models = ['stitching', 'eye', 'lip']\n    for retarget in retargeting_models:\n        module = stitching_retargeting_module[retarget].half()\n        module = torch.compile(module, mode='max-autotune')  # Optimize for inference\n        module.eval()  # Switch to evaluation mode\n        stitching_retargeting_module[retarget] = module\n\n    return compiled_models, stitching_retargeting_module\n\n\ndef warm_up_models(compiled_models, stitching_retargeting_module, inputs):\n    \"\"\"\n    Warm up models to prepare them for benchmarking\n    \"\"\"\n    print(\"Warm up start!\")\n    with torch.no_grad():\n        for _ in range(10):\n            compiled_models['Appearance Feature Extractor'](inputs['source_image'])\n            compiled_models['Motion Extractor'](inputs['source_image'])\n            compiled_models['Warping Network'](inputs['feature_3d'], inputs['kp_driving'], inputs['kp_source'])\n            compiled_models['SPADE Decoder'](inputs['generator_input'])  # Adjust input as required\n            stitching_retargeting_module['stitching'](inputs['feat_stitching'])\n            stitching_retargeting_module['eye'](inputs['feat_eye'])\n            stitching_retargeting_module['lip'](inputs['feat_lip'])\n    print(\"Warm up end!\")\n\n\ndef measure_inference_times(compiled_models, stitching_retargeting_module, inputs):\n    \"\"\"\n    Measure inference times for each model\n    \"\"\"\n    times = {name: [] for name in compiled_models.keys()}\n    times['Stitching and Retargeting Modules'] = []\n\n    overall_times = []\n\n    with torch.no_grad():\n        for _ in range(100):\n            torch.cuda.synchronize()\n            overall_start = time.time()\n\n            start = time.time()\n            compiled_models['Appearance Feature Extractor'](inputs['source_image'])\n            torch.cuda.synchronize()\n            times['Appearance Feature Extractor'].append(time.time() - start)\n\n            start = time.time()\n            compiled_models['Motion Extractor'](inputs['source_image'])\n            torch.cuda.synchronize()\n            times['Motion Extractor'].append(time.time() - start)\n\n            start = time.time()\n            compiled_models['Warping Network'](inputs['feature_3d'], inputs['kp_driving'], inputs['kp_source'])\n            torch.cuda.synchronize()\n            times['Warping Network'].append(time.time() - start)\n\n            start = time.time()\n            compiled_models['SPADE Decoder'](inputs['generator_input'])  # Adjust input as required\n            torch.cuda.synchronize()\n            times['SPADE Decoder'].append(time.time() - start)\n\n            start = time.time()\n            stitching_retargeting_module['stitching'](inputs['feat_stitching'])\n            stitching_retargeting_module['eye'](inputs['feat_eye'])\n            stitching_retargeting_module['lip'](inputs['feat_lip'])\n            torch.cuda.synchronize()\n            times['Stitching and Retargeting Modules'].append(time.time() - start)\n\n            overall_times.append(time.time() - overall_start)\n\n    return times, overall_times\n\n\ndef print_benchmark_results(compiled_models, stitching_retargeting_module, retargeting_models, times, overall_times):\n    \"\"\"\n    Print benchmark results with average and standard deviation of inference times\n    \"\"\"\n    average_times = {name: np.mean(times[name]) * 1000 for name in times.keys()}\n    std_times = {name: np.std(times[name]) * 1000 for name in times.keys()}\n\n    for name, model in compiled_models.items():\n        num_params = sum(p.numel() for p in model.parameters())\n        num_params_in_millions = num_params / 1e6\n        print(f\"Number of parameters for {name}: {num_params_in_millions:.2f} M\")\n\n    for index, retarget in enumerate(retargeting_models):\n        num_params = sum(p.numel() for p in stitching_retargeting_module[retarget].parameters())\n        num_params_in_millions = num_params / 1e6\n        print(f\"Number of parameters for part_{index} in Stitching and Retargeting Modules: {num_params_in_millions:.2f} M\")\n\n    for name, avg_time in average_times.items():\n        std_time = std_times[name]\n        print(f\"Average inference time for {name} over 100 runs: {avg_time:.2f} ms (std: {std_time:.2f} ms)\")\n\n\ndef main():\n    \"\"\"\n    Main function to benchmark speed and model parameters\n    \"\"\"\n    # Load configuration\n    cfg = InferenceConfig()\n    model_config_path = cfg.models_config\n    with open(model_config_path, 'r') as file:\n        model_config = yaml.safe_load(file)\n\n    # Sample input tensors\n    inputs = initialize_inputs(device_id = cfg.device_id)\n\n    # Load and compile models\n    compiled_models, stitching_retargeting_module = load_and_compile_models(cfg, model_config)\n\n    # Warm up models\n    warm_up_models(compiled_models, stitching_retargeting_module, inputs)\n\n    # Measure inference times\n    times, overall_times = measure_inference_times(compiled_models, stitching_retargeting_module, inputs)\n\n    # Print benchmark results\n    print_benchmark_results(compiled_models, stitching_retargeting_module, ['stitching', 'eye', 'lip'], times, overall_times)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}