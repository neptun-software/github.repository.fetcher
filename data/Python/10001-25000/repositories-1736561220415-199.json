{
  "metadata": {
    "timestamp": 1736561220415,
    "page": 199,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "microsoft/Swin-Transformer",
      "stars": 14178,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.82421875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# launch bash\n*.sh\n# nsight system report files\n*.nsys-rep\n*.sqlite\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.43359375,
          "content": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1142578125,
          "content": "    MIT License\n\n    Copyright (c) Microsoft Corporation.\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE\n"
        },
        {
          "name": "MODELHUB.md",
          "type": "blob",
          "size": 39.509765625,
          "content": "Access code for `baidu` is `swin`.\n\n## ImageNet-1K and ImageNet-22K Pretrained Swin-V1 Models\n\n| name | pretrain | resolution |acc@1 | acc@5 | #params | FLOPs | FPS| 22K model | 1K model |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |:---: |:---: |\n| Swin-T | ImageNet-1K | 224x224 | 81.2 | 95.5 | 28M | 4.5G | 755 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth)/[baidu](https://pan.baidu.com/s/156nWJy4Q28rDlrX-rRbI3w)/[config](configs/swin/swin_tiny_patch4_window7_224.yaml)/[log](https://github.com/SwinTransformer/storage/files/7745562/log_swin_tiny_patch4_window7_224.txt) |\n| Swin-S | ImageNet-1K | 224x224 | 83.2 | 96.2 | 50M | 8.7G | 437 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth)/[baidu](https://pan.baidu.com/s/1KFjpj3Efey3LmtE1QqPeQg)/[config](configs/swin/swin_small_patch4_window7_224.yaml)/[log](https://github.com/SwinTransformer/storage/files/7745563/log_swin_small_patch4_window7_224.txt) |\n| Swin-B | ImageNet-1K | 224x224 | 83.5 | 96.5 | 88M | 15.4G | 278  | - | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224.pth)/[baidu](https://pan.baidu.com/s/16bqCTEc70nC_isSsgBSaqQ)/[config](configs/swin/swin_base_patch4_window7_224.yaml)/[log](https://github.com/SwinTransformer/storage/files/7745564/log_swin_base_patch4_window7_224.txt) |\n| Swin-B | ImageNet-1K | 384x384 | 84.5 | 97.0 | 88M | 47.1G | 85 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384.pth)/[baidu](https://pan.baidu.com/s/1xT1cu740-ejW7htUdVLnmw)/[config](configs/swin/swin_base_patch4_window12_384_finetune.yaml) |\n| Swin-T | ImageNet-22K | 224x224 | 80.9 | 96.0 | 28M | 4.5G | 755 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22k.pth)/[baidu](https://pan.baidu.com/s/1vct0VYwwQQ8PYkBjwSSBZQ?pwd=swin)/[config](configs/swin/swin_tiny_patch4_window7_224_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22kto1k_finetune.pth)/[baidu](https://pan.baidu.com/s/1K0OO-nGZDPkR8fm_r83e8Q?pwd=swin)/[config](configs/swin/swin_tiny_patch4_window7_224_22kto1k_finetune.yaml) |\n| Swin-S | ImageNet-22K | 224x224 | 83.2 | 97.0 | 50M | 8.7G | 437 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22k.pth)/[baidu](https://pan.baidu.com/s/11NC1xdT5BAGBgazdTme5Sg?pwd=swin)/[config](configs/swin/swin_small_patch4_window7_224_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22kto1k_finetune.pth)/[baidu](https://pan.baidu.com/s/10RFVfjQJhwPfeHrmxQUaLw?pwd=swin)/[config](configs/swin/swin_small_patch4_window7_224_22kto1k_finetune.yaml) |\n| Swin-B | ImageNet-22K | 224x224 | 85.2 | 97.5 | 88M | 15.4G | 278 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth)/[baidu](https://pan.baidu.com/s/1y1Ec3UlrKSI8IMtEs-oBXA)/[config](configs/swin/swin_base_patch4_window7_224_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth)/[baidu](https://pan.baidu.com/s/1n_wNkcbRxVXit8r_KrfAVg)/[config](configs/swin/swin_base_patch4_window7_224_22kto1k_finetune.yaml) |\n| Swin-B | ImageNet-22K | 384x384 | 86.4 | 98.0 | 88M | 47.1G | 85 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth)/[baidu](https://pan.baidu.com/s/1vwJxnJcVqcLZAw9HaqiR6g) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth)/[baidu](https://pan.baidu.com/s/1caKTSdoLJYoi4WBcnmWuWg)/[config](configs/swin/swin_base_patch4_window12_384_22kto1k_finetune.yaml) |\n| Swin-L | ImageNet-22K | 224x224 | 86.3 | 97.9 | 197M | 34.5G | 141 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth)/[baidu](https://pan.baidu.com/s/1pws3rOTFuOebBYP3h6Kx8w)/[config](configs/swin/swin_large_patch4_window7_224_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22kto1k.pth)/[baidu](https://pan.baidu.com/s/1NkQApMWUhxBGjk1ne6VqBQ)/[config](configs/swin/swin_large_patch4_window7_224_22kto1k_finetune.yaml) |\n| Swin-L | ImageNet-22K | 384x384 | 87.3 | 98.2 | 197M | 103.9G | 42 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)/[baidu](https://pan.baidu.com/s/1sl7o_bJA143OD7UqSLAMoA) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22kto1k.pth)/[baidu](https://pan.baidu.com/s/1X0FLHQyPOC6Kmv2CmgxJvA)/[config](configs/swin/swin_large_patch4_window12_384_22kto1k_finetune.yaml) |\n\n## ImageNet-1K and ImageNet-22K Pretrained Swin-V2 Models\n\n| name | pretrain | resolution | window |acc@1 | acc@5 | #params | FLOPs | FPS |22K model | 1K model |\n|:---------------------:| :---: | :---: | :---: | :---: | :---: | :---: | :---: |:---:|:---: |:---: |\n| SwinV2-T | ImageNet-1K | 256x256 | 8x8 | 81.8 | 95.9 | 28M | 5.9G | 572 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/1RzLkAH_5OtfRCJe6Vlg6rg?pwd=swin)/[config](configs/swinv2/swinv2_tiny_patch4_window8_256.yaml) |\n| SwinV2-S | ImageNet-1K | 256x256 | 8x8 | 83.7 | 96.6 | 50M | 11.5G | 327 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/195PdA41szEduW3jEtRSa4Q?pwd=swin)/[config](configs/swinv2/swinv2_small_patch4_window8_256.yaml) |\n| SwinV2-B | ImageNet-1K | 256x256 | 8x8 | 84.2 | 96.9 | 88M | 20.3G | 217 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/18AfMSz3dPyzIvP1dKuERvQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window8_256.yaml) |\n| SwinV2-T | ImageNet-1K | 256x256 | 16x16 | 82.8 | 96.2 | 28M | 6.6G | 437 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window16_256.pth)/[baidu](https://pan.baidu.com/s/1dyK3cK9Xipmv6RnTtrPocw?pwd=swin)/[config](configs/swinv2/swinv2_tiny_patch4_window16_256.yaml) |\n| SwinV2-S | ImageNet-1K | 256x256 | 16x16 | 84.1 | 96.8 | 50M | 12.6G  | 257 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window16_256.pth)/[baidu](https://pan.baidu.com/s/1ZIPiSfWNKTPp821Ka-Mifw?pwd=swin)/[config](configs/swinv2/swinv2_small_patch4_window16_256.yaml) |\n| SwinV2-B | ImageNet-1K | 256x256 | 16x16 | 84.6 | 97.0 | 88M | 21.8G | 174 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window16_256.pth)/[baidu](https://pan.baidu.com/s/1dlDQGn8BXCmnh7wQSM5Nhw?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window16_256.yaml) |\n| SwinV2-B<sup>\\*</sup> | ImageNet-22K | 256x256 | 16x16 | 86.2 | 97.9 |  88M | 21.8G | 174 | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth)/[baidu](https://pan.baidu.com/s/1Xc2rsSsRQz_sy5mjgfxrMQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window12_192_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.pth)/[baidu](https://pan.baidu.com/s/1sgstld4MgGsZxhUAW7MlmQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.yaml) |\n| SwinV2-B<sup>\\*</sup> | ImageNet-22K | 384x384 | 24x24 | 87.1 | 98.2 | 88M | 54.7G | 57  | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth)/[baidu](https://pan.baidu.com/s/1Xc2rsSsRQz_sy5mjgfxrMQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window12_192_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.pth)/[baidu](https://pan.baidu.com/s/17u3sEQaUYlvfL195rrORzQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.yaml) |\n| SwinV2-L<sup>\\*</sup> | ImageNet-22K | 256x256 | 16x16 | 86.9 | 98.0 | 197M | 47.5G | 95  | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth)/[baidu](https://pan.baidu.com/s/11PhCV7qAGXtZ8dXNgyiGOw?pwd=swin)/[config](configs/swinv2/swinv2_large_patch4_window12_192_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.pth)/[baidu](https://pan.baidu.com/s/1pqp31N80qIWjFPbudzB6Bw?pwd=swin)/[config](configs/swinv2/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.yaml) |\n| SwinV2-L<sup>\\*</sup> | ImageNet-22K | 384x384 | 24x24 | 87.6 | 98.3 | 197M | 115.4G | 33  | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth)/[baidu](https://pan.baidu.com/s/11PhCV7qAGXtZ8dXNgyiGOw?pwd=swin)/[config](configs/swinv2/swinv2_large_patch4_window12_192_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.pth)/[baidu](https://pan.baidu.com/s/13URdNkygr3Xn0N3e6IwjgA?pwd=swin)/[config](configs/swinv2/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.yaml) |\n\nNote:\n\n- SwinV2-B<sup>\\*</sup>  (SwinV2-L<sup>\\*</sup>) with input resolution of 256x256 and 384x384 both fine-tuned from the\n  same pre-training model using a smaller input resolution of 192x192.\n- SwinV2-B<sup>\\*</sup> (384x384) achieves 78.08 acc@1 on ImageNet-1K-V2 while SwinV2-L<sup>\\*</sup> (384x384) achieves\n  78.31.\n\n## ImageNet-1K Pretrained Swin MLP Models\n\n| name | pretrain | resolution |acc@1 | acc@5 | #params | FLOPs | FPS |  1K model |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| [Mixer-B/16](https://arxiv.org/pdf/2105.01601.pdf) | ImageNet-1K | 224x224 | 76.4 | - | 59M | 12.7G | - | [official repo](https://github.com/google-research/vision_transformer) |\n| [ResMLP-S24](https://arxiv.org/abs/2105.03404) | ImageNet-1K | 224x224 | 79.4 | - | 30M | 6.0G | 715 | [timm](https://github.com/rwightman/pytorch-image-models) |\n| [ResMLP-B24](https://arxiv.org/abs/2105.03404) | ImageNet-1K | 224x224 | 81.0 | - | 116M | 23.0G |  231 | [timm](https://github.com/rwightman/pytorch-image-models) |\n| Swin-T/C24 | ImageNet-1K | 256x256 | 81.6 | 95.7 | 28M | 5.9G | 563 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_tiny_c24_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/17k-7l6Sxt7uZ7IV0f26GNQ)/[config](configs/swin/swin_tiny_c24_patch4_window8_256.yaml) |\n| SwinMLP-T/C24 | ImageNet-1K | 256x256 | 79.4 | 94.6 | 20M | 4.0G | 807 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c24_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/1Sa4vP5R0M2RjfIe9HIga-Q)/[config](configs/swin/swin_mlp_tiny_c24_patch4_window8_256.yaml) |\n| SwinMLP-T/C12 | ImageNet-1K | 256x256 | 79.6 | 94.7 | 21M | 4.0G | 792 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c12_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/1mM9J2_DEVZHUB5ASIpFl0w)/[config](configs/swin/swin_mlp_tiny_c12_patch4_window8_256.yaml) |\n| SwinMLP-T/C6 | ImageNet-1K | 256x256 | 79.7 | 94.9 | 23M | 4.0G | 766 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c6_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/1hUTYVT2W1CsjICw-3W-Vjg)/[config](configs/swin/swin_mlp_tiny_c6_patch4_window8_256.yaml) |\n| SwinMLP-B | ImageNet-1K | 224x224 | 81.3 | 95.3 | 61M | 10.4G | 409 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_base_patch4_window7_224.pth)/[baidu](https://pan.baidu.com/s/1zww3dnbX3GxNiGfb-GwyUg)/[config](configs/swin/swin_mlp_base_patch4_window7_224.yaml) |\n\nNote: C24 means each head has 24 channels.\n\n## ImageNet-22K Pretrained Swin-MoE Models\n\n| name | #experts | k | router | resolution | window | IN-22K acc@1 | IN-1K/ft acc@1 | IN-1K/5-shot acc@1 | 22K model |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Swin-MoE-S | 1 (dense) | - | - | 192x192 | 8x8 | 35.5| 83.5 | 70.3 | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.2/swin_moe_small_patch4_window12_192_densebaseline_22k.zip)/[baidu](https://pan.baidu.com/s/1O1m9jT2pGoago_RiRX914w?pwd=swin)/[config](configs/swinmoe/swin_moe_small_patch4_window12_192_densebaseline_22k.yaml) |\n| Swin-MoE-S | 8 | 1 | Linear | 192x192 | 8x8 | 36.8 | 84.5 | 75.2 | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.2/swin_moe_small_patch4_window12_192_8expert_32gpu_22k.zip)/[baidu](https://pan.baidu.com/s/198IlYUrWOxEUp7wNdoJT5Q?pwd=swin)/[config](configs/swinmoe/swin_moe_small_patch4_window12_192_8expert_32gpu_22k.yaml) |\n| Swin-MoE-S | 16 | 1 | Linear |192x192 | 8x8 | 37.6 | 84.9 | 76.5 | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.2/swin_moe_small_patch4_window12_192_16expert_32gpu_22k.zip)/[baidu](https://pan.baidu.com/s/1vRQweedtT42VwMTqe9-r2A?pwd=swin)/[config](configs/swinmoe/swin_moe_small_patch4_window12_192_16expert_32gpu_22k.yaml) |\n| Swin-MoE-S | 32 | 1 | Linear | 192x192 | 8x8 | 37.4 | 84.7 | 75.9 | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.2/swin_moe_small_patch4_window12_192_32expert_32gpu_22k.zip)/[baidu](https://pan.baidu.com/s/1i7rImt5pwO8gJC-PRRuZwQ?pwd=swin)/[config](configs/swinmoe/swin_moe_small_patch4_window12_192_32expert_32gpu_22k.yaml) |\n| Swin-MoE-S | 32 | 1 | Cosine | 192x192 | 8x8 | 37.2 | 84.3 | 75.2 | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.2/swin_moe_small_patch4_window12_192_cosine_router_32expert_32gpu_22k.zip)/[baidu](https://pan.baidu.com/s/1Yghr_12ntSrv01I9yatPDQ?pwd=swin)/[config](configs/swinmoe/swin_moe_small_patch4_window12_192_cosine_router_32expert_32gpu_22k.yaml) |\n| Swin-MoE-S | 64 | 1 | Linear | 192x192 | 8x8 | 37.8 | 84.7 | 75.7 | - |\n| Swin-MoE-S | 128 | 1 | Linear | 192x192 | 8x8 | 37.4 | 84.5 | 75.4 | - |\n| Swin-MoE-B | 1 (dense) | - | - | 192x192 | 8x8 | 37.3 | 85.1 | 75.9 | [config](configs/swinmoe/swin_moe_base_patch4_window12_192_densebaseline_22k.yaml) |\n| Swin-MoE-B | 8 | 1 | Linear | 192x192 | 8x8 | 38.1 | 85.3 | 77.2 | [config](configs/swinmoe/swin_moe_base_patch4_window12_192_8expert_32gpu_22k.yaml) |\n| Swin-MoE-B | 16 | 1 | Linear | 192x192 | 8x8 | 38.7 | 85.5 | 78.2 | [config](configs/swinmoe/swin_moe_base_patch4_window12_192_16expert_32gpu_22k.yaml) |\n| Swin-MoE-B | 32 | 1 | Linear | 192x192 | 8x8 | 38.6 | 85.5 | 77.9 | [config](configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml) |\n| Swin-MoE-B | 32 | 1 | Cosine | 192x192 | 8x8 | 38.5 | 85.3 | 77.3 | [config](configs/swinmoe/swin_moe_base_patch4_window12_192_cosine_router_32expert_32gpu_22k.yaml) |\n| Swin-MoE-B | 32 | 2 | Linear | 192x192 | 8x8 | 38.6 | 85.5 | 78.7 | - |\n\n## SimMIM Pretrained Swin-V2 Models\n\n> Please note that all SimMIM pretrained Swin-V2 models will be stored in the Huggingface repository starting July 2024. For more details, refer to the [huggingface repository](https://huggingface.co/zdaxie/SimMIM).\n\n- **Model size** only includes the backbone weights and excludes weights in the decoders/classification heads.\n- **Batch size** for all models is set to 2048.\n- **Validation loss** is calculated on the ImageNet-1K validation set.\n- **Fine-tuned acc@1** refers to the top-1 accuracy on the ImageNet-1K validation set after fine-tuning.\n\n| name | model size | pre-train dataset | pre-train iterations | validation loss | fine-tuned acc@1 | pre-trained model | fine-tuned model |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| SwinV2-Small | 49M | ImageNet-1K 10% | 125k | 0.4820 | 82.69 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper10_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper10_125k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K 10% | 250k | 0.4961 | 83.11 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper10_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper10_250k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K 10% | 500k | 0.5115 | 83.17 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper10_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper10_500k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K 20% | 125k | 0.4751 | 83.05 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper20_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper20_125k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K 20% | 250k | 0.4722 | 83.56 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper20_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper20_250k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K 20% | 500k | 0.4734 | 83.75 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper20_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper20_500k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K 50% | 125k | 0.4732 | 83.04 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper50_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper50_125k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K 50% | 250k | 0.4681 | 83.67 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper50_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper50_250k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K 50% | 500k | 0.4646 | 83.96 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1kper50_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1kper50_500k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K | 125k | 0.4728 | 82.92 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1k_125k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K | 250k | 0.4674 | 83.66 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1k_250k.pth?download=true) |\n| SwinV2-Small | 49M | ImageNet-1K | 500k | 0.4641 | 84.08 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_small_1k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_small_1k_500k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 10% | 125k | 0.4822 | 83.33 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper10_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper10_125k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 10% | 250k | 0.4997 | 83.60 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper10_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper10_250k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 10% | 500k | 0.5112 | 83.41 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper10_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper10_500k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 20% | 125k | 0.4703 | 83.86 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper20_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper20_125k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 20% | 250k | 0.4679 | 84.37 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper20_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper20_250k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 20% | 500k | 0.4711 | 84.61 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper20_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper20_500k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 50% | 125k | 0.4683 | 84.04 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper50_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper50_125k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 50% | 250k | 0.4633 | 84.57 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper50_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper50_250k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K 50% | 500k | 0.4598 | 84.95 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1kper50_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1kper50_500k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K | 125k | 0.4680 | 84.13 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1k_125k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K | 250k | 0.4626 | 84.65 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1k_250k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-1K | 500k | 0.4588 | 85.04 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_1k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_1k_500k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-22K | 125k | 0.4695 | 84.11 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_22k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_22k_125k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-22K | 250k | 0.4649 | 84.57 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_22k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_22k_250k.pth?download=true) |\n| SwinV2-Base | 87M | ImageNet-22K | 500k | 0.4614 | 85.11 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_base_22k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_base_22k_500k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 10% | 125k | 0.4995 | 83.69 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper10_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper10_125k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 10% | 250k | 0.5140 | 83.66 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper10_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper10_250k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 10% | 500k | 0.5150 | 83.50 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper10_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper10_500k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 20% | 125k | 0.4675 | 84.38 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper20_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper20_125k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 20% | 250k | 0.4746 | 84.71 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper20_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper20_250k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 20% | 500k | 0.4960 | 84.59 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper20_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper20_500k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 50% | 125k | 0.4622 | 84.78 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper50_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper50_125k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 50% | 250k | 0.4566 | 85.38 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper50_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper50_250k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K 50% | 500k | 0.4530 | 85.80 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1kper50_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1kper50_500k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K | 125k | 0.4611 | 84.98 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1k_125k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K | 250k | 0.4552 | 85.45 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1k_250k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-1K | 500k | 0.4507 | 85.91 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_1k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_1k_500k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-22K | 125k | 0.4649 | 84.61 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_22k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_22k_125k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-22K | 250k | 0.4586 | 85.39 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_22k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_22k_250k.pth?download=true) |\n| SwinV2-Large | 195M | ImageNet-22K | 500k | 0.4536 | 85.81 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_large_22k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_large_22k_500k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K 20% | 125k | 0.4789 | 84.35 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1kper20_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1kper20_125k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K 20% | 250k | 0.5038 | 84.16 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1kper20_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1kper20_250k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K 20% | 500k | 0.5071 | 83.44 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1kper20_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1kper20_500k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K 50% | 125k | 0.4549 | 85.09 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1kper50_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1kper50_125k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K 50% | 250k | 0.4511 | 85.64 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1kper50_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1kper50_250k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K 50% | 500k | 0.4559 | 85.69 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1kper50_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1kper50_500k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K | 125k | 0.4531 | 85.23 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1k_125k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K | 250k | 0.4464 | 85.90 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1k_250k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-1K | 500k | 0.4416 | 86.34 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_1k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_1k_500k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-22K | 125k | 0.4564 | 85.14 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_22k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_22k_125k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-22K | 250k | 0.4499 | 85.86 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_22k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_22k_250k.pth?download=true) |\n| SwinV2-Huge | 655M | ImageNet-22K | 500k | 0.4444 | 86.27 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_huge_22k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_huge_22k_500k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-1K 50% | 125k | 0.4534 | 85.44 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_1kper50_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_1kper50_125k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-1K 50% | 250k | 0.4515 | 85.76 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_1kper50_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_1kper50_250k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-1K 50% | 500k | 0.4719 | 85.51 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_1kper50_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_1kper50_500k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-1K | 125k | 0.4513 | 85.57 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_1k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_1k_125k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-1K | 250k | 0.4442 | 86.12 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_1k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_1k_250k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-1K | 500k | 0.4395 | 86.46 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_1k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_1k_500k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-22K | 125k | 0.4544 | 85.39 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_22k_125k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_22k_125k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-22K | 250k | 0.4475 | 85.96 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_22k_250k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_22k_250k.pth?download=true) |\n| SwinV2-giant | 1.06B | ImageNet-22K | 500k | 0.4416 | 86.53 | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_pretrain_models/swinv2_giant_22k_500k.pth?download=true) | [huggingface](https://huggingface.co/zdaxie/SimMIM/resolve/main/simmim_swinv2_finetune_models/finetune_swinv2_giant_22k_500k.pth?download=true) |\n\n## SimMIM Pretrained Swin-V1 Models\n\n**ImageNet-1K Pre-trained and Fine-tuned Models**\n\n| name | pre-train epochs | pre-train resolution | fine-tune resolution | acc@1 | pre-trained model | fine-tuned model |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Swin-Base | 100 | 192x192 | 192x192 | 82.8 | [google](https://drive.google.com/file/d/1Wcbr66JL26FF30Kip9fZa_0lXrDAKP-d/view?usp=sharing)/[config](configs/swin_base__100ep/simmim_pretrain__swin_base__img192_window6__100ep.yaml) | [google](https://drive.google.com/file/d/1RsgHfjB4B1ZYblXEQVT-FPX3WSvBrxcs/view?usp=sharing)/[config](configs/swin_base__100ep/simmim_finetune__swin_base__img192_window6__100ep.yaml) |\n| Swin-Base | 100 | 192x192 | 224x224 | 83.5 | [google](https://drive.google.com/file/d/1Wcbr66JL26FF30Kip9fZa_0lXrDAKP-d/view?usp=sharing)/[config](configs/swin_base__100ep/simmim_pretrain__swin_base__img192_window6__100ep.yaml) | [google](https://drive.google.com/file/d/1mb43BkW56F5smwiX-g7QUUD7f1Rftq8u/view?usp=sharing)/[config](configs/swin_base__100ep/simmim_finetune__swin_base__img224_window7__100ep.yaml) |\n| Swin-Base | 800 | 192x192 | 224x224 | 84.0 | [google](https://drive.google.com/file/d/15zENvGjHlM71uKQ3d2FbljWPubtrPtjl/view?usp=sharing)/[config](configs/swin_base__800ep/simmim_pretrain__swin_base__img192_window6__800ep.yaml) | [google](https://drive.google.com/file/d/1xEKyfMTsdh6TfnYhk5vbw0Yz7a-viZ0w/view?usp=sharing)/[config](configs/swin_base__800ep/simmim_finetune__swin_base__img224_window7__800ep.yaml) |\n| Swin-Large | 800 | 192x192 | 224x224 | 85.4 | [google](https://drive.google.com/file/d/1qDxrTl2YUDB0505_4QrU5LU2R1kKmcBP/view?usp=sharing)/[config](configs/swin_large__800ep/simmim_pretrain__swin_large__img192_window12__800ep.yaml) | [google](https://drive.google.com/file/d/1mf0ZpXttEvFsH87Www4oQ-t8Kwr0x485/view?usp=sharing)/[config](configs/swin_large__800ep/simmim_finetune__swin_large__img224_window14__800ep.yaml) |\n| SwinV2-Huge | 800 | 192x192 | 224x224 | 85.7 | / | / |\n| SwinV2-Huge | 800 | 192x192 | 512x512 | 87.1 | / | / |\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 29.6181640625,
          "content": "# Swin Transformer\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/object-detection-on-coco)](https://paperswithcode.com/sota/object-detection-on-coco?p=swin-transformer-v2-scaling-up-capacity-and)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/instance-segmentation-on-coco)](https://paperswithcode.com/sota/instance-segmentation-on-coco?p=swin-transformer-v2-scaling-up-capacity-and)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/semantic-segmentation-on-ade20k)](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=swin-transformer-v2-scaling-up-capacity-and)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/action-classification-on-kinetics-400)](https://paperswithcode.com/sota/action-classification-on-kinetics-400?p=swin-transformer-v2-scaling-up-capacity-and)\n\nThis repo is the official implementation of [\"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"](https://arxiv.org/pdf/2103.14030.pdf) as well as the follow-ups. It currently includes code and models for the following tasks:\n\n> **Image Classification**: Included in this repo. See [get_started.md](get_started.md) for a quick start.\n\n> **Object Detection and Instance Segmentation**: See [Swin Transformer for Object Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection).\n\n> **Semantic Segmentation**: See [Swin Transformer for Semantic Segmentation](https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation).\n\n> **Video Action Recognition**: See [Video Swin Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer).\n\n> **Semi-Supervised Object Detection**: See [Soft Teacher](https://github.com/microsoft/SoftTeacher).\n\n> **SSL: Contrasitive Learning**: See [Transformer-SSL](https://github.com/SwinTransformer/Transformer-SSL).\n\n> **SSL: Masked Image Modeling**: See [get_started.md#simmim-support](https://github.com/microsoft/Swin-Transformer/blob/main/get_started.md#simmim-support).\n\n> **Mixture-of-Experts**: See [get_started](get_started.md#mixture-of-experts-support) for more instructions.\n\n> **Feature-Distillation**: See [Feature-Distillation](https://github.com/SwinTransformer/Feature-Distillation).\n\n## Updates\n\n***12/29/2022***\n\n1. **Nvidia**'s [FasterTransformer](https://github.com/NVIDIA/FasterTransformer/blob/main/docs/swin_guide.md) now supports Swin Transformer V2 inference, which have significant speed improvements on `T4 and A100 GPUs`.\n\n***11/30/2022***\n\n1. Models and codes of **Feature Distillation** are released. Please refer to [Feature-Distillation](https://github.com/SwinTransformer/Feature-Distillation) for details, and the checkpoints (FD-EsViT-Swin-B, FD-DeiT-ViT-B, FD-DINO-ViT-B, FD-CLIP-ViT-B, FD-CLIP-ViT-L).\n\n***09/24/2022***\n\n1. Merged [SimMIM](https://github.com/microsoft/SimMIM), which is a **Masked Image Modeling** based pre-training approach applicable to Swin and SwinV2 (and also applicable for ViT and ResNet). Please refer to [get started with SimMIM](get_started.md#simmim-support) to play with SimMIM pre-training.\n\n2. Released a series of Swin and SwinV2 models pre-trained using the SimMIM approach (see [MODELHUB for SimMIM](MODELHUB.md#simmim-pretrained-swin-v2-models)), with model size ranging from SwinV2-Small-50M to SwinV2-giant-1B, data size ranging from ImageNet-1K-10% to ImageNet-22K, and iterations from 125k to 500k. You may leverage these models to study the properties of MIM methods. Please look into the [data scaling](https://arxiv.org/abs/2206.04664) paper for more details.\n\n***07/09/2022***\n\n`News`: \n\n1. SwinV2-G achieves `61.4 mIoU` on ADE20K semantic segmentation (+1.5 mIoU over the previous SwinV2-G model), using an additional [feature distillation (FD)](https://github.com/SwinTransformer/Feature-Distillation) approach, **setting a new recrod** on this benchmark. FD is an approach that can generally improve the fine-tuning performance of various pre-trained models, including DeiT, DINO, and CLIP. Particularly, it improves CLIP pre-trained ViT-L by +1.6% to reach `89.0%` on ImageNet-1K image classification, which is **the most accurate ViT-L model**.\n2. Merged a PR from **Nvidia** that links to faster Swin Transformer inference that have significant speed improvements on `T4 and A100 GPUs`.\n3. Merged a PR from **Nvidia** that enables an option to use `pure FP16 (Apex O2)` in training, while almost maintaining the accuracy.\n\n***06/03/2022***\n\n1. Added **Swin-MoE**, the Mixture-of-Experts variant of Swin Transformer implemented using [Tutel](https://github.com/microsoft/tutel) (an optimized Mixture-of-Experts implementation). **Swin-MoE** is introduced in the [TuTel](https://arxiv.org/abs/2206.03382) paper.\n\n***05/12/2022***\n\n1. Pretrained models of [Swin Transformer V2](https://arxiv.org/abs/2111.09883) on ImageNet-1K and ImageNet-22K are released. \n2. ImageNet-22K pretrained models for Swin-V1-Tiny and Swin-V2-Small are released.\n\n***03/02/2022***\n\n1. Swin Transformer V2 and SimMIM got accepted by CVPR 2022. [SimMIM](https://github.com/microsoft/SimMIM) is a self-supervised pre-training approach based on masked image modeling, a key technique that works out the 3-billion-parameter Swin V2 model using `40x less labelled data` than that of previous billion-scale models based on JFT-3B. \n\n***02/09/2022***\n\n1. Integrated into [Huggingface Spaces 🤗](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/Swin-Transformer)\n\n***10/12/2021***\n\n1. Swin Transformer received ICCV 2021 best paper award (Marr Prize).\n\n***08/09/2021***\n1. [Soft Teacher](https://arxiv.org/pdf/2106.09018v2.pdf) will appear at ICCV2021. The code will be released at [GitHub Repo](https://github.com/microsoft/SoftTeacher). `Soft Teacher` is an end-to-end semi-supervisd object detection method, achieving a new record on the COCO test-dev: `61.3 box AP` and `53.0 mask AP`.\n \n***07/03/2021***\n1. Add **Swin MLP**, which is an adaption of `Swin Transformer` by replacing all multi-head self-attention (MHSA) blocks by MLP layers (more precisely it is a group linear layer). The shifted window configuration can also significantly improve the performance of vanilla MLP architectures. \n\n***06/25/2021***\n1. [Video Swin Transformer](https://arxiv.org/abs/2106.13230) is released at [Video-Swin-Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer).\n`Video Swin Transformer` achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including action recognition (`84.9` top-1 accuracy on Kinetics-400 and `86.1` top-1 accuracy on Kinetics-600 with `~20x` less pre-training data and `~3x` smaller model size) and temporal modeling (`69.6` top-1 accuracy on Something-Something v2).\n\n***05/12/2021***\n1. Used as a backbone for `Self-Supervised Learning`: [Transformer-SSL](https://github.com/SwinTransformer/Transformer-SSL)\n\nUsing Swin-Transformer as the backbone for self-supervised learning enables us to evaluate the transferring performance of the learnt representations on down-stream tasks, which is missing in previous works due to the use of ViT/DeiT, which has not been well tamed for down-stream tasks.\n\n***04/12/2021***\n\nInitial commits:\n\n1. Pretrained models on ImageNet-1K ([Swin-T-IN1K](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth), [Swin-S-IN1K](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth), [Swin-B-IN1K](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224.pth)) and ImageNet-22K ([Swin-B-IN22K](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth), [Swin-L-IN22K](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth)) are provided.\n2. The supported code and models for ImageNet-1K image classification, COCO object detection and ADE20K semantic segmentation are provided.\n3. The cuda kernel implementation for the [local relation layer](https://arxiv.org/pdf/1904.11491.pdf) is provided in branch [LR-Net](https://github.com/microsoft/Swin-Transformer/tree/LR-Net).\n\n## Introduction\n\n**Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030), which capably serves as a\ngeneral-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is\ncomputed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention\ncomputation to non-overlapping local windows while also allowing for cross-window connection.\n\nSwin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and\nADE20K semantic segmentation (`53.5 mIoU` on val), surpassing previous models by a large margin.\n\n![teaser](figures/teaser.png)\n\n## Main Results on ImageNet with Pretrained Models\n\n**ImageNet-1K and ImageNet-22K Pretrained Swin-V1 Models**\n\n| name | pretrain | resolution |acc@1 | acc@5 | #params | FLOPs | FPS| 22K model | 1K model |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |:---: |:---: |\n| Swin-T | ImageNet-1K | 224x224 | 81.2 | 95.5 | 28M | 4.5G | 755 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth)/[baidu](https://pan.baidu.com/s/156nWJy4Q28rDlrX-rRbI3w)/[config](configs/swin/swin_tiny_patch4_window7_224.yaml)/[log](https://github.com/SwinTransformer/storage/files/7745562/log_swin_tiny_patch4_window7_224.txt) |\n| Swin-S | ImageNet-1K | 224x224 | 83.2 | 96.2 | 50M | 8.7G | 437 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth)/[baidu](https://pan.baidu.com/s/1KFjpj3Efey3LmtE1QqPeQg)/[config](configs/swin/swin_small_patch4_window7_224.yaml)/[log](https://github.com/SwinTransformer/storage/files/7745563/log_swin_small_patch4_window7_224.txt) |\n| Swin-B | ImageNet-1K | 224x224 | 83.5 | 96.5 | 88M | 15.4G | 278  | - | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224.pth)/[baidu](https://pan.baidu.com/s/16bqCTEc70nC_isSsgBSaqQ)/[config](configs/swin/swin_base_patch4_window7_224.yaml)/[log](https://github.com/SwinTransformer/storage/files/7745564/log_swin_base_patch4_window7_224.txt) |\n| Swin-B | ImageNet-1K | 384x384 | 84.5 | 97.0 | 88M | 47.1G | 85 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384.pth)/[baidu](https://pan.baidu.com/s/1xT1cu740-ejW7htUdVLnmw)/[config](configs/swin/swin_base_patch4_window12_384_finetune.yaml) |\n| Swin-T | ImageNet-22K | 224x224 | 80.9 | 96.0 | 28M | 4.5G | 755 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22k.pth)/[baidu](https://pan.baidu.com/s/1vct0VYwwQQ8PYkBjwSSBZQ?pwd=swin)/[config](configs/swin/swin_tiny_patch4_window7_224_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22kto1k_finetune.pth)/[baidu](https://pan.baidu.com/s/1K0OO-nGZDPkR8fm_r83e8Q?pwd=swin)/[config](configs/swin/swin_tiny_patch4_window7_224_22kto1k_finetune.yaml) |\n| Swin-S | ImageNet-22K | 224x224 | 83.2 | 97.0 | 50M | 8.7G | 437 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22k.pth)/[baidu](https://pan.baidu.com/s/11NC1xdT5BAGBgazdTme5Sg?pwd=swin)/[config](configs/swin/swin_small_patch4_window7_224_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22kto1k_finetune.pth)/[baidu](https://pan.baidu.com/s/10RFVfjQJhwPfeHrmxQUaLw?pwd=swin)/[config](configs/swin/swin_small_patch4_window7_224_22kto1k_finetune.yaml) |\n| Swin-B | ImageNet-22K | 224x224 | 85.2 | 97.5 | 88M | 15.4G | 278 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth)/[baidu](https://pan.baidu.com/s/1y1Ec3UlrKSI8IMtEs-oBXA)/[config](configs/swin/swin_base_patch4_window7_224_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth)/[baidu](https://pan.baidu.com/s/1n_wNkcbRxVXit8r_KrfAVg)/[config](configs/swin/swin_base_patch4_window7_224_22kto1k_finetune.yaml) |\n| Swin-B | ImageNet-22K | 384x384 | 86.4 | 98.0 | 88M | 47.1G | 85 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth)/[baidu](https://pan.baidu.com/s/1vwJxnJcVqcLZAw9HaqiR6g) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth)/[baidu](https://pan.baidu.com/s/1caKTSdoLJYoi4WBcnmWuWg)/[config](configs/swin/swin_base_patch4_window12_384_22kto1k_finetune.yaml) |\n| Swin-L | ImageNet-22K | 224x224 | 86.3 | 97.9 | 197M | 34.5G | 141 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth)/[baidu](https://pan.baidu.com/s/1pws3rOTFuOebBYP3h6Kx8w)/[config](configs/swin/swin_large_patch4_window7_224_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22kto1k.pth)/[baidu](https://pan.baidu.com/s/1NkQApMWUhxBGjk1ne6VqBQ)/[config](configs/swin/swin_large_patch4_window7_224_22kto1k_finetune.yaml) |\n| Swin-L | ImageNet-22K | 384x384 | 87.3 | 98.2 | 197M | 103.9G | 42 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)/[baidu](https://pan.baidu.com/s/1sl7o_bJA143OD7UqSLAMoA) | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22kto1k.pth)/[baidu](https://pan.baidu.com/s/1X0FLHQyPOC6Kmv2CmgxJvA)/[config](configs/swin/swin_large_patch4_window12_384_22kto1k_finetune.yaml) |\n\n**ImageNet-1K and ImageNet-22K Pretrained Swin-V2 Models**\n\n| name | pretrain | resolution | window |acc@1 | acc@5 | #params | FLOPs | FPS |22K model | 1K model |\n|:---------------------:| :---: | :---: | :---: | :---: | :---: | :---: | :---: |:---:|:---: |:---: |\n| SwinV2-T | ImageNet-1K | 256x256 | 8x8 | 81.8 | 95.9 | 28M | 5.9G | 572 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/1RzLkAH_5OtfRCJe6Vlg6rg?pwd=swin)/[config](configs/swinv2/swinv2_tiny_patch4_window8_256.yaml) |\n| SwinV2-S | ImageNet-1K | 256x256 | 8x8 | 83.7 | 96.6 | 50M | 11.5G | 327 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/195PdA41szEduW3jEtRSa4Q?pwd=swin)/[config](configs/swinv2/swinv2_small_patch4_window8_256.yaml) |\n| SwinV2-B | ImageNet-1K | 256x256 | 8x8 | 84.2 | 96.9 | 88M | 20.3G | 217 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/18AfMSz3dPyzIvP1dKuERvQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window8_256.yaml) |\n| SwinV2-T | ImageNet-1K | 256x256 | 16x16 | 82.8 | 96.2 | 28M | 6.6G | 437 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window16_256.pth)/[baidu](https://pan.baidu.com/s/1dyK3cK9Xipmv6RnTtrPocw?pwd=swin)/[config](configs/swinv2/swinv2_tiny_patch4_window16_256.yaml) |\n| SwinV2-S | ImageNet-1K | 256x256 | 16x16 | 84.1 | 96.8 | 50M | 12.6G  | 257 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window16_256.pth)/[baidu](https://pan.baidu.com/s/1ZIPiSfWNKTPp821Ka-Mifw?pwd=swin)/[config](configs/swinv2/swinv2_small_patch4_window16_256.yaml) |\n| SwinV2-B | ImageNet-1K | 256x256 | 16x16 | 84.6 | 97.0 | 88M | 21.8G | 174 | - | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window16_256.pth)/[baidu](https://pan.baidu.com/s/1dlDQGn8BXCmnh7wQSM5Nhw?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window16_256.yaml) |\n| SwinV2-B<sup>\\*</sup> | ImageNet-22K | 256x256 | 16x16 | 86.2 | 97.9 |  88M | 21.8G | 174 | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth)/[baidu](https://pan.baidu.com/s/1Xc2rsSsRQz_sy5mjgfxrMQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window12_192_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.pth)/[baidu](https://pan.baidu.com/s/1sgstld4MgGsZxhUAW7MlmQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.yaml) |\n| SwinV2-B<sup>\\*</sup> | ImageNet-22K | 384x384 | 24x24 | 87.1 | 98.2 | 88M | 54.7G | 57  | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth)/[baidu](https://pan.baidu.com/s/1Xc2rsSsRQz_sy5mjgfxrMQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window12_192_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.pth)/[baidu](https://pan.baidu.com/s/17u3sEQaUYlvfL195rrORzQ?pwd=swin)/[config](configs/swinv2/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.yaml) |\n| SwinV2-L<sup>\\*</sup> | ImageNet-22K | 256x256 | 16x16 | 86.9 | 98.0 | 197M | 47.5G | 95  | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth)/[baidu](https://pan.baidu.com/s/11PhCV7qAGXtZ8dXNgyiGOw?pwd=swin)/[config](configs/swinv2/swinv2_large_patch4_window12_192_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.pth)/[baidu](https://pan.baidu.com/s/1pqp31N80qIWjFPbudzB6Bw?pwd=swin)/[config](configs/swinv2/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.yaml) |\n| SwinV2-L<sup>\\*</sup> | ImageNet-22K | 384x384 | 24x24 | 87.6 | 98.3 | 197M | 115.4G | 33  | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth)/[baidu](https://pan.baidu.com/s/11PhCV7qAGXtZ8dXNgyiGOw?pwd=swin)/[config](configs/swinv2/swinv2_large_patch4_window12_192_22k.yaml) | [github](https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.pth)/[baidu](https://pan.baidu.com/s/13URdNkygr3Xn0N3e6IwjgA?pwd=swin)/[config](configs/swinv2/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.yaml) |\n\nNote: \n- SwinV2-B<sup>\\*</sup>  (SwinV2-L<sup>\\*</sup>) with input resolution of 256x256 and 384x384 both fine-tuned from the same pre-training model using a smaller input resolution of 192x192.\n- SwinV2-B<sup>\\*</sup> (384x384) achieves 78.08 acc@1 on ImageNet-1K-V2 while SwinV2-L<sup>\\*</sup> (384x384) achieves 78.31.\n\n**ImageNet-1K Pretrained Swin MLP Models**\n\n| name | pretrain | resolution |acc@1 | acc@5 | #params | FLOPs | FPS |  1K model |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| [Mixer-B/16](https://arxiv.org/pdf/2105.01601.pdf) | ImageNet-1K | 224x224 | 76.4 | - | 59M | 12.7G | - | [official repo](https://github.com/google-research/vision_transformer) |\n| [ResMLP-S24](https://arxiv.org/abs/2105.03404) | ImageNet-1K | 224x224 | 79.4 | - | 30M | 6.0G | 715 | [timm](https://github.com/rwightman/pytorch-image-models) |\n| [ResMLP-B24](https://arxiv.org/abs/2105.03404) | ImageNet-1K | 224x224 | 81.0 | - | 116M | 23.0G |  231 | [timm](https://github.com/rwightman/pytorch-image-models) |\n| Swin-T/C24 | ImageNet-1K | 256x256 | 81.6 | 95.7 | 28M | 5.9G | 563 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_tiny_c24_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/17k-7l6Sxt7uZ7IV0f26GNQ)/[config](configs/swin/swin_tiny_c24_patch4_window8_256.yaml) |\n| SwinMLP-T/C24 | ImageNet-1K | 256x256 | 79.4 | 94.6 | 20M | 4.0G | 807 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c24_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/1Sa4vP5R0M2RjfIe9HIga-Q)/[config](configs/swin/swin_mlp_tiny_c24_patch4_window8_256.yaml) |\n| SwinMLP-T/C12 | ImageNet-1K | 256x256 | 79.6 | 94.7 | 21M | 4.0G | 792 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c12_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/1mM9J2_DEVZHUB5ASIpFl0w)/[config](configs/swin/swin_mlp_tiny_c12_patch4_window8_256.yaml) |\n| SwinMLP-T/C6 | ImageNet-1K | 256x256 | 79.7 | 94.9 | 23M | 4.0G | 766 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c6_patch4_window8_256.pth)/[baidu](https://pan.baidu.com/s/1hUTYVT2W1CsjICw-3W-Vjg)/[config](configs/swin/swin_mlp_tiny_c6_patch4_window8_256.yaml) |\n| SwinMLP-B | ImageNet-1K | 224x224 | 81.3 | 95.3 | 61M | 10.4G | 409 | [github](https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_base_patch4_window7_224.pth)/[baidu](https://pan.baidu.com/s/1zww3dnbX3GxNiGfb-GwyUg)/[config](configs/swin/swin_mlp_base_patch4_window7_224.yaml) |\n\nNote: access code for `baidu` is `swin`. C24 means each head has 24 channels.\n\n**ImageNet-22K Pretrained Swin-MoE Models**\n\n- Please refer to [get_started](get_started.md#mixture-of-experts-support) for instructions on running Swin-MoE. \n- Pretrained models for Swin-MoE can be found in [MODEL HUB](MODELHUB.md#imagenet-22k-pretrained-swin-moe-models)\n\n## Main Results on Downstream Tasks\n\n**COCO Object Detection (2017 val)**\n\n| Backbone | Method | pretrain | Lr Schd | box mAP | mask mAP | #params | FLOPs |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Swin-T | Mask R-CNN | ImageNet-1K | 3x | 46.0 | 41.6 | 48M | 267G |\n| Swin-S | Mask R-CNN | ImageNet-1K | 3x | 48.5 | 43.3 | 69M | 359G |\n| Swin-T | Cascade Mask R-CNN | ImageNet-1K | 3x | 50.4 | 43.7 | 86M | 745G |\n| Swin-S | Cascade Mask R-CNN | ImageNet-1K |  3x | 51.9 | 45.0 | 107M | 838G |\n| Swin-B | Cascade Mask R-CNN | ImageNet-1K |  3x | 51.9 | 45.0 | 145M | 982G |\n| Swin-T | RepPoints V2 | ImageNet-1K | 3x | 50.0 | - | 45M | 283G |\n| Swin-T | Mask RepPoints V2 | ImageNet-1K | 3x | 50.3 | 43.6 | 47M | 292G |\n| Swin-B | HTC++ | ImageNet-22K | 6x | 56.4 | 49.1 | 160M | 1043G |\n| Swin-L | HTC++ | ImageNet-22K | 3x | 57.1 | 49.5 | 284M | 1470G |\n| Swin-L | HTC++<sup>*</sup> | ImageNet-22K | 3x | 58.0 | 50.4 | 284M | - |\n\nNote: <sup>*</sup> indicates multi-scale testing.\n\n**ADE20K Semantic Segmentation (val)**\n\n| Backbone | Method | pretrain | Crop Size | Lr Schd | mIoU | mIoU (ms+flip) | #params | FLOPs |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Swin-T | UPerNet | ImageNet-1K | 512x512 | 160K | 44.51 | 45.81 | 60M | 945G |\n| Swin-S | UperNet | ImageNet-1K | 512x512 | 160K | 47.64 | 49.47 | 81M | 1038G |\n| Swin-B | UperNet | ImageNet-1K | 512x512 | 160K | 48.13 | 49.72 | 121M | 1188G |\n| Swin-B | UPerNet | ImageNet-22K | 640x640 | 160K | 50.04 | 51.66 | 121M | 1841G |\n| Swin-L | UperNet | ImageNet-22K | 640x640 | 160K | 52.05 | 53.53 | 234M | 3230G |\n\n## Citing Swin Transformer\n\n```\n@inproceedings{liu2021Swin,\n  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},\n  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n  year={2021}\n}\n```\n## Citing Local Relation Networks (the first full-attention visual backbone)\n```\n@inproceedings{hu2019local,\n  title={Local Relation Networks for Image Recognition},\n  author={Hu, Han and Zhang, Zheng and Xie, Zhenda and Lin, Stephen},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n  pages={3464--3473},\n  year={2019}\n}\n```\n## Citing Swin Transformer V2\n```\n@inproceedings{liu2021swinv2,\n  title={Swin Transformer V2: Scaling Up Capacity and Resolution}, \n  author={Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},\n  booktitle={International Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2022}\n}\n```\n## Citing SimMIM (a self-supervised approach that enables SwinV2-G)\n```\n@inproceedings{xie2021simmim,\n  title={SimMIM: A Simple Framework for Masked Image Modeling},\n  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},\n  booktitle={International Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2022}\n}\n```\n## Citing SimMIM-data-scaling\n```\n@article{xie2022data,\n  title={On Data Scaling in Masked Image Modeling},\n  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Wei, Yixuan and Dai, Qi and Hu, Han},\n  journal={arXiv preprint arXiv:2206.04664},\n  year={2022}\n}\n```\n## Citing Swin-MoE\n```\n@misc{hwang2022tutel,\n      title={Tutel: Adaptive Mixture-of-Experts at Scale}, \n      author={Changho Hwang and Wei Cui and Yifan Xiong and Ziyue Yang and Ze Liu and Han Hu and Zilong Wang and Rafael Salas and Jithin Jose and Prabhat Ram and Joe Chau and Peng Cheng and Fan Yang and Mao Yang and Yongqiang Xiong},\n      year={2022},\n      eprint={2206.03382},\n      archivePrefix={arXiv}\n}\n```\n\n## Getting Started\n\n- For **Image Classification**, please see [get_started.md](get_started.md) for detailed instructions.\n- For **Object Detection and Instance Segmentation**, please see [Swin Transformer for Object Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection).\n- For **Semantic Segmentation**, please see [Swin Transformer for Semantic Segmentation](https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation).\n- For **Self-Supervised Learning**, please see [Transformer-SSL](https://github.com/SwinTransformer/Transformer-SSL).\n- For **Video Recognition**, please see [Video Swin Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer).\n\n## Third-party Usage and Experiments\n\n***In this pargraph, we cross link third-party repositories which use Swin and report results. You can let us know by raising an issue*** \n\n(`Note please report accuracy numbers and provide trained models in your new repository to facilitate others to get sense of correctness and model behavior`)\n\n[12/29/2022] Swin Transformers (V2) inference implemented in FasterTransformer: [FasterTransformer](https://github.com/NVIDIA/FasterTransformer/blob/main/docs/swin_guide.md)\n\n[06/30/2022] Swin Transformers (V1) inference implemented in FasterTransformer: [FasterTransformer](https://github.com/NVIDIA/FasterTransformer/blob/main/docs/swin_guide.md)\n\n[05/12/2022] Swin Transformers (V1) implemented in TensorFlow with the pre-trained parameters ported into them. Find the implementation,\nTensorFlow weights, code example here in [this repository](https://github.com/sayakpaul/swin-transformers-tf/).\n\n[04/06/2022] Swin Transformer for Audio Classification: [Hierarchical Token Semantic Audio Transformer](https://github.com/RetroCirce/HTS-Audio-Transformer).\n\n[12/21/2021] Swin Transformer for StyleGAN: [StyleSwin](https://github.com/microsoft/StyleSwin)\n\n[12/13/2021] Swin Transformer for Face Recognition: [FaceX-Zoo](https://github.com/JDAI-CV/FaceX-Zoo)\n\n[08/29/2021] Swin Transformer for Image Restoration: [SwinIR](https://github.com/JingyunLiang/SwinIR)\n\n[08/12/2021] Swin Transformer for person reID: [https://github.com/layumi/Person_reID_baseline_pytorch](https://github.com/layumi/Person_reID_baseline_pytorch)\n\n[06/29/2021] Swin-Transformer in PaddleClas and inference based on whl package: [https://github.com/PaddlePaddle/PaddleClas](https://github.com/PaddlePaddle/PaddleClas)\n\n[04/14/2021] Swin for RetinaNet in Detectron: https://github.com/xiaohu2015/SwinT_detectron2.\n\n[04/16/2021] Included in a famous model zoo: https://github.com/rwightman/pytorch-image-models.\n\n[04/20/2021] Swin-Transformer classifier inference using TorchServe: https://github.com/kamalkraj/Swin-Transformer-Serve\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.71484375,
          "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.5 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://www.microsoft.com/en-us/msrc/pgp-key-msrc).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). \n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://microsoft.com/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://www.microsoft.com/en-us/msrc/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->"
        },
        {
          "name": "SUPPORT.md",
          "type": "blob",
          "size": 1.2841796875,
          "content": "# TODO: The maintainer of this repo has not yet edited this file\r\n\r\n**REPO OWNER**: Do you want Customer Service & Support (CSS) support for this product/project?\r\n\r\n- **No CSS support:** Fill out this template with information about how to file issues and get help.\r\n- **Yes CSS support:** Fill out an intake form at [aka.ms/spot](https://aka.ms/spot). CSS will work with/help you to determine next steps. More details also available at [aka.ms/onboardsupport](https://aka.ms/onboardsupport).\r\n- **Not sure?** Fill out a SPOT intake as though the answer were \"Yes\". CSS will help you decide.\r\n\r\n*Then remove this first heading from this SUPPORT.MD file before publishing your repo.*\r\n\r\n# Support\r\n\r\n## How to file issues and get help  \r\n\r\nThis project uses GitHub Issues to track bugs and feature requests. Please search the existing \r\nissues before filing new issues to avoid duplicates.  For new issues, file your bug or \r\nfeature request as a new Issue.\r\n\r\nFor help and questions about using this project, please **REPO MAINTAINER: INSERT INSTRUCTIONS HERE \r\nFOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER\r\nCHANNEL. WHERE WILL YOU HELP PEOPLE?**.\r\n\r\n## Microsoft Support Policy  \r\n\r\nSupport for this **PROJECT or PRODUCT** is limited to the resources listed above.\r\n"
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 11.5107421875,
          "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------'\n\nimport os\nimport torch\nimport yaml\nfrom yacs.config import CfgNode as CN\n\n# pytorch major version (1.x or 2.x)\nPYTORCH_MAJOR_VERSION = int(torch.__version__.split('.')[0])\n\n_C = CN()\n\n# Base config files\n_C.BASE = ['']\n\n# -----------------------------------------------------------------------------\n# Data settings\n# -----------------------------------------------------------------------------\n_C.DATA = CN()\n# Batch size for a single GPU, could be overwritten by command line argument\n_C.DATA.BATCH_SIZE = 128\n# Path to dataset, could be overwritten by command line argument\n_C.DATA.DATA_PATH = ''\n# Dataset name\n_C.DATA.DATASET = 'imagenet'\n# Input image size\n_C.DATA.IMG_SIZE = 224\n# Interpolation to resize image (random, bilinear, bicubic)\n_C.DATA.INTERPOLATION = 'bicubic'\n# Use zipped dataset instead of folder dataset\n# could be overwritten by command line argument\n_C.DATA.ZIP_MODE = False\n# Cache Data in Memory, could be overwritten by command line argument\n_C.DATA.CACHE_MODE = 'part'\n# Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\n_C.DATA.PIN_MEMORY = True\n# Number of data loading threads\n_C.DATA.NUM_WORKERS = 8\n\n# [SimMIM] Mask patch size for MaskGenerator\n_C.DATA.MASK_PATCH_SIZE = 32\n# [SimMIM] Mask ratio for MaskGenerator\n_C.DATA.MASK_RATIO = 0.6\n\n# -----------------------------------------------------------------------------\n# Model settings\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# Model type\n_C.MODEL.TYPE = 'swin'\n# Model name\n_C.MODEL.NAME = 'swin_tiny_patch4_window7_224'\n# Pretrained weight from checkpoint, could be imagenet22k pretrained weight\n# could be overwritten by command line argument\n_C.MODEL.PRETRAINED = ''\n# Checkpoint to resume, could be overwritten by command line argument\n_C.MODEL.RESUME = ''\n# Number of classes, overwritten in data preparation\n_C.MODEL.NUM_CLASSES = 1000\n# Dropout rate\n_C.MODEL.DROP_RATE = 0.0\n# Drop path rate\n_C.MODEL.DROP_PATH_RATE = 0.1\n# Label Smoothing\n_C.MODEL.LABEL_SMOOTHING = 0.1\n\n# Swin Transformer parameters\n_C.MODEL.SWIN = CN()\n_C.MODEL.SWIN.PATCH_SIZE = 4\n_C.MODEL.SWIN.IN_CHANS = 3\n_C.MODEL.SWIN.EMBED_DIM = 96\n_C.MODEL.SWIN.DEPTHS = [2, 2, 6, 2]\n_C.MODEL.SWIN.NUM_HEADS = [3, 6, 12, 24]\n_C.MODEL.SWIN.WINDOW_SIZE = 7\n_C.MODEL.SWIN.MLP_RATIO = 4.\n_C.MODEL.SWIN.QKV_BIAS = True\n_C.MODEL.SWIN.QK_SCALE = None\n_C.MODEL.SWIN.APE = False\n_C.MODEL.SWIN.PATCH_NORM = True\n\n# Swin Transformer V2 parameters\n_C.MODEL.SWINV2 = CN()\n_C.MODEL.SWINV2.PATCH_SIZE = 4\n_C.MODEL.SWINV2.IN_CHANS = 3\n_C.MODEL.SWINV2.EMBED_DIM = 96\n_C.MODEL.SWINV2.DEPTHS = [2, 2, 6, 2]\n_C.MODEL.SWINV2.NUM_HEADS = [3, 6, 12, 24]\n_C.MODEL.SWINV2.WINDOW_SIZE = 7\n_C.MODEL.SWINV2.MLP_RATIO = 4.\n_C.MODEL.SWINV2.QKV_BIAS = True\n_C.MODEL.SWINV2.APE = False\n_C.MODEL.SWINV2.PATCH_NORM = True\n_C.MODEL.SWINV2.PRETRAINED_WINDOW_SIZES = [0, 0, 0, 0]\n\n# Swin Transformer MoE parameters\n_C.MODEL.SWIN_MOE = CN()\n_C.MODEL.SWIN_MOE.PATCH_SIZE = 4\n_C.MODEL.SWIN_MOE.IN_CHANS = 3\n_C.MODEL.SWIN_MOE.EMBED_DIM = 96\n_C.MODEL.SWIN_MOE.DEPTHS = [2, 2, 6, 2]\n_C.MODEL.SWIN_MOE.NUM_HEADS = [3, 6, 12, 24]\n_C.MODEL.SWIN_MOE.WINDOW_SIZE = 7\n_C.MODEL.SWIN_MOE.MLP_RATIO = 4.\n_C.MODEL.SWIN_MOE.QKV_BIAS = True\n_C.MODEL.SWIN_MOE.QK_SCALE = None\n_C.MODEL.SWIN_MOE.APE = False\n_C.MODEL.SWIN_MOE.PATCH_NORM = True\n_C.MODEL.SWIN_MOE.MLP_FC2_BIAS = True\n_C.MODEL.SWIN_MOE.INIT_STD = 0.02\n_C.MODEL.SWIN_MOE.PRETRAINED_WINDOW_SIZES = [0, 0, 0, 0]\n_C.MODEL.SWIN_MOE.MOE_BLOCKS = [[-1], [-1], [-1], [-1]]\n_C.MODEL.SWIN_MOE.NUM_LOCAL_EXPERTS = 1\n_C.MODEL.SWIN_MOE.TOP_VALUE = 1\n_C.MODEL.SWIN_MOE.CAPACITY_FACTOR = 1.25\n_C.MODEL.SWIN_MOE.COSINE_ROUTER = False\n_C.MODEL.SWIN_MOE.NORMALIZE_GATE = False\n_C.MODEL.SWIN_MOE.USE_BPR = True\n_C.MODEL.SWIN_MOE.IS_GSHARD_LOSS = False\n_C.MODEL.SWIN_MOE.GATE_NOISE = 1.0\n_C.MODEL.SWIN_MOE.COSINE_ROUTER_DIM = 256\n_C.MODEL.SWIN_MOE.COSINE_ROUTER_INIT_T = 0.5\n_C.MODEL.SWIN_MOE.MOE_DROP = 0.0\n_C.MODEL.SWIN_MOE.AUX_LOSS_WEIGHT = 0.01\n\n# Swin MLP parameters\n_C.MODEL.SWIN_MLP = CN()\n_C.MODEL.SWIN_MLP.PATCH_SIZE = 4\n_C.MODEL.SWIN_MLP.IN_CHANS = 3\n_C.MODEL.SWIN_MLP.EMBED_DIM = 96\n_C.MODEL.SWIN_MLP.DEPTHS = [2, 2, 6, 2]\n_C.MODEL.SWIN_MLP.NUM_HEADS = [3, 6, 12, 24]\n_C.MODEL.SWIN_MLP.WINDOW_SIZE = 7\n_C.MODEL.SWIN_MLP.MLP_RATIO = 4.\n_C.MODEL.SWIN_MLP.APE = False\n_C.MODEL.SWIN_MLP.PATCH_NORM = True\n\n# [SimMIM] Norm target during training\n_C.MODEL.SIMMIM = CN()\n_C.MODEL.SIMMIM.NORM_TARGET = CN()\n_C.MODEL.SIMMIM.NORM_TARGET.ENABLE = False\n_C.MODEL.SIMMIM.NORM_TARGET.PATCH_SIZE = 47\n\n# -----------------------------------------------------------------------------\n# Training settings\n# -----------------------------------------------------------------------------\n_C.TRAIN = CN()\n_C.TRAIN.START_EPOCH = 0\n_C.TRAIN.EPOCHS = 300\n_C.TRAIN.WARMUP_EPOCHS = 20\n_C.TRAIN.WEIGHT_DECAY = 0.05\n_C.TRAIN.BASE_LR = 5e-4\n_C.TRAIN.WARMUP_LR = 5e-7\n_C.TRAIN.MIN_LR = 5e-6\n# Clip gradient norm\n_C.TRAIN.CLIP_GRAD = 5.0\n# Auto resume from latest checkpoint\n_C.TRAIN.AUTO_RESUME = True\n# Gradient accumulation steps\n# could be overwritten by command line argument\n_C.TRAIN.ACCUMULATION_STEPS = 1\n# Whether to use gradient checkpointing to save memory\n# could be overwritten by command line argument\n_C.TRAIN.USE_CHECKPOINT = False\n\n# LR scheduler\n_C.TRAIN.LR_SCHEDULER = CN()\n_C.TRAIN.LR_SCHEDULER.NAME = 'cosine'\n# Epoch interval to decay LR, used in StepLRScheduler\n_C.TRAIN.LR_SCHEDULER.DECAY_EPOCHS = 30\n# LR decay rate, used in StepLRScheduler\n_C.TRAIN.LR_SCHEDULER.DECAY_RATE = 0.1\n# warmup_prefix used in CosineLRScheduler\n_C.TRAIN.LR_SCHEDULER.WARMUP_PREFIX = True\n# [SimMIM] Gamma / Multi steps value, used in MultiStepLRScheduler\n_C.TRAIN.LR_SCHEDULER.GAMMA = 0.1\n_C.TRAIN.LR_SCHEDULER.MULTISTEPS = []\n\n# Optimizer\n_C.TRAIN.OPTIMIZER = CN()\n_C.TRAIN.OPTIMIZER.NAME = 'adamw'\n# Optimizer Epsilon\n_C.TRAIN.OPTIMIZER.EPS = 1e-8\n# Optimizer Betas\n_C.TRAIN.OPTIMIZER.BETAS = (0.9, 0.999)\n# SGD momentum\n_C.TRAIN.OPTIMIZER.MOMENTUM = 0.9\n\n# [SimMIM] Layer decay for fine-tuning\n_C.TRAIN.LAYER_DECAY = 1.0\n\n# MoE\n_C.TRAIN.MOE = CN()\n# Only save model on master device\n_C.TRAIN.MOE.SAVE_MASTER = False\n# -----------------------------------------------------------------------------\n# Augmentation settings\n# -----------------------------------------------------------------------------\n_C.AUG = CN()\n# Color jitter factor\n_C.AUG.COLOR_JITTER = 0.4\n# Use AutoAugment policy. \"v0\" or \"original\"\n_C.AUG.AUTO_AUGMENT = 'rand-m9-mstd0.5-inc1'\n# Random erase prob\n_C.AUG.REPROB = 0.25\n# Random erase mode\n_C.AUG.REMODE = 'pixel'\n# Random erase count\n_C.AUG.RECOUNT = 1\n# Mixup alpha, mixup enabled if > 0\n_C.AUG.MIXUP = 0.8\n# Cutmix alpha, cutmix enabled if > 0\n_C.AUG.CUTMIX = 1.0\n# Cutmix min/max ratio, overrides alpha and enables cutmix if set\n_C.AUG.CUTMIX_MINMAX = None\n# Probability of performing mixup or cutmix when either/both is enabled\n_C.AUG.MIXUP_PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled\n_C.AUG.MIXUP_SWITCH_PROB = 0.5\n# How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"\n_C.AUG.MIXUP_MODE = 'batch'\n\n# -----------------------------------------------------------------------------\n# Testing settings\n# -----------------------------------------------------------------------------\n_C.TEST = CN()\n# Whether to use center crop when testing\n_C.TEST.CROP = True\n# Whether to use SequentialSampler as validation sampler\n_C.TEST.SEQUENTIAL = False\n_C.TEST.SHUFFLE = False\n\n# -----------------------------------------------------------------------------\n# Misc\n# -----------------------------------------------------------------------------\n# [SimMIM] Whether to enable pytorch amp, overwritten by command line argument\n_C.ENABLE_AMP = False\n\n# Enable Pytorch automatic mixed precision (amp).\n_C.AMP_ENABLE = True\n# [Deprecated] Mixed precision opt level of apex, if O0, no apex amp is used ('O0', 'O1', 'O2')\n_C.AMP_OPT_LEVEL = ''\n# Path to output folder, overwritten by command line argument\n_C.OUTPUT = ''\n# Tag of experiment, overwritten by command line argument\n_C.TAG = 'default'\n# Frequency to save checkpoint\n_C.SAVE_FREQ = 1\n# Frequency to logging info\n_C.PRINT_FREQ = 10\n# Fixed random seed\n_C.SEED = 0\n# Perform evaluation only, overwritten by command line argument\n_C.EVAL_MODE = False\n# Test throughput only, overwritten by command line argument\n_C.THROUGHPUT_MODE = False\n# local rank for DistributedDataParallel, given by command line argument\n_C.LOCAL_RANK = 0\n# for acceleration\n_C.FUSED_WINDOW_PROCESS = False\n_C.FUSED_LAYERNORM = False\n\n\ndef _update_config_from_file(config, cfg_file):\n    config.defrost()\n    with open(cfg_file, 'r') as f:\n        yaml_cfg = yaml.load(f, Loader=yaml.FullLoader)\n\n    for cfg in yaml_cfg.setdefault('BASE', ['']):\n        if cfg:\n            _update_config_from_file(\n                config, os.path.join(os.path.dirname(cfg_file), cfg)\n            )\n    print('=> merge config from {}'.format(cfg_file))\n    config.merge_from_file(cfg_file)\n    config.freeze()\n\n\ndef update_config(config, args):\n    _update_config_from_file(config, args.cfg)\n\n    config.defrost()\n    if args.opts:\n        config.merge_from_list(args.opts)\n\n    def _check_args(name):\n        if hasattr(args, name) and eval(f'args.{name}'):\n            return True\n        return False\n\n    # merge from specific arguments\n    if _check_args('batch_size'):\n        config.DATA.BATCH_SIZE = args.batch_size\n    if _check_args('data_path'):\n        config.DATA.DATA_PATH = args.data_path\n    if _check_args('zip'):\n        config.DATA.ZIP_MODE = True\n    if _check_args('cache_mode'):\n        config.DATA.CACHE_MODE = args.cache_mode\n    if _check_args('pretrained'):\n        config.MODEL.PRETRAINED = args.pretrained\n    if _check_args('resume'):\n        config.MODEL.RESUME = args.resume\n    if _check_args('accumulation_steps'):\n        config.TRAIN.ACCUMULATION_STEPS = args.accumulation_steps\n    if _check_args('use_checkpoint'):\n        config.TRAIN.USE_CHECKPOINT = True\n    if _check_args('amp_opt_level'):\n        print(\"[warning] Apex amp has been deprecated, please use pytorch amp instead!\")\n        if args.amp_opt_level == 'O0':\n            config.AMP_ENABLE = False\n    if _check_args('disable_amp'):\n        config.AMP_ENABLE = False\n    if _check_args('output'):\n        config.OUTPUT = args.output\n    if _check_args('tag'):\n        config.TAG = args.tag\n    if _check_args('eval'):\n        config.EVAL_MODE = True\n    if _check_args('throughput'):\n        config.THROUGHPUT_MODE = True\n\n    # [SimMIM]\n    if _check_args('enable_amp'):\n        config.ENABLE_AMP = args.enable_amp\n\n    # for acceleration\n    if _check_args('fused_window_process'):\n        config.FUSED_WINDOW_PROCESS = True\n    if _check_args('fused_layernorm'):\n        config.FUSED_LAYERNORM = True\n    ## Overwrite optimizer if not None, currently we use it for [fused_adam, fused_lamb]\n    if _check_args('optim'):\n        config.TRAIN.OPTIMIZER.NAME = args.optim\n\n    # set local rank for distributed training\n    if PYTORCH_MAJOR_VERSION == 1:\n        config.LOCAL_RANK = args.local_rank\n    else:\n        config.LOCAL_RANK = int(os.environ['LOCAL_RANK'])\n\n    # output folder\n    config.OUTPUT = os.path.join(config.OUTPUT, config.MODEL.NAME, config.TAG)\n\n    config.freeze()\n\n\ndef get_config(args):\n    \"\"\"Get a yacs CfgNode object with default values.\"\"\"\n    # Return a clone so that the defaults will not be altered\n    # This is for the \"local variable\" use pattern\n    config = _C.clone()\n    update_config(config, args)\n\n    return config\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "figures",
          "type": "tree",
          "content": null
        },
        {
          "name": "get_started.md",
          "type": "blob",
          "size": 11.828125,
          "content": "# Swin Transformer for Image Classification\n\nThis folder contains the implementation of the Swin Transformer for image classification.\n\n## Model Zoo\n\nPlease refer to [MODEL HUB](MODELHUB.md#imagenet-22k-pretrained-swin-moe-models) for more pre-trained models.\n\n## Usage\n\n### Install\n\nWe recommend using the pytorch docker `nvcr>=21.05` by\nnvidia: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch.\n\n- Clone this repo:\n\n```bash\ngit clone https://github.com/microsoft/Swin-Transformer.git\ncd Swin-Transformer\n```\n\n- Create a conda virtual environment and activate it:\n\n```bash\nconda create -n swin python=3.7 -y\nconda activate swin\n```\n\n- Install `CUDA>=10.2` with `cudnn>=7` following\n  the [official installation instructions](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html)\n- Install `PyTorch>=1.8.0` and `torchvision>=0.9.0` with `CUDA>=10.2`:\n\n```bash\nconda install pytorch==1.8.0 torchvision==0.9.0 cudatoolkit=10.2 -c pytorch\n```\n\n- Install `timm==0.4.12`:\n\n```bash\npip install timm==0.4.12\n```\n\n- Install other requirements:\n\n```bash\npip install opencv-python==4.4.0.46 termcolor==1.1.0 yacs==0.1.8 pyyaml scipy\n```\n\n- Install fused window process for acceleration, activated by passing `--fused_window_process` in the running script\n```bash\ncd kernels/window_process\npython setup.py install #--user\n```\n\n### Data preparation\n\nWe use standard ImageNet dataset, you can download it from http://image-net.org/. We provide the following two ways to\nload data:\n\n- For standard folder dataset, move validation images to labeled sub-folders. The file structure should look like:\n  ```bash\n  $ tree data\n  imagenet\n  ├── train\n  │   ├── class1\n  │   │   ├── img1.jpeg\n  │   │   ├── img2.jpeg\n  │   │   └── ...\n  │   ├── class2\n  │   │   ├── img3.jpeg\n  │   │   └── ...\n  │   └── ...\n  └── val\n      ├── class1\n      │   ├── img4.jpeg\n      │   ├── img5.jpeg\n      │   └── ...\n      ├── class2\n      │   ├── img6.jpeg\n      │   └── ...\n      └── ...\n \n  ```\n- To boost the slow speed when reading images from massive small files, we also support zipped ImageNet, which includes\n  four files:\n    - `train.zip`, `val.zip`: which store the zipped folder for train and validate splits.\n    - `train_map.txt`, `val_map.txt`: which store the relative path in the corresponding zip file and ground truth\n      label. Make sure the data folder looks like this:\n\n  ```bash\n  $ tree data\n  data\n  └── ImageNet-Zip\n      ├── train_map.txt\n      ├── train.zip\n      ├── val_map.txt\n      └── val.zip\n  \n  $ head -n 5 data/ImageNet-Zip/val_map.txt\n  ILSVRC2012_val_00000001.JPEG\t65\n  ILSVRC2012_val_00000002.JPEG\t970\n  ILSVRC2012_val_00000003.JPEG\t230\n  ILSVRC2012_val_00000004.JPEG\t809\n  ILSVRC2012_val_00000005.JPEG\t516\n  \n  $ head -n 5 data/ImageNet-Zip/train_map.txt\n  n01440764/n01440764_10026.JPEG\t0\n  n01440764/n01440764_10027.JPEG\t0\n  n01440764/n01440764_10029.JPEG\t0\n  n01440764/n01440764_10040.JPEG\t0\n  n01440764/n01440764_10042.JPEG\t0\n  ```\n- For ImageNet-22K dataset, make a folder named `fall11_whole` and move all images to labeled sub-folders in this\n  folder. Then download the train-val split\n  file ([ILSVRC2011fall_whole_map_train.txt](https://github.com/SwinTransformer/storage/releases/download/v2.0.1/ILSVRC2011fall_whole_map_train.txt)\n  & [ILSVRC2011fall_whole_map_val.txt](https://github.com/SwinTransformer/storage/releases/download/v2.0.1/ILSVRC2011fall_whole_map_val.txt))\n  , and put them in the parent directory of `fall11_whole`. The file structure should look like:\n\n  ```bash\n    $ tree imagenet22k/\n    imagenet22k/\n    ├── ILSVRC2011fall_whole_map_train.txt\n    ├── ILSVRC2011fall_whole_map_val.txt\n    └── fall11_whole\n        ├── n00004475\n        ├── n00005787\n        ├── n00006024\n        ├── n00006484\n        └── ...\n  ```\n\n### Evaluation\n\nTo evaluate a pre-trained `Swin Transformer` on ImageNet val, run:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node <num-of-gpus-to-use> --master_port 12345 main.py --eval \\\n--cfg <config-file> --resume <checkpoint> --data-path <imagenet-path> \n```\n\nFor example, to evaluate the `Swin-B` with a single GPU:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 1 --master_port 12345 main.py --eval \\\n--cfg configs/swin/swin_base_patch4_window7_224.yaml --resume swin_base_patch4_window7_224.pth --data-path <imagenet-path>\n```\n\n### Training from scratch on ImageNet-1K\n\nTo train a `Swin Transformer` on ImageNet from scratch, run:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node <num-of-gpus-to-use> --master_port 12345  main.py \\ \n--cfg <config-file> --data-path <imagenet-path> [--batch-size <batch-size-per-gpu> --output <output-directory> --tag <job-tag>]\n```\n\n**Notes**:\n\n- To use zipped ImageNet instead of folder dataset, add `--zip` to the parameters.\n    - To cache the dataset in the memory instead of reading from files every time, add `--cache-mode part`, which will\n      shard the dataset into non-overlapping pieces for different GPUs and only load the corresponding one for each GPU.\n- When GPU memory is not enough, you can try the following suggestions:\n    - Use gradient accumulation by adding `--accumulation-steps <steps>`, set appropriate `<steps>` according to your need.\n    - Use gradient checkpointing by adding `--use-checkpoint`, e.g., it saves about 60% memory when training `Swin-B`.\n      Please refer to [this page](https://pytorch.org/docs/stable/checkpoint.html) for more details.\n    - We recommend using multi-node with more GPUs for training very large models, a tutorial can be found\n      in [this page](https://pytorch.org/tutorials/intermediate/dist_tuto.html).\n- To change config options in general, you can use `--opts KEY1 VALUE1 KEY2 VALUE2`, e.g.,\n  `--opts TRAIN.EPOCHS 100 TRAIN.WARMUP_EPOCHS 5` will change total epochs to 100 and warm-up epochs to 5.\n- For additional options, see [config](config.py) and run `python main.py --help` to get detailed message.\n\nFor example, to train `Swin Transformer` with 8 GPU on a single node for 300 epochs, run:\n\n`Swin-T`:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 --master_port 12345  main.py \\\n--cfg configs/swin/swin_tiny_patch4_window7_224.yaml --data-path <imagenet-path> --batch-size 128 \n```\n\n`Swin-S`:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 --master_port 12345  main.py \\\n--cfg configs/swin/swin_small_patch4_window7_224.yaml --data-path <imagenet-path> --batch-size 128 \n```\n\n`Swin-B`:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 --master_port 12345  main.py \\\n--cfg configs/swin/swin_base_patch4_window7_224.yaml --data-path <imagenet-path> --batch-size 64 \\\n--accumulation-steps 2 [--use-checkpoint]\n```\n\n### Pre-training on ImageNet-22K\n\nFor example, to pre-train a `Swin-B` model on ImageNet-22K:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 --master_port 12345  main.py \\\n--cfg configs/swin/swin_base_patch4_window7_224_22k.yaml --data-path <imagenet22k-path> --batch-size 64 \\\n--accumulation-steps 8 [--use-checkpoint]\n```\n\n### Fine-tuning on higher resolution\n\nFor example, to fine-tune a `Swin-B` model pre-trained on 224x224 resolution to 384x384 resolution:\n\n```bashs\npython -m torch.distributed.launch --nproc_per_node 8 --master_port 12345  main.py \\\n--cfg configs/swin/swin_base_patch4_window12_384_finetune.yaml --pretrained swin_base_patch4_window7_224.pth \\\n--data-path <imagenet-path> --batch-size 64 --accumulation-steps 2 [--use-checkpoint]\n```\n\n### Fine-tuning from a ImageNet-22K(21K) pre-trained model\n\nFor example, to fine-tune a `Swin-B` model pre-trained on ImageNet-22K(21K):\n\n```bashs\npython -m torch.distributed.launch --nproc_per_node 8 --master_port 12345  main.py \\\n--cfg configs/swin/swin_base_patch4_window7_224_22kto1k_finetune.yaml --pretrained swin_base_patch4_window7_224_22k.pth \\\n--data-path <imagenet-path> --batch-size 64 --accumulation-steps 2 [--use-checkpoint]\n```\n\n### Throughput\n\nTo measure the throughput, run:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 1 --master_port 12345  main.py \\\n--cfg <config-file> --data-path <imagenet-path> --batch-size 64 --throughput --disable_amp\n```\n\n\n## Mixture-of-Experts Support\n\n### Install [Tutel](https://github.com/microsoft/tutel)\n```bash\npython3 -m pip uninstall tutel -y \npython3 -m pip install --user --upgrade git+https://github.com/microsoft/tutel@main\n```\n\n### Training Swin-MoE \nFor example, to train a `Swin-MoE-S` model with 32 experts on ImageNet-22K with 32 GPUs (4 nodes):\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 --nnode=4 \\\n--node_rank=<node-rank> --master_addr=<master-ip> --master_port 12345  main_moe.py \\\n--cfg configs/swinmoe/swin_moe_small_patch4_window12_192_32expert_32gpu_22k.yaml --data-path <imagenet22k-path> --batch-size 128\n```\n\n### Evaluating Swin-MoE\n\nTo evaluate a `Swin-MoE-S` with 32 experts on ImageNet-22K with 32 GPUs (4 nodes):\n\n1. Download the zip file [swin_moe_small_patch4_window12_192_32expert_32gpu_22k.zip](https://github.com/SwinTransformer/storage/releases/download/v2.0.2/swin_moe_small_patch4_window12_192_32expert_32gpu_22k.zip) which contains the pre-trained models for each rank, and unzip them to the folder \"swin_moe_small_patch4_window12_192_32expert_32gpu_22k\".\n2. Run the following evaluation command, note the checkpoint path should not contain the \".rank\\<x\\>\" suffix.\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 --nnode=4 \\\n--node_rank=<node-rank> --master_addr=<master-ip> --master_port 12345  main_moe.py \\\n--cfg configs/swinmoe/swin_moe_small_patch4_window12_192_32expert_32gpu_22k.yaml --data-path <imagenet22k-path> --batch-size 128 \\\n--resume swin_moe_small_patch4_window12_192_32expert_32gpu_22k/swin_moe_small_patch4_window12_192_32expert_32gpu_22k.pth \n```\n\nMore Swin-MoE models can be found in [MODEL HUB](MODELHUB.md#imagenet-22k-pretrained-swin-moe-models)\n\n## SimMIM Support\n\n### Evaluating provided models\n\nTo evaluate a provided model on ImageNet validation set, run:\n```bash\npython -m torch.distributed.launch --nproc_per_node <num-of-gpus-to-use> main_simmim_ft.py \\\n--eval --cfg <config-file> --resume <checkpoint> --data-path <imagenet-path>\n```\n\nFor example, to evaluate the `Swin Base` model on a single GPU, run:\n```bash\npython -m torch.distributed.launch --nproc_per_node 1 main_simmim_ft.py \\\n--eval --cfg configs/simmim/simmim_finetune__swin_base__img224_window7__800ep.yaml --resume simmim_finetune__swin_base__img224_window7__800ep.pth --data-path <imagenet-path>\n```\n\n### Pre-training with SimMIM\nTo pre-train models with `SimMIM`, run:\n```bash\npython -m torch.distributed.launch --nproc_per_node <num-of-gpus-to-use> main_simmim_pt.py \\ \n--cfg <config-file> --data-path <imagenet-path>/train [--batch-size <batch-size-per-gpu> --output <output-directory> --tag <job-tag>]\n```\n\nFor example, to pre-train `Swin Base` for 800 epochs on one DGX-2 server, run:\n```bash\npython -m torch.distributed.launch --nproc_per_node 16 main_simmim_pt.py \\ \n--cfg configs/simmim/simmim_pretrain__swin_base__img192_window6__800ep.yaml --batch-size 128 --data-path <imagenet-path>/train [--output <output-directory> --tag <job-tag>]\n```\n\n### Fine-tuning pre-trained models\nTo fine-tune models pre-trained by `SimMIM`, run:\n```bash\npython -m torch.distributed.launch --nproc_per_node <num-of-gpus-to-use> main_simmim_ft.py \\ \n--cfg <config-file> --data-path <imagenet-path> --pretrained <pretrained-ckpt> [--batch-size <batch-size-per-gpu> --output <output-directory> --tag <job-tag>]\n```\n\nFor example, to fine-tune `Swin Base` pre-trained by `SimMIM` on one DGX-2 server, run:\n```bash\npython -m torch.distributed.launch --nproc_per_node 16 main_simmim_ft.py \\ \n--cfg configs/simmim/simmim_finetune__swin_base__img224_window7__800ep.yaml --batch-size 128 --data-path <imagenet-path> --pretrained <pretrained-ckpt> [--output <output-directory> --tag <job-tag>]\n```"
        },
        {
          "name": "kernels",
          "type": "tree",
          "content": null
        },
        {
          "name": "logger.py",
          "type": "blob",
          "size": 1.4169921875,
          "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport os\nimport sys\nimport logging\nimport functools\nfrom termcolor import colored\n\n\n@functools.lru_cache()\ndef create_logger(output_dir, dist_rank=0, name=''):\n    # create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.propagate = False\n\n    # create formatter\n    fmt = '[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s'\n    color_fmt = colored('[%(asctime)s %(name)s]', 'green') + \\\n                colored('(%(filename)s %(lineno)d)', 'yellow') + ': %(levelname)s %(message)s'\n\n    # create console handlers for master process\n    if dist_rank == 0:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(logging.DEBUG)\n        console_handler.setFormatter(\n            logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n        logger.addHandler(console_handler)\n\n    # create file handlers\n    file_handler = logging.FileHandler(os.path.join(output_dir, f'log_rank{dist_rank}.txt'), mode='a')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n    logger.addHandler(file_handler)\n\n    return logger\n"
        },
        {
          "name": "lr_scheduler.py",
          "type": "blob",
          "size": 5.3388671875,
          "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport bisect\n\nimport torch\nfrom timm.scheduler.cosine_lr import CosineLRScheduler\nfrom timm.scheduler.step_lr import StepLRScheduler\nfrom timm.scheduler.scheduler import Scheduler\n\n\ndef build_scheduler(config, optimizer, n_iter_per_epoch):\n    num_steps = int(config.TRAIN.EPOCHS * n_iter_per_epoch)\n    warmup_steps = int(config.TRAIN.WARMUP_EPOCHS * n_iter_per_epoch)\n    decay_steps = int(config.TRAIN.LR_SCHEDULER.DECAY_EPOCHS * n_iter_per_epoch)\n    multi_steps = [i * n_iter_per_epoch for i in config.TRAIN.LR_SCHEDULER.MULTISTEPS]\n\n    lr_scheduler = None\n    if config.TRAIN.LR_SCHEDULER.NAME == 'cosine':\n        lr_scheduler = CosineLRScheduler(\n            optimizer,\n            t_initial=(num_steps - warmup_steps) if config.TRAIN.LR_SCHEDULER.WARMUP_PREFIX else num_steps,\n            t_mul=1.,\n            lr_min=config.TRAIN.MIN_LR,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            cycle_limit=1,\n            t_in_epochs=False,\n            warmup_prefix=config.TRAIN.LR_SCHEDULER.WARMUP_PREFIX,\n        )\n    elif config.TRAIN.LR_SCHEDULER.NAME == 'linear':\n        lr_scheduler = LinearLRScheduler(\n            optimizer,\n            t_initial=num_steps,\n            lr_min_rate=0.01,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            t_in_epochs=False,\n        )\n    elif config.TRAIN.LR_SCHEDULER.NAME == 'step':\n        lr_scheduler = StepLRScheduler(\n            optimizer,\n            decay_t=decay_steps,\n            decay_rate=config.TRAIN.LR_SCHEDULER.DECAY_RATE,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            t_in_epochs=False,\n        )\n    elif config.TRAIN.LR_SCHEDULER.NAME == 'multistep':\n        lr_scheduler = MultiStepLRScheduler(\n            optimizer,\n            milestones=multi_steps,\n            gamma=config.TRAIN.LR_SCHEDULER.GAMMA,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            t_in_epochs=False,\n        )\n\n    return lr_scheduler\n\n\nclass LinearLRScheduler(Scheduler):\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 t_initial: int,\n                 lr_min_rate: float,\n                 warmup_t=0,\n                 warmup_lr_init=0.,\n                 t_in_epochs=True,\n                 noise_range_t=None,\n                 noise_pct=0.67,\n                 noise_std=1.0,\n                 noise_seed=42,\n                 initialize=True,\n                 ) -> None:\n        super().__init__(\n            optimizer, param_group_field=\"lr\",\n            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n            initialize=initialize)\n\n        self.t_initial = t_initial\n        self.lr_min_rate = lr_min_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            t = t - self.warmup_t\n            total_t = self.t_initial - self.warmup_t\n            lrs = [v - ((v - v * self.lr_min_rate) * (t / total_t)) for v in self.base_values]\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None\n\n\nclass MultiStepLRScheduler(Scheduler):\n    def __init__(self, optimizer: torch.optim.Optimizer, milestones, gamma=0.1, warmup_t=0, warmup_lr_init=0, t_in_epochs=True) -> None:\n        super().__init__(optimizer, param_group_field=\"lr\")\n        \n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n        \n        assert self.warmup_t <= min(self.milestones)\n    \n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            lrs = [v * (self.gamma ** bisect.bisect_right(self.milestones, t)) for v in self.base_values]\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None\n"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 14.806640625,
          "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport os\nimport time\nimport json\nimport random\nimport argparse\nimport datetime\nimport numpy as np\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\n\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.utils import accuracy, AverageMeter\n\nfrom config import get_config\nfrom models import build_model\nfrom data import build_loader\nfrom lr_scheduler import build_scheduler\nfrom optimizer import build_optimizer\nfrom logger import create_logger\nfrom utils import load_checkpoint, load_pretrained, save_checkpoint, NativeScalerWithGradNormCount, auto_resume_helper, \\\n    reduce_tensor\n\n# pytorch major version (1.x or 2.x)\nPYTORCH_MAJOR_VERSION = int(torch.__version__.split('.')[0])\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('Swin Transformer training and evaluation script', add_help=False)\n    parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file', )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n        default=None,\n        nargs='+',\n    )\n\n    # easy config modification\n    parser.add_argument('--batch-size', type=int, help=\"batch size for single GPU\")\n    parser.add_argument('--data-path', type=str, help='path to dataset')\n    parser.add_argument('--zip', action='store_true', help='use zipped dataset instead of folder dataset')\n    parser.add_argument('--cache-mode', type=str, default='part', choices=['no', 'full', 'part'],\n                        help='no: no cache, '\n                             'full: cache all data, '\n                             'part: sharding the dataset into nonoverlapping pieces and only cache one piece')\n    parser.add_argument('--pretrained',\n                        help='pretrained weight from checkpoint, could be imagenet22k pretrained weight')\n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true',\n                        help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--disable_amp', action='store_true', help='Disable pytorch amp')\n    parser.add_argument('--amp-opt-level', type=str, choices=['O0', 'O1', 'O2'],\n                        help='mixed precision opt level, if O0, no amp is used (deprecated!)')\n    parser.add_argument('--output', default='output', type=str, metavar='PATH',\n                        help='root of output folder, the full path is <output>/<model_name>/<tag> (default: output)')\n    parser.add_argument('--tag', help='tag of experiment')\n    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n\n    # distributed training\n    # for pytorch >= 2.0, use `os.environ['LOCAL_RANK']` instead\n    # (see https://pytorch.org/docs/stable/distributed.html#launch-utility)\n    if PYTORCH_MAJOR_VERSION == 1:\n        parser.add_argument(\"--local_rank\", type=int, required=True, help='local rank for DistributedDataParallel')\n\n    # for acceleration\n    parser.add_argument('--fused_window_process', action='store_true',\n                        help='Fused window shift & window partition, similar for reversed part.')\n    parser.add_argument('--fused_layernorm', action='store_true', help='Use fused layernorm.')\n    ## overwrite optimizer in config (*.yaml) if specified, e.g., fused_adam/fused_lamb\n    parser.add_argument('--optim', type=str,\n                        help='overwrite optimizer if provided, can be adamw/sgd/fused_adam/fused_lamb.')\n\n    args, unparsed = parser.parse_known_args()\n\n    config = get_config(args)\n\n    return args, config\n\n\ndef main(config):\n    dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config)\n\n    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n    model = build_model(config)\n    logger.info(str(model))\n\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info(f\"number of params: {n_parameters}\")\n    if hasattr(model, 'flops'):\n        flops = model.flops()\n        logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n\n    model.cuda()\n    model_without_ddp = model\n\n    optimizer = build_optimizer(config, model)\n    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False)\n    loss_scaler = NativeScalerWithGradNormCount()\n\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train) // config.TRAIN.ACCUMULATION_STEPS)\n    else:\n        lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train))\n\n    if config.AUG.MIXUP > 0.:\n        # smoothing is handled with mixup label transform\n        criterion = SoftTargetCrossEntropy()\n    elif config.MODEL.LABEL_SMOOTHING > 0.:\n        criterion = LabelSmoothingCrossEntropy(smoothing=config.MODEL.LABEL_SMOOTHING)\n    else:\n        criterion = torch.nn.CrossEntropyLoss()\n\n    max_accuracy = 0.0\n\n    if config.TRAIN.AUTO_RESUME:\n        resume_file = auto_resume_helper(config.OUTPUT)\n        if resume_file:\n            if config.MODEL.RESUME:\n                logger.warning(f\"auto-resume changing resume file from {config.MODEL.RESUME} to {resume_file}\")\n            config.defrost()\n            config.MODEL.RESUME = resume_file\n            config.freeze()\n            logger.info(f'auto resuming from {resume_file}')\n        else:\n            logger.info(f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')\n\n    if config.MODEL.RESUME:\n        max_accuracy = load_checkpoint(config, model_without_ddp, optimizer, lr_scheduler, loss_scaler, logger)\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        if config.EVAL_MODE:\n            return\n\n    if config.MODEL.PRETRAINED and (not config.MODEL.RESUME):\n        load_pretrained(config, model_without_ddp, logger)\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n\n    if config.THROUGHPUT_MODE:\n        throughput(data_loader_val, model, logger)\n        return\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n        data_loader_train.sampler.set_epoch(epoch)\n\n        train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler,\n                        loss_scaler)\n        if dist.get_rank() == 0 and (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)):\n            save_checkpoint(config, epoch, model_without_ddp, max_accuracy, optimizer, lr_scheduler, loss_scaler,\n                            logger)\n\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        max_accuracy = max(max_accuracy, acc1)\n        logger.info(f'Max accuracy: {max_accuracy:.2f}%')\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))\n\n\ndef train_one_epoch(config, model, criterion, data_loader, optimizer, epoch, mixup_fn, lr_scheduler, loss_scaler):\n    model.train()\n    optimizer.zero_grad()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    norm_meter = AverageMeter()\n    scaler_meter = AverageMeter()\n\n    start = time.time()\n    end = time.time()\n    for idx, (samples, targets) in enumerate(data_loader):\n        samples = samples.cuda(non_blocking=True)\n        targets = targets.cuda(non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets)\n\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            outputs = model(samples)\n        loss = criterion(outputs, targets)\n        loss = loss / config.TRAIN.ACCUMULATION_STEPS\n\n        # this attribute is added by timm on one optimizer (adahessian)\n        is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n        grad_norm = loss_scaler(loss, optimizer, clip_grad=config.TRAIN.CLIP_GRAD,\n                                parameters=model.parameters(), create_graph=is_second_order,\n                                update_grad=(idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0)\n        if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n            optimizer.zero_grad()\n            lr_scheduler.step_update((epoch * num_steps + idx) // config.TRAIN.ACCUMULATION_STEPS)\n        loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n\n        torch.cuda.synchronize()\n\n        loss_meter.update(loss.item(), targets.size(0))\n        if grad_norm is not None:  # loss_scaler return None if not update\n            norm_meter.update(grad_norm)\n        scaler_meter.update(loss_scale_value)\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            lr = optimizer.param_groups[0]['lr']\n            wd = optimizer.param_groups[0]['weight_decay']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            logger.info(\n                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t wd {wd:.4f}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'grad_norm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n                f'loss_scale {scaler_meter.val:.4f} ({scaler_meter.avg:.4f})\\t'\n                f'mem {memory_used:.0f}MB')\n    epoch_time = time.time() - start\n    logger.info(f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\n@torch.no_grad()\ndef validate(config, data_loader, model):\n    criterion = torch.nn.CrossEntropyLoss()\n    model.eval()\n\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n\n    end = time.time()\n    for idx, (images, target) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            output = model(images)\n\n        # measure accuracy and record loss\n        loss = criterion(output, target)\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        acc1 = reduce_tensor(acc1)\n        acc5 = reduce_tensor(acc5)\n        loss = reduce_tensor(loss)\n\n        loss_meter.update(loss.item(), target.size(0))\n        acc1_meter.update(acc1.item(), target.size(0))\n        acc5_meter.update(acc5.item(), target.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(\n                f'Test: [{idx}/{len(data_loader)}]\\t'\n                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n                f'Mem {memory_used:.0f}MB')\n    logger.info(f' * Acc@1 {acc1_meter.avg:.3f} Acc@5 {acc5_meter.avg:.3f}')\n    return acc1_meter.avg, acc5_meter.avg, loss_meter.avg\n\n\n@torch.no_grad()\ndef throughput(data_loader, model, logger):\n    model.eval()\n\n    for idx, (images, _) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        batch_size = images.shape[0]\n        for i in range(50):\n            model(images)\n        torch.cuda.synchronize()\n        logger.info(f\"throughput averaged with 30 times\")\n        tic1 = time.time()\n        for i in range(30):\n            model(images)\n        torch.cuda.synchronize()\n        tic2 = time.time()\n        logger.info(f\"batch_size {batch_size} throughput {30 * batch_size / (tic2 - tic1)}\")\n        return\n\n\nif __name__ == '__main__':\n    args, config = parse_option()\n\n    if config.AMP_OPT_LEVEL:\n        print(\"[warning] Apex amp has been deprecated, please use pytorch amp instead!\")\n\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ['WORLD_SIZE'])\n        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n    else:\n        rank = -1\n        world_size = -1\n    torch.cuda.set_device(config.LOCAL_RANK)\n    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)\n    torch.distributed.barrier()\n\n    seed = config.SEED + dist.get_rank()\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    # linear scale the learning rate according to total batch size, may not be optimal\n    linear_scaled_lr = config.TRAIN.BASE_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_warmup_lr = config.TRAIN.WARMUP_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_min_lr = config.TRAIN.MIN_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    # gradient accumulation also need to scale the learning rate\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        linear_scaled_lr = linear_scaled_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_warmup_lr = linear_scaled_warmup_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_min_lr = linear_scaled_min_lr * config.TRAIN.ACCUMULATION_STEPS\n    config.defrost()\n    config.TRAIN.BASE_LR = linear_scaled_lr\n    config.TRAIN.WARMUP_LR = linear_scaled_warmup_lr\n    config.TRAIN.MIN_LR = linear_scaled_min_lr\n    config.freeze()\n\n    os.makedirs(config.OUTPUT, exist_ok=True)\n    logger = create_logger(output_dir=config.OUTPUT, dist_rank=dist.get_rank(), name=f\"{config.MODEL.NAME}\")\n\n    if dist.get_rank() == 0:\n        path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n        logger.info(f\"Full config saved to {path}\")\n\n    # print config\n    logger.info(config.dump())\n    logger.info(json.dumps(vars(args)))\n\n    main(config)\n"
        },
        {
          "name": "main_moe.py",
          "type": "blob",
          "size": 16.1162109375,
          "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nfrom tutel import system\n\nimport os\nimport time\nimport json\nimport random\nimport argparse\nimport datetime\nimport numpy as np\nfrom functools import partial\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\n\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.utils import accuracy, AverageMeter\n\nfrom config import get_config\nfrom models import build_model\nfrom data import build_loader\nfrom lr_scheduler import build_scheduler\nfrom optimizer import build_optimizer\nfrom logger import create_logger\nfrom utils import NativeScalerWithGradNormCount, reduce_tensor\nfrom utils_moe import load_checkpoint, load_pretrained, save_checkpoint, auto_resume_helper, hook_scale_grad\n\nassert torch.__version__ >= '1.8.0', \"DDP-based MoE requires Pytorch >= 1.8.0\"\n\n# pytorch major version (1.x or 2.x)\nPYTORCH_MAJOR_VERSION = int(torch.__version__.split('.')[0])\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('Swin Transformer training and evaluation script', add_help=False)\n    parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file', )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n        default=None,\n        nargs='+',\n    )\n\n    # easy config modification\n    parser.add_argument('--batch-size', type=int, help=\"batch size for single GPU\")\n    parser.add_argument('--data-path', type=str, help='path to dataset')\n    parser.add_argument('--zip', action='store_true', help='use zipped dataset instead of folder dataset')\n    parser.add_argument('--cache-mode', type=str, default='part', choices=['no', 'full', 'part'],\n                        help='no: no cache, '\n                             'full: cache all data, '\n                             'part: sharding the dataset into nonoverlapping pieces and only cache one piece')\n    parser.add_argument('--pretrained',\n                        help='pretrained weight from checkpoint, could be imagenet22k pretrained weight')\n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true',\n                        help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--disable_amp', action='store_true', help='Disable pytorch amp')\n    parser.add_argument('--amp-opt-level', type=str, choices=['O0', 'O1', 'O2'],\n                        help='mixed precision opt level, if O0, no amp is used (deprecated!)')\n    parser.add_argument('--output', default='output', type=str, metavar='PATH',\n                        help='root of output folder, the full path is <output>/<model_name>/<tag> (default: output)')\n    parser.add_argument('--tag', help='tag of experiment')\n    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n\n    # distributed training\n    # for pytorch >= 2.0, use `os.environ['LOCAL_RANK']` instead\n    # (see https://pytorch.org/docs/stable/distributed.html#launch-utility)\n    if PYTORCH_MAJOR_VERSION == 1:\n        parser.add_argument(\"--local_rank\", type=int, required=True, help='local rank for DistributedDataParallel')\n\n    args, unparsed = parser.parse_known_args()\n\n    config = get_config(args)\n\n    return args, config\n\n\ndef main(config):\n    dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config)\n\n    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n    model = build_model(config)\n    logger.info(str(model))\n\n    # For Tutel MoE\n    for name, param in model.named_parameters():\n        if param.requires_grad == True and hasattr(param, 'skip_allreduce') and param.skip_allreduce is True:\n            model.add_param_to_skip_allreduce(name)\n            param.register_hook(partial(hook_scale_grad, dist.get_world_size()))\n            logger.info(f\"[rank{dist.get_rank()}] [{name}] skip all_reduce and div {dist.get_world_size()} for grad\")\n\n    n_parameters_single = sum(p.numel() * model.sharded_count if hasattr(p, 'skip_allreduce')\n                              else p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info(f\"number of params single: {n_parameters_single}\")\n    n_parameters_whole = sum(p.numel() * model.sharded_count * model.global_experts if hasattr(p, 'skip_allreduce')\n                             else p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info(f\"number of params whole: {n_parameters_whole}\")\n    if hasattr(model, 'flops'):\n        flops = model.flops()\n        logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n\n    model.cuda(config.LOCAL_RANK)\n    model_without_ddp = model\n\n    optimizer = build_optimizer(config, model)\n    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False)\n    loss_scaler = NativeScalerWithGradNormCount()\n\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train) // config.TRAIN.ACCUMULATION_STEPS)\n    else:\n        lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train))\n\n    if config.AUG.MIXUP > 0.:\n        # smoothing is handled with mixup label transform\n        criterion = SoftTargetCrossEntropy()\n    elif config.MODEL.LABEL_SMOOTHING > 0.:\n        criterion = LabelSmoothingCrossEntropy(smoothing=config.MODEL.LABEL_SMOOTHING)\n    else:\n        criterion = torch.nn.CrossEntropyLoss()\n\n    max_accuracy = 0.0\n\n    if config.TRAIN.AUTO_RESUME:\n        resume_file = auto_resume_helper(config.OUTPUT, config.TRAIN.MOE.SAVE_MASTER)\n        if resume_file:\n            if config.MODEL.RESUME:\n                logger.warning(f\"auto-resume changing resume file from {config.MODEL.RESUME} to {resume_file}\")\n            config.defrost()\n            config.MODEL.RESUME = resume_file\n            config.freeze()\n            logger.info(f'auto resuming from {resume_file}')\n        else:\n            logger.info(f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')\n\n    if config.MODEL.RESUME:\n        max_accuracy = load_checkpoint(config, model_without_ddp, optimizer, lr_scheduler, loss_scaler, logger)\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        if config.EVAL_MODE:\n            return\n\n    if config.MODEL.PRETRAINED and (not config.MODEL.RESUME):\n        load_pretrained(config, model_without_ddp, logger)\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        if config.EVAL_MODE:\n            return\n\n    if config.THROUGHPUT_MODE:\n        throughput(data_loader_val, model, logger)\n        return\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n        data_loader_train.sampler.set_epoch(epoch)\n\n        train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler,\n                        loss_scaler)\n        if (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)):\n            save_checkpoint(config, epoch, model_without_ddp, max_accuracy, optimizer, lr_scheduler, loss_scaler,\n                            logger)\n\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        max_accuracy = max(max_accuracy, acc1)\n        logger.info(f'Max accuracy: {max_accuracy:.2f}%')\n    save_checkpoint(config, 'final', model_without_ddp, max_accuracy, optimizer, lr_scheduler, loss_scaler,\n                    logger, zero_redundancy=True)\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))\n\n\ndef train_one_epoch(config, model, criterion, data_loader, optimizer, epoch, mixup_fn, lr_scheduler, loss_scaler):\n    model.train()\n    optimizer.zero_grad()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    loss_aux_meter = AverageMeter()\n    loss_cls_meter = AverageMeter()\n    norm_meter = AverageMeter()\n    scaler_meter = AverageMeter()\n\n    start = time.time()\n    end = time.time()\n    for idx, (samples, targets) in enumerate(data_loader):\n        samples = samples.cuda(non_blocking=True)\n        targets = targets.cuda(non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets)\n\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            outputs, l_aux = model(samples)\n        l_cls = criterion(outputs, targets)\n        loss = l_cls + l_aux\n        loss = loss / config.TRAIN.ACCUMULATION_STEPS\n\n        # this attribute is added by timm on one optimizer (adahessian)\n        is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n        grad_norm = loss_scaler(loss, optimizer, clip_grad=config.TRAIN.CLIP_GRAD,\n                                parameters=model.parameters(), create_graph=is_second_order,\n                                update_grad=(idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0)\n        if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n            optimizer.zero_grad()\n            lr_scheduler.step_update((epoch * num_steps + idx) // config.TRAIN.ACCUMULATION_STEPS)\n        loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n\n        torch.cuda.synchronize()\n\n        loss_meter.update(loss.item(), targets.size(0))\n        loss_cls_meter.update(l_cls.item(), targets.size(0))\n        loss_aux_meter.update(l_aux if isinstance(l_aux, float) else l_aux.item(), targets.size(0))\n        if grad_norm is not None:  # loss_scaler return None if not update\n            norm_meter.update(grad_norm)\n        scaler_meter.update(loss_scale_value)\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            lr = optimizer.param_groups[0]['lr']\n            wd = optimizer.param_groups[0]['weight_decay']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            logger.info(\n                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t wd {wd:.4f}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'loss-cls {loss_cls_meter.val:.4f} ({loss_cls_meter.avg:.4f})\\t'\n                f'loss-aux {loss_aux_meter.val:.4f} ({loss_aux_meter.avg:.4f})\\t'\n                f'grad_norm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n                f'loss_scale {scaler_meter.val:.4f} ({scaler_meter.avg:.4f})\\t'\n                f'mem {memory_used:.0f}MB')\n    epoch_time = time.time() - start\n    logger.info(f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\n@torch.no_grad()\ndef validate(config, data_loader, model):\n    criterion = torch.nn.CrossEntropyLoss()\n    model.eval()\n\n    batch_time = AverageMeter()\n    loss_cls_meter = AverageMeter()\n    loss_aux_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n\n    end = time.time()\n    for idx, (images, target) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            output, l_aux = model(images)\n\n        # measure accuracy and record loss\n        l_cls = criterion(output, target)\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        acc1 = reduce_tensor(acc1)\n        acc5 = reduce_tensor(acc5)\n\n        loss_cls_meter.update(l_cls.item(), target.size(0))\n        loss_aux_meter.update(l_aux if isinstance(l_aux, float) else l_aux.item(), target.size(0))\n        acc1_meter.update(acc1.item(), target.size(0))\n        acc5_meter.update(acc5.item(), target.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(\n                f'Test: [{idx}/{len(data_loader)}]\\t'\n                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                f'Loss-Cls {loss_cls_meter.val:.4f} ({loss_cls_meter.avg:.4f})\\t'\n                f'Loss-Aux {loss_aux_meter.val:.4f} ({loss_aux_meter.avg:.4f})\\t'\n                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n                f'Mem {memory_used:.0f}MB')\n    logger.info(f' * Acc@1 {acc1_meter.avg:.3f} Acc@5 {acc5_meter.avg:.3f}')\n    return acc1_meter.avg, acc5_meter.avg, loss_cls_meter.avg\n\n\n@torch.no_grad()\ndef throughput(data_loader, model, logger):\n    model.eval()\n\n    for idx, (images, _) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        batch_size = images.shape[0]\n        for i in range(50):\n            model(images)\n        torch.cuda.synchronize()\n        logger.info(f\"throughput averaged with 30 times\")\n        tic1 = time.time()\n        for i in range(30):\n            model(images)\n        torch.cuda.synchronize()\n        tic2 = time.time()\n        logger.info(f\"batch_size {batch_size} throughput {30 * batch_size / (tic2 - tic1)}\")\n        return\n\n\nif __name__ == '__main__':\n    args, config = parse_option()\n\n    if config.AMP_OPT_LEVEL:\n        print(\"[warning] Apex amp has been deprecated, please use pytorch amp instead!\")\n\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ['WORLD_SIZE'])\n        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n    else:\n        rank = -1\n        world_size = -1\n    torch.cuda.set_device(config.LOCAL_RANK)\n    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)\n    torch.distributed.barrier()\n\n    seed = config.SEED + dist.get_rank()\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    # linear scale the learning rate according to total batch size, may not be optimal\n    linear_scaled_lr = config.TRAIN.BASE_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_warmup_lr = config.TRAIN.WARMUP_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_min_lr = config.TRAIN.MIN_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    # gradient accumulation also need to scale the learning rate\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        linear_scaled_lr = linear_scaled_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_warmup_lr = linear_scaled_warmup_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_min_lr = linear_scaled_min_lr * config.TRAIN.ACCUMULATION_STEPS\n    config.defrost()\n    config.TRAIN.BASE_LR = linear_scaled_lr\n    config.TRAIN.WARMUP_LR = linear_scaled_warmup_lr\n    config.TRAIN.MIN_LR = linear_scaled_min_lr\n    config.freeze()\n\n    os.makedirs(config.OUTPUT, exist_ok=True)\n    logger = create_logger(output_dir=config.OUTPUT, dist_rank=dist.get_rank(), name=f\"{config.MODEL.NAME}\")\n\n    if dist.get_rank() == 0:\n        path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n        logger.info(f\"Full config saved to {path}\")\n\n    # print config\n    logger.info(config.dump())\n    logger.info(json.dumps(vars(args)))\n\n    main(config)\n"
        },
        {
          "name": "main_simmim_ft.py",
          "type": "blob",
          "size": 13.7353515625,
          "content": "# --------------------------------------------------------\n# SimMIM\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# Modified by Zhenda Xie\n# --------------------------------------------------------\n\nimport os\nimport time\nimport argparse\nimport datetime\nimport numpy as np\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.cuda.amp as amp\n\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.utils import accuracy, AverageMeter\n\nfrom config import get_config\nfrom models import build_model\nfrom data import build_loader\nfrom lr_scheduler import build_scheduler\nfrom optimizer import build_optimizer\nfrom logger import create_logger\nfrom utils_simmim import load_checkpoint, load_pretrained, save_checkpoint, get_grad_norm, auto_resume_helper, \\\n    reduce_tensor\n\n# pytorch major version (1.x or 2.x)\nPYTORCH_MAJOR_VERSION = int(torch.__version__.split('.')[0])\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('SimMIM fine-tuning script', add_help=False)\n    parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file', )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n        default=None,\n        nargs='+',\n    )\n\n    # easy config modification\n    parser.add_argument('--batch-size', type=int, help=\"batch size for single GPU\")\n    parser.add_argument('--data-path', type=str, help='path to dataset')\n    parser.add_argument('--pretrained', type=str, help='path to pre-trained model')\n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true',\n                        help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--enable-amp', action='store_true')\n    parser.add_argument('--disable-amp', action='store_false', dest='enable_amp')\n    parser.set_defaults(enable_amp=True)\n    parser.add_argument('--output', default='output', type=str, metavar='PATH',\n                        help='root of output folder, the full path is <output>/<model_name>/<tag> (default: output)')\n    parser.add_argument('--tag', help='tag of experiment')\n    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n\n    # distributed training\n    # for pytorch >= 2.0, use `os.environ['LOCAL_RANK']` instead\n    # (see https://pytorch.org/docs/stable/distributed.html#launch-utility)\n    if PYTORCH_MAJOR_VERSION == 1:\n        parser.add_argument(\"--local_rank\", type=int, required=True, help='local rank for DistributedDataParallel')\n\n    args = parser.parse_args()\n\n    config = get_config(args)\n\n    return args, config\n\n\ndef main(config):\n    dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config, simmim=True,\n                                                                                            is_pretrain=False)\n\n    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n    model = build_model(config, is_pretrain=False)\n    model.cuda()\n    logger.info(str(model))\n\n    optimizer = build_optimizer(config, model, simmim=True, is_pretrain=False)\n    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False)\n    model_without_ddp = model.module\n\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info(f\"number of params: {n_parameters}\")\n    if hasattr(model_without_ddp, 'flops'):\n        flops = model_without_ddp.flops()\n        logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n\n    lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train))\n    scaler = amp.GradScaler()\n\n    if config.AUG.MIXUP > 0.:\n        # smoothing is handled with mixup label transform\n        criterion = SoftTargetCrossEntropy()\n    elif config.MODEL.LABEL_SMOOTHING > 0.:\n        criterion = LabelSmoothingCrossEntropy(smoothing=config.MODEL.LABEL_SMOOTHING)\n    else:\n        criterion = torch.nn.CrossEntropyLoss()\n\n    max_accuracy = 0.0\n\n    if config.TRAIN.AUTO_RESUME:\n        resume_file = auto_resume_helper(config.OUTPUT, logger)\n        if resume_file:\n            if config.MODEL.RESUME:\n                logger.warning(f\"auto-resume changing resume file from {config.MODEL.RESUME} to {resume_file}\")\n            config.defrost()\n            config.MODEL.RESUME = resume_file\n            config.freeze()\n            logger.info(f'auto resuming from {resume_file}')\n        else:\n            logger.info(f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')\n\n    if config.MODEL.RESUME:\n        max_accuracy = load_checkpoint(config, model_without_ddp, optimizer, lr_scheduler, scaler, logger)\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        if config.EVAL_MODE:\n            return\n\n    if config.MODEL.PRETRAINED and (not config.MODEL.RESUME):\n        load_pretrained(config, model_without_ddp, logger)\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n\n    if config.THROUGHPUT_MODE:\n        throughput(data_loader_val, model, logger)\n        return\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n        data_loader_train.sampler.set_epoch(epoch)\n\n        train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler, scaler)\n        if dist.get_rank() == 0 and (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)):\n            save_checkpoint(config, epoch, model_without_ddp, max_accuracy, optimizer, lr_scheduler, scaler, logger)\n\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        max_accuracy = max(max_accuracy, acc1)\n        logger.info(f'Max accuracy: {max_accuracy:.2f}%')\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))\n\n\ndef train_one_epoch(config, model, criterion, data_loader, optimizer, epoch, mixup_fn, lr_scheduler, scaler):\n    model.train()\n    optimizer.zero_grad()\n\n    logger.info(f'Current learning rate for different parameter groups: {[it[\"lr\"] for it in optimizer.param_groups]}')\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    norm_meter = AverageMeter()\n    loss_scale_meter = AverageMeter()\n\n    start = time.time()\n    end = time.time()\n    for idx, (samples, targets) in enumerate(data_loader):\n        samples = samples.cuda(non_blocking=True)\n        targets = targets.cuda(non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets)\n\n        outputs = model(samples)\n\n        if config.TRAIN.ACCUMULATION_STEPS > 1:\n            loss = criterion(outputs, targets)\n            loss = loss / config.TRAIN.ACCUMULATION_STEPS\n            scaler.scale(loss).backward()\n            if config.TRAIN.CLIP_GRAD:\n                scaler.unscale_(optimizer)\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n            else:\n                grad_norm = get_grad_norm(model.parameters())\n            if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n                scaler.step(optimizer)\n                optimizer.zero_grad()\n                scaler.update()\n                lr_scheduler.step_update(epoch * num_steps + idx)\n        else:\n            loss = criterion(outputs, targets)\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            if config.TRAIN.CLIP_GRAD:\n                scaler.unscale_(optimizer)\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n            else:\n                grad_norm = get_grad_norm(model.parameters())\n            scaler.step(optimizer)\n            scaler.update()\n            lr_scheduler.step_update(epoch * num_steps + idx)\n\n        torch.cuda.synchronize()\n\n        loss_meter.update(loss.item(), targets.size(0))\n        norm_meter.update(grad_norm)\n        loss_scale_meter.update(scaler.get_scale())\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            lr = optimizer.param_groups[-1]['lr']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            logger.info(\n                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'grad_norm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n                f'loss_scale {loss_scale_meter.val:.4f} ({loss_scale_meter.avg:.4f})\\t'\n                f'mem {memory_used:.0f}MB')\n    epoch_time = time.time() - start\n    logger.info(f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\n@torch.no_grad()\ndef validate(config, data_loader, model):\n    criterion = torch.nn.CrossEntropyLoss()\n    model.eval()\n\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n\n    end = time.time()\n    for idx, (images, target) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        output = model(images)\n\n        # measure accuracy and record loss\n        loss = criterion(output, target)\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        acc1 = reduce_tensor(acc1)\n        acc5 = reduce_tensor(acc5)\n        loss = reduce_tensor(loss)\n\n        loss_meter.update(loss.item(), target.size(0))\n        acc1_meter.update(acc1.item(), target.size(0))\n        acc5_meter.update(acc5.item(), target.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(\n                f'Test: [{idx}/{len(data_loader)}]\\t'\n                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n                f'Mem {memory_used:.0f}MB')\n    logger.info(f' * Acc@1 {acc1_meter.avg:.3f} Acc@5 {acc5_meter.avg:.3f}')\n    return acc1_meter.avg, acc5_meter.avg, loss_meter.avg\n\n\n@torch.no_grad()\ndef throughput(data_loader, model, logger):\n    model.eval()\n\n    for idx, (images, _) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        batch_size = images.shape[0]\n        for i in range(50):\n            model(images)\n        torch.cuda.synchronize()\n        logger.info(f\"throughput averaged with 30 times\")\n        tic1 = time.time()\n        for i in range(30):\n            model(images)\n        torch.cuda.synchronize()\n        tic2 = time.time()\n        logger.info(f\"batch_size {batch_size} throughput {30 * batch_size / (tic2 - tic1)}\")\n        return\n\n\nif __name__ == '__main__':\n    _, config = parse_option()\n\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ['WORLD_SIZE'])\n        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n    else:\n        rank = -1\n        world_size = -1\n    torch.cuda.set_device(config.LOCAL_RANK)\n    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)\n    torch.distributed.barrier()\n\n    seed = config.SEED + dist.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    cudnn.benchmark = True\n\n    # linear scale the learning rate according to total batch size, may not be optimal\n    linear_scaled_lr = config.TRAIN.BASE_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_warmup_lr = config.TRAIN.WARMUP_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_min_lr = config.TRAIN.MIN_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    # gradient accumulation also need to scale the learning rate\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        linear_scaled_lr = linear_scaled_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_warmup_lr = linear_scaled_warmup_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_min_lr = linear_scaled_min_lr * config.TRAIN.ACCUMULATION_STEPS\n    config.defrost()\n    config.TRAIN.BASE_LR = linear_scaled_lr\n    config.TRAIN.WARMUP_LR = linear_scaled_warmup_lr\n    config.TRAIN.MIN_LR = linear_scaled_min_lr\n    config.freeze()\n\n    os.makedirs(config.OUTPUT, exist_ok=True)\n    logger = create_logger(output_dir=config.OUTPUT, dist_rank=dist.get_rank(), name=f\"{config.MODEL.NAME}\")\n\n    if dist.get_rank() == 0:\n        path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n        logger.info(f\"Full config saved to {path}\")\n\n    # print config\n    logger.info(config.dump())\n\n    main(config)\n"
        },
        {
          "name": "main_simmim_pt.py",
          "type": "blob",
          "size": 9.466796875,
          "content": "# --------------------------------------------------------\n# SimMIM\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# Modified by Zhenda Xie\n# --------------------------------------------------------\n\nimport os\nimport time\nimport argparse\nimport datetime\nimport numpy as np\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.cuda.amp as amp\nfrom timm.utils import AverageMeter\n\nfrom config import get_config\nfrom models import build_model\nfrom data import build_loader\nfrom lr_scheduler import build_scheduler\nfrom optimizer import build_optimizer\nfrom logger import create_logger\nfrom utils_simmim import load_checkpoint, save_checkpoint, get_grad_norm, auto_resume_helper\n\n# pytorch major version (1.x or 2.x)\nPYTORCH_MAJOR_VERSION = int(torch.__version__.split('.')[0])\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('SimMIM pre-training script', add_help=False)\n    parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file', )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n        default=None,\n        nargs='+',\n    )\n\n    # easy config modification\n    parser.add_argument('--batch-size', type=int, help=\"batch size for single GPU\")\n    parser.add_argument('--data-path', type=str, help='path to dataset')\n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true',\n                        help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--enable-amp', action='store_true')\n    parser.add_argument('--disable-amp', action='store_false', dest='enable_amp')\n    parser.set_defaults(enable_amp=True)\n    parser.add_argument('--output', default='output', type=str, metavar='PATH',\n                        help='root of output folder, the full path is <output>/<model_name>/<tag> (default: output)')\n    parser.add_argument('--tag', help='tag of experiment')\n\n    # distributed training\n    # for pytorch >= 2.0, use `os.environ['LOCAL_RANK']` instead\n    # (see https://pytorch.org/docs/stable/distributed.html#launch-utility)\n    if PYTORCH_MAJOR_VERSION == 1:\n        parser.add_argument(\"--local_rank\", type=int, required=True, help='local rank for DistributedDataParallel')\n\n    args = parser.parse_args()\n\n    config = get_config(args)\n\n    return args, config\n\n\ndef main(config):\n    data_loader_train = build_loader(config, simmim=True, is_pretrain=True)\n\n    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n    model = build_model(config, is_pretrain=True)\n    model.cuda()\n    logger.info(str(model))\n\n    optimizer = build_optimizer(config, model, simmim=True, is_pretrain=True)\n    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False)\n    model_without_ddp = model.module\n\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info(f\"number of params: {n_parameters}\")\n    if hasattr(model_without_ddp, 'flops'):\n        flops = model_without_ddp.flops()\n        logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n\n    lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train))\n    scaler = amp.GradScaler()\n\n    if config.TRAIN.AUTO_RESUME:\n        resume_file = auto_resume_helper(config.OUTPUT, logger)\n        if resume_file:\n            if config.MODEL.RESUME:\n                logger.warning(f\"auto-resume changing resume file from {config.MODEL.RESUME} to {resume_file}\")\n            config.defrost()\n            config.MODEL.RESUME = resume_file\n            config.freeze()\n            logger.info(f'auto resuming from {resume_file}')\n        else:\n            logger.info(f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')\n\n    if config.MODEL.RESUME:\n        load_checkpoint(config, model_without_ddp, optimizer, lr_scheduler, scaler, logger)\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n        data_loader_train.sampler.set_epoch(epoch)\n\n        train_one_epoch(config, model, data_loader_train, optimizer, epoch, lr_scheduler, scaler)\n        if dist.get_rank() == 0 and (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)):\n            save_checkpoint(config, epoch, model_without_ddp, 0., optimizer, lr_scheduler, scaler, logger)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))\n\n\ndef train_one_epoch(config, model, data_loader, optimizer, epoch, lr_scheduler, scaler):\n    model.train()\n    optimizer.zero_grad()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    norm_meter = AverageMeter()\n    loss_scale_meter = AverageMeter()\n\n    start = time.time()\n    end = time.time()\n    for idx, (img, mask, _) in enumerate(data_loader):\n        img = img.cuda(non_blocking=True)\n        mask = mask.cuda(non_blocking=True)\n\n        with amp.autocast(enabled=config.ENABLE_AMP):\n            loss = model(img, mask)\n\n        if config.TRAIN.ACCUMULATION_STEPS > 1:\n            loss = loss / config.TRAIN.ACCUMULATION_STEPS\n            scaler.scale(loss).backward()\n            if config.TRAIN.CLIP_GRAD:\n                scaler.unscale_(optimizer)\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n            else:\n                grad_norm = get_grad_norm(model.parameters())\n            if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n                scaler.step(optimizer)\n                optimizer.zero_grad()\n                scaler.update()\n                lr_scheduler.step_update(epoch * num_steps + idx)\n        else:\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            if config.TRAIN.CLIP_GRAD:\n                scaler.unscale_(optimizer)\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n            else:\n                grad_norm = get_grad_norm(model.parameters())\n            scaler.step(optimizer)\n            scaler.update()\n            lr_scheduler.step_update(epoch * num_steps + idx)\n\n        torch.cuda.synchronize()\n\n        loss_meter.update(loss.item(), img.size(0))\n        norm_meter.update(grad_norm)\n        loss_scale_meter.update(scaler.get_scale())\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            lr = optimizer.param_groups[0]['lr']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            logger.info(\n                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'grad_norm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n                f'loss_scale {loss_scale_meter.val:.4f} ({loss_scale_meter.avg:.4f})\\t'\n                f'mem {memory_used:.0f}MB')\n    epoch_time = time.time() - start\n    logger.info(f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\nif __name__ == '__main__':\n    _, config = parse_option()\n\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ['WORLD_SIZE'])\n        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n    else:\n        rank = -1\n        world_size = -1\n    torch.cuda.set_device(config.LOCAL_RANK)\n    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)\n    torch.distributed.barrier()\n\n    seed = config.SEED + dist.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    cudnn.benchmark = True\n\n    # linear scale the learning rate according to total batch size, may not be optimal\n    linear_scaled_lr = config.TRAIN.BASE_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_warmup_lr = config.TRAIN.WARMUP_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_min_lr = config.TRAIN.MIN_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    # gradient accumulation also need to scale the learning rate\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        linear_scaled_lr = linear_scaled_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_warmup_lr = linear_scaled_warmup_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_min_lr = linear_scaled_min_lr * config.TRAIN.ACCUMULATION_STEPS\n    config.defrost()\n    config.TRAIN.BASE_LR = linear_scaled_lr\n    config.TRAIN.WARMUP_LR = linear_scaled_warmup_lr\n    config.TRAIN.MIN_LR = linear_scaled_min_lr\n    config.freeze()\n\n    os.makedirs(config.OUTPUT, exist_ok=True)\n    logger = create_logger(output_dir=config.OUTPUT, dist_rank=dist.get_rank(), name=f\"{config.MODEL.NAME}\")\n\n    if dist.get_rank() == 0:\n        path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n        logger.info(f\"Full config saved to {path}\")\n\n    # print config\n    logger.info(config.dump())\n\n    main(config)\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "optimizer.py",
          "type": "blob",
          "size": 6.0380859375,
          "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nfrom functools import partial\nfrom torch import optim as optim\n\ntry:\n    from apex.optimizers import FusedAdam, FusedLAMB\nexcept:\n    FusedAdam = None\n    FusedLAMB = None\n    print(\"To use FusedLAMB or FusedAdam, please install apex.\")\n\n\ndef build_optimizer(config, model, simmim=False, is_pretrain=False):\n    \"\"\"\n    Build optimizer, set weight decay of normalization to 0 by default.\n    \"\"\"\n    skip = {}\n    skip_keywords = {}\n    if hasattr(model, 'no_weight_decay'):\n        skip = model.no_weight_decay()\n    if hasattr(model, 'no_weight_decay_keywords'):\n        skip_keywords = model.no_weight_decay_keywords()\n    if simmim:\n        if is_pretrain:\n            parameters = get_pretrain_param_groups(model, skip, skip_keywords)\n        else:\n            depths = config.MODEL.SWIN.DEPTHS if config.MODEL.TYPE == 'swin' else config.MODEL.SWINV2.DEPTHS\n            num_layers = sum(depths)\n            get_layer_func = partial(get_swin_layer, num_layers=num_layers + 2, depths=depths)\n            scales = list(config.TRAIN.LAYER_DECAY ** i for i in reversed(range(num_layers + 2)))\n            parameters = get_finetune_param_groups(model, config.TRAIN.BASE_LR, config.TRAIN.WEIGHT_DECAY, get_layer_func, scales, skip, skip_keywords)\n    else:\n        parameters = set_weight_decay(model, skip, skip_keywords)\n\n    opt_lower = config.TRAIN.OPTIMIZER.NAME.lower()\n    optimizer = None\n    if opt_lower == 'sgd':\n        optimizer = optim.SGD(parameters, momentum=config.TRAIN.OPTIMIZER.MOMENTUM, nesterov=True,\n                              lr=config.TRAIN.BASE_LR, weight_decay=config.TRAIN.WEIGHT_DECAY)\n    elif opt_lower == 'adamw':\n        optimizer = optim.AdamW(parameters, eps=config.TRAIN.OPTIMIZER.EPS, betas=config.TRAIN.OPTIMIZER.BETAS,\n                                lr=config.TRAIN.BASE_LR, weight_decay=config.TRAIN.WEIGHT_DECAY)\n    elif opt_lower == 'fused_adam':\n        optimizer = FusedAdam(parameters, eps=config.TRAIN.OPTIMIZER.EPS, betas=config.TRAIN.OPTIMIZER.BETAS,\n                              lr=config.TRAIN.BASE_LR, weight_decay=config.TRAIN.WEIGHT_DECAY)\n    elif opt_lower == 'fused_lamb':\n        optimizer = FusedLAMB(parameters, eps=config.TRAIN.OPTIMIZER.EPS, betas=config.TRAIN.OPTIMIZER.BETAS,\n                              lr=config.TRAIN.BASE_LR, weight_decay=config.TRAIN.WEIGHT_DECAY)\n\n    return optimizer\n\n\ndef set_weight_decay(model, skip_list=(), skip_keywords=()):\n    has_decay = []\n    no_decay = []\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n                check_keywords_in_name(name, skip_keywords):\n            no_decay.append(param)\n            # print(f\"{name} has no weight decay\")\n        else:\n            has_decay.append(param)\n    return [{'params': has_decay},\n            {'params': no_decay, 'weight_decay': 0.}]\n\n\ndef check_keywords_in_name(name, keywords=()):\n    isin = False\n    for keyword in keywords:\n        if keyword in name:\n            isin = True\n    return isin\n\n\ndef get_pretrain_param_groups(model, skip_list=(), skip_keywords=()):\n    has_decay = []\n    no_decay = []\n    has_decay_name = []\n    no_decay_name = []\n    \n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n                check_keywords_in_name(name, skip_keywords):\n            no_decay.append(param)\n            no_decay_name.append(name)\n        else:\n            has_decay.append(param)\n            has_decay_name.append(name)\n    return [{'params': has_decay},\n            {'params': no_decay, 'weight_decay': 0.}]\n\n\ndef get_swin_layer(name, num_layers, depths):\n    if name in (\"mask_token\"):\n        return 0\n    elif name.startswith(\"patch_embed\"):\n        return 0\n    elif name.startswith(\"layers\"):\n        layer_id = int(name.split('.')[1])\n        block_id = name.split('.')[3]\n        if block_id == 'reduction' or block_id == 'norm':\n            return sum(depths[:layer_id + 1])\n        layer_id = sum(depths[:layer_id]) + int(block_id)\n        return layer_id + 1\n    else:\n        return num_layers - 1\n\n\ndef get_finetune_param_groups(model, lr, weight_decay, get_layer_func, scales, skip_list=(), skip_keywords=()):\n    parameter_group_names = {}\n    parameter_group_vars = {}\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n                check_keywords_in_name(name, skip_keywords):\n            group_name = \"no_decay\"\n            this_weight_decay = 0.\n        else:\n            group_name = \"decay\"\n            this_weight_decay = weight_decay\n        if get_layer_func is not None:\n            layer_id = get_layer_func(name)\n            group_name = \"layer_%d_%s\" % (layer_id, group_name)\n        else:\n            layer_id = None\n\n        if group_name not in parameter_group_names:\n            if scales is not None:\n                scale = scales[layer_id]\n            else:\n                scale = 1.\n\n            parameter_group_names[group_name] = {\n                \"group_name\": group_name,\n                \"weight_decay\": this_weight_decay,\n                \"params\": [],\n                \"lr\": lr * scale,\n                \"lr_scale\": scale,\n            }\n            parameter_group_vars[group_name] = {\n                \"group_name\": group_name,\n                \"weight_decay\": this_weight_decay,\n                \"params\": [],\n                \"lr\": lr * scale,\n                \"lr_scale\": scale\n            }\n\n        parameter_group_vars[group_name][\"params\"].append(param)\n        parameter_group_names[group_name][\"params\"].append(name)\n    return list(parameter_group_vars.values())\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 9.416015625,
          "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport os\nimport torch\nimport torch.distributed as dist\n\ntry:\n    from torch._six import inf\nexcept:\n    from torch import inf\n\n\ndef load_checkpoint(config, model, optimizer, lr_scheduler, loss_scaler, logger):\n    logger.info(f\"==============> Resuming form {config.MODEL.RESUME}....................\")\n    if config.MODEL.RESUME.startswith('https'):\n        checkpoint = torch.hub.load_state_dict_from_url(\n            config.MODEL.RESUME, map_location='cpu', check_hash=True)\n    else:\n        checkpoint = torch.load(config.MODEL.RESUME, map_location='cpu')\n    msg = model.load_state_dict(checkpoint['model'], strict=False)\n    logger.info(msg)\n    max_accuracy = 0.0\n    if not config.EVAL_MODE and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n        config.defrost()\n        config.TRAIN.START_EPOCH = checkpoint['epoch'] + 1\n        config.freeze()\n        if 'scaler' in checkpoint:\n            loss_scaler.load_state_dict(checkpoint['scaler'])\n        logger.info(f\"=> loaded successfully '{config.MODEL.RESUME}' (epoch {checkpoint['epoch']})\")\n        if 'max_accuracy' in checkpoint:\n            max_accuracy = checkpoint['max_accuracy']\n\n    del checkpoint\n    torch.cuda.empty_cache()\n    return max_accuracy\n\n\ndef load_pretrained(config, model, logger):\n    logger.info(f\"==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......\")\n    checkpoint = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n    state_dict = checkpoint['model']\n\n    # delete relative_position_index since we always re-init it\n    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_position_index\" in k]\n    for k in relative_position_index_keys:\n        del state_dict[k]\n\n    # delete relative_coords_table since we always re-init it\n    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_coords_table\" in k]\n    for k in relative_position_index_keys:\n        del state_dict[k]\n\n    # delete attn_mask since we always re-init it\n    attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n    for k in attn_mask_keys:\n        del state_dict[k]\n\n    # bicubic interpolate relative_position_bias_table if not match\n    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for k in relative_position_bias_table_keys:\n        relative_position_bias_table_pretrained = state_dict[k]\n        relative_position_bias_table_current = model.state_dict()[k]\n        L1, nH1 = relative_position_bias_table_pretrained.size()\n        L2, nH2 = relative_position_bias_table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {k}, passing......\")\n        else:\n            if L1 != L2:\n                # bicubic interpolate relative_position_bias_table if not match\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n                    relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(S2, S2),\n                    mode='bicubic')\n                state_dict[k] = relative_position_bias_table_pretrained_resized.view(nH2, L2).permute(1, 0)\n\n    # bicubic interpolate absolute_pos_embed if not match\n    absolute_pos_embed_keys = [k for k in state_dict.keys() if \"absolute_pos_embed\" in k]\n    for k in absolute_pos_embed_keys:\n        # dpe\n        absolute_pos_embed_pretrained = state_dict[k]\n        absolute_pos_embed_current = model.state_dict()[k]\n        _, L1, C1 = absolute_pos_embed_pretrained.size()\n        _, L2, C2 = absolute_pos_embed_current.size()\n        if C1 != C1:\n            logger.warning(f\"Error in loading {k}, passing......\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(-1, S1, S1, C1)\n                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.permute(0, 3, 1, 2)\n                absolute_pos_embed_pretrained_resized = torch.nn.functional.interpolate(\n                    absolute_pos_embed_pretrained, size=(S2, S2), mode='bicubic')\n                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.permute(0, 2, 3, 1)\n                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.flatten(1, 2)\n                state_dict[k] = absolute_pos_embed_pretrained_resized\n\n    # check classifier, if not match, then re-init classifier to zero\n    head_bias_pretrained = state_dict['head.bias']\n    Nc1 = head_bias_pretrained.shape[0]\n    Nc2 = model.head.bias.shape[0]\n    if (Nc1 != Nc2):\n        if Nc1 == 21841 and Nc2 == 1000:\n            logger.info(\"loading ImageNet-22K weight to ImageNet-1K ......\")\n            map22kto1k_path = f'data/map22kto1k.txt'\n            with open(map22kto1k_path) as f:\n                map22kto1k = f.readlines()\n            map22kto1k = [int(id22k.strip()) for id22k in map22kto1k]\n            state_dict['head.weight'] = state_dict['head.weight'][map22kto1k, :]\n            state_dict['head.bias'] = state_dict['head.bias'][map22kto1k]\n        else:\n            torch.nn.init.constant_(model.head.bias, 0.)\n            torch.nn.init.constant_(model.head.weight, 0.)\n            del state_dict['head.weight']\n            del state_dict['head.bias']\n            logger.warning(f\"Error in loading classifier head, re-init classifier head to 0\")\n\n    msg = model.load_state_dict(state_dict, strict=False)\n    logger.warning(msg)\n\n    logger.info(f\"=> loaded successfully '{config.MODEL.PRETRAINED}'\")\n\n    del checkpoint\n    torch.cuda.empty_cache()\n\n\ndef save_checkpoint(config, epoch, model, max_accuracy, optimizer, lr_scheduler, loss_scaler, logger):\n    save_state = {'model': model.state_dict(),\n                  'optimizer': optimizer.state_dict(),\n                  'lr_scheduler': lr_scheduler.state_dict(),\n                  'max_accuracy': max_accuracy,\n                  'scaler': loss_scaler.state_dict(),\n                  'epoch': epoch,\n                  'config': config}\n\n    save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth')\n    logger.info(f\"{save_path} saving......\")\n    torch.save(save_state, save_path)\n    logger.info(f\"{save_path} saved !!!\")\n\n\ndef get_grad_norm(parameters, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    total_norm = 0\n    for p in parameters:\n        param_norm = p.grad.data.norm(norm_type)\n        total_norm += param_norm.item() ** norm_type\n    total_norm = total_norm ** (1. / norm_type)\n    return total_norm\n\n\ndef auto_resume_helper(output_dir):\n    checkpoints = os.listdir(output_dir)\n    checkpoints = [ckpt for ckpt in checkpoints if ckpt.endswith('pth')]\n    print(f\"All checkpoints founded in {output_dir}: {checkpoints}\")\n    if len(checkpoints) > 0:\n        latest_checkpoint = max([os.path.join(output_dir, d) for d in checkpoints], key=os.path.getmtime)\n        print(f\"The latest checkpoint founded: {latest_checkpoint}\")\n        resume_file = latest_checkpoint\n    else:\n        resume_file = None\n    return resume_file\n\n\ndef reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= dist.get_world_size()\n    return rt\n\n\ndef ampscaler_get_grad_norm(parameters, norm_type: float = 2.0) -> torch.Tensor:\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = [p for p in parameters if p.grad is not None]\n    norm_type = float(norm_type)\n    if len(parameters) == 0:\n        return torch.tensor(0.)\n    device = parameters[0].grad.device\n    if norm_type == inf:\n        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n    else:\n        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(),\n                                                        norm_type).to(device) for p in parameters]), norm_type)\n    return total_norm\n\n\nclass NativeScalerWithGradNormCount:\n    state_dict_key = \"amp_scaler\"\n\n    def __init__(self):\n        self._scaler = torch.cuda.amp.GradScaler()\n\n    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        if update_grad:\n            if clip_grad is not None:\n                assert parameters is not None\n                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n            else:\n                self._scaler.unscale_(optimizer)\n                norm = ampscaler_get_grad_norm(parameters)\n            self._scaler.step(optimizer)\n            self._scaler.update()\n        else:\n            norm = None\n        return norm\n\n    def state_dict(self):\n        return self._scaler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scaler.load_state_dict(state_dict)\n"
        },
        {
          "name": "utils_moe.py",
          "type": "blob",
          "size": 11.1298828125,
          "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport os\nimport torch\nimport torch.distributed as dist\n\n\ndef split_moe_model_state_dict(moe_keys, model_state_dict):\n    moe_model_state_dict = {}\n    non_moe_model_state_dict = {}\n    for (k, v) in model_state_dict.items():\n        if k in moe_keys:\n            moe_model_state_dict[k] = v\n        else:\n            non_moe_model_state_dict[k] = v\n    return moe_model_state_dict, non_moe_model_state_dict\n\n\ndef merge_moe_model_state_dict(moe_model_state_dict, non_moe_model_state_dict):\n    model_state_dict = {}\n    model_state_dict.update(moe_model_state_dict)\n    model_state_dict.update(non_moe_model_state_dict)\n    return model_state_dict\n\n\ndef load_checkpoint(config, model, optimizer, lr_scheduler, loss_scaler, logger):\n    global_rank = dist.get_rank()\n    logger.info(f\"==============> Rank[{global_rank}] Resuming form {config.MODEL.RESUME}....................\")\n    if config.MODEL.RESUME.endswith(f'.pth'):\n        if config.TRAIN.MOE.SAVE_MASTER:\n            resume_path = config.MODEL.RESUME + f'.global'\n        else:\n            resume_path = config.MODEL.RESUME + f'.rank{global_rank}'\n        logger.info(f\"===> Rank[{global_rank}] Re-formatting checkpoint name to {resume_path}......\")\n    else:\n        resume_path = config.MODEL.RESUME\n\n    checkpoint = torch.load(resume_path, map_location='cpu')\n    msg = model.load_state_dict(checkpoint['model'], strict=False)\n    logger.info(msg)\n    max_accuracy = 0.0\n    if not config.EVAL_MODE and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n        config.defrost()\n        config.TRAIN.START_EPOCH = checkpoint['epoch'] + 1\n        config.freeze()\n        if 'scaler' in checkpoint:\n            loss_scaler.load_state_dict(checkpoint['scaler'])\n        logger.info(f\"=>Rank[{global_rank}] loaded successfully '{config.MODEL.RESUME}' (epoch {checkpoint['epoch']})\")\n        if 'max_accuracy' in checkpoint:\n            max_accuracy = checkpoint['max_accuracy']\n\n    del checkpoint\n    torch.cuda.empty_cache()\n    return max_accuracy\n\n\ndef load_pretrained(config, model, logger):\n    global_rank = dist.get_rank()\n    logger.info(f\"==============> Rank[{global_rank}] Loading weight {config.MODEL.PRETRAINED} for fine-tuning......\")\n    if config.MODEL.PRETRAINED.endswith(f'.pth'):\n        if config.TRAIN.MOE.SAVE_MASTER:\n            pretrained_path = config.MODEL.PRETRAINED + f'.global'\n        else:\n            pretrained_path = config.MODEL.PRETRAINED + f'.rank{global_rank}'\n        logger.info(f\"===> Rank[{global_rank}] Re-formatting checkpoint name to {pretrained_path}......\")\n    else:\n        pretrained_path = config.MODEL.PRETRAINED\n\n    if pretrained_path.endswith(f'.rank{global_rank}'):\n        checkpoint = torch.load(pretrained_path, map_location='cpu')\n        if os.path.exists(pretrained_path.replace(f'.rank{global_rank}', f'.master')):\n            checkpoint_master = torch.load(pretrained_path.replace(f'.rank{global_rank}', f'.master'),\n                                           map_location='cpu')\n            state_dict = merge_moe_model_state_dict(checkpoint['model'], checkpoint_master['model'])\n        else:\n            state_dict = checkpoint['model']\n    elif pretrained_path.endswith(f'.pth.global'):\n        checkpoint = torch.load(pretrained_path, map_location='cpu')\n        state_dict = checkpoint['model']\n    else:\n        raise NotImplementedError(f\"{config.MODEL.PRETRAINED} file error...\")\n\n    # delete relative_position_index since we always re-init it\n    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_position_index\" in k]\n    for k in relative_position_index_keys:\n        del state_dict[k]\n\n    # delete relative_coords_table since we always re-init it\n    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_coords_table\" in k]\n    for k in relative_position_index_keys:\n        del state_dict[k]\n\n    # delete attn_mask since we always re-init it\n    attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n    for k in attn_mask_keys:\n        del state_dict[k]\n\n    # bicubic interpolate relative_position_bias_table if not match\n    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for k in relative_position_bias_table_keys:\n        relative_position_bias_table_pretrained = state_dict[k]\n        relative_position_bias_table_current = model.state_dict()[k]\n        L1, nH1 = relative_position_bias_table_pretrained.size()\n        L2, nH2 = relative_position_bias_table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {k}, passing......\")\n        else:\n            if L1 != L2:\n                # bicubic interpolate relative_position_bias_table if not match\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n                    relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(S2, S2),\n                    mode='bicubic')\n                state_dict[k] = relative_position_bias_table_pretrained_resized.view(nH2, L2).permute(1, 0)\n\n    # bicubic interpolate absolute_pos_embed if not match\n    absolute_pos_embed_keys = [k for k in state_dict.keys() if \"absolute_pos_embed\" in k]\n    for k in absolute_pos_embed_keys:\n        # dpe\n        absolute_pos_embed_pretrained = state_dict[k]\n        absolute_pos_embed_current = model.state_dict()[k]\n        _, L1, C1 = absolute_pos_embed_pretrained.size()\n        _, L2, C2 = absolute_pos_embed_current.size()\n        if C1 != C1:\n            logger.warning(f\"Error in loading {k}, passing......\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(-1, S1, S1, C1)\n                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.permute(0, 3, 1, 2)\n                absolute_pos_embed_pretrained_resized = torch.nn.functional.interpolate(\n                    absolute_pos_embed_pretrained, size=(S2, S2), mode='bicubic')\n                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.permute(0, 2, 3, 1)\n                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.flatten(1, 2)\n                state_dict[k] = absolute_pos_embed_pretrained_resized\n\n    # check classifier, if not match, then re-init classifier to zero\n    head_bias_pretrained = state_dict['head.bias']\n    Nc1 = head_bias_pretrained.shape[0]\n    Nc2 = model.head.bias.shape[0]\n    if (Nc1 != Nc2):\n        if Nc1 == 21841 and Nc2 == 1000:\n            logger.info(\"loading ImageNet-22K weight to ImageNet-1K ......\")\n            map22kto1k_path = f'data/map22kto1k.txt'\n            with open(map22kto1k_path) as f:\n                map22kto1k = f.readlines()\n            map22kto1k = [int(id22k.strip()) for id22k in map22kto1k]\n            state_dict['head.weight'] = state_dict['head.weight'][map22kto1k, :]\n            state_dict['head.bias'] = state_dict['head.bias'][map22kto1k]\n        else:\n            torch.nn.init.constant_(model.head.bias, 0.)\n            torch.nn.init.constant_(model.head.weight, 0.)\n            del state_dict['head.weight']\n            del state_dict['head.bias']\n            logger.warning(f\"Error in loading classifier head, re-init classifier head to 0\")\n\n    msg = model.load_state_dict(state_dict, strict=False)\n    logger.warning(msg)\n\n    logger.info(f\"=> loaded successfully '{config.MODEL.PRETRAINED}'\")\n\n    del checkpoint\n    torch.cuda.empty_cache()\n\n\ndef save_checkpoint(config, epoch, model, max_accuracy, optimizer, lr_scheduler, loss_scaler, logger,\n                    zero_redundancy=False):\n    global_rank = dist.get_rank()\n\n    if zero_redundancy:\n        if config.TRAIN.MOE.SAVE_MASTER:\n            save_state = {'model': model.state_dict()}\n            if global_rank == 0:\n                save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth.global')\n                logger.info(f\"{save_path} saving......\")\n                torch.save(save_state, save_path)\n                logger.info(f\"{save_path} saved !!!\")\n        else:\n            moe_model_state_dict, non_moe_model_state_dict = \\\n                split_moe_model_state_dict(model._ddp_params_and_buffers_to_ignore, model.state_dict())\n            save_state = {'model': moe_model_state_dict}\n            save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth.rank{global_rank}')\n            logger.info(f\"{save_path} saving......\")\n            torch.save(save_state, save_path)\n            logger.info(f\"{save_path} saved !!!\")\n            if global_rank == 0:\n                save_state_master = {'model': non_moe_model_state_dict}\n                save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth.master')\n                logger.info(f\"{save_path} saving......\")\n                torch.save(save_state_master, save_path)\n                logger.info(f\"{save_path} saved !!!\")\n    else:\n        save_state = {'model': model.state_dict(),\n                      'optimizer': optimizer.state_dict(),\n                      'lr_scheduler': lr_scheduler.state_dict(),\n                      'max_accuracy': max_accuracy,\n                      'scaler': loss_scaler.state_dict(),\n                      'epoch': epoch,\n                      'config': config}\n        if config.TRAIN.MOE.SAVE_MASTER:\n            if global_rank == 0:\n                save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth.global')\n                logger.info(f\"{save_path} saving......\")\n                torch.save(save_state, save_path)\n                logger.info(f\"{save_path} saved !!!\")\n        else:\n            save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth.rank{global_rank}')\n            logger.info(f\"{save_path} saving......\")\n            torch.save(save_state, save_path)\n            logger.info(f\"{save_path} saved !!!\")\n\n\ndef auto_resume_helper(output_dir, save_master=False):\n    global_rank = dist.get_rank()\n    checkpoints = os.listdir(output_dir)\n    if not save_master:\n        master_checkpoints = [ckpt for ckpt in checkpoints if ckpt.endswith(f'pth.rank0')]\n    else:\n        master_checkpoints = [ckpt for ckpt in checkpoints if ckpt.endswith(f'pth.global')]\n    print(f\"All master checkpoints founded in {output_dir}: {master_checkpoints}\")\n    if len(master_checkpoints) > 0:\n        latest_master_checkpoint = max([os.path.join(output_dir, d) for d in master_checkpoints], key=os.path.getmtime)\n        latest_checkpoint = latest_master_checkpoint.replace('pth.rank0', f'pth.rank{global_rank}')\n        print(f\"The latest checkpoint founded: {latest_checkpoint}\")\n        resume_file = latest_checkpoint\n    else:\n        resume_file = None\n    return resume_file\n\n\ndef hook_scale_grad(scale, tensor):\n    return tensor / scale\n"
        },
        {
          "name": "utils_simmim.py",
          "type": "blob",
          "size": 8.2177734375,
          "content": "# --------------------------------------------------------\n# SimMIM\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# Modified by Zhenda Xie\n# --------------------------------------------------------\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport numpy as np\nfrom scipy import interpolate\n\n\ndef load_checkpoint(config, model, optimizer, lr_scheduler, scaler, logger):\n    logger.info(f\">>>>>>>>>> Resuming from {config.MODEL.RESUME} ..........\")\n    if config.MODEL.RESUME.startswith('https'):\n        checkpoint = torch.hub.load_state_dict_from_url(\n            config.MODEL.RESUME, map_location='cpu', check_hash=True)\n    else:\n        checkpoint = torch.load(config.MODEL.RESUME, map_location='cpu')\n\n    # re-map keys due to name change (only for loading provided models)\n    rpe_mlp_keys = [k for k in checkpoint['model'].keys() if \"rpe_mlp\" in k]\n    for k in rpe_mlp_keys:\n        checkpoint['model'][k.replace('rpe_mlp', 'cpb_mlp')] = checkpoint['model'].pop(k)\n    \n    msg = model.load_state_dict(checkpoint['model'], strict=False)\n    logger.info(msg)\n\n    max_accuracy = 0.0\n    if not config.EVAL_MODE and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'scaler' in checkpoint and 'epoch' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n        scaler.load_state_dict(checkpoint['scaler'])\n\n        config.defrost()\n        config.TRAIN.START_EPOCH = checkpoint['epoch'] + 1\n        config.freeze()\n\n        logger.info(f\"=> loaded successfully '{config.MODEL.RESUME}' (epoch {checkpoint['epoch']})\")\n        if 'max_accuracy' in checkpoint:\n            max_accuracy = checkpoint['max_accuracy']\n        else:\n            max_accuracy = 0.0\n\n    del checkpoint\n    torch.cuda.empty_cache()\n    return max_accuracy\n\n\ndef save_checkpoint(config, epoch, model, max_accuracy, optimizer, lr_scheduler, scaler, logger):\n    save_state = {'model': model.state_dict(),\n                  'optimizer': optimizer.state_dict(),\n                  'lr_scheduler': lr_scheduler.state_dict(),\n                  'scaler': scaler.state_dict(),\n                  'max_accuracy': max_accuracy,\n                  'epoch': epoch,\n                  'config': config}\n\n    save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth')\n    logger.info(f\"{save_path} saving......\")\n    torch.save(save_state, save_path)\n    logger.info(f\"{save_path} saved !!!\")\n\n\ndef get_grad_norm(parameters, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    total_norm = 0\n    for p in parameters:\n        param_norm = p.grad.data.norm(norm_type)\n        total_norm += param_norm.item() ** norm_type\n    total_norm = total_norm ** (1. / norm_type)\n    return total_norm\n\n\ndef auto_resume_helper(output_dir, logger):\n    checkpoints = os.listdir(output_dir)\n    checkpoints = [ckpt for ckpt in checkpoints if ckpt.endswith('pth')]\n    logger.info(f\"All checkpoints founded in {output_dir}: {checkpoints}\")\n    if len(checkpoints) > 0:\n        latest_checkpoint = max([os.path.join(output_dir, d) for d in checkpoints], key=os.path.getmtime)\n        logger.info(f\"The latest checkpoint founded: {latest_checkpoint}\")\n        resume_file = latest_checkpoint\n    else:\n        resume_file = None\n    return resume_file\n\n\ndef reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= dist.get_world_size()\n    return rt\n\n\ndef load_pretrained(config, model, logger):\n    logger.info(f\">>>>>>>>>> Fine-tuned from {config.MODEL.PRETRAINED} ..........\")\n    checkpoint = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n    checkpoint_model = checkpoint['model']\n    \n    if any([True if 'encoder.' in k else False for k in checkpoint_model.keys()]):\n        checkpoint_model = {k.replace('encoder.', ''): v for k, v in checkpoint_model.items() if k.startswith('encoder.')}\n        logger.info('Detect pre-trained model, remove [encoder.] prefix.')\n    else:\n        logger.info('Detect non-pre-trained model, pass without doing anything.')\n\n    if config.MODEL.TYPE in ['swin', 'swinv2']:\n        logger.info(f\">>>>>>>>>> Remapping pre-trained keys for SWIN ..........\")\n        checkpoint = remap_pretrained_keys_swin(model, checkpoint_model, logger)\n    else:\n        raise NotImplementedError\n\n    msg = model.load_state_dict(checkpoint_model, strict=False)\n    logger.info(msg)\n    \n    del checkpoint\n    torch.cuda.empty_cache()\n    logger.info(f\">>>>>>>>>> loaded successfully '{config.MODEL.PRETRAINED}'\")\n    \n\ndef remap_pretrained_keys_swin(model, checkpoint_model, logger):\n    state_dict = model.state_dict()\n    \n    # Geometric interpolation when pre-trained patch size mismatch with fine-tuned patch size\n    all_keys = list(checkpoint_model.keys())\n    for key in all_keys:\n        if \"relative_position_bias_table\" in key:\n            relative_position_bias_table_pretrained = checkpoint_model[key]\n            relative_position_bias_table_current = state_dict[key]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            if nH1 != nH2:\n                logger.info(f\"Error in loading {key}, passing......\")\n            else:\n                if L1 != L2:\n                    logger.info(f\"{key}: Interpolate relative_position_bias_table using geo.\")\n                    src_size = int(L1 ** 0.5)\n                    dst_size = int(L2 ** 0.5)\n\n                    def geometric_progression(a, r, n):\n                        return a * (1.0 - r ** n) / (1.0 - r)\n\n                    left, right = 1.01, 1.5\n                    while right - left > 1e-6:\n                        q = (left + right) / 2.0\n                        gp = geometric_progression(1, q, src_size // 2)\n                        if gp > dst_size // 2:\n                            right = q\n                        else:\n                            left = q\n\n                    # if q > 1.090307:\n                    #     q = 1.090307\n\n                    dis = []\n                    cur = 1\n                    for i in range(src_size // 2):\n                        dis.append(cur)\n                        cur += q ** (i + 1)\n\n                    r_ids = [-_ for _ in reversed(dis)]\n\n                    x = r_ids + [0] + dis\n                    y = r_ids + [0] + dis\n\n                    t = dst_size // 2.0\n                    dx = np.arange(-t, t + 0.1, 1.0)\n                    dy = np.arange(-t, t + 0.1, 1.0)\n\n                    logger.info(\"Original positions = %s\" % str(x))\n                    logger.info(\"Target positions = %s\" % str(dx))\n\n                    all_rel_pos_bias = []\n\n                    for i in range(nH1):\n                        z = relative_position_bias_table_pretrained[:, i].view(src_size, src_size).float().numpy()\n                        f_cubic = interpolate.interp2d(x, y, z, kind='cubic')\n                        all_rel_pos_bias.append(torch.Tensor(f_cubic(dx, dy)).contiguous().view(-1, 1).to(\n                            relative_position_bias_table_pretrained.device))\n\n                    new_rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n                    checkpoint_model[key] = new_rel_pos_bias\n\n    # delete relative_position_index since we always re-init it\n    relative_position_index_keys = [k for k in checkpoint_model.keys() if \"relative_position_index\" in k]\n    for k in relative_position_index_keys:\n        del checkpoint_model[k]\n\n    # delete relative_coords_table since we always re-init it\n    relative_coords_table_keys = [k for k in checkpoint_model.keys() if \"relative_coords_table\" in k]\n    for k in relative_coords_table_keys:\n        del checkpoint_model[k]\n\n    # re-map keys due to name change\n    rpe_mlp_keys = [k for k in checkpoint_model.keys() if \"rpe_mlp\" in k]\n    for k in rpe_mlp_keys:\n        checkpoint_model[k.replace('rpe_mlp', 'cpb_mlp')] = checkpoint_model.pop(k)\n\n    # delete attn_mask since we always re-init it\n    attn_mask_keys = [k for k in checkpoint_model.keys() if \"attn_mask\" in k]\n    for k in attn_mask_keys:\n        del checkpoint_model[k]\n\n    return checkpoint_model\n"
        }
      ]
    }
  ]
}