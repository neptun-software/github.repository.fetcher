{
  "metadata": {
    "timestamp": 1736561142325,
    "page": 89,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ymcui/Chinese-LLaMA-Alpaca",
      "stars": 18627,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0302734375,
          "content": "notebooks/** linguist-vendored\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.021484375,
          "content": ".DS_Store\n*/.DS_Store\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.8095703125,
          "content": "cff-version: 1.2.0\nmessage: \"If you find our resources useful, please cite our paper as below.\"\nauthors:\n- family-names: \"Cui\"\n  given-names: \"Yiming\"\n  orcid: \"https://orcid.org/0000-0002-2452-375X\"\n- family-names: \"Yang\"\n  given-names: \"Ziqing\"\n- family-names: \"Yao\"\n  given-names: \"Xin\"  \ntitle: \"Chinese LLaMA and Alpaca\"\nversion: 4.1\ndate-released: 2023-03-28\nurl: \"https://github.com/ymcui/Chinese-LLaMA-Alpaca\"\npreferred-citation: \n  type: article\n  authors:\n  - family-names: \"Cui\"\n    given-names: \"Yiming\"\n    orcid: \"https://orcid.org/0000-0002-2452-375X\"\n  - family-names: \"Yang\"\n    given-names: \"Ziqing\"\n  - family-names: \"Yao\"\n    given-names: \"Xin\"  \n  title: \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\"\n  journal: \"arXiv pre-print\"\n  year: 2023\n  url: \"https://arxiv.org/abs/2304.08177\"\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 11.095703125,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Yiming Cui, Ziqing Yang, Xin Yao\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.9521484375,
          "content": "# [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)é¡¹ç›®å¯åŠ¨ï¼\n\n[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README.md) | [**ğŸŒEnglish**](./README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki) | [**â“æé—®/Issues**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/issues) | [**ğŸ’¬è®¨è®º/Discussions**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions) | [**âš”ï¸ç«æŠ€åœº/Arena**](http://llm-arena.ymcui.com/)\n\n<p align=\"center\">\n    <br>\n    <img src=\"./pics/banner.png\" width=\"700\"/>\n    <br>\n</p>\n<p align=\"center\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca.svg?color=blue&style=flat-square\">\n    <img alt=\"GitHub release (latest by date)\" src=\"https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca\">\n    <img alt=\"GitHub top language\" src=\"https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca\">\n    <img alt=\"GitHub last commit\" src=\"https://img.shields.io/github/last-commit/ymcui/Chinese-LLaMA-Alpaca\">\n    <a href=\"https://app.codacy.com/gh/ymcui/Chinese-LLaMA-Alpaca/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade\"><img src=\"https://app.codacy.com/project/badge/Grade/1710faac5e634acaabfc26b0a778cdde\"/></a>\n</p>\n\n\n\næœ¬é¡¹ç›®å¼€æºäº†**ä¸­æ–‡LLaMAæ¨¡å‹å’ŒæŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹**ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›å¤§æ¨¡å‹åœ¨ä¸­æ–‡NLPç¤¾åŒºçš„å¼€æ”¾ç ”ç©¶ã€‚è¿™äº›æ¨¡å‹**åœ¨åŸç‰ˆLLaMAçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨**å¹¶ä½¿ç”¨äº†ä¸­æ–‡æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼Œä¸­æ–‡Alpacaæ¨¡å‹è¿›ä¸€æ­¥ä½¿ç”¨äº†ä¸­æ–‡æŒ‡ä»¤æ•°æ®è¿›è¡Œç²¾è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚\n\n**æŠ€æœ¯æŠ¥å‘Šï¼ˆV2ï¼‰**ï¼š[[Cui, Yang, and Yao] Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca](https://arxiv.org/abs/2304.08177)\n\n**æœ¬é¡¹ç›®ä¸»è¦å†…å®¹ï¼š**\n\n- ğŸš€ é’ˆå¯¹åŸç‰ˆLLaMAæ¨¡å‹æ‰©å……äº†ä¸­æ–‡è¯è¡¨ï¼Œæå‡äº†ä¸­æ–‡ç¼–è§£ç æ•ˆç‡ \n- ğŸš€ å¼€æºäº†ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬æ•°æ®é¢„è®­ç»ƒçš„ä¸­æ–‡LLaMAä»¥åŠç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„ä¸­æ–‡Alpaca\n- ğŸš€ å¼€æºäº†é¢„è®­ç»ƒè„šæœ¬ã€æŒ‡ä»¤ç²¾è°ƒè„šæœ¬ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€è¦è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹\n- ğŸš€ å¿«é€Ÿä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘ï¼ˆä¸ªäººPCï¼‰çš„CPU/GPUæœ¬åœ°é‡åŒ–å’Œéƒ¨ç½²ä½“éªŒå¤§æ¨¡å‹\n- ğŸš€ æ”¯æŒ[ğŸ¤—transformers](https://github.com/huggingface/transformers), [llama.cpp](https://github.com/ggerganov/llama.cpp), [text-generation-webui](https://github.com/oobabooga/text-generation-webui), [LlamaChat](https://github.com/alexrozanski/LlamaChat), [LangChain](https://github.com/hwchase17/langchain), [privateGPT](https://github.com/imartinez/privateGPT)ç­‰ç”Ÿæ€\n- ç›®å‰å·²å¼€æºçš„æ¨¡å‹ç‰ˆæœ¬ï¼š7Bï¼ˆåŸºç¡€ç‰ˆã€**Plusç‰ˆ**ã€**Proç‰ˆ**ï¼‰ã€13Bï¼ˆåŸºç¡€ç‰ˆã€**Plusç‰ˆ**ã€**Proç‰ˆ**ï¼‰ã€33Bï¼ˆåŸºç¡€ç‰ˆã€**Plusç‰ˆ**ã€**Proç‰ˆ**ï¼‰\n\nğŸ’¡ ä¸‹å›¾æ˜¯ä¸­æ–‡Alpaca-Plus-7Bæ¨¡å‹åœ¨æœ¬åœ°CPUé‡åŒ–éƒ¨ç½²åçš„å®é™…ä½“éªŒé€Ÿåº¦å’Œæ•ˆæœã€‚\n\n![](./pics/screencast.gif)\n\n----\n\n[**ä¸­æ–‡LLaMA-2&Alpaca-2å¤§æ¨¡å‹**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) | [å¤šæ¨¡æ€ä¸­æ–‡LLaMA&Alpacaå¤§æ¨¡å‹](https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca) | [å¤šæ¨¡æ€VLE](https://github.com/iflytek/VLE) | [ä¸­æ–‡MiniRBT](https://github.com/iflytek/MiniRBT) | [ä¸­æ–‡LERT](https://github.com/ymcui/LERT) | [ä¸­è‹±æ–‡PERT](https://github.com/ymcui/PERT) | [ä¸­æ–‡MacBERT](https://github.com/ymcui/MacBERT) | [ä¸­æ–‡ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [ä¸­æ–‡XLNet](https://github.com/ymcui/Chinese-XLNet) | [ä¸­æ–‡BERT](https://github.com/ymcui/Chinese-BERT-wwm) | [çŸ¥è¯†è’¸é¦å·¥å…·TextBrewer](https://github.com/airaria/TextBrewer) | [æ¨¡å‹è£å‰ªå·¥å…·TextPruner](https://github.com/airaria/TextPruner)\n\n## æ–°é—»\n\n**[2024/04/30] Chinese-LLaMA-Alpaca-3 å·²æ­£å¼å‘å¸ƒï¼Œå¼€æºåŸºäºLlama-3çš„Llama-3-Chinese-8Bå’ŒLlama-3-Chinese-8B-Instructï¼Œæ¨èæ‰€æœ‰ä¸€æœŸã€äºŒæœŸé¡¹ç›®ç”¨æˆ·å‡çº§è‡³ä¸‰ä»£æ¨¡å‹ï¼Œè¯·å‚é˜…ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca-3**\n\n[2024/03/27] æœ¬é¡¹ç›®å·²å…¥é©»æœºå™¨ä¹‹å¿ƒSOTA!æ¨¡å‹å¹³å°ï¼Œæ¬¢è¿å…³æ³¨ï¼šhttps://sota.jiqizhixin.com/project/chinese-llama-alpaca\n\n[2023/08/14] Chinese-LLaMA-Alpaca-2 v2.0ç‰ˆæœ¬å·²æ­£å¼å‘å¸ƒï¼Œå¼€æºChinese-LLaMA-2-13Bå’ŒChinese-Alpaca-2-13Bï¼Œæ¨èæ‰€æœ‰ä¸€æœŸç”¨æˆ·å‡çº§è‡³äºŒä»£æ¨¡å‹ï¼Œè¯·å‚é˜…ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca-2\n\n[2023/07/31] Chinese-LLaMA-Alpaca-2 v1.0ç‰ˆæœ¬å·²æ­£å¼å‘å¸ƒï¼Œè¯·å‚é˜…ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca-2\n\n[2023/07/19] [v5.0ç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v5.0): å‘å¸ƒAlpaca-Proç³»åˆ—æ¨¡å‹ï¼Œæ˜¾è‘—æå‡å›å¤é•¿åº¦å’Œè´¨é‡ï¼›åŒæ—¶å‘å¸ƒPlus-33Bç³»åˆ—æ¨¡å‹ã€‚\n\n[2023/07/19] ğŸš€å¯åŠ¨[ä¸­æ–‡LLaMA-2ã€Alpaca-2å¼€æºå¤§æ¨¡å‹é¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)ï¼Œæ¬¢è¿å…³æ³¨äº†è§£æœ€æ–°ä¿¡æ¯ã€‚\n\n[2023/07/10] Betaæµ‹è¯•é¢„è§ˆï¼Œæå‰äº†è§£å³å°†åˆ°æ¥çš„æ›´æ–°ï¼šè¯¦è§[è®¨è®ºåŒº](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions/732)\n\n[2023/07/07] Chinese-LLaMA-Alpacaå®¶æ—å†æ·»æ–°æˆå‘˜ï¼Œæ¨å‡ºé¢å‘è§†è§‰é—®ç­”ä¸å¯¹è¯çš„[å¤šæ¨¡æ€ä¸­æ–‡LLaMA&Alpacaå¤§æ¨¡å‹](https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca)ï¼Œå‘å¸ƒäº†7Bæµ‹è¯•ç‰ˆæœ¬ã€‚\n\n[2023/06/30] llama.cppä¸‹8K contextæ”¯æŒï¼ˆæ— éœ€å¯¹æ¨¡å‹åšå‡ºä¿®æ”¹ï¼‰ï¼Œç›¸å…³æ–¹æ³•å’Œè®¨è®ºè§[è®¨è®ºåŒº](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions/696)ï¼›transformersä¸‹æ”¯æŒ4K+ contextçš„ä»£ç è¯·å‚è€ƒ[PR#705](https://github.com/ymcui/Chinese-LLaMA-Alpaca/pull/705)\n\n[2023/06/16] [v4.1ç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v4.1): å‘å¸ƒæ–°ç‰ˆæŠ€æœ¯æŠ¥å‘Šã€æ·»åŠ C-Evalè§£ç è„šæœ¬ã€æ·»åŠ ä½èµ„æºæ¨¡å‹åˆå¹¶è„šæœ¬ç­‰ã€‚\n\n[2023/06/08] [v4.0ç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v4.0): å‘å¸ƒä¸­æ–‡LLaMA/Alpaca-33Bã€æ·»åŠ privateGPTä½¿ç”¨ç¤ºä¾‹ã€æ·»åŠ C-Evalç»“æœç­‰ã€‚\n\n## å†…å®¹å¯¼å¼•\n| ç« èŠ‚                                  | æè¿°                                                         |\n| ------------------------------------- | ------------------------------------------------------------ |\n| [â¬æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)        | ä¸­æ–‡LLaMAã€Alpacaå¤§æ¨¡å‹ä¸‹è½½åœ°å€                |\n| [ğŸˆ´åˆå¹¶æ¨¡å‹](#åˆå¹¶æ¨¡å‹) | ï¼ˆé‡è¦ï¼‰ä»‹ç»å¦‚ä½•å°†ä¸‹è½½çš„LoRAæ¨¡å‹ä¸åŸç‰ˆLLaMAåˆå¹¶ |\n| [ğŸ’»æœ¬åœ°æ¨ç†ä¸å¿«é€Ÿéƒ¨ç½²](#æœ¬åœ°æ¨ç†ä¸å¿«é€Ÿéƒ¨ç½²) | ä»‹ç»äº†å¦‚ä½•å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–å¹¶ä½¿ç”¨ä¸ªäººç”µè„‘éƒ¨ç½²å¹¶ä½“éªŒå¤§æ¨¡å‹ |\n| [ğŸ’¯ç³»ç»Ÿæ•ˆæœ](#ç³»ç»Ÿæ•ˆæœ) | ä»‹ç»äº†éƒ¨åˆ†åœºæ™¯å’Œä»»åŠ¡ä¸‹çš„ä½¿ç”¨ä½“éªŒæ•ˆæœ             |\n| [ğŸ“è®­ç»ƒç»†èŠ‚](#è®­ç»ƒç»†èŠ‚) | ä»‹ç»äº†ä¸­æ–‡LLaMAã€Alpacaå¤§æ¨¡å‹çš„è®­ç»ƒç»†èŠ‚ |\n| [â“FAQ](#FAQ) | ä¸€äº›å¸¸è§é—®é¢˜çš„å›å¤ |\n| [âš ï¸å±€é™æ€§](#å±€é™æ€§) | æœ¬é¡¹ç›®æ¶‰åŠæ¨¡å‹çš„å±€é™æ€§ |\n\n\n## æ¨¡å‹ä¸‹è½½\n\n### ç”¨æˆ·é¡»çŸ¥ï¼ˆå¿…è¯»ï¼‰\n\nFacebookå®˜æ–¹å‘å¸ƒçš„[LLaMAæ¨¡å‹ç¦æ­¢å•†ç”¨](https://github.com/facebookresearch/llama)ï¼Œå¹¶ä¸”å®˜æ–¹æ²¡æœ‰æ­£å¼å¼€æºæ¨¡å‹æƒé‡ï¼ˆè™½ç„¶ç½‘ä¸Šå·²ç»æœ‰å¾ˆå¤šç¬¬ä¸‰æ–¹çš„ä¸‹è½½åœ°å€ï¼‰ã€‚ä¸ºäº†éµå¾ªç›¸åº”çš„è®¸å¯ï¼Œ**è¿™é‡Œå‘å¸ƒçš„æ˜¯LoRAæƒé‡**ï¼Œå¯ä»¥ç†è§£ä¸ºåŸLLaMAæ¨¡å‹ä¸Šçš„ä¸€ä¸ªâ€œè¡¥ä¸â€ï¼Œä¸¤è€…åˆå¹¶å³å¯è·å¾—å®Œæ•´ç‰ˆæƒé‡ã€‚ä»¥ä¸‹ä¸­æ–‡LLaMA/Alpaca LoRAæ¨¡å‹æ— æ³•å•ç‹¬ä½¿ç”¨ï¼Œéœ€è¦æ­é…[åŸç‰ˆLLaMAæ¨¡å‹](https://github.com/facebookresearch/llama)ã€‚è¯·å‚è€ƒæœ¬é¡¹ç›®ç»™å‡ºçš„[åˆå¹¶æ¨¡å‹](#åˆå¹¶æ¨¡å‹)æ­¥éª¤é‡æ„æ¨¡å‹ã€‚\n\n### æ¨¡å‹åˆ—è¡¨\n\nä¸‹å›¾å±•ç¤ºäº†æœ¬é¡¹ç›®ä»¥åŠ[äºŒæœŸé¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)æ¨å‡ºçš„æ‰€æœ‰å¤§æ¨¡å‹ä¹‹é—´çš„å…³ç³»ã€‚\n\n![](./pics/models.png)\n\n### æ¨¡å‹é€‰æ‹©æŒ‡å¼•\n\nä¸‹é¢æ˜¯ä¸­æ–‡LLaMAå’ŒAlpacaæ¨¡å‹çš„åŸºæœ¬å¯¹æ¯”ä»¥åŠå»ºè®®ä½¿ç”¨åœºæ™¯ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºï¼‰ï¼Œæ›´å¤šå†…å®¹è§[è®­ç»ƒç»†èŠ‚](#è®­ç»ƒç»†èŠ‚)ã€‚\n\n| å¯¹æ¯”é¡¹                | ä¸­æ–‡LLaMA                                              | ä¸­æ–‡Alpaca                                                   |\n| :-------------------- | ------------------------------------------------------ | ------------------------------------------------------------ |\n| è®­ç»ƒæ–¹å¼              | ä¼ ç»ŸCLM                            | æŒ‡ä»¤ç²¾è°ƒ                                                     |\n| æ¨¡å‹ç±»å‹ | åŸºåº§æ¨¡å‹ | æŒ‡ä»¤ç†è§£æ¨¡å‹ï¼ˆç±»ChatGPTï¼‰ |\n| è®­ç»ƒè¯­æ–™ | æ— æ ‡æ³¨é€šç”¨è¯­æ–™ | æœ‰æ ‡æ³¨æŒ‡ä»¤æ•°æ® |\n| è¯è¡¨å¤§å°<sup>[3]</sup> | 4995**3** | 4995**4**=49953+1ï¼ˆpad tokenï¼‰ |\n| è¾“å…¥æ¨¡æ¿              | ä¸éœ€è¦                                                 | éœ€è¦ç¬¦åˆæ¨¡æ¿è¦æ±‚<sup>[1]</sup> |\n| é€‚ç”¨åœºæ™¯ âœ”ï¸            | æ–‡æœ¬ç»­å†™ï¼šç»™å®šä¸Šæ–‡å†…å®¹ï¼Œè®©æ¨¡å‹ç”Ÿæˆä¸‹æ–‡            | æŒ‡ä»¤ç†è§£ï¼ˆé—®ç­”ã€å†™ä½œã€å»ºè®®ç­‰ï¼‰ï¼›å¤šè½®ä¸Šä¸‹æ–‡ç†è§£ï¼ˆèŠå¤©ç­‰ï¼‰ |\n| ä¸é€‚ç”¨åœºæ™¯ âŒ          | æŒ‡ä»¤ç†è§£ ã€å¤šè½®èŠå¤©ç­‰                                  |  æ–‡æœ¬æ— é™åˆ¶è‡ªç”±ç”Ÿæˆ                                                       |\n| llama.cpp             | ä½¿ç”¨`-p`å‚æ•°æŒ‡å®šä¸Šæ–‡                                   | ä½¿ç”¨`-ins`å‚æ•°å¯åŠ¨æŒ‡ä»¤ç†è§£+èŠå¤©æ¨¡å¼                          |\n| text-generation-webui |  ä¸é€‚åˆchatæ¨¡å¼                              |    ä½¿ç”¨`--cpu`å¯åœ¨æ— æ˜¾å¡å½¢å¼ä¸‹è¿è¡Œ                                                          |\n| LlamaChat             | åŠ è½½æ¨¡å‹æ—¶é€‰æ‹©\"LLaMA\"                                  | åŠ è½½æ¨¡å‹æ—¶é€‰æ‹©\"Alpaca\"                                       |\n| [HFæ¨ç†ä»£ç ](./scripts/inference/inference_hf.py) | æ— éœ€æ·»åŠ é¢å¤–å¯åŠ¨å‚æ•° | å¯åŠ¨æ—¶æ·»åŠ å‚æ•° `--with_prompt`        |\n| [web-demoä»£ç ](./scripts/inference/gradio_demo.py) | ä¸é€‚ç”¨ | ç›´æ¥æä¾›Alpacaæ¨¡å‹ä½ç½®å³å¯ï¼›æ”¯æŒå¤šè½®å¯¹è¯ |\n| [LangChainç¤ºä¾‹](./scripts/langchain) / privateGPT | ä¸é€‚ç”¨ | ç›´æ¥æä¾›Alpacaæ¨¡å‹ä½ç½®å³å¯ |\n| å·²çŸ¥é—®é¢˜              | å¦‚æœä¸æ§åˆ¶ç»ˆæ­¢ï¼Œåˆ™ä¼šä¸€ç›´å†™ä¸‹å»ï¼Œç›´åˆ°è¾¾åˆ°è¾“å‡ºé•¿åº¦ä¸Šé™ã€‚<sup>[2]</sup> | è¯·ä½¿ç”¨Proç‰ˆï¼Œä»¥é¿å…Plusç‰ˆå›å¤è¿‡çŸ­çš„é—®é¢˜ã€‚ |\n\n*[1] llama.cpp/LlamaChat/[HFæ¨ç†ä»£ç ](./scripts/inference/inference_hf.py)/[web-demoä»£ç ](./scripts/inference/gradio_demo.py)/[LangChainç¤ºä¾‹](./scripts/langchain)ç­‰å·²å†…åµŒï¼Œæ— éœ€æ‰‹åŠ¨æ·»åŠ æ¨¡æ¿ã€‚*<br/>\n*[2] å¦‚æœå‡ºç°æ¨¡å‹å›ç­”è´¨é‡ç‰¹åˆ«ä½ã€èƒ¡è¨€ä¹±è¯­ã€ä¸ç†è§£é—®é¢˜ç­‰æƒ…å†µï¼Œè¯·æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„æ¨¡å‹å’Œå¯åŠ¨å‚æ•°ã€‚*<br/>\n*[3] ç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„Alpacaä¼šæ¯”LLaMAå¤šä¸€ä¸ªpad tokenï¼Œ**å› æ­¤è¯·å‹¿æ··ç”¨LLaMA/Alpacaè¯è¡¨**ã€‚*\n\n### æ¨èæ¨¡å‹ä¸‹è½½\n\nä»¥ä¸‹ä¸ºæœ¬é¡¹ç›®æ¨èä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œé€šå¸¸ä½¿ç”¨äº†æ›´å¤šçš„è®­ç»ƒæ•°æ®å’Œä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•å’Œå‚æ•°ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è¿™äº›æ¨¡å‹ï¼ˆå…¶ä½™æ¨¡å‹è¯·æŸ¥çœ‹[å…¶ä»–æ¨¡å‹](#å…¶ä»–æ¨¡å‹)ï¼‰ã€‚**å¦‚å¸Œæœ›ä½“éªŒç±»ChatGPTå¯¹è¯äº¤äº’ï¼Œè¯·ä½¿ç”¨Alpacaæ¨¡å‹ï¼Œè€Œä¸æ˜¯LLaMAæ¨¡å‹ã€‚** å¯¹äºAlpacaæ¨¡å‹ï¼ŒProç‰ˆé’ˆå¯¹å›å¤å†…å®¹è¿‡çŸ­çš„é—®é¢˜è¿›è¡Œæ”¹è¿›ï¼Œæ¨¡å‹å›å¤æ•ˆæœæœ‰æ˜æ˜¾æå‡ï¼›å¦‚æœæ›´åå¥½çŸ­å›å¤ï¼Œè¯·é€‰æ‹©Plusç³»åˆ—ã€‚\n\n| æ¨¡å‹åç§°                  |   ç±»å‹   | è®­ç»ƒæ•°æ® |                   é‡æ„æ¨¡å‹<sup>[1]</sup>                   | å¤§å°<sup>[2]</sup> |                    LoRAä¸‹è½½<sup>[3]</sup>                    |\n| :------------------------ | :------: | :------: | :--------------------------------------------------------: | :----------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-Plus-7B  | åŸºåº§æ¨¡å‹ | é€šç”¨120G |        åŸç‰ˆLLaMA-7B         |        790M        | [[ç™¾åº¦]](https://pan.baidu.com/s/1zvyX9FN-WSRDdrtMARxxfw?pwd=2gtr) [[Google]](https://drive.google.com/file/d/1N97m3rBj-rp-J1X8rgRfluyomEscfAq0/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-plus-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-plus-lora-7b) |\n| Chinese-LLaMA-Plus-13B | åŸºåº§æ¨¡å‹ | é€šç”¨120G |        åŸç‰ˆLLaMA-13B        |        1.0G        | [[ç™¾åº¦]](https://pan.baidu.com/s/1VGpNlrLx5zHuNzLOcTG-xw?pwd=8cvd) [[Google]](https://drive.google.com/file/d/1q0L5Me_1j_9iiRRNfuEFUt3SOjQo3-g3/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-plus-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-plus-lora-13b)|\n| Chinese-LLaMA-Plus-33B ğŸ†• | åŸºåº§æ¨¡å‹ | é€šç”¨120G | åŸç‰ˆLLaMA-33B | 1.3G<sup>[6]</sup> | [[ç™¾åº¦]](https://pan.baidu.com/s/1v2WsSA0RFyVfy7FXY9A2NA?pwd=n8ws) [[Google]](https://drive.google.com/file/d/1S4pBPiIZo7fXqf8hjnFaeE7Z-yZFEta9/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-plus-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-plus-lora-33b)|\n| Chinese-Alpaca-Pro-7B ğŸ†• | æŒ‡ä»¤æ¨¡å‹ | æŒ‡ä»¤4.3M | *åŸç‰ˆLLaMA-7B &<br/>LLaMA-Plus-7B*<sup>[4]</sup> | 1.1G | [[ç™¾åº¦]](https://pan.baidu.com/s/1M7whRwG5DRRkzRXCH4aF3g?pwd=fqpd) [[Google]](https://drive.google.com/file/d/1yfIJ2IXymaTaJ8l7VMnb5LnvQFx3idh-/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-pro-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-pro-lora-7b) |\n| Chinese-Alpaca-Pro-13B ğŸ†• | æŒ‡ä»¤æ¨¡å‹ | æŒ‡ä»¤4.3M | *åŸç‰ˆLLaMA-13B &<br/>LLaMA-Plus-13B<sup>[4]</sup>* | 1.3G | [[ç™¾åº¦]](https://pan.baidu.com/s/1ok5Iiou-MovZa7bFLvt4uA?pwd=m79g) [[Google]](https://drive.google.com/file/d/1IY8PzMje1LM2bIgnniArnmmE8qYaJV_I/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-pro-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-pro-lora-13b)|\n| Chinese-Alpaca-Pro-33B ğŸ†• | æŒ‡ä»¤æ¨¡å‹ | æŒ‡ä»¤4.3M | *åŸç‰ˆLLaMA-33B &<br/>LLaMA-Plus-33B<sup>[4]</sup>* | 2.1G | [[ç™¾åº¦]](https://pan.baidu.com/s/1u2TWZcsG_PZSTnmuu7vwww?pwd=8zj8) [[Google]](https://drive.google.com/file/d/14sFEhRq9c-p8S_TiVYNBnmPr4hk-nhs-/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-pro-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-pro-lora-33b)|\n\n*[1] é‡æ„éœ€è¦åŸç‰ˆLLaMAæ¨¡å‹ï¼Œ[å»LLaMAé¡¹ç›®ç”³è¯·ä½¿ç”¨](https://github.com/facebookresearch/llama)æˆ–å‚è€ƒè¿™ä¸ª[PR](https://github.com/facebookresearch/llama/pull/73/files)ã€‚å› ç‰ˆæƒé—®é¢˜æœ¬é¡¹ç›®æ— æ³•æä¾›ä¸‹è½½é“¾æ¥ã€‚*<br/>\n*[2] ç»è¿‡é‡æ„åçš„æ¨¡å‹å¤§å°æ¯”åŒç­‰é‡çº§çš„åŸç‰ˆLLaMAå¤§ä¸€äº›ï¼ˆä¸»è¦å› ä¸ºæ‰©å……äº†è¯è¡¨ï¼‰ã€‚*<br/>\n*[3] ä¸‹è½½ååŠ¡å¿…æ£€æŸ¥å‹ç¼©åŒ…ä¸­æ¨¡å‹æ–‡ä»¶çš„SHA256æ˜¯å¦ä¸€è‡´ï¼Œè¯·æŸ¥çœ‹[SHA256.md](./SHA256.md)ã€‚*<br/>\n*[4] Alpaca-Plusæ¨¡å‹éœ€è¦åŒæ—¶ä¸‹è½½å¯¹åº”çš„LLaMA-Plusæ¨¡å‹ï¼Œè¯·å‚è€ƒ[åˆå¹¶æ•™ç¨‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/æ‰‹åŠ¨æ¨¡å‹åˆå¹¶ä¸è½¬æ¢#å¤šloraæƒé‡åˆå¹¶é€‚ç”¨äºchinese-alpaca-plus)ã€‚*<br/>\n*[5] æœ‰äº›åœ°æ–¹ç§°ä¸º30Bï¼Œå®é™…ä¸Šæ˜¯Facebookåœ¨å‘å¸ƒæ¨¡å‹æ—¶å†™é”™äº†ï¼Œè®ºæ–‡é‡Œä»ç„¶å†™çš„æ˜¯33Bã€‚*<br/>*[6] é‡‡ç”¨FP16å­˜å‚¨ï¼Œæ•…æ¨¡å‹ä½“ç§¯è¾ƒå°ã€‚*\n\nå‹ç¼©åŒ…å†…æ–‡ä»¶ç›®å½•å¦‚ä¸‹ï¼ˆä»¥Chinese-LLaMA-7Bä¸ºä¾‹ï¼‰ï¼š\n\n```\nchinese_llama_lora_7b/\n  - adapter_config.json\t\t# LoRAæƒé‡é…ç½®æ–‡ä»¶\n  - adapter_model.bin\t\t# LoRAæƒé‡æ–‡ä»¶\n  - special_tokens_map.json\t# special_tokens_mapæ–‡ä»¶\n  - tokenizer_config.json\t# tokenizeré…ç½®æ–‡ä»¶\n  - tokenizer.model\t\t# tokenizeræ–‡ä»¶ \n```\n\n\n### å…¶ä»–æ¨¡å‹ä¸‹è½½\n\nç”±äºè®­ç»ƒæ–¹å¼å’Œè®­ç»ƒæ•°æ®ç­‰å› ç´ å½±å“ï¼Œ**ä»¥ä¸‹æ¨¡å‹å·²ä¸å†æ¨èä½¿ç”¨ï¼ˆç‰¹å®šåœºæ™¯ä¸‹å¯èƒ½ä»ç„¶æœ‰ç”¨ï¼‰**ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­çš„[æ¨èæ¨¡å‹](#æ¨èä¸‹è½½æ¨¡å‹)ã€‚\n\n| æ¨¡å‹åç§°          |   ç±»å‹   | è®­ç»ƒæ•°æ® | é‡æ„æ¨¡å‹ | å¤§å° |                    LoRAä¸‹è½½                    |\n| :---------------- | :------: | :------: | :--------------------: | :----------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-7B  | åŸºåº§æ¨¡å‹ | é€šç”¨20G  |      åŸç‰ˆLLaMA-7B      |        770M        | [[ç™¾åº¦]](https://pan.baidu.com/s/1oORTdpr2TvlkxjpyWtb5Sw?pwd=33hb) [[Google]](https://drive.google.com/file/d/1iQp9T-BHjBjIrFWXq_kIm_cyNmpvv5WN/view?usp=sharing)<br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-lora-7b) |\n| Chinese-LLaMA-13B | åŸºåº§æ¨¡å‹ | é€šç”¨20G  |     åŸç‰ˆLLaMA-13B      |        1.0G        | [[ç™¾åº¦]](https://pan.baidu.com/s/1BxFhYhDMipW7LwI58cGmQQ?pwd=ef3t) [[Google]](https://drive.google.com/file/d/12q9EH4mfKRnoKlbkkhzv1xDwWnroo9VS/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-lora-13b) |\n| Chinese-LLaMA-33B | åŸºåº§æ¨¡å‹ | é€šç”¨20G | åŸç‰ˆLLaMA-33B | 2.7G | [[ç™¾åº¦]](https://pan.baidu.com/s/1-ylGyeM70QZ5vbEug5RD-A?pwd=hp6f) [[Google]](https://drive.google.com/file/d/1NwsLYbuEByUxre5GqTN5EkxiuZSRxUy_/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-lora-33b)|\n| Chinese-Alpaca-7B         | æŒ‡ä»¤æ¨¡å‹ |  æŒ‡ä»¤2M  |                        åŸç‰ˆLLaMA-7B                        |        790M        | [[ç™¾åº¦]](https://pan.baidu.com/s/1xV1UXjh1EPrPtXg6WyG7XQ?pwd=923e) [[Google]](https://drive.google.com/file/d/1JvFhBpekYiueWiUL3AF1TtaWDb3clY5D/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-lora-7b) |\n| Chinese-Alpaca-13B        | æŒ‡ä»¤æ¨¡å‹ |  æŒ‡ä»¤3M  |                       åŸç‰ˆLLaMA-13B                        |        1.1G        | [[ç™¾åº¦]](https://pan.baidu.com/s/1wYoSF58SnU9k0Lndd5VEYg?pwd=mm8i) [[Google]](https://drive.google.com/file/d/1gzMc0xMCpXsXmU1uxFlgQ8VRnWNtDjD8/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-lora-13b)|\n| Chinese-Alpaca-33B | æŒ‡ä»¤æ¨¡å‹ | æŒ‡ä»¤4.3M | åŸç‰ˆLLaMA-33B | 2.8G | [[ç™¾åº¦]](https://pan.baidu.com/s/1fey7lGMMw3GT982l8uJYMg?pwd=2f2s) [[Google]](https://drive.google.com/file/d/1YeSgnZWaRkKdmYa-JHiIlcvqhrDd4-Y4/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-lora-33b) |\n| Chinese-Alpaca-Plus-7B  | æŒ‡ä»¤æ¨¡å‹ |  æŒ‡ä»¤4M  |  *åŸç‰ˆLLaMA-7B &<br/>LLaMA-Plus-7B*  |        1.1G        | [[ç™¾åº¦]](https://pan.baidu.com/s/12tjjxmDWwLBM8Tj_7FAjHg?pwd=32hc) [[Google]](https://drive.google.com/file/d/1EDcTmq6tDmRxqarpapdyDGBE9opY0zrB/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-plus-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-plus-lora-7b) |\n| Chinese-Alpaca-Plus-13B | æŒ‡ä»¤æ¨¡å‹ | æŒ‡ä»¤4.3M | *åŸç‰ˆLLaMA-13B &<br/>LLaMA-Plus-13B* |        1.3G        | [[ç™¾åº¦]](https://pan.baidu.com/s/1Mew4EjBlejWBBB6_WW6vig?pwd=mf5w) [[Google]](https://drive.google.com/file/d/1CcLJvY7XsFAOjfSIqCpDI7jf3EEPDcEF/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-plus-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-plus-lora-13b)|\n| Chinese-Alpaca-Plus-33B | æŒ‡ä»¤æ¨¡å‹ | æŒ‡ä»¤4.3M | *åŸç‰ˆLLaMA-33B &<br/>LLaMA-Plus-33B* | 2.1G | [[ç™¾åº¦]](https://pan.baidu.com/s/1j2prOjiQGB8S5x67Uj8XZw?pwd=3pac) [[Google]](https://drive.google.com/file/d/1YUaT-NOReoF-z1vzj2khwYKdj4Z_ekbO/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-plus-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-plus-lora-33b)|\n\n### ğŸ¤—transformersè°ƒç”¨\n\nå¯ä»¥åœ¨ğŸ¤—Model Hubä¸‹è½½ä»¥ä¸Šæ‰€æœ‰æ¨¡å‹ï¼Œå¹¶ä¸”ä½¿ç”¨[transformers](https://github.com/huggingface/transformers)å’Œ[PEFT](https://github.com/huggingface/peft)è°ƒç”¨ä¸­æ–‡LLaMAæˆ–Alpaca LoRAæ¨¡å‹ã€‚ä»¥ä¸‹æ¨¡å‹è°ƒç”¨åç§°æŒ‡çš„æ˜¯ä½¿ç”¨`.from_pretrained()`ä¸­æŒ‡å®šçš„æ¨¡å‹åç§°ã€‚\n\nè¯¦ç»†æ¸…å•ä¸æ¨¡å‹ä¸‹è½½åœ°å€ï¼šhttps://huggingface.co/hfl\n\n## åˆå¹¶æ¨¡å‹\n\nå‰é¢æåˆ°LoRAæ¨¡å‹æ— æ³•å•ç‹¬ä½¿ç”¨ï¼Œå¿…é¡»ä¸åŸç‰ˆLLaMAè¿›è¡Œåˆå¹¶æ‰èƒ½è½¬ä¸ºå®Œæ•´æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œæ¨¡å‹æ¨ç†ã€é‡åŒ–æˆ–è€…è¿›ä¸€æ­¥è®­ç»ƒã€‚è¯·é€‰æ‹©ä»¥ä¸‹æ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œè½¬æ¢åˆå¹¶ã€‚\n\n| æ–¹å¼         | é€‚ç”¨åœºæ™¯                                                   |                             æ•™ç¨‹                             |\n| :----------- | :--------------------------------------------------------- | :----------------------------------------------------------: |\n| **åœ¨çº¿è½¬æ¢** | Colabç”¨æˆ·å¯åˆ©ç”¨æœ¬é¡¹ç›®æä¾›çš„notebookè¿›è¡Œåœ¨çº¿è½¬æ¢å¹¶é‡åŒ–æ¨¡å‹  | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/åœ¨çº¿æ¨¡å‹åˆå¹¶ä¸è½¬æ¢) |\n| **æ‰‹åŠ¨è½¬æ¢** | ç¦»çº¿æ–¹å¼è½¬æ¢ï¼Œç”Ÿæˆä¸åŒæ ¼å¼çš„æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œé‡åŒ–æˆ–è¿›ä¸€æ­¥ç²¾è°ƒ | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/æ‰‹åŠ¨æ¨¡å‹åˆå¹¶ä¸è½¬æ¢) |\n\nä»¥ä¸‹æ˜¯åˆå¹¶æ¨¡å‹åï¼ŒFP16ç²¾åº¦å’Œ4-bité‡åŒ–åçš„å¤§å°ï¼Œè½¬æ¢å‰ç¡®ä¿æœ¬æœºæœ‰è¶³å¤Ÿçš„å†…å­˜å’Œç£ç›˜ç©ºé—´ï¼ˆæœ€ä½è¦æ±‚ï¼‰ï¼š\n\n| æ¨¡å‹ç‰ˆæœ¬            |   7B   |   13B   |   33B   |   65B   |\n| :------------------ | :----: | :-----: | :-----: | :-----: |\n| åŸæ¨¡å‹å¤§å°ï¼ˆFP16ï¼‰  | 13 GB  |  24 GB  |  60 GB  | 120 GB  |\n| é‡åŒ–åå¤§å°ï¼ˆ8-bitï¼‰ | 7.8 GB | 14.9 GB | 32.4 GB | ~60 GB  |\n| é‡åŒ–åå¤§å°ï¼ˆ4-bitï¼‰ | 3.9 GB | 7.8 GB  | 17.2 GB | 38.5 GB |\n\nå…·ä½“å†…å®¹è¯·å‚è€ƒæœ¬é¡¹ç›® >>> [ğŸ“š GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/æ¨¡å‹åˆå¹¶ä¸è½¬æ¢)\n\n## æœ¬åœ°æ¨ç†ä¸å¿«é€Ÿéƒ¨ç½²\n\næœ¬é¡¹ç›®ä¸­çš„æ¨¡å‹ä¸»è¦æ”¯æŒä»¥ä¸‹é‡åŒ–ã€æ¨ç†å’Œéƒ¨ç½²æ–¹å¼ã€‚\n\n| æ¨ç†å’Œéƒ¨ç½²æ–¹å¼                                               | ç‰¹ç‚¹                                         | å¹³å°  | CPU  | GPU  | é‡åŒ–åŠ è½½ | å›¾å½¢ç•Œé¢ |                             æ•™ç¨‹                             |\n| :----------------------------------------------------------- | -------------------------------------------- | :---: | :--: | :--: | :------: | :------: | :----------------------------------------------------------: |\n| [**llama.cpp**](https://github.com/ggerganov/llama.cpp)      | ä¸°å¯Œçš„é‡åŒ–é€‰é¡¹å’Œé«˜æ•ˆæœ¬åœ°æ¨ç†                 | é€šç”¨  |  âœ…   |  âœ…   |    âœ…     |    âŒ     | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cppé‡åŒ–éƒ¨ç½²) |\n| [**ğŸ¤—Transformers**](https://github.com/huggingface/transformers) | åŸç”Ÿtransformersæ¨ç†æ¥å£                    | é€šç”¨  |  âœ…   |  âœ…   |    âœ…     |    âœ…     | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨Transformersæ¨ç†) |\n| [**text-generation-webui**](https://github.com/oobabooga/text-generation-webui) | å‰ç«¯Web UIç•Œé¢çš„éƒ¨ç½²æ–¹å¼                     | é€šç”¨  |  âœ…   |  âœ…   |    âœ…     |    âœ…     | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨text-generation-webuiæ­å»ºç•Œé¢) |\n| [**LlamaChat**](https://github.com/alexrozanski/LlamaChat)   | macOSä¸‹çš„å›¾å½¢äº¤äº’ç•Œé¢ | MacOS |  âœ…   |  âŒ   |    âœ…     |    âœ…     | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨LlamaChatå›¾å½¢ç•Œé¢ï¼ˆmacOSï¼‰) |\n| [**LangChain**](https://github.com/hwchase17/langchain)      | LLMåº”ç”¨å¼€å‘æ¡†æ¶ï¼Œé€‚ç”¨äºè¿›è¡ŒäºŒæ¬¡å¼€å‘          | é€šç”¨  | âœ…<sup>â€ </sup> |  âœ…   | âœ…<sup>â€ </sup> |    âŒ     | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä¸LangChainè¿›è¡Œé›†æˆ) |\n| [**privateGPT**](https://github.com/imartinez/privateGPT) | åŸºäºLangChainçš„å¤šæ–‡æ¡£æœ¬åœ°é—®ç­”æ¡†æ¶ | é€šç”¨ | âœ… | âœ… | âœ… | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨privateGPTè¿›è¡Œå¤šæ–‡æ¡£é—®ç­”) |\n| [**Colab Gradio Demo**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/gradio_web_demo.ipynb) | Colabä¸­å¯åŠ¨åŸºäºGradioçš„äº¤äº’å¼WebæœåŠ¡ | é€šç”¨ | âœ… | âœ… | âœ… | âŒ | [link](https://colab.research.google.com/github/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/gradio_web_demo.ipynb) |\n| [**APIè°ƒç”¨**](https://platform.openai.com/docs/api-reference) | ä»¿OpenAI APIæ¥å£çš„æœåŠ¡å™¨Demo | é€šç”¨ | âœ… | âœ… | âœ… | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/APIè°ƒç”¨) |\n\n<sup>â€ </sup>: LangChainæ¡†æ¶æ”¯æŒï¼Œä½†æ•™ç¨‹ä¸­æœªå®ç°ï¼›è¯¦ç»†è¯´æ˜è¯·å‚è€ƒLangChainå®˜æ–¹æ–‡æ¡£ã€‚\n\nå…·ä½“å†…å®¹è¯·å‚è€ƒæœ¬é¡¹ç›® >>> [ğŸ“š GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/æ¨¡å‹æ¨ç†ä¸éƒ¨ç½²)\n\n## ç³»ç»Ÿæ•ˆæœ\n\n### ç”Ÿæˆæ•ˆæœè¯„æµ‹\n\nä¸ºäº†å¿«é€Ÿè¯„æµ‹ç›¸å…³æ¨¡å‹çš„å®é™…æ–‡æœ¬ç”Ÿæˆè¡¨ç°ï¼Œæœ¬é¡¹ç›®åœ¨ç»™å®šç›¸åŒçš„promptçš„æƒ…å†µä¸‹ï¼Œåœ¨ä¸€äº›å¸¸è§ä»»åŠ¡ä¸Šå¯¹æ¯”æµ‹è¯•äº†æœ¬é¡¹ç›®çš„ä¸­æ–‡Alpaca-7Bã€ä¸­æ–‡Alpaca-13Bã€ä¸­æ–‡Alpaca-33Bã€ä¸­æ–‡Alpaca-Plus-7Bã€ä¸­æ–‡Alpaca-Plus-13Bçš„æ•ˆæœã€‚ç”Ÿæˆå›å¤å…·æœ‰éšæœºæ€§ï¼Œå—è§£ç è¶…å‚ã€éšæœºç§å­ç­‰å› ç´ å½±å“ã€‚ä»¥ä¸‹ç›¸å…³è¯„æµ‹å¹¶éç»å¯¹ä¸¥è°¨ï¼Œæµ‹è¯•ç»“æœä»…ä¾›æ™¾æ™’å‚è€ƒï¼Œæ¬¢è¿è‡ªè¡Œä½“éªŒã€‚\n\n- è¯¦ç»†è¯„æµ‹ç»“æœåŠç”Ÿæˆæ ·ä¾‹è¯·æŸ¥çœ‹[examplesç›®å½•](./examples)\n- ğŸ“Š Alpacaæ¨¡å‹åœ¨çº¿å¯¹æˆ˜ï¼š[http://llm-arena.ymcui.com](http://llm-arena.ymcui.com/)\n\n### å®¢è§‚æ•ˆæœè¯„æµ‹\n\næœ¬é¡¹ç›®è¿˜åœ¨â€œNLUâ€ç±»å®¢è§‚è¯„æµ‹é›†åˆä¸Šå¯¹ç›¸å…³æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚è¿™ç±»è¯„æµ‹çš„ç»“æœä¸å…·æœ‰ä¸»è§‚æ€§ï¼Œåªéœ€è¦è¾“å‡ºç»™å®šæ ‡ç­¾ï¼ˆéœ€è¦è®¾è®¡æ ‡ç­¾mappingç­–ç•¥ï¼‰ï¼Œå› æ­¤å¯ä»¥ä»å¦å¤–ä¸€ä¸ªä¾§é¢äº†è§£å¤§æ¨¡å‹çš„èƒ½åŠ›ã€‚æœ¬é¡¹ç›®åœ¨è¿‘æœŸæ¨å‡ºçš„[C-Evalè¯„æµ‹æ•°æ®é›†](https://cevalbenchmark.com)ä¸Šæµ‹è¯•äº†ç›¸å…³æ¨¡å‹æ•ˆæœï¼Œå…¶ä¸­æµ‹è¯•é›†åŒ…å«12.3Kä¸ªé€‰æ‹©é¢˜ï¼Œæ¶µç›–52ä¸ªå­¦ç§‘ã€‚ä»¥ä¸‹æ˜¯éƒ¨åˆ†æ¨¡å‹çš„validå’Œtesté›†è¯„æµ‹ç»“æœï¼ˆAverageï¼‰ï¼Œå®Œæ•´ç»“æœè¯·å‚è€ƒ[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2304.08177)ã€‚\n\n| æ¨¡å‹                    | Valid (zero-shot) | Valid (5-shot) | Test (zero-shot) | Test (5-shot) |\n| ----------------------- | :---------------: | :------------: | :--------------: | :-----------: |\n| Chinese-Alpaca-Plus-33B |       46.5        |      46.3      |       44.9       |     43.5      |\n| Chinese-Alpaca-33B      |       43.3        |      42.6      |       41.6       |     40.4      |\n| Chinese-Alpaca-Plus-13B |       43.3        |      42.4      |       41.5       |     39.9      |\n| Chinese-Alpaca-Plus-7B  |       36.7        |      32.9      |       36.4       |     32.3      |\n| Chinese-LLaMA-Plus-33B  |       37.4        |      40.0      |       35.7       |     38.3      |\n| Chinese-LLaMA-33B       |       34.9        |      38.4      |       34.6       |     39.5      |\n| Chinese-LLaMA-Plus-13B  |       27.3        |      34.0      |       27.8       |     33.3      |\n| Chinese-LLaMA-Plus-7B   |       27.3        |      28.3      |       26.9       |     28.4      |\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç»¼åˆè¯„ä¼°å¤§æ¨¡å‹èƒ½åŠ›ä»ç„¶æ˜¯äºŸå¾…è§£å†³çš„é‡è¦è¯¾é¢˜ï¼Œåˆç†è¾©è¯åœ°çœ‹å¾…å¤§æ¨¡å‹ç›¸å…³å„ç§è¯„æµ‹ç»“æœæœ‰åŠ©äºå¤§æ¨¡å‹æŠ€æœ¯çš„è‰¯æ€§å‘å±•ã€‚æ¨èç”¨æˆ·åœ¨è‡ªå·±å…³æ³¨çš„ä»»åŠ¡ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œé€‰æ‹©é€‚é…ç›¸å…³ä»»åŠ¡çš„æ¨¡å‹ã€‚\n\nC-Evalæ¨ç†ä»£ç è¯·å‚è€ƒæœ¬é¡¹ç›® >>> [ğŸ“š GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/C-Evalè¯„æµ‹ç»“æœä¸è„šæœ¬)\n\n## è®­ç»ƒç»†èŠ‚\n\næ•´ä¸ªè®­ç»ƒæµç¨‹åŒ…æ‹¬è¯è¡¨æ‰©å……ã€é¢„è®­ç»ƒå’ŒæŒ‡ä»¤ç²¾è°ƒä¸‰éƒ¨åˆ†ã€‚\n\n- æœ¬é¡¹ç›®çš„æ¨¡å‹å‡åœ¨åŸLLaMAè¯è¡¨çš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡å•è¯ï¼Œä»£ç è¯·å‚è€ƒ[merge_tokenizers.py](./scripts/merge_tokenizer/merge_tokenizers.py)\n- é¢„è®­ç»ƒå’ŒæŒ‡ä»¤ç²¾è°ƒä»£ç å‚è€ƒäº†ğŸ¤—transformersä¸­çš„[run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)å’Œ[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)é¡¹ç›®ä¸­æ•°æ®é›†å¤„ç†çš„ç›¸å…³éƒ¨åˆ†\n- å·²å¼€æºç”¨äºé¢„è®­ç»ƒå’ŒæŒ‡ä»¤ç²¾è°ƒçš„è®­ç»ƒè„šæœ¬ï¼š[é¢„è®­ç»ƒè„šæœ¬Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/é¢„è®­ç»ƒè„šæœ¬)ã€[æŒ‡ä»¤ç²¾è°ƒè„šæœ¬Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/æŒ‡ä»¤ç²¾è°ƒè„šæœ¬)\n\n\nå…·ä½“å†…å®¹è¯·å‚è€ƒæœ¬é¡¹ç›® >>> [ğŸ“š GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/è®­ç»ƒç»†èŠ‚)\n\n## FAQ\n\nFAQä¸­ç»™å‡ºäº†å¸¸è§é—®é¢˜çš„è§£ç­”ï¼Œè¯·åœ¨æIssueå‰åŠ¡å¿…å…ˆæŸ¥çœ‹FAQã€‚\n\n```\né—®é¢˜1ï¼šä¸ºä»€ä¹ˆä¸èƒ½æ”¾å‡ºå®Œæ•´ç‰ˆæœ¬æƒé‡ï¼Ÿ\né—®é¢˜2ï¼šåé¢ä¼šæœ‰33Bã€65Bçš„ç‰ˆæœ¬å—ï¼Ÿ\né—®é¢˜3ï¼šä¸€äº›ä»»åŠ¡ä¸Šæ•ˆæœä¸å¥½ï¼\né—®é¢˜4ï¼šä¸ºä»€ä¹ˆè¦æ‰©å……è¯è¡¨ï¼Ÿç›´æ¥åœ¨åŸç‰ˆLLaMAä¸Šç”¨ä¸­æ–‡é¢„è®­ç»ƒä¸è¡Œå—ï¼Ÿ\né—®é¢˜5ï¼šå›å¤å†…å®¹å¾ˆçŸ­\né—®é¢˜6ï¼šWindowsä¸‹ï¼Œæ¨¡å‹æ— æ³•ç†è§£ä¸­æ–‡ã€ç”Ÿæˆé€Ÿåº¦å¾ˆæ…¢ç­‰é—®é¢˜\né—®é¢˜7ï¼šChinese-LLaMA 13Bæ¨¡å‹æ²¡æ³•ç”¨llama.cppå¯åŠ¨ï¼Œæç¤ºç»´åº¦ä¸ä¸€è‡´\né—®é¢˜8ï¼šChinese-Alpaca-Plusæ•ˆæœå¾ˆå·®\né—®é¢˜9ï¼šæ¨¡å‹åœ¨NLUç±»ä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ†ç±»ç­‰ï¼‰ä¸Šæ•ˆæœä¸å¥½\né—®é¢˜10ï¼šä¸ºä»€ä¹ˆå«33Bï¼Œä¸åº”è¯¥æ˜¯30Bå—ï¼Ÿ\né—®é¢˜11ï¼šæ¨¡å‹åˆå¹¶ä¹‹åSHA256ä¸ä¸€è‡´\n```\n\nå…·ä½“é—®é¢˜å’Œè§£ç­”è¯·å‚è€ƒæœ¬é¡¹ç›® >>> [ğŸ“š GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/å¸¸è§é—®é¢˜)\n\n\n## å±€é™æ€§\n\nè™½ç„¶æœ¬é¡¹ç›®ä¸­çš„æ¨¡å‹å…·å¤‡ä¸€å®šçš„ä¸­æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨å±€é™æ€§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š\n\n- å¯èƒ½ä¼šäº§ç”Ÿä¸å¯é¢„æµ‹çš„æœ‰å®³å†…å®¹ä»¥åŠä¸ç¬¦åˆäººç±»åå¥½å’Œä»·å€¼è§‚çš„å†…å®¹\n- ç”±äºç®—åŠ›å’Œæ•°æ®é—®é¢˜ï¼Œç›¸å…³æ¨¡å‹çš„è®­ç»ƒå¹¶ä¸å……åˆ†ï¼Œä¸­æ–‡ç†è§£èƒ½åŠ›æœ‰å¾…è¿›ä¸€æ­¥æå‡\n- æš‚æ—¶æ²¡æœ‰åœ¨çº¿å¯äº’åŠ¨çš„demoï¼ˆæ³¨ï¼šç”¨æˆ·ä»ç„¶å¯ä»¥è‡ªè¡Œåœ¨æœ¬åœ°éƒ¨ç½²ï¼‰\n\n\n## å¼•ç”¨\n\nå¦‚æœæ‚¨è§‰å¾—æœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©æˆ–ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„ä»£ç æˆ–æ•°æ®ï¼Œè¯·å‚è€ƒå¼•ç”¨æœ¬é¡¹ç›®çš„æŠ€æœ¯æŠ¥å‘Šï¼šhttps://arxiv.org/abs/2304.08177\n```\n@article{chinese-llama-alpaca,\n      title={Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca}, \n      author={Cui, Yiming and Yang, Ziqing and Yao, Xin},\n      journal={arXiv preprint arXiv:2304.08177},\n      url={https://arxiv.org/abs/2304.08177},\n      year={2023}\n}\n```\n\n\n## ç›¸å…³é¡¹ç›®\n\n| é¡¹ç›®åç§°                                                     | ç®€ä»‹                           |  ç±»å‹  |\n| :----------------------------------------------------------- | :----------------------------- | :----: |\n| [**Chinese-LLaMA-Alpaca-2**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)ï¼ˆå®˜æ–¹é¡¹ç›®ï¼‰ | ä¸­æ–‡LLaMA-2ã€Alpaca-2å¤§æ¨¡å‹    |  æ–‡æœ¬  |\n| [**Visual-Chinese-LLaMA-Alpaca**](https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca)ï¼ˆå®˜æ–¹é¡¹ç›®ï¼‰ | å¤šæ¨¡æ€ä¸­æ–‡LLaMA & Alpacaå¤§æ¨¡å‹ | å¤šæ¨¡æ€ |\n\næƒ³è¦åŠ å…¥åˆ—è¡¨ï¼Ÿ>>> [æäº¤ç”³è¯·](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions/740)\n\n## è‡´è°¢\n\næœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å¼€æºé¡¹ç›®äºŒæ¬¡å¼€å‘ï¼Œåœ¨æ­¤å¯¹ç›¸å…³é¡¹ç›®å’Œç ”ç©¶å¼€å‘äººå‘˜è¡¨ç¤ºæ„Ÿè°¢ã€‚\n\n|                        åŸºç¡€æ¨¡å‹ã€ä»£ç                         |                       é‡åŒ–ã€æ¨ç†ã€éƒ¨ç½²                       |                             æ•°æ®                             |\n| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| [LLaMA by Facebook](https://github.com/facebookresearch/llama)<br/>[Alpaca by Stanford](https://github.com/tatsu-lab/stanford_alpaca)<br/>[alpaca-lora by @tloen](https://github.com/tloen/alpaca-lora) | [llama.cpp by @ggerganov](https://github.com/ggerganov/llama.cpp)<br/>[LlamaChat by @alexrozanski]( https://github.com/alexrozanski/LlamaChat)<br/>[text-generation-webui by @oobabooga](https://github.com/oobabooga/text-generation-webui) | [pCLUE and MT data by @brightmart](https://github.com/brightmart/nlp_chinese_corpus)<br/>[oasst1 by OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1) |\n\n## å…è´£å£°æ˜\n\n**æœ¬é¡¹ç›®ç›¸å…³èµ„æºä»…ä¾›å­¦æœ¯ç ”ç©¶ä¹‹ç”¨ï¼Œä¸¥ç¦ç”¨äºå•†ä¸šç”¨é€”ã€‚** ä½¿ç”¨æ¶‰åŠç¬¬ä¸‰æ–¹ä»£ç çš„éƒ¨åˆ†æ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªç›¸åº”çš„å¼€æºåè®®ã€‚æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å—æ¨¡å‹è®¡ç®—ã€éšæœºæ€§å’Œé‡åŒ–ç²¾åº¦æŸå¤±ç­‰å› ç´ å½±å“ï¼Œæœ¬é¡¹ç›®ä¸å¯¹å…¶å‡†ç¡®æ€§ä½œå‡ºä¿è¯ã€‚å¯¹äºæ¨¡å‹è¾“å‡ºçš„ä»»ä½•å†…å®¹ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œäº¦ä¸å¯¹å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœè€Œå¯èƒ½äº§ç”Ÿçš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚æœ¬é¡¹ç›®ç”±ä¸ªäººåŠåä½œè€…ä¸šä½™æ—¶é—´å‘èµ·å¹¶ç»´æŠ¤ï¼Œå› æ­¤æ— æ³•ä¿è¯èƒ½åŠæ—¶å›å¤è§£å†³ç›¸åº”é—®é¢˜ã€‚\n\n\n## é—®é¢˜åé¦ˆ\nå¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚ç¤¼è²Œåœ°æå‡ºé—®é¢˜ï¼Œæ„å»ºå’Œè°çš„è®¨è®ºç¤¾åŒºã€‚\n\n- åœ¨æäº¤é—®é¢˜ä¹‹å‰ï¼Œè¯·å…ˆæŸ¥çœ‹FAQèƒ½å¦è§£å†³é—®é¢˜ï¼ŒåŒæ—¶å»ºè®®æŸ¥é˜…ä»¥å¾€çš„issueæ˜¯å¦èƒ½è§£å†³ä½ çš„é—®é¢˜ã€‚\n- æäº¤é—®é¢˜è¯·ä½¿ç”¨æœ¬é¡¹ç›®è®¾ç½®çš„Issueæ¨¡æ¿ï¼Œä»¥å¸®åŠ©å¿«é€Ÿå®šä½å…·ä½“é—®é¢˜ã€‚\n- é‡å¤ä»¥åŠä¸æœ¬é¡¹ç›®æ— å…³çš„issueä¼šè¢«[stable-bot](https://github.com/marketplace/stale)å¤„ç†ï¼Œæ•¬è¯·è°…è§£ã€‚\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 35.1123046875,
          "content": "# [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3) is launched!\n\n[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README.md) | [**ğŸŒEnglish**](./README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki) | [**â“æé—®/Issues**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/issues) | [**ğŸ’¬è®¨è®º/Discussions**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions) | [**âš”ï¸ç«æŠ€åœº/Arena**](http://llm-arena.ymcui.com/)\n\n<p align=\"center\">\n    <br>\n    <img src=\"./pics/banner.png\" width=\"700\"/>\n    <br>\n</p>\n<p align=\"center\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca.svg?color=blue&style=flat-square\">\n    <img alt=\"GitHub release (latest by date)\" src=\"https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca\">\n    <img alt=\"GitHub top language\" src=\"https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca\">\n    <img alt=\"GitHub last commit\" src=\"https://img.shields.io/github/last-commit/ymcui/Chinese-LLaMA-Alpaca\">\n    <a href=\"https://app.codacy.com/gh/ymcui/Chinese-LLaMA-Alpaca/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade\"><img src=\"https://app.codacy.com/project/badge/Grade/1710faac5e634acaabfc26b0a778cdde\"/></a>\n</p>\n\n\n\nTo promote open research of large models in the Chinese NLP community, this project has open-sourced the **Chinese LLaMA model and the Alpaca large model with instruction fine-tuning**. These models expand the Chinese vocabulary based on the original LLaMA and use Chinese data for secondary pre-training, further enhancing Chinese basic semantic understanding. Additionally, the project uses Chinese instruction data for fine-tuning on the basis of the Chinese LLaMA, significantly improving the model's understanding and execution of instructions.\n\n**Technical Report (V2)**ï¼š[[Cui, Yang, and Yao, 2023] Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca](https://arxiv.org/abs/2304.08177)\n\n**Main contents of this project:**\n\n- ğŸš€ Extended Chinese vocabulary on top of original LLaMA with significant encode/decode efficiency\n- ğŸš€ Open-sourced the Chinese LLaMA (general purpose) and Alpaca (instruction-tuned) \n- ğŸš€ Open-sourced the pre-training and instruction finetuning (SFT) scripts for further tuning on user's data\n- ğŸš€ Quickly deploy and experience the quantized version of the large model on CPU/GPU of your laptop (personal PC)\n- ğŸš€ Support [ğŸ¤—transformers](https://github.com/huggingface/transformers), [llama.cpp](https://github.com/ggerganov/llama.cpp), [text-generation-webui](https://github.com/oobabooga/text-generation-webui), [LlamaChat](https://github.com/alexrozanski/LlamaChat), [LangChain](https://github.com/hwchase17/langchain), , [privateGPT](https://github.com/imartinez/privateGPT), etc.\n- Released versions: 7B (basic, **Plus**, **Pro**), 13B (basic, **Plus**, **Pro**), 33B (basic, **Plus**, **Pro**)\n\nğŸ’¡ The following image shows the actual experience effect of the 7B version model after local deployment (animation unaccelerated, tested on Apple M1 Max).\n\n![](./pics/screencast.gif)\n\n----\n\n[**Chinese-LLaMA-Alpaca-2**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)| [Visual Chinese-LLaMA-Alpaca](https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca) | [Multi-modal VLE](https://github.com/iflytek/VLE) | [Chinese MiniRBT](https://github.com/iflytek/MiniRBT) | [Chinese LERT](https://github.com/ymcui/LERT) | [Chinese-English PERT](https://github.com/ymcui/PERT) | [Chinese MacBERT](https://github.com/ymcui/MacBERT) | [Chinese ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [Chinese XLNet](https://github.com/ymcui/Chinese-XLNet) | [Chinese BERT](https://github.com/ymcui/Chinese-BERT-wwm) | [Knowledge distillation tool TextBrewer](https://github.com/airaria/TextBrewer) | [Model pruning tool TextPruner](https://github.com/airaria/TextPruner)\n\n## News\n\n**[Apr 30, 2024] Chinese-LLaMA-Alpaca-3 project introduces Llama-3-Chinese-8B and Llama-3-Chinese-8B-Instruct, based on Meta's Llama-3. Check: https://github.com/ymcui/Chinese-LLaMA-Alpaca-3**\n\n[Mar 27, 2024] This project is now online at the SOTA! model platform of Synced, see: https://sota.jiqizhixin.com/project/chinese-llama-alpaca\n\n[Aug 14, 2023] Chinese-LLaMA-Alpaca-2 v2.0 released. We open-source Chinese-LLaMA-2-13B and Chinese-Alpaca-2-13B. See https://github.com/ymcui/Chinese-LLaMA-Alpaca-2\n\n[July 19, 2023] [Release v5.0](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v5.0): Release Alpaca-Pro models, significantly improve generation quality. Along with Plus-33B models.\n\n[July 19, 2023] We are launching [Chinese-LLaMA-Alpaca-2 project](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2).\n\n[July 10, 2023] Beta channel preview, know coming updates in advance. See [Discussion](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions/732)\n\n[July 7, 2023] The Chinese-LLaMA-Alpaca family welcomes a new member: [Visual Chinese-LLaMA-Alpaca model](https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca) for visual question answering and chat. The 7B test version is available.\n\n[June 30, 2023] 8K context support with llama.cpp. See [Discussion](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions/696). For 4K+ context support with transformers, see [PR#705](https://github.com/ymcui/Chinese-LLaMA-Alpaca/pull/705).\n\n[June 16, 2023] [Release v4.1](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v4.1): New technical report, add C-Eval inference script, add low-resource model merging script, etc.\n\n[June 8, 2023] [Release v4.0](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v4.0): LLaMA/Alpaca 33B versions are available. We also add privateGPT demo, C-Eval results, etc.\n\n## Content Navigation\n\n| Chapter                                       | Description                                                  |\n| --------------------------------------------- | ------------------------------------------------------------ |\n| [Download](#Model-Download)                         | Download links for Chinese LLaMA and Alpaca                  |\n| [Model Reconstruction](#Model-Reconstruction) | (Important) Explains how to merge downloaded LoRA models with the original LLaMA |\n| [Quick Deployment](#Quick-Deployment)         | Steps for quantize and deploy LLMs on personal computers     |\n| [Example Results](#System-Performance)           | Examples of the system output                                |\n| [Training Details](#Training-Details)         | Introduces the training details of Chinese LLaMA and Alpaca  |\n| [FAQ](#FAQ)                                   | Replies to some common questions                             |\n| [Limitations](#Limitations)                    | Limitations of the models involved in this project           |\n\n## Model Download\n\n### âš ï¸ User Notice (Must Read)\n\nThe official [LLaMA models released by Facebook prohibit commercial use](https://github.com/facebookresearch/llama), and the official model weights have not been open-sourced (although there are many third-party download links available online). In order to comply with the relevant licenses, it is currently not possible to release the complete model weights. We appreciate your understanding. After Facebook fully opens up the model weights, this project will update its policies accordingly. **What is released here are the LoRA weights**, which can be seen as a \"patch\" for the original LLaMA model, and the complete weights can be obtained by merging the two.\n\n### Model Overview\n\nThe following figure depicts all open-sourced models for our projects (including the [second-gen project](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)).\n\n![](./pics/models.png)\n\n### Which model should I use?\n\nThe following table provides a basic comparison of the Chinese LLaMA and Alpaca models, as well as recommended usage scenarios (including, but not limited to). \n\nğŸ’¡ **Plus versions** are trained on more data, which is highly recommended for use.\n\n| Comparison Item                                        | Chinese LLaMA                                                | Chinese Alpaca                                               |\n| ------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Training Method                                        | Traditional CLM (trained on general corpus)                  | Instruction Fine-tuning (trained on instruction data)        |\n| Model Type                                             | Base model                                                   | Instruction-following model (like ChatGPT)                   |\n| Training Data                                          | unsupervised free text                                       | supervised instruction data                                  |\n| Vocab size<sup>[3]</sup>                               | 4995**3**                                                    | 4995**4**=49953+1 (pad token)                                |\n| Input Template                                         | Not required                                                 | Must meet template requirements<sup>[1]</sup>                |\n| Suitable Scenarios âœ”ï¸                                   | Text continuation: Given a context, let the model continue writing | 1. Instruction understanding (Q&A, writing, advice, etc.)<br/>2. Multi-turn context understanding (chat, etc.) |\n| Unsuitable Scenarios âŒ                                 | Instruction understanding, multi-turn chat, etc.             | Unrestricted free text generation                            |\n| llama.cpp                                              | Use `-p` parameter to specify context                        | Use `-ins` parameter to enable instruction understanding + chat mode |\n| text-generation-webui                                  | Not suitable for chat mode                                   | Use `--cpu` to run without a GPU; if not satisfied with generated content, consider modifying prompt |\n| LlamaChat                                              | Choose \"LLaMA\" when loading the model                        | Choose \"Alpaca\" when loading the model                       |\n| [inference_hf.py](./scripts/inference/inference_hf.py) | No additional startup parameters required                    | Add `--with_prompt` parameter when launching                 |\n| [web-demo](./scripts/inference/gradio_demo.py)         | Not applicable                                               | Simply provide the Alpaca model location; support multi-turn conversations |\n| [LangChain-demo](./scripts/langchain) / privateGPT     | Not applicable                                               | Simply provide the Alpaca model location                     |\n| Known Issues                                           | If not controlled for termination, it will continue writing until reaching the output length limit.<sup>[2]</sup> | Please use Pro models to avoid short responses (in Plus series). |\n\n*[1] Templates are built-in for (llama.cpp/LlamaChat/[inference_hf.py](./scripts/inference/inference_hf.py)/[web-demo](./scripts/inference/gradio_demo.py)/[LangChain-demo](./scripts/langchain).*\n\n*[2] If you encounter issues such as low-quality model responses, nonsensical answers, or failure to understand questions, please check whether you are using the correct model and startup parameters for the scenario.*\n\n*[3] Alpaca model has an additional pad token in vocabulary than LLaMA. **Please do not mix LLaMA/Alpaca tokenizers**.*\n\n\n### Recommended Models\n\nBelow is a list of models recommended for this project. These models typically use more training data and optimized model training methods and parameters, so they should be used preferentially (for other models, please check [Other Models](#Other-Models)). **If you want to experience ChatGPT-like interaction, please use the Alpaca model instead of the LLaMA model.** For Alpaca models, please use Pro versions for longer responses. If you prefer shorter response, please use Plus series instead.\n\n| Model                   |            Type             |       Data       |         Required Original Model<sup>[1]</sup>          | Size<sup>[2]</sup> |                 Download Links<sup>[3]</sup>                 |\n| :---------------------- | :-------------------------: | :--------------: | :----------------------------------------------------: | :----------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-Plus-7B  | base model | general 120G |        LLaMA-7B         |        790M        | [[ç™¾åº¦]](https://pan.baidu.com/s/1zvyX9FN-WSRDdrtMARxxfw?pwd=2gtr) [[Google]](https://drive.google.com/file/d/1N97m3rBj-rp-J1X8rgRfluyomEscfAq0/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-plus-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-plus-lora-7b) |\n| Chinese-LLaMA-Plus-13B | base model | general 120G |        LLaMA-13B        |        1.0G        | [[ç™¾åº¦]](https://pan.baidu.com/s/1VGpNlrLx5zHuNzLOcTG-xw?pwd=8cvd) [[Google]](https://drive.google.com/file/d/1q0L5Me_1j_9iiRRNfuEFUt3SOjQo3-g3/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-plus-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-plus-lora-13b) |\n| Chinese-LLaMA-Plus-33B ğŸ†• | base model | general 120G | LLaMA-33B | 1.3G<sup>[6]</sup> | [[ç™¾åº¦]](https://pan.baidu.com/s/1v2WsSA0RFyVfy7FXY9A2NA?pwd=n8ws) [[Google]](https://drive.google.com/file/d/1S4pBPiIZo7fXqf8hjnFaeE7Z-yZFEta9/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-plus-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-plus-lora-33b) |\n| Chinese-Alpaca-Pro-7B ğŸ†• | instruction-following model | instruction 4.3M | *LLaMA-7B &<br/>LLaMA-Plus-7B*<sup>[4]</sup> | 1.1G | [[ç™¾åº¦]](https://pan.baidu.com/s/1M7whRwG5DRRkzRXCH4aF3g?pwd=fqpd) [[Google]](https://drive.google.com/file/d/1yfIJ2IXymaTaJ8l7VMnb5LnvQFx3idh-/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-pro-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-pro-lora-7b) |\n| Chinese-Alpaca-Pro-13B ğŸ†• | instruction-following model | instruction 4.3M | *LLaMA-13B &<br/>LLaMA-Plus-13B<sup>[4]</sup>* | 1.3G | [[ç™¾åº¦]](https://pan.baidu.com/s/1ok5Iiou-MovZa7bFLvt4uA?pwd=m79g) [[Google]](https://drive.google.com/file/d/1IY8PzMje1LM2bIgnniArnmmE8qYaJV_I/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-pro-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-pro-lora-13b) |\n| Chinese-Alpaca-Pro-33B ğŸ†• | instruction-following model | instruction 4.3M | *LLaMA-33B &<br/>LLaMA-Plus-33B<sup>[4]</sup>* | 2.1G | [[ç™¾åº¦]](https://pan.baidu.com/s/1u2TWZcsG_PZSTnmuu7vwww?pwd=8zj8) [[Google]](https://drive.google.com/file/d/14sFEhRq9c-p8S_TiVYNBnmPr4hk-nhs-/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-pro-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-pro-lora-33b) |\n\n**[1]** The original LLaMA model needs to be applied for use in [Facebook-LLaMA](https://github.com/facebookresearch/llama) or refer to this [PR](https://github.com/facebookresearch/llama/pull/73/files). Due to copyright issues, this project cannot provide downloads, and we ask for your understanding.\n\n**[2]** The reconstructed model is slightly larger than the original LLaMA (due to the expanded vocabulary); the 7B model is about 13G+.\n\n**[3]** After downloading, be sure to check whether the SHA256 of the ZIP file is consistent; for the full value, please see [SHA256.md](./SHA256.md).\n\n**[4]** Merging steps for Alpaca-Plus are different from others, please refer to [wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Manual-Conversion#multiple-lora-weights-merging-applicable-to-chinese-alpaca-plus).\n\n**[5]** Also known as 30B model in elsewhere. There was a naming typo in release this model by Facebook. We stick to their original paper naming convention here (and also the actual numbers of weights).\n\n**[6]** Stored in FP16.\n\nThe file directory inside the ZIP file is as follows (using Chinese-LLaMA as an example):\n\n```\nchinese_llama_lora_7b/\n  - adapter_config.json       # LoRA weight configuration file\n  - adapter_model.bin         # LoRA weight file\n  - special_tokens_map.json   # special_tokens_map file\n  - tokenizer_config.json     # tokenizer configuration file\n  - tokenizer.model           # tokenizer file\n```\n\n### Other Models\n\nDue to factors such as training methods and training data, **the models below are no longer recommended (they may still be useful in specific scenarios)**. Please preferentially use the [recommended models](#Recommended-Models) in the previous section.\n\n| Model              |            Type             |      Data      | Required Original Model<sup>[1]</sup> | Size<sup>[2]</sup> |                 Download Links<sup>[3]</sup>                 |\n| :----------------- | :-------------------------: | :------------: | :-----------------------------------: | :----------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-7B  | Base model | general 20G  |      LLaMA-7B      |        770M        | [[ç™¾åº¦]](https://pan.baidu.com/s/1oORTdpr2TvlkxjpyWtb5Sw?pwd=33hb) [[Google]](https://drive.google.com/file/d/1iQp9T-BHjBjIrFWXq_kIm_cyNmpvv5WN/view?usp=sharing)<br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-lora-7b) |\n| Chinese-LLaMA-13B | Base model | general 20G  |     LLaMA-13B      |        1.0G        | [[ç™¾åº¦]](https://pan.baidu.com/s/1BxFhYhDMipW7LwI58cGmQQ?pwd=ef3t) [[Google]](https://drive.google.com/file/d/12q9EH4mfKRnoKlbkkhzv1xDwWnroo9VS/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-lora-13b) |\n| Chinese-LLaMA-33B | Base model | general 20G | LLaMA-33B | 2.7G | [[ç™¾åº¦]](https://pan.baidu.com/s/1-ylGyeM70QZ5vbEug5RD-A?pwd=hp6f) [[Google]](https://drive.google.com/file/d/1NwsLYbuEByUxre5GqTN5EkxiuZSRxUy_/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-lora-33b) |\n| Chinese-Alpaca-7B         | Instruction-following model |  instruction 2M  |                        LLaMA-7B                        |        790M        | [[ç™¾åº¦]](https://pan.baidu.com/s/1xV1UXjh1EPrPtXg6WyG7XQ?pwd=923e) [[Google]](https://drive.google.com/file/d/1JvFhBpekYiueWiUL3AF1TtaWDb3clY5D/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-lora-7b) |\n| Chinese-Alpaca-13B        | Instruction-following model |  instruction 3M  |                       LLaMA-13B                        |        1.1G        | [[ç™¾åº¦]](https://pan.baidu.com/s/1wYoSF58SnU9k0Lndd5VEYg?pwd=mm8i) [[Google]](https://drive.google.com/file/d/1gzMc0xMCpXsXmU1uxFlgQ8VRnWNtDjD8/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-lora-13b) |\n| Chinese-Alpaca-33B | Instruction-following model | instruction 4.3M | LLaMA-33B | 2.8G | [[ç™¾åº¦]](https://pan.baidu.com/s/1fey7lGMMw3GT982l8uJYMg?pwd=2f2s) [[Google]](https://drive.google.com/file/d/1YeSgnZWaRkKdmYa-JHiIlcvqhrDd4-Y4/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-lora-33b) |\n| Chinese-Alpaca-Plus-7B  | Instruction-following model |  instruction 4M  |  *LLaMA-7B &<br/>LLaMA-Plus-7B*  |        1.1G        | [[ç™¾åº¦]](https://pan.baidu.com/s/12tjjxmDWwLBM8Tj_7FAjHg?pwd=32hc) [[Google]](https://drive.google.com/file/d/1EDcTmq6tDmRxqarpapdyDGBE9opY0zrB/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-plus-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-plus-lora-7b) |\n| Chinese-Alpaca-Plus-13B | Instruction-following model | instruction 4.3M | *LLaMA-13B &<br/>LLaMA-Plus-13B* |        1.3G        | [[ç™¾åº¦]](https://pan.baidu.com/s/1Mew4EjBlejWBBB6_WW6vig?pwd=mf5w) [[Google]](https://drive.google.com/file/d/1CcLJvY7XsFAOjfSIqCpDI7jf3EEPDcEF/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-plus-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-plus-lora-13b) |\n| Chinese-Alpaca-Plus-33B | Instruction-following model | instruction 4.3M | *LLaMA-33B &<br/>LLaMA-Plus-33B* | 2.1G | [[ç™¾åº¦]](https://pan.baidu.com/s/1j2prOjiQGB8S5x67Uj8XZw?pwd=3pac) [[Google]](https://drive.google.com/file/d/1YUaT-NOReoF-z1vzj2khwYKdj4Z_ekbO/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-plus-lora-33b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-plus-lora-33b) |\n\n### Use with ğŸ¤—transformers\n\nYou can download all the above models from ğŸ¤—Model Hub and use them with [transformers](https://github.com/huggingface/transformers) and [PEFT](https://github.com/huggingface/peft) to invoke the Chinese LLaMA or Alpaca LoRA models. The model invocation names referred to below are the model names specified in `.from_pretrained()`.\n\nDetailed list and model download link: https://huggingface.co/hfl\n\n## Model Reconstruction\n\nIn order to merge the LoRA model with the original LLaMA for further tuning or inference, two methods are currently provided:\n\n| Method                | Usage                                                        |                           Tutorial                           |\n| :-------------------- | :----------------------------------------------------------- | :----------------------------------------------------------: |\n| **Online conversion** | Suitable for Google Colab users, can use notebook for online conversion and model quantization. | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Online-conversion-with-Colab) |\n| **Manual conversion** | Suitable for offline conversion, generates models in different formats for quantization or further fine-tuning. | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Manual-Conversion) |\n\nThe following is the size of each original model and 4-bit quantization. When converting the corresponding model, make sure that the machine has enough memory and disk space (minimum requirements):\n\n|                    |   7B   |   13B   |   33B   |   65B   |\n| :----------------- | :----: | :-----: | :-----: | :-----: |\n| Originalï¼ˆFP16ï¼‰   | 13 GB  |  24 GB  |  60 GB  | 120 GB  |\n| Quantized (8-bit)  | 7.8 GB | 14.9 GB | 32.4 GB | ~60 GB  |\n| Quantizedï¼ˆ4-bitï¼‰ | 3.9 GB | 7.8 GB  | 17.2 GB | 38.5 GB |\n\nRelated documentation has been moved to the project's >>> [ğŸ“šGitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Model-Reconstruction).\n\n## Quick Deployment\n\nWe mainly provide the following three ways for inference and local deployment.\n\n| Method                                                       | Features                                                     | Platform | CPU  | GPU  | Quantization |  UI  |                           Tutorial                           |\n| :----------------------------------------------------------- | ------------------------------------------------------------ | :------: | :--: | :--: | :----------: | :--: | :----------------------------------------------------------: |\n| [**llama.cpp**](https://github.com/ggerganov/llama.cpp)      | a tool for quantizing model and deploying on local CPU       | General  |  âœ…   |  âœ…   |      âœ…       |  âŒ   | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp-Deployment) |\n| [**ğŸ¤—Transformers**](https://github.com/huggingface/transformers) | original transformers inference method, support CPU/GPU      | General  |  âœ…   |  âœ…   |      âœ…       |  âœ…   | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Inference-with-Transformers) |\n| [**text-generation-webui**](https://github.com/oobabooga/text-generation-webui) | a tool for deploying model as a web UI                       | General  |  âœ…   |  âœ…   |      âœ…       |  âœ…   | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/text-generation-webui) |\n| [**LlamaChat**](https://github.com/alexrozanski/LlamaChat)   | a macOS app that allows you to chat with LLaMA, Alpaca, etc. |  MacOS   |  âœ…   |  âŒ   |      âœ…       |  âœ…   | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Using-LlamaChat-Interface) |\n| [**LangChain**](https://github.com/hwchase17/langchain)      | LLM application development framework, suitable for secondary development | General | âœ…<sup>â€ </sup> |  âœ…   | âœ…<sup>â€ </sup> |    âŒ     | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Integrated-with-LangChain) |\n| [**privateGPT**](https://github.com/imartinez/privateGPT) | LangChain-based multi-document QA framework | General | âœ… | âœ… | âœ… | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Use-privateGPT-for-multi-document-QA) |\n| [**Colab Gradio Demo**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/gradio_web_demo.ipynb) | Running a Gradio web demo in Colab | General | âœ… | âœ… | âœ… | âŒ | [link](https://colab.research.google.com/github/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/gradio_web_demo.ipynb) |\n| [**API Calls**](https://platform.openai.com/docs/api-reference) | A server that implements OPENAI API | General | âœ… | âœ… | âœ… | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/API-Calls) |\n\n\n<sup>â€ </sup>: Supported by LangChain, but not implemented in the tutorial. Please refer to the official LangChain Documentation for details.\n\nRelated documentation has been moved to the project's >>> [ğŸ“šGitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Model-Inference-and-Deployment).\n\n## System Performance\n\n### Generation Performance Test\n\nIn order to quickly evaluate the actual performance of related models, this project compared the effects of Chinese Alpaca-7B, Alpaca-13B, Alpaca-Plus-7B, Alpaca-Plus-13B, and Alpaca-33B on some common tasks given the same prompt. Reply generation is random and is affected by factors such as decoding hyperparameters and random seeds. The following related evaluations are not absolutely rigorous, and the test results are for reference only. Welcome to experience it yourself. \n\n- For detailed evaluation results, please see [examples](./examples)\n- ğŸ“Š Alpaca ChatBot Arena: [http://llm-arena.ymcui.com](http://llm-arena.ymcui.com/)\n\n### NLU Performance Test\n\nThis project also conducted tests on relevant models using the \"NLU\" objective evaluation dataset. The results of this type of evaluation are objective and only require the output of given labels, so they can provide insights into the capabilities of large models from another perspective. In the recently launched [C-Eval dataset](https://cevalbenchmark.com/), this project tested the performance of the relevant models. The test set contains 12.3K multiple-choice questions covering 52 subjects. The following are the evaluation results (average) of some models on the validation and test sets. For complete results, please refer to our [technical report](https://arxiv.org/abs/2304.08177).\n\n| Models                  | Valid (zero-shot) | Valid (5-shot) | Test (zero-shot) | Test (5-shot) |\n| ----------------------- | :---------------: | :------------: | :--------------: | :-----------: |\n| Chinese-Alpaca-Plus-33B |       46.5        |      46.3      |       44.9       |     43.5      |\n| Chinese-Alpaca-33B      |       43.3        |      42.6      |       41.6       |     40.4      |\n| Chinese-Alpaca-Plus-13B |       43.3        |      42.4      |       41.5       |     39.9      |\n| Chinese-Alpaca-Plus-7B  |       36.7        |      32.9      |       36.4       |     32.3      |\n| Chinese-LLaMA-Plus-33B  |       37.4        |      40.0      |       35.7       |     38.3      |\n| Chinese-LLaMA-33B       |       34.9        |      38.4      |       34.6       |     39.5      |\n| Chinese-LLaMA-Plus-13B  |       27.3        |      34.0      |       27.8       |     33.3      |\n| Chinese-LLaMA-Plus-7B   |       27.3        |      28.3      |       26.9       |     28.4      |\n\nIt is important to note that the comprehensive assessment of the capabilities of large models is still an urgent and significant topic to address. It is beneficial to approach the various evaluation results of large models in a rational and balanced manner to promote the healthy development of large-scale model technology. It is recommended for users to conduct tests on their own tasks and choose models that are suitable for the relevant tasks.\n\nFor C-Eval inference code, please refer to >>> [ğŸ“šGitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/C-Eval-performance-and-script).\n\n## Training Details\n\nThe entire training process includes three parts: vocabulary expansion, pre-training, and instruction fine-tuning. Please refer to [merge_tokenizers.py](scripts/merge_tokenizer/merge_tokenizers.py) for vocabulary expansion; refer to [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py) in ğŸ¤—transformers and the relevant parts of dataset processing in the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) project for pre-training and self-instruct fine-tuning.\n\nWe have open-sourced the scripts for pre-training and instruction finetuning (SFT): \n\n- Pre-training: [scripts/training/run_clm_pt_with_peft.py](./scripts/training/run_clm_pt_with_peft.py), refer to [Pre-training Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Pretraining-Script)\n\n- Instruction Finetuning: [scripts/training/run_clm_sft_with_peft.py](./scripts/training/run_clm_sft_with_peft.py), refer to [SFT Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/SFT-Script)\n\nPlease refer to our  >>> [ğŸ“šGitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Training-Details).\n\n\n## FAQ\n\nFAQ provides answers to frequent questions. Please see our FAQ before submitting an issue.\n\n```\nQ1: Why can't you release the complete model weights?\nQ2: Will there be versions of 33B, and 65B in the future?\nQ3: The model doesn't perform well on some tasks!\nQ4: Why expand the vocabulary? Can't you just pre-train the original LLaMA with Chinese data?\nQ5: The reply is very short\nQ6: Under Windows, the model cannot understand Chinese, the generation speed is very slow, etc.\nQ7: Chinese-LLaMA 13B model cannot be launched with llama.cpp, reporting inconsistent dimensions.\nQ8: Chinese-Alpaca-Plus does not show better performance than the others.\nQ9: The model does not perform well on NLU tasks, such as text classification.\nQ10: Why 33B not 30B?\nQ11: Inconsistent SHA256\n```\n\nPlease refer to our  >>> [ğŸ“šGitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/FAQ).\n\n## Limitations\n\nAlthough the models in this project have significantly improved Chinese understanding and generation capabilities compared to the original LLaMA and Alpaca, there are also the following limitations:\n\n- It may produce unpredictable harmful content and content that does not conform to human preferences and values.\n- Due to computing power and data issues, the training of the related models is not sufficient, and the Chinese understanding ability needs to be further improved.\n- There is no online interactive demo available for now (Note: users can still deploy it locally themselves).\n\n## Citation\n\nIf you find the model, data, code in our project useful, please consider citing our work as follows: https://arxiv.org/abs/2304.08177\n\n```\n@article{chinese-llama-alpaca,\n      title={Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca}, \n      author={Cui, Yiming and Yang, Ziqing and Yao, Xin},\n      journal={arXiv preprint arXiv:2304.08177},\n      url={https://arxiv.org/abs/2304.08177},\n      year={2023}\n}\n```\n\n## Related Projects\n\n| Project Name                                                 | Description                             |    Type     |\n| :----------------------------------------------------------- | :-------------------------------------- | :---------: |\n| [**Chinese-LLaMA-Alpaca-2**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) (Official) | Chinese LLaMA-2, Alpaca-2 LLMs          |    Text     |\n| [**Visual-Chinese-LLaMA-Alpaca**](https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca) (Official) | Multi-modal Chinese LLaMA & Alpaca LLMs | Multi-modal |\n\nWant to join this list? >>> [Apply Here](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions/740)\n\n\n## Acknowledgements\n\nThis project is based on the following open-source projects for secondary development, and we would like to express our gratitude to the related projects and research and development personnel.\n\n|                   Foundation Models, Codes                   |             Quantization, Inference, Deployment              |                             Data                             |\n| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| [LLaMA by Facebook](https://github.com/facebookresearch/llama)<br/>[Alpaca by Stanford](https://github.com/tatsu-lab/stanford_alpaca)<br/>[alpaca-lora by @tloen](https://github.com/tloen/alpaca-lora) | [llama.cpp by @ggerganov](https://github.com/ggerganov/llama.cpp)<br/>[LlamaChat by @alexrozanski]( https://github.com/alexrozanski/LlamaChat)<br/>[text-generation-webui by @oobabooga](https://github.com/oobabooga/text-generation-webui) | [pCLUE and translation data by @brightmart](https://github.com/brightmart/nlp_chinese_corpus)<br/>[oasst1 by OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1) |\n\nEpisode: The current logo is automatically generated by GPT-4 with the DALLÂ·E plugin (previously generated by midjourney).\n\n## Disclaimer\n\n**The resources related to this project are for academic research purposes only and are strictly prohibited for commercial use.** When using parts involving third-party code, please strictly follow the corresponding open-source agreements. The content generated by the model is affected by factors such as model calculation, randomness, and quantization accuracy loss. This project cannot guarantee its accuracy. For any content output by the model, this project does not assume any legal responsibility and does not assume responsibility for any losses that may result from the use of related resources and output results.\n\nThis project is initiated and maintained by individuals and collaborators in their spare time, so we cannot guarantee a timely response to resolving relevant issues.\n\n## Feedback\n\nIf you have any questions, please submit them in GitHub Issues.\n\n- Before submitting a question, please check if the FAQ can solve the problem and consult past issues to see if they can help.\n- Please use our dedicated issue template for submitting.\n- Duplicate and unrelated issues will be handled by [stable-bot](https://github.com/marketplace/stale); please understand.\n- Raise questions politely and help build a harmonious discussion community.\n"
        },
        {
          "name": "SHA256.md",
          "type": "blob",
          "size": 8.515625,
          "content": "# SHA256\n\n**ä¸ºäº†ä¿è¯æ–‡ä»¶çš„å®Œæ•´æ€§ï¼Œè¯·ä¸€å®šè¦æ£€æŸ¥ä¸‹åˆ—æ–‡ä»¶SHA256å€¼çš„ä¸€è‡´æ€§ã€‚**\n\n**To ensure the completeness of the model, please check the folllowing SHA256 before using them.**\n\n### Original LLaMA (by Meta AI)\n\n#### consolidated.*.pth format (original)\n\nä¸‹è¡¨å±•ç¤ºäº†Facebookå‘å¸ƒçš„åŸç‰ˆè‹±æ–‡LLaMAçš„SHA256ã€‚\n\nThe followings are SHA256 values for the original LLaMA files.\n\n| Model | Parts | SHA256 (consolidated.*.pth)                                  |\n| ----- | :---: | ------------------------------------------------------------ |\n| 7B    |  00   | 700df0d3013b703a806d2ae7f1bfb8e59814e3d06ae78be0c66368a50059f33d |\n| 13B   |  00   | 745bf4e29a4dd6f411e72976d92b452da1b49168a4f41c951cfcc8051823cf08 |\n| 13B   |  01   | d5ccbcc465c71c0de439a5aeffebe8344c68a519bce70bc7f9f92654ee567085 |\n| 33B   |  00   | e23294a58552d8cdec5b7e8abb87993b97ea6eced4178ff2697c02472539d067 |\n| 33B   |  01   | 4e077b7136c7ae2302e954860cf64930458d3076fcde9443f4d0e939e95903ff |\n| 33B   |  02   | 24a87f01028cbd3a12de551dcedb712346c0b5cbdeff1454e0ddf2df9b675378 |\n| 33B   |  03   | 1adfcef71420886119544949767f6a56cb6339b4d5fcde755d80fe68b49de93b |\n\n#### pytorch.bin format (huggingface)\n\nä¸‹è¡¨å±•ç¤ºäº†åŸç‰ˆè‹±æ–‡LLaMAè½¬æ¢ä¸ºHFæ ¼å¼ä¹‹åçš„SHA256ã€‚å¦‚æœä½ è¦ä½¿ç”¨huggingfaceä¸Šçš„æ¨¡å‹ï¼Œè¯·åŠ¡å¿…æ¯”å¯¹ã€‚\n\nThe followings are SHA256 values for the original LLaMA files (HF format). If you want to use the models on huggingface model hub, make sure to check these values.\n\n| Model | SHA256 (pytorch_model-*.bin)                                       |\n| ----- | ------------------------------------------------------------ |\n| 7B    | 0087155d6df07106c1d910bfeb6aab1be8e612dfbf2b56ddfb4ccbde7dbd50d0<br/>461bc5e50200db7813ff99cc0b9316c48ccbd6aaaa31bf8cf7bee0b64bc3eda3 |\n| 13B   | dd20cdee2637408c6ab88c13f5c27d153bacd0e99f2d55f6a66fbd0269944436<br/>1aba886c5f28d2e2e9b04ab3c4f5bc250c6b06efc1baa3f557677b3097f70e6a<br/>2efc56cddb7877c830bc5b402ee72996aedb5694c9e8007bf1d52d72c0d97d26 |\n| 33B   | 9c2a7223ab5f9cf3d46913d2b776e99cbd6ed93f69991594b92a8cef0c681a78<br/>4984274738e52195f4b1a5b35d719cf0fade6df1f645507d92d61af4dd8dcdfe<br/>64c73932562810c5c33b15bfec5921d3ced0e8cdb3766c214eda2f45fa3edd13<br/>c7d72d11770f5b58eb45c2dd8e19aae2cbd5a03463b564de3945b21825ebacba<br/>174128542031f4ad7ceb6c799e8e5461ec1ca91a72a01402c567e5f6a8b33d8c<br/>80e2cfa18994385fa88f03d500017346dfd6dc1e58e957d046af39d9a7e254fa<br/>065611608159615ced8d38473ee693129a1a0d872ced0ad8daf09290af7c7061 |\n\n\n### Our LLaMA/Alpaca Model\n\n#### Tokenizer.model\n\nä¸‹è¡¨å±•ç¤ºäº†`tokenizer.model`çš„SHA256ã€‚è¯·æ³¨æ„LLaMAä¸Alpacaçš„`tokenizer.model`ä¸åŒã€‚å¯¹äºåŒä¸€ä¸ªæ¨¡å‹ç±»å‹ï¼Œä¸åŒå¤§å°æˆ–è€…ç‰ˆæœ¬çš„`tokenizer.model`æ˜¯ç›¸åŒçš„ã€‚ä¾‹å¦‚ï¼ŒLLaMA-7B, LLaMA-13B, LLaMA-Plus-7Bçš„`tokenizer.model`ç›¸åŒã€‚\n\nThe followings are SHA256 values for `tokenizer.model` files. Note that `tokenizer.model` for LLaMA and Alpaca differ. However, different sizes or versions of LLaMA/Alpaca have the same `tokenizer.model`. For example, LLaMA-7B, LLaMA-13B, LLaMA-Plus-7B's `tokenizer.model` are the same.\n\n| Model Type            | SHA256                                                       |\n| --------------------- | ------------------------------------------------------------ |\n| LLaMA (7B, 13B, 33B)  | e2676d4ca29ca1750f6ff203328d73b189321dc5776ceede037cbd36541d70c0 |\n| Alpaca (7B, 13B, 33B) | 2d967e855b1213a439df6c8ce2791f869c84b4f3b6cfacf22b86440b8192a2f8 |\n\n#### LoRA weight file: adapter_model.bin \n\nä¸‹è¡¨å±•ç¤ºäº†LoRAä¸»ä½“æƒé‡æ–‡ä»¶`adapter_model.bin`çš„SHA256ã€‚\n\nThe followings are SHA256 values for `adapter_model.bin`  files.\n\n| LoRA Model (adapter_model.bin) | SHA256                                                       |\n| ------------------------------ | ------------------------------------------------------------ |\n| Chinese-LLaMA-7B               | 2a2c24d096f5d509f24946fdbd8c25e1ce4a0acb955902f7436d74c0c0379d86 |\n| Chinese-LLaMA-Plus-7B          | 8c928db86b2a0cf73f019832f921eb7e1e069ca21441b4bfa12c4381c6cc46be |\n| Chinese-LLaMA-13B              | 6a4ce789d219bde122f8d9a20371937f2aa2ee86a2311d9f5e303df2e774f9fc |\n| Chinese-LLaMA-Plus-13B         | 784fcff9c4bdf4e77d442a01158e121caf8fcce0f97ffb32396fe7a3617ee7e8 |\n| Chinese-LLaMA-33B              | 93a449bafb71ff1bb74a4a21e64e102e5078e5c3898eb40d013790072a0fa3de |\n| Chinese-LLaMA-Plus-33B         | 16f2544f4b5be9840dbb1a8071a9bc42627ed4232be3b0b600b43f7b4b5f08a7 |\n| Chinese-Alpaca-7B              | 0d9b6ed8e4a7d1ae590a16c89a452a488d66ff07e45487972f61c2b6e46e36de |\n| Chinese-Alpaca-Plus-7B         | 4ee0bf805c312a9a771624d481fbdb4485e1b0a70cd2a8da9f96137f177b795d |\n| Chinese-Alpaca-Pro-7B          | 3cd2776908c3f5efe68bf6cf0248cb0e80fb7c55a52b8406325c9f0ca37b8594 |\n| Chinese-Alpaca-13B             | cb8dda3c005f3343a0740dcd7237fbb600cb14b6bff9b6f3d488c086a2f08ada |\n| Chinese-Alpaca-Plus-13B        | a1fcdcb6d7e1068f925fb36ec78632c76058ba12ba352bed4d44060b8e6f4706 |\n| Chinese-Alpaca-Pro-13B         | f076b20fc2390ddbc35fd56d580d46ea834b33bbae34a4bb3cb7b571e60602e0 |\n| Chinese-Alpaca-33B             | 6b39da4c682e715a9de30b247b7e9b812d2d54f7d320ec9b452000a5cd4d178d |\n| Chinese-Alpaca-Plus-33B        | 411f5b9351abcc33c13a82bdd97ddcff81ad7993a8ddb83085b7ea97fad92fc7 |\n| Chinese-Alpaca-Pro-33B         | 0e7ba4951f605d2c0a7f0bcb983d7f6ed075c8dd23fbbcbc8a8c9643247212a3 |\n\n\n### Merged files (consolidated.*.pth)\n\nä¸‹è¡¨å±•ç¤ºäº†åˆå¹¶LoRAæƒé‡åçš„å…¨é‡æ¨¡å‹æƒé‡ï¼ˆPyTorchç‰ˆï¼‰çš„SHA256ã€‚PyTorchç‰ˆæœ¬ä¸å½±å“å®é™…æƒé‡æ•°æ®ï¼Œä½†å½±å“metaä¿¡æ¯ï¼Œæ‰€ä»¥SHA256ä¹Ÿä¼šä¸åŒã€‚**å»ºè®®åˆå¹¶æ¨¡å‹æ—¶ä½¿ç”¨PyTorch >= 1.13.0ç‰ˆæœ¬ï¼Œä»¥ç¡®ä¿ä»¥ä¸‹SHA256æœ‰å‚è€ƒæ€§ã€‚**\n\nâš ï¸ è¯·ä¼˜å…ˆç¡®ä¿åˆå¹¶å‰çš„åŸºæ¨¡å‹å’ŒLoRAæƒé‡çš„SHA256æ˜¯å¦ä¸ä¸Šè¿°è¡¨ä¸­æ‰€è¿°å€¼ä¸€è‡´ã€‚\n\nThe followings are SHA256 values for merged files (`consolidated.*.pth`). Note that the version of PyTorch does not affect actual weights but meta informations are slightly different. **Please check SHA256 according to PyTorch version >= 1.13.0.**\n\n| Model                   | SHA256 (PyTorch >= 1.13.0)                                   |\n| ----------------------- | ------------------------------------------------------------ |\n| Chinese-LLaMA-7B        | 245427a306e3253db3f534e2a1d7548a8eb781ae8761f9e98979b4aced6b43d8 |\n| Chinese-LLaMA-Plus-7B   | f8d380d63f77a08b7f447f5ec63f0bb1cde9ddeae2207e9f86e6b5f0f95a7955 |\n| Chinese-LLaMA-13B       | aa7f4599487ea2b0d0aca2b522c39370897f9afd9839aac7d02155957f1f019f<br/>3954f3e7f7264994f23800a04423e6563cc1959ac699d9eaaa6801b4f9392ebd |\n| Chinese-LLaMA-Plus-13B  | 4de7d188003c778f216342de2dc5c9a9c74278c701c63a7b6bcd7957f5ebfdf5<br/>ff8046f9eb8b05dd86597c21edd07894aec00b31842a4c11996a4003091ea7c9 |\n| Chinese-LLaMA-33B       | 054e9b7dffa3b92a053ca32acac6e22b27c184ed2b8563f8e44e6570ba416357<br/>a0fe86c45a0819f45a509776d82778b7de75fbff8d37afa97159b24de5448b7b<br/>13df5f74dc7bc1204076b1febef818fb3cec978de27bf8fc85c70e7d62282df9<br/>f4f28106c343c5804613faa9852f29fbc60764366bcb0d37ef2811a17be2d336 |\n| Chinese-LLaMA-Plus-33B  | 0da2fa92c054aa4010ffe1746afbc5bdd89c07b8e418d74debcbf0de74c00692<br/>35e25e7fe47f978d52da6cdae3272b3b4ea42a37ea8ece84a4d758350d188397<br/>c20f80a7da24b4be72a7f617311e3e2e9346a476d85b3f1a28095497bb1857b0<br/>ae5276fe06fd9d039ed295d627c0048d5d96c2390de413f610b093c151370d3c |\n| Chinese-Alpaca-7B       | fbfccc91183169842aac8d093379f0a449b5a26c5ee7a298baf0d556f1499b90 |\n| Chinese-Alpaca-Plus-7B  | 8b8f6551d0d83f93e378622b9f8dad0bec189da6c29d8a78de493e6aee9bd35f |\n| Chinese-Alpaca-Pro-7B   | TBA                                                          |\n| Chinese-Alpaca-13B      | 30cefb5be9091c3e17fbba5d91bf16266a2ddf86cde53370a9982b232ff8a2f4<br/>ce946742b0f122f472e192c3f77d506e0c26578b4b881d07d919553333affecd |\n| Chinese-Alpaca-Plus-13B | 1834558214c1dddc0d8b2826ece086908b9d2293241d0e12cecb48a035ec561b<br/>bf70001600ce166f6ca4ef59df5510f0582cdc119fb74e27d9cf3e4c7b142015 |\n| Chinese-Alpaca-Pro-13B  | TBA                                                          |\n| Chinese-Alpaca-33B      | 72bfe67481c0df1b8c3b536acd15ac42c1163b0727b1beb6409ee31d14cb2490<br/>fd2151ea714a6e0706a60cca5ab7abf8558e665d4cb001481c6df616c0821c16<br/>4a7e3de6881769f9c2413f0867e67da20efdf4502602ab90483cb99c593e51ed<br/>99c81a7a310802dcc579fe96288fbc18d4486f92020eaf925e1c33db8311378a |\n| Chinese-Alpaca-Plus-33B | TBA                                                          |\n| Chinese-Alpaca-Pro-33B  | TBA                                                          |\n\n\n### How To Check SHA256\n\nIn MacOS,\n\n```\n> shasum -a 256 your-model-file\n```\n\nIn Linux, \n\n```\n> sha256sum your-model-file\n```\n\nIn Windows,\n\n```\n> certutil -hashfile your-model-file sha256\n```\n\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1064453125,
          "content": "torch==1.13.1\ngit+https://github.com/huggingface/peft.git@13e53fc\ntransformers==4.30.0\nsentencepiece==0.1.97\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}