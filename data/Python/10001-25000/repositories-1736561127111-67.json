{
  "metadata": {
    "timestamp": 1736561127111,
    "page": 67,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Cinnamon/kotaemon",
      "stars": 19990,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".commitlintrc",
          "type": "blob",
          "size": 0.7734375,
          "content": "{\n  \"extends\": [\"@commitlint/config-conventional\"],\n  \"defaultIgnores\": true,\n  \"rules\": {\n    \"body-leading-blank\": [1, \"always\"],\n    \"body-max-line-length\": [2, \"always\", 100],\n    \"footer-leading-blank\": [1, \"always\"],\n    \"footer-max-line-length\": [2, \"always\", 10000],\n    \"header-max-length\": [2, \"always\", 200],\n    \"subject-case\": [\n      2,\n      \"never\",\n      []\n    ],\n    \"subject-empty\": [2, \"never\"],\n    \"subject-full-stop\": [2, \"never\", \".\"],\n    \"type-case\": [2, \"always\", \"lower-case\"],\n    \"type-empty\": [2, \"never\"],\n    \"type-enum\": [\n      2,\n      \"always\",\n      [\n        \"build\",\n        \"chore\",\n        \"ci\",\n        \"docs\",\n        \"feat\",\n        \"fix\",\n        \"perf\",\n        \"refactor\",\n        \"revert\",\n        \"style\",\n        \"test\"\n      ]\n    ]\n  }\n}\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.142578125,
          "content": ".github/\n.git/\n.mypy_cache/\n__pycache__/\nktem_app_data/\nenv/\n.pre-commit-config.yaml\n.commitlintrc\n.gitignore\n.gitattributes\nREADME.md\n*.zip\n*.sh\n"
        },
        {
          "name": ".env.example",
          "type": "blob",
          "size": 1.3876953125,
          "content": "# this is an example .env file, use it to create your own .env file and place it in the root of the project\n\n# settings for OpenAI\nOPENAI_API_BASE=https://api.openai.com/v1\nOPENAI_API_KEY=<YOUR_OPENAI_KEY>\nOPENAI_CHAT_MODEL=gpt-3.5-turbo\nOPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002\n\n# settings for Azure OpenAI\nAZURE_OPENAI_ENDPOINT=\nAZURE_OPENAI_API_KEY=\nOPENAI_API_VERSION=2024-02-15-preview\nAZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo\nAZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002\n\n# settings for Cohere\nCOHERE_API_KEY=<COHERE_API_KEY>\n\n# settings for local models\nLOCAL_MODEL=llama3.1:8b\nLOCAL_MODEL_EMBEDDINGS=nomic-embed-text\nLOCAL_EMBEDDING_MODEL_DIM = 768\nLOCAL_EMBEDDING_MODEL_MAX_TOKENS = 8192\n\n# settings for GraphRAG\nGRAPHRAG_API_KEY=<YOUR_OPENAI_KEY>\nGRAPHRAG_LLM_MODEL=gpt-4o-mini\nGRAPHRAG_EMBEDDING_MODEL=text-embedding-3-small\n\n# set to true if you want to use customized GraphRAG config file\nUSE_CUSTOMIZED_GRAPHRAG_SETTING=false\n\n# settings for Azure DI\nAZURE_DI_ENDPOINT=\nAZURE_DI_CREDENTIAL=\n\n# settings for Adobe API\n# get free credential at https://acrobatservices.adobe.com/dc-integration-creation-app-cdn/main.html?api=pdf-extract-api\n# also install pip install \"pdfservices-sdk@git+https://github.com/niallcm/pdfservices-python-sdk.git@bump-and-unfreeze-requirements\"\nPDF_SERVICES_CLIENT_ID=\nPDF_SERVICES_CLIENT_SECRET=\n\n# settings for PDF.js\nPDFJS_VERSION_DIST=\"pdfjs-4.0.379-dist\"\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.021484375,
          "content": "*.bat   text eol=crlf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 8.5224609375,
          "content": "# Created by https://www.toptal.com/developers/gitignore/api/python,linux,macos,windows,vim,emacs,visualstudiocode,pycharm\n# Edit at https://www.toptal.com/developers/gitignore?templates=python,linux,macos,windows,vim,emacs,visualstudiocode,pycharm\n\nactivate*\nactivate/*\nkotaemon-env*\n.env\n\n### Emacs ###\n# -*- mode: gitignore; -*-\n*~\n\\#*\\#\n/.emacs.desktop\n/.emacs.desktop.lock\n*.elc\nauto-save-list\ntramp\n.\\#*\n\n# Org-mode\n.org-id-locations\n*_archive\n\n# flymake-mode\n*_flymake.*\n\n# eshell files\n/eshell/history\n/eshell/lastdir\n\n# elpa packages\n/elpa/\n\n# reftex files\n*.rel\n\n# AUCTeX auto folder\n/auto/\n\n# cask packages\n.cask/\ndist/\n\n# Flycheck\nflycheck_*.el\n\n# server auth directory\n/server/\n\n# projectiles files\n.projectile\n\n# directory configuration\n.dir-locals.el\n\n# network security\n/network-security.data\n\n### Linux ###\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n.fuse_hidden*\n\n# KDE directory preferences\n.directory\n\n# Linux trash folder which might appear on any partition or disk\n.Trash-*\n\n# .nfs files are created when an open file is removed but is still being accessed\n.nfs*\n\n### macOS ###\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n### macOS Patch ###\n# iCloud generated files\n*.icloud\n\n### PyCharm ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/**/usage.statistics.xml\n.idea/**/dictionaries\n.idea/**/shelf\n\n# AWS User-specific\n.idea/**/aws.xml\n\n# Generated files\n.idea/**/contentModel.xml\n\n# Sensitive or high-churn files\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n.idea/**/dbnavigator.xml\n\n# Gradle\n.idea/**/gradle.xml\n.idea/**/libraries\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\ncmake-build-*/\n\n# Mongo Explorer plugin\n.idea/**/mongoSettings.xml\n\n# File-based project format\n*.iws\n\n# IntelliJ\nout/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n.idea/replstate.xml\n\n# SonarLint plugin\n.idea/sonarlint/\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n# Editor-based Rest Client\n.idea/httpRequests\n\n# Android studio 3.1+ serialized cache file\n.idea/caches/build_file_checksums.ser\n\n### PyCharm Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n.idea/**/sonarlint/\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n.idea/**/sonarIssues.xml\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n.idea/**/markdown-navigator.xml\n.idea/**/markdown-navigator-enh.xml\n.idea/**/markdown-navigator/\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n.idea/$CACHE_FILE$\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n.idea/codestream.xml\n\n# Azure Toolkit for IntelliJ plugin\n# https://plugins.jetbrains.com/plugin/8053-azure-toolkit-for-intellij\n.idea/**/azureSettings.xml\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n### Python Patch ###\n# Poetry local configuration file - https://python-poetry.org/docs/configuration/#local-configuration\npoetry.toml\n\n# ruff\n.ruff_cache/\n\n# LSP config files\npyrightconfig.json\n\n### Vim ###\n# Swap\n[._]*.s[a-v][a-z]\n!*.svg # comment out if you don't need vector files\n[._]*.sw[a-p]\n[._]s[a-rt-v][a-z]\n[._]ss[a-gi-z]\n[._]sw[a-p]\n\n# Session\nSession.vim\nSessionx.vim\n\n# Temporary\n.netrwhist\n# Auto-generated tag files\ntags\n# Persistent undo\n[._]*.un~\n\n### VisualStudioCode ###\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n!.vscode/*.code-snippets\n\n# Local History for Visual Studio Code\n.history/\n\n# Built Visual Studio Code Extensions\n*.vsix\n\n### VisualStudioCode Patch ###\n# Ignore all local history of files\n.history\n.ionide\n\n### Windows ###\n# Windows thumbnail cache files\nThumbs.db\nThumbs.db:encryptable\nehthumbs.db\nehthumbs_vista.db\n\n# Dump file\n*.stackdump\n\n# Folder config file\n[Dd]esktop.ini\n\n# Recycle Bin used on file shares\n$RECYCLE.BIN/\n\n# Windows Installer files\n*.cab\n*.msi\n*.msix\n*.msm\n*.msp\n\n# Windows shortcuts\n*.lnk\n\n# PDF files\n*.pdf\n!libs/kotaemon/tests/resources/*.pdf\n\n.theflow/\n\n# End of https://www.toptal.com/developers/gitignore/api/python,linux,macos,windows,vim,emacs,visualstudiocode,pycharm\n*.py[coid]\n\nlogs/\n.gitsecret/keys/random_seed\n!*.secret\n.envrc\n.env\n\nS.gpg-agent*\n.vscode/settings.json\nexamples/example1/assets\nstorage/*\n\n# Conda and env storages\n*install_dir/\ndoc_env/\n\n# application data\nktem_app_data/\ngradio_tmp/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.869140625,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n      - id: check-yaml\n        args: [\"--unsafe\"]\n      - id: check-toml\n      - id: end-of-file-fixer\n      - id: trailing-whitespace\n      - id: mixed-line-ending\n      - id: detect-aws-credentials\n        args: [\"--allow-missing-credentials\"]\n      - id: detect-private-key\n      - id: check-added-large-files\n        args: [\"--maxkb=750\"]\n      - id: debug-statements\n  - repo: https://github.com/ambv/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n        language_version: python3\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        args: [\"--profile\", \"black\"]\n        language_version: python3.10\n  - repo: https://github.com/pycqa/flake8\n    rev: 4.0.1\n    hooks:\n      - id: flake8\n        args: [\"--max-line-length\", \"88\", \"--extend-ignore\", \"E203\"]\n  - repo: https://github.com/myint/autoflake\n    rev: v1.4\n    hooks:\n      - id: autoflake\n        args:\n          [\n            \"--in-place\",\n            \"--remove-unused-variables\",\n            \"--remove-all-unused-imports\",\n            \"--ignore-init-module-imports\",\n            \"--exclude=tests/*\",\n          ]\n  - repo: https://github.com/pre-commit/mirrors-prettier\n    rev: v2.7.1\n    hooks:\n      - id: prettier\n        types_or: [markdown, yaml]\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: \"v1.7.1\"\n    hooks:\n      - id: mypy\n        additional_dependencies:\n          [\n            types-PyYAML==6.0.12.11,\n            \"types-requests\",\n            \"sqlmodel\",\n            \"types-Markdown\",\n            types-tzlocal,\n          ]\n        args: [\"--check-untyped-defs\", \"--ignore-missing-imports\"]\n        exclude: \"^templates/\"\n  - repo: https://github.com/codespell-project/codespell\n    rev: v2.2.4\n    hooks:\n      - id: codespell\n        additional_dependencies:\n          - tomli\n"
        },
        {
          "name": ".python-version",
          "type": "blob",
          "size": 0.0048828125,
          "content": "3.10\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.0791015625,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n- Demonstrating empathy and kindness toward other people\n- Being respectful of differing opinions, viewpoints, and experiences\n- Giving and gracefully accepting constructive feedback\n- Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n- Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n- The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n- Trolling, insulting or derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5.83203125,
          "content": "# Contributing to Kotaemon\n\nWelcome üëã to the Kotaemon project! We're thrilled that you're interested in contributing. Whether you're fixing bugs, adding new features, or improving documentation, your efforts are highly appreciated. This guide aims to help you get started with contributing to Kotaemon.\n\n<a href=\"https://github.com/Cinnamon/kotaemon/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=Cinnamon/kotaemon\" />\n</a>\n\n### Table of Contents\n\n1. [üìñ Code of Conduct](#code-of-conduct)\n2. [üîÅ Contributing via Pull Requests](#contributing-via-pull-requests)\n3. [üì• Opening an Issue](#-opening-an-issue)\n4. [üìù Commit Messages](#-commit-messages)\n5. [üßæ License](#-license)\n\n## üìñ Code of Conduct\n\nPlease review our [code of conduct](./CODE_OF_CONDUCT.md), which is in effect at all times. We expect everyone who contributes to this project to honor it.\n\n## üîÅ Contributing via Pull Requests\n\n1. [**Fork the repository**](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo): Click on the [Fork](https://github.com/Cinnamon/kotaemon/fork) button on the repository's page to create a copy of Kotaemon under your GitHub account.\n\n2. [**Clone your code**](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository): Clone your forked repository to your local machine.\n\n3. [**Create new branch**](https://docs.github.com/en/desktop/making-changes-in-a-branch/managing-branches-in-github-desktop): Create a new branch in your forked repo with a descriptive name that reflects your changes.\n\n```sh\ngit checkout -b descriptive-name-for-your-changes\n```\n\n4. **Setup the development environment**: If you are working on the code, make sure to install the necessary dependencies for development\n\n```sh\npip install -e \"libs/kotaemon[dev]\n```\n\n5. **Make your changes**: Ensure your code follows the project's coding style and passes all test cases.\n\n   - Check the coding style\n\n   ```sh\n   pre-commit run --all-files\n   ```\n\n   - Run the tests\n\n   ```sh\n   pytest libs/kotaemon/tests/\n   ```\n\n6. [**Commit your changes**](https://docs.github.com/en/desktop/making-changes-in-a-branch/committing-and-reviewing-changes-to-your-project-in-github-desktop): Once you are done with your changes, add and commit them with clear messages.\n\n```sh\ngit add your_changes.py\ngit commit -m \"clear message described your changes.\"\ngit push -u origin descriptive-name-for-your-changes\n```\n\n7. [**Create a pull request**](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request): When you are satisfied with your changes, [submit a pull request](https://github.com/Cinnamon/kotaemon/compare) from your forked repository to Kotaemon repository. In the pull request, provide a clear description of your changes and any related issues. For the title of the pull request, please refer to our [commit messages convention](#-commit-messages).\n\n8. **Wait for reviews**: Wait for the maintainers to review your pull request. If everything is okay, your changes will be merged into the Kotaemon project.\n\n### GitHub Actions CI Tests\n\nAll pull requests must pass the [GitHub Actions Continuous Integration (CI)](https://docs.github.com/en/actions/about-github-actions/about-continuous-integration-with-github-actions) tests before they can be merged. These tests include coding-style checks, PR title validation, unit tests, etc. to ensure that your changes meet the project's quality standards. Please review and fix any CI failures that arise.\n\n## üì• Opening an Issue\n\nBefore [creating an issues](https://github.com/Cinnamon/kotaemon/issues/new/choose), search through existing issues to ensure you are not opening a duplicate. If you are reporting a bug or issue, please provide a reproducible example to help us quickly identify the problem.\n\n## üìù Commit Messages\n\n### Overview\n\nWe use [Angular convention](https://www.conventionalcommits.org/en/) for commit messages to maintain consistency and clarity in our project history. Please take a moment to familiarize yourself with this convention before making your first commit.\n\n_For the sake of simplicity, we use [squashing merge](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/incorporating-changes-from-a-pull-request/about-pull-request-merges#squash-and-merge-your-commits) with pull requests. Therefore, if you contribute via a pull request, just make sure your PR's title, instead of the whole commits, follows this convention._\n\nCommit format:\n\n```sh\n<gitmoji> <type>(<scope>): <subject>\n<BLANK LINE>\n<body>\n<BLANK LINE>\n<footer>\n```\n\nExamples:\n\n```sh\ndocs(api): update api doc\n```\n\n### Commit types\n\n| Types      | Description                                                   |\n| :--------- | :------------------------------------------------------------ |\n| `feat`     | New features                                                  |\n| `fix`      | Bug fix                                                       |\n| `docs`     | Documentation only changes                                    |\n| `build`    | Changes that affect the build system or external dependencies |\n| `chore`    | Something that doesn‚Äôt fit the other types                    |\n| `ci`       | Changes to our CI configuration files and scripts             |\n| `perf`     | Improve performance                                           |\n| `refactor` | Refactor code                                                 |\n| `revert`   | Revert a previous commit                                      |\n| `style`    | Improve structure/format of the code                          |\n| `test`     | Add, update or pass tests                                     |\n\n## üßæ License\n\nAll contributions will be licensed under the project's license: [Apache License 2.0](https://github.com/Cinnamon/kotaemon/blob/main/LICENSE.txt).\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.7177734375,
          "content": "# Lite version\nFROM python:3.10-slim AS lite\n\n# Common dependencies\nRUN apt-get update -qqy && \\\n    apt-get install -y --no-install-recommends \\\n        ssh \\\n        git \\\n        gcc \\\n        g++ \\\n        poppler-utils \\\n        libpoppler-dev \\\n        unzip \\\n        curl \\\n        cargo\n\n# Setup args\nARG TARGETPLATFORM\nARG TARGETARCH\n\n# Set environment variables\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nENV PYTHONIOENCODING=UTF-8\nENV TARGETARCH=${TARGETARCH}\n\n# Create working directory\nWORKDIR /app\n\n# Download pdfjs\nCOPY scripts/download_pdfjs.sh /app/scripts/download_pdfjs.sh\nRUN chmod +x /app/scripts/download_pdfjs.sh\nENV PDFJS_PREBUILT_DIR=\"/app/libs/ktem/ktem/assets/prebuilt/pdfjs-dist\"\nRUN bash scripts/download_pdfjs.sh $PDFJS_PREBUILT_DIR\n\n# Copy contents\nCOPY . /app\nCOPY .env.example /app/.env\n\n# Install pip packages\nRUN --mount=type=ssh  \\\n    --mount=type=cache,target=/root/.cache/pip  \\\n    pip install -e \"libs/kotaemon\" \\\n    && pip install -e \"libs/ktem\" \\\n    && pip install \"pdfservices-sdk@git+https://github.com/niallcm/pdfservices-python-sdk.git@bump-and-unfreeze-requirements\"\n\nRUN --mount=type=ssh  \\\n    --mount=type=cache,target=/root/.cache/pip  \\\n    if [ \"$TARGETARCH\" = \"amd64\" ]; then pip install \"graphrag<=0.3.6\" future; fi\n\n# Clean up\nRUN apt-get autoremove \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && rm -rf ~/.cache\n\nCMD [\"python\", \"app.py\"]\n\n# Full version\nFROM lite AS full\n\n# Additional dependencies for full version\nRUN apt-get update -qqy && \\\n    apt-get install -y --no-install-recommends \\\n        tesseract-ocr \\\n        tesseract-ocr-jpn \\\n        libsm6 \\\n        libxext6 \\\n        libreoffice \\\n        ffmpeg \\\n        libmagic-dev\n\n# Install torch and torchvision for unstructured\nRUN --mount=type=ssh  \\\n    --mount=type=cache,target=/root/.cache/pip  \\\n    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# Install additional pip packages\nRUN --mount=type=ssh  \\\n    --mount=type=cache,target=/root/.cache/pip  \\\n    pip install -e \"libs/kotaemon[adv]\" \\\n    && pip install unstructured[all-docs]\n\n# Install lightRAG\nENV USE_LIGHTRAG=true\nRUN --mount=type=ssh  \\\n    --mount=type=cache,target=/root/.cache/pip  \\\n    pip install aioboto3 nano-vectordb ollama xxhash \"lightrag-hku<=0.0.8\"\n\nRUN --mount=type=ssh  \\\n    --mount=type=cache,target=/root/.cache/pip  \\\n    pip install \"docling<=2.5.2\"\n\n# Clean up\nRUN apt-get autoremove \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && rm -rf ~/.cache\n\n# Download nltk packages as required for unstructured\n# RUN python -c \"from unstructured.nlp.tokenize import _download_nltk_packages_if_not_present; _download_nltk_packages_if_not_present()\"\n\nCMD [\"python\", \"app.py\"]\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.203125,
          "content": "<div align=\"center\">\n\n# kotaemon\n\nAn open-source clean & customizable RAG UI for chatting with your documents. Built with both end users and\ndevelopers in mind.\n\n![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png)\n\n<a href=\"https://trendshift.io/repositories/11607\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11607\" alt=\"Cinnamon%2Fkotaemon | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n[Live Demo](https://huggingface.co/spaces/cin-model/kotaemon-demo) |\n[Online Install](https://cinnamon.github.io/kotaemon/online_install/) |\n[User Guide](https://cinnamon.github.io/kotaemon/) |\n[Developer Guide](https://cinnamon.github.io/kotaemon/development/) |\n[Feedback](https://github.com/Cinnamon/kotaemon/issues) |\n[Contact](mailto:kotaemon.support@cinnamon.is)\n\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-31013/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n<a href=\"https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon\" target=\"_blank\">\n<img src=\"https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen\" alt=\"docker pull ghcr.io/cinnamon/kotaemon:latest\"></a>\n![download](https://img.shields.io/github/downloads/Cinnamon/kotaemon/total.svg?label=downloads&color=blue)\n<a href='https://huggingface.co/spaces/cin-model/kotaemon-demo'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a>\n<a href=\"https://hellogithub.com/en/repository/d3141471a0244d5798bc654982b263eb\" target=\"_blank\"><img src=\"https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d3141471a0244d5798bc654982b263eb&claim_uid=RLiD9UZ1rEHNaMf&theme=small\" alt=\"FeaturedÔΩúHelloGitHub\" /></a>\n\n</div>\n\n<!-- start-intro -->\n\n## Introduction\n\nThis project serves as a functional RAG UI for both end users who want to do QA on their\ndocuments and developers who want to build their own RAG pipeline.\n<br>\n\n```yml\n+----------------------------------------------------------------------------+\n| End users: Those who use apps built with `kotaemon`.                       |\n| (You use an app like the one in the demo above)                            |\n|     +----------------------------------------------------------------+     |\n|     | Developers: Those who built with `kotaemon`.                   |     |\n|     | (You have `import kotaemon` somewhere in your project)         |     |\n|     |     +----------------------------------------------------+     |     |\n|     |     | Contributors: Those who make `kotaemon` better.    |     |     |\n|     |     | (You make PR to this repo)                         |     |     |\n|     |     +----------------------------------------------------+     |     |\n|     +----------------------------------------------------------------+     |\n+----------------------------------------------------------------------------+\n```\n\n### For end users\n\n- **Clean & Minimalistic UI**: A user-friendly interface for RAG-based QA.\n- **Support for Various LLMs**: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via `ollama` and `llama-cpp-python`).\n- **Easy Installation**: Simple scripts to get you started quickly.\n\n### For developers\n\n- **Framework for RAG Pipelines**: Tools to build your own RAG-based document QA pipeline.\n- **Customizable UI**: See your RAG pipeline in action with the provided UI, built with <a href='https://github.com/gradio-app/gradio'>Gradio <img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>.\n- **Gradio Theme**: If you use Gradio for development, check out our theme here: [kotaemon-gradio-theme](https://github.com/lone17/kotaemon-gradio-theme).\n\n## Key Features\n\n- **Host your own document QA (RAG) web-UI**: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.\n\n- **Organize your LLM & Embedding models**: Support both local LLMs & popular API providers (OpenAI, Azure, Ollama, Groq).\n\n- **Hybrid RAG pipeline**: Sane default RAG pipeline with hybrid (full-text & vector) retriever and re-ranking to ensure best retrieval quality.\n\n- **Multi-modal QA support**: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI).\n\n- **Advanced citations with document preview**: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the _in-browser PDF viewer_ with highlights. Warning when retrieval pipeline return low relevant articles.\n\n- **Support complex reasoning methods**: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with `ReAct`, `ReWOO` and other agents.\n\n- **Configurable settings UI**: You can adjust most important aspects of retrieval & generation process on the UI (incl. prompts).\n\n- **Extensible**: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing & retrieval. `GraphRAG` indexing pipeline is provided as an example.\n\n![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview.png)\n\n## Installation\n\n> If you are not a developer and just want to use the app, please check out our easy-to-follow [User Guide](https://cinnamon.github.io/kotaemon/). Download the `.zip` file from the [latest release](https://github.com/Cinnamon/kotaemon/releases/latest) to get all the newest features and bug fixes.\n\n### System requirements\n\n1. [Python](https://www.python.org/downloads/) >= 3.10\n2. [Docker](https://www.docker.com/): optional, if you [install with Docker](#with-docker-recommended)\n3. [Unstructured](https://docs.unstructured.io/open-source/installation/full-installation#full-installation) if you want to process files other than `.pdf`, `.html`, `.mhtml`, and `.xlsx` documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there.\n\n### With Docker (recommended)\n\n1. We support both `lite` & `full` version of Docker images. With `full`, the extra packages of `unstructured` will be installed as well, it can support additional file types (`.doc`, `.docx`, ...) but the cost is larger docker image size. For most users, the `lite` image should work well in most cases.\n\n   - To use the `lite` version.\n\n     ```bash\n     docker run \\\n     -e GRADIO_SERVER_NAME=0.0.0.0 \\\n     -e GRADIO_SERVER_PORT=7860 \\\n     -p 7860:7860 -it --rm \\\n     ghcr.io/cinnamon/kotaemon:main-lite\n     ```\n\n   - To use the `full` version.\n\n     ```bash\n     docker run \\\n     -e GRADIO_SERVER_NAME=0.0.0.0 \\\n     -e GRADIO_SERVER_PORT=7860 \\\n     -p 7860:7860 -it --rm \\\n     ghcr.io/cinnamon/kotaemon:main-full\n     ```\n\n2. We currently support and test two platforms: `linux/amd64` and `linux/arm64` (for newer Mac). You can specify the platform by passing `--platform` in the `docker run` command. For example:\n\n   ```bash\n   # To run docker with platform linux/arm64\n   docker run \\\n   -e GRADIO_SERVER_NAME=0.0.0.0 \\\n   -e GRADIO_SERVER_PORT=7860 \\\n   -p 7860:7860 -it --rm \\\n   --platform linux/arm64 \\\n   ghcr.io/cinnamon/kotaemon:main-lite\n   ```\n\n3. Once everything is set up correctly, you can go to `http://localhost:7860/` to access the WebUI.\n\n4. We use [GHCR](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) to store docker images, all images can be found [here.](https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon)\n\n### Without Docker\n\n1. Clone and install required packages on a fresh python environment.\n\n   ```shell\n   # optional (setup env)\n   conda create -n kotaemon python=3.10\n   conda activate kotaemon\n\n   # clone this repo\n   git clone https://github.com/Cinnamon/kotaemon\n   cd kotaemon\n\n   pip install -e \"libs/kotaemon[all]\"\n   pip install -e \"libs/ktem\"\n   ```\n\n2. Create a `.env` file in the root of this project. Use `.env.example` as a template\n\n   The `.env` file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs.\n\n3. (Optional) To enable in-browser `PDF_JS` viewer, download [PDF_JS_DIST](https://github.com/mozilla/pdf.js/releases/download/v4.0.379/pdfjs-4.0.379-dist.zip) then extract it to `libs/ktem/ktem/assets/prebuilt`\n\n<img src=\"https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/pdf-viewer-setup.png\" alt=\"pdf-setup\" width=\"300\">\n\n4. Start the web server:\n\n   ```shell\n   python app.py\n   ```\n\n   - The app will be automatically launched in your browser.\n   - Default username and password are both `admin`. You can set up additional users directly through the UI.\n\n   ![Chat tab](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/chat-tab.png)\n\n5. Check the `Resources` tab and `LLMs and Embeddings` and ensure that your `api_key` value is set correctly from your `.env` file. If it is not set, you can set it there.\n\n### Setup GraphRAG\n\n> [!NOTE]\n> Official MS GraphRAG indexing only works with OpenAI or Ollama API.\n> We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon.\n\n<details>\n\n<summary>Setup Nano GRAPHRAG</summary>\n\n- Install nano-GraphRAG: `pip install nano-graphrag`\n- `nano-graphrag` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)\n  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib && pip install chroma-hnswlib`\n- Launch Kotaemon with `USE_NANO_GRAPHRAG=true` environment variable.\n- Set your default LLM & Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG.\n\n</details>\n\n<details>\n\n<summary>Setup LIGHTRAG</summary>\n\n- Install LightRAG: `pip install git+https://github.com/HKUDS/LightRAG.git`\n- `LightRAG` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)\n  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib && pip install chroma-hnswlib`\n- Launch Kotaemon with `USE_LIGHTRAG=true` environment variable.\n- Set your default LLM & Embedding models in Resources setting and it will be recognized automatically from LightRAG.\n\n</details>\n\n<details>\n\n<summary>Setup MS GRAPHRAG</summary>\n\n- **Non-Docker Installation**: If you are not using Docker, install GraphRAG with the following command:\n\n  ```shell\n  pip install \"graphrag<=0.3.6\" future\n  ```\n\n- **Setting Up API KEY**: To use the GraphRAG retriever feature, ensure you set the `GRAPHRAG_API_KEY` environment variable. You can do this directly in your environment or by adding it to a `.env` file.\n- **Using Local Models and Custom Settings**: If you want to use GraphRAG with local models (like `Ollama`) or customize the default LLM and other configurations, set the `USE_CUSTOMIZED_GRAPHRAG_SETTING` environment variable to true. Then, adjust your settings in the `settings.yaml.example` file.\n\n</details>\n\n### Setup Local Models (for local/private RAG)\n\nSee [Local model setup](docs/local_model.md).\n\n### Setup multimodal document parsing (OCR, table parsing, figure extraction)\n\nThese options are available:\n\n- [Azure Document Intelligence (API)](https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence)\n- [Adobe PDF Extract (API)](https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/)\n- [Docling (local, open-source)](https://github.com/DS4SD/docling)\n  - To use Docling, first install required dependencies: `pip install docling`\n\nSelect corresponding loaders in `Settings -> Retrieval Settings -> File loader`\n\n### Customize your application\n\n- By default, all application data is stored in the `./ktem_app_data` folder. You can back up or copy this folder to transfer your installation to a new machine.\n\n- For advanced users or specific use cases, you can customize these files:\n\n  - `flowsettings.py`\n  - `.env`\n\n#### `flowsettings.py`\n\nThis file contains the configuration of your application. You can use the example\n[here](flowsettings.py) as the starting point.\n\n<details>\n\n<summary>Notable settings</summary>\n\n```python\n# setup your preferred document store (with full-text search capabilities)\nKH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore)\n\n# setup your preferred vectorstore (for vector-based search)\nKH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Qdrant)\n\n# Enable / disable multimodal QA\nKH_REASONINGS_USE_MULTIMODAL=True\n\n# Setup your new reasoning pipeline or modify existing one.\nKH_REASONINGS = [\n    \"ktem.reasoning.simple.FullQAPipeline\",\n    \"ktem.reasoning.simple.FullDecomposeQAPipeline\",\n    \"ktem.reasoning.react.ReactAgentPipeline\",\n    \"ktem.reasoning.rewoo.RewooAgentPipeline\",\n]\n```\n\n</details>\n\n#### `.env`\n\nThis file provides another way to configure your models and credentials.\n\n<details>\n\n<summary>Configure model via the .env file</summary>\n\n- Alternatively, you can configure the models via the `.env` file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don't see it, you can create one.\n\n- Currently, the following providers are supported:\n\n  - **OpenAI**\n\n    In the `.env` file, set the `OPENAI_API_KEY` variable with your OpenAI API key in order\n    to enable access to OpenAI's models. There are other variables that can be modified,\n    please feel free to edit them to fit your case. Otherwise, the default parameter should\n    work for most people.\n\n    ```shell\n    OPENAI_API_BASE=https://api.openai.com/v1\n    OPENAI_API_KEY=<your OpenAI API key here>\n    OPENAI_CHAT_MODEL=gpt-3.5-turbo\n    OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002\n    ```\n\n  - **Azure OpenAI**\n\n    For OpenAI models via Azure platform, you need to provide your Azure endpoint and API\n    key. Your might also need to provide your developments' name for the chat model and the\n    embedding model depending on how you set up Azure development.\n\n    ```shell\n    AZURE_OPENAI_ENDPOINT=\n    AZURE_OPENAI_API_KEY=\n    OPENAI_API_VERSION=2024-02-15-preview\n    AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo\n    AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002\n    ```\n\n  - **Local Models**\n\n    - Using `ollama` OpenAI compatible server:\n\n      - Install [ollama](https://github.com/ollama/ollama) and start the application.\n\n      - Pull your model, for example:\n\n        ```shell\n        ollama pull llama3.1:8b\n        ollama pull nomic-embed-text\n        ```\n\n      - Set the model names on web UI and make it as default:\n\n        ![Models](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/models.png)\n\n    - Using `GGUF` with `llama-cpp-python`\n\n      You can search and download a LLM to be ran locally from the [Hugging Face Hub](https://huggingface.co/models). Currently, these model formats are supported:\n\n      - GGUF\n\n        You should choose a model whose size is less than your device's memory and should leave\n        about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available,\n        then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to\n        give better generation but also take more processing time.\n\n        Here are some recommendations and their size in memory:\n\n      - [Qwen1.5-1.8B-Chat-GGUF](https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true): around 2 GB\n\n        Add a new LlamaCpp model with the provided model name on the web UI.\n\n  </details>\n\n### Adding your own RAG pipeline\n\n#### Custom Reasoning Pipeline\n\n1. Check the default pipeline implementation in [here](libs/ktem/ktem/reasoning/simple.py). You can make quick adjustment to how the default QA pipeline work.\n2. Add new `.py` implementation in `libs/ktem/ktem/reasoning/` and later include it in `flowssettings` to enable it on the UI.\n\n#### Custom Indexing Pipeline\n\n- Check sample implementation in `libs/ktem/ktem/index/file/graph`\n\n> (more instruction WIP).\n\n<!-- end-intro -->\n\n## Citation\n\nPlease cite this project as\n\n```BibTeX\n@misc{kotaemon2024,\n    title = {Kotaemon - An open-source RAG-based tool for chatting with any content.},\n    author = {The Kotaemon Team},\n    year = {2024},\n    howpublished = {\\url{https://github.com/Cinnamon/kotaemon}},\n}\n```\n\n## Star History\n\n<a href=\"https://star-history.com/#Cinnamon/kotaemon&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Cinnamon/kotaemon&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Cinnamon/kotaemon&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Cinnamon/kotaemon&type=Date\" />\n </picture>\n</a>\n\n## Contribution\n\nSince our project is actively being developed, we greatly value your feedback and contributions. Please see our [Contributing Guide](https://github.com/Cinnamon/kotaemon/blob/main/CONTRIBUTING.md) to get started. Thank you to all our contributors!\n\n<a href=\"https://github.com/Cinnamon/kotaemon/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=Cinnamon/kotaemon\" />\n</a>\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 0.583984375,
          "content": "import os\n\nfrom theflow.settings import settings as flowsettings\n\nKH_APP_DATA_DIR = getattr(flowsettings, \"KH_APP_DATA_DIR\", \".\")\nGRADIO_TEMP_DIR = os.getenv(\"GRADIO_TEMP_DIR\", None)\n# override GRADIO_TEMP_DIR if it's not set\nif GRADIO_TEMP_DIR is None:\n    GRADIO_TEMP_DIR = os.path.join(KH_APP_DATA_DIR, \"gradio_tmp\")\n    os.environ[\"GRADIO_TEMP_DIR\"] = GRADIO_TEMP_DIR\n\n\nfrom ktem.main import App  # noqa\n\napp = App()\ndemo = app.make()\ndemo.queue().launch(\n    favicon_path=app._favicon,\n    inbrowser=True,\n    allowed_paths=[\n        \"libs/ktem/ktem/assets\",\n        GRADIO_TEMP_DIR,\n    ],\n)\n"
        },
        {
          "name": "doc_env_reqs.txt",
          "type": "blob",
          "size": 0.1982421875,
          "content": "mkdocs\nmkdocstrings[python]\nmkdocs-material\nmkdocs-gen-files\nmkdocs-literate-nav\nmkdocs-git-revision-date-localized-plugin\nmkdocs-section-index\nmkdocs-include-markdown-plugin[cache]\nmdx_truly_sane_lists\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "flowsettings.py",
          "type": "blob",
          "size": 11.0751953125,
          "content": "import os\nfrom importlib.metadata import version\nfrom inspect import currentframe, getframeinfo\nfrom pathlib import Path\n\nfrom decouple import config\nfrom ktem.utils.lang import SUPPORTED_LANGUAGE_MAP\nfrom theflow.settings.default import *  # noqa\n\ncur_frame = currentframe()\nif cur_frame is None:\n    raise ValueError(\"Cannot get the current frame.\")\nthis_file = getframeinfo(cur_frame).filename\nthis_dir = Path(this_file).parent\n\n# change this if your app use a different name\nKH_PACKAGE_NAME = \"kotaemon_app\"\n\nKH_APP_VERSION = config(\"KH_APP_VERSION\", None)\nif not KH_APP_VERSION:\n    try:\n        # Caution: This might produce the wrong version\n        # https://stackoverflow.com/a/59533071\n        KH_APP_VERSION = version(KH_PACKAGE_NAME)\n    except Exception:\n        KH_APP_VERSION = \"local\"\n\nKH_ENABLE_FIRST_SETUP = True\nKH_DEMO_MODE = config(\"KH_DEMO_MODE\", default=False, cast=bool)\nKH_OLLAMA_URL = config(\"KH_OLLAMA_URL\", default=\"http://localhost:11434/v1/\")\n\n# App can be ran from anywhere and it's not trivial to decide where to store app data.\n# So let's use the same directory as the flowsetting.py file.\nKH_APP_DATA_DIR = this_dir / \"ktem_app_data\"\nKH_APP_DATA_EXISTS = KH_APP_DATA_DIR.exists()\nKH_APP_DATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# User data directory\nKH_USER_DATA_DIR = KH_APP_DATA_DIR / \"user_data\"\nKH_USER_DATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# markdown output directory\nKH_MARKDOWN_OUTPUT_DIR = KH_APP_DATA_DIR / \"markdown_cache_dir\"\nKH_MARKDOWN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# chunks output directory\nKH_CHUNKS_OUTPUT_DIR = KH_APP_DATA_DIR / \"chunks_cache_dir\"\nKH_CHUNKS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# zip output directory\nKH_ZIP_OUTPUT_DIR = KH_APP_DATA_DIR / \"zip_cache_dir\"\nKH_ZIP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# zip input directory\nKH_ZIP_INPUT_DIR = KH_APP_DATA_DIR / \"zip_cache_dir_in\"\nKH_ZIP_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# HF models can be big, let's store them in the app data directory so that it's easier\n# for users to manage their storage.\n# ref: https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache\nos.environ[\"HF_HOME\"] = str(KH_APP_DATA_DIR / \"huggingface\")\nos.environ[\"HF_HUB_CACHE\"] = str(KH_APP_DATA_DIR / \"huggingface\")\n\n# doc directory\nKH_DOC_DIR = this_dir / \"docs\"\n\nKH_MODE = \"dev\"\nKH_FEATURE_CHAT_SUGGESTION = config(\n    \"KH_FEATURE_CHAT_SUGGESTION\", default=False, cast=bool\n)\nKH_FEATURE_USER_MANAGEMENT = config(\n    \"KH_FEATURE_USER_MANAGEMENT\", default=True, cast=bool\n)\nKH_USER_CAN_SEE_PUBLIC = None\nKH_FEATURE_USER_MANAGEMENT_ADMIN = str(\n    config(\"KH_FEATURE_USER_MANAGEMENT_ADMIN\", default=\"admin\")\n)\nKH_FEATURE_USER_MANAGEMENT_PASSWORD = str(\n    config(\"KH_FEATURE_USER_MANAGEMENT_PASSWORD\", default=\"admin\")\n)\nKH_ENABLE_ALEMBIC = False\nKH_DATABASE = f\"sqlite:///{KH_USER_DATA_DIR / 'sql.db'}\"\nKH_FILESTORAGE_PATH = str(KH_USER_DATA_DIR / \"files\")\nKH_WEB_SEARCH_BACKEND = (\n    \"kotaemon.indices.retrievers.tavily_web_search.WebSearch\"\n    # \"kotaemon.indices.retrievers.jina_web_search.WebSearch\"\n)\n\nKH_DOCSTORE = {\n    # \"__type__\": \"kotaemon.storages.ElasticsearchDocumentStore\",\n    # \"__type__\": \"kotaemon.storages.SimpleFileDocumentStore\",\n    \"__type__\": \"kotaemon.storages.LanceDBDocumentStore\",\n    \"path\": str(KH_USER_DATA_DIR / \"docstore\"),\n}\nKH_VECTORSTORE = {\n    # \"__type__\": \"kotaemon.storages.LanceDBVectorStore\",\n    \"__type__\": \"kotaemon.storages.ChromaVectorStore\",\n    # \"__type__\": \"kotaemon.storages.MilvusVectorStore\",\n    # \"__type__\": \"kotaemon.storages.QdrantVectorStore\",\n    \"path\": str(KH_USER_DATA_DIR / \"vectorstore\"),\n}\nKH_LLMS = {}\nKH_EMBEDDINGS = {}\nKH_RERANKINGS = {}\n\n# populate options from config\nif config(\"AZURE_OPENAI_API_KEY\", default=\"\") and config(\n    \"AZURE_OPENAI_ENDPOINT\", default=\"\"\n):\n    if config(\"AZURE_OPENAI_CHAT_DEPLOYMENT\", default=\"\"):\n        KH_LLMS[\"azure\"] = {\n            \"spec\": {\n                \"__type__\": \"kotaemon.llms.AzureChatOpenAI\",\n                \"temperature\": 0,\n                \"azure_endpoint\": config(\"AZURE_OPENAI_ENDPOINT\", default=\"\"),\n                \"api_key\": config(\"AZURE_OPENAI_API_KEY\", default=\"\"),\n                \"api_version\": config(\"OPENAI_API_VERSION\", default=\"\")\n                or \"2024-02-15-preview\",\n                \"azure_deployment\": config(\"AZURE_OPENAI_CHAT_DEPLOYMENT\", default=\"\"),\n                \"timeout\": 20,\n            },\n            \"default\": False,\n        }\n    if config(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\", default=\"\"):\n        KH_EMBEDDINGS[\"azure\"] = {\n            \"spec\": {\n                \"__type__\": \"kotaemon.embeddings.AzureOpenAIEmbeddings\",\n                \"azure_endpoint\": config(\"AZURE_OPENAI_ENDPOINT\", default=\"\"),\n                \"api_key\": config(\"AZURE_OPENAI_API_KEY\", default=\"\"),\n                \"api_version\": config(\"OPENAI_API_VERSION\", default=\"\")\n                or \"2024-02-15-preview\",\n                \"azure_deployment\": config(\n                    \"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\", default=\"\"\n                ),\n                \"timeout\": 10,\n            },\n            \"default\": False,\n        }\n\nif config(\"OPENAI_API_KEY\", default=\"\"):\n    KH_LLMS[\"openai\"] = {\n        \"spec\": {\n            \"__type__\": \"kotaemon.llms.ChatOpenAI\",\n            \"temperature\": 0,\n            \"base_url\": config(\"OPENAI_API_BASE\", default=\"\")\n            or \"https://api.openai.com/v1\",\n            \"api_key\": config(\"OPENAI_API_KEY\", default=\"\"),\n            \"model\": config(\"OPENAI_CHAT_MODEL\", default=\"gpt-3.5-turbo\"),\n            \"timeout\": 20,\n        },\n        \"default\": True,\n    }\n    KH_EMBEDDINGS[\"openai\"] = {\n        \"spec\": {\n            \"__type__\": \"kotaemon.embeddings.OpenAIEmbeddings\",\n            \"base_url\": config(\"OPENAI_API_BASE\", default=\"https://api.openai.com/v1\"),\n            \"api_key\": config(\"OPENAI_API_KEY\", default=\"\"),\n            \"model\": config(\n                \"OPENAI_EMBEDDINGS_MODEL\", default=\"text-embedding-ada-002\"\n            ),\n            \"timeout\": 10,\n            \"context_length\": 8191,\n        },\n        \"default\": True,\n    }\n\nif config(\"LOCAL_MODEL\", default=\"\"):\n    KH_LLMS[\"ollama\"] = {\n        \"spec\": {\n            \"__type__\": \"kotaemon.llms.ChatOpenAI\",\n            \"base_url\": KH_OLLAMA_URL,\n            \"model\": config(\"LOCAL_MODEL\", default=\"llama3.1:8b\"),\n            \"api_key\": \"ollama\",\n        },\n        \"default\": False,\n    }\n    KH_EMBEDDINGS[\"ollama\"] = {\n        \"spec\": {\n            \"__type__\": \"kotaemon.embeddings.OpenAIEmbeddings\",\n            \"base_url\": KH_OLLAMA_URL,\n            \"model\": config(\"LOCAL_MODEL_EMBEDDINGS\", default=\"nomic-embed-text\"),\n            \"api_key\": \"ollama\",\n        },\n        \"default\": False,\n    }\n\n    KH_EMBEDDINGS[\"fast_embed\"] = {\n        \"spec\": {\n            \"__type__\": \"kotaemon.embeddings.FastEmbedEmbeddings\",\n            \"model_name\": \"BAAI/bge-base-en-v1.5\",\n        },\n        \"default\": False,\n    }\n\n# additional LLM configurations\nKH_LLMS[\"claude\"] = {\n    \"spec\": {\n        \"__type__\": \"kotaemon.llms.chats.LCAnthropicChat\",\n        \"model_name\": \"claude-3-5-sonnet-20240620\",\n        \"api_key\": \"your-key\",\n    },\n    \"default\": False,\n}\nKH_LLMS[\"google\"] = {\n    \"spec\": {\n        \"__type__\": \"kotaemon.llms.chats.LCGeminiChat\",\n        \"model_name\": \"gemini-1.5-flash\",\n        \"api_key\": config(\"GOOGLE_API_KEY\", default=\"your-key\"),\n    },\n    \"default\": False,\n}\nKH_LLMS[\"groq\"] = {\n    \"spec\": {\n        \"__type__\": \"kotaemon.llms.ChatOpenAI\",\n        \"base_url\": \"https://api.groq.com/openai/v1\",\n        \"model\": \"llama-3.1-8b-instant\",\n        \"api_key\": \"your-key\",\n    },\n    \"default\": False,\n}\nKH_LLMS[\"cohere\"] = {\n    \"spec\": {\n        \"__type__\": \"kotaemon.llms.chats.LCCohereChat\",\n        \"model_name\": \"command-r-plus-08-2024\",\n        \"api_key\": config(\"COHERE_API_KEY\", default=\"your-key\"),\n    },\n    \"default\": False,\n}\n\n# additional embeddings configurations\nKH_EMBEDDINGS[\"cohere\"] = {\n    \"spec\": {\n        \"__type__\": \"kotaemon.embeddings.LCCohereEmbeddings\",\n        \"model\": \"embed-multilingual-v3.0\",\n        \"cohere_api_key\": config(\"COHERE_API_KEY\", default=\"your-key\"),\n        \"user_agent\": \"default\",\n    },\n    \"default\": False,\n}\nKH_EMBEDDINGS[\"google\"] = {\n    \"spec\": {\n        \"__type__\": \"kotaemon.embeddings.LCGoogleEmbeddings\",\n        \"model\": \"models/text-embedding-004\",\n        \"google_api_key\": config(\"GOOGLE_API_KEY\", default=\"your-key\"),\n    }\n}\n# KH_EMBEDDINGS[\"huggingface\"] = {\n#     \"spec\": {\n#         \"__type__\": \"kotaemon.embeddings.LCHuggingFaceEmbeddings\",\n#         \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n#     },\n#     \"default\": False,\n# }\n\n# default reranking models\nKH_RERANKINGS[\"cohere\"] = {\n    \"spec\": {\n        \"__type__\": \"kotaemon.rerankings.CohereReranking\",\n        \"model_name\": \"rerank-multilingual-v2.0\",\n        \"cohere_api_key\": config(\"COHERE_API_KEY\", default=\"\"),\n    },\n    \"default\": True,\n}\n\nKH_REASONINGS = [\n    \"ktem.reasoning.simple.FullQAPipeline\",\n    \"ktem.reasoning.simple.FullDecomposeQAPipeline\",\n    \"ktem.reasoning.react.ReactAgentPipeline\",\n    \"ktem.reasoning.rewoo.RewooAgentPipeline\",\n]\nKH_REASONINGS_USE_MULTIMODAL = config(\"USE_MULTIMODAL\", default=False, cast=bool)\nKH_VLM_ENDPOINT = \"{0}/openai/deployments/{1}/chat/completions?api-version={2}\".format(\n    config(\"AZURE_OPENAI_ENDPOINT\", default=\"\"),\n    config(\"OPENAI_VISION_DEPLOYMENT_NAME\", default=\"gpt-4o\"),\n    config(\"OPENAI_API_VERSION\", default=\"\"),\n)\n\n\nSETTINGS_APP: dict[str, dict] = {}\n\n\nSETTINGS_REASONING = {\n    \"use\": {\n        \"name\": \"Reasoning options\",\n        \"value\": None,\n        \"choices\": [],\n        \"component\": \"radio\",\n    },\n    \"lang\": {\n        \"name\": \"Language\",\n        \"value\": \"en\",\n        \"choices\": [(lang, code) for code, lang in SUPPORTED_LANGUAGE_MAP.items()],\n        \"component\": \"dropdown\",\n    },\n    \"max_context_length\": {\n        \"name\": \"Max context length (LLM)\",\n        \"value\": 32000,\n        \"component\": \"number\",\n    },\n}\n\nUSE_NANO_GRAPHRAG = config(\"USE_NANO_GRAPHRAG\", default=False, cast=bool)\nUSE_LIGHTRAG = config(\"USE_LIGHTRAG\", default=True, cast=bool)\n\nGRAPHRAG_INDEX_TYPES = [\"ktem.index.file.graph.GraphRAGIndex\"]\n\nif USE_NANO_GRAPHRAG:\n    GRAPHRAG_INDEX_TYPES.append(\"ktem.index.file.graph.NanoGraphRAGIndex\")\nif USE_LIGHTRAG:\n    GRAPHRAG_INDEX_TYPES.append(\"ktem.index.file.graph.LightRAGIndex\")\n\nKH_INDEX_TYPES = [\n    \"ktem.index.file.FileIndex\",\n    *GRAPHRAG_INDEX_TYPES,\n]\n\nGRAPHRAG_INDICES = [\n    {\n        \"name\": graph_type.split(\".\")[-1].replace(\"Index\", \"\")\n        + \" Collection\",  # get last name\n        \"config\": {\n            \"supported_file_types\": (\n                \".png, .jpeg, .jpg, .tiff, .tif, .pdf, .xls, .xlsx, .doc, .docx, \"\n                \".pptx, .csv, .html, .mhtml, .txt, .md, .zip\"\n            ),\n            \"private\": False,\n        },\n        \"index_type\": graph_type,\n    }\n    for graph_type in GRAPHRAG_INDEX_TYPES\n]\n\nKH_INDICES = [\n    {\n        \"name\": \"File Collection\",\n        \"config\": {\n            \"supported_file_types\": (\n                \".png, .jpeg, .jpg, .tiff, .tif, .pdf, .xls, .xlsx, .doc, .docx, \"\n                \".pptx, .csv, .html, .mhtml, .txt, .md, .zip\"\n            ),\n            \"private\": False,\n        },\n        \"index_type\": \"ktem.index.file.FileIndex\",\n    },\n    *GRAPHRAG_INDICES,\n]\n"
        },
        {
          "name": "libs",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 3.1796875,
          "content": "repo_name: Cinnamon/kotaemon\nrepo_url: https://github.com/Cinnamon/kotaemon\nsite_name: kotaemon Docs\nedit_uri: edit/main/docs/\n\nnav:\n  - Getting Started:\n      - Quick Start: index.md\n      - Basic Usage: usage.md\n  # - Application:\n  #     - Customize UI: pages/app/customize-ui.md\n  # - Functional description: pages/app/functional-description.md\n  - Development:\n      - development/index.md\n      # - Data & Data Structure Components: development/data-components.md\n      # - Features: pages/app/features.md\n      - Customize flow logic: pages/app/customize-flows.md\n      - Creating a Component: development/create-a-component.md\n      - Components:\n          - Index:\n              - File index: pages/app/index/file.md\n          - Settings:\n              - pages/app/settings/overview.md\n              - pages/app/settings/user-settings.md\n          - Extension:\n              - User management: pages/app/ext/user-management.md\n      - Contributing: development/contributing.md\n  # generated using gen-files + literate-nav\n  - API Reference: reference/\n  - Changelogs: https://github.com/Cinnamon/kotaemon/releases\n  - Issue Tracker: https://github.com/Cinnamon/kotaemon/issues\n  - Live Demo: https://huggingface.co/spaces/cin-model/kotaemon-demo\n\nmarkdown_extensions:\n  - admonition\n  - md_in_html\n  - pymdownx.highlight:\n      use_pygments: true\n      anchor_linenums: true\n      line_spans: __span\n      linenums: true\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.details\n  - pymdownx.extra\n  - pymdownx.tabbed:\n      alternate_style: true\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n  - toc:\n      permalink: true\n      title: Page contents\n  - mdx_truly_sane_lists\n\nplugins:\n  - search\n  - gen-files:\n      scripts:\n        - docs/scripts/generate_reference_docs.py\n  - literate-nav:\n      nav_file: Summary.md\n  - mkdocstrings:\n      handlers:\n        python:\n          options:\n            docstring_options:\n              ignore_init_summary: false\n            filters:\n              - \"!^_\"\n            members_order: source\n            separate_signature: true\n          paths: [libs/kotaemon/kotaemon]\n  - git-revision-date-localized:\n      enable_creation_date: true\n      type: timeago\n      fallback_to_build_date: true\n  - section-index\n  - mkdocs-video\n  - include-markdown\n\ntheme:\n  features:\n    - content.action.edit\n    - content.tabs.link\n    - content.code.annotate\n    - content.code.annotations\n    - content.code.copy\n    - navigation.tabs\n    - navigation.top\n    - navigation.instant\n    - navigation.indexes\n    - toc.follow\n    - search.share\n    - search.highlight\n    - search.suggest\n  name: material\n  custom_dir: docs/theme\n  palette:\n    scheme: dracula\n    primary: deep purple\n    accent: deep purple\n  icon:\n    repo: fontawesome/brands/github\n    edit: material/pencil\n    view: material/eye\n\nextra_css:\n  - extra/css/code_select.css\n  - assets/pymdownx-extras/extra-fb5a2a1c86.css\n\nextra_javascript:\n  - assets/pymdownx-extras/extra-loader-MCFnu0Wd.js\n\nvalidation:\n  absolute_links: warn\n  omitted_files: warn\n  unrecognized_links: warn\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.34375,
          "content": "[build-system]\nrequires = [\"setuptools >= 61.0\", \"wheel\", \"setuptools-git-versioning>=2.0,<3\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\ninclude-package-data = false\npackages.find.include = []\n\n[tool.setuptools-git-versioning]\nenabled = true\ndev_template = \"{tag}\"\ndirty_template = \"{tag}\"\ntag_filter = \"v?\\\\d+(\\\\.\\\\d+)*.*\"\n\n[project]\nname = \"kotaemon-app\"\ndynamic = [\"version\"]\nrequires-python = \">= 3.10\"\ndescription = \"Kotaemon App\"\ndependencies = [\n    \"kotaemon @ git+https://github.com/Cinnamon/kotaemon.git@main#subdirectory=libs/kotaemon\",\n    \"ktem @ git+https://github.com/Cinnamon/kotaemon.git@main#subdirectory=libs/ktem\"\n]\nauthors = [\n    { name = \"@trducng\", email = \"john@cinnamon.is\" },\n    { name = \"@lone17\", email = \"ian@cinnamon.is\" },\n    { name = \"@taprosoft\", email = \"tadashi@cinnamon.is\" },\n    { name = \"@cin-albert\", email = \"albert@cinnamon.is\" },\n]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"Operating System :: OS Independent\",\n]\n\n[project.urls]\nHomepage = \"https://cinnamon.github.io/kotaemon/\"\nRepository = \"https://github.com/Cinnamon/kotaemon/\"\nDocumentation = \"https://cinnamon.github.io/kotaemon/\"\n\n[tool.codespell]\nskip = \"*.js,*.css,*.map\"\n# `llm` abbreviation for large language models\nignore-words-list = \"llm,fo\"\nquiet-level = 3\ncheck-filenames = \"\"\n\n[tool.isort]\nknown_first_party = [\"kotaemon\"]\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "settings.yaml.example",
          "type": "blob",
          "size": 5.5458984375,
          "content": "# This is a sample GraphRAG settings.yaml file that allows users to run the GraphRAG index process with their customized parameters.\n# The parameters in this file will only take effect when the USE_CUSTOMIZED_GRAPHRAG_SETTING is true in .env file.\n# For a comprehensive understanding of GraphRAG parameters, please refer to: https://microsoft.github.io/graphrag/config/json_yaml/.\n\nencoding_model: cl100k_base\nskip_workflows: []\nllm:\n  api_key: ${GRAPHRAG_API_KEY}\n  type: openai_chat # or azure_openai_chat\n  api_base: http://127.0.0.1:11434/v1\n  model: qwen2\n  model_supports_json: true # recommended if this is available for your model.\n  # max_tokens: 4000\n  request_timeout: 1800.0\n  # api_base: https://<instance>.openai.azure.com\n  # api_version: 2024-02-15-preview\n  # organization: <organization_id>\n  # deployment_name: <azure_model_deployment_name>\n  # tokens_per_minute: 150_000 # set a leaky bucket throttle\n  # requests_per_minute: 10_000 # set a leaky bucket throttle\n  # max_retries: 10\n  # max_retry_wait: 10.0\n  # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times\n  concurrent_requests: 5 # the number of parallel inflight requests that may be made\n  # temperature: 0 # temperature for sampling\n  # top_p: 1 # top-p sampling\n  # n: 1 # Number of completions to generate\n\nparallelization:\n  stagger: 0.3\n  # num_threads: 50 # the number of threads to use for parallel processing\n\nasync_mode: threaded # or asyncio\n\nembeddings:\n  ## parallelization: override the global parallelization settings for embeddings\n  async_mode: threaded # or asyncio\n  # target: required # or all\n  # batch_size: 16 # the number of documents to send in a single request\n  # batch_max_tokens: 8191 # the maximum number of tokens to send in a single request\n  llm:\n    api_base: http://localhost:11434/v1\n    api_key: ${GRAPHRAG_API_KEY}\n    model: nomic-embed-text\n    type: openai_embedding\n    # api_base: https://<instance>.openai.azure.com\n    # api_version: 2024-02-15-preview\n    # organization: <organization_id>\n    # deployment_name: <azure_model_deployment_name>\n    # tokens_per_minute: 150_000 # set a leaky bucket throttle\n    # requests_per_minute: 10_000 # set a leaky bucket throttle\n    # max_retries: 10\n    # max_retry_wait: 10.0\n    # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times\n    # concurrent_requests: 25 # the number of parallel inflight requests that may be made\n\nchunks:\n  size: 1200\n  overlap: 100\n  group_by_columns: [id] # by default, we don't allow chunks to cross documents\n\ninput:\n  type: file # or blob\n  file_type: text # or csv\n  base_dir: \"input\"\n  file_encoding: utf-8\n  file_pattern: \".*\\\\.txt$\"\n\ncache:\n  type: file # or blob\n  base_dir: \"cache\"\n  # connection_string: <azure_blob_storage_connection_string>\n  # container_name: <azure_blob_storage_container_name>\n\nstorage:\n  type: file # or blob\n  base_dir: \"output\"\n  # connection_string: <azure_blob_storage_connection_string>\n  # container_name: <azure_blob_storage_container_name>\n\nreporting:\n  type: file # or console, blob\n  base_dir: \"output\"\n  # connection_string: <azure_blob_storage_connection_string>\n  # container_name: <azure_blob_storage_container_name>\n\nentity_extraction:\n  ## strategy: fully override the entity extraction strategy.\n  ##   type: one of graph_intelligence, graph_intelligence_json and nltk\n  ## llm: override the global llm settings for this task\n  ## parallelization: override the global parallelization settings for this task\n  ## async_mode: override the global async_mode settings for this task\n  prompt: \"prompts/entity_extraction.txt\"\n  entity_types: [organization,person,geo,event]\n  max_gleanings: 1\n\nsummarize_descriptions:\n  ## llm: override the global llm settings for this task\n  ## parallelization: override the global parallelization settings for this task\n  ## async_mode: override the global async_mode settings for this task\n  prompt: \"prompts/summarize_descriptions.txt\"\n  max_length: 500\n\nclaim_extraction:\n  ## llm: override the global llm settings for this task\n  ## parallelization: override the global parallelization settings for this task\n  ## async_mode: override the global async_mode settings for this task\n  # enabled: true\n  prompt: \"prompts/claim_extraction.txt\"\n  description: \"Any claims or facts that could be relevant to information discovery.\"\n  max_gleanings: 1\n\ncommunity_reports:\n  ## llm: override the global llm settings for this task\n  ## parallelization: override the global parallelization settings for this task\n  ## async_mode: override the global async_mode settings for this task\n  prompt: \"prompts/community_report.txt\"\n  max_length: 2000\n  max_input_length: 8000\n\ncluster_graph:\n  max_cluster_size: 10\n\nembed_graph:\n  enabled: false # if true, will generate node2vec embeddings for nodes\n  # num_walks: 10\n  # walk_length: 40\n  # window_size: 2\n  # iterations: 3\n  # random_seed: 597832\n\numap:\n  enabled: false # if true, will generate UMAP embeddings for nodes\n\nsnapshots:\n  graphml: false\n  raw_entities: false\n  top_level_nodes: false\n\nlocal_search:\n  # text_unit_prop: 0.5\n  # community_prop: 0.1\n  # conversation_history_max_turns: 5\n  # top_k_mapped_entities: 10\n  # top_k_relationships: 10\n  # llm_temperature: 0 # temperature for sampling\n  # llm_top_p: 1 # top-p sampling\n  # llm_n: 1 # Number of completions to generate\n  # max_tokens: 12000\n\nglobal_search:\n  # llm_temperature: 0 # temperature for sampling\n  # llm_top_p: 1 # top-p sampling\n  # llm_n: 1 # Number of completions to generate\n  # max_tokens: 12000\n  # data_max_tokens: 12000\n  # map_max_tokens: 1000\n  # reduce_max_tokens: 2000\n  # concurrency: 32\n"
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}