{
  "metadata": {
    "timestamp": 1736561338362,
    "page": 360,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jacobgil/pytorch-grad-cam",
      "stars": 10937,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0439453125,
          "content": "tutorials/*.ipynb linguist-documentation=true"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.029296875,
          "content": "build\ndist\n*.egg-info\n**/*.pyc"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0478515625,
          "content": "MIT License\n\nCopyright (c) 2021 Jacob Gildenblat\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.9462890625,
          "content": "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![Build Status](https://github.com/jacobgil/pytorch-grad-cam/workflows/Tests/badge.svg)\n[![Downloads](https://static.pepy.tech/personalized-badge/grad-cam?period=month&units=international_system&left_color=black&right_color=brightgreen&left_text=Monthly%20Downloads)](https://pepy.tech/project/grad-cam)\n[![Downloads](https://static.pepy.tech/personalized-badge/grad-cam?period=total&units=international_system&left_color=black&right_color=blue&left_text=Total%20Downloads)](https://pepy.tech/project/grad-cam)\n\n# Advanced AI explainability for PyTorch\n\n`pip install grad-cam`\n\nDocumentation with advanced tutorials: [https://jacobgil.github.io/pytorch-gradcam-book](https://jacobgil.github.io/pytorch-gradcam-book)\n\n\nThis is a package with state of the art methods for Explainable AI for computer vision.\nThis can be used for diagnosing model predictions, either in production or while\ndeveloping models.\nThe aim is also to serve as a benchmark of algorithms and metrics for research of new explainability methods.\n\n‚≠ê Comprehensive collection of Pixel Attribution methods for Computer Vision.\n\n‚≠ê Tested on many Common CNN Networks and Vision Transformers.\n\n‚≠ê Advanced use cases: Works with Classification, Object Detection, Semantic Segmentation, Embedding-similarity and more.\n\n‚≠ê Includes smoothing methods to make the CAMs look nice.\n\n‚≠ê High performance: full support for batches of images in all methods.\n\n‚≠ê Includes metrics for checking if you can trust the explanations, and tuning them for best performance.\n\n\n![visualization](https://github.com/jacobgil/jacobgil.github.io/blob/master/assets/cam_dog.gif?raw=true\n)\n\n| Method              | What it does                                                                                                                |\n|---------------------|-----------------------------------------------------------------------------------------------------------------------------|\n| GradCAM             | Weight the 2D activations by the average gradient                                                                           |\n| HiResCAM            | Like GradCAM but element-wise multiply the activations with the gradients; provably guaranteed faithfulness for certain models |\n| GradCAMElementWise  | Like GradCAM but element-wise multiply the activations with the gradients then apply a ReLU operation before summing        |\n| GradCAM++           | Like GradCAM but uses second order gradients                                                                                |\n| XGradCAM            | Like GradCAM but scale the gradients by the normalized activations                                                          |\n| AblationCAM         | Zero out activations and measure how the output drops (this repository includes a fast batched implementation)              |\n| ScoreCAM            | Perbutate the image by the scaled activations and measure how the output drops                                              |\n| EigenCAM            | Takes the first principle component of the 2D Activations (no class discrimination, but seems to give great results)        |\n| EigenGradCAM        | Like EigenCAM but with class discrimination: First principle component of Activations*Grad. Looks like GradCAM, but cleaner |\n| LayerCAM            | Spatially weight the activations by positive gradients. Works better especially in lower layers                             |\n| FullGrad            | Computes the gradients of the biases from all over the network, and then sums them                                          |\n| Deep Feature Factorizations           | Non Negative Matrix Factorization on the 2D activations                                                   |\n|  KPCA-CAM           | Like EigenCAM but with Kernel PCA instead of PCA\n## Visual Examples\n\n| What makes the network think the image label is 'pug, pug-dog' | What makes the network think the image label is 'tabby, tabby cat' | Combining Grad-CAM with Guided Backpropagation for the 'pug, pug-dog' class |\n| ---------------------------------------------------------------|--------------------|-----------------------------------------------------------------------------|\n <img src=\"https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog.jpg?raw=true\" width=\"256\" height=\"256\"> | <img src=\"https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cat.jpg?raw=true\" width=\"256\" height=\"256\"> | <img src=\"https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cam_gb_dog.jpg?raw=true\" width=\"256\" height=\"256\"> |\n\n## Object Detection and Semantic Segmentation\n| Object Detection | Semantic Segmentation |\n| -----------------|-----------------------|\n| <img src=\"./examples/both_detection.png\" width=\"256\" height=\"256\"> | <img src=\"./examples/cars_segmentation.png\" width=\"256\" height=\"200\"> |\n\n| 3D Medical Semantic Segmentation |\n| -------------------------- |\n| <img src=\"./examples/multiorgan_segmentation.gif\" width=\"539\">|\n\n## Explaining similarity to other images / embeddings\n<img src=\"./examples/embeddings.png\">\n\n## Deep Feature Factorization\n<img src=\"./examples/dff1.png\">\n<img src=\"./examples/dff2.png\">\n\n## CLIP\n| Explaining the text prompt \"a dog\" | Explaining the text prompt \"a cat\" |\n| -----------------------------------|------------------------------------|\n <img src=\"https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/clip_dog.jpg?raw=true\" width=\"256\" height=\"256\"> | <img src=\"https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/clip_cat.jpg?raw=true\" width=\"256\" height=\"256\"> |\n\n## Classification\n\n#### Resnet50:\n| Category  | Image | GradCAM  |  AblationCAM |  ScoreCAM |\n| ---------|-------|----------|------------|------------|\n| Dog    | ![](./examples/dog_cat.jfif) | ![](./examples/resnet50_dog_gradcam_cam.jpg)     |  ![](./examples/resnet50_dog_ablationcam_cam.jpg)   |![](./examples/resnet50_dog_scorecam_cam.jpg)   |\n| Cat    | ![](./examples/dog_cat.jfif?raw=true) | ![](./examples/resnet50_cat_gradcam_cam.jpg?raw=true)     |  ![](./examples/resnet50_cat_ablationcam_cam.jpg?raw=true)   |![](./examples/resnet50_cat_scorecam_cam.jpg)   |\n\n#### Vision Transfomer (Deit Tiny):\n| Category  | Image | GradCAM  |  AblationCAM |  ScoreCAM |\n| ---------|-------|----------|------------|------------|\n| Dog    | ![](./examples/dog_cat.jfif) | ![](./examples/vit_dog_gradcam_cam.jpg)     |  ![](./examples/vit_dog_ablationcam_cam.jpg)   |![](./examples/vit_dog_scorecam_cam.jpg)   |\n| Cat    | ![](./examples/dog_cat.jfif) | ![](./examples/vit_cat_gradcam_cam.jpg)     |  ![](./examples/vit_cat_ablationcam_cam.jpg)   |![](./examples/vit_cat_scorecam_cam.jpg)   |\n\n#### Swin Transfomer (Tiny window:7 patch:4 input-size:224):\n| Category  | Image | GradCAM  |  AblationCAM |  ScoreCAM |\n| ---------|-------|----------|------------|------------|\n| Dog    | ![](./examples/dog_cat.jfif) | ![](./examples/swinT_dog_gradcam_cam.jpg)     |  ![](./examples/swinT_dog_ablationcam_cam.jpg)   |![](./examples/swinT_dog_scorecam_cam.jpg)   |\n| Cat    | ![](./examples/dog_cat.jfif) | ![](./examples/swinT_cat_gradcam_cam.jpg)     |  ![](./examples/swinT_cat_ablationcam_cam.jpg)   |![](./examples/swinT_cat_scorecam_cam.jpg)   |\n\n\n# Metrics and Evaluation for XAI\n\n<img src=\"./examples/metrics.png\">\n<img src=\"./examples/road.png\">\n\n----------\n\n# Usage examples\n\n```python\nfrom pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom torchvision.models import resnet50\n\nmodel = resnet50(pretrained=True)\ntarget_layers = [model.layer4[-1]]\ninput_tensor = # Create an input tensor image for your model..\n# Note: input_tensor can be a batch tensor with several images!\n\n# We have to specify the target we want to generate the CAM for.\ntargets = [ClassifierOutputTarget(281)]\n\n# Construct the CAM object once, and then re-use it on many images.\nwith GradCAM(model=model, target_layers=target_layers) as cam:\n  # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n  grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n  # In this example grayscale_cam has only one image in the batch:\n  grayscale_cam = grayscale_cam[0, :]\n  visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n  # You can also get the model outputs without having to redo inference\n  model_outputs = cam.outputs\n```\n\n[cam.py](https://github.com/jacobgil/pytorch-grad-cam/blob/master/cam.py) has a more detailed usage example.\n\n----------\n# Choosing the layer(s) to extract activations from\nYou need to choose the target layer to compute the CAM for.\nSome common choices are:\n- FasterRCNN: model.backbone\n- Resnet18 and 50: model.layer4[-1]\n- VGG, densenet161 and mobilenet: model.features[-1]\n- mnasnet1_0: model.layers[-1]\n- ViT: model.blocks[-1].norm1\n- SwinT: model.layers[-1].blocks[-1].norm1\n\n\nIf you pass a list with several layers, the CAM will be averaged accross them.\nThis can be useful if you're not sure what layer will perform best.\n\n----------\n\n# Adapting for new architectures and tasks\n\nMethods like GradCAM were designed for and were originally mostly applied on classification models, \nand specifically CNN classification models.\nHowever you can also use this package on new architectures like Vision Transformers, and on non classification tasks like Object Detection or Semantic Segmentation.\n\nThe be able to adapt to non standard cases, we have two concepts.\n- The reshape transform - how do we convert activations to represent spatial images ?\n- The model targets - What exactly should the explainability method try to explain ?\n\n## The reshape_transform argument\nIn a CNN the intermediate activations in the model are a mult-channel image that have the dimensions channel x rows x cols,\nand the various explainabiltiy methods work with these to produce a new image.\n\nIn case of another architecture, like the Vision Transformer, the shape might be different, like (rows x cols + 1) x channels, or something else.\nThe reshape transform converts the activations back into a multi-channel image, for example by removing the class token in a vision transformer. \nFor examples, check [here](https://github.com/jacobgil/pytorch-grad-cam/blob/master/pytorch_grad_cam/utils/reshape_transforms.py)\n\n## The model_target argument\nThe model target is just a callable that is able to get the model output, and filter it out for the specific scalar output we want to explain.\n\nFor classification tasks, the model target will typically be the output from a specific category.\nThe `targets` parameter passed to the CAM method can then use `ClassifierOutputTarget`:\n```python\ntargets = [ClassifierOutputTarget(281)]\n```\n\nHowever for more advanced cases, you might want a different behaviour.\nCheck [here](https://github.com/jacobgil/pytorch-grad-cam/blob/master/pytorch_grad_cam/utils/model_targets.py) for more examples.\n\n----------\n\n# Tutorials\nHere you can find detailed examples of how to use this for various custom use cases like object detection:\n\nThese point to the new documentation jupter-book for fast rendering.\nThe jupyter notebooks themselves can be found under the tutorials folder in the git repository.\n\n- [Notebook tutorial: XAI Recipes for the HuggingFace ü§ó Image Classification Models](<https://jacobgil.github.io/pytorch-gradcam-book/HuggingFace.html>)\n\n- [Notebook tutorial: Deep Feature Factorizations for better model explainability](<https://jacobgil.github.io/pytorch-gradcam-book/Deep%20Feature%20Factorizations.html>)\n\n- [Notebook tutorial: Class Activation Maps for Object Detection with Faster-RCNN](<https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.html>)\n\n- [Notebook tutorial: Class Activation Maps for YOLO5](<https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html>)\n\n- [Notebook tutorial: Class Activation Maps for Semantic Segmentation](<https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html>)\n\n- [Notebook tutorial: Adapting pixel attribution methods for embedding outputs from models](<https://jacobgil.github.io/pytorch-gradcam-book/Pixel%20Attribution%20for%20embeddings.html>)\n\n- [Notebook tutorial: May the best explanation win. CAM Metrics and Tuning](<https://jacobgil.github.io/pytorch-gradcam-book/CAM%20Metrics%20And%20Tuning%20Tutorial.html>)\n\n- [How it works with Vision/SwinT transformers](tutorials/vision_transformers.md)\n\n\n----------\n\n# Guided backpropagation\n\n```python\nfrom pytorch_grad_cam import GuidedBackpropReLUModel\nfrom pytorch_grad_cam.utils.image import (\n    show_cam_on_image, deprocess_image, preprocess_image\n)\ngb_model = GuidedBackpropReLUModel(model=model, device=model.device())\ngb = gb_model(input_tensor, target_category=None)\n\ncam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])\ncam_gb = deprocess_image(cam_mask * gb)\nresult = deprocess_image(gb)\n```\n\n----------\n\n# Metrics and evaluating the explanations\n\n```python\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputSoftmaxTarget\nfrom pytorch_grad_cam.metrics.cam_mult_image import CamMultImageConfidenceChange\n# Create the metric target, often the confidence drop in a score of some category\nmetric_target = ClassifierOutputSoftmaxTarget(281)\nscores, batch_visualizations = CamMultImageConfidenceChange()(input_tensor, \n  inverse_cams, targets, model, return_visualization=True)\nvisualization = deprocess_image(batch_visualizations[0, :])\n\n# State of the art metric: Remove and Debias\nfrom pytorch_grad_cam.metrics.road import ROADMostRelevantFirst, ROADLeastRelevantFirst\ncam_metric = ROADMostRelevantFirst(percentile=75)\nscores, perturbation_visualizations = cam_metric(input_tensor, \n  grayscale_cams, targets, model, return_visualization=True)\n\n# You can also average across different percentiles, and combine\n# (LeastRelevantFirst - MostRelevantFirst) / 2\nfrom pytorch_grad_cam.metrics.road import ROADMostRelevantFirstAverage,\n                                          ROADLeastRelevantFirstAverage,\n                                          ROADCombined\ncam_metric = ROADCombined(percentiles=[20, 40, 60, 80])\nscores = cam_metric(input_tensor, grayscale_cams, targets, model)\n```\n\n\n# Smoothing to get nice looking CAMs\n\nTo reduce noise in the CAMs, and make it fit better on the objects,\ntwo smoothing methods are supported:\n\n- `aug_smooth=True`\n\n  Test time augmentation: increases the run time by x6.\n\n  Applies a combination of horizontal flips, and mutiplying the image\n  by [1.0, 1.1, 0.9].\n\n  This has the effect of better centering the CAM around the objects.\n\n\n- `eigen_smooth=True`\n\n  First principle component of `activations*weights`\n\n  This has the effect of removing a lot of noise.\n\n\n|AblationCAM | aug smooth | eigen smooth | aug+eigen smooth|\n|------------|------------|--------------|--------------------|\n![](./examples/nosmooth.jpg) | ![](./examples/augsmooth.jpg) | ![](./examples/eigensmooth.jpg) | ![](./examples/eigenaug.jpg) | \n\n----------\n\n# Running the example script:\n\nUsage: `python cam.py --image-path <path_to_image> --method <method> --output-dir <output_dir_path> `\n\n\nTo use with a specific device, like cpu, cuda, cuda:0, mps or hpu:\n`python cam.py --image-path <path_to_image> --device cuda  --output-dir <output_dir_path> `\n\n----------\n\nYou can choose between:\n\n`GradCAM` , `HiResCAM`, `ScoreCAM`, `GradCAMPlusPlus`, `AblationCAM`, `XGradCAM` , `LayerCAM`, `FullGrad` and `EigenCAM`.\n\nSome methods like ScoreCAM and AblationCAM require a large number of forward passes,\nand have a batched implementation.\n\nYou can control the batch size with\n`cam.batch_size = `\n\n----------\n\n## Citation\nIf you use this for research, please cite. Here is an example BibTeX entry:\n\n```\n@misc{jacobgilpytorchcam,\n  title={PyTorch library for CAM methods},\n  author={Jacob Gildenblat and contributors},\n  year={2021},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/jacobgil/pytorch-grad-cam}},\n}\n```\n\n----------\n\n# References\nhttps://arxiv.org/abs/1610.02391 <br>\n`Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra`\n\nhttps://arxiv.org/abs/2011.08891 <br>\n`Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks\nRachel L. Draelos, Lawrence Carin`\n\nhttps://arxiv.org/abs/1710.11063 <br>\n`Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\nAditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian`\n\nhttps://arxiv.org/abs/1910.01279 <br>\n`Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\nHaofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu`\n\nhttps://ieeexplore.ieee.org/abstract/document/9093360/ <br>\n`Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization.\nSaurabh Desai and Harish G Ramaswamy. In WACV, pages 972‚Äì980, 2020`\n\nhttps://arxiv.org/abs/2008.02312 <br>\n`Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\nRuigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li`\n\nhttps://arxiv.org/abs/2008.00299 <br>\n`Eigen-CAM: Class Activation Map using Principal Components\nMohammed Bany Muhammad, Mohammed Yeasin`\n\nhttp://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf <br>\n`LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\nPeng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n\nhttps://arxiv.org/abs/1905.00780 <br>\n`Full-Gradient Representation for Neural Network Visualization\nSuraj Srinivas, Francois Fleuret`\n\nhttps://arxiv.org/abs/1806.10206 <br>\n`Deep Feature Factorization For Concept Discovery\nEdo Collins, Radhakrishna Achanta, Sabine S√ºsstrunk`\n\nhttps://arxiv.org/abs/2410.00267 <br>\n`KPCA-CAM: Visual Explainability of Deep Computer Vision Models using Kernel PCA\nSachin Karmani, Thanushon Sivakaran, Gaurav Prasad, Mehmet Ali, Wenbo Yang, Sheyang Tang`\n"
        },
        {
          "name": "cam.py",
          "type": "blob",
          "size": 5.4853515625,
          "content": "import argparse\nimport os\nimport cv2\nimport numpy as np\nimport torch\nfrom torchvision import models\nfrom pytorch_grad_cam import (\n    GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus,\n    AblationCAM, XGradCAM, EigenCAM, EigenGradCAM,\n    LayerCAM, FullGrad, GradCAMElementWise, KPCA_CAM\n)\nfrom pytorch_grad_cam import GuidedBackpropReLUModel\nfrom pytorch_grad_cam.utils.image import (\n    show_cam_on_image, deprocess_image, preprocess_image\n)\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--device', type=str, default='cpu',\n                        help='Torch device to use')\n    parser.add_argument(\n        '--image-path',\n        type=str,\n        default='./examples/both.png',\n        help='Input image path')\n    parser.add_argument('--aug-smooth', action='store_true',\n                        help='Apply test time augmentation to smooth the CAM')\n    parser.add_argument(\n        '--eigen-smooth',\n        action='store_true',\n        help='Reduce noise by taking the first principle component'\n        'of cam_weights*activations')\n    parser.add_argument('--method', type=str, default='gradcam',\n                        choices=[\n                            'gradcam', 'hirescam', 'gradcam++',\n                            'scorecam', 'xgradcam', 'ablationcam',\n                            'eigencam', 'eigengradcam', 'layercam',\n                            'fullgrad', 'gradcamelementwise', 'kpcacam'\n                        ],\n                        help='CAM method')\n\n    parser.add_argument('--output-dir', type=str, default='output',\n                        help='Output directory to save the images')\n    args = parser.parse_args()\n    \n    if args.device:\n        print(f'Using device \"{args.device}\" for acceleration')\n    else:\n        print('Using CPU for computation')\n\n    return args\n\n\nif __name__ == '__main__':\n    \"\"\" python cam.py -image-path <path_to_image>\n    Example usage of loading an image and computing:\n        1. CAM\n        2. Guided Back Propagation\n        3. Combining both\n    \"\"\"\n\n    args = get_args()\n    methods = {\n        \"gradcam\": GradCAM,\n        \"hirescam\": HiResCAM,\n        \"scorecam\": ScoreCAM,\n        \"gradcam++\": GradCAMPlusPlus,\n        \"ablationcam\": AblationCAM,\n        \"xgradcam\": XGradCAM,\n        \"eigencam\": EigenCAM,\n        \"eigengradcam\": EigenGradCAM,\n        \"layercam\": LayerCAM,\n        \"fullgrad\": FullGrad,\n        \"gradcamelementwise\": GradCAMElementWise,\n        'kpcacam': KPCA_CAM\n    }\n\n    if args.device=='hpu':\n        import habana_frameworks.torch.core as htcore\n\n    model = models.resnet50(pretrained=True).to(torch.device(args.device)).eval()\n\n    # Choose the target layer you want to compute the visualization for.\n    # Usually this will be the last convolutional layer in the model.\n    # Some common choices can be:\n    # Resnet18 and 50: model.layer4\n    # VGG, densenet161: model.features[-1]\n    # mnasnet1_0: model.layers[-1]\n    # You can print the model to help chose the layer\n    # You can pass a list with several target layers,\n    # in that case the CAMs will be computed per layer and then aggregated.\n    # You can also try selecting all layers of a certain type, with e.g:\n    # from pytorch_grad_cam.utils.find_layers import find_layer_types_recursive\n    # find_layer_types_recursive(model, [torch.nn.ReLU])\n    \n    target_layers = [model.layer4]\n\n    rgb_img = cv2.imread(args.image_path, 1)[:, :, ::-1]\n    rgb_img = np.float32(rgb_img) / 255\n    input_tensor = preprocess_image(rgb_img,\n                                    mean=[0.485, 0.456, 0.406],\n                                    std=[0.229, 0.224, 0.225]).to(args.device)\n\n    # We have to specify the target we want to generate\n    # the Class Activation Maps for.\n    # If targets is None, the highest scoring category (for every member in the batch) will be used.\n    # You can target specific categories by\n    # targets = [ClassifierOutputTarget(281)]\n    # targets = [ClassifierOutputTarget(281)]\n    targets = None\n\n    # Using the with statement ensures the context is freed, and you can\n    # recreate different CAM objects in a loop.\n    cam_algorithm = methods[args.method]\n    with cam_algorithm(model=model,\n                       target_layers=target_layers) as cam:\n\n        # AblationCAM and ScoreCAM have batched implementations.\n        # You can override the internal batch size for faster computation.\n        cam.batch_size = 32\n        grayscale_cam = cam(input_tensor=input_tensor,\n                            targets=targets,\n                            aug_smooth=args.aug_smooth,\n                            eigen_smooth=args.eigen_smooth)\n\n        grayscale_cam = grayscale_cam[0, :]\n\n        cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n        cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n\n    gb_model = GuidedBackpropReLUModel(model=model, device=args.device)\n    gb = gb_model(input_tensor, target_category=None)\n\n    cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])\n    cam_gb = deprocess_image(cam_mask * gb)\n    gb = deprocess_image(gb)\n\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    cam_output_path = os.path.join(args.output_dir, f'{args.method}_cam.jpg')\n    gb_output_path = os.path.join(args.output_dir, f'{args.method}_gb.jpg')\n    cam_gb_output_path = os.path.join(args.output_dir, f'{args.method}_cam_gb.jpg')\n\n    cv2.imwrite(cam_output_path, cam_image)\n    cv2.imwrite(gb_output_path, gb)\n    cv2.imwrite(cam_gb_output_path, cam_gb)\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.1005859375,
          "content": "[build-system]\nrequires = [\n    \"setuptools>=42\",\n    \"wheel\"\n]\nbuild-backend = \"setuptools.build_meta\""
        },
        {
          "name": "pytorch_grad_cam",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0908203125,
          "content": "numpy\nPillow\ntorch>=1.7.1\ntorchvision>=0.8.2\nttach\ntqdm\nopencv-python\nmatplotlib\nscikit-learn"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.6416015625,
          "content": "[metadata]\nname = grad-cam\nversion = 1.1.0\nauthor = Jacob Gildenblat\nauthor_email = jacob.gildenblat@gmail.com\ndescription = Many Class Activation Map methods implemented in Pytorch. Including Grad-CAM, Grad-CAM++, Score-CAM, Ablation-CAM and XGrad-CAM\nlong_description = file: README.md\nlong_description_content_type = text/markdown\nurl = https://github.com/jacobgil/pytorch-grad-cam\nproject_urls =\n    Bug Tracker = https://github.com/jacobgil/pytorch-grad-cam/issues\nclassifiers =\n    Programming Language :: Python :: 3\n    License :: OSI Approved :: MIT License\n    Operating System :: OS Independent\n\n[options]\npackages = find:\npython_requires = >=3.6"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.01953125,
          "content": "import setuptools\r\n\r\nwith open('README.md', mode='r', encoding='utf-8') as fh:\r\n    long_description = fh.read()\r\n\r\nwith open(\"requirements.txt\", \"r\") as f:\r\n    requirements = f.readlines()\r\n\r\nsetuptools.setup(\r\n    name='grad-cam',\r\n    version='1.5.4',\r\n    author='Jacob Gildenblat',\r\n    author_email='jacob.gildenblat@gmail.com',\r\n    description='Many Class Activation Map methods implemented in Pytorch for classification, segmentation, object detection and more',\r\n    long_description=long_description,\r\n    long_description_content_type='text/markdown',\r\n    url='https://github.com/jacobgil/pytorch-grad-cam',\r\n    project_urls={\r\n        'Bug Tracker': 'https://github.com/jacobgil/pytorch-grad-cam/issues',\r\n    },\r\n    classifiers=[\r\n        'Programming Language :: Python :: 3',\r\n        'License :: OSI Approved :: MIT License',\r\n        'Operating System :: OS Independent',\r\n    ],\r\n    packages=setuptools.find_packages(\r\n        exclude=[\"*tutorials*\"]),\r\n    python_requires='>=3.8',\r\n    install_requires=requirements)\r\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorials",
          "type": "tree",
          "content": null
        },
        {
          "name": "usage_examples",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}