{
  "metadata": {
    "timestamp": 1736561244326,
    "page": 232,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/ChatGLM3",
      "stars": 13596,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1708984375,
          "content": "__pycache__\n\n# finetune_demo: generated & downloaded files\nfinetune_demo/output\nfinetune_demo/data\nfinetune_demo/formatted_data\nToolAlpaca/\nAdvertiseGen/\n*.gz\n*.idea\n.DS_Store"
        },
        {
          "name": "DEPLOYMENT.md",
          "type": "blob",
          "size": 2.048828125,
          "content": "## ä½æˆæœ¬éƒ¨ç½²\n\n### æ¨¡å‹é‡åŒ–\n\né»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 13GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\",trust_remote_code=True).quantize(4).cuda()\n```\n\næ¨¡å‹é‡åŒ–ä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½æŸå¤±ï¼Œç»è¿‡æµ‹è¯•ï¼ŒChatGLM3-6B åœ¨ 4-bit é‡åŒ–ä¸‹ä»ç„¶èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶æµç•…çš„ç”Ÿæˆã€‚\n\n### CPU éƒ¨ç½²\n\nå¦‚æœä½ æ²¡æœ‰ GPU ç¡¬ä»¶çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½†æ˜¯æ¨ç†é€Ÿåº¦ä¼šæ›´æ…¢ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼ˆéœ€è¦å¤§æ¦‚ 32GB å†…å­˜ï¼‰\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).float()\n```\n\n### Mac éƒ¨ç½²\n\nå¯¹äºæ­è½½äº† Apple Silicon æˆ–è€… AMD GPU çš„ Macï¼Œå¯ä»¥ä½¿ç”¨ MPS åç«¯æ¥åœ¨ GPU ä¸Šè¿è¡Œ ChatGLM3-6Bã€‚éœ€è¦å‚è€ƒ Apple çš„ [å®˜æ–¹è¯´æ˜](https://developer.apple.com/metal/pytorch) å®‰è£… PyTorch-Nightlyï¼ˆæ­£ç¡®çš„ç‰ˆæœ¬å·åº”è¯¥æ˜¯2.x.x.dev2023xxxxï¼Œè€Œä¸æ˜¯ 2.x.xï¼‰ã€‚\n\nç›®å‰åœ¨ MacOS ä¸Šåªæ”¯æŒ[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](README.md#ä»æœ¬åœ°åŠ è½½æ¨¡å‹)ã€‚å°†ä»£ç ä¸­çš„æ¨¡å‹åŠ è½½æ”¹ä¸ºä»æœ¬åœ°åŠ è½½ï¼Œå¹¶ä½¿ç”¨ mps åç«¯ï¼š\n```python\nmodel = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).to('mps')\n```\n\nåŠ è½½åŠç²¾åº¦çš„ ChatGLM3-6B æ¨¡å‹éœ€è¦å¤§æ¦‚ 13GB å†…å­˜ã€‚å†…å­˜è¾ƒå°çš„æœºå™¨ï¼ˆæ¯”å¦‚ 16GB å†…å­˜çš„ MacBook Proï¼‰ï¼Œåœ¨ç©ºä½™å†…å­˜ä¸è¶³çš„æƒ…å†µä¸‹ä¼šä½¿ç”¨ç¡¬ç›˜ä¸Šçš„è™šæ‹Ÿå†…å­˜ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ä¸¥é‡å˜æ…¢ã€‚\n\n### å¤šå¡éƒ¨ç½²\nå¦‚æœä½ æœ‰å¤šå¼  GPUï¼Œä½†æ˜¯æ¯å¼  GPU çš„æ˜¾å­˜å¤§å°éƒ½ä¸è¶³ä»¥å®¹çº³å®Œæ•´çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå¯ä»¥å°†æ¨¡å‹åˆ‡åˆ†åœ¨å¤šå¼ GPUä¸Šã€‚é¦–å…ˆå®‰è£… accelerate: `pip install accelerate`ï¼Œç„¶åé€šè¿‡å¦‚ä¸‹æ–¹æ³•åŠ è½½æ¨¡å‹ï¼š\n\n```python\nfrom utils import load_model_on_gpus\n\nmodel = load_model_on_gpus(\"THUDM/chatglm3-6b\", num_gpus=2)\n```\nå³å¯å°†æ¨¡å‹éƒ¨ç½²åˆ°ä¸¤å¼  GPU ä¸Šè¿›è¡Œæ¨ç†ã€‚ä½ å¯ä»¥å°† `num_gpus` æ”¹ä¸ºä½ å¸Œæœ›ä½¿ç”¨çš„ GPU æ•°ã€‚é»˜è®¤æ˜¯å‡åŒ€åˆ‡åˆ†çš„ï¼Œä½ ä¹Ÿå¯ä»¥ä¼ å…¥ `device_map` å‚æ•°æ¥è‡ªå·±æŒ‡å®šã€‚ "
        },
        {
          "name": "DEPLOYMENT_en.md",
          "type": "blob",
          "size": 2.25,
          "content": "## Low-Cost Deployment\n\n### Model Quantization\n\nBy default, the model is loaded with FP16 precision, running the above code requires about 13GB of VRAM. If your GPU's VRAM is limited, you can try loading the model quantitatively, as follows:\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\",trust_remote_code=True).quantize(4).cuda()\n```\n\nModel quantization will bring some performance loss. Through testing, ChatGLM3-6B can still perform natural and smooth generation under 4-bit quantization.\n\n### CPU Deployment\n\nIf you don't have GPU hardware, you can also run inference on the CPU, but the inference speed will be slower. The usage is as follows (requires about 32GB of memory):\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).float()\n```\n\n### Mac Deployment\n\nFor Macs equipped with Apple Silicon or AMD GPUs, the MPS backend can be used to run ChatGLM3-6B on the GPU. Refer to Apple's [official instructions](https://developer.apple.com/metal/pytorch) to install PyTorch-Nightly (the correct version number should be 2.x.x.dev2023xxxx, not 2.x.x).\n\nCurrently, only [loading the model locally](README_en.md#load-model-locally) is supported on MacOS. Change the model loading in the code to load locally and use the MPS backend:\n\n```python\nmodel = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).to('mps')\n```\n\nLoading the half-precision ChatGLM3-6B model requires about 13GB of memory. Machines with smaller memory (such as a 16GB memory MacBook Pro) will use virtual memory on the hard disk when there is insufficient free memory, resulting in a significant slowdown in inference speed.\n\n### Multi-GPU Deployment\n\nIf you have multiple GPUs, but each GPU's VRAM size is not enough to accommodate the complete model, then the model can be split across multiple GPUs. First, install accelerate: `pip install accelerate`, and then load the model through the following methods:\n\n```python\nfrom utils import load_model_on_gpus\n\nmodel = load_model_on_gpus(\"THUDM/chatglm3-6b\", num_gpus=2)\n```\n\nThis allows the model to be deployed on two GPUs for inference. You can change `num_gpus` to the number of GPUs you want to use. It is evenly split by default, but you can also pass the `device_map` parameter to specify it yourself."
        },
        {
          "name": "Intel_device_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0869140625,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2024 ChatGLM team @ Zhipu AI\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 5.056640625,
          "content": "The ChatGLM3-6B License\n\n1. å®šä¹‰\n\nâ€œè®¸å¯æ–¹â€æ˜¯æŒ‡åˆ†å‘å…¶è½¯ä»¶çš„ ChatGLM3-6B æ¨¡å‹å›¢é˜Ÿã€‚\n\nâ€œè½¯ä»¶â€æ˜¯æŒ‡æ ¹æ®æœ¬è®¸å¯æä¾›çš„ ChatGLM3-6B æ¨¡å‹å‚æ•°ã€‚\n\n2. è®¸å¯æˆäºˆ\n\næ ¹æ®æœ¬è®¸å¯çš„æ¡æ¬¾å’Œæ¡ä»¶ï¼Œè®¸å¯æ–¹ç‰¹æ­¤æˆäºˆæ‚¨éæ’ä»–æ€§ã€å…¨çƒæ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯å†è®¸å¯ã€å¯æ’¤é”€ã€å…ç‰ˆç¨çš„ç‰ˆæƒè®¸å¯ã€‚\næœ¬è®¸å¯å…è®¸æ‚¨å…è´¹ä½¿ç”¨æœ¬ä»“åº“ä¸­çš„æ‰€æœ‰å¼€æºæ¨¡å‹è¿›è¡Œå­¦æœ¯ç ”ç©¶ï¼Œå¯¹äºå¸Œæœ›å°†æ¨¡å‹ç”¨äºå•†ä¸šç›®çš„çš„ç”¨æˆ·ï¼Œéœ€åœ¨[è¿™é‡Œ](https://open.bigmodel.cn/mla/form)å®Œæˆç™»è®°ã€‚\nç»è¿‡ç™»è®°çš„ç”¨æˆ·å¯ä»¥å…è´¹ä½¿ç”¨æœ¬æ¨¡å‹è¿›è¡Œå•†ä¸šæ´»åŠ¨ï¼Œä½†å¿…é¡»éµå®ˆæœ¬è®¸å¯çš„æ‰€æœ‰æ¡æ¬¾å’Œæ¡ä»¶ã€‚\nä¸Šè¿°ç‰ˆæƒå£°æ˜å’Œæœ¬è®¸å¯å£°æ˜åº”åŒ…å«åœ¨æœ¬è½¯ä»¶çš„æ‰€æœ‰å‰¯æœ¬æˆ–é‡è¦éƒ¨åˆ†ä¸­ã€‚\n\n3.é™åˆ¶\n\næ‚¨ä¸å¾—å‡ºäºä»»ä½•å†›äº‹æˆ–éæ³•ç›®çš„ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹ã€åˆå¹¶ã€å‘å¸ƒã€åˆ†å‘ã€å¤åˆ¶æˆ–åˆ›å»ºæœ¬è½¯ä»¶çš„å…¨éƒ¨æˆ–éƒ¨åˆ†è¡ç”Ÿä½œå“ã€‚\n\næ‚¨ä¸å¾—åˆ©ç”¨æœ¬è½¯ä»¶ä»äº‹ä»»ä½•å±å®³å›½å®¶å®‰å…¨å’Œå›½å®¶ç»Ÿä¸€ï¼Œå±å®³ç¤¾ä¼šå…¬å…±åˆ©ç›ŠåŠå…¬åºè‰¯ä¿—ï¼Œä¾µçŠ¯ä»–äººå•†ä¸šç§˜å¯†ã€çŸ¥è¯†äº§æƒã€åèª‰æƒã€è‚–åƒæƒã€è´¢äº§æƒç­‰æƒç›Šçš„è¡Œä¸ºã€‚\n\næ‚¨åœ¨ä½¿ç”¨ä¸­åº”éµå¾ªä½¿ç”¨åœ°æ‰€é€‚ç”¨çš„æ³•å¾‹æ³•è§„æ”¿ç­–ã€é“å¾·è§„èŒƒç­‰è¦æ±‚ã€‚\n\n4.å…è´£å£°æ˜\n\næœ¬è½¯ä»¶â€œæŒ‰åŸæ ·â€æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¯¹é€‚é”€æ€§ã€ç‰¹å®šç”¨é€”çš„é€‚ç”¨æ€§å’Œéä¾µæƒæ€§çš„ä¿è¯ã€‚ åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œä½œè€…æˆ–ç‰ˆæƒæŒæœ‰äººå‡ä¸å¯¹ä»»ä½•ç´¢èµ”ã€æŸå®³æˆ–å…¶ä»–è´£ä»»è´Ÿè´£ï¼Œæ— è®ºæ˜¯åœ¨åˆåŒè¯‰è®¼ã€ä¾µæƒè¡Œä¸ºè¿˜æ˜¯å…¶ä»–æ–¹é¢ï¼Œç”±è½¯ä»¶æˆ–è½¯ä»¶çš„ä½¿ç”¨æˆ–å…¶ä»–äº¤æ˜“å¼•èµ·ã€ç”±è½¯ä»¶å¼•èµ·æˆ–ä¸ä¹‹ç›¸å…³ è½¯ä»¶ã€‚\n\n5. è´£ä»»é™åˆ¶\n\né™¤é€‚ç”¨æ³•å¾‹ç¦æ­¢çš„èŒƒå›´å¤–ï¼Œåœ¨ä»»ä½•æƒ…å†µä¸‹ä¸”æ ¹æ®ä»»ä½•æ³•å¾‹ç†è®ºï¼Œæ— è®ºæ˜¯åŸºäºä¾µæƒè¡Œä¸ºã€ç–å¿½ã€åˆåŒã€è´£ä»»æˆ–å…¶ä»–åŸå› ï¼Œä»»ä½•è®¸å¯æ–¹å‡ä¸å¯¹æ‚¨æ‰¿æ‹…ä»»ä½•ç›´æ¥ã€é—´æ¥ã€ç‰¹æ®Šã€å¶ç„¶ã€ç¤ºèŒƒæ€§ã€ æˆ–é—´æ¥æŸå®³ï¼Œæˆ–ä»»ä½•å…¶ä»–å•†ä¸šæŸå¤±ï¼Œå³ä½¿è®¸å¯äººå·²è¢«å‘ŠçŸ¥æ­¤ç±»æŸå®³çš„å¯èƒ½æ€§ã€‚\n\n6.äº‰è®®è§£å†³\n\næœ¬è®¸å¯å—ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹ç®¡è¾–å¹¶æŒ‰å…¶è§£é‡Šã€‚ å› æœ¬è®¸å¯å¼•èµ·çš„æˆ–ä¸æœ¬è®¸å¯æœ‰å…³çš„ä»»ä½•äº‰è®®åº”æäº¤åŒ—äº¬å¸‚æµ·æ·€åŒºäººæ°‘æ³•é™¢ã€‚\n\nè¯·æ³¨æ„ï¼Œè®¸å¯è¯å¯èƒ½ä¼šæ›´æ–°åˆ°æ›´å…¨é¢çš„ç‰ˆæœ¬ã€‚ æœ‰å…³è®¸å¯å’Œç‰ˆæƒçš„ä»»ä½•é—®é¢˜ï¼Œè¯·é€šè¿‡ license@zhipuai.cn ä¸æˆ‘ä»¬è”ç³»ã€‚\n\n1. Definitions\n\nâ€œLicensorâ€ means the ChatGLM3-6B Model Team that distributes its Software.\n\nâ€œSoftwareâ€ means the ChatGLM3-6B model parameters made available under this license.\n\n2. License Grant\n\nUnder the terms and conditions of this license, the Licensor hereby grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license.\nThis license permits you to use all open-source models in this repository for academic research free. Users who wish to use the models for commercial purposes must register [here](https://open.bigmodel.cn/mla/form).\nRegistered users may use the models for commercial activities free of charge, but must comply with all terms and conditions of this license.\nThe license notice shall be included in all copies or substantial portions of the Software.\n\n3. Restrictions\nYou are not allowed to use, copy, modify, merge, publish, distribute, copy or create all or part of the derivative works of this software for any military or illegal purposes.\nYou are not allowed to use this software to engage in any behavior that endangers national security and unity, endangers social public interests and public order, infringes on the rights and interests of others such as trade secrets, intellectual property rights, reputation rights, portrait rights, and property rights.\nYou should comply with the applicable laws, regulations, policies, ethical standards, and other requirements in the place of use during use.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of Peopleâ€™s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us at license@zhipuai.cn.\n"
        },
        {
          "name": "PROMPT.md",
          "type": "blob",
          "size": 6.7236328125,
          "content": "## ChatGLM3 å¯¹è¯æ ¼å¼\nä¸ºäº†é¿å…ç”¨æˆ·è¾“å…¥çš„æ³¨å…¥æ”»å‡»ï¼Œä»¥åŠç»Ÿä¸€ Code Interpreterï¼ŒTool & Agent ç­‰ä»»åŠ¡çš„è¾“å…¥ï¼ŒChatGLM3 é‡‡ç”¨äº†å…¨æ–°çš„å¯¹è¯æ ¼å¼ã€‚\n\n### è§„å®š\n#### æ•´ä½“ç»“æ„\nChatGLM3 å¯¹è¯çš„æ ¼å¼ç”±è‹¥å¹²å¯¹è¯ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªå¯¹è¯åŒ…å«å¯¹è¯å¤´å’Œå†…å®¹ï¼Œä¸€ä¸ªå…¸å‹çš„å¤šè½®å¯¹è¯ç»“æ„å¦‚ä¸‹\n```text\n<|system|>\nYou are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\n<|user|>\nHello\n<|assistant|>\nHello, I'm ChatGLM3. What can I assist you today?\n```\n**å®é™…ä¸­æ¯è½®å¯¹è¯å†…å®¹å¹¶ä¸ä¸€å®šä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†ç¾è§‚ï¼Œä¸‹åŒ**\n\n#### å¯¹è¯å¤´\nå¯¹è¯å¤´å å®Œæ•´çš„ä¸€è¡Œï¼Œæ ¼å¼ä¸º\n```text\n<|role|>{metadata}\n```\nå…¶ä¸­ `<|role|>` éƒ¨åˆ†ä½¿ç”¨ special token è¡¨ç¤ºï¼Œæ— æ³•ä»æ–‡æœ¬å½¢å¼è¢« tokenizer ç¼–ç ä»¥é˜²æ­¢æ³¨å…¥ã€‚metadata éƒ¨åˆ†é‡‡ç”¨çº¯æ–‡æœ¬è¡¨ç¤ºï¼Œä¸ºå¯é€‰å†…å®¹ã€‚\n* `<|system|>`ï¼šç³»ç»Ÿä¿¡æ¯ï¼Œè®¾è®¡ä¸Šå¯ç©¿æ’äºå¯¹è¯ä¸­ï¼Œ**ä½†ç›®å‰è§„å®šä»…å¯ä»¥å‡ºç°åœ¨å¼€å¤´**\n* `<|user|>`ï¼šç”¨æˆ·\n  - ä¸ä¼šè¿ç»­å‡ºç°å¤šä¸ªæ¥è‡ª `<|user|>` çš„ä¿¡æ¯\n* `<|assistant|>`ï¼šAI åŠ©æ‰‹\n  - åœ¨å‡ºç°ä¹‹å‰å¿…é¡»æœ‰ä¸€ä¸ªæ¥è‡ª `<|user|>` çš„ä¿¡æ¯\n* `<|observation|>`ï¼šå¤–éƒ¨çš„è¿”å›ç»“æœ\n  - å¿…é¡»åœ¨ `<|assistant|>` çš„ä¿¡æ¯ä¹‹å\n\n### æ ·ä¾‹åœºæ™¯\n\nä¸ºæå‡å¯è¯»æ€§ï¼Œä¸‹åˆ—æ ·ä¾‹åœºæ™¯ä¸­è¡¨ç¤ºè§’è‰²çš„ special token å‰å‡é¢å¤–æ·»åŠ äº†ä¸€ä¸ªæ¢è¡Œç¬¦ã€‚å®é™…ä½¿ç”¨åŠ tokenizer å®ç°ä¸­å‡æ— éœ€é¢å¤–æ·»åŠ è¿™ä¸€æ¢è¡Œã€‚\n\n#### å¤šè½®å¯¹è¯\n* æœ‰ä¸”ä»…æœ‰ `<|user|>`ã€`<|assistant|>`ã€`<|system|>` ä¸‰ç§ role\n```text\n<|system|>\nYou are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\n<|user|>\nHello\n<|assistant|>\nHello, I'm ChatGLM3. What can I assist you today?\n```\n\n#### å·¥å…·è°ƒç”¨\n````\n<|system|>\nAnswer the following questions as best as you can. You have access to the following tools:\n[\n    {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"unit\": {\"type\": \"string\"},\n            },\n            \"required\": [\"location\"],\n        },\n    }\n]\n<|user|>\nä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\n<|assistant|>\nå¥½çš„ï¼Œè®©æˆ‘ä»¬æ¥æŸ¥çœ‹ä»Šå¤©çš„å¤©æ°”\n<|assistant|>get_current_weather\n```python\ntool_call(location=\"beijing\", unit=\"celsius\")\n```\n<|observation|>\n{\"temperature\": 22}\n<|assistant|>\næ ¹æ®æŸ¥è¯¢ç»“æœï¼Œä»Šå¤©åŒ—äº¬çš„æ°”æ¸©ä¸º 22 æ‘„æ°åº¦ã€‚\n````\n\n#### ä»£ç æ‰§è¡Œ\n* æœ‰ `<|user|>`ã€`<|assistant|>`ã€`<|system|>`ã€`<|observation|>` å››ç§ roleã€‚å…¶ä¸­ `<|assistant|>` çš„ metadata åªæœ‰ interpreterã€‚\n\n`````text\n<|system|>\nä½ æ˜¯ä¸€ä½æ™ºèƒ½AIåŠ©æ‰‹ï¼Œä½ å«ChatGLM3ï¼Œä½ è¿æ¥ç€ä¸€å°ç”µè„‘ï¼Œä½†è¯·æ³¨æ„ä¸èƒ½è”ç½‘ã€‚åœ¨ä½¿ç”¨Pythonè§£å†³ä»»åŠ¡æ—¶ï¼Œä½ å¯ä»¥è¿è¡Œä»£ç å¹¶å¾—åˆ°ç»“æœï¼Œå¦‚æœè¿è¡Œç»“æœæœ‰é”™è¯¯ï¼Œä½ éœ€è¦å°½å¯èƒ½å¯¹ä»£ç è¿›è¡Œæ”¹è¿›ã€‚ä½ å¯ä»¥å¤„ç†ç”¨æˆ·ä¸Šä¼ åˆ°ç”µè„‘ä¸Šçš„æ–‡ä»¶ï¼Œæ–‡ä»¶é»˜è®¤å­˜å‚¨è·¯å¾„æ˜¯/mnt/data/ã€‚\n<|user|>\n#File: /mnt/data/metadata.jsonl\n#Size: 35380\n#File uploaded\næ–‡ä»¶ä¸­æ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼ï¼Ÿ\n<|assistant|>\né¦–å…ˆï¼Œæˆ‘å°†è¯»å–æ‚¨æä¾›çš„æ–‡ä»¶ï¼Œå¹¶æŸ¥çœ‹å…¶å†…å®¹ä»¥ç¡®å®šæ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼ã€‚\n\næˆ‘ä»¬ä»åŠ è½½æ–‡ä»¶å¼€å§‹ï¼š\n<|assistant|>interpreter\n```python\nimport json\n\n# Load the uploaded file\ndata = []\nwith open(\"/mnt/data/metadata.jsonl\", 'r') as file:\n    for line in file:\n        data.append(json.loads(line))\n\n# Display the first few records to get a sense of the data\ndata[:5]\n```\n<|observation|>\n```result\n[{'file_name': 'aceinthehole.png',\n  'name': 'Ace in the Hole',\n  'type': 'survivor',\n  'description': 'Lady Luck always seems to be throwing something good your way.'},\n {'file_name': 'adrenaline.png',\n  'name': 'Adrenaline',\n  'type': 'survivor',\n  'description': 'You are fuelled by unexpected energy when on the verge of escape.'},\n {'file_name': 'aftercare.png',\n  'name': 'Aftercare',\n  'type': 'survivor',\n  'description': \"Unlocks potential in one's aura reading ability.\"},\n {'file_name': 'agitation.png',\n  'name': 'Agitation',\n  'type': 'killer',\n  'description': 'You get excited in anticipation of hooking your prey.'},\n {'file_name': 'alert.png',\n  'name': 'Alert',\n  'type': 'survivor',\n  'description': 'Your acute senses are on high alert.'}]\n```\n<|assistant|>\nè¯¥æ–‡ä»¶çœ‹èµ·æ¥åŒ…å«æœ‰å…³æŸäº›æ¡ç›®çš„å…ƒæ•°æ®ï¼Œæ¯ä¸ªæ¡ç›®æœ‰ä»¥ä¸‹å­—æ®µï¼š\n\n\n- `file_name`: æ–‡ä»¶åç§°\n- `name`: åç§°\n- `type`: ç±»å‹ï¼ˆä¾‹å¦‚ \"survivor\" æˆ– \"killer\"ï¼‰\n- `description`: æè¿°\n\næ¥ä¸‹æ¥ï¼Œæˆ‘å°†æ£€æŸ¥è¿™äº›å­—æ®µæ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼ã€‚\n<|assistant|>interpreter\n```python\n# Check for missing values in each column\nmissing_values = {key: sum(1 for item in data if key not in item) for key in data[0].keys()}\n\nmissing_values\n```\n<|observation|>\n```result\n{'file_name': 0, 'name': 0, 'type': 0, 'description': 0}\n```\n<|assistant|>\næ¯ä¸ªå­—æ®µéƒ½æ²¡æœ‰ç¼ºå¤±å€¼ã€‚è¿™æ„å‘³ç€æ¯ä¸ªæ¡ç›®éƒ½å®Œæ•´ã€‚\n\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»»ä½•æ½œåœ¨çš„å¼‚å¸¸å€¼ã€‚è¿™é€šå¸¸éœ€è¦æ›´æ·±å…¥çš„åˆ†æå’Œå¯¹æ•°æ®çš„äº†è§£ï¼Œä½†æˆ‘å¯ä»¥ä»ä¸€äº›åŸºæœ¬çš„ç»Ÿè®¡å’Œåˆ†å¸ƒå¼€å§‹è¿›è¡Œæ£€æŸ¥ã€‚ä¸ºäº†è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹â€œtypeâ€å­—æ®µçš„å€¼åˆ†å¸ƒï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•æ„å¤–çš„ç±»åˆ«ã€‚\n<|assistant|>interpreter\n```python\n# Check the distribution of values in the 'type' column\ntype_distribution = {key: sum(1 for item in data if item['type'] == key) for key in set(item['type'] for item in data)}\n\ntype_distribution\n```\n<|observation|>\n```result\n{'survivor': 116, 'killer': 103}\n```\n<|assistant|>\nâ€œtypeâ€å­—æ®µæœ‰ä¸¤ä¸ªå€¼ï¼šâ€œsurvivorâ€å’Œâ€œkillerâ€ã€‚å…¶ä¸­ï¼Œâ€œsurvivorâ€æœ‰116ä¸ªæ¡ç›®ï¼Œè€Œâ€œkillerâ€æœ‰103ä¸ªæ¡ç›®ã€‚\n\nä»è¿™ä¸ªåˆæ­¥çš„æ£€æŸ¥ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰å‘ç°ä»»ä½•æ˜æ˜¾çš„å¼‚å¸¸å€¼ã€‚\n\nå¦‚æœæ‚¨æœ‰ä»»ä½•ç‰¹å®šçš„æ ‡å‡†æˆ–å…¶ä»–å­—æ®µéœ€è¦æ£€æŸ¥ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ›´è¯¦ç»†çš„åˆ†æã€‚æ‚¨è¿˜å¸Œæœ›æˆ‘æ£€æŸ¥ä»€ä¹ˆå‘¢ï¼Ÿ\n<|user|>\nå¸®æˆ‘ç”»ä¸€ä¸ªçˆ±å¿ƒ\n<|assistant|>interpreter\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# çˆ±å¿ƒå½¢çŠ¶çš„å‚æ•°æ–¹ç¨‹\ndef heart(t):\n    x = 16 * np.sin(t) ** 3\n    y = 13 * np.cos(t) - 5 * np.cos(2 * t) - 2 * np.cos(3 * t) - np.cos(4 * t)\n    return x, y\n\nt = np.linspace(0, 2 * np.pi, 1000)\nx, y = heart(t)\n\nplt.figure(figsize=(6, 6))\nplt.plot(x, y, color='red')\nplt.axis('equal')\nplt.axis('off')\nplt.show()\n```\n<|observation|>\n```result\n[Image]\n```\n<|assistant|>\nè¿™æ˜¯ä¸€ä¸ªçˆ±å¿ƒå½¢çŠ¶ã€‚æˆ‘ä½¿ç”¨äº†å‚æ•°æ–¹ç¨‹æ¥æè¿°è¿™ä¸ªå½¢çŠ¶ï¼Œå¹¶ä½¿ç”¨matplotlibè¿›è¡Œäº†ç»˜åˆ¶ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•å…¶ä»–éœ€æ±‚æˆ–é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚\n<|user|>  # End\n````"
        },
        {
          "name": "PROMPT_en.md",
          "type": "blob",
          "size": 6.951171875,
          "content": "## ChatGLM3 Chat Format\nTo avoid injection attacks from user input, and to unify the input of Code Interpreter, Tool & Agent and other tasks, ChatGLM3 adopts a brand-new dialogue format.\n\n### Regulations\n#### Overall Structure\nThe format of the ChatGLM3 dialogue consists of several conversations, each of which contains a dialogue header and content. A typical multi-turn dialogue structure is as follows:\n```text\n<|system|>\nYou are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\n<|user|>\nHello\n<|assistant|>\nHello, I'm ChatGLM3. What can I assist you today?\n```\n\n#### Chat Header\nThe chat header occupies a complete line, formatted as:\n```text\n<|role|>{metadata}\n```\nWhere `<|role|>` part is represented in a special token,  which canâ€™t be encoded by the tokenizer from the text form to prevent injection attacks. The `metadata` part is represented in plain texts and is optional content.\n* `<|system|>`: System information, which can be interspersed in the dialogue in design, **but currently only appears at the beginning**\n* `<|user|>`: User\n  - Multiple messages from `<|user|>` will not appear continuously\n* `<|assistant|>`: AI assistant\n  - There must be a message from `<|user|>` before it appears\n* `<|observation|>`: External return result\n  - Must be after the message from `<|assistant|>`\n\n### Example Scenarios\n\nFor better readability, an extra `\\n` is added before each role special token. This extra `\\n` should not be added in actual use and tokenizer implementation.\n\n#### Multi-turn Dialogue\n* There are only three roles: `<|user|>`, `<|assistant|>`, and `<|system|>`.\n```text\n<|system|>\nYou are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\n<|user|>\nHello\n<|assistant|>\nHello, I'm ChatGLM3. What can I assist you today?\n```\n\n#### Tool Calling\n````\n<|system|>\nAnswer the following questions as best as you can. You have access to the following tools:\n[\n    {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"unit\": {\"type\": \"string\"},\n            },\n            \"required\": [\"location\"],\n        },\n    }\n]\n<|user|>\nWhat's the weather in Beijing today?\n<|assistant|>\nOkay, let's look up the weather in Bejing today.\n<|assistant|>get_current_weather\n```python\ntool_call(location=\"beijing\", unit=\"celsius\")\n```\n<|observation|>\n{\"temperature\": 22}\n<|assistant|>\nAccording to the query results, the temperature in Beijing today is 22 degrees Celsius.\n````\n\n#### Code Execution\n* Four available roles: `<|user|>`, `<|assistant|>`, `<|system|>`, and `<|observation|>`, where `metadata` of `<|assistant|>` can only be interpreter.\n\n`````text\n<|system|>\nYou are an intelligent AI assistant, named ChatGLM3. You are connected to a computer, but please note that you cannot connect to the internet. When using Python to solve tasks, you can run the code and get the results. If there are errors in the running results, you need to improve the code as much as possible. You can process files uploaded by users to your computer, and the default storage path for files is /mnt/data/.\n<|user|>\n#File: /mnt/data/metadata.jsonl\n#Size: 35380\n#File uploaded\nAre there missing or abnormal values in the file?\n<|assistant|>\nFirstly, I will read the file you provided and review its contents to determine if there are missing or abnormal values.\n\nWe start by loading the file:\n<|assistant|>interpreter\n```python\nimport json\n\n# Load the uploaded file\ndata = []\nwith open(\"/mnt/data/metadata.jsonl\", 'r') as file:\n    for line in file:\n        data.append(json.loads(line))\n\n# Display the first few records to get a sense of the data\ndata[:5]\n```\n<|observation|>\n```result\n[{'file_name': 'aceinthehole.png',\n  'name': 'Ace in the Hole',\n  'type': 'survivor',\n  'description': 'Lady Luck always seems to be throwing something good your way.'},\n {'file_name': 'adrenaline.png',\n  'name': 'Adrenaline',\n  'type': 'survivor',\n  'description': 'You are fuelled by unexpected energy when on the verge of escape.'},\n {'file_name': 'aftercare.png',\n  'name': 'Aftercare',\n  'type': 'survivor',\n  'description': \"Unlocks potential in one's aura reading ability.\"},\n {'file_name': 'agitation.png',\n  'name': 'Agitation',\n  'type': 'killer',\n  'description': 'You get excited in anticipation of hooking your prey.'},\n {'file_name': 'alert.png',\n  'name': 'Alert',\n  'type': 'survivor',\n  'description': 'Your acute senses are on high alert.'}]\n```\n<|assistant|>\nThis file appears to contain metadata about certain items, each with the following fields:\n\n- `file_name`\n- `name`\n- `type` (e.g. \"survivor\" or \"killer\")\n- `description`\n\nNext, I will check for missing values in these fields.\n<|assistant|>interpreter\n```python\n# Check for missing values in each column\nmissing_values = {key: sum(1 for item in data if key not in item) for key in data[0].keys()}\n\nmissing_values\n```\n<|observation|>\n```result\n{'file_name': 0, 'name': 0, 'type': 0, 'description': 0}\n```\n<|assistant|>\nEach field has no missing values. This means that each entry is complete.\n\nNext, we can check for any potential outliers. This usually requires deeper analysis and understanding of the data, but I can start by examining some basic statistics and distributions. To do this, we can check the value distribution of the \"type\" field to see if there are any unexpected categories.\n<|assistant|>interpreter\n```python\n# Check the distribution of values in the 'type' column\ntype_distribution = {key: sum(1 for item in data if item['type'] == key) for key in set(item['type'] for item in data)}\n\ntype_distribution\n```\n<|observation|>\n```result\n{'survivor': 116, 'killer': 103}\n```\n<|assistant|>\nThe 'type' field has two values: 'survivor' and 'killer'. Among them, 'survivor' has 116 entries, while 'killer' has 103 entries.\n\nFrom this preliminary examination, we did not find any obvious abnormal values.\n\nIf you have any specific standards or other fields that need to be checked, I can provide you with a more detailed analysis. What else do you want me to check?\n<|user|>\nHelp me draw a heart\n<|assistant|>interpreter\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# The parametric equation for the shape of a heart\ndef heart(t):\n    x = 16 * np.sin(t) ** 3\n    y = 13 * np.cos(t) - 5 * np.cos(2 * t) - 2 * np.cos(3 * t) - np.cos(4 * t)\n    return x, y\n\nt = np.linspace(0, 2 * np.pi, 1000)\nx, y = heart(t)\n\nplt.figure(figsize=(6, 6))\nplt.plot(x, y, color='red')\nplt.axis('equal')\nplt.axis('off')\nplt.show()\n```\n<|observation|>\n```result\n[Image]\n```\n<|assistant|>\nThis is a shape of heart. I used parametric equations to describe this shape and drew it using Matplotlib. If you have any other needs or questions, please feel free to let me know.\n<|user|>  # End\n````\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.6591796875,
          "content": "# ChatGLM3\n\n<p align=\"center\">\nğŸ“„<a href=\"https://arxiv.org/pdf/2406.12793\" target=\"_blank\"> Report </a> â€¢ ğŸ¤— <a href=\"https://huggingface.co/THUDM/chatglm3-6b\" target=\"_blank\">HF Repo</a> â€¢ ğŸ¤– <a href=\"https://modelscope.cn/models/ZhipuAI/chatglm3-6b\" target=\"_blank\">ModelScope</a> â€¢ ğŸŸ£ <a href=\"https://www.wisemodel.cn/models/ZhipuAI/chatglm3-6b\" target=\"_blank\">WiseModel</a> â€¢ ğŸ“” <a href=\"https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof\" target=\"_blank\">Document</a> â€¢  ğŸ§° <a href=\"https://openxlab.org.cn/models/hot/THUDM\" target=\"_blank\">OpenXLab</a> â€¢ ğŸ¦ <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a><br>\n</p>\n<p align=\"center\">\n    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href=\"https://discord.gg/fK2dz4bg\" target=\"_blank\">Discord</a> å’Œ <a href=\"resources/WECHAT.md\" target=\"_blank\">å¾®ä¿¡</a>\n</p>\n<p align=\"center\">\nğŸ“åœ¨ <a href=\"https://www.chatglm.cn\">chatglm.cn</a> ä½“éªŒæ›´å¤§è§„æ¨¡çš„ ChatGLM æ¨¡å‹ã€‚\n</p>\n\n[Read this in English.](./README_en.md)\n\nğŸ“” å…³äº`ChatGLM3-6B` æ›´ä¸ºè¯¦ç»†çš„ä½¿ç”¨ä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒ\n\n+ [ChatGLM3 å¼€æ”¾æŠ€æœ¯æ–‡æ¡£](https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof?from=from_copylink)\n+ [Bilibili video](https://www.bilibili.com/video/BV1uC4y1J7yA)\n+ [YouTube video](https://www.youtube.com/watch?v=Pw9PB6R7ORA)\n\n## GLM-4 å¼€æºæ¨¡å‹å’ŒAPI\n\næˆ‘ä»¬å·²ç»å‘å¸ƒæœ€æ–°çš„ **GLM-4** æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæœ‰äº†æ–°çš„çªç ´ï¼Œæ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ¸ é“ä½“éªŒæˆ‘ä»¬çš„æœ€æ–°æ¨¡å‹ã€‚\n\n+ [GLM-4 å¼€æºæ¨¡å‹](https://github.com/THUDM/GLM-4) æˆ‘ä»¬å·²ç»å¼€æºäº† GLM-4-9B ç³»åˆ—æ¨¡å‹ï¼Œåœ¨å„é¡¹æŒ‡æ ‡çš„æµ‹è¯•ä¸Šæœ‰æ˜æ˜¾æå‡ï¼Œæ¬¢è¿å°è¯•ã€‚\n+ [æ™ºè°±æ¸…è¨€](https://chatglm.cn/main/detail?fr=ecology_x) ä½“éªŒæœ€æ–°ç‰ˆ GLM-4ï¼ŒåŒ…æ‹¬ **GLMsï¼ŒAll tools**ç­‰åŠŸèƒ½ã€‚\n+ [APIå¹³å°](https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9) æ–°ä¸€ä»£ API å¹³å°å·²ç»ä¸Šçº¿ï¼Œæ‚¨å¯ä»¥ç›´æ¥åœ¨\n  API\n  å¹³å°ä¸Šä½“éªŒ `GLM-4-0520`ã€`GLM-4-air`ã€`GLM-4-airx`ã€`GLM-4-flash`ã€`GLM-4`ã€`GLM-3-Turbo`ã€`CharacterGLM-3`ï¼Œ`CogView-3`\n  ç­‰æ–°æ¨¡å‹ã€‚\n  å…¶ä¸­`GLM-4`ã€`GLM-3-Turbo`ä¸¤ä¸ªæ¨¡å‹æ”¯æŒäº† `System Prompt`ã€`Function Call`ã€ `Retrieval`ã€`Web_Search`ç­‰æ–°åŠŸèƒ½ï¼Œæ¬¢è¿ä½“éªŒã€‚\n\n+ [GLM-4 API å¼€æºæ•™ç¨‹](https://github.com/MetaGLM/glm-cookbook/) GLM-4 APIæ•™ç¨‹å’ŒåŸºç¡€åº”ç”¨ï¼Œæ¬¢è¿å°è¯•ã€‚\n  APIç›¸å…³é—®é¢˜å¯ä»¥åœ¨æœ¬å¼€æºæ•™ç¨‹ç–‘é—®ï¼Œæˆ–è€…ä½¿ç”¨ [GLM-4 API AIåŠ©æ‰‹](https://open.bigmodel.cn/shareapp/v1/?share_code=sQwt5qyqYVaNh1O_87p8O)\n  æ¥è·å¾—å¸¸è§é—®é¢˜çš„å¸®åŠ©ã€‚\n\n-----\n\n## ChatGLM3 ä»‹ç»\n\n**ChatGLM3** æ˜¯æ™ºè°±AIå’Œæ¸…åå¤§å­¦ KEG å®éªŒå®¤è”åˆå‘å¸ƒçš„å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚ChatGLM3-6B æ˜¯ ChatGLM3\nç³»åˆ—ä¸­çš„å¼€æºæ¨¡å‹ï¼Œåœ¨ä¿ç•™äº†å‰ä¸¤ä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›ä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¸Šï¼ŒChatGLM3-6B å¼•å…¥äº†å¦‚ä¸‹ç‰¹æ€§ï¼š\n\n1. **æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼š** ChatGLM3-6B çš„åŸºç¡€æ¨¡å‹ ChatGLM3-6B-Base\n   é‡‡ç”¨äº†æ›´å¤šæ ·çš„è®­ç»ƒæ•°æ®ã€æ›´å……åˆ†çš„è®­ç»ƒæ­¥æ•°å’Œæ›´åˆç†çš„è®­ç»ƒç­–ç•¥ã€‚åœ¨è¯­ä¹‰ã€æ•°å­¦ã€æ¨ç†ã€ä»£ç ã€çŸ¥è¯†ç­‰ä¸åŒè§’åº¦çš„æ•°æ®é›†ä¸Šæµ‹è¯„æ˜¾ç¤ºï¼Œ*\n   *ChatGLM3-6B-Base å…·æœ‰åœ¨ 10B ä»¥ä¸‹çš„åŸºç¡€æ¨¡å‹ä¸­æœ€å¼ºçš„æ€§èƒ½**ã€‚\n2. **æ›´å®Œæ•´çš„åŠŸèƒ½æ”¯æŒï¼š** ChatGLM3-6B é‡‡ç”¨äº†å…¨æ–°è®¾è®¡çš„ [Prompt æ ¼å¼](PROMPT.md)\n   ï¼Œé™¤æ­£å¸¸çš„å¤šè½®å¯¹è¯å¤–ã€‚åŒæ—¶åŸç”Ÿæ”¯æŒ[å·¥å…·è°ƒç”¨](tools_using_demo/README.md)ï¼ˆFunction Callï¼‰ã€ä»£ç æ‰§è¡Œï¼ˆCode Interpreterï¼‰å’Œ\n   Agent ä»»åŠ¡ç­‰å¤æ‚åœºæ™¯ã€‚\n3. **æ›´å…¨é¢çš„å¼€æºåºåˆ—ï¼š** é™¤äº†å¯¹è¯æ¨¡å‹ [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b)\n   å¤–ï¼Œè¿˜å¼€æºäº†åŸºç¡€æ¨¡å‹ [ChatGLM3-6B-Base](https://huggingface.co/THUDM/chatglm3-6b-base)\n   ã€é•¿æ–‡æœ¬å¯¹è¯æ¨¡å‹ [ChatGLM3-6B-32K](https://huggingface.co/THUDM/chatglm3-6b-32k)\n   å’Œè¿›ä¸€æ­¥å¼ºåŒ–äº†å¯¹äºé•¿æ–‡æœ¬ç†è§£èƒ½åŠ›çš„ [ChatGLM3-6B-128K](https://huggingface.co/THUDM/chatglm3-6b-128k)ã€‚ä»¥ä¸Šæ‰€æœ‰æƒé‡å¯¹å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾**\n   ï¼Œåœ¨å¡«å†™ [é—®å·](https://open.bigmodel.cn/mla/form) è¿›è¡Œç™»è®°å**äº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨**ã€‚\n\n-----\n\nChatGLM3 å¼€æºæ¨¡å‹æ—¨åœ¨ä¸å¼€æºç¤¾åŒºä¸€èµ·æ¨åŠ¨å¤§æ¨¡å‹æŠ€æœ¯å‘å±•ï¼Œæ³è¯·å¼€å‘è€…å’Œå¤§å®¶éµå®ˆ [å¼€æºåè®®](MODEL_LICENSE)\nï¼Œå‹¿å°†å¼€æºæ¨¡å‹å’Œä»£ç åŠåŸºäºå¼€æºé¡¹ç›®äº§ç”Ÿçš„è¡ç”Ÿç‰©ç”¨äºä»»ä½•å¯èƒ½ç»™å›½å®¶å’Œç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ä»¥åŠç”¨äºä»»ä½•æœªç»è¿‡å®‰å…¨è¯„ä¼°å’Œå¤‡æ¡ˆçš„æœåŠ¡ã€‚ç›®å‰ï¼Œæœ¬é¡¹ç›®å›¢é˜ŸæœªåŸºäº\n**ChatGLM3 å¼€æºæ¨¡å‹**å¼€å‘ä»»ä½•åº”ç”¨ï¼ŒåŒ…æ‹¬ç½‘é¡µç«¯ã€å®‰å“ã€è‹¹æœ iOS åŠ Windows App ç­‰åº”ç”¨ã€‚\n\nå°½ç®¡æ¨¡å‹åœ¨è®­ç»ƒçš„å„ä¸ªé˜¶æ®µéƒ½å°½åŠ›ç¡®ä¿æ•°æ®çš„åˆè§„æ€§å’Œå‡†ç¡®æ€§ï¼Œä½†ç”±äº ChatGLM3-6B\næ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä¸”æ¨¡å‹å—æ¦‚ç‡éšæœºæ€§å› ç´ å½±å“ï¼Œæ— æ³•ä¿è¯è¾“å‡ºå†…å®¹çš„å‡†ç¡®ã€‚åŒæ—¶æ¨¡å‹çš„è¾“å‡ºå®¹æ˜“è¢«ç”¨æˆ·çš„è¾“å…¥è¯¯å¯¼ã€‚*\n*æœ¬é¡¹ç›®ä¸æ‰¿æ‹…å¼€æºæ¨¡å‹å’Œä»£ç å¯¼è‡´çš„æ•°æ®å®‰å…¨ã€èˆ†æƒ…é£é™©æˆ–å‘ç”Ÿä»»ä½•æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­ã€ä¸å½“åˆ©ç”¨è€Œäº§ç”Ÿçš„é£é™©å’Œè´£ä»»ã€‚**\n\n## æ¨¡å‹åˆ—è¡¨\n                      \n|      Model       | Seq Length |                                                                                                                                              Download                                                                                                                                    |\n|:----------------:|:----------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n|   ChatGLM3-6B    |     8k     |        [HuggingFace](https://huggingface.co/THUDM/chatglm3-6b) \\| [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b) \\| [WiseModel](https://www.wisemodel.cn/models/ZhipuAI/chatglm3-6b)         \\| [OpenXLab](https://openxlab.org.cn/models/detail/THUDM/chatglm3-6b)      |\n| ChatGLM3-6B-Base |     8k     | [HuggingFace](https://huggingface.co/THUDM/chatglm3-6b-base) \\| [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-base) \\| [WiseModel](https://www.wisemodel.cn/models/ZhipuAI/chatglm3-6b-base) \\| [OpenXLabl](https://openxlab.org.cn/models/detail/THUDM/chatglm3-6b-base)|\n| ChatGLM3-6B-32K  |    32k     |  [HuggingFace](https://huggingface.co/THUDM/chatglm3-6b-32k) \\| [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-32k) \\| [WiseModel](https://www.wisemodel.cn/models/ZhipuAI/chatglm3-6b-32k)   \\| [OpenXLab](https://openxlab.org.cn/models/detail/THUDM/chatglm3-6b-32k)  |\n| ChatGLM3-6B-128K |    128k    |     [HuggingFace](https://huggingface.co/THUDM/chatglm3-6b-128k) ï½œ [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-128k)\\| [OpenXLab](https://openxlab.org.cn/models/detail/THUDM/chatglm3-6b-128k) |\n\n<br> è¯·æ³¨æ„ï¼Œæ‰€æœ‰æ¨¡å‹çš„æœ€æ–°æ›´æ–°éƒ½ä¼šåœ¨ Huggingface ç‡å…ˆå‘å¸ƒã€‚ ModelScope å’Œ WiseModel ç”±äºæ²¡æœ‰ä¸ Huggingface åŒæ­¥ï¼Œéœ€è¦å¼€å‘äººå‘˜æ‰‹åŠ¨æ›´æ–°ï¼Œå¯èƒ½ä¼šåœ¨\nHuggingface æ›´æ–°åä¸€æ®µæ—¶é—´å†…åŒæ­¥æ›´æ–°ã€‚\n\n## å‹æƒ…é“¾æ¥\n\nä»¥ä¸‹ä¼˜ç§€å¼€æºä»“åº“å·²ç»å¯¹ ChatGLM3-6B æ¨¡å‹æ·±åº¦æ”¯æŒï¼Œæ¬¢è¿å¤§å®¶æ‰©å±•å­¦ä¹ ã€‚\n\næ¨ç†åŠ é€Ÿï¼š\n\n* [chatglm.cpp](https://github.com/li-plus/chatglm.cpp): ç±»ä¼¼ llama.cpp çš„é‡åŒ–åŠ é€Ÿæ¨ç†æ–¹æ¡ˆï¼Œå®ç°ç¬”è®°æœ¬ä¸Šå®æ—¶å¯¹è¯\n* [ChatGLM3-TPU](https://github.com/sophgo/ChatGLM3-TPU): é‡‡ç”¨TPUåŠ é€Ÿæ¨ç†æ–¹æ¡ˆï¼Œåœ¨ç®—èƒ½ç«¯ä¾§èŠ¯ç‰‡BM1684Xï¼ˆ16T@FP16ï¼Œå†…å­˜16Gï¼‰ä¸Šå®æ—¶è¿è¡Œçº¦7.5 token/s\n* [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main):\n  NVIDIAå¼€å‘çš„é«˜æ€§èƒ½ GPU åŠ é€Ÿæ¨ç†æ–¹æ¡ˆï¼Œå¯ä»¥å‚è€ƒæ­¤ [æ­¥éª¤](./tensorrt_llm_demo/README.md) éƒ¨ç½² ChatGLM3-6B æ¨¡å‹\n* [OpenVINO](https://github.com/openvinotoolkit): \nIntel å¼€å‘çš„é«˜æ€§èƒ½ CPU å’Œ GPU åŠ é€Ÿæ¨ç†æ–¹æ¡ˆï¼Œå¯ä»¥å‚è€ƒæ­¤ [æ­¥éª¤](./Intel_device_demo/openvino_demo/README.md) éƒ¨ç½² ChatGLM3-6B æ¨¡å‹\n\né«˜æ•ˆå¾®è°ƒï¼š\n\n* [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory): ä¼˜ç§€æ˜“ä¸Šæ‰‹çš„é«˜æ•ˆå¾®è°ƒæ¡†æ¶ã€‚\n\nåº”ç”¨æ¡†æ¶ï¼š\n\n* [LangChain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat): åŸºäº ChatGLM ç­‰å¤§è¯­è¨€æ¨¡å‹ä¸ Langchain\n  ç­‰åº”ç”¨æ¡†æ¶å®ç°ï¼Œå¼€æºã€å¯ç¦»çº¿éƒ¨ç½²çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å¤§æ¨¡å‹çŸ¥è¯†åº“é¡¹ç›®ã€‚\n\n* [BISHENG](https://github.com/dataelement/bisheng): å¼€æºå¤§æ¨¡å‹åº”ç”¨å¼€å‘å¹³å°,èµ‹èƒ½å’ŒåŠ é€Ÿå¤§æ¨¡å‹åº”ç”¨å¼€å‘è½åœ°ï¼Œå¸®åŠ©ç”¨æˆ·ä»¥æœ€ä½³ä½“éªŒè¿›å…¥ä¸‹ä¸€ä»£åº”ç”¨å¼€å‘æ¨¡å¼ã€‚\n\n* [RAGFlow](https://github.com/infiniflow/ragflow): RAGFlow æ˜¯ä¸€æ¬¾åŸºäºæ·±åº¦æ–‡æ¡£ç†è§£æ„å»ºçš„å¼€æº RAGï¼ˆRetrieval-Augmented Generationï¼‰å¼•æ“ã€‚å¯ä¸ºå„ç§è§„æ¨¡çš„ä¼ä¸šåŠä¸ªäººæä¾›ä¸€å¥—ç²¾ç®€çš„ RAG å·¥ä½œæµç¨‹ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é’ˆå¯¹ç”¨æˆ·å„ç±»ä¸åŒçš„å¤æ‚æ ¼å¼æ•°æ®æä¾›å¯é çš„é—®ç­”ä»¥åŠæœ‰ç†æœ‰æ®çš„å¼•ç”¨ã€‚\n\n## è¯„æµ‹ç»“æœ\n\n### å…¸å‹ä»»åŠ¡\n\næˆ‘ä»¬é€‰å–äº† 8 ä¸ªä¸­è‹±æ–‡å…¸å‹æ•°æ®é›†ï¼Œåœ¨ ChatGLM3-6B (base) ç‰ˆæœ¬ä¸Šè¿›è¡Œäº†æ€§èƒ½æµ‹è¯•ã€‚\n\n| Model            | GSM8K | MATH | BBH  | MMLU | C-Eval | CMMLU | MBPP | AGIEval |\n|------------------|:-----:|:----:|:----:|:----:|:------:|:-----:|:----:|:-------:|\n| ChatGLM2-6B-Base | 32.4  | 6.5  | 33.7 | 47.9 |  51.7  | 50.0  |  -   |    -    |\n| Best Baseline    | 52.1  | 13.1 | 45.0 | 60.1 |  63.5  | 62.2  | 47.5 |  45.8   |\n| ChatGLM3-6B-Base | 72.3  | 25.7 | 66.1 | 61.4 |  69.0  | 67.5  | 52.4 |  53.7   |\n\n> Best Baseline æŒ‡çš„æ˜¯æˆªæ­¢ 2023å¹´10æœˆ27æ—¥ã€æ¨¡å‹å‚æ•°åœ¨ 10B ä»¥ä¸‹ã€åœ¨å¯¹åº”æ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸åŒ…æ‹¬åªé’ˆå¯¹æŸä¸€é¡¹ä»»åŠ¡è®­ç»ƒè€Œæœªä¿æŒé€šç”¨èƒ½åŠ›çš„æ¨¡å‹ã€‚\n\n> å¯¹ ChatGLM3-6B-Base çš„æµ‹è¯•ä¸­ï¼ŒBBH é‡‡ç”¨ 3-shot æµ‹è¯•ï¼Œéœ€è¦æ¨ç†çš„ GSM8Kã€MATH é‡‡ç”¨ 0-shot CoT æµ‹è¯•ï¼ŒMBPP é‡‡ç”¨ 0-shot\n> ç”Ÿæˆåè¿è¡Œæµ‹ä¾‹è®¡ç®— Pass@1 ï¼Œå…¶ä»–é€‰æ‹©é¢˜ç±»å‹æ•°æ®é›†å‡é‡‡ç”¨ 0-shot æµ‹è¯•ã€‚\n\næˆ‘ä»¬åœ¨å¤šä¸ªé•¿æ–‡æœ¬åº”ç”¨åœºæ™¯ä¸‹å¯¹ ChatGLM3-6B-32K è¿›è¡Œäº†äººå·¥è¯„ä¼°æµ‹è¯•ã€‚ä¸äºŒä»£æ¨¡å‹ç›¸æ¯”ï¼Œå…¶æ•ˆæœå¹³å‡æå‡äº†è¶…è¿‡\n50%ã€‚åœ¨è®ºæ–‡é˜…è¯»ã€æ–‡æ¡£æ‘˜è¦å’Œè´¢æŠ¥åˆ†æç­‰åº”ç”¨ä¸­ï¼Œè¿™ç§æå‡å°¤ä¸ºæ˜¾è‘—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨ LongBench è¯„æµ‹é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ï¼Œå…·ä½“ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤º\n\n| Model           |  å¹³å‡  | Summary | Single-Doc QA | Multi-Doc QA | Code | Few-shot | Synthetic | \n|-----------------|:----:|:-------:|:-------------:|:------------:|:----:|:--------:|:---------:|\n| ChatGLM2-6B-32K | 41.5 |  24.8   |     37.6      |     34.7     | 52.8 |   51.3   |   47.7    | \n| ChatGLM3-6B-32K | 50.2 |  26.6   |     45.8      |     46.1     | 56.2 |   61.2   |    65     |\n\n## ä½¿ç”¨æ–¹å¼\n\n### ç¯å¢ƒå®‰è£…\n\né¦–å…ˆéœ€è¦ä¸‹è½½æœ¬ä»“åº“ï¼š\n\n```shell\ngit clone https://github.com/THUDM/ChatGLM3\ncd ChatGLM3\n```\n\nç„¶åä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š\n\n```\npip install -r requirements.txt\n```\n\n+ ä¸ºäº†ä¿è¯ `torch` çš„ç‰ˆæœ¬æ­£ç¡®ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ [å®˜æ–¹æ–‡æ¡£](https://pytorch.org/get-started/locally/) çš„è¯´æ˜å®‰è£…ã€‚\n\n### ç»¼åˆ Demo\n\næˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé›†æˆä»¥ä¸‹ä¸‰ç§åŠŸèƒ½çš„ç»¼åˆ Demoï¼Œè¿è¡Œæ–¹æ³•è¯·å‚è€ƒ[ç»¼åˆ Demo](composite_demo/README.md)\n\n- Chat: å¯¹è¯æ¨¡å¼ï¼Œåœ¨æ­¤æ¨¡å¼ä¸‹å¯ä»¥ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚\n- Tool: å·¥å…·æ¨¡å¼ï¼Œæ¨¡å‹é™¤äº†å¯¹è¯å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡å·¥å…·è¿›è¡Œå…¶ä»–æ“ä½œã€‚\n\n<img src=\"resources/tool.png\" width=\"400\">\n\n- Code Interpreter: ä»£ç è§£é‡Šå™¨æ¨¡å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸€ä¸ª Jupyter ç¯å¢ƒä¸­æ‰§è¡Œä»£ç å¹¶è·å–ç»“æœï¼Œä»¥å®Œæˆå¤æ‚ä»»åŠ¡ã€‚\n\n<img src=\"resources/heart.png\" width=\"400\">\n\n### ä»£ç è°ƒç”¨\n\nå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š\n\n```\n>> from transformers import AutoTokenizer, AutoModel\n>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n>> model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True, device='cuda')\n>> model = model.eval()\n>> response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n>> print(response)\n\nä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3 - 6B, å¾ˆé«˜å…´è§åˆ°ä½ , æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n>> response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n>> print(response)\n\næ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ, ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:\n\n1.åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨: ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯, ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ, å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚\n2.åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ: ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚, å®‰é™, é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“, å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚\n3.æ”¾æ¾èº«å¿ƒ: åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨, ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡, å¬äº›è½»æŸ”çš„éŸ³ä¹, é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰, æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘, ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚\n4.é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™: å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨, ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™, ä¾‹å¦‚å’–å•¡, èŒ¶å’Œå¯ä¹ã€‚\n5.é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…: åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…, ä¾‹å¦‚çœ‹ç”µå½±, ç©æ¸¸æˆæˆ–å·¥ä½œç­‰, å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚\n6.å°è¯•å‘¼å¸æŠ€å·§: æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§, å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘, ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”, ä¿æŒå‡ ç§’é’Ÿ, ç„¶åç¼“æ…¢å‘¼æ°”ã€‚\n\nå¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡, ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶, å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚\n```\n\n#### ä»æœ¬åœ°åŠ è½½æ¨¡å‹\n\nä»¥ä¸Šä»£ç ä¼šç”± `transformers`\nè‡ªåŠ¨ä¸‹è½½æ¨¡å‹å®ç°å’Œå‚æ•°ã€‚å®Œæ•´çš„æ¨¡å‹å®ç°åœ¨ [Hugging Face Hub](https://huggingface.co/THUDM/chatglm3-6b)\nã€‚å¦‚æœä½ çš„ç½‘ç»œç¯å¢ƒè¾ƒå·®ï¼Œä¸‹è½½æ¨¡å‹å‚æ•°å¯èƒ½ä¼šèŠ±è´¹è¾ƒé•¿æ—¶é—´ç”šè‡³å¤±è´¥ã€‚æ­¤æ—¶å¯ä»¥å…ˆå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶åä»æœ¬åœ°åŠ è½½ã€‚\n\nä» Hugging Face Hub\nä¸‹è½½æ¨¡å‹éœ€è¦å…ˆ[å®‰è£…Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)\nï¼Œç„¶åè¿è¡Œ\n\n```Shell\ngit clone https://huggingface.co/THUDM/chatglm3-6b\n```\n\nå¦‚æœä»ä½ ä» HuggingFace ä¸‹è½½æ¯”è¾ƒæ…¢ï¼Œä¹Ÿå¯ä»¥ä» [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b)\nä¸­ä¸‹è½½ã€‚\n\n### æ¨¡å‹å¾®è°ƒ\n\næˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¾®è°ƒ ChatGLM3-6B æ¨¡å‹çš„åŸºç¡€å¥—ä»¶ï¼Œå¯ä»¥ç”¨æ¥å¾®è°ƒ ChatGLM3-6B æ¨¡å‹ã€‚å¾®è°ƒå¥—ä»¶çš„ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ\n[å¾®è°ƒå¥—ä»¶](finetune_demo/README.md)ã€‚\n\n### ç½‘é¡µç‰ˆå¯¹è¯ Demo\n\n![web-demo](resources/web-demo.gif)\nå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨åŸºäº Gradio çš„ç½‘é¡µç‰ˆ demoï¼š\n\n```shell\npython web_demo_gradio.py\n```\n\n![web-demo](resources/web-demo2.png)\n\nå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨åŸºäº Streamlit çš„ç½‘é¡µç‰ˆ demoï¼š\n\n```shell\nstreamlit run web_demo_streamlit.py\n```\n\nç½‘é¡µç‰ˆ demo ä¼šè¿è¡Œä¸€ä¸ª Web Serverï¼Œå¹¶è¾“å‡ºåœ°å€ã€‚åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡ºçš„åœ°å€å³å¯ä½¿ç”¨ã€‚ ç»æµ‹è¯•ï¼ŒåŸºäº Streamlit çš„ç½‘é¡µç‰ˆ Demo ä¼šæ›´æµç•…ã€‚\n\n### å‘½ä»¤è¡Œå¯¹è¯ Demo\n\n![cli-demo](resources/cli-demo.png)\n\nè¿è¡Œä»“åº“ä¸­ [cli_demo.py](basic_demo/cli_demo.py)ï¼š\n\n```shell\npython cli_demo.py\n```\n\nç¨‹åºä¼šåœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ `clear` å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥ `stop` ç»ˆæ­¢ç¨‹åºã€‚\n\n### LangChain Demo\n\nä»£ç å®ç°è¯·å‚è€ƒ [LangChain Demo](langchain_demo/README.md)ã€‚\n\n#### å·¥å…·è°ƒç”¨\n\nå…³äºå·¥å…·è°ƒç”¨çš„æ–¹æ³•è¯·å‚è€ƒ [å·¥å…·è°ƒç”¨](tools_using_demo/README.md)ã€‚\n\n#### OpenAI API / Zhipu API Demo\n\næˆ‘ä»¬å·²ç»æ¨å‡ºäº† OpenAI / ZhipuAI æ ¼å¼çš„ å¼€æºæ¨¡å‹ API éƒ¨ç½²ä»£ç ï¼Œå¯ä»¥ä½œä¸ºä»»æ„åŸºäº ChatGPT çš„åº”ç”¨çš„åç«¯ã€‚\nç›®å‰ï¼Œå¯ä»¥é€šè¿‡è¿è¡Œä»“åº“ä¸­çš„ [api_server.py](openai_api_demo/api_server.py) è¿›è¡Œéƒ¨ç½²\n\n```shell\ncd openai_api_demo\npython api_server.py\n```\n\nåŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿä¹¦å†™äº†ä¸€ä¸ªç¤ºä¾‹ä»£ç ï¼Œç”¨æ¥æµ‹è¯•APIè°ƒç”¨çš„æ€§èƒ½ã€‚\n\n+ OpenAI æµ‹è¯•è„šæœ¬ï¼š[openai_api_request.py](openai_api_demo/openai_api_request.py)\n+ ZhipuAI æµ‹è¯•è„šæœ¬ï¼š[zhipu_api_request.py](openai_api_demo/zhipu_api_request.py)\n+ ä½¿ç”¨Curlè¿›è¡Œæµ‹è¯•\n  \n+ chat Curl æµ‹è¯•\n```shell\ncurl -X POST \"http://127.0.0.1:8000/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\\\"model\\\": \\\"chatglm3-6b\\\", \\\"messages\\\": [{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"ä½ å¥½ï¼Œç»™æˆ‘è®²ä¸€ä¸ªæ•…äº‹ï¼Œå¤§æ¦‚100å­—\\\"}], \\\"stream\\\": false, \\\"max_tokens\\\": 100, \\\"temperature\\\": 0.8, \\\"top_p\\\": 0.8}\"\n````\n\n+ Standard openai interface agent-chat Curl æµ‹è¯•\n```shell\ncurl -X POST \"http://127.0.0.1:8000/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\\\"model\\\": \\\"chatglm3-6b\\\", \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"37ä¹˜ä»¥8åŠ 7é™¤2ç­‰äºå¤šå°‘ï¼Ÿ\\\"}], \"tools\": [{\"name\": \"track\", \"description\": \"è¿½è¸ªæŒ‡å®šè‚¡ç¥¨çš„å®æ—¶ä»·æ ¼\",\n          \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"description\": \"éœ€è¦è¿½è¸ªçš„è‚¡ç¥¨ä»£ç \"}},\n                         \"required\": []}},\n         {\"name\": \"Calculator\", \"description\": \"æ•°å­¦è®¡ç®—å™¨ï¼Œè®¡ç®—æ•°å­¦é—®é¢˜\",\n          \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"description\": \"è¦è®¡ç®—çš„æ•°å­¦å…¬å¼\"}},\n                         \"required\": []}}\n         ], \\\"stream\\\": true, \\\"max_tokens\\\": 100, \\\"temperature\\\": 0.8, \\\"top_p\\\": 0.8}\"\n````\n\n+ Openai style custom interface agent-chat Curl æµ‹è¯•ï¼ˆä½ éœ€è¦å®ç°è‡ªå®šä¹‰çš„å·¥å…·æè¿°è„šæœ¬openai_api_demo/tools/schema.pyçš„å†…å®¹ï¼Œå¹¶ä¸”å°†api_server.pyä¸­AGENT_CONTROLLERæŒ‡å®šä¸º'true'ï¼‰ï¼š\n```shell\ncurl -X POST \"http://127.0.0.1:8000/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\\\"model\\\": \\\"chatglm3-6b\\\", \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"37ä¹˜ä»¥8åŠ 7é™¤2ç­‰äºå¤šå°‘ï¼Ÿ\\\"}], \\\"stream\\\": true, \\\"max_tokens\\\": 100, \\\"temperature\\\": 0.8, \\\"top_p\\\": 0.8}\"\n````\nè¯¥æ¥å£ç”¨äºopenaié£æ ¼çš„è‡ªå®šä¹‰å·¥å…·ç®±çš„è‡ªä¸»è°ƒåº¦ã€‚å…·æœ‰è°ƒåº¦å¼‚å¸¸çš„è‡ªå¤„ç†å›å¤èƒ½åŠ›ï¼Œæ— éœ€å¦å¤–å®ç°è°ƒåº¦ç®—æ³•ï¼Œç”¨æˆ·æ— éœ€api_keyã€‚\n\n\n+ ä½¿ç”¨Pythonè¿›è¡Œæµ‹è¯•\n\n```shell\ncd openai_api_demo\npython openai_api_request.py\n```\n\nå¦‚æœæµ‹è¯•æˆåŠŸï¼Œåˆ™æ¨¡å‹åº”è¯¥è¿”å›ä¸€æ®µæ•…äº‹ã€‚\n\n## ä½æˆæœ¬éƒ¨ç½²\n\n### æ¨¡å‹é‡åŒ–\n\né»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 13GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).quantize(4).cuda()\n```\n\næ¨¡å‹é‡åŒ–ä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½æŸå¤±ï¼Œç»è¿‡æµ‹è¯•ï¼ŒChatGLM3-6B åœ¨ 4-bit é‡åŒ–ä¸‹ä»ç„¶èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶æµç•…çš„ç”Ÿæˆã€‚\n\n### CPU éƒ¨ç½²\n\nå¦‚æœä½ æ²¡æœ‰ GPU ç¡¬ä»¶çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½†æ˜¯æ¨ç†é€Ÿåº¦ä¼šæ›´æ…¢ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼ˆéœ€è¦å¤§æ¦‚ 32GB å†…å­˜ï¼‰\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).float()\n```\n\n### Mac éƒ¨ç½²\n\nå¯¹äºæ­è½½äº† Apple Silicon æˆ–è€… AMD GPU çš„ Macï¼Œå¯ä»¥ä½¿ç”¨ MPS åç«¯æ¥åœ¨ GPU ä¸Šè¿è¡Œ ChatGLM3-6Bã€‚éœ€è¦å‚è€ƒ Apple\nçš„ [å®˜æ–¹è¯´æ˜](https://developer.apple.com/metal/pytorch) å®‰è£… PyTorch-Nightlyï¼ˆæ­£ç¡®çš„ç‰ˆæœ¬å·åº”è¯¥æ˜¯2.x.x.dev2023xxxxï¼Œè€Œä¸æ˜¯\n2.x.xï¼‰ã€‚\n\nç›®å‰åœ¨ MacOS ä¸Šåªæ”¯æŒ[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](README.md#ä»æœ¬åœ°åŠ è½½æ¨¡å‹)ã€‚å°†ä»£ç ä¸­çš„æ¨¡å‹åŠ è½½æ”¹ä¸ºä»æœ¬åœ°åŠ è½½ï¼Œå¹¶ä½¿ç”¨ mps åç«¯ï¼š\n\n```python\nmodel = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).to('mps')\n```\n\nåŠ è½½åŠç²¾åº¦çš„ ChatGLM3-6B æ¨¡å‹éœ€è¦å¤§æ¦‚ 13GB å†…å­˜ã€‚å†…å­˜è¾ƒå°çš„æœºå™¨ï¼ˆæ¯”å¦‚ 16GB å†…å­˜çš„ MacBook\nProï¼‰ï¼Œåœ¨ç©ºä½™å†…å­˜ä¸è¶³çš„æƒ…å†µä¸‹ä¼šä½¿ç”¨ç¡¬ç›˜ä¸Šçš„è™šæ‹Ÿå†…å­˜ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ä¸¥é‡å˜æ…¢ã€‚\n\n### å¤šå¡éƒ¨ç½²\n\nå¦‚æœä½ æœ‰å¤šå¼  GPUï¼Œä½†æ˜¯æ¯å¼  GPU çš„æ˜¾å­˜å¤§å°éƒ½ä¸è¶³ä»¥å®¹çº³å®Œæ•´çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå¯ä»¥å°†æ¨¡å‹åˆ‡åˆ†åœ¨å¤šå¼ GPUä¸Šã€‚é¦–å…ˆå®‰è£…\naccelerate: `pip install accelerate`ï¼Œç„¶åå³å¯æ­£å¸¸åŠ è½½æ¨¡å‹ã€‚\n\n### OpenVINO Demo\n\nChatGLM3-6B å·²ç»æ”¯æŒä½¿ç”¨ OpenVINO\nå·¥å…·åŒ…è¿›è¡ŒåŠ é€Ÿæ¨ç†ï¼Œåœ¨è‹±ç‰¹å°”çš„GPUå’ŒGPUè®¾å¤‡ä¸Šæœ‰è¾ƒå¤§æ¨ç†é€Ÿåº¦æå‡ã€‚å…·ä½“ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ [OpenVINO Demo](Intel_device_demo/openvino_demo/README.md)ã€‚\n\n### TensorRT-LLM Demo\n\nChatGLM3-6Bå·²ç»æ”¯æŒä½¿ç”¨ TensorRT-LLM\nå·¥å…·åŒ…è¿›è¡ŒåŠ é€Ÿæ¨ç†ï¼Œæ¨¡å‹æ¨ç†é€Ÿåº¦å¾—åˆ°å¤šå€çš„æå‡ã€‚å…·ä½“ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ [TensorRT-LLM Demo](tensorrt_llm_demo/tensorrt_llm_cli_demo.py)\nå’Œ å®˜æ–¹æŠ€æœ¯æ–‡æ¡£ã€‚\n\n## å¼•ç”¨\n\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ã€‚\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 22.6201171875,
          "content": "# ChatGLM3\n\n<p align=\"center\">\nğŸ“„<a href=\"https://arxiv.org/pdf/2406.12793\" target=\"_blank\"> Report </a> â€¢ ğŸ¤— <a href=\"https://huggingface.co/THUDM/chatglm3-6b\" target=\"_blank\">HF Repo</a> â€¢ ğŸ¤– <a href=\"https://modelscope.cn/models/ZhipuAI/chatglm3-6b\" target=\"_blank\">ModelScope</a> â€¢ ğŸŸ£ <a href=\"https://www.wisemodel.cn/models/ZhipuAI/chatglm3-6b\" target=\"_blank\">WiseModel</a> â€¢ ğŸ“” <a href=\"https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof\" target=\"_blank\">Document</a> â€¢  ğŸ§° <a href=\"https://openxlab.org.cn/models/hot/THUDM\" target=\"_blank\">OpenXLab</a> â€¢ ğŸ¦ <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a><br>\n</p>\n<p align=\"center\">\n    ğŸ‘‹ Join our  <a href=\"https://discord.gg/fK2dz4bg\" target=\"_blank\">Discord</a> and <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<p align=\"center\">\nğŸ“Experience the larger-scale ChatGLM model at <a href=\"https://www.chatglm.cn\">chatglm.cn</a>\n</p>\n\nğŸ“” About `ChatGLM3-6B`For more detailed usage information, please refer to:\n\n+ [ChatGLM3 technical documentation](https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof?from=from_copylink)\n+ [Bilibili video](https://www.bilibili.com/video/BV1uC4y1J7yA)\n+ [YouTube video](https://www.youtube.com/watch?v=Pw9PB6R7ORA)\n\n## GLM-4 Open Source Model and API\n\nWe have released the latest **GLM-4** model, which has made new breakthroughs in multiple indicators. You can directly\nexperience our latest model in the following two channels.\n\n+ [GLM-4 open source model](https://github.com/THUDM/GLM-4) We have open sourced the GLM-4-9B series models, which have\n  significantly improved the performance of various indicators. Welcome to try.\n+ [Zhipu Qingyan](https://chatglm.cn/main/detail?fr=ecology_x) Experience the latest version of GLM-4, including **GLMs,\n  All tools** and other functions.\n+ [API platform](https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9) The new generation of API\n  platform has been launched. You can directly experience new models such\n  as `GLM-4-0520`, `GLM-4-air`, `GLM-4-airx`, `GLM-4-flash`, `GLM-4`, `GLM-3-Turbo`, `CharacterGLM-3`, `CogView-3` on\n  the API platform.\n  Among them, the two models `GLM-4` and `GLM-3-Turbo` support new functions such\n  as `System Prompt`, `Function Call`, `Retrieval`, and `Web_Search`. You are welcome to experience them.\n\n+ [GLM4 API open source tutorial](https://github.com/MetaGLM/glm-cookbook/) GLM-4 API tutorial and basic applications,\n  welcome to try.\n  API-related questions can be asked in this open source tutorial, or\n  use [GLM-4 API AI Assistant](https://open.bigmodel.cn/shareapp/v1/?share_code=sQwt5qyqYVaNh1O_87p8O)\n  to get help with common problems.\n\n-----\n\n## ChatGLM3 Introduction\n\n**ChatGLM3** is a generation of pre-trained dialogue models jointly released by Zhipu AI and Tsinghua KEG. ChatGLM3-6B\nis the open-source model in the ChatGLM3 series, maintaining many excellent features of the first two generations such\nas smooth dialogue and low deployment threshold, while introducing the following features:\n\n1. **Stronger Base Model:** The base model of ChatGLM3-6B, ChatGLM3-6B-Base, adopts a more diverse training dataset,\n   more sufficient training steps, and a more reasonable training strategy. Evaluations on datasets from various\n   perspectives such as semantics, mathematics, reasoning, code, and knowledge show that **ChatGLM3-6B-Base has the\n   strongest performance among base models below 10B**.\n\n2. **More Complete Function Support:** ChatGLM3-6B adopts a newly designed [Prompt format](PROMPT_en.md), supporting\n   multi-turn dialogues as usual. It also natively supports [tool invocation](tools_using_demo/README_en.md) (Function\n   Call), code execution (Code Interpreter), and Agent tasks in complex scenarios.\n\n3. **More Comprehensive Open-source Series:** In addition to the dialogue\n   model [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b), the basic\n   model [ChatGLM3-6B-Base](https://huggingface.co/THUDM/chatglm3-6b-base), the long-text dialogue\n   model [ChatGLM3-6B-32K](https://huggingface.co/THUDM/chatglm3-6b-32k) and further strengthens the ability to\n   understand long texts [ChatGLM3-6B-128K](https://huggingface.co/THUDM/chatglm3-6b-128k) have also been open-sourced.\n   All these weights are **fully open** for academic research, and **free commercial use is also allowed** after\n   registration via a [questionnaire](https://open.bigmodel.cn/mla/form).\n\n-----\n\nThe ChatGLM3 open-source model aims to promote the development of large-model technology together with the open-source\ncommunity. Developers and everyone are earnestly requested to comply with the [open-source protocol](MODEL_LICENSE), and\nnot to use the open-source models, codes, and derivatives for any purposes that might harm the nation and society, and\nfor any services that have not been evaluated and filed for safety. Currently, no applications, including web, Android,\nApple iOS, and Windows App, have been developed based on the **ChatGLM3 open-source model** by our project team.\n\nAlthough every effort has been made to ensure the compliance and accuracy of the data at various stages of model\ntraining, due to the smaller scale of the ChatGLM3-6B model and the influence of probabilistic randomness factors, the\naccuracy of output content cannot be guaranteed. The model output is also easily misled by user input. **This project\ndoes not assume risks and liabilities caused by data security, public opinion risks, or any misleading, abuse,\ndissemination, and improper use of open-source models and codes.**\n\n## Model List\n\n|      Model       | Seq Length |                                                              Download                                                               \n|:----------------:|:----------:|:-----------------------------------------------------------------------------------------------------------------------------------:\n|   ChatGLM3-6B    |     8k     |      [HuggingFace](https://huggingface.co/THUDM/chatglm3-6b) \\| [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b)      \n| ChatGLM3-6B-Base |     8k     | [HuggingFace](https://huggingface.co/THUDM/chatglm3-6b-base) \\| [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-base) \n| ChatGLM3-6B-32K  |    32k     |  [HuggingFace](https://huggingface.co/THUDM/chatglm3-6b-32k) \\| [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-32k)  \n| ChatGLM3-6B-128K |    128k    | [HuggingFace](https://huggingface.co/THUDM/chatglm3-6b-128k) ï½œ [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-128k)  \n\n## Projects\n\nThe following excellent open source repositories have in-depth support for the ChatGLM3-6B model, and everyone is\nwelcome to expand their learning.\n\nInference acceleration:\n\n* [chatglm.cpp](https://github.com/li-plus/chatglm.cpp): Real-time inference on your laptop accelerated by quantization,\n  similar to llama.cpp.\n* [ChatGLM3-TPU](https://github.com/sophgo/ChatGLM3-TPU): Using the TPU accelerated inference solution, it runs about\n  7.5 token/s in real time on the end-side chip BM1684X (16T@FP16, 16G DDR).\n* [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main): A high-performance GPU-accelerated inference\n  solution developed by NVIDIA, you can refer to these [steps](./tensorrt_llm_demo/README.md) to deploy ChatGLM3.\n* [OpenVINO](https://github.com/openvinotoolkit): A high-performance CPU and GPU accelerated inference solution\n  developed by Intel, you can refer to this [step](./Intel_device_demo/openvino_demo/README.md) to deploy the\n  ChatGLM3-6B model\n\nEfficient fine-tuning:\n\n* [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory): An excellent, easy-to-use and efficient fine-tuning\n  framework.\n\nApplication framework:\n\n* [LangChain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat): Based on large language models such as\n  ChatGLM and application frameworks such as Langchain, open source and offline deployable retrieval enhancement\n  generation (RAG) large Model knowledge base project.\n\n* [BISHENG](https://github.com/dataelement/bisheng): open-source platform for developing LLM applications. It empowers\n  and accelerates the development of LLM applications and helps users to enter the next generation of application\n  development mode with the best experience.\n\n* [RAGFlow](https://github.com/infiniflow/ragflow): An open-source RAG (Retrieval-Augmented Generation) engine based on\n  deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large\n  Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various\n  complex formatted data.\n\n## Evaluation Results\n\n### Typical Tasks\n\nWe selected 8 typical Chinese-English datasets and conducted performance tests on the ChatGLM3-6B (base) version.\n\n| Model            | GSM8K | MATH | BBH  | MMLU | C-Eval | CMMLU | MBPP | AGIEval |\n|------------------|:-----:|:----:|:----:|:----:|:------:|:-----:|:----:|:-------:|\n| ChatGLM2-6B-Base | 32.4  | 6.5  | 33.7 | 47.9 |  51.7  | 50.0  |  -   |    -    |\n| Best Baseline    | 52.1  | 13.1 | 45.0 | 60.1 |  63.5  | 62.2  | 47.5 |  45.8   |\n| ChatGLM3-6B-Base | 72.3  | 25.7 | 66.1 | 61.4 |  69.0  | 67.5  | 52.4 |  53.7   |\n\n> \"Best Baseline\" refers to the pre-trained models that perform best on the corresponding datasets with model parameters\n> below 10B, excluding models that are trained specifically for a single task and do not maintain general capabilities.\n\n> In the tests of ChatGLM3-6B-Base, BBH used a 3-shot test, GSM8K and MATH that require inference used a 0-shot CoT\n> test, MBPP used a 0-shot generation followed by running test cases to calculate Pass@1, and other multiple-choice type\n> datasets all used a 0-shot test.\n\nWe have conducted manual evaluation tests on ChatGLM3-6B-32K in multiple long-text application scenarios. Compared with\nthe second-generation model, its effect has improved by more than 50% on average. In applications such as paper reading,\ndocument summarization, and financial report analysis, this improvement is particularly significant. In addition, we\nalso tested the model on the LongBench evaluation set, and the specific results are shown in the table below.\n\n| Model           | Average | Summary | Single-Doc QA | Multi-Doc QA | Code | Few-shot | Synthetic | \n|-----------------|:-------:|:-------:|:-------------:|:------------:|:----:|:--------:|:---------:|\n| ChatGLM2-6B-32K |  41.5   |  24.8   |     37.6      |     34.7     | 52.8 |   51.3   |   47.7    | \n| ChatGLM3-6B-32K |  50.2   |  26.6   |     45.8      |     46.1     | 56.2 |   61.2   |    65     |\n\n## How to Use\n\n### Environment Installation\n\nFirst, you need to download this repository:\n\n```shell\ngit clone https://github.com/THUDM/ChatGLM3\ncd ChatGLM3\n```\n\nThen use pip to install the dependencies:\n\n```\npip install -r requirements.txt\n```\n\n+ In order to ensure that the version of `torch` is correct, please strictly follow the instructions\n  of [official documentation](https://pytorch.org/get-started/locally/) for installation.\n\n### Integrated Demo\n\nWe provide an integrated demo that incorporates the following three functionalities. Please refer\nto [Integrated Demo](composite_demo/README_en.md) for how to run it.\n\n- Chat: Dialogue mode, where you can interact with the model.\n- Tool: Tool mode, where in addition to dialogue, the model can also perform other operations using tools.\n  ![tool](resources/tool_en.png)\n- Code Interpreter: Code interpreter mode, where the model can execute code in a Jupyter environment and obtain results\n  to complete complex tasks.\n  ![code](resources/code_en.gif)\n\n### Usage\n\nThe ChatGLM model can be called to start a conversation using the following code:\n\n```\n>> from transformers import AutoTokenizer, AutoModel\n>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n>> model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True, device='cuda')\n>> model = model.eval()\n>> response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n>> print(response)\n\nHello ğŸ‘‹! I am the AI assistant ChatGLM3-6B, nice to meet you, feel free to ask me any questions.\n\n>> response, history = model.chat(tokenizer, \"What should I do if I can't sleep at night?\", history=history)\n>> print(response)\n\nHaving trouble sleeping at night may make you feel anxious or uncomfortable, but here are some ways to help you fall asleep:\n\n1. Develop a regular sleep schedule: Keeping a regular sleep schedule can help you develop healthy sleep habits and make it easier for you to fall asleep. Try to go to bed at the same time every day and get up at the same time.\n\n2. Create a comfortable sleeping environment: Make sure the sleeping environment is comfortable, quiet, dark and at a suitable temperature. You can use comfortable bedding and keep the room ventilated.\n\n3. Relax your body and mind: Doing some relaxing activities before bed, such as taking a hot bath, listening to some soft music, reading some interesting books, etc., can help relieve tension and anxiety and make it easier for you to fall asleep.\n\n4. Avoid drinking caffeinated beverages: Caffeine is a stimulant that can affect your sleep quality. Try to avoid drinking caffeinated beverages such as coffee, tea and cola before bed.\n\n5. Avoid doing things that are not related to sleep in bed: Doing things that are not related to sleep in bed, such as watching movies, playing games or working, etc., may interfere with your sleep.\n6. Try breathing techniques: Deep breathing is a relaxation technique that can help you relieve tension and anxiety, making it easier for you to fall asleep. Try breathing in slowly, holding it for a few seconds, and then exhaling slowly.\n\nIf these methods don't help you fall asleep, you may consider consulting a doctor or sleep specialist for further advice.\n```\n\n#### Load Model Locally\n\nThe above code will automatically download the model implementation and parameters by `transformers`. The complete model\nimplementation is available on [Hugging Face Hub](https://huggingface.co/THUDM/chatglm3-6b). If your network environment\nis poor, downloading model parameters might take a long time or even fail. In this case, you can first download the\nmodel to your local machine, and then load it from there.\n\nTo download the model from Hugging Face Hub, you need\nto [install Git LFS](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)\nfirst, then run\n\n```Shell\ngit clone https://huggingface.co/THUDM/chatglm3-6b\n```\n\nIf the download from HuggingFace is slow, you can also download it\nfrom [ModelScope](https://modelscope.cn/models/ZhipuAI/chatglm3-6b).\n\n# Model Fine-tuning\n\nWe provide a basic fine-tuning framework for ChatGLM3-6B. You can use it to fine-tune the model on your own dataset. For\nmore details, please refer to [Fine-tuning Demo](finetune_demo/README_en.md).\n\n### Web-based Dialogue Demo\n\n![web-demo](resources/web-demo.gif)\nYou can launch a web-based demo using Gradio with the following command:\n\n```shell\npython web_demo_gradio.py\n```\n\n![web-demo](resources/web-demo2.png)\n\nYou can launch a web-based demo using Streamlit with the following command:\n\n```shell\nstreamlit run web_demo_streamlit.py\n```\n\nThe web-based demo will run a Web Server and output an address. You can use it by opening the output address in a\nbrowser. Based on tests, the web-based demo using Streamlit runs more smoothly.\n\n### Command Line Dialogue Demo\n\n![cli-demo](resources/cli-demo.png)\n\nRun [cli_demo.py](basic_demo/cli_demo.py) in the repository:\n\n```shell\npython cli_demo.py\n```\n\nThe program will interact in the command line, enter instructions in the command line and hit enter to generate a\nresponse. Enter `clear` to clear the dialogue history, enter `stop` to terminate the program.\n\n### OpenAI API /Zhipu API Demo\n\nWe have launched open source model API deployment code in OpenAI / ZhipuAI format, which can be used as the backend of\nany ChatGPT-based application.\nCurrently, you can deploy by running [api_server.py](openai_api_demo/api_server.py) in the warehouse\n\n```shell\ncd openai_api_demo\npython api_server.py\n```\n\nAt the same time, we also wrote a sample code to test the performance of API calls.\n\n+ OpenAI test script: [openai_api_request.py](openai_api_demo/openai_api_request.py)\n+ ZhipuAI test script: [zhipu_api_request.py](openai_api_demo/zhipu_api_request.py)\n+ Test with Curl\n\n+ chat Curl test\n\n```shell\ncurl -X POST \"http://127.0.0.1:8000/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\\\"model\\\": \\\"chatglm3-6b\\\", \\\"messages\\\": [{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"ä½ å¥½ï¼Œç»™æˆ‘è®²ä¸€ä¸ªæ•…äº‹ï¼Œå¤§æ¦‚100å­—\\\"}], \\\"stream\\\": false, \\\"max_tokens\\\": 100, \\\"temperature\\\": 0.8, \\\"top_p\\\": 0.8}\"\n````\n\n+ Standard openai interface agent-chat Curl test\n\n```shell\ncurl -X POST \"http://127.0.0.1:8000/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\\\"model\\\": \\\"chatglm3-6b\\\", \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"37ä¹˜ä»¥8åŠ 7é™¤2ç­‰äºå¤šå°‘ï¼Ÿ\\\"}], \"tools\": [{\"name\": \"track\", \"description\": \"è¿½è¸ªæŒ‡å®šè‚¡ç¥¨çš„å®æ—¶ä»·æ ¼\",\n          \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"description\": \"éœ€è¦è¿½è¸ªçš„è‚¡ç¥¨ä»£ç \"}},\n                         \"required\": []}},\n         {\"name\": \"Calculator\", \"description\": \"æ•°å­¦è®¡ç®—å™¨ï¼Œè®¡ç®—æ•°å­¦é—®é¢˜\",\n          \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"description\": \"è¦è®¡ç®—çš„æ•°å­¦å…¬å¼\"}},\n                         \"required\": []}}\n         ], \\\"stream\\\": true, \\\"max_tokens\\\": 100, \\\"temperature\\\": 0.8, \\\"top_p\\\": 0.8}\"\n````\n\n+ Openai style custom interface agent-chat Curl test (You need to implement the contents of the custom tool description\n  script openai_api_demo/tools/schema.py, and specify AGENT_CONTROLLER in api_server.py as 'true')ï¼š\n\n```shell\ncurl -X POST \"http://127.0.0.1:8000/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\\\"model\\\": \\\"chatglm3-6b\\\", \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"37ä¹˜ä»¥8åŠ 7é™¤2ç­‰äºå¤šå°‘ï¼Ÿ\\\"}], \\\"stream\\\": true, \\\"max_tokens\\\": 100, \\\"temperature\\\": 0.8, \\\"top_p\\\": 0.8}\"\n````\n\nThis interface is used for autonomous scheduling of OpenAI-style custom toolboxes. It has the ability to self-process\nand respond to scheduling exceptions, without the need to implement additional scheduling algorithms, and users do not\nneed an api_key.\n\n+ Testing with Python\n\n```shell\ncd openai_api_demo\npython openai_api_request.py\n```\n\nIf the test is successful, the model should return a story.\n\n### Tool Invocation\n\nFor methods of tool invocation, please refer to [Tool Invocation](tools_using_demo/README_en.md).\n\n## Low-Cost Deployment\n\n### Model Quantization\n\nBy default, the model is loaded with FP16 precision, running the above code requires about 13GB of VRAM. If your GPU's\nVRAM is limited, you can try loading the model quantitatively, as follows:\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).quantize(4).cuda()\n```\n\nModel quantization will bring some performance loss. Through testing, ChatGLM3-6B can still perform natural and smooth\ngeneration under 4-bit quantization.\n\n### CPU Deployment\n\nIf you don't have GPU hardware, you can also run inference on the CPU, but the inference speed will be slower. The usage\nis as follows (requires about 32GB of memory):\n\n```python\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).float()\n```\n\n### Mac Deployment\n\nFor Macs equipped with Apple Silicon or AMD GPUs, the MPS backend can be used to run ChatGLM3-6B on the GPU. Refer to\nApple's [official instructions](https://developer.apple.com/metal/pytorch) to install PyTorch-Nightly (the correct\nversion number should be 2.x.x.dev2023xxxx, not 2.x.x).\n\nCurrently, only [loading the model locally](README_en.md#load-model-locally) is supported on MacOS. Change the model\nloading in the code to load locally and use the MPS backend:\n\n```python\nmodel = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).to('mps')\n```\n\nLoading the half-precision ChatGLM3-6B model requires about 13GB of memory. Machines with smaller memory (such as a 16GB\nmemory MacBook Pro) will use virtual memory on the hard disk when there is insufficient free memory, resulting in a\nsignificant slowdown in inference speed.\n\n### Multi-GPU Deployment\n\nIf you have multiple GPUs, but each GPU's VRAM size is not enough to accommodate the complete model, then the model can\nbe split across multiple GPUs. First, install accelerate: `pip install accelerate`, and then load the model as usual.\n\n### OpenVINO Demo\n\nChatGLM3-6B already supports the use of OpenVINO\nThe toolkit accelerates inference and has a greater inference speed improvement on Intel's GPUs and GPU devices. For\nspecific usage, please refer to [OpenVINO Demo](Intel_device_demo/openvino_demo/README.md).\n\n### TensorRT-LLM Demo\n\nChatGLM3-6B now supports accelerated inference using the TensorRT-LLM toolkit, significantly improving model inference\nspeed. For specific usage, please refer to the [TensorRT-LLM Demo](tensorrt_llm_demo/tensorrt_llm_cli_demo.py) and the\nofficial technical documentation.\n\n## Citation\n\nIf you find our work helpful, please consider citing the following papers.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```"
        },
        {
          "name": "basic_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "composite_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "langchain_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_api_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.486328125,
          "content": "# basic requirements\n\ntransformers==4.40.0\ncpm_kernels>=1.0.11\ntorch>=2.3.0\nvllm>=0.4.2\ngradio>=4.26.0\nsentencepiece>=0.2.0\nsentence_transformers>=2.7.0\naccelerate>=0.29.2\nstreamlit>=1.33.0\nfastapi>=0.110.0\nloguru~=0.7.2\nmdtex2html>=1.3.0\nlatex2mathml>=3.77.0\njupyter_client>=8.6.1\n\n# for openai demo\nopenai>=1.30.1\npydantic>=2.7.1\nsse-starlette>=2.1.0\nuvicorn>=0.29.0\ntimm>=0.9.16\ntiktoken>=0.6.0\n\n# for langchain demo\n\nlangchain>=0.2.1\nlangchain_community>=0.2.0\nlangchainhub>=0.1.15\narxiv>=2.1.0"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "tensorrt_llm_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools_using_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "update_requirements.sh",
          "type": "blob",
          "size": 0.234375,
          "content": "#!/bin/bash\n\npython -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple\n\nwhile read requirement; do\n    python -m pip install --upgrade \"$requirement\" -i https://pypi.tuna.tsinghua.edu.cn/simple\ndone < requirements.txt\n"
        }
      ]
    }
  ]
}