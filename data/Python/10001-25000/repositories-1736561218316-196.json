{
  "metadata": {
    "timestamp": 1736561218316,
    "page": 196,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "eosphoros-ai/DB-GPT",
      "stars": 14251,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.173828125,
          "content": ".env\n.git/\n./.mypy_cache/\nmodels/\nplugins/\npilot/data\npilot/message\nlogs/\nvenv/\nweb/node_modules/\nweb/.next/\nweb/.env\ndocs/node_modules/\nbuild/\ndocs/build/\ndocs/Dockerfile-deploy"
        },
        {
          "name": ".env.template",
          "type": "blob",
          "size": 14.1884765625,
          "content": "#*******************************************************************#\n#**             DB-GPT  - GENERAL SETTINGS                        **#  \n#*******************************************************************#\n\n#*******************************************************************#\n#**                        Webserver Port                         **#\n#*******************************************************************#\n# DBGPT_WEBSERVER_PORT=5670\n## Whether to enable the new web UI, enabled by default,False use old ui\n# USE_NEW_WEB_UI=True\n#*******************************************************************#\n#***                       LLM PROVIDER                          ***#\n#*******************************************************************#\n\n# TEMPERATURE=0\n\n#*******************************************************************#\n#**                         LLM MODELS                            **#\n#*******************************************************************#\n# LLM_MODEL, see dbgpt/configs/model_config.LLM_MODEL_CONFIG\nLLM_MODEL=glm-4-9b-chat\n## LLM model path, by default, DB-GPT will read the model path from LLM_MODEL_CONFIG based on the LLM_MODEL.\n## Of course you can specify your model path according to LLM_MODEL_PATH\n## In DB-GPT, the priority from high to low to read model path:\n##    1. environment variable with key: {LLM_MODEL}_MODEL_PATH (Avoid multi-model conflicts)\n##    2. environment variable with key: MODEL_PATH\n##    3. environment variable with key: LLM_MODEL_PATH\n##    4. the config in dbgpt/configs/model_config.LLM_MODEL_CONFIG\n# LLM_MODEL_PATH=/app/models/glm-4-9b-chat\n# LLM_PROMPT_TEMPLATE=vicuna_v1.1\nMODEL_SERVER=http://127.0.0.1:8000\nLIMIT_MODEL_CONCURRENCY=5\nMAX_POSITION_EMBEDDINGS=4096\nQUANTIZE_QLORA=True\nQUANTIZE_8bit=True\n# QUANTIZE_4bit=False\n## SMART_LLM_MODEL - Smart language model (Default: vicuna-13b)\n## FAST_LLM_MODEL - Fast language model (Default: chatglm-6b)\n# SMART_LLM_MODEL=vicuna-13b\n# FAST_LLM_MODEL=chatglm-6b\n## Proxy llm backend, this configuration is only valid when \"LLM_MODEL=proxyllm\", When we use the rest API provided by deployment frameworks like fastchat as a proxyllm, \n## \"PROXYLLM_BACKEND\" is the model they actually deploy. We can use \"PROXYLLM_BACKEND\" to load the prompt of the corresponding scene. \n# PROXYLLM_BACKEND=\n\n### You can configure parameters for a specific model with {model name}_{config key}=xxx\n### See dbgpt/model/parameter.py\n## prompt template for current model\n# llama_cpp_prompt_template=vicuna_v1.1\n## llama-2-70b must be 8\n# llama_cpp_n_gqa=8\n## Model path\n# llama_cpp_model_path=/data/models/TheBloke/vicuna-13B-v1.5-GGUF/vicuna-13b-v1.5.Q4_K_M.gguf\n\n### LLM cache\n## Enable Model cache\n# MODEL_CACHE_ENABLE=True\n## The storage type of model cache, now supports: memory, disk\n# MODEL_CACHE_STORAGE_TYPE=disk\n## The max cache data in memory, we always store cache data in memory fist for high speed. \n# MODEL_CACHE_MAX_MEMORY_MB=256\n## The dir to save cache data, this configuration is only valid when MODEL_CACHE_STORAGE_TYPE=disk\n## The default dir is pilot/data/model_cache\n# MODEL_CACHE_STORAGE_DISK_DIR=\n\n#*******************************************************************#\n#**                         EMBEDDING SETTINGS                    **#\n#*******************************************************************#\nEMBEDDING_MODEL=text2vec\nEMBEDDING_MODEL_MAX_SEQ_LEN=512\n#EMBEDDING_MODEL=m3e-large\n#EMBEDDING_MODEL=bge-large-en\n#EMBEDDING_MODEL=bge-large-zh\nKNOWLEDGE_CHUNK_SIZE=500\nKNOWLEDGE_SEARCH_TOP_SIZE=5\nKNOWLEDGE_GRAPH_SEARCH_TOP_SIZE=200\n## Maximum number of chunks to load at once, if your single document is too large,\n## you can set this value to a higher value for better performance.\n## if out of memory when load large document, you can set this value to a lower value.\n# KNOWLEDGE_MAX_CHUNKS_ONCE_LOAD=10\n## Maximum number of threads to use when loading chunks, please make sure your vector db can support multi-threading.\n# KNOWLEDGE_MAX_THREADS=1\n#KNOWLEDGE_CHUNK_OVERLAP=50\n# Control whether to display the source document of knowledge on the front end.\nKNOWLEDGE_CHAT_SHOW_RELATIONS=False\n# Whether to enable Chat Knowledge Search Rewrite Mode\nKNOWLEDGE_SEARCH_REWRITE=False\n## EMBEDDING_TOKENIZER   - Tokenizer to use for chunking large inputs\n## EMBEDDING_TOKEN_LIMIT - Chunk size limit for large inputs\n# EMBEDDING_MODEL=all-MiniLM-L6-v2\n# EMBEDDING_TOKENIZER=all-MiniLM-L6-v2\n# EMBEDDING_TOKEN_LIMIT=8191\n\n## Openai embedding model, See dbgpt/model/parameter.py\n# EMBEDDING_MODEL=proxy_openai\n# proxy_openai_proxy_server_url=https://api.openai.com/v1\n# proxy_openai_proxy_api_key={your-openai-sk}\n# proxy_openai_proxy_backend=text-embedding-ada-002\n\n\n## qwen embedding model, See dbgpt/model/parameter.py\n# EMBEDDING_MODEL=proxy_tongyi\n# proxy_tongyi_proxy_backend=text-embedding-v1\n# proxy_tongyi_proxy_api_key={your-api-key}\n\n## qianfan embedding model, See dbgpt/model/parameter.py\n#EMBEDDING_MODEL=proxy_qianfan\n#proxy_qianfan_proxy_backend=bge-large-zh\n#proxy_qianfan_proxy_api_key={your-api-key}\n#proxy_qianfan_proxy_api_secret={your-secret-key}\n\n\n## Common HTTP embedding model\n# EMBEDDING_MODEL=proxy_http_openapi\n# proxy_http_openapi_proxy_server_url=http://localhost:8100/api/v1/embeddings\n# proxy_http_openapi_proxy_api_key=1dce29a6d66b4e2dbfec67044edbb924\n# proxy_http_openapi_proxy_backend=text2vec\n\n#*******************************************************************#\n#**                         RERANK SETTINGS                       **#\n#*******************************************************************#\n## Rerank model\n# RERANK_MODEL=bge-reranker-base\n## If you not set RERANK_MODEL_PATH, DB-GPT will read the model path from EMBEDDING_MODEL_CONFIG based on the RERANK_MODEL.\n# RERANK_MODEL_PATH=\n## The number of rerank results to return\n# RERANK_TOP_K=3\n\n## Common HTTP rerank model\n# RERANK_MODEL=rerank_proxy_http_openapi\n# rerank_proxy_http_openapi_proxy_server_url=http://127.0.0.1:8100/api/v1/beta/relevance\n# rerank_proxy_http_openapi_proxy_api_key={your-api-key}\n# rerank_proxy_http_openapi_proxy_backend=bge-reranker-base\n\n\n\n\n#*******************************************************************#\n#**                  DB-GPT METADATA DATABASE SETTINGS            **#\n#*******************************************************************#\n### SQLite database (Current default database)\nLOCAL_DB_TYPE=sqlite\n\n### MYSQL database\n# LOCAL_DB_TYPE=mysql\n# LOCAL_DB_USER=root\n# LOCAL_DB_PASSWORD={your_password}\n# LOCAL_DB_HOST=127.0.0.1\n# LOCAL_DB_PORT=3306\n# LOCAL_DB_NAME=dbgpt\n### This option determines the storage location of conversation records. The default is not configured to the old version of duckdb. It can be optionally db or file (if the value is db, the database configured by LOCAL_DB will be used)\n#CHAT_HISTORY_STORE_TYPE=db\n\n#*******************************************************************#\n#**                         COMMANDS                              **#\n#*******************************************************************#\nEXECUTE_LOCAL_COMMANDS=False\n\n#*******************************************************************#\n#**            VECTOR STORE / KNOWLEDGE GRAPH SETTINGS            **#\n#*******************************************************************#\nVECTOR_STORE_TYPE=Chroma\nGRAPH_STORE_TYPE=TuGraph\nKNOWLEDGE_GRAPH_EXTRACT_SEARCH_TOP_SIZE=5\nKNOWLEDGE_GRAPH_EXTRACT_SEARCH_RECALL_SCORE=0.3\nKNOWLEDGE_GRAPH_COMMUNITY_SEARCH_TOP_SIZE=20\nKNOWLEDGE_GRAPH_COMMUNITY_SEARCH_RECALL_SCORE=0.0\n\nGRAPH_COMMUNITY_SUMMARY_ENABLED=True  # enable the graph community summary\nTRIPLET_GRAPH_ENABLED=True  # enable the graph search for triplets\nDOCUMENT_GRAPH_ENABLED=True  # enable the graph search for documents and chunks\n\nKNOWLEDGE_GRAPH_CHUNK_SEARCH_TOP_SIZE=5  # the top size of knowledge graph search for chunks\nKNOWLEDGE_GRAPH_EXTRACTION_BATCH_SIZE=20  # the batch size of triplet extraction from the text\nCOMMUNITY_SUMMARY_BATCH_SIZE=20  # the batch size of parallel community summary process\n\n### Chroma vector db config\n#CHROMA_PERSIST_PATH=/root/DB-GPT/pilot/data\n\n### Milvus vector db config\n#VECTOR_STORE_TYPE=Milvus\n#MILVUS_URL=127.0.0.1\n#MILVUS_PORT=19530\n#MILVUS_USERNAME\n#MILVUS_PASSWORD\n#MILVUS_SECURE=\n\n### Weaviate vector db config\n#VECTOR_STORE_TYPE=Weaviate\n#WEAVIATE_URL=https://kt-region-m8hcy0wc.weaviate.network\n\n## ElasticSearch vector db config\n#VECTOR_STORE_TYPE=ElasticSearch\nELASTICSEARCH_URL=127.0.0.1\nELASTICSEARCH_PORT=9200\nELASTICSEARCH_USERNAME=elastic\nELASTICSEARCH_PASSWORD={your_password}\n\n### TuGraph config\n#TUGRAPH_HOST=127.0.0.1\n#TUGRAPH_PORT=7687\n#TUGRAPH_USERNAME=admin\n#TUGRAPH_PASSWORD=73@TuGraph\n#TUGRAPH_VERTEX_TYPE=entity\n#TUGRAPH_EDGE_TYPE=relation\n#TUGRAPH_PLUGIN_NAMES=leiden\n\n#*******************************************************************#\n#**                  WebServer Language Support                   **#\n#*******************************************************************#\n# en, zh, fr, ja, ko, ru\nLANGUAGE=en\n#LANGUAGE=zh\n\n\n#*******************************************************************#\n# **    PROXY_SERVER (openai interface | chatGPT proxy service), use chatGPT as your LLM.\n# ** if your server can visit openai, please set PROXY_SERVER_URL=https://api.openai.com/v1/chat/completions\n# ** else if you have a chatgpt proxy server, you can set PROXY_SERVER_URL={your-proxy-serverip:port/xxx}\n#*******************************************************************#\nPROXY_API_KEY={your-openai-sk}\nPROXY_SERVER_URL=https://api.openai.com/v1/chat/completions\n\n# from https://bard.google.com/     f12-> application-> __Secure-1PSID\nBARD_PROXY_API_KEY={your-bard-token}\n\n#*******************************************************************#\n# **  PROXY_SERVER +                                              **#\n#*******************************************************************#\n\n# Aliyun tongyi\nTONGYI_PROXY_API_KEY={your-tongyi-sk}\n\n## Baidu wenxin\n#WEN_XIN_MODEL_VERSION={version}\n#WEN_XIN_API_KEY={your-wenxin-sk}\n#WEN_XIN_API_SECRET={your-wenxin-sct}\n\n## Zhipu\n#ZHIPU_MODEL_VERSION={version}\n#ZHIPU_PROXY_API_KEY={your-zhipu-sk}\n\n## Baichuan\n#BAICHUN_MODEL_NAME={version}\n#BAICHUAN_PROXY_API_KEY={your-baichuan-sk}\n#BAICHUAN_PROXY_API_SECRET={your-baichuan-sct}\n\n# Xunfei Spark\n#XUNFEI_SPARK_API_PASSWORD={your_api_password}\n#XUNFEI_SPARK_API_MODEL={version}\n\n## Yi Proxyllm, https://platform.lingyiwanwu.com/docs\n#YI_MODEL_VERSION=yi-34b-chat-0205\n#YI_API_BASE=https://api.lingyiwanwu.com/v1\n#YI_API_KEY={your-yi-api-key}\n\n## Moonshot Proxyllm, https://platform.moonshot.cn/docs/\n# MOONSHOT_MODEL_VERSION=moonshot-v1-8k\n# MOONSHOT_API_BASE=https://api.moonshot.cn/v1\n# MOONSHOT_API_KEY={your-moonshot-api-key}\n\n## Deepseek Proxyllm, https://platform.deepseek.com/api-docs/\n# DEEPSEEK_MODEL_VERSION=deepseek-chat\n# DEEPSEEK_API_BASE=https://api.deepseek.com/v1\n# DEEPSEEK_API_KEY={your-deepseek-api-key}\n\n\n#*******************************************************************#\n#**    SUMMARY_CONFIG                                             **#\n#*******************************************************************#\nSUMMARY_CONFIG=FAST\n\n#*******************************************************************#\n#**    MUlti-GPU                                                  **#\n#*******************************************************************#\n## See https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/\n## If CUDA_VISIBLE_DEVICES is not configured, all available gpus will be used\n# CUDA_VISIBLE_DEVICES=0\n## You can configure the maximum memory used by each GPU.\n# MAX_GPU_MEMORY=16Gib\n\n#*******************************************************************#\n#**                         LOG                                   **#\n#*******************************************************************#\n# FATAL, ERROR, WARNING, WARNING, INFO, DEBUG, NOTSET\nDBGPT_LOG_LEVEL=INFO\n# LOG dir, default: ./logs\n#DBGPT_LOG_DIR=\n\n\n#*******************************************************************#\n#**                         API_KEYS                              **#\n#*******************************************************************#\n# API_KEYS - The list of API keys that are allowed to access the API. Each of the below are an option, separated by commas.\n# API_KEYS=dbgpt\n\n#*******************************************************************#\n#**                         ENCRYPT                               **#\n#*******************************************************************#\n# ENCRYPT KEY - The key used to encrypt and decrypt the data\n# ENCRYPT_KEY=your_secret_key\n\n#*******************************************************************#\n#**                         File Server                           **#\n#*******************************************************************#\n## The local storage path of the file server, the default is pilot/data/file_server\n# FILE_SERVER_LOCAL_STORAGE_PATH =\n\n#*******************************************************************#\n#**                     Application Config                        **#\n#*******************************************************************#\n## Non-streaming scene retries\n# DBGPT_APP_SCENE_NON_STREAMING_RETRIES_BASE=1\n## Non-streaming scene parallelism\n# DBGPT_APP_SCENE_NON_STREAMING_PARALLELISM_BASE=1\n\n#*******************************************************************#\n#**                   Observability Config                        **#\n#*******************************************************************#\n## Whether to enable DB-GPT send trace to OpenTelemetry\n# TRACER_TO_OPEN_TELEMETRY=False\n## Following configurations are only valid when TRACER_TO_OPEN_TELEMETRY=True\n## More details see https://opentelemetry-python.readthedocs.io/en/latest/exporter/otlp/otlp.html\n# OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4317\n# OTEL_EXPORTER_OTLP_TRACES_INSECURE=False\n# OTEL_EXPORTER_OTLP_TRACES_CERTIFICATE=\n# OTEL_EXPORTER_OTLP_TRACES_HEADERS=\n# OTEL_EXPORTER_OTLP_TRACES_TIMEOUT=\n# OTEL_EXPORTER_OTLP_TRACES_COMPRESSION=\n\n#*******************************************************************#\n#**                     FINANCIAL CHAT Config                     **#\n#*******************************************************************#\n# FIN_REPORT_MODEL=/app/models/bge-large-zh\n\n## Turn off notebook display Python flow , which is enabled by default\nNOTE_BOOK_ENABLE=False\n\n## The agent historical message retention configuration defaults to the last two rounds.\n# MESSAGES_KEEP_START_ROUNDS=0\n# MESSAGES_KEEP_END_ROUNDS=2\n\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.3291015625,
          "content": "[flake8]\nexclude =\n  .eggs/\n  build/\n  */tests/*\n  *_private\nmax-line-length = 88\ninline-quotes = \"\nignore =\n  C408\n  C417\n  E121\n  E123\n  E126\n  E203\n  E226\n  E231\n  E24\n  E704\n  W503\n  W504\n  W605\n  I\n  N\n  B001\n  B002\n  B003\n  B004\n  B005\n  B007\n  B008\n  B009\n  B010\n  B011\n  B012\n  B013\n  B014\n  B015\n  B016\n  B017\navoid-escape = no\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.501953125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\nmessage/\ndbgpt/util/extensions/\n.env*\n.vscode\n.idea\n.chroma\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\n\nvar/\nwheels/\n/models/\n# Soft link\n/models\nplugins/\n\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n*.zuo\n*.zip\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test_py / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n# *.mo\n*.pot\n\n# Django stuff:\n*.log\n*.log.*\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n.DS_Store\nlogs\nnltk_data\n.vectordb\npilot/data/\npilot/nltk_data\npilot/mock_datas/db-gpt-test.db.wal\n\nlogswebserver.log.*\n.history/*\n.plugin_env\n/pilot/meta_data/alembic/versions/*\n/pilot/meta_data/*.db\n# Ignore for now\nthirdparty\n\n#web\n# dependencies\n/web/node_modules\n/web/yarn.lock\n\n.idea\n# next.js\n/web/.next/\n/web/out/\n\n# production\n/web/build\n\n# debug\n/web/npm-debug.log*\n/web/yarn-debug.log*\n/web/yarn-error.log*\n\n# local env files\n/web/.env.prod\n/web/.env\n\n# typescript\n*.tsbuildinfo\n/web/next-env.d.ts\n\n# Ignore awel DAG visualization files\n/examples/**/*.gv\n/examples/**/*.gv.pdf\n/i18n/locales/**/**/*_ai_translated.po\n/i18n/locales/**/**/*~\n"
        },
        {
          "name": ".isort.cfg",
          "type": "blob",
          "size": 0.3740234375,
          "content": "[settings]\n# This is to make isort compatible with Black. See\n# https://black.readthedocs.io/en/stable/the_black_code_style.html#how-black-wraps-lines.\nline_length=88\nprofile=black\nmulti_line_output=3\ninclude_trailing_comma=True\nuse_parentheses=True\nfloat_to_top=True\nfilter_files=True\n\nskip_glob=examples/notebook/*\nsections=FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER,AFTERRAY\n"
        },
        {
          "name": ".mypy.ini",
          "type": "blob",
          "size": 2.09375,
          "content": "[mypy]\nexclude = /tests/\n# plugins = pydantic.mypy\n[mypy-dbgpt.rag.*]\nstrict_optional = False\nignore_missing_imports = True\nfollow_imports = skip\n\n[mypy-dbgpt.app.*]\nfollow_imports = skip\n\n[mypy-dbgpt.serve.*]\nfollow_imports = skip\n\n[mypy-dbgpt.model.*]\nfollow_imports = skip\n\n[mypy-dbgpt.util.*]\nfollow_imports = skip\n\n[mypy-graphviz.*]\nignore_missing_imports = True\n\n[mypy-cachetools.*]\nignore_missing_imports = True\n\n[mypy-coloredlogs.*]\nignore_missing_imports = True\n\n[mypy-termcolor.*]\nignore_missing_imports = True\n\n[mypy-pydantic.*]\nstrict_optional = False\nignore_missing_imports = True\nfollow_imports = skip\n\n[mypy-sentence_transformers.*]\nignore_missing_imports = True\n\n[mypy-InstructorEmbedding.*]\nignore_missing_imports = True\n\n[mypy-llama_index.*]\nignore_missing_imports = True\n\n[mypy-langchain.*]\nignore_missing_imports = True\n\n[mypy-pptx.*]\nignore_missing_imports = True\n\n[mypy-docx.*]\nignore_missing_imports = True\n\n[mypy-markdown.*]\nignore_missing_imports = True\n\n[mypy-auto_gpt_plugin_template.*]\nignore_missing_imports = True\n\n[mypy-spacy.*]\nignore_missing_imports = True\nfollow_imports = skip\n\n[mypy-jieba.*]\nignore_missing_imports = True\n\n# Storage\n[mypy-msgpack.*]\nignore_missing_imports = True\n\n[mypy-rocksdict.*]\nignore_missing_imports = True\n\n[mypy-weaviate.*]\nignore_missing_imports = True\n\n[mypy-pymilvus.*]\nignore_missing_imports = True\n\n[mypy-elasticsearch.*]\nignore_missing_imports = True\n\n[mypy-cryptography.*]\nignore_missing_imports = True\n\n# Datasource\n[mypy-pyspark.*]\nignore_missing_imports = True\n\n[mypy-regex.*]\nignore_missing_imports = True\n\n[mypy-sqlparse.*]\nignore_missing_imports = True\n\n[mypy-clickhouse_connect.*]\nignore_missing_imports = True\n\n[mypy-fastchat.protocol.api_protocol]\nignore_missing_imports = True\n\n[mypy-neo4j.*]\nignore_missing_imports = True\n\n# Agent\n[mypy-seaborn.*]\nignore_missing_imports = True\n\n[mypy-unstructured.*]\nignore_missing_imports = True\n\n[mypy-rich.*]\nignore_missing_imports = True\n\n[mypy-ollama.*]\nignore_missing_imports = True\n\n[mypy-networkx.*]\nignore_missing_imports = True\n\n[mypy-pypdf.*]\nignore_missing_imports = True\n\n[mypy-qianfan.*]\nignore_missing_imports = True"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.111328125,
          "content": "# Please run command `pre-commit install` to install pre-commit hook\r\nrepos:\r\n  - repo: local\r\n    hooks:\r\n      - id: python-fmt\r\n        name: Python Format\r\n        entry: make fmt-check\r\n        language: system\r\n        exclude: '^dbgpt/app/static/|^web/'\r\n        types: [python]\r\n        stages: [commit]\r\n        pass_filenames: false\r\n        args: []\r\n      - id: python-test\r\n        name: Python Unit Test\r\n        entry: make test\r\n        language: system\r\n        exclude: '^dbgpt/app/static/|^web/'\r\n        types: [python]\r\n        stages: [commit]\r\n        pass_filenames: false\r\n        args: []\r\n      - id: python-test-doc\r\n        name: Python Doc Test\r\n        entry: make test-doc\r\n        language: system\r\n        exclude: '^dbgpt/app/static/|^web/'\r\n        types: [python]\r\n        stages: [commit]\r\n        pass_filenames: false\r\n        args: []\r\n      - id: python-lint-mypy\r\n        name: Python Lint mypy\r\n        entry: make mypy\r\n        language: system\r\n        exclude: '^dbgpt/app/static/|^web/'\r\n        types: [python]\r\n        stages: [commit]\r\n        pass_filenames: false\r\n        args: []\r\n\r\n"
        },
        {
          "name": "CODE_OF_CONDUCT",
          "type": "blob",
          "size": 5.056640625,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n[INSERT CONTACT METHOD].\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n*Community Impact*: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n*Consequence*: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n*Community Impact*: A violation through a single incident or series of\nactions.\n\n*Consequence*: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n*Community Impact*: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n*Consequence*: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n*Community Impact*: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n*Consequence*: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.015625,
          "content": "# Contribution \n\nFirst of all, thank you for considering contributing to this project. \nIt's people like you that make it a reality for the community. There are many ways to contribute, and we appreciate all of them.\n\nThis guide will help you get started with contributing to this project.\n\n## Fork The Repository\n\n1. Fork the repository you want to contribute to by clicking the \"Fork\" button on the project page.\n\n2. Clone the repository to your local machine using the following command:\n\n```\ngit clone https://github.com/<YOUR-GITHUB-USERNAME>/DB-GPT\n```\nPlease replace `<YOUR-GITHUB-USERNAME>` with your GitHub username.\n\n\n## Create A New Development Environment\n\n1. Create a new virtual environment using the following command:\n```\n# Make sure python >= 3.10\nconda create -n dbgpt_env python=3.10\nconda activate dbgpt_env\n```\n\n2. Change to the project directory using the following command:\n```\ncd DB-GPT\n```\n\n3. Install the project from the local source using the following command:\n```\n# it will take some minutes\npip install -e \".[default]\"\n```\n\n4. Install development requirements\n```\npip install -r requirements/dev-requirements.txt\npip install -r requirements/lint-requirements.txt\n```\n\n5. Install pre-commit hooks\n```\npre-commit install\n```\n\n6. Install `make` command\nThe `make` command has been installed by default on most Unix-based systems. If you not \nhave it, you can install it by searching on the internet.\n\n## New Branch And Make Changes\n\n1. Create a new branch for your changes using the following command:\n```\ngit checkout -b <branch-name>\n```\nPlease replace `<branch-name>` with a descriptive name for your branch.\n\n2. Make your changes to the code or documentation.\n\n3. Add tests for your changes if necessary.\n\n4. Format your code using the following command:\n```\nmake fmt\n```\n\n5. Run the tests using the following command:\n```\nmake test\n```\n\n6. Check types using the following command:\n```\nmake mypy\n```\n\n7. Check lint using the following command:\n```\nmake fmt-check\n```\n\n8. If all checks pass, you can add and commit your changes using the following commands:\n```\ngit add xxxx\n```\nmake sure to replace `xxxx` with the files you want to commit.\n\nthen commit your changes using the following command:\n```\ngit commit -m \"your commit message\"\n```\nPlease replace `your commit message` with a meaningful commit message.\n\nIt will take some time to get used to the process, but it's worth it. And it will run \nall git hooks and checks before you commit. If it fails, you need to fix the issues \nthen re-commit it.\n\n9. Push the changes to your forked repository using the following command:\n```\ngit push origin <branch-name>\n```\n\n## Create A Pull Request\n\n1. Go to the GitHub website and navigate to your forked repository.\n\n2. Click the \"New pull request\" button.\n\n3. Select the branch you just pushed to and the branch you want to merge into on the original repository.\nWrite necessary information about your changes and click \"Create pull request\".\n\n4. Wait for the project maintainer to review your changes and provide feedback.\n\nThat's it you made it ğŸ£â­â­\n\n"
        },
        {
          "name": "DISCKAIMER.md",
          "type": "blob",
          "size": 1.796875,
          "content": "# User Agreement and Disclaimer\n\n1. If you do not agree with any content of this statement, please stop using this software immediately. Once you start using this software product and service, it means that you have agreed to all the contents of this statement\n\n2. This disclaimer applies to all users of this software. This software reserves the right to modify and update this statement at any time, and notify users in the form of Github Readme, software updates, etc. Please review regularly and abide by the latest disclaimer.\n\n3. The original design intention of this project is to provide a basic framework/tool â€‹â€‹set, mainly focusing on RAGs,Agents, AWEL, etc. To keep the project simple and easy to use, we intentionally did not integrate any form of user login, authentication or authorization mechanism.\n\n4. If you plan to deploy this project into a production environment, it is strongly recommended to connect to existing third-party authentication services (such as OAuth, OpenID Connect, etc.) according to your specific needs, or to develop and maintain a complete set of user management and permissions yourself. control system.\n\n5. We encourage all developers to follow best practices to keep user data secure, but this is beyond the scope of this project. Therefore, always take appropriate security measures when handling sensitive information.\n\n6. Users are responsible for the security configuration in their applications, including but not limited to user account management, password policies, access control lists, etc.\n\n7. The project authors and contributors are not legally responsible for any direct or indirect losses caused by the use of this software.\n\n\nPlease read and understand all the contents of this disclaimer carefully before using this software, thank you for your understanding and support.\n\n \n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2023 magic.chen\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0576171875,
          "content": "include LICENSE\ninclude README.md\ninclude requirements.txt\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 3.72265625,
          "content": ".DEFAULT_GOAL := help\n\nSHELL=/bin/bash\nVENV = venv\n\n# Detect the operating system and set the virtualenv bin directory\nifeq ($(OS),Windows_NT)\n\tVENV_BIN=$(VENV)/Scripts\nelse\n\tVENV_BIN=$(VENV)/bin\nendif\n\nsetup: $(VENV)/bin/activate\n\n$(VENV)/bin/activate: $(VENV)/.venv-timestamp\n\n$(VENV)/.venv-timestamp: setup.py requirements\n\t# Create new virtual environment if setup.py has changed\n\tpython3 -m venv $(VENV)\n\t$(VENV_BIN)/pip install --upgrade pip\n\t$(VENV_BIN)/pip install -r requirements/dev-requirements.txt\n\t$(VENV_BIN)/pip install -r requirements/lint-requirements.txt\n\ttouch $(VENV)/.venv-timestamp\n\ntestenv: $(VENV)/.testenv\n\n$(VENV)/.testenv: $(VENV)/bin/activate\n\t# $(VENV_BIN)/pip install -e \".[framework]\"\n\t# the openai optional dependency is include framework and rag dependencies\n\t$(VENV_BIN)/pip install -e \".[openai]\"\n\ttouch $(VENV)/.testenv\n\n\n.PHONY: fmt\nfmt: setup ## Format Python code\n\t# TODO: Use isort to sort Python imports.\n\t# https://github.com/PyCQA/isort\n\t# $(VENV_BIN)/isort .\n\t$(VENV_BIN)/isort dbgpt/\n\t$(VENV_BIN)/isort --extend-skip=\"examples/notebook\" examples\n\t# https://github.com/psf/black\n\t$(VENV_BIN)/black --extend-exclude=\"examples/notebook\" .\n\t# TODO: Use blackdoc to format Python doctests.\n\t# https://blackdoc.readthedocs.io/en/latest/\n\t# $(VENV_BIN)/blackdoc .\n\t$(VENV_BIN)/blackdoc dbgpt\n\t$(VENV_BIN)/blackdoc examples\n\t# TODO: Use flake8 to enforce Python style guide.\n\t# https://flake8.pycqa.org/en/latest/\n\t$(VENV_BIN)/flake8 dbgpt/core/ dbgpt/rag/ dbgpt/storage/ dbgpt/datasource/ dbgpt/client/ dbgpt/agent/ dbgpt/vis/ dbgpt/experimental/\n\t# TODO: More package checks with flake8.\n\n.PHONY: fmt-check\nfmt-check: setup ## Check Python code formatting and style without making changes\n\t$(VENV_BIN)/isort --check-only dbgpt/\n\t$(VENV_BIN)/isort --check-only --extend-skip=\"examples/notebook\" examples\n\t$(VENV_BIN)/black --check --extend-exclude=\"examples/notebook\" .\n\t$(VENV_BIN)/blackdoc --check dbgpt examples\n\t$(VENV_BIN)/flake8 dbgpt/core/ dbgpt/rag/ dbgpt/storage/ dbgpt/datasource/ dbgpt/client/ dbgpt/agent/ dbgpt/vis/ dbgpt/experimental/\n\n.PHONY: pre-commit\npre-commit: fmt-check test test-doc mypy ## Run formatting and unit tests before committing\n\ntest: $(VENV)/.testenv ## Run unit tests\n\t$(VENV_BIN)/pytest dbgpt\n\n.PHONY: test-doc\ntest-doc: $(VENV)/.testenv ## Run doctests\n\t# -k \"not test_\" skips tests that are not doctests.\n\t$(VENV_BIN)/pytest --doctest-modules -k \"not test_\" dbgpt/core\n\n.PHONY: mypy\nmypy: $(VENV)/.testenv ## Run mypy checks\n\t# https://github.com/python/mypy\n\t$(VENV_BIN)/mypy --config-file .mypy.ini dbgpt/rag/ dbgpt/datasource/ dbgpt/client/ dbgpt/agent/ dbgpt/vis/ dbgpt/experimental/\n\t# rag depends on core and storage, so we not need to check it again.\n\t# $(VENV_BIN)/mypy --config-file .mypy.ini dbgpt/storage/\n\t# $(VENV_BIN)/mypy --config-file .mypy.ini dbgpt/core/\n\t# TODO: More package checks with mypy.\n\n.PHONY: coverage\ncoverage: setup ## Run tests and report coverage\n\t$(VENV_BIN)/pytest dbgpt --cov=dbgpt\n\n.PHONY: clean\nclean: ## Clean up the environment\n\trm -rf $(VENV)\n\tfind . -type f -name '*.pyc' -delete\n\tfind . -type d -name '__pycache__' -delete\n\tfind . -type d -name '.pytest_cache' -delete\n\tfind . -type d -name '.coverage' -delete\n\n.PHONY: clean-dist\nclean-dist: ## Clean up the distribution\n\trm -rf dist/ *.egg-info build/\n\n.PHONY: package\npackage: clean-dist ## Package the project for distribution\n\tIS_DEV_MODE=false python setup.py sdist bdist_wheel\n\n.PHONY: upload\nupload: ## Upload the package to PyPI\n\t# upload to testpypi: twine upload --repository testpypi dist/*\n\ttwine upload dist/*\n\n.PHONY: help\nhelp:  ## Display this help screen\n\t@echo \"Available commands:\"\n\t@grep -E '^[a-z.A-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}; {printf \"  \\033[36m%-18s\\033[0m %s\\n\", $$1, $$2}' | sort"
        },
        {
          "name": "README.ja.md",
          "type": "blob",
          "size": 18.044921875,
          "content": "# DB-GPT: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã®å¯¾è©±ã‚’é©æ–°ã™ã‚‹ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆLLMæŠ€è¡“\n\n<p align=\"left\">\n  <img src=\"./assets/LOGO.png\" width=\"100%\" />\n</p>\n\n<div align=\"center\">\n  <p>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"stars\" src=\"https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"forks\" src=\"https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://opensource.org/licenses/MIT\">\n      <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" />\n    </a>\n     <a href=\"https://github.com/eosphoros-ai/DB-GPT/releases\">\n      <img alt=\"Release Notes\" src=\"https://img.shields.io/github/release/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT/issues\">\n      <img alt=\"Open Issues\" src=\"https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://discord.gg/7uQnPuveTY\">\n      <img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat\" />\n    </a>\n    <a href=\"https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA\">\n      <img alt=\"Slack\" src=\"https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack\" />\n    </a>\n    <a href=\"https://codespaces.new/eosphoros-ai/DB-GPT\">\n      <img alt=\"Open in GitHub Codespaces\" src=\"https://github.com/codespaces/badge.svg\" />\n    </a>\n  </p>\n\n[**è‹±èª**](README.md) | [**ä¸­å›½èª**](README.zh.md) | [**Discord**](https://discord.gg/7uQnPuveTY) | [**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**](https://docs.dbgpt.site) | [**å¾®ä¿¡**](https://github.com/eosphoros-ai/DB-GPT/blob/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC) | [**ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**](https://github.com/eosphoros-ai/community) | [**è«–æ–‡**](https://arxiv.org/pdf/2312.17449.pdf)\n\n</div>\n\n## DB-GPTã¨ã¯ä½•ã‹ï¼Ÿ\n\nğŸ¤– **DB-GPTã¯ã€AWELï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¼è¨€èªï¼‰ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‚™ãˆãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®AIãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ—ãƒªé–‹ç™ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚**\n\nå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®åˆ†é‡ã§ã®ã‚¤ãƒ³ãƒ•ãƒ©ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€SMMFï¼ˆãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ç®¡ç†ï¼‰ã€Text2SQLåŠ¹æœã®æœ€é©åŒ–ã€RAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨æœ€é©åŒ–ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å”åŠ›ã€AWELï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ãªã©ã€è¤‡æ•°ã®æŠ€è¡“æ©Ÿèƒ½ã®é–‹ç™ºã‚’é€šã˜ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ã§ä¾¿åˆ©ã«ã—ã¾ã™ã€‚\n\nğŸš€ **ãƒ‡ãƒ¼ã‚¿3.0æ™‚ä»£ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åŸºç›¤ã¨ã—ã¦ã€ä¼æ¥­ã‚„é–‹ç™ºè€…ãŒã‚ˆã‚Šå°‘ãªã„ã‚³ãƒ¼ãƒ‰ã§ç‹¬è‡ªã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚**\n\n### AIãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ—ãƒª\n---\n- ğŸ”¥ğŸ”¥ğŸ”¥ [V0.5.0ãƒªãƒªãƒ¼ã‚¹ | ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é€šã˜ã¦ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™º](https://docs.dbgpt.site/docs/changelog/Released_V0.5.0)\n---\n\n![Data-awels](https://github.com/eosphoros-ai/DB-GPT/assets/17919400/37d116fc-d9dd-4efa-b4df-9ab02b22541c)\n\n![Data-Apps](https://github.com/eosphoros-ai/DB-GPT/assets/17919400/a7bf6d65-92d1-4f0e-aaf0-259ccdde22fd)\n\n![dashboard-images](https://github.com/eosphoros-ai/DB-GPT/assets/17919400/1849a79a-f7fd-40cf-bc9c-b117a041dd6a)\n\n## ç›®æ¬¡\n- [ç´¹ä»‹](#ç´¹ä»‹)\n- [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)\n- [ç‰¹å¾´](#ç‰¹å¾´)\n- [è²¢çŒ®](#è²¢çŒ®)\n- [é€£çµ¡å…ˆ](#é€£çµ¡å…ˆæƒ…å ±)\n\n## ç´¹ä»‹\nDB-GPTã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ä»¥ä¸‹ã®å›³ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ï¼š\n\n<p align=\"center\">\n  <img src=\"./assets/dbgpt.png\" width=\"800\" />\n</p>\n\nã‚³ã‚¢æ©Ÿèƒ½ã«ã¯ä»¥ä¸‹ã®éƒ¨åˆ†ãŒå«ã¾ã‚Œã¾ã™ï¼š\n\n- **RAGï¼ˆRetrieval Augmented Generationï¼‰**ï¼šç¾åœ¨ã€RAGã¯æœ€ã‚‚å®Ÿç”¨çš„ã«å®Ÿè£…ã•ã‚Œã€ç·Šæ€¥ã«å¿…è¦ã¨ã•ã‚Œã‚‹é ˜åŸŸã§ã™ã€‚DB-GPTã¯ã€RAGã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€RAGã«åŸºã¥ããƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ã™ã§ã«å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚\n\n- **GBIï¼ˆGenerative Business Intelligenceï¼‰**ï¼šGenerative BIã¯DB-GPTãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ã‚¢æ©Ÿèƒ½ã®1ã¤ã§ã‚ã‚Šã€ä¼æ¥­ã®ãƒ¬ãƒãƒ¼ãƒˆåˆ†æã¨ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹æŠ€è¡“ã‚’æä¾›ã—ã¾ã™ã€‚\n\n- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ï¼šãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ä»»æ„ã®ä¼æ¥­ãŒå‚ç›´ãŠã‚ˆã³ãƒ‹ãƒƒãƒãªãƒ‰ãƒ¡ã‚¤ãƒ³ã§å®Ÿè£…ã™ã‚‹ãŸã‚ã«ä¸å¯æ¬ ãªæ©Ÿèƒ½ã§ã™ã€‚DB-GPTã¯ã€DB-GPTãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«çµ±åˆã•ã‚Œã‚‹å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚æœ€è¿‘ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å–ã‚Šçµ„ã¿ã§ã¯ã€Spiderãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦82.5%ã®å®Ÿè¡Œç²¾åº¦ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚\n\n- **ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ï¼šDB-GPTã¯ã€ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ç¶™ç¶šçš„ã«æ„æ€æ±ºå®šã‚’è¡Œã„ã€å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹è‡ªå·±é€²åŒ–å‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚\n\n- **ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼**ï¼šãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã¯ã€ä¸»ã«å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®æ™‚ä»£ã«ãŠã‘ã‚‹ä¿¡é ¼ã§ãã‚‹çŸ¥è­˜ã¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å‡¦ç†ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã™ã€‚\n\n- **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**ï¼šDB-GPTã®ã‚³ã‚¢æ©Ÿèƒ½ã«ç”Ÿç”£ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«æ¥ç¶šã™ã‚‹ãŸã‚ã«ã€ã•ã¾ã–ã¾ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆã—ã¾ã™ã€‚\n\n### ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n- [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub) å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ä¸Šã§ã®æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€é«˜æ€§èƒ½ãªText-to-SQLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€‚\n\n- [dbgpts](https://github.com/eosphoros-ai/dbgpts)  dbgptsã¯ã€DB-GPTä¸Šã§æ§‹ç¯‰ã•ã‚ŒãŸã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ—ãƒªã€AWELã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã€AWELãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€ãŠã‚ˆã³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å«ã‚€å…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚\n\n#### Text2SQLãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n- ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹LLM\n  - [x] LLaMA\n  - [x] LLaMA-2\n  - [x] BLOOM\n  - [x] BLOOMZ\n  - [x] Falcon\n  - [x] Baichuan\n  - [x] Baichuan2\n  - [x] InternLM\n  - [x] Qwen\n  - [x] XVERSE\n  - [x] ChatGLM2\n\n-  SFTç²¾åº¦\n2023å¹´10æœˆ10æ—¥ç¾åœ¨ã€ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¦130å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€Spiderãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§GPT-4ã‚’è¶…ãˆã‚‹å®Ÿè¡Œç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸï¼\n\n[Text2SQLãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- [DB-GPT-Plugins](https://github.com/eosphoros-ai/DB-GPT-Plugins) Auto-GPTãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ç›´æ¥å®Ÿè¡Œã§ãã‚‹DB-GPTãƒ—ãƒ©ã‚°ã‚¤ãƒ³\n- [GPT-Vis](https://github.com/eosphoros-ai/GPT-Vis) å¯è¦–åŒ–ãƒ—ãƒ­ãƒˆã‚³ãƒ«\n\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=macos&logoColor=F0F0F0)\n![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)\n\n[**ä½¿ç”¨ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«**](http://docs.dbgpt.site/docs/overview)\n- [**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**](http://docs.dbgpt.site/docs/installation)\n  - [Docker](https://docs.dbgpt.site/docs/installation/docker)\n  - [ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰](https://docs.dbgpt.site/docs/installation/sourcecode)\n- [**ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ**](http://docs.dbgpt.site/docs/quickstart)\n- [**ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³**](http://docs.dbgpt.site/docs/operation_manual)\n  - [ã‚¢ãƒ—ãƒªã®ä½¿ç”¨](https://docs.dbgpt.site/docs/application/app_usage)\n  - [AWELãƒ•ãƒ­ãƒ¼ã®ä½¿ç”¨](https://docs.dbgpt.site/docs/application/awel_flow_usage)\n- [**ãƒ‡ãƒãƒƒã‚°**](http://docs.dbgpt.site/docs/operation_manual/advanced_tutorial/debugging)\n- [**é«˜åº¦ãªä½¿ç”¨æ³•**](https://docs.dbgpt.site/docs/application/advanced_tutorial/cli)\n  - [SMMF](https://docs.dbgpt.site/docs/application/advanced_tutorial/smmf)\n  - [ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](https://docs.dbgpt.site/docs/application/fine_tuning_manual/dbgpt_hub)\n  - [AWEL](http://docs.dbgpt.cn/docs/awel/tutorial)\n\n## ç‰¹å¾´\n\nç¾åœ¨ã€ç§ãŸã¡ã¯ã„ãã¤ã‹ã®ä¸»è¦ãªæ©Ÿèƒ½ã‚’ç´¹ä»‹ã—ã¦ã€ç¾åœ¨ã®èƒ½åŠ›ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼š\n- **ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³Q&Aï¼†ãƒ‡ãƒ¼ã‚¿å‡¦ç†**\n\n  DB-GPTãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ã‚’æ”¹å–„ã—ã€æ§‹é€ åŒ–ãŠã‚ˆã³éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã®åŠ¹ç‡çš„ãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨æ¤œç´¢ã‚’å¯èƒ½ã«ã™ã‚‹ä¸€é€£ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã«ã¯ã€è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ã‚µãƒãƒ¼ãƒˆã€ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®çµ±åˆã€ãŠã‚ˆã³å¤§é‡ã®æƒ…å ±ã‚’åŠ¹æœçš„ã«ç®¡ç†ã™ã‚‹ãŸã‚ã®çµ±ä¸€ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨æ¤œç´¢æ©Ÿèƒ½ãŒå«ã¾ã‚Œã¾ã™ã€‚\n\n- **ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼†GBIï¼ˆGenerative Business Intelligenceï¼‰**\n\n  DB-GPTãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€Excelã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹ãªã©ã®ã•ã¾ã–ã¾ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨ã®è‡ªç„¶è¨€èªã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªå¯¾è©±ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’ç…§ä¼šãŠã‚ˆã³å–å¾—ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç°¡ç´ åŒ–ã—ã€ç›´æ„Ÿçš„ãªä¼šè©±ã‚’è¡Œã„ã€æ´å¯Ÿã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€DB-GPTã¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è²´é‡ãªãƒ‡ãƒ¼ã‚¿ã®è¦ç´„ã¨è§£é‡ˆã‚’æä¾›ã—ã¾ã™ã€‚\n\n- **ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼†ãƒ—ãƒ©ã‚°ã‚¤ãƒ³**\n\n  ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã€Auto-GPTãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒˆã‚³ãƒ«æ¨™æº–ã«æº–æ‹ ã—ã¦ã„ã¾ã™ã€‚\n\n- **è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°Text2SQL**\n\n  ç§ãŸã¡ã¯ã¾ãŸã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã€Text2SQLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€LoRA/QLoRA/Pturningãªã©ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã‚’ä¸­å¿ƒã«ã€è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è»½é‡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€Text-to-SQLãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¢ã‚»ãƒ³ãƒ–ãƒªãƒ©ã‚¤ãƒ³ã®ã‚ˆã†ã«ç°¡å˜ã«ã—ã¾ã™ã€‚[DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- **SMMFï¼ˆã‚µãƒ¼ãƒ“ã‚¹æŒ‡å‘ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰**\n\n  ç§ãŸã¡ã¯ã€LLaMA/LLaMA2ã€Baichuanã€ChatGLMã€Wenxinã€Tongyiã€Zhipuãªã©ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãŠã‚ˆã³APIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®æ•°åã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’å«ã‚€å¹…åºƒã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n\n  - ãƒ‹ãƒ¥ãƒ¼ã‚¹\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-34B-Chat](https://huggingface.co/01-ai/Yi-1.5-34B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-9B-Chat](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-110B-Chat](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2b-it](https://huggingface.co/google/gemma-2b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [SOLAR-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen-72B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)\n  - [ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹LLMã®è©³ç´°](http://docs.dbgpt.site/docs/modules/smmf)\n\n- **ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**\n\n  ç§ãŸã¡ã¯ã€ã•ã¾ã–ã¾ãªæŠ€è¡“ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’ç¢ºä¿ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã¯ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆåŒ–ã¨ãƒ—ãƒ­ã‚­ã‚·ã®éè­˜åˆ¥åŒ–ãŒå«ã¾ã‚Œã¾ã™ã€‚\n\n- ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹\n  - [ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹](http://docs.dbgpt.site/docs/modules/connections)\n\n## ç”»åƒ\nğŸŒ [AutoDLã‚¤ãƒ¡ãƒ¼ã‚¸](https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt)\n\n### è¨€èªåˆ‡ã‚Šæ›¿ãˆ\n    .envè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§LANGUAGEãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ã¦ã€ç•°ãªã‚‹è¨€èªã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‹±èªã§ã™ï¼ˆä¸­å›½èªï¼šzhã€è‹±èªï¼šenã€ä»–ã®è¨€èªã¯å¾Œã§è¿½åŠ ã•ã‚Œã¾ã™ï¼‰ã€‚\n\n## è²¢çŒ®\n\n- æ–°ã—ã„è²¢çŒ®ã®ãŸã‚ã®è©³ç´°ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[è²¢çŒ®æ–¹æ³•](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n### è²¢çŒ®è€…ã‚¦ã‚©ãƒ¼ãƒ«\n<a href=\"https://github.com/eosphoros-ai/DB-GPT/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&max=200\" />\n</a>\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\nMITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼ˆMITï¼‰\n\n## å¼•ç”¨\nã‚‚ã—`DB-GPT`ãŒã‚ãªãŸã®ç ”ç©¶ã‚„é–‹ç™ºã«å½¹ç«‹ã¤ã¨æ„Ÿã˜ãŸå ´åˆã€ä»¥ä¸‹ã®è«–æ–‡ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚\n\nDB-GPTã®å…¨ä½“çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„å ´åˆã¯ã€<a href=\"https://arxiv.org/abs/2312.17449\" target=\"_blank\">è«–æ–‡</a>ã¨<a href=\"https://arxiv.org/abs/2404.10209\" target=\"_blank\">è«–æ–‡</a>ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚\n\nDB-GPTã‚’ä½¿ç”¨ã—ã¦Agenté–‹ç™ºã«é–¢ã™ã‚‹å†…å®¹ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„å ´åˆã¯ã€<a href=\"https://arxiv.org/abs/2412.13520\" target=\"_blank\">è«–æ–‡</a>ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚ \n```bibtex\n@article{xue2023dbgpt,\n      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, \n      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},\n      year={2023},\n      journal={arXiv preprint arXiv:2312.17449},\n      url={https://arxiv.org/abs/2312.17449}\n}\n@misc{huang2024romasrolebasedmultiagentdatabase,\n      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, \n      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},\n      year={2024},\n      eprint={2412.13520},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2412.13520}, \n}\n@inproceedings{xue2024demonstration,\n      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, \n      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},\n      year={2024},\n      booktitle = \"Proceedings of the VLDB Endowment\",\n      url={https://arxiv.org/abs/2404.10209}\n}\n```\n\n## é€£çµ¡å…ˆæƒ…å ±\nã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ§‹ç¯‰ã«é–¢ã™ã‚‹ã‚¢ã‚¤ãƒ‡ã‚¢ãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚\n[![](https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat)](https://discord.gg/7uQnPuveTY)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=csunny/DB-GPT&type=Date)](https://star-history.com/#csunny/DB-GPT)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.283203125,
          "content": "# DB-GPT: AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents\n\n<p align=\"left\">\n  <img src=\"./assets/LOGO.png\" width=\"100%\" />\n</p>\n\n<div align=\"center\">\n  <p>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"stars\" src=\"https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"forks\" src=\"https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://opensource.org/licenses/MIT\">\n      <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" />\n    </a>\n     <a href=\"https://github.com/eosphoros-ai/DB-GPT/releases\">\n      <img alt=\"Release Notes\" src=\"https://img.shields.io/github/release/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT/issues\">\n      <img alt=\"Open Issues\" src=\"https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://discord.gg/7uQnPuveTY\">\n      <img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat\" />\n    </a>\n    <a href=\"https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA\">\n      <img alt=\"Slack\" src=\"https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack\" />\n    </a>\n    <a href=\"https://codespaces.new/eosphoros-ai/DB-GPT\">\n      <img alt=\"Open in GitHub Codespaces\" src=\"https://github.com/codespaces/badge.svg\" />\n    </a>\n  </p>\n\n\n[**ç®€ä½“ä¸­æ–‡**](README.zh.md) | [**æ—¥æœ¬èª**](README.ja.md) | [**Discord**](https://discord.gg/7uQnPuveTY) | [**Documents**](https://docs.dbgpt.site) | [**å¾®ä¿¡**](https://github.com/eosphoros-ai/DB-GPT/blob/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC) | [**Community**](https://github.com/eosphoros-ai/community) | [**Paper**](https://arxiv.org/pdf/2312.17449.pdf)\n\n</div>\n\n## What is DB-GPT?\n\nğŸ¤– **DB-GPT is an open source AI native data app development framework with AWEL(Agentic Workflow Expression Language) and agents**. \n\nThe purpose is to build infrastructure in the field of large models, through the development of multiple technical capabilities such as multi-model management (SMMF), Text2SQL effect optimization, RAG framework and optimization, Multi-Agents framework collaboration, AWEL (agent workflow orchestration), etc. Which makes large model applications with data simpler and more convenient.\n\nğŸš€ **In the Data 3.0 era, based on models and databases, enterprises and developers can build their own bespoke applications with less code.**\n\n### DISCKAIMER\n- [disckaimer](./DISCKAIMER.md)\n\n### AI-Native Data App \n---\n- ğŸ”¥ğŸ”¥ğŸ”¥ [Released V0.6.0 | A set of significant upgrades](https://docs.dbgpt.cn/docs/changelog/Released_V0.6.0)\n  - [The AWEL upgrade to 2.0]()\n  - [GraphRAG]()\n  - [AI Native Data App construction and management]()\n  - [The GPT-Vis upgrade, supporting a variety of visualization charts]()\n  - [Support Text2NLU and Text2GQL fine-tuning]()\n  - [Support Intent recognition, slot filling, and Prompt management]()\n\n---\n\n![app_chat_v0 6](https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113)\n\n![app_manage_chat_data_v0 6](https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611)\n\n![chat_dashboard_display_v0 6](https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9)\n\n![agent_prompt_awel_v0 6](https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc)\n\n## Contents\n- [Introduction](#introduction)\n- [Install](#install)\n- [Features](#features)\n- [Contribution](#contribution)\n- [Contact](#contact-information)\n\n## Introduction \nThe architecture of DB-GPT is shown in the following figure:\n\n<p align=\"center\">\n  <img src=\"./assets/dbgpt.png\" width=\"800\" />\n</p>\n\nThe core capabilities include the following parts:\n\n- **RAG (Retrieval Augmented Generation)**: RAG is currently the most practically implemented and urgently needed domain. DB-GPT has already implemented a framework based on RAG, allowing users to build knowledge-based applications using the RAG capabilities of DB-GPT.\n\n- **GBI (Generative Business Intelligence)**: Generative BI is one of the core capabilities of the DB-GPT project, providing the foundational data intelligence technology to build enterprise report analysis and business insights.\n\n- **Fine-tuning Framework**: Model fine-tuning is an indispensable capability for any enterprise to implement in vertical and niche domains. DB-GPT provides a complete fine-tuning framework that integrates seamlessly with the DB-GPT project. In recent fine-tuning efforts, an accuracy rate based on the Spider dataset has been achieved at 82.5%.\n\n- **Data-Driven Multi-Agents Framework**: DB-GPT offers a data-driven self-evolving multi-agents framework, aiming to continuously make decisions and execute based on data.\n\n- **Data Factory**: The Data Factory is mainly about cleaning and processing trustworthy knowledge and data in the era of large models.\n\n- **Data Sources**: Integrating various data sources to seamlessly connect production business data to the core capabilities of DB-GPT.\n\n### SubModule\n- [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub) Text-to-SQL workflow with high performance by applying Supervised Fine-Tuning (SFT) on Large Language Models (LLMs).\n\n- [dbgpts](https://github.com/eosphoros-ai/dbgpts)  dbgpts is the official repository which contains some data appsã€AWEL operatorsã€AWEL workflow templates and agents which build upon DB-GPT.\n\n#### Text2SQL Finetune\n- support llms\n  - [x] LLaMA\n  - [x] LLaMA-2\n  - [x] BLOOM\n  - [x] BLOOMZ\n  - [x] Falcon\n  - [x] Baichuan\n  - [x] Baichuan2\n  - [x] InternLM\n  - [x] Qwen\n  - [x] XVERSE\n  - [x] ChatGLM2\n\n-  SFT Accuracy\nAs of October 10, 2023, through the fine-tuning of an open-source model with 13 billion parameters using this project, we have achieved execution accuracy on the Spider dataset that surpasses even GPT-4!\n\n[More Information about Text2SQL finetune](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- [DB-GPT-Plugins](https://github.com/eosphoros-ai/DB-GPT-Plugins) DB-GPT Plugins that can run Auto-GPT plugin directly\n- [GPT-Vis](https://github.com/eosphoros-ai/GPT-Vis) Visualization protocol\n\n## Install \n![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=macos&logoColor=F0F0F0)\n![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)\n\n[**Usage Tutorial**](http://docs.dbgpt.cn/docs/overview)\n- [**Install**](http://docs.dbgpt.cn/docs/installation)\n  - [Docker](http://docs.dbgpt.cn/docs/installation/docker)\n  - [Source Code](http://docs.dbgpt.cn/docs/installation/sourcecode)\n- [**Quickstart**](http://docs.dbgpt.cn/docs/quickstart)\n- [**Application**](http://docs.dbgpt.cn/docs/operation_manual)\n  - [Development Guide](http://docs.dbgpt.cn/docs/cookbook/app/data_analysis_app_develop) \n  - [App Usage](http://docs.dbgpt.cn/docs/application/app_usage)\n  - [AWEL Flow Usage](http://docs.dbgpt.cn/docs/application/awel_flow_usage)\n- [**Debugging**](http://docs.dbgpt.cn/docs/operation_manual/advanced_tutorial/debugging)\n- [**Advanced Usage**](http://docs.dbgpt.cn/docs/application/advanced_tutorial/cli)\n  - [SMMF](http://docs.dbgpt.cn/docs/application/advanced_tutorial/smmf)\n  - [Finetune](http://docs.dbgpt.cn/docs/application/fine_tuning_manual/dbgpt_hub)\n  - [AWEL](http://docs.dbgpt.cn/docs/awel/tutorial)\n\n\n## Features\n\nAt present, we have introduced several key features to showcase our current capabilities:\n- **Private Domain Q&A & Data Processing**\n\n  The DB-GPT project offers a range of functionalities designed to improve knowledge base construction and enable efficient storage and retrieval of both structured and unstructured data. These functionalities include built-in support for uploading multiple file formats, the ability to integrate custom data extraction plug-ins, and unified vector storage and retrieval capabilities for effectively managing large volumes of information.\n\n- **Multi-Data Source & GBI(Generative Business intelligence)**\n\n  The DB-GPT project facilitates seamless natural language interaction with diverse data sources, including Excel, databases, and data warehouses. It simplifies the process of querying and retrieving information from these sources, empowering users to engage in intuitive conversations and gain insights. Moreover, DB-GPT supports the generation of analytical reports, providing users with valuable data summaries and interpretations.\n\n- **Multi-Agents&Plugins**\n\n  It offers support for custom plug-ins to perform various tasks and natively integrates the Auto-GPT plug-in model. The Agents protocol adheres to the Agent Protocol standard.\n\n- **Automated Fine-tuning text2SQL**\n\n  We've also developed an automated fine-tuning lightweight framework centred on large language models (LLMs), Text2SQL datasets, LoRA/QLoRA/Pturning, and other fine-tuning methods. This framework simplifies Text-to-SQL fine-tuning, making it as straightforward as an assembly line process. [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- **SMMF(Service-oriented Multi-model Management Framework)**\n\n  We offer extensive model support, including dozens of large language models (LLMs) from both open-source and API agents, such as LLaMA/LLaMA2, Baichuan, ChatGLM, Wenxin, Tongyi, Zhipu, and many more. \n\n  - News\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-34B-Chat](https://huggingface.co/01-ai/Yi-1.5-34B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-9B-Chat](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-110B-Chat](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2b-it](https://huggingface.co/google/gemma-2b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [SOLAR-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen-72B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)\n  - [More Supported LLMs](http://docs.dbgpt.site/docs/modules/smmf)\n\n- **Privacy and Security**\n  \n  We ensure the privacy and security of data through the implementation of various technologies, including privatized large models and proxy desensitization.\n\n- Support Datasources\n  - [Datasources](http://docs.dbgpt.cn/docs/modules/connections)\n\n## Image\nğŸŒ [AutoDL Image](https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt)\n\n\n### Language Switching\n    In the .env configuration file, modify the LANGUAGE parameter to switch to different languages. The default is English (Chinese: zh, English: en, other languages to be added later).\n\n## Contribution\n\n- To check detailed guidelines for new contributions, please refer [how to contribute](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)\n\n### Contributors Wall\n<a href=\"https://github.com/eosphoros-ai/DB-GPT/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&max=200\" />\n</a>\n\n\n## Licence\nThe MIT License (MIT)\n\n## Citation\nIf you want to understand the overall architecture of DB-GPT, please cite <a href=\"https://arxiv.org/abs/2312.17449\" target=\"_blank\">paper</a> and <a href=\"https:// arxiv.org/abs/2404.10209\" target=\"_blank\">Paper</a>\n\nIf you want to learn about using DB-GPT for Agent development, please cite the <a href=\"https://arxiv.org/abs/2412.13520\" target=\"_blank\">paper</a>\n```bibtex\n@article{xue2023dbgpt,\n      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, \n      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},\n      year={2023},\n      journal={arXiv preprint arXiv:2312.17449},\n      url={https://arxiv.org/abs/2312.17449}\n}\n@misc{huang2024romasrolebasedmultiagentdatabase,\n      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, \n      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},\n      year={2024},\n      eprint={2412.13520},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2412.13520}, \n}\n@inproceedings{xue2024demonstration,\n      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, \n      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},\n      year={2024},\n      booktitle = \"Proceedings of the VLDB Endowment\",\n      url={https://arxiv.org/abs/2404.10209}\n}\n```\n\n\n## Contact Information\nWe are working on building a community, if you have any ideas for building the community, feel free to contact us.\n[![](https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat)](https://discord.gg/7uQnPuveTY)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=csunny/DB-GPT&type=Date)](https://star-history.com/#csunny/DB-GPT)\n"
        },
        {
          "name": "README.zh.md",
          "type": "blob",
          "size": 17.5859375,
          "content": "# DB-GPT: AIåŸç”Ÿæ•°æ®åº”ç”¨å¼€å‘æ¡†æ¶\n\n<p align=\"left\">\n  <img src=\"./assets/LOGO.png\" width=\"100%\" />\n</p>\n\n\n<div align=\"center\">\n  <p>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"stars\" src=\"https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"forks\" src=\"https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://opensource.org/licenses/MIT\">\n      <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" />\n    </a>\n     <a href=\"https://github.com/eosphoros-ai/DB-GPT/releases\">\n      <img alt=\"Release Notes\" src=\"https://img.shields.io/github/release/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT/issues\">\n      <img alt=\"Open Issues\" src=\"https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://discord.gg/7uQnPuveTY\">\n      <img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat\" />\n    </a>\n    <a href=\"https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA\">\n      <img alt=\"Slack\" src=\"https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack\" />\n    </a>\n    <a href=\"https://codespaces.new/eosphoros-ai/DB-GPT\">\n      <img alt=\"Open in GitHub Codespaces\" src=\"https://github.com/codespaces/badge.svg\" />\n    </a>\n  </p>\n\n[**English**](README.md) | [**Discord**](https://discord.gg/7uQnPuveTY) | [**æ–‡æ¡£**](https://www.yuque.com/eosphoros/dbgpt-docs/bex30nsv60ru0fmx) | [**å¾®ä¿¡**](https://github.com/eosphoros-ai/DB-GPT/blob/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC) | [**ç¤¾åŒº**](https://github.com/eosphoros-ai/community) | [**Paper**](https://arxiv.org/pdf/2312.17449.pdf)\n</div>\n\n## DB-GPT æ˜¯ä»€ä¹ˆï¼Ÿ\n\nğŸ¤–ï¸ **DB-GPTæ˜¯ä¸€ä¸ªå¼€æºçš„AIåŸç”Ÿæ•°æ®åº”ç”¨å¼€å‘æ¡†æ¶(AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents)ã€‚**\n\nç›®çš„æ˜¯æ„å»ºå¤§æ¨¡å‹é¢†åŸŸçš„åŸºç¡€è®¾æ–½ï¼Œé€šè¿‡å¼€å‘å¤šæ¨¡å‹ç®¡ç†(SMMF)ã€Text2SQLæ•ˆæœä¼˜åŒ–ã€RAGæ¡†æ¶ä»¥åŠä¼˜åŒ–ã€Multi-Agentsæ¡†æ¶åä½œã€AWEL(æ™ºèƒ½ä½“å·¥ä½œæµç¼–æ’)ç­‰å¤šç§æŠ€æœ¯èƒ½åŠ›ï¼Œè®©å›´ç»•æ•°æ®åº“æ„å»ºå¤§æ¨¡å‹åº”ç”¨æ›´ç®€å•ï¼Œæ›´æ–¹ä¾¿ã€‚ \n\nğŸš€ **æ•°æ®3.0 æ—¶ä»£ï¼ŒåŸºäºæ¨¡å‹ã€æ•°æ®åº“ï¼Œä¼ä¸š/å¼€å‘è€…å¯ä»¥ç”¨æ›´å°‘çš„ä»£ç æ­å»ºè‡ªå·±çš„ä¸“å±åº”ç”¨ã€‚**\n\n## æ•ˆæœæ¼”ç¤º\n\n### AIåŸç”Ÿæ•°æ®æ™ºèƒ½åº”ç”¨\n---\n- [V0.6.0å‘å¸ƒâ€”â€”ä¸€ç³»åˆ—é‡å¤§åŠŸèƒ½æ›´æ–°](https://www.yuque.com/eosphoros/dbgpt-docs/fho86kk4e9y4rkpd)\n  - AWELåè®®å‡çº§2.0ï¼Œæ”¯æŒæ›´å¤æ‚çš„ç¼–æ’ï¼ŒåŒæ—¶ä¼˜åŒ–äº†å‰ç«¯å¯è§†åŒ–ä¸äº¤äº’èƒ½åŠ›ã€‚\n  - æ”¯æŒæ•°æ®åº”ç”¨çš„åˆ›å»ºä¸ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œæä¾›å¤šç§åº”ç”¨æ„å»ºæ¨¡å¼ã€‚1. å¤šæ™ºèƒ½ä½“è‡ªåŠ¨è§„åˆ’æ¨¡å¼ã€2. ä»»åŠ¡æµç¼–æ’æ¨¡å¼ã€3. å•ä¸€æ™ºèƒ½ä½“æ¨¡å¼ã€4. åŸç”Ÿåº”ç”¨æ¨¡å¼\n  - GraphRAGæ”¯æŒå›¾ç¤¾åŒºæ‘˜è¦ä¸æ··åˆæ£€ç´¢ï¼Œæ€§èƒ½ä¸æ£€ç´¢æ•ˆæœæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒæ—¶æ”¯æŒä¸°å¯Œçš„å‰ç«¯å¯è§†åŒ–ã€‚\n  - æ”¯æŒæ„å›¾è¯†åˆ«ã€æ§½ä½å¡«å……ä¸Promptç®¡ç†ã€‚\n  - GPT-Viså‰ç«¯å¯è§†åŒ–å‡çº§ï¼Œæ”¯æŒæ›´ä¸°å¯Œçš„å¯è§†åŒ–å›¾è¡¨ã€‚ \n  - æ”¯æŒText2NLUä¸Text2GQLå¾®è°ƒ, å³æ–°å¢æ„å›¾åˆ†ç±»ä¸ä»è‡ªç„¶è¯­è¨€åˆ°å›¾è¯­è¨€çš„å¾®è°ƒã€‚ \n\n\n### Data Agents \n\n![app_chat_v0 6](https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113)\n\n![app_manage_chat_data_v0 6](https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611)\n\n![chat_dashboard_display_v0 6](https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9)\n\n![agent_prompt_awel_v0 6](https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc)\n\n\n## ç›®å½•\n- [æ¶æ„æ–¹æ¡ˆ](#æ¶æ„æ–¹æ¡ˆ)\n- [å®‰è£…](#å®‰è£…)\n- [ç‰¹æ€§ç®€ä»‹](#ç‰¹æ€§ä¸€è§ˆ)\n- [è´¡çŒ®](#è´¡çŒ®)\n- [è·¯çº¿å›¾](#è·¯çº¿å›¾)\n- [è”ç³»æˆ‘ä»¬](#è”ç³»æˆ‘ä»¬)\n\n## æ¶æ„æ–¹æ¡ˆ\n\n<p align=\"center\">\n  <img src=\"./assets/dbgpt.png\" width=\"800px\" />\n</p>\n\næ ¸å¿ƒèƒ½åŠ›ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†:\n- **RAG(Retrieval Augmented Generation)**ï¼ŒRAGæ˜¯å½“ä¸‹è½åœ°å®è·µæœ€å¤šï¼Œä¹Ÿæ˜¯æœ€è¿«åˆ‡çš„é¢†åŸŸï¼ŒDB-GPTç›®å‰å·²ç»å®ç°äº†ä¸€å¥—åŸºäºRAGçš„æ¡†æ¶ï¼Œç”¨æˆ·å¯ä»¥åŸºäºDB-GPTçš„RAGèƒ½åŠ›æ„å»ºçŸ¥è¯†ç±»åº”ç”¨ã€‚ \n\n- **GBI**ï¼šç”Ÿæˆå¼BIæ˜¯DB-GPTé¡¹ç›®çš„æ ¸å¿ƒèƒ½åŠ›ä¹‹ä¸€ï¼Œä¸ºæ„å»ºä¼ä¸šæŠ¥è¡¨åˆ†æã€ä¸šåŠ¡æ´å¯Ÿæä¾›åŸºç¡€çš„æ•°æ™ºåŒ–æŠ€æœ¯ä¿éšœã€‚ \n\n- **å¾®è°ƒæ¡†æ¶**:  æ¨¡å‹å¾®è°ƒæ˜¯ä»»ä½•ä¸€ä¸ªä¼ä¸šåœ¨å‚ç›´ã€ç»†åˆ†é¢†åŸŸè½åœ°ä¸å¯æˆ–ç¼ºçš„èƒ½åŠ›ï¼ŒDB-GPTæä¾›äº†å®Œæ•´çš„å¾®è°ƒæ¡†æ¶ï¼Œå®ç°ä¸DB-GPTé¡¹ç›®çš„æ— ç¼æ‰“é€šï¼Œåœ¨æœ€è¿‘çš„å¾®è°ƒä¸­ï¼ŒåŸºäºspiderçš„å‡†ç¡®ç‡å·²ç»åšåˆ°äº†82.5%\n\n- **æ•°æ®é©±åŠ¨çš„Multi-Agentsæ¡†æ¶**:  DB-GPTæä¾›äº†æ•°æ®é©±åŠ¨çš„è‡ªè¿›åŒ–Multi-Agentsæ¡†æ¶ï¼Œç›®æ ‡æ˜¯å¯ä»¥æŒç»­åŸºäºæ•°æ®åšå†³ç­–ä¸æ‰§è¡Œã€‚ \n\n- **æ•°æ®å·¥å‚**: æ•°æ®å·¥å‚ä¸»è¦æ˜¯åœ¨å¤§æ¨¡å‹æ—¶ä»£ï¼Œåšå¯ä¿¡çŸ¥è¯†ã€æ•°æ®çš„æ¸…æ´—åŠ å·¥ã€‚ \n\n- **æ•°æ®æº**: å¯¹æ¥å„ç±»æ•°æ®æºï¼Œå®ç°ç”Ÿäº§ä¸šåŠ¡æ•°æ®æ— ç¼å¯¹æ¥åˆ°DB-GPTæ ¸å¿ƒèƒ½åŠ›ã€‚ \n\n### RAGç”Ÿäº§è½åœ°å®è·µæ¶æ„\n<p align=\"center\">\n  <img src=\"./assets/RAG-IN-ACTION.jpg\" width=\"800px\" />\n</p>\n\n### å­æ¨¡å—\n- [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub) é€šè¿‡å¾®è°ƒæ¥æŒç»­æå‡Text2SQLæ•ˆæœ \n- [DB-GPT-Plugins](https://github.com/eosphoros-ai/DB-GPT-Plugins) DB-GPT æ’ä»¶ä»“åº“, å…¼å®¹Auto-GPT\n- [GPT-Vis](https://github.com/eosphoros-ai/DB-GPT-Web) å¯è§†åŒ–åè®® \n\n- [dbgpts](https://github.com/eosphoros-ai/dbgpts)  dbgpts æ˜¯å®˜æ–¹æä¾›çš„æ•°æ®åº”ç”¨ä»“åº“, åŒ…å«æ•°æ®æ™ºèƒ½åº”ç”¨, æ™ºèƒ½ä½“ç¼–æ’æµç¨‹æ¨¡ç‰ˆ, é€šç”¨ç®—å­ç­‰æ„å»ºåœ¨DB-GPTä¹‹ä¸Šçš„èµ„æºã€‚ \n\n## å®‰è£…\n\n![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=macos&logoColor=F0F0F0)\n![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)\n\n[**æ•™ç¨‹**](https://www.yuque.com/eosphoros/dbgpt-docs/bex30nsv60ru0fmx)\n- [**å¿«é€Ÿå¼€å§‹**](https://www.yuque.com/eosphoros/dbgpt-docs/ew0kf1plm0bru2ga)\n  - [æºç å®‰è£…](https://www.yuque.com/eosphoros/dbgpt-docs/urh3fcx8tu0s9xmb)\n  - [Dockerå®‰è£…](https://www.yuque.com/eosphoros/dbgpt-docs/glf87qg4xxcyrp89)\n  - [Docker Composeå®‰è£…](https://www.yuque.com/eosphoros/dbgpt-docs/wwdu11e0v5nkfzin)\n- [**ä½¿ç”¨æ‰‹å†Œ**](https://www.yuque.com/eosphoros/dbgpt-docs/tkspdd0tcy2vlnu4)\n  - [çŸ¥è¯†åº“](https://www.yuque.com/eosphoros/dbgpt-docs/ycyz3d9b62fccqxh)\n  - [æ•°æ®å¯¹è¯](https://www.yuque.com/eosphoros/dbgpt-docs/gd9hbhi1dextqgbz)\n  - [Excelå¯¹è¯](https://www.yuque.com/eosphoros/dbgpt-docs/prugoype0xd2g4bb)\n  - [æ•°æ®åº“å¯¹è¯](https://www.yuque.com/eosphoros/dbgpt-docs/wswpv3zcm2c9snmg)\n  - [æŠ¥è¡¨åˆ†æ](https://www.yuque.com/eosphoros/dbgpt-docs/vsv49p33eg4p5xc1)\n  - [Agents](https://www.yuque.com/eosphoros/dbgpt-docs/pom41m7oqtdd57hm)\n- [**è¿›é˜¶æ•™ç¨‹**](https://www.yuque.com/eosphoros/dbgpt-docs/dxalqb8wsv2xkm5f)\n  - [æ•°æ™ºåº”ç”¨å¼€å‘](https://www.yuque.com/eosphoros/dbgpt-docs/ancwnrsk9agc6e4w)\n  - [æ™ºèƒ½ä½“å·¥ä½œæµä½¿ç”¨](https://www.yuque.com/eosphoros/dbgpt-docs/hcomfb3yrleg7gmq)\n  - [æ™ºèƒ½åº”ç”¨ä½¿ç”¨](https://www.yuque.com/eosphoros/dbgpt-docs/aiagvxeb86iarq6r)\n  - [å¤šæ¨¡å‹ç®¡ç†](https://www.yuque.com/eosphoros/dbgpt-docs/huzgcf2abzvqy8uv)\n  - [å‘½ä»¤è¡Œä½¿ç”¨](https://www.yuque.com/eosphoros/dbgpt-docs/gd4kgumgd004aly8)\n- [**æ¨¡å‹æœåŠ¡éƒ¨ç½²**](https://www.yuque.com/eosphoros/dbgpt-docs/vubxiv9cqed5mc6o)\n  - [å•æœºéƒ¨ç½²](https://www.yuque.com/eosphoros/dbgpt-docs/kwg1ed88lu5fgawb)\n  - [é›†ç¾¤éƒ¨ç½²](https://www.yuque.com/eosphoros/dbgpt-docs/gmbp9619ytyn2v1s)\n  - [vLLM](https://www.yuque.com/eosphoros/dbgpt-docs/bhy9igdvanx1uluf)\n- [**å¦‚ä½•Debug**](https://www.yuque.com/eosphoros/dbgpt-docs/eyg0ocbc2ce3q95r)\n- [**AWEL**](https://www.yuque.com/eosphoros/dbgpt-docs/zozbzslbfk0m0op5)\n- [**FAQ**](https://www.yuque.com/eosphoros/dbgpt-docs/gomtc46qonmyt44l)\n\n## ç‰¹æ€§ä¸€è§ˆ\n- **ç§åŸŸé—®ç­”&æ•°æ®å¤„ç†&RAG**\n\n  æ”¯æŒå†…ç½®ã€å¤šæ–‡ä»¶æ ¼å¼ä¸Šä¼ ã€æ’ä»¶è‡ªæŠ“å–ç­‰æ–¹å¼è‡ªå®šä¹‰æ„å»ºçŸ¥è¯†åº“ï¼Œå¯¹æµ·é‡ç»“æ„åŒ–ï¼Œéç»“æ„åŒ–æ•°æ®åšç»Ÿä¸€å‘é‡å­˜å‚¨ä¸æ£€ç´¢\n\n- **å¤šæ•°æ®æº&GBI**\n\n  æ”¯æŒè‡ªç„¶è¯­è¨€ä¸Excelã€æ•°æ®åº“ã€æ•°ä»“ç­‰å¤šç§æ•°æ®æºäº¤äº’ï¼Œå¹¶æ”¯æŒåˆ†ææŠ¥å‘Šã€‚\n\n- **è‡ªåŠ¨åŒ–å¾®è°ƒ**\n\n  å›´ç»•å¤§è¯­è¨€æ¨¡å‹ã€Text2SQLæ•°æ®é›†ã€LoRA/QLoRA/Pturningç­‰å¾®è°ƒæ–¹æ³•æ„å»ºçš„è‡ªåŠ¨åŒ–å¾®è°ƒè½»é‡æ¡†æ¶, è®©TextSQLå¾®è°ƒåƒæµæ°´çº¿ä¸€æ ·æ–¹ä¾¿ã€‚è¯¦è§: [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- **æ•°æ®é©±åŠ¨çš„Agentsæ’ä»¶**\n\n  æ”¯æŒè‡ªå®šä¹‰æ’ä»¶æ‰§è¡Œä»»åŠ¡ï¼ŒåŸç”Ÿæ”¯æŒAuto-GPTæ’ä»¶æ¨¡å‹ï¼ŒAgentsåè®®é‡‡ç”¨Agent Protocolæ ‡å‡†\n\n- **å¤šæ¨¡å‹æ”¯æŒä¸ç®¡ç†**\n\n  æµ·é‡æ¨¡å‹æ”¯æŒï¼ŒåŒ…æ‹¬å¼€æºã€APIä»£ç†ç­‰å‡ åç§å¤§è¯­è¨€æ¨¡å‹ã€‚å¦‚LLaMA/LLaMA2ã€Baichuanã€ChatGLMã€æ–‡å¿ƒã€é€šä¹‰ã€æ™ºè°±ç­‰ã€‚å½“å‰å·²æ”¯æŒå¦‚ä¸‹æ¨¡å‹: \n\n  - æ–°å¢æ”¯æŒæ¨¡å‹\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-34B-Chat](https://huggingface.co/01-ai/Yi-1.5-34B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-9B-Chat](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-110B-Chat](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2b-it](https://huggingface.co/google/gemma-2b-it)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [SOLAR-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen-72B-Chat)\n    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)\n  - [æ›´å¤šå¼€æºæ¨¡å‹](https://www.yuque.com/eosphoros/dbgpt-docs/iqaaqwriwhp6zslc#qQktR)\n\n  - æ”¯æŒåœ¨çº¿ä»£ç†æ¨¡å‹\n    - [x] [DeepSeek.deepseek-chat](https://platform.deepseek.com/api-docs/)\n    - [x] [Ollama.API](https://github.com/ollama/ollama/blob/main/docs/api.md)\n    - [x] [æœˆä¹‹æš—é¢.Moonshot](https://platform.moonshot.cn/docs/)\n    - [x] [é›¶ä¸€ä¸‡ç‰©.Yi](https://platform.lingyiwanwu.com/docs)\n    - [x] [OpenAIÂ·ChatGPT](https://api.openai.com/)\n    - [x] [ç™¾å·Â·Baichuan](https://platform.baichuan-ai.com/)\n    - [x] [é˜¿é‡ŒÂ·é€šä¹‰](https://www.aliyun.com/product/dashscope)\n    - [x] [ç™¾åº¦Â·æ–‡å¿ƒ](https://cloud.baidu.com/product/wenxinworkshop?track=dingbutonglan)\n    - [x] [æ™ºè°±Â·ChatGLM](http://open.bigmodel.cn/)\n    - [x] [è®¯é£Â·æ˜Ÿç«](https://xinghuo.xfyun.cn/)\n    - [x] [GoogleÂ·Bard](https://bard.google.com/)\n    - [x] [GoogleÂ·Gemini](https://makersuite.google.com/app/apikey)\n\n- **éšç§å®‰å…¨**\n\n  é€šè¿‡ç§æœ‰åŒ–å¤§æ¨¡å‹ã€ä»£ç†è„±æ•ç­‰å¤šç§æŠ€æœ¯ä¿éšœæ•°æ®çš„éšç§å®‰å…¨ã€‚\n\n- [æ”¯æŒæ•°æ®æº](https://www.yuque.com/eosphoros/dbgpt-docs/rc4r27ybmdwg9472)\n\n\n\n## Image\n\nğŸŒ [AutoDLé•œåƒ](https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt)\n\nğŸŒ [å°ç¨‹åºäº‘éƒ¨ç½²](https://www.yuque.com/eosphoros/dbgpt-docs/ek12ly8k661tbyn8)\n\n### å¤šè¯­è¨€åˆ‡æ¢\n\nåœ¨.env é…ç½®æ–‡ä»¶å½“ä¸­ï¼Œä¿®æ”¹LANGUAGEå‚æ•°æ¥åˆ‡æ¢ä½¿ç”¨ä¸åŒçš„è¯­è¨€ï¼Œé»˜è®¤æ˜¯è‹±æ–‡(ä¸­æ–‡zh, è‹±æ–‡en, å…¶ä»–è¯­è¨€å¾…è¡¥å……)\n\n## ä½¿ç”¨è¯´æ˜\n\n### å¤šæ¨¡å‹ä½¿ç”¨\n\n- [ä½¿ç”¨æŒ‡å—](https://www.yuque.com/eosphoros/dbgpt-docs/huzgcf2abzvqy8uv)\n\n### æ•°æ®Agentsä½¿ç”¨\n\n- [æ•°æ®Agents](https://www.yuque.com/eosphoros/dbgpt-docs/gwz4rayfuwz78fbq)\n\n## è´¡çŒ®\n\næ›´åŠ è¯¦ç»†çš„è´¡çŒ®æŒ‡å—è¯·å‚è€ƒ[å¦‚ä½•è´¡çŒ®](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªç”¨äºæ•°æ®åº“çš„å¤æ‚ä¸”åˆ›æ–°çš„å·¥å…·, æˆ‘ä»¬çš„é¡¹ç›®ä¹Ÿåœ¨ç´§æ€¥çš„å¼€å‘å½“ä¸­, ä¼šé™†ç»­å‘å¸ƒä¸€äº›æ–°çš„featureã€‚å¦‚åœ¨ä½¿ç”¨å½“ä¸­æœ‰ä»»ä½•å…·ä½“é—®é¢˜, ä¼˜å…ˆåœ¨é¡¹ç›®ä¸‹æissue, å¦‚æœ‰éœ€è¦, è¯·è”ç³»å¦‚ä¸‹å¾®ä¿¡ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©ï¼ŒåŒæ—¶ä¹Ÿéå¸¸æ¬¢è¿å¤§å®¶å‚ä¸åˆ°é¡¹ç›®å»ºè®¾ä¸­ã€‚\n\n### è´¡çŒ®è€…æ¦œå• \n<a href=\"https://github.com/eosphoros-ai/DB-GPT/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&max=200\" />\n</a>\n\n\n## Licence\n\nThe MIT License (MIT)\n\n## å¼•ç”¨\nå¦‚æœæ‚¨å‘ç°`DB-GPT`å¯¹æ‚¨çš„ç ”ç©¶æˆ–å¼€å‘æœ‰ç”¨ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼Œå…¶ä¸­ï¼š\n\nå¦‚æœæ‚¨æƒ³äº†è§£DB-GPTæ•´ä½“æ¶æ„ï¼Œè¯·å¼•ç”¨<a href=\"https://arxiv.org/abs/2312.17449\" target=\"_blank\">è®ºæ–‡</a>å’Œ<a href=\"https://arxiv.org/abs/2404.10209\" target=\"_blank\">è®ºæ–‡</a>\n\nå¦‚æœæ‚¨æƒ³äº†è§£ä½¿ç”¨DB-GPTè¿›è¡ŒAgentå¼€å‘ç›¸å…³çš„å†…å®¹ï¼Œè¯·å¼•ç”¨<a href=\"https://arxiv.org/abs/2412.13520\" target=\"_blank\">è®ºæ–‡</a>\n\n```bibtex\n@article{xue2023dbgpt,\n      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, \n      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},\n      year={2023},\n      journal={arXiv preprint arXiv:2312.17449},\n      url={https://arxiv.org/abs/2312.17449}\n}\n@misc{huang2024romasrolebasedmultiagentdatabase,\n      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, \n      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},\n      year={2024},\n      eprint={2412.13520},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2412.13520}, \n}\n@inproceedings{xue2024demonstration,\n      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, \n      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},\n      year={2024},\n      booktitle = \"Proceedings of the VLDB Endowment\",\n      url={https://arxiv.org/abs/2404.10209}\n}\n```\n\n## è”ç³»æˆ‘ä»¬\n\n  **è¯´æ˜: ç”±äºå¾®ä¿¡ç¾¤äººæ•°ä¸Šé™çš„é™åˆ¶, æˆ‘ä»¬çš„ç­”ç–‘ä¸é—®é¢˜æ”¯æŒä¼˜å…ˆä¼šåœ¨é’‰é’‰å¤§ç¾¤è¿›è¡Œã€‚**\n<div style=\"display: flex; justify-content: space-around;\">\n    <figure style=\"display: flex; flex-direction: column;\">\n        <img src=\"./assets/ding.jpg\" alt=\"å›¾ç‰‡2\" style=\"width: 220px;\">\n        <p style=\"text-align: center;\">\n          é’‰é’‰\n        </p>\n    </figure>\n    <figure style=\"display: flex; flex-direction: column;\">\n        <img src=\"./assets/wechat.jpg\" alt=\"å›¾ç‰‡1\" style=\"width: 200px;\">\n        <p style=\"text-align: center;\">\n          å¾®ä¿¡\n        </p> \n    </figure>\n</div>\n\n<!-- <p align=\"center\">\n  <img src=\"./assets/wechat.jpg\" width=\"300px\" />\n</p> -->\n\n[![Star History Chart](https://api.star-history.com/svg?repos=csunny/DB-GPT&type=Date)](https://star-history.com/#csunny/DB-GPT)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "dbgpt",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 1.4150390625,
          "content": "version: '3.10'\n\nservices:\n  db:\n    image: mysql/mysql-server\n    environment:\n      MYSQL_USER: 'user'\n      MYSQL_PASSWORD: 'password'\n      MYSQL_ROOT_PASSWORD: 'aa123456'\n    ports:\n      - 3306:3306\n    volumes:\n      - dbgpt-myql-db:/var/lib/mysql\n      - ./docker/examples/my.cnf:/etc/my.cnf\n      - ./docker/examples/sqls:/docker-entrypoint-initdb.d\n      - ./assets/schema/dbgpt.sql:/docker-entrypoint-initdb.d/dbgpt.sql\n    restart: unless-stopped\n    networks:\n      - dbgptnet\n  webserver:\n    image: eosphorosai/dbgpt:latest\n    command: python3 dbgpt/app/dbgpt_server.py\n    environment:\n      - LOCAL_DB_HOST=db\n      - LOCAL_DB_PASSWORD=aa123456\n      - ALLOWLISTED_PLUGINS=db_dashboard\n      - LLM_MODEL=glm-4-9b-chat\n    depends_on:\n      - db\n    volumes:\n      - /data:/data\n      # Please modify it to your own model directory\n      - /data/models:/app/models\n      - dbgpt-data:/app/pilot/data\n      - dbgpt-message:/app/pilot/message\n    env_file:\n      - .env.template\n    ports:\n      - 5670:5670/tcp\n    # webserver may be failed, it must wait all sqls in /docker-entrypoint-initdb.d execute finish.\n    restart: unless-stopped\n    networks:\n      - dbgptnet\n    ipc: host\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              capabilities: [gpu]\nvolumes:\n  dbgpt-myql-db:\n  dbgpt-data:\n  dbgpt-message:\nnetworks:\n  dbgptnet:\n    driver: bridge\n    name: dbgptnet"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "i18n",
          "type": "tree",
          "content": null
        },
        {
          "name": "pilot",
          "type": "tree",
          "content": null
        },
        {
          "name": "plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 26.72265625,
          "content": "import functools\nimport json\nimport os\nimport platform\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport urllib.request\nfrom enum import Enum\nfrom typing import Callable, List, Optional, Tuple\nfrom urllib.parse import quote, urlparse\n\nimport setuptools\nfrom setuptools import find_packages\n\nwith open(\"README.md\", mode=\"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nIS_DEV_MODE = os.getenv(\"IS_DEV_MODE\", \"true\").lower() == \"true\"\n# If you modify the version, please modify the version in the following files:\n# dbgpt/_version.py\nDB_GPT_VERSION = os.getenv(\"DB_GPT_VERSION\", \"0.6.3\")\n\nBUILD_NO_CACHE = os.getenv(\"BUILD_NO_CACHE\", \"true\").lower() == \"true\"\nLLAMA_CPP_GPU_ACCELERATION = (\n    os.getenv(\"LLAMA_CPP_GPU_ACCELERATION\", \"true\").lower() == \"true\"\n)\nBUILD_FROM_SOURCE = os.getenv(\"BUILD_FROM_SOURCE\", \"false\").lower() == \"true\"\nBUILD_FROM_SOURCE_URL_FAST_CHAT = os.getenv(\n    \"BUILD_FROM_SOURCE_URL_FAST_CHAT\", \"git+https://github.com/lm-sys/FastChat.git\"\n)\nBUILD_VERSION_OPENAI = os.getenv(\"BUILD_VERSION_OPENAI\")\nINCLUDE_QUANTIZATION = os.getenv(\"INCLUDE_QUANTIZATION\", \"true\").lower() == \"true\"\nINCLUDE_OBSERVABILITY = os.getenv(\"INCLUDE_OBSERVABILITY\", \"true\").lower() == \"true\"\n\n\ndef parse_requirements(file_name: str) -> List[str]:\n    with open(file_name) as f:\n        return [\n            require.strip()\n            for require in f\n            if require.strip() and not require.startswith(\"#\")\n        ]\n\n\ndef find_python():\n    python_path = sys.executable\n    print(python_path)\n    if not python_path:\n        print(\"Python command not found.\")\n        return None\n    return python_path\n\n\ndef get_latest_version(package_name: str, index_url: str, default_version: str):\n    python_command = find_python()\n    if not python_command:\n        print(\"Python command not found.\")\n        return default_version\n\n    command_index_versions = [\n        python_command,\n        \"-m\",\n        \"pip\",\n        \"index\",\n        \"versions\",\n        package_name,\n        \"--index-url\",\n        index_url,\n    ]\n\n    result_index_versions = subprocess.run(\n        command_index_versions, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    if result_index_versions.returncode == 0:\n        output = result_index_versions.stdout.decode()\n        lines = output.split(\"\\n\")\n        for line in lines:\n            if \"Available versions:\" in line:\n                available_versions = line.split(\":\")[1].strip()\n                latest_version = available_versions.split(\",\")[0].strip()\n                # Query for compatibility with the latest version of torch\n                if package_name == \"torch\" or \"torchvision\":\n                    latest_version = latest_version.split(\"+\")[0]\n                return latest_version\n    else:\n        command_simulate_install = [\n            python_command,\n            \"-m\",\n            \"pip\",\n            \"install\",\n            f\"{package_name}==\",\n        ]\n\n        result_simulate_install = subprocess.run(\n            command_simulate_install, stderr=subprocess.PIPE\n        )\n        print(result_simulate_install)\n        stderr_output = result_simulate_install.stderr.decode()\n        print(stderr_output)\n        match = re.search(r\"from versions: (.+?)\\)\", stderr_output)\n        if match:\n            available_versions = match.group(1).split(\", \")\n            latest_version = available_versions[-1].strip()\n            return latest_version\n    return default_version\n\n\ndef encode_url(package_url: str) -> str:\n    parsed_url = urlparse(package_url)\n    encoded_path = quote(parsed_url.path)\n    safe_url = parsed_url._replace(path=encoded_path).geturl()\n    return safe_url, parsed_url.path\n\n\ndef cache_package(package_url: str, package_name: str, is_windows: bool = False):\n    safe_url, parsed_url = encode_url(package_url)\n    if BUILD_NO_CACHE:\n        return safe_url\n\n    from pip._internal.utils.appdirs import user_cache_dir\n\n    filename = os.path.basename(parsed_url)\n    cache_dir = os.path.join(user_cache_dir(\"pip\"), \"http\", \"wheels\", package_name)\n    os.makedirs(cache_dir, exist_ok=True)\n\n    local_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(local_path):\n        temp_path = local_path + \".tmp\"\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n        try:\n            print(f\"Download {safe_url} to {local_path}\")\n            urllib.request.urlretrieve(safe_url, temp_path)\n            shutil.move(temp_path, local_path)\n        finally:\n            if os.path.exists(temp_path):\n                os.remove(temp_path)\n    return f\"file:///{local_path}\" if is_windows else f\"file://{local_path}\"\n\n\nclass SetupSpec:\n    def __init__(self) -> None:\n        self.extras: dict = {}\n        self.install_requires: List[str] = []\n\n    @property\n    def unique_extras(self) -> dict[str, list[str]]:\n        unique_extras = {}\n        for k, v in self.extras.items():\n            unique_extras[k] = list(set(v))\n        return unique_extras\n\n\nsetup_spec = SetupSpec()\n\n\nclass AVXType(Enum):\n    BASIC = \"basic\"\n    AVX = \"AVX\"\n    AVX2 = \"AVX2\"\n    AVX512 = \"AVX512\"\n\n    @staticmethod\n    def of_type(avx: str):\n        for item in AVXType:\n            if item._value_ == avx:\n                return item\n        return None\n\n\nclass OSType(Enum):\n    WINDOWS = \"win\"\n    LINUX = \"linux\"\n    DARWIN = \"darwin\"\n    OTHER = \"other\"\n\n\n@functools.cache\ndef get_cpu_avx_support() -> Tuple[OSType, AVXType]:\n    system = platform.system()\n    os_type = OSType.OTHER\n    cpu_avx = AVXType.BASIC\n    env_cpu_avx = AVXType.of_type(os.getenv(\"DBGPT_LLAMA_CPP_AVX\"))\n\n    if \"windows\" in system.lower():\n        os_type = OSType.WINDOWS\n        output = \"avx2\"\n        print(\"Current platform is windows, use avx2 as default cpu architecture\")\n    elif system == \"Linux\":\n        os_type = OSType.LINUX\n        if os.path.exists(\"/etc/alpine-release\"):\n            # For Alpine, we'll check /proc/cpuinfo directly\n            with open(\"/proc/cpuinfo\", \"r\") as f:\n                output = f.read()\n        else:\n            result = subprocess.run(\n                [\"lscpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n            )\n            output = result.stdout.decode()\n    elif system == \"Darwin\":\n        os_type = OSType.DARWIN\n        result = subprocess.run(\n            [\"sysctl\", \"-a\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        output = result.stdout.decode()\n    else:\n        os_type = OSType.OTHER\n        print(\"Unsupported OS to get cpu avx, use default\")\n        return os_type, env_cpu_avx if env_cpu_avx else cpu_avx\n\n    if \"avx512\" in output.lower():\n        cpu_avx = AVXType.AVX512\n    elif \"avx2\" in output.lower():\n        cpu_avx = AVXType.AVX2\n    elif \"avx \" in output.lower():\n        # cpu_avx =  AVXType.AVX\n        pass\n    return os_type, env_cpu_avx if env_cpu_avx else cpu_avx\n\n\ndef get_cuda_version_from_torch():\n    try:\n        import torch\n\n        return torch.version.cuda\n    except:\n        return None\n\n\ndef get_cuda_version_from_nvcc():\n    try:\n        output = subprocess.check_output([\"nvcc\", \"--version\"])\n        version_line = [\n            line for line in output.decode(\"utf-8\").split(\"\\n\") if \"release\" in line\n        ][0]\n        return version_line.split(\"release\")[-1].strip().split(\",\")[0]\n    except:\n        return None\n\n\ndef get_cuda_version_from_nvidia_smi():\n    try:\n        output = subprocess.check_output([\"nvidia-smi\"]).decode(\"utf-8\")\n        match = re.search(r\"CUDA Version:\\s+(\\d+\\.\\d+)\", output)\n        if match:\n            return match.group(1)\n        else:\n            return None\n    except:\n        return None\n\n\ndef get_cuda_version() -> str:\n    try:\n        cuda_version = get_cuda_version_from_torch()\n        if not cuda_version:\n            cuda_version = get_cuda_version_from_nvcc()\n        if not cuda_version:\n            cuda_version = get_cuda_version_from_nvidia_smi()\n        return cuda_version\n    except Exception:\n        return None\n\n\ndef _build_wheels(\n    pkg_name: str,\n    pkg_version: str,\n    base_url: str = None,\n    base_url_func: Callable[[str, str, str], str] = None,\n    pkg_file_func: Callable[[str, str, str, str, OSType], str] = None,\n    supported_cuda_versions: List[str] = [\"11.8\", \"12.1\"],\n) -> Optional[str]:\n    \"\"\"\n    Build the URL for the package wheel file based on the package name, version, and CUDA version.\n    Args:\n        pkg_name (str): The name of the package.\n        pkg_version (str): The version of the package.\n        base_url (str): The base URL for downloading the package.\n        base_url_func (Callable): A function to generate the base URL.\n        pkg_file_func (Callable): build package file function.\n            function params: pkg_name, pkg_version, cuda_version, py_version, OSType\n        supported_cuda_versions (List[str]): The list of supported CUDA versions.\n    Returns:\n        Optional[str]: The URL for the package wheel file.\n    \"\"\"\n    os_type, _ = get_cpu_avx_support()\n    cuda_version = get_cuda_version()\n    py_version = platform.python_version()\n    py_version = \"cp\" + \"\".join(py_version.split(\".\")[0:2])\n    if os_type == OSType.DARWIN or not cuda_version:\n        return None\n\n    if cuda_version in supported_cuda_versions:\n        cuda_version = cuda_version\n    else:\n        print(\n            f\"Warning: Your CUDA version {cuda_version} is not in our set supported_cuda_versions , we will use our set version.\"\n        )\n        if cuda_version < \"12.1\":\n            cuda_version = supported_cuda_versions[0]\n        else:\n            cuda_version = supported_cuda_versions[-1]\n\n    cuda_version = \"cu\" + cuda_version.replace(\".\", \"\")\n    os_pkg_name = \"linux_x86_64\" if os_type == OSType.LINUX else \"win_amd64\"\n    if base_url_func:\n        base_url = base_url_func(pkg_version, cuda_version, py_version)\n        if base_url and base_url.endswith(\"/\"):\n            base_url = base_url[:-1]\n    if pkg_file_func:\n        full_pkg_file = pkg_file_func(\n            pkg_name, pkg_version, cuda_version, py_version, os_type\n        )\n    else:\n        full_pkg_file = f\"{pkg_name}-{pkg_version}+{cuda_version}-{py_version}-{py_version}-{os_pkg_name}.whl\"\n    if not base_url:\n        return full_pkg_file\n    else:\n        return f\"{base_url}/{full_pkg_file}\"\n\n\ndef torch_requires(\n    torch_version: str = \"2.2.1\",\n    torchvision_version: str = \"0.17.1\",\n    torchaudio_version: str = \"2.2.1\",\n):\n    os_type, _ = get_cpu_avx_support()\n    torch_pkgs = [\n        f\"torch=={torch_version}\",\n        f\"torchvision=={torchvision_version}\",\n        f\"torchaudio=={torchaudio_version}\",\n    ]\n    # Initialize torch_cuda_pkgs for non-Darwin OSes;\n    # it will be the same as torch_pkgs for Darwin or when no specific CUDA handling is needed\n    torch_cuda_pkgs = torch_pkgs[:]\n\n    if os_type != OSType.DARWIN:\n        supported_versions = [\"11.8\", \"12.1\"]\n        base_url_func = lambda v, x, y: f\"https://download.pytorch.org/whl/{x}\"\n        torch_url = _build_wheels(\n            \"torch\",\n            torch_version,\n            base_url_func=base_url_func,\n            supported_cuda_versions=supported_versions,\n        )\n        torchvision_url = _build_wheels(\n            \"torchvision\",\n            torchvision_version,\n            base_url_func=base_url_func,\n            supported_cuda_versions=supported_versions,\n        )\n\n        # Cache and add CUDA-dependent packages if URLs are available\n        if torch_url:\n            torch_url_cached = cache_package(\n                torch_url, \"torch\", os_type == OSType.WINDOWS\n            )\n            torch_cuda_pkgs[0] = f\"torch @ {torch_url_cached}\"\n        if torchvision_url:\n            torchvision_url_cached = cache_package(\n                torchvision_url, \"torchvision\", os_type == OSType.WINDOWS\n            )\n            torch_cuda_pkgs[1] = f\"torchvision @ {torchvision_url_cached}\"\n\n    # Assuming 'setup_spec' is a dictionary where we're adding these dependencies\n    setup_spec.extras[\"torch\"] = torch_pkgs\n    setup_spec.extras[\"torch_cpu\"] = torch_pkgs\n    setup_spec.extras[\"torch_cuda\"] = torch_cuda_pkgs\n\n\ndef llama_cpp_python_cuda_requires():\n    cuda_version = get_cuda_version()\n    supported_cuda_versions = [\"11.8\", \"12.1\"]\n    device = \"cpu\"\n    if not cuda_version:\n        print(\"CUDA not support, use cpu version\")\n        return\n    if not LLAMA_CPP_GPU_ACCELERATION:\n        print(\"Disable GPU acceleration\")\n        return\n    # Supports GPU acceleration\n    if cuda_version <= \"11.8\" and not None:\n        device = \"cu\" + supported_cuda_versions[0].replace(\".\", \"\")\n    else:\n        device = \"cu\" + supported_cuda_versions[-1].replace(\".\", \"\")\n    os_type, cpu_avx = get_cpu_avx_support()\n    print(f\"OS: {os_type}, cpu avx: {cpu_avx}\")\n    supported_os = [OSType.WINDOWS, OSType.LINUX]\n    if os_type not in supported_os:\n        print(\n            f\"llama_cpp_python_cuda just support in os: {[r._value_ for r in supported_os]}\"\n        )\n        return\n    cpu_device = \"\"\n    if cpu_avx == AVXType.AVX2 or cpu_avx == AVXType.AVX512:\n        cpu_device = \"avx\"\n    else:\n        cpu_device = \"basic\"\n    device += cpu_device\n    base_url = \"https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui\"\n    llama_cpp_version = \"0.2.26\"\n    py_version = \"cp310\"\n    os_pkg_name = \"manylinux_2_31_x86_64\" if os_type == OSType.LINUX else \"win_amd64\"\n    extra_index_url = f\"{base_url}/llama_cpp_python_cuda-{llama_cpp_version}+{device}-{py_version}-{py_version}-{os_pkg_name}.whl\"\n    extra_index_url, _ = encode_url(extra_index_url)\n    print(f\"Install llama_cpp_python_cuda from {extra_index_url}\")\n\n    setup_spec.extras[\"llama_cpp\"].append(f\"llama_cpp_python_cuda @ {extra_index_url}\")\n\n\ndef core_requires():\n    \"\"\"\n    pip install dbgpt or pip install \"dbgpt[core]\"\n    \"\"\"\n    setup_spec.extras[\"core\"] = [\n        \"aiohttp==3.8.4\",\n        \"chardet==5.1.0\",\n        \"importlib-resources==5.12.0\",\n        \"python-dotenv==1.0.0\",\n        \"cachetools\",\n        \"pydantic>=2.6.0\",\n        # For AWEL type checking\n        \"typeguard\",\n        # Snowflake no additional dependencies.\n        \"snowflake-id\",\n        \"typing_inspect\",\n    ]\n    # For DB-GPT python client SDK\n    setup_spec.extras[\"client\"] = setup_spec.extras[\"core\"] + [\n        \"httpx\",\n        \"fastapi>=0.100.0,<0.113.0\",\n        # For retry, chromadb need tenacity<=8.3.0\n        \"tenacity<=8.3.0\",\n    ]\n    # Simple command line dependencies\n    setup_spec.extras[\"cli\"] = setup_spec.extras[\"client\"] + [\n        \"prettytable\",\n        \"click\",\n        \"psutil==5.9.4\",\n        \"colorama==0.4.6\",\n        \"tomlkit\",\n        \"rich\",\n    ]\n    # Agent dependencies\n    setup_spec.extras[\"agent\"] = setup_spec.extras[\"cli\"] + [\n        \"termcolor\",\n        # https://github.com/eosphoros-ai/DB-GPT/issues/551\n        # TODO: remove pandas dependency\n        # alpine can't install pandas by default\n        \"pandas==2.0.3\",\n        # numpy should less than 2.0.0\n        \"numpy>=1.21.0,<2.0.0\",\n    ]\n\n    # Just use by DB-GPT internal, we should find the smallest dependency set for run\n    # we core unit test.\n    # The dependency \"framework\" is too large for now.\n    setup_spec.extras[\"simple_framework\"] = setup_spec.extras[\"agent\"] + [\n        \"jinja2\",\n        \"uvicorn\",\n        \"shortuuid\",\n        # 2.0.29 not support duckdb now\n        \"SQLAlchemy>=2.0.25, <2.0.29\",\n        # for cache\n        \"msgpack\",\n        # for AWEL operator serialization\n        \"cloudpickle\",\n        # for cache\n        # TODO: pympler has not been updated for a long time and needs to\n        #  find a new toolkit.\n        \"pympler\",\n        \"duckdb\",\n        \"duckdb-engine==0.9.1\",\n        # lightweight python library for scheduling jobs\n        \"schedule\",\n        # For datasource subpackage\n        \"sqlparse==0.4.4\",\n    ]\n    # TODO: remove fschat from simple_framework\n    if BUILD_FROM_SOURCE:\n        setup_spec.extras[\"simple_framework\"].append(\n            f\"fschat @ {BUILD_FROM_SOURCE_URL_FAST_CHAT}\"\n        )\n    else:\n        setup_spec.extras[\"simple_framework\"].append(\"fschat\")\n\n    setup_spec.extras[\"framework\"] = setup_spec.extras[\"simple_framework\"] + [\n        \"coloredlogs\",\n        \"seaborn\",\n        \"auto-gpt-plugin-template\",\n        \"gTTS==2.3.1\",\n        \"pymysql\",\n        \"jsonschema\",\n        # TODO move transformers to default\n        # \"transformers>=4.31.0\",\n        \"transformers>=4.34.0\",\n        \"alembic==1.12.0\",\n        # for excel\n        \"openpyxl==3.1.2\",\n        \"chardet==5.1.0\",\n        \"xlrd==2.0.1\",\n        \"aiofiles\",\n        # for agent\n        \"GitPython\",\n        # For AWEL dag visualization, graphviz is a small package, also we can move it to default.\n        \"graphviz\",\n        # For security\n        \"cryptography\",\n        # For high performance RPC communication in code execution\n        \"pyzmq\",\n    ]\n\n\ndef code_execution_requires():\n    \"\"\"\n    pip install \"dbgpt[code]\"\n\n    Code execution dependencies.\n    \"\"\"\n    setup_spec.extras[\"code\"] = setup_spec.extras[\"core\"] + [\n        \"msgpack\",\n        # for AWEL operator serialization\n        \"cloudpickle\",\n        \"lyric-py>=0.1.6\",\n        \"lyric-py-worker>=0.1.6\",\n        \"lyric-js-worker>=0.1.6\",\n        \"lyric-component-ts-transpiling>=0.1.6\",\n    ]\n\n\ndef knowledge_requires():\n    \"\"\"\n    pip install \"dbgpt[rag]\"\n    \"\"\"\n    setup_spec.extras[\"rag\"] = setup_spec.extras[\"vstore\"] + [\n        \"spacy==3.7\",\n        \"markdown\",\n        \"bs4\",\n        \"python-pptx\",\n        \"python-docx\",\n        \"pypdf\",\n        \"pdfplumber\",\n        \"python-multipart\",\n        \"sentence-transformers\",\n    ]\n\n    setup_spec.extras[\"graph_rag\"] = setup_spec.extras[\"rag\"] + [\n        \"neo4j\",\n        \"dbgpt-tugraph-plugins>=0.1.1\",\n    ]\n\n\ndef llama_cpp_requires():\n    \"\"\"\n    pip install \"dbgpt[llama_cpp]\"\n    \"\"\"\n    setup_spec.extras[\"llama_cpp_server\"] = [\"llama-cpp-server-py\"]\n    setup_spec.extras[\"llama_cpp\"] = setup_spec.extras[\"llama_cpp_server\"] + [\n        \"llama-cpp-python\"\n    ]\n    llama_cpp_python_cuda_requires()\n\n\ndef _build_autoawq_requires() -> Optional[str]:\n    os_type, _ = get_cpu_avx_support()\n    if os_type == OSType.DARWIN:\n        return None\n    return \"auto-gptq\"\n\n\ndef quantization_requires():\n    os_type, _ = get_cpu_avx_support()\n    quantization_pkgs = []\n    if os_type == OSType.WINDOWS:\n        # For Windows, fetch a specific bitsandbytes WHL package\n        latest_version = get_latest_version(\n            \"bitsandbytes\",\n            \"https://jllllll.github.io/bitsandbytes-windows-webui\",\n            \"0.41.1\",\n        )\n        whl_url = f\"https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-{latest_version}-py3-none-win_amd64.whl\"\n        local_pkg_path = cache_package(whl_url, \"bitsandbytes\", True)\n        setup_spec.extras[\"bitsandbytes\"] = [f\"bitsandbytes @ {local_pkg_path}\"]\n    else:\n        setup_spec.extras[\"bitsandbytes\"] = [\"bitsandbytes\"]\n\n    if os_type != OSType.DARWIN:\n        # Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\n        # autoawq requirements:\n        # 1. Compute Capability 7.5 (sm75). Turing and later architectures are supported.\n        # 2. CUDA Toolkit 11.8 and later.\n        cuda_version = get_cuda_version()\n        # autoawq_latest_version = get_latest_version(\"autoawq\", \"\", \"0.2.4\")\n        if cuda_version is None or cuda_version == \"12.1\":\n            quantization_pkgs.extend([\"autoawq\", _build_autoawq_requires(), \"optimum\"])\n        else:\n            # TODO(yyhhyy): Add autoawq install method for CUDA version 11.8\n            quantization_pkgs.extend([\"autoawq\", _build_autoawq_requires(), \"optimum\"])\n\n    setup_spec.extras[\"quantization\"] = (\n        [\"cpm_kernels\"] + quantization_pkgs + setup_spec.extras[\"bitsandbytes\"]\n    )\n\n\ndef all_vector_store_requires():\n    \"\"\"\n    pip install \"dbgpt[vstore]\"\n    \"\"\"\n    setup_spec.extras[\"vstore\"] = [\n        \"chromadb>=0.4.22\",\n    ]\n    setup_spec.extras[\"vstore_weaviate\"] = setup_spec.extras[\"vstore\"] + [\n        # \"protobuf\",\n        # \"grpcio\",\n        # weaviate depends on grpc which version is very low, we should install it\n        # manually.\n        \"weaviate-client\",\n    ]\n    setup_spec.extras[\"vstore_milvus\"] = setup_spec.extras[\"vstore\"] + [\n        \"pymilvus\",\n    ]\n    setup_spec.extras[\"vstore_all\"] = (\n        setup_spec.extras[\"vstore\"]\n        + setup_spec.extras[\"vstore_weaviate\"]\n        + setup_spec.extras[\"vstore_milvus\"]\n    )\n\n\ndef all_datasource_requires():\n    \"\"\"\n    pip install \"dbgpt[datasource]\"\n    \"\"\"\n    setup_spec.extras[\"datasource\"] = [\n        # \"sqlparse==0.4.4\",\n        \"pymysql\",\n    ]\n    # If you want to install psycopg2 and mysqlclient in ubuntu, you should install\n    # libpq-dev and libmysqlclient-dev first.\n    setup_spec.extras[\"datasource_all\"] = setup_spec.extras[\"datasource\"] + [\n        \"pyspark\",\n        \"pymssql\",\n        # install psycopg2-binary when you are in a virtual environment\n        # pip install psycopg2-binary\n        \"psycopg2\",\n        # mysqlclient 2.2.x have pkg-config issue on 3.10+\n        \"mysqlclient==2.1.0\",\n        # pydoris is too old, we should find a new package to replace it.\n        \"pydoris>=1.0.2,<2.0.0\",\n        \"clickhouse-connect\",\n        \"pyhive\",\n        \"thrift\",\n        \"thrift_sasl\",\n        \"vertica_python\",\n    ]\n\n\ndef openai_requires():\n    \"\"\"\n    pip install \"dbgpt[openai]\"\n    \"\"\"\n    setup_spec.extras[\"openai\"] = [\"tiktoken\"]\n    if BUILD_VERSION_OPENAI:\n        # Read openai sdk version from env\n        setup_spec.extras[\"openai\"].append(f\"openai=={BUILD_VERSION_OPENAI}\")\n    else:\n        setup_spec.extras[\"openai\"].append(\"openai\")\n\n    if INCLUDE_OBSERVABILITY:\n        setup_spec.extras[\"openai\"] += setup_spec.extras[\"observability\"]\n\n    setup_spec.extras[\"openai\"] += setup_spec.extras[\"framework\"]\n    setup_spec.extras[\"openai\"] += setup_spec.extras[\"rag\"]\n\n\ndef proxy_requires():\n    \"\"\"\n    pip install \"dbgpt[proxy]\"\n    \"\"\"\n    setup_spec.extras[\"proxy\"] = setup_spec.extras[\"openai\"] + [\"anthropic\"]\n\n\ndef gpt4all_requires():\n    \"\"\"\n    pip install \"dbgpt[gpt4all]\"\n    \"\"\"\n    setup_spec.extras[\"gpt4all\"] = [\"gpt4all\"]\n\n\ndef vllm_requires():\n    \"\"\"\n    pip install \"dbgpt[vllm]\"\n    \"\"\"\n    setup_spec.extras[\"vllm\"] = [\"vllm\"]\n\n\ndef cache_requires():\n    \"\"\"\n    pip install \"dbgpt[cache]\"\n    \"\"\"\n    setup_spec.extras[\"cache\"] = [\"rocksdict\"]\n\n\ndef observability_requires():\n    \"\"\"\n    pip install \"dbgpt[observability]\"\n\n    Send DB-GPT traces to OpenTelemetry compatible backends.\n    \"\"\"\n    setup_spec.extras[\"observability\"] = [\n        \"opentelemetry-api\",\n        \"opentelemetry-sdk\",\n        \"opentelemetry-exporter-otlp\",\n    ]\n\n\ndef default_requires():\n    \"\"\"\n    pip install \"dbgpt[default]\"\n    \"\"\"\n    setup_spec.extras[\"default\"] = [\n        # \"tokenizers==0.13.3\",\n        \"tokenizers>=0.14\",\n        \"accelerate>=0.20.3\",\n        \"zhipuai\",\n        \"dashscope\",\n        \"chardet\",\n        \"sentencepiece\",\n        \"ollama\",\n        \"qianfan\",\n        \"libro>=0.1.25\",\n        \"poetry\",\n    ]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"framework\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"rag\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"graph_rag\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"datasource\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"torch\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"cache\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"proxy\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"code\"]\n    if INCLUDE_QUANTIZATION:\n        # Add quantization extra to default, default is True\n        setup_spec.extras[\"default\"] += setup_spec.extras[\"quantization\"]\n    if INCLUDE_OBSERVABILITY:\n        setup_spec.extras[\"default\"] += setup_spec.extras[\"observability\"]\n\n\ndef all_requires():\n    requires = set()\n    for _, pkgs in setup_spec.extras.items():\n        for pkg in pkgs:\n            requires.add(pkg)\n    setup_spec.extras[\"all\"] = list(requires)\n\n\ndef init_install_requires():\n    setup_spec.install_requires += setup_spec.extras[\"core\"]\n    print(f\"Install requires: \\n{','.join(setup_spec.install_requires)}\")\n\n\ncore_requires()\ncode_execution_requires()\ntorch_requires()\nllama_cpp_requires()\nquantization_requires()\n\nall_vector_store_requires()\nall_datasource_requires()\nknowledge_requires()\ngpt4all_requires()\nvllm_requires()\ncache_requires()\nobservability_requires()\n\nopenai_requires()\nproxy_requires()\n# must be last\ndefault_requires()\nall_requires()\ninit_install_requires()\n\n# Packages to exclude when IS_DEV_MODE is False\nexcluded_packages = [\"tests\", \"*.tests\", \"*.tests.*\", \"examples\"]\n\nif IS_DEV_MODE:\n    packages = find_packages(exclude=excluded_packages)\nelse:\n    packages = find_packages(\n        exclude=excluded_packages,\n        include=[\n            \"dbgpt\",\n            \"dbgpt._private\",\n            \"dbgpt._private.*\",\n            \"dbgpt.agent\",\n            \"dbgpt.agent.*\",\n            \"dbgpt.cli\",\n            \"dbgpt.cli.*\",\n            \"dbgpt.client\",\n            \"dbgpt.client.*\",\n            \"dbgpt.configs\",\n            \"dbgpt.configs.*\",\n            \"dbgpt.core\",\n            \"dbgpt.core.*\",\n            \"dbgpt.datasource\",\n            \"dbgpt.datasource.*\",\n            \"dbgpt.experimental\",\n            \"dbgpt.experimental.*\",\n            \"dbgpt.model\",\n            \"dbgpt.model.proxy\",\n            \"dbgpt.model.proxy.*\",\n            \"dbgpt.model.operators\",\n            \"dbgpt.model.operators.*\",\n            \"dbgpt.model.utils\",\n            \"dbgpt.model.utils.*\",\n            \"dbgpt.model.adapter\",\n            \"dbgpt.rag\",\n            \"dbgpt.rag.*\",\n            \"dbgpt.storage\",\n            \"dbgpt.storage.*\",\n            \"dbgpt.util\",\n            \"dbgpt.util.*\",\n            \"dbgpt.vis\",\n            \"dbgpt.vis.*\",\n        ],\n    )\n\n\nclass PrintExtrasCommand(setuptools.Command):\n    description = \"print extras_require\"\n    user_options = [\n        (\"output=\", \"o\", \"Path to output the extras_require JSON\"),\n    ]\n\n    def initialize_options(self):\n        self.output = None\n\n    def finalize_options(self):\n        if self.output is None:\n            raise ValueError(\"output is not set\")\n\n    def run(self):\n        with open(self.output, \"w\") as f:\n            json.dump(setup_spec.unique_extras, f, indent=2)\n\n\nsetuptools.setup(\n    name=\"dbgpt\",\n    packages=packages,\n    version=DB_GPT_VERSION,\n    author=\"csunny\",\n    author_email=\"cfqcsunny@gmail.com\",\n    description=\"DB-GPT is an experimental open-source project that uses localized GPT \"\n    \"large models to interact with your data and environment.\"\n    \" With this solution, you can be assured that there is no risk of data leakage, \"\n    \"and your data is 100% private and secure.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    install_requires=setup_spec.install_requires,\n    url=\"https://github.com/eosphoros-ai/DB-GPT\",\n    license=\"https://opensource.org/license/mit/\",\n    python_requires=\">=3.10\",\n    extras_require=setup_spec.unique_extras,\n    cmdclass={\n        \"print_extras\": PrintExtrasCommand,\n    },\n    entry_points={\n        \"console_scripts\": [\n            \"dbgpt=dbgpt.cli.cli_scripts:main\",\n        ],\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}