{
  "metadata": {
    "timestamp": 1736561218316,
    "page": 196,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "eosphoros-ai/DB-GPT",
      "stars": 14251,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.173828125,
          "content": ".env\n.git/\n./.mypy_cache/\nmodels/\nplugins/\npilot/data\npilot/message\nlogs/\nvenv/\nweb/node_modules/\nweb/.next/\nweb/.env\ndocs/node_modules/\nbuild/\ndocs/build/\ndocs/Dockerfile-deploy"
        },
        {
          "name": ".env.template",
          "type": "blob",
          "size": 14.1884765625,
          "content": "#*******************************************************************#\n#**             DB-GPT  - GENERAL SETTINGS                        **#  \n#*******************************************************************#\n\n#*******************************************************************#\n#**                        Webserver Port                         **#\n#*******************************************************************#\n# DBGPT_WEBSERVER_PORT=5670\n## Whether to enable the new web UI, enabled by default,False use old ui\n# USE_NEW_WEB_UI=True\n#*******************************************************************#\n#***                       LLM PROVIDER                          ***#\n#*******************************************************************#\n\n# TEMPERATURE=0\n\n#*******************************************************************#\n#**                         LLM MODELS                            **#\n#*******************************************************************#\n# LLM_MODEL, see dbgpt/configs/model_config.LLM_MODEL_CONFIG\nLLM_MODEL=glm-4-9b-chat\n## LLM model path, by default, DB-GPT will read the model path from LLM_MODEL_CONFIG based on the LLM_MODEL.\n## Of course you can specify your model path according to LLM_MODEL_PATH\n## In DB-GPT, the priority from high to low to read model path:\n##    1. environment variable with key: {LLM_MODEL}_MODEL_PATH (Avoid multi-model conflicts)\n##    2. environment variable with key: MODEL_PATH\n##    3. environment variable with key: LLM_MODEL_PATH\n##    4. the config in dbgpt/configs/model_config.LLM_MODEL_CONFIG\n# LLM_MODEL_PATH=/app/models/glm-4-9b-chat\n# LLM_PROMPT_TEMPLATE=vicuna_v1.1\nMODEL_SERVER=http://127.0.0.1:8000\nLIMIT_MODEL_CONCURRENCY=5\nMAX_POSITION_EMBEDDINGS=4096\nQUANTIZE_QLORA=True\nQUANTIZE_8bit=True\n# QUANTIZE_4bit=False\n## SMART_LLM_MODEL - Smart language model (Default: vicuna-13b)\n## FAST_LLM_MODEL - Fast language model (Default: chatglm-6b)\n# SMART_LLM_MODEL=vicuna-13b\n# FAST_LLM_MODEL=chatglm-6b\n## Proxy llm backend, this configuration is only valid when \"LLM_MODEL=proxyllm\", When we use the rest API provided by deployment frameworks like fastchat as a proxyllm, \n## \"PROXYLLM_BACKEND\" is the model they actually deploy. We can use \"PROXYLLM_BACKEND\" to load the prompt of the corresponding scene. \n# PROXYLLM_BACKEND=\n\n### You can configure parameters for a specific model with {model name}_{config key}=xxx\n### See dbgpt/model/parameter.py\n## prompt template for current model\n# llama_cpp_prompt_template=vicuna_v1.1\n## llama-2-70b must be 8\n# llama_cpp_n_gqa=8\n## Model path\n# llama_cpp_model_path=/data/models/TheBloke/vicuna-13B-v1.5-GGUF/vicuna-13b-v1.5.Q4_K_M.gguf\n\n### LLM cache\n## Enable Model cache\n# MODEL_CACHE_ENABLE=True\n## The storage type of model cache, now supports: memory, disk\n# MODEL_CACHE_STORAGE_TYPE=disk\n## The max cache data in memory, we always store cache data in memory fist for high speed. \n# MODEL_CACHE_MAX_MEMORY_MB=256\n## The dir to save cache data, this configuration is only valid when MODEL_CACHE_STORAGE_TYPE=disk\n## The default dir is pilot/data/model_cache\n# MODEL_CACHE_STORAGE_DISK_DIR=\n\n#*******************************************************************#\n#**                         EMBEDDING SETTINGS                    **#\n#*******************************************************************#\nEMBEDDING_MODEL=text2vec\nEMBEDDING_MODEL_MAX_SEQ_LEN=512\n#EMBEDDING_MODEL=m3e-large\n#EMBEDDING_MODEL=bge-large-en\n#EMBEDDING_MODEL=bge-large-zh\nKNOWLEDGE_CHUNK_SIZE=500\nKNOWLEDGE_SEARCH_TOP_SIZE=5\nKNOWLEDGE_GRAPH_SEARCH_TOP_SIZE=200\n## Maximum number of chunks to load at once, if your single document is too large,\n## you can set this value to a higher value for better performance.\n## if out of memory when load large document, you can set this value to a lower value.\n# KNOWLEDGE_MAX_CHUNKS_ONCE_LOAD=10\n## Maximum number of threads to use when loading chunks, please make sure your vector db can support multi-threading.\n# KNOWLEDGE_MAX_THREADS=1\n#KNOWLEDGE_CHUNK_OVERLAP=50\n# Control whether to display the source document of knowledge on the front end.\nKNOWLEDGE_CHAT_SHOW_RELATIONS=False\n# Whether to enable Chat Knowledge Search Rewrite Mode\nKNOWLEDGE_SEARCH_REWRITE=False\n## EMBEDDING_TOKENIZER   - Tokenizer to use for chunking large inputs\n## EMBEDDING_TOKEN_LIMIT - Chunk size limit for large inputs\n# EMBEDDING_MODEL=all-MiniLM-L6-v2\n# EMBEDDING_TOKENIZER=all-MiniLM-L6-v2\n# EMBEDDING_TOKEN_LIMIT=8191\n\n## Openai embedding model, See dbgpt/model/parameter.py\n# EMBEDDING_MODEL=proxy_openai\n# proxy_openai_proxy_server_url=https://api.openai.com/v1\n# proxy_openai_proxy_api_key={your-openai-sk}\n# proxy_openai_proxy_backend=text-embedding-ada-002\n\n\n## qwen embedding model, See dbgpt/model/parameter.py\n# EMBEDDING_MODEL=proxy_tongyi\n# proxy_tongyi_proxy_backend=text-embedding-v1\n# proxy_tongyi_proxy_api_key={your-api-key}\n\n## qianfan embedding model, See dbgpt/model/parameter.py\n#EMBEDDING_MODEL=proxy_qianfan\n#proxy_qianfan_proxy_backend=bge-large-zh\n#proxy_qianfan_proxy_api_key={your-api-key}\n#proxy_qianfan_proxy_api_secret={your-secret-key}\n\n\n## Common HTTP embedding model\n# EMBEDDING_MODEL=proxy_http_openapi\n# proxy_http_openapi_proxy_server_url=http://localhost:8100/api/v1/embeddings\n# proxy_http_openapi_proxy_api_key=1dce29a6d66b4e2dbfec67044edbb924\n# proxy_http_openapi_proxy_backend=text2vec\n\n#*******************************************************************#\n#**                         RERANK SETTINGS                       **#\n#*******************************************************************#\n## Rerank model\n# RERANK_MODEL=bge-reranker-base\n## If you not set RERANK_MODEL_PATH, DB-GPT will read the model path from EMBEDDING_MODEL_CONFIG based on the RERANK_MODEL.\n# RERANK_MODEL_PATH=\n## The number of rerank results to return\n# RERANK_TOP_K=3\n\n## Common HTTP rerank model\n# RERANK_MODEL=rerank_proxy_http_openapi\n# rerank_proxy_http_openapi_proxy_server_url=http://127.0.0.1:8100/api/v1/beta/relevance\n# rerank_proxy_http_openapi_proxy_api_key={your-api-key}\n# rerank_proxy_http_openapi_proxy_backend=bge-reranker-base\n\n\n\n\n#*******************************************************************#\n#**                  DB-GPT METADATA DATABASE SETTINGS            **#\n#*******************************************************************#\n### SQLite database (Current default database)\nLOCAL_DB_TYPE=sqlite\n\n### MYSQL database\n# LOCAL_DB_TYPE=mysql\n# LOCAL_DB_USER=root\n# LOCAL_DB_PASSWORD={your_password}\n# LOCAL_DB_HOST=127.0.0.1\n# LOCAL_DB_PORT=3306\n# LOCAL_DB_NAME=dbgpt\n### This option determines the storage location of conversation records. The default is not configured to the old version of duckdb. It can be optionally db or file (if the value is db, the database configured by LOCAL_DB will be used)\n#CHAT_HISTORY_STORE_TYPE=db\n\n#*******************************************************************#\n#**                         COMMANDS                              **#\n#*******************************************************************#\nEXECUTE_LOCAL_COMMANDS=False\n\n#*******************************************************************#\n#**            VECTOR STORE / KNOWLEDGE GRAPH SETTINGS            **#\n#*******************************************************************#\nVECTOR_STORE_TYPE=Chroma\nGRAPH_STORE_TYPE=TuGraph\nKNOWLEDGE_GRAPH_EXTRACT_SEARCH_TOP_SIZE=5\nKNOWLEDGE_GRAPH_EXTRACT_SEARCH_RECALL_SCORE=0.3\nKNOWLEDGE_GRAPH_COMMUNITY_SEARCH_TOP_SIZE=20\nKNOWLEDGE_GRAPH_COMMUNITY_SEARCH_RECALL_SCORE=0.0\n\nGRAPH_COMMUNITY_SUMMARY_ENABLED=True  # enable the graph community summary\nTRIPLET_GRAPH_ENABLED=True  # enable the graph search for triplets\nDOCUMENT_GRAPH_ENABLED=True  # enable the graph search for documents and chunks\n\nKNOWLEDGE_GRAPH_CHUNK_SEARCH_TOP_SIZE=5  # the top size of knowledge graph search for chunks\nKNOWLEDGE_GRAPH_EXTRACTION_BATCH_SIZE=20  # the batch size of triplet extraction from the text\nCOMMUNITY_SUMMARY_BATCH_SIZE=20  # the batch size of parallel community summary process\n\n### Chroma vector db config\n#CHROMA_PERSIST_PATH=/root/DB-GPT/pilot/data\n\n### Milvus vector db config\n#VECTOR_STORE_TYPE=Milvus\n#MILVUS_URL=127.0.0.1\n#MILVUS_PORT=19530\n#MILVUS_USERNAME\n#MILVUS_PASSWORD\n#MILVUS_SECURE=\n\n### Weaviate vector db config\n#VECTOR_STORE_TYPE=Weaviate\n#WEAVIATE_URL=https://kt-region-m8hcy0wc.weaviate.network\n\n## ElasticSearch vector db config\n#VECTOR_STORE_TYPE=ElasticSearch\nELASTICSEARCH_URL=127.0.0.1\nELASTICSEARCH_PORT=9200\nELASTICSEARCH_USERNAME=elastic\nELASTICSEARCH_PASSWORD={your_password}\n\n### TuGraph config\n#TUGRAPH_HOST=127.0.0.1\n#TUGRAPH_PORT=7687\n#TUGRAPH_USERNAME=admin\n#TUGRAPH_PASSWORD=73@TuGraph\n#TUGRAPH_VERTEX_TYPE=entity\n#TUGRAPH_EDGE_TYPE=relation\n#TUGRAPH_PLUGIN_NAMES=leiden\n\n#*******************************************************************#\n#**                  WebServer Language Support                   **#\n#*******************************************************************#\n# en, zh, fr, ja, ko, ru\nLANGUAGE=en\n#LANGUAGE=zh\n\n\n#*******************************************************************#\n# **    PROXY_SERVER (openai interface | chatGPT proxy service), use chatGPT as your LLM.\n# ** if your server can visit openai, please set PROXY_SERVER_URL=https://api.openai.com/v1/chat/completions\n# ** else if you have a chatgpt proxy server, you can set PROXY_SERVER_URL={your-proxy-serverip:port/xxx}\n#*******************************************************************#\nPROXY_API_KEY={your-openai-sk}\nPROXY_SERVER_URL=https://api.openai.com/v1/chat/completions\n\n# from https://bard.google.com/     f12-> application-> __Secure-1PSID\nBARD_PROXY_API_KEY={your-bard-token}\n\n#*******************************************************************#\n# **  PROXY_SERVER +                                              **#\n#*******************************************************************#\n\n# Aliyun tongyi\nTONGYI_PROXY_API_KEY={your-tongyi-sk}\n\n## Baidu wenxin\n#WEN_XIN_MODEL_VERSION={version}\n#WEN_XIN_API_KEY={your-wenxin-sk}\n#WEN_XIN_API_SECRET={your-wenxin-sct}\n\n## Zhipu\n#ZHIPU_MODEL_VERSION={version}\n#ZHIPU_PROXY_API_KEY={your-zhipu-sk}\n\n## Baichuan\n#BAICHUN_MODEL_NAME={version}\n#BAICHUAN_PROXY_API_KEY={your-baichuan-sk}\n#BAICHUAN_PROXY_API_SECRET={your-baichuan-sct}\n\n# Xunfei Spark\n#XUNFEI_SPARK_API_PASSWORD={your_api_password}\n#XUNFEI_SPARK_API_MODEL={version}\n\n## Yi Proxyllm, https://platform.lingyiwanwu.com/docs\n#YI_MODEL_VERSION=yi-34b-chat-0205\n#YI_API_BASE=https://api.lingyiwanwu.com/v1\n#YI_API_KEY={your-yi-api-key}\n\n## Moonshot Proxyllm, https://platform.moonshot.cn/docs/\n# MOONSHOT_MODEL_VERSION=moonshot-v1-8k\n# MOONSHOT_API_BASE=https://api.moonshot.cn/v1\n# MOONSHOT_API_KEY={your-moonshot-api-key}\n\n## Deepseek Proxyllm, https://platform.deepseek.com/api-docs/\n# DEEPSEEK_MODEL_VERSION=deepseek-chat\n# DEEPSEEK_API_BASE=https://api.deepseek.com/v1\n# DEEPSEEK_API_KEY={your-deepseek-api-key}\n\n\n#*******************************************************************#\n#**    SUMMARY_CONFIG                                             **#\n#*******************************************************************#\nSUMMARY_CONFIG=FAST\n\n#*******************************************************************#\n#**    MUlti-GPU                                                  **#\n#*******************************************************************#\n## See https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/\n## If CUDA_VISIBLE_DEVICES is not configured, all available gpus will be used\n# CUDA_VISIBLE_DEVICES=0\n## You can configure the maximum memory used by each GPU.\n# MAX_GPU_MEMORY=16Gib\n\n#*******************************************************************#\n#**                         LOG                                   **#\n#*******************************************************************#\n# FATAL, ERROR, WARNING, WARNING, INFO, DEBUG, NOTSET\nDBGPT_LOG_LEVEL=INFO\n# LOG dir, default: ./logs\n#DBGPT_LOG_DIR=\n\n\n#*******************************************************************#\n#**                         API_KEYS                              **#\n#*******************************************************************#\n# API_KEYS - The list of API keys that are allowed to access the API. Each of the below are an option, separated by commas.\n# API_KEYS=dbgpt\n\n#*******************************************************************#\n#**                         ENCRYPT                               **#\n#*******************************************************************#\n# ENCRYPT KEY - The key used to encrypt and decrypt the data\n# ENCRYPT_KEY=your_secret_key\n\n#*******************************************************************#\n#**                         File Server                           **#\n#*******************************************************************#\n## The local storage path of the file server, the default is pilot/data/file_server\n# FILE_SERVER_LOCAL_STORAGE_PATH =\n\n#*******************************************************************#\n#**                     Application Config                        **#\n#*******************************************************************#\n## Non-streaming scene retries\n# DBGPT_APP_SCENE_NON_STREAMING_RETRIES_BASE=1\n## Non-streaming scene parallelism\n# DBGPT_APP_SCENE_NON_STREAMING_PARALLELISM_BASE=1\n\n#*******************************************************************#\n#**                   Observability Config                        **#\n#*******************************************************************#\n## Whether to enable DB-GPT send trace to OpenTelemetry\n# TRACER_TO_OPEN_TELEMETRY=False\n## Following configurations are only valid when TRACER_TO_OPEN_TELEMETRY=True\n## More details see https://opentelemetry-python.readthedocs.io/en/latest/exporter/otlp/otlp.html\n# OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4317\n# OTEL_EXPORTER_OTLP_TRACES_INSECURE=False\n# OTEL_EXPORTER_OTLP_TRACES_CERTIFICATE=\n# OTEL_EXPORTER_OTLP_TRACES_HEADERS=\n# OTEL_EXPORTER_OTLP_TRACES_TIMEOUT=\n# OTEL_EXPORTER_OTLP_TRACES_COMPRESSION=\n\n#*******************************************************************#\n#**                     FINANCIAL CHAT Config                     **#\n#*******************************************************************#\n# FIN_REPORT_MODEL=/app/models/bge-large-zh\n\n## Turn off notebook display Python flow , which is enabled by default\nNOTE_BOOK_ENABLE=False\n\n## The agent historical message retention configuration defaults to the last two rounds.\n# MESSAGES_KEEP_START_ROUNDS=0\n# MESSAGES_KEEP_END_ROUNDS=2\n\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.3291015625,
          "content": "[flake8]\nexclude =\n  .eggs/\n  build/\n  */tests/*\n  *_private\nmax-line-length = 88\ninline-quotes = \"\nignore =\n  C408\n  C417\n  E121\n  E123\n  E126\n  E203\n  E226\n  E231\n  E24\n  E704\n  W503\n  W504\n  W605\n  I\n  N\n  B001\n  B002\n  B003\n  B004\n  B005\n  B007\n  B008\n  B009\n  B010\n  B011\n  B012\n  B013\n  B014\n  B015\n  B016\n  B017\navoid-escape = no\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.501953125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\nmessage/\ndbgpt/util/extensions/\n.env*\n.vscode\n.idea\n.chroma\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\n\nvar/\nwheels/\n/models/\n# Soft link\n/models\nplugins/\n\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n*.zuo\n*.zip\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test_py / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n# *.mo\n*.pot\n\n# Django stuff:\n*.log\n*.log.*\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n.DS_Store\nlogs\nnltk_data\n.vectordb\npilot/data/\npilot/nltk_data\npilot/mock_datas/db-gpt-test.db.wal\n\nlogswebserver.log.*\n.history/*\n.plugin_env\n/pilot/meta_data/alembic/versions/*\n/pilot/meta_data/*.db\n# Ignore for now\nthirdparty\n\n#web\n# dependencies\n/web/node_modules\n/web/yarn.lock\n\n.idea\n# next.js\n/web/.next/\n/web/out/\n\n# production\n/web/build\n\n# debug\n/web/npm-debug.log*\n/web/yarn-debug.log*\n/web/yarn-error.log*\n\n# local env files\n/web/.env.prod\n/web/.env\n\n# typescript\n*.tsbuildinfo\n/web/next-env.d.ts\n\n# Ignore awel DAG visualization files\n/examples/**/*.gv\n/examples/**/*.gv.pdf\n/i18n/locales/**/**/*_ai_translated.po\n/i18n/locales/**/**/*~\n"
        },
        {
          "name": ".isort.cfg",
          "type": "blob",
          "size": 0.3740234375,
          "content": "[settings]\n# This is to make isort compatible with Black. See\n# https://black.readthedocs.io/en/stable/the_black_code_style.html#how-black-wraps-lines.\nline_length=88\nprofile=black\nmulti_line_output=3\ninclude_trailing_comma=True\nuse_parentheses=True\nfloat_to_top=True\nfilter_files=True\n\nskip_glob=examples/notebook/*\nsections=FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER,AFTERRAY\n"
        },
        {
          "name": ".mypy.ini",
          "type": "blob",
          "size": 2.09375,
          "content": "[mypy]\nexclude = /tests/\n# plugins = pydantic.mypy\n[mypy-dbgpt.rag.*]\nstrict_optional = False\nignore_missing_imports = True\nfollow_imports = skip\n\n[mypy-dbgpt.app.*]\nfollow_imports = skip\n\n[mypy-dbgpt.serve.*]\nfollow_imports = skip\n\n[mypy-dbgpt.model.*]\nfollow_imports = skip\n\n[mypy-dbgpt.util.*]\nfollow_imports = skip\n\n[mypy-graphviz.*]\nignore_missing_imports = True\n\n[mypy-cachetools.*]\nignore_missing_imports = True\n\n[mypy-coloredlogs.*]\nignore_missing_imports = True\n\n[mypy-termcolor.*]\nignore_missing_imports = True\n\n[mypy-pydantic.*]\nstrict_optional = False\nignore_missing_imports = True\nfollow_imports = skip\n\n[mypy-sentence_transformers.*]\nignore_missing_imports = True\n\n[mypy-InstructorEmbedding.*]\nignore_missing_imports = True\n\n[mypy-llama_index.*]\nignore_missing_imports = True\n\n[mypy-langchain.*]\nignore_missing_imports = True\n\n[mypy-pptx.*]\nignore_missing_imports = True\n\n[mypy-docx.*]\nignore_missing_imports = True\n\n[mypy-markdown.*]\nignore_missing_imports = True\n\n[mypy-auto_gpt_plugin_template.*]\nignore_missing_imports = True\n\n[mypy-spacy.*]\nignore_missing_imports = True\nfollow_imports = skip\n\n[mypy-jieba.*]\nignore_missing_imports = True\n\n# Storage\n[mypy-msgpack.*]\nignore_missing_imports = True\n\n[mypy-rocksdict.*]\nignore_missing_imports = True\n\n[mypy-weaviate.*]\nignore_missing_imports = True\n\n[mypy-pymilvus.*]\nignore_missing_imports = True\n\n[mypy-elasticsearch.*]\nignore_missing_imports = True\n\n[mypy-cryptography.*]\nignore_missing_imports = True\n\n# Datasource\n[mypy-pyspark.*]\nignore_missing_imports = True\n\n[mypy-regex.*]\nignore_missing_imports = True\n\n[mypy-sqlparse.*]\nignore_missing_imports = True\n\n[mypy-clickhouse_connect.*]\nignore_missing_imports = True\n\n[mypy-fastchat.protocol.api_protocol]\nignore_missing_imports = True\n\n[mypy-neo4j.*]\nignore_missing_imports = True\n\n# Agent\n[mypy-seaborn.*]\nignore_missing_imports = True\n\n[mypy-unstructured.*]\nignore_missing_imports = True\n\n[mypy-rich.*]\nignore_missing_imports = True\n\n[mypy-ollama.*]\nignore_missing_imports = True\n\n[mypy-networkx.*]\nignore_missing_imports = True\n\n[mypy-pypdf.*]\nignore_missing_imports = True\n\n[mypy-qianfan.*]\nignore_missing_imports = True"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.111328125,
          "content": "# Please run command `pre-commit install` to install pre-commit hook\r\nrepos:\r\n  - repo: local\r\n    hooks:\r\n      - id: python-fmt\r\n        name: Python Format\r\n        entry: make fmt-check\r\n        language: system\r\n        exclude: '^dbgpt/app/static/|^web/'\r\n        types: [python]\r\n        stages: [commit]\r\n        pass_filenames: false\r\n        args: []\r\n      - id: python-test\r\n        name: Python Unit Test\r\n        entry: make test\r\n        language: system\r\n        exclude: '^dbgpt/app/static/|^web/'\r\n        types: [python]\r\n        stages: [commit]\r\n        pass_filenames: false\r\n        args: []\r\n      - id: python-test-doc\r\n        name: Python Doc Test\r\n        entry: make test-doc\r\n        language: system\r\n        exclude: '^dbgpt/app/static/|^web/'\r\n        types: [python]\r\n        stages: [commit]\r\n        pass_filenames: false\r\n        args: []\r\n      - id: python-lint-mypy\r\n        name: Python Lint mypy\r\n        entry: make mypy\r\n        language: system\r\n        exclude: '^dbgpt/app/static/|^web/'\r\n        types: [python]\r\n        stages: [commit]\r\n        pass_filenames: false\r\n        args: []\r\n\r\n"
        },
        {
          "name": "CODE_OF_CONDUCT",
          "type": "blob",
          "size": 5.056640625,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n[INSERT CONTACT METHOD].\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n*Community Impact*: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n*Consequence*: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n*Community Impact*: A violation through a single incident or series of\nactions.\n\n*Consequence*: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n*Community Impact*: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n*Consequence*: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n*Community Impact*: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n*Consequence*: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.015625,
          "content": "# Contribution \n\nFirst of all, thank you for considering contributing to this project. \nIt's people like you that make it a reality for the community. There are many ways to contribute, and we appreciate all of them.\n\nThis guide will help you get started with contributing to this project.\n\n## Fork The Repository\n\n1. Fork the repository you want to contribute to by clicking the \"Fork\" button on the project page.\n\n2. Clone the repository to your local machine using the following command:\n\n```\ngit clone https://github.com/<YOUR-GITHUB-USERNAME>/DB-GPT\n```\nPlease replace `<YOUR-GITHUB-USERNAME>` with your GitHub username.\n\n\n## Create A New Development Environment\n\n1. Create a new virtual environment using the following command:\n```\n# Make sure python >= 3.10\nconda create -n dbgpt_env python=3.10\nconda activate dbgpt_env\n```\n\n2. Change to the project directory using the following command:\n```\ncd DB-GPT\n```\n\n3. Install the project from the local source using the following command:\n```\n# it will take some minutes\npip install -e \".[default]\"\n```\n\n4. Install development requirements\n```\npip install -r requirements/dev-requirements.txt\npip install -r requirements/lint-requirements.txt\n```\n\n5. Install pre-commit hooks\n```\npre-commit install\n```\n\n6. Install `make` command\nThe `make` command has been installed by default on most Unix-based systems. If you not \nhave it, you can install it by searching on the internet.\n\n## New Branch And Make Changes\n\n1. Create a new branch for your changes using the following command:\n```\ngit checkout -b <branch-name>\n```\nPlease replace `<branch-name>` with a descriptive name for your branch.\n\n2. Make your changes to the code or documentation.\n\n3. Add tests for your changes if necessary.\n\n4. Format your code using the following command:\n```\nmake fmt\n```\n\n5. Run the tests using the following command:\n```\nmake test\n```\n\n6. Check types using the following command:\n```\nmake mypy\n```\n\n7. Check lint using the following command:\n```\nmake fmt-check\n```\n\n8. If all checks pass, you can add and commit your changes using the following commands:\n```\ngit add xxxx\n```\nmake sure to replace `xxxx` with the files you want to commit.\n\nthen commit your changes using the following command:\n```\ngit commit -m \"your commit message\"\n```\nPlease replace `your commit message` with a meaningful commit message.\n\nIt will take some time to get used to the process, but it's worth it. And it will run \nall git hooks and checks before you commit. If it fails, you need to fix the issues \nthen re-commit it.\n\n9. Push the changes to your forked repository using the following command:\n```\ngit push origin <branch-name>\n```\n\n## Create A Pull Request\n\n1. Go to the GitHub website and navigate to your forked repository.\n\n2. Click the \"New pull request\" button.\n\n3. Select the branch you just pushed to and the branch you want to merge into on the original repository.\nWrite necessary information about your changes and click \"Create pull request\".\n\n4. Wait for the project maintainer to review your changes and provide feedback.\n\nThat's it you made it 🐣⭐⭐\n\n"
        },
        {
          "name": "DISCKAIMER.md",
          "type": "blob",
          "size": 1.796875,
          "content": "# User Agreement and Disclaimer\n\n1. If you do not agree with any content of this statement, please stop using this software immediately. Once you start using this software product and service, it means that you have agreed to all the contents of this statement\n\n2. This disclaimer applies to all users of this software. This software reserves the right to modify and update this statement at any time, and notify users in the form of Github Readme, software updates, etc. Please review regularly and abide by the latest disclaimer.\n\n3. The original design intention of this project is to provide a basic framework/tool ​​set, mainly focusing on RAGs,Agents, AWEL, etc. To keep the project simple and easy to use, we intentionally did not integrate any form of user login, authentication or authorization mechanism.\n\n4. If you plan to deploy this project into a production environment, it is strongly recommended to connect to existing third-party authentication services (such as OAuth, OpenID Connect, etc.) according to your specific needs, or to develop and maintain a complete set of user management and permissions yourself. control system.\n\n5. We encourage all developers to follow best practices to keep user data secure, but this is beyond the scope of this project. Therefore, always take appropriate security measures when handling sensitive information.\n\n6. Users are responsible for the security configuration in their applications, including but not limited to user account management, password policies, access control lists, etc.\n\n7. The project authors and contributors are not legally responsible for any direct or indirect losses caused by the use of this software.\n\n\nPlease read and understand all the contents of this disclaimer carefully before using this software, thank you for your understanding and support.\n\n \n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2023 magic.chen\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0576171875,
          "content": "include LICENSE\ninclude README.md\ninclude requirements.txt\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 3.72265625,
          "content": ".DEFAULT_GOAL := help\n\nSHELL=/bin/bash\nVENV = venv\n\n# Detect the operating system and set the virtualenv bin directory\nifeq ($(OS),Windows_NT)\n\tVENV_BIN=$(VENV)/Scripts\nelse\n\tVENV_BIN=$(VENV)/bin\nendif\n\nsetup: $(VENV)/bin/activate\n\n$(VENV)/bin/activate: $(VENV)/.venv-timestamp\n\n$(VENV)/.venv-timestamp: setup.py requirements\n\t# Create new virtual environment if setup.py has changed\n\tpython3 -m venv $(VENV)\n\t$(VENV_BIN)/pip install --upgrade pip\n\t$(VENV_BIN)/pip install -r requirements/dev-requirements.txt\n\t$(VENV_BIN)/pip install -r requirements/lint-requirements.txt\n\ttouch $(VENV)/.venv-timestamp\n\ntestenv: $(VENV)/.testenv\n\n$(VENV)/.testenv: $(VENV)/bin/activate\n\t# $(VENV_BIN)/pip install -e \".[framework]\"\n\t# the openai optional dependency is include framework and rag dependencies\n\t$(VENV_BIN)/pip install -e \".[openai]\"\n\ttouch $(VENV)/.testenv\n\n\n.PHONY: fmt\nfmt: setup ## Format Python code\n\t# TODO: Use isort to sort Python imports.\n\t# https://github.com/PyCQA/isort\n\t# $(VENV_BIN)/isort .\n\t$(VENV_BIN)/isort dbgpt/\n\t$(VENV_BIN)/isort --extend-skip=\"examples/notebook\" examples\n\t# https://github.com/psf/black\n\t$(VENV_BIN)/black --extend-exclude=\"examples/notebook\" .\n\t# TODO: Use blackdoc to format Python doctests.\n\t# https://blackdoc.readthedocs.io/en/latest/\n\t# $(VENV_BIN)/blackdoc .\n\t$(VENV_BIN)/blackdoc dbgpt\n\t$(VENV_BIN)/blackdoc examples\n\t# TODO: Use flake8 to enforce Python style guide.\n\t# https://flake8.pycqa.org/en/latest/\n\t$(VENV_BIN)/flake8 dbgpt/core/ dbgpt/rag/ dbgpt/storage/ dbgpt/datasource/ dbgpt/client/ dbgpt/agent/ dbgpt/vis/ dbgpt/experimental/\n\t# TODO: More package checks with flake8.\n\n.PHONY: fmt-check\nfmt-check: setup ## Check Python code formatting and style without making changes\n\t$(VENV_BIN)/isort --check-only dbgpt/\n\t$(VENV_BIN)/isort --check-only --extend-skip=\"examples/notebook\" examples\n\t$(VENV_BIN)/black --check --extend-exclude=\"examples/notebook\" .\n\t$(VENV_BIN)/blackdoc --check dbgpt examples\n\t$(VENV_BIN)/flake8 dbgpt/core/ dbgpt/rag/ dbgpt/storage/ dbgpt/datasource/ dbgpt/client/ dbgpt/agent/ dbgpt/vis/ dbgpt/experimental/\n\n.PHONY: pre-commit\npre-commit: fmt-check test test-doc mypy ## Run formatting and unit tests before committing\n\ntest: $(VENV)/.testenv ## Run unit tests\n\t$(VENV_BIN)/pytest dbgpt\n\n.PHONY: test-doc\ntest-doc: $(VENV)/.testenv ## Run doctests\n\t# -k \"not test_\" skips tests that are not doctests.\n\t$(VENV_BIN)/pytest --doctest-modules -k \"not test_\" dbgpt/core\n\n.PHONY: mypy\nmypy: $(VENV)/.testenv ## Run mypy checks\n\t# https://github.com/python/mypy\n\t$(VENV_BIN)/mypy --config-file .mypy.ini dbgpt/rag/ dbgpt/datasource/ dbgpt/client/ dbgpt/agent/ dbgpt/vis/ dbgpt/experimental/\n\t# rag depends on core and storage, so we not need to check it again.\n\t# $(VENV_BIN)/mypy --config-file .mypy.ini dbgpt/storage/\n\t# $(VENV_BIN)/mypy --config-file .mypy.ini dbgpt/core/\n\t# TODO: More package checks with mypy.\n\n.PHONY: coverage\ncoverage: setup ## Run tests and report coverage\n\t$(VENV_BIN)/pytest dbgpt --cov=dbgpt\n\n.PHONY: clean\nclean: ## Clean up the environment\n\trm -rf $(VENV)\n\tfind . -type f -name '*.pyc' -delete\n\tfind . -type d -name '__pycache__' -delete\n\tfind . -type d -name '.pytest_cache' -delete\n\tfind . -type d -name '.coverage' -delete\n\n.PHONY: clean-dist\nclean-dist: ## Clean up the distribution\n\trm -rf dist/ *.egg-info build/\n\n.PHONY: package\npackage: clean-dist ## Package the project for distribution\n\tIS_DEV_MODE=false python setup.py sdist bdist_wheel\n\n.PHONY: upload\nupload: ## Upload the package to PyPI\n\t# upload to testpypi: twine upload --repository testpypi dist/*\n\ttwine upload dist/*\n\n.PHONY: help\nhelp:  ## Display this help screen\n\t@echo \"Available commands:\"\n\t@grep -E '^[a-z.A-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}; {printf \"  \\033[36m%-18s\\033[0m %s\\n\", $$1, $$2}' | sort"
        },
        {
          "name": "README.ja.md",
          "type": "blob",
          "size": 18.044921875,
          "content": "# DB-GPT: データベースとの対話を革新するプライベートLLM技術\n\n<p align=\"left\">\n  <img src=\"./assets/LOGO.png\" width=\"100%\" />\n</p>\n\n<div align=\"center\">\n  <p>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"stars\" src=\"https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"forks\" src=\"https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://opensource.org/licenses/MIT\">\n      <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" />\n    </a>\n     <a href=\"https://github.com/eosphoros-ai/DB-GPT/releases\">\n      <img alt=\"Release Notes\" src=\"https://img.shields.io/github/release/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT/issues\">\n      <img alt=\"Open Issues\" src=\"https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://discord.gg/7uQnPuveTY\">\n      <img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat\" />\n    </a>\n    <a href=\"https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA\">\n      <img alt=\"Slack\" src=\"https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack\" />\n    </a>\n    <a href=\"https://codespaces.new/eosphoros-ai/DB-GPT\">\n      <img alt=\"Open in GitHub Codespaces\" src=\"https://github.com/codespaces/badge.svg\" />\n    </a>\n  </p>\n\n[**英語**](README.md) | [**中国語**](README.zh.md) | [**Discord**](https://discord.gg/7uQnPuveTY) | [**ドキュメント**](https://docs.dbgpt.site) | [**微信**](https://github.com/eosphoros-ai/DB-GPT/blob/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC) | [**コミュニティ**](https://github.com/eosphoros-ai/community) | [**論文**](https://arxiv.org/pdf/2312.17449.pdf)\n\n</div>\n\n## DB-GPTとは何か？\n\n🤖 **DB-GPTは、AWEL（エージェントワークフロー式言語）とエージェントを備えたオープンソースのAIネイティブデータアプリ開発フレームワークです。**\n\n大規模モデルの分野でのインフラを構築することを目的としており、SMMF（マルチモデル管理）、Text2SQL効果の最適化、RAGフレームワークと最適化、マルチエージェントフレームワークの協力、AWEL（エージェントワークフローのオーケストレーション）など、複数の技術機能の開発を通じて、データを使用した大規模モデルアプリケーションをよりシンプルで便利にします。\n\n🚀 **データ3.0時代には、モデルとデータベースを基盤として、企業や開発者がより少ないコードで独自のアプリケーションを構築できます。**\n\n### AIネイティブデータアプリ\n---\n- 🔥🔥🔥 [V0.5.0リリース | ワークフローとエージェントを通じてネイティブデータアプリケーションを開発](https://docs.dbgpt.site/docs/changelog/Released_V0.5.0)\n---\n\n![Data-awels](https://github.com/eosphoros-ai/DB-GPT/assets/17919400/37d116fc-d9dd-4efa-b4df-9ab02b22541c)\n\n![Data-Apps](https://github.com/eosphoros-ai/DB-GPT/assets/17919400/a7bf6d65-92d1-4f0e-aaf0-259ccdde22fd)\n\n![dashboard-images](https://github.com/eosphoros-ai/DB-GPT/assets/17919400/1849a79a-f7fd-40cf-bc9c-b117a041dd6a)\n\n## 目次\n- [紹介](#紹介)\n- [インストール](#インストール)\n- [特徴](#特徴)\n- [貢献](#貢献)\n- [連絡先](#連絡先情報)\n\n## 紹介\nDB-GPTのアーキテクチャは以下の図に示されています：\n\n<p align=\"center\">\n  <img src=\"./assets/dbgpt.png\" width=\"800\" />\n</p>\n\nコア機能には以下の部分が含まれます：\n\n- **RAG（Retrieval Augmented Generation）**：現在、RAGは最も実用的に実装され、緊急に必要とされる領域です。DB-GPTは、RAGの機能を使用して知識ベースのアプリケーションを構築できるようにする、RAGに基づくフレームワークをすでに実装しています。\n\n- **GBI（Generative Business Intelligence）**：Generative BIはDB-GPTプロジェクトのコア機能の1つであり、企業のレポート分析とビジネスインサイトを構築するための基本的なデータインテリジェンス技術を提供します。\n\n- **ファインチューニングフレームワーク**：モデルのファインチューニングは、任意の企業が垂直およびニッチなドメインで実装するために不可欠な機能です。DB-GPTは、DB-GPTプロジェクトとシームレスに統合される完全なファインチューニングフレームワークを提供します。最近のファインチューニングの取り組みでは、Spiderデータセットに基づいて82.5%の実行精度を達成しています。\n\n- **データ駆動型マルチエージェントフレームワーク**：DB-GPTは、データに基づいて継続的に意思決定を行い、実行するためのデータ駆動型自己進化型マルチエージェントフレームワークを提供します。\n\n- **データファクトリー**：データファクトリーは、主に大規模モデルの時代における信頼できる知識とデータのクリーニングと処理に関するものです。\n\n- **データソース**：DB-GPTのコア機能に生産ビジネスデータをシームレスに接続するために、さまざまなデータソースを統合します。\n\n### サブモジュール\n- [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub) 大規模言語モデル（LLM）上での教師ありファインチューニング（SFT）を適用することにより、高性能なText-to-SQLワークフロー。\n\n- [dbgpts](https://github.com/eosphoros-ai/dbgpts)  dbgptsは、DB-GPT上で構築されたいくつかのデータアプリ、AWELオペレータ、AWELワークフローテンプレート、およびエージェントを含む公式リポジトリです。\n\n#### Text2SQLファインチューニング\n- サポートされているLLM\n  - [x] LLaMA\n  - [x] LLaMA-2\n  - [x] BLOOM\n  - [x] BLOOMZ\n  - [x] Falcon\n  - [x] Baichuan\n  - [x] Baichuan2\n  - [x] InternLM\n  - [x] Qwen\n  - [x] XVERSE\n  - [x] ChatGLM2\n\n-  SFT精度\n2023年10月10日現在、このプロジェクトを使用して130億パラメータのオープンソースモデルをファインチューニングすることにより、SpiderデータセットでGPT-4を超える実行精度を達成しました！\n\n[Text2SQLファインチューニングに関する詳細情報](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- [DB-GPT-Plugins](https://github.com/eosphoros-ai/DB-GPT-Plugins) Auto-GPTプラグインを直接実行できるDB-GPTプラグイン\n- [GPT-Vis](https://github.com/eosphoros-ai/GPT-Vis) 可視化プロトコル\n\n## インストール\n![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=macos&logoColor=F0F0F0)\n![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)\n\n[**使用チュートリアル**](http://docs.dbgpt.site/docs/overview)\n- [**インストール**](http://docs.dbgpt.site/docs/installation)\n  - [Docker](https://docs.dbgpt.site/docs/installation/docker)\n  - [ソースコード](https://docs.dbgpt.site/docs/installation/sourcecode)\n- [**クイックスタート**](http://docs.dbgpt.site/docs/quickstart)\n- [**アプリケーション**](http://docs.dbgpt.site/docs/operation_manual)\n  - [アプリの使用](https://docs.dbgpt.site/docs/application/app_usage)\n  - [AWELフローの使用](https://docs.dbgpt.site/docs/application/awel_flow_usage)\n- [**デバッグ**](http://docs.dbgpt.site/docs/operation_manual/advanced_tutorial/debugging)\n- [**高度な使用法**](https://docs.dbgpt.site/docs/application/advanced_tutorial/cli)\n  - [SMMF](https://docs.dbgpt.site/docs/application/advanced_tutorial/smmf)\n  - [ファインチューニング](https://docs.dbgpt.site/docs/application/fine_tuning_manual/dbgpt_hub)\n  - [AWEL](http://docs.dbgpt.cn/docs/awel/tutorial)\n\n## 特徴\n\n現在、私たちはいくつかの主要な機能を紹介して、現在の能力を示しています：\n- **プライベートドメインQ&A＆データ処理**\n\n  DB-GPTプロジェクトは、知識ベースの構築を改善し、構造化および非構造化データの両方の効率的なストレージと検索を可能にする一連の機能を提供します。これらの機能には、複数のファイル形式のアップロードのサポート、カスタムデータ抽出プラグインの統合、および大量の情報を効果的に管理するための統一されたベクトルストレージと検索機能が含まれます。\n\n- **マルチデータソース＆GBI（Generative Business Intelligence）**\n\n  DB-GPTプロジェクトは、Excel、データベース、データウェアハウスなどのさまざまなデータソースとの自然言語のシームレスな対話を容易にします。これらのソースから情報を照会および取得するプロセスを簡素化し、直感的な会話を行い、洞察を得ることができます。さらに、DB-GPTは分析レポートの生成をサポートし、ユーザーに貴重なデータの要約と解釈を提供します。\n\n- **マルチエージェント＆プラグイン**\n\n  さまざまなタスクを実行するためのカスタムプラグインのサポートを提供し、Auto-GPTプラグインモデルをネイティブにサポートしています。エージェントプロトコルは、エージェントプロトコル標準に準拠しています。\n\n- **自動ファインチューニングText2SQL**\n\n  私たちはまた、大規模言語モデル（LLM）、Text2SQLデータセット、LoRA/QLoRA/Pturningなどのファインチューニング方法を中心に、自動ファインチューニングの軽量フレームワークを開発しました。このフレームワークは、Text-to-SQLファインチューニングをアセンブリラインのように簡単にします。[DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- **SMMF（サービス指向マルチモデル管理フレームワーク）**\n\n  私たちは、LLaMA/LLaMA2、Baichuan、ChatGLM、Wenxin、Tongyi、Zhipuなど、オープンソースおよびAPIエージェントからの数十の大規模言語モデル（LLM）を含む幅広いモデルをサポートしています。\n\n  - ニュース\n    - 🔥🔥🔥  [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n    - 🔥🔥🔥  [gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it)\n    - 🔥🔥🔥  [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it)\n    - 🔥🔥🔥  [DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)\n    - 🔥🔥🔥  [DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)\n    - 🔥🔥🔥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - 🔥🔥🔥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - 🔥🔥🔥  [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n    - 🔥🔥🔥  [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)\n    - 🔥🔥🔥  [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)\n    - 🔥🔥🔥  [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)\n    - 🔥🔥🔥  [Yi-1.5-34B-Chat](https://huggingface.co/01-ai/Yi-1.5-34B-Chat)\n    - 🔥🔥🔥  [Yi-1.5-9B-Chat](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)\n    - 🔥🔥🔥  [Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-110B-Chat](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)\n    - 🔥🔥🔥  [Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n    - 🔥🔥🔥  [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)\n    - 🔥🔥🔥  [Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)\n    - 🔥🔥🔥  [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)\n    - 🔥🔥🔥  [gemma-2b-it](https://huggingface.co/google/gemma-2b-it)\n    - 🔥🔥🔥  [SOLAR-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)\n    - 🔥🔥🔥  [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n    - 🔥🔥🔥  [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen-72B-Chat)\n    - 🔥🔥🔥  [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)\n  - [サポートされているLLMの詳細](http://docs.dbgpt.site/docs/modules/smmf)\n\n- **プライバシーとセキュリティ**\n\n  私たちは、さまざまな技術を実装することにより、データのプライバシーとセキュリティを確保しています。これには、大規模モデルのプライベート化とプロキシの非識別化が含まれます。\n\n- サポートされているデータソース\n  - [データソース](http://docs.dbgpt.site/docs/modules/connections)\n\n## 画像\n🌐 [AutoDLイメージ](https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt)\n\n### 言語切り替え\n    .env設定ファイルでLANGUAGEパラメータを変更して、異なる言語に切り替えることができます。デフォルトは英語です（中国語：zh、英語：en、他の言語は後で追加されます）。\n\n## 貢献\n\n- 新しい貢献のための詳細なガイドラインを確認するには、[貢献方法](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)を参照してください。\n\n### 貢献者ウォール\n<a href=\"https://github.com/eosphoros-ai/DB-GPT/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&max=200\" />\n</a>\n\n## ライセンス\nMITライセンス（MIT）\n\n## 引用\nもし`DB-GPT`があなたの研究や開発に役立つと感じた場合、以下の論文を引用してください。\n\nDB-GPTの全体的なアーキテクチャについて知りたい場合は、<a href=\"https://arxiv.org/abs/2312.17449\" target=\"_blank\">論文</a>と<a href=\"https://arxiv.org/abs/2404.10209\" target=\"_blank\">論文</a>を引用してください。\n\nDB-GPTを使用してAgent開発に関する内容について知りたい場合は、<a href=\"https://arxiv.org/abs/2412.13520\" target=\"_blank\">論文</a>を引用してください。 \n```bibtex\n@article{xue2023dbgpt,\n      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, \n      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},\n      year={2023},\n      journal={arXiv preprint arXiv:2312.17449},\n      url={https://arxiv.org/abs/2312.17449}\n}\n@misc{huang2024romasrolebasedmultiagentdatabase,\n      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, \n      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},\n      year={2024},\n      eprint={2412.13520},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2412.13520}, \n}\n@inproceedings{xue2024demonstration,\n      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, \n      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},\n      year={2024},\n      booktitle = \"Proceedings of the VLDB Endowment\",\n      url={https://arxiv.org/abs/2404.10209}\n}\n```\n\n## 連絡先情報\nコミュニティを構築するために取り組んでいます。コミュニティの構築に関するアイデアがあれば、お気軽にお問い合わせください。\n[![](https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat)](https://discord.gg/7uQnPuveTY)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=csunny/DB-GPT&type=Date)](https://star-history.com/#csunny/DB-GPT)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.283203125,
          "content": "# DB-GPT: AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents\n\n<p align=\"left\">\n  <img src=\"./assets/LOGO.png\" width=\"100%\" />\n</p>\n\n<div align=\"center\">\n  <p>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"stars\" src=\"https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"forks\" src=\"https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://opensource.org/licenses/MIT\">\n      <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" />\n    </a>\n     <a href=\"https://github.com/eosphoros-ai/DB-GPT/releases\">\n      <img alt=\"Release Notes\" src=\"https://img.shields.io/github/release/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT/issues\">\n      <img alt=\"Open Issues\" src=\"https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://discord.gg/7uQnPuveTY\">\n      <img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat\" />\n    </a>\n    <a href=\"https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA\">\n      <img alt=\"Slack\" src=\"https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack\" />\n    </a>\n    <a href=\"https://codespaces.new/eosphoros-ai/DB-GPT\">\n      <img alt=\"Open in GitHub Codespaces\" src=\"https://github.com/codespaces/badge.svg\" />\n    </a>\n  </p>\n\n\n[**简体中文**](README.zh.md) | [**日本語**](README.ja.md) | [**Discord**](https://discord.gg/7uQnPuveTY) | [**Documents**](https://docs.dbgpt.site) | [**微信**](https://github.com/eosphoros-ai/DB-GPT/blob/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC) | [**Community**](https://github.com/eosphoros-ai/community) | [**Paper**](https://arxiv.org/pdf/2312.17449.pdf)\n\n</div>\n\n## What is DB-GPT?\n\n🤖 **DB-GPT is an open source AI native data app development framework with AWEL(Agentic Workflow Expression Language) and agents**. \n\nThe purpose is to build infrastructure in the field of large models, through the development of multiple technical capabilities such as multi-model management (SMMF), Text2SQL effect optimization, RAG framework and optimization, Multi-Agents framework collaboration, AWEL (agent workflow orchestration), etc. Which makes large model applications with data simpler and more convenient.\n\n🚀 **In the Data 3.0 era, based on models and databases, enterprises and developers can build their own bespoke applications with less code.**\n\n### DISCKAIMER\n- [disckaimer](./DISCKAIMER.md)\n\n### AI-Native Data App \n---\n- 🔥🔥🔥 [Released V0.6.0 | A set of significant upgrades](https://docs.dbgpt.cn/docs/changelog/Released_V0.6.0)\n  - [The AWEL upgrade to 2.0]()\n  - [GraphRAG]()\n  - [AI Native Data App construction and management]()\n  - [The GPT-Vis upgrade, supporting a variety of visualization charts]()\n  - [Support Text2NLU and Text2GQL fine-tuning]()\n  - [Support Intent recognition, slot filling, and Prompt management]()\n\n---\n\n![app_chat_v0 6](https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113)\n\n![app_manage_chat_data_v0 6](https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611)\n\n![chat_dashboard_display_v0 6](https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9)\n\n![agent_prompt_awel_v0 6](https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc)\n\n## Contents\n- [Introduction](#introduction)\n- [Install](#install)\n- [Features](#features)\n- [Contribution](#contribution)\n- [Contact](#contact-information)\n\n## Introduction \nThe architecture of DB-GPT is shown in the following figure:\n\n<p align=\"center\">\n  <img src=\"./assets/dbgpt.png\" width=\"800\" />\n</p>\n\nThe core capabilities include the following parts:\n\n- **RAG (Retrieval Augmented Generation)**: RAG is currently the most practically implemented and urgently needed domain. DB-GPT has already implemented a framework based on RAG, allowing users to build knowledge-based applications using the RAG capabilities of DB-GPT.\n\n- **GBI (Generative Business Intelligence)**: Generative BI is one of the core capabilities of the DB-GPT project, providing the foundational data intelligence technology to build enterprise report analysis and business insights.\n\n- **Fine-tuning Framework**: Model fine-tuning is an indispensable capability for any enterprise to implement in vertical and niche domains. DB-GPT provides a complete fine-tuning framework that integrates seamlessly with the DB-GPT project. In recent fine-tuning efforts, an accuracy rate based on the Spider dataset has been achieved at 82.5%.\n\n- **Data-Driven Multi-Agents Framework**: DB-GPT offers a data-driven self-evolving multi-agents framework, aiming to continuously make decisions and execute based on data.\n\n- **Data Factory**: The Data Factory is mainly about cleaning and processing trustworthy knowledge and data in the era of large models.\n\n- **Data Sources**: Integrating various data sources to seamlessly connect production business data to the core capabilities of DB-GPT.\n\n### SubModule\n- [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub) Text-to-SQL workflow with high performance by applying Supervised Fine-Tuning (SFT) on Large Language Models (LLMs).\n\n- [dbgpts](https://github.com/eosphoros-ai/dbgpts)  dbgpts is the official repository which contains some data apps、AWEL operators、AWEL workflow templates and agents which build upon DB-GPT.\n\n#### Text2SQL Finetune\n- support llms\n  - [x] LLaMA\n  - [x] LLaMA-2\n  - [x] BLOOM\n  - [x] BLOOMZ\n  - [x] Falcon\n  - [x] Baichuan\n  - [x] Baichuan2\n  - [x] InternLM\n  - [x] Qwen\n  - [x] XVERSE\n  - [x] ChatGLM2\n\n-  SFT Accuracy\nAs of October 10, 2023, through the fine-tuning of an open-source model with 13 billion parameters using this project, we have achieved execution accuracy on the Spider dataset that surpasses even GPT-4!\n\n[More Information about Text2SQL finetune](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- [DB-GPT-Plugins](https://github.com/eosphoros-ai/DB-GPT-Plugins) DB-GPT Plugins that can run Auto-GPT plugin directly\n- [GPT-Vis](https://github.com/eosphoros-ai/GPT-Vis) Visualization protocol\n\n## Install \n![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=macos&logoColor=F0F0F0)\n![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)\n\n[**Usage Tutorial**](http://docs.dbgpt.cn/docs/overview)\n- [**Install**](http://docs.dbgpt.cn/docs/installation)\n  - [Docker](http://docs.dbgpt.cn/docs/installation/docker)\n  - [Source Code](http://docs.dbgpt.cn/docs/installation/sourcecode)\n- [**Quickstart**](http://docs.dbgpt.cn/docs/quickstart)\n- [**Application**](http://docs.dbgpt.cn/docs/operation_manual)\n  - [Development Guide](http://docs.dbgpt.cn/docs/cookbook/app/data_analysis_app_develop) \n  - [App Usage](http://docs.dbgpt.cn/docs/application/app_usage)\n  - [AWEL Flow Usage](http://docs.dbgpt.cn/docs/application/awel_flow_usage)\n- [**Debugging**](http://docs.dbgpt.cn/docs/operation_manual/advanced_tutorial/debugging)\n- [**Advanced Usage**](http://docs.dbgpt.cn/docs/application/advanced_tutorial/cli)\n  - [SMMF](http://docs.dbgpt.cn/docs/application/advanced_tutorial/smmf)\n  - [Finetune](http://docs.dbgpt.cn/docs/application/fine_tuning_manual/dbgpt_hub)\n  - [AWEL](http://docs.dbgpt.cn/docs/awel/tutorial)\n\n\n## Features\n\nAt present, we have introduced several key features to showcase our current capabilities:\n- **Private Domain Q&A & Data Processing**\n\n  The DB-GPT project offers a range of functionalities designed to improve knowledge base construction and enable efficient storage and retrieval of both structured and unstructured data. These functionalities include built-in support for uploading multiple file formats, the ability to integrate custom data extraction plug-ins, and unified vector storage and retrieval capabilities for effectively managing large volumes of information.\n\n- **Multi-Data Source & GBI(Generative Business intelligence)**\n\n  The DB-GPT project facilitates seamless natural language interaction with diverse data sources, including Excel, databases, and data warehouses. It simplifies the process of querying and retrieving information from these sources, empowering users to engage in intuitive conversations and gain insights. Moreover, DB-GPT supports the generation of analytical reports, providing users with valuable data summaries and interpretations.\n\n- **Multi-Agents&Plugins**\n\n  It offers support for custom plug-ins to perform various tasks and natively integrates the Auto-GPT plug-in model. The Agents protocol adheres to the Agent Protocol standard.\n\n- **Automated Fine-tuning text2SQL**\n\n  We've also developed an automated fine-tuning lightweight framework centred on large language models (LLMs), Text2SQL datasets, LoRA/QLoRA/Pturning, and other fine-tuning methods. This framework simplifies Text-to-SQL fine-tuning, making it as straightforward as an assembly line process. [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- **SMMF(Service-oriented Multi-model Management Framework)**\n\n  We offer extensive model support, including dozens of large language models (LLMs) from both open-source and API agents, such as LLaMA/LLaMA2, Baichuan, ChatGLM, Wenxin, Tongyi, Zhipu, and many more. \n\n  - News\n    - 🔥🔥🔥  [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n    - 🔥🔥🔥  [gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it)\n    - 🔥🔥🔥  [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it)\n    - 🔥🔥🔥  [DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)\n    - 🔥🔥🔥  [DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)\n    - 🔥🔥🔥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - 🔥🔥🔥  [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n    - 🔥🔥🔥  [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)\n    - 🔥🔥🔥  [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)\n    - 🔥🔥🔥  [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)\n    - 🔥🔥🔥  [Yi-1.5-34B-Chat](https://huggingface.co/01-ai/Yi-1.5-34B-Chat)\n    - 🔥🔥🔥  [Yi-1.5-9B-Chat](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)\n    - 🔥🔥🔥  [Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-110B-Chat](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)\n    - 🔥🔥🔥  [Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n    - 🔥🔥🔥  [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)\n    - 🔥🔥🔥  [Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)\n    - 🔥🔥🔥  [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)\n    - 🔥🔥🔥  [gemma-2b-it](https://huggingface.co/google/gemma-2b-it)\n    - 🔥🔥🔥  [SOLAR-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)\n    - 🔥🔥🔥  [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n    - 🔥🔥🔥  [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen-72B-Chat)\n    - 🔥🔥🔥  [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)\n  - [More Supported LLMs](http://docs.dbgpt.site/docs/modules/smmf)\n\n- **Privacy and Security**\n  \n  We ensure the privacy and security of data through the implementation of various technologies, including privatized large models and proxy desensitization.\n\n- Support Datasources\n  - [Datasources](http://docs.dbgpt.cn/docs/modules/connections)\n\n## Image\n🌐 [AutoDL Image](https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt)\n\n\n### Language Switching\n    In the .env configuration file, modify the LANGUAGE parameter to switch to different languages. The default is English (Chinese: zh, English: en, other languages to be added later).\n\n## Contribution\n\n- To check detailed guidelines for new contributions, please refer [how to contribute](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)\n\n### Contributors Wall\n<a href=\"https://github.com/eosphoros-ai/DB-GPT/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&max=200\" />\n</a>\n\n\n## Licence\nThe MIT License (MIT)\n\n## Citation\nIf you want to understand the overall architecture of DB-GPT, please cite <a href=\"https://arxiv.org/abs/2312.17449\" target=\"_blank\">paper</a> and <a href=\"https:// arxiv.org/abs/2404.10209\" target=\"_blank\">Paper</a>\n\nIf you want to learn about using DB-GPT for Agent development, please cite the <a href=\"https://arxiv.org/abs/2412.13520\" target=\"_blank\">paper</a>\n```bibtex\n@article{xue2023dbgpt,\n      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, \n      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},\n      year={2023},\n      journal={arXiv preprint arXiv:2312.17449},\n      url={https://arxiv.org/abs/2312.17449}\n}\n@misc{huang2024romasrolebasedmultiagentdatabase,\n      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, \n      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},\n      year={2024},\n      eprint={2412.13520},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2412.13520}, \n}\n@inproceedings{xue2024demonstration,\n      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, \n      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},\n      year={2024},\n      booktitle = \"Proceedings of the VLDB Endowment\",\n      url={https://arxiv.org/abs/2404.10209}\n}\n```\n\n\n## Contact Information\nWe are working on building a community, if you have any ideas for building the community, feel free to contact us.\n[![](https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat)](https://discord.gg/7uQnPuveTY)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=csunny/DB-GPT&type=Date)](https://star-history.com/#csunny/DB-GPT)\n"
        },
        {
          "name": "README.zh.md",
          "type": "blob",
          "size": 17.5859375,
          "content": "# DB-GPT: AI原生数据应用开发框架\n\n<p align=\"left\">\n  <img src=\"./assets/LOGO.png\" width=\"100%\" />\n</p>\n\n\n<div align=\"center\">\n  <p>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"stars\" src=\"https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT\">\n        <img alt=\"forks\" src=\"https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social\" />\n    </a>\n    <a href=\"https://opensource.org/licenses/MIT\">\n      <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" />\n    </a>\n     <a href=\"https://github.com/eosphoros-ai/DB-GPT/releases\">\n      <img alt=\"Release Notes\" src=\"https://img.shields.io/github/release/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://github.com/eosphoros-ai/DB-GPT/issues\">\n      <img alt=\"Open Issues\" src=\"https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT\" />\n    </a>\n    <a href=\"https://discord.gg/7uQnPuveTY\">\n      <img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat\" />\n    </a>\n    <a href=\"https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA\">\n      <img alt=\"Slack\" src=\"https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack\" />\n    </a>\n    <a href=\"https://codespaces.new/eosphoros-ai/DB-GPT\">\n      <img alt=\"Open in GitHub Codespaces\" src=\"https://github.com/codespaces/badge.svg\" />\n    </a>\n  </p>\n\n[**English**](README.md) | [**Discord**](https://discord.gg/7uQnPuveTY) | [**文档**](https://www.yuque.com/eosphoros/dbgpt-docs/bex30nsv60ru0fmx) | [**微信**](https://github.com/eosphoros-ai/DB-GPT/blob/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC) | [**社区**](https://github.com/eosphoros-ai/community) | [**Paper**](https://arxiv.org/pdf/2312.17449.pdf)\n</div>\n\n## DB-GPT 是什么？\n\n🤖️ **DB-GPT是一个开源的AI原生数据应用开发框架(AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents)。**\n\n目的是构建大模型领域的基础设施，通过开发多模型管理(SMMF)、Text2SQL效果优化、RAG框架以及优化、Multi-Agents框架协作、AWEL(智能体工作流编排)等多种技术能力，让围绕数据库构建大模型应用更简单，更方便。 \n\n🚀 **数据3.0 时代，基于模型、数据库，企业/开发者可以用更少的代码搭建自己的专属应用。**\n\n## 效果演示\n\n### AI原生数据智能应用\n---\n- [V0.6.0发布——一系列重大功能更新](https://www.yuque.com/eosphoros/dbgpt-docs/fho86kk4e9y4rkpd)\n  - AWEL协议升级2.0，支持更复杂的编排，同时优化了前端可视化与交互能力。\n  - 支持数据应用的创建与生命周期管理，提供多种应用构建模式。1. 多智能体自动规划模式、2. 任务流编排模式、3. 单一智能体模式、4. 原生应用模式\n  - GraphRAG支持图社区摘要与混合检索，性能与检索效果有显著优势，同时支持丰富的前端可视化。\n  - 支持意图识别、槽位填充与Prompt管理。\n  - GPT-Vis前端可视化升级，支持更丰富的可视化图表。 \n  - 支持Text2NLU与Text2GQL微调, 即新增意图分类与从自然语言到图语言的微调。 \n\n\n### Data Agents \n\n![app_chat_v0 6](https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113)\n\n![app_manage_chat_data_v0 6](https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611)\n\n![chat_dashboard_display_v0 6](https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9)\n\n![agent_prompt_awel_v0 6](https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc)\n\n\n## 目录\n- [架构方案](#架构方案)\n- [安装](#安装)\n- [特性简介](#特性一览)\n- [贡献](#贡献)\n- [路线图](#路线图)\n- [联系我们](#联系我们)\n\n## 架构方案\n\n<p align=\"center\">\n  <img src=\"./assets/dbgpt.png\" width=\"800px\" />\n</p>\n\n核心能力主要有以下几个部分:\n- **RAG(Retrieval Augmented Generation)**，RAG是当下落地实践最多，也是最迫切的领域，DB-GPT目前已经实现了一套基于RAG的框架，用户可以基于DB-GPT的RAG能力构建知识类应用。 \n\n- **GBI**：生成式BI是DB-GPT项目的核心能力之一，为构建企业报表分析、业务洞察提供基础的数智化技术保障。 \n\n- **微调框架**:  模型微调是任何一个企业在垂直、细分领域落地不可或缺的能力，DB-GPT提供了完整的微调框架，实现与DB-GPT项目的无缝打通，在最近的微调中，基于spider的准确率已经做到了82.5%\n\n- **数据驱动的Multi-Agents框架**:  DB-GPT提供了数据驱动的自进化Multi-Agents框架，目标是可以持续基于数据做决策与执行。 \n\n- **数据工厂**: 数据工厂主要是在大模型时代，做可信知识、数据的清洗加工。 \n\n- **数据源**: 对接各类数据源，实现生产业务数据无缝对接到DB-GPT核心能力。 \n\n### RAG生产落地实践架构\n<p align=\"center\">\n  <img src=\"./assets/RAG-IN-ACTION.jpg\" width=\"800px\" />\n</p>\n\n### 子模块\n- [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub) 通过微调来持续提升Text2SQL效果 \n- [DB-GPT-Plugins](https://github.com/eosphoros-ai/DB-GPT-Plugins) DB-GPT 插件仓库, 兼容Auto-GPT\n- [GPT-Vis](https://github.com/eosphoros-ai/DB-GPT-Web) 可视化协议 \n\n- [dbgpts](https://github.com/eosphoros-ai/dbgpts)  dbgpts 是官方提供的数据应用仓库, 包含数据智能应用, 智能体编排流程模版, 通用算子等构建在DB-GPT之上的资源。 \n\n## 安装\n\n![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=macos&logoColor=F0F0F0)\n![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)\n\n[**教程**](https://www.yuque.com/eosphoros/dbgpt-docs/bex30nsv60ru0fmx)\n- [**快速开始**](https://www.yuque.com/eosphoros/dbgpt-docs/ew0kf1plm0bru2ga)\n  - [源码安装](https://www.yuque.com/eosphoros/dbgpt-docs/urh3fcx8tu0s9xmb)\n  - [Docker安装](https://www.yuque.com/eosphoros/dbgpt-docs/glf87qg4xxcyrp89)\n  - [Docker Compose安装](https://www.yuque.com/eosphoros/dbgpt-docs/wwdu11e0v5nkfzin)\n- [**使用手册**](https://www.yuque.com/eosphoros/dbgpt-docs/tkspdd0tcy2vlnu4)\n  - [知识库](https://www.yuque.com/eosphoros/dbgpt-docs/ycyz3d9b62fccqxh)\n  - [数据对话](https://www.yuque.com/eosphoros/dbgpt-docs/gd9hbhi1dextqgbz)\n  - [Excel对话](https://www.yuque.com/eosphoros/dbgpt-docs/prugoype0xd2g4bb)\n  - [数据库对话](https://www.yuque.com/eosphoros/dbgpt-docs/wswpv3zcm2c9snmg)\n  - [报表分析](https://www.yuque.com/eosphoros/dbgpt-docs/vsv49p33eg4p5xc1)\n  - [Agents](https://www.yuque.com/eosphoros/dbgpt-docs/pom41m7oqtdd57hm)\n- [**进阶教程**](https://www.yuque.com/eosphoros/dbgpt-docs/dxalqb8wsv2xkm5f)\n  - [数智应用开发](https://www.yuque.com/eosphoros/dbgpt-docs/ancwnrsk9agc6e4w)\n  - [智能体工作流使用](https://www.yuque.com/eosphoros/dbgpt-docs/hcomfb3yrleg7gmq)\n  - [智能应用使用](https://www.yuque.com/eosphoros/dbgpt-docs/aiagvxeb86iarq6r)\n  - [多模型管理](https://www.yuque.com/eosphoros/dbgpt-docs/huzgcf2abzvqy8uv)\n  - [命令行使用](https://www.yuque.com/eosphoros/dbgpt-docs/gd4kgumgd004aly8)\n- [**模型服务部署**](https://www.yuque.com/eosphoros/dbgpt-docs/vubxiv9cqed5mc6o)\n  - [单机部署](https://www.yuque.com/eosphoros/dbgpt-docs/kwg1ed88lu5fgawb)\n  - [集群部署](https://www.yuque.com/eosphoros/dbgpt-docs/gmbp9619ytyn2v1s)\n  - [vLLM](https://www.yuque.com/eosphoros/dbgpt-docs/bhy9igdvanx1uluf)\n- [**如何Debug**](https://www.yuque.com/eosphoros/dbgpt-docs/eyg0ocbc2ce3q95r)\n- [**AWEL**](https://www.yuque.com/eosphoros/dbgpt-docs/zozbzslbfk0m0op5)\n- [**FAQ**](https://www.yuque.com/eosphoros/dbgpt-docs/gomtc46qonmyt44l)\n\n## 特性一览\n- **私域问答&数据处理&RAG**\n\n  支持内置、多文件格式上传、插件自抓取等方式自定义构建知识库，对海量结构化，非结构化数据做统一向量存储与检索\n\n- **多数据源&GBI**\n\n  支持自然语言与Excel、数据库、数仓等多种数据源交互，并支持分析报告。\n\n- **自动化微调**\n\n  围绕大语言模型、Text2SQL数据集、LoRA/QLoRA/Pturning等微调方法构建的自动化微调轻量框架, 让TextSQL微调像流水线一样方便。详见: [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub)\n\n- **数据驱动的Agents插件**\n\n  支持自定义插件执行任务，原生支持Auto-GPT插件模型，Agents协议采用Agent Protocol标准\n\n- **多模型支持与管理**\n\n  海量模型支持，包括开源、API代理等几十种大语言模型。如LLaMA/LLaMA2、Baichuan、ChatGLM、文心、通义、智谱等。当前已支持如下模型: \n\n  - 新增支持模型\n    - 🔥🔥🔥  [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n    - 🔥🔥🔥  [gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it)\n    - 🔥🔥🔥  [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it)\n    - 🔥🔥🔥  [DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)\n    - 🔥🔥🔥  [DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)\n    - 🔥🔥🔥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - 🔥🔥🔥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)\n    - 🔥🔥🔥  [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n    - 🔥🔥🔥  [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n    - 🔥🔥🔥  [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n    - 🔥🔥🔥  [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)\n    - 🔥🔥🔥  [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)\n    - 🔥🔥🔥  [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)\n    - 🔥🔥🔥  [Yi-1.5-34B-Chat](https://huggingface.co/01-ai/Yi-1.5-34B-Chat)\n    - 🔥🔥🔥  [Yi-1.5-9B-Chat](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)\n    - 🔥🔥🔥  [Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-110B-Chat](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)\n    - 🔥🔥🔥  [Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)\n    - 🔥🔥🔥  [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n    - 🔥🔥🔥  [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)\n    - 🔥🔥🔥  [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)\n    - 🔥🔥🔥  [Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)\n    - 🔥🔥🔥  [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)\n    - 🔥🔥🔥  [gemma-2b-it](https://huggingface.co/google/gemma-2b-it)\n    - 🔥🔥🔥  [SOLAR-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)\n    - 🔥🔥🔥  [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n    - 🔥🔥🔥  [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen-72B-Chat)\n    - 🔥🔥🔥  [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)\n  - [更多开源模型](https://www.yuque.com/eosphoros/dbgpt-docs/iqaaqwriwhp6zslc#qQktR)\n\n  - 支持在线代理模型\n    - [x] [DeepSeek.deepseek-chat](https://platform.deepseek.com/api-docs/)\n    - [x] [Ollama.API](https://github.com/ollama/ollama/blob/main/docs/api.md)\n    - [x] [月之暗面.Moonshot](https://platform.moonshot.cn/docs/)\n    - [x] [零一万物.Yi](https://platform.lingyiwanwu.com/docs)\n    - [x] [OpenAI·ChatGPT](https://api.openai.com/)\n    - [x] [百川·Baichuan](https://platform.baichuan-ai.com/)\n    - [x] [阿里·通义](https://www.aliyun.com/product/dashscope)\n    - [x] [百度·文心](https://cloud.baidu.com/product/wenxinworkshop?track=dingbutonglan)\n    - [x] [智谱·ChatGLM](http://open.bigmodel.cn/)\n    - [x] [讯飞·星火](https://xinghuo.xfyun.cn/)\n    - [x] [Google·Bard](https://bard.google.com/)\n    - [x] [Google·Gemini](https://makersuite.google.com/app/apikey)\n\n- **隐私安全**\n\n  通过私有化大模型、代理脱敏等多种技术保障数据的隐私安全。\n\n- [支持数据源](https://www.yuque.com/eosphoros/dbgpt-docs/rc4r27ybmdwg9472)\n\n\n\n## Image\n\n🌐 [AutoDL镜像](https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt)\n\n🌐 [小程序云部署](https://www.yuque.com/eosphoros/dbgpt-docs/ek12ly8k661tbyn8)\n\n### 多语言切换\n\n在.env 配置文件当中，修改LANGUAGE参数来切换使用不同的语言，默认是英文(中文zh, 英文en, 其他语言待补充)\n\n## 使用说明\n\n### 多模型使用\n\n- [使用指南](https://www.yuque.com/eosphoros/dbgpt-docs/huzgcf2abzvqy8uv)\n\n### 数据Agents使用\n\n- [数据Agents](https://www.yuque.com/eosphoros/dbgpt-docs/gwz4rayfuwz78fbq)\n\n## 贡献\n\n更加详细的贡献指南请参考[如何贡献](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)。\n\n这是一个用于数据库的复杂且创新的工具, 我们的项目也在紧急的开发当中, 会陆续发布一些新的feature。如在使用当中有任何具体问题, 优先在项目下提issue, 如有需要, 请联系如下微信，我会尽力提供帮助，同时也非常欢迎大家参与到项目建设中。\n\n### 贡献者榜单 \n<a href=\"https://github.com/eosphoros-ai/DB-GPT/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&max=200\" />\n</a>\n\n\n## Licence\n\nThe MIT License (MIT)\n\n## 引用\n如果您发现`DB-GPT`对您的研究或开发有用，请引用以下论文，其中：\n\n如果您想了解DB-GPT整体架构，请引用<a href=\"https://arxiv.org/abs/2312.17449\" target=\"_blank\">论文</a>和<a href=\"https://arxiv.org/abs/2404.10209\" target=\"_blank\">论文</a>\n\n如果您想了解使用DB-GPT进行Agent开发相关的内容，请引用<a href=\"https://arxiv.org/abs/2412.13520\" target=\"_blank\">论文</a>\n\n```bibtex\n@article{xue2023dbgpt,\n      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, \n      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},\n      year={2023},\n      journal={arXiv preprint arXiv:2312.17449},\n      url={https://arxiv.org/abs/2312.17449}\n}\n@misc{huang2024romasrolebasedmultiagentdatabase,\n      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, \n      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},\n      year={2024},\n      eprint={2412.13520},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2412.13520}, \n}\n@inproceedings{xue2024demonstration,\n      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, \n      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},\n      year={2024},\n      booktitle = \"Proceedings of the VLDB Endowment\",\n      url={https://arxiv.org/abs/2404.10209}\n}\n```\n\n## 联系我们\n\n  **说明: 由于微信群人数上限的限制, 我们的答疑与问题支持优先会在钉钉大群进行。**\n<div style=\"display: flex; justify-content: space-around;\">\n    <figure style=\"display: flex; flex-direction: column;\">\n        <img src=\"./assets/ding.jpg\" alt=\"图片2\" style=\"width: 220px;\">\n        <p style=\"text-align: center;\">\n          钉钉\n        </p>\n    </figure>\n    <figure style=\"display: flex; flex-direction: column;\">\n        <img src=\"./assets/wechat.jpg\" alt=\"图片1\" style=\"width: 200px;\">\n        <p style=\"text-align: center;\">\n          微信\n        </p> \n    </figure>\n</div>\n\n<!-- <p align=\"center\">\n  <img src=\"./assets/wechat.jpg\" width=\"300px\" />\n</p> -->\n\n[![Star History Chart](https://api.star-history.com/svg?repos=csunny/DB-GPT&type=Date)](https://star-history.com/#csunny/DB-GPT)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "dbgpt",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 1.4150390625,
          "content": "version: '3.10'\n\nservices:\n  db:\n    image: mysql/mysql-server\n    environment:\n      MYSQL_USER: 'user'\n      MYSQL_PASSWORD: 'password'\n      MYSQL_ROOT_PASSWORD: 'aa123456'\n    ports:\n      - 3306:3306\n    volumes:\n      - dbgpt-myql-db:/var/lib/mysql\n      - ./docker/examples/my.cnf:/etc/my.cnf\n      - ./docker/examples/sqls:/docker-entrypoint-initdb.d\n      - ./assets/schema/dbgpt.sql:/docker-entrypoint-initdb.d/dbgpt.sql\n    restart: unless-stopped\n    networks:\n      - dbgptnet\n  webserver:\n    image: eosphorosai/dbgpt:latest\n    command: python3 dbgpt/app/dbgpt_server.py\n    environment:\n      - LOCAL_DB_HOST=db\n      - LOCAL_DB_PASSWORD=aa123456\n      - ALLOWLISTED_PLUGINS=db_dashboard\n      - LLM_MODEL=glm-4-9b-chat\n    depends_on:\n      - db\n    volumes:\n      - /data:/data\n      # Please modify it to your own model directory\n      - /data/models:/app/models\n      - dbgpt-data:/app/pilot/data\n      - dbgpt-message:/app/pilot/message\n    env_file:\n      - .env.template\n    ports:\n      - 5670:5670/tcp\n    # webserver may be failed, it must wait all sqls in /docker-entrypoint-initdb.d execute finish.\n    restart: unless-stopped\n    networks:\n      - dbgptnet\n    ipc: host\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              capabilities: [gpu]\nvolumes:\n  dbgpt-myql-db:\n  dbgpt-data:\n  dbgpt-message:\nnetworks:\n  dbgptnet:\n    driver: bridge\n    name: dbgptnet"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "i18n",
          "type": "tree",
          "content": null
        },
        {
          "name": "pilot",
          "type": "tree",
          "content": null
        },
        {
          "name": "plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 26.72265625,
          "content": "import functools\nimport json\nimport os\nimport platform\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport urllib.request\nfrom enum import Enum\nfrom typing import Callable, List, Optional, Tuple\nfrom urllib.parse import quote, urlparse\n\nimport setuptools\nfrom setuptools import find_packages\n\nwith open(\"README.md\", mode=\"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nIS_DEV_MODE = os.getenv(\"IS_DEV_MODE\", \"true\").lower() == \"true\"\n# If you modify the version, please modify the version in the following files:\n# dbgpt/_version.py\nDB_GPT_VERSION = os.getenv(\"DB_GPT_VERSION\", \"0.6.3\")\n\nBUILD_NO_CACHE = os.getenv(\"BUILD_NO_CACHE\", \"true\").lower() == \"true\"\nLLAMA_CPP_GPU_ACCELERATION = (\n    os.getenv(\"LLAMA_CPP_GPU_ACCELERATION\", \"true\").lower() == \"true\"\n)\nBUILD_FROM_SOURCE = os.getenv(\"BUILD_FROM_SOURCE\", \"false\").lower() == \"true\"\nBUILD_FROM_SOURCE_URL_FAST_CHAT = os.getenv(\n    \"BUILD_FROM_SOURCE_URL_FAST_CHAT\", \"git+https://github.com/lm-sys/FastChat.git\"\n)\nBUILD_VERSION_OPENAI = os.getenv(\"BUILD_VERSION_OPENAI\")\nINCLUDE_QUANTIZATION = os.getenv(\"INCLUDE_QUANTIZATION\", \"true\").lower() == \"true\"\nINCLUDE_OBSERVABILITY = os.getenv(\"INCLUDE_OBSERVABILITY\", \"true\").lower() == \"true\"\n\n\ndef parse_requirements(file_name: str) -> List[str]:\n    with open(file_name) as f:\n        return [\n            require.strip()\n            for require in f\n            if require.strip() and not require.startswith(\"#\")\n        ]\n\n\ndef find_python():\n    python_path = sys.executable\n    print(python_path)\n    if not python_path:\n        print(\"Python command not found.\")\n        return None\n    return python_path\n\n\ndef get_latest_version(package_name: str, index_url: str, default_version: str):\n    python_command = find_python()\n    if not python_command:\n        print(\"Python command not found.\")\n        return default_version\n\n    command_index_versions = [\n        python_command,\n        \"-m\",\n        \"pip\",\n        \"index\",\n        \"versions\",\n        package_name,\n        \"--index-url\",\n        index_url,\n    ]\n\n    result_index_versions = subprocess.run(\n        command_index_versions, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    if result_index_versions.returncode == 0:\n        output = result_index_versions.stdout.decode()\n        lines = output.split(\"\\n\")\n        for line in lines:\n            if \"Available versions:\" in line:\n                available_versions = line.split(\":\")[1].strip()\n                latest_version = available_versions.split(\",\")[0].strip()\n                # Query for compatibility with the latest version of torch\n                if package_name == \"torch\" or \"torchvision\":\n                    latest_version = latest_version.split(\"+\")[0]\n                return latest_version\n    else:\n        command_simulate_install = [\n            python_command,\n            \"-m\",\n            \"pip\",\n            \"install\",\n            f\"{package_name}==\",\n        ]\n\n        result_simulate_install = subprocess.run(\n            command_simulate_install, stderr=subprocess.PIPE\n        )\n        print(result_simulate_install)\n        stderr_output = result_simulate_install.stderr.decode()\n        print(stderr_output)\n        match = re.search(r\"from versions: (.+?)\\)\", stderr_output)\n        if match:\n            available_versions = match.group(1).split(\", \")\n            latest_version = available_versions[-1].strip()\n            return latest_version\n    return default_version\n\n\ndef encode_url(package_url: str) -> str:\n    parsed_url = urlparse(package_url)\n    encoded_path = quote(parsed_url.path)\n    safe_url = parsed_url._replace(path=encoded_path).geturl()\n    return safe_url, parsed_url.path\n\n\ndef cache_package(package_url: str, package_name: str, is_windows: bool = False):\n    safe_url, parsed_url = encode_url(package_url)\n    if BUILD_NO_CACHE:\n        return safe_url\n\n    from pip._internal.utils.appdirs import user_cache_dir\n\n    filename = os.path.basename(parsed_url)\n    cache_dir = os.path.join(user_cache_dir(\"pip\"), \"http\", \"wheels\", package_name)\n    os.makedirs(cache_dir, exist_ok=True)\n\n    local_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(local_path):\n        temp_path = local_path + \".tmp\"\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n        try:\n            print(f\"Download {safe_url} to {local_path}\")\n            urllib.request.urlretrieve(safe_url, temp_path)\n            shutil.move(temp_path, local_path)\n        finally:\n            if os.path.exists(temp_path):\n                os.remove(temp_path)\n    return f\"file:///{local_path}\" if is_windows else f\"file://{local_path}\"\n\n\nclass SetupSpec:\n    def __init__(self) -> None:\n        self.extras: dict = {}\n        self.install_requires: List[str] = []\n\n    @property\n    def unique_extras(self) -> dict[str, list[str]]:\n        unique_extras = {}\n        for k, v in self.extras.items():\n            unique_extras[k] = list(set(v))\n        return unique_extras\n\n\nsetup_spec = SetupSpec()\n\n\nclass AVXType(Enum):\n    BASIC = \"basic\"\n    AVX = \"AVX\"\n    AVX2 = \"AVX2\"\n    AVX512 = \"AVX512\"\n\n    @staticmethod\n    def of_type(avx: str):\n        for item in AVXType:\n            if item._value_ == avx:\n                return item\n        return None\n\n\nclass OSType(Enum):\n    WINDOWS = \"win\"\n    LINUX = \"linux\"\n    DARWIN = \"darwin\"\n    OTHER = \"other\"\n\n\n@functools.cache\ndef get_cpu_avx_support() -> Tuple[OSType, AVXType]:\n    system = platform.system()\n    os_type = OSType.OTHER\n    cpu_avx = AVXType.BASIC\n    env_cpu_avx = AVXType.of_type(os.getenv(\"DBGPT_LLAMA_CPP_AVX\"))\n\n    if \"windows\" in system.lower():\n        os_type = OSType.WINDOWS\n        output = \"avx2\"\n        print(\"Current platform is windows, use avx2 as default cpu architecture\")\n    elif system == \"Linux\":\n        os_type = OSType.LINUX\n        if os.path.exists(\"/etc/alpine-release\"):\n            # For Alpine, we'll check /proc/cpuinfo directly\n            with open(\"/proc/cpuinfo\", \"r\") as f:\n                output = f.read()\n        else:\n            result = subprocess.run(\n                [\"lscpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n            )\n            output = result.stdout.decode()\n    elif system == \"Darwin\":\n        os_type = OSType.DARWIN\n        result = subprocess.run(\n            [\"sysctl\", \"-a\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        output = result.stdout.decode()\n    else:\n        os_type = OSType.OTHER\n        print(\"Unsupported OS to get cpu avx, use default\")\n        return os_type, env_cpu_avx if env_cpu_avx else cpu_avx\n\n    if \"avx512\" in output.lower():\n        cpu_avx = AVXType.AVX512\n    elif \"avx2\" in output.lower():\n        cpu_avx = AVXType.AVX2\n    elif \"avx \" in output.lower():\n        # cpu_avx =  AVXType.AVX\n        pass\n    return os_type, env_cpu_avx if env_cpu_avx else cpu_avx\n\n\ndef get_cuda_version_from_torch():\n    try:\n        import torch\n\n        return torch.version.cuda\n    except:\n        return None\n\n\ndef get_cuda_version_from_nvcc():\n    try:\n        output = subprocess.check_output([\"nvcc\", \"--version\"])\n        version_line = [\n            line for line in output.decode(\"utf-8\").split(\"\\n\") if \"release\" in line\n        ][0]\n        return version_line.split(\"release\")[-1].strip().split(\",\")[0]\n    except:\n        return None\n\n\ndef get_cuda_version_from_nvidia_smi():\n    try:\n        output = subprocess.check_output([\"nvidia-smi\"]).decode(\"utf-8\")\n        match = re.search(r\"CUDA Version:\\s+(\\d+\\.\\d+)\", output)\n        if match:\n            return match.group(1)\n        else:\n            return None\n    except:\n        return None\n\n\ndef get_cuda_version() -> str:\n    try:\n        cuda_version = get_cuda_version_from_torch()\n        if not cuda_version:\n            cuda_version = get_cuda_version_from_nvcc()\n        if not cuda_version:\n            cuda_version = get_cuda_version_from_nvidia_smi()\n        return cuda_version\n    except Exception:\n        return None\n\n\ndef _build_wheels(\n    pkg_name: str,\n    pkg_version: str,\n    base_url: str = None,\n    base_url_func: Callable[[str, str, str], str] = None,\n    pkg_file_func: Callable[[str, str, str, str, OSType], str] = None,\n    supported_cuda_versions: List[str] = [\"11.8\", \"12.1\"],\n) -> Optional[str]:\n    \"\"\"\n    Build the URL for the package wheel file based on the package name, version, and CUDA version.\n    Args:\n        pkg_name (str): The name of the package.\n        pkg_version (str): The version of the package.\n        base_url (str): The base URL for downloading the package.\n        base_url_func (Callable): A function to generate the base URL.\n        pkg_file_func (Callable): build package file function.\n            function params: pkg_name, pkg_version, cuda_version, py_version, OSType\n        supported_cuda_versions (List[str]): The list of supported CUDA versions.\n    Returns:\n        Optional[str]: The URL for the package wheel file.\n    \"\"\"\n    os_type, _ = get_cpu_avx_support()\n    cuda_version = get_cuda_version()\n    py_version = platform.python_version()\n    py_version = \"cp\" + \"\".join(py_version.split(\".\")[0:2])\n    if os_type == OSType.DARWIN or not cuda_version:\n        return None\n\n    if cuda_version in supported_cuda_versions:\n        cuda_version = cuda_version\n    else:\n        print(\n            f\"Warning: Your CUDA version {cuda_version} is not in our set supported_cuda_versions , we will use our set version.\"\n        )\n        if cuda_version < \"12.1\":\n            cuda_version = supported_cuda_versions[0]\n        else:\n            cuda_version = supported_cuda_versions[-1]\n\n    cuda_version = \"cu\" + cuda_version.replace(\".\", \"\")\n    os_pkg_name = \"linux_x86_64\" if os_type == OSType.LINUX else \"win_amd64\"\n    if base_url_func:\n        base_url = base_url_func(pkg_version, cuda_version, py_version)\n        if base_url and base_url.endswith(\"/\"):\n            base_url = base_url[:-1]\n    if pkg_file_func:\n        full_pkg_file = pkg_file_func(\n            pkg_name, pkg_version, cuda_version, py_version, os_type\n        )\n    else:\n        full_pkg_file = f\"{pkg_name}-{pkg_version}+{cuda_version}-{py_version}-{py_version}-{os_pkg_name}.whl\"\n    if not base_url:\n        return full_pkg_file\n    else:\n        return f\"{base_url}/{full_pkg_file}\"\n\n\ndef torch_requires(\n    torch_version: str = \"2.2.1\",\n    torchvision_version: str = \"0.17.1\",\n    torchaudio_version: str = \"2.2.1\",\n):\n    os_type, _ = get_cpu_avx_support()\n    torch_pkgs = [\n        f\"torch=={torch_version}\",\n        f\"torchvision=={torchvision_version}\",\n        f\"torchaudio=={torchaudio_version}\",\n    ]\n    # Initialize torch_cuda_pkgs for non-Darwin OSes;\n    # it will be the same as torch_pkgs for Darwin or when no specific CUDA handling is needed\n    torch_cuda_pkgs = torch_pkgs[:]\n\n    if os_type != OSType.DARWIN:\n        supported_versions = [\"11.8\", \"12.1\"]\n        base_url_func = lambda v, x, y: f\"https://download.pytorch.org/whl/{x}\"\n        torch_url = _build_wheels(\n            \"torch\",\n            torch_version,\n            base_url_func=base_url_func,\n            supported_cuda_versions=supported_versions,\n        )\n        torchvision_url = _build_wheels(\n            \"torchvision\",\n            torchvision_version,\n            base_url_func=base_url_func,\n            supported_cuda_versions=supported_versions,\n        )\n\n        # Cache and add CUDA-dependent packages if URLs are available\n        if torch_url:\n            torch_url_cached = cache_package(\n                torch_url, \"torch\", os_type == OSType.WINDOWS\n            )\n            torch_cuda_pkgs[0] = f\"torch @ {torch_url_cached}\"\n        if torchvision_url:\n            torchvision_url_cached = cache_package(\n                torchvision_url, \"torchvision\", os_type == OSType.WINDOWS\n            )\n            torch_cuda_pkgs[1] = f\"torchvision @ {torchvision_url_cached}\"\n\n    # Assuming 'setup_spec' is a dictionary where we're adding these dependencies\n    setup_spec.extras[\"torch\"] = torch_pkgs\n    setup_spec.extras[\"torch_cpu\"] = torch_pkgs\n    setup_spec.extras[\"torch_cuda\"] = torch_cuda_pkgs\n\n\ndef llama_cpp_python_cuda_requires():\n    cuda_version = get_cuda_version()\n    supported_cuda_versions = [\"11.8\", \"12.1\"]\n    device = \"cpu\"\n    if not cuda_version:\n        print(\"CUDA not support, use cpu version\")\n        return\n    if not LLAMA_CPP_GPU_ACCELERATION:\n        print(\"Disable GPU acceleration\")\n        return\n    # Supports GPU acceleration\n    if cuda_version <= \"11.8\" and not None:\n        device = \"cu\" + supported_cuda_versions[0].replace(\".\", \"\")\n    else:\n        device = \"cu\" + supported_cuda_versions[-1].replace(\".\", \"\")\n    os_type, cpu_avx = get_cpu_avx_support()\n    print(f\"OS: {os_type}, cpu avx: {cpu_avx}\")\n    supported_os = [OSType.WINDOWS, OSType.LINUX]\n    if os_type not in supported_os:\n        print(\n            f\"llama_cpp_python_cuda just support in os: {[r._value_ for r in supported_os]}\"\n        )\n        return\n    cpu_device = \"\"\n    if cpu_avx == AVXType.AVX2 or cpu_avx == AVXType.AVX512:\n        cpu_device = \"avx\"\n    else:\n        cpu_device = \"basic\"\n    device += cpu_device\n    base_url = \"https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui\"\n    llama_cpp_version = \"0.2.26\"\n    py_version = \"cp310\"\n    os_pkg_name = \"manylinux_2_31_x86_64\" if os_type == OSType.LINUX else \"win_amd64\"\n    extra_index_url = f\"{base_url}/llama_cpp_python_cuda-{llama_cpp_version}+{device}-{py_version}-{py_version}-{os_pkg_name}.whl\"\n    extra_index_url, _ = encode_url(extra_index_url)\n    print(f\"Install llama_cpp_python_cuda from {extra_index_url}\")\n\n    setup_spec.extras[\"llama_cpp\"].append(f\"llama_cpp_python_cuda @ {extra_index_url}\")\n\n\ndef core_requires():\n    \"\"\"\n    pip install dbgpt or pip install \"dbgpt[core]\"\n    \"\"\"\n    setup_spec.extras[\"core\"] = [\n        \"aiohttp==3.8.4\",\n        \"chardet==5.1.0\",\n        \"importlib-resources==5.12.0\",\n        \"python-dotenv==1.0.0\",\n        \"cachetools\",\n        \"pydantic>=2.6.0\",\n        # For AWEL type checking\n        \"typeguard\",\n        # Snowflake no additional dependencies.\n        \"snowflake-id\",\n        \"typing_inspect\",\n    ]\n    # For DB-GPT python client SDK\n    setup_spec.extras[\"client\"] = setup_spec.extras[\"core\"] + [\n        \"httpx\",\n        \"fastapi>=0.100.0,<0.113.0\",\n        # For retry, chromadb need tenacity<=8.3.0\n        \"tenacity<=8.3.0\",\n    ]\n    # Simple command line dependencies\n    setup_spec.extras[\"cli\"] = setup_spec.extras[\"client\"] + [\n        \"prettytable\",\n        \"click\",\n        \"psutil==5.9.4\",\n        \"colorama==0.4.6\",\n        \"tomlkit\",\n        \"rich\",\n    ]\n    # Agent dependencies\n    setup_spec.extras[\"agent\"] = setup_spec.extras[\"cli\"] + [\n        \"termcolor\",\n        # https://github.com/eosphoros-ai/DB-GPT/issues/551\n        # TODO: remove pandas dependency\n        # alpine can't install pandas by default\n        \"pandas==2.0.3\",\n        # numpy should less than 2.0.0\n        \"numpy>=1.21.0,<2.0.0\",\n    ]\n\n    # Just use by DB-GPT internal, we should find the smallest dependency set for run\n    # we core unit test.\n    # The dependency \"framework\" is too large for now.\n    setup_spec.extras[\"simple_framework\"] = setup_spec.extras[\"agent\"] + [\n        \"jinja2\",\n        \"uvicorn\",\n        \"shortuuid\",\n        # 2.0.29 not support duckdb now\n        \"SQLAlchemy>=2.0.25, <2.0.29\",\n        # for cache\n        \"msgpack\",\n        # for AWEL operator serialization\n        \"cloudpickle\",\n        # for cache\n        # TODO: pympler has not been updated for a long time and needs to\n        #  find a new toolkit.\n        \"pympler\",\n        \"duckdb\",\n        \"duckdb-engine==0.9.1\",\n        # lightweight python library for scheduling jobs\n        \"schedule\",\n        # For datasource subpackage\n        \"sqlparse==0.4.4\",\n    ]\n    # TODO: remove fschat from simple_framework\n    if BUILD_FROM_SOURCE:\n        setup_spec.extras[\"simple_framework\"].append(\n            f\"fschat @ {BUILD_FROM_SOURCE_URL_FAST_CHAT}\"\n        )\n    else:\n        setup_spec.extras[\"simple_framework\"].append(\"fschat\")\n\n    setup_spec.extras[\"framework\"] = setup_spec.extras[\"simple_framework\"] + [\n        \"coloredlogs\",\n        \"seaborn\",\n        \"auto-gpt-plugin-template\",\n        \"gTTS==2.3.1\",\n        \"pymysql\",\n        \"jsonschema\",\n        # TODO move transformers to default\n        # \"transformers>=4.31.0\",\n        \"transformers>=4.34.0\",\n        \"alembic==1.12.0\",\n        # for excel\n        \"openpyxl==3.1.2\",\n        \"chardet==5.1.0\",\n        \"xlrd==2.0.1\",\n        \"aiofiles\",\n        # for agent\n        \"GitPython\",\n        # For AWEL dag visualization, graphviz is a small package, also we can move it to default.\n        \"graphviz\",\n        # For security\n        \"cryptography\",\n        # For high performance RPC communication in code execution\n        \"pyzmq\",\n    ]\n\n\ndef code_execution_requires():\n    \"\"\"\n    pip install \"dbgpt[code]\"\n\n    Code execution dependencies.\n    \"\"\"\n    setup_spec.extras[\"code\"] = setup_spec.extras[\"core\"] + [\n        \"msgpack\",\n        # for AWEL operator serialization\n        \"cloudpickle\",\n        \"lyric-py>=0.1.6\",\n        \"lyric-py-worker>=0.1.6\",\n        \"lyric-js-worker>=0.1.6\",\n        \"lyric-component-ts-transpiling>=0.1.6\",\n    ]\n\n\ndef knowledge_requires():\n    \"\"\"\n    pip install \"dbgpt[rag]\"\n    \"\"\"\n    setup_spec.extras[\"rag\"] = setup_spec.extras[\"vstore\"] + [\n        \"spacy==3.7\",\n        \"markdown\",\n        \"bs4\",\n        \"python-pptx\",\n        \"python-docx\",\n        \"pypdf\",\n        \"pdfplumber\",\n        \"python-multipart\",\n        \"sentence-transformers\",\n    ]\n\n    setup_spec.extras[\"graph_rag\"] = setup_spec.extras[\"rag\"] + [\n        \"neo4j\",\n        \"dbgpt-tugraph-plugins>=0.1.1\",\n    ]\n\n\ndef llama_cpp_requires():\n    \"\"\"\n    pip install \"dbgpt[llama_cpp]\"\n    \"\"\"\n    setup_spec.extras[\"llama_cpp_server\"] = [\"llama-cpp-server-py\"]\n    setup_spec.extras[\"llama_cpp\"] = setup_spec.extras[\"llama_cpp_server\"] + [\n        \"llama-cpp-python\"\n    ]\n    llama_cpp_python_cuda_requires()\n\n\ndef _build_autoawq_requires() -> Optional[str]:\n    os_type, _ = get_cpu_avx_support()\n    if os_type == OSType.DARWIN:\n        return None\n    return \"auto-gptq\"\n\n\ndef quantization_requires():\n    os_type, _ = get_cpu_avx_support()\n    quantization_pkgs = []\n    if os_type == OSType.WINDOWS:\n        # For Windows, fetch a specific bitsandbytes WHL package\n        latest_version = get_latest_version(\n            \"bitsandbytes\",\n            \"https://jllllll.github.io/bitsandbytes-windows-webui\",\n            \"0.41.1\",\n        )\n        whl_url = f\"https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-{latest_version}-py3-none-win_amd64.whl\"\n        local_pkg_path = cache_package(whl_url, \"bitsandbytes\", True)\n        setup_spec.extras[\"bitsandbytes\"] = [f\"bitsandbytes @ {local_pkg_path}\"]\n    else:\n        setup_spec.extras[\"bitsandbytes\"] = [\"bitsandbytes\"]\n\n    if os_type != OSType.DARWIN:\n        # Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\n        # autoawq requirements:\n        # 1. Compute Capability 7.5 (sm75). Turing and later architectures are supported.\n        # 2. CUDA Toolkit 11.8 and later.\n        cuda_version = get_cuda_version()\n        # autoawq_latest_version = get_latest_version(\"autoawq\", \"\", \"0.2.4\")\n        if cuda_version is None or cuda_version == \"12.1\":\n            quantization_pkgs.extend([\"autoawq\", _build_autoawq_requires(), \"optimum\"])\n        else:\n            # TODO(yyhhyy): Add autoawq install method for CUDA version 11.8\n            quantization_pkgs.extend([\"autoawq\", _build_autoawq_requires(), \"optimum\"])\n\n    setup_spec.extras[\"quantization\"] = (\n        [\"cpm_kernels\"] + quantization_pkgs + setup_spec.extras[\"bitsandbytes\"]\n    )\n\n\ndef all_vector_store_requires():\n    \"\"\"\n    pip install \"dbgpt[vstore]\"\n    \"\"\"\n    setup_spec.extras[\"vstore\"] = [\n        \"chromadb>=0.4.22\",\n    ]\n    setup_spec.extras[\"vstore_weaviate\"] = setup_spec.extras[\"vstore\"] + [\n        # \"protobuf\",\n        # \"grpcio\",\n        # weaviate depends on grpc which version is very low, we should install it\n        # manually.\n        \"weaviate-client\",\n    ]\n    setup_spec.extras[\"vstore_milvus\"] = setup_spec.extras[\"vstore\"] + [\n        \"pymilvus\",\n    ]\n    setup_spec.extras[\"vstore_all\"] = (\n        setup_spec.extras[\"vstore\"]\n        + setup_spec.extras[\"vstore_weaviate\"]\n        + setup_spec.extras[\"vstore_milvus\"]\n    )\n\n\ndef all_datasource_requires():\n    \"\"\"\n    pip install \"dbgpt[datasource]\"\n    \"\"\"\n    setup_spec.extras[\"datasource\"] = [\n        # \"sqlparse==0.4.4\",\n        \"pymysql\",\n    ]\n    # If you want to install psycopg2 and mysqlclient in ubuntu, you should install\n    # libpq-dev and libmysqlclient-dev first.\n    setup_spec.extras[\"datasource_all\"] = setup_spec.extras[\"datasource\"] + [\n        \"pyspark\",\n        \"pymssql\",\n        # install psycopg2-binary when you are in a virtual environment\n        # pip install psycopg2-binary\n        \"psycopg2\",\n        # mysqlclient 2.2.x have pkg-config issue on 3.10+\n        \"mysqlclient==2.1.0\",\n        # pydoris is too old, we should find a new package to replace it.\n        \"pydoris>=1.0.2,<2.0.0\",\n        \"clickhouse-connect\",\n        \"pyhive\",\n        \"thrift\",\n        \"thrift_sasl\",\n        \"vertica_python\",\n    ]\n\n\ndef openai_requires():\n    \"\"\"\n    pip install \"dbgpt[openai]\"\n    \"\"\"\n    setup_spec.extras[\"openai\"] = [\"tiktoken\"]\n    if BUILD_VERSION_OPENAI:\n        # Read openai sdk version from env\n        setup_spec.extras[\"openai\"].append(f\"openai=={BUILD_VERSION_OPENAI}\")\n    else:\n        setup_spec.extras[\"openai\"].append(\"openai\")\n\n    if INCLUDE_OBSERVABILITY:\n        setup_spec.extras[\"openai\"] += setup_spec.extras[\"observability\"]\n\n    setup_spec.extras[\"openai\"] += setup_spec.extras[\"framework\"]\n    setup_spec.extras[\"openai\"] += setup_spec.extras[\"rag\"]\n\n\ndef proxy_requires():\n    \"\"\"\n    pip install \"dbgpt[proxy]\"\n    \"\"\"\n    setup_spec.extras[\"proxy\"] = setup_spec.extras[\"openai\"] + [\"anthropic\"]\n\n\ndef gpt4all_requires():\n    \"\"\"\n    pip install \"dbgpt[gpt4all]\"\n    \"\"\"\n    setup_spec.extras[\"gpt4all\"] = [\"gpt4all\"]\n\n\ndef vllm_requires():\n    \"\"\"\n    pip install \"dbgpt[vllm]\"\n    \"\"\"\n    setup_spec.extras[\"vllm\"] = [\"vllm\"]\n\n\ndef cache_requires():\n    \"\"\"\n    pip install \"dbgpt[cache]\"\n    \"\"\"\n    setup_spec.extras[\"cache\"] = [\"rocksdict\"]\n\n\ndef observability_requires():\n    \"\"\"\n    pip install \"dbgpt[observability]\"\n\n    Send DB-GPT traces to OpenTelemetry compatible backends.\n    \"\"\"\n    setup_spec.extras[\"observability\"] = [\n        \"opentelemetry-api\",\n        \"opentelemetry-sdk\",\n        \"opentelemetry-exporter-otlp\",\n    ]\n\n\ndef default_requires():\n    \"\"\"\n    pip install \"dbgpt[default]\"\n    \"\"\"\n    setup_spec.extras[\"default\"] = [\n        # \"tokenizers==0.13.3\",\n        \"tokenizers>=0.14\",\n        \"accelerate>=0.20.3\",\n        \"zhipuai\",\n        \"dashscope\",\n        \"chardet\",\n        \"sentencepiece\",\n        \"ollama\",\n        \"qianfan\",\n        \"libro>=0.1.25\",\n        \"poetry\",\n    ]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"framework\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"rag\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"graph_rag\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"datasource\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"torch\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"cache\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"proxy\"]\n    setup_spec.extras[\"default\"] += setup_spec.extras[\"code\"]\n    if INCLUDE_QUANTIZATION:\n        # Add quantization extra to default, default is True\n        setup_spec.extras[\"default\"] += setup_spec.extras[\"quantization\"]\n    if INCLUDE_OBSERVABILITY:\n        setup_spec.extras[\"default\"] += setup_spec.extras[\"observability\"]\n\n\ndef all_requires():\n    requires = set()\n    for _, pkgs in setup_spec.extras.items():\n        for pkg in pkgs:\n            requires.add(pkg)\n    setup_spec.extras[\"all\"] = list(requires)\n\n\ndef init_install_requires():\n    setup_spec.install_requires += setup_spec.extras[\"core\"]\n    print(f\"Install requires: \\n{','.join(setup_spec.install_requires)}\")\n\n\ncore_requires()\ncode_execution_requires()\ntorch_requires()\nllama_cpp_requires()\nquantization_requires()\n\nall_vector_store_requires()\nall_datasource_requires()\nknowledge_requires()\ngpt4all_requires()\nvllm_requires()\ncache_requires()\nobservability_requires()\n\nopenai_requires()\nproxy_requires()\n# must be last\ndefault_requires()\nall_requires()\ninit_install_requires()\n\n# Packages to exclude when IS_DEV_MODE is False\nexcluded_packages = [\"tests\", \"*.tests\", \"*.tests.*\", \"examples\"]\n\nif IS_DEV_MODE:\n    packages = find_packages(exclude=excluded_packages)\nelse:\n    packages = find_packages(\n        exclude=excluded_packages,\n        include=[\n            \"dbgpt\",\n            \"dbgpt._private\",\n            \"dbgpt._private.*\",\n            \"dbgpt.agent\",\n            \"dbgpt.agent.*\",\n            \"dbgpt.cli\",\n            \"dbgpt.cli.*\",\n            \"dbgpt.client\",\n            \"dbgpt.client.*\",\n            \"dbgpt.configs\",\n            \"dbgpt.configs.*\",\n            \"dbgpt.core\",\n            \"dbgpt.core.*\",\n            \"dbgpt.datasource\",\n            \"dbgpt.datasource.*\",\n            \"dbgpt.experimental\",\n            \"dbgpt.experimental.*\",\n            \"dbgpt.model\",\n            \"dbgpt.model.proxy\",\n            \"dbgpt.model.proxy.*\",\n            \"dbgpt.model.operators\",\n            \"dbgpt.model.operators.*\",\n            \"dbgpt.model.utils\",\n            \"dbgpt.model.utils.*\",\n            \"dbgpt.model.adapter\",\n            \"dbgpt.rag\",\n            \"dbgpt.rag.*\",\n            \"dbgpt.storage\",\n            \"dbgpt.storage.*\",\n            \"dbgpt.util\",\n            \"dbgpt.util.*\",\n            \"dbgpt.vis\",\n            \"dbgpt.vis.*\",\n        ],\n    )\n\n\nclass PrintExtrasCommand(setuptools.Command):\n    description = \"print extras_require\"\n    user_options = [\n        (\"output=\", \"o\", \"Path to output the extras_require JSON\"),\n    ]\n\n    def initialize_options(self):\n        self.output = None\n\n    def finalize_options(self):\n        if self.output is None:\n            raise ValueError(\"output is not set\")\n\n    def run(self):\n        with open(self.output, \"w\") as f:\n            json.dump(setup_spec.unique_extras, f, indent=2)\n\n\nsetuptools.setup(\n    name=\"dbgpt\",\n    packages=packages,\n    version=DB_GPT_VERSION,\n    author=\"csunny\",\n    author_email=\"cfqcsunny@gmail.com\",\n    description=\"DB-GPT is an experimental open-source project that uses localized GPT \"\n    \"large models to interact with your data and environment.\"\n    \" With this solution, you can be assured that there is no risk of data leakage, \"\n    \"and your data is 100% private and secure.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    install_requires=setup_spec.install_requires,\n    url=\"https://github.com/eosphoros-ai/DB-GPT\",\n    license=\"https://opensource.org/license/mit/\",\n    python_requires=\">=3.10\",\n    extras_require=setup_spec.unique_extras,\n    cmdclass={\n        \"print_extras\": PrintExtrasCommand,\n    },\n    entry_points={\n        \"console_scripts\": [\n            \"dbgpt=dbgpt.cli.cli_scripts:main\",\n        ],\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}