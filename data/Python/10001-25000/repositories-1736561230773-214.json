{
  "metadata": {
    "timestamp": 1736561230773,
    "page": 214,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/detr",
      "stars": 13862,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1845703125,
          "content": ".nfs*\n*.ipynb\n*.pyc\n.dumbo.json\n.DS_Store\n.*.swp\n*.pth\n**/__pycache__/**\n.ipynb_checkpoints/\ndatasets/data/\nexperiment-*\n*.tmp\n*.pkl\n**/.mypy_cache/*\n.mypy_cache/*\nnot_tracked_dir/\n.vscode\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.3203125,
          "content": "FROM pytorch/pytorch:1.5-cuda10.1-cudnn7-runtime\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update -qq && \\\n    apt-get install -y git vim libgtk2.0-dev && \\\n    rm -rf /var/cache/apk/*\n\nRUN pip --no-cache-dir install Cython\n\nCOPY requirements.txt /workspace\n\nRUN pip --no-cache-dir install -r /workspace/requirements.txt\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.087890625,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2020 - present, Facebook, Inc\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.61328125,
          "content": "**DEâ«¶TR**: End-to-End Object Detection with Transformers\n========\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)\n\nPyTorch training code and pretrained models for **DETR** (**DE**tection **TR**ansformer).\nWe replace the full complex hand-crafted object detection pipeline with a Transformer, and match Faster R-CNN with a ResNet-50, obtaining **42 AP** on COCO using half the computation power (FLOPs) and the same number of parameters. Inference in 50 lines of PyTorch.\n\n![DETR](.github/DETR.png)\n\n**What it is**. Unlike traditional computer vision techniques, DETR approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture. \nGiven a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. Due to this parallel nature, DETR is very fast and efficient.\n\n**About the code**. We believe that object detection should not be more difficult than classification,\nand should not require complex libraries for training and inference.\nDETR is very simple to implement and experiment with, and we provide a\n[standalone Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb)\nshowing how to do inference with DETR in only a few lines of PyTorch code.\nTraining code follows this idea - it is not a library,\nbut simply a [main.py](main.py) importing model and criterion\ndefinitions with standard training loops.\n\nAdditionnally, we provide a Detectron2 wrapper in the d2/ folder. See the readme there for more information.\n\nFor details see [End-to-End Object Detection with Transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\n\nSee our [blog post](https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/) to learn more about end to end object detection with transformers.\n# Model Zoo\nWe provide baseline DETR and DETR-DC5 models, and plan to include more in future.\nAP is computed on COCO 2017 val5k, and inference time is over the first 100 val5k COCO images,\nwith torchscript transformer.\n\n<table>\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>backbone</th>\n      <th>schedule</th>\n      <th>inf_time</th>\n      <th>box AP</th>\n      <th>url</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DETR</td>\n      <td>R50</td>\n      <td>500</td>\n      <td>0.036</td>\n      <td>42.0</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detr/logs/detr-r50_log.txt\">logs</a></td>\n      <td>159Mb</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DETR-DC5</td>\n      <td>R50</td>\n      <td>500</td>\n      <td>0.083</td>\n      <td>43.3</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-f0fb7ef5.pth\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detr/logs/detr-r50-dc5_log.txt\">logs</a></td>\n      <td>159Mb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DETR</td>\n      <td>R101</td>\n      <td>500</td>\n      <td>0.050</td>\n      <td>43.5</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detr/logs/detr-r101_log.txt\">logs</a></td>\n      <td>232Mb</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DETR-DC5</td>\n      <td>R101</td>\n      <td>500</td>\n      <td>0.097</td>\n      <td>44.9</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r101-dc5-a2e86def.pth\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detr/logs/detr-r101-dc5_log.txt\">logs</a></td>\n      <td>232Mb</td>\n    </tr>\n  </tbody>\n</table>\n\nCOCO val5k evaluation results can be found in this [gist](https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918).\n\nThe models are also available via torch hub,\nto load DETR R50 with pretrained weights simply do:\n```python\nmodel = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)\n```\n\n\nCOCO panoptic val5k models:\n<table>\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>backbone</th>\n      <th>box AP</th>\n      <th>segm AP</th>\n      <th>PQ</th>\n      <th>url</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DETR</td>\n      <td>R50</td>\n      <td>38.8</td>\n      <td>31.1</td>\n      <td>43.4</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r50-panoptic-00ce5173.pth\">download</a></td>\n      <td>165Mb</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DETR-DC5</td>\n      <td>R50</td>\n      <td>40.2</td>\n      <td>31.9</td>\n      <td>44.6</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-panoptic-da08f1b1.pth\">download</a></td>\n      <td>165Mb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DETR</td>\n      <td>R101</td>\n      <td>40.1</td>\n      <td>33</td>\n      <td>45.1</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r101-panoptic-40021d53.pth\">download</a></td>\n      <td>237Mb</td>\n    </tr>\n  </tbody>\n</table>\n\nCheckout our [panoptic colab](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb)\nto see how to use and visualize DETR's panoptic segmentation prediction.\n\n# Notebooks\n\nWe provide a few notebooks in colab to help you get a grasp on DETR:\n* [DETR's hands on Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_attention.ipynb): Shows how to load a model from hub, generate predictions, then visualize the attention of the model (similar to the figures of the paper)\n* [Standalone Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb): In this notebook, we demonstrate how to implement a simplified version of DETR from the grounds up in 50 lines of Python, then visualize the predictions. It is a good starting point if you want to gain better understanding the architecture and poke around before diving in the codebase.\n* [Panoptic Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb): Demonstrates how to use DETR for panoptic segmentation and plot the predictions.\n\n\n# Usage - Object detection\nThere are no extra compiled components in DETR and package dependencies are minimal,\nso the code is very simple to use. We provide instructions how to install dependencies via conda.\nFirst, clone the repository locally:\n```\ngit clone https://github.com/facebookresearch/detr.git\n```\nThen, install PyTorch 1.5+ and torchvision 0.6+:\n```\nconda install -c pytorch pytorch torchvision\n```\nInstall pycocotools (for evaluation on COCO) and scipy (for training):\n```\nconda install cython scipy\npip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n```\nThat's it, should be good to train and evaluate detection models.\n\n(optional) to work with panoptic install panopticapi:\n```\npip install git+https://github.com/cocodataset/panopticapi.git\n```\n\n## Data preparation\n\nDownload and extract COCO 2017 train and val images with annotations from\n[http://cocodataset.org](http://cocodataset.org/#download).\nWe expect the directory structure to be the following:\n```\npath/to/coco/\n  annotations/  # annotation json files\n  train2017/    # train images\n  val2017/      # val images\n```\n\n## Training\nTo train baseline DETR on a single node with 8 gpus for 300 epochs run:\n```\npython -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco \n```\nA single epoch takes 28 minutes, so 300 epoch training\ntakes around 6 days on a single machine with 8 V100 cards.\nTo ease reproduction of our results we provide\n[results and training logs](https://gist.github.com/szagoruyko/b4c3b2c3627294fc369b899987385a3f)\nfor 150 epoch schedule (3 days on a single machine), achieving 39.5/60.3 AP/AP50.\n\nWe train DETR with AdamW setting learning rate in the transformer to 1e-4 and 1e-5 in the backbone.\nHorizontal flips, scales and crops are used for augmentation.\nImages are rescaled to have min size 800 and max size 1333.\nThe transformer is trained with dropout of 0.1, and the whole model is trained with grad clip of 0.1.\n\n\n## Evaluation\nTo evaluate DETR R50 on COCO val5k with a single GPU run:\n```\npython main.py --batch_size 2 --no_aux_loss --eval --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth --coco_path /path/to/coco\n```\nWe provide results for all DETR detection models in this\n[gist](https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918).\nNote that numbers vary depending on batch size (number of images) per GPU.\nNon-DC5 models were trained with batch size 2, and DC5 with 1,\nso DC5 models show a significant drop in AP if evaluated with more\nthan 1 image per GPU.\n\n## Multinode training\nDistributed training is available via Slurm and [submitit](https://github.com/facebookincubator/submitit):\n```\npip install submitit\n```\nTrain baseline DETR-6-6 model on 4 nodes for 300 epochs:\n```\npython run_with_submitit.py --timeout 3000 --coco_path /path/to/coco\n```\n\n# Usage - Segmentation\n\nWe show that it is relatively straightforward to extend DETR to predict segmentation masks. We mainly demonstrate strong panoptic segmentation results.\n\n## Data preparation\n\nFor panoptic segmentation, you need the panoptic annotations additionally to the coco dataset (see above for the coco dataset). You need to download and extract the [annotations](http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip).\nWe expect the directory structure to be the following:\n```\npath/to/coco_panoptic/\n  annotations/  # annotation json files\n  panoptic_train2017/    # train panoptic annotations\n  panoptic_val2017/      # val panoptic annotations\n```\n\n## Training\n\nWe recommend training segmentation in two stages: first train DETR to detect all the boxes, and then train the segmentation head.\nFor panoptic segmentation, DETR must learn to detect boxes for both stuff and things classes. You can train it on a single node with 8 gpus for 300 epochs with:\n```\npython -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic --dataset_file coco_panoptic --output_dir /output/path/box_model\n```\nFor instance segmentation, you can simply train a normal box model (or used a pre-trained one we provide).\n\nOnce you have a box model checkpoint, you need to freeze it, and train the segmentation head in isolation.\nFor panoptic segmentation you can train on a single node with 8 gpus for 25 epochs:\n```\npython -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --masks --epochs 25 --lr_drop 15 --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic  --dataset_file coco_panoptic --frozen_weights /output/path/box_model/checkpoint.pth --output_dir /output/path/segm_model\n```\nFor instance segmentation only, simply remove the `dataset_file` and `coco_panoptic_path` arguments from the above command line.\n\n# License\nDETR is released under the Apache 2.0 license. Please see the [LICENSE](LICENSE) file for more information.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n"
        },
        {
          "name": "d2",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "engine.py",
          "type": "blob",
          "size": 6.470703125,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\"\"\"\nTrain and eval functions used in main.py\n\"\"\"\nimport math\nimport os\nimport sys\nfrom typing import Iterable\n\nimport torch\n\nimport util.misc as utils\nfrom datasets.coco_eval import CocoEvaluator\nfrom datasets.panoptic_eval import PanopticEvaluator\n\n\ndef train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, max_norm: float = 0):\n    model.train()\n    criterion.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = 10\n\n    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n        samples = samples.to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        outputs = model(samples)\n        loss_dict = criterion(outputs, targets)\n        weight_dict = criterion.weight_dict\n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n                                      for k, v in loss_dict_reduced.items()}\n        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n\n        loss_value = losses_reduced_scaled.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict_reduced)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        if max_norm > 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n        optimizer.step()\n\n        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n\n\n@torch.no_grad()\ndef evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n    model.eval()\n    criterion.eval()\n\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    header = 'Test:'\n\n    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n\n    panoptic_evaluator = None\n    if 'panoptic' in postprocessors.keys():\n        panoptic_evaluator = PanopticEvaluator(\n            data_loader.dataset.ann_file,\n            data_loader.dataset.ann_folder,\n            output_dir=os.path.join(output_dir, \"panoptic_eval\"),\n        )\n\n    for samples, targets in metric_logger.log_every(data_loader, 10, header):\n        samples = samples.to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        outputs = model(samples)\n        loss_dict = criterion(outputs, targets)\n        weight_dict = criterion.weight_dict\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n                                      for k, v in loss_dict_reduced.items()}\n        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n                             **loss_dict_reduced_scaled,\n                             **loss_dict_reduced_unscaled)\n        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n\n        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n        results = postprocessors['bbox'](outputs, orig_target_sizes)\n        if 'segm' in postprocessors.keys():\n            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n        if coco_evaluator is not None:\n            coco_evaluator.update(res)\n\n        if panoptic_evaluator is not None:\n            res_pano = postprocessors[\"panoptic\"](outputs, target_sizes, orig_target_sizes)\n            for i, target in enumerate(targets):\n                image_id = target[\"image_id\"].item()\n                file_name = f\"{image_id:012d}.png\"\n                res_pano[i][\"image_id\"] = image_id\n                res_pano[i][\"file_name\"] = file_name\n\n            panoptic_evaluator.update(res_pano)\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    if coco_evaluator is not None:\n        coco_evaluator.synchronize_between_processes()\n    if panoptic_evaluator is not None:\n        panoptic_evaluator.synchronize_between_processes()\n\n    # accumulate predictions from all images\n    if coco_evaluator is not None:\n        coco_evaluator.accumulate()\n        coco_evaluator.summarize()\n    panoptic_res = None\n    if panoptic_evaluator is not None:\n        panoptic_res = panoptic_evaluator.summarize()\n    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n    if coco_evaluator is not None:\n        if 'bbox' in postprocessors.keys():\n            stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n        if 'segm' in postprocessors.keys():\n            stats['coco_eval_masks'] = coco_evaluator.coco_eval['segm'].stats.tolist()\n    if panoptic_res is not None:\n        stats['PQ_all'] = panoptic_res[\"All\"]\n        stats['PQ_th'] = panoptic_res[\"Things\"]\n        stats['PQ_st'] = panoptic_res[\"Stuff\"]\n    return stats, coco_evaluator\n"
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 6.1181640625,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport torch\n\nfrom models.backbone import Backbone, Joiner\nfrom models.detr import DETR, PostProcess\nfrom models.position_encoding import PositionEmbeddingSine\nfrom models.segmentation import DETRsegm, PostProcessPanoptic\nfrom models.transformer import Transformer\n\ndependencies = [\"torch\", \"torchvision\"]\n\n\ndef _make_detr(backbone_name: str, dilation=False, num_classes=91, mask=False):\n    hidden_dim = 256\n    backbone = Backbone(backbone_name, train_backbone=True, return_interm_layers=mask, dilation=dilation)\n    pos_enc = PositionEmbeddingSine(hidden_dim // 2, normalize=True)\n    backbone_with_pos_enc = Joiner(backbone, pos_enc)\n    backbone_with_pos_enc.num_channels = backbone.num_channels\n    transformer = Transformer(d_model=hidden_dim, return_intermediate_dec=True)\n    detr = DETR(backbone_with_pos_enc, transformer, num_classes=num_classes, num_queries=100)\n    if mask:\n        return DETRsegm(detr)\n    return detr\n\n\ndef detr_resnet50(pretrained=False, num_classes=91, return_postprocessor=False):\n    \"\"\"\n    DETR R50 with 6 encoder and 6 decoder layers.\n\n    Achieves 42/62.4 AP/AP50 on COCO val5k.\n    \"\"\"\n    model = _make_detr(\"resnet50\", dilation=False, num_classes=num_classes)\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\", map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    if return_postprocessor:\n        return model, PostProcess()\n    return model\n\n\ndef detr_resnet50_dc5(pretrained=False, num_classes=91, return_postprocessor=False):\n    \"\"\"\n    DETR-DC5 R50 with 6 encoder and 6 decoder layers.\n\n    The last block of ResNet-50 has dilation to increase\n    output resolution.\n    Achieves 43.3/63.1 AP/AP50 on COCO val5k.\n    \"\"\"\n    model = _make_detr(\"resnet50\", dilation=True, num_classes=num_classes)\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-f0fb7ef5.pth\", map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    if return_postprocessor:\n        return model, PostProcess()\n    return model\n\n\ndef detr_resnet101(pretrained=False, num_classes=91, return_postprocessor=False):\n    \"\"\"\n    DETR-DC5 R101 with 6 encoder and 6 decoder layers.\n\n    Achieves 43.5/63.8 AP/AP50 on COCO val5k.\n    \"\"\"\n    model = _make_detr(\"resnet101\", dilation=False, num_classes=num_classes)\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth\", map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    if return_postprocessor:\n        return model, PostProcess()\n    return model\n\n\ndef detr_resnet101_dc5(pretrained=False, num_classes=91, return_postprocessor=False):\n    \"\"\"\n    DETR-DC5 R101 with 6 encoder and 6 decoder layers.\n\n    The last block of ResNet-101 has dilation to increase\n    output resolution.\n    Achieves 44.9/64.7 AP/AP50 on COCO val5k.\n    \"\"\"\n    model = _make_detr(\"resnet101\", dilation=True, num_classes=num_classes)\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r101-dc5-a2e86def.pth\", map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    if return_postprocessor:\n        return model, PostProcess()\n    return model\n\n\ndef detr_resnet50_panoptic(\n    pretrained=False, num_classes=250, threshold=0.85, return_postprocessor=False\n):\n    \"\"\"\n    DETR R50 with 6 encoder and 6 decoder layers.\n    Achieves 43.4 PQ on COCO val5k.\n\n   threshold is the minimum confidence required for keeping segments in the prediction\n    \"\"\"\n    model = _make_detr(\"resnet50\", dilation=False, num_classes=num_classes, mask=True)\n    is_thing_map = {i: i <= 90 for i in range(250)}\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r50-panoptic-00ce5173.pth\",\n            map_location=\"cpu\",\n            check_hash=True,\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    if return_postprocessor:\n        return model, PostProcessPanoptic(is_thing_map, threshold=threshold)\n    return model\n\n\ndef detr_resnet50_dc5_panoptic(\n    pretrained=False, num_classes=250, threshold=0.85, return_postprocessor=False\n):\n    \"\"\"\n    DETR-DC5 R50 with 6 encoder and 6 decoder layers.\n\n    The last block of ResNet-50 has dilation to increase\n    output resolution.\n    Achieves 44.6 on COCO val5k.\n\n   threshold is the minimum confidence required for keeping segments in the prediction\n    \"\"\"\n    model = _make_detr(\"resnet50\", dilation=True, num_classes=num_classes, mask=True)\n    is_thing_map = {i: i <= 90 for i in range(250)}\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-panoptic-da08f1b1.pth\",\n            map_location=\"cpu\",\n            check_hash=True,\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    if return_postprocessor:\n        return model, PostProcessPanoptic(is_thing_map, threshold=threshold)\n    return model\n\n\ndef detr_resnet101_panoptic(\n    pretrained=False, num_classes=250, threshold=0.85, return_postprocessor=False\n):\n    \"\"\"\n    DETR-DC5 R101 with 6 encoder and 6 decoder layers.\n\n    Achieves 45.1 PQ on COCO val5k.\n\n   threshold is the minimum confidence required for keeping segments in the prediction\n    \"\"\"\n    model = _make_detr(\"resnet101\", dilation=False, num_classes=num_classes, mask=True)\n    is_thing_map = {i: i <= 90 for i in range(250)}\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r101-panoptic-40021d53.pth\",\n            map_location=\"cpu\",\n            check_hash=True,\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    if return_postprocessor:\n        return model, PostProcessPanoptic(is_thing_map, threshold=threshold)\n    return model\n"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 11.26171875,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport argparse\nimport datetime\nimport json\nimport random\nimport time\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nimport datasets\nimport util.misc as utils\nfrom datasets import build_dataset, get_coco_api_from_dataset\nfrom engine import evaluate, train_one_epoch\nfrom models import build_model\n\n\ndef get_args_parser():\n    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n    parser.add_argument('--lr', default=1e-4, type=float)\n    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n    parser.add_argument('--batch_size', default=2, type=int)\n    parser.add_argument('--weight_decay', default=1e-4, type=float)\n    parser.add_argument('--epochs', default=300, type=int)\n    parser.add_argument('--lr_drop', default=200, type=int)\n    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n                        help='gradient clipping max norm')\n\n    # Model parameters\n    parser.add_argument('--frozen_weights', type=str, default=None,\n                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n    # * Backbone\n    parser.add_argument('--backbone', default='resnet50', type=str,\n                        help=\"Name of the convolutional backbone to use\")\n    parser.add_argument('--dilation', action='store_true',\n                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n                        help=\"Type of positional embedding to use on top of the image features\")\n\n    # * Transformer\n    parser.add_argument('--enc_layers', default=6, type=int,\n                        help=\"Number of encoding layers in the transformer\")\n    parser.add_argument('--dec_layers', default=6, type=int,\n                        help=\"Number of decoding layers in the transformer\")\n    parser.add_argument('--dim_feedforward', default=2048, type=int,\n                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n    parser.add_argument('--hidden_dim', default=256, type=int,\n                        help=\"Size of the embeddings (dimension of the transformer)\")\n    parser.add_argument('--dropout', default=0.1, type=float,\n                        help=\"Dropout applied in the transformer\")\n    parser.add_argument('--nheads', default=8, type=int,\n                        help=\"Number of attention heads inside the transformer's attentions\")\n    parser.add_argument('--num_queries', default=100, type=int,\n                        help=\"Number of query slots\")\n    parser.add_argument('--pre_norm', action='store_true')\n\n    # * Segmentation\n    parser.add_argument('--masks', action='store_true',\n                        help=\"Train segmentation head if the flag is provided\")\n\n    # Loss\n    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n    # * Matcher\n    parser.add_argument('--set_cost_class', default=1, type=float,\n                        help=\"Class coefficient in the matching cost\")\n    parser.add_argument('--set_cost_bbox', default=5, type=float,\n                        help=\"L1 box coefficient in the matching cost\")\n    parser.add_argument('--set_cost_giou', default=2, type=float,\n                        help=\"giou box coefficient in the matching cost\")\n    # * Loss coefficients\n    parser.add_argument('--mask_loss_coef', default=1, type=float)\n    parser.add_argument('--dice_loss_coef', default=1, type=float)\n    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n    parser.add_argument('--giou_loss_coef', default=2, type=float)\n    parser.add_argument('--eos_coef', default=0.1, type=float,\n                        help=\"Relative classification weight of the no-object class\")\n\n    # dataset parameters\n    parser.add_argument('--dataset_file', default='coco')\n    parser.add_argument('--coco_path', type=str)\n    parser.add_argument('--coco_panoptic_path', type=str)\n    parser.add_argument('--remove_difficult', action='store_true')\n\n    parser.add_argument('--output_dir', default='',\n                        help='path where to save, empty for no saving')\n    parser.add_argument('--device', default='cuda',\n                        help='device to use for training / testing')\n    parser.add_argument('--seed', default=42, type=int)\n    parser.add_argument('--resume', default='', help='resume from checkpoint')\n    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n                        help='start epoch')\n    parser.add_argument('--eval', action='store_true')\n    parser.add_argument('--num_workers', default=2, type=int)\n\n    # distributed training parameters\n    parser.add_argument('--world_size', default=1, type=int,\n                        help='number of distributed processes')\n    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n    return parser\n\n\ndef main(args):\n    utils.init_distributed_mode(args)\n    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n\n    if args.frozen_weights is not None:\n        assert args.masks, \"Frozen training is meant for segmentation only\"\n    print(args)\n\n    device = torch.device(args.device)\n\n    # fix the seed for reproducibility\n    seed = args.seed + utils.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n    model, criterion, postprocessors = build_model(args)\n    model.to(device)\n\n    model_without_ddp = model\n    if args.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n        model_without_ddp = model.module\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print('number of params:', n_parameters)\n\n    param_dicts = [\n        {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n        {\n            \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n            \"lr\": args.lr_backbone,\n        },\n    ]\n    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n                                  weight_decay=args.weight_decay)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n\n    dataset_train = build_dataset(image_set='train', args=args)\n    dataset_val = build_dataset(image_set='val', args=args)\n\n    if args.distributed:\n        sampler_train = DistributedSampler(dataset_train)\n        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n    else:\n        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n\n    batch_sampler_train = torch.utils.data.BatchSampler(\n        sampler_train, args.batch_size, drop_last=True)\n\n    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n                                   collate_fn=utils.collate_fn, num_workers=args.num_workers)\n    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n\n    if args.dataset_file == \"coco_panoptic\":\n        # We also evaluate AP during panoptic training, on original coco DS\n        coco_val = datasets.coco.build(\"val\", args)\n        base_ds = get_coco_api_from_dataset(coco_val)\n    else:\n        base_ds = get_coco_api_from_dataset(dataset_val)\n\n    if args.frozen_weights is not None:\n        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n\n    output_dir = Path(args.output_dir)\n    if args.resume:\n        if args.resume.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(\n                args.resume, map_location='cpu', check_hash=True)\n        else:\n            checkpoint = torch.load(args.resume, map_location='cpu')\n        model_without_ddp.load_state_dict(checkpoint['model'])\n        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n            args.start_epoch = checkpoint['epoch'] + 1\n\n    if args.eval:\n        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n                                              data_loader_val, base_ds, device, args.output_dir)\n        if args.output_dir:\n            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n        return\n\n    print(\"Start training\")\n    start_time = time.time()\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            sampler_train.set_epoch(epoch)\n        train_stats = train_one_epoch(\n            model, criterion, data_loader_train, optimizer, device, epoch,\n            args.clip_max_norm)\n        lr_scheduler.step()\n        if args.output_dir:\n            checkpoint_paths = [output_dir / 'checkpoint.pth']\n            # extra checkpoint before LR drop and every 100 epochs\n            if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n            for checkpoint_path in checkpoint_paths:\n                utils.save_on_master({\n                    'model': model_without_ddp.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'lr_scheduler': lr_scheduler.state_dict(),\n                    'epoch': epoch,\n                    'args': args,\n                }, checkpoint_path)\n\n        test_stats, coco_evaluator = evaluate(\n            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n        )\n\n        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n                     **{f'test_{k}': v for k, v in test_stats.items()},\n                     'epoch': epoch,\n                     'n_parameters': n_parameters}\n\n        if args.output_dir and utils.is_main_process():\n            with (output_dir / \"log.txt\").open(\"a\") as f:\n                f.write(json.dumps(log_stats) + \"\\n\")\n\n            # for evaluation logs\n            if coco_evaluator is not None:\n                (output_dir / 'eval').mkdir(exist_ok=True)\n                if \"bbox\" in coco_evaluator.coco_eval:\n                    filenames = ['latest.pth']\n                    if epoch % 50 == 0:\n                        filenames.append(f'{epoch:03}.pth')\n                    for name in filenames:\n                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n                                   output_dir / \"eval\" / name)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Training time {}'.format(total_time_str))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n    args = parser.parse_args()\n    if args.output_dir:\n        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n    main(args)\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.21875,
          "content": "cython\ngit+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&egg=pycocotools\nsubmitit\ntorch>=1.5.0\ntorchvision>=0.6.0\ngit+https://github.com/cocodataset/panopticapi.git#egg=panopticapi\nscipy\nonnx\nonnxruntime\n"
        },
        {
          "name": "run_with_submitit.py",
          "type": "blob",
          "size": 3.39453125,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\"\"\"\nA script to run multinode training with submitit.\n\"\"\"\nimport argparse\nimport os\nimport uuid\nfrom pathlib import Path\n\nimport main as detection\nimport submitit\n\n\ndef parse_args():\n    detection_parser = detection.get_args_parser()\n    parser = argparse.ArgumentParser(\"Submitit for detection\", parents=[detection_parser])\n    parser.add_argument(\"--ngpus\", default=8, type=int, help=\"Number of gpus to request on each node\")\n    parser.add_argument(\"--nodes\", default=4, type=int, help=\"Number of nodes to request\")\n    parser.add_argument(\"--timeout\", default=60, type=int, help=\"Duration of the job\")\n    parser.add_argument(\"--job_dir\", default=\"\", type=str, help=\"Job dir. Leave empty for automatic.\")\n    return parser.parse_args()\n\n\ndef get_shared_folder() -> Path:\n    user = os.getenv(\"USER\")\n    if Path(\"/checkpoint/\").is_dir():\n        p = Path(f\"/checkpoint/{user}/experiments\")\n        p.mkdir(exist_ok=True)\n        return p\n    raise RuntimeError(\"No shared folder available\")\n\n\ndef get_init_file():\n    # Init file must not exist, but it's parent dir must exist.\n    os.makedirs(str(get_shared_folder()), exist_ok=True)\n    init_file = get_shared_folder() / f\"{uuid.uuid4().hex}_init\"\n    if init_file.exists():\n        os.remove(str(init_file))\n    return init_file\n\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n\n    def __call__(self):\n        import main as detection\n\n        self._setup_gpu_args()\n        detection.main(self.args)\n\n    def checkpoint(self):\n        import os\n        import submitit\n        from pathlib import Path\n\n        self.args.dist_url = get_init_file().as_uri()\n        checkpoint_file = os.path.join(self.args.output_dir, \"checkpoint.pth\")\n        if os.path.exists(checkpoint_file):\n            self.args.resume = checkpoint_file\n        print(\"Requeuing \", self.args)\n        empty_trainer = type(self)(self.args)\n        return submitit.helpers.DelayedSubmission(empty_trainer)\n\n    def _setup_gpu_args(self):\n        import submitit\n        from pathlib import Path\n\n        job_env = submitit.JobEnvironment()\n        self.args.output_dir = Path(str(self.args.output_dir).replace(\"%j\", str(job_env.job_id)))\n        self.args.gpu = job_env.local_rank\n        self.args.rank = job_env.global_rank\n        self.args.world_size = job_env.num_tasks\n        print(f\"Process group: {job_env.num_tasks} tasks, rank: {job_env.global_rank}\")\n\n\ndef main():\n    args = parse_args()\n    if args.job_dir == \"\":\n        args.job_dir = get_shared_folder() / \"%j\"\n\n    # Note that the folder will depend on the job_id, to easily track experiments\n    executor = submitit.AutoExecutor(folder=args.job_dir, slurm_max_num_timeout=30)\n\n    # cluster setup is defined by environment variables\n    num_gpus_per_node = args.ngpus\n    nodes = args.nodes\n    timeout_min = args.timeout\n\n    executor.update_parameters(\n        mem_gb=40 * num_gpus_per_node,\n        gpus_per_node=num_gpus_per_node,\n        tasks_per_node=num_gpus_per_node,  # one task per GPU\n        cpus_per_task=10,\n        nodes=nodes,\n        timeout_min=timeout_min,  # max is 60 * 72\n    )\n\n    executor.update_parameters(name=\"detr\")\n\n    args.dist_url = get_init_file().as_uri()\n    args.output_dir = args.job_dir\n\n    trainer = Trainer(args)\n    job = executor.submit(trainer)\n\n    print(\"Submitted job_id:\", job.job_id)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "test_all.py",
          "type": "blob",
          "size": 8.599609375,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport io\nimport unittest\n\nimport torch\nfrom torch import nn, Tensor\nfrom typing import List\n\nfrom models.matcher import HungarianMatcher\nfrom models.position_encoding import PositionEmbeddingSine, PositionEmbeddingLearned\nfrom models.backbone import Backbone, Joiner, BackboneBase\nfrom util import box_ops\nfrom util.misc import nested_tensor_from_tensor_list\nfrom hubconf import detr_resnet50, detr_resnet50_panoptic\n\n# onnxruntime requires python 3.5 or above\ntry:\n    import onnxruntime\nexcept ImportError:\n    onnxruntime = None\n\n\nclass Tester(unittest.TestCase):\n\n    def test_box_cxcywh_to_xyxy(self):\n        t = torch.rand(10, 4)\n        r = box_ops.box_xyxy_to_cxcywh(box_ops.box_cxcywh_to_xyxy(t))\n        self.assertLess((t - r).abs().max(), 1e-5)\n\n    @staticmethod\n    def indices_torch2python(indices):\n        return [(i.tolist(), j.tolist()) for i, j in indices]\n\n    def test_hungarian(self):\n        n_queries, n_targets, n_classes = 100, 15, 91\n        logits = torch.rand(1, n_queries, n_classes + 1)\n        boxes = torch.rand(1, n_queries, 4)\n        tgt_labels = torch.randint(high=n_classes, size=(n_targets,))\n        tgt_boxes = torch.rand(n_targets, 4)\n        matcher = HungarianMatcher()\n        targets = [{'labels': tgt_labels, 'boxes': tgt_boxes}]\n        indices_single = matcher({'pred_logits': logits, 'pred_boxes': boxes}, targets)\n        indices_batched = matcher({'pred_logits': logits.repeat(2, 1, 1),\n                                   'pred_boxes': boxes.repeat(2, 1, 1)}, targets * 2)\n        self.assertEqual(len(indices_single[0][0]), n_targets)\n        self.assertEqual(len(indices_single[0][1]), n_targets)\n        self.assertEqual(self.indices_torch2python(indices_single),\n                         self.indices_torch2python([indices_batched[0]]))\n        self.assertEqual(self.indices_torch2python(indices_single),\n                         self.indices_torch2python([indices_batched[1]]))\n\n        # test with empty targets\n        tgt_labels_empty = torch.randint(high=n_classes, size=(0,))\n        tgt_boxes_empty = torch.rand(0, 4)\n        targets_empty = [{'labels': tgt_labels_empty, 'boxes': tgt_boxes_empty}]\n        indices = matcher({'pred_logits': logits.repeat(2, 1, 1),\n                           'pred_boxes': boxes.repeat(2, 1, 1)}, targets + targets_empty)\n        self.assertEqual(len(indices[1][0]), 0)\n        indices = matcher({'pred_logits': logits.repeat(2, 1, 1),\n                           'pred_boxes': boxes.repeat(2, 1, 1)}, targets_empty * 2)\n        self.assertEqual(len(indices[0][0]), 0)\n\n    def test_position_encoding_script(self):\n        m1, m2 = PositionEmbeddingSine(), PositionEmbeddingLearned()\n        mm1, mm2 = torch.jit.script(m1), torch.jit.script(m2)  # noqa\n\n    def test_backbone_script(self):\n        backbone = Backbone('resnet50', True, False, False)\n        torch.jit.script(backbone)  # noqa\n\n    def test_model_script_detection(self):\n        model = detr_resnet50(pretrained=False).eval()\n        scripted_model = torch.jit.script(model)\n        x = nested_tensor_from_tensor_list([torch.rand(3, 200, 200), torch.rand(3, 200, 250)])\n        out = model(x)\n        out_script = scripted_model(x)\n        self.assertTrue(out[\"pred_logits\"].equal(out_script[\"pred_logits\"]))\n        self.assertTrue(out[\"pred_boxes\"].equal(out_script[\"pred_boxes\"]))\n\n    def test_model_script_panoptic(self):\n        model = detr_resnet50_panoptic(pretrained=False).eval()\n        scripted_model = torch.jit.script(model)\n        x = nested_tensor_from_tensor_list([torch.rand(3, 200, 200), torch.rand(3, 200, 250)])\n        out = model(x)\n        out_script = scripted_model(x)\n        self.assertTrue(out[\"pred_logits\"].equal(out_script[\"pred_logits\"]))\n        self.assertTrue(out[\"pred_boxes\"].equal(out_script[\"pred_boxes\"]))\n        self.assertTrue(out[\"pred_masks\"].equal(out_script[\"pred_masks\"]))\n\n    def test_model_detection_different_inputs(self):\n        model = detr_resnet50(pretrained=False).eval()\n        # support NestedTensor\n        x = nested_tensor_from_tensor_list([torch.rand(3, 200, 200), torch.rand(3, 200, 250)])\n        out = model(x)\n        self.assertIn('pred_logits', out)\n        # and 4d Tensor\n        x = torch.rand(1, 3, 200, 200)\n        out = model(x)\n        self.assertIn('pred_logits', out)\n        # and List[Tensor[C, H, W]]\n        x = torch.rand(3, 200, 200)\n        out = model([x])\n        self.assertIn('pred_logits', out)\n\n    def test_warpped_model_script_detection(self):\n        class WrappedDETR(nn.Module):\n            def __init__(self, model):\n                super().__init__()\n                self.model = model\n\n            def forward(self, inputs: List[Tensor]):\n                sample = nested_tensor_from_tensor_list(inputs)\n                return self.model(sample)\n\n        model = detr_resnet50(pretrained=False)\n        wrapped_model = WrappedDETR(model)\n        wrapped_model.eval()\n        scripted_model = torch.jit.script(wrapped_model)\n        x = [torch.rand(3, 200, 200), torch.rand(3, 200, 250)]\n        out = wrapped_model(x)\n        out_script = scripted_model(x)\n        self.assertTrue(out[\"pred_logits\"].equal(out_script[\"pred_logits\"]))\n        self.assertTrue(out[\"pred_boxes\"].equal(out_script[\"pred_boxes\"]))\n\n\n@unittest.skipIf(onnxruntime is None, 'ONNX Runtime unavailable')\nclass ONNXExporterTester(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        torch.manual_seed(123)\n\n    def run_model(self, model, inputs_list, tolerate_small_mismatch=False, do_constant_folding=True, dynamic_axes=None,\n                  output_names=None, input_names=None):\n        model.eval()\n\n        onnx_io = io.BytesIO()\n        # export to onnx with the first input\n        torch.onnx.export(model, inputs_list[0], onnx_io,\n                          do_constant_folding=do_constant_folding, opset_version=12,\n                          dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names)\n        # validate the exported model with onnx runtime\n        for test_inputs in inputs_list:\n            with torch.no_grad():\n                if isinstance(test_inputs, torch.Tensor) or isinstance(test_inputs, list):\n                    test_inputs = (nested_tensor_from_tensor_list(test_inputs),)\n                test_ouputs = model(*test_inputs)\n                if isinstance(test_ouputs, torch.Tensor):\n                    test_ouputs = (test_ouputs,)\n            self.ort_validate(onnx_io, test_inputs, test_ouputs, tolerate_small_mismatch)\n\n    def ort_validate(self, onnx_io, inputs, outputs, tolerate_small_mismatch=False):\n\n        inputs, _ = torch.jit._flatten(inputs)\n        outputs, _ = torch.jit._flatten(outputs)\n\n        def to_numpy(tensor):\n            if tensor.requires_grad:\n                return tensor.detach().cpu().numpy()\n            else:\n                return tensor.cpu().numpy()\n\n        inputs = list(map(to_numpy, inputs))\n        outputs = list(map(to_numpy, outputs))\n\n        ort_session = onnxruntime.InferenceSession(onnx_io.getvalue())\n        # compute onnxruntime output prediction\n        ort_inputs = dict((ort_session.get_inputs()[i].name, inpt) for i, inpt in enumerate(inputs))\n        ort_outs = ort_session.run(None, ort_inputs)\n        for i, element in enumerate(outputs):\n            try:\n                torch.testing.assert_allclose(element, ort_outs[i], rtol=1e-03, atol=1e-05)\n            except AssertionError as error:\n                if tolerate_small_mismatch:\n                    self.assertIn(\"(0.00%)\", str(error), str(error))\n                else:\n                    raise\n\n    def test_model_onnx_detection(self):\n        model = detr_resnet50(pretrained=False).eval()\n        dummy_image = torch.ones(1, 3, 800, 800) * 0.3\n        model(dummy_image)\n\n        # Test exported model on images of different size, or dummy input\n        self.run_model(\n            model,\n            [(torch.rand(1, 3, 750, 800),)],\n            input_names=[\"inputs\"],\n            output_names=[\"pred_logits\", \"pred_boxes\"],\n            tolerate_small_mismatch=True,\n        )\n\n    @unittest.skip(\"CI doesn't have enough memory\")\n    def test_model_onnx_detection_panoptic(self):\n        model = detr_resnet50_panoptic(pretrained=False).eval()\n        dummy_image = torch.ones(1, 3, 800, 800) * 0.3\n        model(dummy_image)\n\n        # Test exported model on images of different size, or dummy input\n        self.run_model(\n            model,\n            [(torch.rand(1, 3, 750, 800),)],\n            input_names=[\"inputs\"],\n            output_names=[\"pred_logits\", \"pred_boxes\", \"pred_masks\"],\n            tolerate_small_mismatch=True,\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.0634765625,
          "content": "[flake8]\nmax-line-length = 120\nignore = F401,E402,F403,W503,W504\n"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}