{
  "metadata": {
    "timestamp": 1736561178362,
    "page": 139,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "sczhou/CodeFormer",
      "stars": 16226,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.3759765625,
          "content": ".vscode\n\n# ignored files\nversion.py\n\n# ignored files with suffix\n*.html\n# *.png\n# *.jpeg\n# *.jpg\n*.pt\n*.gif\n*.pth\n*.dat\n*.zip\n\n# template\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# project\nresults/\nexperiments/\ntb_logger/\nrun.sh\n*debug*\n*_old*\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.6767578125,
          "content": "S-Lab License 1.0\n\nCopyright 2022 S-Lab\n\nRedistribution and use for non-commercial purpose in source and \nbinary forms, with or without modification, are permitted provided \nthat the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright \n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright \n   notice, this list of conditions and the following disclaimer in \n   the documentation and/or other materials provided with the \n   distribution.\n\n3. Neither the name of the copyright holder nor the names of its \n   contributors may be used to endorse or promote products derived \n   from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT \nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR \nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT \nHOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, \nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT \nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, \nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY \nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT \n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE \nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nIn the event that redistribution and/or use for commercial purpose in \nsource or binary forms, with or without modification is required, \nplease contact the contributor(s) of the work."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.767578125,
          "content": "<p align=\"center\">\n  <img src=\"assets/CodeFormer_logo.png\" height=110>\n</p>\n\n## Towards Robust Blind Face Restoration with Codebook Lookup Transformer (NeurIPS 2022)\n\n[Paper](https://arxiv.org/abs/2206.11253) | [Project Page](https://shangchenzhou.com/projects/CodeFormer/) | [Video](https://youtu.be/d3VDpkXlueI)\n\n\n<a href=\"https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"google colab logo\"></a> [![Hugging Face](https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/sczhou/CodeFormer) [![Replicate](https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue)](https://replicate.com/sczhou/codeformer) [![OpenXLab](https://img.shields.io/badge/Demo-%F0%9F%90%BC%20OpenXLab-blue)](https://openxlab.org.cn/apps/detail/ShangchenZhou/CodeFormer) ![Visitors](https://api.infinitescript.com/badgen/count?name=sczhou/CodeFormer&ltext=Visitors)\n\n\n[Shangchen Zhou](https://shangchenzhou.com/), [Kelvin C.K. Chan](https://ckkelvinchan.github.io/), [Chongyi Li](https://li-chongyi.github.io/), [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) \n\nS-Lab, Nanyang Technological University\n\n<img src=\"assets/network.jpg\" width=\"800px\"/>\n\n\n:star: If CodeFormer is helpful to your images or projects, please help star this repo. Thanks! :hugs: \n\n\n### Update\n- **2023.07.20**: Integrated to :panda_face: [OpenXLab](https://openxlab.org.cn/apps). Try out online demo! [![OpenXLab](https://img.shields.io/badge/Demo-%F0%9F%90%BC%20OpenXLab-blue)](https://openxlab.org.cn/apps/detail/ShangchenZhou/CodeFormer)\n- **2023.04.19**: :whale: Training codes and config files are public available now.\n- **2023.04.09**: Add features of inpainting and colorization for cropped and aligned face images.\n- **2023.02.10**: Include `dlib` as a new face detector option, it produces more accurate face identity.\n- **2022.10.05**: Support video input `--input_path [YOUR_VIDEO.mp4]`. Try it to enhance your videos! :clapper: \n- **2022.09.14**: Integrated to :hugs: [Hugging Face](https://huggingface.co/spaces). Try out online demo! [![Hugging Face](https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/sczhou/CodeFormer)\n- **2022.09.09**: Integrated to :rocket: [Replicate](https://replicate.com/explore). Try out online demo! [![Replicate](https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue)](https://replicate.com/sczhou/codeformer)\n- [**More**](docs/history_changelog.md)\n\n### TODO\n- [x] Add training code and config files\n- [x] Add checkpoint and script for face inpainting\n- [x] Add checkpoint and script for face colorization\n- [x] ~~Add background image enhancement~~\n\n#### :panda_face: Try Enhancing Old Photos / Fixing AI-arts\n[<img src=\"assets/imgsli_1.jpg\" height=\"226px\"/>](https://imgsli.com/MTI3NTE2) [<img src=\"assets/imgsli_2.jpg\" height=\"226px\"/>](https://imgsli.com/MTI3NTE1) [<img src=\"assets/imgsli_3.jpg\" height=\"226px\"/>](https://imgsli.com/MTI3NTIw) \n\n#### Face Restoration\n\n<img src=\"assets/restoration_result1.png\" width=\"400px\"/> <img src=\"assets/restoration_result2.png\" width=\"400px\"/>\n<img src=\"assets/restoration_result3.png\" width=\"400px\"/> <img src=\"assets/restoration_result4.png\" width=\"400px\"/>\n\n#### Face Color Enhancement and Restoration\n\n<img src=\"assets/color_enhancement_result1.png\" width=\"400px\"/> <img src=\"assets/color_enhancement_result2.png\" width=\"400px\"/>\n\n#### Face Inpainting\n\n<img src=\"assets/inpainting_result1.png\" width=\"400px\"/> <img src=\"assets/inpainting_result2.png\" width=\"400px\"/>\n\n\n\n### Dependencies and Installation\n\n- Pytorch >= 1.7.1\n- CUDA >= 10.1\n- Other required packages in `requirements.txt`\n```\n# git clone this repository\ngit clone https://github.com/sczhou/CodeFormer\ncd CodeFormer\n\n# create new anaconda env\nconda create -n codeformer python=3.8 -y\nconda activate codeformer\n\n# install python dependencies\npip3 install -r requirements.txt\npython basicsr/setup.py develop\nconda install -c conda-forge dlib (only for face detection or cropping with dlib)\n```\n<!-- conda install -c conda-forge dlib -->\n\n### Quick Inference\n\n#### Download Pre-trained Models:\nDownload the facelib and dlib pretrained models from [[Releases](https://github.com/sczhou/CodeFormer/releases/tag/v0.1.0) | [Google Drive](https://drive.google.com/drive/folders/1b_3qwrzY_kTQh0-SnBoGBgOrJ_PLZSKm?usp=sharing) | [OneDrive](https://entuedu-my.sharepoint.com/:f:/g/personal/s200094_e_ntu_edu_sg/EvDxR7FcAbZMp_MA9ouq7aQB8XTppMb3-T0uGZ_2anI2mg?e=DXsJFo)] to the `weights/facelib` folder. You can manually download the pretrained models OR download by running the following command:\n```\npython scripts/download_pretrained_models.py facelib\npython scripts/download_pretrained_models.py dlib (only for dlib face detector)\n```\n\nDownload the CodeFormer pretrained models from [[Releases](https://github.com/sczhou/CodeFormer/releases/tag/v0.1.0) | [Google Drive](https://drive.google.com/drive/folders/1CNNByjHDFt0b95q54yMVp6Ifo5iuU6QS?usp=sharing) | [OneDrive](https://entuedu-my.sharepoint.com/:f:/g/personal/s200094_e_ntu_edu_sg/EoKFj4wo8cdIn2-TY2IV6CYBhZ0pIG4kUOeHdPR_A5nlbg?e=AO8UN9)] to the `weights/CodeFormer` folder. You can manually download the pretrained models OR download by running the following command:\n```\npython scripts/download_pretrained_models.py CodeFormer\n```\n\n#### Prepare Testing Data:\nYou can put the testing images in the `inputs/TestWhole` folder. If you would like to test on cropped and aligned faces, you can put them in the `inputs/cropped_faces` folder. You can get the cropped and aligned faces by running the following command:\n```\n# you may need to install dlib via: conda install -c conda-forge dlib\npython scripts/crop_align_face.py -i [input folder] -o [output folder]\n```\n\n\n#### Testing:\n[Note] If you want to compare CodeFormer in your paper, please run the following command indicating `--has_aligned` (for cropped and aligned face), as the command for the whole image will involve a process of face-background fusion that may damage hair texture on the boundary, which leads to unfair comparison.\n\nFidelity weight *w* lays in [0, 1]. Generally, smaller *w* tends to produce a higher-quality result, while larger *w* yields a higher-fidelity result. The results will be saved in the `results` folder.\n\n\n🧑🏻 Face Restoration (cropped and aligned face)\n```\n# For cropped and aligned faces (512x512)\npython inference_codeformer.py -w 0.5 --has_aligned --input_path [image folder]|[image path]\n```\n\n:framed_picture: Whole Image Enhancement\n```\n# For whole image\n# Add '--bg_upsampler realesrgan' to enhance the background regions with Real-ESRGAN\n# Add '--face_upsample' to further upsample restorated face with Real-ESRGAN\npython inference_codeformer.py -w 0.7 --input_path [image folder]|[image path]\n```\n\n:clapper: Video Enhancement\n```\n# For Windows/Mac users, please install ffmpeg first\nconda install -c conda-forge ffmpeg\n```\n```\n# For video clips\n# Video path should end with '.mp4'|'.mov'|'.avi'\npython inference_codeformer.py --bg_upsampler realesrgan --face_upsample -w 1.0 --input_path [video path]\n```\n\n🌈 Face Colorization (cropped and aligned face)\n```\n# For cropped and aligned faces (512x512)\n# Colorize black and white or faded photo\npython inference_colorization.py --input_path [image folder]|[image path]\n```\n\n🎨 Face Inpainting (cropped and aligned face)\n```\n# For cropped and aligned faces (512x512)\n# Inputs could be masked by white brush using an image editing app (e.g., Photoshop) \n# (check out the examples in inputs/masked_faces)\npython inference_inpainting.py --input_path [image folder]|[image path]\n```\n### Training:\nThe training commands can be found in the documents: [English](docs/train.md) **|** [简体中文](docs/train_CN.md).\n\n### Citation\nIf our work is useful for your research, please consider citing:\n\n    @inproceedings{zhou2022codeformer,\n        author = {Zhou, Shangchen and Chan, Kelvin C.K. and Li, Chongyi and Loy, Chen Change},\n        title = {Towards Robust Blind Face Restoration with Codebook Lookup TransFormer},\n        booktitle = {NeurIPS},\n        year = {2022}\n    }\n\n### License\n\nThis project is licensed under <a rel=\"license\" href=\"https://github.com/sczhou/CodeFormer/blob/master/LICENSE\">NTU S-Lab License 1.0</a>. Redistribution and use should follow this license.\n\n### Acknowledgement\n\nThis project is based on [BasicSR](https://github.com/XPixelGroup/BasicSR). Some codes are brought from [Unleashing Transformers](https://github.com/samb-t/unleashing-transformers), [YOLOv5-face](https://github.com/deepcam-cn/yolov5-face), and [FaceXLib](https://github.com/xinntao/facexlib). We also adopt [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) to support background image enhancement. Thanks for their awesome works.\n\n### Contact\nIf you have any questions, please feel free to reach me out at `shangchenzhou@gmail.com`. \n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "basicsr",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "facelib",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference_codeformer.py",
          "type": "blob",
          "size": 12.3935546875,
          "content": "import os\nimport cv2\nimport argparse\nimport glob\nimport torch\nfrom torchvision.transforms.functional import normalize\nfrom basicsr.utils import imwrite, img2tensor, tensor2img\nfrom basicsr.utils.download_util import load_file_from_url\nfrom basicsr.utils.misc import gpu_is_available, get_device\nfrom facelib.utils.face_restoration_helper import FaceRestoreHelper\nfrom facelib.utils.misc import is_gray\n\nfrom basicsr.utils.registry import ARCH_REGISTRY\n\npretrain_model_url = {\n    'restoration': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth',\n}\n\ndef set_realesrgan():\n    from basicsr.archs.rrdbnet_arch import RRDBNet\n    from basicsr.utils.realesrgan_utils import RealESRGANer\n\n    use_half = False\n    if torch.cuda.is_available(): # set False in CPU/MPS mode\n        no_half_gpu_list = ['1650', '1660'] # set False for GPUs that don't support f16\n        if not True in [gpu in torch.cuda.get_device_name(0) for gpu in no_half_gpu_list]:\n            use_half = True\n\n    model = RRDBNet(\n        num_in_ch=3,\n        num_out_ch=3,\n        num_feat=64,\n        num_block=23,\n        num_grow_ch=32,\n        scale=2,\n    )\n    upsampler = RealESRGANer(\n        scale=2,\n        model_path=\"https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/RealESRGAN_x2plus.pth\",\n        model=model,\n        tile=args.bg_tile,\n        tile_pad=40,\n        pre_pad=0,\n        half=use_half\n    )\n\n    if not gpu_is_available():  # CPU\n        import warnings\n        warnings.warn('Running on CPU now! Make sure your PyTorch version matches your CUDA.'\n                        'The unoptimized RealESRGAN is slow on CPU. '\n                        'If you want to disable it, please remove `--bg_upsampler` and `--face_upsample` in command.',\n                        category=RuntimeWarning)\n    return upsampler\n\nif __name__ == '__main__':\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    device = get_device()\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-i', '--input_path', type=str, default='./inputs/whole_imgs', \n            help='Input image, video or folder. Default: inputs/whole_imgs')\n    parser.add_argument('-o', '--output_path', type=str, default=None, \n            help='Output folder. Default: results/<input_name>_<w>')\n    parser.add_argument('-w', '--fidelity_weight', type=float, default=0.5, \n            help='Balance the quality and fidelity. Default: 0.5')\n    parser.add_argument('-s', '--upscale', type=int, default=2, \n            help='The final upsampling scale of the image. Default: 2')\n    parser.add_argument('--has_aligned', action='store_true', help='Input are cropped and aligned faces. Default: False')\n    parser.add_argument('--only_center_face', action='store_true', help='Only restore the center face. Default: False')\n    parser.add_argument('--draw_box', action='store_true', help='Draw the bounding box for the detected faces. Default: False')\n    # large det_model: 'YOLOv5l', 'retinaface_resnet50'\n    # small det_model: 'YOLOv5n', 'retinaface_mobile0.25'\n    parser.add_argument('--detection_model', type=str, default='retinaface_resnet50', \n            help='Face detector. Optional: retinaface_resnet50, retinaface_mobile0.25, YOLOv5l, YOLOv5n, dlib. \\\n                Default: retinaface_resnet50')\n    parser.add_argument('--bg_upsampler', type=str, default='None', help='Background upsampler. Optional: realesrgan')\n    parser.add_argument('--face_upsample', action='store_true', help='Face upsampler after enhancement. Default: False')\n    parser.add_argument('--bg_tile', type=int, default=400, help='Tile size for background sampler. Default: 400')\n    parser.add_argument('--suffix', type=str, default=None, help='Suffix of the restored faces. Default: None')\n    parser.add_argument('--save_video_fps', type=float, default=None, help='Frame rate for saving video. Default: None')\n\n    args = parser.parse_args()\n\n    # ------------------------ input & output ------------------------\n    w = args.fidelity_weight\n    input_video = False\n    if args.input_path.endswith(('jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG')): # input single img path\n        input_img_list = [args.input_path]\n        result_root = f'results/test_img_{w}'\n    elif args.input_path.endswith(('mp4', 'mov', 'avi', 'MP4', 'MOV', 'AVI')): # input video path\n        from basicsr.utils.video_util import VideoReader, VideoWriter\n        input_img_list = []\n        vidreader = VideoReader(args.input_path)\n        image = vidreader.get_frame()\n        while image is not None:\n            input_img_list.append(image)\n            image = vidreader.get_frame()\n        audio = vidreader.get_audio()\n        fps = vidreader.get_fps() if args.save_video_fps is None else args.save_video_fps   \n        video_name = os.path.basename(args.input_path)[:-4]\n        result_root = f'results/{video_name}_{w}'\n        input_video = True\n        vidreader.close()\n    else: # input img folder\n        if args.input_path.endswith('/'):  # solve when path ends with /\n            args.input_path = args.input_path[:-1]\n        # scan all the jpg and png images\n        input_img_list = sorted(glob.glob(os.path.join(args.input_path, '*.[jpJP][pnPN]*[gG]')))\n        result_root = f'results/{os.path.basename(args.input_path)}_{w}'\n\n    if not args.output_path is None: # set output path\n        result_root = args.output_path\n\n    test_img_num = len(input_img_list)\n    if test_img_num == 0:\n        raise FileNotFoundError('No input image/video is found...\\n' \n            '\\tNote that --input_path for video should end with .mp4|.mov|.avi')\n\n    # ------------------ set up background upsampler ------------------\n    if args.bg_upsampler == 'realesrgan':\n        bg_upsampler = set_realesrgan()\n    else:\n        bg_upsampler = None\n\n    # ------------------ set up face upsampler ------------------\n    if args.face_upsample:\n        if bg_upsampler is not None:\n            face_upsampler = bg_upsampler\n        else:\n            face_upsampler = set_realesrgan()\n    else:\n        face_upsampler = None\n\n    # ------------------ set up CodeFormer restorer -------------------\n    net = ARCH_REGISTRY.get('CodeFormer')(dim_embd=512, codebook_size=1024, n_head=8, n_layers=9, \n                                            connect_list=['32', '64', '128', '256']).to(device)\n    \n    # ckpt_path = 'weights/CodeFormer/codeformer.pth'\n    ckpt_path = load_file_from_url(url=pretrain_model_url['restoration'], \n                                    model_dir='weights/CodeFormer', progress=True, file_name=None)\n    checkpoint = torch.load(ckpt_path)['params_ema']\n    net.load_state_dict(checkpoint)\n    net.eval()\n\n    # ------------------ set up FaceRestoreHelper -------------------\n    # large det_model: 'YOLOv5l', 'retinaface_resnet50'\n    # small det_model: 'YOLOv5n', 'retinaface_mobile0.25'\n    if not args.has_aligned: \n        print(f'Face detection model: {args.detection_model}')\n    if bg_upsampler is not None: \n        print(f'Background upsampling: True, Face upsampling: {args.face_upsample}')\n    else:\n        print(f'Background upsampling: False, Face upsampling: {args.face_upsample}')\n\n    face_helper = FaceRestoreHelper(\n        args.upscale,\n        face_size=512,\n        crop_ratio=(1, 1),\n        det_model = args.detection_model,\n        save_ext='png',\n        use_parse=True,\n        device=device)\n\n    # -------------------- start to processing ---------------------\n    for i, img_path in enumerate(input_img_list):\n        # clean all the intermediate results to process the next image\n        face_helper.clean_all()\n        \n        if isinstance(img_path, str):\n            img_name = os.path.basename(img_path)\n            basename, ext = os.path.splitext(img_name)\n            print(f'[{i+1}/{test_img_num}] Processing: {img_name}')\n            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        else: # for video processing\n            basename = str(i).zfill(6)\n            img_name = f'{video_name}_{basename}' if input_video else basename\n            print(f'[{i+1}/{test_img_num}] Processing: {img_name}')\n            img = img_path\n\n        if args.has_aligned: \n            # the input faces are already cropped and aligned\n            img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_LINEAR)\n            face_helper.is_gray = is_gray(img, threshold=10)\n            if face_helper.is_gray:\n                print('Grayscale input: True')\n            face_helper.cropped_faces = [img]\n        else:\n            face_helper.read_image(img)\n            # get face landmarks for each face\n            num_det_faces = face_helper.get_face_landmarks_5(\n                only_center_face=args.only_center_face, resize=640, eye_dist_threshold=5)\n            print(f'\\tdetect {num_det_faces} faces')\n            # align and warp each face\n            face_helper.align_warp_face()\n\n        # face restoration for each cropped face\n        for idx, cropped_face in enumerate(face_helper.cropped_faces):\n            # prepare data\n            cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n            normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n            cropped_face_t = cropped_face_t.unsqueeze(0).to(device)\n\n            try:\n                with torch.no_grad():\n                    output = net(cropped_face_t, w=w, adain=True)[0]\n                    restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1))\n                del output\n                torch.cuda.empty_cache()\n            except Exception as error:\n                print(f'\\tFailed inference for CodeFormer: {error}')\n                restored_face = tensor2img(cropped_face_t, rgb2bgr=True, min_max=(-1, 1))\n\n            restored_face = restored_face.astype('uint8')\n            face_helper.add_restored_face(restored_face, cropped_face)\n\n        # paste_back\n        if not args.has_aligned:\n            # upsample the background\n            if bg_upsampler is not None:\n                # Now only support RealESRGAN for upsampling background\n                bg_img = bg_upsampler.enhance(img, outscale=args.upscale)[0]\n            else:\n                bg_img = None\n            face_helper.get_inverse_affine(None)\n            # paste each restored face to the input image\n            if args.face_upsample and face_upsampler is not None: \n                restored_img = face_helper.paste_faces_to_input_image(upsample_img=bg_img, draw_box=args.draw_box, face_upsampler=face_upsampler)\n            else:\n                restored_img = face_helper.paste_faces_to_input_image(upsample_img=bg_img, draw_box=args.draw_box)\n\n        # save faces\n        for idx, (cropped_face, restored_face) in enumerate(zip(face_helper.cropped_faces, face_helper.restored_faces)):\n            # save cropped face\n            if not args.has_aligned: \n                save_crop_path = os.path.join(result_root, 'cropped_faces', f'{basename}_{idx:02d}.png')\n                imwrite(cropped_face, save_crop_path)\n            # save restored face\n            if args.has_aligned:\n                save_face_name = f'{basename}.png'\n            else:\n                save_face_name = f'{basename}_{idx:02d}.png'\n            if args.suffix is not None:\n                save_face_name = f'{save_face_name[:-4]}_{args.suffix}.png'\n            save_restore_path = os.path.join(result_root, 'restored_faces', save_face_name)\n            imwrite(restored_face, save_restore_path)\n\n        # save restored img\n        if not args.has_aligned and restored_img is not None:\n            if args.suffix is not None:\n                basename = f'{basename}_{args.suffix}'\n            save_restore_path = os.path.join(result_root, 'final_results', f'{basename}.png')\n            imwrite(restored_img, save_restore_path)\n\n    # save enhanced video\n    if input_video:\n        print('Video Saving...')\n        # load images\n        video_frames = []\n        img_list = sorted(glob.glob(os.path.join(result_root, 'final_results', '*.[jp][pn]g')))\n        for img_path in img_list:\n            img = cv2.imread(img_path)\n            video_frames.append(img)\n        # write images to video\n        height, width = video_frames[0].shape[:2]\n        if args.suffix is not None:\n            video_name = f'{video_name}_{args.suffix}.png'\n        save_restore_path = os.path.join(result_root, f'{video_name}.mp4')\n        vidwriter = VideoWriter(save_restore_path, height, width, fps, audio)\n         \n        for f in video_frames:\n            vidwriter.write_frame(f)\n        vidwriter.close()\n\n    print(f'\\nAll results are saved in {result_root}')\n"
        },
        {
          "name": "inference_colorization.py",
          "type": "blob",
          "size": 4.0283203125,
          "content": "import os\nimport cv2\nimport argparse\nimport glob\nimport torch\nfrom torchvision.transforms.functional import normalize\nfrom basicsr.utils import imwrite, img2tensor, tensor2img\nfrom basicsr.utils.download_util import load_file_from_url\nfrom basicsr.utils.misc import get_device\nfrom basicsr.utils.registry import ARCH_REGISTRY\n\npretrain_model_url = 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer_colorization.pth'\n\nif __name__ == '__main__':\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    device = get_device()\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-i', '--input_path', type=str, default='./inputs/gray_faces', \n                    help='Input image or folder. Default: inputs/gray_faces')\n    parser.add_argument('-o', '--output_path', type=str, default=None, \n                    help='Output folder. Default: results/<input_name>')\n    parser.add_argument('--suffix', type=str, default=None, \n                    help='Suffix of the restored faces. Default: None')\n    args = parser.parse_args()\n\n    # ------------------------ input & output ------------------------\n    print('[NOTE] The input face images should be aligned and cropped to a resolution of 512x512.')\n    if args.input_path.endswith(('jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG')): # input single img path\n        input_img_list = [args.input_path]\n        result_root = f'results/test_colorization_img'\n    else: # input img folder\n        if args.input_path.endswith('/'):  # solve when path ends with /\n            args.input_path = args.input_path[:-1]\n        # scan all the jpg and png images\n        input_img_list = sorted(glob.glob(os.path.join(args.input_path, '*.[jpJP][pnPN]*[gG]')))\n        result_root = f'results/{os.path.basename(args.input_path)}'\n\n    if not args.output_path is None: # set output path\n        result_root = args.output_path\n\n    test_img_num = len(input_img_list)\n\n    # ------------------ set up CodeFormer restorer -------------------\n    net = ARCH_REGISTRY.get('CodeFormer')(dim_embd=512, codebook_size=1024, n_head=8, n_layers=9, \n                                            connect_list=['32', '64', '128']).to(device)\n    \n    # ckpt_path = 'weights/CodeFormer/codeformer.pth'\n    ckpt_path = load_file_from_url(url=pretrain_model_url, \n                                    model_dir='weights/CodeFormer', progress=True, file_name=None)\n    checkpoint = torch.load(ckpt_path)['params_ema']\n    net.load_state_dict(checkpoint)\n    net.eval()\n\n    # -------------------- start to processing ---------------------\n    for i, img_path in enumerate(input_img_list):\n        img_name = os.path.basename(img_path)\n        basename, ext = os.path.splitext(img_name)\n        print(f'[{i+1}/{test_img_num}] Processing: {img_name}')\n        input_face = cv2.imread(img_path)\n        assert input_face.shape[:2] == (512, 512), 'Input resolution must be 512x512 for colorization.'\n        # input_face = cv2.resize(input_face, (512, 512), interpolation=cv2.INTER_LINEAR)\n        input_face = img2tensor(input_face / 255., bgr2rgb=True, float32=True)\n        normalize(input_face, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n        input_face = input_face.unsqueeze(0).to(device)\n        try:\n            with torch.no_grad():\n                # w is fixed to 0 since we didn't train the Stage III for colorization\n                output_face = net(input_face, w=0, adain=True)[0] \n                save_face = tensor2img(output_face, rgb2bgr=True, min_max=(-1, 1))\n            del output_face\n            torch.cuda.empty_cache()\n        except Exception as error:\n            print(f'\\tFailed inference for CodeFormer: {error}')\n            save_face = tensor2img(input_face, rgb2bgr=True, min_max=(-1, 1))\n\n        save_face = save_face.astype('uint8')\n\n        # save face\n        if args.suffix is not None:\n            basename = f'{basename}_{args.suffix}'\n        save_restore_path = os.path.join(result_root, f'{basename}.png')\n        imwrite(save_face, save_restore_path)\n\n    print(f'\\nAll results are saved in {result_root}')\n\n"
        },
        {
          "name": "inference_inpainting.py",
          "type": "blob",
          "size": 4.26171875,
          "content": "import os\nimport cv2\nimport argparse\nimport glob\nimport torch\nfrom torchvision.transforms.functional import normalize\nfrom basicsr.utils import imwrite, img2tensor, tensor2img\nfrom basicsr.utils.download_util import load_file_from_url\nfrom basicsr.utils.misc import get_device\nfrom basicsr.utils.registry import ARCH_REGISTRY\n\npretrain_model_url = 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer_inpainting.pth'\n\nif __name__ == '__main__':\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    device = get_device()\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-i', '--input_path', type=str, default='./inputs/masked_faces', \n                    help='Input image or folder. Default: inputs/masked_faces')\n    parser.add_argument('-o', '--output_path', type=str, default=None, \n                    help='Output folder. Default: results/<input_name>')\n    parser.add_argument('--suffix', type=str, default=None, \n                    help='Suffix of the restored faces. Default: None')\n    args = parser.parse_args()\n\n    # ------------------------ input & output ------------------------\n    print('[NOTE] The input face images should be aligned and cropped to a resolution of 512x512.')\n    if args.input_path.endswith(('jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG')): # input single img path\n        input_img_list = [args.input_path]\n        result_root = f'results/test_inpainting_img'\n    else: # input img folder\n        if args.input_path.endswith('/'):  # solve when path ends with /\n            args.input_path = args.input_path[:-1]\n        # scan all the jpg and png images\n        input_img_list = sorted(glob.glob(os.path.join(args.input_path, '*.[jpJP][pnPN]*[gG]')))\n        result_root = f'results/{os.path.basename(args.input_path)}'\n\n    if not args.output_path is None: # set output path\n        result_root = args.output_path\n\n    test_img_num = len(input_img_list)\n\n    # ------------------ set up CodeFormer restorer -------------------\n    net = ARCH_REGISTRY.get('CodeFormer')(dim_embd=512, codebook_size=512, n_head=8, n_layers=9, \n                                            connect_list=['32', '64', '128']).to(device)\n    \n    # ckpt_path = 'weights/CodeFormer/codeformer.pth'\n    ckpt_path = load_file_from_url(url=pretrain_model_url, \n                                    model_dir='weights/CodeFormer', progress=True, file_name=None)\n    checkpoint = torch.load(ckpt_path)['params_ema']\n    net.load_state_dict(checkpoint)\n    net.eval()\n\n    # -------------------- start to processing ---------------------\n    for i, img_path in enumerate(input_img_list):\n        img_name = os.path.basename(img_path)\n        basename, ext = os.path.splitext(img_name)\n        print(f'[{i+1}/{test_img_num}] Processing: {img_name}')\n        input_face = cv2.imread(img_path)\n        assert input_face.shape[:2] == (512, 512), 'Input resolution must be 512x512 for inpainting.'\n        # input_face = cv2.resize(input_face, (512, 512), interpolation=cv2.INTER_LINEAR)\n        input_face = img2tensor(input_face / 255., bgr2rgb=True, float32=True)\n        normalize(input_face, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n        input_face = input_face.unsqueeze(0).to(device)\n        try:\n            with torch.no_grad():\n                mask = torch.zeros(512, 512)\n                m_ind = torch.sum(input_face[0], dim=0)\n                mask[m_ind==3] = 1.0\n                mask = mask.view(1, 1, 512, 512).to(device)\n                # w is fixed to 1, adain=False for inpainting\n                output_face = net(input_face, w=1, adain=False)[0]\n                output_face = (1-mask)*input_face + mask*output_face\n                save_face = tensor2img(output_face, rgb2bgr=True, min_max=(-1, 1))\n            del output_face\n            torch.cuda.empty_cache()\n        except Exception as error:\n            print(f'\\tFailed inference for CodeFormer: {error}')\n            save_face = tensor2img(input_face, rgb2bgr=True, min_max=(-1, 1))\n\n        save_face = save_face.astype('uint8')\n\n        # save face\n        if args.suffix is not None:\n            basename = f'{basename}_{args.suffix}'\n        save_restore_path = os.path.join(result_root, f'{basename}.png')\n        imwrite(save_face, save_restore_path)\n\n    print(f'\\nAll results are saved in {result_root}')\n\n"
        },
        {
          "name": "inputs",
          "type": "tree",
          "content": null
        },
        {
          "name": "options",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.189453125,
          "content": "addict\nfuture\nlmdb\nnumpy\nopencv-python\nPillow\npyyaml\nrequests\nscikit-image\nscipy\ntb-nightly\ntorch>=1.7.1\ntorchvision\ntqdm\nyapf\nlpips\ngdown # supports downloading the large file from Google Drive"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "web-demos",
          "type": "tree",
          "content": null
        },
        {
          "name": "weights",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}