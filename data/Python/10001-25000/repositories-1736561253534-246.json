{
  "metadata": {
    "timestamp": 1736561253534,
    "page": 246,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "m-bain/whisperX",
      "stars": 13273,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.333984375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# UV\n#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#uv.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control\n.pdm.toml\n.pdm-python\n.pdm-build/\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n# PyPI configuration file\n.pypirc"
        },
        {
          "name": "EXAMPLES.md",
          "type": "blob",
          "size": 1.2294921875,
          "content": "# More Examples\n\n## Other Languages\n\nFor non-english ASR, it is best to use the `large` whisper model. Alignment models are automatically picked by the chosen language from the default [lists](https://github.com/m-bain/whisperX/blob/main/whisperx/alignment.py#L18).\n\nCurrently support default models tested for {en, fr, de, es, it, ja, zh, nl}\n\n\nIf the detected language is not in this list, you need to find a phoneme-based ASR model from [huggingface model hub](https://huggingface.co/models) and test it on your data.\n\n### French\n    whisperx --model large --language fr examples/sample_fr_01.wav\n\n\nhttps://user-images.githubusercontent.com/36994049/208298804-31c49d6f-6787-444e-a53f-e93c52706752.mov\n\n\n### German\n    whisperx --model large --language de examples/sample_de_01.wav\n\n\nhttps://user-images.githubusercontent.com/36994049/208298811-e36002ba-3698-4731-97d4-0aebd07e0eb3.mov\n\n\n### Italian\n    whisperx --model large --language de examples/sample_it_01.wav\n\n\nhttps://user-images.githubusercontent.com/36994049/208298819-6f462b2c-8cae-4c54-b8e1-90855794efc7.mov\n\n\n### Japanese\n    whisperx --model large --language ja examples/sample_ja_01.wav\n\n\nhttps://user-images.githubusercontent.com/19920981/208731743-311f2360-b73b-4c60-809d-aaf3cd7e06f4.mov\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.2666015625,
          "content": "BSD 2-Clause License\n\nCopyright (c) 2024, Max Bain\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0654296875,
          "content": "include whisperx/assets/*\ninclude LICENSE\ninclude requirements.txt\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.7265625,
          "content": "<h1 align=\"center\">WhisperX</h1>\n\n<p align=\"center\">\n  <a href=\"https://github.com/m-bain/whisperX/stargazers\">\n    <img src=\"https://img.shields.io/github/stars/m-bain/whisperX.svg?colorA=orange&colorB=orange&logo=github\"\n         alt=\"GitHub stars\">\n  </a>\n  <a href=\"https://github.com/m-bain/whisperX/issues\">\n        <img src=\"https://img.shields.io/github/issues/m-bain/whisperx.svg\"\n             alt=\"GitHub issues\">\n  </a>\n  <a href=\"https://github.com/m-bain/whisperX/blob/master/LICENSE\">\n        <img src=\"https://img.shields.io/github/license/m-bain/whisperX.svg\"\n             alt=\"GitHub license\">\n  </a>\n  <a href=\"https://arxiv.org/abs/2303.00747\">\n        <img src=\"http://img.shields.io/badge/Arxiv-2303.00747-B31B1B.svg\"\n             alt=\"ArXiv paper\">\n  </a>\n  <a href=\"https://twitter.com/intent/tweet?text=&url=https%3A%2F%2Fgithub.com%2Fm-bain%2FwhisperX\">\n  <img src=\"https://img.shields.io/twitter/url/https/github.com/m-bain/whisperX.svg?style=social\" alt=\"Twitter\">\n  </a>      \n</p>\n\n\n<img width=\"1216\" align=\"center\" alt=\"whisperx-arch\" src=\"https://raw.githubusercontent.com/m-bain/whisperX/refs/heads/main/figures/pipeline.png\">\n\n\n<!-- <p align=\"left\">Whisper-Based Automatic Speech Recognition (ASR) with improved timestamp accuracy + quality via forced phoneme alignment and voice-activity based batching for fast inference.</p> -->\n\n\n<!-- <h2 align=\"left\", id=\"what-is-it\">What is it üîé</h2> -->\n\n\nThis repository provides fast automatic speech recognition (70x realtime with large-v2) with word-level timestamps and speaker diarization.\n\n- ‚ö°Ô∏è Batched inference for 70x realtime transcription using whisper large-v2\n- ü™∂ [faster-whisper](https://github.com/guillaumekln/faster-whisper) backend, requires <8GB gpu memory for large-v2 with beam_size=5\n- üéØ Accurate word-level timestamps using wav2vec2 alignment\n- üëØ‚Äç‚ôÇÔ∏è Multispeaker ASR using speaker diarization from [pyannote-audio](https://github.com/pyannote/pyannote-audio) (speaker ID labels) \n- üó£Ô∏è VAD preprocessing, reduces hallucination & batching with no WER degradation\n\n\n\n**Whisper** is an ASR model [developed by OpenAI](https://github.com/openai/whisper), trained on a large dataset of diverse audio. Whilst it does produces highly accurate transcriptions, the corresponding timestamps are at the utterance-level, not per word, and can be inaccurate by several seconds. OpenAI's whisper does not natively support batching.\n\n**Phoneme-Based ASR** A suite of models finetuned to recognise the smallest unit of speech distinguishing one word from another, e.g. the element p in \"tap\". A popular example model is [wav2vec2.0](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self).\n\n**Forced Alignment** refers to the process by which orthographic transcriptions are aligned to audio recordings to automatically generate phone level segmentation.\n\n**Voice Activity Detection (VAD)** is the detection of the presence or absence of human speech.\n\n**Speaker Diarization** is the process of partitioning an audio stream containing human speech into homogeneous segments according to the identity of each speaker.\n\n<h2 align=\"left\", id=\"highlights\">Newüö®</h2>\n\n- 1st place at [Ego4d transcription challenge](https://eval.ai/web/challenges/challenge-page/1637/leaderboard/3931/WER)  üèÜ\n- _WhisperX_ accepted at INTERSPEECH 2023 \n- v3 transcript segment-per-sentence: using nltk sent_tokenize for better subtitlting & better diarization\n- v3 released, 70x speed-up open-sourced. Using batched whisper with [faster-whisper](https://github.com/guillaumekln/faster-whisper) backend!\n- v2 released, code cleanup, imports whisper library VAD filtering is now turned on by default, as in the paper.\n- Paper dropüéìüë®‚Äçüè´! Please see our [ArxiV preprint](https://arxiv.org/abs/2303.00747) for benchmarking and details of WhisperX. We also introduce more efficient batch inference resulting in large-v2 with *60-70x REAL TIME speed.\n\n<h2 align=\"left\" id=\"setup\">Setup ‚öôÔ∏è</h2>\nTested for PyTorch 2.0, Python 3.10 (use other versions at your own risk!)\n\nGPU execution requires the NVIDIA libraries cuBLAS 11.x and cuDNN 8.x to be installed on the system. Please refer to the [CTranslate2 documentation](https://opennmt.net/CTranslate2/installation.html).\n\n\n### 1. Create Python3.10 environment\n\n`conda create --name whisperx python=3.10`\n\n`conda activate whisperx`\n\n\n### 2. Install PyTorch, e.g. for Linux and Windows CUDA11.8:\n\n`conda install pytorch==2.0.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia`\n\nSee other methods [here.](https://pytorch.org/get-started/previous-versions/#v200)\n\n### 3. Install WhisperX\n\nYou have several installation options:\n\n#### Option A: Stable Release (recommended)\nInstall the latest stable version from PyPI:\n\n```bash\npip install whisperx\n```\n\n#### Option B: Development Version\nInstall the latest development version directly from GitHub (may be unstable):\n\n```bash\npip install git+https://github.com/m-bain/whisperx.git\n```\n\nIf already installed, update to the most recent commit:\n\n```bash\npip install git+https://github.com/m-bain/whisperx.git --upgrade\n```\n\n#### Option C: Development Mode\nIf you wish to modify the package, clone and install in editable mode:\n```bash\ngit clone https://github.com/m-bain/whisperX.git\ncd whisperX\npip install -e .\n```\n\n> **Note**: The development version may contain experimental features and bugs. Use the stable PyPI release for production environments.\n\nYou may also need to install ffmpeg, rust etc. Follow openAI instructions here https://github.com/openai/whisper#setup.\n\n### Speaker Diarization\nTo **enable Speaker Diarization**, include your Hugging Face access token (read) that you can generate from [Here](https://huggingface.co/settings/tokens) after the `--hf_token` argument and accept the user agreement for the following models: [Segmentation](https://huggingface.co/pyannote/segmentation-3.0) and [Speaker-Diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1) (if you choose to use Speaker-Diarization 2.x, follow requirements [here](https://huggingface.co/pyannote/speaker-diarization) instead.)\n\n> **Note**<br>\n> As of Oct 11, 2023, there is a known issue regarding slow performance with pyannote/Speaker-Diarization-3.0 in whisperX. It is due to dependency conflicts between faster-whisper and pyannote-audio 3.0.0. Please see [this issue](https://github.com/m-bain/whisperX/issues/499) for more details and potential workarounds.\n\n\n<h2 align=\"left\" id=\"example\">Usage üí¨ (command line)</h2>\n\n### English\n\nRun whisper on example segment (using default params, whisper small) add `--highlight_words True` to visualise word timings in the .srt file.\n\n    whisperx examples/sample01.wav\n\n\nResult using *WhisperX* with forced alignment to wav2vec2.0 large:\n\nhttps://user-images.githubusercontent.com/36994049/208253969-7e35fe2a-7541-434a-ae91-8e919540555d.mp4\n\nCompare this to original whisper out the box, where many transcriptions are out of sync:\n\nhttps://user-images.githubusercontent.com/36994049/207743923-b4f0d537-29ae-4be2-b404-bb941db73652.mov\n\n\nFor increased timestamp accuracy, at the cost of higher gpu mem, use bigger models (bigger alignment model not found to be that helpful, see paper) e.g.\n\n    whisperx examples/sample01.wav --model large-v2 --align_model WAV2VEC2_ASR_LARGE_LV60K_960H --batch_size 4\n\n\nTo label the transcript with speaker ID's (set number of speakers if known e.g. `--min_speakers 2` `--max_speakers 2`):\n\n    whisperx examples/sample01.wav --model large-v2 --diarize --highlight_words True\n\nTo run on CPU instead of GPU (and for running on Mac OS X):\n\n    whisperx examples/sample01.wav --compute_type int8\n\n### Other languages\n\nThe phoneme ASR alignment model is *language-specific*, for tested languages these models are [automatically picked from torchaudio pipelines or huggingface](https://github.com/m-bain/whisperX/blob/e909f2f766b23b2000f2d95df41f9b844ac53e49/whisperx/transcribe.py#L22).\nJust pass in the `--language` code, and use the whisper `--model large`.\n\nCurrently default models provided for `{en, fr, de, es, it, ja, zh, nl, uk, pt}`. If the detected language is not in this list, you need to find a phoneme-based ASR model from [huggingface model hub](https://huggingface.co/models) and test it on your data.\n\n\n#### E.g. German\n    whisperx --model large-v2 --language de examples/sample_de_01.wav\n\nhttps://user-images.githubusercontent.com/36994049/208298811-e36002ba-3698-4731-97d4-0aebd07e0eb3.mov\n\n\nSee more examples in other languages [here](EXAMPLES.md).\n\n## Python usage  üêç\n\n```python\nimport whisperx\nimport gc \n\ndevice = \"cuda\" \naudio_file = \"audio.mp3\"\nbatch_size = 16 # reduce if low on GPU mem\ncompute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n\n# 1. Transcribe with original whisper (batched)\nmodel = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n\n# save model to local path (optional)\n# model_dir = \"/path/\"\n# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n\naudio = whisperx.load_audio(audio_file)\nresult = model.transcribe(audio, batch_size=batch_size)\nprint(result[\"segments\"]) # before alignment\n\n# delete model if low on GPU resources\n# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n\n# 2. Align whisper output\nmodel_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\nresult = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n\nprint(result[\"segments\"]) # after alignment\n\n# delete model if low on GPU resources\n# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n\n# 3. Assign speaker labels\ndiarize_model = whisperx.DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)\n\n# add min/max number of speakers if known\ndiarize_segments = diarize_model(audio)\n# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n\nresult = whisperx.assign_word_speakers(diarize_segments, result)\nprint(diarize_segments)\nprint(result[\"segments\"]) # segments are now assigned speaker IDs\n```\n\n## Demos üöÄ\n\n[![Replicate (large-v3](https://img.shields.io/static/v1?label=Replicate+WhisperX+large-v3&message=Demo+%26+Cloud+API&color=blue)](https://replicate.com/victor-upmeet/whisperx) \n[![Replicate (large-v2](https://img.shields.io/static/v1?label=Replicate+WhisperX+large-v2&message=Demo+%26+Cloud+API&color=blue)](https://replicate.com/daanelson/whisperx) \n[![Replicate (medium)](https://img.shields.io/static/v1?label=Replicate+WhisperX+medium&message=Demo+%26+Cloud+API&color=blue)](https://replicate.com/carnifexer/whisperx) \n\nIf you don't have access to your own GPUs, use the links above to try out WhisperX. \n\n<h2 align=\"left\" id=\"whisper-mod\">Technical Details üë∑‚Äç‚ôÇÔ∏è</h2>\n\nFor specific details on the batching and alignment, the effect of VAD, as well as the chosen alignment model, see the preprint [paper](https://www.robots.ox.ac.uk/~vgg/publications/2023/Bain23/bain23.pdf).\n\nTo reduce GPU memory requirements, try any of the following (2. & 3. can affect quality):\n1.  reduce batch size, e.g. `--batch_size 4`\n2. use a smaller ASR model `--model base`\n3. Use lighter compute type `--compute_type int8`\n\nTranscription differences from openai's whisper:\n1. Transcription without timestamps. To enable single pass batching, whisper inference is performed `--without_timestamps True`, this ensures 1 forward pass per sample in the batch. However, this can cause discrepancies the default whisper output.\n2. VAD-based segment transcription, unlike the buffered transcription of openai's. In the WhisperX paper we show this reduces WER, and enables accurate batched inference\n3.  `--condition_on_prev_text` is set to `False` by default (reduces hallucination)\n\n<h2 align=\"left\" id=\"limitations\">Limitations ‚ö†Ô∏è</h2>\n\n- Transcript words which do not contain characters in the alignment models dictionary e.g. \"2014.\" or \"¬£13.60\" cannot be aligned and therefore are not given a timing.\n- Overlapping speech is not handled particularly well by whisper nor whisperx\n- Diarization is far from perfect\n- Language specific wav2vec2 model is needed\n\n\n<h2 align=\"left\" id=\"contribute\">Contribute üßë‚Äçüè´</h2>\n\nIf you are multilingual, a major way you can contribute to this project is to find phoneme models on huggingface (or train your own) and test them on speech for the target language. If the results look good send a pull request and some examples showing its success.\n\nBug finding and pull requests are also highly appreciated to keep this project going, since it's already diverging from the original research scope.\n\n<h2 align=\"left\" id=\"coming-soon\">TODO üóì</h2>\n\n* [x] Multilingual init\n\n* [x] Automatic align model selection based on language detection\n\n* [x] Python usage\n\n* [x] Incorporating  speaker diarization\n\n* [x] Model flush, for low gpu mem resources\n\n* [x] Faster-whisper backend\n\n* [x] Add max-line etc. see (openai's whisper utils.py)\n\n* [x] Sentence-level segments (nltk toolbox)\n\n* [x] Improve alignment logic\n\n* [ ] update examples with diarization and word highlighting\n\n* [ ] Subtitle .ass output <- bring this back (removed in v3)\n\n* [ ] Add benchmarking code (TEDLIUM for spd/WER & word segmentation)\n\n* [ ] Allow silero-vad as alternative VAD option\n\n* [ ] Improve diarization (word level). *Harder than first thought...*\n\n\n<h2 align=\"left\" id=\"contact\">Contact/Support üìá</h2>\n\n\nContact maxhbain@gmail.com for queries.\n\n<a href=\"https://www.buymeacoffee.com/maxhbain\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/default-orange.png\" alt=\"Buy Me A Coffee\" height=\"41\" width=\"174\"></a>\n\n\n<h2 align=\"left\" id=\"acks\">Acknowledgements üôè</h2>\n\nThis work, and my PhD, is supported by the [VGG (Visual Geometry Group)](https://www.robots.ox.ac.uk/~vgg/) and the University of Oxford.\n\nOf course, this is builds on [openAI's whisper](https://github.com/openai/whisper).\nBorrows important alignment code from [PyTorch tutorial on forced alignment](https://pytorch.org/tutorials/intermediate/forced_alignment_with_torchaudio_tutorial.html)\nAnd uses the wonderful pyannote VAD / Diarization https://github.com/pyannote/pyannote-audio\n\n\nValuable VAD & Diarization Models from [pyannote audio](https://github.com/pyannote/pyannote-audio)\n\nGreat backend from [faster-whisper](https://github.com/guillaumekln/faster-whisper) and [CTranslate2](https://github.com/OpenNMT/CTranslate2)\n\nThose who have [supported this work financially](https://www.buymeacoffee.com/maxhbain) üôè\n\nFinally, thanks to the OS [contributors](https://github.com/m-bain/whisperX/graphs/contributors) of this project, keeping it going and identifying bugs.\n\n<h2 align=\"left\" id=\"cite\">Citation</h2>\nIf you use this in your research, please cite the paper:\n\n```bibtex\n@article{bain2022whisperx,\n  title={WhisperX: Time-Accurate Speech Transcription of Long-Form Audio},\n  author={Bain, Max and Huh, Jaesung and Han, Tengda and Zisserman, Andrew},\n  journal={INTERSPEECH 2023},\n  year={2023}\n}\n```\n"
        },
        {
          "name": "figures",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1005859375,
          "content": "torch>=2\ntorchaudio>=2\nfaster-whisper==1.1.0\nctranslate2<4.5.0\ntransformers\npandas\nsetuptools>=65\nnltk\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.9462890625,
          "content": "import os\n\nimport pkg_resources\nfrom setuptools import find_packages, setup\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nsetup(\n    name=\"whisperx\",\n    py_modules=[\"whisperx\"],\n    version=\"3.3.1\",\n    description=\"Time-Accurate Automatic Speech Recognition using Whisper.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    python_requires=\">=3.9, <3.13\",\n    author=\"Max Bain\",\n    url=\"https://github.com/m-bain/whisperx\",\n    license=\"BSD-2-Clause\",\n    packages=find_packages(exclude=[\"tests*\"]),\n    install_requires=[\n        str(r)\n        for r in pkg_resources.parse_requirements(\n            open(os.path.join(os.path.dirname(__file__), \"requirements.txt\"))\n        )\n    ]\n    + [f\"pyannote.audio==3.3.2\"],\n    entry_points={\n        \"console_scripts\": [\"whisperx=whisperx.transcribe:cli\"],\n    },\n    include_package_data=True,\n    extras_require={\"dev\": [\"pytest\"]},\n)\n"
        },
        {
          "name": "whisperx",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}