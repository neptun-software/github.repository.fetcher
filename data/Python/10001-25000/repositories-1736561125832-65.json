{
  "metadata": {
    "timestamp": 1736561125832,
    "page": 65,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PromtEngineer/localGPT",
      "stars": 20236,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.04296875,
          "content": "*\n!*.py\n!requirements.txt\n!SOURCE_DOCUMENTS\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.2568359375,
          "content": "# http://editorconfig.org\n\nroot = true\n\n[*]\ncharset = utf-8\nend_of_line = lf\ninsert_final_newline = true\ntrim_trailing_whitespace = true\n\n[*.{py,rst,ini}]\nindent_style = space\nindent_size = 4\n\n[*.{html,css,scss,json,yml,xml}]\nindent_style = space\nindent_size = 2\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.0654296875,
          "content": "[flake8]\nexclude = docs\nmax-line-length = 119\nextend-ignore = E203\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.0849609375,
          "content": "# Ignore vscode\n/.vscode\n/DB\n/models\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\n#MacOS\n.DS_Store\nSOURCE_DOCUMENTS/.DS_Store"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.01171875,
          "content": "default_stages: [commit]\n\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-json\n      - id: check-toml\n      - id: check-xml\n      - id: check-yaml\n      - id: debug-statements\n      - id: check-builtin-literals\n      - id: check-case-conflict\n      - id: detect-private-key\n\n  - repo: https://github.com/pre-commit/mirrors-prettier\n    rev: \"v3.0.0-alpha.9-for-vscode\"\n    hooks:\n      - id: prettier\n        args: [\"--tab-width\", \"2\"]\n\n  - repo: https://github.com/asottile/pyupgrade\n    rev: v3.4.0\n    hooks:\n      - id: pyupgrade\n        args: [--py311-plus]\n        exclude: hooks/\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/PyCQA/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n\n  - repo: https://github.com/PyCQA/flake8\n    rev: 6.0.0\n    hooks:\n      - id: flake8\n\nci:\n  autoupdate_schedule: weekly\n  skip: []\n  submodules: false\n"
        },
        {
          "name": ".pyup.yml",
          "type": "blob",
          "size": 0.3525390625,
          "content": "# configure updates globally\n# default: all\n# allowed: all, insecure, False\nupdate: all\n\n# configure dependency pinning globally\n# default: True\n# allowed: True, False\npin: True\n\n# add a label to pull requests, default is not set\n# requires private repo permissions, even on public repos\n# default: empty\nlabel_prs: update\n\nrequirements:\n  - \"requirements.txt\"\n"
        },
        {
          "name": "3.20.2",
          "type": "blob",
          "size": 0.0927734375,
          "content": "Requirement already satisfied: protobuf in c:\\users\\kevin\\anaconda3\\lib\\site-packages (4.24.4)\n"
        },
        {
          "name": "ACKNOWLEDGEMENT.md",
          "type": "blob",
          "size": 0.416015625,
          "content": "# Acknowledgments\n\nSome code was taken or inspired from other projects:-\n\n- [CookieCutter Django][cookiecutter-django]\n  - `pre-commit-config.yaml` is taken from there with almost no changes\n  - `github-actions.yml` is inspired by `gitlab-ci.yml`\n  - `.pyup.yml`, `.flake8`, `.editorconfig`, `pyproject.toml` are taken from there with minor changes,\n\n[cookiecutter-django]: https://github.com/cookiecutter/cookiecutter-django\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.9794921875,
          "content": "# How to Contribute\n\nAlways happy to get issues identified and pull requests!\n\n## General considerations\n\n1. Keep it small. The smaller the change, the more likely we are to accept.\n2. Changes that fix a current issue get priority for review.\n3. Check out [GitHub guide][submit-a-pr] if you've never created a pull request before.\n\n## Getting started\n\n1. Fork the repo\n2. Clone your fork\n3. Create a branch for your changes\n\nThis last step is very important, don't start developing from master, it'll cause pain if you need to send another change later.\n\nTIP: If you're working on a GitHub issue, name your branch after the issue number, e.g. `issue-123-<ISSUE-NAME>`. This will help us keep track of what you're working on. If there is not an issue for what you're working on, create one first please. Someone else might be working on the same thing, or we might have a reason for not wanting to do it.\n\n## Pre-commit\n\nGitHub Actions is going to run Pre-commit hooks on your PR. If the hooks fail, you will need to fix them before your PR can be merged. It will save you a lot of time if you run the hooks locally before you push your changes. To do that, you need to install pre-commit on your local machine.\n\n```shell\npip install pre-commit\n```\n\nOnce installed, you need to add the pre-commit hooks to your local repo.\n\n```shell\npre-commit install\n```\n\nNow, every time you commit, the hooks will run and check your code. If they fail, you will need to fix them before you can commit.\n\nIf it happened that you committed changes already without having pre-commit hooks and do not want to reset and recommit again, you can run the following command to run the hooks on your local repo.\n\n```shell\npre-commit run --all-files\n```\n\n## Help Us Improve This Documentation\n\nIf you find that something is missing or have suggestions for improvements, please submit a PR.\n\n[submit-a-pr]: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.2685546875,
          "content": "# syntax=docker/dockerfile:1\n# Build as `docker build . -t localgpt`, requires BuildKit.\n# Run as `docker run -it --mount src=\"$HOME/.cache\",target=/root/.cache,type=bind --gpus=all localgpt`, requires Nvidia container toolkit.\n\nFROM nvidia/cuda:11.7.1-runtime-ubuntu22.04\nRUN apt-get update && apt-get install -y software-properties-common\nRUN apt-get install -y g++-11 make python3 python-is-python3 pip\n# only copy what's needed at every step to optimize layer cache\nCOPY ./requirements.txt .\n# use BuildKit cache mount to drastically reduce redownloading from pip on repeated builds\nRUN --mount=type=cache,target=/root/.cache CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --timeout 100 -r requirements.txt llama-cpp-python==0.1.83\nCOPY SOURCE_DOCUMENTS ./SOURCE_DOCUMENTS\nCOPY ingest.py constants.py ./\n# Docker BuildKit does not support GPU during *docker build* time right now, only during *docker run*.\n# See <https://github.com/moby/buildkit/issues/1436>.\n# If this changes in the future you can `docker build --build-arg device_type=cuda  . -t localgpt` (+GPU argument to be determined).\nARG device_type=cpu\nRUN --mount=type=cache,target=/root/.cache python ingest.py --device_type $device_type\nCOPY . .\nENV device_type=cuda\nCMD python run_localGPT.py --device_type $device_type\n"
        },
        {
          "name": "Dockerfile_hpu",
          "type": "blob",
          "size": 1.7353515625,
          "content": "FROM vault.habana.ai/gaudi-docker/1.17.0/ubuntu22.04/habanalabs/pytorch-installer-2.3.1:latest\n\nENV HABANA_VISIBLE_DEVICES=all\nENV OMPI_MCA_btl_vader_single_copy_mechanism=none\nENV PT_HPU_LAZY_ACC_PAR_MODE=0\nENV PT_HPU_ENABLE_LAZY_COLLECTIVES=1\n\n# Install linux packages\nENV DEBIAN_FRONTEND=\"noninteractive\"  TZ=Etc/UTC\nRUN apt-get update && apt-get install -y tzdata bash-completion python3-pip openssh-server \\\n    vim git iputils-ping net-tools protobuf-compiler curl bc gawk tmux \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Add repo contents\nADD localGPT /root/localGPT\nWORKDIR /root/localGPT\n\n# Install python packages\nRUN pip install --upgrade pip \\\n    && pip install langchain-experimental==0.0.62 \\\n    && pip install langchain==0.0.329 \\\n    && pip install protobuf==3.20.2 \\\n    && pip install grpcio-tools \\\n    && pip install pymilvus==2.4.0 \\\n    && pip install chromadb==0.5.15 \\\n    && pip install llama-cpp-python==0.1.66 \\\n    && pip install pdfminer.six==20221105 \\\n    && pip install transformers==4.43.1 \\\n    && pip install optimum[habana]==1.13.1 \\\n    && pip install InstructorEmbedding==1.0.1 \\\n    && pip install sentence-transformers==3.0.1 \\\n    && pip install faiss-cpu==1.7.4 \\\n    && pip install huggingface_hub==0.16.4 \\\n    && pip install protobuf==3.20.2 \\\n    && pip install auto-gptq==0.2.2 \\\n    && pip install docx2txt unstructured unstructured[pdf] urllib3 accelerate \\\n    && pip install bitsandbytes \\\n    && pip install click flask requests openpyxl \\\n    && pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.17.0 \\\n    && pip install python-multipart \\\n    && pip install fastapi \\\n    && pip install uvicorn \\\n    && pip install gptcache==0.1.43 \\\n    && pip install pypdf==4.3.1 \\\n    && pip install python-jose[cryptography]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.7294921875,
          "content": "# LocalGPT: Secure, Local Conversations with Your Documents 🌐\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/2947\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/2947\" alt=\"PromtEngineer%2FlocalGPT | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n[![GitHub Stars](https://img.shields.io/github/stars/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/stargazers)\n[![GitHub Forks](https://img.shields.io/github/forks/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/network/members)\n[![GitHub Issues](https://img.shields.io/github/issues/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/issues)\n[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/pulls)\n[![License](https://img.shields.io/github/license/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/blob/main/LICENSE)\n\n🚨🚨 You can run localGPT on a pre-configured [Virtual Machine](https://bit.ly/localGPT). Make sure to use the code: PromptEngineering to get 50% off. I will get a small commision!\n\n**LocalGPT** is an open-source initiative that allows you to converse with your documents without compromising your privacy. With everything running locally, you can be assured that no data ever leaves your computer. Dive into the world of secure, local document interactions with LocalGPT.\n\n## Features 🌟\n- **Utmost Privacy**: Your data remains on your computer, ensuring 100% security.\n- **Versatile Model Support**: Seamlessly integrate a variety of open-source models, including HF, GPTQ, GGML, and GGUF.\n- **Diverse Embeddings**: Choose from a range of open-source embeddings.\n- **Reuse Your LLM**: Once downloaded, reuse your LLM without the need for repeated downloads.\n- **Chat History**: Remembers your previous conversations (in a session).\n- **API**: LocalGPT has an API that you can use for building RAG Applications.\n- **Graphical Interface**: LocalGPT comes with two GUIs, one uses the API and the other is standalone (based on streamlit).\n- **GPU, CPU, HPU & MPS Support**: Supports multiple platforms out of the box, Chat with your data using `CUDA`, `CPU`, `HPU (Intel® Gaudi®)` or `MPS` and more!\n\n## Dive Deeper with Our Videos 🎥\n- [Detailed code-walkthrough](https://youtu.be/MlyoObdIHyo)\n- [Llama-2 with LocalGPT](https://youtu.be/lbFmceo4D5E)\n- [Adding Chat History](https://youtu.be/d7otIM_MCZs)\n- [LocalGPT - Updated (09/17/2023)](https://youtu.be/G_prHSKX9d4)\n\n## Technical Details 🛠️\nBy selecting the right local models and the power of `LangChain` you can run the entire RAG pipeline locally, without any data leaving your environment, and with reasonable performance.\n\n- `ingest.py` uses `LangChain` tools to parse the document and create embeddings locally using `InstructorEmbeddings`. It then stores the result in a local vector database using `Chroma` vector store.\n- `run_localGPT.py` uses a local LLM to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the docs.\n- You can replace this local LLM with any other LLM from the HuggingFace. Make sure whatever LLM you select is in the HF format.\n\nThis project was inspired by the original [privateGPT](https://github.com/imartinez/privateGPT).\n\n## Built Using 🧩\n- [LangChain](https://github.com/hwchase17/langchain)\n- [HuggingFace LLMs](https://huggingface.co/models)\n- [InstructorEmbeddings](https://instructor-embedding.github.io/)\n- [LLAMACPP](https://github.com/abetlen/llama-cpp-python)\n- [ChromaDB](https://www.trychroma.com/)\n- [Streamlit](https://streamlit.io/)\n\n# Environment Setup 🌍\n\n1. 📥 Clone the repo using git:\n\n```shell\ngit clone https://github.com/PromtEngineer/localGPT.git\n```\n\n2. 🐍 Install [conda](https://www.anaconda.com/download) for virtual environment management. Create and activate a new virtual environment.\n\n```shell\nconda create -n localGPT python=3.10.0\nconda activate localGPT\n```\n\n3. 🛠️ Install the dependencies using pip\n\nTo set up your environment to run the code, first install all requirements:\n\n```shell\npip install -r requirements.txt\n```\n\n***Installing LLAMA-CPP :***\n\nLocalGPT uses [LlamaCpp-Python](https://github.com/abetlen/llama-cpp-python) for GGML (you will need llama-cpp-python <=0.1.76) and GGUF (llama-cpp-python >=0.1.83) models.\n\nTo run the quantized Llama3 model, ensure you have llama-cpp-python version 0.2.62 or higher installed.\n\nIf you want to use BLAS or Metal with [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal) you can set appropriate flags:\n\nFor `NVIDIA` GPUs support, use `cuBLAS`\n\n```shell\n# Example: cuBLAS\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n```\n\nFor Apple Metal (`M1/M2`) support, use\n\n```shell\n# Example: METAL\nCMAKE_ARGS=\"-DLLAMA_METAL=on\"  FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n```\nFor more details, please refer to [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal)\n\n## Docker 🐳\n\nInstalling the required packages for GPU inference on NVIDIA GPUs, like gcc 11 and CUDA 11, may cause conflicts with other packages in your system.\nAs an alternative to Conda, you can use Docker with the provided Dockerfile.\nIt includes CUDA, your system just needs Docker, BuildKit, your NVIDIA GPU driver and the NVIDIA container toolkit.\nBuild as `docker build -t localgpt .`, requires BuildKit.\nDocker BuildKit does not support GPU during *docker build* time right now, only during *docker run*.\nRun as `docker run -it --mount src=\"$HOME/.cache\",target=/root/.cache,type=bind --gpus=all localgpt`.\nFor running the code on Intel® Gaudi® HPU, use the following Dockerfile - `Dockerfile_hpu`.\n\n## Test dataset\n\nFor testing, this repository comes with [Constitution of USA](https://constitutioncenter.org/media/files/constitution.pdf) as an example file to use.\n\n## Ingesting your OWN Data.\nPut your files in the `SOURCE_DOCUMENTS` folder. You can put multiple folders within the `SOURCE_DOCUMENTS` folder and the code will recursively read your files.\n\n### Support file formats:\nLocalGPT currently supports the following file formats. LocalGPT uses `LangChain` for loading these file formats. The code in `constants.py` uses a `DOCUMENT_MAP` dictionary to map a file format to the corresponding loader. In order to add support for another file format, simply add this dictionary with the file format and the corresponding loader from [LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n\n```shell\nDOCUMENT_MAP = {\n    \".txt\": TextLoader,\n    \".md\": TextLoader,\n    \".py\": TextLoader,\n    \".pdf\": PDFMinerLoader,\n    \".csv\": CSVLoader,\n    \".xls\": UnstructuredExcelLoader,\n    \".xlsx\": UnstructuredExcelLoader,\n    \".docx\": Docx2txtLoader,\n    \".doc\": Docx2txtLoader,\n}\n```\n\n### Ingest\n\nRun the following command to ingest all the data.\n\nIf you have `cuda` setup on your system.\n\n```shell\npython ingest.py\n```\nYou will see an output like this:\n<img width=\"1110\" alt=\"Screenshot 2023-09-14 at 3 36 27 PM\" src=\"https://github.com/PromtEngineer/localGPT/assets/134474669/c9274e9a-842c-49b9-8d95-606c3d80011f\">\n\n\nUse the device type argument to specify a given device.\nTo run on `cpu`\n\n```sh\npython ingest.py --device_type cpu\n```\n\nTo run on `M1/M2`\n\n```sh\npython ingest.py --device_type mps\n```\n\nUse help for a full list of supported devices.\n\n```sh\npython ingest.py --help\n```\n\nThis will create a new folder called `DB` and use it for the newly created vector store. You can ingest as many documents as you want, and all will be accumulated in the local embeddings database.\nIf you want to start from an empty database, delete the `DB` and reingest your documents.\n\nNote: When you run this for the first time, it will need internet access to download the embedding model (default: `Instructor Embedding`). In the subsequent runs, no data will leave your local environment and you can ingest data without internet connection.\n\n## Ask questions to your documents, locally!\n\nIn order to chat with your documents, run the following command (by default, it will run on `cuda`).\n\n```shell\npython run_localGPT.py\n```\nYou can also specify the device type just like `ingest.py`\n\n```shell\npython run_localGPT.py --device_type mps # to run on Apple silicon\n```\n\n```shell\n# To run on Intel® Gaudi® hpu\nMODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\" # in constants.py\npython run_localGPT.py --device_type hpu\n```\n\nThis will load the ingested vector store and embedding model. You will be presented with a prompt:\n\n```shell\n> Enter a query:\n```\n\nAfter typing your question, hit enter. LocalGPT will take some time based on your hardware. You will get a response like this below.\n<img width=\"1312\" alt=\"Screenshot 2023-09-14 at 3 33 19 PM\" src=\"https://github.com/PromtEngineer/localGPT/assets/134474669/a7268de9-ade0-420b-a00b-ed12207dbe41\">\n\nOnce the answer is generated, you can then ask another question without re-running the script, just wait for the prompt again.\n\n\n***Note:*** When you run this for the first time, it will need internet connection to download the LLM (default: `TheBloke/Llama-2-7b-Chat-GGUF`). After that you can turn off your internet connection, and the script inference would still work. No data gets out of your local environment.\n\nType `exit` to finish the script.\n\n### Extra Options with run_localGPT.py\n\nYou can use the `--show_sources` flag with `run_localGPT.py` to show which chunks were retrieved by the embedding model. By default, it will show 4 different sources/chunks. You can change the number of sources/chunks\n\n```shell\npython run_localGPT.py --show_sources\n```\n\nAnother option is to enable chat history. ***Note***: This is disabled by default and can be enabled by using the  `--use_history` flag. The context window is limited so keep in mind enabling history will use it and might overflow.\n\n```shell\npython run_localGPT.py --use_history\n```\n\nYou can store user questions and model responses with flag `--save_qa` into a csv file `/local_chat_history/qa_log.csv`. Every interaction will be stored. \n\n```shell\npython run_localGPT.py --save_qa\n```\n\n# Run the Graphical User Interface\n\n1. Open `constants.py` in an editor of your choice and depending on choice add the LLM you want to use. By default, the following model will be used:\n\n   ```shell\n   MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n   MODEL_BASENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n   ```\n\n3. Open up a terminal and activate your python environment that contains the dependencies installed from requirements.txt.\n\n4. Navigate to the `/LOCALGPT` directory.\n\n5. Run the following command `python run_localGPT_API.py`. The API should being to run.\n\n6. Wait until everything has loaded in. You should see something like `INFO:werkzeug:Press CTRL+C to quit`.\n\n7. Open up a second terminal and activate the same python environment.\n\n8. Navigate to the `/LOCALGPT/localGPTUI` directory.\n\n9. Run the command `python localGPTUI.py`.\n\n10. Open up a web browser and go the address `http://localhost:5111/`.\n\n\n# How to select different LLM models?\n\nTo change the models you will need to set both `MODEL_ID` and `MODEL_BASENAME`.\n\n1. Open up `constants.py` in the editor of your choice.\n2. Change the `MODEL_ID` and `MODEL_BASENAME`. If you are using a quantized model (`GGML`, `GPTQ`, `GGUF`), you will need to provide `MODEL_BASENAME`. For unquantized models, set `MODEL_BASENAME` to `NONE`\n5. There are a number of example models from HuggingFace that have already been tested to be run with the original trained model (ending with HF or have a .bin in its \"Files and versions\"), and quantized models (ending with GPTQ or have a .no-act-order or .safetensors in its \"Files and versions\").\n6. For models that end with HF or have a .bin inside its \"Files and versions\" on its HuggingFace page.\n\n   - Make sure you have a `MODEL_ID` selected. For example -> `MODEL_ID = \"TheBloke/guanaco-7B-HF\"`\n   - Go to the [HuggingFace Repo](https://huggingface.co/TheBloke/guanaco-7B-HF)\n\n7. For models that contain GPTQ in its name and or have a .no-act-order or .safetensors extension inside its \"Files and versions on its HuggingFace page.\n\n   - Make sure you have a `MODEL_ID` selected. For example -> model_id = `\"TheBloke/wizardLM-7B-GPTQ\"`\n   - Got to the corresponding [HuggingFace Repo](https://huggingface.co/TheBloke/wizardLM-7B-GPTQ) and select \"Files and versions\".\n   - Pick one of the model names and set it as  `MODEL_BASENAME`. For example -> `MODEL_BASENAME = \"wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors\"`\n\n8. Follow the same steps for `GGUF` and `GGML` models.\n\n# GPU and VRAM Requirements\n\nBelow is the VRAM requirement for different models depending on their size (Billions of parameters). The estimates in the table does not include VRAM used by the Embedding models - which use an additional 2GB-7GB of VRAM depending on the model.\n\n| Mode Size (B) | float32   | float16   | GPTQ 8bit      | GPTQ 4bit          |\n| ------- | --------- | --------- | -------------- | ------------------ |\n| 7B      | 28 GB     | 14 GB     | 7 GB - 9 GB    | 3.5 GB - 5 GB      |\n| 13B     | 52 GB     | 26 GB     | 13 GB - 15 GB  | 6.5 GB - 8 GB      |\n| 32B     | 130 GB    | 65 GB     | 32.5 GB - 35 GB| 16.25 GB - 19 GB   |\n| 65B     | 260.8 GB  | 130.4 GB  | 65.2 GB - 67 GB| 32.6 GB - 35 GB    |\n\n\n# System Requirements\n\n## Python Version\n\nTo use this software, you must have Python 3.10 or later installed. Earlier versions of Python will not compile.\n\n## C++ Compiler\n\nIf you encounter an error while building a wheel during the `pip install` process, you may need to install a C++ compiler on your computer.\n\n### For Windows 10/11\n\nTo install a C++ compiler on Windows 10/11, follow these steps:\n\n1. Install Visual Studio 2022.\n2. Make sure the following components are selected:\n   - Universal Windows Platform development\n   - C++ CMake tools for Windows\n3. Download the MinGW installer from the [MinGW website](https://sourceforge.net/projects/mingw/).\n4. Run the installer and select the \"gcc\" component.\n\n### NVIDIA Driver's Issues:\n\nFollow this [page](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04) to install NVIDIA Drivers.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=PromtEngineer/localGPT&type=Date)](https://star-history.com/#PromtEngineer/localGPT&Date)\n\n# Disclaimer\n\nThis is a test project to validate the feasibility of a fully local solution for question answering using LLMs and Vector embeddings. It is not production ready, and it is not meant to be used in production. Vicuna-7B is based on the Llama model so that has the original Llama license.\n\n# Common Errors\n\n - [Torch not compatible with CUDA enabled](https://github.com/pytorch/pytorch/issues/30664)\n\n   -  Get CUDA version\n      ```shell\n      nvcc --version\n      ```\n      ```shell\n      nvidia-smi\n      ```\n   - Try installing PyTorch depending on your CUDA version\n      ```shell\n         conda install -c pytorch torchvision cudatoolkit=10.1 pytorch\n      ```\n   - If it doesn't work, try reinstalling\n      ```shell\n         pip uninstall torch\n         pip cache purge\n         pip install torch -f https://download.pytorch.org/whl/torch_stable.html\n      ```\n\n- [ERROR: pip's dependency resolver does not currently take into account all the packages that are installed](https://stackoverflow.com/questions/72672196/error-pips-dependency-resolver-does-not-currently-take-into-account-all-the-pa/76604141#76604141)\n  ```shell\n     pip install h5py\n     pip install typing-extensions\n     pip install wheel\n  ```\n- [Failed to import transformers](https://github.com/huggingface/transformers/issues/11262)\n  - Try re-install\n    ```shell\n       conda uninstall tokenizers, transformers\n       pip install transformers\n    ```\n"
        },
        {
          "name": "SOURCE_DOCUMENTS",
          "type": "tree",
          "content": null
        },
        {
          "name": "constants.py",
          "type": "blob",
          "size": 7.90234375,
          "content": "import os\n\n# from dotenv import load_dotenv\nfrom chromadb.config import Settings\n\n# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/excel.html?highlight=xlsx#microsoft-excel\nfrom langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\nfrom langchain.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader\nfrom langchain.document_loaders import UnstructuredHTMLLoader\n\n\n# load_dotenv()\nROOT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))\n\n# Define the folder for storing database\nSOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n\nPERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\"\n\nMODELS_PATH = \"./models\"\n\n# Can be changed to a specific number\nINGEST_THREADS = os.cpu_count() or 8\n\n# Define the Chroma settings\nCHROMA_SETTINGS = Settings(\n    anonymized_telemetry=False,\n    is_persistent=True,\n)\n\n# Context Window and Max New Tokens\nCONTEXT_WINDOW_SIZE = 8096\nMAX_NEW_TOKENS = CONTEXT_WINDOW_SIZE  # int(CONTEXT_WINDOW_SIZE/4)\n\n#### If you get a \"not enough space in the buffer\" error, you should reduce the values below, start with half of the original values and keep halving the value until the error stops appearing\n\nN_GPU_LAYERS = 100  # Llama-2-70B has 83 layers\nN_BATCH = 512\n\n### From experimenting with the Llama-2-7B-Chat-GGML model on 8GB VRAM, these values work:\n# N_GPU_LAYERS = 20\n# N_BATCH = 512\n\n\n# https://python.langchain.com/en/latest/_modules/langchain/document_loaders/excel.html#UnstructuredExcelLoader\nDOCUMENT_MAP = {\n    \".html\": UnstructuredHTMLLoader,\n    \".txt\": TextLoader,\n    \".md\": UnstructuredMarkdownLoader,\n    \".py\": TextLoader,\n    # \".pdf\": PDFMinerLoader,\n    \".pdf\": UnstructuredFileLoader,\n    \".csv\": CSVLoader,\n    \".xls\": UnstructuredExcelLoader,\n    \".xlsx\": UnstructuredExcelLoader,\n    \".docx\": Docx2txtLoader,\n    \".doc\": Docx2txtLoader,\n}\n\n# Default Instructor Model\nEMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"  # Uses 1.5 GB of VRAM (High Accuracy with lower VRAM usage)\n\n####\n#### OTHER EMBEDDING MODEL OPTIONS\n####\n\n# EMBEDDING_MODEL_NAME = \"hkunlp/instructor-xl\" # Uses 5 GB of VRAM (Most Accurate of all models)\n# EMBEDDING_MODEL_NAME = \"intfloat/e5-large-v2\" # Uses 1.5 GB of VRAM (A little less accurate than instructor-large)\n# EMBEDDING_MODEL_NAME = \"intfloat/e5-base-v2\" # Uses 0.5 GB of VRAM (A good model for lower VRAM GPUs)\n# EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\" # Uses 0.2 GB of VRAM (Less accurate but fastest - only requires 150mb of vram)\n\n####\n#### MULTILINGUAL EMBEDDING MODELS\n####\n\n# EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large\" # Uses 2.5 GB of VRAM\n# EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-base\" # Uses 1.2 GB of VRAM\n\n\n#### SELECT AN OPEN SOURCE LLM (LARGE LANGUAGE MODEL)\n# Select the Model ID and model_basename\n# load the LLM for generating Natural Language responses\n\n#### GPU VRAM Memory required for LLM Models (ONLY) by Billion Parameter value (B Model)\n#### Does not include VRAM used by Embedding Models - which use an additional 2GB-7GB of VRAM depending on the model.\n####\n#### (B Model)   (float32)    (float16)    (GPTQ 8bit)         (GPTQ 4bit)\n####    7b         28 GB        14 GB       7 GB - 9 GB        3.5 GB - 5 GB\n####    13b        52 GB        26 GB       13 GB - 15 GB      6.5 GB - 8 GB\n####    32b        130 GB       65 GB       32.5 GB - 35 GB    16.25 GB - 19 GB\n####    65b        260.8 GB     130.4 GB    65.2 GB - 67 GB    32.6 GB -  - 35 GB\n\n# MODEL_ID = \"TheBloke/Llama-2-7B-Chat-GGML\"\n# MODEL_BASENAME = \"llama-2-7b-chat.ggmlv3.q4_0.bin\"\n\n####\n#### (FOR GGUF MODELS)\n####\n\n# MODEL_ID = \"TheBloke/Llama-2-13b-Chat-GGUF\"\n# MODEL_BASENAME = \"llama-2-13b-chat.Q4_K_M.gguf\"\n\n# MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n# MODEL_BASENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n\n# MODEL_ID = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\"\n# MODEL_BASENAME = \"Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\"\n\n# Use mistral to run on hpu\n# MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n# LLAMA 3 # use for Apple Silicon\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nMODEL_BASENAME = None\n\n# LLAMA 3 # use for NVIDIA GPUs\n# MODEL_ID = \"unsloth/llama-3-8b-bnb-4bit\"\n# MODEL_BASENAME = None\n\n# MODEL_ID = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n# MODEL_BASENAME = \"mistral-7b-instruct-v0.1.Q8_0.gguf\"\n\n# MODEL_ID = \"TheBloke/Llama-2-70b-Chat-GGUF\"\n# MODEL_BASENAME = \"llama-2-70b-chat.Q4_K_M.gguf\"\n\n####\n#### (FOR HF MODELS)\n####\n\n# MODEL_ID = \"NousResearch/Llama-2-7b-chat-hf\"\n# MODEL_BASENAME = None\n# MODEL_ID = \"TheBloke/vicuna-7B-1.1-HF\"\n# MODEL_BASENAME = None\n# MODEL_ID = \"TheBloke/Wizard-Vicuna-7B-Uncensored-HF\"\n# MODEL_ID = \"TheBloke/guanaco-7B-HF\"\n# MODEL_ID = 'NousResearch/Nous-Hermes-13b' # Requires ~ 23GB VRAM. Using STransformers\n# alongside will 100% create OOM on 24GB cards.\n# llm = load_model(device_type, model_id=model_id)\n\n####\n#### (FOR GPTQ QUANTIZED) Select a llm model based on your GPU and VRAM GB. Does not include Embedding Models VRAM usage.\n####\n\n##### 48GB VRAM Graphics Cards (RTX 6000, RTX A6000 and other 48GB VRAM GPUs) #####\n\n### 65b GPTQ LLM Models for 48GB GPUs (*** With best embedding model: hkunlp/instructor-xl ***)\n# MODEL_ID = \"TheBloke/guanaco-65B-GPTQ\"\n# MODEL_BASENAME = \"model.safetensors\"\n# MODEL_ID = \"TheBloke/Airoboros-65B-GPT4-2.0-GPTQ\"\n# MODEL_BASENAME = \"model.safetensors\"\n# MODEL_ID = \"TheBloke/gpt4-alpaca-lora_mlp-65B-GPTQ\"\n# MODEL_BASENAME = \"model.safetensors\"\n# MODEL_ID = \"TheBloke/Upstage-Llama1-65B-Instruct-GPTQ\"\n# MODEL_BASENAME = \"model.safetensors\"\n\n##### 24GB VRAM Graphics Cards (RTX 3090 - RTX 4090 (35% Faster) - RTX A5000 - RTX A5500) #####\n\n### 13b GPTQ Models for 24GB GPUs (*** With best embedding model: hkunlp/instructor-xl ***)\n# MODEL_ID = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n# MODEL_BASENAME = \"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n# MODEL_ID = \"TheBloke/vicuna-13B-v1.5-GPTQ\"\n# MODEL_BASENAME = \"model.safetensors\"\n# MODEL_ID = \"TheBloke/Nous-Hermes-13B-GPTQ\"\n# MODEL_BASENAME = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\n# MODEL_ID = \"TheBloke/WizardLM-13B-V1.2-GPTQ\"\n# MODEL_BASENAME = \"gptq_model-4bit-128g.safetensors\n\n### 30b GPTQ Models for 24GB GPUs (*** Requires using intfloat/e5-base-v2 instead of hkunlp/instructor-large as embedding model ***)\n# MODEL_ID = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n# MODEL_BASENAME = \"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit--1g.act.order.safetensors\"\n# MODEL_ID = \"TheBloke/WizardLM-30B-Uncensored-GPTQ\"\n# MODEL_BASENAME = \"WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors\"\n\n##### 8-10GB VRAM Graphics Cards (RTX 3080 - RTX 3080 Ti - RTX 3070 Ti - 3060 Ti - RTX 2000 Series, Quadro RTX 4000, 5000, 6000) #####\n### (*** Requires using intfloat/e5-small-v2 instead of hkunlp/instructor-large as embedding model ***)\n\n### 7b GPTQ Models for 8GB GPUs\n# MODEL_ID = \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\"\n# MODEL_BASENAME = \"Wizard-Vicuna-7B-Uncensored-GPTQ-4bit-128g.no-act.order.safetensors\"\n# MODEL_ID = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n# MODEL_BASENAME = \"WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n# MODEL_ID = \"TheBloke/wizardLM-7B-GPTQ\"\n# MODEL_BASENAME = \"wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors\"\n\n####\n#### (FOR GGML) (Quantized cpu+gpu+mps) models - check if they support llama.cpp\n####\n\n# MODEL_ID = \"TheBloke/wizard-vicuna-13B-GGML\"\n# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q4_0.bin\"\n# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q6_K.bin\"\n# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q2_K.bin\"\n# MODEL_ID = \"TheBloke/orca_mini_3B-GGML\"\n# MODEL_BASENAME = \"orca-mini-3b.ggmlv3.q4_0.bin\"\n\n####\n#### (FOR AWQ QUANTIZED) Select a llm model based on your GPU and VRAM GB. Does not include Embedding Models VRAM usage.\n### (*** MODEL_BASENAME is not actually used but have to contain .awq so the correct model loading is used ***)\n### (*** Compute capability 7.5 (sm75) and CUDA Toolkit 11.8+ are required ***)\n####\n# MODEL_ID = \"TheBloke/Llama-2-7B-Chat-AWQ\"\n# MODEL_BASENAME = \"model.safetensors.awq\"\n"
        },
        {
          "name": "crawl.py",
          "type": "blob",
          "size": 2.6044921875,
          "content": "import os\nimport shutil\nimport click\nimport subprocess\n\nfrom constants import (\n    DOCUMENT_MAP,\n    SOURCE_DIRECTORY\n)\n\ndef logToFile(logentry):\n   file1 = open(\"crawl.log\",\"a\")\n   file1.write(logentry + \"\\n\")\n   file1.close()\n   print(logentry + \"\\n\")\n\n@click.command()\n@click.option(\n    \"--device_type\",\n    default=\"cuda\",\n    type=click.Choice(\n        [\n            \"cpu\",\n            \"cuda\",\n            \"ipu\",\n            \"xpu\",\n            \"mkldnn\",\n            \"opengl\",\n            \"opencl\",\n            \"ideep\",\n            \"hip\",\n            \"ve\",\n            \"fpga\",\n            \"ort\",\n            \"xla\",\n            \"lazy\",\n            \"vulkan\",\n            \"mps\",\n            \"meta\",\n            \"hpu\",\n            \"mtia\",\n        ],\n    ),\n    help=\"Device to run on. (Default is cuda)\",\n)\n@click.option(\n    \"--landing_directory\",\n    default=\"./LANDING_DOCUMENTS\"\n)\n@click.option(\n    \"--processed_directory\",\n    default=\"./PROCESSED_DOCUMENTS\"\n)\n@click.option(\n    \"--error_directory\",\n    default=\"./ERROR_DOCUMENTS\"\n)\n@click.option(\n    \"--unsupported_directory\",\n    default=\"./UNSUPPORTED_DOCUMENTS\"\n)\n\ndef main(device_type, landing_directory, processed_directory, error_directory, unsupported_directory):\n    paths = []\n\n    os.makedirs(processed_directory, exist_ok=True)\n    os.makedirs(error_directory, exist_ok=True)\n    os.makedirs(unsupported_directory, exist_ok=True)\n\n    for root, _, files in os.walk(landing_directory):\n        for file_name in files:\n            file_extension = os.path.splitext(file_name)[1]\n            short_filename = os.path.basename(file_name)\n\n            if not os.path.isdir(root + \"/\" + file_name):\n               if file_extension in DOCUMENT_MAP.keys():\n                   shutil.move(root + \"/\" + file_name, SOURCE_DIRECTORY+ \"/\" + short_filename)\n                   logToFile(\"START: \" + root + \"/\" + short_filename)\n                   process = subprocess.Popen(\"python ingest.py --device_type=\" + device_type, shell=True, stdout=subprocess.PIPE)\n                   process.wait()\n                   if process.returncode > 0:\n                       shutil.move(SOURCE_DIRECTORY + \"/\" + short_filename, error_directory + \"/\" + short_filename)\n                       logToFile(\"ERROR: \" + root + \"/\" + short_filename)\n                   else:\n                       logToFile(\"VALID: \" + root + \"/\" + short_filename)\n                       shutil.move(SOURCE_DIRECTORY + \"/\" + short_filename, processed_directory+ \"/\" + short_filename)\n               else:\n                   shutil.move(root + \"/\" + file_name, unsupported_directory+ \"/\" + short_filename)\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "gaudi_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "ingest.py",
          "type": "blob",
          "size": 5.96875,
          "content": "import logging\nimport os\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n\nimport click\nimport torch\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import Language, RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom utils import get_embeddings\n\nfrom constants import (\n    CHROMA_SETTINGS,\n    DOCUMENT_MAP,\n    EMBEDDING_MODEL_NAME,\n    INGEST_THREADS,\n    PERSIST_DIRECTORY,\n    SOURCE_DIRECTORY,\n)\n\nimport nltk\nnltk.download('punkt_tab')\nnltk.download('averaged_perceptron_tagger_eng')\n\ndef file_log(logentry):\n    file1 = open(\"file_ingest.log\", \"a\")\n    file1.write(logentry + \"\\n\")\n    file1.close()\n    print(logentry + \"\\n\")\n\n\ndef load_single_document(file_path: str) -> Document:\n    # Loads a single document from a file path\n    try:\n        file_extension = os.path.splitext(file_path)[1]\n        loader_class = DOCUMENT_MAP.get(file_extension)\n        if loader_class:\n            file_log(file_path + \" loaded.\")\n            loader = loader_class(file_path)\n        else:\n            file_log(file_path + \" document type is undefined.\")\n            raise ValueError(\"Document type is undefined\")\n        return loader.load()[0]\n    except Exception as ex:\n        file_log(\"%s loading error: \\n%s\" % (file_path, ex))\n        return None\n\n\ndef load_document_batch(filepaths):\n    logging.info(\"Loading document batch\")\n    # create a thread pool\n    with ThreadPoolExecutor(len(filepaths)) as exe:\n        # load files\n        futures = [exe.submit(load_single_document, name) for name in filepaths]\n        # collect data\n        if futures is None:\n            file_log(name + \" failed to submit\")\n            return None\n        else:\n            data_list = [future.result() for future in futures]\n            # return data and file paths\n            return (data_list, filepaths)\n\n\ndef load_documents(source_dir: str) -> list[Document]:\n    # Loads all documents from the source documents directory, including nested folders\n    paths = []\n    for root, _, files in os.walk(source_dir):\n        for file_name in files:\n            print(\"Importing: \" + file_name)\n            file_extension = os.path.splitext(file_name)[1]\n            source_file_path = os.path.join(root, file_name)\n            if file_extension in DOCUMENT_MAP.keys():\n                paths.append(source_file_path)\n\n    # Have at least one worker and at most INGEST_THREADS workers\n    n_workers = min(INGEST_THREADS, max(len(paths), 1))\n    chunksize = round(len(paths) / n_workers)\n    docs = []\n    with ProcessPoolExecutor(n_workers) as executor:\n        futures = []\n        # split the load operations into chunks\n        for i in range(0, len(paths), chunksize):\n            # select a chunk of filenames\n            filepaths = paths[i : (i + chunksize)]\n            # submit the task\n            try:\n                future = executor.submit(load_document_batch, filepaths)\n            except Exception as ex:\n                file_log(\"executor task failed: %s\" % (ex))\n                future = None\n            if future is not None:\n                futures.append(future)\n        # process all results\n        for future in as_completed(futures):\n            # open the file and load the data\n            try:\n                contents, _ = future.result()\n                docs.extend(contents)\n            except Exception as ex:\n                file_log(\"Exception: %s\" % (ex))\n\n    return docs\n\n\ndef split_documents(documents: list[Document]) -> tuple[list[Document], list[Document]]:\n    # Splits documents for correct Text Splitter\n    text_docs, python_docs = [], []\n    for doc in documents:\n        if doc is not None:\n            file_extension = os.path.splitext(doc.metadata[\"source\"])[1]\n            if file_extension == \".py\":\n                python_docs.append(doc)\n            else:\n                text_docs.append(doc)\n    return text_docs, python_docs\n\n\n@click.command()\n@click.option(\n    \"--device_type\",\n    default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    type=click.Choice(\n        [\n            \"cpu\",\n            \"cuda\",\n            \"ipu\",\n            \"xpu\",\n            \"mkldnn\",\n            \"opengl\",\n            \"opencl\",\n            \"ideep\",\n            \"hip\",\n            \"ve\",\n            \"fpga\",\n            \"ort\",\n            \"xla\",\n            \"lazy\",\n            \"vulkan\",\n            \"mps\",\n            \"meta\",\n            \"hpu\",\n            \"mtia\",\n        ],\n    ),\n    help=\"Device to run on. (Default is cuda)\",\n)\ndef main(device_type):\n    # Load documents and split in chunks\n    logging.info(f\"Loading documents from {SOURCE_DIRECTORY}\")\n    documents = load_documents(SOURCE_DIRECTORY)\n    text_documents, python_documents = split_documents(documents)\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n    python_splitter = RecursiveCharacterTextSplitter.from_language(\n        language=Language.PYTHON, chunk_size=880, chunk_overlap=200\n    )\n    texts = text_splitter.split_documents(text_documents)\n    texts.extend(python_splitter.split_documents(python_documents))\n    logging.info(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")\n    logging.info(f\"Split into {len(texts)} chunks of text\")\n\n    \"\"\"\n    (1) Chooses an appropriate langchain library based on the enbedding model name.  Matching code is contained within fun_localGPT.py.\n    \n    (2) Provides additional arguments for instructor and BGE models to improve results, pursuant to the instructions contained on\n    their respective huggingface repository, project page or github repository.\n    \"\"\"\n\n    embeddings = get_embeddings(device_type)\n\n    logging.info(f\"Loaded embeddings from {EMBEDDING_MODEL_NAME}\")\n\n    db = Chroma.from_documents(\n        texts,\n        embeddings,\n        persist_directory=PERSIST_DIRECTORY,\n        client_settings=CHROMA_SETTINGS,\n    )\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s\", level=logging.INFO\n    )\n    main()\n"
        },
        {
          "name": "load_models.py",
          "type": "blob",
          "size": 8.2724609375,
          "content": "import sys\n\nimport torch\n\nif sys.platform != \"darwin\":\n    from auto_gptq import AutoGPTQForCausalLM\n\nfrom huggingface_hub import hf_hub_download\nfrom langchain.llms import LlamaCpp\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n\nfrom constants import CONTEXT_WINDOW_SIZE, MAX_NEW_TOKENS, MODELS_PATH, N_BATCH, N_GPU_LAYERS\n\n\ndef load_quantized_model_gguf_ggml(model_id, model_basename, device_type, logging):\n    \"\"\"\n    Load a GGUF/GGML quantized model using LlamaCpp.\n\n    This function attempts to load a GGUF/GGML quantized model using the LlamaCpp library.\n    If the model is of type GGML, and newer version of LLAMA-CPP is used which does not support GGML,\n    it logs a message indicating that LLAMA-CPP has dropped support for GGML.\n\n    Parameters:\n    - model_id (str): The identifier for the model on HuggingFace Hub.\n    - model_basename (str): The base name of the model file.\n    - device_type (str): The type of device where the model will run, e.g., 'mps', 'cuda', etc.\n    - logging (logging.Logger): Logger instance for logging messages.\n\n    Returns:\n    - LlamaCpp: An instance of the LlamaCpp model if successful, otherwise None.\n\n    Notes:\n    - The function uses the `hf_hub_download` function to download the model from the HuggingFace Hub.\n    - The number of GPU layers is set based on the device type.\n    \"\"\"\n\n    try:\n        logging.info(\"Using Llamacpp for GGUF/GGML quantized models\")\n        model_path = hf_hub_download(\n            repo_id=model_id,\n            filename=model_basename,\n            resume_download=True,\n            cache_dir=MODELS_PATH,\n        )\n        kwargs = {\n            \"model_path\": model_path,\n            \"n_ctx\": CONTEXT_WINDOW_SIZE,\n            \"max_tokens\": MAX_NEW_TOKENS,\n            \"n_batch\": N_BATCH,  # set this based on your GPU & CPU RAM\n        }\n        if device_type.lower() == \"mps\":\n            kwargs[\"n_gpu_layers\"] = 1\n        if device_type.lower() == \"cuda\":\n            kwargs[\"n_gpu_layers\"] = N_GPU_LAYERS  # set this based on your GPU\n\n        return LlamaCpp(**kwargs)\n    except TypeError:\n        if \"ggml\" in model_basename:\n            logging.INFO(\"If you were using GGML model, LLAMA-CPP Dropped Support, Use GGUF Instead\")\n        return None\n\n\ndef load_quantized_model_qptq(model_id, model_basename, device_type, logging):\n    \"\"\"\n    Load a GPTQ quantized model using AutoGPTQForCausalLM.\n\n    This function loads a quantized model that ends with GPTQ and may have variations\n    of .no-act.order or .safetensors in their HuggingFace repo.\n    It will not work for Macs, as AutoGPTQ only supports Linux and Windows:\n    - Nvidia CUDA (Windows and Linux)\n    - AMD ROCm (Linux only)\n    - CPU QiGen (Linux only, new and experimental)\n\n    Parameters:\n    - model_id (str): The identifier for the model on HuggingFace Hub.\n    - model_basename (str): The base name of the model file.\n    - device_type (str): The type of device where the model will run.\n    - logging (logging.Logger): Logger instance for logging messages.\n\n    Returns:\n    - model (AutoGPTQForCausalLM): The loaded quantized model.\n    - tokenizer (AutoTokenizer): The tokenizer associated with the model.\n\n    Notes:\n    - The function checks for the \".safetensors\" ending in the model_basename and removes it if present.\n    \"\"\"\n\n    if sys.platform == \"darwin\":\n        logging.INFO(\"GPTQ models will NOT work on Mac devices. Please choose a different model.\")\n        return None, None\n\n    # The code supports all huggingface models that ends with GPTQ and have some variation\n    # of .no-act.order or .safetensors in their HF repo.\n    logging.info(\"Using AutoGPTQForCausalLM for quantized models\")\n\n    if \".safetensors\" in model_basename:\n        # Remove the \".safetensors\" ending if present\n        model_basename = model_basename.replace(\".safetensors\", \"\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    logging.info(\"Tokenizer loaded\")\n\n    model = AutoGPTQForCausalLM.from_quantized(\n        model_id,\n        model_basename=model_basename,\n        use_safetensors=True,\n        trust_remote_code=True,\n        device_map=\"auto\",\n        use_triton=False,\n        quantize_config=None,\n    )\n    return model, tokenizer\n\n\ndef load_full_model(model_id, model_basename, device_type, logging):\n    \"\"\"\n    Load a full model using either LlamaTokenizer or AutoModelForCausalLM.\n\n    This function loads a full model based on the specified device type.\n    If the device type is 'mps' or 'cpu', it uses LlamaTokenizer and LlamaForCausalLM.\n    Otherwise, it uses AutoModelForCausalLM.\n\n    Parameters:\n    - model_id (str): The identifier for the model on HuggingFace Hub.\n    - model_basename (str): The base name of the model file.\n    - device_type (str): The type of device where the model will run.\n    - logging (logging.Logger): Logger instance for logging messages.\n\n    Returns:\n    - model (Union[LlamaForCausalLM, AutoModelForCausalLM]): The loaded model.\n    - tokenizer (Union[LlamaTokenizer, AutoTokenizer]): The tokenizer associated with the model.\n\n    Notes:\n    - The function uses the `from_pretrained` method to load both the model and the tokenizer.\n    - Additional settings are provided for NVIDIA GPUs, such as loading in 4-bit and setting the compute dtype.\n    \"\"\"\n\n    if device_type.lower() in [\"mps\", \"cpu\", \"hpu\"]:\n        logging.info(\"Using AutoModelForCausalLM\")\n        # tokenizer = LlamaTokenizer.from_pretrained(model_id, cache_dir=\"./models/\")\n        # model = LlamaForCausalLM.from_pretrained(model_id, cache_dir=\"./models/\")\n\n        model = AutoModelForCausalLM.from_pretrained(model_id,\n                                            #  quantization_config=quantization_config,\n                                            #  low_cpu_mem_usage=True,\n                                            #  torch_dtype=\"auto\",\n                                             torch_dtype=torch.bfloat16,\n                                             device_map=\"auto\",\n                                             cache_dir=\"./models/\")\n\n        tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"./models/\")\n    else:\n        logging.info(\"Using AutoModelForCausalLM for full models\")\n        tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"./models/\")\n        logging.info(\"Tokenizer loaded\")\n        bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.float16\n                )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            device_map=\"auto\",\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            cache_dir=MODELS_PATH,\n            trust_remote_code=True,  # set these if you are using NVIDIA GPU\n            quantization_config=bnb_config\n           # load_in_4bit=True,\n           # bnb_4bit_quant_type=\"nf4\",\n           # bnb_4bit_compute_dtype=torch.float16,\n           # max_memory={0: \"15GB\"},  # Uncomment this line with you encounter CUDA out of memory errors\n        )\n\n        model.tie_weights()\n    return model, tokenizer\n\n\ndef load_quantized_model_awq(model_id, logging):\n    \"\"\"\n    Load a AWQ quantized model using AutoModelForCausalLM.\n\n    This function loads a quantized model that ends with AWQ.\n    It will not work for Macs as AutoAWQ currently only supports Nvidia GPUs.\n\n    Parameters:\n    - model_id (str): The identifier for the model on HuggingFace Hub.\n    - logging (logging.Logger): Logger instance for logging messages.\n\n    Returns:\n    - model (AutoModelForCausalLM): The loaded quantized model.\n    - tokenizer (AutoTokenizer): The tokenizer associated with the model.\n\n    \"\"\"\n\n    if sys.platform == \"darwin\":\n        logging.INFO(\"AWQ models will NOT work on Mac devices. Please choose a different model.\")\n        return None, None\n\n    # The code supports all huggingface models that ends with AWQ.\n    logging.info(\"Using AutoModelForCausalLM for AWQ quantized models\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    logging.info(\"Tokenizer loaded\")\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        use_safetensors=True,\n        trust_remote_code=True,\n        device_map=\"auto\",\n    )\n    return model, tokenizer\n"
        },
        {
          "name": "localGPTUI",
          "type": "tree",
          "content": null
        },
        {
          "name": "localGPT_UI.py",
          "type": "blob",
          "size": 3.8603515625,
          "content": "import torch\nimport subprocess\nimport streamlit as st\nfrom run_localGPT import load_model\nfrom langchain.vectorstores import Chroma\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom streamlit_extras.add_vertical_space import add_vertical_space\nfrom langchain.prompts import PromptTemplate\nfrom langchain.memory import ConversationBufferMemory\n\n\ndef model_memory():\n    # Adding history to the model.\n    template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer,\\\n    just say that you don't know, don't try to make up an answer.\n\n    {context}\n\n    {history}\n    Question: {question}\n    Helpful Answer:\"\"\"\n\n    prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\n    memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n\n    return prompt, memory\n\n\n# Sidebar contents\nwith st.sidebar:\n    st.title(\"🤗💬 Converse with your Data\")\n    st.markdown(\n        \"\"\"\n    ## About\n    This app is an LLM-powered chatbot built using:\n    - [Streamlit](https://streamlit.io/)\n    - [LangChain](https://python.langchain.com/)\n    - [LocalGPT](https://github.com/PromtEngineer/localGPT) \n \n    \"\"\"\n    )\n    add_vertical_space(5)\n    st.write(\"Made with ❤️ by [Prompt Engineer](https://youtube.com/@engineerprompt)\")\n\n\nif torch.backends.mps.is_available():\n    DEVICE_TYPE = \"mps\"\nelif torch.cuda.is_available():\n    DEVICE_TYPE = \"cuda\"\nelse:\n    DEVICE_TYPE = \"cpu\"\n\n\n# if \"result\" not in st.session_state:\n#     # Run the document ingestion process.\n#     run_langest_commands = [\"python\", \"ingest.py\"]\n#     run_langest_commands.append(\"--device_type\")\n#     run_langest_commands.append(DEVICE_TYPE)\n\n#     result = subprocess.run(run_langest_commands, capture_output=True)\n#     st.session_state.result = result\n\n# Define the retreiver\n# load the vectorstore\nif \"EMBEDDINGS\" not in st.session_state:\n    EMBEDDINGS = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": DEVICE_TYPE})\n    st.session_state.EMBEDDINGS = EMBEDDINGS\n\nif \"DB\" not in st.session_state:\n    DB = Chroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=st.session_state.EMBEDDINGS,\n        client_settings=CHROMA_SETTINGS,\n    )\n    st.session_state.DB = DB\n\nif \"RETRIEVER\" not in st.session_state:\n    RETRIEVER = DB.as_retriever()\n    st.session_state.RETRIEVER = RETRIEVER\n\nif \"LLM\" not in st.session_state:\n    LLM = load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n    st.session_state[\"LLM\"] = LLM\n\n\nif \"QA\" not in st.session_state:\n    prompt, memory = model_memory()\n\n    QA = RetrievalQA.from_chain_type(\n        llm=LLM,\n        chain_type=\"stuff\",\n        retriever=RETRIEVER,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    st.session_state[\"QA\"] = QA\n\nst.title(\"LocalGPT App 💬\")\n# Create a text input box for the user\nprompt = st.text_input(\"Input your prompt here\")\n# while True:\n\n# If the user hits enter\nif prompt:\n    # Then pass the prompt to the LLM\n    response = st.session_state[\"QA\"](prompt)\n    answer, docs = response[\"result\"], response[\"source_documents\"]\n    # ...and write it out to the screen\n    st.write(answer)\n\n    # With a streamlit expander\n    with st.expander(\"Document Similarity Search\"):\n        # Find the relevant pages\n        search = st.session_state.DB.similarity_search_with_score(prompt)\n        # Write out the first\n        for i, doc in enumerate(search):\n            # print(doc)\n            st.write(f\"Source Document # {i+1} : {doc[0].metadata['source'].split('/')[-1]}\")\n            st.write(doc[0].page_content)\n            st.write(\"--------------------------------\")\n"
        },
        {
          "name": "prompt_template_utils.py",
          "type": "blob",
          "size": 4.1552734375,
          "content": "\"\"\"\nThis file implements prompt template for llama based models. \nModify the prompt template based on the model you select. \nThis seems to have significant impact on the output of the LLM.\n\"\"\"\n\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\n\n# this is specific to Llama-2.\n\nsystem_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\nRead the given context before answering questions and think step by step. If you can not answer a user question based on \nthe provided context, inform the user. Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\"\n\n\ndef get_prompt_template(system_prompt=system_prompt, promptTemplate_type=None, history=False):\n    if promptTemplate_type == \"llama\":\n        B_INST, E_INST = \"[INST]\", \"[/INST]\"\n        B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n        SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n        if history:\n            instruction = \"\"\"\n            Context: {history} \\n {context}\n            User: {question}\"\"\"\n\n            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n            prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=prompt_template)\n        else:\n            instruction = \"\"\"\n            Context: {context}\n            User: {question}\"\"\"\n\n            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n            prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n\n    elif promptTemplate_type == \"llama3\":\n\n        B_INST, E_INST = \"<|start_header_id|>user<|end_header_id|>\", \"<|eot_id|>\"\n        B_SYS, E_SYS = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \", \"<|eot_id|>\"\n        ASSISTANT_INST = \"<|start_header_id|>assistant<|end_header_id|>\"\n        SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n        if history:\n            instruction = \"\"\"\n            Context: {history} \\n {context}\n            User: {question}\"\"\"\n\n            prompt_template = SYSTEM_PROMPT + B_INST + instruction + ASSISTANT_INST\n            prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=prompt_template)\n        else:\n            instruction = \"\"\"\n            Context: {context}\n            User: {question}\"\"\"\n\n            prompt_template = SYSTEM_PROMPT + B_INST + instruction + ASSISTANT_INST\n            prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n\n    elif promptTemplate_type == \"mistral\":\n        B_INST, E_INST = \"<s>[INST] \", \" [/INST]\"\n        if history:\n            prompt_template = (\n                B_INST\n                + system_prompt\n                + \"\"\"\n    \n            Context: {history} \\n {context}\n            User: {question}\"\"\"\n                + E_INST\n            )\n            prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=prompt_template)\n        else:\n            prompt_template = (\n                B_INST\n                + system_prompt\n                + \"\"\"\n            \n            Context: {context}\n            User: {question}\"\"\"\n                + E_INST\n            )\n            prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n    else:\n        # change this based on the model you have selected.\n        if history:\n            prompt_template = (\n                system_prompt\n                + \"\"\"\n    \n            Context: {history} \\n {context}\n            User: {question}\n            Answer:\"\"\"\n            )\n            prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=prompt_template)\n        else:\n            prompt_template = (\n                system_prompt\n                + \"\"\"\n            \n            Context: {context}\n            User: {question}\n            Answer:\"\"\"\n            )\n            prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n\n    memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n\n    print(f\"Here is the prompt used: {prompt}\")\n\n    return (\n        prompt,\n        memory,\n    )\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.205078125,
          "content": "# ==== black ====\n[tool.black]\nline-length = 119\ntarget-version = ['py311']\n\n\n# ==== isort ====\n[tool.isort]\nprofile = \"black\"\nline_length = 119\nknown_first_party = [\n    \"tests\",\n    \"scripts\",\n    \"hooks\",\n]\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.7001953125,
          "content": "# Natural Language Processing\nlangchain==0.0.267\nchromadb==0.4.6\npdfminer.six==20221105\nInstructorEmbedding\nsentence-transformers==2.2.2\nfaiss-cpu\nhuggingface_hub\ntransformers\nautoawq; sys_platform != 'darwin'\nprotobuf==3.20.2; sys_platform != 'darwin'\nprotobuf==3.20.2; sys_platform == 'darwin' and platform_machine != 'arm64'\nprotobuf==3.20.3; sys_platform == 'darwin' and platform_machine == 'arm64'\nauto-gptq==0.6.0; sys_platform != 'darwin'\ndocx2txt\nunstructured\nunstructured[pdf]\n\n# Utilities\nurllib3==1.26.6\naccelerate\nbitsandbytes ; sys_platform != 'win32'\nbitsandbytes-windows ; sys_platform == 'win32'\nclick\nflask\nrequests\n\n# Streamlit related\nstreamlit\nStreamlit-extras\n\n# Excel File Manipulation\nopenpyxl\n"
        },
        {
          "name": "run_localGPT.py",
          "type": "blob",
          "size": 10.6240234375,
          "content": "import os\nimport logging\nimport click\nimport torch\nimport utils\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\nfrom langchain.callbacks.manager import CallbackManager\n\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\nfrom prompt_template_utils import get_prompt_template\nfrom utils import get_embeddings\n\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    GenerationConfig,\n    pipeline,\n)\n\nfrom load_models import (\n    load_quantized_model_awq,\n    load_quantized_model_gguf_ggml,\n    load_quantized_model_qptq,\n    load_full_model,\n)\n\nfrom constants import (\n    EMBEDDING_MODEL_NAME,\n    PERSIST_DIRECTORY,\n    MODEL_ID,\n    MODEL_BASENAME,\n    MAX_NEW_TOKENS,\n    MODELS_PATH,\n    CHROMA_SETTINGS,    \n)\n\n\ndef load_model(device_type, model_id, model_basename=None, LOGGING=logging):\n    \"\"\"\n    Select a model for text generation using the HuggingFace library.\n    If you are running this for the first time, it will download a model for you.\n    subsequent runs will use the model from the disk.\n\n    Args:\n        device_type (str): Type of device to use, e.g., \"cuda\" for GPU or \"cpu\" for CPU.\n        model_id (str): Identifier of the model to load from HuggingFace's model hub.\n        model_basename (str, optional): Basename of the model if using quantized models.\n            Defaults to None.\n\n    Returns:\n        HuggingFacePipeline: A pipeline object for text generation using the loaded model.\n\n    Raises:\n        ValueError: If an unsupported model or device type is provided.\n    \"\"\"\n    logging.info(f\"Loading Model: {model_id}, on: {device_type}\")\n    logging.info(\"This action can take a few minutes!\")\n    \n    if model_basename is not None:\n        if \".gguf\" in model_basename.lower():\n            llm = load_quantized_model_gguf_ggml(model_id, model_basename, device_type, LOGGING)\n            return llm\n        elif \".ggml\" in model_basename.lower():\n            model, tokenizer = load_quantized_model_gguf_ggml(model_id, model_basename, device_type, LOGGING)\n        elif \".awq\" in model_basename.lower():\n            model, tokenizer = load_quantized_model_awq(model_id, LOGGING)\n        else:\n            model, tokenizer = load_quantized_model_qptq(model_id, model_basename, device_type, LOGGING)\n    else:\n        model, tokenizer = load_full_model(model_id, model_basename, device_type, LOGGING)\n\n    # Load configuration from the model to avoid warnings\n    generation_config = GenerationConfig.from_pretrained(model_id)\n    # see here for details:\n    # https://huggingface.co/docs/transformers/\n    # main_classes/text_generation#transformers.GenerationConfig.from_pretrained.returns\n\n    # Create a pipeline for text generation\n    if device_type == \"hpu\":\n        from gaudi_utils.pipeline import GaudiTextGenerationPipeline\n\n        pipe = GaudiTextGenerationPipeline(\n            model_name_or_path=model_id,\n            max_new_tokens=1000,\n            temperature=0.2,\n            top_p=0.95,\n            repetition_penalty=1.15,\n            do_sample=True,\n            max_padding_length=5000,\n        )\n        pipe.compile_graph()\n    else:\n        pipe = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_length=MAX_NEW_TOKENS,\n        temperature=0.2,\n        # top_p=0.95,\n        repetition_penalty=1.15,\n        generation_config=generation_config,\n    )\n\n    local_llm = HuggingFacePipeline(pipeline=pipe)\n    logging.info(\"Local LLM Loaded\")\n\n    return local_llm\n\n\ndef retrieval_qa_pipline(device_type, use_history, promptTemplate_type=\"llama\"):\n    \"\"\"\n    Initializes and returns a retrieval-based Question Answering (QA) pipeline.\n\n    This function sets up a QA system that retrieves relevant information using embeddings\n    from the HuggingFace library. It then answers questions based on the retrieved information.\n\n    Parameters:\n    - device_type (str): Specifies the type of device where the model will run, e.g., 'cpu', 'cuda', etc.\n    - use_history (bool): Flag to determine whether to use chat history or not.\n\n    Returns:\n    - RetrievalQA: An initialized retrieval-based QA system.\n\n    Notes:\n    - The function uses embeddings from the HuggingFace library, either instruction-based or regular.\n    - The Chroma class is used to load a vector store containing pre-computed embeddings.\n    - The retriever fetches relevant documents or data based on a query.\n    - The prompt and memory, obtained from the `get_prompt_template` function, might be used in the QA system.\n    - The model is loaded onto the specified device using its ID and basename.\n    - The QA system retrieves relevant documents using the retriever and then answers questions based on those documents.\n    \"\"\"\n\n    \"\"\"\n    (1) Chooses an appropriate langchain library based on the enbedding model name.  Matching code is contained within ingest.py.\n\n    (2) Provides additional arguments for instructor and BGE models to improve results, pursuant to the instructions contained on\n    their respective huggingface repository, project page or github repository.\n    \"\"\"\n    if device_type == \"hpu\":\n        from gaudi_utils.embeddings import load_embeddings\n\n        embeddings = load_embeddings()\n    else:\n        embeddings = get_embeddings(device_type)\n\n    logging.info(f\"Loaded embeddings from {EMBEDDING_MODEL_NAME}\")\n\n    # load the vectorstore\n    db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n    retriever = db.as_retriever()\n\n    # get the prompt template and memory if set by the user.\n    prompt, memory = get_prompt_template(promptTemplate_type=promptTemplate_type, history=use_history)\n\n    # load the llm pipeline\n    llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME, LOGGING=logging)\n\n    if use_history:\n        qa = RetrievalQA.from_chain_type(\n            llm=llm,\n            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n            retriever=retriever,\n            return_source_documents=True,  # verbose=True,\n            callbacks=callback_manager,\n            chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n        )\n    else:\n        qa = RetrievalQA.from_chain_type(\n            llm=llm,\n            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n            retriever=retriever,\n            return_source_documents=True,  # verbose=True,\n            callbacks=callback_manager,\n            chain_type_kwargs={\n                \"prompt\": prompt,\n            },\n        )\n\n    return qa\n\n\n# chose device typ to run on as well as to show source documents.\n@click.command()\n@click.option(\n    \"--device_type\",\n    default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    type=click.Choice(\n        [\n            \"cpu\",\n            \"cuda\",\n            \"ipu\",\n            \"xpu\",\n            \"mkldnn\",\n            \"opengl\",\n            \"opencl\",\n            \"ideep\",\n            \"hip\",\n            \"ve\",\n            \"fpga\",\n            \"ort\",\n            \"xla\",\n            \"lazy\",\n            \"vulkan\",\n            \"mps\",\n            \"meta\",\n            \"hpu\",\n            \"mtia\",\n        ],\n    ),\n    help=\"Device to run on. (Default is cuda)\",\n)\n@click.option(\n    \"--show_sources\",\n    \"-s\",\n    is_flag=True,\n    help=\"Show sources along with answers (Default is False)\",\n)\n@click.option(\n    \"--use_history\",\n    \"-h\",\n    is_flag=True,\n    help=\"Use history (Default is False)\",\n)\n@click.option(\n    \"--model_type\",\n    default=\"llama3\",\n    type=click.Choice(\n        [\"llama3\", \"llama\", \"mistral\", \"non_llama\"],\n    ),\n    help=\"model type, llama3, llama, mistral or non_llama\",\n)\n@click.option(\n    \"--save_qa\",\n    is_flag=True,\n    help=\"whether to save Q&A pairs to a CSV file (Default is False)\",\n)\ndef main(device_type, show_sources, use_history, model_type, save_qa):\n    \"\"\"\n    Implements the main information retrieval task for a localGPT.\n\n    This function sets up the QA system by loading the necessary embeddings, vectorstore, and LLM model.\n    It then enters an interactive loop where the user can input queries and receive answers. Optionally,\n    the source documents used to derive the answers can also be displayed.\n\n    Parameters:\n    - device_type (str): Specifies the type of device where the model will run, e.g., 'cpu', 'mps', 'cuda', etc.\n    - show_sources (bool): Flag to determine whether to display the source documents used for answering.\n    - use_history (bool): Flag to determine whether to use chat history or not.\n\n    Notes:\n    - Logging information includes the device type, whether source documents are displayed, and the use of history.\n    - If the models directory does not exist, it creates a new one to store models.\n    - The user can exit the interactive loop by entering \"exit\".\n    - The source documents are displayed if the show_sources flag is set to True.\n\n    \"\"\"\n\n    logging.info(f\"Running on: {device_type}\")\n    logging.info(f\"Display Source Documents set to: {show_sources}\")\n    logging.info(f\"Use history set to: {use_history}\")\n\n    # check if models directory do not exist, create a new one and store models here.\n    if not os.path.exists(MODELS_PATH):\n        os.mkdir(MODELS_PATH)\n\n    qa = retrieval_qa_pipline(device_type, use_history, promptTemplate_type=model_type)\n    # Interactive questions and answers\n    while True:\n        query = input(\"\\nEnter a query: \")\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n            for document in docs:\n                print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n                print(document.page_content)\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n\n        # Log the Q&A to CSV only if save_qa is True\n        if save_qa:\n            utils.log_to_csv(query, answer)\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s\", level=logging.INFO\n    )\n    main()\n"
        },
        {
          "name": "run_localGPT_API.py",
          "type": "blob",
          "size": 6.490234375,
          "content": "import logging\nimport os\nimport shutil\nimport subprocess\nimport argparse\n\nimport torch\nfrom flask import Flask, jsonify, request\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\n\n# from langchain.embeddings import HuggingFaceEmbeddings\nfrom run_localGPT import load_model\nfrom prompt_template_utils import get_prompt_template\n\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom werkzeug.utils import secure_filename\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME\n\n# API queue addition\nfrom threading import Lock\n\nrequest_lock = Lock()\n\n\nif torch.backends.mps.is_available():\n    DEVICE_TYPE = \"mps\"\nelif torch.cuda.is_available():\n    DEVICE_TYPE = \"cuda\"\nelse:\n    DEVICE_TYPE = \"cpu\"\n\nSHOW_SOURCES = True\nlogging.info(f\"Running on: {DEVICE_TYPE}\")\nlogging.info(f\"Display Source Documents set to: {SHOW_SOURCES}\")\n\nEMBEDDINGS = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": DEVICE_TYPE})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# EMBEDDINGS = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n# if os.path.exists(PERSIST_DIRECTORY):\n#     try:\n#         shutil.rmtree(PERSIST_DIRECTORY)\n#     except OSError as e:\n#         print(f\"Error: {e.filename} - {e.strerror}.\")\n# else:\n#     print(\"The directory does not exist\")\n\n# run_langest_commands = [\"python\", \"ingest.py\"]\n# if DEVICE_TYPE == \"cpu\":\n#     run_langest_commands.append(\"--device_type\")\n#     run_langest_commands.append(DEVICE_TYPE)\n\n# result = subprocess.run(run_langest_commands, capture_output=True)\n# if result.returncode != 0:\n#     raise FileNotFoundError(\n#         \"No files were found inside SOURCE_DOCUMENTS, please put a starter file inside before starting the API!\"\n#     )\n\n# load the vectorstore\nDB = Chroma(\n    persist_directory=PERSIST_DIRECTORY,\n    embedding_function=EMBEDDINGS,\n    client_settings=CHROMA_SETTINGS,\n)\n\nRETRIEVER = DB.as_retriever()\n\nLLM = load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nprompt, memory = get_prompt_template(promptTemplate_type=\"llama\", history=False)\n\nQA = RetrievalQA.from_chain_type(\n    llm=LLM,\n    chain_type=\"stuff\",\n    retriever=RETRIEVER,\n    return_source_documents=SHOW_SOURCES,\n    chain_type_kwargs={\n        \"prompt\": prompt,\n    },\n)\n\napp = Flask(__name__)\n\n\n@app.route(\"/api/delete_source\", methods=[\"GET\"])\ndef delete_source_route():\n    folder_name = \"SOURCE_DOCUMENTS\"\n\n    if os.path.exists(folder_name):\n        shutil.rmtree(folder_name)\n\n    os.makedirs(folder_name)\n\n    return jsonify({\"message\": f\"Folder '{folder_name}' successfully deleted and recreated.\"})\n\n\n@app.route(\"/api/save_document\", methods=[\"GET\", \"POST\"])\ndef save_document_route():\n    if \"document\" not in request.files:\n        return \"No document part\", 400\n    file = request.files[\"document\"]\n    if file.filename == \"\":\n        return \"No selected file\", 400\n    if file:\n        filename = secure_filename(file.filename)\n        folder_path = \"SOURCE_DOCUMENTS\"\n        if not os.path.exists(folder_path):\n            os.makedirs(folder_path)\n        file_path = os.path.join(folder_path, filename)\n        file.save(file_path)\n        return \"File saved successfully\", 200\n\n\n@app.route(\"/api/run_ingest\", methods=[\"GET\"])\ndef run_ingest_route():\n    global DB\n    global RETRIEVER\n    global QA\n    try:\n        if os.path.exists(PERSIST_DIRECTORY):\n            try:\n                shutil.rmtree(PERSIST_DIRECTORY)\n            except OSError as e:\n                print(f\"Error: {e.filename} - {e.strerror}.\")\n        else:\n            print(\"The directory does not exist\")\n\n        run_langest_commands = [\"python\", \"ingest.py\"]\n        if DEVICE_TYPE == \"cpu\":\n            run_langest_commands.append(\"--device_type\")\n            run_langest_commands.append(DEVICE_TYPE)\n\n        result = subprocess.run(run_langest_commands, capture_output=True)\n        if result.returncode != 0:\n            return \"Script execution failed: {}\".format(result.stderr.decode(\"utf-8\")), 500\n        # load the vectorstore\n        DB = Chroma(\n            persist_directory=PERSIST_DIRECTORY,\n            embedding_function=EMBEDDINGS,\n            client_settings=CHROMA_SETTINGS,\n        )\n        RETRIEVER = DB.as_retriever()\n        prompt, memory = get_prompt_template(promptTemplate_type=\"llama\", history=False)\n\n        QA = RetrievalQA.from_chain_type(\n            llm=LLM,\n            chain_type=\"stuff\",\n            retriever=RETRIEVER,\n            return_source_documents=SHOW_SOURCES,\n            chain_type_kwargs={\n                \"prompt\": prompt,\n            },\n        )\n        return \"Script executed successfully: {}\".format(result.stdout.decode(\"utf-8\")), 200\n    except Exception as e:\n        return f\"Error occurred: {str(e)}\", 500\n\n\n@app.route(\"/api/prompt_route\", methods=[\"GET\", \"POST\"])\ndef prompt_route():\n    global QA\n    global request_lock  # Make sure to use the global lock instance\n    user_prompt = request.form.get(\"user_prompt\")\n    if user_prompt:\n        # Acquire the lock before processing the prompt\n        with request_lock:\n            # print(f'User Prompt: {user_prompt}')              \n            # Get the answer from the chain\n            res = QA(user_prompt)\n            answer, docs = res[\"result\"], res[\"source_documents\"]\n\n            prompt_response_dict = {\n                \"Prompt\": user_prompt,\n                \"Answer\": answer,\n            }\n\n            prompt_response_dict[\"Sources\"] = []\n            for document in docs:\n                prompt_response_dict[\"Sources\"].append(\n                    (os.path.basename(str(document.metadata[\"source\"])), str(document.page_content))\n                )\n\n        return jsonify(prompt_response_dict), 200\n    else:\n        return \"No user prompt received\", 400\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--port\", type=int, default=5110, help=\"Port to run the API on. Defaults to 5110.\")\n    parser.add_argument(\n        \"--host\",\n        type=str,\n        default=\"127.0.0.1\",\n        help=\"Host to run the UI on. Defaults to 127.0.0.1. \"\n        \"Set to 0.0.0.0 to make the UI externally \"\n        \"accessible from other devices.\",\n    )\n    args = parser.parse_args()\n\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s\", level=logging.INFO\n    )\n    app.run(debug=False, host=args.host, port=args.port)\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 1.8837890625,
          "content": "import os\nimport csv\nfrom datetime import datetime\nfrom constants import EMBEDDING_MODEL_NAME\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n\ndef log_to_csv(question, answer):\n\n    log_dir, log_file = \"local_chat_history\", \"qa_log.csv\"\n    # Ensure log directory exists, create if not\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # Construct the full file path\n    log_path = os.path.join(log_dir, log_file)\n\n    # Check if file exists, if not create and write headers\n    if not os.path.isfile(log_path):\n        with open(log_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n            writer = csv.writer(file)\n            writer.writerow([\"timestamp\", \"question\", \"answer\"])\n\n    # Append the log entry\n    with open(log_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n        writer = csv.writer(file)\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        writer.writerow([timestamp, question, answer])\n\n\ndef get_embeddings(device_type=\"cuda\"):\n    if \"instructor\" in EMBEDDING_MODEL_NAME:\n        return HuggingFaceInstructEmbeddings(\n            model_name=EMBEDDING_MODEL_NAME,\n            model_kwargs={\"device\": device_type},\n            embed_instruction=\"Represent the document for retrieval:\",\n            query_instruction=\"Represent the question for retrieving supporting documents:\",\n        )\n\n    elif \"bge\" in EMBEDDING_MODEL_NAME:\n        return HuggingFaceBgeEmbeddings(\n            model_name=EMBEDDING_MODEL_NAME,\n            model_kwargs={\"device\": device_type},\n            query_instruction=\"Represent this sentence for searching relevant passages:\",\n        )\n\n    else:\n        return HuggingFaceEmbeddings(\n            model_name=EMBEDDING_MODEL_NAME,\n            model_kwargs={\"device\": device_type},\n        )\n"
        }
      ]
    }
  ]
}