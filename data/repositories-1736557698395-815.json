{
  "metadata": {
    "timestamp": 1736557698395,
    "page": 815,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "nagadomi/waifu2x",
      "stars": 27675,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0,
          "content": ".git\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.04,
          "content": "models/**/*.json binary\n*.t7 binary\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3,
          "content": "*~\nwork/\ncache/*.png\ncache/url_*\ndata/*\n!data/.gitkeep\n\nmodels/*\n!models/anime_style_art\n!models/anime_style_art_rgb\n!models/ukbench\n!models/photo\n!models/upconv_7\n!models/upconv_7l\n!models/srresnet_12l\n!models/vgg_7\n!models/cunet\nmodels/*/*.png\nmodels/*/*/*.png\n\nwaifu2x.log\nwaifu2x-*.log\nwaifu2x_*.log\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.61,
          "content": "FROM nagadomi/torch7:cuda10.1-cudnn7-devel-ubuntu18.04\n\nRUN apt-get update && apt-get install -y --no-install-recommends --force-yes \\\n  libsnappy-dev \\\n  graphicsmagick \\\n  libgraphicsmagick1-dev \\\n  libssl1.0-dev \\\n  ca-certificates \\\n  git && \\\n  rm -rf /var/lib/apt/lists/*\n\nRUN \\\n  luarocks install graphicsmagick && \\\n  luarocks install lua-csnappy && \\\n  luarocks install md5 && \\\n  luarocks install uuid && \\\n  luarocks install csvigo && \\\n  PREFIX=$HOME/torch/install luarocks install turbo\n\n# suppress message `tput: No value for $TERM and no -T specified`\nENV TERM xterm\n\nCOPY . /root/waifu2x\n\nWORKDIR /root/waifu2x\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.07,
          "content": "The MIT License\n\nCopyright (C) 2015 nagadomi <nagadomi@nurs.or.jp>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.31,
          "content": "images/miku_*: CC BY-NC by piapro (http://piapro.net/en_for_creators.html)\nwebgen/assets/bg.png: Generated by chibichara maker (http://tetrabo.com/chibichara/).\nlib/sRGB2014.lua: sRGB2014.icc from http://www.color.org/srgbprofiles.xalter\nlib/RandomBinaryConvolution.lua: from from https://github.com/juefeix/lbcnn.torch\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.08,
          "content": "# waifu2x\n\nImage Super-Resolution for Anime-style art using Deep Convolutional Neural Networks.\nAnd it supports photo.\n\nThe demo application can be found at https://waifu2x.udp.jp/ (Cloud version), https://unlimited.waifu2x.net/ (In-Browser version).\n\n## 2023/02 PyTorch version\n\n[nunif](https://github.com/nagadomi/nunif)\n\nwaifu2x development has already been moved to the repository above.\n\n## Summary\n\nClick to see the slide show.\n\n![slide](https://raw.githubusercontent.com/nagadomi/waifu2x/master/images/slide.png)\n\n## References\n\nwaifu2x is inspired by SRCNN [1]. 2D character picture (HatsuneMiku) is licensed under CC BY-NC by piapro [2].\n\n- [1] Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, \"Image Super-Resolution Using Deep Convolutional Networks\", http://arxiv.org/abs/1501.00092\n- [2] \"For Creators\", https://piapro.net/intl/en_for_creators.html\n## Public AMI\n\nTODO\n\n## Third Party Software\n\n[Third-Party](https://github.com/nagadomi/waifu2x/wiki/Third-Party)\n\nIf you are a windows user, I recommend you to use [waifu2x-caffe](https://github.com/lltcggie/waifu2x-caffe)(Just download from `releases` tab), [waifu2x-ncnn-vulkan](https://github.com/nihui/waifu2x-ncnn-vulkan) or [waifu2x-conver-cpp](https://github.com/DeadSix27/waifu2x-converter-cpp).\n\n## Dependencies\n\n### Hardware\n- NVIDIA GPU\n\n### Platform\n\n- [Torch7](http://torch.ch/)\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-toolkit)\n\n### LuaRocks packages (excludes torch7's default packages)\n- lua-csnappy\n- md5\n- uuid\n- csvigo\n- [turbo](https://github.com/kernelsauce/turbo)\n\n## Installation\n\n### Setting Up the Command Line Tool Environment\n (on Ubuntu 16.04)\n\n#### Install CUDA\n\nSee: [NVIDIA CUDA Getting Started Guide for Linux](http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#ubuntu-installation)\n\nDownload [CUDA](http://developer.nvidia.com/cuda-downloads)\n\n```\nsudo dpkg -i cuda-repo-ubuntu1404_7.5-18_amd64.deb\nsudo apt-get update\nsudo apt-get install cuda\n```\n\n#### Install Package\n\n```\nsudo apt-get install libsnappy-dev\nsudo apt-get install libgraphicsmagick1-dev\nsudo apt-get install libssl1.0-dev # for web server\n```\n\nNote: waifu2x requires little-cms2 linked graphicsmagick. if you use macOS/homebrew, See [#174](https://github.com/nagadomi/waifu2x/issues/174#issuecomment-384466451).\n\n#### Install Torch7\n\nSee: [Getting started with Torch](http://torch.ch/docs/getting-started.html).\n\n- For CUDA9.x/CUDA8.x, see [#222](https://github.com/nagadomi/waifu2x/issues/222)\n- For CUDA10.x, see [#253](https://github.com/nagadomi/waifu2x/issues/253#issuecomment-445448928)\n\n#### Getting waifu2x\n\n```\ngit clone --depth 1 https://github.com/nagadomi/waifu2x.git\n```\n\nand install lua modules.\n\n```\ncd waifu2x\n./install_lua_modules.sh\n```\n\n#### Validation\n\nTesting the waifu2x command line tool.\n```\nth waifu2x.lua\n```\n\n## Web Application\n```\nth web.lua\n```\n\nView at: http://localhost:8812/\n\n## Command line tools\nNotes: If you have cuDNN library, than you can use cuDNN with `-force_cudnn 1` option. cuDNN is too much faster than default kernel. If you got GPU out of memory error, you can avoid it with `-crop_size` option (e.g. `-crop_size 128`).\n\n### Noise Reduction\n```\nth waifu2x.lua -m noise -noise_level 1 -i input_image.png -o output_image.png\n```\n```\nth waifu2x.lua -m noise -noise_level 0 -i input_image.png -o output_image.png\nth waifu2x.lua -m noise -noise_level 2 -i input_image.png -o output_image.png\nth waifu2x.lua -m noise -noise_level 3 -i input_image.png -o output_image.png\n```\n\n### 2x Upscaling\n```\nth waifu2x.lua -m scale -i input_image.png -o output_image.png\n```\n\n### Noise Reduction + 2x Upscaling\n```\nth waifu2x.lua -m noise_scale -noise_level 1 -i input_image.png -o output_image.png\n```\n```\nth waifu2x.lua -m noise_scale -noise_level 0 -i input_image.png -o output_image.png\nth waifu2x.lua -m noise_scale -noise_level 2 -i input_image.png -o output_image.png\nth waifu2x.lua -m noise_scale -noise_level 3 -i input_image.png -o output_image.png\n```\n\n### Batch conversion\n\n```\nfind /path/to/imagedir -name \"*.png\" -o -name \"*.jpg\" > image_list.txt\nth waifu2x.lua -m scale -l ./image_list.txt -o /path/to/outputdir/prefix_%d.png\n```\n\nThe output format supports `%s` and `%d`(e.g. %06d). `%s` will be replaced the basename of the source filename. `%d` will be replaced a sequence number.\nFor example, when input filename is `piyo.png`, `%s_%03d.png` will be replaced `piyo_001.png`.\n\nSee also `th waifu2x.lua -h`.\n\n### Using photo model\n\nPlease add `-model_dir models/photo` to command line option, if you want to use photo model.\nFor example,\n\n```\nth waifu2x.lua -model_dir models/photo -m scale -i input_image.png -o output_image.png\n```\n\n### Video Encoding\n\n\\* `avconv` is alias of `ffmpeg` on Ubuntu 14.04.\n\nExtracting images and audio from a video. (range: 00:09:00 ~ 00:12:00)\n```\nmkdir frames\navconv -i data/raw.avi -ss 00:09:00 -t 00:03:00 -r 24 -f image2 frames/%06d.png\navconv -i data/raw.avi -ss 00:09:00 -t 00:03:00 audio.mp3\n```\n\nGenerating a image list.\n```\nfind ./frames -name \"*.png\" |sort > data/frame.txt\n```\n\nwaifu2x (for example, noise reduction)\n```\nmkdir new_frames\nth waifu2x.lua -m noise -noise_level 1 -resume 1 -l data/frame.txt -o new_frames/%d.png\n```\n\nGenerating a video from waifu2xed images and audio.\n```\navconv -f image2 -framerate 24 -i new_frames/%d.png -i audio.mp3 -r 24 -vcodec libx264 -crf 16 video.mp4\n```\n\n## Train Your Own Model\nNote1: If you have cuDNN library, you can use cudnn kernel with `-backend cudnn` option. And, you can convert trained cudnn model to cunn model with `tools/rebuild.lua`.\n\nNote2: The command that was used to train for waifu2x's pretrained models is available at `appendix/train_upconv_7_art.sh`, `appendix/train_upconv_7_photo.sh`. Maybe it is helpful.\n\n### Data Preparation\n\nGenrating a file list.\n```\nfind /path/to/image/dir -name \"*.png\" > data/image_list.txt\n```\nYou should use noise free images. In my case, waifu2x is trained with 6000 high-resolution-noise-free-PNG images.\n\nConverting training data.\n```\nth convert_data.lua\n```\n\n### Train a Noise Reduction(level1) model\n\n```\nmkdir models/my_model\nth train.lua -model_dir models/my_model -method noise -noise_level 1 -test images/miku_noisy.png\n# usage\nth waifu2x.lua -model_dir models/my_model -m noise -noise_level 1 -i images/miku_noisy.png -o output.png\n```\nYou can check the performance of model with `models/my_model/noise1_best.png`.\n\n### Train a Noise Reduction(level2) model\n\n```\nth train.lua -model_dir models/my_model -method noise -noise_level 2 -test images/miku_noisy.png\n# usage\nth waifu2x.lua -model_dir models/my_model -m noise -noise_level 2 -i images/miku_noisy.png -o output.png\n```\nYou can check the performance of model with `models/my_model/noise2_best.png`.\n\n### Train a 2x UpScaling model\n\n```\nth train.lua -model upconv_7 -model_dir models/my_model -method scale -scale 2 -test images/miku_small.png\n# usage\nth waifu2x.lua -model_dir models/my_model -m scale -scale 2 -i images/miku_small.png -o output.png\n```\nYou can check the performance of model with `models/my_model/scale2.0x_best.png`.\n\n### Train a 2x and noise reduction fusion model\n\n```\nth train.lua -model upconv_7 -model_dir models/my_model -method noise_scale -scale 2 -noise_level 1 -test images/miku_small.png\n# usage\nth waifu2x.lua -model_dir models/my_model -m noise_scale -scale 2 -noise_level 1 -i images/miku_small.png -o output.png\n```\nYou can check the performance of model with `models/my_model/noise1_scale2.0x_best.png`.\n\n## Docker\n\n( Docker image is available at https://hub.docker.com/r/nagadomi/waifu2x )\n\nRequires [nvidia-docker](https://github.com/NVIDIA/nvidia-docker).\n\n```\ndocker build -t waifu2x .\ndocker run --gpus all -p 8812:8812 waifu2x th web.lua\ndocker run --gpus all -v `pwd`/images:/images waifu2x th waifu2x.lua -force_cudnn 1 -m scale -scale 2 -i /images/miku_small.png -o /images/output.png\n```\n\nNote that running waifu2x in without [JIT caching](https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-understand-fat-binaries-jit-caching/) is very slow, which is what would happen if you use docker.\nFor a workaround, you can mount a host volume to the `CUDA_CACHE_PATH`, for instance,\n\n```\ndocker run --gpus all -v $PWD/ComputeCache:/root/.nv/ComputeCache waifu2x th waifu2x.lua --help\n```\n"
        },
        {
          "name": "appendix",
          "type": "tree",
          "content": null
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cache",
          "type": "tree",
          "content": null
        },
        {
          "name": "convert_data.lua",
          "type": "blob",
          "size": 5.37,
          "content": "require 'pl'\nlocal __FILE__ = (function() return string.gsub(debug.getinfo(2, 'S').source, \"^@\", \"\") end)()\npackage.path = path.join(path.dirname(__FILE__), \"lib\", \"?.lua;\") .. package.path\n\nrequire 'image'\nlocal cjson = require 'cjson'\nlocal csvigo = require 'csvigo'\nlocal compression = require 'compression'\nlocal settings = require 'settings'\nlocal image_loader = require 'image_loader'\nlocal iproc = require 'iproc'\nlocal alpha_util = require 'alpha_util'\n\nlocal function crop_if_large(src, max_size)\n   if max_size < 0 then\n      return src\n   end\n   local tries = 4\n   if src:size(2) >= max_size and src:size(3) >= max_size then\n      local rect\n      for i = 1, tries do\n\t local yi = torch.random(0, src:size(2) - max_size)\n\t local xi = torch.random(0, src:size(3) - max_size)\n\t rect = iproc.crop(src, xi, yi, xi + max_size, yi + max_size)\n\t -- ignore simple background\n\t if rect:float():std() >= 0 then\n\t    break\n\t end\n      end\n      return rect\n   else\n      return src\n   end\nend\nlocal function crop_if_large_pair(x, y, max_size)\n   if max_size < 0 then\n      return x, y\n   end\n   local scale_y = y:size(2) / x:size(2)\n   local mod = 4\n   assert(x:size(3) == (y:size(3) / scale_y))\n\n   local tries = 4\n   if y:size(2) > max_size and y:size(3) > max_size then\n      assert(max_size % 4 == 0)\n      local rect_x, rect_y\n      for i = 1, tries do\n\t local yi = torch.random(0, y:size(2) - max_size)\n\t local xi = torch.random(0, y:size(3) - max_size)\n\t if mod then\n\t    yi = yi - (yi % mod)\n\t    xi = xi - (xi % mod)\n\t end\n\t rect_y = iproc.crop(y, xi, yi, xi + max_size, yi + max_size)\n\t rect_x = iproc.crop(y, xi / scale_y, yi / scale_y, xi / scale_y + max_size / scale_y, yi / scale_y + max_size / scale_y)\n\t -- ignore simple background\n\t if rect_y:float():std() >= 0 then\n\t    break\n\t end\n      end\n      return rect_x, rect_y\n   else\n      return x, y\n   end\nend\nlocal function padding_x(x, pad, x_zero)\n   if pad > 0 then\n      if x_zero then\n\t x = iproc.zero_padding(x, pad, pad, pad, pad)\n      else\n\t x = iproc.padding(x, pad, pad, pad, pad)\n      end\n   end\n   return x\nend\nlocal function padding_xy(x, y, pad, x_zero, y_zero)\n   local scale = y:size(2) / x:size(2)\n   if pad > 0 then\n      if x_zero then\n\t x = iproc.zero_padding(x, pad, pad, pad, pad)\n      else\n\t x = iproc.padding(x, pad, pad, pad, pad)\n      end\n      if y_zero then\n\t y = iproc.zero_padding(y, pad * scale, pad * scale, pad * scale, pad * scale)\n      else\n\t y = iproc.padding(y, pad * scale, pad * scale, pad * scale, pad * scale)\n      end\n   end\n   return x, y\nend\nlocal function load_images(list)\n   local MARGIN = 32\n   local csv = csvigo.load({path = list, verbose = false, mode = \"raw\"})\n   local x = {}\n   local skip_notice = false\n   for i = 1, #csv do\n      local filters = nil\n      local filename = csv[i][1]\n      local csv_meta = csv[i][2]\n      if csv_meta and csv_meta:len() > 0 then\n\t csv_meta = cjson.decode(csv_meta)\n      end\n      if csv_meta and csv_meta.filters then\n\t filters = csv_meta.filters\n      end\n      local basename_y = path.basename(filename)\n      local im, meta = image_loader.load_byte(filename)\n      local skip = false\n      local alpha_color = torch.random(0, 1)\n\n      if im then\n\t if meta and meta.alpha then\n\t    if settings.use_transparent_png then\n\t       im = alpha_util.fill(im, meta.alpha, alpha_color)\n\t    else\n\t       skip = true\n\t    end\n\t end\n\t if skip then\n\t    if not skip_notice then\n\t       io.stderr:write(\"skip transparent png (settings.use_transparent_png=0)\\n\")\n\t       skip_notice = true\n\t    end\n\t else\n\t    if csv_meta and csv_meta.x then\n\t       -- method == user\n\t       local yy = im\n\t       local xx, meta2 = image_loader.load_byte(csv_meta.x)\n\t       if settings.invert_x then\n\t\t  xx = (-(xx:long()) + 255):byte()\n\t       end\n\n\t       if xx then\n\t\t  if meta2 and meta2.alpha then\n\t\t     xx = alpha_util.fill(xx, meta2.alpha, alpha_color)\n\t\t  end\n\t\t  xx, yy = crop_if_large_pair(xx, yy, settings.max_training_image_size)\n\t\t  xx, yy = padding_xy(xx, yy, settings.padding, settings.padding_x_zero, settings.padding_y_zero)\n\t\t  if settings.grayscale then\n\t\t     xx = iproc.rgb2y(xx)\n\t\t     yy = iproc.rgb2y(yy)\n\t\t  end\n\t\t  table.insert(x, {{y = compression.compress(yy), x = compression.compress(xx)},\n\t\t\t\t  {data = {filters = filters, has_x = true, basename = basename_y}}})\n\t       else\n\t\t  io.stderr:write(string.format(\"\\n%s: skip: load error.\\n\", csv_meta.x))\n\t       end\n\t    else\n\t       im = crop_if_large(im, settings.max_training_image_size)\n\t       im = iproc.crop_mod4(im)\n\t       im = padding_x(im, settings.padding, settings.padding_x_zero)\n\t       local scale = 1.0\n\t       if settings.random_half_rate > 0.0 then\n\t\t  scale = 2.0\n\t       end\n\t       if im:size(2) > (settings.crop_size * scale + MARGIN) and im:size(3) > (settings.crop_size * scale + MARGIN) then\n\t\t  if settings.grayscale then\n\t\t     im = iproc.rgb2y(im)\n\t\t  end\n\t\t  table.insert(x, {compression.compress(im), {data = {filters = filters, basename = basename_y}}})\n\t       else\n\t\t  io.stderr:write(string.format(\"\\n%s: skip: image is too small (%d > size).\\n\", filename, settings.crop_size * scale + MARGIN))\n\t       end\n\t    end\n\t end\n      else\n\t io.stderr:write(string.format(\"\\n%s: skip: load error.\\n\", filename))\n      end\n      xlua.progress(i, #csv)\n      if i % 10 == 0 then\n\t collectgarbage()\n      end\n   end\n   return x\nend\n\ntorch.manualSeed(settings.seed)\nprint(settings)\nlocal x = load_images(settings.image_list)\ntorch.save(settings.images, x)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "image_generators",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "install_lua_modules.sh",
          "type": "blob",
          "size": 0.48,
          "content": "#!/bin/bash\n\nCUDNN_BRANCH=R7 # for cudnn7\nCUDNN_WORK_DIR=.cudnn\n\ninstall_cudnn()\n{\n    rm -fr $CUDNN_WORK_DIR\n    git clone https://github.com/soumith/cudnn.torch.git -b $CUDNN_BRANCH $CUDNN_WORK_DIR\n    cd $CUDNN_WORK_DIR\n    luarocks make cudnn-scm-1.rockspec\n    cd ..\n    rm -fr $CUDNN_WORK_DIR\n}\n\nluarocks install graphicsmagick\nluarocks install lua-csnappy\nluarocks install md5\nluarocks install uuid\nluarocks install csvigo\ninstall_cudnn\nPREFIX=$HOME/torch/install luarocks install turbo\n"
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.lua",
          "type": "blob",
          "size": 24.45,
          "content": "require 'pl'\nlocal __FILE__ = (function() return string.gsub(debug.getinfo(2, 'S').source, \"^@\", \"\") end)()\npackage.path = path.join(path.dirname(__FILE__), \"lib\", \"?.lua;\") .. package.path\nrequire 'optim'\nrequire 'xlua'\nrequire 'image'\nrequire 'w2nn'\nlocal threads = require 'threads'\nlocal settings = require 'settings'\nlocal srcnn = require 'srcnn'\nlocal minibatch_adam = require 'minibatch_adam'\nlocal iproc = require 'iproc'\nlocal reconstruct = require 'reconstruct'\nlocal image_loader = require 'image_loader'\n\nlocal function save_test_scale(model, rgb, file)\n   local up = reconstruct.scale(model, settings.scale, rgb)\n   image.save(file, up)\nend\nlocal function save_test_jpeg(model, rgb, file)\n   local im, count = reconstruct.image(model, rgb)\n   image.save(file, im)\nend\nlocal function save_test_user(model, rgb, file)\n   if settings.scale == 1 then\n      save_test_jpeg(model, rgb, file)\n   else\n      save_test_scale(model, rgb, file)\n   end\nend\nlocal function split_data(x, test_size)\n   if settings.validation_filename_split then\n      if not (x[1][2].data and x[1][2].data.basename) then\n\t error(\"`images.t` does not have basename info. You need to re-run `convert_data.lua`.\")\n      end\n      local basename_db = {}\n      for i = 1, #x do\n\t local meta = x[i][2].data\n\t if basename_db[meta.basename] then\n\t    table.insert(basename_db[meta.basename], x[i])\n\t else\n\t    basename_db[meta.basename] = {x[i]}\n\t end\n      end\n      local basename_list = {}\n      for k, v in pairs(basename_db) do\n\t table.insert(basename_list, v)\n      end\n      local index = torch.randperm(#basename_list)\n      local train_x = {}\n      local valid_x = {}\n      local pos = 1\n      for i = 1, #basename_list do\n\t if #valid_x >= test_size then\n\t    break\n\t end\n\t local xs = basename_list[index[pos]]\n\t for j = 1, #xs do\n\t    table.insert(valid_x, xs[j])\n\t end\n\t pos = pos + 1\n      end\n      for i = pos, #basename_list do\n\t local xs = basename_list[index[i]]\n\t for j = 1, #xs do\n\t    table.insert(train_x, xs[j])\n\t end\n      end\n      return train_x, valid_x\n   else\n      local index = torch.randperm(#x)\n      local train_size = #x - test_size\n      local train_x = {}\n      local valid_x = {}\n      for i = 1, train_size do\n\t train_x[i] = x[index[i]]\n      end\n      for i = 1, test_size do\n\t valid_x[i] = x[index[train_size + i]]\n      end\n      return train_x, valid_x\n   end\nend\n\nlocal g_transform_pool = nil\nlocal g_mutex = nil\nlocal g_mutex_id = nil\nlocal function transform_pool_init(has_resize, offset)\n   local nthread = torch.getnumthreads()\n   if (settings.thread > 0) then\n      nthread = settings.thread\n   end\n   g_mutex = threads.Mutex()\n   g_mutex_id = g_mutex:id()\n   g_transform_pool = threads.Threads(\n      nthread,\n      threads.safe(\n      function(threadid)\n\t require 'pl'\n\t local __FILE__ = (function() return string.gsub(debug.getinfo(2, 'S').source, \"^@\", \"\") end)()\n\t package.path = path.join(path.dirname(__FILE__), \"lib\", \"?.lua;\") .. package.path\n\t require 'torch'\n\t require 'nn'\n\t require 'cunn'\n\n\t torch.setnumthreads(1)\n\t torch.setdefaulttensortype(\"torch.FloatTensor\")\n\n\t local threads = require 'threads'\n\t local compression = require 'compression'\n\t local pairwise_transform = require 'pairwise_transform'\n\n\t function transformer(x, is_validation, n)\n\t    local mutex = threads.Mutex(g_mutex_id)\n\t    local meta = {data = {}}\n\t    local y = nil\n\t    if type(x) == \"table\" and type(x[2]) == \"table\" then\n\t       meta = x[2]\n\t       if x[1].x and x[1].y then\n\t\t  y = compression.decompress(x[1].y)\n\t\t  x = compression.decompress(x[1].x)\n\t       else\n\t\t  x = compression.decompress(x[1])\n\t       end\n\t    else\n\t       x = compression.decompress(x)\n\t    end\n\t    n = n or settings.patches\n\t    if is_validation == nil then is_validation = false end\n\t    local random_color_noise_rate = nil \n\t    local random_overlay_rate = nil\n\t    local active_cropping_rate = nil\n\t    local active_cropping_tries = nil\n\t    if is_validation then\n\t       active_cropping_rate = settings.active_cropping_rate\n\t       active_cropping_tries = settings.active_cropping_tries\n\t       random_color_noise_rate = 0.0\n\t       random_overlay_rate = 0.0\n\t    else\n\t       active_cropping_rate = settings.active_cropping_rate\n\t       active_cropping_tries = settings.active_cropping_tries\n\t       random_color_noise_rate = settings.random_color_noise_rate\n\t       random_overlay_rate = settings.random_overlay_rate\n\t    end\n\t    if settings.method == \"scale\" then\n\t       local conf = tablex.update({\n\t\t     mutex = mutex,\n\t\t     downsampling_filters = settings.downsampling_filters,\n\t\t     random_half_rate = settings.random_half_rate,\n\t\t     random_color_noise_rate = random_color_noise_rate,\n\t\t     random_overlay_rate = random_overlay_rate,\n\t\t     random_unsharp_mask_rate = settings.random_unsharp_mask_rate,\n\t\t     random_blur_rate = settings.random_blur_rate,\n\t\t     random_blur_size = settings.random_blur_size,\n\t\t     random_blur_sigma_min = settings.random_blur_sigma_min,\n\t\t     random_blur_sigma_max = settings.random_blur_sigma_max,\n\t\t     max_size = settings.max_size,\n\t\t     active_cropping_rate = active_cropping_rate,\n\t\t     active_cropping_tries = active_cropping_tries,\n\t\t     rgb = (settings.color == \"rgb\"),\n\t\t     x_upsampling = not has_resize,\n\t\t     resize_blur_min = settings.resize_blur_min,\n\t\t     resize_blur_max = settings.resize_blur_max}, meta)\n\t       return pairwise_transform.scale(x,\n\t\t\t\t\t       settings.scale,\n\t\t\t\t\t       settings.crop_size, offset,\n\t\t\t\t\t       n, conf)\n\t    elseif settings.method == \"noise\" then\n\t       local conf = tablex.update({\n\t\t     mutex = mutex,\n\t\t     random_half_rate = settings.random_half_rate,\n\t\t     random_color_noise_rate = random_color_noise_rate,\n\t\t     random_overlay_rate = random_overlay_rate,\n\t\t     random_unsharp_mask_rate = settings.random_unsharp_mask_rate,\n\t\t     random_blur_rate = settings.random_blur_rate,\n\t\t     random_blur_size = settings.random_blur_size,\n\t\t     random_blur_sigma_min = settings.random_blur_sigma_min,\n\t\t     random_blur_sigma_max = settings.random_blur_sigma_max,\n\t\t     max_size = settings.max_size,\n\t\t     jpeg_chroma_subsampling_rate = settings.jpeg_chroma_subsampling_rate,\n\t\t     active_cropping_rate = active_cropping_rate,\n\t\t     active_cropping_tries = active_cropping_tries,\n\t\t     nr_rate = settings.nr_rate,\n\t\t     rgb = (settings.color == \"rgb\")}, meta)\n\t       return pairwise_transform.jpeg(x,\n\t\t\t\t\t      settings.style,\n\t\t\t\t\t      settings.noise_level,\n\t\t\t\t\t      settings.crop_size, offset,\n\t\t\t\t\t      n, conf)\n\t    elseif settings.method == \"noise_scale\" then\n\t       local conf = tablex.update({\n\t\t     mutex = mutex,\n\t\t     downsampling_filters = settings.downsampling_filters,\n\t\t     random_half_rate = settings.random_half_rate,\n\t\t     random_color_noise_rate = random_color_noise_rate,\n\t\t     random_overlay_rate = random_overlay_rate,\n\t\t     random_unsharp_mask_rate = settings.random_unsharp_mask_rate,\n\t\t     random_blur_rate = settings.random_blur_rate,\n\t\t     random_blur_size = settings.random_blur_size,\n\t\t     random_blur_sigma_min = settings.random_blur_sigma_min,\n\t\t     random_blur_sigma_max = settings.random_blur_sigma_max,\n\t\t     max_size = settings.max_size,\n\t\t     jpeg_chroma_subsampling_rate = settings.jpeg_chroma_subsampling_rate,\n\t\t     nr_rate = settings.nr_rate,\n\t\t     active_cropping_rate = active_cropping_rate,\n\t\t     active_cropping_tries = active_cropping_tries,\n\t\t     rgb = (settings.color == \"rgb\"),\n\t\t     x_upsampling = not has_resize,\n\t\t     resize_blur_min = settings.resize_blur_min,\n\t\t     resize_blur_max = settings.resize_blur_max}, meta)\n\t       return pairwise_transform.jpeg_scale(x,\n\t\t\t\t\t\t    settings.scale,\n\t\t\t\t\t\t    settings.style,\n\t\t\t\t\t\t    settings.noise_level,\n\t\t\t\t\t\t    settings.crop_size, offset,\n\t\t\t\t\t\t    n, conf)\n\t    elseif settings.method == \"user\" then\n\t       local random_erasing_rate = 0\n\t       local random_erasing_n = 0\n\t       local random_erasing_rect_min = 0\n\t       local random_erasing_rect_max = 0\n\t       if is_validation then\n\t       else\n\t\t  random_erasing_rate = settings.random_erasing_rate\n\t\t  random_erasing_n = settings.random_erasing_n\n\t\t  random_erasing_rect_min = settings.random_erasing_rect_min\n\t\t  random_erasing_rect_max = settings.random_erasing_rect_max\n\t       end\n\t       local conf = tablex.update({\n\t\t     gcn = settings.gcn,\n\t\t     max_size = settings.max_size,\n\t\t     active_cropping_rate = active_cropping_rate,\n\t\t     active_cropping_tries = active_cropping_tries,\n\t\t     random_pairwise_rotate_rate = settings.random_pairwise_rotate_rate,\n\t\t     random_pairwise_rotate_min = settings.random_pairwise_rotate_min,\n\t\t     random_pairwise_rotate_max = settings.random_pairwise_rotate_max,\n\t\t     random_pairwise_scale_rate = settings.random_pairwise_scale_rate,\n\t\t     random_pairwise_scale_min = settings.random_pairwise_scale_min,\n\t\t     random_pairwise_scale_max = settings.random_pairwise_scale_max,\n\t\t     random_pairwise_negate_rate = settings.random_pairwise_negate_rate,\n\t\t     random_pairwise_negate_x_rate = settings.random_pairwise_negate_x_rate,\n\t\t     pairwise_y_binary = settings.pairwise_y_binary,\n\t\t     pairwise_flip = settings.pairwise_flip,\n\t\t     random_erasing_rate = random_erasing_rate,\n\t\t     random_erasing_n = random_erasing_n,\n\t\t     random_erasing_rect_min = random_erasing_rect_min,\n\t\t     random_erasing_rect_max = random_erasing_rect_max,\n\t\t     rgb = (settings.color == \"rgb\")}, meta)\n\t       return pairwise_transform.user(x, y,\n\t\t\t\t\t      settings.crop_size, offset,\n\t\t\t\t\t      n, conf)\n\t    end\n\t end\n      end)\n   )\n   g_transform_pool:synchronize()\nend\n\nlocal function make_validation_set(x, n, patches)\n   local nthread = torch.getnumthreads()\n   if (settings.thread > 0) then\n      nthread = settings.thread\n   end\n   n = n or 4\n   local validation_patches = math.min(16, patches or 16)\n   local data = {}\n\n   g_transform_pool:synchronize()\n   torch.setnumthreads(1) -- 1\n\n   for i = 1, #x do\n      for k = 1, math.max(n / validation_patches, 1) do\n\t local input = x[i]\n\t g_transform_pool:addjob(\n\t    function()\n\t       local xy = transformer(input, true, validation_patches)\n\t       return xy\n\t    end,\n\t    function(xy)\n\t       for j = 1, #xy do\n\t\t  table.insert(data, {x = xy[j][1], y = xy[j][2]})\n\t       end\n\t    end\n\t )\n      end\n      if i % 20 == 0 then\n\t collectgarbage()\n\t g_transform_pool:synchronize()\n\t xlua.progress(i, #x)\n      end\n   end\n   g_transform_pool:synchronize()\n   torch.setnumthreads(nthread) -- revert\n\n   local new_data = {}\n   local perm = torch.randperm(#data)\n   for i = 1, perm:size(1) do\n      new_data[i] = data[perm[i]]\n   end\n   data = new_data\n   return data\nend\nlocal function validate(model, criterion, eval_metric, data, batch_size)\n   local psnr = 0\n   local loss = 0\n   local mse = 0\n   local loss_count = 0\n   local inputs_tmp = torch.Tensor(batch_size,\n\t\t\t\t   data[1].x:size(1), \n\t\t\t\t   data[1].x:size(2),\n\t\t\t\t   data[1].x:size(3)):zero()\n   local targets_tmp = torch.Tensor(batch_size,\n\t\t\t\t    data[1].y:size(1),\n\t\t\t\t    data[1].y:size(2),\n\t\t\t\t    data[1].y:size(3)):zero()\n   local inputs = inputs_tmp:clone():cuda()\n   local targets = targets_tmp:clone():cuda()\n   for t = 1, #data, batch_size do\n      if t + batch_size -1 > #data then\n\t break\n      end\n      for i = 1, batch_size do\n         inputs_tmp[i]:copy(data[t + i - 1].x)\n\t targets_tmp[i]:copy(data[t + i - 1].y)\n      end\n      inputs:copy(inputs_tmp)\n      targets:copy(targets_tmp)\n      local z = model:forward(inputs)\n      local batch_mse = eval_metric:forward(z, targets)\n      loss = loss + criterion:forward(z, targets)\n      mse = mse + batch_mse\n      psnr = psnr + (10 * math.log10(1 / (batch_mse + 1.0e-6)))\n      loss_count = loss_count + 1\n      if loss_count % 10 == 0 then\n\t xlua.progress(t, #data)\n\t collectgarbage()\n      end\n   end\n   xlua.progress(#data, #data)\n   return {loss = loss / loss_count, MSE = mse / loss_count, PSNR = psnr / loss_count}\nend\n\nlocal function create_criterion(model)\n   if settings.loss == \"huber\" then\n      if reconstruct.is_rgb(model) then\n\t local offset = reconstruct.offset_size(model)\n\t local output_w = settings.crop_size - offset * 2\n\t local weight = torch.Tensor(3, output_w * output_w)\n\t weight[1]:fill(0.29891 * 3) -- R\n\t weight[2]:fill(0.58661 * 3) -- G\n\t weight[3]:fill(0.11448 * 3) -- B\n\t return w2nn.ClippedWeightedHuberCriterion(weight, 0.1, {0.0, 1.0}):cuda()\n      else\n\t local offset = reconstruct.offset_size(model)\n\t local output_w = settings.crop_size - offset * 2\n\t local weight = torch.Tensor(1, output_w * output_w)\n\t weight[1]:fill(1.0)\n\t return w2nn.ClippedWeightedHuberCriterion(weight, 0.1, {0.0, 1.0}):cuda()\n      end\n   elseif settings.loss == \"l1\" then\n      return w2nn.L1Criterion():cuda()\n   elseif settings.loss == \"mse\" then\n      return w2nn.ClippedMSECriterion(0, 1.0):cuda()\n   elseif settings.loss == \"bce\" then\n      local bce = nn.BCECriterion()\n      bce.sizeAverage = true\n      return bce:cuda()\n   elseif settings.loss == \"aux_bce\" then\n      local aux = w2nn.AuxiliaryLossCriterion(nn.BCECriterion)\n      aux.sizeAverage = true\n      return aux:cuda()\n   elseif settings.loss == \"aux_huber\" then\n      local args = {}\n      if reconstruct.is_rgb(model) then\n\t local offset = reconstruct.offset_size(model)\n\t local output_w = settings.crop_size - offset * 2\n\t local weight = torch.Tensor(3, output_w * output_w)\n\t weight[1]:fill(0.29891 * 3) -- R\n\t weight[2]:fill(0.58661 * 3) -- G\n\t weight[3]:fill(0.11448 * 3) -- B\n\t args = {weight, 0.1, {0.0, 1.0}}\n      else\n\t local offset = reconstruct.offset_size(model)\n\t local output_w = settings.crop_size - offset * 2\n\t local weight = torch.Tensor(1, output_w * output_w)\n\t weight[1]:fill(1.0)\n\t args = {weight, 0.1, {0.0, 1.0}}\n      end\n      local aux = w2nn.AuxiliaryLossCriterion(w2nn.ClippedWeightedHuberCriterion, args)\n      return aux:cuda()\n   elseif settings.loss == \"lbp\" then\n      if reconstruct.is_rgb(model) then\n\t return w2nn.LBPCriterion(3, 128):cuda()\n      else\n\t return w2nn.LBPCriterion(1, 128):cuda()\n      end\n   elseif settings.loss == \"lbp2\" then\n      if reconstruct.is_rgb(model) then\n\t return w2nn.LBPCriterion(3, 128, 3, 2):cuda()\n      else\n\t return w2nn.LBPCriterion(1, 128, 3, 2):cuda()\n      end\n   elseif settings.loss == \"aux_lbp\" then\n      if reconstruct.is_rgb(model) then\n\t return w2nn.AuxiliaryLossCriterion(w2nn.LBPCriterion, {3, 128}):cuda()\n      else\n\t return w2nn.AuxiliaryLossCriterion(w2nn.LBPCriterion, {1, 128}):cuda()\n      end\n   elseif settings.loss == \"aux_lbp2\" then\n      if reconstruct.is_rgb(model) then\n\t return w2nn.AuxiliaryLossCriterion(w2nn.LBPCriterion, {3, 128, 3, 2}):cuda()\n      else\n\t return w2nn.AuxiliaryLossCriterion(w2nn.LBPCriterion, {1, 128, 3, 2}):cuda()\n      end\n   else\n      error(\"unsupported loss ..\" .. settings.loss)\n   end\nend\n\nlocal function resampling(x, y, train_x)\n   local c = 1\n   local shuffle = torch.randperm(#train_x)\n   local nthread = torch.getnumthreads()\n   if (settings.thread > 0) then\n      nthread = settings.thread\n   end\n   torch.setnumthreads(1) -- 1\n\n   for t = 1, #train_x do\n      local input = train_x[shuffle[t]]\n      g_transform_pool:addjob(\n\t function()\n\t    local xy = transformer(input, false, settings.patches)\n\t    return xy\n\t end,\n\t function(xy)\n\t    for i = 1, #xy do\n\t       if c <= x:size(1) then\n\t\t  x[c]:copy(xy[i][1])\n\t\t  y[c]:copy(xy[i][2])\n\t\t  c = c + 1\n\t       else\n\t\t  break\n\t       end\n\t    end\n\t end\n      )\n      if t % 50 == 0 then\n\t collectgarbage()\n\t g_transform_pool:synchronize()\n\t xlua.progress(t, #train_x)\n      end\n      if c > x:size(1) then\n\t break\n      end\n   end\n   g_transform_pool:synchronize()\n   xlua.progress(#train_x, #train_x)\n   torch.setnumthreads(nthread) -- revert\nend\nlocal function get_oracle_data(x, y, instance_loss, k, samples)\n   local index = torch.LongTensor(instance_loss:size(1))\n   local dummy = torch.Tensor(instance_loss:size(1))\n   torch.topk(dummy, index, instance_loss, k, 1, true)\n   print(\"MSE of all data: \" ..instance_loss:mean() .. \", MSE of oracle data: \" .. dummy:mean())\n   local shuffle = torch.randperm(k)\n   local x_s = x:size()\n   local y_s = y:size()\n   x_s[1] = samples\n   y_s[1] = samples\n   local oracle_x = torch.Tensor(table.unpack(torch.totable(x_s)))\n   local oracle_y = torch.Tensor(table.unpack(torch.totable(y_s)))\n\n   for i = 1, samples do\n      oracle_x[i]:copy(x[index[shuffle[i]]])\n      oracle_y[i]:copy(y[index[shuffle[i]]])\n   end\n   return oracle_x, oracle_y\nend\n\nlocal function remove_small_image(x)\n   local compression = require 'compression'\n   local new_x = {}\n   for i = 1, #x do\n      local xe, meta, x_s\n      xe = x[i]\n      if type(x) == \"table\" and type(x[2]) == \"table\" then\n\t if xe[1].x and xe[1].y then\n\t    x_s = compression.size(xe[1].y) -- y size\n\t else\n\t    x_s = compression.size(xe[1])\n\t end\n      else\n\t x_s = compression.size(xe)\n      end\n      if x_s[2] / settings.scale > settings.crop_size + 32 and\n      x_s[3] / settings.scale > settings.crop_size + 32 then\n\t table.insert(new_x, x[i])\n      end\n      if i % 100 == 0 then\n\t collectgarbage()\n      end\n   end\n   print(string.format(\"%d small images are removed\", #x - #new_x))\n\n   return new_x\nend\nlocal function plot(train, valid)\n   gnuplot.plot({\n\t {'training', torch.Tensor(train), '-'},\n\t {'validation', torch.Tensor(valid), '-'}})\nend\nlocal function train()\n   local x = torch.load(settings.images)\n   if settings.method ~= \"user\" then\n      x = remove_small_image(x)\n   end\n   local train_x, valid_x = split_data(x, math.max(math.floor(settings.validation_rate * #x), 1))\n   local hist_train = {}\n   local hist_valid = {}\n   local adam_config = {\n      xLearningRate = settings.learning_rate,\n      xBatchSize = settings.batch_size,\n      xLearningRateDecay = settings.learning_rate_decay,\n      xInstanceLoss = (settings.oracle_rate > 0)\n   }\n   local model\n   if settings.resume:len() > 0 then\n      model = w2nn.load_model(settings.resume, settings.backend == \"cudnn\", \"ascii\")\n      adam_config.xEvalCount = math.floor((#train_x * settings.patches) / settings.batch_size) * settings.batch_size * settings.inner_epoch * (settings.resume_epoch - 1)\n      print(string.format(\"set eval count = %d\", adam_config.xEvalCount))\n      if adam_config.xEvalCount > 0 then\n\t adam_config.learningRate = adam_config.xLearningRate / (1 + adam_config.xEvalCount * adam_config.xLearningRateDecay)\n\t print(string.format(\"set learning rate = %E\", adam_config.learningRate))\n      else\n\t adam_config.xEvalCount = 0\n\t adam_config.learningRate = adam_config.xLearningRate\n      end\n   else\n      if stringx.endswith(settings.model, \".lua\") then\n\t local create_model = dofile(settings.model)\n\t model = create_model(srcnn, settings)\n      else\n\t model = srcnn.create(settings.model, settings.backend, settings.color)\n      end\n   end\n   if model.w2nn_input_size then\n      if settings.crop_size ~= model.w2nn_input_size then\n\t io.stderr:write(string.format(\"warning: crop_size is replaced with %d\\n\",\n\t\t\t\t       model.w2nn_input_size))\n\t settings.crop_size = model.w2nn_input_size\n      end\n   end\n   if model.w2nn_gcn then\n      settings.gcn = true\n   else\n      settings.gcn = false\n   end\n   dir.makepath(settings.model_dir)\n\n   local offset = reconstruct.offset_size(model)\n   transform_pool_init(reconstruct.has_resize(model), offset)\n\n   local criterion = create_criterion(model)\n   local eval_metric = nil\n   if settings.loss:find(\"aux_\") ~= nil then\n      eval_metric = w2nn.AuxiliaryLossCriterion(w2nn.ClippedMSECriterion):cuda()\n   else\n      eval_metric = w2nn.ClippedMSECriterion():cuda()\n   end\n   local ch = nil\n   if settings.color == \"y\" then\n      ch = 1\n   elseif settings.color == \"rgb\" then\n      ch = 3\n   end\n   local best_score = 1000.0\n   print(\"# make validation-set\")\n   local valid_xy = make_validation_set(valid_x, \n\t\t\t\t\tsettings.validation_crops,\n\t\t\t\t\tsettings.patches)\n   valid_x = nil\n   \n   collectgarbage()\n   model:cuda()\n   print(\"load .. \" .. #train_x)\n\n   local x = nil\n   local y = torch.Tensor(settings.patches * #train_x,\n\t\t\t  ch * (settings.crop_size - offset * 2) * (settings.crop_size - offset * 2)):zero()\n   if reconstruct.has_resize(model) then\n      x = torch.Tensor(settings.patches * #train_x,\n\t\t       ch, settings.crop_size / settings.scale, settings.crop_size / settings.scale)\n   else\n      x = torch.Tensor(settings.patches * #train_x,\n\t\t       ch, settings.crop_size, settings.crop_size)\n   end\n   local instance_loss = nil\n   local pmodel = w2nn.data_parallel(model, settings.gpu)\n   for epoch = settings.resume_epoch, settings.epoch do\n      pmodel:training()\n      print(\"# \" .. epoch)\n      if adam_config.learningRate then\n\t print(\"learning rate: \" .. adam_config.learningRate)\n      end\n      print(\"## resampling\")\n      if instance_loss then\n\t -- active learning\n\t local oracle_k = math.min(x:size(1) * (settings.oracle_rate * (1 / (1 - settings.oracle_drop_rate))), x:size(1))\n\t local oracle_n = math.min(x:size(1) * settings.oracle_rate, x:size(1))\n\t if oracle_n > 0 then\n\t    local oracle_x, oracle_y = get_oracle_data(x, y, instance_loss, oracle_k, oracle_n)\n\t    resampling(x:narrow(1, oracle_x:size(1) + 1, x:size(1)-oracle_x:size(1)),\n\t\t       y:narrow(1, oracle_x:size(1) + 1, x:size(1) - oracle_x:size(1)), train_x)\n\t    x:narrow(1, 1, oracle_x:size(1)):copy(oracle_x)\n\t    y:narrow(1, 1, oracle_y:size(1)):copy(oracle_y)\n\n\t    local draw_n = math.floor(math.sqrt(oracle_x:size(1), 0.5))\n\t    if draw_n > 100 then\n\t       draw_n = 100\n\t    end\n\t    image.save(path.join(settings.model_dir, \"oracle_x.png\"), \n\t\t       image.toDisplayTensor({\n\t\t\t     input = oracle_x:narrow(1, 1, draw_n * draw_n),\n\t\t\t     padding = 2,\n\t\t\t     nrow = draw_n,\n\t\t\t     min = 0,\n\t\t\t     max = 1}))\n\t else\n\t    resampling(x, y, train_x)\n\t end\n      else\n\t resampling(x, y, train_x, pairwise_func)\n      end\n      collectgarbage()\n      instance_loss = torch.Tensor(x:size(1)):zero()\n\n      for i = 1, settings.inner_epoch do\n\t pmodel:training()\n\t local train_score, il = minibatch_adam(pmodel, criterion, eval_metric, x, y, adam_config)\n\t instance_loss:copy(il)\n\t print(train_score)\n\t pmodel:evaluate()\n\t print(\"# validation\")\n\t local score = validate(pmodel, criterion, eval_metric, valid_xy, adam_config.xBatchSize)\n\t table.insert(hist_train, train_score.loss)\n\t table.insert(hist_valid, score.loss)\n\t if settings.plot then\n\t    plot(hist_train, hist_valid)\n\t end\n\t local score_for_update\n\t if settings.update_criterion == \"mse\" then\n\t    score_for_update = score.MSE\n\t else\n\t    score_for_update = score.loss\n\t end\n\t if score_for_update < best_score then\n\t    local test_image = image_loader.load_float(settings.test) -- reload\n\t    best_score = score_for_update\n\t    print(\"* model has updated\")\n\t    if settings.save_history then\n\t       pmodel:clearState()\n\t       torch.save(settings.model_file_best, model, \"ascii\")\n\t       torch.save(string.format(settings.model_file, epoch, i), model, \"ascii\")\n\t       if settings.method == \"noise\" then\n\t\t  local log = path.join(settings.model_dir,\n\t\t\t\t\t(\"noise%d_best.%d-%d.png\"):format(settings.noise_level,\n\t\t\t\t\t\t\t\t\t  epoch, i))\n\t\t  save_test_jpeg(model, test_image, log)\n\t       elseif settings.method == \"scale\" then\n\t\t  local log = path.join(settings.model_dir,\n\t\t\t\t\t(\"scale%.1f_best.%d-%d.png\"):format(settings.scale,\n\t\t\t\t\t\t\t\t\t    epoch, i))\n\t\t  save_test_scale(model, test_image, log)\n\t       elseif settings.method == \"noise_scale\" then\n\t\t  local log = path.join(settings.model_dir,\n\t\t\t\t\t(\"noise%d_scale%.1f_best.%d-%d.png\"):format(settings.noise_level, \n\t\t\t\t\t\t\t\t\t\t    settings.scale,\n\t\t\t\t\t\t\t\t\t\t    epoch, i))\n\t\t  save_test_scale(model, test_image, log)\n\t       elseif settings.method == \"user\" then\n\t\t  local log = path.join(settings.model_dir,\n\t\t\t\t\t(\"%s_best.%d-%d.png\"):format(settings.name, \n\t\t\t\t\t\t\t\t     epoch, i))\n\t\t  save_test_user(model, test_image, log)\n\t       end\n\t    else\n\t       pmodel:clearState()\n\t       torch.save(settings.model_file, model, \"ascii\")\n\t       if settings.method == \"noise\" then\n\t\t  local log = path.join(settings.model_dir,\n\t\t\t\t\t(\"noise%d_best.png\"):format(settings.noise_level))\n\t\t  save_test_jpeg(model, test_image, log)\n\t       elseif settings.method == \"scale\" then\n\t\t  local log = path.join(settings.model_dir,\n\t\t\t\t\t(\"scale%.1f_best.png\"):format(settings.scale))\n\t\t  save_test_scale(model, test_image, log)\n\t       elseif settings.method == \"noise_scale\" then\n\t\t  local log = path.join(settings.model_dir,\n\t\t\t\t\t(\"noise%d_scale%.1f_best.png\"):format(settings.noise_level, \n\t\t\t\t\t\t\t\t\t      settings.scale))\n\t\t  save_test_scale(model, test_image, log)\n\t       elseif settings.method == \"user\" then\n\t\t  local log = path.join(settings.model_dir,\n\t\t\t\t\t(\"%s_best.png\"):format(settings.name))\n\t\t  save_test_user(model, test_image, log)\n\t       end\n\t    end\n\t end\n\t print(\"Batch-wise PSNR: \" .. score.PSNR .. \", loss: \" .. score.loss .. \", MSE: \" .. score.MSE .. \", best: \" .. best_score)\n\t collectgarbage()\n      end\n   end\nend\ntorch.manualSeed(settings.seed)\ncutorch.manualSeed(settings.seed)\nprint(settings)\ntrain()\n"
        },
        {
          "name": "waifu2x.lua",
          "type": "blob",
          "size": 11.43,
          "content": "require 'pl'\nlocal __FILE__ = (function() return string.gsub(debug.getinfo(2, 'S').source, \"^@\", \"\") end)()\npackage.path = path.join(path.dirname(__FILE__), \"lib\", \"?.lua;\") .. package.path\nrequire 'sys'\nrequire 'w2nn'\nlocal iproc = require 'iproc'\nlocal reconstruct = require 'reconstruct'\nlocal image_loader = require 'image_loader'\nlocal alpha_util = require 'alpha_util'\n\ntorch.setdefaulttensortype('torch.FloatTensor')\n\nlocal function format_output(opt, src, no)\n   no = no or 1\n   local name = path.basename(src)\n   local e = path.extension(name)\n   local basename = name:sub(0, name:len() - e:len())\n   \n   if opt.o == \"(auto)\" then\n      return path.join(path.dirname(src), string.format(\"%s_%s.png\", basename, opt.m))\n   else\n      local basename_pos = opt.o:find(\"%%s\")\n      local no_pos = opt.o:find(\"%%%d*d\")\n      if basename_pos ~= nil and no_pos ~= nil then\n\t if basename_pos < no_pos then\n\t    return string.format(opt.o, basename, no)\n\t else\n\t    return string.format(opt.o, no, basename)\n\t end\n      elseif basename_pos ~= nil then\n\t return string.format(opt.o, basename)\n      elseif no_pos ~= nil then\n\t return string.format(opt.o, no)\n      else\n\t return opt.o\n      end\n   end\nend\n\nlocal function convert_image(opt)\n   local x, meta = image_loader.load_float(opt.i)\n   if not x then\n      error(string.format(\"failed to load image: %s\", opt.i))\n   end\n   local alpha = meta.alpha\n   local new_x = nil\n   local scale_f, image_f\n\n   if opt.tta == 1 then\n      scale_f = function(model, scale, x, block_size, batch_size)\n\t return reconstruct.scale_tta(model, opt.tta_level,\n\t\t\t\t      scale, x, block_size, batch_size)\n      end\n      image_f = function(model, x, block_size, batch_size)\n\t return reconstruct.image_tta(model, opt.tta_level,\n\t\t\t\t      x, block_size, batch_size)\n      end\n   else\n      scale_f = reconstruct.scale\n      image_f = reconstruct.image\n   end\n   opt.o = format_output(opt, opt.i)\n   if opt.m == \"noise\" then\n      local model_path = path.join(opt.model_dir, (\"noise%d_model.t7\"):format(opt.noise_level))\n      local model = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n      if not model then\n\t error(\"Load Error: \" .. model_path)\n      end\n      local t = sys.clock()\n      new_x = image_f(model, x, opt.crop_size, opt.batch_size)\n      new_x = alpha_util.composite(new_x, alpha)\n      if not opt.q then\n\t print(opt.o .. \": \" .. (sys.clock() - t) .. \" sec\")\n      end\n   elseif opt.m == \"scale\" then\n      local model_path = path.join(opt.model_dir, (\"scale%.1fx_model.t7\"):format(opt.scale))\n      local model = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n      if not model then\n\t error(\"Load Error: \" .. model_path)\n      end\n      local t = sys.clock()\n      x = alpha_util.make_border(x, alpha, reconstruct.offset_size(model))\n      new_x = scale_f(model, opt.scale, x, opt.crop_size, opt.batch_size, opt.batch_size)\n      new_x = alpha_util.composite(new_x, alpha, model)\n      if not opt.q then\n\t print(opt.o .. \": \" .. (sys.clock() - t) .. \" sec\")\n      end\n   elseif opt.m == \"noise_scale\" then\n      local model_path = path.join(opt.model_dir, (\"noise%d_scale%.1fx_model.t7\"):format(opt.noise_level, opt.scale))\n      if path.exists(model_path) then\n\t local scale_model_path = path.join(opt.model_dir, (\"scale%.1fx_model.t7\"):format(opt.scale))\n\t local t, scale_model = pcall(w2nn.load_model, scale_model_path, opt.force_cudnn)\n\t local model = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n\t if not t then\n\t    scale_model = model\n\t end\n\t local t = sys.clock()\n\t x = alpha_util.make_border(x, alpha, reconstruct.offset_size(scale_model))\n\t new_x = scale_f(model, opt.scale, x, opt.crop_size, opt.batch_size)\n\t new_x = alpha_util.composite(new_x, alpha, scale_model)\n\t if not opt.q then\n\t    print(opt.o .. \": \" .. (sys.clock() - t) .. \" sec\")\n\t end\n      else\n\t local noise_model_path = path.join(opt.model_dir, (\"noise%d_model.t7\"):format(opt.noise_level))\n\t local noise_model = w2nn.load_model(noise_model_path, opt.force_cudnn, opt.load_mode)\n\t local scale_model_path = path.join(opt.model_dir, (\"scale%.1fx_model.t7\"):format(opt.scale))\n\t local scale_model = w2nn.load_model(scale_model_path, opt.force_cudnn, opt.load_mode)\n\t local t = sys.clock()\n\t x = alpha_util.make_border(x, alpha, reconstruct.offset_size(scale_model))\n\t x = image_f(noise_model, x, opt.crop_size, opt.batch_size)\n\t new_x = scale_f(scale_model, opt.scale, x, opt.crop_size, opt.batch_size)\n\t new_x = alpha_util.composite(new_x, alpha, scale_model)\n\t if not opt.q then\n\t    print(opt.o .. \": \" .. (sys.clock() - t) .. \" sec\")\n\t end\n      end\n   elseif opt.m == \"user\" then\n      local model_path = opt.model_path\n      local model = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n      if not model then\n\t error(\"Load Error: \" .. model_path)\n      end\n      local t = sys.clock()\n\n      x = alpha_util.make_border(x, alpha, reconstruct.offset_size(model))\n      if opt.scale == 1 then\n\t new_x = image_f(model, x, opt.crop_size, opt.batch_size)\n      else\n\t new_x = scale_f(model, opt.scale, x, opt.crop_size, opt.batch_size)\n      end\n      new_x = alpha_util.composite(new_x, alpha) -- TODO: should it use model?\n      if not opt.q then\n\t print(opt.o .. \": \" .. (sys.clock() - t) .. \" sec\")\n      end\n   else\n      error(\"undefined method:\" .. opt.method)\n   end\n   image_loader.save_png(opt.o, new_x, tablex.update({depth = opt.depth, inplace = true}, meta))\nend\nlocal function convert_frames(opt)\n   local model_path, scale_model, t\n   local noise_scale_model = {}\n   local noise_model = {}\n   local user_model = nil\n   local scale_f, image_f\n   if opt.tta == 1 then\n      scale_f = function(model, scale, x, block_size, batch_size)\n\t return reconstruct.scale_tta(model, opt.tta_level,\n\t\t\t\t      scale, x, block_size, batch_size)\n      end\n      image_f = function(model, x, block_size, batch_size)\n\t return reconstruct.image_tta(model, opt.tta_level,\n\t\t\t\t      x, block_size, batch_size)\n      end\n   else\n      scale_f = reconstruct.scale\n      image_f = reconstruct.image\n   end\n   if opt.m == \"scale\" then\n      model_path = path.join(opt.model_dir, (\"scale%.1fx_model.t7\"):format(opt.scale))\n      scale_model = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n   elseif opt.m == \"noise\" then\n      model_path = path.join(opt.model_dir, string.format(\"noise%d_model.t7\", opt.noise_level))\n      noise_model[opt.noise_level] = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n   elseif opt.m == \"noise_scale\" then\n      local model_path = path.join(opt.model_dir, (\"noise%d_scale%.1fx_model.t7\"):format(opt.noise_level, opt.scale))\n      if path.exists(model_path) then\n\t noise_scale_model[opt.noise_level] = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n\t model_path = path.join(opt.model_dir, (\"scale%.1fx_model.t7\"):format(opt.scale))\n\t t, scale_model = pcall(w2nn.load_model, model_path, opt.force_cudnn, opt.load_mode)\n\t if not t then\n\t    scale_model = noise_scale_model[opt.noise_level]\n\t end\n      else\n\t model_path = path.join(opt.model_dir, (\"scale%.1fx_model.t7\"):format(opt.scale))\n\t scale_model = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n\t model_path = path.join(opt.model_dir, string.format(\"noise%d_model.t7\", opt.noise_level))\n\t noise_model[opt.noise_level] = w2nn.load_model(model_path, opt.force_cudnn, opt.load_mode)\n      end\n   elseif opt.m == \"user\" then\n      user_model = w2nn.load_model(opt.model_path, opt.force_cudnn, opt.load_mode)\n   end\n   local fp = io.open(opt.l)\n   if not fp then\n      error(\"Open Error: \" .. opt.l)\n   end\n   local count = 0\n   local lines = {}\n   for line in fp:lines() do\n      table.insert(lines, line)\n   end\n   fp:close()\n   \n   for i = 1, #lines do\n      local output = format_output(opt, lines[i], i)\n      if opt.resume == 0 or path.exists(output) == false then\n\t local x, meta = image_loader.load_float(lines[i])\n\t if not x then\n\t    io.stderr:write(string.format(\"failed to load image: %s\\n\", lines[i]))\n\t else\n\t    local alpha = meta.alpha\n\t    local new_x = nil\n\t    if opt.m == \"noise\" then\n\t       new_x = image_f(noise_model[opt.noise_level], x, opt.crop_size, opt.batch_size)\n\t       new_x = alpha_util.composite(new_x, alpha)\n\t    elseif opt.m == \"scale\" then\n\t       x = alpha_util.make_border(x, alpha, reconstruct.offset_size(scale_model))\n\t       new_x = scale_f(scale_model, opt.scale, x, opt.crop_size, opt.batch_size)\n\t       new_x = alpha_util.composite(new_x, alpha, scale_model)\n\t    elseif opt.m == \"noise_scale\" then\n\t       x = alpha_util.make_border(x, alpha, reconstruct.offset_size(scale_model))\n\t       if noise_scale_model[opt.noise_level] then\n\t\t  new_x = scale_f(noise_scale_model[opt.noise_level], opt.scale, x, opt.crop_size, opt.batch_size)\n\t       else\n\t\t  x = image_f(noise_model[opt.noise_level], x, opt.crop_size, opt.batch_size)\n\t\t  new_x = scale_f(scale_model, opt.scale, x, opt.crop_size, opt.batch_size)\n\t       end\n\t       new_x = alpha_util.composite(new_x, alpha, scale_model)\n\t    elseif opt.m == \"user\" then\n\t       x = alpha_util.make_border(x, alpha, reconstruct.offset_size(user_model))\n\t       if opt.scale == 1 then\n\t\t  new_x = image_f(user_model, x, opt.crop_size, opt.batch_size)\n\t       else\n\t\t  new_x = scale_f(user_model, opt.scale, x, opt.crop_size, opt.batch_size)\n\t       end\n\t       new_x = alpha_util.composite(new_x, alpha)\n\t    else\n\t       error(\"undefined method:\" .. opt.method)\n\t    end\n\t    image_loader.save_png(output, new_x, \n\t\t\t\t  tablex.update({depth = opt.depth, inplace = true}, meta))\n\t end\n\t if not opt.q then\n\t    xlua.progress(i, #lines)\n\t end\n\t if i % 10 == 0 then\n\t    collectgarbage()\n\t end\n      else\n\t if not opt.q then\n\t    xlua.progress(i, #lines)\n\t end\n      end\n   end\nend\nlocal function waifu2x()\n   local cmd = torch.CmdLine()\n   cmd:text()\n   cmd:text(\"waifu2x\")\n   cmd:text(\"Options:\")\n   cmd:option(\"-i\", \"images/miku_small.png\", 'path to input image')\n   cmd:option(\"-l\", \"\", 'path to image-list.txt')\n   cmd:option(\"-scale\", 2, 'scale factor')\n   cmd:option(\"-o\", \"(auto)\", 'path to output file')\n   cmd:option(\"-depth\", 8, 'bit-depth of the output image (8|16)')\n   cmd:option(\"-model_dir\", \"./models/cunet/art\", 'path to model directory')\n   cmd:option(\"-name\", \"user\", 'model name for user method')\n   cmd:option(\"-m\", \"noise_scale\", 'method (noise|scale|noise_scale|user)')\n   cmd:option(\"-method\", \"\", 'same as -m')\n   cmd:option(\"-noise_level\", 1, '(0|1|2|3)')\n   cmd:option(\"-crop_size\", 256, 'patch size per process')\n   cmd:option(\"-batch_size\", 1, 'batch_size')\n   cmd:option(\"-resume\", 0, \"skip existing files (0|1)\")\n   cmd:option(\"-thread\", -1, \"number of CPU threads\")\n   cmd:option(\"-tta\", 0, 'use TTA mode. It is slow but slightly high quality (0|1)')\n   cmd:option(\"-tta_level\", 8, 'TTA level (2|4|8). A higher value makes better quality output but slow')\n   cmd:option(\"-force_cudnn\", 0, 'use cuDNN backend (0|1)')\n   cmd:option(\"-q\", 0, 'quiet (0|1)')\n   cmd:option(\"-gpu\", 1, 'Device ID')\n   cmd:option(\"-load_mode\", \"ascii\", \"ascii/binary\")\n\n   local opt = cmd:parse(arg)\n   if opt.method:len() > 0 then\n      opt.m = opt.method\n   end\n   if opt.thread > 0 then\n      torch.setnumthreads(opt.thread)\n   end\n   cutorch.setDevice(opt.gpu)\n   if cudnn then\n      cudnn.fastest = true\n      if opt.l:len() > 0 then\n\t cudnn.benchmark = true -- find fastest algo\n      else\n\t cudnn.benchmark = false\n      end\n   end\n   opt.force_cudnn = opt.force_cudnn == 1\n   opt.q = opt.q == 1\n   opt.model_path = path.join(opt.model_dir, string.format(\"%s_model.t7\", opt.name))\n\n   if string.len(opt.l) == 0 then\n      convert_image(opt)\n   else\n      convert_frames(opt)\n   end\nend\nwaifu2x()\n"
        },
        {
          "name": "web.lua",
          "type": "blob",
          "size": 17.54,
          "content": "require 'pl'\nlocal __FILE__ = (function() return string.gsub(debug.getinfo(2, 'S').source, \"^@\", \"\") end)()\nlocal ROOT = path.dirname(__FILE__)\npackage.path = path.join(ROOT, \"lib\", \"?.lua;\") .. package.path\n_G.TURBO_SSL = true\n\nrequire 'w2nn'\nlocal uuid = require 'uuid'\nlocal ffi = require 'ffi'\nlocal md5 = require 'md5'\nlocal iproc = require 'iproc'\nlocal reconstruct = require 'reconstruct'\nlocal image_loader = require 'image_loader'\nlocal alpha_util = require 'alpha_util'\nlocal compression = require 'compression'\nlocal gm = require 'graphicsmagick'\n\n-- Note:  turbo and xlua has different implementation of string:split().\n--         Therefore, string:split() has conflict issue.\n--         In this script, use turbo's string:split().\nlocal turbo = require 'turbo'\n\nlocal cmd = torch.CmdLine()\ncmd:text()\ncmd:text(\"waifu2x-api\")\ncmd:text(\"Options:\")\ncmd:option(\"-port\", 8812, 'listen port')\ncmd:option(\"-gpu\", 1, 'Device ID')\ncmd:option(\"-enable_tta\", 0, 'enable TTA query(0|1)')\ncmd:option(\"-crop_size\", 256, 'patch size per process')\ncmd:option(\"-batch_size\", 1, 'batch size')\ncmd:option(\"-thread\", -1, 'number of CPU threads')\ncmd:option(\"-force_cudnn\", 0, 'use cuDNN backend (0|1)')\ncmd:option(\"-max_pixels\", 3000 * 3000, 'maximum number of output image pixels (e.g. 3000x3000=9000000)')\ncmd:option(\"-curl_request_timeout\", 60, \"request_timeout for curl\")\ncmd:option(\"-curl_connect_timeout\", 60, \"connect_timeout for curl\")\ncmd:option(\"-curl_max_redirects\", 2, \"max_redirects for curl\")\ncmd:option(\"-max_body_size\", 5 * 1024 * 1024, \"maximum allowed size for uploaded files\")\ncmd:option(\"-cache_max\", 200, \"number of cached images on RAM\")\n\nlocal opt = cmd:parse(arg)\ncutorch.setDevice(opt.gpu)\ntorch.setdefaulttensortype('torch.FloatTensor')\nif opt.thread > 0 then\n   torch.setnumthreads(opt.thread)\nend\nif cudnn then\n   cudnn.fastest = true\n   cudnn.benchmark = true\nend\nopt.force_cudnn = opt.force_cudnn == 1\nopt.enable_tta = opt.enable_tta == 1\n--local ART_MODEL_DIR = path.join(ROOT, \"models\", \"upconv_7\", \"art\")\nlocal ART_MODEL_DIR = path.join(ROOT, \"models\", \"cunet\", \"art\")\nlocal PHOTO_MODEL_DIR = path.join(ROOT, \"models\", \"upconv_7\", \"photo\")\nlocal art_model = {\n   scale = w2nn.load_model(path.join(ART_MODEL_DIR, \"scale2.0x_model.t7\"), opt.force_cudnn),\n   noise0_scale = w2nn.load_model(path.join(ART_MODEL_DIR, \"noise0_scale2.0x_model.t7\"), opt.force_cudnn),\n   noise1_scale = w2nn.load_model(path.join(ART_MODEL_DIR, \"noise1_scale2.0x_model.t7\"), opt.force_cudnn),\n   noise2_scale = w2nn.load_model(path.join(ART_MODEL_DIR, \"noise2_scale2.0x_model.t7\"), opt.force_cudnn),\n   noise3_scale = w2nn.load_model(path.join(ART_MODEL_DIR, \"noise3_scale2.0x_model.t7\"), opt.force_cudnn),\n   noise0 = w2nn.load_model(path.join(ART_MODEL_DIR, \"noise0_model.t7\"), opt.force_cudnn),\n   noise1 = w2nn.load_model(path.join(ART_MODEL_DIR, \"noise1_model.t7\"), opt.force_cudnn),\n   noise2 = w2nn.load_model(path.join(ART_MODEL_DIR, \"noise2_model.t7\"), opt.force_cudnn),\n   noise3 = w2nn.load_model(path.join(ART_MODEL_DIR, \"noise3_model.t7\"), opt.force_cudnn)\n}\nlocal photo_model = {\n   scale = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"scale2.0x_model.t7\"), opt.force_cudnn),\n   noise0_scale = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"noise0_scale2.0x_model.t7\"), opt.force_cudnn),\n   noise1_scale = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"noise1_scale2.0x_model.t7\"), opt.force_cudnn),\n   noise2_scale = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"noise2_scale2.0x_model.t7\"), opt.force_cudnn),\n   noise3_scale = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"noise3_scale2.0x_model.t7\"), opt.force_cudnn),\n   noise0 = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"noise0_model.t7\"), opt.force_cudnn),\n   noise1 = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"noise1_model.t7\"), opt.force_cudnn),\n   noise2 = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"noise2_model.t7\"), opt.force_cudnn),\n   noise3 = w2nn.load_model(path.join(PHOTO_MODEL_DIR, \"noise3_model.t7\"), opt.force_cudnn)\n}\ncollectgarbage()\nlocal CLEANUP_MODEL = true -- if you are using the low memory GPU, you could use this flag.\nlocal CACHE_DIR = path.join(ROOT, \"cache\")\nlocal MAX_NOISE_IMAGE = opt.max_pixels\nlocal MAX_SCALE_IMAGE = (math.sqrt(opt.max_pixels) / 2)^2\nlocal PNG_DEPTH = 8\nlocal CURL_OPTIONS = {\n   request_timeout = opt.curl_request_timeout,\n   connect_timeout = opt.curl_connect_timeout,\n   allow_redirects = true,\n   max_redirects = opt.curl_max_redirects\n}\nlocal CURL_MAX_SIZE = opt.max_body_size\n\nlocal function valid_size(x, scale, tta_level)\n   if scale <= 0 then\n      local limit = math.pow(math.floor(math.pow(MAX_NOISE_IMAGE / tta_level, 0.5)), 2)\n      return x:size(2) * x:size(3) <= limit\n   else\n      local limit = math.pow(math.floor(math.pow(MAX_SCALE_IMAGE / tta_level, 0.5)), 2)\n      return x:size(2) * x:size(3) <= limit\n   end\nend\nlocal function auto_tta_level(x, scale)\n   local limit2, limit4, limit8\n   if scale <= 0 then\n      limit2 = math.pow(math.floor(math.pow(MAX_NOISE_IMAGE / 2, 0.5)), 2)\n      limit4 = math.pow(math.floor(math.pow(MAX_NOISE_IMAGE / 4, 0.5)), 2)\n      limit8 = math.pow(math.floor(math.pow(MAX_NOISE_IMAGE / 8, 0.5)), 2)\n   else\n      limit2 = math.pow(math.floor(math.pow(MAX_SCALE_IMAGE / 2, 0.5)), 2)\n      limit4 = math.pow(math.floor(math.pow(MAX_SCALE_IMAGE / 4, 0.5)), 2)\n      limit8 = math.pow(math.floor(math.pow(MAX_SCALE_IMAGE / 8, 0.5)), 2)\n   end\n   local px = x:size(2) * x:size(3)\n   if px <= limit8 then\n      return 8\n   elseif px <= limit4 then\n      return 4\n   elseif px <= limit2 then\n      return 2\n   else\n      return 1\n   end\nend\nlocal function cache_url(url)\n   local hash = md5.sumhexa(url)\n   local cache_file = path.join(CACHE_DIR, \"url_\" .. hash)\n   if path.exists(cache_file) then\n      return image_loader.load_float(cache_file)\n   else\n      local res = coroutine.yield(\n\t turbo.async.HTTPClient({verify_ca=false},\n\t    nil,\n\t    CURL_MAX_SIZE):fetch(url, CURL_OPTIONS)\n      )\n      if res.code == 200 then\n\t local content_type = res.headers:get(\"Content-Type\", true)\n\t if type(content_type) == \"table\" then\n\t    content_type = content_type[1]\n\t end\n\t if content_type and content_type:find(\"image\") then\n\t    local fp = io.open(cache_file, \"wb\")\n\t    local blob = res.body\n\t    fp:write(blob)\n\t    fp:close()\n\t    return image_loader.decode_float(blob)\n\t end\n      end\n   end\n   return nil, nil\nend\nlocal function get_image(req)\n   local file_info = req:get_arguments(\"file\")\n   local url = req:get_argument(\"url\", \"\")\n   local file = nil\n   local filename = nil\n   if file_info and #file_info == 1 then\n      file = file_info[1][1]\n      local disp = file_info[1][\"content-disposition\"]\n      if disp and disp[\"filename\"] then\n\t filename = path.basename(disp[\"filename\"])\n      end\n   end\n   if file and file:len() > 0 then\n      local x, meta = image_loader.decode_float(file)\n      return x, meta, filename\n   elseif url and url:len() > 0 then\n      local x, meta = cache_url(url)\n      return x, meta, filename\n   end\n   return nil, nil, nil\nend\nlocal function cleanup_model(model)\n   if CLEANUP_MODEL then\n      model:clearState() -- release GPU memory\n   end\nend\n\n-- cache\nlocal g_cache = {}\nlocal function cache_count()\n   local count = 0\n   for _ in pairs(g_cache) do\n      count = count + 1\n   end\n   return count\nend\nlocal function cache_remove_old()\n   local old_time = nil\n   local old_key = nil\n   for k, v in pairs(g_cache) do\n      if old_time == nil or old_time > v.updated_at then\n\t old_key = k\n\t old_time = v.updated_at\n      end\n   end\n   if old_key then\n      g_cache[old_key] = nil\n   end\nend\nlocal function cache_compress(raw_image)\n   if raw_image then\n      compressed_image = compression.compress(iproc.float2byte(raw_image))\n      return compressed_image\n   else\n      return nil\n   end\nend\nlocal function cache_decompress(compressed_image)\n   if compressed_image then\n      local raw_image = compression.decompress(compressed_image)\n      return iproc.byte2float(raw_image)\n   else\n      return nil\n   end\nend\nlocal function cache_get(filename)\n   local cache = g_cache[filename]\n   if cache then\n      return {image = cache_decompress(cache.image),\n\t      alpha = cache_decompress(cache.alpha)}\n   else\n      return nil\n   end\nend\nlocal function cache_put(filename, image, alpha)\n   g_cache[filename] = {image = cache_compress(image),\n\t\t\talpha = cache_compress(alpha),\n\t\t\tupdated_at = os.time()};\n   local count = cache_count(g_cache)\n   if count > opt.cache_max then\n      cache_remove_old()\n   end\nend\nlocal function convert(x, meta, options)\n   local cache_file = path.join(CACHE_DIR, options.prefix .. \".png\")\n   local alpha = meta.alpha\n   local alpha_orig = alpha\n   local cache = cache_get(cache_file)\n\n   if cache then\n      meta = tablex.copy(meta)\n      meta.alpha = cache.alpha\n      return cache.image, meta\n   else\n      local model = nil\n      if options.style == \"art\" then\n\t model = art_model\n      elseif options.style == \"photo\" then\n\t model = photo_model\n      end\n      if options.border then\n\t x = alpha_util.make_border(x, alpha_orig, reconstruct.offset_size(model.scale))\n      end\n      if (options.method == \"scale\" or\n\t     options.method == \"noise0_scale\" or\n\t     options.method == \"noise1_scale\" or\n\t     options.method == \"noise2_scale\" or\n\t     options.method == \"noise3_scale\")\n      then\n\t x = reconstruct.scale_tta(model[options.method], options.tta_level, 2.0, x,\n\t\t\t\t   opt.crop_size, opt.batch_size)\n\t if alpha then\n\t    if not (alpha:size(2) == x:size(2) and alpha:size(3) == x:size(3)) then\n\t       alpha = reconstruct.scale(model.scale, 2.0, alpha,\n\t\t\t\t\t opt.crop_size, opt.batch_size)\n\t       cleanup_model(model.scale)\n\t    end\n\t end\n\t cleanup_model(model[options.method])\n      elseif (options.method == \"noise0\" or\n\t\t options.method == \"noise1\" or\n\t\t options.method == \"noise2\" or\n\t\t options.method == \"noise3\")\n      then\n\t x = reconstruct.image_tta(model[options.method], options.tta_level,\n\t\t\t\t   x, opt.crop_size, opt.batch_size)\n\t cleanup_model(model[options.method])\n      end\n      cache_put(cache_file, x, alpha)\n      meta = tablex.copy(meta)\n      meta.alpha = alpha\n      return x, meta\n   end\nend\nlocal function client_disconnected(handler)\n   return not(handler.request and\n\t\t handler.request.connection and\n\t\t handler.request.connection.stream and\n\t\t (not handler.request.connection.stream:closed()))\nend\nlocal function make_output_filename(filename, mode)\n   local e = path.extension(filename)\n   local base = filename:sub(0, filename:len() - e:len())\n   if mode then\n      return base .. \"_waifu2x_\" .. mode .. \".png\"\n   else\n      return base .. \".png\"\n   end\nend\n\nlocal APIHandler = class(\"APIHandler\", turbo.web.RequestHandler)\nfunction APIHandler:post()\n   if client_disconnected(self) then\n      self:set_status(400)\n      self:write(\"client disconnected\")\n      return\n   end\n   local x, meta, filename = get_image(self)\n   local scale = tonumber(self:get_argument(\"scale\", \"-1\"))\n   local noise = tonumber(self:get_argument(\"noise\", \"-1\"))\n   local tta_level = tonumber(self:get_argument(\"tta_level\", \"1\"))\n   local style = self:get_argument(\"style\", \"art\")\n   local download = (self:get_argument(\"download\", \"\")):len()\n\n   if client_disconnected(self) then\n      self:set_status(400)\n      self:write(\"client disconnected\")\n      return\n   end\n   if opt.enable_tta then\n      if tta_level == 0 then\n\t tta_level = auto_tta_level(x, scale)\n      end\n      if not (tta_level == 0 or tta_level == 1 or tta_level == 2 or tta_level == 4 or tta_level == 8) then\n\t tta_level = 1\n      end\n   else\n      tta_level = 1\n   end\n   if style ~= \"art\" then\n      style = \"photo\" -- style must be art or photo\n   end\n   if x and valid_size(x, scale, tta_level) then\n      local prefix = nil\n      if (noise >= 0 or scale > 0) then\n\t local hash = md5.sumhexa(meta.blob)\n\t local alpha_prefix = style .. \"_\" .. hash .. \"_alpha\"\n\t local border = false\n\t if scale >= 0 and meta.alpha then\n\t    border = true\n\t end\n\t if (scale == 1 or scale == 2) and (noise < 0) then\n\t    prefix = style .. \"_scale_tta_\"  .. tta_level .. \"_\"\n\t    x, meta = convert(x, meta, {method = \"scale\",\n\t\t\t\t\tstyle = style,\n\t\t\t\t\ttta_level = tta_level,\n\t\t\t\t\tprefix = prefix .. hash,\n\t\t\t\t\talpha_prefix = alpha_prefix,\n\t\t\t\t\tborder = border})\n\t    if scale == 1 then\n\t       x = iproc.scale(x, x:size(3) * (1.6 / 2.0), x:size(2) * (1.6 / 2.0), \"Sinc\")\n\t    end\n\t elseif (scale == 1 or scale == 2) and (noise == 0 or noise == 1 or noise == 2 or noise == 3) then\n\t    prefix = style .. string.format(\"_noise%d_scale_tta_\", noise)  .. tta_level .. \"_\"\n\t    x, meta = convert(x, meta, {method = string.format(\"noise%d_scale\", noise),\n\t\t\t\t\tstyle = style,\n\t\t\t\t\ttta_level = tta_level,\n\t\t\t\t\tprefix = prefix .. hash,\n\t\t\t\t\talpha_prefix = alpha_prefix,\n\t\t\t\t\tborder = border})\n\t    if scale == 1 then\n\t       x = iproc.scale(x, x:size(3) * (1.6 / 2.0), x:size(2) * (1.6 / 2.0), \"Sinc\")\n\t    end\n\t elseif (noise == 0 or noise == 1 or noise == 2 or noise == 3) then\n\t    prefix = style .. string.format(\"_noise%d_tta_\", noise) .. tta_level .. \"_\"\n\t    x = convert(x, meta, {method = string.format(\"noise%d\", noise), \n\t\t\t\t  style = style, \n\t\t\t\t  tta_level = tta_level,\n\t\t\t\t  prefix = prefix .. hash,\n\t\t\t\t  alpha_prefix = alpha_prefix,\n\t\t\t\t  border = border})\n\t    border = false\n\t end\n      end\n      local name = nil\n      if filename then \n\t if prefix then\n\t    name = make_output_filename(filename, prefix:sub(0, prefix:len()-1))\n\t else\n\t    name = make_output_filename(filename, nil)\n\t end\n      else\n\t name = uuid() .. \".png\"\n      end\n      local blob = image_loader.encode_png(alpha_util.composite(x, meta.alpha),\n\t\t\t\t\t   tablex.update({depth = PNG_DEPTH, inplace = true}, meta))\n\n      self:set_header(\"Content-Length\", string.format(\"%d\", #blob))\n      if download > 0 then\n\t self:set_header(\"Content-Type\", \"application/octet-stream\")\n\t self:set_header(\"Content-Disposition\", string.format('attachment; filename=\"%s\"', name))\n      else\n\t self:set_header(\"Content-Type\", \"image/png\")\n\t self:set_header(\"Content-Disposition\", string.format('inline; filename=\"%s\"', name))\n      end\n      self:write(blob)\n   else\n      if not x then\n\t self:set_status(400)\n\t self:write(\"ERROR: An error occurred. (unsupported image format/connection timeout/file is too large)\")\n      else\n\t self:set_status(400)\n\t self:write(\"ERROR: image size exceeds maximum allowable size.\")\n      end\n   end\n   collectgarbage()\nend\nlocal FormHandler = class(\"FormHandler\", turbo.web.RequestHandler)\nlocal index_ja = file.read(path.join(ROOT, \"assets\", \"index.ja.html\"))\nlocal index_ru = file.read(path.join(ROOT, \"assets\", \"index.ru.html\"))\nlocal index_pt = file.read(path.join(ROOT, \"assets\", \"index.pt.html\"))\nlocal index_es = file.read(path.join(ROOT, \"assets\", \"index.es.html\"))\nlocal index_fr = file.read(path.join(ROOT, \"assets\", \"index.fr.html\"))\nlocal index_de = file.read(path.join(ROOT, \"assets\", \"index.de.html\"))\nlocal index_tr = file.read(path.join(ROOT, \"assets\", \"index.tr.html\"))\nlocal index_zh_cn = file.read(path.join(ROOT, \"assets\", \"index.zh-CN.html\"))\nlocal index_zh_tw = file.read(path.join(ROOT, \"assets\", \"index.zh-TW.html\"))\nlocal index_ko = file.read(path.join(ROOT, \"assets\", \"index.ko.html\"))\nlocal index_nl = file.read(path.join(ROOT, \"assets\", \"index.nl.html\"))\nlocal index_ca = file.read(path.join(ROOT, \"assets\", \"index.ca.html\"))\nlocal index_ro = file.read(path.join(ROOT, \"assets\", \"index.ro.html\"))\nlocal index_it = file.read(path.join(ROOT, \"assets\", \"index.it.html\"))\nlocal index_eo = file.read(path.join(ROOT, \"assets\", \"index.eo.html\"))\nlocal index_no = file.read(path.join(ROOT, \"assets\", \"index.no.html\"))\nlocal index_uk = file.read(path.join(ROOT, \"assets\", \"index.uk.html\"))\nlocal index_pl = file.read(path.join(ROOT, \"assets\", \"index.pl.html\"))\nlocal index_bg = file.read(path.join(ROOT, \"assets\", \"index.bg.html\"))\n\nlocal index_en = file.read(path.join(ROOT, \"assets\", \"index.html\"))\nfunction FormHandler:get()\n   local lang = self.request.headers:get(\"Accept-Language\")\n   if lang then\n      local langs = utils.split(lang, \",\")\n      for i = 1, #langs do\n\t langs[i] = utils.split(langs[i], \";\")[1]\n      end\n      if langs[1] == \"ja\" then\n\t self:write(index_ja)\n      elseif langs[1] == \"ru\" then\n\t self:write(index_ru)\n      elseif langs[1] == \"pt\" or langs[1] == \"pt-BR\" then\n\t self:write(index_pt)\n      elseif langs[1] == \"es\" or langs[1] == \"es-ES\" then\n\t self:write(index_es)\n      elseif langs[1] == \"fr\" then\n\t self:write(index_fr)\n      elseif langs[1] == \"de\" then\n\t self:write(index_de)\n      elseif langs[1] == \"tr\" then\n\t self:write(index_tr)\n      elseif langs[1] == \"zh-CN\" or langs[1] == \"zh\" then\n\t self:write(index_zh_cn)\n      elseif langs[1] == \"zh-TW\" then\n\t self:write(index_zh_tw)\n      elseif langs[1] == \"ko\" then\n\t self:write(index_ko)\n      elseif langs[1] == \"nl\" then\n\t self:write(index_nl)\n      elseif langs[1] == \"ca\" or langs[1] == \"ca-ES\" or langs[1] == \"ca-FR\" or langs[1] == \"ca-IT\" or langs[1] == \"ca-AD\" then\n\t self:write(index_ca)\n      elseif langs[1] == \"ro\" then\n\t self:write(index_ro)\n      elseif langs[1] == \"it\" then\n\t self:write(index_it)\n      elseif langs[1] == \"eo\" then\n\t self:write(index_eo)\n      elseif langs[1] == \"no\" then\n\t self:write(index_no)\n      elseif langs[1] == \"uk\" then\n\t self:write(index_uk)\n      elseif langs[1] == \"pl\" then\n\t self:write(index_pl)\n      elseif langs[1] == \"bg\" then\n\t self:write(index_bg)\n      else\n\t self:write(index_en)\n      end\n   else\n      self:write(index_en)\n   end\nend\nturbo.log.categories = {\n   [\"success\"] = true,\n   [\"notice\"] = false,\n   [\"warning\"] = true,\n   [\"error\"] = true,\n   [\"debug\"] = false,\n   [\"development\"] = false\n}\nlocal app = turbo.web.Application:new(\n   {\n      {\"^/$\", FormHandler},\n      {\"^/api$\", APIHandler},\n      {\"^/([%a%d%.%-_]+)$\", turbo.web.StaticFileHandler, path.join(ROOT, \"assets/\")},\n   }\n)\napp:listen(opt.port, \"0.0.0.0\", {max_body_size = CURL_MAX_SIZE})\nturbo.ioloop.instance():start()\n"
        },
        {
          "name": "webgen",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}