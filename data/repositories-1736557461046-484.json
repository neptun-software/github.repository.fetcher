{
  "metadata": {
    "timestamp": 1736557461046,
    "page": 484,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "exacity/deeplearningbook-chinese",
      "stars": 35951,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.26,
          "content": ".*\n*.aux\n*.gz\n*.aux\n*.backup \n*.log\n*.kilepr\n*.toc\n*.out\n*.acn\n*.acr\n*.alg\n*.brf\n*.pdf\n*.xdy\n*.bbl\n*.blg\n*.idx\n*.ind\n*/figures\n*.sh\n*.glo\n*.ilg\n*.ist\nagreement.jpg\ndlbook_cn_public.tex\ndlbook_cn_public.bib\nfont\ndocs/_site/\ndocs/images/\n\nmain.bib\nmain.tex\n\n!donation.pdf\n\n"
        },
        {
          "name": "Chapter1",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter10",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter11",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter12",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter13",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter14",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter15",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter16",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter17",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter18",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter19",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter2",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter20",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter3",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter4",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter5",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter6",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter7",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter8",
          "type": "tree",
          "content": null
        },
        {
          "name": "Chapter9",
          "type": "tree",
          "content": null
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.25,
          "content": "all:\n\txelatex dlbook_cn.tex && bibtex dlbook_cn.aux && texindy dlbook_cn.idx && makeglossaries dlbook_cn && xelatex dlbook_cn.tex && xelatex dlbook_cn.tex\n\nclean:\n\tfind . -type f -iregex '.*\\.\\(aux\\|log\\|toc\\|backup\\|acr\\|brf\\|gz\\|acn\\|xdy\\|alg\\)$$'  -delete\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.54,
          "content": "# Deep Learning 中文翻译\n\n在众多网友的帮助和校对下，中文版终于出版了。尽管还有很多问题，但至少90%的内容是可读的，并且是准确的。\n我们尽可能地保留了原书[Deep Learning](http://www.deeplearningbook.org/)中的意思并保留原书的语句。\n\n然而我们水平有限，我们无法消除众多读者的方差。我们仍需要大家的建议和帮助，一起减小翻译的偏差。\n\n大家所要做的就是阅读，然后汇总你的建议，提issue（最好不要一个一个地提）。如果你确定你的建议不需要商量，可以直接发起PR。\n\n对应的翻译者：\n  - 第1、4、7、10、14、20章及第12.4、12.5节由 @swordyork 负责\n  - 第2、5、8、11、15、18章由 @liber145 负责\n  - 第3、6、9章由 @KevinLee1110 负责\n  - 第13、16、17、19章及第12.1至12.3节由 @futianfan 负责\n\n\n\n面向的读者\n--------------------\n\n请直接下载[PDF](https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf)阅读。\n不打算提供EPUB等格式，如有需要请自行修改。\n\n这一版准确性已经有所提高，读者可以以中文版为主、英文版为辅来阅读学习，但我们仍建议研究者阅读[原版](http://www.deeplearningbook.org/)。\n\n\n\n出版及开源原因\n--------------------\n\n本书由人民邮电出版社出版，如果你觉得中文版PDF对你有所帮助，希望你能支持下纸质正版书籍。\n如果你觉得中文版不行，希望你能多提建议。非常感谢各位！\n纸质版也会进一步更新，需要大家更多的建议和意见，一起完善中文版。\n\n纸质版目前在人民邮电出版社的异步社区出售，见[地址](http://www.epubit.com.cn/book/details/4278)。\n价格不低，但看了样本之后，我们认为物有所值。\n注意，我们不会通过媒体进行宣传，希望大家先看电子版内容，再判断是否购买纸质版。\n\n\n以下是开源的具体原因：\n\n 1. 我们不是文学工作者，不专职翻译。单靠我们，无法给出今天的翻译，众多网友都给我们提出了宝贵的建议，因此开源帮了很大的忙。出版社会给我们稿费（我们也不知道多少，可能2万左右），我们也不好意思自己用，商量之后觉得捐出是最合适的，以所有贡献过的网友的名义（我们把稿费捐给了杉树公益，用于4名贵州高中生三年的生活费，见[捐赠情况](https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf)）。\n 2. PDF电子版对于技术类书籍来说是很重要的，随时需要查询，拿着纸质版到处走显然不合适。国外很多技术书籍都有对应的电子版（虽然不一定是正版），而国内的几乎没有。个人认为这是出版社或者作者认为国民素质还没有高到主动为知识付费的境界，所以不愿意\"泄露\"电子版。时代在进步，我们也需要改变。特别是翻译作品普遍质量不高的情况下，要敢为天下先。\n 3. 深度学习发展太快，日新月异，所以我们希望大家更早地学到相关的知识。我觉得原作者开放PDF电子版也有类似的考虑，也就是先阅读后付费。我们认为中国人口素质已经足够高，懂得为知识付费。当然这不是付给我们的，是付给出版社的，出版社再付给原作者。我们不希望中文版的销量因PDF电子版的存在而下滑。出版社只有值回了版权才能在以后引进更多的优秀书籍。我们这个开源翻译先例也不会成为一个反面案例，以后才会有更多的PDF电子版。\n 4. 开源也涉及版权问题，出于版权原因，我们不再更新此初版PDF文件，请大家以最终的纸质版为准。（但源码会一直更新）\n\n\n\n致谢\n--------------------\n\n我们有3个类别的校对人员。\n - 负责人也就是对应的翻译者。\n - 简单阅读，对语句不通顺或难以理解的地方提出修改意见。\n - 中英对比，进行中英对应阅读，排除少翻错翻的情况。\n\n所有校对建议都保存在各章的`annotations.txt`文件中。\n\n| 章节 | 负责人 | 简单阅读 | 中英对比 |\n| ------------ | ------------ | ------------ | ------------ |\n| [第一章 前言](https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/) | @swordyork | lc, @SiriusXDJ, @corenel, @NeutronT | @linzhp |\n| [第二章 线性代数](https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/) | @liber145 | @SiriusXDJ, @angrymidiao | @badpoem |\n| [第三章 概率与信息论](https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/) | @KevinLee1110 | @SiriusXDJ | @kkpoker, @Peiyan |\n| [第四章 数值计算](https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/) | @swordyork | @zhangyafeikimi | @hengqujushi |\n| [第五章 机器学习基础](https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/) | @liber145 | @wheaio, @huangpingchun | @fairmiracle, @linzhp |\n| [第六章 深度前馈网络](https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/) | @KevinLee1110 | David_Chow, @linzhp, @sailordiary |  |\n| [第七章 深度学习中的正则化](https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/) | @swordyork | | @NBZCC |\n| [第八章 深度模型中的优化](https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/) | @liber145 | @happynoom, @codeVerySlow |  @huangpingchun |\n| [第九章 卷积网络](https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/) | @KevinLee1110 | @zhaoyu611, @corenel | @zhiding |\n| [第十章 序列建模：循环和递归网络](https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/) | @swordyork | lc | @zhaoyu611, @yinruiqing |\n| [第十一章 实践方法论](https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/) | @liber145 |  |  |\n| [第十二章 应用](https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/) | @swordyork, @futianfan |  | @corenel |\n| [第十三章 线性因子模型](https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/) | @futianfan | @cloudygoose | @ZhiweiYang |\n| [第十四章 自编码器](https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/) | @swordyork |  | @Seaball, @huangpingchun |\n| [第十五章 表示学习](https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/) | @liber145 | @cnscottzheng | |\n| [第十六章 深度学习中的结构化概率模型](https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/) | @futianfan | |\n| [第十七章 蒙特卡罗方法](https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/) | @futianfan |  | @sailordiary  |\n| [第十八章 面对配分函数](https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/) | @liber145 | | @tankeco |\n| [第十九章 近似推断](https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/) | @futianfan | | @sailordiary, @hengqujushi, huanghaojun |\n| [第二十章 深度生成模型](https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/) | @swordyork | | |\n| 参考文献 | | | @pkuwwt |\n\n我们会在纸质版正式出版的时候，在书中致谢，正式感谢各位作出贡献的同学！\n\n还有很多同学提出了不少建议，我们都列在此处。\n\n@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz \n@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc \n@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp \n@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88\n@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY\nFlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC\n@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool\n@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd\n\n如有遗漏，请务必通知我们，可以发邮件至`echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode`。\n这是我们必须要感谢的，所以不要不好意思。\n\n\nTODO\n---------\n\n 1. 排版\n\n\n\n注意\n-----------\n\n - 各种问题或者建议可以提issue，建议使用中文。 \n - 由于版权问题，我们不能将图片和bib上传，请见谅。\n - Due to copyright issues, we would not upload figures and the bib file.\n - 可用于学习研究目的，不得用于任何商业行为。谢谢！\n\n\n\nMarkdown格式\n------------\n这种格式确实比较重要，方便查阅，也方便索引。初步转换后，生成网页，具体见[deeplearningbook-chinese](https://exacity.github.io/deeplearningbook-chinese)。\n注意，这种转换没有把图放进去，也不会放图。目前使用单个[脚本](scripts/convert2md.sh)，基于latex文件转换，以后可能会更改但原则是不直接修改[md文件](docs/_posts)。\n需要的同学可以自行修改[脚本](scripts/convert2md.sh)。\n\n\n\nHTML格式\n------------\n读者可以使用[pdf2htmlEX](https://github.com/coolwanglu/pdf2htmlEX)进行转换，直接将PDF转换为HTML。\n\n\n\nUpdating.....\n"
        },
        {
          "name": "acknowledgments.tex",
          "type": "blob",
          "size": 10.31,
          "content": "% !Mode:: \"TeX:UTF-8\"\n\\chapter*{中文版致谢}\n\\addcontentsline{toc}{chapter}{致谢}\n\n首先，我们要感谢原作者在本书翻译时给予我们的大力帮助。特别是，原作者和我们分享了书中的原图和参考文献库，这极大节省了我们的时间和精力。\n\n本书涉及的内容博大且思想深刻，如果没有众多同学和网友的帮助，我们不可能顺利完成翻译。\n \n我们才疏学浅而受此重任，深知自身水平难以将本书翻译得很准确。\n因此我们完成草稿后，将书稿公开于Github，及早接受网友的批评和建议。\n以下网友为本书的翻译草稿提供了很多及时的反馈和宝贵的修改意见：\n@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz @weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang \n@oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc @bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai \n@Elvinczp @endymecy @9578577 @linzhp @cnscottzheng @germany-zhu @zhangyafeikimi @showgood163 @kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao\n@ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 @EmisXXY @FlyingFire @vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung \n@imageslr @indam @XuLYC @zhouqingping @freedomRen @runPenguin @piantou\n \n在此期间，我们四位译者再次进行了校对并且相互之间也校对了一遍。\n然而仅仅通过我们的校对，实在难以发现翻译中存在的问题。\n因此，我们邀请一些同学和网友帮助我们校对。\n经过他们的校对，本书的翻译质量得到了极大的提升。\n在此我们一一列出，以表示我们由衷的感谢！\n \n\\begin{itemize}\n\\item  第一章（引言）： 刘畅、许丁杰、潘雨粟和NeutronT对本章进行了阅读，并对很多语句提出了不少修改建议。林中鹏进行了校对，他提出了很多独到的修改建议。\n\\item  第二章（线性代数）：许丁杰和骆徐圣阅读本章，并修改语句。李若愚进行了校对，提出了很多细心的建议。\n\\item  第三章（概率与信息论）：许丁杰阅读本章，并修改语句。李培炎和何翊卓进行了校对，并修改了很多中文用词，使翻译更加准确。\n\\item  第四章（数值计算）：张亚霏阅读本章，并对其他章节也有提出了一些修改建议。张源源进行了校对，并指出了原文可能存在的问题，非常仔细。\n\\item  第五章（机器学习基础）：郭浩和黄平春阅读本章，并修改语句。李东和林中鹏进行了校对。本章篇幅较长，能够有现在的翻译质量离不开这四位的贡献。\n\\item  第六章（深度前馈网络）：周卫林、林中鹏和张远航阅读本章，并提出修改意见。\n\\item  第七章（深度学习中的正则化）：周柏村进行了非常细心的校对，指出了大量问题，令翻译更加准确。\n\\item  第八章（深度模型中的优化）：房晓宇和吴翔阅读本章。黄平春进行了校对，他提出的很多建议让行文更加流畅易懂。\n\\item  第九章（卷积网络）：赵雨和潘雨粟阅读本章，并润色语句。丁志铭进行了非常仔细的校对，并指出很多翻译问题。\n\\item  第十章（序列建模：循环和递归网络）：刘畅阅读本章。赵雨提供了详细的校对建议，尹瑞清根据他的翻译版本，给我们的版本提出了很多建议。虽然仍存在一些分歧，但我们两个版本的整合，让翻译质量提升很多。\n\\item  第十二章（应用）：潘雨粟进行了校对，在他的校对之前，本章阅读起来比较困难。他提供的修改建议，不仅提高了行文流畅度，还提升了译文的准确度。\n\\item  第十三章（线性因子模型）：贺天行阅读本章，修改语句。杨志伟校对本章，润色大量语句。\n\\item  第十四章（自编码器）：李雨慧和黄平春进行了校对。李雨慧提升了语言的流畅度，黄平春纠正了不少错误，提高了准确性。\n\\item  第十五章（表示学习）：cnscottzheng阅读本章，并修改语句。\n\\item  第十七章（蒙特卡罗方法）：张远航提供了非常细致的校对，后续还校对了一遍，使译文质量大大提升。\n\\item  第十八章（直面配分函数）：吴家楠进行了校对，提升了译文准确性和可读性。\n\\item  第十九章（近似推断）：黄浩军、张远航和张源源进行了校对。这章虽篇幅不大，但内容有深度，译文在三位的帮助下提高了准确度。\n\\end{itemize}\n \n所有校对的修改建议都保存在Github上，再次感谢以上同学和网友的付出。\n经过这五个多月的修改，草稿慢慢变成了初稿。\n尽管还有很多问题，但大部分内容是可读的，并且是准确的。\n当然目前的翻译仍存在一些没有及时发现的问题，因此翻译也将持续更新，不断修改。\n我们非常希望读者能到Github提建议，并且非常欢迎，无论多么小的修改建议，都是非常宝贵的。\n\n此外，我们还要感谢魏太云学长，他帮助我们与出版社沟通交流，并给予了我们很多排版上的指导。\n\n最后，感谢我们的导师张志华教授，没有老师的支持，我们难以完成翻译。\n\n\n\\chapter*{原书致谢}\n\n如果没有他人的贡献，这本书将不可能完成。\n我们感谢为本书提出建议和帮助组织内容结构的人：\nGuillaume Alain, Kyunghyun Cho, \\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre, David Krueger, Hugo Larochelle, Razvan Pascanu and Thomas Roh\\'ee 。\n\n\n我们感谢为本书内容提供反馈的人。其中一些人对许多章都给出了建议：\nMart\\'in Abadi, Guillaume Alain, Ion Androutsopoulos, Fred Bertsch, Olexa Bilaniuk, Ufuk Can Biçici, Matko Bo\\v{s}njak, John Boersma, Greg Brockman, Alexandre de Brébisson, Pierre Luc Carrier, Sarath Chandar, Pawel Chilinski, Mark Daoust, Oleg Dashevskii, Laurent Dinh, Stephan Dreseitl, Jim Fan, Miao Fan, Meire Fortunato, Fr\\'ed\\'eric Francis, Nando de Freitas, \\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre, Jurgen Van Gael, Javier Alonso Garc\\'ia, Jonathan Hunt, Gopi Jeyaram, Chingiz Kabytayev, Lukasz Kaiser, Varun Kanade, Asifullah Khan, Akiel Khan, John King, Diederik P. Kingma, Yann LeCun, Rudolf Mathey, Matías Mattamala, Abhinav Maurya, Kevin Murphy, Oleg Mürk, Roman Novak, Augustus Q. Odena, Simon Pavlik, Karl Pichotta, Eddie Pierce, Kari Pulli, Roussel Rahman, Tapani Raiko, Anurag Ranjan, Johannes Roith, Mihaela Rosca, Halis Sak, César Salgado, Grigory Sapunov, Yoshinori Sasaki, Mike Schuster, Julian Serban, Nir Shabat, Ken Shirriff, Andre Simpelo, Scott Stanley, David Sussillo, Ilya Sutskever, Carles Gelada Sáez, Graham Taylor, Valentin Tolmer, Massimiliano Tomassoli, An Tran, Shubhendu Trivedi, Alexey Umnov, Vincent Vanhoucke, Marco Visentini-Scarzanella, Martin Vita, David Warde-Farley, Dustin Webb, Kelvin Xu, Wei Xue, Ke Yang, Li Yao, Zygmunt Zaj\\k{a}c and Ozan \\c{C}a\\u{g}layan.\n\n\n我们也要感谢对单个章节提供有效反馈的人：\n\n\n\\begin{itemize}\n\\item 数学符号： Zhang Yuanhang.\n\\item  第一章（引言）： \nYusuf Akgul, Sebastien Bratieres, Samira Ebrahimi, Charlie Gorichanaz, Brendan Loudermilk, Eric Morris, Cosmin Pârvulescu and Alfredo Solano.\n\\item  第二章（线性代数）：\nAmjad Almahairi, Nikola Bani\\'{c}, Kevin Bennett, Philippe Castonguay, Oscar Chang, Eric Fosler-Lussier, Andrey Khalyavin, Sergey Oreshkov, Istv\\'an Petr\\'as, Dennis Prangle, Thomas Roh\\'ee, Gitanjali Gulve Sehgal, Colby Toland, Alessandro Vitale and Bob Welland.\n\\item  第三章（概率与信息论）：\nJohn Philip Anderson, Kai Arulkumaran, Vincent Dumoulin, Rui Fa, Stephan Gouws, Artem Oboturov, Antti Rasmus, Alexey Surkov and Volker Tresp.\n\\item  第四章（数值计算）：\nTran Lam AnIan Fischer and Hu Yuhuang.\n\\item  第五章（机器学习基础）：\nDzmitry Bahdanau, Justin Domingue, Nikhil Garg, Makoto Otsuka, Bob Pepin, Philip Popien, Emmanuel Rayner, Peter Shepard, Kee-Bong Song, Zheng Sun and Andy Wu.\n\\item 第六章（深度前馈网络）：\nUriel Berdugo, Fabrizio Bottarel, Elizabeth Burl, Ishan Durugkar, Jeff Hlywa, Jong Wook Kim, David Krueger and Aditya Kumar Praharaj.\n\\item 第七章（深度学习中的正则化）：\nMorten Kolbæk, Kshitij Lauria, Inkyu Lee, Sunil Mohan, Hai Phong Phan and Joshua Salisbury.\n\\item  第八章（深度模型中的优化）：\nMarcel Ackermann, Peter Armitage, Rowel Atienza, Andrew Brock, Tegan Maharaj, James Martens, Kashif Rasul, Klaus Strobl and Nicholas Turner.\n\\item 第九章（卷积网络）：\nMart\\'in Arjovsky, Eugene Brevdo, Konstantin Divilov, Eric Jensen, Mehdi Mirza, Alex Paino, Marjorie Sayer, Ryan Stout and Wentao Wu.\n\\item 第十章（序列建模：循环和递归网络）：\nGökçen Eraslan, Steven Hickson, Razvan Pascanu, Lorenzo von Ritter, Rui Rodrigues, Dmitriy Serdyuk, Dongyu Shi and Kaiyu Yang.\n\\item 第十一章（实践方法论）：\nDaniel Beckstein.\n\\item 第十二章（应用）：\nGeorge Dahl, Vladimir Nekrasov and Ribana Roscher.\n\\item 第十三章（线性因子模型）：\nJayanth Koushik.\n\\item 第十五章（表示学习）：\n    Kunal Ghosh.\n\\item 第十六章（ 深度学习中的结构化概率模型）： \n    Minh Lê and Anton Varfolom.\n\\item 第十八章（直面配分函数）：\n\tSam Bowman.\n\\item 第十九章（近似推断）：\nYujia Bao.\n\\item 第二十章（深度生成模型）：\nNicolas Chapados, Daniel Galvez, Wenming Ma, Fady Medhat, Shakir Mohamed and Gr\\'egoire Montavon.\n\\item 参考文献：\nLukas Michelbacher and Leslie N. Smith.\n\\end{itemize}\n% CHECK: make sure the chapters are still in order\n\n\n我们还要感谢那些允许我们从他们的出版物中复制图片、数据的人。\n我们在图片标题的文字中注明了他们的贡献。\n\n我们还要感谢Lu Wang为我们写了pdf2htmlEX，我们用它来制作这本书的网页版本，Lu Wang还帮助我们改进了生成的HTML的质量。\n\n\n我们还要感谢Ian的妻子Daniela Flori Goodfellow，在Ian的写作过程中的耐心支持和检查。\n\n\n我们还要感谢Google Brain团队提供了学术环境，从而使得Ian能够花费大量时间写作此书并接受同行的反馈和指导。\n我们特别感谢Ian的前任经理Greg Corrado 和他的现任经理Samy Bengio对这个项目的支持。\n最后我们还要感谢Geoffrey Hinton在写作困难时的鼓励。\n\n\n"
        },
        {
          "name": "acknowledgments_github.md",
          "type": "blob",
          "size": 6.77,
          "content": "#致谢\n\n\n2016年12月8日\n------------\n - @tttwwy ==> 链接问题\n - @tankeco ==> Chapter12 语句不通\n - @fairmiracle ==> Chapter2 公式问题，Chapter4 邻域==>领域\n - @GageGao ==> Chapter5 Iris在里面被翻译成虹膜，应该翻译成鸢尾花\n - @huangpingchun ==> Chapter1 \"多么\"误打为\"多少\"了, Chapter5大量建议，详见[issue](https://github.com/exacity/deeplearningbook-chinese/issues/10)\n - @MaHongP ==> Chapter6 纠正词语, p139 ，\"仿射\"，\"及其\" \n - @acgtyrant ==> 翻译者改进英式中文\n - @yanhuibin315 ==> gitbook\n - @Buttonwood ==> Chapter2 dot product\n - @titicacafz ==> Chapter2 dot product\n - @weijy026a ==> Chapter1 Inventors翻译建议\n - @RuiZhang1993 ==> Chapter2 公式有误\n - @zymiboxpay ==> Chapter2 等号消失\n - @xingkongliang ==> Chapter2 公式有误\n - @oisc ==>  Chapter2 公式2.83 2.84有误 是arg max\n\n\n\n2016年12月9日\n------------\n - @fairmiracle ==> Chapter4 公式，语句，详见[issue](https://github.com/exacity/deeplearningbook-chinese/issues/3#issuecomment-265854595).\n - @huangpingchun ==> Chapter1 邻域==>领域\n - @tielei ==> Chapter9 equivariance\n - @yuduowu ==> contact issue\n - @minoriwww ==> Chapter2 \"排布\"==>\"排列\"\n - @khty2000 ==> Chapter2 \"X{-S}\"问题，矩阵横列错误\n - @Qingmu ==> 用WinEdit打开文件问题\n - @tielei ==> Chapter9 公式的index问题\n\n\n\n2016年12月10日\n-------------\n - @fairmiracle ==> Chapter5 \"\\Vy\"==>\"\\Vx\"；多余括号；in action，more frequently，more formally 提议校对，imputation 翻译建议\n - @huangpingchun ==> Chapter5 not completely formal or distinct concepts, VC维，imputation of missing data 翻译建议\n - @tielei ==> Chapter9 \"full convolution\"；翻译建议；\"tiling range\"\n\n\n\n2016年12月12日\n-------------\n - @fairmiracle ==> Chapter6 公式错误\n - @huangpingchun ==> Chapter7 \"模型平均\"重复\n\n\n\n2016年12月13日\n-------------\n - @huangpingchun ==> inference的统一翻译 \n - @HC-2016 ==> Chapter6 单位阵还是对角阵问题，详见[issue](https://github.com/exacity/deeplearningbook-chinese/issues/19#issuecomment-266683442).\n - @xiaomingabc ==> Chapter1 病句问题\n\n\n\n2016年12月14日\n--------------\n - @fairmiracle ==> Chapter7 \\norm 括号打错\n - @bengordai ==> Chapter1 区分出合法邮件与垃圾邮件\n\n\n\n2016年12月15日\n--------------\n - @huangpingchun ==> Chapter5 \"supervised\"误译为\"无监督, Chapter7 模块翻译\n - @Bojian ==> Chapter5 \"supervised\"误译为\"无监督\"，\"三维空间中球状流形\"翻译建议\n\n\n\n2016年12月16日\n-------------\n - @huangpingchun ==> \"infinite\" 翻译统一化\n - @JoyFYan ==> 错字\"植\"\n - @fairmiracle ==> 修正翻译\"based on making small local moves\"\n\n2016年12月18日\n--------------\n - @bengordai ==> Chapter6, \"until\"和\"address\"的翻译；双引号\n\n\n2016年12月20日\n-------------\n - @fairmiracle ==> Chapter8, f \n\n\n2016年12月20日\n-------------\n - @zdx3578 ==> 错别字\"如果\"改成\"如何\"\n\n\n2016年12月21日\n-------------\n - @huangpingchun ==> Chapter8, vanishing Long-Term Dependencies翻译\n\n\n2016年12月22日\n-------------\n - @PassStory ==> Chapter2, m -> n\n - @imwebson ==> Chapter2, m -> n\n - @HC-2016 ==> Chapter6, 语句问题；错别字\"相应\"改成\"响应\"\n - @Elvinczp ==> Chapter6, 错别字\"网路\"改成\"网络\"\n - @imwebson ==> Chapter6, \"winner-take-all\"翻译成\"赢者通吃\"\n\n\n2016年12月23日\n-------------\n - @zdx3578 ==> Chapter5, one-hot\n - @wlbksy  ==> Chapter14, 编译问题\n - @zdx3578 ==> Chapter6, 错别字\"有\"改成\"由\", \"难么\"改成\"那么\"\"\n\n\n2016年12月25日\n---------------\n - @roachsinai ==> Chapter10, 第315行公式错误\n - @minoriwww ==> Chapter6, \"loosely\"翻译为\"或多或少地\"；\"funciton\"翻译为\"功能\"\n\n\n2016年12月26日\n---------------\n - @zdx3578 ==> Chapter8, 280页翻译\n\n\n2016年12月27日\n---------------\n - @huangpingchun ==> Chapter8, 8.7.4节翻译\n\n\n2016年12月29日\n---------------\n - @zdx3578 ==> Chapter11, 11.4节翻译\n\n\n2016年12月31日\n---------------\n - @endymecy ==> Chapter10, 公式20错误\n\n\n2017年1月3日\n---------------\n - @endymecy ==> Chapter14, 语句问题\n\n\n2017年1月5日\n---------------\n - @tonyzeng2016 ==> Chapter8, 8.1.2最后一段翻译建议\n - @zdx3578 ==> Chapter18, \"intractable\"翻译建议\n\n\n2017年1月7日\n---------------\n - @zdx3578 ==> Chapter20, 公式错误，少子图\n - name:YUE-DaJiong ==> Chapter20, discriminator network\n\n\n2017年1月11日\n---------------\n - @HeimingX ==> Chapter5，5.9节翻译\n\n\n2017年2月4日\n---------------\n - @zhangyafeikimi ==> Chapter8，\"树木\"=>\"数目\"\n\n\n2017年2月7日\n---------------\n - @germany-zhu  ==> Chapter1, 语音术语\n - @kangqf ==> Chapter8, loccal_minima 笔误\n\n\n2017年3月17日\n---------------\n - @yaoxiuyong ==> Chapter 5, 5.1.4节特征权重增加／减少校对\n\n2017年4月5日\n---------------\n - @sailordiary ==> Chapter3, \"Reverend\"=>\"牧师\"，详见[issue](https://github.com/exacity/deeplearningbook-chinese/issues/53)\n\n2017年4月7日\n---------------\n - @poerin ==> Chapter6, 去掉\"第\"\n\n2017年4月9日\n---------------\n - @ninesunqian ==> Chapter6, \"二进制函数\"=>\"二值型函数\"\n\n2017年4月21日\n---------------\n - @JiaqiYao ==> Chapter6, 公式`q_i - p_i`的推导，详见[issue](https://github.com/exacity/deeplearningbook-chinese/issues/66)\n - @zhouqingping  ==> Chapter 7, 有放回采样\n\n\n2017年4月25日\n---------------\n - @ninesunqian ==> Chapter 8, line search, gaussian value 翻译调整\n\n2017年4月28日\n---------------\n - @sailordinary ==> Chapter 8, 语句调整\n - @freedomRen ==> Chapter 8, 语句调整\n\n2017年4月30日\n---------------\n - @ninesunqian ==> Chapter9, 图9.18子图顺序问题\n\n2017年5月2日\n---------------\n - @jt827859032 ==> Chapter5, 5.1节一处翻译调整\n - @demolpc ==> Chapter5, 5.2节三处翻译调整\n - @Sofring, @wenlei ==> Chapter 1, “人体”翻译\n\n2017年5月5日\n---------------\n - @jt827859032 ==> Chapter5, 5.1节一处翻译调整\n\n2017年5月21日\n---------------\n - @neo0801 ==> Chapter2, 2.2节一处翻译调整\n\n2017年6月7日\n---------------\n - @tjliupeng ==> Chapter 5, 翻译调整\n\n2017年6月21日\n---------------\n - @xiaolangyuxin ==> Chapter2, 翻译校正\n\n2017年6月23日\n---------------\n - @whatbeg ==> Section 8.7, 两处翻译调整\n\n2017年7月14日\n---------------\n - @xiaolangyuxin ==> Section 4.4, 翻译调整\n\n2017年7月17日\n---------------\n - @tjliupeng ==> Section 8.4，翻译调整\n\n2017年7月30日\n---------------\n - @xiaolangyuxin ==> Section 8.4, \"多块\" -> \"多快\"\n\n2017年10月14日\n---------------\n - @CarlKing5019 ==> Chapter 2, 翻译调整\n\n2017年10月28日\n---------------\n - @Godricly ==> Section 8.2.1，翻译调整\n\n2017年11月27日\n---------------\n - @12wang3 ==> Section 5.3.1，翻译指正。\n\n2018年5月22日\n---------------\n - @caisp ==> Section 2.4，翻译调整\n\n2018年11月26日\n---------------\n - @CristopherOh ==> Section 5.3，翻译指正\n"
        },
        {
          "name": "applied_math_and_machine_learning_basics.tex",
          "type": "blob",
          "size": 0.96,
          "content": "% !Mode:: \"TeX:UTF-8\"\n\\part{应用数学与机器学习基础}\n\\label{part:applied_math_and_machine_learning_basics}\n\n\\newpage\n\n本书这一部分将介绍理解\\gls{DL}所需的基本数学概念。\n我们从应用数学的一般概念开始，这能使我们定义许多变量的函数，找到这些函数的最高和最低点，并量化信念度。\n\n接着，我们描述\\gls{ML}的基本目标，并描述如何实现这些目标。\n我们需要指定代表某些信念的模型、设计衡量这些信念与现实对应程度的\\gls{cost_function}以及使用训练算法最小化这个\\gls{cost_function}。\n\n\n这个基本框架是广泛多样的\\gls{ML}算法的基础，其中也包括非深度的\\gls{ML}方法。\n在本书的后续部分，我们将在这个框架下开发\\gls{DL}算法。\n\n\\input{Chapter2/linear_algebra.tex}\n\\input{Chapter3/probability_and_information_theory.tex}\n\\input{Chapter4/numerical_computation.tex}\n\\input{Chapter5/machine_learning_basics.tex}\n\n"
        },
        {
          "name": "breakcites.sty",
          "type": "blob",
          "size": 0.6,
          "content": "% breakcites\n% Style file to allow citations to be broken across lines.\n% - Don Hosek   3/14/89 (LaTeX209-Version)\n% - Leo Broska 02/20/97 (LaTeX-2e-Version)\n%\n\\def\\@citex[#1]#2{%\n  \\let\\@citea\\@empty\n  \\@cite{\\@for\\@citeb:=#2\\do\n    {\\@citea\\def\\@citea{,\\penalty\\@m\\ }%\n     \\edef\\@citeb{\\expandafter\\@firstofone\\@citeb}%\n     \\if@filesw\\immediate\\write\\@auxout{\\string\\citation{\\@citeb}}\\fi\n     \\@ifundefined{b@\\@citeb}{\\mbox{\\reset@font\\bfseries ?}%\n       \\G@refundefinedtrue\n       \\@latex@warning\n         {Citation `\\@citeb' on page \\thepage \\space undefined}}%\n       {\\csname b@\\@citeb\\endcsname}}}{#1}}\n\n"
        },
        {
          "name": "deep_learning_research.tex",
          "type": "blob",
          "size": 4.21,
          "content": "% !Mode:: \"TeX:UTF-8\"\n\\part{深度学习研究}\n\\label{part:deep_learning_research}\n\n\\newpage\n本书这一部分描述目前研究社群所追求的、更有远见和更先进的\\gls{DL}方法。\n\n在本书的前两部分，我们已经展示了如何解决\\gls{supervised_learning}问题，即在给定足够的映射样本的情况下，学习将一个向量映射到另一个。\n\n我们想要解决的问题并不全都属于这个类别。\n我们可能希望生成新的样本、或确定一个点的似然性、或处理缺失值以及利用一组大量的未标记样本或相关任务的样本。\n当前应用于工业的最先进技术的缺点是我们的学习算法需要大量的监督数据才能实现良好的精度。\n在本书这一部分，我们讨论一些推测性的方法，来减少现有模型工作所需的标注数据量，并适用于更广泛的任务。 \n实现这些目标通常需要某种形式的\\gls{unsupervised}或\\gls{semi_supervised}学习。\n\n许多\\gls{DL}算法被设计为处理\\gls{unsupervised_learning}问题，但不像\\gls{DL}已经在很大程度上解决了各种任务的\\gls{supervised_learning}问题，没有一个算法能以同样的方式真正解决\\gls{unsupervised_learning}问题。\n在本书这一部分，我们描述\\gls{unsupervised_learning}的现有方法和一些如何在这一领域取得进展的流行思想。\n\n\\gls{unsupervised_learning}困难的核心原因是被建模的随机变量的高维度。\n这带来了两个不同的挑战：统计挑战和计算挑战。\n\\emph{统计挑战}与泛化相关：我们可能想要区分的配置数会随着感兴趣的维度数指数增长，并且这快速变得比可能具有的（或者在有限计算资源下使用的）样本数大得多。\n与高维分布相关联的\\emph{计算挑战}之所以会出现，是因为用于学习或使用训练模型的许多算法（特别是基于估计显式概率函数的算法）涉及难处理的计算量，并且随维数呈指数增长。\n\n使用概率模型，这种计算挑战来自执行难解的\\gls{inference}或归一化分布。\n\\begin{itemize}\n \\item \\emph{难解的\\gls{inference}}：\\gls{inference}主要在\\chapref{chap:approximate_inference}讨论。\n\\gls{inference}关于捕获$a$，$b$和$c$上联合分布的模型，给定其他变量$b$的情况下，猜测一些变量$a$的可能值。\n为了计算这样的条件概率，我们需要对变量$c$的值求和，以及计算对$a$和$c$的值求和的归一化常数。\n \\item \\emph{难解的归一化常数（\\gls{partition_function}）}：\\gls{partition_function}主要在\\chapref{chap:confronting_the_partition_function}讨论。\n归一化概率函数的常数在\\gls{inference}（上文）以及学习中出现。\n许多概率模型涉及这样的归一化常数。\n不幸的是，学习这样的模型通常需要相对于模型参数计算\\gls{partition_function}对数的梯度。\n该计算通常与计算\\gls{partition_function}本身一样难解。\n\\glsacr{mcmc}（\\chapref{chap:monte_carlo_methods}）通常用于处理\\gls{partition_function}。\n不幸的是，当模型分布的模式众多且分离良好时，\\glssymbol{mcmc}方法会出现问题，特别是在高维空间中（\\secref{sec:the_challenge_of_mixing_between_separated_modes}）。\n\\end{itemize}\n\n面对这些难以处理的计算的一种方法是近似它们，如在本书的第三部分中讨论的，研究者已经提出了许多方法。\n这里还讨论另一种有趣的方式是通过设计模型，完全避免这些难以处理的计算，因此不需要这些计算的方法是非常有吸引力的。\n近年来，研究者已经提出了数种具有该动机的生成模型。\n其中\\chapref{chap:deep_generative_models}讨论了各种各样的现代生成式建模方法。\n\n第三部分对于研究者来说是最重要的，研究者想要了解\\gls{DL}领域的广度，并将领域推向真正的\\gls{AI}。\n\n\n\n\\input{Chapter13/linear_factor_models.tex}\n\\input{Chapter14/autoencoders.tex}\n\\input{Chapter15/representation_learning.tex}\n\\input{Chapter16/structured_probabilistic_modelling.tex}\n\\input{Chapter17/monte_carlo_methods.tex}\n\\input{Chapter18/confronting_the_partition_function.tex}\n\\input{Chapter19/approximate_inference.tex}\n\\input{Chapter20/deep_generative_models.tex}\n"
        },
        {
          "name": "deep_networks_modern_practices.tex",
          "type": "blob",
          "size": 2.17,
          "content": "% !Mode:: \"TeX:UTF-8\"\n\\part{深度网络：现代实践}\n\\label{part:deep_networks_modern_practices}\n\n\\newpage\n本书这一部分总结现代\\gls{DL}用于解决实际应用的现状。\n\n\\gls{DL}有着悠久的历史和许多愿景。\n数种提出的方法尚未完全结出果实。\n数个雄心勃勃的目标尚未实现。\n这些较不发达的\\gls{DL}分支将出现在本书的最后部分。\n\n这一部分仅关注那些基本上已在工业中大量使用的技术方法。\n\n现代\\gls{DL}为\\gls{supervised_learning}提供了一个强大的框架。\n通过添加更多层以及向层内添加更多单元，\\gls{deep_network}可以表示复杂性不断增加的函数。\n给定足够大的模型和足够大的标注训练数据集，我们可以通过\\gls{DL}将输入向量映射到输出向量，完成大多数对人来说能迅速处理的任务。\n其他任务，比如不能被描述为将一个向量与另一个相关联的任务，或者对于一个人来说足够困难并需要时间思考和反复琢磨才能完成的任务，现在仍然超出了\\gls{DL}的能力范围。\n\n% ??\n本书这一部分描述参数化函数近似技术的核心，几乎所有现代实际应用的\\gls{DL}背后都用到了这一技术。\n首先，我们描述用于表示这些函数的前馈\\gls{deep_network}模型。\n接着，我们提出正则化和优化这种模型的高级技术。\n将这些模型扩展到大输入（如高分辨率图像或长时间序列）需要专门化。\n我们将会介绍扩展到大图像的\\gls{convolutional_network}和用于处理时间序列的\\gls{RNN}。\n最后，我们提出实用方法的一般准则，有助于设计、构建和配置一些涉及\\gls{DL}的应用，并回顾其中一些应用。\n\n这些章节对于从业者来说是最重要的，也就是现在想开始实现和使用\\gls{DL}算法解决现实问题的人需要阅读这些章节。\n\n\n\\input{Chapter6/deep_feedforward_networks.tex}\n\\input{Chapter7/regularization.tex}\n\\input{Chapter8/optimization_for_training_deep_models.tex} \n\\input{Chapter9/convolutional_networks.tex}\n\\input{Chapter10/sequence_modeling_rnn.tex}\n\\input{Chapter11/practical_methodology.tex}\n\\input{Chapter12/applications.tex}\n\n\n"
        },
        {
          "name": "dlbook_cn.bib",
          "type": "blob",
          "size": 0,
          "content": " \n"
        },
        {
          "name": "dlbook_cn.tex",
          "type": "blob",
          "size": 3.94,
          "content": "% !Mode:: \"TeX:UTF-8\"\n\\documentclass[twoside,nofonts,fancyhdr,openany,UTF8]{ctexbook}\n\\usepackage{natbib}\n\n% CJK related\n\\setCJKmainfont[AutoFakeBold=true]{Adobe Song Std}\n\\setCJKsansfont{Adobe Heiti Std}\n\\setCJKmonofont{Adobe Fangsong Std}\n\\CTEXsetup[format={\\raggedright}]{chapter}\n\\CTEXsetup[format={\\Large\\bfseries}]{section}\n\\CTEXsetup[format={\\large\\bfseries}]{subsection}\n\\CTEXsetup[format={\\normalsize\\bfseries}]{subsubsection}\n\n% Needed for some foreign characters\n\\usepackage[T1]{fontenc}\n\n\\usepackage{amsmath}\n\\usepackage{subfigure}\n\\usepackage{amsfonts}\n\\usepackage{amsthm}\n\\usepackage{multirow}\n\\usepackage{colortbl}\n\\usepackage{booktabs}\n% This allows us to cite chapters by name, which was useful for making the\n% acknowledgements page\n\\usepackage{nameref}\n\\usepackage{breakcites}\n\n\\usepackage[tocindentauto]{tocstyle}\n\\usetocstyle{standard}\n\n\\usepackage{bm}\n\\usepackage{float}\n\\newcommand{\\boldindex}[1]{\\textbf{\\hyperpage{#1}}}\n\\usepackage{makeidx}\\makeindex\n% Make bibliography and index appear in table of contents\n\\usepackage[nottoc]{tocbibind}\n\\usepackage[font=small]{caption}\n\n\\usepackage[section]{placeins}\n\\usepackage[chapter]{algorithm}\n\\usepackage{algorithmic}\n% Include chapter number in algorithm number\n\\renewcommand{\\thealgorithm}{\\arabic{chapter}.\\arabic{algorithm}}\n\\makeatletter\n\\renewcommand*{\\ALG@name}{算法}\n\\makeatother\n\n\n\n\\usepackage[pdfpagelabels=true,\npdffitwindow=false,\npdfview=FitH,\npdfstartview=FitH,\npagebackref=true,\nbreaklinks=true,\ncolorlinks=false,\nbookmarks=true,\nhidelinks=true,\nbookmarksnumbered=true,\nbookmarksopen=true,\nbookmarksopenlevel=1,\nbookmarksdepth=1,\nplainpages=true]{hyperref}\n\\usepackage{bookmark}\n\n\\usepackage{zref-abspage}\n\\setcounter{tocdepth}{3}\n\\setcounter{secnumdepth}{3}\n\n\n% my page\n\\usepackage[vcentering,dvips]{geometry}\n\\geometry{papersize={7in,9in},bottom=3pc,top=5pc,left=5pc,right=5pc,bmargin=4.5pc,footskip=18pt,headsep=25pt}\n\\setlength\\emergencystretch{1.5em}\n\n% my command\n\\newcommand{\\firstgls}[1]{\\textbf{\\,\\gls{#1}}（\\glsdesc{#1}）}\n\\newcommand{\\firstacr}[1]{\\textbf{\\,\\gls{#1}}（\\glssymbol{#1}）}\n\\newcommand{\\glsacr}[1]{\\gls{#1}（\\glssymbol{#1}）}\n\\newcommand{\\firstall}[1]{\\textbf{\\,\\gls{#1}}（\\glsdesc{#1}, \\glssymbol{#1}）}\n\\newcommand{\\ENNAME}[1]{\\text{#1}}\n\\newcommand{\\NUMTEXT}[1]{\\text{#1}}\n\\newcommand{\\figref}[1]{图\\,\\ref{#1}\\,}\n\\newcommand{\\chapref}[1]{第\\ref{#1}章}\n\\newcommand{\\secref}[1]{第\\,\\ref{#1}\\,节}\n\\newcommand{\\algref}[1]{算法\\,\\ref{#1}\\,}\n\\newcommand{\\eqnref}[1]{式\\,\\eqref{#1}\\,}\n\n\n% Draft\n\\usepackage{draftwatermark}\n\\SetWatermarkText{DRAFT}\n\\SetWatermarkLightness{0.9}\n\\usepackage{background}\n\\SetBgContents{仅供学习使用，不得用于商业目的。\n\\url{https://github.com/exacity/deeplearningbook-chinese}}\n\\SetBgScale{1}\n\\SetBgAngle{0}\n\\SetBgOpacity{1}\n\\SetBgColor{red}\n\\SetBgPosition{current page.north}\n\\SetBgVshift{-0.5cm}\n\n\\newif\\ifOpenSource\n\\OpenSourcetrue\n\n% http://tex.stackexchange.com/questions/198140/glossaries-and-custom-section-headings-broken  \\glsentrytext!\n\\usepackage[nomain,acronym,xindy,toc,nopostdot]{glossaries}\n\\makeglossaries\n\\usepackage[xindy]{imakeidx}\n\\makeindex\n\n\n% symbol and \n\\include{math_symbol}\n\\include{terminology}\n\n\n\n% title \n\\title{\\Huge\\textbf{深度学习}}\n\\author{}\n\\date{\\today}\n\n\n\\begin{document}\n\\frontmatter\n\n\\maketitle\n\\cleardoublepage\n\n% From en book -B\n\\setlength{\\parskip}{0.25 \\baselineskip}\n% Sean said to make figures 26 picas wide\n\\newlength{\\figwidth}\n\\setlength{\\figwidth}{26pc}\n% Spacing between notation sections\n\\newlength{\\notationgap}\n\\setlength{\\notationgap}{1pc}\n% From en book -E\n\n\n\\tableofcontents\n\n\\newpage\n\\input{acknowledgments.tex}\n\\input{website.tex}\n\\input{notation.tex}\n\\mainmatter\n\n\\input{Chapter1/introduction.tex}\n\\input{applied_math_and_machine_learning_basics.tex}\n\\input{deep_networks_modern_practices.tex}\n\\input{deep_learning_research.tex}\n\n\n\\backmatter\n\\bookmarksetup{startatroot}\n\n\\appendix\n\n\\small{\n\\bibliography{dlbook_cn}\n\\bibliographystyle{natbib}\n\\clearpage\n}\n\n\\printglossary[title=术语]\n\\end{document}\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "donation.pdf",
          "type": "blob",
          "size": 172.68,
          "content": null
        },
        {
          "name": "figure.pdf",
          "type": "blob",
          "size": 40.63,
          "content": null
        },
        {
          "name": "math_symbol.tex",
          "type": "blob",
          "size": 8.99,
          "content": "% !Mode:: \"TeX:UTF-8\"\n\\newcommand{\\argmax}{\\arg\\max}\n\\newcommand{\\argmin}{\\arg\\min}\n\\newcommand{\\sigmoid}{\\text{sigmoid}}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\Tr}{\\text{Tr}}\n\n\\newcommand{\\Var}{\\text{Var}}\n\\newcommand{\\Cov}{\\text{Cov}}\n\\newcommand{\\plim}{\\text{plim}}\n\\newcommand{\\Tsp}{\\top}\n\n% Scala\n\\newcommand{\\Sa}{\\mathit{a}}\n\\newcommand{\\Sb}{\\mathit{b}}\n\\newcommand{\\Sc}{\\mathit{c}}\n\\newcommand{\\Sd}{\\mathit{d}}\n\\newcommand{\\Se}{\\mathit{e}}\n\\newcommand{\\Sf}{\\mathit{f}}\n\\newcommand{\\Sg}{\\mathit{g}}\n\\newcommand{\\Sh}{\\mathit{h}}\n\\newcommand{\\Si}{\\mathit{i}}\n\\newcommand{\\Sj}{\\mathit{j}}\n\\newcommand{\\Sk}{\\mathit{k}}\n\\newcommand{\\Sl}{\\mathit{l}}\n\\newcommand{\\Sm}{\\mathit{m}}\n\\newcommand{\\Sn}{\\mathit{n}}\n\\newcommand{\\So}{\\mathit{o}}\n\\newcommand{\\Sp}{\\mathit{p}}\n\\newcommand{\\Sq}{\\mathit{q}}\n\\newcommand{\\Sr}{\\mathit{r}}\n\\newcommand{\\Ss}{\\mathit{s}}\n\\newcommand{\\St}{\\mathit{t}}\n\\newcommand{\\Su}{\\mathit{u}}\n\\newcommand{\\Sv}{\\mathit{v}}\n\\newcommand{\\Sw}{\\mathit{w}}\n\\newcommand{\\Sx}{\\mathit{x}}\n\\newcommand{\\Sy}{\\mathit{y}}\n\\newcommand{\\Sz}{\\mathit{z}}\n\n\\newcommand{\\SA}{\\mathit{A}}\n\\newcommand{\\SB}{\\mathit{B}}\n\\newcommand{\\SC}{\\mathit{C}}\n\\newcommand{\\SD}{\\mathit{D}}\n\\newcommand{\\SE}{\\mathit{E}}\n\\newcommand{\\SF}{\\mathit{F}}\n\\newcommand{\\SG}{\\mathit{G}}\n\\newcommand{\\SH}{\\mathit{H}}\n\\newcommand{\\SJ}{\\mathit{J}}\n\\newcommand{\\SK}{\\mathit{K}}\n\\newcommand{\\SI}{\\mathit{L}}\n\\newcommand{\\SM}{\\mathit{M}}\n\\newcommand{\\SN}{\\mathit{N}}\n\\newcommand{\\SO}{\\mathit{O}}\n\\newcommand{\\SP}{\\mathit{P}}\n\\newcommand{\\SQ}{\\mathit{Q}}\n\\newcommand{\\SR}{\\mathit{R}}\n\\newcommand{\\ST}{\\mathit{T}}\n\\newcommand{\\SU}{\\mathit{U}}\n\\newcommand{\\SV}{\\mathit{V}}\n\\newcommand{\\SW}{\\mathit{W}}\n\\newcommand{\\SX}{\\mathit{X}}\n\\newcommand{\\SY}{\\mathit{Y}}\n\\newcommand{\\SZ}{\\mathit{Z}}\n\n\n\n% Vector\n\\newcommand{\\Va}{\\boldsymbol{\\mathit{a}}}\n\\newcommand{\\Vb}{\\boldsymbol{\\mathit{b}}}\n\\newcommand{\\Vc}{\\boldsymbol{\\mathit{c}}}\n\\newcommand{\\Vd}{\\boldsymbol{\\mathit{d}}}\n\\newcommand{\\Ve}{\\boldsymbol{\\mathit{e}}}\n\\newcommand{\\Vf}{\\boldsymbol{\\mathit{f}}}\n\\newcommand{\\Vg}{\\boldsymbol{\\mathit{g}}}\n\\newcommand{\\Vh}{\\boldsymbol{\\mathit{h}}}\n\\newcommand{\\Vi}{\\boldsymbol{\\mathit{i}}}\n\\newcommand{\\Vj}{\\boldsymbol{\\mathit{j}}}\n\\newcommand{\\Vk}{\\boldsymbol{\\mathit{k}}}\n\\newcommand{\\Vl}{\\boldsymbol{\\mathit{l}}}\n\\newcommand{\\Vm}{\\boldsymbol{\\mathit{m}}}\n\\newcommand{\\Vn}{\\boldsymbol{\\mathit{n}}}\n\\newcommand{\\Vo}{\\boldsymbol{\\mathit{o}}}\n\\newcommand{\\Vp}{\\boldsymbol{\\mathit{p}}}\n\\newcommand{\\Vq}{\\boldsymbol{\\mathit{q}}}\n\\newcommand{\\Vr}{\\boldsymbol{\\mathit{r}}}\n\\newcommand{\\Vs}{\\boldsymbol{\\mathit{s}}}\n\\newcommand{\\Vt}{\\boldsymbol{\\mathit{t}}}\n\\newcommand{\\Vu}{\\boldsymbol{\\mathit{u}}}\n\\newcommand{\\Vv}{\\boldsymbol{\\mathit{v}}}\n\\newcommand{\\Vw}{\\boldsymbol{\\mathit{w}}}\n\\newcommand{\\Vx}{\\boldsymbol{\\mathit{x}}}\n\\newcommand{\\Vy}{\\boldsymbol{\\mathit{y}}}\n\\newcommand{\\Vz}{\\boldsymbol{\\mathit{z}}}\n\n% Matrix\n\\newcommand{\\MA}{\\boldsymbol{\\mathit{A}}}\n\\newcommand{\\MB}{\\boldsymbol{\\mathit{B}}}\n\\newcommand{\\MC}{\\boldsymbol{\\mathit{C}}}\n\\newcommand{\\MD}{\\boldsymbol{\\mathit{D}}}\n\\newcommand{\\ME}{\\boldsymbol{\\mathit{E}}}\n\\newcommand{\\MF}{\\boldsymbol{\\mathit{F}}}\n\\newcommand{\\MG}{\\boldsymbol{\\mathit{G}}}\n\\newcommand{\\MH}{\\boldsymbol{\\mathit{H}}}\n\\newcommand{\\MI}{\\boldsymbol{\\mathit{I}}}\n\\newcommand{\\MJ}{\\boldsymbol{\\mathit{J}}}\n\\newcommand{\\MK}{\\boldsymbol{\\mathit{K}}}\n\\newcommand{\\ML}{\\boldsymbol{\\mathit{L}}}\n\\newcommand{\\MM}{\\boldsymbol{\\mathit{M}}}\n\\newcommand{\\MN}{\\boldsymbol{\\mathit{N}}}\n\\newcommand{\\MO}{\\boldsymbol{\\mathit{O}}}\n\\newcommand{\\MP}{\\boldsymbol{\\mathit{P}}}\n\\newcommand{\\MQ}{\\boldsymbol{\\mathit{Q}}}\n\\newcommand{\\MR}{\\boldsymbol{\\mathit{R}}}\n\\newcommand{\\MS}{\\boldsymbol{\\mathit{S}}}\n\\newcommand{\\MT}{\\boldsymbol{\\mathit{T}}}\n\\newcommand{\\MU}{\\boldsymbol{\\mathit{U}}}\n\\newcommand{\\MV}{\\boldsymbol{\\mathit{V}}}\n\\newcommand{\\MW}{\\boldsymbol{\\mathit{W}}}\n\\newcommand{\\MX}{\\boldsymbol{\\mathit{X}}}\n\\newcommand{\\MY}{\\boldsymbol{\\mathit{Y}}}\n\\newcommand{\\MZ}{\\boldsymbol{\\mathit{Z}}}\n\n\n%Tensor\n\\newcommand{\\TSA}{\\textsf{\\textbf{A}}}\n\\newcommand{\\TSB}{\\textsf{\\textbf{B}}}\n\\newcommand{\\TSC}{\\textsf{\\textbf{C}}}\n\\newcommand{\\TSD}{\\textsf{\\textbf{D}}}\n\\newcommand{\\TSE}{\\textsf{\\textbf{E}}}\n\\newcommand{\\TSF}{\\textsf{\\textbf{F}}}\n\\newcommand{\\TSG}{\\textsf{\\textbf{G}}}\n\\newcommand{\\TSH}{\\textsf{\\textbf{H}}}\n\\newcommand{\\TSI}{\\textsf{\\textbf{I}}}\n\\newcommand{\\TSJ}{\\textsf{\\textbf{J}}}\n\\newcommand{\\TSK}{\\textsf{\\textbf{K}}}\n\\newcommand{\\TSL}{\\textsf{\\textbf{L}}}\n\\newcommand{\\TSM}{\\textsf{\\textbf{M}}}\n\\newcommand{\\TSN}{\\textsf{\\textbf{N}}}\n\\newcommand{\\TSO}{\\textsf{\\textbf{O}}}\n\\newcommand{\\TSP}{\\textsf{\\textbf{P}}}\n\\newcommand{\\TSQ}{\\textsf{\\textbf{Q}}}\n\\newcommand{\\TSR}{\\textsf{\\textbf{R}}}\n\\newcommand{\\TSS}{\\textsf{\\textbf{S}}}\n\\newcommand{\\TST}{\\textsf{\\textbf{T}}}\n\\newcommand{\\TSU}{\\textsf{\\textbf{U}}}\n\\newcommand{\\TSV}{\\textsf{\\textbf{V}}}\n\\newcommand{\\TSW}{\\textsf{\\textbf{W}}}\n\\newcommand{\\TSX}{\\textsf{\\textbf{X}}}\n\\newcommand{\\TSY}{\\textsf{\\textbf{Y}}}\n\\newcommand{\\TSZ}{\\textsf{\\textbf{Z}}}\n\n% Tensor Element\n\\newcommand{\\TEA}{\\textit{\\textsf{A}}}\n\\newcommand{\\TEB}{\\textit{\\textsf{B}}}\n\\newcommand{\\TEC}{\\textit{\\textsf{C}}}\n\\newcommand{\\TED}{\\textit{\\textsf{D}}}\n\\newcommand{\\TEE}{\\textit{\\textsf{E}}}\n\\newcommand{\\TEF}{\\textit{\\textsf{F}}}\n\\newcommand{\\TEG}{\\textit{\\textsf{G}}}\n\\newcommand{\\TEH}{\\textit{\\textsf{H}}}\n\\newcommand{\\TEI}{\\textit{\\textsf{I}}}\n\\newcommand{\\TEJ}{\\textit{\\textsf{J}}}\n\\newcommand{\\TEK}{\\textit{\\textsf{K}}}\n\\newcommand{\\TEL}{\\textit{\\textsf{L}}}\n\\newcommand{\\TEM}{\\textit{\\textsf{M}}}\n\\newcommand{\\TEN}{\\textit{\\textsf{N}}}\n\\newcommand{\\TEO}{\\textit{\\textsf{O}}}\n\\newcommand{\\TEP}{\\textit{\\textsf{P}}}\n\\newcommand{\\TEQ}{\\textit{\\textsf{Q}}}\n\\newcommand{\\TER}{\\textit{\\textsf{R}}}\n\\newcommand{\\TES}{\\textit{\\textsf{S}}}\n\\newcommand{\\TET}{\\textit{\\textsf{T}}}\n\\newcommand{\\TEU}{\\textit{\\textsf{U}}}\n\\newcommand{\\TEV}{\\textit{\\textsf{V}}}\n\\newcommand{\\TEW}{\\textit{\\textsf{W}}}\n\\newcommand{\\TEX}{\\textit{\\textsf{X}}}\n\\newcommand{\\TEY}{\\textit{\\textsf{Y}}}\n\\newcommand{\\TEZ}{\\textit{\\textsf{Z}}}\n\n% Random Scala\n\\newcommand{\\RSa}{\\mathrm{a}}\n\\newcommand{\\RSb}{\\mathrm{b}}\n\\newcommand{\\RSc}{\\mathrm{c}}\n\\newcommand{\\RSd}{\\mathrm{d}}\n\\newcommand{\\RSe}{\\mathrm{e}}\n\\newcommand{\\RSf}{\\mathrm{f}}\n\\newcommand{\\RSg}{\\mathrm{g}}\n\\newcommand{\\RSh}{\\mathrm{h}}\n\\newcommand{\\RSi}{\\mathrm{i}}\n\\newcommand{\\RSj}{\\mathrm{j}}\n\\newcommand{\\RSk}{\\mathrm{k}}\n\\newcommand{\\RSl}{\\mathrm{l}}\n\\newcommand{\\RSm}{\\mathrm{m}}\n\\newcommand{\\RSn}{\\mathrm{n}}\n\\newcommand{\\RSo}{\\mathrm{o}}\n\\newcommand{\\RSp}{\\mathrm{p}}\n\\newcommand{\\RSq}{\\mathrm{q}}\n\\newcommand{\\RSr}{\\mathrm{r}}\n\\newcommand{\\RSs}{\\mathrm{s}}\n\\newcommand{\\RSt}{\\mathrm{t}}\n\\newcommand{\\RSu}{\\mathrm{u}}\n\\newcommand{\\RSv}{\\mathrm{v}}\n\\newcommand{\\RSw}{\\mathrm{w}}\n\\newcommand{\\RSx}{\\mathrm{x}}\n\\newcommand{\\RSy}{\\mathrm{y}}\n\\newcommand{\\RSz}{\\mathrm{z}}\n\n\n% Random Vector\n\\newcommand{\\RVa}{\\mathbf{a}}\n\\newcommand{\\RVb}{\\mathbf{b}}\n\\newcommand{\\RVc}{\\mathbf{c}}\n\\newcommand{\\RVd}{\\mathbf{d}}\n\\newcommand{\\RVe}{\\mathbf{e}}\n\\newcommand{\\RVf}{\\mathbf{f}}\n\\newcommand{\\RVg}{\\mathbf{g}}\n\\newcommand{\\RVh}{\\mathbf{h}}\n\\newcommand{\\RVi}{\\mathbf{i}}\n\\newcommand{\\RVj}{\\mathbf{j}}\n\\newcommand{\\RVk}{\\mathbf{k}}\n\\newcommand{\\RVl}{\\mathbf{l}}\n\\newcommand{\\RVm}{\\mathbf{m}}\n\\newcommand{\\RVn}{\\mathbf{n}}\n\\newcommand{\\RVo}{\\mathbf{o}}\n\\newcommand{\\RVp}{\\mathbf{p}}\n\\newcommand{\\RVq}{\\mathbf{q}}\n\\newcommand{\\RVr}{\\mathbf{r}}\n\\newcommand{\\RVs}{\\mathbf{s}}\n\\newcommand{\\RVt}{\\mathbf{t}}\n\\newcommand{\\RVu}{\\mathbf{u}}\n\\newcommand{\\RVv}{\\mathbf{v}}\n\\newcommand{\\RVw}{\\mathbf{w}}\n\\newcommand{\\RVx}{\\mathbf{x}}\n\\newcommand{\\RVy}{\\mathbf{y}}\n\\newcommand{\\RVz}{\\mathbf{z}}\n\n% Random Matrix\n% will be added later\n\\newcommand{\\RMX}{\\boldsymbol{\\mathrm{X}}}\n\\newcommand{\\RMA}{\\boldsymbol{\\mathrm{A}}}\n\n\\newcommand{\\Valpha}{\\boldsymbol{\\alpha}}\n\\newcommand{\\Vbeta}{\\boldsymbol{\\beta}}\n\\newcommand{\\Vtheta}{\\boldsymbol{\\theta}}\n\\newcommand{\\Vlambda}{\\boldsymbol{\\lambda}}\n\\newcommand{\\VLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Vepsilon}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Vmu}{\\boldsymbol{\\mu}}\n\\newcommand{\\VPhi}{\\boldsymbol{\\Phi}}\n\\newcommand{\\Vsigma}{\\boldsymbol{\\sigma}}\n\\newcommand{\\VSigma}{\\boldsymbol{\\Sigma}}\n\\newcommand{\\Vrho}{\\boldsymbol{\\rho}}\n\\newcommand{\\Vgamma}{\\boldsymbol{\\gamma}}\n\\newcommand{\\Vomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\Vpsi}{\\boldsymbol{\\psi}}\n\\newcommand{\\Vzeta}{\\boldsymbol{\\zeta}}\n\\newcommand{\\Vone}{\\boldsymbol{1}}\n\n\n\\newcommand{\\CalB}{\\mathcal{B}}\n\\newcommand{\\CalC}{\\mathcal{C}}\n\\newcommand{\\CalG}{\\mathcal{G}}\n\\newcommand{\\CalH}{\\mathcal{H}}\n\\newcommand{\\CalL}{\\mathcal{L}}\n\\newcommand{\\CalM}{\\mathcal{M}}\n\\newcommand{\\CalN}{\\mathcal{N}}\n\\newcommand{\\CalO}{\\mathcal{O}}\n\\newcommand{\\CalD}{\\mathcal{D}}\n\\newcommand{\\CalU}{\\mathcal{U}}\n\\newcommand{\\CalF}{\\mathcal{F}}\n\\newcommand{\\CalT}{\\mathcal{T}}\n\n% Set\n\\newcommand{\\SetA}{\\mathbb{A}}\n\\newcommand{\\SetB}{\\mathbb{B}}\n\\newcommand{\\SetD}{\\mathbb{D}}\n\\newcommand{\\SetE}{\\mathbb{E}}\n\\newcommand{\\SetG}{\\mathbb{G}}\n\\newcommand{\\SetL}{\\mathbb{L}}\n\\newcommand{\\SetN}{\\mathbb{N}}\n\\newcommand{\\SetR}{\\mathbb{R}}\n\\newcommand{\\SetS}{\\mathbb{S}}\n\\newcommand{\\SetT}{\\mathbb{T}}\n\\newcommand{\\SetV}{\\mathbb{V}}\n\\newcommand{\\SetX}{\\mathbb{X}}\n\\newcommand{\\SetY}{\\mathbb{Y}}\n"
        },
        {
          "name": "natbib.bst",
          "type": "blob",
          "size": 25.61,
          "content": "%% \r\n%% This is file `natbib.bst', generated \r\n%% on <1994/9/16> with the docstrip utility (2.2h).\r\n%% \r\n%% The original source files were:\r\n%% \r\n%% genbst.mbs  (with options: `ay,nat,seq-lab,nm-rev,dt-beg,yr-par,vol-bf,\r\n%%                             volp-com,etal-it')\r\n%% ---------------------------------------- \r\n%% *** Personal bib style, PWD *** \r\n%% \r\n%% (Here are the specifications of the source file)\r\n%% \\ProvidesFile{genbst.mbs}[1994/09/16 1.5 (PWD)]\r\n%%   For use with BibTeX version 0.99a or later\r\n%%     and with LaTeX 2.09 or 2e\r\n%%-------------------------------------------------------------------\r\n%% NOTICE:\r\n%% This file may be used for non-profit purposes.\r\n%% It may not be distributed in exchange for money,\r\n%%   other than distribution costs.\r\n%%\r\n%% The author provides it `as is' and does not guarantee it in any way.\r\n%%\r\n%% Copyright (C) 1994 Patrick W. Daly\r\n%% Max-Planck-Institut f\\\"ur Aeronomie\r\n%% Postfach 20\r\n%% D-37189 Katlenburg-Lindau\r\n%% Germany\r\n%%\r\n%% E-mail:\r\n%% SPAN--     nsp::linmpi::daly    (note nsp also known as ecd1)\r\n%% Internet-- daly@linmpi.dnet.gwdg.de\r\n%%-----------------------------------------------------------\r\n%% \\CharacterTable\r\n%%  {Upper-case    \\A\\B\\C\\D\\E\\F\\G\\H\\I\\J\\K\\L\\M\\N\\O\\P\\Q\\R\\S\\T\\U\\V\\W\\X\\Y\\Z\r\n%%   Lower-case    \\a\\b\\c\\d\\e\\f\\g\\h\\i\\j\\k\\l\\m\\n\\o\\p\\q\\r\\s\\t\\u\\v\\w\\x\\y\\z\r\n%%   Digits        \\0\\1\\2\\3\\4\\5\\6\\7\\8\\9\r\n%%   Exclamation   \\!     Double quote  \\\"     Hash (number) \\#\r\n%%   Dollar        \\$     Percent       \\%     Ampersand     \\&\r\n%%   Acute accent  \\'     Left paren    \\(     Right paren   \\)\r\n%%   Asterisk      \\*     Plus          \\+     Comma         \\,\r\n%%   Minus         \\-     Point         \\.     Solidus       \\/\r\n%%   Colon         \\:     Semicolon     \\;     Less than     \\<\r\n%%   Equals        \\=     Greater than  \\>     Question mark \\?\r\n%%   Commercial at \\@     Left bracket  \\[     Backslash     \\\\\r\n%%   Right bracket \\]     Circumflex    \\^     Underscore    \\_\r\n%%   Grave accent  \\`     Left brace    \\{     Vertical bar  \\|\r\n%%   Right brace   \\}     Tilde         \\~}\r\n%%---------------------------------------------------------------------\r\n % This is an author-year citation style bibliography. As such, it is\r\n % non-standard LaTeX, and requires a special package file to function properly.\r\n % Such a package is    natbib.sty   by Patrick W. Daly\r\n % The form of the \\bibitem entries is\r\n %   \\bibitem[Jones et al.(1990)]{key}...\r\n %   \\bibitem[Jones et al.(1990)Jones, Baker, and Smith]{key}...\r\n % The essential feature is that the label (the part in brackets) consists\r\n % of the author names, as they should appear in the citation, with the year\r\n % in parentheses following. There must be no space before the opening\r\n % parenthesis!\r\n % With natbib v5.3, a full list of authors may also follow the year.\r\n % In natbib.sty, it is possible to define the type of enclosures that is\r\n % really wanted (brackets or parentheses), but in either case, there must\r\n % be parentheses in the label.\r\n % The \\cite command functions as follows:\r\n %   \\cite{key} ==>>                Jones et al. (1990)\r\n %   \\cite[]{key} ==>>              (Jones et al., 1990)\r\n %   \\cite[chap. 2]{key} ==>>       (Jones et al., 1990, chap. 2)\r\n %   \\cite[e.g.][]{key} ==>>        (e.g. Jones et al., 1990)\r\n %   \\cite[e.g.][p. 32]{key} ==>>   (e.g. Jones et al., p. 32)\r\n %   \\citeauthor{key}               Jones et al.\r\n %   \\citefullauthor{key}           Jones, Baker, and Smith\r\n %   \\citeyear{key}                 1990\r\n%%---------------------------------------------------------------------\r\n\r\nENTRY\r\n  { address\r\n    author\r\n    booktitle\r\n    chapter\r\n    edition\r\n    editor\r\n    howpublished\r\n    institution\r\n    journal\r\n    key\r\n    month\r\n    note\r\n    number\r\n    organization\r\n    pages\r\n    publisher\r\n    school\r\n    series\r\n    title\r\n    type\r\n    volume\r\n    year\r\n  }\r\n  {}\r\n  { label extra.label sort.label }\r\n\r\nINTEGERS { output.state before.all mid.sentence after.sentence after.block }\r\n\r\nFUNCTION {init.state.consts}\r\n{ #0 'before.all :=\r\n  #1 'mid.sentence :=\r\n  #2 'after.sentence :=\r\n  #3 'after.block :=\r\n}\r\n\r\nSTRINGS { s t }\r\n\r\nFUNCTION {output.nonnull}\r\n{ 's :=\r\n  output.state mid.sentence =\r\n    { \", \" * write$ }\r\n    { output.state after.block =\r\n        { add.period$ write$\r\n          newline$\r\n          \"\\newblock \" write$\r\n        }\r\n        { output.state before.all =\r\n            'write$\r\n            { add.period$ \" \" * write$ }\r\n          if$\r\n        }\r\n      if$\r\n      mid.sentence 'output.state :=\r\n    }\r\n  if$\r\n  s\r\n}\r\n\r\nFUNCTION {output}\r\n{ duplicate$ empty$\r\n    'pop$\r\n    'output.nonnull\r\n  if$\r\n}\r\n\r\nFUNCTION {output.check}\r\n{ 't :=\r\n  duplicate$ empty$\r\n    { pop$ \"empty \" t * \" in \" * cite$ * warning$ }\r\n    'output.nonnull\r\n  if$\r\n}\r\n\r\nFUNCTION {fin.entry}\r\n{ add.period$\r\n  write$\r\n  newline$\r\n}\r\n\r\nFUNCTION {new.block}\r\n{ output.state before.all =\r\n    'skip$\r\n    { after.block 'output.state := }\r\n  if$\r\n}\r\n\r\nFUNCTION {new.sentence}\r\n{ output.state after.block =\r\n    'skip$\r\n    { output.state before.all =\r\n        'skip$\r\n        { after.sentence 'output.state := }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {not}\r\n{   { #0 }\r\n    { #1 }\r\n  if$\r\n}\r\n\r\nFUNCTION {and}\r\n{   'skip$\r\n    { pop$ #0 }\r\n  if$\r\n}\r\n\r\nFUNCTION {or}\r\n{   { pop$ #1 }\r\n    'skip$\r\n  if$\r\n}\r\n\r\nFUNCTION {non.stop}\r\n{ duplicate$\r\n   \"}\" * add.period$\r\n   #-1 #1 substring$ \".\" =\r\n}\r\n\r\nFUNCTION {new.block.checkb}\r\n{ empty$\r\n  swap$ empty$\r\n  and\r\n    'skip$\r\n    'new.block\r\n  if$\r\n}\r\n\r\nFUNCTION {field.or.null}\r\n{ duplicate$ empty$\r\n    { pop$ \"\" }\r\n    'skip$\r\n  if$\r\n}\r\n\r\nFUNCTION {emphasize}\r\n{ duplicate$ empty$\r\n    { pop$ \"\" }\r\n    { \"{\\em \" swap$ * non.stop\r\n        { \"\\/}\" * }\r\n        { \"}\" * }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {bolden}\r\n{ duplicate$ empty$\r\n    { pop$ \"\" }\r\n    { \"{\\bf \" swap$ * \"}\" * }\r\n  if$\r\n}\r\n\r\nINTEGERS { nameptr namesleft numnames }\r\n\r\nFUNCTION {format.names}\r\n{ 's :=\r\n  #1 'nameptr :=\r\n  s num.names$ 'numnames :=\r\n  numnames 'namesleft :=\r\n    { namesleft #0 > }\r\n    { s nameptr\r\n      \"{vv~}{ll}{, jj}{, f.}\" format.name$ 't :=\r\n      nameptr #1 >\r\n        {\r\n          namesleft #1 >\r\n            { \", \" * t * }\r\n            {\r\n              numnames #2 >\r\n                { \",\" * }\r\n                'skip$\r\n              if$\r\n              t \"others\" =\r\n                { \" \" * \"et~al.\" emphasize * }\r\n                { \" and \" * t * }\r\n              if$\r\n            }\r\n          if$\r\n        }\r\n        't\r\n      if$\r\n      nameptr #1 + 'nameptr :=\r\n      namesleft #1 - 'namesleft :=\r\n    }\r\n  while$\r\n}\r\n\r\nFUNCTION {format.names.ed}\r\n{ 's :=\r\n  #1 'nameptr :=\r\n  s num.names$ 'numnames :=\r\n  numnames 'namesleft :=\r\n    { namesleft #0 > }\r\n    { s nameptr\r\n      \"{f.~}{vv~}{ll}{, jj}\"\r\n      format.name$ 't :=\r\n      nameptr #1 >\r\n        {\r\n          namesleft #1 >\r\n            { \", \" * t * }\r\n            {\r\n              numnames #2 >\r\n                { \",\" * }\r\n                'skip$\r\n              if$\r\n              t \"others\" =\r\n                { \" \" * \"et~al.\" emphasize * }\r\n                { \" and \" * t * }\r\n              if$\r\n            }\r\n          if$\r\n        }\r\n        't\r\n      if$\r\n      nameptr #1 + 'nameptr :=\r\n      namesleft #1 - 'namesleft :=\r\n    }\r\n  while$\r\n}\r\n\r\nFUNCTION {format.key}\r\n{ empty$\r\n    { key field.or.null }\r\n    { \"\" }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.authors}\r\n{ author empty$\r\n    { \"\" }\r\n    { author format.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.editors}\r\n{ editor empty$\r\n    { \"\" }\r\n    { editor format.names\r\n      editor num.names$ #1 >\r\n        { \", editors\" * }\r\n        { \", editor\" * }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.in.editors}\r\n{ editor empty$\r\n    { \"\" }\r\n    { editor format.names.ed\r\n      editor num.names$ #1 >\r\n        { \", editors\" * }\r\n        { \", editor\" * }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.title}\r\n{ title empty$\r\n    { \"\" }\r\n    { title \"t\" change.case$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.full.names}\r\n{'s :=\r\n  #1 'nameptr :=\r\n  s num.names$ 'numnames :=\r\n  numnames 'namesleft :=\r\n    { namesleft #0 > }\r\n    { s nameptr\r\n      \"{vv~}{ll}\" format.name$ 't :=\r\n      nameptr #1 >\r\n        {\r\n          namesleft #1 >\r\n            { \", \" * t * }\r\n            {\r\n              numnames #2 >\r\n                { \",\" * }\r\n                'skip$\r\n              if$\r\n              t \"others\" =\r\n                { \" \" * \"et~al.\" emphasize * }\r\n                { \" and \" * t * }\r\n              if$\r\n            }\r\n          if$\r\n        }\r\n        't\r\n      if$\r\n      nameptr #1 + 'nameptr :=\r\n      namesleft #1 - 'namesleft :=\r\n    }\r\n  while$\r\n}\r\n\r\nFUNCTION {author.editor.key.full}\r\n{ author empty$\r\n    { editor empty$\r\n        { key empty$\r\n            { cite$ #1 #3 substring$ }\r\n            'key\r\n          if$\r\n        }\r\n        { editor format.full.names }\r\n      if$\r\n    }\r\n    { author format.full.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {author.key.full}\r\n{ author empty$\r\n    { key empty$\r\n         { cite$ #1 #3 substring$ }\r\n          'key\r\n      if$\r\n    }\r\n    { author format.full.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {editor.key.full}\r\n{ editor empty$\r\n    { key empty$\r\n         { cite$ #1 #3 substring$ }\r\n          'key\r\n      if$\r\n    }\r\n    { editor format.full.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {make.full.names}\r\n{ type$ \"book\" =\r\n  type$ \"inbook\" =\r\n  or\r\n    'author.editor.key.full\r\n    { type$ \"proceedings\" =\r\n        'editor.key.full\r\n        'author.key.full\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {output.bibitem}\r\n{ newline$\r\n  \"\\bibitem[\" write$\r\n  label write$\r\n  \")\" make.full.names * \"]{\" * write$\r\n  cite$ write$\r\n  \"}\" write$\r\n  newline$\r\n  \"\"\r\n  before.all 'output.state :=\r\n}\r\n\r\nFUNCTION {n.dashify}\r\n{ 't :=\r\n  \"\"\r\n    { t empty$ not }\r\n    { t #1 #1 substring$ \"-\" =\r\n        { t #1 #2 substring$ \"--\" = not\r\n            { \"--\" *\r\n              t #2 global.max$ substring$ 't :=\r\n            }\r\n            {   { t #1 #1 substring$ \"-\" = }\r\n                { \"-\" *\r\n                  t #2 global.max$ substring$ 't :=\r\n                }\r\n              while$\r\n            }\r\n          if$\r\n        }\r\n        { t #1 #1 substring$ *\r\n          t #2 global.max$ substring$ 't :=\r\n        }\r\n      if$\r\n    }\r\n  while$\r\n}\r\n\r\nFUNCTION {word.in}\r\n{ \"In \" }\r\n\r\nFUNCTION {format.date}\r\n{ year duplicate$ empty$\r\n    { \"empty year in \" cite$ * \"; set to ????\" * warning$\r\n       pop$ \"????\" }\r\n    'skip$\r\n  if$\r\n  before.all 'output.state :=\r\n  \" (\" swap$ * extra.label * \")\" *\r\n}\r\n\r\nFUNCTION {format.btitle}\r\n{ title emphasize\r\n}\r\n\r\nFUNCTION {tie.or.space.connect}\r\n{ duplicate$ text.length$ #3 <\r\n    { \"~\" }\r\n    { \" \" }\r\n  if$\r\n  swap$ * *\r\n}\r\n\r\nFUNCTION {either.or.check}\r\n{ empty$\r\n    'pop$\r\n    { \"can't use both \" swap$ * \" fields in \" * cite$ * warning$ }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.bvolume}\r\n{ volume empty$\r\n    { \"\" }\r\n    { \"volume\" volume tie.or.space.connect\r\n      series empty$\r\n        'skip$\r\n        { \" of \" * series emphasize * }\r\n      if$\r\n      \"volume and number\" number either.or.check\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.number.series}\r\n{ volume empty$\r\n    { number empty$\r\n        { series field.or.null }\r\n        { output.state mid.sentence =\r\n            { \"number\" }\r\n            { \"Number\" }\r\n          if$\r\n          number tie.or.space.connect\r\n          series empty$\r\n            { \"there's a number but no series in \" cite$ * warning$ }\r\n            { \" in \" * series * }\r\n          if$\r\n        }\r\n      if$\r\n    }\r\n    { \"\" }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.edition}\r\n{ edition empty$\r\n    { \"\" }\r\n    { output.state mid.sentence =\r\n        { edition \"l\" change.case$ \" edition\" * }\r\n        { edition \"t\" change.case$ \" edition\" * }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nINTEGERS { multiresult }\r\n\r\nFUNCTION {multi.page.check}\r\n{ 't :=\r\n  #0 'multiresult :=\r\n    { multiresult not\r\n      t empty$ not\r\n      and\r\n    }\r\n    { t #1 #1 substring$\r\n      duplicate$ \"-\" =\r\n      swap$ duplicate$ \",\" =\r\n      swap$ \"+\" =\r\n      or or\r\n        { #1 'multiresult := }\r\n        { t #2 global.max$ substring$ 't := }\r\n      if$\r\n    }\r\n  while$\r\n  multiresult\r\n}\r\n\r\nFUNCTION {format.pages}\r\n{ pages empty$\r\n    { \"\" }\r\n    { pages multi.page.check\r\n        { \"pages\" pages n.dashify tie.or.space.connect }\r\n        { \"page\" pages tie.or.space.connect }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.vol.num.pages}\r\n{ volume field.or.null\r\n  bolden\r\n  number empty$\r\n    'skip$\r\n    { \"(\" number * \")\" * *\r\n      volume empty$\r\n        { \"there's a number but no volume in \" cite$ * warning$ }\r\n        'skip$\r\n      if$\r\n    }\r\n  if$\r\n  pages empty$\r\n    'skip$\r\n    { duplicate$ empty$\r\n        { pop$ format.pages }\r\n        { \", \" * pages n.dashify * }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.chapter.pages}\r\n{ chapter empty$\r\n    'format.pages\r\n    { type empty$\r\n        { \"chapter\" }\r\n        { type \"l\" change.case$ }\r\n      if$\r\n      chapter tie.or.space.connect\r\n      pages empty$\r\n        'skip$\r\n        { \", \" * format.pages * }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.in.ed.booktitle}\r\n{ booktitle empty$\r\n    { \"\" }\r\n    { editor empty$\r\n        { word.in booktitle emphasize * }\r\n        { word.in format.in.editors * \", \" * booktitle emphasize * }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.thesis.type}\r\n{ type empty$\r\n    'skip$\r\n    { pop$\r\n      type \"t\" change.case$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.tr.number}\r\n{ type empty$\r\n    { \"Technical Report\" }\r\n    'type\r\n  if$\r\n  number empty$\r\n    { \"t\" change.case$ }\r\n    { number tie.or.space.connect }\r\n  if$\r\n}\r\n\r\nFUNCTION {format.article.crossref}\r\n{\r\n  word.in\r\n  \"\\cite{\" * crossref * \"}\" *\r\n}\r\n\r\nFUNCTION {format.book.crossref}\r\n{ volume empty$\r\n    { \"empty volume in \" cite$ * \"'s crossref of \" * crossref * warning$\r\n      word.in\r\n    }\r\n    { \"Volume\" volume tie.or.space.connect\r\n      \" of \" *\r\n    }\r\n  if$\r\n  \"\\cite{\" * crossref * \"}\" *\r\n}\r\n\r\nFUNCTION {format.incoll.inproc.crossref}\r\n{\r\n  word.in\r\n  \"\\cite{\" * crossref * \"}\" *\r\n}\r\n\r\nFUNCTION {article}\r\n{ output.bibitem\r\n  format.authors \"author\" output.check\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.title \"title\" output.check\r\n  new.block\r\n  crossref missing$\r\n    { journal emphasize \"journal\" output.check\r\n      format.vol.num.pages output\r\n    }\r\n    { format.article.crossref output.nonnull\r\n      format.pages output\r\n    }\r\n  if$\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {book}\r\n{ output.bibitem\r\n  author empty$\r\n    { format.editors \"author and editor\" output.check\r\n      editor format.key output\r\n    }\r\n    { format.authors output.nonnull\r\n      crossref missing$\r\n        { \"author and editor\" editor either.or.check }\r\n        'skip$\r\n      if$\r\n    }\r\n  if$\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.btitle \"title\" output.check\r\n  crossref missing$\r\n    { format.bvolume output\r\n      new.block\r\n      format.number.series output\r\n      new.sentence\r\n      publisher \"publisher\" output.check\r\n      address output\r\n    }\r\n    {\r\n      new.block\r\n      format.book.crossref output.nonnull\r\n    }\r\n  if$\r\n  format.edition output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {booklet}\r\n{ output.bibitem\r\n  format.authors output\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.title \"title\" output.check\r\n  new.block\r\n  howpublished output\r\n  address output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {inbook}\r\n{ output.bibitem\r\n  author empty$\r\n    { format.editors \"author and editor\" output.check\r\n      editor format.key output\r\n    }\r\n    { format.authors output.nonnull\r\n      crossref missing$\r\n        { \"author and editor\" editor either.or.check }\r\n        'skip$\r\n      if$\r\n    }\r\n  if$\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.btitle \"title\" output.check\r\n  crossref missing$\r\n    { format.bvolume output\r\n      format.chapter.pages \"chapter and pages\" output.check\r\n      new.block\r\n      format.number.series output\r\n      new.sentence\r\n      publisher \"publisher\" output.check\r\n      address output\r\n    }\r\n    { format.chapter.pages \"chapter and pages\" output.check\r\n      new.block\r\n      format.book.crossref output.nonnull\r\n    }\r\n  if$\r\n  format.edition output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {incollection}\r\n{ output.bibitem\r\n  format.authors \"author\" output.check\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.title \"title\" output.check\r\n  new.block\r\n  crossref missing$\r\n    { format.in.ed.booktitle \"booktitle\" output.check\r\n      format.bvolume output\r\n      format.number.series output\r\n      format.chapter.pages output\r\n      new.sentence\r\n      publisher \"publisher\" output.check\r\n      address output\r\n      format.edition output\r\n    }\r\n    { format.incoll.inproc.crossref output.nonnull\r\n      format.chapter.pages output\r\n    }\r\n  if$\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {inproceedings}\r\n{ output.bibitem\r\n  format.authors \"author\" output.check\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.title \"title\" output.check\r\n  new.block\r\n  crossref missing$\r\n    { format.in.ed.booktitle \"booktitle\" output.check\r\n      format.bvolume output\r\n      format.number.series output\r\n      format.pages output\r\n      address output\r\n      new.sentence\r\n      organization output\r\n      publisher output\r\n    }\r\n    { format.incoll.inproc.crossref output.nonnull\r\n      format.pages output\r\n    }\r\n  if$\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {conference} { inproceedings }\r\n\r\nFUNCTION {manual}\r\n{ output.bibitem\r\n  format.authors output\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.btitle \"title\" output.check\r\n  organization address new.block.checkb\r\n  organization output\r\n  address output\r\n  format.edition output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {mastersthesis}\r\n{ output.bibitem\r\n  format.authors \"author\" output.check\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.btitle \"title\" output.check\r\n  new.block\r\n  \"Master's thesis\" format.thesis.type output.nonnull\r\n  school \"school\" output.check\r\n  address output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {misc}\r\n{ output.bibitem\r\n  format.authors output\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.title output\r\n  new.block\r\n  howpublished output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {phdthesis}\r\n{ output.bibitem\r\n  format.authors \"author\" output.check\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.btitle \"title\" output.check\r\n  new.block\r\n  \"Ph.D. thesis\" format.thesis.type output.nonnull\r\n  school \"school\" output.check\r\n  address output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {proceedings}\r\n{ output.bibitem\r\n  format.editors output\r\n  editor format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.btitle \"title\" output.check\r\n  format.bvolume output\r\n  format.number.series output\r\n  address output\r\n  new.sentence\r\n  organization output\r\n  publisher output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {techreport}\r\n{ output.bibitem\r\n  format.authors \"author\" output.check\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.title \"title\" output.check\r\n  new.block\r\n  format.tr.number output.nonnull\r\n  institution \"institution\" output.check\r\n  address output\r\n  new.block\r\n  note output\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {unpublished}\r\n{ output.bibitem\r\n  format.authors \"author\" output.check\r\n  author format.key output\r\n  format.date \"year\" output.check\r\n  new.block\r\n  format.title \"title\" output.check\r\n  new.block\r\n  note \"note\" output.check\r\n  fin.entry\r\n}\r\n\r\nFUNCTION {default.type} { misc }\r\n\r\nMACRO {jan} {\"January\"}\r\n\r\nMACRO {feb} {\"February\"}\r\n\r\nMACRO {mar} {\"March\"}\r\n\r\nMACRO {apr} {\"April\"}\r\n\r\nMACRO {may} {\"May\"}\r\n\r\nMACRO {jun} {\"June\"}\r\n\r\nMACRO {jul} {\"July\"}\r\n\r\nMACRO {aug} {\"August\"}\r\n\r\nMACRO {sep} {\"September\"}\r\n\r\nMACRO {oct} {\"October\"}\r\n\r\nMACRO {nov} {\"November\"}\r\n\r\nMACRO {dec} {\"December\"}\r\n\r\nMACRO {acmcs} {\"ACM Computing Surveys\"}\r\n\r\nMACRO {acta} {\"Acta Informatica\"}\r\n\r\nMACRO {cacm} {\"Communications of the ACM\"}\r\n\r\nMACRO {ibmjrd} {\"IBM Journal of Research and Development\"}\r\n\r\nMACRO {ibmsj} {\"IBM Systems Journal\"}\r\n\r\nMACRO {ieeese} {\"IEEE Transactions on Software Engineering\"}\r\n\r\nMACRO {ieeetc} {\"IEEE Transactions on Computers\"}\r\n\r\nMACRO {ieeetcad}\r\n {\"IEEE Transactions on Computer-Aided Design of Integrated Circuits\"}\r\n\r\nMACRO {ipl} {\"Information Processing Letters\"}\r\n\r\nMACRO {jacm} {\"Journal of the ACM\"}\r\n\r\nMACRO {jcss} {\"Journal of Computer and System Sciences\"}\r\n\r\nMACRO {scp} {\"Science of Computer Programming\"}\r\n\r\nMACRO {sicomp} {\"SIAM Journal on Computing\"}\r\n\r\nMACRO {tocs} {\"ACM Transactions on Computer Systems\"}\r\n\r\nMACRO {tods} {\"ACM Transactions on Database Systems\"}\r\n\r\nMACRO {tog} {\"ACM Transactions on Graphics\"}\r\n\r\nMACRO {toms} {\"ACM Transactions on Mathematical Software\"}\r\n\r\nMACRO {toois} {\"ACM Transactions on Office Information Systems\"}\r\n\r\nMACRO {toplas} {\"ACM Transactions on Programming Languages and Systems\"}\r\n\r\nMACRO {tcs} {\"Theoretical Computer Science\"}\r\n\r\nREAD\r\n\r\nFUNCTION {sortify}\r\n{ purify$\r\n  \"l\" change.case$\r\n}\r\n\r\nINTEGERS { len }\r\n\r\nFUNCTION {chop.word}\r\n{ 's :=\r\n  'len :=\r\n  s #1 len substring$ =\r\n    { s len #1 + global.max$ substring$ }\r\n    's\r\n  if$\r\n}\r\n\r\nFUNCTION {format.lab.names}\r\n{ 's :=\r\n  s #1 \"{vv~}{ll}\" format.name$\r\n  s num.names$ duplicate$\r\n  #2 >\r\n    { pop$ \" \" * \"et~al.\" emphasize * }\r\n    { #2 <\r\n        'skip$\r\n        { s #2 \"{ff }{vv }{ll}{ jj}\" format.name$ \"others\" =\r\n            { \" \" * \"et~al.\" emphasize * }\r\n            { \" and \" * s #2 \"{vv~}{ll}\" format.name$ * }\r\n          if$\r\n        }\r\n      if$\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {author.key.label}\r\n{ author empty$\r\n    { key empty$\r\n        { cite$ #1 #3 substring$ }\r\n        'key\r\n      if$\r\n    }\r\n    { author format.lab.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {author.editor.key.label}\r\n{ author empty$\r\n    { editor empty$\r\n        { key empty$\r\n            { cite$ #1 #3 substring$ }\r\n            'key\r\n          if$\r\n        }\r\n        { editor format.lab.names }\r\n      if$\r\n    }\r\n    { author format.lab.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {editor.key.label}\r\n{ editor empty$\r\n    { key empty$\r\n        { cite$ #1 #3 substring$ }\r\n        'key\r\n      if$\r\n    }\r\n    { editor format.lab.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {calc.label}\r\n{ type$ \"book\" =\r\n  type$ \"inbook\" =\r\n  or\r\n    'author.editor.key.label\r\n    { type$ \"proceedings\" =\r\n        'editor.key.label\r\n        'author.key.label\r\n      if$\r\n    }\r\n  if$\r\n  \"(\"\r\n  *\r\n  year duplicate$ empty$\r\n     { pop$ \"????\" }\r\n     { purify$ #-1 #4 substring$ }\r\n  if$\r\n  *\r\n  'label :=\r\n}\r\n\r\nFUNCTION {sort.format.names}\r\n{ 's :=\r\n  #1 'nameptr :=\r\n  \"\"\r\n  s num.names$ 'numnames :=\r\n  numnames 'namesleft :=\r\n    { namesleft #0 > }\r\n    { nameptr #1 >\r\n        { \"   \" * }\r\n        'skip$\r\n      if$\r\n      s nameptr\r\n      \"{vv{ } }{ll{ }}{  f{ }}{  jj{ }}\"\r\n      format.name$ 't :=\r\n      nameptr numnames = t \"others\" = and\r\n        { \"et al\" * }\r\n        { numnames #2 > nameptr #2 = and\r\n          { \"zzzzzz\" * #1 'namesleft := }\r\n          { t sortify * }\r\n        if$\r\n        }\r\n      if$\r\n      nameptr #1 + 'nameptr :=\r\n      namesleft #1 - 'namesleft :=\r\n    }\r\n  while$\r\n}\r\n\r\nFUNCTION {sort.format.title}\r\n{ 't :=\r\n  \"A \" #2\r\n    \"An \" #3\r\n      \"The \" #4 t chop.word\r\n    chop.word\r\n  chop.word\r\n  sortify\r\n  #1 global.max$ substring$\r\n}\r\n\r\nFUNCTION {author.sort}\r\n{ author empty$\r\n    { key empty$\r\n        { \"to sort, need author or key in \" cite$ * warning$\r\n          \"\"\r\n        }\r\n        { key sortify }\r\n      if$\r\n    }\r\n    { author sort.format.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {author.editor.sort}\r\n{ author empty$\r\n    { editor empty$\r\n        { key empty$\r\n            { \"to sort, need author, editor, or key in \" cite$ * warning$\r\n              \"\"\r\n            }\r\n            { key sortify }\r\n          if$\r\n        }\r\n        { editor sort.format.names }\r\n      if$\r\n    }\r\n    { author sort.format.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {editor.sort}\r\n{ editor empty$\r\n    { key empty$\r\n        { \"to sort, need editor or key in \" cite$ * warning$\r\n          \"\"\r\n        }\r\n        { key sortify }\r\n      if$\r\n    }\r\n    { editor sort.format.names }\r\n  if$\r\n}\r\n\r\nFUNCTION {presort}\r\n{ calc.label\r\n  label sortify\r\n  \"    \"\r\n  *\r\n  type$ \"book\" =\r\n  type$ \"inbook\" =\r\n  or\r\n    'author.editor.sort\r\n    { type$ \"proceedings\" =\r\n        'editor.sort\r\n        'author.sort\r\n      if$\r\n    }\r\n  if$\r\n  #1 entry.max$ substring$\r\n  'sort.label :=\r\n  sort.label\r\n  *\r\n  \"    \"\r\n  *\r\n  title field.or.null\r\n  sort.format.title\r\n  *\r\n  #1 entry.max$ substring$\r\n  'sort.key$ :=\r\n}\r\n\r\nITERATE {presort}\r\n\r\nSORT\r\n\r\nSTRINGS { last.label next.extra }\r\n\r\nINTEGERS { last.extra.num }\r\n\r\nFUNCTION {initialize.extra.label.stuff}\r\n{ #0 int.to.chr$ 'last.label :=\r\n  \"\" 'next.extra :=\r\n  #0 'last.extra.num :=\r\n}\r\n\r\nFUNCTION {forward.pass}\r\n{ last.label label =\r\n    { last.extra.num #1 + 'last.extra.num :=\r\n      last.extra.num int.to.chr$ 'extra.label :=\r\n    }\r\n    { \"a\" chr.to.int$ 'last.extra.num :=\r\n      \"\" 'extra.label :=\r\n      label 'last.label :=\r\n    }\r\n  if$\r\n}\r\n\r\nFUNCTION {reverse.pass}\r\n{ next.extra \"b\" =\r\n    { \"a\" 'extra.label := }\r\n    'skip$\r\n  if$\r\n  extra.label 'next.extra :=\r\n  label extra.label * 'label :=\r\n}\r\n\r\nEXECUTE {initialize.extra.label.stuff}\r\n\r\nITERATE {forward.pass}\r\n\r\nREVERSE {reverse.pass}\r\n\r\nFUNCTION {bib.sort.order}\r\n{ sort.label\r\n  \"    \"\r\n  *\r\n  year field.or.null sortify\r\n  *\r\n  \"    \"\r\n  *\r\n  title field.or.null\r\n  sort.format.title\r\n  *\r\n  #1 entry.max$ substring$\r\n  'sort.key$ :=\r\n}\r\n\r\nITERATE {bib.sort.order}\r\n\r\nSORT\r\n\r\nFUNCTION {begin.bib}\r\n{ preamble$ empty$\r\n    'skip$\r\n    { preamble$ write$ newline$ }\r\n  if$\r\n  \"\\begin{thebibliography}{}\" write$ newline$\r\n}\r\n\r\nEXECUTE {begin.bib}\r\n\r\nEXECUTE {init.state.consts}\r\n\r\nITERATE {call.type$}\r\n\r\nFUNCTION {end.bib}\r\n{ newline$\r\n  \"\\end{thebibliography}\" write$ newline$\r\n}\r\n\r\nEXECUTE {end.bib}\r\n%% End of customized bst file \r\n\r\n"
        },
        {
          "name": "notation.tex",
          "type": "blob",
          "size": 6.87,
          "content": "% !Mode:: \"TeX:UTF-8\"\n%TODO\n\\chapter*{数学符号}\n\\label{notation}\n\n\\addcontentsline{toc}{chapter}{数学符号}\n\n\n本节简要介绍本书所使用的数学符号。 \n我们在\\chapref{chap:linear_algebra}至\\chapref{chap:numerical_computation}中描述大多数数学概念，如果你不熟悉任何相应的数学概念，可以参考对应的章节。\n\n\\vspace{\\notationgap}\n\\begin{minipage}{\\textwidth}\n\\centerline{\\bf 数和数组}\n\\bgroup\n% The \\arraystretch definition here increases the space between rows in the table,\n% so that \\displaystyle math has more vertical space.\n\\def\\arraystretch{1.5}\n\\begin{tabular}{cp{3.25in}}\n$\\displaystyle a$ & 标量 (整数或实数) \\\\\n$\\displaystyle \\Va$ & 向量 \\\\\n$\\displaystyle \\MA$ & 矩阵 \\\\\n$\\displaystyle \\TSA$ & 张量 \\\\\n$\\displaystyle \\MI_n$ & $n$行$n$列的\\gls{identity_matrix} \\\\\n    $\\displaystyle \\MI$ &  维度蕴含于上下文的\\gls{identity_matrix} \\\\\n$\\displaystyle \\Ve^{(i)}$ & 标准基向量$[0,\\dots,0,1,0,\\dots,0]$，其中索引$i$处值为1 \\\\ \n$\\displaystyle \\text{diag}(\\Va)$ & 对角方阵，其中对角元素由$\\Va$给定 \\\\\n$\\displaystyle \\RSa$ & 标量随机变量 \\\\\n$\\displaystyle \\RVa$ & 向量随机变量 \\\\\n$\\displaystyle \\RMA$ & 矩阵随机变量 \\\\\n\\end{tabular}\n\\egroup\n\\end{minipage}\n\n\\vspace{\\notationgap}\n\\begin{minipage}{\\textwidth}\n\\centerline{\\bf 集合和图}\n\\bgroup\n\\def\\arraystretch{1.5}\n\\begin{tabular}{cp{3.25in}}\n$\\displaystyle \\SetA$ & 集合 \\\\\n$\\displaystyle \\SetR$ & 实数集 \\\\\n$\\displaystyle \\{0, 1\\}$ & 包含0和1的集合 \\\\\n$\\displaystyle \\{0, 1, \\dots, n \\}$ & 包含$0$和$n$之间所有整数的集合 \\\\\n$\\displaystyle [a, b]$ & 包含$a$和$b$的实数区间 \\\\\n$\\displaystyle (a, b]$ & 不包含$a$但包含$b$的实数区间 \\\\\n$\\displaystyle \\SetA \\backslash \\SetB$ & 差集，即其元素包含于$\\SetA$但不包含于$\\SetB$\\\\\n$\\displaystyle \\CalG$ & 图 \\\\\n$\\displaystyle Pa_\\CalG(\\RSx_i)$ & 图$\\CalG$中$\\RSx_i$的父节点\n\\end{tabular}\n\\egroup\n\\end{minipage}\n\n\\vspace{\\notationgap}\n\\begin{minipage}{\\textwidth}\n\\centerline{\\bf 索引}\n\\bgroup\n\\def\\arraystretch{1.5}\n\\begin{tabular}{cp{3.25in}}\n$\\displaystyle a_i$ & 向量$\\Va$的第$i$个元素，其中索引从1开始  \\\\\n$\\displaystyle a_{-i}$ & 除了第$i$个元素，$\\Va$的所有元素 \\\\\n$\\displaystyle A_{i,j}$ & 矩阵$\\MA$的$i,j$元素 \\\\\n$\\displaystyle \\MA_{i, :}$ & 矩阵$\\MA$的第$i$行 \\\\\n$\\displaystyle \\MA_{:, i}$ & 矩阵$\\MA$的第$i$列 \\\\\n$\\displaystyle \\TEA_{i, j, k}$ & 3维张量$\\TSA$的$(i, j, k)$元素   \\\\\n$\\displaystyle \\TSA_{:, :, i}$ & 3维张量的2维切片 \\\\\n$\\displaystyle \\RSa_i$ & 随机向量$\\RVa$的第$i$个元素 \\\\\n\\end{tabular} \n\\egroup\n\\end{minipage}\n\n\\vspace{\\notationgap}\n\\begin{minipage}{\\textwidth}\n\\centerline{\\bf 线性代数中的操作}\n\\bgroup\n\\def\\arraystretch{1.5}\n\\begin{tabular}{cp{3.25in}}\n$\\displaystyle \\MA^\\top$ & 矩阵$\\MA$的转置 \\\\\n$\\displaystyle \\MA^+$ & $\\MA$的\\gls{Moore} \\\\\n    $\\displaystyle \\MA \\odot \\MB $ &  $\\MA$和$\\MB$的逐元素乘积（\\gls{hadamard_product}） \\\\\n$\\displaystyle \\mathrm{det}(\\MA)$ & $\\MA$的行列式 \\\\\n\\end{tabular} \n\\egroup\n\\end{minipage}\n\n\\vspace{\\notationgap}\n\\begin{minipage}{\\textwidth}\n\\centerline{\\bf 微积分}\n\\bgroup\n\\def\\arraystretch{1.5}\n\\begin{tabular}{cp{3.25in}}\n$\\displaystyle\\frac{d y} {d x}$ &  $y$关于$x$的导数 \\\\\n$\\displaystyle \\frac{\\partial y} {\\partial x} $ &  $y$关于$x$的偏导 \\\\\n$\\displaystyle \\nabla_{\\Vx} y $ & $y$关于$\\Vx$的梯度 \\\\\n$\\displaystyle \\nabla_{\\MX} y $ & $y$关于$\\MX$的矩阵导数 \\\\\n$\\displaystyle \\nabla_{\\TSX} y $ &  $y$关于$\\TSX$求导后的张量 \\\\\n$\\displaystyle \\frac{\\partial f}{\\partial \\Vx} $ &$f: \\SetR^n \\rightarrow \\SetR^m$的\\gls{jacobian}矩阵$\\MJ \\in \\SetR^{m\\times n}$   \\\\\n$\\displaystyle \\nabla_{\\Vx}^2 f(\\Vx)\\text{ or }\\MH( f)(\\Vx)$ &  $f$在点$\\Vx$处的\\gls{hessian}矩阵 \\\\\n$\\displaystyle \\int f(\\Vx) d\\Vx $ & $\\Vx$整个域上的定积分 \\\\\n$\\displaystyle \\int_\\SetS f(\\Vx) d\\Vx$ & 集合$\\SetS$上关于$\\Vx$的定积分 \\\\\n\\end{tabular}\n\\egroup\n\\end{minipage}\n\n\\vspace{\\notationgap}\n\\begin{minipage}{\\textwidth}\n\\centerline{\\bf 概率和信息论}\n\\bgroup\n\\def\\arraystretch{1.5}\n\\begin{tabular}{cp{3.25in}}\n$\\displaystyle \\RSa \\bot \\RSb$ &  $\\RSa$和$\\RSb$相互独立的随机变量 \\\\\n$\\displaystyle \\RSa \\bot \\RSb \\mid \\RSc $ &  给定$\\RSc$后条件独立 \\\\\n$\\displaystyle P(\\RSa)$ & 离散变量上的概率分布 \\\\\n$\\displaystyle p(\\RSa)$ & 连续变量（或变量类型未指定时）上的概率分布  \\\\\n$\\displaystyle \\RSa \\sim P$ &  具有分布$P$的随机变量$\\RSa$\\\\\n$\\displaystyle  \\SetE_{\\RSx\\sim P} [ f(x) ]\\text{ or } \\SetE f(x)$ & $f(x)$关于$P(\\RSx)$的期望 \\\\\n$\\displaystyle \\Var(f(x)) $ &  $f(x)$在分布$P(\\RSx)$下的方差 \\\\\n$\\displaystyle \\Cov(f(x),g(x)) $ &  $f(x)$和$g(x)$在分布$P(\\RSx)$下的协方差 \\\\\n$\\displaystyle H(\\RSx) $ & 随机变量$\\RSx$的\\gls{Shannon_entropy} \\\\\n$\\displaystyle D_{\\text{KL}} ( P \\Vert Q ) $ & P和Q的\\gls{KL_divergence} \\\\\n$\\displaystyle \\mathcal{N} ( \\Vx ; \\Vmu , \\VSigma)$ & 均值为$\\Vmu$协方差为$\\VSigma$，$\\Vx$上的\\gls{gaussian_distribution} \\\\\n\\end{tabular}\n\\egroup\n\\end{minipage}\n\n\\vspace{\\notationgap}\n\\begin{minipage}{\\textwidth}\n\\centerline{\\bf 函数}\n\\bgroup\n\\def\\arraystretch{1.5}\n\\begin{tabular}{cp{3.25in}}\n$\\displaystyle f: \\SetA \\rightarrow \\SetB$ & 定义域为$\\SetA$值域为$\\SetB$的函数$f$ \\\\\n$\\displaystyle f \\circ g $ &  $f$和$g$的组合 \\\\\n$\\displaystyle f(\\Vx ; \\Vtheta) $ &  由$\\Vtheta$参数化，关于$\\Vx$的函数（有时为简化表示，我们忽略$\\Vtheta$记为$f(\\Vx)$ ）\\\\\n$\\displaystyle \\log x$ & $x$的自然对数 \\\\\n$\\displaystyle \\sigma(x)$ & Logistic sigmoid, $\\displaystyle \\frac{1} {1 + \\exp(-x)}$ \\\\\n$\\displaystyle \\zeta(x)$ & Softplus, $\\log(1 + \\exp(x))$ \\\\\n$\\displaystyle || \\Vx ||_p $ & $\\Vx$的$L^p$范数 \\\\\n$\\displaystyle || \\Vx || $ &  $\\Vx$的$L^2$范数 \\\\\n$\\displaystyle x^+$ & $x$的正数部分, 即$\\max(0,x)$\\\\\n$\\displaystyle \\textbf{1}_\\mathrm{condition}$ & 如果条件为真则为1，否则为0\\\\ \n\\end{tabular}\n\\egroup\n\\end{minipage}\n\n有时候我们使用函数$f$，它的参数是一个标量，但应用到一个向量、矩阵或张量： $f(\\Vx)$, $f(\\MX)$, or $f(\\TSX)$ 。\n这表示逐元素地将$f$应用于数组。\n例如，$\\TSC = \\sigma(\\TSX)$，则对于所有合法的$i$、$j$和$k$， $\\TEC_{i,j,k} = \\sigma(\\TEX_{i,j,k})$。\n\n\n\\vspace{\\notationgap}\n\\begin{minipage}{\\textwidth}\n\\centerline{\\bf 数据集和分布}\n\\bgroup\n\\def\\arraystretch{1.5}\n\\begin{tabular}{cp{3.25in}}\n$\\displaystyle p_{\\text{data}}$ & 数据生成分布 \\\\\n$\\displaystyle \\hat{p}_{\\text{train}}$ & 由训练集定义的经验分布 \\\\\n$\\displaystyle \\SetX$ & 训练样本的集合 \\\\\n$\\displaystyle \\Vx^{(i)}$ & 数据集的第$i$个样本（输入）\\\\\n$\\displaystyle y^{(i)}\\text{ or }\\Vy^{(i)}$ & \\gls{supervised_learning}中与$\\Vx^{(i)}$关联的目标 \\\\\n$\\displaystyle \\MX$ & $m \\times n$ 的矩阵，其中行$\\MX_{i,:}$为输入样本$\\Vx^{(i)}$ \\\\\n\\end{tabular} \n\\egroup\n\\end{minipage}\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "terminology.tex",
          "type": "blob",
          "size": 105.63,
          "content": "% !Mode:: \"TeX:UTF-8\"\n\\newglossaryentry{DL}\n{\n  name=深度学习,\n  description={deep learning},\n  sort={deep learning},\n}\n\n\\newglossaryentry{knowledge_base}\n{\n  name=知识库,\n  description={knowledge base},\n  sort={knowledge base},\n}\n\n\\newglossaryentry{ML}\n{\n  name=机器学习,\n  description={machine learning},\n  sort={machine learning},\n}\n\n\\newglossaryentry{ML_model}\n{\n  name=机器学习模型,\n  description={machine learning model},\n  sort={machine learning model},\n}\n\n\\newglossaryentry{ebv}\n{\n  name=基本单位向量,\n  description={elementary basis vectors},\n  sort={elementary basis vectors},\n}\n\n\\newglossaryentry{logistic_regression}\n{\n  name=逻辑回归,\n  description={logistic regression},\n  sort={logistic regression},\n}\n\n\\newglossaryentry{regression}\n{\n  name=回归,\n  description={regression},\n  sort={regression},\n}\n\n\\newglossaryentry{AI}\n{\n  name=人工智能,\n  description={artificial intelligence},\n  sort={artificial intelligence},\n  symbol={AI}\n}\n\n\\newglossaryentry{naive_bayes}\n{\n  name=朴素贝叶斯,\n  description={naive Bayes},\n  sort={naive Bayes},\n}\n\n\\newglossaryentry{representation}\n{\n  name=表示,\n  description={representation},\n  sort={representation},\n}\n\n\\newglossaryentry{representation_learning}\n{\n  name=表示学习,\n  description={representation learning},\n  sort={representation learning},\n}\n\n\\newglossaryentry{AE}\n{\n  name=自编码器,\n  description={autoencoder},\n  sort={autoencoder},\n}\n\n\\newglossaryentry{encoder}\n{\n  name=编码器,\n  description={encoder},\n  sort={encoder},\n}\n\n\\newglossaryentry{decoder}\n{\n  name=解码器,\n  description={decoder},\n  sort={decoder},\n}\n\n\\newglossaryentry{MLP}\n{\n  name=多层感知机,\n  description={multilayer perceptron},\n  sort={multilayer perceptron},\n  symbol={MLP}\n}\n\n\\newglossaryentry{cybernetics}\n{\n  name=控制论,\n  description={cybernetics},\n  sort={cybernetics},\n}\n\n\\newglossaryentry{connectionism}\n{\n  name=联结主义,\n  description={connectionism},\n  sort={connectionism},\n}\n\n\\newglossaryentry{ANN}\n{\n  name=人工神经网络,\n  description={artificial neural network},\n  sort={artificial neural network},\n  symbol={ANN}\n}\n\n\\newglossaryentry{NN}\n{\n  name=神经网络,\n  description={neural network},\n  sort={neural network},\n}\n\n\\newglossaryentry{SGD}\n{\n  name=随机梯度下降,\n  description={stochastic gradient descent},\n  sort={stochastic gradient descent},\n  symbol={SGD}\n}\n\n\\newglossaryentry{linear_model}\n{\n  name=线性模型,\n  description={linear model},\n  sort={linear model},\n}\n\n\\newglossaryentry{mode}\n{\n  name=峰值,\n  description={mode},\n  sort={mode},\n}\n\n\\newglossaryentry{unimodal}\n{\n  name=单峰值,\n  description={unimodal},\n  sort={unimodal},\n}\n\n\\newglossaryentry{modality}\n{\n  name=模态,\n  description={modality},\n  sort={modality},\n}\n\n\\newglossaryentry{multimodal}\n{\n  name=多峰值,\n  description={multimodal},\n  sort={multimodal},\n}\n\n\\newglossaryentry{linear_regression}\n{\n  name=线性回归,\n  description={linear regression},\n  sort={linear regression},\n}\n\n\\newglossaryentry{ReLU}\n{\n  name=整流线性单元,\n  description={rectified linear unit},\n  sort={rectified linear unit},\n  symbol={ReLU}\n}\n\n\\newglossaryentry{distributed_representation}\n{\n  name=分布式表示,\n  description={distributed representation},\n  sort={distributed representation},\n}\n\n\\newglossaryentry{nondistributed_representation}\n{\n  name=非分布式表示,\n  description={nondistributed representation},\n  sort={nondistributed representation},\n}\n\n\\newglossaryentry{nondistributed}\n{\n  name=非分布式,\n  description={nondistributed},\n  sort={nondistributed},\n}\n\n\\newglossaryentry{hidden_unit}\n{\n  name=隐藏单元,\n  description={hidden unit},\n  sort={hidden unit},\n}\n\n\\newglossaryentry{LSTM}\n{\n  name=长短期记忆,\n  description={long short-term memory},\n  sort={long short-term memory},\n  symbol={LSTM}\n}\n\n\\newglossaryentry{DBN}\n{\n  name=深度信念网络,\n  description={deep belief network},\n  sort={deep belief network},\n  symbol={DBN}\n}\n\n\\newglossaryentry{RNN}\n{\n  name=循环神经网络,\n  description={recurrent neural network},\n  sort={recurrent neural network},\n  symbol={RNN}\n}\n\n\\newglossaryentry{recurrence}\n{\n  name=循环,\n  description={recurrence},\n  sort={recurrence},\n}\n\n\\newglossaryentry{RL}\n{\n  name=强化学习,\n  description={reinforcement learning},\n  sort={reinforcement learning},\n}\n\n\\newglossaryentry{inference}\n{\n  name=推断,\n  description={inference},\n  sort={inference},\n}\n\n\\newglossaryentry{overflow}\n{\n  name=上溢,\n  description={overflow},\n  sort={overflow},\n}\n\n\\newglossaryentry{underflow}\n{\n  name=下溢,\n  description={underflow},\n  sort={underflow},\n}\n\n\\newglossaryentry{softmax}\n{\n  name=softmax函数,\n  description={softmax function},\n  sort={softmax function},\n}\n\n\\newglossaryentry{softmax_chap15}\n{\n  name=softmax,\n  description={softmax},\n  sort={softmax},\n}\n\n\\newglossaryentry{underestimation}\n{\n  name=欠估计,\n  description={underestimation},\n  sort={underestimation},\n}\n\n\\newglossaryentry{overestimation}\n{\n  name=过估计,\n  description={overestimation},\n  sort={overestimation},\n}\n\n\\newglossaryentry{softmax_unit}\n{\n  name=softmax单元,\n  description={softmax unit},\n  sort={softmax unit},\n}\n\n\\newglossaryentry{softmax_chap11}\n{\n  name=softmax,\n  description={softmax},\n  sort={softmax},\n}\n\n\\newglossaryentry{multinoulli}\n{\n  name=Multinoulli分布,\n  description={multinoulli distribution},\n  sort={multinoulli distribution},\n}\n\n\\newglossaryentry{poor_conditioning}\n{\n  name=病态条件,\n  description={poor conditioning},\n  sort={poor conditioning},\n}\n\n\\newglossaryentry{objective_function}\n{\n  name=目标函数,\n  description={objective function},\n  sort={objective function},\n}\n\n\\newglossaryentry{objective}\n{\n  name=目标,\n  description={objective},\n  sort={objective},\n}\n\n\\newglossaryentry{criterion}\n{\n  name=准则,\n  description={criterion},\n  sort={criterion},\n}\n\n\\newglossaryentry{cost_function}\n{\n  name=代价函数,\n  description={cost function},\n  sort={cost function},\n}\n\n\\newglossaryentry{cost}\n{\n  name=代价,\n  description={cost},\n  sort={cost},\n}\n\n\\newglossaryentry{loss_function}\n{\n  name=损失函数,\n  description={loss function},\n  sort={loss function},\n}\n\n\\newglossaryentry{prcurve}\n{\n  name=PR曲线,\n  description={PR curve},\n  sort={PR curve},\n}\n\n\\newglossaryentry{fscore}\n{\n  name=F分数,\n  description={F-score},\n  sort={F-score},\n}\n\n\\newglossaryentry{loss}\n{\n  name=损失,\n  description={loss},\n  sort={loss},\n}\n\n\\newglossaryentry{error_function}\n{\n  name=误差函数,\n  description={error function},\n  sort={error function},\n}\n\n\\newglossaryentry{GD}\n{\n  name=梯度下降,\n  description={gradient descent},\n  sort={gradient descent},\n}\n\n\\newglossaryentry{local_descent}\n{\n  name=局部下降,\n  description={local descent},\n  sort={local descent},\n}\n\n\\newglossaryentry{steepest}\n{\n  name=最陡下降,\n  description={steepest descent},\n  sort={steepest descent},\n}\n\n\\newglossaryentry{GA}\n{\n  name=梯度上升,\n  description={gradient ascent},\n  sort={gradient ascent},\n}\n\n\\newglossaryentry{derivative}\n{\n  name=导数,\n  description={derivative},\n  sort={derivative},\n}\n\n\\newglossaryentry{critical_points}\n{\n  name=临界点,\n  description={critical point},\n  sort={critical point},\n}\n\n\\newglossaryentry{stationary_point}\n{\n  name=驻点,\n  description={stationary point},\n  sort={stationary point},\n}\n\n\\newglossaryentry{local_minimum}\n{\n  name=局部极小点,\n  description={local minimum},\n  sort={local minimum},\n}\n\n\\newglossaryentry{minimum}\n{\n  name=极小点,\n  description={minimum},\n  sort={minimum},\n}\n\n\\newglossaryentry{local_minima}\n{\n  name=局部极小值,\n  description={local minima},\n  sort={local minima},\n}\n\n\\newglossaryentry{minima}\n{\n  name=极小值,\n  description={minima},\n  sort={minima},\n}\n\n\\newglossaryentry{global_minima}\n{\n  name=全局极小值,\n  description={global minima},\n  sort={global minima},\n}\n\n\\newglossaryentry{local_maxima}\n{\n  name=局部极大值,\n  description={local maxima},\n  sort={local maxima},\n}\n\n\\newglossaryentry{maxima}\n{\n  name=极大值,\n  description={maxima},\n  sort={maxima},\n}\n\n\\newglossaryentry{local_maximum}\n{\n  name=局部极大点,\n  description={local maximum},\n  sort={local maximum},\n}\n\n\\newglossaryentry{saddle_points}\n{\n  name=鞍点,\n  description={saddle point},\n  sort={saddle point},\n}\n\n\\newglossaryentry{global_minimum}\n{\n  name=全局最小点,\n  description={global minimum},\n  sort={global minimum},\n}\n\n\\newglossaryentry{partial_derivatives}\n{\n  name=偏导数,\n  description={partial derivative},\n  sort={partial derivative},\n}\n\n\\newglossaryentry{gradient}\n{\n  name=梯度,\n  description={gradient},\n  sort={gradient},\n}\n\n\\newglossaryentry{identifiable}\n{\n  name=可辨认的,\n  description={identifiable},\n  sort={identifiable},\n}\n\n\\newglossaryentry{directional_derivative}\n{\n  name=方向导数,\n  description={directional derivative},\n  sort={directional derivative},\n}\n\n\\newglossaryentry{line_search}\n{\n  name=线搜索,\n  description={line search},\n  sort={line search},\n}\n\n\\newglossaryentry{example}\n{\n  name=样本,\n  description={example},\n  sort={example},\n}\n\n\\newglossaryentry{hill_climbing}\n{\n  name=爬山,\n  description={hill climbing},\n  sort={hill climbing},\n}\n\n\\newglossaryentry{ill_conditioning}\n{\n  name=病态,\n  description={ill conditioning},\n  sort={ill conditioning},\n}\n\n\\newglossaryentry{jacobian}\n{\n  name=Jacobian,\n  description={Jacobian},\n  sort={Jacobian},\n}\n\n\\newglossaryentry{hessian}\n{\n  name=Hessian,\n  description={Hessian},\n  sort={Hessian},\n}\n\n\\newglossaryentry{second_derivative}\n{\n  name=二阶导数,\n  description={second derivative},\n  sort={second derivative},\n}\n\n\\newglossaryentry{curvature}\n{\n  name=曲率,\n  description={curvature},\n  sort={curvature},\n}\n\n\\newglossaryentry{taylor}\n{\n  name=泰勒,\n  description={taylor},\n  sort={taylor},\n}\n\n\\newglossaryentry{second_derivative_test}\n{\n  name=二阶导数测试,\n  description={second derivative test},\n  sort={second derivative test},\n}\n\n\\newglossaryentry{newton_method}\n{\n  name=牛顿法,\n  description={Newton's method},\n  sort={Newton's method},\n}\n\n\\newglossaryentry{second_order_method}\n{\n  name=二阶方法,\n  description={second-order method},\n  sort={second-order method},\n}\n\n\\newglossaryentry{first_order_method}\n{\n  name=一阶方法,\n  description={first-order method},\n  sort={first-order method},\n}\n\n\\newglossaryentry{lipschitz}\n{\n  name=Lipschitz,\n  description={Lipschitz},\n  sort={Lipschitz},\n}\n\n\\newglossaryentry{lipschitz_continuous}\n{\n  name=Lipschitz连续,\n  description={Lipschitz continuous},\n  sort={Lipschitz continuous},\n}\n\n\\newglossaryentry{lipschitz_constant}\n{\n  name=Lipschitz常数,\n  description={Lipschitz constant},\n  sort={Lipschitz constant},\n}\n\n\\newglossaryentry{convex_optimization}\n{\n  name=凸优化,\n  description={Convex optimization},\n  sort={Convex optimization},\n}\n\n\\newglossaryentry{nonconvex}\n{\n  name=非凸,\n  description={nonconvex},\n  sort={nonconvex},\n}\n\n\\newglossaryentry{nume_optimization}\n{\n  name=数值优化,\n  description={numerical optimization},\n  sort={numerical optimization},\n}\n\n\\newglossaryentry{constrained_optimization}\n{\n  name=约束优化,\n  description={constrained optimization},\n  sort={constrained optimization},\n}\n\n\\newglossaryentry{feasible}\n{\n  name=可行,\n  description={feasible},\n  sort={feasible},\n}\n\n\\newglossaryentry{KKT}\n{\n  name=Karush–Kuhn–Tucker,\n  description={Karush–Kuhn–Tucker},\n  sort={Karush–Kuhn–Tucker},\n  symbol={KKT}\n}\n\n\\newglossaryentry{generalized_lagrangian}\n{\n  name=广义Lagrangian,\n  description={generalized Lagrangian},\n  sort={generalized Lagrangian},\n}\n\n\\newglossaryentry{generalized_lagrange_function}\n{\n  name=广义Lagrange函数,\n  description={generalized Lagrange function},\n  sort={generalized Lagrange function},\n}\n\n\\newglossaryentry{equality_constraints}\n{\n  name=等式约束,\n  description={equality constraint},\n  sort={equality constraint},\n}\n\n\\newglossaryentry{inequality_constraints}\n{\n  name=不等式约束,\n  description={inequality constraint},\n  sort={inequality constraint},\n}\n\n\\newglossaryentry{regularization}\n{\n  name=正则化,\n  description={regularization},\n  sort={regularization},\n}\n\n\\newglossaryentry{regularizer}\n{\n  name=正则化项,\n  description={regularizer},\n  sort={regularizer},\n}\n\n\\newglossaryentry{regularize}\n{\n  name=正则化,\n  description={regularize},\n  sort={regularize},\n}\n\n\\newglossaryentry{generalization}\n{\n  name=泛化,\n  description={generalization},\n  sort={generalization},\n}\n\n\\newglossaryentry{generalize}\n{\n  name=泛化,\n  description={generalize},\n  sort={generalize},\n}\n\n\\newglossaryentry{underfitting}\n{\n  name=欠拟合,\n  description={underfitting},\n  sort={underfitting},\n}\n\n\\newglossaryentry{overfitting}\n{\n  name=过拟合,\n  description={overfitting},\n  sort={overfitting},\n}\n\n\\newglossaryentry{bias_sta}\n{\n  name=偏差,\n  description={bias in statistics},\n  sort={bias in statistics},\n}\n\n\\newglossaryentry{BIAS}\n{\n  name=偏差,\n  description={biass},\n  sort={biass},\n}\n\n\\newglossaryentry{bias_aff}\n{\n  name=偏置,\n  description={bias in affine function},\n  sort={bias in affine function},\n}\n\n\\newglossaryentry{variance}\n{\n  name=方差,\n  description={variance},\n  sort={variance},\n}\n\n\\newglossaryentry{ensemble}\n{\n  name=集成,\n  description={ensemble},\n  sort={ensemble},\n}\n\n\\newglossaryentry{estimator}\n{\n  name=估计,\n  description={estimator},\n  sort={estimator},\n}\n\n\\newglossaryentry{weight_decay}\n{\n  name=权重衰减,\n  description={weight decay},\n  sort={weight decay},\n}\n\n\\newglossaryentry{ridge_regression}\n{\n  name=岭回归,\n  description={ridge regression},\n  sort={ridge regression},\n}\n\n\\newglossaryentry{tikhonov_regularization}\n{\n  name=Tikhonov正则,\n  description={Tikhonov regularization},\n  sort={Tikhonov regularization},\n}\n\n\\newglossaryentry{covariance}\n{\n  name=协方差,\n  description={covariance},\n  sort={covariance},\n}\n\n\\newglossaryentry{sparse}\n{\n  name=稀疏,\n  description={sparse},\n  sort={sparse},\n}\n\n\\newglossaryentry{feature_selection}\n{\n  name=特征选择,\n  description={feature selection},\n  sort={feature selection},\n}\n\n\\newglossaryentry{feature_extractor}\n{\n  name=特征提取器,\n  description={feature extractor},\n  sort={feature extractor},\n}\n\n\\newglossaryentry{MAP}\n{\n  name=最大后验,\n  description={Maximum A Posteriori},\n  sort={Maximum A Posteriori},\n  symbol={MAP}\n}\n\n\\newglossaryentry{pooling}\n{\n  name=池化,\n  description={pooling},\n  sort={pooling},\n}\n\n\\newglossaryentry{dropout}\n{\n  name=Dropout,\n  description={Dropout},\n  sort={dropout},\n}\n\n\\newglossaryentry{monte_carlo}\n{\n  name=蒙特卡罗,\n  description={Monte Carlo},\n  sort={Monte Carlo},\n}\n\n\\newglossaryentry{early_stopping}\n{\n  name=提前终止,\n  description={early stopping},\n  sort={early stopping},\n}\n\n\\newglossaryentry{CNN}\n{\n  name=卷积神经网络,\n  description={convolutional neural network},\n  sort={convolutional neural network},\n  symbol={CNN}\n}\n\n\\newglossaryentry{mcmc}\n{\n  name=马尔可夫链蒙特卡罗,\n  description={Markov Chain Monte Carlo},\n  symbol={MCMC},\n  sort={Markov Chain Monte Carlo},\n}\n\n\\newglossaryentry{tempering_transition}\n{\n  name=回火转移,\n  description={tempered transition},\n  sort={tempered transition},\n}\n\n\\newglossaryentry{markov_chain}\n{\n  name=马尔可夫链,\n  description={Markov Chain},\n  sort={Markov Chain},\n}\n\n\\newglossaryentry{harris_chain}\n{\n  name=哈里斯链,\n  description={Harris Chain},\n  sort={Harris Chain},\n}\n\n\\newglossaryentry{minibatch}\n{\n  name=小批量,\n  description={minibatch},\n  sort={minibatch},\n}\n\n\\newglossaryentry{importance_sampling}\n{\n  name=重要采样,\n  description={Importance Sampling},\n  sort={Importance Sampling},\n}\n\n\\newglossaryentry{undirected_model}\n{\n  name=无向模型,\n  description={undirected Model},\n  sort={undirected Model},\n}\n\n\\newglossaryentry{partition_function}\n{\n  name=配分函数,\n  description={Partition Function},\n  sort={Partition Function},\n}\n\n\\newglossaryentry{law_of_large_numbers}\n{\n  name=大数定理,\n  description={Law of large number},\n  sort={Law of large number},\n}\n\n\\newglossaryentry{central_limit_theorem}\n{\n  name=中心极限定理,\n  description={central limit theorem},\n  sort={central limit theorem},\n}\n\n\\newglossaryentry{energy_based_model}\n{\n  name=基于能量的模型,\n  description={Energy-based model},\n  symbol={EBM},\n  sort={Energy-based model},\n}\n\n\\newglossaryentry{tempering}\n{\n  name=回火,\n  description={tempering},\n  sort={tempering},\n}\n\n\\newglossaryentry{biased_importance_sampling}\n{\n  name=有偏重要采样,\n  description={biased importance sampling},\n  sort={biased importance sampling},\n}\n\n\\newglossaryentry{VAE}\n{\n  name=变分自编码器,\n  description={variational auto-encoder},\n  sort={variational auto-encoder},\n  symbol={VAE},\n}\n\n\\newglossaryentry{CV}\n{\n  name=计算机视觉,\n  description={Computer Vision},\n  sort={Computer Vision},\n}\n\n\\newglossaryentry{SR}\n{\n  name=语音识别,\n  description={Speech Recognition},\n  sort={Speech Recognition},\n}\n\n\\newglossaryentry{NLP}\n{\n  name=自然语言处理,\n  description={Natural Language Processing},\n  sort={Natural Language Processing},\n  symbol={NLP}\n}\n\n\\newglossaryentry{RBM}\n{\n  name=受限玻尔兹曼机,\n  description={Restricted Boltzmann Machine},\n  sort={Restricted Boltzmann Machine},\n  symbol={RBM}\n}\n\n\\newglossaryentry{discriminative_RBM}\n{\n  name=判别RBM,\n  description={discriminative RBM},\n  sort={discriminative RBM},\n}\n\n\\newglossaryentry{Boltzmann}\n{\n  name=玻尔兹曼,\n  description={Boltzmann},\n  sort={Boltzmann},\n}\n\n\\newglossaryentry{BM}\n{\n  name=玻尔兹曼机,\n  description={Boltzmann Machine},\n  sort={Boltzmann Machine},\n}\n\n\\newglossaryentry{DBM}\n{\n  name=深度玻尔兹曼机,\n  description={Deep Boltzmann Machine},\n  sort={Deep Boltzmann Machine},\n  symbol={DBM}\n}\n\n\\newglossaryentry{CBM}\n{\n  name=卷积玻尔兹曼机,\n  description={Convolutional Boltzmann Machine},\n  sort={Convolutional Boltzmann Machine},\n  symbol={CBM}\n}\n\n\\newglossaryentry{directed_model}\n{\n  name=有向模型,\n  description={Directed Model},\n  sort={Directed Model},\n}\n\n\\newglossaryentry{ancestral_sampling}\n{\n  name=原始采样,\n  description={Ancestral Sampling},\n  sort={Ancestral Sampling},\n}\n\n\\newglossaryentry{stochastic_matrix}\n{\n  name=随机矩阵,\n  description={Stochastic Matrix},\n  sort={Stochastic Matrix},\n}\n\n\\newglossaryentry{stationary_distribution}\n{\n  name=平稳分布,\n  description={Stationary Distribution},\n  sort={Stationary Distribution},\n}\n\n\\newglossaryentry{equilibrium_distribution}\n{\n  name=均衡分布,\n  description={Equilibrium Distribution},\n  sort={Equilibrium Distribution},\n}\n\n\\newglossaryentry{index}\n{\n  name=索引,\n  description={index of matrix},\n  sort={index of matrix},\n}\n\n\\newglossaryentry{burn_in}\n{\n  name=磨合,\n  description={Burning-in},\n  sort={Burning-in},\n}\n\n\\newglossaryentry{mixing_time}\n{\n  name=混合时间,\n  description={Mixing Time},\n  sort={Mixing Time},\n}\n\n\\newglossaryentry{mixing}\n{\n  name=混合,\n  description={Mixing},\n  sort={Mixing},\n}\n\n\\newglossaryentry{gibbs_sampling}\n{\n  name=Gibbs采样,\n  description={Gibbs Sampling},\n  sort={Gibbs Sampling},\n}\n\n\\newglossaryentry{block_gibbs_sampling}\n{\n  name=块吉布斯采样,\n  description={block Gibbs Sampling},\n  sort={block Gibbs Sampling},\n}\n\n\\newglossaryentry{gibbs_steps}\n{\n  name=吉布斯步数,\n  description={Gibbs steps},\n  sort={Gibbs steps},\n}\n\n\\newglossaryentry{bagging}\n{\n  name=Bagging,\n  description={bootstrap aggregating},\n  sort={bagging},\n}\n\n\\newglossaryentry{mask}\n{\n  name=掩码,\n  description={mask},\n  sort={mask},\n}\n\n\\newglossaryentry{batch_normalization}\n{\n  name=批标准化,\n  description={batch normalization},\n  sort={batch normalization},\n}\n\n\\newglossaryentry{Batch_normalization}\n{\n  name=批标准化,\n  description={Batch normalization},\n  sort={Batch normalization},\n}\n\n\\newglossaryentry{parameter_sharing}\n{\n  name=参数共享,\n  description={parameter sharing},\n  sort={parameter sharing},\n}\n\n\\newglossaryentry{KL}\n{\n  name=KL散度,\n  description={KL divergence},\n  sort={KL},\n}\n\n\\newglossaryentry{temperature}\n{\n  name=温度,\n  description={temperature},\n  sort={temperature},\n}\n\n\\newglossaryentry{critical_temperatures}\n{\n  name=临界温度,\n  description={critical temperatures},\n  sort={critical temperatures},\n}\n\n\\newglossaryentry{parallel_tempering}\n{\n  name=并行回火,\n  description={parallel tempering},\n  sort={parallel tempering},\n}\n\n\\newglossaryentry{ASR}\n{\n  name=自动语音识别,\n  description={Automatic Speech Recognition},\n  sort={Automatic Speech Recognition},\n  symbol={ASR}\n}\n\n\\newglossaryentry{GP_GPU}\n{\n  name=通用GPU,\n  description={general purpose GPU},\n  sort={general purpose GPU},\n}\n\n\\newglossaryentry{coalesced}\n{\n  name=级联,\n  description={coalesced},\n  sort={coalesced},\n}\n\n\\newglossaryentry{warp}\n{\n  name=warp,\n  description={warp},\n  sort={warp},\n}\n\n\\newglossaryentry{data_parallelism}\n{\n  name=数据并行,\n  description={data parallelism},\n  sort={data parallelism},\n}\n\n\\newglossaryentry{model_parallelism}\n{\n  name=模型并行,\n  description={model parallelism},\n  sort={model parallelism},\n}\n\n\\newglossaryentry{ASGD}\n{\n  name=异步随机梯度下降,\n  description={Asynchoronous Stochastic Gradient Descent},\n  sort={Asynchoronous Stochastic Gradient Descent},\n}\n\n\\newglossaryentry{parameter_server}\n{\n  name=参数服务器,\n  description={parameter server},\n  sort={parameter server},\n}\n\n\\newglossaryentry{model_compression}\n{\n  name=模型压缩,\n  description={model compression},\n  sort={model compression},\n}\n\n\\newglossaryentry{dynamic_structure}\n{\n  name=动态结构,\n  description={dynamic structure},\n  sort={dynamic structure},\n}\n\n\\newglossaryentry{conditional_computation}\n{\n  name=条件计算,\n  description={conditional computation},\n  sort={conditional computation},\n}\n\n\\newglossaryentry{sphering}\n{\n  name=sphering,\n  description={sphering},\n  sort={sphering},\n}\n\n\\newglossaryentry{GCN}\n{\n  name=全局对比度归一化,\n  description={Global contrast normalization},\n  sort={Global contrast normalization},\n  symbol={GCN}\n}\n\n\\newglossaryentry{LCN}\n{\n  name=局部对比度归一化,\n  description={local contrast normalization},\n  symbol={LCN},\n  sort={local contrast normalization},\n}\n\n\\newglossaryentry{HMM}\n{\n  name=隐马尔可夫模型,\n  description={Hidden Markov Model},\n  sort={Hidden Markov Model},\n  symbol={HMM}\n}\n\n\\newglossaryentry{GMM}\n{\n  name=高斯混合模型,\n  description={Gaussian Mixture Model},\n  sort={Gaussian Mixture Model},\n  symbol={GMM}\n}\n\n\\newglossaryentry{transcribe}\n{\n  name=转录,\n  description={transcribe},\n  sort={transcribe},\n}\n\n\\newglossaryentry{PCA}\n{\n  name=主成分分析,\n  description={principal components analysis},\n  sort={principal components analysis},\n  symbol={PCA}\n}\n\n\\newglossaryentry{FA}\n{\n  name=因子分析,\n  description={factor analysis},\n  sort={factor analysis},\n}\n\n\\newglossaryentry{ICA}\n{\n  name=独立成分分析,\n  description={independent component analysis},\n  sort={independent component analysis},\n  symbol={ICA}\n}\n\n\\newglossaryentry{tICA}\n{\n  name=地质ICA,\n  description={topographic ICA},\n  sort={topo independent component analysis},\n}\n\n\\newglossaryentry{sparse_coding}\n{\n  name=稀疏编码,\n  description={sparse coding},\n  sort={sparse coding},\n}\n\n\\newglossaryentry{fixed_point_arithmetic}\n{\n  name=定点运算,\n  description={fixed-point arithmetic},\n  sort={fixed-point arithmetic},\n}\n\n\\newglossaryentry{float_point_arithmetic}\n{\n  name=浮点运算,\n  description={float-point arithmetic},\n  sort={float-point arithmetic},\n}\n\n\\newglossaryentry{GPU}\n{\n  name=图形处理器,\n  description={Graphics Processing Unit},\n  sort={Graphics Processing Unit},\n  symbol={GPU}\n}\n\n\\newglossaryentry{generative_model}\n{\n  name=生成模型,\n  description={generative model},\n  sort={generative model},\n}\n\n\\newglossaryentry{generative_modeling}\n{\n  name=生成式建模,\n  description={generative modeling},\n  sort={generative modeling},\n}\n\n\\newglossaryentry{dataset_augmentation}\n{\n  name=数据集增强,\n  description={dataset augmentation},\n  sort={dataset augmentation},\n}\n\n\\newglossaryentry{whitening}\n{\n  name=白化,\n  description={whitening},\n  sort={whitening},\n}\n\n\\newglossaryentry{DNN}\n{\n  name=深度神经网络,\n  description={DNN},\n  sort={DNN},\n}\n\n\\newglossaryentry{end_to_end}\n{\n  name=端到端的,\n  description={end-to-end},\n  sort={end-to-end},\n}\n\n\\newglossaryentry{structured_probabilistic_models}\n{\n  name=结构化概率模型,\n  description={structured probabilistic model},\n  sort={structured probabilistic model},\n}\n\n\\newglossaryentry{graphical_models}\n{\n  name=图模型,\n  description={graphical model},\n  sort={graphical model},\n}\n\n\\newglossaryentry{directed_graphical_model}\n{\n  name=有向图模型,\n  description={directed graphical model},\n  sort={directed graphical model},\n}\n\n\\newglossaryentry{dependency}\n{\n  name=依赖,\n  description={dependency},\n  sort={dependency},\n}\n\n\\newglossaryentry{bayesian_network}\n{\n  name=贝叶斯网络,\n  description={Bayesian network},\n  sort={Bayesian network},\n}\n\n\\newglossaryentry{model_averaging}\n{\n  name=模型平均,\n  description={model averaging},\n  sort={model averaging},\n}\n\n\\newglossaryentry{boosting}\n{\n  name=Boosting,\n  description={Boosting},\n  sort={Boosting},\n}\n\n\\newglossaryentry{weight_scaling_inference_rule}\n{\n  name=权重比例推断规则,\n  description={weight scaling inference rule},\n  sort={weight scaling inference rule},\n}\n\n\\newglossaryentry{statement}\n{\n  name=声明,\n  description={statement},\n  sort={statement},\n}\n\n\\newglossaryentry{quantum_mechanics}\n{\n  name=量子力学,\n  description={quantum mechanics},\n  sort={quantum mechanics},\n}\n\n\\newglossaryentry{subatomic}\n{\n  name=亚原子,\n  description={subatomic},\n  sort={subatomic},\n}\n\n\\newglossaryentry{fidelity}\n{\n  name=逼真度,\n  description={fidelity},\n  sort={fidelity},\n}\n\n\\newglossaryentry{degree_of_belief}\n{\n  name=信任度,\n  description={degree of belief},\n  sort={degree of belief},\n}\n\n\\newglossaryentry{frequentist_probability}\n{\n  name=频率派概率,\n  description={frequentist probability},\n  sort={frequentist probability},\n}\n\n\\newglossaryentry{subsample}\n{\n  name=子采样,\n  description={subsample},\n  sort={subsample},\n}\n\n\\newglossaryentry{bayesian_probability}\n{\n  name=贝叶斯概率,\n  description={Bayesian probability},\n  sort={Bayesian probability},\n}\n\n\\newglossaryentry{likelihood}\n{\n  name=似然,\n  description={likelihood},\n  sort={likelihood},\n}\n\n\\newglossaryentry{RV}\n{\n  name=随机变量,\n  description={random variable},\n  sort={random variable},\n}\n\n\\newglossaryentry{PD}\n{\n  name=概率分布,\n  description={probability distribution},\n  sort={probability distribution},\n}\n\n\\newglossaryentry{PMF}\n{\n  name=概率质量函数,\n  description={probability mass function},\n  sort={probability mass function},\n  symbol={PMF}\n}\n\n\\newglossaryentry{joint_probability_distribution}\n{\n  name=联合概率分布,\n  description={joint probability distribution},\n  sort={joint probability distribution},\n}\n\n\\newglossaryentry{normalized}\n{\n  name=归一化的,\n  description={normalized},\n  sort={normalized},\n}\n\n\\newglossaryentry{uniform_distribution}\n{\n  name=均匀分布,\n  description={uniform distribution},\n  sort={uniform distribution},\n}\n\n\\newglossaryentry{PDF}\n{\n  name=概率密度函数,\n  description={probability density function},\n  sort={probability density function},\n  symbol={PDF}\n}\n\n\\newglossaryentry{cumulative_function}\n{\n  name=累积函数,\n  description={cumulative function},\n  sort={cumulative function},\n}\n\n\\newglossaryentry{marginal_probability_distribution}\n{\n  name=边缘概率分布,\n  description={marginal probability distribution},\n  sort={marginal probability distribution},\n}\n\n\\newglossaryentry{sum_rule}\n{\n  name=求和法则,\n  description={sum rule},\n  sort={sum rule},\n}\n\n\\newglossaryentry{conditional_probability}\n{\n  name=条件概率,\n  description={conditional probability},\n  sort={conditional probability},\n}\n\n\\newglossaryentry{intervention_query}\n{\n  name=干预查询,\n  description={intervention query},\n  sort={intervention query},\n}\n\n\\newglossaryentry{causal_modeling}\n{\n  name=因果模型,\n  description={causal modeling},\n  sort={causal modeling},\n}\n\n\\newglossaryentry{causal_factor}\n{\n  name=因果因子,\n  description={causal factor},\n  sort={causal factor},\n}\n\n\\newglossaryentry{chain_rule}\n{\n  name=链式法则,\n  description={chain rule},\n  sort={chain rule},\n}\n\n\\newglossaryentry{product_rule}\n{\n  name=乘法法则,\n  description={product rule},\n  sort={product rule},\n}\n\n\\newglossaryentry{independent}\n{\n  name=相互独立的,\n  description={independent},\n  sort={independent},\n}\n\n\\newglossaryentry{conditionally_independent}\n{\n  name=条件独立的,\n  description={conditionally independent},\n  sort={conditionally independent},\n}\n\n\\newglossaryentry{expectation}\n{\n  name=期望,\n  description={expectation},\n  sort={expectation},\n}\n\n\\newglossaryentry{expected_value}\n{\n  name=期望值,\n  description={expected value},\n  sort={expected value},\n}\n\n\\newglossaryentry{example:chap5}\n{\n  name=样本,\n  description={example},\n  sort={example},\n}\n\n\\newglossaryentry{feature}\n{\n  name=特征,\n  description={feature},\n  sort={feature},\n}\n\n\\newglossaryentry{accuracy}\n{\n  name=准确率,\n  description={accuracy},\n  sort={accuracy},\n}\n\n\\newglossaryentry{error_rate}\n{\n  name=错误率,\n  description={error rate},\n  sort={error rate},\n}\n\n\\newglossaryentry{training_set}\n{\n  name=训练集,\n  description={training set},\n  sort={training set},\n}\n\n\\newglossaryentry{explanatory_factor}\n{\n  name=解释因子,\n  description={explanatory factort},\n  sort={explanatory factor},\n}\n\n\\newglossaryentry{underlying}\n{\n  name=潜在,\n  description={underlying},\n  sort={underlying},\n}\n\n\\newglossaryentry{underlying_cause}\n{\n  name=潜在成因,\n  description={underlying cause},\n  sort={underlying cause},\n}\n\n\\newglossaryentry{test_set}\n{\n  name=测试集,\n  description={test set},\n  sort={test set},\n}\n\n\\newglossaryentry{performance_measures}\n{\n  name=性能度量,\n  description={performance measures},\n  sort={performance measures},\n}\n\n\\newglossaryentry{experience}\n{\n  name=经验,\n  description={experience, E},\n  sort={experience, E},\n}\n\n\\newglossaryentry{unsupervised}\n{\n  name=无监督,\n  description={unsupervised},\n  sort={unsupervised},\n}\n\n\\newglossaryentry{supervised}\n{\n  name=监督,\n  description={supervised},\n  sort={supervised},\n}\n\n\\newglossaryentry{semi_supervised}\n{\n  name=半监督,\n  description={semi-supervised},\n  sort={semi-supervised},\n}\n\n\\newglossaryentry{supervised_learning}\n{\n  name=监督学习,\n  description={supervised learning},\n  sort={supervised learning},\n}\n\n\\newglossaryentry{unsupervised_learning}\n{\n  name=无监督学习,\n  description={unsupervised learning},\n  sort={unsupervised learning},\n}\n\n\\newglossaryentry{dataset}\n{\n  name=数据集,\n  description={dataset},\n  sort={dataset},\n}\n\n\\newglossaryentry{setting}\n{\n  name=情景,\n  description={setting},\n  sort={setting},\n}\n\n\\newglossaryentry{data_points}\n{\n  name=数据点,\n  description={data point},\n  sort={data point},\n}\n\n\\newglossaryentry{label}\n{\n  name=标签,\n  description={label},\n  sort={label},\n}\n\n\\newglossaryentry{labeled}\n{\n  name=标注,\n  description={labeled},\n  sort={labeled},\n}\n\n\\newglossaryentry{unlabeled}\n{\n  name=未标注,\n  description={unlabeled},\n  sort={unlabeled},\n}\n\n\\newglossaryentry{target}\n{\n  name=目标,\n  description={target},\n  sort={target},\n}\n\n\\newglossaryentry{reinforcement_learning}\n{\n  name=强化学习,\n  description={reinforcement learning},\n  sort={reinforcement learning},\n}\n\n\\newglossaryentry{design_matrix}\n{\n  name=设计矩阵,\n  description={design matrix},\n  sort={design matrix},\n}\n\n\\newglossaryentry{parameters}\n{\n  name=参数,\n  description={parameter},\n  sort={parameter},\n}\n\n\\newglossaryentry{weights}\n{\n  name=权重,\n  description={weight},\n  sort={weight},\n}\n\n\\newglossaryentry{mean_squared_error}\n{\n  name=均方误差,\n  description={mean squared error},\n  sort={mean squared error},\n  symbol={MSE}\n}\n\n\\newglossaryentry{normal_equations}\n{\n  name=正规方程,\n  description={normal equation},\n  sort={normal equation},\n}\n\n\\newglossaryentry{training_error}\n{\n  name=训练误差,\n  description={training error},\n  sort={training error},\n}\n\n\\newglossaryentry{generalization_error}\n{\n  name=泛化误差,\n  description={generalization error},\n  sort={generalization error},\n}\n\n\\newglossaryentry{test_error}\n{\n  name=测试误差,\n  description={test error},\n  sort={test error},\n}\n\n\\newglossaryentry{hypothesis_space}\n{\n  name=假设空间,\n  description={hypothesis space},\n  sort={hypothesis space},\n}\n\n\\newglossaryentry{capacity}\n{\n  name=容量,\n  description={capacity},\n  sort={capacity},\n}\n\n\\newglossaryentry{representational_capacity}\n{\n  name=表示容量,\n  description={representational capacity},\n  sort={representational capacity},\n}\n\n\\newglossaryentry{effective_capacity}\n{\n  name=有效容量,\n  description={effective capacity},\n  sort={effective capacity},\n}\n\n\\newglossaryentry{linear_threshold_units}\n{\n  name=线性阈值单元,\n  description={linear threshold units},\n  sort={linear threshold units},\n}\n\n\\newglossaryentry{nonparametric}\n{\n  name=非参数,\n  description={non-parametric},\n  sort={non-parametric},\n}\n\n\\newglossaryentry{nearest_neighbor_regression}\n{\n  name=最近邻回归,\n  description={nearest neighbor regression},\n  sort={nearest neighbor regression},\n}\n\n\\newglossaryentry{nearest_neighbor}\n{\n  name=最近邻,\n  description={nearest neighbor},\n  sort={nearest neighbor},\n}\n\n\\newglossaryentry{bayes_error}\n{\n  name=贝叶斯误差,\n  description={Bayes error},\n  sort={Bayes error},\n}\n\n\\newglossaryentry{no_free_lunch_theorem}\n{\n  name=没有免费午餐定理,\n  description={no free lunch theorem},\n  sort={no free lunch theorem},\n}\n\n\\newglossaryentry{validation_set}\n{\n  name=验证集,\n  description={validation set},\n  sort={validation set},\n}\n\n\\newglossaryentry{benchmarks}\n{\n  name=基准,\n  description={bechmark},\n  sort={bechmark},\n}\n\n\\newglossaryentry{baseline}\n{\n  name=基准,\n  description={baseline},\n  sort={beseline}\n}\n\n\\newglossaryentry{point_estimator}\n{\n  name=点估计,\n  description={point estimator},\n  sort={point estimator},\n}\n\n\\newglossaryentry{estimator:chap5}\n{\n  name=估计量,\n  description={estimator},\n  sort={estimator},\n}\n\n\\newglossaryentry{statistics}\n{\n  name=统计量,\n  description={statistics},\n  sort={statistics},\n}\n\n\\newglossaryentry{unbiased}\n{\n  name=无偏,\n  description={unbiased},\n  sort={unbiased},\n}\n\n\\newglossaryentry{biased}\n{\n  name=有偏,\n  description={biased},\n  sort={biased},\n}\n\n\\newglossaryentry{asynchronous}\n{\n  name=异步,\n  description={asynchronous},\n  sort={asynchronous},\n}\n\n\\newglossaryentry{asymptotically_unbiased}\n{\n  name=渐近无偏,\n  description={asymptotically unbiased},\n  sort={asymptotically unbiased},\n}\n\n\\newglossaryentry{sample_mean}\n{\n  name=样本均值, %采样均值？\n  description={sample mean},\n  sort={sample mean},\n}\n\n\\newglossaryentry{sample_variance}\n{\n  name=样本方差, %采样方差？\n  description={sample variance},\n  sort={sample variance},\n}\n\n\\newglossaryentry{unbiased_sample_variance}\n{\n  name=无偏样本方差, %采样方差？\n  description={unbiased sample variance},\n  sort={unbiased sample variance},\n}\n\n\\newglossaryentry{standard_error}\n{\n  name=标准差,\n  description={standard error},\n  symbol={SE},\n  sort={standard error},\n}\n\n\\newglossaryentry{consistency}\n{\n  name=一致性,\n  description={consistency},\n  sort={consistency},\n}\n\n\\newglossaryentry{almost_sure}\n{\n  name=几乎必然,\n  description={almost sure},\n  sort={almost sure},\n}\n\n\\newglossaryentry{almost_sure_convergence}\n{\n  name=几乎必然收敛,\n  description={almost sure convergence},\n  sort={almost sure convergence},\n}\n\n\\newglossaryentry{statistical_efficiency}\n{\n  name=统计效率,\n  description={statistic efficiency},\n  sort={statistic efficiency},\n}\n\n\\newglossaryentry{parametric_case}\n{\n  name=有参情况,\n  description={parametric case},\n  sort={parametric case},\n}\n\n\\newglossaryentry{frequentist_statistics}\n{\n  name=频率派统计,\n  description={frequentist statistics},\n  sort={frequentist statistics},\n}\n\n\\newglossaryentry{bayesian_statistics}\n{\n  name=贝叶斯统计,\n  description={Bayesian statistics},\n  sort={Bayesian statistics},\n}\n\n\\newglossaryentry{prior_probability_distribution}\n{\n  name=先验概率分布,\n  description={prior probability distribution},\n  sort={prior probability distribution},\n}\n\n\\newglossaryentry{maximum_a_posteriori}\n{\n  name=最大后验,\n  description={maximum a posteriori},\n  sort={maximum a posteriori},\n}\n\n\\newglossaryentry{maximum_likelihood_estimation}\n{\n  name=最大似然估计,\n  description={maximum likelihood estimation},\n  sort={maximum likelihood estimation},\n}\n\n\\newglossaryentry{maximum_likelihood}\n{\n  name=最大似然,\n  description={maximum likelihood},\n  sort={maximum likelihood},\n}\n\n\\newglossaryentry{kernel_trick}\n{\n  name=核技巧,\n  description={kernel trick},\n  sort={kernel trick},\n}\n\n\\newglossaryentry{kernel}\n{\n  name=核函数,\n  description={kernel function},\n  sort={kernel function},\n}\n\n\\newglossaryentry{gaussian_kernel}\n{\n  name=高斯核,\n  description={Gaussian kernel},\n  sort={Gaussian kernel},\n}\n\n\\newglossaryentry{kernel_machines}\n{\n  name=核机器,\n  description={kernel machine},\n  sort={kernel machine},\n}\n\n\\newglossaryentry{kernel_methods}\n{\n  name=核方法,\n  description={kernel method},\n  sort={kernel method},\n}\n\n\\newglossaryentry{support_vectors}\n{\n  name=支持向量,\n  description={support vector},\n  sort={support vector},\n}\n\n\\newglossaryentry{SVM}\n{\n  name=支持向量机,\n  description={support vector machine},\n  symbol={SVM}\n}\n\n\\newglossaryentry{phoneme}\n{\n  name=音素,\n  description={phoneme},\n  sort={phoneme},\n}\n\n\\newglossaryentry{acoustic}\n{\n  name=声学,\n  description={acoustic},\n  sort={acoustic},\n}\n\n\\newglossaryentry{phonetic}\n{\n  name=语音,\n  description={phonetic},\n  sort={phonetic},\n}\n\n\\newglossaryentry{mixture_of_experts}\n{\n  name=专家混合体,\n  description={mixture of experts},\n  sort={mixture of experts},\n}\n\n\\newglossaryentry{gauss_mixture}\n{\n  name=高斯混合体,\n  description={Gaussian mixtures},\n  sort={Gaussian mixtures},\n}\n\n\\newglossaryentry{hard_mixture_of_experts}\n{\n  name=硬专家混合体, \n  description={hard mixture of experts},\n  sort={hard mixture of experts},\n}\n\n\\newglossaryentry{gater}\n{\n  name=选通器,\n  description={gater},\n  sort={gater},\n}\n\n\\newglossaryentry{expert_network}\n{\n  name=专家网络,\n  description={expert network},\n  sort={expert network},\n}\n\n\\newglossaryentry{attention_mechanism}\n{\n  name=注意力机制,\n  description={attention mechanism},\n  sort={attention mechanism},\n}\n\n\\newglossaryentry{fast_dropout}\n{\n  name=快速Dropout,\n  description={fast dropout},\n  sort={fast dropout},\n}\n\n\\newglossaryentry{dropout_boosting}\n{\n  name=Dropout Boosting,\n  description={Dropout Boosting},\n  sort={dropout boosting},\n}\n\n\\newglossaryentry{adversarial_example}\n{\n  name=对抗样本,\n  description={adversarial example},\n  sort={adversarial example},\n}\n\n\\newglossaryentry{adversarial}\n{\n  name=对抗,\n  description={adversarial},\n  sort={adversarial},\n}\n\n\\newglossaryentry{virtual_adversarial_example}\n{\n  name=虚拟对抗样本,\n  description={virtual adversarial example},\n  sort={virtual adversarial example},\n}\n\n\\newglossaryentry{adversarial_training}\n{\n  name=对抗训练,\n  description={adversarial training},\n  sort={adversarial training},\n}\n\n\\newglossaryentry{virtual_adversarial_training}\n{\n  name=虚拟对抗训练,\n  description={virtual adversarial training},\n  sort={virtual adversarial training}\n}\n\n\\newglossaryentry{tangent_distance}\n{\n  name=切面距离,\n  description={tangent distance},\n  sort={tangent distance},\n}\n\n\\newglossaryentry{tangent_prop}\n{\n  name=正切传播,\n  description={tangent prop},\n  sort={tangent prop},\n}\n\n\\newglossaryentry{tangent_propagation}\n{\n  name=正切传播,\n  description={tangent propagation}\n}\n\n\\newglossaryentry{double_backprop}\n{\n  name=双反向传播,\n  description={double backprop},\n  sort={double backprop},\n}\n\n\\newglossaryentry{EM}\n{\n  name=期望最大化,\n  description={expectation maximization},\n  sort={expectation maximization},\n  symbol={EM}\n}\n\n\\newglossaryentry{mean_field}\n{\n  name=均值场,\n  description={mean-field},\n  sort={mean-field},\n}\n\n\\newglossaryentry{ELBO}\n{\n  name=证据下界,\n  description={evidence lower bound},\n  sort={evidence lower bound},\n  symbol={ELBO}\n}\n\n\\newglossaryentry{variational_free_energy}\n{\n  name=变分自由能,\n  description={variational free energy},\n  sort={variational free energy},\n}\n\n\\newglossaryentry{structured_variational_inference}\n{\n  name=结构化变分推断,\n  description={structured variational inference},\n  sort={structured variational inference},\n}\n\n\\newglossaryentry{variational_inference}\n{\n  name=变分推断,\n  description={variational inference},\n  sort={variational inference},\n}\n\n\\newglossaryentry{binary_sparse_coding}\n{\n  name=二值稀疏编码,\n  description={binary sparse coding},\n  sort={binary sparse coding},\n}\n\n\\newglossaryentry{feedforward_network}\n{\n  name=前馈网络,\n  description={feedforward network},\n  sort={feedforward network},\n}\n\n\\newglossaryentry{transition}\n{\n  name=转移,\n  description={transition},\n  sort={transition},\n}\n\n\\newglossaryentry{reconstruction}\n{\n  name=重构,\n  description={reconstruction},\n  sort={reconstruction},\n}\n\n\\newglossaryentry{GSN}\n{\n  name=生成随机网络,\n  description={generative stochastic network},\n  sort={generative stochastic network},\n  symbol={GSN}\n}\n\n\\newglossaryentry{score_matching}\n{\n  name=得分匹配,\n  description={score matching},\n  sort={score matching},\n}\n\n\\newglossaryentry{factorial}\n{\n  name=因子,\n  description={factorial},\n  sort={factorial},\n}\n\n\\newglossaryentry{factorized}\n{\n  name=分解的,\n  description={factorized},\n  sort={factorized},\n}\n\n\\newglossaryentry{meanfield}\n{\n  name=均匀场,\n  description={meanfield},\n  sort={meanfield},\n}\n\n\\newglossaryentry{MLE}\n{\n  name=最大似然估计,\n  description={maximum likelihood estimation},\n  sort={maximum likelihood estimation},\n}\n\n\\newglossaryentry{PPCA}\n{\n  name=概率PCA,\n  description={probabilistic PCA},\n  sort={probabilistic PCA},\n  symbol={PPCA}\n}\n\n\\newglossaryentry{SGA}\n{\n  name=随机梯度上升,\n  description={Stochastic Gradient Ascent},\n  sort={Stochastic Gradient Ascent},\n}\n\n\\newglossaryentry{clique}\n{\n  name=团,\n  description={clique},\n  sort={clique},\n}\n\n\\newglossaryentry{dirac_distribution}\n{\n  name=Dirac分布,\n  description={dirac distribution},\n  sort={dirac distribution},\n}\n\n\\newglossaryentry{fixed_point_equation}\n{\n  name=不动点方程,\n  description={fixed point equation},\n  sort={fixed point equation},\n}\n\n\\newglossaryentry{calculus_of_variations}\n{\n  name=变分法,\n  description={calculus of variations},\n  sort={calculus of variations},\n}\n\n\\newglossaryentry{wake_sleep}\n{\n  name=醒眠,\n  description={wake sleep},\n  sort={wake sleep},\n}\n\n\\newglossaryentry{BN}\n{\n  name=信念网络,\n  description={belief network},\n  sort={belief network},\n}\n\n\\newglossaryentry{MRF}\n{\n  name=马尔可夫随机场,\n  description={Markov random field},\n  sort={Markov random field},\n  symbol={MRF}\n}\n\n\\newglossaryentry{markov_network}\n{\n  name=马尔可夫网络,\n  description={Markov network},\n  sort={Markov network},\n}\n\n\\newglossaryentry{log_linear_model}\n{\n  name=对数线性模型,\n  description={log-linear model},\n  sort={log-linear model},\n}\n\n\\newglossaryentry{product_of_expert}\n{\n  name=专家之积,\n  description={product of expert},\n  sort={product of expert},\n}\n\n\\newglossaryentry{free_energy}\n{\n  name=自由能,\n  description={free energy},\n  sort={free energy},\n}\n\n\\newglossaryentry{harmony}\n{\n  name=harmony,\n  description={harmony},\n  sort={harmony},\n}\n\n\\newglossaryentry{separation}\n{\n  name=分离,\n  description={separation},\n  sort={separation},\n}\n\n\\newglossaryentry{separate}\n{\n  name=分离的,\n  description={separate},\n  sort={separate},\n}\n\n\\newglossaryentry{dseparation}\n{\n  name=d-分离,\n  description={d-separation},\n  sort={d-separation},\n}\n\n\\newglossaryentry{local_conditional_probability_distribution}\n{\n  name=局部条件概率分布,\n  description={local conditional probability distribution},\n  sort={local conditional probability distribution},\n}\n\n\\newglossaryentry{conditional_probability_distribution}\n{\n  name=条件概率分布,\n  description={conditional probability distribution}\n}\n\n\\newglossaryentry{boltzmann_distribution}\n{\n  name=玻尔兹曼分布,\n  description={Boltzmann distribution},\n  sort={Boltzmann distribution},\n}\n\n\\newglossaryentry{gibbs_distribution}\n{\n  name=吉布斯分布,\n  description={Gibbs distribution},\n  sort={Gibbs distribution},\n}\n\n\\newglossaryentry{energy_function}\n{\n  name=能量函数,\n  description={energy function},\n  sort={energy function},\n}\n\n\\newglossaryentry{immorality}\n{\n  name=不道德,\n  description={immorality},\n  sort={immorality},\n}\n\n\\newglossaryentry{moralization}\n{\n  name=道德化,\n  description={moralization},\n  sort={moralization},\n}\n\n\\newglossaryentry{moralized_graph}\n{\n  name=道德图,\n  description={moralized graph},\n  sort={moralized graph},\n}\n\n\\newglossaryentry{standard_deviation}\n{\n  name=标准差,\n  description={standard deviation},\n  symbol={SD},\n  sort={standard deviation},\n}\n\n\\newglossaryentry{correlation}\n{\n  name=相关系数,\n  description={correlation},\n  sort={correlation},\n}\n\n\\newglossaryentry{standard_normal_distribution}\n{\n  name=标准正态分布,\n  description={standard normal distribution},\n  sort={standard normal distribution},\n}\n\n\\newglossaryentry{covariance_matrix}\n{\n  name=协方差矩阵,\n  description={covariance matrix},\n  sort={covariance matrix},\n}\n\n\\newglossaryentry{bernoulli_distribution}\n{\n  name=Bernoulli分布,\n  description={Bernoulli distribution},\n  sort={Bernoulli distribution},\n}\n\n\\newglossaryentry{bernoulli_output_distribution}\n{\n  name=Bernoulli输出分布,\n  description={Bernoulli output distribution},\n  sort={Bernoulli output distribution},\n}\n\n\\newglossaryentry{multinoulli_distribution}\n{\n  name=Multinoulli分布,\n  description={multinoulli distribution},\n  sort={multinoulli distribution},\n}\n\n\\newglossaryentry{multinoulli_output_distribution}\n{\n  name=Multinoulli输出分布,\n  description={multinoulli output distribution},\n  sort={multinoulli output distribution},\n}\n\n\\newglossaryentry{categorical_distribution}\n{\n  name=范畴分布,\n  description={categorical distribution},\n  sort={categorical distribution},\n}\n\n\\newglossaryentry{multinomial_distribution}\n{\n  name=多项式分布,\n  description={multinomial distribution},\n  sort={multinomial distribution},\n}\n\n\\newglossaryentry{normal_distribution}\n{\n  name=正态分布,\n  description={normal distribution},\n  sort={normal distribution},\n}\n\n\\newglossaryentry{gaussian_distribution}\n{\n  name=高斯分布,\n  description={Gaussian distribution},\n  sort={Gaussian distribution},\n}\n\n\\newglossaryentry{precision}\n{\n  name=精度,\n  description={precision},\n  sort={precision},\n}\n\n\\newglossaryentry{multivariate_normal_distribution}\n{\n  name=多维正态分布,\n  description={multivariate normal distribution},\n  sort={multivariate normal distribution},\n}\n\n\\newglossaryentry{precision_matrix}\n{\n  name=精度矩阵,\n  description={precision matrix},\n  sort={precision matrix},\n}\n\n\\newglossaryentry{isotropic}\n{\n  name=各向同性,\n  description={isotropic},\n  sort={isotropic},\n}\n\n\\newglossaryentry{exponential_distribution}\n{\n  name=指数分布,\n  description={exponential distribution},\n  sort={exponential distribution},\n}\n\n\\newglossaryentry{indicator_function}\n{\n  name=指示函数,\n  description={indicator function},\n  sort={indicator function},\n}\n\n\\newglossaryentry{laplace_distribution}\n{\n  name=Laplace分布,\n  description={Laplace distribution},\n  sort={Laplace distribution},\n}\n\n\\newglossaryentry{dirac_delta_function}\n{\n  name=Dirac delta函数,\n  description={Dirac delta function},\n  sort={Dirac delta function},\n}\n\n\\newglossaryentry{generalized_function}\n{\n  name=广义函数,\n  description={generalized function},\n  sort={generalized function},\n}\n\n\\newglossaryentry{empirical_distribution}\n{\n  name=经验分布,\n  description={empirical distribution},\n  sort={empirical distribution},\n}\n\n\\newglossaryentry{empirical_frequency}\n{\n  name=经验频率,\n  description={empirical frequency},\n  sort={empirical frequency},\n}\n\n\\newglossaryentry{mixture_distribution}\n{\n  name=混合分布,\n  description={mixture distribution},\n  sort={mixture distribution},\n}\n\n\\newglossaryentry{latent_variable}\n{\n  name=潜变量,\n  description={latent variable},\n  sort={latent variable},\n}\n\n\\newglossaryentry{hidden_variable}\n{\n  name=隐藏变量,\n  description={hidden variable},\n  sort={hidden variable},\n}\n\n\\newglossaryentry{prior_probability}\n{\n  name=先验概率,\n  description={prior probability},\n  sort={prior probability},\n}\n\n\\newglossaryentry{posterior_probability}\n{\n  name=后验概率,\n  description={posterior probability},\n  sort={posterior probability},\n}\n\n\\newglossaryentry{universal_approximator}\n{\n  name=万能近似器,\n  description={universal approximator},\n  sort={universal approximator},\n}\n\n\\newglossaryentry{universal_function_approximator}\n{\n  name=万能函数近似器,\n  description={universal function approximator},\n  sort={universal function approximator},\n}\n\n\\newglossaryentry{logistic_sigmoid}\n{\n  name=logistic sigmoid,\n  description={logistic sigmoid},\n  sort={logistic sigmoid},\n}\n\n\\newglossaryentry{sigmoid}\n{\n  name=sigmoid,\n  description={sigmoid},\n  sort={sigmoid},\n}\n\n\\newglossaryentry{saturate}\n{\n  name=饱和,\n  description={saturate},\n  sort={saturate},\n}\n\n\\newglossaryentry{softplus_function}\n{\n  name=softplus函数,\n  description={softplus function},\n  sort={softplus function},\n}\n\n\\newglossaryentry{logit}\n{\n  name=分对数,\n  description={logit},\n  sort={logit},\n}\n\n\\newglossaryentry{positive_part_function}\n{\n  name=正部函数,\n  description={positive part function},\n  sort={positive part function},\n}\n\n\\newglossaryentry{negative part function}\n{\n  name=负部函数,\n  description={negative part function},\n  sort={negative part function},\n}\n\n\\newglossaryentry{bayes_rule}\n{\n  name=贝叶斯规则,\n  description={Bayes' rule},\n  sort={Bayes' rule},\n}\n\n\\newglossaryentry{measure_theory}\n{\n  name=测度论,\n  description={measure theory},\n  sort={measure theory},\n}\n\n\\newglossaryentry{measure_zero}\n{\n  name=零测度,\n  description={measure zero},\n  sort={measure zero},\n}\n\n\\newglossaryentry{almost_everywhere}\n{\n  name=几乎处处,\n  description={almost everywhere},\n  sort={almost everywhere},\n}\n\n\\newglossaryentry{jacobian_matrix}\n{\n  name=Jacobian矩阵,\n  description={Jacobian matrix},\n  sort={Jacobian matrix},\n}\n\n\\newglossaryentry{self_information}\n{\n  name=自信息,\n  description={self-information},\n  sort={self-information},\n}\n\n\\newglossaryentry{nats}\n{\n  name=奈特,\n  description={nats},\n  sort={nats},\n}\n\n\\newglossaryentry{bits}\n{\n  name=比特,\n  description={bit},\n  sort={bit},\n}\n\n\\newglossaryentry{shannons}\n{\n  name=香农,\n  description={shannons},\n  sort={shannons},\n}\n\n\\newglossaryentry{Shannon_entropy}\n{\n  name=香农熵,\n  description={Shannon entropy},\n  sort={Shannon entropy},\n}\n\n\\newglossaryentry{differential_entropy}\n{\n  name=微分熵,\n  description={differential entropy},\n  sort={differential entropy},\n}\n\n\\newglossaryentry{differential_equation}\n{\n  name=微分方程,\n  description={differential equation},\n  sort={differential equation},\n}\n\n\\newglossaryentry{KL_divergence}\n{\n  name=KL散度,\n  description={Kullback-Leibler (KL) divergence},\n  sort={Kullback-Leibler (KL) divergence},\n}\n\n\\newglossaryentry{cross_entropy}\n{\n  name=交叉熵,\n  description={cross-entropy},\n  sort={cross-entropy},\n}\n\n\\newglossaryentry{entropy}\n{\n  name=熵,\n  description={entropy},\n  sort={entropy},\n}\n\n\\newglossaryentry{factorization}\n{\n  name=分解,\n  description={factorization},\n  sort={factorization},\n}\n\n\\newglossaryentry{structured_probabilistic_model}\n{\n  name=结构化概率模型,\n  description={structured probabilistic model},\n  sort={structured probabilistic model},\n}\n\n\\newglossaryentry{graphical_model}\n{\n  name=图模型,\n  description={graphical model},\n  sort={graphical model},\n}\n\n\\newglossaryentry{backoff}\n{\n  name=回退,\n  description={back-off},\n  sort={back-off},\n}\n\n\\newglossaryentry{directed}\n{\n  name=有向,\n  description={directed},\n  sort={directed},\n}\n\n\\newglossaryentry{undirected}\n{\n  name=无向,\n  description={undirected},\n  sort={undirected},\n}\n\n\\newglossaryentry{undirected_graphical_model}\n{\n  name=无向图模型,\n  description={undirected graphical model}\n}\n\n\\newglossaryentry{proportional}\n{\n  name=成比例,\n  description={proportional},\n  sort={proportional},\n}\n\n\\newglossaryentry{description}\n{\n  name=描述,\n  description={description},\n  sort={description},\n}\n\n\\newglossaryentry{decision_tree}\n{\n  name=决策树,\n  description={decision tree},\n  sort={decision tree},\n}\n\n\\newglossaryentry{factor_graph}\n{\n  name=因子图,\n  description={factor graph},\n  sort={factor graph},\n}\n\n\\newglossaryentry{structure_learning}\n{\n  name=结构学习,\n  description={structure learning},\n  sort={structure learning},\n}\n\n\\newglossaryentry{loopy_belief_propagation}\n{\n  name=环状信念传播,\n  description={loopy belief propagation},\n  sort={loopy belief propagation},\n}\n\n\\newglossaryentry{harmonium}\n{\n  name=簧风琴,\n  description={harmonium},\n  sort={harmonium},\n}\n\n\\newglossaryentry{convolutional_network}\n{\n  name=卷积网络,\n  description={convolutional network},\n  sort={convolutional network},\n}\n\n\\newglossaryentry{convolutional_net}\n{\n  name=卷积网络,\n  description={convolutional net},\n  sort={convolutional net},\n}\n\n\\newglossaryentry{main_diagonal}\n{\n  name=主对角线,\n  description={main diagonal},\n  sort={main diagonal},\n}\n\n\\newglossaryentry{transpose}\n{\n  name=转置,\n  description={transpose},\n  sort={transpose},\n}\n\n\\newglossaryentry{broadcasting}\n{\n  name=广播,\n  description={broadcasting},\n  sort={broadcasting},\n}\n\n\\newglossaryentry{matrix_product}\n{\n  name=矩阵乘积,\n  description={matrix product},\n  sort={matrix product},\n}\n\n\\newglossaryentry{adagrad}\n{\n  name=AdaGrad,\n  description={AdaGrad},\n  sort={AdaGrad},\n}\n\n\\newglossaryentry{element_wise_product}\n{\n  name=元素对应乘积,\n  description={element-wise product},\n  sort={element-wise product},\n}\n\n\\newglossaryentry{hadamard_product}\n{\n  name=Hadamard乘积,\n  description={Hadamard product},\n  sort={Hadamard product},\n}\n\n\\newglossaryentry{clique_potential}\n{\n  name=团势能,\n  description={clique potential},\n  sort={clique potential},\n}\n\n\\newglossaryentry{factor}\n{\n  name=因子,\n  description={factor},\n  sort={factor},\n}\n\n\\newglossaryentry{unnormalized_probability_function}\n{\n  name=未归一化概率函数,\n  description={unnormalized probability function},\n  sort={unnormalized probability function},\n}\n\n\\newglossaryentry{recurrent_network}\n{\n  name=循环网络,\n  description={recurrent network},\n  sort={recurrent network},\n}\n\n\\newglossaryentry{vanish_explode_gradient}\n{\n  name=梯度消失与爆炸问题,\n  description={vanishing and exploding gradient problem},\n  sort={vanishing and exploding gradient problem },\n}\n\n\\newglossaryentry{vanish_gradient}\n{\n  name=梯度消失,\n  description={vanishing gradient},\n  sort={vanishing gradient},\n}\n\n\\newglossaryentry{explode_gradient}\n{\n  name=梯度爆炸,\n  description={exploding gradient},\n  sort={exploding gradient},\n}\n\n\\newglossaryentry{computational_graph}\n{\n  name=计算图,\n  description={computational graph},\n  sort={computational graph},\n}\n\n\\newglossaryentry{unfolding}\n{\n  name=展开,\n  description={unfolding},\n  sort={unfolding},\n}\n\n\\newglossaryentry{invert}\n{\n  name=求逆,\n  description={invert},\n  sort={invert},\n}\n\n\\newglossaryentry{time_step}\n{\n  name=时间步,\n  description={time step},\n  sort={time step},\n}\n\n\\newglossaryentry{n_gram}\n{\n  name=$n$-gram,\n  description={n-gram},\n  sort={n-gram},\n}\n\n\\newglossaryentry{curse_of_dimensionality}\n{\n  name=维数灾难,\n  description={curse of dimensionality},\n  sort={curse of dimensionality},\n}\n\n\\newglossaryentry{smoothness_prior}\n{\n  name=平滑先验,\n  description={smoothness prior},\n  sort={smoothness prior},\n}\n\n\\newglossaryentry{local_constancy_prior}\n{\n  name=局部不变性先验,\n  description={local constancy prior},\n  sort={local constancy prior},\n}\n\n\\newglossaryentry{local_kernel}\n{\n  name=局部核,\n  description={local kernel},\n  sort={local kernel},\n}\n\n\\newglossaryentry{manifold}\n{\n  name=流形,\n  description={manifold},\n  sort={manifold},\n}\n\n\\newglossaryentry{manifold_tangent_classifier}\n{\n  name=流形正切分类器,\n  description={manifold tangent classifier}\n}\n\n\\newglossaryentry{manifold_learning}\n{\n  name=流形学习,\n  description={manifold learning},\n  sort={manifold learning},\n}\n\n\\newglossaryentry{manifold_hypothesis}\n{\n  name=流形假设,\n  description={manifold hypothesis},\n  sort={manifold hypothesis},\n}\n\n\\newglossaryentry{loop}\n{\n  name=环,\n  description={loop},\n  sort={loop},\n}\n\n\\newglossaryentry{chord}\n{\n  name=弦,\n  description={chord},\n  sort={chord},\n}\n\n\\newglossaryentry{chordal_graph}\n{\n  name=弦图,\n  description={chordal graph},\n  sort={chordal graph},\n}\n\n\\newglossaryentry{triangulated_graph}\n{\n  name=三角形化图,\n  description={triangulated graph},\n  sort={triangulated graph},\n}\n\n\\newglossaryentry{triangulate}\n{\n  name=三角形化,\n  description={triangulate},\n  sort={triangulate},\n}\n\n\\newglossaryentry{risk}\n{\n  name=风险,\n  description={risk},\n  sort={risk},\n}\n\n\\newglossaryentry{empirical_risk}\n{\n  name=经验风险,\n  description={empirical risk},\n  sort={empirical risk},\n}\n\n\\newglossaryentry{empirical_risk_minimization}\n{\n  name=经验风险最小化,\n  description={empirical risk minimization},\n  sort={empirical risk minimization},\n}\n\n\\newglossaryentry{surrogate_loss_function}\n{\n  name=代理损失函数,\n  description={surrogate loss function},\n  sort={surrogate loss function},\n}\n\n\\newglossaryentry{batch}\n{\n  name=批量,\n  description={batch},\n  sort={batch},\n}\n\n\\newglossaryentry{deterministic}\n{\n  name=确定性,\n  description={deterministic},\n  sort={deterministic},\n}\n\n\\newglossaryentry{stochastic}\n{\n  name=随机,\n  description={stochastic},\n  sort={stochastic},\n}\n\n\\newglossaryentry{online}\n{\n  name=在线,\n  description={online},\n  sort={online},\n}\n\n\\newglossaryentry{minibatch_stochastic}\n{\n  name=小批量随机, %小批量\n  description={minibatch stochastic},\n  sort={minibatch stochastic},\n}\n\n\\newglossaryentry{stream}\n{\n  name=流,\n  description={stream},\n  sort={stream},\n}\n\n\\newglossaryentry{model_identifiability}\n{\n  name=模型可辨识性,\n  description={model identifiability},\n  sort={model identifiability},\n}\n\n\\newglossaryentry{weight_space_symmetry}\n{\n  name=权重空间对称性,\n  description={weight space symmetry},\n  sort={weight space symmetry},\n}\n\n\\newglossaryentry{saddle_free_newton_method}\n{\n  name=无鞍牛顿法,\n  description={saddle-free Newton method},\n  sort={saddle-free Newton method},\n}\n\n\\newglossaryentry{gradient_clipping} \n{\n  name=梯度截断,\n  description={gradient clipping},\n  sort={gradient clipping},\n}\n\n\\newglossaryentry{power_method}\n{\n  name=幂方法,\n  description={power method},\n  sort={power method},\n}\n\n\\newglossaryentry{linear_factor}\n{\n  name=线性因子模型,\n  description={linear factor model},\n  sort={linear factor model},\n}\n\n\\newglossaryentry{forward_propagation}\n{\n  name=前向传播,\n  description={forward propagation},\n  sort={forward propagation},\n}\n\n\\newglossaryentry{backward_propagation}\n{\n  name=反向传播,\n  description={backward propagation},\n  sort={backward propagation},\n}\n\n\\newglossaryentry{unfolded_graph}\n{\n  name=展开图,\n  description={unfolded graph},\n  sort={unfolded graph},\n}\n\n\\newglossaryentry{BPTT}\n{\n  name=通过时间反向传播,\n  description={back-propagation through time},\n  sort={back-propagation through time},\n  symbol={BPTT}\n}\n\n\\newglossaryentry{teacher_forcing}\n{\n  name=导师驱动过程,\n  description={teacher forcing},\n  sort={teacher forcing},\n}\n\n\\newglossaryentry{stationary}\n{\n  name=平稳的,\n  description={stationary},\n  sort={stationary},\n}\n\n\\newglossaryentry{deep_feedforward_network}\n{\n  name=深度前馈网络,\n  description={deep feedforward network},\n  sort={deep feedforward network},\n}\n\n\\newglossaryentry{feedforward_neural_network}\n{\n  name=前馈神经网络,\n  description={feedforward neural network},\n  sort={feedforward neural network},\n}\n\n\\newglossaryentry{feedforward}\n{\n  name=前向,\n  description={feedforward},\n  sort={feedforward},\n}\n\n\\newglossaryentry{feedback}\n{\n  name=反馈,\n  description={feedback},\n  sort={feedback},\n}\n\n\\newglossaryentry{network}\n{\n  name=网络,\n  description={network},\n  sort={network},\n}\n\n\\newglossaryentry{first_layer}\n{\n  name=第一层,\n  description={first layer},\n  sort={first layer},\n}\n\n\\newglossaryentry{second_layer}\n{\n  name=第二层,\n  description={second layer},\n  sort={second layer},\n}\n\n\\newglossaryentry{depth}\n{\n  name=深度,\n  description={depth},\n  sort={depth},\n}\n\n\\newglossaryentry{output_layer}\n{\n  name=输出层,\n  description={output layer},\n  sort={output layer},\n}\n\n\\newglossaryentry{hidden_layer}\n{\n  name=隐藏层,\n  description={hidden layer},\n  sort={hidden layer},\n}\n\n\\newglossaryentry{width}\n{\n  name=宽度,\n  description={width},\n  sort={width},\n}\n\n\\newglossaryentry{unit}\n{\n  name=单元,\n  description={unit},\n  sort={unit},\n}\n\n\\newglossaryentry{activation_function}\n{\n  name=激活函数,\n  description={activation function},\n  sort={activation function},\n}\n\n\\newglossaryentry{dbd}\n{\n  name=delta-bar-delta,\n  description={delta-bar-delta},\n  sort={delta-bar-delta},\n}\n\n\\newglossaryentry{gaussian_output_distribution}\n{\n  name=高斯输出分布,\n  description={Gaussian output distribution},\n  sort={Gaussian output distribution},\n}\n\n\\newglossaryentry{back_propagation}\n{\n  name=反向传播,\n  description={back propagation},\n  sort={back propagation},\n}\n\n\\newglossaryentry{back_propagate}\n{\n  name=反向传播,\n  description={back propagate},\n  sort={back propagate},\n}\n\n\\newglossaryentry{BP}\n{\n  name=反向传播,\n  description={backprop},\n  sort={backprop},\n  symbol={BP}\n}\n\n\\newglossaryentry{functional}\n{\n  name=泛函,\n  description={functional},\n  sort={functional},\n}\n\n\\newglossaryentry{mean_absolute_error}\n{\n  name=平均绝对误差,\n  description={mean absolute error},\n  sort={mean absolute error},\n}\n\n\\newglossaryentry{winner_take_all}\n{\n  name=赢者通吃,\n  description={winner-take-all},\n  sort={winner-take-all},\n}\n\n\\newglossaryentry{heteroscedastic}\n{\n  name=异方差,\n  description={heteroscedastic},\n  sort={heteroscedastic},\n}\n\n\\newglossaryentry{mixture_density_network}\n{\n  name=混合密度网络,\n  description={mixture density network},\n  sort={mixture density network},\n}\n\n\\newglossaryentry{clip_gradients}\n{\n  name=梯度截断,\n  description={clip gradient},\n  sort={clip gradient},\n}\n\n\\newglossaryentry{absolute_value_rectification}\n{\n  name=绝对值整流,\n  description={absolute value rectification},\n  sort={absolute value rectification},\n}\n\n\\newglossaryentry{leaky_ReLU}   %有没有更好的翻译\n{\n  name=渗漏整流线性单元,\n  description={Leaky ReLU},\n  sort={Leaky ReLU},\n  symbol={Leaky ReLU}\n}\n\n\\newglossaryentry{PReLU}\n{\n  name=参数化整流线性单元,\n  description={parametric ReLU},\n  sort={parametric ReLU},\n  symbol={PReLU}\n}\n\n\\newglossaryentry{maxout_unit}\n{\n  name=maxout单元,\n  description={maxout unit},\n  sort={maxout unit},\n}\n\n\\newglossaryentry{maxout}\n{\n  name=maxout,\n  description={maxout},\n  symbol={maxout}\n}\n\n\\newglossaryentry{catastrophic_forgetting}\n{\n  name=灾难遗忘,\n  description={catastrophic forgetting},\n  sort={catastrophic forgetting},\n}\n\n\\newglossaryentry{RBF}\n{\n  name=径向基函数,\n  description={radial basis function},\n  sort={radial basis function},\n  symbol={RBF}\n}\n\n\\newglossaryentry{softplus}\n{\n  name=softplus,\n  description={softplus},\n  sort={softplus},\n}\n\n\\newglossaryentry{hard_tanh}\n{\n  name=硬双曲正切函数,\n  description={hard tanh},\n  sort={hard tanh},\n}\n\n\\newglossaryentry{architecture}\n{\n  name=架构,\n  description={architecture},\n  sort={architecture},\n}\n\n\\newglossaryentry{universal_approximation_theorem}\n{\n  name=万能近似定理, %万能逼近定理?\n  description={universal approximation theorem},\n  sort={universal approximation theorem},\n}\n\n\\newglossaryentry{operation}\n{\n  name=操作,\n  description={operation},\n  sort={operation},\n}\n\n\\newglossaryentry{symbol}\n{\n  name=符号,\n  description={symbol},\n  sort={symbol},\n}\n\n\\newglossaryentry{numeric_value}\n{\n  name=数值,\n  description={numeric value},\n  sort={numeric value},\n}\n\n\\newglossaryentry{dynamic_programming}\n{\n  name=动态规划,\n  description={dynamic programming},\n  sort={dynamic programming},\n}\n\n\\newglossaryentry{automatic_differentiation}\n{\n  name=自动微分,\n  description={automatic differentiation},\n  sort={automatic differentiation},\n}\n\n\\newglossaryentry{reverse_mode_accumulation}\n{\n  name=反向模式累加,\n  description={reverse mode accumulation},\n  sort={reverse mode accumulation},\n}\n\n\\newglossaryentry{forward_mode_accumulation}\n{\n  name=前向模式累加,\n  description={forward mode accumulation},\n  sort={forward mode accumulation},\n}\n\n\\newglossaryentry{Krylov_methods}\n{\n  name=Krylov方法,\n  description={Krylov method},\n  sort={Krylov method},\n}\n\n\\newglossaryentry{parallel_distributed_processing}\n{\n  name=并行分布式处理,\n  description={Parallel Distributed Processing},\n  sort={Parallel Distributed Processing},\n}\n\n\\newglossaryentry{sparse_activation}\n{\n  name=稀疏激活,\n  description={sparse activation},\n  sort={sparse activation},\n}\n\n\\newglossaryentry{damping}\n{\n  name=衰减,\n  description={damping},\n  sort={damping},\n}\n\n\\newglossaryentry{learned}\n{\n  name=学成,\n  description={learned},\n  sort={learned},\n}\n\n\\newglossaryentry{message_passing}\n{\n  name=信息传输,\n  description={message passing},\n  sort={message passing},\n}\n\n\\newglossaryentry{functional_derivative}\n{\n  name=泛函导数,\n  description={functional derivative},\n  sort={functional derivative},\n}\n\n\\newglossaryentry{variational_derivative}\n{\n  name=变分导数,\n  description={variational derivative},\n  sort={variational derivative},\n}\n\n\\newglossaryentry{excess_error}\n{\n  name=额外误差,\n  description={excess error},\n  sort={excess error},\n}\n\n\\newglossaryentry{momentum}\n{\n  name=动量,\n  description={momentum},\n  sort={momentum},\n}\n\n\\newglossaryentry{nmomentum}\n{\n  name=Nesterov 动量,\n  description={Nesterov momentum},\n  sort={Nesterov momentum},\n}\n\n\\newglossaryentry{chaos}\n{\n  name=混沌,\n  description={chaos},\n  sort={chaos},\n}\n\n\\newglossaryentry{normalized_initialization}\n{\n  name=标准初始化, % ??\n  description={normalized initialization},\n  sort={normalized initialization},\n}\n\n\\newglossaryentry{sparse_initialization}\n{\n  name=稀疏初始化,\n  description={sparse initialization},\n  sort={sparse initialization},\n}\n\n\\newglossaryentry{conjugate_directions}\n{\n  name=共轭方向,\n  description={conjugate directions},\n  sort={conjugate directions},\n}\n\n\\newglossaryentry{conjugate}\n{\n  name=共轭,\n  description={conjugate},\n  sort={conjugate},\n}\n\n\\newglossaryentry{conditional_independent}\n{\n  name=条件独立,\n  description={conditionally independent},\n  sort={conditionally independent},\n}\n\n\\newglossaryentry{ensemble_learning}\n{\n  name=集成学习,\n  description={ensemble learning},\n  sort={ensemble learning},\n}\n\n\\newglossaryentry{NICE}\n{\n  name=非线性独立成分估计,\n  description={nonlinear independent components estimation},\n  sort={nonlinear independent components estimation},\n  symbol={NICE}\n}\n\n\\newglossaryentry{ISA}\n{\n  name=独立子空间分析,\n  description={independent subspace analysis},\n  sort={independent subspace analysis},\n}\n\n\\newglossaryentry{SFA}\n{\n  name=慢特征分析,\n  description={slow feature analysis},\n  sort={slow feature analysis},\n  symbol={SFA}\n}\n\n\\newglossaryentry{slow_principle}\n{\n  name=慢性原则,\n  description={slowness principle},\n  sort={slowness principle},\n}\n\n\\newglossaryentry{rectified_linear}\n{\n  name=整流线性,\n  description={rectified linear},\n  sort={rectified linear},\n}\n\n\\newglossaryentry{rectified_linear_transformation}\n{\n  name=整流线性变换,\n  description={rectified linear transformation},\n  sort={rectified linear transformation},\n}\n\n\\newglossaryentry{rectifier_network}\n{\n  name=整流网络,\n  description={rectifier network},\n  sort={rectifier network},\n}\n\n\\newglossaryentry{coordinate_descent}\n{\n  name=坐标下降,\n  description={coordinate descent},\n  sort={coordinate descent},\n}\n\n\\newglossaryentry{coordinate_ascent}\n{\n  name=坐标上升,\n  description={coordinate ascent},\n  sort={coordinate ascent},\n}\n\n\\newglossaryentry{block_coordinate_descent}\n{\n  name=块坐标下降,\n  description={block coordinate descent},\n  sort={block coordinate descent},\n}\n\n\\newglossaryentry{pretraining}\n{\n  name=预训练,\n  description={pretraining},\n  sort={pretraining},\n}\n\n\\newglossaryentry{unsupervised_pretraining}\n{\n  name=无监督预训练,\n  description={unsupervised pretraining},\n  sort={unsupervised pretraining},\n}\n\n\\newglossaryentry{greedy_layer_wise_unsupervised_pretraining}\n{\n  name=贪心逐层无监督预训练,\n  description={greedy layer-wise unsupervised pretraining},\n  sort={greedy layer-wise unsupervised pretraining},\n}\n\n\\newglossaryentry{greedy_layer_wise_pretraining}\n{\n  name=贪心逐层预训练,\n  description={greedy layer-wise pretraining},\n  sort={greedy layer-wise pretraining},\n}\n\n\\newglossaryentry{greedy_layer_wise_training}\n{\n  name=贪心逐层训练,\n  description={greedy layer-wise training},\n  sort={greedy layer-wise training},\n}\n\n\\newglossaryentry{layer_wise}\n{\n  name=逐层的,\n  description={layer-wise},\n  sort={layer-wise},\n}\n\n\\newglossaryentry{greedy_algorithm}\n{\n  name=贪心算法,\n  description={greedy algorithm},\n  sort={greedy algorithm},\n}\n\n\\newglossaryentry{greedy}\n{\n  name=贪心,\n  description={greedy},\n  sort={greedy},\n}\n\n\\newglossaryentry{fine_tuning}\n{\n  name=精调,\n  description={fine-tuning},\n  sort={fine-tuning},\n}\n\n\\newglossaryentry{greedy_supervised_pretraining}\n{\n  name=贪心监督预训练,\n  description={greedy supervised pretraining},\n  sort={greedy supervised pretraining},\n}\n\n\\newglossaryentry{continuation_method}\n{\n  name=延拓法,\n  description={continuation method},\n  sort={continuation method},\n}\n\n\\newglossaryentry{curriculum_learning}\n{\n  name=课程学习,\n  description={curriculum learning},\n  sort={curriculum learning},\n}\n\n\\newglossaryentry{shaping}\n{\n  name=塑造, % ？ 整形\n  description={shaping},\n  sort={shaping},\n}\n\n\\newglossaryentry{stochastic_curriculum}\n{\n  name=随机课程,\n  description={stochastic curriculum},\n  sort={stochastic curriculum},\n}\n\n\\newglossaryentry{recall}\n{\n  name=召回率,\n  description={recall},\n  sort={recall},\n}\n\n\\newglossaryentry{coverage}\n{\n  name=覆盖,\n  description={coverage},\n  sort={coverage},\n}\n\n\\newglossaryentry{hyperparameter_optimization}\n{\n  name=超参数优化,\n  description={hyperparameter optimization},\n  sort={hyperparameter optimization},\n}\n\n\\newglossaryentry{hyperparameter}\n{\n  name=超参数,\n  description={hyperparameter},\n  sort={hyperparameter}\n}\n\n\\newglossaryentry{grid_search}\n{\n  name=网格搜索,\n  description={grid search},\n  sort={grid search},\n}\n\n\\newglossaryentry{finite_difference}\n{\n  name=有限差分,\n  description={finite difference},\n  sort={finite difference},\n}\n\n\\newglossaryentry{centered_difference}\n{\n  name=中心差分,\n  description={centered difference},\n  sort={centered difference},\n}\n\n\\newglossaryentry{e_step}\n{\n  name=E步,\n  description={expectation step},\n  sort={expectation step},\n  symbol={E step}\n}\n\n\\newglossaryentry{m_step}\n{\n  name=M步,\n  description={maximization step},\n  sort={maximization step},\n  symbol={M step}\n}\n\n\\newglossaryentry{euler_lagrange_eqn}\n{\n  name=欧拉-拉格朗日方程,\n  description={Euler-Lagrange Equation},\n  sort={Euler-Lagrange Equation},\n}\n\n\\newglossaryentry{lagrange_multi}\n{\n  name=拉格朗日乘子,\n  description={Lagrange multiplier},\n  sort={Lagrange multiplier},\n}\n\n\\newglossaryentry{ESN}\n{\n  name=回声状态网络,\n  description={echo state network},\n  sort={echo state network},\n  symbol={ESN}\n}\n\n\\newglossaryentry{liquid_state_machines}\n{\n  name=流体状态机,\n  description={liquid state machine},\n  sort={liquid state machine},\n}\n\n\\newglossaryentry{reservoir_computing}\n{\n  name=储层计算,\n  description={reservoir computing},\n  sort={reservoir computing},\n}\n\n\\newglossaryentry{spectral_radius}\n{\n  name=谱半径,\n  description={spectral radius},\n  sort={spectral radius},\n}\n\n\\newglossaryentry{contractive}\n{\n  name=收缩,\n  description={contractive},\n  sort={contractive},\n}\n\n\\newglossaryentry{long_term_dependency}\n{\n  name=长期依赖,\n  description={long-term dependency},\n  sort={long-term dependency},\n}\n\n\\newglossaryentry{skip_connection}\n{\n  name=跳跃连接,\n  description={skip connection},\n  sort={skip connection},\n}\n\n\\newglossaryentry{leaky_unit}\n{\n  name=渗漏单元,\n  description={leaky unit},\n  sort={leaky unit},\n}\n\n\\newglossaryentry{gated_rnn}\n{\n  name=门控RNN,\n  description={gated RNN},\n  sort={gated RNN},\n}\n\n\\newglossaryentry{gated}\n{\n  name=门控,\n  description={gated},\n  sort={gated},\n}\n\n\\newglossaryentry{convolution}\n{\n  name=卷积,\n  description={convolution},\n  sort={convolution},\n}\n\n\\newglossaryentry{input}\n{\n  name=输入,\n  description={input},\n  sort={input},\n}\n\n\\newglossaryentry{input_distribution}\n{\n  name=输入分布,\n  description={input distribution},\n  sort={input distribution},\n}\n\n\\newglossaryentry{output}\n{\n  name=输出,\n  description={output},\n  sort={output},\n}\n\n\\newglossaryentry{feature_map}\n{\n  name=特征映射,\n  description={feature map},\n  sort={feature map},\n}\n\n\\newglossaryentry{flip}\n{\n  name=翻转,\n  description={flip},\n  sort={flip},\n}\n\n\\newglossaryentry{cross_correlation}\n{\n  name=互相关函数,\n  description={cross-correlation},\n  sort={cross-correlation},\n}\n\n\\newglossaryentry{Toeplitz_matrix}\n{\n  name=Toeplitz矩阵,\n  description={Toeplitz matrix},\n  sort={Toeplitz matrix},\n}\n\n\\newglossaryentry{doubly_block_circulant_matrix}\n{\n  name=双重分块循环矩阵,\n  description={doubly block circulant matrix},\n  sort={doubly block circulant matrix},\n}\n\n\\newglossaryentry{sparse_interactions}\n{\n  name=稀疏交互,\n  description={sparse interactions},\n  sort={sparse interactions},\n}\n\n\\newglossaryentry{equivariant_representations}\n{\n  name=等变表示,\n  description={equivariant representations},\n  sort={equivariant representations},\n}\n\n\\newglossaryentry{sparse_connectivity}\n{\n  name=稀疏连接,\n  description={sparse connectivity},\n  sort={sparse connectivity},\n}\n\n\\newglossaryentry{sparse_weights}\n{\n  name=稀疏权重,\n  description={sparse weights},\n  sort={sparse weights},\n}\n\n\\newglossaryentry{receptive_field}\n{\n  name=接受域,\n  description={receptive field},\n  sort={receptive field},\n}\n\n\\newglossaryentry{tied_weights}\n{\n  name=绑定的权重,\n  description={tied weights},\n  sort={tied weights},\n}\n\n\\newglossaryentry{equivariance}\n{\n  name=等变,\n  description={equivariance},\n  sort={equivariance},\n}\n\n\\newglossaryentry{detector_stage}\n{\n  name=探测级,\n  description={detector stage},\n  sort={detector stage},\n}\n\n\\newglossaryentry{symbolic_representation}\n{\n  name=符号表示,\n  description={symbolic representation},\n  sort={symbolic representation},\n}\n\n\\newglossaryentry{pooling_funciton}\n{\n  name=池化函数,\n  description={pooling function},\n  sort={pooling function},\n}\n\n\\newglossaryentry{max_pooling}\n{\n  name=最大池化,\n  description={max pooling},\n  sort={max pooling},\n}\n\n\\newglossaryentry{pool}\n{\n  name=池,\n  description={pool},\n  sort={max pool},\n}\n\n\\newglossaryentry{invariant}\n{\n  name=不变,\n  description={invariant},\n  sort={invariant},\n}\n\n\\newglossaryentry{permutation_invariant}\n{\n  name=置换不变性,\n  description={permutation invariant},\n  sort={permutation invariant},\n}\n\n\\newglossaryentry{stride}\n{\n  name=步幅,\n  description={stride},\n  sort={stride},\n}\n\n\\newglossaryentry{downsampling}\n{\n  name=降采样,\n  description={downsampling},\n  sort={downsampling},\n}\n\n\\newglossaryentry{valid}\n{\n  name=有效,\n  description={valid},\n  sort={valid},\n}\n\n\\newglossaryentry{same}\n{\n  name=相同,\n  description={same},\n  sort={same},\n}\n\n\\newglossaryentry{full}\n{\n  name=全,\n  description={full},\n  sort={full},\n}\n\n\\newglossaryentry{unshared_convolution}\n{\n  name=非共享卷积,\n  description={unshared convolution},\n  sort={unshared convolution},\n}\n\n\\newglossaryentry{tiled_convolution}\n{\n  name=平铺卷积,\n  description={tiled convolution},\n  sort={tiled convolution},\n}\n\n\\newglossaryentry{recurrent_convolutional_network}\n{\n  name=循环卷积网络,\n  description={recurrent convolutional network},\n  sort={recurrent convolutional network},\n}\n\n\\newglossaryentry{Fourier_transform}\n{\n  name=傅立叶变换,\n  description={Fourier transform},\n  sort={Fourier transform},\n}\n\n\\newglossaryentry{separable}\n{\n  name=可分离的,\n  description={separable},\n  sort={separable},\n}\n\n\\newglossaryentry{primary_visual_cortex}\n{\n  name=初级视觉皮层,\n  description={primary visual cortex},\n  sort={primary visual cortex},\n}\n\n\\newglossaryentry{simple_cells}\n{\n  name=简单细胞,\n  description={simple cell},\n  sort={simple cell},\n}\n\n\\newglossaryentry{complex_cells}\n{\n  name=复杂细胞,\n  description={complex cell},\n  sort={complex cell},\n}\n\n\\newglossaryentry{fovea}\n{\n  name=中央凹,\n  description={fovea},\n  sort={fovea},\n}\n\n\\newglossaryentry{saccade}\n{\n  name=扫视,\n  description={saccade},\n  sort={saccade},\n}\n\n\\newglossaryentry{TDNNs}\n{\n  name=时延神经网络,\n  description={time delay neural network},\n  sort={time delay neural network},\n  symbol={TDNN}\n}\n\n\\newglossaryentry{reverse_correlation}\n{\n  name=反向相关,\n  description={reverse correlation},\n  sort={reverse correlation},\n}\n\n\\newglossaryentry{Gabor_function}\n{\n  name=Gabor函数,\n  description={Gabor function},\n  sort={Gabor function},\n}\n\n\\newglossaryentry{quadrature_pair}\n{\n  name=象限对,\n  description={quadrature pair},\n  sort={quadrature pair},\n}\n\n\\newglossaryentry{gated_recurrent_unit}\n{\n  name=门控循环单元,\n  description={gated recurrent unit},\n  sort={gated recurrent unit},\n  symbol={GRU}\n}\n\n\\newglossaryentry{gated_recurrent_net}\n{\n  name=门控循环网络,\n  description={gated recurrent net},\n  sort={gated recurrent net}, \n}\n\n\\newglossaryentry{forget_gate}\n{\n  name=遗忘门,\n  description={forget gate},\n  sort={forget gate},\n}\n\n\\newglossaryentry{clipping_gradient}\n{\n  name=截断梯度,\n  description={clipping the gradient},\n  sort={clipping the gradient},\n}\n\n\\newglossaryentry{memory_network}\n{\n  name=记忆网络,\n  description={memory network},\n  sort={memory network},\n}\n\n\\newglossaryentry{NTM}\n{\n  name=神经网络图灵机,\n  description={neural Turing machine},\n  sort={neural Turing machine},\n  symbol={NTM}\n}\n\n\\newglossaryentry{fine_tune}\n{\n  name=精调,\n  description={fine-tune},\n  sort={fine-tune},\n}\n\n\\newglossaryentry{explaining_away}\n{\n  name=相消解释,\n  description={explaining away},\n  sort={explaining away},\n}\n\n\\newglossaryentry{explaining_away_effect}\n{\n  name=相消解释作用,\n  description={explaining away effect},\n  sort={explaining away effect},\n}\n\n\\newglossaryentry{common_cause}\n{\n  name=共因,\n  description={common cause},\n  sort={common cause},\n}\n\n\\newglossaryentry{code}\n{\n  name=编码,\n  description={code},\n  sort={code},\n}\n\n\\newglossaryentry{recirculation}\n{\n  name=再循环,\n  description={recirculation},\n  sort={recirculation},\n}\n\n\\newglossaryentry{undercomplete}\n{\n  name=欠完备,\n  description={undercomplete},\n  sort={undercomplete},\n}\n\n\\newglossaryentry{complete_graph}\n{\n  name=完全图,\n  description={complete graph},\n  sort={complete graph},\n}\n\n\\newglossaryentry{underdetermined}\n{\n  name=欠定的,\n  description={underdetermined},\n  sort={underdetermined},\n}\n\n\\newglossaryentry{overcomplete}\n{\n  name=过完备,\n  description={overcomplete},\n  sort={overcomplete},\n}\n\n\\newglossaryentry{denoising}\n{\n  name=去噪,\n  description={denoising},\n  sort={denoising},\n}\n\n\\newglossaryentry{denoise}\n{\n  name=去噪,\n  description={denoise}\n}\n\n\\newglossaryentry{DAE}\n{\n  name=去噪自编码器,\n  description={denoising autoencoder},\n  sort={denoising autoencoder},\n  symbol={DAE}\n}\n\n\\newglossaryentry{CAE}\n{\n  name=收缩自编码器,\n  description={contractive autoencoder},\n  sort={contractive autoencoder},\n  symbol={CAE}\n}\n\n\\newglossaryentry{reconstruction_error}\n{\n  name=重构误差,\n  description={reconstruction error},\n  sort={reconstruction error},\n}\n\n\\newglossaryentry{gradient_field}\n{\n  name=梯度场,\n  description={gradient field},\n  sort={gradient field},\n}\n\n\\newglossaryentry{denoising_score_matching}\n{\n  name=去噪得分匹配,\n  description={denoising score matching},\n  sort={denoising score matching},\n}\n\n\\newglossaryentry{score}\n{\n  name=得分,\n  description={score},\n  sort={score},\n}\n\n\\newglossaryentry{tangent_plane}\n{\n  name=切平面,\n  description={tangent plane},\n  sort={tangent plane},\n}\n\n\\newglossaryentry{nearest_neighbor_graph}\n{\n  name=最近邻图,\n  description={nearest neighbor graph},\n  sort={nearest neighbor graph},\n}\n\n\\newglossaryentry{embedding}\n{\n  name=嵌入,\n  description={embedding},\n  sort={embedding},\n}\n\n\\newglossaryentry{PSD}\n{\n  name=预测稀疏分解,\n  description={predictive sparse decomposition},\n  sort={predictive sparse decomposition},\n  symbol={PSD}\n}\n\n\\newglossaryentry{learned_approximate_inference}\n{\n  name=学习近似推断,\n  description={learned approximate inference},\n  sort={learned approximate inference},\n}\n\n\\newglossaryentry{approximate_inference}\n{\n  name=近似推断,\n  description={approximate inference},\n  sort={approximate inference},\n}\n\n\\newglossaryentry{information_retrieval}\n{\n  name=信息检索,\n  description={information retrieval},\n  sort={information retrieval},\n}\n\n\\newglossaryentry{semantic_hashing}\n{\n  name=语义哈希,\n  description={semantic hashing},\n  sort={semantic hashing},\n}\n\n\\newglossaryentry{dimensionality_reduction}\n{\n  name=降维,\n  description={dimensionality reduction},\n  sort={dimensionality reduction},\n}\n\n\\newglossaryentry{helmholtz_machine}\n{\n  name=Helmholtz机,\n  description={Helmholtz machine},\n  sort={Helmholtz machine},\n}\n\n\\newglossaryentry{contrastive_divergence}\n{\n  name=对比散度,\n  description={contrastive divergence},\n  symbol={CD},\n  sort={contrastive divergence},\n}\n\n\\newglossaryentry{language_model}\n{\n  name=语言模型,\n  description={language model},\n  sort={language model}，\n}\n\n\\newglossaryentry{token}\n{\n  name=标记,\n  description={token},\n  sort={token}，\n}\n\n\\newglossaryentry{unigram}\n{\n  name=一元语法,\n  description={unigram},\n  sort={unigram},\n}\n\n\\newglossaryentry{bigram}\n{\n  name=二元语法,\n  description={bigram},\n  sort={bigram},\n}\n\n\\newglossaryentry{trigram}\n{\n  name=三元语法,\n  description={trigram},\n  sort={trigram},\n}\n\n\\newglossaryentry{smoothing}\n{\n  name=平滑,\n  description={smoothing},\n  sort={smoothing},\n}\n\n\\newglossaryentry{NLM}\n{\n  name=神经语言模型,\n  description={Neural Language Model},\n  sort={Neural Language Model},\n  symbol={NLM}\n}\n\n\\newglossaryentry{cascade}\n{\n  name=级联,\n  description={cascade},\n  sort={cascade},\n}\n\n\\newglossaryentry{policy_gradient}\n{\n  name=策略梯度,\n  description={policy gradient},\n  sort={policy gradient},\n}\n\n\\newglossaryentry{DGM}\n{\n  name=深度生成模型,\n  description={deep generative model},\n  sort={deep generative model},\n}\n\n\\newglossaryentry{model}\n{\n  name=模型,\n  description={model},\n  sort={model},\n}\n\n\\newglossaryentry{layer}\n{\n  name=层,\n  description={layer},\n  sort={layer},\n}\n\n\\newglossaryentry{greedy_unsupervised_pretraining}\n{\n  name=贪心无监督预训练,\n  description={greedy unsupervised pretraining},\n  sort={greedy unsupervised pretraining},\n}\n\n\\newglossaryentry{semi_supervised_learning}\n{\n  name=半监督学习,\n  description={semi-supervised learning},\n  sort={semi-supervised learning},\n}\n\n\\newglossaryentry{supervised_model}\n{\n  name=监督模型,\n  description={supervised model},\n  sort={supervised model},\n}\n\n\\newglossaryentry{word_embeddings}\n{\n  name=词嵌入,\n  description={word embedding},\n  sort={word embedding},\n}\n\n\\newglossaryentry{one_hot}\n{\n  name=one-hot,\n  description={one-hot},\n  sort={one-hot},\n}\n\n\\newglossaryentry{supervised_pretraining}\n{\n  name=监督预训练,\n  description={supervised pretraining},\n  sort={supervised pretraining},\n}\n\n\\newglossaryentry{transfer_learning}\n{\n  name=迁移学习,\n  description={transfer learning},\n  sort={transfer learning},\n}\n\n\\newglossaryentry{learner}\n{\n  name=学习器,\n  description={learner},\n  sort={learner},\n}\n\n\\newglossaryentry{multitask_learning}\n{\n  name=多任务学习,\n  description={multitask learning},\n  sort={multitask learning},\n}\n\n\\newglossaryentry{domain_adaption}\n{\n  name=领域自适应,\n  description={domain adaption}  ,\n  sort={domain adaption}  ,\n}\n\n\\newglossaryentry{concept_drift}\n{\n  name=概念漂移,\n  description={concept drift},\n  sort={concept drift},\n}\n\n\\newglossaryentry{one_shot_learning}\n{\n  name=一次学习,\n  description={one-shot learning},\n  sort={one-shot learning},\n}\n\n\\newglossaryentry{zero_shot_learning}\n{\n  name=零次学习,\n  description={zero-shot learning},\n  sort={zero-shot learning},\n}\n\n\\newglossaryentry{zero_data_learning}\n{\n  name=零数据学习,\n  description={zero-data learning},\n  sort={zero-data learning},\n}\n\n\\newglossaryentry{multimodal_learning}\n{\n  name=多模态学习,\n  description={multimodal learning},\n  sort={multimodal learning},\n}\n\n\\newglossaryentry{generative_adversarial_networks}\n{\n  name=生成式对抗网络,\n  description={generative adversarial network},\n  sort={generative adversarial network},\n  symbol={GAN}\n}\n\n\\newglossaryentry{generative_adversarial_framework}\n{\n  name=生成式对抗框架,\n  description={generative adversarial framework},\n  sort={generative adversarial framework},\n}\n\n\\newglossaryentry{GAN}\n{\n  name=生成式对抗网络,\n  description={generative adversarial network},\n  sort={generative adversarial network},\n  symbol={GAN}\n}\n\n\\newglossaryentry{feedforward_classifier}\n{\n  name=前馈分类器,\n  description={feedforward classifier},\n  sort={feedforward classifier},\n}\n\n\\newglossaryentry{sum_product_network}\n{\n  name=和-积网络,\n  description={sum-product network},\n  sort={sum-product network},\n  symbol={SPN}\n}\n\n\\newglossaryentry{deep_circuit}\n{\n  name=深度回路,\n  description={deep circuit},\n  sort={deep circuit},\n}\n\n\\newglossaryentry{shadow_circuit}\n{\n  name=浅度回路,\n  description={shadow circuit},\n  sort={shadow circuit},\n}\n\n\\newglossaryentry{linear_classifier}\n{\n  name=线性分类器,\n  description={linear classifier},\n  sort={linear classifier},\n}\n\n\\newglossaryentry{positive_phase}\n{\n  name=正相,\n  description={positive phase},\n  sort={positive phase},\n}\n\n\\newglossaryentry{negative_phase}\n{\n  name=负相,\n  description={negative phase},\n  sort={negative phase},\n}\n\n\\newglossaryentry{leibniz_rule}\n{\n  name=莱布尼兹法则,\n  description={Leibniz's rule},\n  sort={Leibniz's rule},\n}\n\n\\newglossaryentry{lebesgue_integrable}\n{\n  name=勒贝格可积,\n  description={Lebesgue-integrable},\n  sort={Lebesgue-integrable},\n}\n\n\\newglossaryentry{spurious_modes}\n{\n  name=虚假模态,\n  description={spurious modes},\n  sort={spurious modes},\n}\n\n\\newglossaryentry{SML}\n{\n  name=随机最大似然,\n  symbol={SML},\n  description={stochastic maximum likelihood},\n  sort={stochastic maximum likelihood},\n}\n\n\\newglossaryentry{persistent_contrastive_divergence}\n{\n  name=持续性对比散度,\n  description={persistent contrastive divergence},\n  sort={persistent contrastive divergence},\n  symbol={PCD}\n}\n\n\\newglossaryentry{FPCD}\n{\n  name=快速持续性对比散度,\n  symbol={FPCD},\n  description={fast persistent contrastive divergence},\n  sort={fast persistent contrastive divergence},\n}\n\n\\newglossaryentry{pseudolikelihood}\n{\n  name=伪似然,\n  description={pseudolikelihood},\n  sort={pseudolikelihood},\n}\n\n\\newglossaryentry{generalized_pseudolikelihood_estimator}\n{\n  name=广义伪似然估计,\n  description={generalized pseudolikelihood estimator},\n  sort={generalized pseudolikelihood estimator},\n}\n\n\\newglossaryentry{generalized_pseudolikelihood}\n{\n  name=广义伪似然,\n  description={generalized pseudolikelihood},\n  sort={generalized pseudolikelihood},\n}\n\n\\newglossaryentry{GSM}\n{\n  name=广义得分匹配,\n  description={generalized score matching},\n  sort={generalized score matching},\n  symbol={GSM}\n}\n\n\\newglossaryentry{ratio_matching}\n{\n  name=比率匹配,\n  description={ratio matching},\n  sort={ratio matching},\n}\n\n\\newglossaryentry{NCE}\n{\n  name=噪声对比估计,\n  description={noise-contrastive estimation},\n  sort={noise-contrastive estimation},\n  symbol={NCE}\n}\n\n\\newglossaryentry{noise_distribution}\n{\n  name=噪声分布,\n  description={noise distribution},\n  sort={noise distribution},\n}\n\n\\newglossaryentry{noise}\n{\n  name=噪声,\n  description={noise},\n  sort={noise},\n}\n\n\\newglossaryentry{self_contrastive_estimation}\n{\n  name=自对比估计,\n  description={self-contrastive estimation},\n  sort={self-contrastive estimation},\n}\n\n\\newglossaryentry{iid_chap18}\n{\n  name=独立同分布,\n  description={independent identically distributed},\n  sort={independent identically distributed},\n  symbol={i.i.d.}\n}\n\n\\newglossaryentry{AIS}\n{\n  name=退火重要采样,\n  description={annealed importance sampling},\n  sort={annealed importance sampling},\n  symbol={AIS}\n}\n\n\\newglossaryentry{bridge_sampling}\n{\n  name=桥式采样,\n  description={bridge sampling},\n  sort={bridge sampling},\n}\n\n\\newglossaryentry{linked_importance_sampling}\n{\n  name=链接重要采样,\n  description={linked importance sampling},\n  sort={linked importance sampling},\n}\n\n\\newglossaryentry{ASIC}\n{\n  name=专用集成电路,\n  description={application-specific integrated circuit},\n  sort={application-specific integrated circuit},\n  symbol={ASIC}\n}\n\n\\newglossaryentry{FPGA}\n{\n  name=现场可编程门阵列,\n  description={field programmable gated array},\n  sort={field programmable gated array},\n  symbol={FPGA}\n}\n\n\\newglossaryentry{scalar}\n{\n  name=标量,\n  description={scalar},\n  sort={scalar},\n}\n\n\\newglossaryentry{vector}\n{\n  name=向量,\n  description={vector},\n  sort={vector},\n}\n\n\\newglossaryentry{matrix}\n{\n  name=矩阵,\n  description={matrix},\n  sort={matrix},\n}\n\n\\newglossaryentry{tensor}\n{\n  name=张量,\n  description={tensor},\n  sort={tensor},\n}\n\n\\newglossaryentry{dot_product}\n{\n  name=点积,\n  description={dot product},\n  sort={dot product},\n}\n\n\\newglossaryentry{inner_product}\n{\n  name=内积,\n  description={inner product},\n  sort={inner product}\n}\n\n\\newglossaryentry{square}\n{\n  name=方阵,\n  description={square},\n  sort={square},\n}\n\n\\newglossaryentry{singular}\n{\n  name=奇异的,\n  description={singular},\n  sort={singular},\n}\n\n\\newglossaryentry{norm}\n{\n  name=范数,\n  description={norm},\n  sort={norm},\n}\n\n\\newglossaryentry{triangle_inequality}\n{\n  name=三角不等式,\n  description={triangle inequality},\n  sort={triangle inequality},\n}\n\n\\newglossaryentry{euclidean_norm}\n{\n  name=欧几里得范数,\n  description={Euclidean norm},\n  sort={Euclidean norm},\n}\n\n\\newglossaryentry{max_norm}\n{\n  name=最大范数,\n  description={max norm},\n  sort={max norm},\n}\n\n\\newglossaryentry{frobenius_norm}\n{\n  name=Frobenius 范数,\n  description={Frobenius norm},\n  sort={Frobenius norm},\n}\n\n\\newglossaryentry{diagonal_matrix}\n{\n  name=对角矩阵,\n  description={diagonal matrix},\n  sort={diagonal matrix},\n}\n\n\\newglossaryentry{symmetric}\n{\n  name=对称,\n  description={symmetric},\n  sort={symmetric},\n}\n\n\\newglossaryentry{unit_vector}\n{\n  name=单位向量,\n  description={unit vector},\n  sort={unit vector},\n}\n\n\\newglossaryentry{unit_norm}\n{\n  name=单位范数,\n  description={unit norm},\n  sort={unit norm},\n}\n\n\\newglossaryentry{orthogonal}\n{\n  name=正交,\n  description={orthogonal},\n  sort={orthogonal},\n}\n\n\\newglossaryentry{orthogonal_matrix}\n{\n  name=正交矩阵,\n  description={orthogonal matrix},\n  sort={orthogonal matrix},\n}\n\n\\newglossaryentry{orthonormal}\n{\n  name=标准正交,\n  description={orthonormal},\n  sort={orthonormal},\n}\n\n\\newglossaryentry{eigendecomposition}\n{\n  name=特征分解,\n  description={eigendecomposition},\n  sort={eigendecomposition},\n}\n\n\\newglossaryentry{eigenvector}\n{\n  name=特征向量,\n  description={eigenvector},\n  sort={eigenvector},\n}\n\n\\newglossaryentry{eigenvalue}\n{\n  name=特征值,\n  description={eigenvalue},\n  sort={eigenvalue},\n}\n\n\\newglossaryentry{decompose}\n{\n  name=分解,\n  description={decompose},\n  sort={decompose},\n}\n\n\\newglossaryentry{P_D}\n{\n  name=正定,\n  description={positive definite},\n  sort={positive definite},\n}\n\n\\newglossaryentry{ND}\n{\n  name=负定,\n  description={negative definite},\n  sort={negative definite},\n}\n\n\\newglossaryentry{NSD}\n{\n  name=半负定,\n  description={negative semidefinite},\n  sort={negative semidefinite},\n}\n\n\\newglossaryentry{P_SD}\n{\n  name=半正定,\n  description={positive semidefinite},\n  sort={positive semidefinite},\n}\n\n\\newglossaryentry{SVD}\n{\n  name=奇异值分解,\n  description={singular value decomposition},\n  sort={singular value decomposition},\n  symbol={SVD}\n}\n\n\\newglossaryentry{Svalue}\n{\n  name=奇异值,\n  description={singular value},\n  sort={singular value},\n}\n\n\\newglossaryentry{Svector}\n{\n  name=奇异向量,\n  description={singular vector},\n  sort={singular vector},\n}\n\n\\newglossaryentry{left_Svector}\n{\n  name=左奇异向量,\n  description={left singular vector},\n  sort={left singular vector},\n}\n\n\\newglossaryentry{right_Svector}\n{\n  name=右奇异向量,\n  description={right singular vector},\n  sort={right singular vector},\n}\n\n\\newglossaryentry{left_Evector}\n{\n  name=左特征向量,\n  description={left eigenvector},\n  sort={left eigenvector},\n}\n\n\\newglossaryentry{right_Evector}\n{\n  name=右特征向量,\n  description={right eigenvector},\n  sort={right eigenvector},\n}\n\n\\newglossaryentry{Moore}\n{\n  name=Moore-Penrose 伪逆,\n  description={Moore-Penrose pseudoinverse},\n  sort={Moore-Penrose pseudoinverse},\n}\n\n\\newglossaryentry{identity_matrix}\n{\n  name=单位矩阵,\n  description={identity matrix},\n  sort={identity matrix},\n}\n\n\\newglossaryentry{matrix_inverse}\n{\n  name=矩阵逆,\n  description={matrix inversion},\n  sort={matrix inversion},\n}\n\n\\newglossaryentry{origin}\n{\n  name=原点,\n  description={origin},\n  sort={origin},\n}\n\n\\newglossaryentry{linear_combination}\n{\n  name=线性组合,\n  description={linear combination},\n  sort={linear combination},\n}\n\n\\newglossaryentry{column_space}\n{\n  name=列空间,\n  description={column space},\n  sort={column space},\n}\n\n\\newglossaryentry{range}\n{\n  name=值域,\n  description={range},\n  sort={range},\n}\n\n\\newglossaryentry{linear_depend}\n{\n  name=线性相关,\n  description={linear dependency},\n  sort={linear dependency},\n}\n\n\\newglossaryentry{linear_dependence}\n{\n  name=线性相关,\n  description={linear dependence},\n  sort={linear dependence}\n}\n\n\\newglossaryentry{linearly_independent}\n{\n  name=线性无关,\n  description={linearly independent},\n  sort={linearly independent}\n}\n\n\\newglossaryentry{column}\n{\n  name=列,\n  description={column},\n  sort={column},\n}\n\n\\newglossaryentry{row}\n{\n  name=行,\n  description={row},\n  sort={row},\n}\n\n\\newglossaryentry{span}\n{\n  name=生成子空间,\n  description={span},\n  sort={span},\n}\n\n\\newglossaryentry{SLT}\n{\n  name=统计学习理论,\n  description={statistical learning theory},\n  sort={statistical learning theory},\n}\n\n\\newglossaryentry{DGP}\n{\n  name=数据生成过程,\n  description={data generating process},\n  sort={data generating process},\n}\n\n\\newglossaryentry{iid}\n{\n  name=独立同分布假设,\n  description={i.i.d. assumption},\n  sort={i.i.d. assumption},\n}\n\n\\newglossaryentry{id}\n{\n  name=同分布的,\n  description={identically distributed},\n  sort={identically distributed},\n}\n\n\\newglossaryentry{DGD}\n{\n  name=数据生成分布,\n  description={data generating distribution},\n  sort={data generating distribution},\n}\n\n\\newglossaryentry{large_learning_step}\n{\n  name=大学习步骤,\n  description={large learning step},\n  sort={large learning step},\n}\n\n\\newglossaryentry{variable_elimination}\n{\n  name=变量消去,\n  description={variable elimination},\n  sort={variable elimination},\n}\n\n\\newglossaryentry{OR}\n{\n  name=奥卡姆剃刀,\n  description={Occam's razor},\n  sort={Occam's razor},\n}\n\n\\newglossaryentry{VC}\n{\n  name=Vapnik-Chervonenkis维度,\n  description={Vapnik-Chervonenkis dimension},\n  sort={Vapnik-Chervonenkis dimension},\n  symbol={VC}\n}\n\n\\newglossaryentry{unsupervised_learning_algorithm}\n{\n  name=无监督学习算法,\n  description={unsupervised learning algorithm},\n  sort={unsupervised learning algorithm},\n}\n\n\\newglossaryentry{supervised_learning_algorithm}\n{\n  name=监督学习算法,\n  description={supervised learning algorithm},\n  sort={supervised learning algorithm},\n}\n\n\\newglossaryentry{word_embedding}\n{\n  name=词嵌入,\n  description={word embedding},\n  sort={word embedding},\n}\n\n\\newglossaryentry{shortlist}\n{\n  name=短列表,\n  description={shortlist},\n  sort={shortlist},\n}\n\n\\newglossaryentry{NMT}\n{\n  name=神经机器翻译,\n  description={Neural Machine Translation},\n  sort={Neural Machine Translation},\n  symbol={NMT}\n}\n\n\\newglossaryentry{machine_translation}\n{\n  name=机器翻译,\n  description={machine translation},\n  sort={machine translation}\n}\n\n\\newglossaryentry{recommender_system}\n{\n  name=推荐系统,\n  description={recommender system},\n  sort={recommender system},\n}\n\n\\newglossaryentry{proposal_distribution}\n{\n  name=提议分布,\n  description={proposal distribution},\n  sort={proposal distribution},\n}\n\n\\newglossaryentry{bag_of_words}\n{\n  name=词袋,\n  description={bag of words},\n  sort={bag of words},\n}\n\n\\newglossaryentry{collaborative_filtering}\n{\n  name=协同过滤,\n  description={collaborative filtering},\n  sort={collaborative filtering},\n}\n\n\\newglossaryentry{exploration}\n{\n  name=探索,\n  description={exploration},\n  sort={exploration},\n}\n\n\\newglossaryentry{exploitation}\n{\n  name=利用,\n  description={exploitation},\n  sort={exploitation},\n}\n\n\\newglossaryentry{bandit}\n{\n  name=bandit ,\n  description={bandit},\n  sort={bandit},\n}\n\n\\newglossaryentry{contextual_bandit}\n{\n  name=contextual bandit ,\n  description={contextual bandit},\n  sort={contextual bandit},\n}\n\n\\newglossaryentry{policy}\n{\n  name=策略,\n  description={policy},\n  sort={policy},\n}\n\n\\newglossaryentry{relation}\n{\n  name=关系,\n  description={relation},\n  sort={relation},\n}\n\n\\newglossaryentry{binary_relation}\n{\n  name=二元关系,\n  description={binary relation},\n  sort={binary relation},\n}\n\n\\newglossaryentry{attribute}\n{\n  name=属性,\n  description={attribute},\n  sort={attribute},\n}\n\n\\newglossaryentry{relational_database}\n{\n  name=关系型数据库,\n  description={relational database},\n  sort={relational database},\n}\n\n\\newglossaryentry{link_prediction}\n{\n  name=链接预测,\n  description={link prediction},\n  sort={link prediction},\n}\n\n\\newglossaryentry{word_sense_disambiguation}\n{\n  name=词义消歧,\n  description={word-sense disambiguation},\n  sort={word-sense disambiguation},\n}\n\n\\newglossaryentry{error_metric}\n{\n  name=误差度量,\n  description={error metric},\n  sort={error metric},\n}\n\n\\newglossaryentry{performance_metrics}\n{\n  name=性能度量,\n  description={performance metrics},\n  sort={performance metrics},\n}\n\n\\newglossaryentry{transcription_system}\n{\n  name=转录系统,\n  description={transcription system},\n  sort={transcription system},\n}\n\n\\newglossaryentry{BFGS}\n{\n  name=BFGS,\n  description={BFGS},\n  sort={BFGS},\n}\n\n\\newglossaryentry{LBFGS}\n{\n  name=L-BFGS,\n  description={L-BFGS},\n  sort={L-BFGS},\n}\n\n\\newglossaryentry{CG}\n{\n  name=共轭梯度,\n  description={conjugate gradient},\n  sort={conjugate gradient},\n  symbol={CG}\n}\n\n\\newglossaryentry{nonlinear_CG}\n{\n  name=非线性共轭梯度,\n  description={nonlinear conjugate gradients},\n  sort={nonlinear conjugate gradients},\n}\n\n\\newglossaryentry{bayesian_inference}\n{\n  name=贝叶斯推断,\n  description={Bayesian inference},\n  sort={Bayesian inference},\n}\n\n\\newglossaryentry{online_learning}\n{\n  name=在线学习,\n  description={online learning},\n  sort={online learning},\n}\n\n\\newglossaryentry{layer_wise_pretraining}\n{\n  name=逐层预训练,\n  description={layer-wise pretraining},\n  sort={layer-wise pretraining},\n}\n\n\\newglossaryentry{MPDBM}\n{\n  name=多预测深度玻尔兹曼机,\n  description={multi-prediction deep Boltzmann machine},\n  sort={multi-prediction deep Boltzmann machine},\n  symbol={MP-DBM}\n}\n\n\\newglossaryentry{GBRBM}\n{\n  name=Gaussian-Bernoulli RBM,\n  description={Gaussian-Bernoulli RBM},\n  sort={Gaussian-Bernoulli RBM},\n  symbol={GB-RBM}\n}\n\n\\newglossaryentry{gaussian_rbm}\n{\n  name=高斯RBM,\n  description={Gaussian RBM},\n  sort={Gaussian RBM}\n}\n\n\\newglossaryentry{mcrbm}\n{\n  name=均值和协方差RBM,\n  description={mean and covariance RBM},\n  sort={mean and covariance RBM},\n  symbol={mcRBM},\n}\n\n\\newglossaryentry{mcrbm2}\n{\n  name=均值-协方差RBM,\n  description={mean-covariance restricted Boltzmann machine},\n  sort={mean-covariance restricted Boltzmann machine},\n  symbol={mcRBM2},\n}\n\n\\newglossaryentry{crbm}\n{\n  name=协方差RBM,\n  description={covariance RBM},\n  sort={covariance RBM},\n  symbol={cRBM},\n}\n\n\\newglossaryentry{mpot}\n{\n  name=学生$t$分布均值乘积,\n  description={mean product of Student t-distribution},\n  sort={mean product of Student t-distribution},\n  symbol={mPoT},\n}\n\n\\newglossaryentry{ssrbm}\n{\n  name=尖峰和平板RBM,\n  description={spike and slab RBM},\n  sort={spike and slab RBM},\n  symbol={ssRBM},\n}\n\n\\newglossaryentry{gamma_distribution}\n{\n  name=Gamma分布,\n  description={Gamma distribution},\n  sort={Gamma distribution},\n}\n\n\\newglossaryentry{convolutional_bm}\n{\n  name=卷积玻尔兹曼机 ,\n  description={convolutional Boltzmann machine},\n  sort={convolutional Boltzmann machine},\n}\n\n\\newglossaryentry{reparametrization_trick}\n{\n  name=重参数化技巧,\n  description={reparametrization trick},\n  sort={reparametrization trick},\n}\n\n\\newglossaryentry{reparametrization}\n{\n  name=重参数化,\n  description={reparametrization},\n  sort={reparametrization},\n}\n\n\\newglossaryentry{variance_reduction}\n{\n  name=方差减小,\n  description={variance reduction},\n  sort={variance reduction},\n}\n\n\\newglossaryentry{sigmoid_bn}\n{\n  name=sigmoid信念网络,\n  description={sigmoid Belief Network},\n  sort={sigmoid Belief Network},\n}\n\n\\newglossaryentry{auto_regressive_network}\n{\n  name=自回归网络,\n  description={auto-regressive network},\n  sort={auto-regressive network}\n}\n\n\\newglossaryentry{generator_network}\n{\n  name=生成器网络,\n  description={generator network},\n  sort={generator network}\n}\n\n\\newglossaryentry{discriminator_network}\n{\n  name=判别器网络,\n  description={discriminator network},\n  sort={discriminator network},\n}\n\n\\newglossaryentry{generative_moment_matching_network}\n{\n  name=生成矩匹配网络,\n  description={generative moment matching network},\n  sort={generative moment matching network},\n}\n\n\\newglossaryentry{moment_matching}\n{\n  name=矩匹配,\n  description={moment matching},\n  sort={moment matching},\n}\n\n\\newglossaryentry{moment}\n{\n  name=矩,\n  description={moment},\n  sort={moment},\n}\n\n\\newglossaryentry{MMD}\n{\n  name=最大平均偏差,\n  description={maximum mean discrepancy},\n  sort={maximum mean discrepancy},\n  symbol={MMD}\n}\n\n\\newglossaryentry{linear_auto_regressive_network}\n{\n  name=线性自回归网络,\n  description={linear auto-regressive network},\n  sort={linear auto-regressive network}\n}\n\n\\newglossaryentry{neural_auto_regressive_network}\n{\n  name=神经自回归网络,\n  description={neural auto-regressive network},\n  sort={neural auto-regressive network}\n}\n\n\\newglossaryentry{NADE}\n{\n  name=神经自回归密度估计器,\n  description={neural auto-regressive density estimator},\n  sort={neural auto-regressive density estimator},\n  symbol={NADE}\n}\n\n\\newglossaryentry{detailed_balance}\n{\n  name=细致平衡,\n  description={detailed balance},\n  sort={detailed balance},\n}\n\n\\newglossaryentry{ABC}\n{\n  name=近似贝叶斯计算,\n  description={approximate Bayesian computation},\n  sort={approximate Bayesian computationA},\n  symbol={ABC}\n}\n\n\\newglossaryentry{visible_layer}\n{\n  name=可见层,\n  description={visible layer},\n  sort={visible layer},\n}\n\n\\newglossaryentry{infinite}\n{\n  name=无限,\n  description={infinite},\n  sort={infinite},\n}\n\n\\newglossaryentry{deep_model}\n{\n  name=深度模型,\n  description={deep model},\n  sort={deep model},\n}\n\n\\newglossaryentry{deep_network}\n{\n  name=深度网络,\n  description={deep network},\n  sort={deep network},\n}\n\n\\newglossaryentry{tolerance}\n{\n  name=容差,\n  description={tolerance},\n  sort={tolerance},\n}\n\n\\newglossaryentry{learning_rate}\n{\n  name=学习率,\n  description={learning rate},\n  sort={learning rate},\n}\n\n\\newglossaryentry{ss}\n{\n  name=尖峰和平板,\n  description={spike and slab},\n  sort={spike and slab},\n}\n\n\\newglossaryentry{context_specific_independence}\n{\n  name=特定环境下的独立,\n  description={context-specific independences},\n  sort={context-specific independences},\n}\n\n\\newglossaryentry{coparent}\n{\n  name=共父,\n  description={coparent},\n  sort={coparent},\n}\n\n\\newglossaryentry{srbm}\n{\n  name=半受限玻尔兹曼机,\n  description={semi-restricted Boltzmann Machine},\n  sort={semi-restricted Boltzmann Machine},\n}\n\n\\newglossaryentry{underfit_regime}\n{\n  name=欠拟合机制,\n  description={underfitting regime},\n  sort={underfitting regime},\n}\n\n\\newglossaryentry{overfit_regime}\n{\n  name=过拟合机制,\n  description={overfitting regime},\n  sort={overfitting regime},\n}\n\n\\newglossaryentry{optimal_capacity}\n{\n  name=最佳容量,\n  description={optimal capacity},\n  sort={optimal capacity},\n}\n\n\\newglossaryentry{error_bar}\n{\n  name=误差条,\n  description={error bar},\n  sort={error bar},\n}\n\n\\newglossaryentry{vstructure}\n{\n  name=V-结构,\n  description={V-structure},\n  sort={V-structure},\n}\n\n\\newglossaryentry{collider}\n{\n  name=碰撞情况,\n  description={the collider case},\n  sort={the collider case},\n}\n\n\\newglossaryentry{epochs}\n{\n  name=轮数,\n  description={epochs},\n  sort={epochs},\n}\n\n\\newglossaryentry{epoch}\n{\n  name=轮,\n  description={epoch},\n  sort={epoch},\n}\n\n\\newglossaryentry{logarithmic_scale}\n{\n  name=对数尺度,\n  description={logarithmic scale},\n  sort={logarithmic scale}\n}\n\n\\newglossaryentry{random_search}\n{\n  name=随机搜索,\n  description={random search},\n  sort={random search}\n}\n\n\\newglossaryentry{closed_form_solution}\n{\n  name=闭式解,\n  description={closed form solution},\n  sort={closed form solution}\n}\n\n\\newglossaryentry{object_recognition}\n{\n  name=对象识别,\n  description={object recognition},\n  sort={object recognition}\n}\n\n\\newglossaryentry{piecewise}\n{\n  name=分段,\n  description={piecewise},\n  sort={piecewise},\n}\n\n\\newglossaryentry{alternative_splicing_dataset}\n{\n  name=选择性剪接数据集,\n  description={alternative splicing dataset},\n  sort={alternative splicing dataset},\n}\n\n\\newglossaryentry{hamming_distance}\n{\n  name=汉明距离,\n  description={Hamming distance},\n  sort={Hamming distance}\n}\n\n\\newglossaryentry{visible_variable}\n{\n  name=可见变量,\n  description={visible variable},\n  sort={visible variable},\n}\n\n\\newglossaryentry{approxi_inference}\n{\n  name=近似推断,\n  description={approximate inference},\n  sort={approximate inference},\n}\n\n\\newglossaryentry{exact_inference}\n{\n  name=精确推断,\n  description={exact inference},\n  sort={exact inference},\n}\n\n\\newglossaryentry{latent}\n{\n  name=潜在,\n  description={latent},\n  sort={latent},\n}\n\n\\newglossaryentry{latent_layer}\n{\n  name=潜层,\n  description={latent layer},\n  sort={latent layer},\n}\n\n\\newglossaryentry{knowledge_graph}\n{\n  name=知识图谱,\n  description={knowledge graph},\n  sort={knowledge graph},\n}\n\n\\newglossaryentry{factors_of_variation}\n{\n  name=变差因素,\n  description={factors of variation},\n  sort={factors of variation},\n}\n\n\\newglossaryentry{isomap}\n{\n  name=Isomap,\n  description={Isomap},\n  sort={isomap},\n}\n"
        },
        {
          "name": "website.tex",
          "type": "blob",
          "size": 0.3,
          "content": "% !Mode:: \"TeX:UTF-8\"\n\\chapter*{网站}\n\\addcontentsline{toc}{chapter}{网站}\n\n\\centerline{www.deeplearningbook.org}\n\n\\ \\\\\n\\ \\\\\n\\ \\\\\n\n这本书伴随有上述网站。\n网站提供了各种补充材料，包括练习、讲义幻灯片、错误更正以及其他应该对读者和讲师有用的资源。\n"
        }
      ]
    }
  ]
}